@article{SKOREKOSIKOWSKA2015421,
title = {A comparative thermodynamic, economic and risk analysis concerning implementation of oxy-combustion power plants integrated with cryogenic and hybrid air separation units},
journal = {Energy Conversion and Management},
volume = {92},
pages = {421-430},
year = {2015},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2014.12.079},
url = {https://www.sciencedirect.com/science/article/pii/S0196890414011182},
author = {Anna Skorek-Osikowska and Łukasz Bartela and Janusz Kotowicz},
keywords = {Oxy-fuel combustion, Hybrid membrane–cryogenic ASU, Thermodynamic analysis, Economic analysis, Risk analysis},
abstract = {This paper presents a comparison of two types of oxy-combustion power plant that differ from each other in terms of the method of oxygen separation. For the purpose of the analysis, detailed thermodynamic models of oxy-fuel power plants with gross power of approximately 460MW were built. In the first variant (Case 1), the plant is integrated with a cryogenic air separation unit (ASU). In the second variant (Case 2), the plant is integrated with a hybrid membrane–cryogenic installation. The models were built and optimized using the GateCycle, Aspen Plus and Aspen Custom Modeller software packages and with the use of our own computational codes. The results of the thermodynamic evaluation of the systems, which primarily uses indicators such as the auxiliary power and efficiencies of the whole system and of the individual components that constitute the unit, are presented. Better plant performance is observed for Case 2, which has a net efficiency of electricity generation that is 1.1 percentage points greater than that of Case 1. For the selected structure of the system, an economic analysis of the solutions was made. This analysis accounts for different scenarios of the functioning of the Emission Trading Scheme and includes detailed estimates of the investment costs in both cases. As an indicator of profitability, the break-even price of electricity was used primarily. The results of the analysis for the assumptions made are presented in this paper. A system with a hybrid air separation unit has slightly better economic performance. The break-even price of electricity in this case is approximately 3.4€/MWh less than for the system with a cryogenic unit. The main risk factors concerning implementation of such technologies were identified. The analysis of risk connected with the selection of these technologies was performed with the use of two methods – sensitivity analysis and a Monte Carlo method. The values of the probability were calculated, and the main results are presented and discussed in the paper.}
}
@article{SARTI1999217,
title = {A physically based model to simulate maxillo-facial surgery from 3D CT images},
journal = {Future Generation Computer Systems},
volume = {15},
number = {2},
pages = {217-221},
year = {1999},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(98)00065-X},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X9800065X},
author = {Alessandro Sarti and Roberto Gori and Claudio Lamberti},
keywords = {Virtual surgery, Surgery planning, Tissue displacement simulation},
abstract = {Computer-based surgery simulation represents a rapidly emerging and increasingly important area of research that combines a number of disciplines for the common purpose of improving health care. Generally, the goal of computer-based surgery simulation is to enable a surgeon to experiment with different surgical procedures in an artificial environment. This paper describes an approach for elastic modelling of human tissue based on the use of embedded boundary condition techniques. Embedded boundary condition models allow to simulate the cranio-facial surgery directly on the grid of the 3D CT image of the patient. Previously simulated operations have been performed using surface models or by using a low detailed model of the tissue volume. The approach proposed here involves complete 3D modelling of the solid highly detailed structure of the object, starting from the information present in the 3D diagnostic images. Due to the huge amount of data and the computational complexity of the problem, a parallel version of the software has been implemented on the supercomputer CRAY T3E. The application of this approach for modelling the elastic deformation of human tissue in response to movement of bones is demonstrated both on the visible human data set of the National Library of Medicine and on the CT data set of real patients.}
}
@incollection{SCHMID1982101,
title = {DESIGN OF DIGITAL STATE-FEEDBACK CONTROLLERS USING A CAD PACKAGE},
editor = {A.K. MAHALANABIS},
booktitle = {Theory and Application of Digital Control},
publisher = {Pergamon},
pages = {101-108},
year = {1982},
isbn = {978-0-08-027618-2},
doi = {https://doi.org/10.1016/B978-0-08-027618-2.50022-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008027618250022X},
author = {Chr. Schmid and G. Juen},
abstract = {CAD methods for multivariable discrete-time optimal state-feedback control systems including integral output feedback and observers with known and/or unknown inputs are discussed. Depending on the performance index different combinations of feedback and feedforward controllers are obtained. The effects of measured and observed disturbances and set point changes are shown by simulation studies of a 14th order steam generator subsystem. The implementation of theory, algorithms and software is sketched.}
}
@article{VIEIRA201521,
title = {On Different Approaches to Simulate the Mechanical Behavior of Scaffolds during Degradation},
journal = {Procedia Engineering},
volume = {110},
pages = {21-28},
year = {2015},
note = {4th International Conference on Tissue Engineering, ICTE2015, An ECCOMAS Thematic Conference},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815012461},
author = {André C. Vieira and Rui M. Guedes and Volnei Tita},
keywords = {biodegradable polymers, hydrolytic degradation, constitutive models, simulation, mechanical behaviour},
abstract = {Only using biodegradable polymers is possible to develop new regenerative concepts for implantable biomedical devices, to be used in tissue engineering, where the biomaterials will temporarily replace the biomechanical functions, and these will be gradually transferred to the neo-tissue formed over the scaffolds, while the materials will degrade and ultimately erode and be assimilated by the host tissue. These types of scaffold concepts find many applications, in the market or under study, for the regeneration of non vascular tissues, such as cartilage, vascular stents, ligaments, etc. However, there still is a lack of design and dimensioning tools for these devices and methods to predict and simulate the mechanical behavior during hydrolytic degradation. Many times, the convergence to an optimal solution is obtained by iterative “trial and error”, becoming a costly project. It is common to use Finite Element Methods (FEM) in problems with complex geometries and boundary conditions, enabling the simulation of the 3D mechanical behavior of the device in the initial step of degradation. In the ambit of this context, the main scope of this work is to review the current methodologies able to simulate the mechanical behavior in biodegradable polymers, during several steps of degradation. Hence, the convergence to an optimal solution can be obtained computationally, through material models implemented in a FEM software package, such as ABAQUS. Ideally, the device should degrade its mechanical properties compatibly with the required life cycle and according to the regeneration time of the biologic tissue. Therefore, in this work the equations commonly used to describe the diffusion of water and hydrolysis kinetics will be reviewed. Furthermore, constitutive models commonly used to predict the mechanical behavior of polymers are also reviewed. Due to the nonlinear nature of the stress vs. strain relation, the classical linear elastic model is not valid for simulation under large strains. Current designs of biodegradable devices are carried out by considering hyperelastic or elastoplastic behavior and neglect any changing on the mechanical behavior with degradation. Concomitantly to its nonlinear nature, the mechanical behavior of polymeric materials is also time dependent. The mechanical behavior of polymers, under large deformations and dynamic loading at varying strain rates, is a combination of elastoplastic behavior, typical of metals, and a viscous behavior typical of fluids. Different combinations of hyperelastic, plastic and viscous models can be used to describe their mechanical behavior. Since this mechanical behavior will evolve during degradation, recent approaches that will be reviewed in this work, enable to associate the evolution of material model parameters with the hydrolysis kinetics and therefore simulate the mechanical behavior of biodegradable structures during its hydrolytic degradation.}
}
@article{JUHASZ198327,
title = {PCDB — A Process Control Data Base Management System},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {3},
pages = {27-30},
year = {1983},
note = {12th IFAC/IFIP Workshop on Real Time Programming 1983, Hatfield, UK, 29-31 March 1983},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)62593-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701762593X},
author = {G. Juhász and K. Kovács and I. Sári},
keywords = {Data base, data handling, data processing, minicomputers, process control},
abstract = {Complex systems for process control applications demand more powerful data structures and file organization capabilities than provided in traditional file access services. Data base management technology developed in the past decade offers a solution to this problem. However, commercially available software packages usually require large configuration and they are mainly business oriented. PCDB has been developed as a tool for systems engineers and programmers. Its application simplifies program development and maintenance process control systems. PCDB provides uniform data handling for disc resident as well as core resident data. PCDB is implemented on DEC PDP-11 compatible (Hungarian made) TPA 1140 and TPA 1148 minicomputers.}
}
@article{ELMOURSI199475,
title = {Algorithms and applications of nonlinear parameter estimation of orthogonal regression models},
journal = {Advances in Engineering Software},
volume = {21},
number = {2},
pages = {75-85},
year = {1994},
issn = {0965-9978},
doi = {https://doi.org/10.1016/0965-9978(94)90034-5},
url = {https://www.sciencedirect.com/science/article/pii/0965997894900345},
author = {Alsaied K.M. Elmoursi and Helmut Gfrerer},
keywords = {nonlinear least squares, orthogonal regression, Gauss-Newton method, sparse matrix, Marquardt regularization, orthogonal transformation},
abstract = {In many applications in science and engineering, there arise some curve fitting or parameter estimation problems which cannot be solved by classical methods. In the classical regression model, only the observations of the dependent variables (system response) are subject to some uncertainties. However, in many practical applications, the independent variables are also observed with errors, which cannot be neglected. This leads to the so-called orthogonal regression model which increases the dimensions of the problem extremely. In this paper, a small but complete software package for parameter estimation of orthogonal regression models for nonlinear problems is presented in an ALGOL-like form. An efficient implementation of the regularized Gauss-Newton method is used with minimal computational costs (in the order of a new seconds) where orthogonal transformations are applied exploiting the special structure (sparsity) of the problem. The algorithms are tested and numerical results are reported for many practical problems.}
}
@incollection{GREENBERG1984135,
title = {DISTRIBUTED MICROCOMPUTER-BASED CONTROL OF MULTIPLE SIGNALIZED TRAFFIC INTERSECTIONS**The paper is based on a part ofa senior undergraduate engineering project performed by P. Greenberg and A. Trabelsi, under advise of Prof. D. Tabak during the 1981/82 academic year.},
editor = {D. KLAMT and R. LAUBER},
booktitle = {Control in Transportation Systems},
publisher = {Pergamon},
pages = {135-140},
year = {1984},
isbn = {978-0-08-029365-3},
doi = {https://doi.org/10.1016/B978-0-08-029365-3.50025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080293653500250},
author = {P. Greenberg and A. Trabelsi and D. Tabak},
abstract = {Microcomputer-based control of a signalized urban multi-intersection system is considered. A hierarchical control configuration involving a central controller, which utilizes also a mainframe computer, regional controllers and local controllers at' each intersection, is proposed. An optimization procedure, utilizing the mixed-integer programming approach, and maximizing the green light band, is implemented by the central controller for all intersections. The mixed-integer programming algorithm is run on a CDC CYBER 170/720 computer. A complete hardware design of the controllers at all levels, utilizing Intel 8085 (for the central controller) and Intel 8031 (for the regional and local controllers) has been performed. A software package for all controllers has been prepared. The whole system has been simulated on the CYBER 170 and on an Intel MDS-23l Development System.}
}
@incollection{PARSONS1989127,
title = {CTRL-C AND ACSL USED IN THE TEACHING OF CONTROL ENGINEERING},
editor = {D.A. LINKENS and D.P. ATHERTON},
booktitle = {Trends in Control and Measurement Education},
publisher = {Pergamon},
address = {Oxford},
pages = {127-131},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035736-2},
doi = {https://doi.org/10.1016/B978-0-08-035736-2.50029-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357362500293},
author = {J.T. Parsons and P.R. Warner and A.S. White},
abstract = {In the new Finniston B.Eng. courses greater emphasis is placed on innovation and design and this has led to a reappraisal of teaching methods, highlighting synthesis rather than analysis in problem solving. Although much of the analytical material remains in the courses, computer software packages are used to free the students from repetitious and tedious mathematical work in order to concentrate on the study and design of systems that bear a closer resemblance to those that might be met in practice. CTRL-C and ACSL are sophisticated computer packages which permit the modelling of complex systems using both classical and modern techniques. These have been installed on a VAX 785 minicomputer. Student centred learning is achieved by implementing models/problems in the computer's public domain so that the students may have access to the programs and work through at their own pace. Links between the lecture and practice are enhanced by setting up workstations in the laboratory. The students have adopted the scheme most enthusiastically and are inhibited only by the number of interactive terminals that can adequately be accommodated at any one time by the VAX computer. The assessment of the student's performance is matched to the change in the learning process and caters for the role which CTRL-C and ACSL play in developing engineering competence. The method is cost effective, a feature that must in the future play an ever increasing role in the running of engineering courses.}
}
@incollection{DENHAM1983667,
title = {A ROBUST COMPUTATIONAL APPROACH TO CONTROL SYSTEM ANALYSIS AND DESIGN},
editor = {G.G. LEININGER},
booktitle = {Computer Aided Design of Multivariable Technological Systems},
publisher = {Pergamon},
pages = {667-672},
year = {1983},
isbn = {978-0-08-029357-8},
doi = {https://doi.org/10.1016/B978-0-08-029357-8.50102-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080293578501025},
author = {M.J. Denham and C.J. Benson and T.W.C. Williams},
abstract = {For many years the theory of control system analysis and design has been developed with little consideration for the reliable numerical computation of a solution to the analysis or design problem. The growing industrial interest in the use of recently developed multivariable control system design methods on modern, complex systems has exposed and highlighted the computational deficiencies of these methods and the need for a reformulation of the problems in a way which will directly result in efficient and numerically sound computational algorithms for their solution. Many of the results of recent research in numerical linear algebra have direct relevance to this, and this paper demonstrates how the theory underlying modern control system design methods can be represented in a way which uses these results. The implementation of the resulting algorithms in software and the design of a subroutine library for control engineering (SLICE) to contain the algorithms is also described. Some computational results are given to demonstrate the performance of the algorithms.}
}
@article{CASTROMEDINA2014229,
title = {Modelización numérica del comportamiento estructural cíclico de barras esbeltas de acero con pandeo restringido},
journal = {Revista Internacional de Métodos Numéricos para Cálculo y Diseño en Ingeniería},
volume = {30},
number = {4},
pages = {229-237},
year = {2014},
issn = {0213-1315},
doi = {https://doi.org/10.1016/j.rimni.2013.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S021313151400011X},
author = {J.C. {Castro Medina} and F. {López Almansa} and S. Oller},
keywords = {Disipadores de energía, Barras de pandeo restringido, Control pasivo, Simulación numérica, Modelo de daño, Plasticidad, Energy dissipators, Buckling-restrained braces, Passive control, Numerical simulation, Damage model, Plasticity},
abstract = {Resumen
Este trabajo presenta un modelo numérico del comportamiento estructural cíclico de barras de pandeo restringido, comúnmente utilizadas como una alternativa a las clásicas barras de arriostramiento concéntrico para protección sismorresistente de pórticos de edificios y otras estructuras. Estos dispositivos se componen normalmente de un núcleo de acero esbelto recubierto por una carcasa de mayor rigidez que tiene por objeto impedir su pandeo cuando se encuentra comprimido. La carcasa puede ser de mortero o de acero, y una interfaz de deslizamiento está interpuesta entre el núcleo y la carcasa para evitar la transferencia excesiva de tensiones tangenciales. El comportamiento del núcleo de acero se describe mediante un modelo de daño y plasticidad, el comportamiento de la carcasa de mortero se describe mediante un modelo de daño isótropo y el comportamiento de la interfaz de deslizamiento se describe mediante un modelo de penalización de contacto. Estos 3 modelos se implementan en el paquete de software Abaqus siguiendo una formulación explícita. En un artículo previo publicado en una revista de ingeniería sísmica se describió someramente el modelo, se comprobó de forma preliminar su capacidad para reproducir el comportamiento cíclico de barras de pandeo restringido y se compararon satisfactoriamente sus resultados con los de ensayos experimentales; el objetivo de este trabajo es describir el modelo en profundidad y discutir en mayor extensión las valoraciones acerca de su utilidad.
This work presents a numerical model of the cyclic structural behavior of dissipative buckling-restrained braces, commonly used as an alternative to classical concentric braces for seismic protection of building frames and other structures. Such devices are usually composed of a slender steel core embedded in a stockiest casing that is intended to prevent its buckling when it is under compression. The casing is made either of mortar or steel, and a sliding interface is interposed between the core and the casing to prevent excessive shear stress transfer. The behavior of the steel core is described by a damage and plasticity model; the behavior of the mortar casing is described by an isotropic damage model and the sliding behavior of the interface is described by a contact penalty model. These 3 models are implemented in the Abaqus software package following an explicit formulation. In a previous article (published in an earthquake engineering journal) the model was briefly described, its ability to reproduce the cyclical behavior of buckling-restrained braces was preliminarily pointed out and their results were satisfactorily compared with those of experimental tests. The aim of this paper is to describe the model thoroughly and to present new judgments about its usefulness.}
}
@article{GURTLER1987473,
title = {CAD Standard parts file — A DIN project in the standards committee on tabular layouts for article characteristics},
journal = {Computer Standards & Interfaces},
volume = {6},
number = {4},
pages = {473-481},
year = {1987},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(87)90029-8},
url = {https://www.sciencedirect.com/science/article/pii/0920548987900298},
author = {Guido Gürtler},
keywords = {CAD, Standard Parts, CAD-Standard-Parts Library},
abstract = {The project was started in December 84, to define the necessary data records, to handle standard parts on CAD systems and work stations. Basis of the data records were the article characteristics, documented in the German Standard DIN 4000. The aim is to add information on the logical structure of the standard parts from the tables in the “Product Standards”. During the first year of work it was noted that the user of the data will have a more efficient implementation, if corresponding standardized software is available to produce the selected graphic- and model-representation in the CAD systems. This is to be realized in close cooperation with the German NAM 96.4 (Standardization Committee for Manufacturing of Machines) and the corresponding ISO/TC 184 SC 4 (Industrial Automation, External Representation of Product Definition Data) as well as vendors and users of CAD systems. The software will be realized on a FORTRAN basis. The relevant FORTRAN-Extension has been documented in the Prestandard DIN V 66304. The results of the project offered by DIN shall be: 1.- standardized data records and software for the mostly used standard parts2.- tested and actualized files and subroutine-libraries3.- DIN certificates in a neutral form, e.g. independent from features of specific CAD systems.}
}
@incollection{PERLMAN1981267,
title = {PLANA/1000 — phase locked automatic network analyzer, accuracy enhancement software for use with the HP/1000},
editor = {Helen K. Brown},
booktitle = {Minicomputer Research and Applications},
publisher = {Pergamon},
pages = {267-269},
year = {1981},
isbn = {978-0-08-027567-3},
doi = {https://doi.org/10.1016/B978-0-08-027567-3.50024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008027567350024X},
author = {Barry S. Perlman},
abstract = {ABSTRACT
PLANA/1000 is a package of accuracy enhancement software designed to automate the HP-8409 analyzer by using a disc-based HP-1000 minicomputer as the system controller.1 It provides for hardware calibration, device measurement, complex vector mathematics, and error correction of transmission and reflection measurements, listing and plotting of data output, disc and tape file management, an interactive operator interface, and control of the phase-locked frequency source subsystem and other instrumentation using the HP-IB**HP-IB is Hewlett-Packard's implementation of the IEEE 488 instrument bus interface standard. interface.}
}
@article{WIDYAN2014522,
title = {Operating point stability analysis of SMIB power system equipped with high PV penetration},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {55},
pages = {522-530},
year = {2014},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2013.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0142061513004134},
author = {Mohammad S. Widyan and Rolf E. Hanitsch},
keywords = {Operating point power system stability, Photovoltaic generator characteristics, DC–DC switch mode converter},
abstract = {This paper presents the operating point stability analysis of a Single Machine Infinite Bus (SMIB) power system equipped with high Photovoltaic (PV) penetration. The detailed dynamical model of the synchronous generator in dq reference frame is considered including the dynamics of the damper windings and Automatic Voltage Regulator (AVR). A DC–DC buck–boost switch mode converter is placed as an intermediate stage between the PV array and the inverter. The main function of this implementation is to inject the voltage corresponding to the PV generator Maximum Power Point (MPP) by automatic adjusting its duty cycle. The PV array is designed to provide a maximum output power of about 0.78pu at the full solar irradiance. The nonlinearity of the output characteristics of the PV generator is taken into account. Operating point stability analysis is performed by extracting the eigenvalues of the linearized model around the operating point at different solar irradiances. System response after successive step changes on the synchronous generator input mechanical power at three solar irradiances based on the complete nonlinear dynamical model is investigated. For given synchronous generator input parameters, the response of the system after successive step changes on the solar intensity is addressed. It is found that high PV penetration via DC–DC buck–boost converter and DC–AC inverter is practically possible, experiences a stable operating point and can withstand successive step changes on system parameters in case of practical solar irradiance levels. All numerical simulations are conducted using MATLAB software package by building the code required.}
}
@incollection{WILCOX1971285,
title = {Behavioral Misconceptions Facing the Software Engineer},
editor = {JULIUS T. TOU},
series = {SEN Report Series Software Engineering},
publisher = {Elsevier},
volume = {2},
pages = {285-287},
year = {1971},
booktitle = {Computer and Information Sciences–1969},
issn = {1386-369X},
doi = {https://doi.org/10.1016/B978-0-12-696202-4.50024-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780126962024500243},
author = {Richard H. Wilcox},
abstract = {Publisher Summary
This chapter discusses behavioral misconceptions facing the software engineer. Design of software, for information systems, proceeds on the basis of several tacit assumptions concerning behavioral patterns. The standard approach specifies design of an information system to serve all members of a technical staff. Software design and engineering is based implicitly upon thoughtless misconceptions concerning user behavior. These can concern interest of the potential customer, validity of introspection as a design guide, identification of the primary user, absence of secondary system functions, and adequacy of initial design. User behavior patterns can determine whether the system is ultimately successful or unsuccessful, regardless of technical excellence. Sophisticated computer based information systems are frequently installed on the basis of organizational status, and any such system, which lacks demonstration packages to show off its capabilities easily, has poor relationships with organization management. All software designs must provide for subsequent modification and tuning after initial implementation.}
}
@article{COHEN2005927,
title = {Closed-loop approaches to control of a wake flow modeled by the Ginzburg–Landau equation},
journal = {Computers & Fluids},
volume = {34},
number = {8},
pages = {927-949},
year = {2005},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2004.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S004579300400115X},
author = {Kelly Cohen and Stefan Siegel and Thomas McLaughlin and Eric Gillies and James Myatt},
abstract = {A short computational program was undertaken to evaluate the effectiveness of a closed-loop control strategy for the stabilization of an unstable bluff-body flow. In this effort, the non-linear one-dimensional Ginzburg–Landau wake model at 20% above the critical Reynolds number was studied. The numerical model, which is a non-linear partial differential equation with complex coefficients, was solved using the FEMLAB®/MATLAB® software packages and validated by comparison with published literature. At first, a model independent approach was attempted for wake suppression using feedback control. The closed-loop system was controlled using a conventional proportional-integral-derivative (PID) controller as well as a non-linear fuzzy controller. A single sensor is used for feedback, and the actuator is represented by altering the boundary conditions of the cylinder. Simulation results indicate that for a single sensor scheme, the increase in the sophistication of the control results in significantly shorter settling times. However, there is only a marginal improvement concerning the suppression of the wake at higher Reynolds numbers. The feedback control design was then augmented by switching over to a model-dependent controller. Based on computationally generated data obtained from solving the unforced wake, a low-dimensional model of the wake was developed and evaluated. The low-dimensional model of the unforced Ginzburg–Landau equation captures more than 99.8% of the kinetic energy using just two modes. Two sensors, placed in the absolutely unstable region of the wake, are used for real-time estimation of the first two modes. The estimator was developed using the linear stochastic estimation scheme. Finally, the loop is closed using a PID controller that provides the command input to the variable boundary conditions of the model. This method is relatively simple and easy to implement in a real-time scenario. The control approach, applied to the 300 node FEMLAB® model at 20% above the unforced critical Reynolds number stabilizes the entire wake. Compared to the model-independent controllers, the controller based on the low-dimensional model is far more effective in the suppression of the wake at higher Reynolds numbers. Furthermore, while the latter approach employs only the estimated temporal amplitude of the first mode of the imaginary part of the amplitude, all higher modes are stabilized. This suggests that the higher order modes are caused by a secondary instability that is suppressed once the primary instability is controlled.}
}
@incollection{CUNO1986325,
title = {APPLICATION OF A MULTIVARIABLE ROBUST CONTROLLER DESIGN METHOD TO HARD-COAL PREPARATION},
editor = {P. MARTIN LARSEN and N.E. HANSEN},
booktitle = {Computer Aided Design in Control and Engineering Systems},
publisher = {Pergamon},
pages = {325-330},
year = {1986},
isbn = {978-0-08-032557-6},
doi = {https://doi.org/10.1016/B978-0-08-032557-6.50064-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325576500649},
author = {B. Cuno and W. Neddermeyer},
abstract = {The paper describes an application of a robust controller design method for calculating the parameters of a controller for a multivariable hard-coal preparation process. The parameters are optimized via a sequential design method in the frequency domain, being based on a combination of the Horowitz/Sidi design (1972). Mayne's (1973) sequential design and Steinhauser/Kreisselmeier's (1979) vector performance criterion method. The design was carried out using a relatively sophisticated CAD-program. The paper shows how the parameters are calculated with this CAD-method and discusses the simulated control results. The simulation is compared with the measured results at the plant after implementation of the designed controller on a process computer AEG 80/30 with a real time software control package ARSI**ARSI is a trademark of AEG.}
}
@article{NIEVERGELT1994297,
title = {Complexity, Algorithms, Programs, Systems: The Shifting Focus},
journal = {Journal of Symbolic Computation},
volume = {17},
number = {4},
pages = {297-310},
year = {1994},
issn = {0747-7171},
doi = {https://doi.org/10.1006/jsco.1994.1020},
url = {https://www.sciencedirect.com/science/article/pii/S0747717184710200},
author = {Jurg Nievergelt},
abstract = {We investigate the changing relationship between the small research community of theoretical computer scientists and the much larger community of computer users, in particular, the technology transfer problem of how to exploit theoretical insights that can lead to better products. Our recommendation can be summarized in four points: 1 The computing community is impressed by usable tools and by little else. Although a powerful theorem or ån elegant algorithm may be a useful tool for a fellow theoretician, by and large, the only tools directly usable by the general computing community are systems. No systems, no impact! 2 System development means programming-in-the-large, but the algorithms research community so far has learned only how to program in-the-small. 3 Algorithm researchers must enshrine their algorithms not merely in individual elegant programs, but collectively in useful application packages aimed at some identifiable user group. 4 Since the development of software systems easily turns into a full-time activity that requires different skills from those of algorithms research, we must strive to develop techniques that lets a small group of algorithm researchers develop simply structured, open-ended systems whose kernel can be implemented with an effort of the order of 1 man-year. Low-complexity systems is the goal!}
}
@incollection{JACIC1986305,
title = {COMPUTER – AIDED DESIGN OF INDUSTRIAL AUTOMATION},
editor = {P. MARTIN LARSEN and N.E. HANSEN},
booktitle = {Computer Aided Design in Control and Engineering Systems},
publisher = {Pergamon},
pages = {305-310},
year = {1986},
isbn = {978-0-08-032557-6},
doi = {https://doi.org/10.1016/B978-0-08-032557-6.50060-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325576500601},
author = {L.A. Jacić},
abstract = {In recent years, computers with their great computational power have assumed very important role in the logic design of industrial digital control systems. The implementation of large-scale digital systems is an example of one design problem which can be impractical to solve by hand. How to enable production engineer to improve the existing control network, or to design the absolutely new one, presuming, as it is very often, that he is not a control design specialist. On the basis of new technological demands, an engineer should form a desired control specification which represent the input format to available computer. Computer should produce desired logic equations as a basis for the immediate realization, since the system is reliable and minimal in sense of number of required components. Software for complete design of digital control networks will be presented in the paper. It is applicable for treating deterministic as well as stochastic systems, both combinational and sequential etc. The concept of use of interactive computer graphics and other available computer aids in synthesis process is given. A relevant example is included in order to illustrate the program package.}
}
@incollection{MER1983255,
title = {OVIDE: A SOFTWARE PACKAGE FOR VERIFYING AND VALIDATING PETRI NETS},
editor = {G. FERRATE and E.A. PUENTE},
booktitle = {Software for Computer Control 1982},
publisher = {Pergamon},
pages = {255-260},
year = {1983},
isbn = {978-0-08-029352-3},
doi = {https://doi.org/10.1016/B978-0-08-029352-3.50039-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080293523500398},
author = {E. Le Mer},
abstract = {Managing with the increasing complexity of large scale systems, especially real-time ones, become more and more difficult without the aid of a formal and powerful design methodology; the latter must be supported by automatic tools dealing with the resulting models in order to verify and validate these models. We introduce such a package based upon Petri-nets theory by which we can ensure the correct ness of the specifications before the implementation phasis, mainly, concerning parallelism, synchronization, concurrency; OVIDE is an interactive graphic tool allowing different kinds of analysis : set of reachable markings, reductions, invariants, …}
}
@article{WOODWARD1997357,
title = {Detecting logic errors in discrete-event simulation: reverse engineering through event graphs},
journal = {Simulation Practice and Theory},
volume = {5},
number = {4},
pages = {357-376},
year = {1997},
issn = {0928-4869},
doi = {https://doi.org/10.1016/S0928-4869(96)00002-X},
url = {https://www.sciencedirect.com/science/article/pii/S092848699600002X},
author = {Ernest E. Woodward and Gerald T. Mackulak},
keywords = {Discrete-event simulation, Event graphs, Simulation models, General purpose simulation code, Reverse engineering, Model verification, Simulation debugging, Result analysis},
abstract = {Several inherent constraints remain in the model development process, even though modern enhancements to simulation environments have provided tools for code generation, debugging, and tracing. To develop a simulation model, the simulation analyst still needs to have expertise in a number of different fields, e.g., probability, statistics, design of experiments, modeling, systems engineering, software engineering, and computer programming. Although several simulation packages implement syntactic-checks and semantic-consistency-checks, typically, the simulation analyst needs to possess output-analysis-knowledge specifically aimed at verifying and checking the simulation code. Reverse engineering a graphical model, e.g., an event graph, from general purpose simulation code demonstrates an enhancement to the model development process. A reverse engineering step allows an analyst to check, both, the static and dynamic properties of the coded simulation model. Even though the reverse engineering produces an event-oriented view, the enhanced model development process provides a systematic approach for conversion from other world views. Overall, this enhanced process provides a framework which yields better analysis techniques. Better diagnostic assistance is achieved when viewing a combination of static and dynamic properties of the simulation code. Now, the analyst is able to find logical/execution errors, e.g., errors related to resource deadlocks, before running simulation experiments. Since the graphical model is generated from the simulation code, and the process combines views, the analyst also has a better framework for verifying the coded simulation model. Also, the reverse engineering step provides a structural model useful in converting between different simulation languages or systems. Improvements to the techniques for conversion between languages will facilitate reuse of existing programmed models.}
}
@article{MILES2010752,
title = {OpenFlyData: An exemplar data web integrating gene expression data on the fruit fly Drosophila melanogaster},
journal = {Journal of Biomedical Informatics},
volume = {43},
number = {5},
pages = {752-761},
year = {2010},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2010.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1532046410000511},
author = {Alistair Miles and Jun Zhao and Graham Klyne and Helen White-Cooper and David Shotton},
keywords = {Chado, Data integration, Data web, , Gene expression, Performance, RDF, SPARQL, Triple store, User interface},
abstract = {Motivation: Integrating heterogeneous data across distributed sources is a major requirement for in silico bioinformatics supporting translational research. For example, genome-scale data on patterns of gene expression in the fruit fly Drosophila melanogaster are widely used in functional genomic studies in many organisms to inform candidate gene selection and validate experimental results. However, current data integration solutions tend to be heavy weight, and require significant initial and ongoing investment of effort. Development of a common Web-based data integration infrastructure (a.k.a. data web), using Semantic Web standards, promises to alleviate these difficulties, but little is known about the feasibility, costs, risks or practical means of migrating to such an infrastructure. Results: We describe the development of OpenFlyData, a proof-of-concept system integrating gene expression data on D. melanogaster, combining Semantic Web standards with light-weight approaches to Web programming based on Web 2.0 design patterns. To support researchers designing and validating functional genomic studies, OpenFlyData includes user-facing search applications providing intuitive access to and comparison of gene expression data from FlyAtlas, the BDGP in situ database, and FlyTED, using data from FlyBase to expand and disambiguate gene names. OpenFlyData’s services are also openly accessible, and are available for reuse by other bioinformaticians and application developers. Semi-automated methods and tools were developed to support labour- and knowledge-intensive tasks involved in deploying SPARQL services. These include methods for generating ontologies and relational-to-RDF mappings for relational databases, which we illustrate using the FlyBase Chado database schema; and methods for mapping gene identifiers between databases. The advantages of using Semantic Web standards for biomedical data integration are discussed, as are open issues. In particular, although the performance of open source SPARQL implementations is sufficient to query gene expression data directly from user-facing applications such as Web-based data fusions (a.k.a. mashups), we found open SPARQL endpoints to be vulnerable to denial-of-service-type problems, which must be mitigated to ensure reliability of services based on this standard. These results are relevant to data integration activities in translational bioinformatics. Availability: The gene expression search applications and SPARQL endpoints developed for OpenFlyData are deployed at http://openflydata.org. FlyUI, a library of JavaScript widgets providing re-usable user-interface components for Drosophila gene expression data, is available at http://flyui.googlecode.com. Software and ontologies to support transformation of data from FlyBase, FlyAtlas, BDGP and FlyTED to RDF are available at http://openflydata.googlecode.com. SPARQLite, an implementation of the SPARQL protocol, is available at http://sparqlite.googlecode.com. All software is provided under the GPL version 3 open source license.}
}
@article{KOTTA19821265,
title = {On-Line Eigenvector Algorithms for the Identification of Dynamic Systems},
journal = {IFAC Proceedings Volumes},
volume = {15},
number = {4},
pages = {1265-1269},
year = {1982},
note = {6th IFAC Symposium on Identification and System Parameter Estimation, Washington USA, 7-11 June},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)63171-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017631719},
author = {Ü. Kotta},
keywords = {Parameter estimation, recursive methods, multivariable systems, linear systems, eigenvalues, eigenvectors},
abstract = {The paper deals with the problem of on-line identification of linear multivariable discrete-time dynamic systems from noisy measurements. It is assumed that the input-output model of the system is known up to a finite set of parameters. Two on-line versions of the eigenvector (Levin's) method for estimating system parameters are proposed. The algorithms are obtained by the recursive computation of the least generalized eigenvalue and its corresponding eigenvector of a product-moment matrix of observed input-output data. They are based on the application of two simple numerical methods - the power method and the method of steepest descent - for computing the dominant eigenvector of a positive definite matrix. The proposed algorithms have been implemented in the interactive software package for computer aided design of control systems.}
}
@article{GREENBERG1983135,
title = {Distributed Microcomputer-Based Control of Multiple Signalized Traffic Intersections},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {4},
pages = {135-140},
year = {1983},
note = {4th IFAC/IFIP/IFORS Conference on Control in Transportation Systems , Baden-Baden, FRG, 20-22 April 1983},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)62554-0},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017625540},
author = {P. Greenberg and A. Trabelsi and D. Tabak},
keywords = {Traffic control, microcomputer-based control, mixed integer programming application},
abstract = {Microcomputer-based control of a signalized urban multi-intersection system is considered. A hierarchical control configuration involving a central controller, which utilizes also a mainframe computer, regional controllers and local controllers at each intersection, is proposed. An optimization procedure, utilizing the mixed-integer programming approach, and maximizing the green light band, is implemented by the central controller for all intersections. The mixed-integer programming algorithm is run on a CDC CYBER 170/720 computer. A complete hardware design of the controllers at all levels, utilizing Intel 8085 (for the central controller) and Intel 8051 (for the regional and local controllers) has been performed. A software package for all controllers has been prepared. The whole system has been simulated on the CYBER 170 and on an Intel MDS-251 Development System.}
}
@article{KEVICZKY1976143,
title = {On Simultaneous Optimal Control of Raw Material Blending and a Ball Grinding Mill},
journal = {IFAC Proceedings Volumes},
volume = {9},
number = {5},
pages = {143-158},
year = {1976},
note = {2nd IFAC Symposium on Automation in Mining, Mineral and Metal Processing, Johannesburg, S Africa, 13-17 September},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)67200-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017672008},
author = {L. Keviczky and R. Haber and J. Kolostori and M. Hilger},
abstract = {In this paper an off-line computer control system developed for use in a cement raw material blending plant is described. A desk-top calculator is used to implement a new self-tuning (ST) minimum variance (MV) regulator algorithm developed for multiple-input, multiple-output (MIMO) systems. After a brief survey of technology the algorithm of the MIMO-ST-MV regulator is discussed. A description of the whole off-line computer control system follows. A program written in the PROCAL language is provided to illustrate computeraided design devices which facilitate the development of the software for the above-mentioned control tasks. The main steps in the preparation of a real-time software package for the raw material blending procedure as applicable to the present phase of the research are also considered. Starting from the material balance equations and the basic dynamic behaviour a new type of mill model has been constructed for control purposes. This new model describes the static and dynamic behaviour of the mill more exactly than previously published models. It contains conventional elements (integrator, summator, dead-time, nonlinearity) which can easily be simulated by computer techniques. The construction of the model follows the geometrical arrangement of the mill and thus its structure is clear and the effect of any modifications can easily be determined from the model. The model also produces both static and dynamic unstable phenomena. It is useful for designing extremum control equipment and in adjusting its parameters via simulation. Generally this model facilitates the design of conventional control loops of ball mills and the preparation of DDC circuits.}
}
@article{JAMSA1983189,
title = {Design and Experimental Evaluation of a Multivariable Grinding Circuit Control System},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {15},
pages = {189-197},
year = {1983},
note = {4th IFAC Symposium on Automation in Mining, Mineral and Metal Processing 1983, Helsinki, Finland, 22-25 August 1983},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)64268-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701764268X},
author = {S.-L. Jämsä and H. Melama and J. Penttinen},
keywords = {Multivariable control systems, Inverse Nyquist Array method, Computer-aided control system design, Grinding, Control engineering computer applications},
abstract = {The multivariable control that was designed for the primary grinding in the Vuonos concentrator can control important variables like the final particle size and the density of the cyclone feed. The controller was designed for the plant using an interactive CACSD-software package which included the use of the inverse Nyquist array method. The control algorithms designed were implemented in the control computer in Vuonos and there was good agreement between the simulated results and the full scale tests: Changes of the setpoint of either one controlled output variable produced satisfactory responses, while the other controlled output simultaneously remained close to the desired constant value. The multivariable controller designed has been taken into continuous use in Vuonos. Autocovariance and spectral density functions have been calculated in order to analyse the behaviour of the final particle size and the density of the cyclone feed. According to the results obtained in the design by the inverse Nyquist array method and the experiments at the Vuonos Mill, this technique is suitable for the design of multivariable control of grinding. A practical advantage of this design technique is the simplicity of the final controller and the use of standard control algorithms.}
}
@article{GILL199221,
title = {Information management and analysis system for groundwater data in Thailand},
journal = {Computers & Geosciences},
volume = {18},
number = {1},
pages = {21-28},
year = {1992},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(92)90055-V},
url = {https://www.sciencedirect.com/science/article/pii/009830049290055V},
author = {D. Gill and P. Luckananurung},
keywords = {Groundwater DBMS, Hydrogeology, Thailand},
abstract = {The Ground Water Division of the Thai Department of Mineral Resources maintains a large archive of groundwater data with information on some 50,000 water wells. Each well file contains information on well location, well completion, borehole geology, water levels, water quality, and pumping tests. In order to enable efficient use of this information a computer-based system for information management and analysis was created. The project was sponsored by the United Nations Development Program and the Thai Department of Mineral Resources. The system was designed to serve users who lack prior training in automated data processing. Access is through a friendly user/system dialogue. Tasks are segmented into a number of logical steps, each of which is managed by a separate screen. Selective retrieval is possible by four different methods of area definition and by compliance with user-specified constraints on any combination of database variables. The main types of outputs are: (1) files of retrieved data, screened according to users' specifications; (2) an assortment of pre-formatted reports; (3) computed geochemical parameters and various diagrams of water chemistry derived therefrom; (4) bivariate scatter diagrams and linear regression analysis; (5) posting of data and computed results on maps; and (6) hydraulic aquifer characteristics as computed from pumping tests. Data are entered directly from formatted screens. Most records can be copied directly from hand-written documents. The database-management program performs data integrity checks in real time, enabling corrections at the time of input. The system software can be grouped into: (1) database administration and maintenance—these functions are carried out by the SIR/DBMS software package; (2) user communication interface for task definition and execution control—the interface is written in the operating system command language (VMS/DCL) and in FORTRAN 77; and (3) scientific data-processing programs, written in FORTRAN 77. The system was implemented on a DEC MicroVAX II computer.}
}
@article{PINIPRATO2012590,
title = {Integrated management of cogeneration plants and district heating networks},
journal = {Applied Energy},
volume = {97},
pages = {590-600},
year = {2012},
note = {Energy Solutions for a Sustainable World - Proceedings of the Third International Conference on Applied Energy, May 16-18, 2011 - Perugia, Italy},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2012.02.038},
url = {https://www.sciencedirect.com/science/article/pii/S0306261912001377},
author = {Alessandro {Pini Prato} and Fabrizio Strobino and Marco Broccardo and Luigi {Parodi Giusino}},
keywords = {Combined heat and power, District heating networks, Dynamic heat storage, Energy saving, Simulation engineering, Mathematical models},
abstract = {Combined Heat and Power based District Heating Networks (CHP/DHN) systems represent nowadays one of the most efficient technologies, as an alternative to standard space heating solutions, leading to lower GHG emission in atmosphere. Dynamic modelling of district heating networks is of considerable importance in order to investigate suitable control strategies aimed to optimize the heat production and to manage the system transients following changes in the required heat. Through the analogous electrical systems modelling approach, a component software library has been developed for icon-based dynamic simulation of district heating networks, implementing the mathematical models in Matlab/Simulink environment. The calculation procedure that translates into practice the approach described above, complies with a main flow chart designed to allow for the simulation of the system operation in a quicker-than-real time, throughout the controlled time interval, while imposing the assumed duty and site conditions. The procedure’s main body, as well as being utilized to assess the feasibility and to carry out the conceptual design, once the plant has started up, can be employed as a guide to the operator. Research activity has been focused on the development and integration of a code for dynamic simulation of heat distribution networks with a code for thermo-economics optimization of CHP systems, increasing the possibility of optimizing the matching between CHP plant and thermal users, through the exploitation of thermal storage capacity of the networks. A CHP-based district heating project in Northern Italy was considered as a test case. Results show that integrated management of cogeneration plants and district heating networks allows for the achievement of significant advantages both in terms of economic competitiveness and energy saving: in particular it has been highlighted that only through the support of an intelligent management system it is possible to maximize the potential benefits offered by the exploitation of district heating networks dynamic heat storage capacity.}
}
@article{BULGAKOV1992869,
title = {Fast algorithms for multi-grid solvers of three-dimensional boundary value problems in structural analysis},
journal = {Computers & Structures},
volume = {44},
number = {4},
pages = {869-875},
year = {1992},
note = {Special Issue: Computational Structures Technology},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(92)90474-E},
url = {https://www.sciencedirect.com/science/article/pii/004579499290474E},
author = {V.E. Bulgakov and M.V. Belyi},
abstract = {A large number of analyses in structural mechanics may be treated using the three-dimensional boundary value formulation of the problem. The use of the standard finite element techniques, typically comprising local stiffness matrix assembling and a direct eliminating procedure for solving resulting systems of equations, is usually computationally very demanding. It is natural to employ here multi-grid methods giving a good convergence in solving boundary value elliptic problems. But in spite of the fact that these methods avoid a costly eliminating procedure, the difficulties associated with programming costs, grid generation, storage requirements and introducing a coarse problem concept, remain acute. There are some potential methods to overcome these difficulties by allowing some simplifications. We propose one of them associated with constructing a simplified data structure based on logically rectangular grids which leads to developing efficient vectorial algorithms for executing basic multi-grid operations. These algorithms appeared to be easy to program and good for pre- and post-processing, suitable for parallel processing and very efficient when implemented as a software package for heat condition and stress-strain state analyses of various engineering structures, such as reactor vessels, dam structures, building fragments and others. We propose the substructuring algorithm in the framework of a multi-grid method which is useful for treating problems with cyclic and complicated domains. It is based on dividing the domain into simple parts each admitting the simplified data structure. We also represent the multi-grid method previously developed to demonstrate the possibility of using the algorithmic approaches. In conclusion we consider the example of an actual problem illustrating the 3-D finite element analysis of a reactor vessel.}
}
@article{GRUNLOH201785,
title = {A novel multi-scale domain overlapping CFD/STH coupling methodology for multi-dimensional flows relevant to nuclear applications},
journal = {Nuclear Engineering and Design},
volume = {318},
pages = {85-108},
year = {2017},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2017.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0029549317301383},
author = {T.P. Grunloh and A. Manera},
keywords = {Multiscale, Coupling, Domain overlapping, CFD, STAR-CCM+},
abstract = {A novel multi-scale domain overlapping coupling methodology designed to couple a computational fluid dynamics (CFD) code with a system thermal hydraulic (STH) code was developed and its performance was investigated. The methodology has been implemented in the coupling infrastructure code Janus, developed at the University of Michigan, providing methods for the on-the-fly data transfer through memory between the commercial CFD code STAR-CCM+ and the US NRC best-estimate thermal hydraulic system code TRACE. Coupling between these two software packages is motivated by the desire to extend the range of applicability of TRACE to scenarios in which local momentum and energy transfer are important, such as three-dimensional mixing of localized slugs of deborated or cold water in the downcomer and lower plenum of a reactor pressure vessel. The intra-fluid shear forces necessary to correctly capture these effects are neglected in the TRACE equations of motion, but are readily calculated from CFD solutions. CFD/STH coupling implementations therefore have applications in reactor transients such as boron dilution scenarios, Anticipated Transient Without SCRAM (ATWS) and Main Steam Line Break (MSLB). The proposed method is based on aliasing all spatial sources and sinks of momentum in the CFD domain as frictional losses in the system code domain. The internal velocity fields and, consequently, the inertial component of the pressure field are maintained consistent between the CFD and STH domains through a complementary velocity-matching interface. In this paper, coupled simulations are performed on Cartesian and cylindrical geometry with emphasis on consistency, convergence, and stability during transient scenarios. Results show that the presented domain overlapping coupling method is capable of adjusting pressure and velocity profiles of multi-dimensional system code solutions to match CFD solutions accurately. Important characteristics of transient simulations were found to include the background flow rate, specifically the stabilizing effect of viscous forces, as well as the time derivative of the flow rate. Under certain adverse conditions, the basic coupling method is found to produce unstable behavior. A stabilization method for adjusting CFD data is laid out and found to significantly improve the method’s performance under the most challenging conditions. Recommendations are laid out for further improving the coupling via advanced time stepping methods.}
}
@article{AGARWAL201267,
title = {FPGA based variable frequency AC to AC power conversion},
journal = {Electric Power Systems Research},
volume = {90},
pages = {67-78},
year = {2012},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2012.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378779612001034},
author = {Anshul Agarwal and Vineeta Agarwal},
keywords = {Field-programmable gate array (FPGA), Cycloinverter, Cycloconverter, Trapezoidal modulation (TM), Frequency converter, Total harmonic distortion (THD)},
abstract = {AC/AC variable frequency power conversion system is proposed which makes use of a cycloconverter in newer form, ac–ac matrix converter. An attempt has been made to operate this matrix converter both in conventional low frequency ac–ac converter, cycloconverter and new high frequency ac–ac converter, cycloinverter. The ability to directly affect the frequency conversion of power without any intermediate stage involving DC power is a huge advantage of the system. The undesirable harmonic components in the output of the matrix converter have been minimized using an advanced modulation technique called as trapezoidal modulation technique. The technique offers several advantages compared to other modulation techniques in terms of easy and fast real-time waveform generation with higher fundamental output voltage. The converter is simulated using well known software package MATLAB. Simulations results are presented for both cycloconverter as well as for cycloinverter. It has been found that for cycloinverter operation the total harmonic distortion (THD) is more as compared to cycloconverter mode of operation. The simulated results are also validated with experimental results by implementing the trigger controller circuit to generate trapezoidal modulated trigger signal for matrix converter on field programmable gate array (FPGA). Peripheral input–output and FPGA interfacing has been developed through Xilinx 9.2i using very high speed integrated circuit hardware description language (VHDL). The circuit has been tested qualitatively by observing various waveforms on digital storage oscilloscope (DSO). The operation of proposed system has been found satisfactory.}
}
@article{CHEN2004373,
title = {An enhanced asymptotic homogenization method of the static and dynamics of elastic composite laminates},
journal = {Computers & Structures},
volume = {82},
number = {4},
pages = {373-382},
year = {2004},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2003.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0045794903004450},
author = {Chien-Ming Chen and Noboru Kikuchi and Farzad Rostam-Abadi},
keywords = {Homogenization theory, Composite materials},
abstract = {The homogenization method has been used often in analyzing the composite materials but has had limited use with the laminated structures. This is due to a limitation of the homogenization theory, which assumes the unit cell should be periodically represented in any specific area. The main attraction of the homogenization method is its systematic capability to develop the homogenized macroscopic constitutive relation of composite materials and to compute the stresses in the microstructure of composites. In other words, both the macro- and micromechanics can be treated in the same context. In this paper, we will develop an enhanced asymptotic homogenization method (EHM); this method is developed from the partial asymptotic expansion and the condensed third-order shear deformation theory. This theory uses the reduced matrix method to consolidate all the stiffness matrices into four stiffness matrices (the extension; coupling; bending and transverse shear stiffness matrices) in the same manner as with the first-order shear deformation theory (FSDT). Two FORTRAN programs PRELAM and POSTLAM [Int. J. Comput. Struct. 76 (2000) 319; An enhanced asymptotic homogenization method of elastic composite laminates, Ph.D. Thesis, The University of Michigan, 1999] were also developed from the finite element implementation of the enhanced homogenization method. The pre-processor, PRELAM, reads the unit cell model and generates the homogeneous material stiffness matrix for a laminated structure. The post-processor, POSTLAM, calculates the local stress distribution based on the strains and curvatures obtained from the global structure analysis generated by the commercial finite element software, i.e., ABAQUS or MSC/NASTRAN. By the nature of homogenization method, these computational methods are capable of handling geometrically complicated microstructures and predicting the microscopic stress distributions. Since, the number of unknown variables for EHM and FSDT are the same, this implies that the results from EHM is easily utilized by ABAQUS in the global analysis. Therefore, one of the main benefits to EHM is that it is readily applied in commercial FEM packages, but not that it is as accurate as other methods. Numerical examples are also presented to validate these two programs.}
}
@incollection{WARWICK1989153,
title = {CONTROL SYSTEMS: A FIRST COURSE},
editor = {D.A. LINKENS and D.P. ATHERTON},
booktitle = {Trends in Control and Measurement Education},
publisher = {Pergamon},
address = {Oxford},
pages = {153-156},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035736-2},
doi = {https://doi.org/10.1016/B978-0-08-035736-2.50034-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357362500347},
author = {K. Warwick},
abstract = {This paper is concerned with the topics studied in an introductory course on Control Systems. Fundamental analysis, design and implementation must all be catered for in an up to date fashion, such that the course is of benefit as either a stand-alone package for general Engineering students or as a foundation for much deeper study for specialist Control Engineers. It is important at this stage for the student to experience the breadth of the Control Systems area, and to understand, in simple terms, how the different topics are connected. Obviously, it is not possible in one paper to set out a course specification for every institution to follow, but rather the intention here is to stress the need for an introductory course to keep pace with modern trends, and suggestions are made as to how this can be done at the present time. Topics highlighted are state-space methods, PID control and digital control, but principally computer control, in that this covers such as CAD, software simulation and Artificial Intelligence.}
}
@article{KRATZER19781,
title = {Interfacing Real-time Operating Systems to Process Control Languages},
journal = {IFAC Proceedings Volumes},
volume = {11},
number = {2},
pages = {1-16},
year = {1978},
note = {IFAC/IFIP Workshop on Real Time Programming 1978, Mariehamn/Aland, Finland, 19-21 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65894-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017658944},
author = {G. Kratzer and G. Schrott},
keywords = {computer control, portable software, real-time operating systems, real-time programming languages, standards},
abstract = {The paper gives an overview over an operating system which provides for a flexible interface to support a wide spectrum of real-time language facilities. Due to its modular structure it can be tailored to the requirements of the language. In many cases this is achieved by an automatic, dialogue controlled generation of the system. For more serious changes well defined “basic functions” of the system have to be changed or new functions can easily be introduced. On the assembler level the system requests (SVC) are available as macro statements defined similar to real-time language syntax. A list of these macro calls is explicitly presented to give an overview over an implementation for the pdp 11 computer family; it shows the range in the definition of such a macro library. Based on this lowest level three methods to implement system requests in real-time languages are discussed. If the language allows assembler insertions the macros can immediately be used. If the real-time facilities of the language are connected via subroutine calls, they are realized by adding code procedures which contain the corresponding macros. In the third case, real-time operations are part of the syntax and can be compiled using the macros as inline code or generating adequate subroutine calls. These three methods are discussed with respect to readability, portability, software-safety, runtime efficiency and implementation costs. Implementations are illustrated by a few examples.}
}
@incollection{BERNADOU1986155,
title = {MODULEF: A LIBRARY OF COMPUTER PROCEDURES FOR FINITE ELEMENT ANALYSIS},
editor = {A. NIKU-LARI},
booktitle = {Structural Analysis Systems},
publisher = {Pergamon},
pages = {155-173},
year = {1986},
isbn = {978-0-08-032582-8},
doi = {https://doi.org/10.1016/B978-0-08-032582-8.50019-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325828500191},
author = {M. Bernadou and P.L. George and P. Laug and M. Vidrascu},
abstract = {ABSTRACT
MQDULEF is a general purpose finite element computer program developed by the MODULEF CLUB (see Appendix at the end of the paper). Created by INRIA in 1974, this Club brings together French and foreign universities, and private or public industrial companies with the goal of designing and implementing a library of finite element modules. Some existing module capabilities include solutions to:–steady state or time-dependent, linear or nonlinear 2-D, 3-D and axis heat conduction problems–static or dynamic linear or nonlinear 2-D, 3-D and axis elasticity problems–elasticity problems for beams plates and shells–fluid mechanics problems. The various algorithms and software attributes such as modularity, portability and dynamic memory allocation make the MODULEF library a powerful tool for Research and Development. Modular structures allow simple modifications and program additions. Not only one, but several solutions to a problem can be readily implemented, and therefore the relative merits of various approaches can be easily assessed. An interactive data generation system is available. It is suitable for automatic mesh generation, generation of boundary conditions and generation of data needed for the main steps used in finite element computing programs}
}
@article{KLAN1989289,
title = {Implementation and Application of Single-Board Microcomputers-Based Simple Auto-Tuners},
journal = {IFAC Proceedings Volumes},
volume = {22},
number = {16},
pages = {289-291},
year = {1989},
note = {IFAC Workshop on Evaluation of Adaptive Control Strategies in Industrial Applications, Tbilisi, USSR, 17-20 October},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53026-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017530268},
author = {P. Klán and J. Maršík},
keywords = {Signal-based process control, PID-based control, heuristic algorithms package, personal computers, single-board microcomputers, dual-in-line package, identification-free algorithms, auto-tuning control},
abstract = {A class of signal-based identification-free auto-tuners, simple algorithmically provided and implemented on portable single-board microcomputers is found to be well suited to control a wide range of industrial processes met in practice, which are uncertain in the sense that it is not possible to determine what the future output of the process precisely will be. The implementation of a class of auto-tuning controllers using portable single-board microcomputers is described. The class is found to be well suited to solving a range of industrial control problems through a well-known PID-based control. The ubiquitous PID algorithm was chosen because of its familiarity, timeproven utility, and applicability to the majority of industrial feedback control problems and its operations are intuitively understood by most process engineers. Adaption loops work like an ordinary control loop. For evaluating them no special test disturbances are necessary, the current control process being quite sufficient for the auto-tuning. Hardware and software details of portable single-board microcomputers, SCX-48, SCX.51, and SCX-80, respectively are presented with particular reference to the use of the new auto-tuning algorithms. The hardware has been designed with MCS 48, MCS 51, MCS 80, MCS 88 sets of components and is equipped with a serial port to connect to certain personal computer through an RS 232/CCITT V.24 voltage levels. These controllers were succesfully tested on simulated and real industrial plants.}
}
@incollection{BRYAN1984217,
title = {DEVELOPMENT AND USE OF A CADCAM SYSTEM FOR COMPLEX SURFACES},
editor = {Joanna Wexler},
booktitle = {CAD84},
publisher = {Butterworth-Heinemann},
pages = {217-222},
year = {1984},
isbn = {978-0-408-01440-3},
doi = {https://doi.org/10.1016/B978-0-408-01440-3.50029-X},
url = {https://www.sciencedirect.com/science/article/pii/B978040801440350029X},
author = {M J Bryan and P Parkin and J Stark},
abstract = {This paper is divided into four sections which together give a full description of how SYSTRID has been developed and how it is used. The first section covers some of the mathematical background to the system and shows how this is related to the design and manufacturing requirements. The second section describes how, from the computer point of view, the SYSTRID software package is developed and maintained to run on several different computers and graphic screens. The third section outlines the WHL implementation and the use to which this company has put SYSTRID. The final section describes future developments of SYSTRID.}
}
@incollection{KRATZER1979149,
title = {DESIGN AND IMPLEMENTATION OF PROCESS CONTROL SOFTWARE UNDER REALISTIC ENVIRONMENT CONDITIONS},
editor = {M. NOVAK},
booktitle = {Software for Computer Control},
publisher = {Pergamon},
pages = {149-153},
year = {1979},
isbn = {978-0-08-024448-8},
doi = {https://doi.org/10.1016/B978-0-08-024448-8.50029-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244488500290},
author = {G. Kratzer},
abstract = {A software package is described which offers a systematic approach to development of industrial process control software. The procedure of control program implementation splits up in 3 basic steps or phases. Phase 1 covers control algorithm design and offline simulation. During phase 2 the real control programs are installed on process computer using results and experience of step one. Programs are thoroughly tested and evaluated in realtime environment. The interplay of control software performance on the whole and its control behaviour can be investigated in closed loop simulation. Within phase 3 control software is tested on a dual computer system consisting of one computer simulating and a second computer controlling the plant. Both computers are coupled by realistic process interface equipment and are working asynchronously. Plant simulation and control software operate in parallel and in real-time. Phase 3 simulation supports final tuning and adapting of control algorithms and parameters. Concerning software engineering aspects an overall test and performance evaluation of the control system can be done. The system establishes a software and control engineering tool which makes control software development less costly and more reliable. Especially the transfer to real plant will become less risky after testing under realistic environment conditions.}
}
@article{SUNDQVIST1987395,
title = {The use of LOTUS 1-2-3 in statistics},
journal = {Computers in Biology and Medicine},
volume = {17},
number = {6},
pages = {395-399},
year = {1987},
issn = {0010-4825},
doi = {https://doi.org/10.1016/0010-4825(87)90057-6},
url = {https://www.sciencedirect.com/science/article/pii/0010482587900576},
author = {Christer Sundqvist and Kristian Enkvist},
keywords = {Computers, LOTUS 1-2-3, Statistics, Spreadsheet, Biological data},
abstract = {This paper describes a convenient way of performing statistical tests in biology. The recent development of powerful spreadsheet programs for microcomputers has made it possible to easily apply various statistical significance tests on biological data. Presently the following tests have been implemented in the LOTUS 1-2-3 framework: Student's t-test, chi-square test, analysis of variance (single classification random ANOVA), Student-Neumann-Kuels test, correlation analysis and analysis of linear regression (single and multilevel design). The most important advantages gained by using 1-2-3 instead of the commercial statistical software packages are the simplicity of entering data, the possibility of asking “what-if?” questions, the simple, but useful graphical presentation of data and the case of actual building of the tests.}
}
@article{LOUREIRO1999425,
title = {A systems engineering environment for integrated satellite development},
journal = {Acta Astronautica},
volume = {44},
number = {7},
pages = {425-435},
year = {1999},
note = {Pacific RIM: A Rapidly Expanding Space Market},
issn = {0094-5765},
doi = {https://doi.org/10.1016/S0094-5765(99)00089-2},
url = {https://www.sciencedirect.com/science/article/pii/S0094576599000892},
author = {Mr.G. Loureiro and Dr.P.G. Leaney},
abstract = {Space mission implementation faces a very dynamic environment with fast-paced information technology advancement and shrinking space budgets. A more focused use of decreasing public investments in space requires a cost reduction over their entire life cycle, up to the end of the useful life of a spacecraft. The anticipation of cost, schedule, risk and performance requirements from all over the product life cycle to the early stages of product development is generally recognised as a necessary condition to reduce life cycle cost. In order to cope with the intrinsic functional complexity of space products, such requirements engineering activity must be performed in a structured way within a systems engineering approach. This paper aims to describe how Cradle, a commercial systems engineering environment software package, can be used for integrated satellite development, taking into consideration functional and life cycle process requirements. Cradle has requirements management, system modelling, performance modelling, configuration management and document generation capabilities integrated in the same environment. Also, the paper provides some examples of application and highlights how Cradle can enhance the satellite development related activities performed by the Brazilian Institute for Space Research (INPE).}
}
@article{PILLMANN1980189,
title = {A3.1 : The Process Computer Control System of the Hydroelectric Power Stations Along the River Danube in Austria},
journal = {IFAC Proceedings Volumes},
volume = {13},
number = {9},
pages = {189-196},
year = {1980},
note = {6th IFAC/IFIP Conference on Digital Computer Applications to Process Control, Dusseldorf, Germany, 14-17 October},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)64568-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017645683},
author = {W. Pillmann and R. Stefanich},
keywords = {Hydro-electric power plant, power station control, digital computer applications, flow control, water level control, direct digital control, identification, digital simulation},
abstract = {In Austria the control centers of the six hydroelectric power stations along the river Danube are equipped with process computers. The control systems are designed for advanced control of the plants, including multilevel process control, interactive man-machine interfaces, advanced monitoring, security related functions and for supervising the hydroelectric plant cascade in the future. During implementation particular attentions was paid to the water level and flow rate control system in the power stations. The principles of design, the realisation of this direct digital control and the up to date experiences with the controller operation are presented in this paper. Furthermore hardware structure of the control center, the construction principles of the software package and the man machine interface are discussed briefly}
}
@article{NIKZAD2015132,
title = {Scheduling of multi-component products in a two-stage flexible flow shop},
journal = {Applied Soft Computing},
volume = {32},
pages = {132-143},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615001490},
author = {Fatemeh Nikzad and Javad Rezaeian and Iraj Mahdavi and Iman Rastgar},
keywords = {Flexible flow shop, Dedicated assembly lines, Two-process components, Imperialist competitive algorithm, Simulated annealing algorithm, Taguchi design of experiment},
abstract = {In this research, the problem of scheduling and sequencing of two-stage assembly-type flexible flow shop with dedicated assembly lines, which produce different products according to requested demand during the planning horizon with the aim of minimizing maximum completion time of products is investigated. The first stage consists of several parallel machines in site I with different speeds in processing components and one machine in site II, and the second stage consists of two dedicated assembly lines. Each product requires several kinds of components with different sizes. Each component has its own structure which leading to difference processing times to assemble. Products composed of only single-process components are assigned to the first assembly line and products composed of at least a two-process component are assigned to the second assembly line. Components are placed on the related dedicated assembly line in the second phase after being completed on the assigned machines in the first phase and final products will be produced by assembling the components. The main contribution of our work is development of a new mathematical model in flexible flow shop scheduling problem and presentation of a new methodology for solving the proposed model. Flexible flow shop problems being an NP-hard problem, therefore we proposed a hybrid meta-heuristic method as a combination of simulated annealing (SA) and imperialist competitive algorithms (ICA). We implement our obtained algorithm and the ones obtained by the LINGO9 software package. Various parameters and operators of the proposed Meta-heuristic algorithm are discussed and calibrated by means of Taguchi statistical technique.}
}
@article{KAO199855,
title = {Development of a collaborative CAD/CAM system},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {14},
number = {1},
pages = {55-68},
year = {1998},
issn = {0736-5845},
doi = {https://doi.org/10.1016/S0736-5845(97)00014-8},
url = {https://www.sciencedirect.com/science/article/pii/S0736584597000148},
author = {Yung-Chou Kao and Grier C.I. Lin},
keywords = {Collaborative CAD/CAM, Network communication, Remote CAD/CAM, CAD coediting},
abstract = {This paper presents the development of a collaborative CAD/CAM system (COCADCAM). COCADCAM extends an existing single-location CAD/CAM system to a multi-location CAD/CAM application so that two geographically dispersed CAD/CAM users can work together on a three-dimensional CAD-geometry coediting and CAD-related tasks collaboratively and dynamically. COCADCAM dynamically supports CAD data communication that are not available in traditional single-location CAD/CAM. The dynamic data communication is achieved through the development of networking algorithms and CAD/CAM functions in this paper. The networking algorithms based on UNIX Interprocess Communication (IPC), the Network File System (NFS), and a connection-oriented client and server model under the Transmission Control Protocol/Internet Protocol (TCP/IP) suite. The CAD/CAM functions included surface modelling, simulation of a milling toolpath, and post-processing of an NC program following collaborative CAD-geometry coediting, which are directly or indirectly supported by the Application Programming Interface (API) of the CAD/CAM software. The networking algorithms and CAD/CAM functions together can facilitate an environment for CAD-geometry coediting and related tasks such as design, analysis and manufacture. COCADCAM has been successfully implemented through local area network (LAN) and the Internet; a remote machining cell is also linked so that the generated NC program based on a coedited free-form surface can be used for the physical machining operation. The algorithm proposed by COCADCAM can be referenced for the extension of other single-location CAD/CAM systems to multi-location applications.}
}
@article{ABASOV1990245,
title = {Computer-based system for exploration, optimization, and reserve estimation at the Bakhar field, South Apsheron, Azerbaijan, SSR},
journal = {Computers & Geosciences},
volume = {16},
number = {2},
pages = {245-249},
year = {1990},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(90)90131-C},
url = {https://www.sciencedirect.com/science/article/pii/009830049090131C},
author = {M.T. Abasov and I.S. Djafarov and G.I. Askerov},
keywords = {Drilling patterns, Geostatistics, Kriging, Variogram},
abstract = {This paper presents a review of computerized geostatistical methods for determining optimum exploration drilling patterns, and for calculating oil and gas reserves at the Bakhar field. Experimental variograms of porosity and hydrocarbon saturation are calculated and modeled. Ordinary kriging then is used to determine global estimates of each parameter for different horizons of the reservoir. Universal kriging is used to map the nonstationary elevation of formation tops. With prior knowledge of a parameter's spatial correlation structure, the kriging variance can be used as a criterion for determining the optimum number and location of wells for reserve estimation purposes. This is the basis of an iterative algorithm for an optimum drilling plan in the exploration and development of a field. Geostatistical methods are well suited to spatial estimation problems in hydrocarbon exploration and production and have been implemented in a comprehensive software package.}
}
@incollection{BAHLKE1987311,
title = {The User Interface of PSG Programming Environments},
editor = {H.-J. BULLINGER and B. SHACKEL},
booktitle = {Human–Computer Interaction–INTERACT '87},
publisher = {North-Holland},
address = {Amsterdam},
pages = {311-315},
year = {1987},
isbn = {978-0-444-70304-0},
doi = {https://doi.org/10.1016/B978-0-444-70304-0.50056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044470304050056X},
author = {Rolf Bahlke and Manfred Hunkel},
abstract = {The PSG system is a generator for interactive language-specific programming environments. A generated environment consists of a language-based hybrid editor, a library system, and an interpreter augmented by a debugging system. The user of a PSG generated environment communicates via a uniform interface with any of the above system components. This paper presents in detail the user interface and its design considerations of the PSG implementation running on ICL-PERQ and pcs-CADMUS workstations equipped with a raster-graphics display and a pointing device. The environment and its user interface have been designed to support both, casual and experienced users. In order to achieve this goal, the environment is devoid of any command language: to trigger a certain task, the user simply has to select the appropriate item from a menu of alternatives. The editor allows structure-oriented editing as well as conventional text editing. Although the user interface has been primarily designed to be driven by PSG environments, it could nevertheless be employed by other text-processing software. In addition to a screen-oriented text-editor, the interface provides windowing, static menus and pop-up menus, and access to the trigger buttons of the pointing device.}
}
@article{UNDERWOOD1980259,
title = {A variable-step central difference method for structural dynamics analysis- part 2. Implementation and performance evaluation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {23},
number = {3},
pages = {259-279},
year = {1980},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(80)90009-2},
url = {https://www.sciencedirect.com/science/article/pii/0045782580900092},
author = {P.G. Underwood and K.C. Park},
abstract = {The variable-step central difference method developed in part 1 is implemented as a stand-alone software package that is easily accessed by existing structural dynamics analyzers (i.e. finite element, finite difference discrete element computer codes) through a common data structure, input/output (I/O) manager and a few user-supplied control and interface routines. The performance of this package is evaluated through a computer study of four sample problems that embody a large class of response characteristics: linear to highly nonlinear, wave to structural to uncoupled component response, narrow to wide frequency spread, no damping to overdamping, and accuracy-critical to numerical-stability-critical response. The present method is successful in meeting these response characteristics with effectiveness in all the above cases with the single exception of pure wave propagation.}
}
@article{LIN200966,
title = {Object-oriented development and application of a nonlinear structural analysis framework},
journal = {Advances in Engineering Software},
volume = {40},
number = {1},
pages = {66-82},
year = {2009},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2008.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0965997808000562},
author = {Bo-Zhou Lin and Ming-Chieh Chuang and Keh-Chyuan Tsai},
keywords = {PISA3D, GISA3D, VISA3D, GUI, Design pattern, Object-oriented, Nonlinear structural analysis},
abstract = {This paper describes the framework and application of numerical simulation software on earthquake engineering research and practice. The analysis kernel is developed at the National Center for Research on Earthquake Engineering (NCREE) and is entitled as “Platform of Inelastic Structural Analysis for 3D systems (PISA3D)”. The design of PISA3D framework adopts the Design Pattern and the Unified Process. PISA3D provides structural modeling and high computational efficiency for engineers and researchers to simulate the responses of nonlinear systems under various kinds of load effects. It includes static or cyclic loads, displacements, earthquake ground accelerations, and earthquake aftershocks. PISA3D is easy to extend and maintain due to its object-oriented nature. Advanced users can derive or compose its objects’ libraries to perform different types of structural analyses. Based on object-oriented techniques, VISA3D (Visualization of Inelastic Structural Analysis for 3D systems) has been implemented with usage of OpenGL for 3D graphics and MFC for graphical user interface (GUI). Its framework allows further extension on new input formats and new element types. VISA3D has been mainly developed as a post-processor to examine the analytical results of PISA3D through 2D/3D static or dynamic graphic approaches. It includes graphical checking of the structural model, mode shapes, deformations, extents and locations of plastic hinges, plotting of nodal velocity, acceleration, and energy distribution time histories. This paper then introduces NCREE’s recent development on the pre-processing framework GISA3D (Graphical Interface of Inelastic Structural Analysis for 3D systems). The GISA3D does not only supply features as a “post-processor”, but also fully supports operations of “model generation” via mouse motion. Users can create, remove, modify and set elements/nodes through mouse clicking, dragging and selecting. Finally, this paper illustrates the networked sub-structural pseudo dynamic tests using PISA3D as the analysis engine, and concludes with several successful applications of PISA3D/VISA3D/GISA3D on various researches and actual structural engineering projects.}
}
@article{ALJEHANI2018404,
title = {Design and optimization of a hybrid air conditioning system with thermal energy storage using phase change composite},
journal = {Energy Conversion and Management},
volume = {169},
pages = {404-418},
year = {2018},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2018.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S019689041830517X},
author = {Ahmed Aljehani and Siddique Ali K. Razack and Ludwig Nitsche and Said Al-Hallaj},
keywords = {Electricity’s demand side management, Air conditioning, Thermal energy storage, Latent heat storage, Phase change material},
abstract = {This paper evaluates the use of a phase change composite (PCC) material consisting of paraffin wax (n-Tetradecane) and expanded graphite as a potential storage medium for cold thermal energy storage (TES) systems to support air conditioning applications. The PCC-TES system is proposed to be integrated with the vapor compression refrigeration cycle of an air conditioning (AC) system. The use of this PCC material is novel because of its unique material and thermal characteristics as compared to ice or chilled water that are predominantly used in commercial TES systems for air cooling applications. The work of this paper proposed and tested a hypothesis, which suggests that integrating a conventional AC with a PCC-TES would result in significant benefits concerning compressor size, compressor efficiency, electricity consumed and CO2 emissions. The proposed integration would also contribute to reduce electricity demand during peak hours and reduce necessity to build more expensive power plants and distribution lines. To test the hypothesis, a simulation model in Aspen Plus® software was prepared. However, Aspen Plus® does not have a built-in library to predict PCC’s melting and solidification behaviors. Therefore, an analytical heat transfer model was written as a system of equations in Fortran code into Aspen Plus® calculation block to simulate the phase change behavior and associated characteristics. The overall simulation model, which was designed specifically for this research work, consists of two main parts that communicate with each other. The first part simulates the AC’s refrigeration loop using the built-in Aspen Plus® components and the second part implements the PCC heat transfer model written within the calculation block of Aspen Plus®. The simulation model was validated by crosschecking the calculated results with actual experimental data from an actual 4 kWh PCC-TES benchtop thermal storage system. Very good agreement was observed between the simulations and laboratory data. Simulated performance of the proposed integration between the AC and the PCC-TES indicated the potential to (1) downsize the compressor by 50%, (2) lower electrical consumption by the compressor by 30%, (3) lower CO2 emissions by 30%, and (4) double the compressor efficiency during off and mid peak hours. The present work is a conceptual design and optimization study and does not account for integration inefficiencies, energy losses, real-world operation complexity, and added capital cost of TES integration with AC systems.}
}
@article{SCHMIDT1995461,
title = {The RFX centralized control, data acquisition and machine protection systems},
journal = {Fusion Engineering and Design},
volume = {25},
number = {4},
pages = {461-496},
year = {1995},
issn = {0920-3796},
doi = {https://doi.org/10.1016/0920-3796(94)00284-E},
url = {https://www.sciencedirect.com/science/article/pii/092037969400284E},
author = {V Schmidt and G Flor and O.N Hemming and A Luchetta and G Manduchi and S Vitturi},
abstract = {RFX employs a centralized approach to the tasks of control, monitoring, data acquisition and machine protection. In this paper we describe the requirements, the structure, the components, and the operation of the corresponding systems of RFX. To guarantee a high degree of reliability, a strict subdivision has been imposed from the very start, between the control, monitoring and data acquisition system SIGMA (“sistema di gestione, monitoraggio ed acquisizione dati”) and the global fast machine protection system SGPR (“sistema generale di protezione rapida”). SIGMA has been designed for the following signals: about 5000 slow, mostly digital, signals, which provide non-shot-related information from the plant and commands to the plant, and about 1000 channels of fast (2 kHz–1 MHz), shot-related data from the plant, which produce up to 20 Mbytes of data per shot. In addition, fast plant-wide timing signals (precision better than 10 μs) have to be provided. SIGMA employs two distinct technologies: industrial-type programmable controllers (PLCs) handle the slow signals; a centralized VAX-VMS computer cluster with front end according to the CAMAC standard takes care of the fast system components. The CAMAC-based system covers both the fast data acquisition and the timing requirements. Workstations are used as operator consoles for the fast part of the system; personal computers are used as consoles to the PLCs. All components of the system communicate via a single, fibre optic Ethernet. A single PLC acts as supervisor of the entire shot sequence. The PLCs are programmed in an assembler-type language at the lower level and in a language according to the grafcet standard at the higher level. The PC-based consoles employ a commercial package for plant monitoring and control. The VAX-based systems run a purpose-developed software package, known as mds-plus, which provides integrated operation of timing and fast data acquisition. mds-plus is a joint software development with the MIT Plasma Fusion Center and the Los Alamos National Laboratory. The machine protection system SGPR has to deal with up to 50 possible requests for fast intervention and to distribute the corresponding command signals with an overall reaction time of 1 ms. SGPR is based on dedicated hardware which implements decision logic for intervention requests of four different urgency levels. It despatches the intervention commands to the corresponding protective devices in the various local units. All signal paths and the decision logic are duplicated with continuous automatic checks for integrity. Single transmission from the sensors and to the protective devices is by duplicated fibre optic lines with continuous self-test. Operation of RFX is under complete control of SIGMA from a single, central control room. It houses the supervisor console, all subsystem consoles (PCs and workstations) as well as some additional equipment (SGPR display, printers, television equipment, etc.). Shot execution follows a strict sequence, which is implemented as a unique state machine on all subsystems (PLCs and VAX computers). The supervisor console is the operator interface to the system-wide state machine which controls the shot sequence. The performance of the system is considered satisfactory with scope for further improvement. The cycle time of the PLCs is below 200 ms; the picture update time on the PC-based consoles is below 3 s. The fast system acquires all channels (at the moment around 15 Mbytes of uncompressed data per shot) within around 10 min. This time includes the display of several hundreds of measurement channels on different workstations in the central control room, and the automatic execution of a number of data analysis programs.}
}
@article{YAVARI2021101956,
title = {Thermal modeling in metal additive manufacturing using graph theory – Application to laser powder bed fusion of a large volume impeller},
journal = {Additive Manufacturing},
volume = {41},
pages = {101956},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.101956},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421001214},
author = {Reza Yavari and Richard Williams and Alex Riensche and Paul A. Hooper and Kevin D. Cole and Lars Jacquemetton and Harold (Scott) Halliday and Prahalada Krishna Rao},
keywords = {Metal additive manufacturing, Thermal history, Mesh-free simulation, Graph theory, Large volume parts},
abstract = {Despite its potential to overcome the design and processing barriers of traditional subtractive and formative manufacturing techniques, the use of laser powder bed fusion (LPBF) metal additive manufacturing is currently limited due to its tendency to create flaws. A multitude of LPBF-related flaws, such as part-level deformation, cracking, and porosity are linked to the spatiotemporal temperature distribution in the part during the process. The temperature distribution, also called the thermal history, is a function of several factors encompassing material properties, part geometry and orientation, processing parameters, placement of supports, among others. These broad range of factors are difficult and expensive to optimize through empirical testing alone. Consequently, fast and accurate models to predict the thermal history are valuable for mitigating flaw formation in LPBF-processed parts. In our prior works, we developed a graph theory-based approach for predicting the temperature distribution in LPBF parts. This mesh-free approach was compared with both non-proprietary and commercial finite element packages, and the thermal history predictions were experimentally validated with in-situ infrared thermal imaging data. It was found that the graph theory-derived thermal history predictions converged within 30–50% of the time of non-proprietary finite element analysis for a similar level of prediction error. However, these prior efforts were based on small prismatic and cylinder-shaped LPBF parts. In this paper, our objective was to scale the graph theory approach to predict the thermal history of large volume, complex geometry LPBF parts. To realize this objective, we developed and applied three computational strategies to predict the thermal history of a stainless steel (SAE 316L) impeller having outside diameter 155 mm and vertical height 35 mm (700 layers). The impeller was processed on a Renishaw AM250 LPBF system and required 16 h to complete. During the process, in-situ layer-by-layer steady state surface temperature measurements for the impeller were obtained using a calibrated longwave infrared thermal camera. As an example of the outcome, on implementing one of the three strategies reported in this work, which did not reduce or simplify the part geometry, the thermal history of the impeller was predicted with approximate mean absolute error of 6% (standard deviation 0.8%) and root mean square error 23 K (standard deviation 3.7 K). Moreover, the thermal history was simulated within 40 min using desktop computing, which is considerably less than the 16 h required to build the impeller part. Furthermore, the graph theory thermal history predictions were compared with a proprietary LPBF thermal modeling software and non-proprietary finite element simulation. For a similar level of root mean square error (28 K), the graph theory approach converged in 17 min, vs. 4.5 h for non-proprietary finite element analysis.}
}
@article{SHYUE2001678,
title = {A Fluid-Mixture Type Algorithm for Compressible Multicomponent Flow with Mie–Grüneisen Equation of State},
journal = {Journal of Computational Physics},
volume = {171},
number = {2},
pages = {678-707},
year = {2001},
issn = {0021-9991},
doi = {https://doi.org/10.1006/jcph.2001.6801},
url = {https://www.sciencedirect.com/science/article/pii/S0021999101968019},
author = {Keh-Ming Shyue},
abstract = {A simple interface-capturing approach proposed previously by the author for efficient numerical resolution of multicomponent problems with a van der Waals fluid [J. Comput. Phys., 156 (1999), pp. 43–88] is extended to a more general case with real materials characterized by a Mie–Grüneisen equation of state. As before, the flow regime of interests is assumed to be homogeneous with no jumps in the pressure and velocity (the normal component of it) across the interfaces that separate two regions of different fluid components. The algorithm uses a mixture type of the model system that is formed by combining the Euler equations of gas dynamics for the basic conserved variables and an additional set of effective equations for the problem-dependent material quantities. In this approach, the latter equations are introduced in the algorithm primarily for an easy computation of the pressure from the equation of state, and are derived so as to ensure a consistent modeling of the energy equation near the interfaces where two or more fluid components are present in a grid cell, and also the fulfillment of the mass equation in the other single component regions. A standard high-resolution wave propagation method designed originally for single component flows is generalized to solve the proposed system for multicomponent flows, giving an efficient implementation of the algorithm. Several numerical results are presented in both one and two space dimensions that show the feasibility of the method with the Roe Riemann solver as applied to a reasonable class of practical problems without introducing any spurious oscillations in the pressure near the interfaces. This includes results obtained using a multicomponent version of the AMRCLAW software package of Berger and LeVeque for the simulation of the impact of an underwater aluminum plate to a copper plate in two space dimensions.}
}
@incollection{CSER198775,
title = {MUSIC: A TOOL FOR SIMULATION AND REAL-TIME CONTROL},
editor = {D. FLORIAN and V. HAASE},
booktitle = {Software for Computer Control 1986},
publisher = {Pergamon},
address = {Amsterdam},
pages = {75-79},
year = {1987},
series = {IFAC Symposia Series},
isbn = {978-0-08-034083-8},
doi = {https://doi.org/10.1016/B978-0-08-034083-8.50017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008034083850017X},
author = {J. Cser and P.M. Bruijn and H.B. Verbruggen},
abstract = {A program package, called MUSIC, has been developed for implementing simulation and real-time control algorithms on PDP-11 and VAX family computers. The control structure can be described by means of a block diagram. The blocks represent the algorithms and control functions, while the topology is defined by the connections of the blocks. A set of standard blocks is available and the user can define and add his own blocks to the system using a standard software interface. Blocks can have adjustable parameters and an arbitrary internal complexity. From the selected set of blocks an actual simulation or real-time control program and a database can be built using a program generator. The interaction with this control program is performed via a separate command interface program. With this interface the user may supply interactively all the information describing the control system. A user-written optional third program can also be added to the system to perform additional calculations outside the control loop or to realise individual command interfaces. A graphic display facility facilitates the evaluation of results and the comparison of a set of solutions. The whole system is embedded in a user-friendly menu, so the user hardly needs any computer-dependent particular knowledge.}
}
@incollection{DIMIROVSKI1990257,
title = {EXPERT SYSTEM FOR RECOGNITION AND TYPICAL IDENTIFICATION OF DYNAMIC PROCESS MODELS},
editor = {R. HUSSON},
booktitle = {Advanced Information Processing in Automatic Control (AIPAC '89)},
publisher = {Pergamon},
address = {Oxford},
pages = {257-262},
year = {1990},
series = {IFAC Symposia Series},
isbn = {978-0-08-037034-7},
doi = {https://doi.org/10.1016/B978-0-08-037034-7.50046-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080370347500468},
author = {G.M. Dimirovski and B.L. Crvenkovski and D.M. Joskovski},
abstract = {ABSTRACT
Expert systems (ES) for support in inginering aplication are attracting extensive research Investments with respect to all resources resently. ES for support in pattern recognition, model selection and Identification, skilled value judgement and decision making in large scale computerized systems for supervision, control and management of complex tehnological objects are to be come indispencible. They have to be based on both Artificial intelligence (Al) and Systems Engineering (SE) techniques. There has been designed and Implemented with us an Al system, with the nessery interactive package for a personal computer, called ESTIO and aimed at process behaviour recognition and typical identification using certain classes of well posed models in the role of a knowledge base. It has been used for Identification purposes by people with basic knowledge in systems and control, and no knowledge in Al and software engineering.}
}
@article{FAVENESI1990171,
title = {Distributed finite element analysis using a transputer network},
journal = {Computing Systems in Engineering},
volume = {1},
number = {2},
pages = {171-182},
year = {1990},
note = {Computational Technology for Flight Vehicles},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(90)90005-6},
url = {https://www.sciencedirect.com/science/article/pii/0956052190900056},
author = {J. Favenesi and A. Danial and J. Tombrello and J. Watson},
abstract = {Relatively recent innovations in chip level processing architectures have resulted in a family of powerful, low cost devices, specifically designed for concurrent, parallel computation—the transputers. An innovative parallel software research effort is described which has resulted in an impressive demonstration of the low cost potential that distributed parallel algorithms and transputer networks offer specifically to finite element structural analysis—and by relatively direct extension to other compute intense engineering and scientific problems. The principal objective of the research was to demonstrate the cost effective acceleration of real world finite element structural analysis problems using a transputer-based parallel processing network. This objective was accomplished in the form of a parallel processing workstation. The resulting workstation is a desktop size, low maintenance computing unit capable of supercomputer performance at approximately two orders of magnitude lower cost. The demonstration capability implemented permits linear static structural analysis equivalent to commercially available NASTRAN. Finite element model files, using an on-line pre-processing module or external pre-processing packages, are downloaded to a network of 32 transputers for accelerated solution. The system currently executes at about one third CRAY X-MP24 speed but additional acceleration potential exists. Results are presented for a NASA selected demonstration problem of a space shuttle main engine turbine blade model with 1500 nodes and 4500 independent degrees of freedom. The NASA CRAY X-MP24 required 23.9 s to obtain the solution while the transputer network, operated from an IBM PC-AT host, required 71.7 s. Innovative parallel sparse solution techniques and supporting hardware configuration features are the principal topics discussed.}
}
@incollection{KRATZER19811,
title = {INTERFACING REAL-TIME OPERATING SYSTEMS TO PROCESS CONTROL LANGUAGES},
editor = {M.I. HALPERN},
booktitle = {Annual Review in Automatic Programming},
publisher = {Pergamon},
pages = {1-16},
year = {1981},
isbn = {978-0-08-020242-6},
doi = {https://doi.org/10.1016/B978-0-08-020242-6.50006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008020242650006X},
author = {G. Kratzer and G. Schrott},
abstract = {The paper gives an overview over an operating system which provides for a flexible interface to support a wide spectrum of real-time language facilities. Due to its modular structure it can be tailored to the requirements of the language. In many cases this is achieved by an automatic, dialogue controlled generation of the system. For more serious changes well defined “basic functions” of the system have to be changed or new functions can easily be introduced. On the assembler level the system requests (SVC) are available as macro statements defined similar to real-time language syntax. A list of these macro calls is explicitly presented to give an overview over an implementation for the pdp 11 computer family; it shows the range in the definition of such a macro library. Based on this lowest level three methods to implement system requests in real-time languages are discussed. If the language allows assembler insertions the macros can immediately be used. If the real-time facilities of the language are connected via subroutine calls, they are realized by adding code procedures which contain the corresponding macros. In the third case, real-time operations are part of the syntax and can be compiled using the macros as inline code or generating adequate subroutine calls. These three methods are discussed with respect to readability, portability, software-safety, run-time efficiency and implementation costs. Implementations are illustrated by a few examples.}
}
@article{BIRELY2012455,
title = {A model for the practical nonlinear analysis of reinforced-concrete frames including joint flexibility},
journal = {Engineering Structures},
volume = {34},
pages = {455-465},
year = {2012},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2011.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0141029611003725},
author = {Anna C. Birely and Laura N. Lowes and Dawn E. Lehman},
keywords = {Beam–column joints, Analytical models, Rotational springs, Ductility},
abstract = {A model is developed to simulate the nonlinear response of planar reinforced-concrete frames including all sources of flexibility. Conventional modeling approaches consider only beam and column flexibility using concentrated plasticity or springs to model this response. Although the joint may contribute the majority of the deformation, its deformability is typically not included in practice. In part, this is because few reliable, practical approaches for modeling all sources of frame nonlinearity are available. The research presented herein was undertaken to develop a practical, accurate nonlinear model for reinforced concrete frames. The model is appropriate for predicting the earthquake response of planar frames for which the nonlinearity is controlled by yielding of beams and/or non-ductile response of joints and is compatible with the ASCE/SEI Standard 41-06 nonlinear static procedure. The model was developed to facilitate implementation in commercial software packages commonly used for this type of nonlinear analysis. The nonlinearity is simulated by introducing a dual-hinge lumped-plasticity beam element to model the beams framing into the joint. The dual-hinge comprises two rotational springs in series; one spring simulates beam flexural response and one spring simulates joint response. Hinge parameters were determined using data from 45 planar frame sub-assemblage tests. Application of the model to simulate the response of these sub-assemblages shows that the model provides accurate simulation of stiffness, strength, drift capacity and response mechanism for frames with a wide range of design parameters.}
}
@article{LEVINSON199693,
title = {Exchanging SGML documents using internet mail and MIME},
journal = {Computer Standards & Interfaces},
volume = {18},
number = {1},
pages = {93-102},
year = {1996},
note = {Special Issue: SGML into the Nineties},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(95)00030-5},
url = {https://www.sciencedirect.com/science/article/pii/0920548995000305},
author = {Edward Levinson},
keywords = {Electronic mail, Document exchange, Multi-media, Internet, SGML, SDIF, MIME},
abstract = {The Multipurpose Internet Mail Extensions (MIME) provides an extensible capability to receive, via electronic mail, more than just plain text. Its capabilities include audio, graphics and video. As an Internet Draft Standard [4] it is being widely deployed in commercial and public domain mail systems. The extensibility makes it an attractive vehicle for the exchange of documents that use the Standard Generalized Markup Language (SGML) [8]. This paper reports on work in integrating the MIME and SGML standards. MIME is the result of an Internet Engineering Task Force effort to expand the capabilities of Internet mail without disturbing the existing base of text mail systems. It provides mechanisms for content labelling and multiple content parts within the Internet message body. There are seven basic MIME content types, text, image, video, audio, application, message, and multipart. Multipart indicates a message that contains multiple body parts each of which is an independent message part; the others represent atomic message units. Each content type has associated with it a set of subtypes that the MUA uses to precisely identify the contents and then to invoke the appropriate software to process that content. MIME can also be used to create an ad hoc encoding for the SGML Document Interchange Format (SDIF) [7] data stream, allowing a MIME capable mail user agent (MUA) to directly display the encoded documents. For SGML and SDIF processing new subtypes are proposed that identify the documents and allow for the appropriate processing. When exchanging an SGML document the document's internal entity and process structure must be transferred along with the files. Maximum utility occurs when these structures are represented in a system independent manner. The MIME approach exposes those structure and transports them by providing them with a canonical representation. An experimental implementation was built using the publicly available software packages, sgmls (a SGML parser) and mh (a mail user agent). They were modified to generate and read MIME encoded SGML documents.}
}
@article{CALISE2017530,
title = {Polygeneration system based on PEMFC, CPVT and electrolyzer: Dynamic simulation and energetic and economic analysis},
journal = {Applied Energy},
volume = {192},
pages = {530-542},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2016.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0306261916311059},
author = {Francesco Calise and Rafal Damian Figaj and Nicola Massarotti and Alessandro Mauro and Laura Vanoli},
keywords = {Concentrated photovoltaic/thermal collector, Proton exchange membrane fuel cell, Electrolyzer, Heating and cooling system, Transient analysis},
abstract = {This paper presents a dynamic simulation model and an energetic and economic analysis of novel polygeneration system. The system integrates: cogenerative Proton Exchange Membrane Fuel Cell (PEMFC), Concentrated PhotoVoltaic-Thermal (CPVT) collectors, alkaline electrolyzer and single-stage LiBr/H2O absorption chiller. The plant is designed to supply electrical energy, space heating or cooling and domestic hot water for a small university building. The system produces hydrogen and oxygen, the first one is stored and then it is supplied to the fuel cell, while the second one is sold. The electrolyzer system is powered only by the CPVT collectors, only a small amount of the solar electrical energy is available to the user. Such electric energy along with the one produced by the PEM fuel cell are used by the user and/or supplied to the grid. The system is designed and dynamically simulated using TRNSYS software package. This study is based on a model previously developed by the authors. In particular, the system was modified in order to implement the new components (CPVT, alkaline electrolyzer, hydrogen and oxygen system) in this work. Special attention is paid to the control strategy of the proposed system in order to achieve the optimal system configuration. Daily, weekly and yearly results carried out with the dynamic simulation are presented. Finally, a sensitivity analysis was performed in order to determine the system performance as a function of the main design parameters. The energetic and economic analysis shows that the system can ensure significant energy savings and it can be profitable in presence of a capital investment incentive. The total energy efficiency of the CPVT collectors, calculated with respect to the beam radiation, is above 80% and the fuel cell electrical and thermal efficiencies resulted 35.0% and 43.4%, respectively. The hydrogen production of the electrolyzer system covers 4.3% of the fuel cell hydrogen demand. The Simple Pay Back period, in case of incentive, results of about 5years when the optimal FC nominal power of 100kW is selected.}
}
@article{COURY2000587,
title = {Effects of progressive levels of industrial automation on force and repetitive movements of the wrist},
journal = {International Journal of Industrial Ergonomics},
volume = {25},
number = {6},
pages = {587-595},
year = {2000},
issn = {0169-8141},
doi = {https://doi.org/10.1016/S0169-8141(99)00045-1},
url = {https://www.sciencedirect.com/science/article/pii/S0169814199000451},
author = {Helenice Jane Cote Gil Coury and Jorge {Alfredo Léo} and Shrawan Kumar},
keywords = {Wrist movements, Electrogoniometer, Repetitive movements, Work-related musculoskeletal disorders},
abstract = {The purpose of this research was to compare the repetition of wrist movements and force produced by workers when packing pencils in manual, semi-automated and automated industrial operations. The study was conducted in a multinational pencil company which had the three production systems operating simultaneously. Highly skilled workers were in a job rotation schedule between the systems. A portable electrogoniometer set was used for measuring the wrist movements. The software V.3.11 (Biometrics prototype design) was used to compute the number of repetitions of 5° or more motion of the wrist. Highly repetitive tasks were identified in all packaging operations involving the three different and progressive stages of production automation. The highest frequencies and most stereotyped movements were recorded in the semi-automated operations, followed by the manual and automated operations. The operations required force application between 9.1 and 12.3% of maximal voluntary contraction between three operations. These results were analysed using analysis of variance. The analysis showed significant differences in frequencies of wrist motion (p<0.05) between the production systems. However, force required by the three operations were not significantly different. The findings indicate that partial automation does not necessarily decrease or eliminate motions performed by human operators. Thus the goal of automation and its level must be carefully considerated prior to implementation. Relevance to industry The intensification of the industrial work seems to be influencing job contents, working movements and the rates of musculoskeletal disorders. The study of the relation between repetition and progressive levels of industrial automation may contribute to the understanding of this event.}
}
@article{DIERING1988151,
title = {Three-dimensional stress analysis: A formulation for mining problems},
journal = {Computers and Geotechnics},
volume = {5},
number = {2},
pages = {151-170},
year = {1988},
note = {Special Issue on Mining},
issn = {0266-352X},
doi = {https://doi.org/10.1016/0266-352X(88)90043-2},
url = {https://www.sciencedirect.com/science/article/pii/0266352X88900432},
author = {J.A.C. Diering and Y.S. Yu},
abstract = {Mathematical and computational aspects of a three-dimensional boundary element formulation designed specifically for application to mining rock mechanics problems are described. Stress analysis of mining geometries differs in many respects from equivalent analyses of civil or mechanical engineering. These differences are highlighted and the manner in which these differences are accomodated by the formulation is described. The formulation has been implemented into a program BEAP (Boundary Element Analysis Package). A test problem is presented and compared with other formulations. Design goals for the program include the following: ease of use, ability to run on personal computers, use of an iterative equation solver to reduce total CPU for large problems, use of single precision to store matrix coefficients thereby reducing disk storage requirements, limited application to multi-subregion problems and easy inter-facing to third party software for pre- and post-processing. Finally, a practical mining example is presented. It is concluded that the formulation is very well suited to analysis of mining rock mechanics problems, providing a sound basis for future developments in this field.}
}
@article{CHEN1992225,
title = {Development of vision impaired task and assignment lexicon via an ergonomics and expert system approach},
journal = {International Journal of Industrial Ergonomics},
volume = {10},
number = {3},
pages = {225-240},
year = {1992},
issn = {0169-8141},
doi = {https://doi.org/10.1016/0169-8141(92)90036-Y},
url = {https://www.sciencedirect.com/science/article/pii/016981419290036Y},
author = {Jen-Gwo Chen},
keywords = {Work measurement, workplace design, visually impaired, expert systems},
abstract = {Visually impaired people can be placed in appropriate types of jobs compatible with their abilities through rehabilitation and training. However, since their performance may be constrained by their physical limitations they cannot function efficiently and effectively without a fair work measurement system and the proper workplace design. The objective of this study was to develop an integrated computerized system, entitled VITAL (Vision Impaired Task and Assignment Lexicon), to perform work measurement and workplace design tasks for the visually impaired workers. VITAL consists of three modules: Ergonomics Consultation, Disability Index, and Work Measurement. The system, implemented on a microcomputer, was developed using three software packages, namely, EXSYS, FoxBASE+, and QuickBASIC. The ergonomicc consultation module gives recommendations regarding workplace design by analyzing an individual's vocational evaluation test record with ergonomics considerations via an interactive question and answer session. The disability index module evaluates the individual's residual capacities in performing industrial assembly tasks. The work measurement module simulates and evaluates the proposed workplace designs and assesses the amount of time required for tasks performed. The system performance evaluation indicated that VITAL could be used as a tool to help nonprofessional vocation rehabilitators perform work measurement and workplace design tasks.}
}
@article{KRATZER19801,
title = {Interfacing real-time operating systems to process control languages},
journal = {Annual Review in Automatic Programming},
volume = {9},
pages = {1-16},
year = {1980},
note = {Proceedings of the IFAC/IFIP Workshop},
issn = {0066-4138},
doi = {https://doi.org/10.1016/0066-4138(80)90001-4},
url = {https://www.sciencedirect.com/science/article/pii/0066413880900014},
author = {G. Kratzer and G. Schrott},
keywords = {computer control, portable software, real-time operating systems, real-time programming languages, standards},
abstract = {The paper gives an overview over an operating system which provides for a flexible interface to support a wide spectrum of real-time language facilities. Due to its modular structure it can be tailored to the requirements of the language. In many cases this is achieved by an automatic, dialogue controlled generation of the system. For more serious changes well defined “basic functions” of the system have to be changed or new functions can easily be introduced. On the assembler level the system requests (SVC) are available as macro statements defined similar to real-time language syntax. A list of these macro calls is explicitly presented to give an overview over an implementation for the pdp 11 computer family; it shows the range in the definition of such a macro library. Based on this lowest level three methods to implement system requests in real-time languages are discussed. If the language allows assembler insertions the macros can immediately be used. If the real-time facilities of the language are connected via subroutine calls, they are realized by adding code procedures which contain the corresponding macros. In the third case, real-time operations are part of the syntax and can be compiled using the macros as inline code or generating adequate subroutine calls. These three methods are discussed with respect to readability, portability, software-safety, runtime efficiency and implementation costs. Implementations are illustrated by a few examples.}
}
@incollection{MARTINEZ198953,
title = {WASTE-WATER TREATMENT TEST PLANT CONTROL: COMPUTER AIDED MODERN CONTROL TEACHING},
editor = {D.A. LINKENS and D.P. ATHERTON},
booktitle = {Trends in Control and Measurement Education},
publisher = {Pergamon},
address = {Oxford},
pages = {53-59},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035736-2},
doi = {https://doi.org/10.1016/B978-0-08-035736-2.50016-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357362500165},
author = {M. Martinez and J. Salt and P. Albertos and F. Morant},
abstract = {An experimental work on the control design for waste-water treatment test-plant control computer aided modern control teaching has been conceived. The work presents three sucesive stages: First, the student must identify the process with the instrumentation available at the control laboratory. He shall use a personal computer with A/D and D/A converters connected to the process as well as a software package suitable for identification. In a second step, the student must analyze the process as well as design a digital regulator to get the desired behaviour of the global process according to the requeriments previously established by the teacher. In the last step, each student or group of them must verify their design in the test plant and must explain the conclusions about the practical results they have obtained. Our work has been prepared on the test plant in such a way that different control schemes can be implemented.}
}
@article{BOSCHE201290,
title = {Plane-based registration of construction laser scans with 3D/4D building models},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {1},
pages = {90-102},
year = {2012},
note = {Network and Supply Chain System Integration for Mass Customization and Sustainable Behavior},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000784},
author = {Frédéric Bosché},
keywords = {Construction, Coarse Registration, Laser Scan, Point Cloud, 3D model, BIM},
abstract = {With the development of building information modelling (BIM) and terrestrial laser scanning (TLS) in the architecture, engineering, construction and facility management (AEC/FM) industry, the registration of site laser scans and project 3D (BIM) models in a common coordinate system is becoming critical to effective project control. The co-registration of 3D datasets is normally performed in two steps: coarse registration followed by fine registration. Focusing on the coarse registration, model-scan registration has been well investigated in the past, but it is shown in this article that the context of the AEC/FM industry presents specific (1) constraints that make fully-automated registration very complex and often ill-posed, and (2) advantages that can be leveraged to develop simpler yet effective registration methods. This paper thus presents a novel semi-automated plane-based registration system for coarse registration of laser scanned 3D point clouds with project 3D models in the context of the AEC/FM industry. The system is based on the extraction of planes from the laser scanned point cloud and project 3D/4D model. Planes are automatically extracted from the 3D/4D model. For the point cloud data, two methods are investigated. The first one is fully automated, and the second is a semi-automated but effective one-click RANSAC-supported extraction method. In both cases, planes are then manually but intuitively matched by the user. Experiments, which compare the proposed system to software packages commonly used in the AEC/FM industry, demonstrate that at least as good registration quality can be achieved by the proposed system, in a simpler and faster way. It is concluded that, in the AEC/FM context, the proposed plane-based registration system is a compelling alternative to standard point-based registration techniques.}
}
@incollection{HO1982103,
title = {Chapter 3 - High-Performance Computer Packaging and the Thin-Film Multichip Module},
editor = {Norman G. Einspruch},
series = {VLSI Electronics Microstructure Science},
publisher = {Elsevier},
volume = {5},
pages = {103-143},
year = {1982},
issn = {0736-7031},
doi = {https://doi.org/10.1016/B978-0-12-234105-2.50008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780122341052500082},
author = {C.W. HO},
abstract = {Publisher Summary
This chapter presents an introduction to the trend for high-performance computer packaging and the thin-film multichip module. High-performance computers can be characterized in a number of ways. One way is to examine the data width. High-performance computers usually have a data width of 72 bits whereas medium- to low-performance computers have data widths of 32 bits or less. Another way is to compare the machine instruction set supported by the computer. High-performance computers may have floating-point instructions and/or vector-processing capabilities implemented by using separate hardware whereas lower performance computers emulate these instructions through software by using simpler machine instructions. In designing and manufacturing these high-performance computers, there are several factors that limit the number of circuits that can be integrated into the chip. The capability for removing heat from the chip is certainly one limiting factor. The design flexibility needed to make many wiring changes on the chip for a complex machine prevents the optimal use of silicon for each chip. High-speed computers are dominated by bipolar devices that, inherently, require more processing steps than the field-effect transistor (FET) devices, and this tends to limit the yield and the integration level compared with the FETs. For a thin-film module carrying several chips, each containing a few thousand circuits, the total module can contain a few hundred thousand circuits, which can constitute the CPU of a future high-end computer or a significant portion of the CPU, and can, therefore, make a very powerful computation entity. The module can further be extended into more than one plane pair of wiring to allow even higher interconnection densities for submicron silicon chips.}
}
@incollection{PILLMANN1980189,
title = {THE PROCESS COMPUTER CONTROL SYSTEM OF THE HYDROELECTRIC POWER STATIONS ALONG THE RIVER DANUBE IN AUSTRIA},
editor = {R. Isermann and H. Kaltenecker},
booktitle = {Digital Computer Applications to Process Control},
publisher = {Pergamon},
pages = {189-196},
year = {1980},
isbn = {978-0-08-026749-4},
doi = {https://doi.org/10.1016/B978-0-08-026749-4.50025-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080267494500254},
author = {W. Pillmann and R. Stefanich},
abstract = {Abstract
In Austria the control centers of the six hydroelectric power stations along the river Danube are equipped with process computers. The control systems are designed for advanced control of the plants, including multilevel process control, interactive man-machine interfaces, advanced monitoring, security related functions and for supervising the hydroelectric plant cascade in the future. During implementation particular attentions was paid to the water level and flow rate control system in the power stations. The principles of design, the realisation of this direct digital control and the up to date experiences with the controller operation are presented in this paper. Furthermore hardware structure of the control center, the construction principles of the software package and the man machine interface are discussed briefly.}
}
@article{VANPAEPEGEM20011,
title = {Experimental set-up for and numerical modelling of bending fatigue experiments on plain woven glass/epoxy composites},
journal = {Composite Structures},
volume = {51},
number = {1},
pages = {1-8},
year = {2001},
issn = {0263-8223},
doi = {https://doi.org/10.1016/S0263-8223(00)00092-1},
url = {https://www.sciencedirect.com/science/article/pii/S0263822300000921},
author = {W. {Van Paepegem} and J. Degrieck},
keywords = {Fatigue, Composites, Bending experiments, Damage model, Residual stiffness},
abstract = {In general, fatigue of fibre-reinforced composite materials is a quite complex phenomenon, and a large research effort is being spent on it today. Due to deficiencies in the current life prediction methodologies for these materials, composite structures are often overdesigned: large factors of safety are adopted and extensive prototype-testing is required to allow for an acceptable life time prediction. This paper presents an investigation of the fatigue performance of plain woven glass/epoxy composite materials and of the numerical modelling of these composites’ behaviour under fatigue. First, the experimental set-up, which has been developed for bending fatigue experiments, is discussed. The materials used are plain woven glass/epoxy specimens in two configurations: [#0°]8 and [#45°]8. Experiments show that these two specimen types, although being made of the same material, have a quite different damage behaviour and that the stiffness degradation follows a different path. Next, a numerical model is presented which allows one to describe the degradation behaviour of the composite specimen during its fatigue life. This model has been implemented in a mathematical software package (MathcadTM) and proves to be a useful tool to study the fatigue degradation behaviour of composite materials.}
}
@article{RUB1976171,
title = {Operating System Modules for Process Control Application},
journal = {IFAC Proceedings Volumes},
volume = {9},
number = {2},
pages = {171-174},
year = {1976},
note = {1st IFAC/IFIP Symposium on Software For Computer Control, Tallinn, USSR, 25-28 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)67420-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017674202},
author = {W. Rüb and G. Schrott},
abstract = {Any effort towards portable software for special process control problems has to provide for efficient, economic and adaptable operating system design. In this paper, an approach is shown developing a basic layer suitable for constructing a dedicated realtime operating system with little ef-fort. These functions are divided into mod-ules according to their scope: interrupt handling, processor dispatching, synchro-nization, communication, short-time sched-uling and primitive input/output. The re-sulting program package yields a tool for the construction of a realtime operating system. Hardware-dependent parts are func-tionally described and strategies are sep-arated into modules which allows easy mod-ifications to adapt to special applica-tions. The concept is illustrated with a set of functions implemented for the PDP11 family. Construction, implementation de-tails and experiences are given.}
}
@incollection{TAMRAKAR2018159,
title = {Chapter 7 - Advanced multiphase hybrid model development of fluidized bed wet granulation processes},
editor = {Ravendra Singh and Zhihong Yuan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {41},
pages = {159-187},
year = {2018},
booktitle = {Process Systems Engineering for Pharmaceutical Manufacturing},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63963-9.00007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639639000075},
author = {Ashutosh Tamrakar and Dheeraj R. Devarampally and Rohit Ramachandran},
keywords = {Fluidized bed granulation, Hybrid particulate modeling, Granulation, Population balance model, Discrete element model, Computational fluid dynamics},
abstract = {In pharmaceutical manufacturing, fluidized bed granulation is one of the common processing options available to achieve better flowability of powders through size enlargement of primary particles. In fact over the last 50 years, various fluidized bed operations, including freezing, drying, impregnation, coating, etc., have become a common place in the chemical processing industry due to the high level of contacts between fluids and solids attainable in a fluid bed system. These complex interactions between the fluid and particles also mean that simulating fluidized beds are still a challenging endeavor. In general, computational fluid dynamics (CFD) packages are employed to model the pressure drops in fluids; however, the presence of high concentration of solids and the complexity of granulation behavior require more advanced particle models than are available with CFD software. As a result, hybrid frameworks that utilize the strength of particulate simulations such as discrete element method (DEM) and bulk granulation modeling such as population balance model (PBM) in conjunction with CFD information are the next steps to developing practical fluid bed granulation models. This chapter aims to provide a comprehensive description of the development, implementation, and application of advanced modeling techniques for particulate processes occurring during wet granulation in a top-spray fluid bed unit operation. The presented framework demonstrates a practical process model development methodology by efficiently coupling multiphase simulation techniques (CFD-DEM-PBM), which can be used for effective process design, development, and scale-up purposes.}
}
@article{MENS200211,
title = {Barcelona, Spain, October 7-8, 2002: Graph-Based Tools (GraBaTs 2002)},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {72},
number = {2},
pages = {11-13},
year = {2002},
note = {GraBaTs 2002, Graph-Based Tools (First International Conference on Graph Transformation)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/S1571-0661(05)80523-7},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105805237},
author = {Tom Mens and Andy Schürr and Gabriele Taentzer},
abstract = {Graphs are well-known, well-understood, and frequently used means to depict networks of related items. They are successfully used as the underlying mathematical concept in various application domains. In all these domains tools are developed that store, retrieve, manipulate and display graphs as underlying data structures, despite of the fact that in most cases these graphs have a different name such as object diagrams, (meta) class diagrams, hyper documents, semantic webs etc. It is the purpose of this workshop to summarize the state of the art of graph-based tool development, bring together developers of graph-based tools in different application fields and to encourage new tool development cooperations. Motivation Graphs are an obvious means to describe structural aspects in various fields of computer science. They have been successfully used in application areas such as compiler compiler toolkits, constraint solving problems, generation of CASE tools, pattern recognition techniques, program analysis, software engineering, software evolution, software visualization and animation, and visual languages. In all these areas tools have been developed that use graphs as an important underlying data structure. Since graphs are a very general structure mechanism, it is a challenge to handle graphs in an effective way. Using graphs inside tools the following topics play an important role: efficient graph algorithms, empirical and experimental results on the scalability of graphs, reusable graph-manipulating software components, software architectures and frameworks for graph-based tools, standard data exchange formats for graphs, more general graph-based tool integration techniques, and meta CASE tools or generators for graph-based tools. The aim of the workshop on graph-based tools (GraBaTs) is to bring together developers of all kinds of graph-based tools in order to exchange their experiences, problems, and solutions concerning the efficient handling of graphs. The GraBaTs workshop is, therefore, of special relevance for the http://link.springer.de/link/service/series/0558/tocs/t2505.htm 1st Intl. Conference on Graph Transformation (ICGT) which hosts GraBaTs as a satellite event: In many cases the application of graph transformation technology requires the existence of reliable, user-friendly and efficiently working graph transformation tools. These tools in turn have to be built on top of basic services or frameworks for graphs, which are the main topic of our workshop. Today, several graph transformation tool implementations have emerged which do not share any basic graph services (e.g. for graph pattern matching or graph layout purposes) and which implement rather different graph concepts and graph transformation approaches. Some of these tools - as a kind of survey of the state of the art - were presented in a special session, which is part of the main conference as well as of this satellite workshop. The presented tools are AGG, DiaGen, Fujaba, GenGED, and UPGRADE. The GraBaTs workshop was held for 1 12 days. Its schedule contained in addition to the afore-mentioned session on graph transformation tools, an invited talk by Tiziana Margaria (University of Dortmund, Germany) on ETI, an electronic tool integration platform where graph-based tools will play an important role. Apart from four sessions with presentations of 15 accepted papers (out of 19 submissions) on various graph-based tools and tool-relevant topics, a successful discussion ''Towards Standard Exchange Formats for Graph and Graph Transformation'' took place. Workshop Issues The workshop aims at bringing together tool developers from different fields, dealing with graphs from different perspectives. In the following, we give an overview on the most important perspectives. Meta-modeling by Graphs For a long time the syntax and static semantics of most visual modeling or programming languages was only defined by means of characteristic examples and informal descriptions. To improve this situation the visual language community invented grammar-based formalisms for the definition of the syntax of their languages, such as constraint grammars, graph grammars, relational grammars, etc. Unfortunately it turned out that the grammar-based definition of visual languages is rather complicated compared with the meta-modeling approach developed in parallel. The Meta-modeling approach for the definition of visual languages uses a combination of class diagrams (ER-diagrams, etc.) and predicate logic expressions (Z, OCL, etc.) to define the syntax and static semantics of visual languages. It became popular with the standardization of the OO-modeling language UML and is used by various meta-modeling (meta-CASE) tools which are able to generate domain-specific CASE tools. The so-called MOF approach (Meta-Object Facility) is one attempt to come up with a meta-modeling standard. Despite of its limited expressiveness (compared with ER diagrams or UML class diagrams) MOF builds the basis for the formal definition of UML and other visual languages. All meta-modeling approaches used nowadays have one common property: they offer graph-like diagrams for the definition of the structure (syntax) of graph-like diagram languages. Therefore, meta-modeling is in fact the formal definition of graph languages by means of graphs which are instances of “meta” graph languages. As a consequence, meta-CASE tools are a special class of graph-based tools, which need at least basic services for storing, visualizing, and analyzing graphs. Graph Visualization Facilities for visualizing graphs are needed by all kinds of graph-based tools, independent of the fact whether they are e.g. used for meta-modeling or rule-based programming purposes. Furthermore, graph visualization techniques are the most important means for visualizing various aspects of softwarearchitectures, the dynamic behavior of running systems, their evolution history, and so forth. Software components developed for these purposes usually have to deal with huge graphs and need services for making these graphs persistent, for introducing abstractions based on hierarchical graph models, for computing reasonable graph layouts (efficiently), and for displaying graphs effectively using “fish-eye-techniques” and the like. And last but not least, graph visualization techniques are often employed for teaching purposes in computer science courses on “data structures and (graph) algorithms”. To summarize, almost all kinds of graph-based tools urgently need efficiently and effectively working graph visualization services, whereas graph visualization tools may profit from research activities on graph query and graph transformation engines for the computation of graph abstractions or views. We, therefore, hope that this workshop encourages researchers to start new cooperations, such as adapting graph visualization tools to the needs of graph manipulation tools or exploiting graph manipulation and transformation techniques to compute sensible abstractions of huge graphs. Graph Queries and Graph Algorithms Most, if not all, graph-based tools use to a certain degree software components (libraries, subsystems, etc.) for executing graph queries and/or various kinds of standard graph algorithms. For example, graph transformation tools rely on rather sophisticated means for computing graph matches (rule occurrences) and graph-based reverse engineering tools need rather powerful query engines for determining critical substructures of software architectures. On the other hand, quite a number of database management systems have already been developed using graphs (networks of related objects) as the underlying data model and offering query languages based on graph path expressions or even graph transformations. Vice versa, graph transformation languages like PROGRES are not only used for specifying and visualizing graph algorithms, but incorporate many elements of database query languages such as means for the construction of indexes, the materialization and incremental update of views, etc. Therefore, we like to encourage tool developers again to start cooperating across the boundaries of research areas. Graph Transformation Graph transformation means the rule-based manipulation of graphs. Several graph transformation approaches have emerged which differ w.r.t. to the underlying kind of graphs as well as in the way how rules are applied to graphs, i.e. graph transformation takes place. The kind of graphs used by these tools include labeled, directed graphs, hypergraphs, and graph structures. Their rules, the basic means to manipulate graphs, differ w.r.t. to the formal definition of their semantics, the way how occurrences (matches) are searched for, and how matching rules are applied eventually. In tools, graph transformation is applied to visual languages, specification, code generation, verification, restructuring, evolution and programming of software systems, etc. Developers of graph transformation tools may profit from other workshop participants concerning more efficient realizations of basic functionality, while developers of other graph-based tools might find the graph transformation paradigm attractive to implement certain graph manipulations. The workshop may also provide insights to apply these tools to other application domains. Common Exchange Formats for Graphs and Graph Transformation To support interoperability between various graph-based tools, several initiatives on the development of common exchange formats for graphs have been founded. These formats are all based on the extensible markup language XML developed to interchange documents of arbitrary types. Preceding events like three subgroup meetings of the EU Working Group APPLIGRAPH, a Workshop on Standard Exchange Formats, and a satellite workshop of the 8th Intl. Symposium on Graph Drawing (GD 2000)discussed various ideas which are currently converging to one format being GXL. During the GraBaTs workshop a further discussion round on this topic was organized focusing especially on graph layout and graph attributes. Another topic of interest for this discussion is an exchange format for graph transformation systems called GTXL, which is under development and which will be built on top of GXL. Workshop Organizers The Program Committee of the workshop consists of: Luciano Baresi (Italy)Giuseppe Di Battista (Italy)Ulrik Brandes (Germany)Scott Marshall (The Netherlands)Tom Mens (Belgium) (Co-chair)Andy Schürr (Germany) (Co-chair)Gabriele Taentzer (Germany) (Co-chair)Andreas Winter (Germany)Albert Zündorf (Germany) We are very grateful to Hartmut Ehrig for his help with the organization of the Workshop as satellite event of the 1st Int. Conference on Graph Transformation (ICGT) and to Mike Mislove, one of the Managing Editors of the ENTCS series. Thanks are also due to Fernando Orejas and his local organizers at UPC in Barcelona who supplied preprints of this volume for all workshop participants.}
}
@article{SADOWSKI2013143,
title = {Solid or shell finite elements to model thick cylindrical tubes and shells under global bending},
journal = {International Journal of Mechanical Sciences},
volume = {74},
pages = {143-153},
year = {2013},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2013.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020740313001628},
author = {Adam J. Sadowski and J. Michael Rotter},
keywords = {Shell finite elements, Continuum solid finite elements, Plasticity, Strain hardening, Thick cylindrical shells, Bending},
abstract = {This paper explores the use of solid continuum finite elements and shell finite elements in the modelling of the nonlinear plastic buckling behaviour of cylindrical metal tubes and shells under global bending. The assumptions of shell analysis become increasingly uncertain as the ratio of the radius of curvature to the thickness becomes smaller, but the classical literature does not draw a clear line to define when a shell treatment is inappropriate and a continuum model becomes essential. This is a particularly important question for the bending of tubular members, pipelines, chimneys, piles, towers and similar structures. This study is therefore concerned solely with the uniform bending of thin tubes or thick shells which fail by plastic buckling well into the strain-hardening range. The analyses employ finite element formulations available in the commercial software ABAQUS because this is the most widely used tool for parametric research studies in this domain with an extensive and diverse element library. The results are of general validity and are applicable to other finite element implementations. This paper thus seeks to determine the adequacy of a thin or thick shell approximation, taking into account geometric nonlinearity, complex equilibrium paths, limit points and bifurcation buckling, extensive material ductility and linear strain hardening. It aims to establish when it is viable to employ shell elements and when this decision will lead to outcomes that lack sufficient precision for engineering design purposes. The results show that both thin and thick (shear-flexible) shell elements may give a reasonably accurate prediction of the buckling moment under global uniform bending for cylindrical tubes as thick as R/t=10. A finite strain and thick shell formulation is additionally shown to model the ductility of such thick tubes well, even when ovalisation of the cross-section and strain hardening are included. The use of solid continuum elements to model tubes in bending is found to become increasingly uneconomical as the R/t ratio rises above 25 with reduced advantages over shell elements, both in terms of the accuracy of the solution and the computation time.}
}
@article{RITTER199497,
title = {Prototype Implementation of the Integrated Genomic Database},
journal = {Computers and Biomedical Research},
volume = {27},
number = {2},
pages = {97-115},
year = {1994},
issn = {0010-4809},
doi = {https://doi.org/10.1006/cbmr.1994.1011},
url = {https://www.sciencedirect.com/science/article/pii/S0010480984710111},
author = {O. Ritter and P. Kocab and M. Senger and D. Wolf and S. Suhai},
abstract = {We aim to develop an open software system to handle human genome data. The system, called Integrated Genomic Database (IGD), will integrate information from many genomic databases and experimental resources into a comprehensive target-end database (IGD TED). Users will access front-end client systems (IGD FRED) to download data of interest to their computers and merge them with their own local data. FREDs will provide persistent storage of, and instant access to, retrieved data; a friendly graphical interface; tools for querying, browsing, analyzing, and editing local data; interface to external analysis; and tools for communicating with the outside world. The TED will be accessible over the network (online and offline) as a read-only resource for multiple clients. It collects data from major databases for nucleotide and protein sequences and structures, genome maps, experimental reagents, phenotypes, and bibliographic data, and sets of raw data produced at genome centers and laboratories. Beside character-based access via Gopher, WAIS, FTP, and several query language interfaces to the TED, we will develop a specialized front-end client, IGD FRED, with its own database manager, based on the ACEDB program. The FRED will support graphical display methods for sequence feature maps, chromosomal genetic and physical maps, and experimental objects like clone grids, etc. FRED will also provide an interface to important analysis software packages and tools for submitting data to external databases in their own format.}
}
@article{STROEVEN2001189,
title = {Reconstructions by SPACE of the Interfacial Transition Zone},
journal = {Cement and Concrete Composites},
volume = {23},
number = {2},
pages = {189-200},
year = {2001},
note = {Special Theme Issue on Image Analysis},
issn = {0958-9465},
doi = {https://doi.org/10.1016/S0958-9465(00)00076-7},
url = {https://www.sciencedirect.com/science/article/pii/S0958946500000767},
author = {Piet Stroeven and Martijn Stroeven},
keywords = {Composition, Configuration, Hydration, Interfacial transition zone, Material model, Particle packing, Stereology, Structure-sensitivity},
abstract = {Of particular interest in concrete technology is the so-called Interfacial Transition Zone (ITZ). Conflicting experimental evidences as to the internal structure and extent of the ITZ, and even as to the very existence of it, make it attractive to confront such observations with a coherent picture of the ITZ produced by computer-simulation, although based on a simplified model concept. This contribution outlines the scientific framework for interpretation of the so-called packing phenomenon. A dynamic computer-simulation system, Software Package for the Assessment of Compositional Evolution, with the acronym SPACE was recently developed by the second author, and has been used for the present purpose. In addition to properly simulating composition of particulate materials, as can also be achieved by conventional systems based on random sequential procedures, it has been demonstrated earlier that SPACE can also more accurately reproduce configuration of particles in such materials. An hydration algorithm is additionally implemented in SPACE. Some preliminary information on gradient structures in the ITZ are discussed. The extent of packing gradients in the fresh and hardened state will be shown to vary with changes in material composition (water to cement ratio, cement fineness) and in the configuration-sensitivity of the parameter being studied. This implies structural gradients to extend further away from aggregate grain surfaces the more sensitive the very parameter is to packing configuration. Particular emphasis is given to this fundamental aspect, and not to properly estimating the ITZ's thickness of this model cement paste near boundaries. Tools are provided to extract 2-D structural data from imaginary section planes (or from imaginary thin section projections) of the ITZ parallel to the interface. By making use of stereological methods, such data are given a 3-D structural meaning. Of course, direct information on porosity, packing characteristics and local particle distributions is available in 3-D. Therefore, in some cases direct 3-D measurements are possible, as well as visualisation of parts of the structure.}
}
@article{KAZANTZIS2000115,
title = {Singular PDEs and the single-step formulation of feedback linearization with pole placement},
journal = {Systems & Control Letters},
volume = {39},
number = {2},
pages = {115-122},
year = {2000},
issn = {0167-6911},
doi = {https://doi.org/10.1016/S0167-6911(99)00096-1},
url = {https://www.sciencedirect.com/science/article/pii/S0167691199000961},
author = {Nikolaos Kazantzis and Costas Kravaris},
keywords = {Nonlinear systems, Feedback linearization, Pole placement, Singular PDEs, Lyapunov's auxiliary theorem},
abstract = {The present work proposes a new formulation and approach to the problem of feedback linearization with pole placement. The problem under consideration is not treated within the context of geometric exact feedback linearization, where restrictive conditions arise from a two-step design method (transformation of the original nonlinear system into a linear one in controllable canonical form with an external reference input, and the subsequent employment of linear pole-placement techniques). In the present work, the problem is formulated in a single step, using tools from singular PDE theory. In particular, the mathematical formulation of the problem is realized via a system of first-order quasi-linear singular PDEs and a rather general set of necessary and sufficient conditions for solvability is derived, by using Lyapunov's auxiliary theorem. The solution to the system of singular PDEs is locally analytic and this enables the development of a series solution method, that is easily programmable with the aid of a symbolic software package. Under a simultaneous implementation of a nonlinear coordinate transformation and a nonlinear state feedback law computed through the solution of the system of singular PDEs, both feedback linearization and pole-placement design objectives may be accomplished in a single step, effectively overcoming the restrictions of the other approaches by bypassing the intermediate step of transforming the original system into a linear controllable one with an external reference input.}
}
@article{GLUECKSTERN1985469,
title = {Use of microprocessors for control, data aquisition and on line performance evaluation in a reverse osmosis plant},
journal = {Desalination},
volume = {55},
pages = {469-480},
year = {1985},
issn = {0011-9164},
doi = {https://doi.org/10.1016/0011-9164(85)80091-6},
url = {https://www.sciencedirect.com/science/article/pii/0011916485800916},
author = {P. Glueckstern and M. Wilf and J. Etgar and J. Ricklis},
abstract = {Design and implementation of microprocessor's control and data aquisition in a large (over 3 MGD) reverse osmosis desalting plant will be reported. The RO plant desalt brackish water of about 6,000 ppm TDS salinity and supplies potable water to the town of Eilat. The first desalting unit of the plant commenced operation in 1978. The control system of the plant was initially based on conventional electromechanical relays, timers and counters. The measuring instruments gave local indication only. An evaluation performed a year ago indicated that in order to optimize the plant operation and to improve data aquisition, a modernization of the instrumentation and control system is required. The new control system is based on programable controlers (PLC) receiving digital and analoge signals of the field measured parameters through sensors and transmiters. The PLC transfers the information into an industrial computer which controls the operation of the desalting plant according to the software package especially prepared for this plant. The control network includes also an on line personal computer for storage of historical data and reports generation. The new system is at the final stages of construction.}
}
@article{JACOME201663,
title = {BIOMedical Search Engine Framework: Lightweight and customized implementation of domain-specific biomedical search engines},
journal = {Computer Methods and Programs in Biomedicine},
volume = {131},
pages = {63-77},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715300560},
author = {Alberto G. Jácome and Florentino Fdez-Riverola and Anália Lourenço},
keywords = {Search engine framework, Biomedical literature, Vertical engine, Text mining, Web application},
abstract = {Background and objectives
Text mining and semantic analysis approaches can be applied to the construction of biomedical domain-specific search engines and provide an attractive alternative to create personalized and enhanced search experiences. Therefore, this work introduces the new open-source BIOMedical Search Engine Framework for the fast and lightweight development of domain-specific search engines. The rationale behind this framework is to incorporate core features typically available in search engine frameworks with flexible and extensible technologies to retrieve biomedical documents, annotate meaningful domain concepts, and develop highly customized Web search interfaces.
Methods
The BIOMedical Search Engine Framework integrates taggers for major biomedical concepts, such as diseases, drugs, genes, proteins, compounds and organisms, and enables the use of domain-specific controlled vocabulary. Technologies from the Typesafe Reactive Platform, the AngularJS JavaScript framework and the Bootstrap HTML/CSS framework support the customization of the domain-oriented search application. Moreover, the RESTful API of the BIOMedical Search Engine Framework allows the integration of the search engine into existing systems or a complete web interface personalization.
Results
The construction of the Smart Drug Search is described as proof-of-concept of the BIOMedical Search Engine Framework. This public search engine catalogs scientific literature about antimicrobial resistance, microbial virulence and topics alike. The keyword-based queries of the users are transformed into concepts and search results are presented and ranked accordingly. The semantic graph view portraits all the concepts found in the results, and the researcher may look into the relevance of different concepts, the strength of direct relations, and non-trivial, indirect relations. The number of occurrences of the concept shows its importance to the query, and the frequency of concept co-occurrence is indicative of biological relations meaningful to that particular scope of research. Conversely, indirect concept associations, i.e. concepts related by other intermediary concepts, can be useful to integrate information from different studies and look into non-trivial relations.
Conclusions
The BIOMedical Search Engine Framework supports the development of domain-specific search engines. The key strengths of the framework are modularity and extensibilityin terms of software design, the use of open-source consolidated Web technologies, and the ability to integrate any number of biomedical text mining tools and information resources. Currently, the Smart Drug Search keeps over 1,186,000 documents, containing more than 11,854,000 annotations for 77,200 different concepts. The Smart Drug Search is publicly accessible at http://sing.ei.uvigo.es/sds/. The BIOMedical Search Engine Framework is freely available for non-commercial use at https://github.com/agjacome/biomsef.}
}
@incollection{MACKEY1987603,
title = {Effective Educational Courseware (The Microcomputer as a Learning Medium)},
editor = {JEF MOONEN and TJEERD PLOMP},
booktitle = {Eurit 86: Developments in Educational Software and Courseware},
publisher = {Pergamon},
pages = {603-609},
year = {1987},
isbn = {978-0-08-032693-1},
doi = {https://doi.org/10.1016/B978-0-08-032693-1.50094-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080326931500940},
author = {Brendan Mackey},
abstract = {ABSTRACT
The Paper reflects the experiences of a teacher involved in the use of Information Technology in the classroom since the mid-'70s. It is argued that much of the original promise has remained unfulfilled, mainly due to a failure to establish the true relevance of Information Technology to the Education process. A brief outline of the development of present-day courseware is given, and emphasis placed on the problems encountered in implementing it in schools. Have we succeeded only in “automating” some of the worst aspects of the “old” system? Have we really overcome the “Quiz Syndrome” (so much Software/Courseware is little more than question/answer routines). The vexing question is asked: What is Courseware, and what distinguishes it from Software? An attempt is made to define true Courseware, and the role of Courseware in the formation and presentation of knowledge to the learner. The most recent developments are discussed (Artificial Intelligence, Generic Software, Applications — finally, a possible approach is outlined based on the use of Information Handling packages designed specifically for Educational use, to be integrated into the teaching and learning processes, leading towards the use of the Computer as a true learning medium.}
}
@incollection{DATTA1989293,
title = {PARALLEL AND LARGE SCALE MATRIX COMPUTATIONS IN CONTROL: SOME IDEAS****Permanent Address: Department of Mathematical Sciences, Northern Illinois University, DeKalb, IL 60115},
editor = {CHEN ZHEN-YU},
booktitle = {Computer Aided Design in Control Systems 1988},
publisher = {Pergamon},
address = {Oxford},
pages = {293-299},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035738-6},
doi = {https://doi.org/10.1016/B978-0-08-035738-6.50051-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357386500513},
author = {B.N. Datta and K. Datta},
abstract = {The design and analysis of time-invariant linear control systems give rise to a variety of interesting linear algebra problems. Numerically effective methods now exist for several of these problems. However, algorithms for large scale computations and efficient parallel algorithms for these problems are virtually nonexistent. In this paper, we propose several efficient generalpurpose parallel algorithms for single-input and multi-input eigenvalue assignment problems. A desirable feature of these algorithms is that they are composed of simple linear algebraic operations such as matrix-vector multiplication, solution of a linear system, and computations of eigensystem and singular values of a symmetric matrix, for which efficient parallel algorithms have already been developed and parallel software libraries are being built based on these algorithms. The proposed algorithms thus have potential for implementations on some existing and future parallel processors. We also propose a numerical method for Sylvester matrix equation arising in the construction of Luenberger observer. The method does not need reduction to “condensed” forms and is thus suitable for large and sparse matrices. The method also exhibits certain parallelism.}
}
@article{WYSS1971209,
title = {STAP-12: A library system for on-line assimilation and off-line analysis of event/time data},
journal = {Computer Programs in Biomedicine},
volume = {1},
number = {4},
pages = {209-218},
year = {1971},
issn = {0010-468X},
doi = {https://doi.org/10.1016/0010-468X(71)90012-2},
url = {https://www.sciencedirect.com/science/article/pii/0010468X71900122},
author = {U.R. Wyss and H. Handwerker},
keywords = {Neurophysiology, Spike Train Analysis, Point Process, PDP-12},
abstract = {An open ended library system for neuronal spike train analysis is presented. The design is based on the experience with STAP-8, a spike train analysis program for the PDP-8/AX08 system. The system software and all I/O and analysis programs are written in assembly language for the standard PDP-12/A equipment. Single- or multi-channel assimilation of unit activity is possible by means of either AD-conversion of membrane potentials or event recording with Schmitt-trigger inputs. Multi-unit records of a single, non-selelctive electrode are sorted with respect to pre-defined threshold parameters. Data files (spike trains) with up to 2048 event times are saved on a LINC tape train library for off-line analysis. A set of 12 analysis programs is part of the basic STAP-12 system and covers the standard procedures, as interval histogram, autocorrelogram, crosscorrelogram, latency crosscorrelogram, auto- and crosscovariance estimates, serial correlogram, and Poisson parameter estimates. The user may extend the capacity of STAP-12, implementing his own specific programs.}
}
@article{PALMER200170,
title = {Abstracts of Recent Articles and Literature},
journal = {Computers & Security},
volume = {20},
number = {1},
pages = {70-74},
year = {2001},
issn = {0167-4048},
doi = {https://doi.org/10.1016/S0167-4048(01)01022-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167404801010227},
author = {Chloë Palmer and Helen Meyer},
abstract = {Law on cracking security codes toughened, Amy Harmon. The US copyright office has passed a new law making it illegal to break the security methods in place to prevent the copying of digital music, books and movies. The law will update the existing 1998 Digital Millennium Copyright Act and will come into effect immediately. The decision was opposed by groups such as Universities, programmers and libraries who argue that copyrighted work should be archived and leant out. Programmers also claimed that reverse-engineering was a valid technique in the cause of learning how technology works. The Association of American Universities has stated that in order to maintain civil liberties, there must be broad exemptions to allow for “fair use” of purchased copyrighted digital goods. The Toronto Star, 30 October, E3. Game could hold key to Net security, Gareth Cook. According to top mathematicians, the key to cracking encryption algorithms could be linked to the popular Minesweeper game which comes free with the Windows operating system. The problem, the ‘P versus NP conjecture’, asks why some problems are so difficult to solve using a computer, while others can be solved very quickly. Problems like putting a list into alphabetical order, which are easily solved by a computer are classified as P problems, and those which cannot are classed as NP. A prominent specialist challenged the conjecture in the spring edition of the Mathematical Intelligencer journal, saying that the conjecture would be proved false if the Minesweeper logic game could be cracked, for any sized board, using a computer program. If the conjecture is true, then certain types of problem will always remain insoluble by computers and the current system of encryption algorithms will continue to be considered to be safe. The Clay Mathematical Institute considers the matter to be so important that they have offered a US $1 million prize for a solution. The impact of such a discovery would be to mathematically map the boundary beyond which computers, no matter how powerful, cannot succeed. The Boston Globe, 1 November, B1, B4. MOJ proposes new laws to crack down on hackers. The Japanese Ministry of Justice has announced that it intends to toughen up Taiwan’s Criminal Code as it relates to hackers. Those who break into government, military or financial institutions would face up to 15 year jail sentences. Officials also say that any form of hacking over the Internet should be made illegal. The code will address holes in legislation relating to computer crimes and will also criminalize the distribution of pornography and selling of prohibited substances while regulating the responsibilities of ISPs, pornographic websites and those which facilitate gambling. The China Post, 23 October. Canada’s computer security loopholes called threat to US. David Akin. James Adams, senior security advisor to the US, told the PopTech2000 conference that Canada is incapable of defending itself against attacks against its telecommunications and computer networks. This endangers the US because “Canada is essentially the back door into America”, due to the amount of major US corporations which have Canadian offices. The private sector in the US and Canada do not have the right to share information gleaned by the security agencies, which leaves it more vulnerable that those from countries like France, Israel, China, Russia and India. The criticism comes as 30 countries including the US, but not Canada, are gearing up to simulate information warfare attacks. Adams also warned against purchasing hardware and software from France, China, India or Russia, “We know without any question whatsoever that a lot of things that are sold in the United States, be it a firewall or some software, are infected with implants from these countries so that anything that goes across their networks, or is stored in their programs is sent back to the country of manufacture.” Canadian National Post, 30 October. Viruses the next generation, Kim Zetter. Viruses are getting faster and more prolific. They have been around since the early 60s when they infected through floppy disks, the ‘sneaker net’. They can go around the world in minutes, sent by E-mail. With the trend within industry to use the same off the shelf products — Windows, Word, Outlook — viruses can hit millions of machines in one fell swoop. Ron Moritz at Symantec, an anti-virus company, said, “If the virus writers ever thought through their programs, we would see much more virulent viruses that would really do damage.” The threat is growing worse, and although anti-virus software is important, it can only target a virus once it has hit a few computers. In the future, viruses are likely to spread without you having to open an E-mail — a stage beyond Bubbleboy and Kak. Fred Cohen, who coined the phrase ‘computer virus’, describes mini-viruses which will dodge detection and spawn sub-variants which will cluster and attack various parts of your computer. They have already hit PDAs and mobile phones and Linux will be the next victims. The automation of inoculation has been considered as a possible protection, however it is feared that this would pass too much control onto the software suppliers — and the consequences of a hacker intercepting such a transfer would be disastrous. Others want to ban the writing of malicious code, but this is deemed to be against the principals of freedom of speech. Moritz predicts that the next generation of viruses will combine swift propagation like the Melissa and LoveLetter viruses with a destructive payload — something that neither of these two debilitating viruses actually had. NewLove followed on from LoveLetter the next month, June 1999, and closely resembles a super-virus. It had a damaging payload, overwriting system files and making computers inoperable, and could spread quickly. However, fortunately, it failed because it was so acerbic that it destroyed Outlook before it could be mailed on, and so failed to multiply. But the greatest fear is that the technology will be used for cyber-warfare. Protection in the form of anti-virus software affords some comfort, but the users must also learn caution when on the Internet or using E-mail. PC World, December, p.p. 191–203. IPSec VPNs: they’re not all virtually the same, Isaac Hillson. What you need from a VPN is speed of throughput, simple management and security that you can trust. All of the products out there have been assembled to the same IPSec standards, however there are massive differences in functionality. An independent test of 12 systems was commissioned from Network Test (www.networktest.com). The results were worrying, as some were “alarmingly vulnerable to attack”. Some of the products were 10 times faster than others, there was varying interoperability with Certificate Authorities and PKI standards were a bit of a let down. However, the best systems are definitely worth a look. The top three were Intel’s Netstructure 3130, Lucent’s VPN firewall and Alcatel’s Secure VPN Solution. The systems tested ranged from $2000 to $21 000 and full results can be found at www.commweb.com. Computer Telephony, November, p. 138. Shockwave virus creates little damage, Jaikumar Vijayan. The virus Shockwave, first reported at the end of November appears to be spreading more slowly than was expected thanks, at least in part, to an increased awareness of the spread of E-mail viruses. The virus disguises itself as an attachment to an E-mail called creative.exe and appears to come from a familiar E-mail address. If a recipient double clicks on this attachment the virus replicates onto the victim’s system and sends new copies of itself via E-mail to all the addresses in the E-mail address book. It doesn’t destroy any files, but it does rename some graphics and .zip files. Computerworld, 11 December 2000, p. 24. Is Carnivore dangerous? Controversy continues, Michael Meehan. Researchers studying the FBI’s Carnivore E-mail surveillance system have “serious concerns” about the controversial technology, despite a generally favourable draft report. The five researchers said that the review carried out by the IIT Research Institute “appears to represent a good-faith effort at [an] independent review” of the surveillance system. But, they added, “the limited nature of the analysis described in the draft report simply cannot support a conclusion that Carnivore is correct, safe or always consistent with legal limitations”. Carnivore can be legally deployed only to monitor alleged criminal activity under a court order, but privacy advocates have said that they are worried that the software could lead to widespread and random surveillance of E-mail messages. The researchers took particular issue with what they said are vague and alterable audit trails within Carnivore. Without a definitive audit trail it would be impossible to determine who had monitored what. One of the researchers said that Carnivore could be misconfigured to monitor almost anything. Computerworld, 11 December 2000, p. 24. Virus vigilance, Deborah Radcliff. In the year since ‘I LoveYou’ struck variants of this self-replicating virus are still reaching the mailbox. It seems as though a new variant strikes every day. In a two-week period a plethora of new viruses were reported, including variants of Romeo & Juliet, Navidad, Afeto and Shockwave. Like Melissa and I LoveYou they all take advantage of mail programs to spread. The problem with today’s viruses is two-fold: not only can they be easily altered to change their signatures, but they also have tempting attachment types to persuade users to open them. No matter how often you tell users not to open executables some will succumb to temptation and training is not enough. It is necessary to set up filters to block executable attachments before they enter the desktop. Blocking file types known to carry viruses and Trojan horses sounds extreme, but if these attachments aren’t used for business purposes the decision is easier. Microsoft Outlook 2000 E-mail security patch was designed to filter a number of executable file types. In addition, a number of vendors offer filtering products that can block custom-specified file types and even subject lines. So keep your anti-virus software up to date, educate your users and block unnecessary executables at the gate. Computerworld, 11 December 2000, p. 68. Bush eyes overhaul of E-security, Dan Verton. In the United States, national security experts are preparing for what could be a major change in the way the Government and the private sector prevent cyberattacks. President Elect Bush plans to appoint an IT ‘czar’ to better manage the Government’s IT investments. That move is likely to involve reorganizing the federal critical infrastructure protection effort and possibly changing the role of the FBI’s National Infrastructure Protection Center (NIPC). Changes to NIPC could involve asking Congress for new legislation to make it easier for the national security community to get access to investigative information, making NIPC subordinate to a federal IT czar or security officer, or starting from scratch with a different type of organization. Computerworld, 18 December 2000, p. 1, 85. Hospital confirms hacker stole 5000 patient files, Marc Songini. A major hospital in Seattle, Washington, USA confirmed that during last summer a hacker penetrated its computer network and stole files containing information on 5000 patients. Officials at the University of Washington Medical Center said that the hacker, calling himself ‘Kane’, stole users’ passwords and copied thousands of files. The hacker gained access to the hospital’s network through an exposed Linux server in the hospital’s pathology department. The medical centre suspected a breach but was unaware that the files had been stolen until Kane provided information about the intrusion to SecurityFocus.com, a website that focuses on security issues. Kane, who is believed to live in the Netherlands, sent the files to the website to verify that he had accessed sensitive data. Kane it is believed, views himself as an ethical hacker. He used sniffer software to steal the electronic identifications of a number of hospital employees from the exposed server and then used the credentials to access files related to patients in the medical centre’s cardiology and rehabilitation departments. Computerworld, 18 December 2000, p. 7. Hacker cracks Egghead’s security. Online technology retailer Egghead.com had revealed that a hacker penetrated its computer systems, possibly including the customer databases where the company stores credit-card numbers and other personal information about the users of its website. Egghead reported the hacking incident to credit-card companies “as a precautionary measure”, in case card numbers had been stolen by the intruder. Computerworld, 1 January 2001, p. 6. Big brother plan slammed, Steve Ranger. A proposal by the security services in the UK to collect and keep records on phone and Web usage for seven years by everyone in the country has been attacked by MPs and industry experts. The document, written by the deputy director general of the National Criminal Intelligence Service, said that telephone operators and Internet service providers should hold onto phone records, mobile phone location data, E-mail and Web usage information for a year. The data would then by moved to a government data warehouse for another six years. A budget of £3 million has been proposed to set up the warehouse, which would be combined with the project to develop the controversial ‘black boxes’ which ISPs would plug into systems to snoop on E-mail and Web activity. The communications industry is concerned that it may by caught between demands from law enforcement to retain information and the Data Protection Act, under which it must delete information as soon as it can. Computing, 7 December 2000, p. 3. Network Associates says code is safe, Ian Lynch. The anti-virus company, Network Associates, denies that its anti-virus source code was compromised by hackers following a breach of security at two of the company’s Brazilian sites. “No files or data were compromised by the hack”, said Jack Clark. The company claims that the hackers only accessed sites hosted by a local contractor, Brazilian Internet service provider Matrix. Clark admitted that the company was embarrassed by the incident. “It’s embarrassing. Using a local ISP was deemed to be a better solution at the time, as it was thought to give our Brazilian customers better performance. It appears that Matrix was slower responding to a published security vulnerability than we’d have liked it to have been.” Computing, 7 December 2000, p. 8. IBM provides key to faster secure communications, John Geralds. IBM has developed a way of simultaneously performing software encryption and authentication that it claims is twice as fast as other methods. The security algorithm, which has yet to be given a name, uses the same secret key to encrypt and decrypt a message. Existing approaches require encryption and authentication to be performed separately to ensure secure communications. Using parallel processors gives even faster performance as the encryption is spread over multiple processors. IBM also expects the algorithm to be used in fibre optic networks and several E-business applications. The company has submitted the algorithm to the National Institute of Standards as a standard for securing communications. Computing, 7 December 2000, p. 10. National security threatened by Internet, studies say, Dan Verton. During the next 15 years, the US will face a new breed of Internet-enabled terrorists, criminals and national adversaries that could launch attacks with computer viruses and logic bombs. A report by the CIA’s National Intelligence Council (NIC) mentioned critical electronic infrastructure protection and information warfare only briefly, it warned Americans that adversaries worldwide are hard at work developing tools to bring down the USA’s private sector infrastructure. Another report, by the Center for Strategic and International Studies, went even further, warning of a future cyber arms race and the rise of terrorist groups supported by “computer-literate youngsters” bent on disrupting the Internet. China is of particular concern because it’s devising strategies for unrestricted electronic warfare. Online extortion and falsification of shipping manifests by criminals, and attempts by countries to use hacking techniques to evade trade sanctions are a growing concern. Officials are also becoming increasingly concerned with the proliferation of ‘always on’ Internet applications, such as modems and network printers. Hackers are finding ways to penetrate these devices and possibly use them as launching pads for more devastating distributed denial-of-service attacks. Computerworld, 1 January 2001, p. 7. CSSA targets security issues for E-commerce, Andy McCue. E-commerce companies will be told by their trade associations to take a more targeted approach to security. The Computing Services and Software Association (CSSA) is working with the Alliance for Electronic Business in the UK to spearhead a programme that will allow companies to report security incidents anonymously for analysis by security experts. Users will have access to tailored guidelines for their industry sector based on the principles of BS7799. The proposals were discussed by representatives from the UK’s DTI, the Confederation of British Industry, CMG Admiral, Microsoft, Unisys, Baltimore Technologies and the Post Office. Computing, 14 December 2000, p. 6. Security spend must rise, Liesbeth Evers. According to a research report released by Forrester Research, security managers plan to triple their budgets over the next four years. However, even with this increase, half of the 50 security managers questioned said that the lack of money would prevent them from implementing all the security measures that were required. Frank Prince, the author of the report, The Security Market, predicts that by 2005, many companies would have implemented automated E-business systems such as hands-free procurement. “Automated processes need automated security”, he said. “Firms will struggle with cross-company security, as this requires agreeing with standards at many levels.” Companies should make business managers responsible for all aspects of security, and should formalize this by creating an integrated corporate security policy. Over time, criminals will discover new ways to use the Internet to aid crime. Computing, 7 December 2000, p. 67. Crossing the wireless security gap, Alan Radding. Wireless security is difficult to implement, requiring organizations to piece together myriad technologies. Most organizations would prefer to support only a single security model for E-commerce, preferably the Internet model in use today. E-commerce in the wired world relies primarily on Secure Sockets Layer (SSL), but when you try to move this approach to the wireless world, you immediately encounter problems starting with cellular phones with Wireless Application Protocol (WAP) capabilities. WAP phones are pretty limited when it comes to security and lack the CPU power and memory necessary for RSA encryption, a key element of SSL. WAP devices include their own security protocol, Wireless Transport Layer Security (WTLS). This is equivalent to SSL but it uses less resource-intensive encryption algorithms, such as elliptic-curve cryptography. Wireless messages travel through the air to the carrier’s transmitter, where they are received and passed to a gateway that funnels them into the conventional wired network for transmission to the destination. At the gateway, the WTLS message is converted into SSL. For a brief moment, the message sits unencrypted inside the gateway, creating a security vulnerability. Many of the obstacles confronting wireless security will disappear with the widespread adoption of third-generation wireless technology. The third-generation phones will be IP-based and sport more processing power, memory and bandwidth, which should allow end-to-end SSL security. By combining third-generation wireless with smart cards and biometrics, organizations should finally have a unified security system that works for both the wireless and wired worlds. Computerworld, 1 January 2001, pp. 52–53.}
}
@incollection{SCHEJBAL1990147,
title = {Methods and Techniques of the Prediction of Metallic and Nonmetallic Raw Materials Using Microcomputers in Czechoslovakia},
editor = {GABOR GAÁL and DANIEL F. MERRIAM},
booktitle = {Computer Applications in Resource Estimation},
publisher = {Pergamon},
address = {Amsterdam},
pages = {147-154},
year = {1990},
series = {Computers and Geology},
isbn = {978-0-08-037245-7},
doi = {https://doi.org/10.1016/B978-0-08-037245-7.50016-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080372457500160},
author = {C. Schejbal and J. Hruska},
abstract = {ABSTRACT
During the last ten years, various methods and computer techniques have been designed or derived from existing models of mineral-potential assessment in Czechoslovakia. Three centers have reached practical and scientific results of national and international importance regarding recent microcmputers:(1)Geoindustria in Jihlava - statistical models, particularly those based on pattern recognition, also including comprehensive heuristic principles for regional assessment (system “Prognos”). The construction of a metallogenic-geochemical and geophysical scheme for 1:100,000 to 1:500,000 geological-deposit synthetic maps has been implemented on a H-P 9845 B including graphics for the HP 7580.(2)Mining geology center for computer applications and geostatistics at the Faculty of Mining and Geology in Ostrava has developed and currently exploits a Geostratistical Software Package “Micro GAD” to solve ore-reserve estimation and sampling problems. A supplemental spatial model on ore-district prediction of volumetric values of metal included applies multivariate statistics. Advanced Geostatistical Methods for Geology and Mining now are being tested for PC-TNS, a compatible Czechoslovak variety of IBM-PC.(3)Mathematical geology unit at Geological exploration Enterprise in Spisska Nova Ves, Slovakia, using Olivetti PC facilities, has elaborated a comprehensive system of geochemical and structure-metallogenic data evaluation for mineral-potential assessment focused on mineral deposits mapping on a 1:25,000 scale. Statistical methods used are mostly multiple regression with factor or characteristic analysis.}
}
@article{VANEPPS1986512,
title = {A structure for enhancing user participation in model development},
journal = {Computers & Industrial Engineering},
volume = {11},
number = {1},
pages = {512-515},
year = {1986},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(86)90144-0},
url = {https://www.sciencedirect.com/science/article/pii/0360835286901440},
author = {Timothy J. VanEpps},
keywords = {Mathematical modeling, Integrated steel mill, Energy management, Energy modeling},
abstract = {Mathmatical modeling is the peaceful coexistence of theory and practice. If either of these aspects becomes dominant the results may be detrimental to the acceptance and usefulness of the model. When the gap between theory and practice is effectively bridged, the chances for success of the model are greatly enhanced. Often the client (end user) is treated as a resource outside the scope of the problem solution. This isolation of the client from the solution technique makes it difficult for him to understand, defend, and implement the answers produced by the model. Making the client an integral part of linking the physical problem to the model solution, raises his or her understanding and participation level to where the modeler and client interact more productively. This gap is being bridged at Armco, specifically in the forecasting of energy supply and demand. The method developed borrows an important concept from the area of Artificial Intelligence, in that the storage and manipulation of the energy system “knowledge” has been separated from the actual model program code. Using a VAX 11/780 and an “off the shelf” query language the energy engineer can manipulate the variables which define the energy systems. Software has been developed to retrieve information from the knowledge base. Using this information, this software creates source code and manual data input programs for a complete model package. This paper describes the development and implementation of this system in the context of a comprehensive Energy Management Computer System at an integrated steelmaking facility.}
}
@article{DATTA1988293,
title = {Parallel and Large Scale Matrix Computations in Control: Some Ideas**},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {8},
pages = {293-299},
year = {1988},
note = {4th IFAC Symposium on computer aided Design in Control Systems 1988, Beijing, PRC, 23-25 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54968-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701754968X},
author = {B.N. Datta and K. Datta},
keywords = {Large scale computations, parallel computations, pole-placement algorithms, observer matrix equation},
abstract = {The design and analysis of time-invariant linear control systems give rise to a variety of interesting linear algebra problems. Numerically effective methods now exist for several of these problems. However, algorithms for large scale computations and efficient parallel algorithms for these problems are virtually nonexistent. In this paper, we propose several efficient generalpurpose parallel algorithms for single-input and multi-input eigenvalue assignment problems. A desirable feature of these algorithms is that they are composed of simple linear algebraic operations such as matrix-vector multiplication, solution of a linear system, and computations of eigensystem and singular values of a symmetric matrix, for which efficient parallel algorithms have already been developed and parallel software libraries are being built based on these algorithms. The proposed algorithms thus have potential for implementations on some existing and future parallel processors. We also propose a numerical method for Sylvester matrix equation arising in the construction of Luenberger observer. The method does not need reduction to”condensed” forms and is thus suitable for large and sparse matrices. The method also exhibits certain parallelism.}
}
@article{CRAWFORD1996677,
title = {Parallel benchmarks of turbulence in complex geometries},
journal = {Computers & Fluids},
volume = {25},
number = {7},
pages = {677-698},
year = {1996},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(96)00023-0},
url = {https://www.sciencedirect.com/science/article/pii/0045793096000230},
author = {Catherine H. Crawford and Constantinos Evangelinos and David Newman and George Em Karniadakis},
abstract = {In this paper we present benchmark results from the parallel implementation of the three-dimensional Navier-Stokes solver Prism on different parallel platforms of current interest: IBM SP2 (all three types of processors), SGI Power Challenge XL and Cray C90. The numerical method is based on mixed spectral element-Fourier expansions in (x − y) and z-directions, respectively. Each one (or a group) of the Fourier modes can be computed on a separate processor as the linear contributions in Navier-Stokes equations (Helmholtz solvers) are completely uncoupled. Coupling is obtained via the non-linear contributions (advection terms) and requires a global transpose of the data and one-dimensional multiple-point FFTs. We first analyze the computational complexity of Prism identifying basic computational kernels and communication bottlenecks. These kernels are benchmarked separately on several computer architectures. More specifically, we obtain timings for BLAS routines on all aforementioned processors but also on the SP1, Cray J90, the Intel Paragon, and the DEC AlphaServer 8400 5/300. A two-dimensional version of Prism is also benchmarked on most processors testing both direct and iterative methods of solution. Next, we define two three-dimensional benchmark flow problems in prototype complex geometries, and measure parallel scalability and performance using different message-passing libraries. Special emphasis is placed on runtime data processing, e.g. turbulence statistics computed during the simulation. Our results provide a measure of MFlops sustained performance of individual processors, and indicate the limitations of the current communications software for typical problems in computational mechanics based on spectral or finite element discretizations.}
}
@article{JACIC1985305,
title = {Computer - Aided Design of Industrial Automation},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {8},
pages = {305-310},
year = {1985},
note = {3rd IFAC/IFIP Symposium on Computer Aided Design in Control and Engineering Systems: Advanced Tools for Modern Technology, Lyngby, Denmark, 31 July-2 August 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60386-0},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017603860},
author = {L.A. Jacić},
keywords = {Digital systems, digital control, Computer-aided logic design, finite automata, sequential machines, industrial control, hazards and race conditions, sequential control systems},
abstract = {In recent years, computers with their great computational power have assumed very important role in the logic design of industrial digital control systems. The implementation of large-scale digital systems is an example of one design problem which can be impractical to solve by hand. How to enable production engineer to improve the existing control network, or to design the absolutely new one, presuming, as it is very olten, that he is not a control design specialist. On the basis of new technological demands, an engineer should form a desired control specification which represent the input format to available computer. Computer should produce desired logic equations as a basis for the immediate realization, since the system is reliable and minimal in sense of number of required components. Software for complete design of digital control networks will be presented in the paper. It is applicable for treating deterministic as well as stochastic systems, both combinational and sequential etc. The concept of use of interactive computer graphics and other available computer aids in synthesis process is given. A relevant example is included in order to illustrate the program package.}
}
@article{CSER198675,
title = {MUSIC: A Tool for Simulation and Real-time Control},
journal = {IFAC Proceedings Volumes},
volume = {19},
number = {6},
pages = {75-79},
year = {1986},
note = {4th IFAC/IFIP Symposium on Software for Computer Control 1986, Graz, Austria, 20-23 May 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)59727-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701759727X},
author = {J. Cser and P.M. Bruijn and H.B. Verbruggen},
keywords = {Control engineering computer applications, simulation, real-time control, direct digital control},
abstract = {A program package, called MUSIC, has been developed for implementing simulation and real-time control algorithms on PDP-11 and VAX family computers. The control structure can be described by means of a block diagram. The blocks represent the algorithms and control functions, while the topology is defined by the connections of the blocks. A set of standard blocks is available and the user can define and add his own blocks to the system using a standard software interface. Blocks can have adjustable parameters and an arbitrary internal complexity. From the selected set of blocks an actual simulation or real-time control program and a database can be built using a program generator. The interaction with this control program is performed via a separate command interface program. With this interface the user may supply interactively all the information describing the control system. A user-written optional third program can also be added to the system to perform additional calculations outside the control loop or to realise individual command interfaces. A graphic display facility facilitates the evaluation of results and the comparison of a set of solutions. The whole system is embedded in a user-friendly menu, so the user hardly needs any computer-dependent particular knowledge.}
}
@incollection{BARRAUD1986139,
title = {SIRENA +: A VERSATILE INTERACTIVE SYSTEM FOR SIMULATION, IDENTIFICATION AND CONTROL DESIGN},
editor = {P. MARTIN LARSEN and N.E. HANSEN},
booktitle = {Computer Aided Design in Control and Engineering Systems},
publisher = {Pergamon},
pages = {139-144},
year = {1986},
isbn = {978-0-08-032557-6},
doi = {https://doi.org/10.1016/B978-0-08-032557-6.50031-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325576500315},
author = {A. Barraud and P. Laporte and S. Gentil},
abstract = {This paper deals with an interactive system for simulation, identification and control of linear systems described by state-space or transfer-functions, in a continuous, discrete or sampled representation. Based upon a university prototype, SIRENA + has now reached an industrial quality level. Written in FORTRAN-77, this system has been implemented on a large variety of computers. Robustness has been ensured by a high quality mathematical software developed from and in a similar way as some well known libraries (Harwell, Nats projects, Matlab,…). Its modular structure (700 routines/60000 lines) greatly enhances its maintenability and evolution. The system is user friendly, with an on-line help and error checking, files saving options, and online graphical features. Commands are prompted adaptively according to the working context and to the user skillness. SIRENA + performs all the steps necessary for control design : simulation, identification and control.}
}
@article{MONTESANO201572,
title = {A synergistic damage mechanics based multiscale model for composite laminates subjected to multiaxial strains},
journal = {Mechanics of Materials},
volume = {83},
pages = {72-89},
year = {2015},
issn = {0167-6636},
doi = {https://doi.org/10.1016/j.mechmat.2015.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S016766361500006X},
author = {John Montesano and Chandra Veer Singh},
keywords = {Synergistic damage mechanics, Multiscale modeling, Micromechanics, Multiaxial strain, Multidirectional laminates, Stiffness degradation},
abstract = {A multiscale model based on synergistic damage mechanics is developed for predicting the elastic response of symmetric composite laminates containing matrix cracks in plies of multiple orientations, and subjected to an arbitrary multiaxial strain state. On the micromechanical scale, the proposed multiscale modeling approach invokes three-dimensional finite element analysis to characterize the multiaxial damage state within the cracked multidirectional laminate, and evaluate damage constants required in the damage constitutive model. These damage constants capture the ply constraint effects acting on the surface displacements of the developed matrix cracks in all off-axis and on-axis plies. The representative volume element describing the applied multiaxial stress state within the laminate is developed through finite element models using periodic boundary conditions, which are necessary to accurately represent the physical problem. The developed micromechanical models also allow for prediction of the laminate’s shear deformation response. The model is shown to accurately capture the nonlinear stiffness degradation exhibited by cross-ply, quasi-isotropic and angle-ply laminates containing matrix cracks in multiple plies and subjected to various multiaxial stress states. The prediction results are validated by available experimental data and compared with independent three-dimensional finite element calculations. The multiscale model can easily be implemented into a commercial finite element software package in order to predict stiffness degradation in composite structures. This will provide a means to predict the integrity and durability of these structures, and ultimately lead to damage-tolerant designs.}
}
@article{VISHWANATH2009330,
title = {Patterns and changes in prescriber attitudes toward PDA prescription-assistive technology},
journal = {International Journal of Medical Informatics},
volume = {78},
number = {5},
pages = {330-339},
year = {2009},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2008.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1386505608001780},
author = {Arun Vishwanath and Linda Brodsky and Steve Shaha and Michael Leonard and Michael Cimino},
keywords = {Prescription-assistive technology, Prescriber behavior, Physician technology adoption, Patient safety, Medication safety},
abstract = {Context
Medication error prevention is a priority for the U.S. healthcare system in the 21st century. Use of technology is considered by some as critical to achieve this goal. Knowledge of the attitudinal barriers to such adoption, however, is limited.
Objective
To determine the attitudes of frontline prescriber clinicians towards technology in general, and PDAs specifically, before and after introduction of a PDA in the clinical setting of medication prescribing.
Design
A pre- and post-intervention web-based survey, 12–14 months apart.
Setting
Academic tertiary care children's hospital.
Participants
Total of 244 prescriber clinicians.
Intervention
Distribution of a PDA with pediatric-specific medication prescribing information after completion of an on-line medication safety certification and other safety focused educational sessions.
Main outcome measures
Ratings (5-point Likert scale) reflecting perceptions and attitudes towards technology in general and technology in medical settings along with self-reported usage of the PDA for Rx.
Results
Early Adopters and Late Adopters were identified statistically, and the group membership reflected their prior exposure to and ownership of other technologies. Early Adopters tended to be younger and less experienced clinically (e.g., residents) and more frequent owners and users of technology. Early Adopters expressed significantly more favorable attitudes toward technology and PDAs on both pre- to post-intervention survey occasions. They also utilized the PDA for Rx more often than LAs. Interestingly, PDA use for Early Adopters was based on its ease of use, while PDA use among later adopters was based on its clinical usefulness.
Conclusions
Provision of point of care information using PDAs and a user-friendly, pediatric-specific medication information software package did not positively affect the attitudes of prescriber clinicians among those already favorable toward technology. However, a significant change was found among those with initially less favorable attitudes. Organizations need to understand the nature of both Early and Late Adopters and plan appropriately for managing the respective needs and expectations when potentially beneficial technologies are introduced. In order to ensure the success of an implementation, the training and supportive interventions need to be carefully designed and specifically catered to the personality-based outcome expectations of the prescriber.}
}
@article{WARWICK1988153,
title = {Control Systems: A First Course},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {6},
pages = {153-156},
year = {1988},
note = {IFAC Symposium on Trends in Control and Measurement Education, Swansea, UK, 11-13 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53856-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017538562},
author = {K. Warwick},
keywords = {Education, computer control, PID control, state-space methods, digital control, z-transForms, signal processing, process control, Nyquist criterion},
abstract = {This paper is concerned with the topics studied in an introductory course on Control Systems. Fundamental analysis, design and implementation must all be catered for in an up to date fashion, such that the course is of benefit as either a standalone package for general Engineering students or as a foundation for much deeper study for specialist Control Engineers. It is important at this stage for the student to experience the breadth of the Control Systems area, and to understand, in simple terms, how the different topics are connected. Obviously, it is not possible in one paper to set out a course specification for every institution to follow, but rather the intention here is to stress the need for an introductory course to keep pace with modern trends, and suggestions are made as to how this can be done at the present time. Topics highlighted are state-space methods, PID control and digital control, but principally computer control, in that this covers such as CAD, software simulation and Artificial Intelligence.}
}
@incollection{KARPPINEN1984135,
title = {THE LARGE-SCALE RESEARCH SIMULATOR NORS FOR MAN-MACHINE SYSTEMS DEVELOPMENT},
editor = {C. MAFFEZZONI},
booktitle = {Modelling and Control of Electric Power Plants},
publisher = {Pergamon},
pages = {135-142},
year = {1984},
isbn = {978-0-08-031163-0},
doi = {https://doi.org/10.1016/B978-0-08-031163-0.50023-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080311630500231},
author = {J. Karppinen and E. Stokke},
abstract = {The use of real-time simulators for control system development is a well established application area. With the growing number of large-scale training simulators their use for other purposes than training such as human factors studies and control room improvements is becoming a common practice. For the development and evaluation of highly sophisticated operator support systems the use of large-scale plant simulators as test facilities is becoming a practical necessity. These development trends are leading to the introduction of a new type of real-time simulators, the dedicated large-scale research simulators. The NORS (NOkia Research Simulator) is of this new type. It was implemented during 1983 at the OECD Halden Reactor Project's Man-Machine Systems Laboratory (HAMMLAB). The HAMMLAB research programme on advanced computer and colour display based control rooms covers the establishment of experimental techniques and methods for the new facility, basic Man-Machine Systems research with development of concepts and theories, experiments on information presentation and exercise of control and experimental validation of advanced computerized operator support systems. The NORS simulator was designed to effectively support the needs of the versatile research programme. The design goal was to combine the simulation scope and quality of large-scale training simulators with a flexible software and hardware structure readily adaptable to various experimental conditions. Novel features of the simulator software include extensive use of computer aided modelling and generic modelling packages for simulation of plant systems and instrumentation, highly modular structure and comprehensive use of a data base and software development system.}
}
@incollection{ISERMANN198325,
title = {ON THE DEVELOPMENT AND IMPLEMENTATION OF PARAMETER-ADAPTIVE CONTROLLERS},
editor = {G. FERRATE and E.A. PUENTE},
booktitle = {Software for Computer Control 1982},
publisher = {Pergamon},
pages = {25-49},
year = {1983},
isbn = {978-0-08-029352-3},
doi = {https://doi.org/10.1016/B978-0-08-029352-3.50008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080293523500088},
author = {R. Isermann and K.-H. Lachmann},
abstract = {The design of parameter-adaptive control algorithms which are based on proper parameter estimation and linear control algorithms is considered for single-input single-output processes. The resulting adaptive control algorithms work well if some pre-conditions for the stability and convergence are satisfied. In practice these pre-conditions may be violated. Therefore a supervision level is introduced which monitors faulty functions and takes appropriate actions. Further improvements are obtained by designing a “parameter-interface” between the parameter estimation and the controller design. Various steps for the application are considered and software packages and the implementation on microcomputer controllers are described.}
}