@article{KALOMIROS2011496,
title = {Design and hardware implementation of a stereo-matching system based on dynamic programming},
journal = {Microprocessors and Microsystems},
volume = {35},
number = {5},
pages = {496-509},
year = {2011},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2011.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0141933111000603},
author = {J.A. Kalomiros and J. Lygouras},
keywords = {Hardware design, Real-time systems, Stereo vision, Dynamic programming},
abstract = {A new real-time stereo system is presented based on a hardware implementation of an efficient Dynamic Programming algorithm. A simple state-machine calculates the cost-matrix along the diagonal of the 2-D disparity space for each epipolar pair of image scan-lines. Minimum transition costs are stored in embedded RAM and are used to backtrack disparities at clock rate. All calculations are within a pre-determined slice of the cost plane, representing the useful disparity range. The system is designed as a VHDL library component and is implemented as a SoC in a medium-capacity Field Programmable Gate Array chip. It can process stereo-pairs in full VGA resolution at a rate of 25 Mpixels/s and produces 8-bit dense disparity maps within a range of disparities up to 65 pixels. The design is evaluated comparing to ground truth and in terms of resource usage. It is also compared to a software implementation of the Dynamic Programming algorithm and to other FPGA-based stereo systems.}
}
@article{HARRISON2013263,
title = {Warm forming die design, part I: Experimental validation of a novel thermal finite element modeling code},
journal = {Journal of Manufacturing Processes},
volume = {15},
number = {2},
pages = {263-272},
year = {2013},
note = {Advances in Materials Forming},
issn = {1526-6125},
doi = {https://doi.org/10.1016/j.jmapro.2013.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1526612513000133},
author = {N.R. Harrison and V. Rubek and P.A. Friedman},
keywords = {Warm forming, Die design, Thermal simulation, Metal forming, Aluminum},
abstract = {Finite element analysis (FEA) has become an invaluable tool in the design of sheet metal stamping dies and processes. FEA has gained widespread acceptance as the best method of optimizing dies for conventional stamping processes. More recently, FEA has been shown to be an effective method of designing tooling for sheet forming processes. In this work, an FEA based approach is applied to the warm stamping (warm forming) process. This work introduces a new thermal finite element analysis software called PASSAGE®/Forming (PASSAGE) that enables the up-front design of the thermal management of warm forming dies. This thermal finite element analysis software is designed to specifically handle the forming and optimization scenarios related to the heating of a stamping die while minimizing user interface time. In this work, PASSAGE has been applied to a simple block of steel embedded with cartridge heaters to validate the prediction capability of this software under two different heating conditions. The results show that PASSAGE is capable of predicting the actual steady-state temperature distribution within the block with an acceptable level of accuracy while yielding notable information to the user with respect to specifying power requirements. A finite element software package like PASSAGE is a valuable tool that will aid greatly in the implementation of warm forming as a manufacturing process beyond the scope of the laboratory and into production.}
}
@article{BISHOP2006224,
title = {UNIFYING CONTROL DESIGN AND IMPLEMENTATION IN AN UNDERGRADUATE CONTROL SYSTEMS LABORATORY},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {6},
pages = {224-228},
year = {2006},
note = {7th IFAC Symposium on Advances in Control Education},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060621-3-ES-2905.00040},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015331347},
author = {Robert H. Bishop and Nicholas Lin and Eduardo Gildin and Jeanne Sullivan Falcon},
keywords = {laboratory education, control systems, real-time},
abstract = {This paper discusses the development and evolution of an undergraduate teaching laboratory for control systems in the Aerospace Engineering Department at the University of Texas at Austin. Initially, the laboratory was developed using disparate software packages for design, analysis and real-time implementation. In the second semester of teaching the course, LabVIEW from National Instruments was utilized for all pre-laboratory homework as well as for the laboratory experiments. The student feedback for the course improved significantly between the first and second semesters of teaching the course. This improvement is shown to be strongly related to the introduction of a unified, hands-on approach using LabVIEW.}
}
@article{PECORELLI2020110693,
title = {A large empirical assessment of the role of data balancing in machine-learning-based code smell detection},
journal = {Journal of Systems and Software},
volume = {169},
pages = {110693},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110693},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301448},
author = {Fabiano Pecorelli and Dario {Di Nucci} and Coen {De Roover} and Andrea {De Lucia}},
keywords = {Code smells, Machine learning, Data balancing, Object oriented, Model view controller},
abstract = {Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics is used to detect smelly code components. However, these techniques suffer from subjective interpretations, a low agreement between detectors, and threshold dependability. To overcome these limitations, previous work applied Machine-Learning that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine-Learning is not always suitable for code smell detection due to the highly imbalanced nature of the problem. In this study, we investigate five approaches to mitigate data imbalance issues to understand their impact on Machine Learning-based approaches for code smell detection in Object-Oriented systems and those implementing the Model-View-Controller pattern. Our findings show that avoiding balancing does not dramatically impact accuracy. Existing data balancing techniques are inadequate for code smell detection leading to poor accuracy for Machine-Learning-based approaches. Therefore, new metrics to exploit different software characteristics and new techniques to effectively combine them are needed.}
}
@article{WILBERT2013345,
title = {Building extensible frameworks for data processing: The case of MDP, Modular toolkit for Data Processing},
journal = {Journal of Computational Science},
volume = {4},
number = {5},
pages = {345-351},
year = {2013},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2011.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877750311000913},
author = {Niko Wilbert and Tiziano Zito and Rike-Benjamin Schuppner and Zbigniew Jędrzejewski-Szmek and Laurenz Wiskott and Pietro Berkes},
keywords = {Machine learning, Python, Scientific computing, Computational neuroscience},
abstract = {Data processing is a ubiquitous task in scientific research, and much energy is spent on the development of appropriate algorithms. It is thus relatively easy to find software implementations of the most common methods. On the other hand, when building concrete applications, developers are often confronted with several additional chores that need to be carried out beside the individual processing steps. These include for example training and executing a sequence of several algorithms, writing code that can be executed in parallel on several processors, or producing a visual description of the application. The Modular toolkit for Data Processing (MDP) is an open source Python library that provides an implementation of several widespread algorithms and offers a unified framework to combine them to build more complex data processing architectures. In this paper we concentrate on some of the newer features of MDP, focusing on the choices made to automatize repetitive tasks for users and developers. In particular, we describe the support for parallel computing and how this is implemented via a flexible extension mechanism. We also briefly discuss the support for algorithms that require bi-directional data flow.}
}
@article{GROPP1996789,
title = {A high-performance, portable implementation of the MPI message passing interface standard},
journal = {Parallel Computing},
volume = {22},
number = {6},
pages = {789-828},
year = {1996},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(96)00024-5},
url = {https://www.sciencedirect.com/science/article/pii/0167819196000245},
author = {William Gropp and Ewing Lusk and Nathan Doss and Anthony Skjellum},
keywords = {Message passing interface, Parallel programming environment, Benchmark, Performance, Portability, MPI-2},
abstract = {MPI (Message Passing Interface) is a specification for a standard library for message passing that was defined by the MPI Forum, a broadly based group of parallel computer vendors, library writers, and applications specialists. Multiple implementations of MPI have been developed. In this paper, we describe MPICH, unique among existing implementations in its design goal of combining portability with high performance. We document its portability and performance and describe the architecture by which these features are simultaneously achieved. We also discuss the set of tools that accompany the free distribution of MPICH, which constitute the beginnings of a portable parallel programming environment. A project of this scope inevitably imparts lessons about parallel computing, the specification being followed, the current hardware and software environment for parallel computing, and project management; we describe those we have learned. Finally, we discuss future developments for MPICH, including those necessary to accommodate extensions to the MPI Standard now being contemplated by the MPI Forum.}
}
@article{VIDAL20161,
title = {Over-exposed classes in Java: An empirical study},
journal = {Computer Languages, Systems & Structures},
volume = {46},
pages = {1-19},
year = {2016},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1477842415300531},
author = {S. Vidal and A. Bergel and J.A. Díaz-Pace and C. Marcos},
keywords = {Class accessibility, Modularity, Over-exposed classes, Java systems},
abstract = {Java access modifiers regulate interactions among software components. In particular, class modifiers specify which classes from a component are publicly exposed and therefore belong to the component public interface. Restricting the accessibility as specified by a programmer is key to ensure a proper software modularity. It has been said that failing to do so is likely to produce maintenance problems, poor system quality, and architecture decay. However, how developers uses class access modifiers or how inadequate access modifiers affect software systems has not been investigated yet in the literature. In this work, we empirically analyze the use of class access modifiers across a collection of 15 Java libraries and 15 applications, totaling over 3.6M lines of code. We have found that an average of 25% of classes are over-exposed, i.e., classes defined with an accessibility that is broader than necessary. A number of code patterns involving over-exposed classes have been formalized, characterizing programmers׳ habits. Furthermore, we propose an Eclipse plugin to make component public interfaces match with the programmer׳s intent.}
}
@article{GARCIA198851,
title = {Specification, design and Modula-2 implementation of a low cost industrial control system},
journal = {Annual Review in Automatic Programming},
volume = {14},
pages = {51-56},
year = {1988},
note = {Real Time Programming 1988},
issn = {0066-4138},
doi = {https://doi.org/10.1016/0066-4138(88)90008-0},
url = {https://www.sciencedirect.com/science/article/pii/0066413888900080},
author = {D Garcia and H Lopez and J Tuya and A Diez},
keywords = {Software Engineering, Real time control, Modula-2 language, Concurrency, Computer-aided training},
abstract = {The purpose of this paper is to show the application of modern software engineering techniques to the construction of a small/medium size package for real time control. This package, denoted by SCM (acronym of “Sistema de Control Modular”), has been written in Modula-2 and presents all advantages offered by this high-level structured programming language. It has been implemented on a PC-AT compatible computer.}
}
@incollection{GANNEY2014133,
title = {Chapter 9 - Software Engineering},
editor = {Azzam Taktak and Paul Ganney and Dave Long and Paul White},
booktitle = {Clinical Engineering},
publisher = {Academic Press},
address = {Oxford},
pages = {133-170},
year = {2014},
isbn = {978-0-12-396961-3},
doi = {https://doi.org/10.1016/B978-0-12-396961-3.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123969613000093},
author = {Paul S. Ganney and Sandhya Pisharody and Edwin Claridge},
keywords = {Software development, Software management, Operating system, Language selection, Programming, Software life cycle, Procedural programming, Object-oriented programming, Functional programming, Real-time programming, Embedded programming, Software validation, Software verification, Software quality assurance, Database, Spreadsheet, Flat file, Normal form, SQL, Image processing, Finite element analysis, Artificial intelligence, Expert systems, Equipment management database, Device tracking},
abstract = {This chapter looks at software development, management, and typical applications. Common operating systems and the techniques required for selecting the appropriate programming language for a project or task are described. Features common to all programming languages are examined, as well as the software life cycle: both the steps within it and models of implementation. Several types of programming are described, including procedural, object-oriented, functional, real-time, and embedded. Next, software quality is considered, followed by an examination of databases, both simple and structured, including normalization and the manipulation of data using Structured Query Language. The second half of the chapter describes some typical applications (plus some sample packages) within clinical computing: image processing, finite element analysis, artificial intelligence, expert systems, equipment management databases, and device tracking systems.}
}
@article{CARACCIOLO2021105759,
title = {Elliptic tori in FPU non-linear chains with a small number of nodes},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {97},
pages = {105759},
year = {2021},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2021.105759},
url = {https://www.sciencedirect.com/science/article/pii/S1007570421000708},
author = {Chiara Caracciolo and Ugo Locatelli},
keywords = {FPU problem, Lower dimensional invariant tori, KAM theory, Normal form methods, Perturbation theory for Hamiltonian systems},
abstract = {We revisit an algorithm constructing elliptic tori, that was originally designed for applications to planetary hamiltonian systems. The scheme is adapted to properly work with models of chains of N+1 particles interacting via anharmonic potentials, thus covering also the case of FPU chains. After having preliminarily settled the Hamiltonian in a suitable way, we perform a sequence of canonical transformations removing the undesired perturbative terms by an iterative procedure. This is done by using the Lie series approach, that is explicitly implemented in a programming code with the help of a software package, which is especially designed for computer algebra manipulations. In the cases of FPU chains with N=4,8, we successfully apply our new algorithm to the construction of elliptic tori for wide sets of the parameter ruling the size of the perturbation, i.e., the total energy of the system. Moreover, we explore the stability regions surrounding 1D elliptic tori. We compare our semi-analytical results with those provided by numerical explorations of the FPU-model dynamics, where the latter ones are obtained by using techniques based on the so called frequency analysis. We find that our procedure works up to values of the total energy that are of the same order of magnitude with respect to the maximal ones, for which elliptic tori are detected by numerical methods.}
}
@article{NEKOO2021102641,
title = {A benchmark mechatronics platform to assess the inspection around pipes with variable pitch quadrotor for industrial sites},
journal = {Mechatronics},
volume = {79},
pages = {102641},
year = {2021},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2021.102641},
url = {https://www.sciencedirect.com/science/article/pii/S095741582100115X},
author = {Saeed Rafee Nekoo and José Ángel Acosta and Guillermo Heredia and Anibal Ollero},
keywords = {SDRE, Variable-pitch, Mechatronics system, Inspection, Quadrotor, Optimal control},
abstract = {This paper investigates the inspection-of-pipe topic in a new framework, by rotation around a pipe, peculiar to industrial sites and refineries. The evolution of the ultimate system requires prototype design and preliminary tests. A new benchmark has been designed and built to mimic the rotation around a pipe, with the main purpose of assessing the different types of rotors and control systems. The benchmark control system presents a mechatronics package including mechanical design and machining, electronics and motor drive, motor-blade installation, computer programming, and control implementation. The benchmark is also modular, working with two modes of one- and two-degree-of-freedom (DoF), easily interchangeable. To cover a full rotation, conventional fixed-pitch drones fail to provide negative thrusts; nonetheless, variable-pitch (VP) rotor quadcopters can produce that in both directions. A closed-loop nonlinear optimal method is chosen as a controller, so-called, “the state-dependent Riccati equation (SDRE)” approach. Optimal control policies are challenging for experimentation though it has been successfully done in this report. The advantage of the VP is also illustrated in a rotation plus radial motion in comparison with fixed-pitch rotors while a wind gust disturbs the inspection task. The proposed VP system compensated the disturbance while the fixed pitch was pushed away by the wind gust. The solution methods to the SDRE were mixed, a closed-form exact solution for the one-DoF system, and a numerical one for the two-DoF. Solving the Riccati online in each time step is a critical issue that was effectively solved by the implementation approach, through online communication with MATLAB software. Both simulations and experiments have been performed along with a discussion to prove the application of VP systems in rotary-motion pipe inspection.}
}
@article{FARI2023294,
title = {Advanced GNC-oriented modeling and simulation of Vertical Landing vehicles with fuel slosh dynamics},
journal = {Acta Astronautica},
volume = {204},
pages = {294-306},
year = {2023},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2022.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S009457652200707X},
author = {Stefano Farì and David Seelbinder and Stephan Theil},
keywords = {Slosh dynamics, Modelica, Object-oriented modeling, Simulation, GNC, Vertical landing vehicle, Lunar lander, Reusable launch vehicle},
abstract = {This paper introduces a flexible framework to enable high-fidelity simulations of the fuel slosh dynamics for verification of the Guidance, Navigation and Control (GNC) algorithms. The sloshing phenomenon must be tackled during the control system design phase and the GNC software run and validated within appropriate simulation frameworks. Equivalent mechanical models can well approximate sloshing under specific assumptions, allowing the derivation of the multibody vehicle’s equations of motion; however, while suitable for analysis and control synthesis purposes, this approach presents several limitations for implementation as a simulation model. In this work, the multibody plant embedding slosh dynamics is modeled by means of the DLR’s Vertical Landing Vehicles Library (VLVLib) written using the object-oriented Modelica modeling language. The advantages of using Modelica are explored throughout the paper. Given that control synthesis, analyses and simulation campaigns are usually performed in the Matlab/Simulink environment, a core point of the proposed solution is to achieve a good synergy within the latter and the Modelica plant model with the least readaptation burden for the developer. The strategies to achieve a full integration and verification tool chain are illustrated. The potential of this approach is demonstrated via two different test cases: a lunar landing scenario of the ALINA spacecraft developed by PTS, and CALLISTO reusable rocket demonstrator.}
}
@article{YARMOHAMMADI201891,
title = {Automated performance measurement for 3D building modeling decisions},
journal = {Automation in Construction},
volume = {93},
pages = {91-111},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518300050},
author = {Saman Yarmohammadi and Daniel Castro-Lacouture},
keywords = {BIM, Data analytics, Application programming interface, Modeling team, Performance measurement},
abstract = {Building information modeling (BIM) is instrumental in documenting design, enhancing customer experience, and improving product functionality in capital projects. However, high-quality building models do not happen by accident, but rather because of a managed process that involves several participants from different disciplines and backgrounds. Throughout this process, the different priorities of design modelers often result in conflicts that can negatively impact project outcomes. To prevent such unwanted outcomes from occurring, the modeling process needs to be effectively managed. This effective management requires an ability to closely monitor the modeling process and correctly measure the modelers' performance. Nevertheless, existing methods of performance monitoring in building design practices lack an objective measurement system to quantify modeling progress. The widespread utilization of BIM tools presents a unique opportunity to retrieve granular design process data and conduct accurate performance measurements. This research improves upon previous efforts by presenting a novel application programming interface (API)-enabled approach to (a) automatically collect detailed model development data directly from BIM software packages in real-time, and (b) efficiently calculate several modeling performance measures during schematic and design development phases of building projects. These indicators can be used to properly arrange modeling teams in the quest for high-quality building models. The specific objectives of this study to examine the feasibility of a proposed automated design performance measurement framework, and to identify optimal modeling team configurations using empirical performance information. A passive data recording approach allows for the real-time capture of comprehensive user interface (UI) interaction and model element modification events. The proposed framework is implemented as an Autodesk Revit plugin. Next, an experiment is conducted to capture data using the developed Revit plugin. Experiment participants' individual production rates are estimated to establish the validity of the proposed approach to identify the optimal design team configuration. The presented approach uses the earliest due date (EDD) sequencing rule in combination with the critical path method (CPM) to calculate the maximum lateness for different design team arrangements.}
}
@article{LUO20181198,
title = {Feasibility study of a simulation software tool development for dynamic modelling and transient control of adiabatic compressed air energy storage with its electrical power system applications},
journal = {Applied Energy},
volume = {228},
pages = {1198-1219},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.06.068},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918309371},
author = {Xing Luo and Mark Dooner and Wei He and Jihong Wang and Yaowang Li and Decai Li and Oleh Kiselychnyk},
keywords = {Adiabatic compressed air energy storage, Thermal energy storage, Simulation software tool, Time-dependent dynamic modelling, Transient control, Electrical power system},
abstract = {The field of large-scale electrical energy storage is growing rapidly in both academia and industry, which has driven a fast increase in the research and development on adiabatic compressed air energy storage. The significant challenge of adiabatic compressed air energy storage with its thermal energy storage is in the complexity of the system dynamic characteristics arising from the multi-physical (pneumatic, thermal, mechanical and electrical) processes. This has led to a strong demand for simulation software tools specifically for dynamic modelling and transient control of relevant multi-scale components, subsystems and whole systems with different configurations. The paper presents a feasibility study of a simulation tool development implemented by the University of Warwick Engineering team to achieve this purpose. The developed tool includes a range of validated simulation models from the fields of pneumatics, thermodynamics, heat transfer, electrical machines and power grids. The structure of the developed tool is introduced and a component library is built up on the Matlab/Simulink platform. The mathematical descriptions of key components are presented, which precedes a presentation of four case studies of different applications. The case studies demonstrate that the simulation software tool can be used for dynamic modelling of multi-scale adiabatic compressed air energy storage components and systems, real performance analysis, dynamic control strategy implementation and feasibility studies of applications of adiabatic compressed air energy storage integrated with power grids. The paper concludes that the continued development and use of such a tool is both feasible and valuable.}
}
@article{DEHEIJ199631,
title = {Assessment of logistics software by means of data models applied to bills of material, routings and recipes},
journal = {Computers in Industry},
volume = {31},
number = {1},
pages = {31-43},
year = {1996},
issn = {0166-3615},
doi = {https://doi.org/10.1016/0166-3615(96)00034-6},
url = {https://www.sciencedirect.com/science/article/pii/0166361596000346},
author = {Jos C.J. {de Heij} and RenéM.A. Caubo},
keywords = {Data model, Software selection, Logistics software, Reference model, Standard software package, Bill of material},
abstract = {The paper describes a method for evaluating software systems on the basis of their data structure. The core of the method is a Reference Data Model (RDM) with which one can assess both standard software packages and manufacturing situations. The projections give insight into the opportunities (and constraints) of the systems to be assessed. The implementation of the data structure also shows the possible (additional) functionality of a system. For illustration's sake we have applied the method to a specific functional area: bills of material, routings and recipes (BRRs).}
}
@article{NINNESS2005838,
title = {THE UNIVERSITY OF NEWCASTLE IDENTIFICATION TOOLBOX (UNIT)},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {838-843},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.00141},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016361535},
author = {Brett Ninness and Adrian Wills and Stuart Gibson},
keywords = {Parameter Estimation, System Identification, Software},
abstract = {This paper describes a MATLAB-based software package for estimation of dynamic systems. A wide range of standard estimation approaches are supported. These include the use of non-parametric, subspace-based and prediction-error algorithms coupled (in the latter case) with either MIMO state space or MISO polynomial model structures. A key feature of the software is the implementation of several new techniques that have been investigated by the authors. These include the estimation of non-linear models, the use of non-standard model parametrizations, and the employment of Expectation Maximisation (EM) methods.}
}
@article{RENDELL2000887,
title = {Computational chemistry on Fujitsu vector–parallel processors: Development and performance of applications software},
journal = {Parallel Computing},
volume = {26},
number = {7},
pages = {887-911},
year = {2000},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(00)00017-X},
url = {https://www.sciencedirect.com/science/article/pii/S016781910000017X},
author = {Alistair P. Rendell and Andrey Bliznyuk and Thomas Huber and Ross H. Nobes and Elena V. Akhmatskaya and Herbert A. Früchtl and Paul W.-C. Kung and Victor Milman and Han Lung},
keywords = {Fujitsu supercomputers, Molecular dynamics, Semiempirical quantum chemistry, Ab initio quantum chemistry, Parallelisation, Performance},
abstract = {In this and a preceding paper, we provide an introduction to the Fujitsu VPP range of vector–parallel supercomputers and to some of the computational chemistry software available for the VPP. Here, we consider the implementation and performance of seven popular chemistry application packages. The codes discussed range from classical molecular dynamics to semiempirical and ab initio quantum chemistry. All have evolved from sequential codes, and have typically been parallelised using a replicated data approach. As such they are well suited to the large-memory/fast-processor architecture of the VPP. For one code, CASTEP, a distributed-memory data-driven parallelisation scheme is presented.}
}
@article{MAHMOUDI2010172,
title = {Web-based interactive 2D/3D medical image processing and visualization software},
journal = {Computer Methods and Programs in Biomedicine},
volume = {98},
number = {2},
pages = {172-182},
year = {2010},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2009.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169260709003022},
author = {Seyyed Ehsan Mahmoudi and Alireza Akhondi-Asl and Roohollah Rahmani and Shahrooz Faghih-Roohi and Vahid Taimouri and Ahmad Sabouri and Hamid Soltanian-Zadeh},
keywords = {Web-based software tools, 2D and 3D processing and visualization, Medical imaging and analysis},
abstract = {There are many medical image processing software tools available for research and diagnosis purposes. However, most of these tools are available only as local applications. This limits the accessibility of the software to a specific machine, and thus the data and processing power of that application are not available to other workstations. Further, there are operating system and processing power limitations which prevent such applications from running on every type of workstation. By developing web-based tools, it is possible for users to access the medical image processing functionalities wherever the internet is available. In this paper, we introduce a pure web-based, interactive, extendable, 2D and 3D medical image processing and visualization application that requires no client installation. Our software uses a four-layered design consisting of an algorithm layer, web-user-interface layer, server communication layer, and wrapper layer. To compete with extendibility of the current local medical image processing software, each layer is highly independent of other layers. A wide range of medical image preprocessing, registration, and segmentation methods are implemented using open source libraries. Desktop-like user interaction is provided by using AJAX technology in the web-user-interface. For the visualization functionality of the software, the VRML standard is used to provide 3D features over the web. Integration of these technologies has allowed implementation of our purely web-based software with high functionality without requiring powerful computational resources in the client side. The user-interface is designed such that the users can select appropriate parameters for practical research and clinical studies.}
}
@article{BERNASCONI201511,
title = {On the error resilience of ordered binary decision diagrams},
journal = {Theoretical Computer Science},
volume = {595},
pages = {11-33},
year = {2015},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.05.050},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515005149},
author = {Anna Bernasconi and Valentina Ciriani and Lorenzo Lago},
keywords = {Binary decision diagrams, Error resilient data structures},
abstract = {An Ordered Binary Decision Diagram (OBDD) is a data structure that is used in an increasing number of fields of Computer Science (e.g., logic synthesis, program verification, data mining, bioinformatics, and data protection) for representing and manipulating discrete structures and Boolean functions. The purpose of this paper is to study the error resilience of OBDDs and to design a resilient version of this data structure, i.e., a self-repairing OBDD. In particular, we describe some strategies that make reduced ordered OBDDs resilient to errors in the indices, that are associated to the input variables, or in the pointers (i.e., OBDD edges) of the nodes. These strategies exploit the inherent redundancy of the data structure, as well as the redundancy introduced by its efficient implementations. The solutions we propose allow the exact restoring of the original OBDD and are suitable to be applied to classical software packages for the manipulation of OBDDs currently in use. Another result of the paper is the definition of a new canonical OBDD model, called Index-Resilient Reduced OBDD, which guarantees that a node with a faulty index has a reconstruction cost O(r), where r is the number of nodes with corrupted index. Experimental results on a classical benchmark suite validate the proposed approaches.}
}
@article{BESTA2013349,
title = {MATLAB Interfacing: Real-time Implementation of a Fuzzy Logic Controller},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {32},
pages = {349-354},
year = {2013},
note = {10th IFAC International Symposium on Dynamics and Control of Process Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20131218-3-IN-2045.00189},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015382823},
author = {Chandra Shekar Besta and Anil Kumar Kastala and Prabhaker Reddy Ginuga and Ramesh Kumar Vadeghar},
keywords = {MATLAB, Interfacing, FLC},
abstract = {In this work, the design and evaluation of a fuzzy logic control of liquid flow process is analyzed experimentally using MATLAB package. MATLAB is a widely used software environment for research and teaching applications on control and automation. The interface is a collection of hardware and software modules used to flexibly connect a plant, process or instrument (etc.) to a digital computer. The experimental performance of proposed fuzzy logic control is carried out on existing computer control of flow process. The program of Real-time data acquisition and control has been developed using modules called, “To Instrument” and “Query Instrument” of MATLAB for experimental work. Thus, The present implementation of intelligent fuzzy logic control on real-time basis is a pioneering work at laboratory scale. It is considered to be a great contribution in area of advanced process control systems. The simulation and experimental results clearly shows that the Intelligent Fuzzy Logic Controller gives a better control without overshoots of liquid flow rate in comparison with conventional PID controller.}
}
@article{LETTNER2020351,
title = {An integrated approach for power transformer modeling and manufacturing},
journal = {Procedia Manufacturing},
volume = {42},
pages = {351-355},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.076},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306417},
author = {Christian Lettner and Michael Moser and Josef Pichler},
keywords = {cutting stock problem, manufacturing, optimization, machine learning, software engineering},
abstract = {Essential characteristics of smart factories, such as flexibility and resource efficiency, can be leveraged and improved by the power of machine learning and optimization techniques. For instance, the manufacturing process of a power transformer core constitutes a highly complex optimization problem. It involves creating a cost optimal slitting plan that meets all customer requirements and at the same time takes into account flexible and short-term constraints from production (e.g. current available metal bands in stock). As many of these constraints rely on forecasts, a learning system may provide the necessary predictions for these constraints. In addition, companies apply and maintain engineering software for a variety of tasks in construction, simulation, and interpretation of data. For instance, electrical engineers use a variety of tools to design an initial model of a power transformer according to customer requirements and constraints. Such tools often incorporate knowledge that serves as input for optimization and forecast models as described before. If these models are improved over time using external machine learning libraries, the newly developed models must find their way back into the implementation of engineering tools. Knowledge scattered across multiple software systems bears risk of being inconsistent. Furthermore, keeping knowledge consistent without a systematic approach is time-consuming and error-prone. In this paper, we describe an approach that leverages software engineering methods and tools and that supports knowledge transfer between software systems for optimization and modelling tasks. The approach follows the idea of a single source of knowledge together with transformation into different representations, as required by different engineering tasks. The proposed approach was introduced at an industrial site to improve the manufacturing process of power transformer cores.}
}
@article{AKBAS2019174,
title = {Thermal-hydraulics and neutronic code coupling for RELAP/SCDAPSIM/MOD4.0},
journal = {Nuclear Engineering and Design},
volume = {344},
pages = {174-182},
year = {2019},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2019.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0029549318302863},
author = {Sabahattin Akbas and Victor Martinez-Quiroga and Fatih Aydogan and Chris Allison and Abderrafi M. Ougouag},
abstract = {The design and analysis of energy systems requires robust and reliable computer codes that produce realistic results. The governing equations and closure models must represent the physical behaviors of the energy systems. Nuclear power plants, as the quintessential example of complex energy systems, must be modeled with such high-fidelity and accurate nuclear system codes in which thermal-hydraulics, neutronics and fuel performance models must be coupled for in order to produce high-fidelity predictions. Most commonly, the interplay between the various subsystems of a nuclear power plant (nuclear fuel, fluids, neutronics, control rods, power conversion, etc.) is modeled through a combination of different codes that are externally coupled. The objective of this paper is to present work that couples the RELAP/SCDAP SIM/MOD4.0 code with the NESTLE neutronic code within the 3DKIN package, thus providing the latter with updated neutron kinetic capabilites. The version of the RELAP/SCDAPSIM/MOD4.0 code used in this work has been developed by Innovate System Software (ISS) and is the latest in the series of such code versions developed as part of the international SCDAP Development and Training Program (SDTP) for best-estimate analysis to model reactor transients including severe accident phenomena. The new feature of 3DKIN enhances the simulation of the Nuclear Power Plants (NPP) response under accidental and operational scenarios in which high reliability of neutronic feedback is needed. The RELAP/SCDAPSIM code is described and the improvements implemented in it are presented, including nodal kinetics library as well the coupling method that is applied. Finally, a BWR transient with unexpected injection of subcooled water as specified in an OECD Benchmark has been simulated in order to assess the reliability of the package and the performance of the coupling. Results are compared to those of the other OECD benchmark participants. The comparison of fuel temperature, effective multiplication factor (keff) and power distribution results display good agreement with values within the range of those of other participants.}
}
@article{KHAN20141468,
title = {High Performance Message-passing InfiniBand Communication Device for Java HPC},
journal = {Procedia Computer Science},
volume = {29},
pages = {1468-1479},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.133},
url = {https://www.sciencedirect.com/science/article/pii/S187705091400310X},
author = {Omar Khan and Mohsan Jameel and Aamir Shafi},
keywords = {MPJ Express, Java MPI, Java InfiniBand},
abstract = {MPJ Express is a Java messaging system that implements an MPI-like interface. It is used for writing parallel Java applications on High Performance Computing (HPC) hardware including commodity clusters. The software is capable of executing in multicore and cluster mode. In the cluster mode, it currently supports Ethernet and Myrinet based interconnects and provide specialized communication devices for these networks. One recent trend in distributed memory parallel hardware is the emergence of InfiniBand interconnect, which is a high-performance proprietary network and provides low latency and high bandwidth for parallel MPI applications. Currently there is no direct support available in Java (and hence MPJ Express) to exploit the performance benefits of InfiniBand networks. The only option to run distributed Java programs over InfiniBand networks is to rely on TCP/IP emulation layers like IP over InfiniBand (IPoIB) and Sockets Direct Protocol (SDP), which provide poor communication performance. To tackle this issue in the context of MPJ Express, this paper presents a low-level communication device called ibdev that can be used to execute parallel Java applications on InfiniBand clusters. MPJ Express is based on a layered architecture and hence users can opt to use ibdev at runtime on an InfiniBand equipped commodity cluster. ibdev improves Java application performance with access to InfiniBand hardware using native verbs API. Our performance evaluation reveals that MPJ Express achieves much better latency and bandwidth using this new device, compared to IPoIB and SDP. Improvement in communication performance is also evident in NAS parallel benchmark results where ibdev helps MPJ Express achieve better scalability and speedups as compared to IPoIB and SDP. The results show that it is possible to reduce the performance gap between Java and native languages with efficient support for low level communication libraries.}
}
@article{MARTTINEN1988409,
title = {A CAD Package for Process Analysis and Control Design},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {8},
pages = {409-413},
year = {1988},
note = {4th IFAC Symposium on computer aided Design in Control Systems 1988, Beijing, PRC, 23-25 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54987-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017549873},
author = {A. Marttinen and U. Kortela},
keywords = {Computer-aided system design, computer software, control system analysis, process control, modelling},
abstract = {The principles of implementing a computer-aided control system design and process analysis package are introduced. Although there are many commercial software packages available nowadays, there is still a shortage of suitable process-oriented CADCS-software. This paper is therefore especially conserned with methodology for building complex process models of use both for process analysis and for control design purposes. The usefulness of the packages always depends on the flexibility and simplicity of the user interface. We also emphasize its importance.}
}
@article{DAUBER201215,
title = {Achieving higher accuracies for process simulations by implementing the new reference equation for natural gases},
journal = {Computers & Chemical Engineering},
volume = {37},
pages = {15-21},
year = {2012},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2011.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0098135411002808},
author = {Florian Dauber and Roland Span},
keywords = {GERG-2008, Equation of state, CAPE-OPEN, Simulation, Liquefied natural gas, Liquefaction},
abstract = {For efficient design and operation of technical processes, accurate simulations are essential. Therefore, a precise representation of thermophysical properties using an adequate thermodynamic model is necessary. The GERG-2008 (Kunz et al., 2007, Kunz and Wagner, submitted for publication) is the new reference equation of state for natural gases consisting of up to 21 specific compounds. As the equation of state describes the gas and liquid phase as well as the super-critical region and the vapour–liquid equilibrium with the highest accuracy available, it has high potential for accurate process-modeling. In order to implement the software available for the new equation into various common simulation tools, the GERG-2008 Property Package has been developed, which complies with the CAPE-OPEN standards specification. The influence of thermodynamic models on the simulation of the only liquefaction plant for natural gas existing in Europe is investigated. Results show significant improvements in accuracy for simulations using the new thermodynamic model.}
}
@incollection{SANDROCK2009859,
title = {Dynamic simulation of Chemical Engineering systems using OpenModelica and CAPE-OPEN},
editor = {Jacek Jeżowski and Jan Thullie},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {26},
pages = {859-864},
year = {2009},
booktitle = {19th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(09)70143-9},
url = {https://www.sciencedirect.com/science/article/pii/S1570794609701439},
author = {Carl Sandrock and Philip L. {de Vaal}},
keywords = {Simulation, modelica},
abstract = {Modelica has emerged as a strong contender in the arena of dynamic simulation languages. It was developed to be a standard, with an open specification and a large and usable standard library. OpenModelica is an Open Source implementation of a Modelica compiler and environment which is being developed actively. Modelica's object-oriented design makes it easy to develop chemical engineering unit operations and connect them to one another. Unfortunately, most proprietary databases of thermodynamic and physical properties and reaction data are not supplied in equation form, but rather as part of closed software. This means that such data must be exchanged with the programs that contain them if they are to be used in custom simulation codes. The CAPE-OPEN specification provides a standard architecture for these exchanges, in addition to support for incorporating new unit operations or algorithms into existing proprietary simulations. In this study, a Modelica library allowing interface between Modelica and CAPE-OPEN is developed. Its functionality is demonstrated using a model of a ten plate distillation column simulated in OpenModelica on a Linux machine, with thermodynamic and property data from Honeywell Unisim on aWindows machine. The data interfacing was done over a TCP/IP network using CORBA. It is found that real-time operation is possible, but that network overhead makes up a significant fraction of the running time, posing problems for faster-than-realtime off-line simulation and optimization.}
}
@article{LIESLEHTO1991283,
title = {An Expert System with a Hypercard Interface},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {4},
pages = {283-288},
year = {1991},
note = {IFAC Symposium on Computer Aided Design in Control Systems, Swansea, UK, 15-17 July 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54285-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017542858},
author = {J. Lieslehto and J.T. Tanttu and H.N. Koivo},
keywords = {Expert systems, multivariable control systems, PID control, hypertext, computer-aided design},
abstract = {In this paper a software package for the design of centralized and decentralized mult ivariable controllers is represented. The software package consists of numerical calculation programs and an pypert. system. The paper first, describes the different subtasks of the multi variable controller design. Next a short descriptions of the design and analysis methods included in the software package is given. The description of the implementation of the expert system finishes the paper}
}
@article{SERBAN2001187,
title = {COOPT — a software package for optimal control of large-scale differential–algebraic equation systems},
journal = {Mathematics and Computers in Simulation},
volume = {56},
number = {2},
pages = {187-203},
year = {2001},
note = {Method of lines},
issn = {0378-4754},
doi = {https://doi.org/10.1016/S0378-4754(01)00289-0},
url = {https://www.sciencedirect.com/science/article/pii/S0378475401002890},
author = {Radu Serban and Linda R. Petzold},
keywords = {Differential–algebraic equations, Sensitivity analysis, Optimal control, Sequential quadratic programming methods},
abstract = {This paper describes the functionality and implementation of COOPT. This software package implements a direct method with modified multiple shooting type techniques for solving optimal control problems of large-scale differential–algebraic equation (DAE) systems. The basic approach in COOPT is to divide the original time interval into multiple shooting intervals, with the DAEs solved numerically on the subintervals at each optimization iteration. Continuity constraints are imposed across the subintervals. The resulting optimization problem is solved by sparse sequential quadratic programming (SQP) methods. Partial derivative matrices needed for the optimization are generated by DAE sensitivity software. The sensitivity equations to be solved are generated via automatic differentiation. COOPT has been successfully used in solving optimal control problems arising from a wide variety of applications, such as chemical vapor deposition of superconducting thin films, spacecraft trajectory design and contingency/recovery problems, and computation of cell traction forces in tissue engineering.}
}
@article{SHTUB1995173,
title = {Distributed database for project control},
journal = {International Journal of Project Management},
volume = {13},
number = {3},
pages = {173-176},
year = {1995},
issn = {0263-7863},
doi = {https://doi.org/10.1016/0263-7863(94)00005-W},
url = {https://www.sciencedirect.com/science/article/pii/026378639400005W},
author = {Avraham Shtub},
keywords = {distributed project environments, control, databases},
abstract = {The paper addresses the issue of control in a distributed project environment. The major problem that management encounter in such environments stems from the difficulty of integrating and processing the information generated, collected and stored in the databases used by each of the parties taking part in the project. A model is developed by which communication is established between any number of databases used by project-management software packages. A data structure is proposed as the basis for a distributed database. This structure is designed to facilitate the monitoring and control of projects carried out by several organizations in a simple manner. The proposed data structure was tested using two commercial PM software packages and a PM application based on dbase-3. The successful implementation indicates that a distributed database for project management and control is a useful approach that should be considered when projects are performed by several organizations using a variety of project-management software systems.}
}
@article{ERRIPRADEEP2021103667,
title = {Blockchain-aided information exchange records for design liability control and improved security},
journal = {Automation in Construction},
volume = {126},
pages = {103667},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001187},
author = {Abhinaw Sai {Erri Pradeep} and Tak Wing Yiu and Yang Zou and Robert Amor},
keywords = {Blockchain, Construction industry, Information exchange, Building Information Modelling (BIM), Record-keeping, Design liability, Information security, Smart contracts, Ethereum, Design science research, Prototype evaluation},
abstract = {With the recent advances in Information and Communication Technologies in the construction industry, information is exchanged digitally with little regard to the contracts that govern them. Although parties collaborating in project design are contracted to the client, they transact with each other when using BIM and other collaborative practices without any direct contractual relationship among themselves. This results in a lack of design liability control and an increase in claims and disputes. Further, the use of multiple software packages results in the exposure of data to third parties, data corruption and compromise in data privacy (using data for unintended purposes), data integrity (unauthorised access to sensitive data), and data longevity (loss of data post-handover). This study investigates blockchain technology (BCT) to address these issues using a design science research method. The current information exchange processes were mapped to identify the critical transactions that may benefit from record-keeping on the blockchain. Next, a prototype was designed to demonstrate and evaluate the proposed BCT integrated process models. Three key project processes, design review, design coordination and request for information; and two potential conflict scenarios during and post-construction were simulated as part of the evaluation. The prototype's implementation exhibits BCT's ability to record snapshots of individual design inputs to the overall project design and to enable a clear and long-term record of key exchange transactions. This improves the design liability control for contributing stakeholders and the auditability of the exchange records. Further, the proofs derived from such a system are independent of any third-party storage or subscription. Given the nature of records stored in a blockchain, the existence, integrity, and authenticity of information along with its associated metadata can be verified in the long-term as well. Therefore, BCT could be a supplementary technology that supports the existing information exchange systems.}
}
@article{WANG2021102088,
title = {Framework and deployment of a cloud-based advanced planning and scheduling system},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {70},
pages = {102088},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102088},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302982},
author = {Li-Chih Wang and Chun-Chih Chen and Jen-Li Liu and Pei-Chun Chu},
keywords = {Cloud-Based System, Advanced Planning and Scheduling, Cloud Computing, Industry 4.0},
abstract = {Many small and medium-sized manufacturing enterprises (SMEs) have already implemented enterprise resource planning (ERP) and manufacturing execution system (MES) and began to start the journey of cloud manufacturing; however, the high cost of hardware and software investment, implementation, and maintenance usually hinder SMEs from adopting an advanced planning and scheduling (APS) system. This paper aims to develop a cloud-based APS (C-APS) system framework, the service structure, and approach of deploying the C-APS system in a public cloud infrastructure platform and service provider or hybrid cloud platform. The package diagram is proposed for building the C-APS system's virtual factory model to improve modeling efficiency and data stability. The C-APS system is a cloud-based and object-oriented software; its simulation-based scheduling engine can generate the significant production and operations schedule, and has the characteristics of on-demand self-service, quickly expanding and adjusting to the virtual plant model. The C-APS system's application in a leading automotive part assembly company's printed circuit board production scheduling shows that the input planning data model is easy to maintain. The scheduling quality is high; the computing time is short and acceptable for practical application.}
}
@article{BLANQUER200051,
title = {Robust and Efficient Software for Control Problems: The SLICOT Library},
journal = {IFAC Proceedings Volumes},
volume = {33},
number = {6},
pages = {51-56},
year = {2000},
note = {6th IFAC Workshop on Algorithms and Architectures for Real-Time Control 2000 (AARTC'2000), Palma de Mallorca, Spain, 15-17 May 2000},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)35447-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017354472},
author = {I. Blanquer and V. Hernández and A. Vidal and E. Arias},
keywords = {Computer-aided control systems design, Parallel algorithms, Numerical algorithms, Robust control, Model reduction, Nonlinear control systems, Identification},
abstract = {This article describes the library SLICOT (Software Library In COntrol Theory) [40] [7] [43] [2] that provides Fortran 77 implementation and also MATLAB and SciLAB [39] interfaces for numerical algorithms for computations in systems and control theory. The library is build up on an efficient kernel of basic numerical linear algebra subroutines and provides tools for solving matrix equations, model reduction, subspaceidentification, robust control and non-linear control. SLICOT is freely available and it has been tested on many industrial problems.}
}
@article{HARRISON1996417,
title = {Integrating multiple and diverse abstract knowledge types in real-time embedded systems},
journal = {Knowledge-Based Systems},
volume = {9},
number = {7},
pages = {417-434},
year = {1996},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(96)01052-0},
url = {https://www.sciencedirect.com/science/article/pii/S0950705196010520},
author = {Alan Harrison and Peter G. Thomas},
keywords = {Real-time embedded system, Knowledge-based techniques, Knowledge-based components, Abstract knowledge types, Blackboard architecture, Software engineering principles, Ada},
abstract = {Designers of large-scale real-time systems are increasingly turning to knowledge-based techniques in order to solve complex problems. This paper identifies three essential needs to support the implementation of these systems: first, the need to provide a variety of knowledge-based components that can be used to model the diverse expert domains being encountered; second, the need to provide the user with the means of creating multiple independent instances of the knowledge-based components; and third, the need to provide an integrating environment in which the knowledge-based instances may be controlled. This paper uses ideas derived from the concept of abstract data types and recommends the construction of a library of diverse knowledge-based components, called abstract knowledge types, and that multiple instances of the abstract knowledge types be integrated and controlled using a blackboard architecture. A prototype component library and a blackboard have been implemented in Ada in order to take advantage of a real-time language which supports software engineering principles through a well defined and enforced standard. The use of abstract knowledge types gives a uniform software engineered approach to the development and integration of both conventional and knowledge-based components.}
}
@article{NAERT2021102001,
title = {Interactive and targeted runtime verification using a debugger-based architecture},
journal = {Journal of Systems Architecture},
volume = {115},
pages = {102001},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000187},
author = {Paul Naert and Seyed Vahid Azhari and Michel Dagenais},
keywords = {Runtime verification, Dynamic instrumentation, Debugging},
abstract = {Runtime verification of software (RV) often relies on two categories of tools : dynamic heavy-weight tools, which significantly impact performance, and lighter and more efficient but static tools, which require recompiling the binary. In this paper we propose a new framework for building efficient and targeted dynamic RV tools, bridging the gap between those two categories. This framework is separated into two domains : source and binary. On the source level, a modular development environment provides a custom user interface which allows for precise targeting of instrumentation, as well as advanced interactivity. The binary level revolves around a debugger, which controls binary manipulation and library loading. In order to create fully dynamic tools, we added new instrumentation capabilities to the GNU debugger, using trampoline-based probes to inject code in the binary efficiently and interactively. Our framework focuses on accessibility for users via the graphical interface, and for developers by making it easy to adapt existing tools and by relying on popular programming languages such as Python and C++. As a demonstration of our framework capabilities, we provide a significantly faster implementation of conditional breakpoints for GDB, as well as targeted and fully dynamic versions of two state-of-the-art runtime verification tools : Address Sanitizer and Data Watch.}
}
@article{AHMADABADIAN2017185,
title = {Clustering and selecting vantage images in a low-cost system for 3D reconstruction of texture-less objects},
journal = {Measurement},
volume = {99},
pages = {185-191},
year = {2017},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2016.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0263224116307242},
author = {Ali Hosseininaveh Ahmadabadian and R. Yazdan and A. Karami and M. Moradi and F. Ghorbani},
keywords = {3D modelling, Texture-less objects, Low-cost system, Structure from motion, Dense matching, Image clustering and selection, Imaging network design},
abstract = {Structure from Motion is one of the image-based methods for 3D reconstruction. This technique coupled with Dense Multi-View Matching can provide an accurate and dense point cloud from texture-full objects. Although the procedure in these techniques is now fully automatic, capturing images of the object in suitable position is difficult, especially when the object is texture-less and needs projectors to provide a pattern on the object. Even though the images can be captured, the processing time in Dense Matching software is directly related to the number of images. A package including hardware and software is presented in this paper for solving these issues. Regarding the hardware, a low-cost and portable system for 3D modeling of texture-less objects is proposed. This system includes a rotation table designed and developed by using a stepper motor and a light circular plate. The system also has eight laser light sources with very dense and strong beams which provide a relatively appropriate pattern on texture-less objects. Regarding the software, Imaging Network Designer (IND), recently developed by the first author, was adjusted to be exploited by the proposed system. IND can both design a complete imaging network and cluster and select vantages images in a dense imaging network. In this system, regarding to the step of stepper motor, images are semi automatically taken by a digital camera. The images were imported in Structure From Motion (SFM) procedures implemented in Agisoft Photoscan to align images and provide a rough model. IND then used this information, to cluster and select vantage images for dense image matching. The comparison between normal procedure and using IND in the proposed system showed the importance of IND in reducing the elapsed time by 30 percentages. To evaluate the accuracy of the proposed system, two dark objects were considered and the point clouds of these objects were obtained by both the proposed system and a structured light system called GOM ATOS Compact Scan 2M laser scanner with an accuracy of 0.02mm. The comparison between point clouds in three different tests showed the ability of the proposed system in delivering a 3D model for the texture-less objects with an accuracy of 0.3mm.}
}
@article{MALINOWSKI1995119,
title = {Computer-Based Analysis of Complex Systems; Basic Concepts and Software Environments},
journal = {IFAC Proceedings Volumes},
volume = {28},
number = {10},
pages = {119-123},
year = {1995},
note = {7th IFAC Symposium on Large Scale Systems: Theory and Applications 1995, London, UK, 11-13 July, 1995},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51503-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017515037},
author = {K. Malinowski and E. Niewiadomska-Szynkiewicz},
keywords = {Complex systems, computer systems, computer-aided engineering, computer-aided system design, computer simulation, large scale systems, control system analysis, systems engineering, software tools},
abstract = {The paper is concerned with computational research for large scale systems. The main objective is to present basic ideas related to computer-based analysis and design of complex systems. The particular attention is focused on the software environment which provide framework for control structures analysis and design. The important issues related to such software systems, their design, implementation and use is discussed. Some guiding principles for the design of computer tools suitable for computational research are presented. Two basic directions to take when developing software environment, i.e., general purpose (universal) and problem dedicated (specialised) systemsare distinguished and described. The examples of several specialised software packages described for complex systems analysis and simulation are presented in the final part of the paper.}
}
@article{SILANI201430,
title = {A semi-concurrent multiscale approach for modeling damage in nanocomposites},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {74},
pages = {30-38},
year = {2014},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2014.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167844214000871},
author = {Mohammad Silani and Saeed Ziaei-Rad and Hossein Talebi and Timon Rabczuk},
keywords = {Semi-concurrent multiscale methods, Nanocomposites, Damage mechanics, Finite element analysis (FEA)},
abstract = {The paper presents an effective implementation of a semi-concurrent multiscale method in the commercial finite element software package ABAQUS. The method is applied to the pre-localized damage initiation and propagation in the fully exfoliated clay/epoxy nanocomposite. The obtained results of the proposed method is also compared with the hierarchical multiscale approach. This method can be easily used to get a better understanding of damage mechanism in the nanocomposite materials in order to improve the constitutive models and to support the future design of those materials.}
}
@article{CHANG2011232,
title = {Structural planning and implementation of a microprocessor-based human–machine interface in a steam-explosion process application},
journal = {Computer Standards & Interfaces},
volume = {33},
number = {3},
pages = {232-248},
year = {2011},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2010.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0920548910000553},
author = {Ruey-Fong Chang and Chen-Wei Chang and Kuo-Hsiung Tseng and Cheng-Lun Chiang and Wen-Shiow Kao and Wen-Jang Chen},
keywords = {Steam-explosion process, Microprocessor based, Human–machine interface, Supervisory control, Graphical user interface},
abstract = {In this paper, small-scale microprocessor-based human–machine interface (HMI) and programmable logic controller (PLC) standard software libraries are applied to plan, design, implement, and construct models for creating the modular application software. The graphical user interface (GUI) functions used in a specific biomass steam-explosion process, intended to be integrated with the PLC device for HMI, thereby providing excellent GUI-based supervision and control functions. Reviewing these specific case study results, we find that the major advantage is maximum system operating performance at minimum cost. Also, the GUI/HMI operation effectively provides user-friendly and reliable interactions. This GUI/HMI approach is a very cost-effective technique.}
}
@article{PELLEGRINI1997153,
title = {Graph partitioning based methods and tools for scientific computing},
journal = {Parallel Computing},
volume = {23},
number = {1},
pages = {153-164},
year = {1997},
note = {Environment and tools for parallel scientific computing},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(96)00102-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819196001020},
author = {François Pellegrini},
keywords = {Static mapping, Graph partitioning, Dual recursive bipartitioning, Domain decomposition},
abstract = {The combinatorial optimization problem of assigning the communicating coexisting processes of a parallel program onto a parallel machine so as to minimize its overall execution time is called static mapping. This paper presents the work that has been carried out to date at the LaBRI on static mapping. We introduce a static mapping algorithm based on the recursive bipartitioning of both the source process graph and the target architecture graph and describe the capabilities of SCOTCH 3.1, a software package that implements this method. SCOTCH can efficiently map any weighted source graph onto any weighted target graph in a time linear in the number of source edges and logarithmic in the number of target vertices. We give brief descriptions of the algorithm and its bipartitioning methods and compare the performance of our mapper with respect to other mapping and partitioning software packages.}
}
@article{YANG2004249,
title = {A web-based platform for computer simulation of seismic ground response},
journal = {Advances in Engineering Software},
volume = {35},
number = {5},
pages = {249-259},
year = {2004},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2004.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0965997804000286},
author = {Zhaohui Yang and Jinchi Lu and Ahmed Elgamal},
keywords = {Internet computing, World Wide Web, Software engineering, Java, Graphical user interface, Liquefaction, Seismic response},
abstract = {The Internet provides an open environment for more efficient development and utilization of engineering software. This article presents a generic web-based platform for conducting model-based numerical simulations online. The platform distributes pre- and post-processing components to the user computer, and only retains core computational functions on the server machine. Design of this platform addresses Internet-specific issues such as supports for multiple users, integration of various programming languages or modules on both client and server sides, and the concerns of Internet traffic/security. As an implementation of this platform, a web site is developed for online execution of a solid–fluid fully coupled nonlinear Finite Element code, to conduct simulations of seismic ground response and liquefaction effects. At this web site, users can select the soil composition and input seismic excitation from built-in material/motion libraries, or define their own material properties and/or input motions. The output interface allows graphical rendering of simulation results, animations, and automated report generation. All software packages employed in this work are well tested and documented freeware, and can be easily adapted for execution of other computational codes.}
}
@article{BASTOS2005203,
title = {Automatically building customized circuit-based simulation models using symbolic computing},
journal = {Mathematics and Computers in Simulation},
volume = {70},
number = {4},
pages = {203-220},
year = {2005},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2005.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378475405001977},
author = {J. Bastos and A. Monti},
keywords = {Automatic code generation, Problem-solving environment (PSE) tools, Symbolic computing, Circuit simulation},
abstract = {Nowadays, most scientists and engineers rely on computer simulations to analyze, design, and prototype complex systems. Scientific and engineering system models are implemented in a variety of simulation environments; however, limited model libraries are often provided to users who are compelled to create their own customized models. Developing customized simulation models is a time-consuming and error-prone task which requires software developer skills. The difficulty of creating customized models presents the need for user-friendly tools that assist users in their code development process. This paper describes a tool named RCMAG that automatically builds customized models for the Virtual Test Bed (VTB) [http://vtb.engr.sc.edu/, The Virtual Test Bed Overview, Architecture, and Downloads] simulation environment. VTB is an interactive software system that provides a problem-solving environment (PSE) for simulation, prototyping, and advanced visualization of large-scale multi-disciplinary systems. VTB complies with a simulation standard named VHDL-AMS to enforce natural (circuit), signal, and data coupling between entities from diverse disciplines. The PSE tool RCMAG automatically builds naturally coupled or circuit-based VTB entities from input data provided by the user. RCMAG accepts both numeric and symbolic input data from the user, manages it to create code, and releases the compiled model object, which is ready to be added to the VTB model library and be used in a simulation.}
}
@article{MAGOON201335,
title = {Design and implementation of a next-generation software interface for on-the-fly quantum and force field calculations in automated reaction mechanism generation},
journal = {Computers & Chemical Engineering},
volume = {52},
pages = {35-45},
year = {2013},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2012.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0098135412003754},
author = {Gregory R. Magoon and William H. Green},
keywords = {Automated reaction mechanism generation, Parameter estimation, Thermodynamic properties, Quantum chemistry, Molecular structure, Cheminformatics},
abstract = {A software interface for performing on-the-fly quantum and force field calculations has been developed and integrated into RMG, an open-source reaction mechanism generation software package, to provide needed estimates of thermodynamic parameters. These estimates based on three-dimensional molecular geometries bypasses the traditional group-additivity-based approach, which can suffer from lack of availability of necessary parameters; this issue is particularly evident for polycyclic species with fused rings, which would require ad hoc ring corrections in the group-additivity framework. In addition to making extensive use of open-source tools, the interface takes advantage of recent developments from several fields, including three-dimensional geometry embedding, force fields, and chemical structure representation, along with enhanced robustness of quantum chemistry codes. The effectiveness of the new approach is demonstrated for a computer-constructed model of combustion of the synthetic jet fuel JP-10. The interface also establishes a framework for future improvements in the chemical fidelity of computer-generated kinetic models.}
}
@article{GARCIA2011834,
title = {Design guidelines for software processes knowledge repository development},
journal = {Information and Software Technology},
volume = {53},
number = {8},
pages = {834-850},
year = {2011},
note = {Advances in functional size measurement and effort estimation - Extended best papers},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911000619},
author = {Javier García and Antonio Amescua and María-Isabel Sánchez and Leonardo Bermón},
keywords = {Software engineering, Software process technology, Knowledge management, Agile development, Web 2.0, Wiki},
abstract = {Context
Staff turnover in organizations is an important issue that should be taken into account mainly for two reasons: 1.Employees carry an organization’s knowledge in their heads and take it with them wherever they go2.Knowledge accessibility is limited to the amount of knowledge employees want to share
Objective
The aim of this work is to provide a set of guidelines to develop knowledge-based Process Asset Libraries (PAL) to store software engineering best practices, implemented as a wiki.
Method
Fieldwork was carried out in a 2-year training course in agile development. This was validated in two phases (with and without PAL), which were subdivided into two stages: Training and Project.
Results
The study demonstrates that, on the one hand, the learning process can be facilitated using PAL to transfer software process knowledge, and on the other hand, products were developed by junior software engineers with a greater degree of independence.
Conclusion
PAL, as a knowledge repository, helps software engineers to learn about development processes and improves the use of agile processes.}
}
@article{CALMET1997221,
title = {Towards the Mathematics Software Bus},
journal = {Theoretical Computer Science},
volume = {187},
number = {1},
pages = {221-230},
year = {1997},
issn = {0304-3975},
doi = {https://doi.org/10.1016/S0304-3975(97)00066-2},
url = {https://www.sciencedirect.com/science/article/pii/S0304397597000662},
author = {Jacques Calmet and Karsten Homann},
keywords = {Software environments, Symbolic mathematical computing},
abstract = {The Mathematics Software Bus is a software environment for combining heterogeneous systems performing any kind of mathematical computation. Such an environment will provide combinations of graphics, editing and computation tools through interfaces to already existing powerful software by flexible and powerful semantically integration. Communication and cooperation mechanisms for logical and symbolic computation systems enable to study and solve new classes of problems and to perform efficient computation in mathematics through cooperating specialized packages. We give an overview on the need for cooperation in solving mathematical problems and illustrate the advantages by several well-known examples. The needs and requirements for the Mathematics Software Bus and its architecture are demonstrated through some implementations of powerful interfaces between mathematical services.}
}
@article{TYCH201219,
title = {A Matlab software framework for dynamic model emulation},
journal = {Environmental Modelling & Software},
volume = {34},
pages = {19-29},
year = {2012},
note = {Emulation techniques for the reduction and sensitivity analysis of complex environmental models},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2011.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815211001915},
author = {W. Tych and P.C. Young},
keywords = {Emulation, DACE, DBM, Dominant modes, Transfer function, Matlab},
abstract = {The paper describes a software framework for implementing the main stages of the Data Based Mechanistic (DBM) modelling approach to the reduced order emulation (meta-modelling) of large dynamic system computer models, within the Matlab software environment. The framework exploits routines in the CAPTAIN Toolbox to identify and estimate transfer function models that reflect the dominant modes of the dynamic behaviour in the large model. This allows for the ‘nominal emulation’ and validation of the large model for a single, specified set of parameters; as well as ‘stand-alone, full emulation’ based on the construction and validation of hyper-dimensional maps between a user-specified range of large model parameters and the parameters of the associated, low order transfer function models. The software framework uses the multivariable structure constructs available within Matlab™ to form a small library of routines that will become part of the Captain Toolbox. The library is formed around special data structures that facilitate multivariable operations and visualisations which both enhance the efficiency of the emulation modelling analysis and the modeller’s interaction with the process of emulation. The nature of the analysis is illustrated by a topical example concerned with the emulation of the OTIS computer simulation model for the transport and dispersion of solutes in a river system.}
}
@article{DIEZSIERRA2017243,
title = {A rainfall analysis and forecasting tool},
journal = {Environmental Modelling & Software},
volume = {97},
pages = {243-258},
year = {2017},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2017.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815217302323},
author = {Javier Diez-Sierra and Manuel {del Jesus}},
keywords = {Rainfall modeling, Weather types, Spatial analysis, Kriging, Rainfall extremes},
abstract = {MENSEI-L is a stand-alone software tool for the automatic analysis of pluviometric networks, that also provides three-day rainfall forecasts based on weather types. The software tool, implemented in Python and R, is able to fill missing values in original daily data series and to generate synthetic pluviometers in ungauged locations, by means of kriging techniques. MENSEI-L also characterizes punctual and spatial, average and extreme distributions of precipitation for the complete pluviometric network. Tenerife (Canary Islands, Spain) is used as study site to evaluate the capabilities of MENSEI-L and the implicit rainfall analysis methodology that it implements. MENSEI-L proves to be a useful tool to extract information from dense observation networks where manual analysis is not practical.}
}
@article{POPESCU2005130,
title = {OPTIMAL DECISIONS FOR LARGE SCALE SYSTEMS},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {130-135},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.01562},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016375747},
author = {Dumitru Popescu and Mihaela Mateescu and Bogdan Ciubotaru},
keywords = {large scale systems, decomposition techniques, optimization methods, software package},
abstract = {This paper presents the software package SISCON, dedicated to the evaluation of optimal decisions for large scale systems. SISCON evaluates mathematical models nonlinear systems and computes the optimal decisions solving the mathematical nonlinear programming problems. The LSS is a complex system structure and global approach computation can not be carried out. After decomposition of the large-scale problems, the corresponding sub-problems are solved using standard optimization techniques. SISCON is used in control and supervision strategies for complex systems, improving the support of real-time applications. An application on steel plant, focused on the combustion process optimization of pre-heating installations, was implemented.}
}
@incollection{TANASE2011643,
title = {Software application for intelligent control of a bioprocess. Case study},
editor = {E.N. Pistikopoulos and M.C. Georgiadis and A.C. Kokossis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {29},
pages = {643-647},
year = {2011},
booktitle = {21st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-53711-9.50129-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444537119501292},
author = {Cristina Tănase and Mihai Caramihai and Camelia Ungureanu and Gheorghe Sârbu and Ana Aurelia Chirvase and Ovidiu Muntean},
keywords = {intelligent control, fuzzy model, bioprocess optimization},
abstract = {The research described in this paper is a part of a larger experimental project dealing with the bioprocess optimisation for an immunomodulator product preparation extracted from the harvested cells. The bioprocess is performed in 100L Bioengineering bioreactor with 42L cultivation medium, equipped with pH, temperature, dissolved oxygen, and agitation controllers. The main objective of this paper is to present a case study to demonstrate that intelligent control, describing the complexity of the biological process in a qualitative and subjective manner as perceived by human operator, is an efficient control strategy for this kind of bioprocesses. In order to simulate the bioprocess evolution, an intelligent control structure, based on fuzzy logic has been designed. BIOSIM, an original developed software package, implements such a control structure. The simulation study has showed that the fuzzy technique is quite appropriate for this non-linear, time varying system vs. the classical control method based on a priori model.}
}
@article{BORCEA2017177,
title = {PICADOR: End-to-end encrypted Publish–Subscribe information distribution with proxy re-encryption},
journal = {Future Generation Computer Systems},
volume = {71},
pages = {177-191},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303983},
author = {Cristian Borcea and Arnab “Bobby” Deb Gupta and Yuriy Polyakov and Kurt Rohloff and Gerard Ryan},
keywords = {Security, Encryption, Information brokering},
abstract = {This article presents PICADOR, a system for end-to-end encrypted Publish–Subscribe information distribution with proxy re-encryption. PICADOR is designed for topic-based Pub/Sub systems and provides end-to-end payload confidentiality. The main novelty of PICADOR is that it provides an information distribution service with end-to-end encryption where publishers and subscribers do not need to establish shared encryption and decryption keys. Multiple publishers post encrypted information to a Pub/Sub broker which uses Proxy Re-Encryption (PRE) to convert this information into a representation that can only be decrypted by approved subscribers. The broker is unable to decrypt the information. To support PICADOR, we design and implement a novel PRE scheme that leverages a general lattice encryption software library. We prototype our system using a scalable Java-based information substrate that supports topic-based Pub/Sub operations. We experimentally evaluate performance and scalability tradeoffs in the context of enterprise and mobile applications. We discuss design tradeoffs and application-specific customizations.}
}
@article{TAK2018330,
title = {Conceptual design of new data integration and process system for KSTAR data scheduling},
journal = {Fusion Engineering and Design},
volume = {129},
pages = {330-333},
year = {2018},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2018.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0920379618300334},
author = {Taehyun Tak and Jaesic Hong and Kaprai Park and Woongryul Lee and Taegu Lee and Hyunsun Han and Giil Kwon and Jinseop Park},
keywords = {KSTAR, Fusion, Dataflow, Automation, Many-task, Big Data},
abstract = {The KSTAR control and data acquisition systems mainly use data storage layer of MDSPlus for diagnostic data and channel archiver for EPICS-based control system data. In addition to these storage systems, KSTAR has various types of data such as user logs from Relational Database (RDB) and various types of logs from the control system. A large scientific machine like KSTAR is needed to implement various types of use cases for scheduling data and data analysis. The goal of a new data integration and process system is to design the KSTAR data scheduling on top of the Pulse Automation and Scheduling System (PASS) according to KSTAR events. The KSTAR Data Integration System (KDIS) is designed by using Big Data software infrastructures and frameworks. The KDIS handles events that are synchronized with the KSTAR EPICS events and other data sources such as the rest API and logs for integrating and processing data from different data sources and for visualizing data. In this paper, we explain the detailed design concept of KDIS and demonstrate a data scheduling use case with this system.}
}
@article{CONNELL2002453,
title = {A framework for immersive FEM visualisation using transparent object communication in a distributed network environment},
journal = {Advances in Engineering Software},
volume = {33},
number = {7},
pages = {453-459},
year = {2002},
note = {Engineering Computational Technology & Computational Structures Technology},
issn = {0965-9978},
doi = {https://doi.org/10.1016/S0965-9978(02)00063-7},
url = {https://www.sciencedirect.com/science/article/pii/S0965997802000637},
author = {Mike Connell and Odd Tullberg},
keywords = {OOP, Virtual reality, Design, Visualisation, Distributed computing, Java, RMI},
abstract = {We present the implementation of a software framework for conducting interactive FE simulations within Virtual Environments (VEs), and show how this can be used for the visualisation of loading on large-scale structures. Approximation methods that allow results to be displayed within the VE before the FEA is complete are also discussed. The framework is built using modular object orientated technology, and exists as a distributed application running over a WAN. Use of modern distributed object libraries allows this parallelism to be largely transparent within the framework. The use of strictly enforced software interfaces provides a clean separation between our framework and the modules that provide services and functionality.}
}
@article{LEE201894,
title = {New control systems at KSTAR compatible with ITER standard technologies},
journal = {Fusion Engineering and Design},
volume = {129},
pages = {94-98},
year = {2018},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2018.02.077},
url = {https://www.sciencedirect.com/science/article/pii/S0920379618301789},
author = {Woongryol Lee and Taegu Lee and Giil Kwon and Taehyun Tak and Jinseop Park and Myungkyu Kim and Yeon-jung Kim and Jaesic Hong},
keywords = {KSTAR, EPICS, SDN, TCN, MTCA.4, Real-time application},
abstract = {Through the lessons learned during the ITER control, data access and communication (CODAC) tasks and the interaction with various domestic and overseas communities, the KSTAR control system is being improved with new functions. We have adopted ITER synchronous data bus network (SDN) and time communication network (TCN) to increase the flexibility of control system design and improve system performance. Absolute time-based discharge operations and high-speed data communication interfaces are standardized as KSTAR standard software components. We used the reconfigurable I/O (RIO) library provided by CODAC for the renovation of a fast interlock system, and effectively implemented a real-time core structure through ITER for the next generation real-time control systems. A new hardware platform standard has been adopted for system standardization and convenience of maintenance international collaboration.}
}
@article{TAHA2020100140,
title = {Estimation performance of the lightning protection system in an urban 110 ​kV grounding grid substation},
journal = {Results in Engineering},
volume = {6},
pages = {100140},
year = {2020},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2020.100140},
url = {https://www.sciencedirect.com/science/article/pii/S2590123020300463},
author = {Mohammed Abdaldaim Taha and Lin Li and Ping Wang},
keywords = {Lightning protection system (LPS), NFPA 780, IEC 62305-3, Fast fourier transform (FFT), CDEGS software, 110 ​kV substation},
abstract = {A lightning protection system (LPS) of an urban 110 ​kV substation is designed and analyzed according to NFPA 780 and IEC 62305-3 standards. The analysis of the LPS is established on the value of risk assessment. The total area of the substation has been described based on the modern designs represented by the real operational efficiency of the facility. This study aims to improve the understanding of the unexpected manner of the grounding system beneath lightning currents by clarifying the basic concepts of the lightning protection level. The mode was represented by the CDEGS software package, which provides effective geometrical modeling with object and result visualization. Furthermore, module and automated Fast Fourier Transform (FFT) is implemented in this study to simulate electromagnetic fields in the time and high-frequency domains. These current values are compared to the desired protection levels within the standards. The study results show that for the improved protection of the primary system against lightning, the total power grid must be considered as a source of improvement for studying shielding influence and the protection levels provided inside this substation. The lightning behavior of overhead transmission lines is associated with line flashover protection consequent to lightning strokes.}
}
@article{PATIKIRIKORALA20122678,
title = {An evaluation of multi-model self-managing control schemes for adaptive performance management of software systems},
journal = {Journal of Systems and Software},
volume = {85},
number = {12},
pages = {2678-2696},
year = {2012},
note = {Self-Adaptive Systems},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.05.077},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212001628},
author = {Tharindu Patikirikorala and Alan Colman and Jun Han and Liuping Wang},
keywords = {Feedback control, Adaptive control, Reconfiguring control, Self-managing systems, Quality of service, Multi-model},
abstract = {Due to the increasing complexity of software systems and the dynamic unpredictable environments they operate in, methodologies to incorporate self-adaptation into these systems have been investigated in recent years. The feedback control loop has been one of the key concepts used in building self-adaptive software systems to manage their performance among other quality aspects. In order to design an effective feedback control loop for a software system, modeling the behavior of the software system with sufficient accuracy is paramount. In general, there are many environmental conditions and system states that impact on the performance of a software system. As a consequence, it is impractical to characterize the diverse behavior of such a software system using a single system model. To represent such highly nonlinear behavior and to provide effective runtime control, the design, integration and self-management (automatic switching) of multiple system models and controllers are required. In this paper, we investigate a control engineering approach, called Multi-Model Switching and Tuning (MMST) adaptive control, to assess its effectiveness for the adaptive performance management of software systems. We have conducted a range of experiments with two of the most promising MMST adaptive control schemes under different operating conditions of a representative software system. The experiment results have shown that the MMST control schemes are superior in managing the performance of the software system, compared with a number of other control schemes based on a single model. We have also investigated the impact of the configuration parameters for the MMST schemes to provide design guidance. A library of MMST schemes has been implemented to aid the software engineer in developing MMST-based self-managing control schemes for software systems.}
}
@article{TSAI2010476,
title = {A three-stage framework for introducing a 4D tool in large consulting firms},
journal = {Advanced Engineering Informatics},
volume = {24},
number = {4},
pages = {476-489},
year = {2010},
note = {Construction Informatics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2010.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1474034610000261},
author = {Meng-Han Tsai and Shih-Chung Kang and Shang-Hsien Hsieh},
keywords = {4D tool, Usability test, Workflow},
abstract = {The increase in the use of 4D management tools in recent years within the construction industry has been phenomenal, partly due to the increasing support available in commercial software packages, and partly in response to a greater demand for efficient construction management. However, successfully implementing a 4D management tool in an engineering firm for use in actual projects remains a challenging task. This paper presents the authors’ experiences of implementing an in-house 4D management tool at a large engineering, procurement and construction (EPC) firm with a long history of design–build projects. A three-stage consulting framework of system evaluation, usability study, and management plan (SUM) was proposed and implemented for a firm of this size, which included three parts: (1) System evaluation: requirement analysis and performance evaluation of both hardware and software components of the 4D tool; (2) Usability study: usability tests and improvement of the 4D tool; and (3) Management plan: workflow re-engineering for the firm to be able to successfully implement and apply the 4D management tool to actual projects. We found that the SUM framework was able to effectively identify major problems when introducing a 4D tool to a large design–build project and helped to minimize its own impact on the firm’s business processes.}
}
@article{GARCIA198851,
title = {Specification, design and modula-2 implementation of a low cost industrial control system},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {3},
pages = {51-56},
year = {1988},
note = {15th IFAC/IFIP Workshop on Real Time Programming 1988, Valencia, Spain, 25-27 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-036236-6.50012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080362366500126},
author = {D. Garcia and H. Lopez and J. Tuya and A. Diez},
keywords = {Software Engineering, Real time control, Modula-2 language, Concurrency, Computer-aided training},
abstract = {The purpose of this paper is to show the application of modern software engineering techniques to the construction of a small/medium size package for real time control. This package, denoted by SCM (acronym of “Sistema de Control Modular”), has been written in Modula-2 and presents all advantages offered by this high-level structured programming language. It has been implemented on a PC-AT compatible computer.}
}
@article{DOBRESCU2019131,
title = {Process simulation platform for virtual manufacturing systems evaluation},
journal = {Computers in Industry},
volume = {104},
pages = {131-140},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517306218},
author = {Radu Dobrescu and Daniel Merezeanu and Stefan Mocanu},
keywords = {Virtual manufacturing systems, Hybrid process simulation, Context-aware systems, Cloud computing, Hardware-in-loop, Sensor-cloud infrastructure, Software reusability},
abstract = {The paper presents the design of a cloud simulation platform and the associated services that will provide the computing resources and services for hybrid simulation of Virtual Manufacturing Systems. At the core of the research is the design of a framework defined as Virtual Development Environment (VDE). Associated to VDE is a set of block functions and algorithms defined as software assets, stored in a large repository implemented within an on-line cloud supported library. The platform offers flexibility concerning the continuous testing and updates regarding the algorithms in order to improve the existing ones.}
}
@article{SCHONAUER2000269,
title = {Numerical engineering: design of PDE black-box solvers},
journal = {Mathematics and Computers in Simulation},
volume = {54},
number = {4},
pages = {269-277},
year = {2000},
issn = {0378-4754},
doi = {https://doi.org/10.1016/S0378-4754(00)00188-9},
url = {https://www.sciencedirect.com/science/article/pii/S0378475400001889},
author = {Willi Schönauer},
keywords = {G4 Mathematical software, Algorithm design and analysis, Efficiency, Parallel and vector implementations, Reliability and robustness},
abstract = {The design of PDE black-box solvers (for nonlinear systems of elliptic and parabolic PDEs) needs many compromises between efficiency and robustness which we call ‘Numerical Engineering’. The requirements for a black-box solver are formulated and the way how to meet them is presented, guided by many years of practical experience in the design of the program packages fidisol/cadsol, vecfem and linsol. The basic approach to the new finite difference element method (fdem) program package, an FDM on an unstructured FEM grid, is discussed. The common feature of all these methods is the error equation that allows a transparent balancing of all errors. The discretization errors are estimated from difference formulae of different consistency orders. The error balancing must include the iterative solution of the large and sparse linear systems by the linsol program package. The real challenge is the parallelization on distributed memory parallel computers which is solved by corresponding data structures with optimal communication patterns and redistribution after each grid refinement cycle.}
}
@article{TARSITANO2021100484,
title = {Predicting cosmological observables with PyCosmo},
journal = {Astronomy and Computing},
volume = {36},
pages = {100484},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2021.100484},
url = {https://www.sciencedirect.com/science/article/pii/S221313372100038X},
author = {F. Tarsitano and U. Schmitt and A. Refregier and J. Fluri and R. Sgier and A. Nicola and J. Herbel and A. Amara and T. Kacprzak and L. Heisenberg},
keywords = {Cosmology, Theory, Models, Python},
abstract = {Current and upcoming cosmological experiments open a new era of precision cosmology, thus demanding accurate theoretical predictions for cosmological observables. Because of the complexity of the codes delivering such predictions, reaching a high level of numerical accuracy is challenging. Among the codes already fulfilling this task, PyCosmo is a Python-based framework providing solutions to the Einstein–Boltzmann equations and accurate predictions for cosmological observables. We present the first public release of the code, which is valid in ΛCDM cosmology. The novel aspect of this version is that the user can work within a Python framework, either locally or through an online platform, called PyCosmo Hub. In this work we first describe how the observables are implemented. Then, we check the accuracy of the theoretical predictions for background quantities, power spectra and Limber and beyond-Limber angular power spectra by comparison with other codes: the Core Cosmology Library (CCL), CLASS, HMCode and iCosmo. In our analysis we quantify the agreement of PyCosmo with the other codes, for a range of cosmological models, monitored through a series of unit tests. PyCosmo, conceived as a multi-purpose cosmology calculation tool in Python, is designed to be interactive and user-friendly. The PyCosmo Hub is accessible from this link: https://cosmology.ethz.ch/research/software-lab/PyCosmo.html. On this platform the users can perform their own computations using Jupyter Notebooks without the need of installing any software, access to the results presented in this work and benefit from tutorial notebooks illustrating the usage of the code. The link above also redirects to the code release and documentation.}
}
@article{MARKOVSKY20131422,
title = {A software package for system identification in the behavioral setting},
journal = {Control Engineering Practice},
volume = {21},
number = {10},
pages = {1422-1436},
year = {2013},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2013.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0967066113001147},
author = {Ivan Markovsky},
keywords = {System identification, Model reduction, Behavioral approach, Missing data, Low-rank approximation, Reproducible research},
abstract = {An identification problem with no a priori separation of the variables into inputs and outputs and representation invariant approximation criterion is considered. The model class consists of linear time-invariant systems of bounded complexity and the approximation criterion is the minimum of a weighted 2-norm distance between the given time series and a time series that is consistent with the model. The problem is equivalent to and is solved as a mosaic-Hankel structured low-rank approximation problem. Software implementing the approach is developed and tested on benchmark problems. Additional nonstandard features of the software are specification of exact and missing variables and identification from multiple experiments.}
}
@article{HAWICK20101795,
title = {Automated and parallel code generation for finite-differencing stencils with arbitrary data types},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {1795-1803},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910002024},
author = {K.A. Hawick and D.P. Playne},
keywords = {Partial differential equation, Stencil, Parallel code generation, GPU, CUDA},
abstract = {Finite-Differencing and other regular and direct approaches to solving partial differential equations (PDEs) are methods that fit well on data-parallel computer systems. These problems continue to arise in many application areas of computational science and engineering but still offer some programming challenges as they are not readily incorporated into a general standard software library that could cover all possible PDEs. Achieving high performance on numerical solutions to PDEs generally requires exposure of the field data structures and application of knowledge of how best to map them to the memory and processing architecture of a particular parallel computer system. Stencil methods for solving PDEs are however readily implemented as semi-automatically generated skeletal frameworks. We have implemented semi-automated stencil source code generators for a number of target programming languages including data-parallel languages such as CUDA for graphics processing units (GPUs). We report on some performance evaluations for our generated PDE simulations on GPUs and other platforms. In this article we focus on (diffusive) PDEs with a non-trivial data type requirement such as having vector or complex field variables. We discuss the issues and compromises involved implementing equation solvers with fields comprising arbitrary data types on GPUs and other current compute devices.}
}
@article{OLOFSSON201954,
title = {GPdoemd: A Python package for design of experiments for model discrimination},
journal = {Computers & Chemical Engineering},
volume = {125},
pages = {54-70},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419300468},
author = {Simon Olofsson and Lukas Hebing and Sebastian Niedenführ and Marc Peter Deisenroth and Ruth Misener},
keywords = {Design of experiments, Model discrimination, Jensen–Rényi divergence, Gaussian processes, Open-source software},
abstract = {Model discrimination identifies a mathematical model that usefully explains and predicts a given system’s behaviour. Researchers will often have several models, i.e. hypotheses, about an underlying system mechanism, but insufficient experimental data to discriminate between the models, i.e. discard inaccurate models. Given rival mathematical models and an initial experimental data set, optimal design of experiments suggests maximally informative experimental observations that maximise a design criterion weighted by prediction uncertainty. The model uncertainty requires gradients, which may not be readily available for black-box models. This paper (i) proposes a new design criterion using the Jensen-Rényi divergence, and (ii) develops a novel method replacing black-box models with Gaussian process surrogates. Using the surrogates, we marginalise out the model parameters with approximate inference. Results show these contributions working well for both classical and new test instances. We also (iii) introduce and discuss GPdoemd, the open-source implementation of the Gaussian process surrogate method.}
}
@article{ATHANASOPOULOS2015149,
title = {Extracting REST resource models from procedure-oriented service interfaces},
journal = {Journal of Systems and Software},
volume = {100},
pages = {149-166},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.10.038},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002362},
author = {Michael Athanasopoulos and Kostas Kontogiannis},
keywords = {REST, Software reengineering, Service oriented architectures},
abstract = {During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-* stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service descriptions are analyzed, using natural language processing techniques and graph transformations, in order to yield a collection of hierarchically organized elements forming REST resources that semantically correspond to the functionality offered by the service. The proposed approach has been applied as a proof of concept with positive results, for the extraction of resource models from a sizable number of procedure-oriented Web Service interfaces that have been obtained from an open service directory.}
}
@article{KEPNER2003360,
title = {A multi-threaded fast convolver for dynamically parallel image filtering},
journal = {Journal of Parallel and Distributed Computing},
volume = {63},
number = {3},
pages = {360-372},
year = {2003},
issn = {0743-7315},
doi = {https://doi.org/10.1016/S0743-7315(02)00054-0},
url = {https://www.sciencedirect.com/science/article/pii/S0743731502000540},
author = {Jeremy Kepner},
keywords = {Image processing, Parallel algorithms, Multi-threaded, Open standards, High level languages},
abstract = {2D convolution is a staple of digital image processing. The advent of large format imagers makes it possible to literally “pave” with silicon the focal plane of an optical sensor, which results in very large images that can require a significant amount computation to process. Filtering of large images via 2D convolutions is often complicated by a variety of effects (e.g., non-uniformities found in wide field of view instruments) which must be compensated for in the filtering process by changing the filter across the image. This paper describes a fast (FFT based) method for convolving images with slowly varying filters. A parallel version of the method is implemented using a multi-threaded approach, which allows more efficient load balancing and a simpler software architecture. The method has been implemented within a high level interpreted language (IDL), while also exploiting open standards vector libraries (VSIPL) and open standards parallel directives (OpenMP). The parallel approach and software architecture are generally applicable to a variety of algorithms and has the advantage of enabling users to obtain the convenience of an easy operating environment while also delivering high performance using a fully portable code.}
}
@article{RAWALD2017101,
title = {PyRQA—Conducting recurrence quantification analysis on very long time series efficiently},
journal = {Computers & Geosciences},
volume = {104},
pages = {101-108},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2016.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416307439},
author = {Tobias Rawald and Mike Sips and Norbert Marwan},
keywords = {Time series analysis, Recurrence analysis, RQA, Software, Distributed processing, Parallel algorithm},
abstract = {PyRQA is a software package that efficiently conducts recurrence quantification analysis (RQA) on time series consisting of more than one million data points. RQA is a method from non-linear time series analysis that quantifies the recurrent behaviour of systems. Existing implementations to RQA are not capable of analysing such very long time series at all or require large amounts of time to calculate the quantitative measures. PyRQA overcomes their limitations by conducting the RQA computations in a highly parallel manner. Building on the OpenCL framework, PyRQA leverages the computing capabilities of a variety of parallel hardware architectures, such as GPUs. The underlying computing approach partitions the RQA computations and enables to employ multiple compute devices at the same time. The goal of this publication is to demonstrate the features and the runtime efficiency of PyRQA. For this purpose we employ a real-world example, comparing the dynamics of two climatological time series, and a synthetic example, reducing the runtime regarding the analysis of a series consisting of over one million data points from almost eight hours using state-of-the-art RQA software to roughly 69s using PyRQA.}
}
@article{KADDOURI2006377,
title = {NLSOFT: An interactive graphical software for designing nonlinear controllers},
journal = {Mathematics and Computers in Simulation},
volume = {71},
number = {4},
pages = {377-384},
year = {2006},
note = {Modeling and Simulation of Electric Machines, Converters and Systems},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2006.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0378475406000383},
author = {A. Kaddouri and S. Blais and M. Ghribi and O. Akhrif},
keywords = {Feedback linearization technique, Nonlinear control, Digital signal processors (DSP), Graphical user interface (GUI)},
abstract = {This paper describes new software called NLSoft, developed for the design of nonlinear controllers based on the well-known feedback linearization technique. NLSoft is a software package containing several symbolic manipulation modules, which includes differential geometric tools for the design and simulation of control systems. NLSoft presents a user-friendly graphical user interface (GUI) as well as a new and powerful module permitting the calculation time of linearizing control laws considering several digital signal processors (DSPs) characteristics. These facilitate the real-time implementation of the control system. NLSoft is validated considering a six order dynamic nonlinear system.}
}
@article{KETCHEL2007131,
title = {Computer-aided manufacturing of spherical mechanisms},
journal = {Mechanism and Machine Theory},
volume = {42},
number = {2},
pages = {131-146},
year = {2007},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2006.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X06001881},
author = {John S. Ketchel and Pierre M. Larochelle},
abstract = {In this paper a computer-aided manufacturing methodology for spherical four-bar mechanisms is presented. First, the kinematics of spherical mechanisms are reviewed as they pertain to their manufacture. This is followed by a brief review of some of the current computer-aided design (CAD) software for spherical four-bar mechanisms, e.g., Sphinx, SphinxPC, Isis, and Osiris. These software packages provide the three-dimensional visualization and computational capabilities necessary to design spherical four-bar mechanisms. However, to date no tools exist to aid in the manufacture of spherical mechanisms. Finally, we present our implementation of this CAM methodology for spherical four-bar mechanisms called SphinxCAM. SphinxCAM, when used with the CAD tools mentioned above, facilitates the design, visualization, prototyping and manufacture of spherical four-bar mechanisms.}
}
@article{DRAGONI2010267,
title = {Designing bonded joints by means of the JointCalc software},
journal = {International Journal of Adhesion and Adhesives},
volume = {30},
number = {5},
pages = {267-280},
year = {2010},
note = {Special Issue on Joint Design},
issn = {0143-7496},
doi = {https://doi.org/10.1016/j.ijadhadh.2009.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0143749610000254},
author = {E. Dragoni and L. Goglio and F. Kleiner},
keywords = {Epoxy, Stress analysis, Joint design, Software},
abstract = {This paper describes the theoretical framework, the experimental background and the software implementation of the computer package JointCalc for the strength analysis and design of adhesively bonded joints. Developed by Henkel AG in collaboration with three Italian Universities, JointCalc was designed to be accessible to non-experts, intuitive for occasional users and also general enough to include most of the joint configurations encountered in practice. For the calculation of adhesive stresses, JointCalc implements the analytical elastic solutions available in the literature for the fundamental joint geometries (single and double-lap joints, single and double-strap joints, peel joints and cylindrical joints). A key aspect of JointCalc is the experimental failure criterion adopted, represented by an admissible region in the peel-shear stress plane. The implementation required the creation of an experimental database, specifically built for the set of 14 adhesives (mostly epoxies) considered. A distinguished asset of JointCalc is its intuitive graphical interface that enables the user to choose the desired joint configuration, input the data and examine the results in a straightforward way. Since its appearance in 2003, JointCalc has been applied to the design of bonded assemblies covering a wide spectrum of industrial applications. Three of those applications (a seat back mounting, a shear punch solenoid and a screening device) are presented at the end of the paper as case studies. The disclosed data show that, provided that thoughtful engineering judgement is applied to idealize the real joints, JointCalc strength predictions closely match the experimental findings.}
}
@article{ORLOV201653,
title = {Application of Numerical time Integration Schemes to Continuously Variable Transmission Dynamics Analysis},
journal = {Procedia Computer Science},
volume = {101},
pages = {53-57},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916326758},
author = {Stepan Orlov and Aleksey Kuzin and Natalia Melnikova and Nikolay Shabrov},
keywords = {large-scale models, time integration schemes, nonlinear problems, stiff systems},
abstract = {Numerical analysis of large-scale and multidisciplinary problems on high-performance computer (HPC) systems is one of the main computational challenges of the 21st century. Modelling the dynamics of a continuously variable transmission (CVT) is one of such problems. CVT has been used in the automotive industry for last decades. The complexity of physical interactions in CVT does not allow engineers to use any of the general purpose commercial software packages for CVT design. Our project aims to develop a mathematical model, computational algorithms and problem oriented in- house software for simulating CVT dynamics. One of the important steps in this project is the study of explicit and implicit time integration schemes for the stiff system of ordinary differential equations, as well as implementations of these schemes on HPC systems.}
}
@article{BURGER201915,
title = {Design, implementation and simulation of an MPC algorithm for switched nonlinear systems under combinatorial constraints},
journal = {Journal of Process Control},
volume = {81},
pages = {15-30},
year = {2019},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0959152419303592},
author = {Adrian Bürger and Clemens Zeile and Angelika Altmann-Dieses and Sebastian Sager and Moritz Diehl},
keywords = {Model predictive control, Switched dynamic systems, Mixed-integer nonlinear programming, Optimal control, Approximation methods and heuristics},
abstract = {Within this work, we present a warm-started algorithm for Model Predictive Control (MPC) of switched nonlinear systems under combinatorial constraints based on Combinatorial Integral Approximation (CIA). To facilitate high-speed solutions, we introduce a preprocessing step for complexity reduction of CIA problems, and include this approach within a new toolbox for solution of CIA problems with special focus on MPC. The proposed algorithm is implemented and utilized within an MPC simulation study for a solar thermal climate system with nonlinear system behavior and uncertain operation conditions. The results are analyzed in terms of solution quality, constraint satisfaction and runtime of the solution steps, showing the applicability of the proposed algorithm and implementations.}
}
@article{CORRAL201511,
title = {A study of energy-aware implementation techniques: Redistribution of computational jobs in mobile apps},
journal = {Sustainable Computing: Informatics and Systems},
volume = {7},
pages = {11-23},
year = {2015},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2014.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2210537914000912},
author = {Luis Corral and Anton B. Georgiev and Alberto Sillitti and Giancarlo Succi},
keywords = {Android, Energy, Green, Mobile, Offloading, Reallocation},
abstract = {As devices like smartphones and tablets have been adopted by millions of users, the offer of mobile software products and services has as well evolved significantly. Current mobile software products require hardware resources, computer job and network connectivity that is reflected in increasing power needs. This power demand represents a major challenge to the strong autonomy requirement of mobile devices, powered by batteries. As a consequence, one of the most important qualities of mobile software development is the ability to produce applications that consume energy resources wisely. To accomplish this goal, different approaches have been proposed, including energy-aware software design and implementation techniques. The energy aware software design techniques considered in this work are two: Method Reallocation, which refers to the placement of pieces of code in different execution scopes within a single target (e.g., kernel space, application space, shared library space), and Method Offloading, which refers to the placement of pieces of code in external resources in different scopes (for instance a remote server). Both techniques aim to economize resources like processing power, and memory usage, but always upon the expenses carried by interfacing, communication and network overhead. Our goal is to investigate how each one can contribute to reduce the overall energy consumption of a mobile software application. As an experiment, we utilized a mobile application that runs software benchmarks coded in Java and C. We exercised the benchmarks in different execution scopes within the handheld target and a remote server counterpart, measuring the amount of energy required to complete each job. After determining the energy consumed by each routine for each execution scope, we identified in what cases it is convenient to reallocate the processing job, and when it is advisable to offload it to an external execution environment.}
}
@article{GADO2023116657,
title = {Utilization of triply periodic minimal surfaces for performance enhancement of adsorption cooling systems: Computational fluid dynamics analysis},
journal = {Energy Conversion and Management},
volume = {277},
pages = {116657},
year = {2023},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2023.116657},
url = {https://www.sciencedirect.com/science/article/pii/S0196890423000031},
author = {Mohamed G. Gado and Shinichi Ookawara and Hamdy Hassan},
keywords = {Adsorption cooling, Heat transfer enhancement, Specific cooling power (SCP), Triply periodic minimal surface (TPMS), Performance enhancement},
abstract = {In this study, triply periodic minimal surface (TPMS) structures have been implemented in adsorber/desorber to improve the performance of adsorption cooling systems (ACSs). The use of metal TPMS-based structures considerably increased the effective thermal conductivity of the porous media/metal composite due to its large surface area to porous media volume. A fully-three-dimensional CFD model was constructed using Ansys Fluent and validated based on relevant previous studies. Five distinct TPMS-based structures, including Gyroid, Diamond, I-graph-wrapped package, Primitive, and Lidinoid were investigated and compared with conventional Fins-based structures under different porosities, constant adsorption times, and optimal adsorption times. The specific surface area and geometric tortuosity of the adopted structures were evaluated using GeoDict software. The results demonstrated that Lidinoid attained the cyclic specific cooling power (SCP) enhancement of 12.4 % compared to Fins-based structures under the constant adsorption time of 500 s and the consistent porosity of 0.5. Meanwhile, when operated at the optimal adsorption time for each structure, Lidinoid improved the cyclic SCP by 17.5 %, while Primitive showed 6.9 % decrease in cyclic SCP compared to Fins-based structure. The cyclic SCP of TPMS structures was better than that of Fins-based structure at medium and higher porosities of 0.5 and 0.8, respectively. However, at lower porosity of 0.2, Fins-based structures outperformed TPMS structures in cyclic SCP. In terms of daily performance, Gyroid, Diamond, and Primitive structures showed faster kinetics and more potential for ACSs applications. The results indicated that Primitive structure achieved 51.6 % increase in cooling capacity per day compared to conventional Fins-based structure. In conclusion, this study has well demonstrated that the exploitation of TPMS opens new avenues for next-generation adsorption cooling applications with superior performance.}
}
@article{RADZIKOWSKI1987595,
title = {Computer-Aided Design of Information Systems for Large-Scale Project Management},
journal = {IFAC Proceedings Volumes},
volume = {20},
number = {9},
pages = {595-600},
year = {1987},
note = {4th IFAC/IFORS Symposium on Large Scale Systems: Theory and Applications 1986, Zurich, Switzerland, 26-29 August 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)55772-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017557729},
author = {W. Radzikowski},
keywords = {Computer-aided system design, decision support systems, management systems, PERT, project management},
abstract = {Implementation of the software package PSL/PSA (Problem Statement Language/Problem Statement Analyzer) described in this paper deals with some problems of analysis and design of information systems for large-scale project management: (1) Configuration of an information system for large-scale project management. (2) Implementation process of the PSL/PSA software package. (3) Presentation of some Problem Statement Analyzer reports. (4) Some results of the information system implementation based on IBM 370/148 computer, under the operating system VM/CMS. (5) Benefits achieved with the PSL/PSA software package implementation.}
}
@article{GREMME2005965,
title = {Engineering a software tool for gene structure prediction in higher organisms},
journal = {Information and Software Technology},
volume = {47},
number = {15},
pages = {965-978},
year = {2005},
note = {Most Cited Journal Articles in Software Engineering - 1999},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2005.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584905001345},
author = {Gordon Gremme and Volker Brendel and Michael E. Sparks and Stefan Kurtz},
keywords = {Computational biology, Genome annotation, Similarity-based gene structure prediction, Intron cutout technique, Incremental updates},
abstract = {The research area now commonly called ‘bioinformatics’ has brought together biologists, computer scientists, statisticians, and scientists of many other fields of expertise to work on computational solutions to biological problems. A large number of algorithms and software packages are freely available for many specific tasks, such as sequence alignment, molecular phylogeny reconstruction, or protein structure determination. Rapidly changing needs and demands on data handling capacity challenge the application providers to consistently keep pace. In practice, this has led to many incremental advances and re-writing of code that present the user community with confusing options and a large overhead from non-standardized implementations that need to be integrated into existing work flows. This situation gives much scope for contributions by software engineers. In this article, we describe an example of engineering a software tool for a specific bioinformatics task known as spliced alignment. The problem was motivated by disabling limitations in an original, ad hoc, and yet widely popular implementation by one of the authors. The present collaboration has led to a robust, highly versatile, and extensible tool (named GenomeThreader) that not only overcomes the limitations of the earlier implementation but greatly improves space and time requirements.}
}
@article{KWON19996047,
title = {Network-based software-in-the-loop simulation for real-time control system},
journal = {IFAC Proceedings Volumes},
volume = {32},
number = {2},
pages = {6047-6052},
year = {1999},
note = {14th IFAC World Congress 1999, Beijing, Chia, 5-9 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)57032-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017570329},
author = {Wook Hyun Kwon and Seong-Gyu Choi and Ki Baek Kim},
keywords = {Software-In-the-Loop (STL) Simulation, Computer-Aided Control System Design, Ethernet, Time-Scaling},
abstract = {This paper suggests a real-time implementation method for software-in-the-loop (SIL) simulation for control systems. The implementation is composed of a PC for the controllers, a PC for the plant, a general-purpose computer-aided control system design (CACSD)package, and an Ethernet network. The Ethernet network is investigated in terms of control issues such as sampling interval, network-induced time delay, use with multiple I/O points and data synchronization. A performance evaluation of software-in-the-loop simulation is carried out subject to a computation delay and a sampling interval. To reduce the effects of the time delay, particularly for fast plants, a time-scaling method is introduced for slow and fast motions. It is demonstrated that this real-time software-in-the-loop simulation will be very useful.}
}
@article{STRUTHERS1997S83,
title = {Use of intelligent software agents in an industrial pressure relief and blowdown design tool},
journal = {Computers & Chemical Engineering},
volume = {21},
pages = {S83-S88},
year = {1997},
note = {Supplement to Computers and Chemical Engineering},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(97)87483-2},
url = {https://www.sciencedirect.com/science/article/pii/S0098135497874832},
author = {A. Struthers},
abstract = {Modelling effort in the process industry has traditionally focussed on the development, implementation and solution of the actual flowsheet or unit operation based models, i.e. the equations to use and package to solve them. This paper describes research that concentrates on how to model the higher level design processes and tasks that use these models as part of their implementation. The operation of a tool to aid one such process, Pressure Relief and Blowdown Studies, is illustrated. The generic design and use of novel Agent Based software technology to}
}
@article{VUKOBRATOVIC1987345,
title = {A General Organization of Software System for Control Synthesis of Robots},
journal = {IFAC Proceedings Volumes},
volume = {20},
number = {5, Part 4},
pages = {345-350},
year = {1987},
note = {10th Triennial IFAC Congress on Automatic Control - 1987 Volume IV, Munich, Germany, 27-31 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)55339-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017553392},
author = {M.K. Vukobratović and D.M. Stokić},
keywords = {Computer-aided design, control system synthesis, large-scale systems, robots, servomechanisms, stability},
abstract = {The paper presents the organization of the software package for computer-aided control synthesis of manipulation robots. We emphasize the role of this package for educational purposes. The software package has been succesfully applied for synthesis of control for various manipulation types and structures. The software package is intended for synthesis of lower control levels: tactical and executive control levels. Two modes of this software system are implemented: the first mode is intended for interactive control synthesis by the robots designers, and the second mode (the so-called educational mode) is intended for educational purposes. In this paper the second mode is described and the benefits that can be expected in education by application of this package are discussed}
}
@article{CHANGJIANG2017498,
title = {A Design and Implementation of Mobile Video Surveillance Terminal Base on ARM},
journal = {Procedia Computer Science},
volume = {107},
pages = {498-502},
year = {2017},
note = {Advances in Information and Communication Technology: Proceedings of 7th International Congress of Information and Communication Technology (ICICT2017)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.03.097},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917303721},
author = {Jin Changjiang and Wang Jin and Li Mingfu and Liao Zhichuan},
keywords = {Linux operating system, Mobile video, surveillance terminal, Real-time},
abstract = {A composition of unfixed video surveillance terminal in view of the hardware platform of S3C2440, and the core of the Linux operating system in the article. Design the system's software and hardware platform solutions, surveillance terminal use RTP protocol to receive H.264 video stream through wireless network. The method of how to deal with the disorder and loss of video packets when the unstable network is proposed. With the help of open source f.f.m. peg library for decoding and displaying the scene of surveillance. The experiments show that surveillance terminal has flexible portable features, for QCIF resolution has better real-time surveillance result.}
}
@article{VINCENTELLI20062,
title = {CHALLENGES AND OPPORTUNITIES FOR SYSTEM THEORY IN EMBEDDED CONTROLLER DESIGN},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {5},
pages = {2-3},
year = {2006},
note = {2nd IFAC Conference on Analysis and Design of Hybrid Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060607-3-IT-3902.00004},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015328470},
author = {Alberto Sangiovanni Vincentelli},
keywords = {Embedded Systems, Systems Design, Systems Methodology, Control Applications, Distributed Control},
abstract = {Embedded controllers are essential in today electronic systems to assure that the behaviour of complex systems as cars, airplanes, trains, building security management systems, is compliant to strict safety constraints. I will review the evolution of embedded systems and the challenges that must be faced in their design. I will also present methodologies aimed at simplifying and speeding the design process. The role of hybrid systems in the development of embedded controllers will be outlined. Future applications such as wireless sensor networks in an industrial plant will also be presented. The ability of integrating an exponentially increasing number of transistors within a chip, the ever-expanding use of electronic embedded systems to control increasingly many aspects of the “real world”, and the trend to interconnect more and more such systems (often from different manufacturers) into a global network, are creating a nightmarish scenario for embedded system designers. Complexity and scope are exploding into the three inter-related but independently growing directions mentioned above, while teams are even shrinking in size to further reduce costs. In this scenario the three challenges that are taking center stage are: Heterogeneity and Complexity of the Hardware Platform. The trends mentioned above result in exponential complexity growth of the features that can be implemented in hardware. The integration capabilities make it possible to build real complex system on a chip including analog and RF components. The decision of what to place on a chip is no longer dictated by the amount of circuitry that can be placed on the chip but by reliability, yield and ultimately cost (it is well known that analog and RF components force to use more conservative manufacturing lines with more processing steps than pure digital ICs). Even if manufacturing concerns suggest to implement hardware in separate chips, the resulting package may still be very small given the advances in packaging technology yielding the concept of System-in-Package (SiP). Pure digital chips are also featuring an increasing number of components. Design time, cost and manufacturing unpredictability for deep submicron technology make the use of custom hardware implementations appealing only for products that are addressing a very large market and for experienced and financially rich companies. Even for these companies, the present design methodologies are not yielding the necessary productivity forcing them to increase beyond reason the size of design and verification teams. These IC companies (for example Intel, AMD and TI) are looking increasingly to system design methods to allow them to assemble large chips out of pre-designed components and to reduce validation costs. In this context, the adoption of design models above RTL and of communication mechanism among components with guaranteed properties and standard interfaces is only a matter of time. Embedded Software Complexity. Given the cost and risks associated to developing hardware solutions, an increasing number of companies is selecting hardware platforms that can be customized by reconfiguration and/or by software programmability. In particular, software is taking the lion's share of the implementation budgets and cost. In cell phones, more than 1 Million lines of code is standard today, while in automobiles the estimated number of lines by 2010 is 100 Millions. The number of lines of source code of embedded software required for defense avionics systems is also growing exponentially. However, as this happens, the complexity explosion of the software component causes serious concerns for the final quality of the products and the productivity of the engineering forces. In transportation, the productivity of embedded software writers using the traditional methods of software development ranges in the few tens of lines per day. The reasons for such a low productivity are in the time needed for verification of the system and long redesign cycles that come from the need of developing full system prototypes for the lack of appropriate virtual engineering methods and tools for embedded software. Embedded software is substantially different from traditional software for commercial and corporate applications: by virtue of being embedded in a surrounding system, the software must be able to continuously react to stimuli in the desired way, i.e., within bounds on timing, power consumed and cost. Verifying the correctness of the system requires that the model of the software be transformed to include information that involve physical quantities to retain only what is relevant to the task at hand. In traditional software systems, the abstraction process leaves out all the physical aspects of the systems as only the functional aspects of the code matter. Integration Complexity. A standard technique to deal with complexity is decomposing “top-down” the system into subsystems. This approach, which has been customarily adopted by the semiconductor industry for years, has limitation as a designer or a group of designers has to fully comprehend the entire system and to partition appropriately its various parts, a difficult task given the enormous complexity of today's systems. Hence, the future is one of developing systems by composing pieces that all or in part have already been pre-designed or designed independently by other design groups or even companies. This has been done routinely in vertical design chains for example in the transportation vertical, albeit in a heuristic and ad hoc way. The resulting lack of an overall understanding of the interplay of the sub-systems and of the difficulties encountered in integrating very complex parts causes system integration to become a nightmare in the system industry. For example, Jurgen Hubbert, then in charge of the Mercedes-Benz passenger car division, publicly stated in 2003: “The industry is fighting to solve problems that are coming from electronics and companies that introduce new technologies face additional risks. We have experienced blackouts on our cockpit management and navigation command system and there have been problems with telephone connections and seat heating.“ I believe that in today's environment this state is the rule for the leading system OEMs let them operate in the transportation domain, in multimedia systems, in communication, rather than the exception. The source of these problems is clearly the increased complexity but also the difficulty of the OEMs in managing the integration and maintenance process with subsystems that come from different suppliers who use different design methods, different software architecture, different hardware platforms, different (and often proprietary) Real-Time Operating Systems. Therefore, there is a need for standards in the software and hardware domains that will allow plug-and-play of subsystems and their implementation while the competitive advantage of an OEM will increasingly reside on novel and compelling functionalities. I will present a methodology to cope with some of these problems and that can use hybrid system modeling. I will review how this methodology can be applied to the design of embedded controllers for the automotive industry. Finally I will present the application of the methodology and of hybrid systems to the design of wireless sensor networks in an industrial environment.}
}
@article{DEUBLER2001105,
title = {Employing multiple views to separate large-scale software systems},
journal = {Journal of Systems and Software},
volume = {56},
number = {2},
pages = {105-113},
year = {2001},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(00)00091-1},
url = {https://www.sciencedirect.com/science/article/pii/S0164121200000911},
author = {Hanns-Helmuth Deubler},
keywords = {Large software systems, Object-oriented technique, Information hiding, Clusters, Subjects, Views, Metaclasses},
abstract = {In order to achieve a well-separated system structure including hiding of information on a need-to-know basis it is highly desirable to provide multiple views of an interface for different subjects. In this context subjects may be structural parts of the system such as classes, or, preferably, architectural entities like clusters, subsystems or application areas. Establishing a view of an interface means that a particular subject may only use a selected subset of it, if at all. In this article several approaches cited in the literature are discussed in the light of the demands made by large-scale commercially installed software systems. It is shown that these approaches in large systems either lead to a considerable performance degradation or to a software structure which is somewhat difficult to survey and which does not possess satisfactory extensibility. The proposed technique neither reduces the performance of the system nor encumbers the existing software structure nor does it unnecessarily increase development and maintenance costs. It lies on the organizational level and is applied during the production process of a subject entity. The approach is flexible in that it does not depend on the actual notions of subjects and suppliers of interfaces in a software system. Metaclasses are introduced for establishing the respective views of a server class for different subjects. They are deduced from the server classes by the developers by tailoring the set of usable public operations and are collected in include libraries or description files which are specific to every subject. By analogy, different views of a global function can be introduced. The proposed technique permits to use several implementation languages and programming paradigms within a system in parallel, and proved to be well applicable in a large commercially established software system helping to reduce the coupling dependencies between different components.}
}
@article{MAREK2015100,
title = {Introduction to dynamic program analysis with DiSL},
journal = {Science of Computer Programming},
volume = {98},
pages = {100-115},
year = {2015},
note = {Fifth issue of Experimental Software and Toolkits (EST): A special issue on Academics Modelling with Eclipse (ACME2012)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642314000070},
author = {Lukáš Marek and Yudi Zheng and Danilo Ansaloni and Lubomír Bulej and Aibek Sarimbekov and Walter Binder and Petr Tůma},
keywords = {Dynamic program analysis, Bytecode instrumentation, Aspect-oriented programming, Domain-specific languages, Java Virtual Machine},
abstract = {Dynamic program analysis (DPA) tools assist in many software engineering and development tasks, such as profiling, program comprehension, and performance model construction and calibration. On the Java platform, many DPA tools are implemented either using aspect-oriented programming (AOP), or rely on bytecode instrumentation to modify the base program code. The pointcut/advice model found in AOP enables rapid tool development, but does not allow expressing certain instrumentations due to limitations of mainstream AOP languages—developers thus use bytecode manipulation to gain more expressiveness and performance. However, while the existing bytecode manipulation libraries handle some low-level details, they still make tool development tedious and error-prone. Targeting this issue, we provide the first complete presentation of DiSL, an open-source instrumentation framework that reconciles the conciseness of the AOP pointcut/advice model and the expressiveness and performance achievable with bytecode manipulation libraries. Specifically, we extend our previous work to provide an overview of the DiSL architecture, advanced features, and the programming model. We also include case studies illustrating successful deployment of DiSL-based DPA tools.}
}
@article{HINTZ1992313,
title = {Microcontroller software design using Petri Tables},
journal = {Journal of Microcomputer Applications},
volume = {15},
number = {4},
pages = {313-325},
year = {1992},
issn = {0745-7138},
doi = {https://doi.org/10.1016/0745-7138(92)90012-T},
url = {https://www.sciencedirect.com/science/article/pii/074571389290012T},
author = {Kenneth J. Hintz},
abstract = {A new methodology for high-level, microcontroller software design is presented utilizing the concept of a Colored Petri Net as implemented in a Petri Table. This approach is implementation independent and more general than earlier finite state machine approaches in that interrupts are handled more naturally as token-originating places rather than exceptions to the normal program flow. Commercial software packages are available for simulating Petri Nets allowing for the testing and evaluating of the software design before it is implemented. The tabular form of the Petri Table also makes it easier to document the software using normal word-processing packages, and there is a direct transformation possible from transitions in the Petri Table to modules of code. Since the high-level design is already in text-file form, it can easily be incorporated in the source code and selectively printed along with it using conditional compiles yielding printouts which contain only the source code, only the comments, or the two merged together.}
}
@article{JASIEWICZ20111525,
title = {A new GRASS GIS fuzzy inference system for massive data analysis},
journal = {Computers & Geosciences},
volume = {37},
number = {9},
pages = {1525-1531},
year = {2011},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2010.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0098300410003171},
author = {JarosŁaw Jasiewicz},
keywords = {Fuzzy logic, GIS, Data analysis, Inference system, GRASS},
abstract = {GIS systems are frequently coupled with fuzzy logic systems implemented in statistical packages. For large GIS data sets including millions or tens of millions of cells, such an approach is relatively time-consuming. For very large data sets there is also an input/output bottleneck between the GIS and external software. The aim of this paper is to present low-level implementation of Mamdani’s fuzzy inference system designed to work with massive GIS data sets, using the GRASS GIS raster data processing engine.}
}
@article{YA2021113766,
title = {An open-source ABAQUS implementation of the scaled boundary finite element method to study interfacial problems using polyhedral meshes},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {381},
pages = {113766},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113766},
url = {https://www.sciencedirect.com/science/article/pii/S004578252100102X},
author = {Shukai Ya and Sascha Eisenträger and Chongmin Song and Jianbo Li},
keywords = {Scaled boundary finite element method, ABAQUS UEL, Polyhedral element, Interfacial problems},
abstract = {The scaled boundary finite element method (SBFEM) is capable of generating polyhedral elements with an arbitrary number of surfaces. This salient feature significantly alleviates the meshing burden being a bottleneck in the analysis pipeline in the standard finite element method (FEM). In this paper, we implement polyhedral elements based on the SBFEM into the commercial finite element software ABAQUS. To this end, user elements are provided through the user subroutine UEL. Detailed explanations regarding the data structures and implementational aspects of the procedures are given. The focus of the current implementation is on interfacial problems and therefore, element-based surfaces are created on polyhedral user elements to establish interactions. This is achieved by an overlay of standard finite elements with negligible stiffness, provided in the ABAQUS element library, with polyhedral user elements. By means of several numerical examples, the advantages of polyhedral elements regarding the treatment of non-matching interfaces and automatic mesh generation are clearly demonstrated. Thus, the performance of ABAQUS for problems involving interfaces is augmented based on the availability of polyhedral meshes. Due to the implementation of polyhedral user elements, ABAQUS can directly handle complex geometries given in the form of digital images or stereolithography (STL) files. In order to facilitate the use of the proposed approach, the code of the UEL is published open-source and can be downloaded from https://github.com/ShukaiYa/SBFEM-UEL.}
}
@article{BARTH201671,
title = {Design of an eye-in-hand sensing and servo control framework for harvesting robotics in dense vegetation},
journal = {Biosystems Engineering},
volume = {146},
pages = {71-84},
year = {2016},
note = {Special Issue: Advances in Robotic Agriculture for Crops},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2015.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1537511015001816},
author = {Ruud Barth and Jochen Hemming and Eldert J. {van Henten}},
keywords = {Framework, Harvest robots, Visual servo control, ROS, SLAM},
abstract = {A modular software framework design that allows flexible implementation of eye-in-hand sensing and motion control for agricultural robotics in dense vegetation is reported. Harvesting robots in cultivars with dense vegetation require multiple viewpoints and on-line trajectory adjustments in order to reduce the amount of false negatives and correct for fruit movement. In contrast to specialised software, the framework proposed aims to support a wide variety of agricultural use cases, hardware and extensions. A set of Robotic Operating System (ROS) nodes was created to ensure modularity and separation of concerns, implementing functionalities for application control, robot motion control, image acquisition, fruit detection, visual servo control and simultaneous localisation and mapping (SLAM) for monocular relative depth estimation and scene reconstruction. Coordination functionality was implemented by the application control node with a finite state machine. In order to provide visual servo control and simultaneous localisation and mapping functionalities, off-the-shelf libraries Visual Servoing Platform library (ViSP) and Large Scale Direct SLAM (LSD-SLAM) were wrapped in ROS nodes. The capabilities of the framework are demonstrated by an example implementation for use with a sweet-pepper crop, combined with hardware consisting of a Baxter robot and a colour camera placed on its end-effector. Qualitative tests were performed under laboratory conditions using an artificial dense vegetation sweet-pepper crop. Results indicated the framework can be implemented for sensing and robot motion control in sweet-pepper using visual information from the end-effector. Future research to apply the framework to other use-cases and validate the performance of its components in servo applications under real greenhouse conditions is suggested.}
}
@article{IBRAHIMBEGOVIC20148,
title = {Code-coupling strategy for efficient development of computer software in multiscale and multiphysics nonlinear evolution problems in computational mechanics},
journal = {Advances in Engineering Software},
volume = {72},
pages = {8-17},
year = {2014},
note = {Special Issue dedicated to Professor Zdeněk Bittnar on the occasion of his Seventieth Birthday: Part 2},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813001129},
author = {Adnan Ibrahimbegovic and Rainer Niekamp and Christophe Kassiotis and Damijan Markovic and Hermann G. Matthies},
keywords = {Software development, Code coupling, Multiscale, Multiphysics, Nonlinear evolution problem, Monolithic code at linkage time},
abstract = {In this work we seek to provide an efficient approach to development of software computational platform for the currently very active research domain of multiphysics and multiscale analysis in fully nonlinear setting. The typical problem to be solved is nonlinear evolution problem, with different scales in space and time. We show here that a successful solution to such a problem requires gathering the sound theoretical formulation, the most appropriate discrete approximation and the efficient numerical implementation. We show in particular that the most efficient numerical implementation is obtained by reusing the existing codes, in order to accelerate the code development and validation. The key element that makes such an approach possible is the Component Template Library (CTL), presented in this work. We show that the CTL allows to seamlessly merge the existing software products into a single code at compilation time, regardless of their ‘heterogeneities’ in terms of programming language or redundancy in use of local variables. A couple of illustrative problems of fluid–structure interaction and multiscale nonlinear analysis are presented in order to confirm the advantage of the proposed approach.}
}
@article{MUZAN20121329,
title = {Implementation of Industrial Robot for Painting Applications},
journal = {Procedia Engineering},
volume = {41},
pages = {1329-1335},
year = {2012},
note = {International Symposium on Robotics and Intelligent Sensors 2012 (IRIS 2012)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2012.07.318},
url = {https://www.sciencedirect.com/science/article/pii/S187770581202718X},
author = {Ijeoma W. Muzan and Tarig Faisal and H.M.A.A. Al-Assadi and Mahmud Iwan},
keywords = {ABB Industrial Robot, Painting Robot, IRC5 Controller, Flexpendant, Robotstudio, SolidWorks},
abstract = {Robot for painting is one of the earliest applications for industrial robot, however, the precision and finishing for the painting is an important issue for any painting job. Accordingly, the aim of this project is utilize an industrial robot (ABB robot model IRB1410) for painting applications. The robot was programmed to paint alphabets using its Flexpendant. The FlexPendant was used to manually teach the robot how to follow the paths for specific targets of letters. The robot End Effector (painting tool) was chosen and mounted on the robot to perform an effective painting task. It was programmed based on its functionality. Finally suitable painting environment was designing. Two software packages were used in this project. The Computer Aided Design (CAD) of the system work-objects and end effector was programmed based on Solidworks software. Robotstudio Software used to program the paths and target of the alphabets to be painted by the IRB1400 Robot which generate a RAPID GUI code used for robot interfacing. The final results demonstrate that implementation such system helps to boost the quality of painting, reduce paint consumption and improve safety.}
}
@article{RAVN19971541,
title = {Modelling and Calibration with Mechatronic Blockset for Simulink},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {11},
pages = {1541-1546},
year = {1997},
note = {IFAC Symposium on System Identification (SYSID'97), Kitakyushu, Fukuoka, Japan, 8-11 July 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)43062-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701743062X},
author = {Ole Ravn and Maciej Szymkat},
keywords = {Modelling, Mechatronics, CACSD, Simulation},
abstract = {The paper describes the design considerations for a software tool for modelling and simulation of mechatronic systems. The tool is based on a concept enabling the designer to pick component models that match the physical components of the system to be modelled from a block library. Another important feature of the tool is the ability to change the complexity of the simulation model in a simple, powerful and well structured way. This feature makes it simple for the designer to evaluate the influence of including different aspects of the components. The complexity can be changed both on the component level and for the whole model. The library that can be extended by the user contains all the standard components, DC-motors, potentiometers, encoders etc. The library is presently being tested in different projects and the response of these users is being incorporated in the code. The Mechatronic Simulink Library blockset is implemented basing on MATLAB and Simulink and has been used to model several mechatronic systems.}
}
@article{AZVINE1991335,
title = {Implementing the QFT Method in the MATLAB Environment},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {4},
pages = {335-340},
year = {1991},
note = {IFAC Symposium on Computer Aided Design in Control Systems, Swansea, UK, 15-17 July 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54294-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017542949},
author = {B. Azvine and R.J. Wynne},
keywords = {Robustness, Feedback, Plant uncertainty, Computer software, Quantitative design},
abstract = {The development of a computer aided design package based on the Quantitative Feedback Theory (QFT) is discussed. The aim is to provide a tool to deal with the problem of control system design for real systems where performance tolerances are tight and models have significant degrees of uncertainty. MATLAB has been used as the working environment for the QFT package which adds to the extensive collection of compatible programmes and toolboxes already available. The suitability of the modular structure of the QFT approach for computer implementation is shown. To denlonstrate the power of the programme the application to the design of a controller for an aero-engine which has non-linear characteristics is considered. It is shown that the technique is applicable to the design of highly non-linear systems and a solution is presented which offers improvements on the output performance compared with a conventional approach.}
}
@article{FIENEN201514,
title = {A cross-validation package driving Netica with python},
journal = {Environmental Modelling & Software},
volume = {63},
pages = {14-23},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002606},
author = {Michael N. Fienen and Nathaniel G. Plant},
keywords = {Cross-validation, Bayesian networks, Uncertainty, Probability, Python, Netica, Prediction},
abstract = {Bayesian networks (BNs) are powerful tools for probabilistically simulating natural systems and emulating process models. Cross validation is a technique to avoid overfitting resulting from overly complex BNs. Overfitting reduces predictive skill. Cross-validation for BNs is known but rarely implemented due partly to a lack of software tools designed to work with available BN packages. CVNetica is open-source, written in Python, and extends the Netica software package to perform cross-validation and read, rebuild, and learn BNs from data. Insights gained from cross-validation and implications on prediction versus description are illustrated with: a data-driven oceanographic application; and a model-emulation application. These examples show that overfitting occurs when BNs become more complex than allowed by supporting data and overfitting incurs computational costs as well as causing a reduction in prediction skill. CVNetica evaluates overfitting using several complexity metrics (we used level of discretization) and its impact on performance metrics (we used skill).}
}
@article{SINGH20101108,
title = {ICAS-PAT: A software for design, analysis and validation of PAT systems},
journal = {Computers & Chemical Engineering},
volume = {34},
number = {7},
pages = {1108-1136},
year = {2010},
note = {Process Modeling and Control in Drug Development and Manufacturing},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2009.06.021},
url = {https://www.sciencedirect.com/science/article/pii/S0098135409001732},
author = {Ravendra Singh and Krist V. Gernaey and Rafiqul Gani},
keywords = {Process monitoring, Quality control, PAT, Software, Pharmaceutical tablet},
abstract = {In chemicals based product manufacturing, as in pharmaceutical, food and agrochemical industries, efficient and consistent process monitoring and analysis systems (PAT systems) have a very important role. These PAT systems ensure that the chemicals based product is manufactured with the specified end product qualities. In an earlier article, Singh et al. [Singh, R., Gernaey, K. V., Gani, R. (2009). Model-based computer-aided framework for design of process monitoring and analysis systems. Computers & Chemical Engineering, 33, 22–42] proposed the use of a systematic model and data based methodology to design appropriate PAT systems. This methodology has now been implemented into a systematic computer-aided framework to develop a software (ICAS-PAT) for design, validation and analysis of PAT systems. Two supporting tools needed by ICAS-PAT have also been developed: a knowledge base (consisting of process knowledge as well as knowledge on measurement methods and tools) and a generic model library (consisting of process operational models). Through a tablet manufacturing process example, the application of ICAS-PAT is illustrated, highlighting as well, the main features of the software.}
}
@article{ANDRIAMAMONJY2018166,
title = {An automated IFC-based workflow for building energy performance simulation with Modelica},
journal = {Automation in Construction},
volume = {91},
pages = {166-181},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517308282},
author = {Ando Andriamamonjy and Dirk Saelens and Ralf Klein},
keywords = {BIM, IFC4, Energy efficiency, Building performance simulation, Building life cycle},
abstract = {Steadily increasing use of Building Information Modeling (BIM) in all phases of building's lifecycle, together with more attention for openBIM and growing software support for the most recent version of the Industry Foundation Classes (IFC 4) have created a very promising context for an even broader application of Building Energy Performance Simulation (BEPS). At the same time, an urgent need for modeling guidelines and standardization becomes evident. A well-defined BIM-based workflow and a set of tools that fully exploit and extend the possibilities of the openBIM-technology can make the difference when it comes to reliability and cost of BEPS to design, build and operate high-performance buildings. This paper describes the essential elements of this integrated workflow, explains why openBIM comprises much more than just a standardized file-format and what is achieved with the already available technology, namely the Information Delivery Manual (IDM) and a newly developed Model View Definition. This MVD is tailored to the needs of Building Energy Performance Simulation (BEPS) that uses the Modelica language together with a specific library (IDEAS) and can easily be adapted to other libraries. In this project, several tools have been developed to closely integrate BEPS and IFC4. The simulation engine now gets the vast majority of the required input directly from the IFC4-file. For the implementation of the tools, the PYTHON language and the open source library IfcOpenShell are used. A case study is presented, that was used for extensive tests of the proposed approach and the implemented tools. The essential benefits of this new workflow are illustrated, and the feasibility is demonstrated. Opportunities and remaining bottlenecks are identified to encourage further development of BIM software to fully support IFC4 as an information source for BEPS. Besides some improvements of the proprietary class structure and functionality, enabling the export of IFC4 files based on custom MVDs is one required key feature.}
}
@article{KNIGHT199251,
title = {Flowes: An intelligent computational fluid dynamics system},
journal = {Engineering Applications of Artificial Intelligence},
volume = {5},
number = {1},
pages = {51-58},
year = {1992},
issn = {0952-1976},
doi = {https://doi.org/10.1016/0952-1976(92)90097-4},
url = {https://www.sciencedirect.com/science/article/pii/0952197692900974},
author = {B. Knight and M. Petridis},
keywords = {Computational fluid dynamics, artificial intelligence, blackboard system, intelligent front end},
abstract = {In this paper, experience in the design, implementation and usage of an experimental computational fluid dynamics software package incorporating an intelligent knowledge-based component is described. The types and sources of knowledge which can be included in a system are discussed with reference to a simple example, and the advantages of their inclusion in a system are given. A “blackboard” architecture for the system is described, and the importance of software engineering aspects of the total system is pointed out. Examples of the types of rule are given, and their mode of operation. The paper concludes with a summary of the general lessons to be learned from this example development concerning the incorporation of intelligence into engineering software.}
}
@article{SUGANDHI2016783,
title = {Use of EPICS and Python technology for the development of a computational toolkit for high heat flux testing of plasma facing components},
journal = {Fusion Engineering and Design},
volume = {112},
pages = {783-787},
year = {2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2016.04.036},
url = {https://www.sciencedirect.com/science/article/pii/S0920379616303271},
author = {Ritesh Sugandhi and Rajamannar Swamy and Samir Khirwadkar},
keywords = {Critical heat flux, Plasma facing component, Numerical optimization, EPICS, Python},
abstract = {The high heat flux testing and characterization of the divertor and first wall components are a challenging engineering problem of a tokamak. These components are subject to steady state and transient heat load of high magnitude. Therefore, the accurate prediction and control of the cooling parameters is crucial to prevent burnout. The prediction of the cooling parameters is based on the numerical solution of the critical heat flux (CHF) model. In a test facility for high heat flux testing of plasma facing components (PFC), the integration of computations and experimental control is an essential requirement. Experimental physics and industrial control system (EPICS) provides powerful tools for steering controls, data simulation, hardware interfacing and wider usability. Python provides an open source alternative for numerical computations and scripting. We have integrated these two open source technologies to develop a graphical software for a typical high heat flux experiment. The implementation uses EPICS based tools namely IOC (I/O controller) server, control system studio (CSS) and Python based tools namely Numpy, Scipy, Matplotlib and NOSE. EPICS and Python are integrated using PyEpics library. This toolkit is currently under operation at high heat flux test facility at Institute for Plasma Research (IPR) and is also useful for the experimental labs working in the similar research areas. The paper reports the software architectural design, implementation tools and rationale for their selection, test and validation.}
}
@incollection{VASSEUR2010167,
title = {Chapter 13 - uIP — A Lightweight IP Stack},
editor = {Jean-Philippe Vasseur and Adam Dunkels},
booktitle = {Interconnecting Smart Objects with IP},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {167-182},
year = {2010},
isbn = {978-0-12-375165-2},
doi = {https://doi.org/10.1016/B978-0-12-375165-2.00013-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123751652000132},
author = {Jean-Philippe Vasseur and Adam Dunkels},
abstract = {Publisher Summary
To communicate using the IP, a device needs an IP stack, which in turn is a software system that implements the IP protocols enabling IP communication. This chapter focuses on the widely used open source uIP stack that was first released in 2001, becoming the first IP stack for smart objects. The uIP TCP/IP stack is an implementation of the IP stack specifically designed to meet the strict memory requirements of smart objects and other networked embedded systems. Since its first release, the uIP stack has seen a significant industrial adoption, and the software is now used in various systems and products such as oil pipeline monitoring systems, global container tracking systems, car engines, and pico satellites. In 2008, it became the first IP stack for smart objects to be certified under the IPv6 Ready program. It implements the most important protocols in the IP stack such as IP, ICMP, UDP, and TCP and contains both an IPv4 and an IPv6 implementation. Application layer protocols such as HTTP and SNMP are implemented on top of uIP. The code and memory footprint of uIP is very small, compared to IP stacks used for general purpose operating systems, and it requires only a few kilobytes of code to implement a full IPv6 stack. To achieve such a small footprint, the design of uIP is intentionally made simple where the packets are processed in sequence, the buffer management mechanism is kept simple, and the application API is event-driven.}
}
@article{HUANG2021107096,
title = {An optimum design problem in estimating the shape of perforated pins and splitters in a plate-pin-fin heat sink},
journal = {International Journal of Thermal Sciences},
volume = {170},
pages = {107096},
year = {2021},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2021.107096},
url = {https://www.sciencedirect.com/science/article/pii/S1290072921002593},
author = {Cheng-Hung Huang and Yuan-Rei Huang},
keywords = {Optimum design, Plate-Pin-Fin Heat Sink, Perforated Pins, Splitters, Thermal performance factor},
abstract = {The optimal shape of a Plate-Pin-Fin Heat Sink with Perforations and Splitters (PPFHS-PS) is designed in this work using the software package CFD-ACE+ and the Levenberg-Marquardt method (LMM) to determine the maximum thermal performance factor η under a fixed fin volume constraint. In the present work, the implementations of perforations and splitters on a pin fin are examined, and the diameter of the pin, the perforated diameter of the pin and the length of the splitter are considered as the design variables. The hydrothermal performances of a Plate-Pin-Fin Heat Sink (PPFHS), a Plate-Pin-Fin Heat Sink with Perforations (PPFHS–P) and a Plate-Pin-Fin Heat Sink with Splitters (PPFHS–S) are compared. It is found that a great enhancement in the thermal performance factor can be resulted for the PPFHS-PS. The numerical design cases indicated that when considering an incoming velocity of 5.0 m/s, the percentage improvements of η for the PPFHS-P, PPFHS-S and design #2 PPFHS-PS are 4.9%, 5.2% and 10.1% higher than that of the original PPFHS. It indicates that the individual contribution of thermal performance factor improvement of the PPFHS-P and PPFHS-S can be added if the perforations and splitters are utilized simultaneously in the PPFHS-PS design. Finally, experimental verifications are conducted on the manufactured PPFHS-P, PPFHS-S and design #2 PPFHS-PS modules, and the measurement results of the temperature distributions and pressure drops indicate that the experimental data matched quite well with the computational data for those heat sinks.}
}
@article{CAO2021107801,
title = {Neural random subspace},
journal = {Pattern Recognition},
volume = {112},
pages = {107801},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107801},
url = {https://www.sciencedirect.com/science/article/pii/S003132032030604X},
author = {Yun-Hao Cao and Jianxin Wu and Hanchen Wang and Joan Lasenby},
keywords = {Random subspace, Ensemble learning, Deep neural networks},
abstract = {The random subspace method, also known as the pillar of random forests, is good at making precise and robust predictions. However, there is as yet no straightforward way to combine it with deep learning. In this paper, we therefore propose Neural Random Subspace (NRS), a novel deep learning based random subspace method. In contrast to previous forest methods, NRS enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy. Furthermore, as a non-linear component to be encoded into Convolutional Neural Networks (CNNs), NRS learns non-linear feature representations in CNNs more efficiently than contemporary, higher-order pooling methods, producing excellent results with negligible increase in parameters, floating point operations (FLOPs) and real running time. Compared with random subspaces, random forests and gradient boosting decision trees (GBDTs), NRS demonstrates superior performance on 35 machine learning datasets. Moreover, on both 2D image and 3D point cloud recognition tasks, integration of NRS with CNN architectures achieves consistent improvements with only incremental cost.}
}
@incollection{MCCOMB2003281,
title = {Chapter 15 - Getting Started},
editor = {Dave McComb},
booktitle = {Semantics in Business Systems},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {281-287},
year = {2003},
series = {The Savvy Manager;#x0027;s Guides},
isbn = {978-1-55860-917-4},
doi = {https://doi.org/10.1016/B978-155860917-4/50017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781558609174500179},
author = {Dave McComb},
abstract = {Publisher Summary
This chapter reveals that some areas of semantics are changing rapidly. It discusses that in the early days of relational databases or object-oriented programming, it mattered less which technology was picked. The important thing was that one moved in that direction and learned what one needed to learn, individually and corporately, to apply these paradigms to one's systems. The chapter also discusses application development, selecting packaged applications, implementing an application package, systems integration, middleware upgrade, knowledge or content-related initiatives, business to business and e-commerce initiatives, Web site upgrades, platform rationalization, software maintenance, and application architecture.}
}
@article{KONG200381,
title = {A Windows-native 3D plastic injection mold design system},
journal = {Journal of Materials Processing Technology},
volume = {139},
number = {1},
pages = {81-89},
year = {2003},
note = {IMCC2000 Vol. 2 S.I.},
issn = {0924-0136},
doi = {https://doi.org/10.1016/S0924-0136(03)00186-9},
url = {https://www.sciencedirect.com/science/article/pii/S0924013603001869},
author = {L. Kong and J.Y.H. Fuh and K.S. Lee and X.L. Liu and L.S. Ling and Y.F. Zhang and A.Y.C. Nee},
keywords = {Plastic injection mold, Windows, CAD, Parting},
abstract = {3D solid-modeling revolution has reached the design mainstream. While high-end 3D solid-modeling systems have been on engineers’ workstation at large aerospace, consumer products, and automobile companies for years, many smaller companies are now making the switch from workstations to PC. One reason for the shift is that the flexibility and advancement of Windows-native/NT has let software developers create applications that are affordable and easy to use. High-end users are finding that mid-range solid modelers, such as SolidWorks, have met their needs. SolidWorks was chosen as the platform due to the Windows-native design environment, powerful assembly capabilities, ease-of-use, rapid learning curve, and affordable price. A Windows-native 3D plastic injection mold designs system has been implemented on an NT through interfacing Visual C++ codes with the commercial software, SolidWorks 99 and API. The system provides a designer with an interactive computer-aided design environment, which can both speed up the mold design process and facilitate standardization.}
}
@article{LEONI2020100006,
title = {Finite element modelling of the filler wire feeding in the hybrid metal extrusion & bonding (HYB) process},
journal = {Journal of Advanced Joining Processes},
volume = {1},
pages = {100006},
year = {2020},
issn = {2666-3309},
doi = {https://doi.org/10.1016/j.jajp.2020.100006},
url = {https://www.sciencedirect.com/science/article/pii/S2666330920300042},
author = {Francesco Leoni and Øystein Grong and Lise Sandnes and Torgeir Welo and Filippo Berto},
keywords = {Finite element modelling, Continuous extrusion, Aluminium filler wire feeding, Hybrid Metal Extrusion and Bonding},
abstract = {HYB is a new solid-state joining method for metals and alloys that utilises continuous extrusion as a technique to enable aluminium filler metal additions. In the present paper a finite element (FE) model for the filler wire feeding inside the HYB PinPoint extruder has been developed and implemented within the commercial software package Deform 3D ™. Based on a comparison with experimental data for the extruder housing temperature, the drive spindle torque and the wire feed rate, it is concluded that the FE model is sufficiently relevant and comprehensive to be used in future developments of the HYB process. In addition to improved wire feeding control, a model-based approach will make it possible to reduce the engineering time and costs involved in case a modification of the design is needed to optimise the extruder performance in a real joining situation.}
}