@ARTICLE{6875995,
author={Mühlbacher, Thomas and Piringer, Harald and Gratzl, Samuel and Sedlmair, Michael and Streit, Marc},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations},
year={2014},
volume={20},
number={12},
pages={1643-1652},
abstract={An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.},
keywords={Algorithm design and analysis;Context;Data visualization;Software algorithms;Complexity theory;Approximation algorithms;Visualization;Visual analytics infrastructures;integration;interactive algorithms;user involvement;problem subdivision},
doi={10.1109/TVCG.2014.2346578},
ISSN={1941-0506},
month={Dec},}
@INPROCEEDINGS{8809499,
author={Wang, Xinda and Sun, Kun and Batcheller, Archer and Jajodia, Sushil},
booktitle={2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
title={Detecting "0-Day" Vulnerability: An Empirical Study of Secret Security Patch in OSS},
year={2019},
volume={},
number={},
pages={485-492},
abstract={Security patches in open source software (OSS) not only provide security fixes to identified vulnerabilities, but also make the vulnerable code public to the attackers. Therefore, armored attackers may misuse this information to launch N-day attacks on unpatched OSS versions. The best practice for preventing this type of N-day attacks is to keep upgrading the software to the latest version in no time. However, due to the concerns on reputation and easy software development management, software vendors may choose to secretly patch their vulnerabilities in a new version without reporting them to CVE or even providing any explicit description in their change logs. When those secretly patched vulnerabilities are being identified by armored attackers, they can be turned into powerful "0-day" attacks, which can be exploited to compromise not only unpatched version of the same software, but also similar types of OSS (e.g., SSL libraries) that may contain the same vulnerability due to code clone or similar design/implementation logic. Therefore, it is critical to identify secret security patches and downgrade the risk of those "0-day" attacks to at least "n-day" attacks. In this paper, we develop a defense system and implement a toolset to automatically identify secret security patches in open source software. To distinguish security patches from other patches, we first build a security patch database that contains more than 4700 security patches mapping to the records in CVE list. Next, we identify a set of features to help distinguish security patches from non-security ones using machine learning approaches. Finally, we use code clone identification mechanisms to discover similar patches or vulnerabilities in similar types of OSS. The experimental results show our approach can achieve good detection performance. A case study on OpenSSL, LibreSSL, and BoringSSL discovers 12 secret security patches.},
keywords={Security;Databases;Machine learning;Open source software;Training;Computer bugs;security patch;vulnerability detection;open source software},
doi={10.1109/DSN.2019.00056},
ISSN={1530-0889},
month={June},}
@ARTICLE{4545203,
author={Reviriego, P. and Maestro, J. A. and Ruano, O.},
journal={IEEE Transactions on Nuclear Science},
title={Efficient Protection Techniques Against SEUs for Adaptive Filters: An Echo Canceller Case Study},
year={2008},
volume={55},
number={3},
pages={1700-1707},
abstract={In this paper, novel protection techniques against soft errors for adaptive filters are presented. The new techniques are based on the use of system knowledge in terms of both filter structure and functionality, as well as the application tolerance to soft errors. Adaptive filters by nature recover from soft errors on their coefficients, but in existing implementations the recovery time can exceed what is acceptable for many applications. The proposed techniques dramatically reduce the recovery time after a soft error with an acceptable increment on circuit complexity, as they rely on reusing existing logic. To illustrate these techniques, a case study is presented in which their effectiveness is evaluated using a software-based fault injection platform. Also, their complexity is estimated in terms of the number of equivalent gates generated for the synthesized circuit implementation using a commercial ASIC library.},
keywords={Protection;Single event transient;Adaptive filters;Echo cancellers;Application software;Complexity theory;Logic circuits;Circuit faults;Circuit synthesis;Application specific integrated circuits;Adaptive filters;digital filters;radiation hardening;redundancy;single event upsets (SEUs)},
doi={10.1109/TNS.2008.924053},
ISSN={1558-1578},
month={June},}
@INPROCEEDINGS{9042120,
author={Rădescu, Radu and Dragu, Mihaela},
booktitle={2019 11th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)},
title={Automatic Analysis of Potential Hazard Events Using Unmanned Aerial Vehicles},
year={2019},
volume={},
number={},
pages={1-6},
abstract={This paper is motivated by the possibility of developing a wide variety of applications and domains in which Unmanned Aerial Vehicles (UAVs) can be used globally for various purposes. UAVs are currently used by public administrations and security forces such as police, fire brigades, civil protection, research institutions, construction, and agriculture entities. The purpose of this paper is to facilitate the handling of UAVs to retrieve various data from the environment. The drone (UAV) visits some points to collect data (image and/or video input) from sensors like GPS, camera, gyroscope, and accelerometer. GPS sensor coordinates are used to compare the data taken with subsequent results through processing with specialized software. The drone is used as an access gate with built-in sensors. Certain hazard events (fires, floods, avalanches, landslides) are not limited to narrow geographical areas, but can impact the environment by triggering negative chain events. 3D modeling offers a wide range of possibilities to prevent potential hazard events, or, if such an event has occurred, makes it possible to monitor the affected area and assess the damage by comparing the area in the pre-event configuration with the after-event one. After image processing and data acquisition, a report is generated that includes the map and the 3D model of the analyzed object. A hazard is an agent that has the potential to cause damage to a particular target. Terms such as risk or danger can be used in similar contexts. TensorFlow is an open source software library in high-performance computing. Flexible architecture allows easy deployment of computing on a variety of platforms (CPU, GPU, TPU), from desktop to server or mobile devices. We used the learning transfer: at first we used a model that was already prepared for another problem, and then we re-qualified it on a similar problem. Deep learning from scratch can take several days, but learning transfer can be done shortly. We applied Python along with TensorFlow to train an image classifier and classify images with it. We formed a consistent set of training pictures, using three labels: fire, flood (detectable hazards) and nature (non-hazard images). We then re-qualified an efficient, small-sized neural network by (re)training the image set in order to get the best results in the hazards prediction selection process with a progressive higher accuracy as (re) training evolves at optimal rating. With Python and OpenCV technologies, we used four decision algorithms to generate prediction of hazard: Support Vector Machine, Naive Bayes, Logistic Regression, and Decision Tree Classifier. Each generated report includes precision, recall, f1-score, and support indices, depending on the class and intervals used. We also used the confusion matrix as an alternative method to evaluate the classification accuracy. Analyzing the 4 algorithms we noticed that they behave differently. Training using TensorFlow generated better results than the other methods. For the main classes tested hazard is recognized up to 99%.},
keywords={UAV;hazard;classifiers;decision;prediction},
doi={10.1109/ECAI46879.2019.9042120},
ISSN={},
month={June},}
@INPROCEEDINGS{6027533,
author={Bangerter, Endre and Krenn, Stephan and Seifriz, Martial and Ultes-Nitsche, Ulrich},
booktitle={2011 Information Security for South Africa},
title={cPLC — A cryptographic programming language and compiler},
year={2011},
volume={},
number={},
pages={1-8},
abstract={Cryptographic two-party protocols are used ubiquitously in everyday life. While some of these protocols are easy to understand and implement (e.g., key exchange or transmission of encrypted data), many of them are much more complex (e.g., e-banking and e-voting applications, or anonymous authentication and credential systems). For a software engineer without appropriate cryptographic skills the implementation of such protocols is often difficult, time consuming and error-prone. For this reason, a number of compilers supporting programmers have been published in recent years. However, they are either designed for very specific cryptographic primitives (e.g., zero-knowledge proofs of knowledge), or they only offer a very low level of abstraction and thus again demand substantial mathematical and cryptographic skills from the programmer. Finally, some of the existing compilers do not produce executable code, but only metacode which has to be instantiated with mathematical libraries, encryption routines, etc. before it can actually be used. In this paper we present a cryptographically aware compiler which is equally useful to cryptographers who want to benchmark protocols designed on paper, and to programmers who want to implement complex security sensitive protocols without having to understand all subtleties. Our tool offers a high level of abstraction and outputs well-structured and documented Java code. We believe that our compiler can contribute to shortening the development cycles of cryptographic applications and to reducing their error-proneness.},
keywords={Protocols;Cryptography;Graphical user interfaces;Java;Benchmark testing;Program processors;Cryptographic compiler;software engineering;security protocols},
doi={10.1109/ISSA.2011.6027533},
ISSN={2330-9881},
month={Aug},}
@INPROCEEDINGS{8526817,
author={Shah, Rinku and Vutukuru, Mythili and Kulkarni, Purushottam},
booktitle={2018 IEEE 26th International Conference on Network Protocols (ICNP)},
title={Cuttlefish: Hierarchical SDN Controllers with Adaptive Offload},
year={2018},
volume={},
number={},
pages={198-208},
abstract={Offloading computation to local controllers (closer to switches) has been a popular approach to designing scalable SDN controllers. We observe that, in addition to the offload of local switch-specific state, a subset of global state can also be offloaded to, and accessed at local controllers with suitable synchronization. We present the design and implementation of Cuttlefish, an SDN controller framework that adaptively offloads a portion of the application state (and computation) to local controllers. Cuttlefish uses developer-specified input to identify control messages that can be correctly processed at local controllers, and makes offloading decisions based on the cost of synchronizing the offloaded state across controllers. SDN applications use the Cuttlefish API to access the offloaded state, and Cuttlefish transparently manages the state synchronization, and redirection of control messages to the appropriate (central or local) controller. We have implemented Cuttlefish using the Floodlight SDN controller. Our evaluation shows that Cuttlefish applications achieve ~2X higher control plane throughput and ~50% lower control plane latency as compared to the traditional SDN design.},
keywords={Control systems;Synchronization;Process control;Long Term Evolution;Logic gates;Throughput;Scalability;software-defined networking, scalability, controller framework},
doi={10.1109/ICNP.2018.00029},
ISSN={1092-1648},
month={Sep.},}
@ARTICLE{8288807,
author={},
journal={ISO/IEC/IEEE 26512:2017(E)},
title={ISO/IEC/IEEE International Standard - Systems and software engineering - Requirements for acquirers and suppliers of information for users},
year={2017},
volume={},
number={},
pages={1-47},
abstract={This document was developed to assist users of ISO/IEC/IEEE 15288:2015 or ISO/IEC/IEEE 12207 to acquire or supply information for users as part of the system or life cycle processes. It defines the documentation process from the acquirer's standpoint and the supplier's standpoint. This document covers the requirements for information items used in the acquisition of user documentation products: the acquisition plan, document specification, statement of work, request for proposals, and the proposal. It provides an overview of the information management processes which may require acquisition and supply of system or software user documentation products and services. It addresses the preparation of requirements for user documentation. These requirements are central to the user documentation specification and statement of work. It includes requirements for primary document outputs of the acquisition and supply process: the request for proposal and the proposal for user documentation products and services. It also discusses the use of a Document Plan in the acquisition and supply processes. This document is independent of the software tools that may be used to produce documentation, and applies to both printed documentation and on-screen documentation. Its guidance is applicable to user documentation for systems including hardware as well as software.},
keywords={IEEE Standards;IEC Standards;ISO Standards;Software engineering;Systems engineering and theory;Requirements engineering},
doi={10.1109/IEEESTD.2017.8288807},
ISSN={},
month={Nov},}
@ARTICLE{7739983,
author={},
journal={ISO/IEC/IEEE FDIS P26512_D1, October 2016},
title={ISO/IEC/IEEE Draft International Standard - Systems and Software Engineering -- Requirements for Acquirers and Suppliers of Information for Users},
year={2016},
volume={},
number={},
pages={1-43},
abstract={},
keywords={IEEE Standards;IEC Standards;ISO Standards;Software engineering},
doi={},
ISSN={},
month={Jan},}
@INPROCEEDINGS{9724446,
author={Yuan, Bin and Sun, Shengyao and Deng, Xianjun and Zou, Deqing and Chen, Haoyu and Li, Shenghui and Jin, Hai},
booktitle={2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)},
title={Automatically derived stateful network functions including non-field attributes},
year={2021},
volume={},
number={},
pages={944-951},
abstract={The modern network consists of thousands of network devices from different suppliers that perform distinct code-pendent functions, such as routing, switching, modifying header fields, and access control across physical and virtual networks. Because of the network complexity, the network is prone to a wide range of errors, such as false-positive configuration, software errors, or unexpected interactions across protocols. These errors can lead to loops, sub-optimal routing, path leaks, black holes, and access control violations that make services unavailable, vulnerable to exploitation, or prone to attacks (e.g., DDoS attacks). To mitigate these problems, network operators deploy many different stateful network functions, like firewalls, NATs, load balancers, and intrusion-prevention boxes. They have become an important part of networks today, so it is critical to verify that these network functions are the same as expected deployments. All static network verification tools are meant to rigorously check network software or configuration for bugs before deployment. They usually use handwritten models or limited derivation models that are error-prone and ignore the fact that even the same type of network functions (from different vendors) still have different implementation details. In this paper, we propose a tool that can automatically synthesize more realistic and high-fidelity models that include stateful network functions with non-field attributes. We design an inferring algorithm, implement the transformation between data packages and symbolic packages, and obtain a finite state machine that can accurately express the actions of black-box network functions for a given configuration.},
keywords={Access control;Performance evaluation;Privacy;Protocols;Computational modeling;Switches;Routing;network function model;network verification;software defined network},
doi={10.1109/TrustCom53373.2021.00132},
ISSN={2324-9013},
month={Oct},}
@INPROCEEDINGS{7371714,
author={Morales, Laura Victoria and Murillo, Andres Felipe and Rueda, Sandra Julieta},
booktitle={2015 IEEE 14th International Symposium on Network Computing and Applications},
title={Extending the Floodlight Controller},
year={2015},
volume={},
number={},
pages={126-133},
abstract={Software Defined Networking (SDN) emerges as an option to implement security features difficult to develop and deploy in traditional network infrastructures. SDN has a programmable component that can build a global view of the actual state of a network and change network configuration to react to actual events: a controller. Additionally, a controller's functionality may be extended to meet specific requirements. This work studies the features that Floodlight, a Java based SDN controller, offers to extend its behavior. Previous works have studied Floodlight architecture and performance, but not these features. To meet the goal, we selected a known security context for traditional networks: DDoS detection and mitigation. This paper presents design and implementation of the CDM(Collection, Detection, and Mitigation) module, a statistical-based DDoS detection module that extends Floodlight. Statistical algorithms are a good fit for SDN, they have low memory and CPU demands, and can react to changes in network configuration. The module also uses Java features to establish an interface for statistical-based detection algorithms, enabling administrators to use libraries of algorithms and select some of them according to their systems. The results show that Floodlight is easy to extend and flexible. It is also efficient regarding CPU, but requires more memory than other controllers. The collection, detection, and mitigation algorithms run fast, although the time window required to detect statistical change bounds reaction times.},
keywords={Protocols;Computer crime;Entropy;Context;Servers;Network topology;Software Defined Networking;Statistical-based DDoS Detection Algorithms;Floodlight},
doi={10.1109/NCA.2015.11},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6613298,
author={Wagner, Christian},
booktitle={2013 IEEE Symposium on Advances in Type-2 Fuzzy Logic Systems (T2FUZZ)},
title={Juzzy - A Java based toolkit for Type-2 Fuzzy Logic},
year={2013},
volume={},
number={},
pages={45-52},
abstract={In this paper we introduce a Java based toolkit for the development of type-1, interval type-2 and (zSlices based) general type-2 Fuzzy Logic Systems (FLSs) which is available as a free download. We describe our motivation for the release of such a toolkit in Java - to improve the accessibility to specifically type-2 FLSs to users both in industry and academia beyond the fuzzy logic research community, as well as to facilitate the application of FLSs in real world applications from web and cloud based deployments to multi-agent based control, for example in smart buildings. To the best of our knowledge it is the first package or toolkit that enables the straightforward design and implementation of type-1, interval and general type-2 FLSs. We showcase and review the toolkit's features and provide sample implementations of the different FLSs, together with explanations and source code. Finally, we conclude with future developments and a call for feedback and contributions to drive the further development of the toolkit.},
keywords={Fuzzy sets;Fuzzy logic;Java;MATLAB;Fuzzy systems;Communities;fuzzy systems;type-2;toolkit;fuzzy inference;software;open-source;multi-core;Java},
doi={10.1109/T2FZZ.2013.6613298},
ISSN={},
month={April},}
@ARTICLE{7478038,
author={Lin, Liang and Zhou, Liang and Zhu, Yi and Hua, Yujie and Mao, Jun-Fa and Yin, Wen-Yan},
journal={IEEE Transactions on Electromagnetic Compatibility},
title={Improvement in Cavity and Model Designs of LDMOS Power Amplifier for Suppressing Metallic Shielding Cover Effects},
year={2016},
volume={58},
number={5},
pages={1617-1628},
abstract={Some key technical issues are addressed for improving electromagnetic compatibility (EMC) design of metallic shielding cavity for achieving better RF performance of LDMOSFET-based power amplifier (PA). With the help of an Automation Testing Bench, its input-output responses are at first measured and compared, where an interactive influence between the PA and PCB is studied in detail. The internal and outer matching structures of LDMOSFET on the PCB are simulated using the commercial full-wave software HFSS. Perturbation theory is used to analyze the shielding cover effects with different package sizes and cavity heights sensitivity considered. Some guidelines for selecting suitable inner shielding structure and cavity height are given, and an improved model for both LDMOSFET and PA module is proposed and analyzed. Furthermore, it is integrated with the lumped-element circuit model of die and cosimulated using the software ADS 2013, and good agreement between the simulated and measured results is obtained. The most sensitive feedback path of electromagnetic interference inside the shielding cavity is characterized and a metallic isolation wall is introduced between the input (gate) and output (drain) of the LDMOSFET die for suppressing inner coupling effects. This research should be useful for the development of compact and miniaturized PA modules with our desired EMC performance.},
keywords={Cavity resonators;Bonding;Wires;Electromagnetic compatibility;Frequency measurement;Gain;Integrated circuit modeling;Cover shielding;feedback path;impedance match;LDMOSFET;multiple bonding wires;power amplifier;printed circuit board (PCB)},
doi={10.1109/TEMC.2016.2566677},
ISSN={1558-187X},
month={Oct},}
@INPROCEEDINGS{9276556,
author={Ramanathan, Murali Krishna and Clapp, Lazaro and Barik, Rajkishore and Sridharan, Manu},
booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
title={Piranha: Reducing Feature Flag Debt at Uber},
year={2020},
volume={},
number={},
pages={221-230},
abstract={Feature flags are commonly used in mobile app development and can introduce technical debt related to deleting their usage from the codebase. This can adversely affect the overall reliability of the apps and increase their maintenance complexity. Reducing this debt without imposing additional overheads on the developers necessitates the design of novel tools and automated workflows.In this paper, we describe the design and implementation of PIRANHA, an automated code refactoring tool which is used to automatically generate differential revisions (a.k. a diffs) to delete code corresponding to stale feature flags. PIRANHA takes as input the name of the flag, expected treatment behavior, and the name of the flag’s author. It analyzes the ASTs of the program to generate appropriate refactorings which are packaged into a diff. The diffis assigned to the author of the flag for further processing, who can land it after performing any additional refactorings.We have implemented PIRANHA to delete code in Objective-C, Java, and Swift programs, and deployed it to handle stale flags in multiple Uber apps. We present our experiences with the deployment of PIRANHA from Dec 2017 to May 2019, including the following highlights: (a) generated code cleanup diffs for 1381 flags (17% of total flags), (b) 65% of the diffs landed without any changes, (c) over 85% of the generated diffs compile and pass tests successfully, (d) around 80% of the diffs affect more than one file, (e) developers process more than 88% of the generated diffs, (f) 75% of the generated diffs are processed within a week, and (g) PIRANHA diffs have been interacted with by~200 developers across Uber. Piranha is available as open source at https://github.com/uber/ piranha.CCS CONCEPTS• Software and its engineering → Software maintenance tools.},
keywords={Tools;Testing;Software;Organizations;Mobile applications;Complexity theory;Buildings;code refactoring;feature flags;software maintenance;abstract syntax trees},
doi={},
ISSN={},
month={Oct},}
@ARTICLE{9691158,
author={Garcia, Lionel J and Timmermans, Mathilde and Pozuelos, Francisco J and Ducrot, Elsa and Gillon, Michaël and Delrez, Laetitia and Wells, Robert D and Jehin, Emmanuël},
journal={Monthly Notices of the Royal Astronomical Society},
title={PROSE: a PYTHON framework for modular astronomical images processing},
year={2021},
volume={509},
number={4},
pages={4817-4828},
abstract={To reduce and analyse astronomical images, astronomers can rely on a wide range of libraries providing low-level implementations of legacy algorithms. However, combining these routines into robust and functional pipelines requires a major effort that often ends up in instrument-specific and poorly maintainable tools, yielding products that suffer from a low level of reproducibility and portability. In this context, we present PROSE, a PYTHON framework to build modular and maintainable image processing pipelines. Built for astronomy, it is instrument-agnostic and allows the construction of pipelines using a wide range of building blocks, pre-implemented or user-defined. With this architecture, our package provides basic tools to deal with common tasks, such as automatic reduction and photometric extraction. To demonstrate its potential, we use its default photometric pipeline to process 26 TESS candidates follow-up observations and compare their products to the ones obtained with ASTROIMAGEJ, the reference software for such endeavours. We show that PROSE produces light curves with lower white and red noise while requiring less user interactions and offering richer functionalities for reporting.},
keywords={instrumentation: detectors;methods: data analysis;planetary systems},
doi={10.1093/mnras/stab3113},
ISSN={1365-2966},
month={Sep.},}
@INPROCEEDINGS{933841,
author={Vasilko, M. and Machacek, L. and Matej, M. and Stepien, P. and Holloway, S.},
booktitle={Proceedings 12th International Workshop on Rapid System Prototyping. RSP 2001},
title={A rapid prototyping methodology and platform for seamless communication systems},
year={2001},
volume={},
number={},
pages={70-76},
abstract={The availability of reconfigurable technologies has enabled the construction of flexible systems allowing run-time reconfiguration of system hardware and software functions. "Seamless communications" (also known in radio communications world as "reconfigurable" or "software-defined" radio) is one of the areas where technologies allowing runtime reconfiguration are highly desirable. This paper presents a new rapid prototyping methodology and platform for prototyping generic seamless communication systems. The methodology combines a C-based software design flow targeting host and DSP processors, and a rapid FPGA hardware design flow based on Handel-C-a C-like programming language. The hardware design flow also supports the generation of partial FPGA configurations. A library of parametrised communication modules was developed to facilitate the rapid construction of common communication architectures. A PC-based prototyping platform provides a set of custom hardware interfaces for prototyping systems with radio-frequency (RF), infrared (IR) and generic wide-bandwidth communication links. The feasibility of the presented methodology was tested on several simple demonstrator applications.},
keywords={Prototypes;Hardware;Software prototyping;Field programmable gate arrays;Radio frequency;Communication system software;Software systems;Radio communication;Runtime;Software design},
doi={10.1109/IWRSP.2001.933841},
ISSN={},
month={June},}
@INPROCEEDINGS{9137114,
author={Khodajou-Chokami, Hamidreza and Bitarafan, Adeleh and Dylov, Dmitry V. and Soleymani Baghshah, Mahdieh and Hosseini, Seyed Abolfazl},
booktitle={2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)},
title={Personalized Computational Human Phantoms via a Hybrid Model-based Deep Learning Method},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Computed tomography (CT) simulators are versatile tools for scanning protocol evaluation, optimization of geometrical design parameters, assessment of image reconstruction algorithms, and evaluation of the impact of future innovations attempting to improve the performance of CT scanners. Computational human phantoms (CHPs) play a key role in simulators for the radiation dosimetry and assessment of image quality tasks in the medical x-ray systems. Since the construction of patient-specific CHPs can be both difficult and time-consuming, nominal standard/reference CHPs have been established, yielding significant discrepancies in the special design and optimization demands of patient dose and imaging protocols for most medical applications. Therefore, the aim of this work was to develop a personalized Monte-Carlo (MC) CT simulator equipped with a fast and well-structured tool-kit called DeepSegNet for automatic generation of patient-specific CHPs based on MRI images, working under two principal algorithms. To this end, we first developed a 3D convolutional neural network (3DCNN) for the automated segmentation of 3D MRI images to detect anatomical organs/tissues. Then, a 3D voxel merging (3DVM) algorithm constructing CHPs and making fast MC calculations were developed. The proposed 3DCNN benefits from the main merit of residual networks by designing a 15-layer model. Next, the 3DVM algorithm utilizes the segmented data acquired from the former step, to create realistic and optimized CHPs by material mapping and voxel size manipulating. The performance of our 3DCNN model on 20 patients as test cases was 84.54% and 74.52% in terms of average accuracy and Dice-Coefficient, respectively, outperforming SegNet, as a comparable method by 2%. Finally, we developed an MC CT simulator by implementing a set of our generated CHPs. The efficiency of our 3DVM algorithm in constructing CHPs was assessed in terms of MC execution time and the number of merged voxels representing occupied storage memory and compared to the existing lattice method. Besides, the accuracy of our 3DVM investigated through the estimation of patient dose maps and image reconstruction. Results demonstrated a significant reduction of about 96% in the number of voxels and a 15% reduction in MC execution time for x-ray photon transportation while keeping the same accuracy. Therefore, this software package has a strong potential in the optimization of therapeutic and radiological imaging procedures.},
keywords={Image segmentation;Three-dimensional displays;Protocols;Computed tomography;Magnetic resonance imaging;Software algorithms;Task analysis;computational human phantoms;follow-up CT imaging;semantic segmentation;convolutional neural networks;monte-carlo method;voxel merging algorithm;lattice method},
doi={10.1109/MeMeA49120.2020.9137114},
ISSN={},
month={June},}
@INPROCEEDINGS{4658602,
author={Balaji, V and Ramakrishna, P.V.},
booktitle={2007 International Conference on Intelligent and Advanced Systems},
title={Design studies on the implementation of On Board Control, Signal Acquisition and Communication (OBCSAC) system on FPGA/ASIC platforms},
year={2007},
volume={},
number={},
pages={1340-1344},
abstract={The on board control, signal acquisition and communication (OBCSAC) subsystem is a critical component of all satellite systems. This paper presents the results of a feasibility study on realizing the design subsystem first on FPGA, and then on ASIC. Apart from giving the design details and results on a typical basic configuration, a number of extensions of this architecture have been studied and the results are presented. Specifically, starting from the realization of the basic configuration on a commercial antifuse FPGA, the studies carried out include (a) mapping the vital configuration onto an equivalent radiation hardened antifuse FPGA (b) mapping the same configuration onto a commercial FPGA after incorporating dasiaTriple Modular Redundancy (TMR) manually at the RTL level, (c) altering the basic architecture significantly to conform to the CCSDS standard prescribed for the OBCSAC of any satellite and mapping the design onto to FPGAs as well as on ASICS first without TMR and then separately incorporating TMR for each case. The basic configuration realized in an FPGA had been extensively tested in hardware under various expected environmental conditions and the FPGA utilization details are provided. For the ASIC design, the details up to the layout/GDSII stage are presented. Further, in the case of the ASIC, the test patterns for the netlist were generated based on ATPG software and the details of fault coverage are provided. The FPGA designs were carried out with Actel SX72A and AX1000 device families and the ASIC designs were carried out using the AMIS 0.35u CMOS libraries. The results presented in this work will serve as a design guideline in terms of FPGA/ASIC area, power, speed, and fault coverage and fault tolerance to those working in the area of system design.},
keywords={Field programmable gate arrays;Tunneling magnetoresistance;Application specific integrated circuits;Telemetry;Satellites;Clocks;Encoding;FPGA;ASIC Design;CCSDS;TMR},
doi={10.1109/ICIAS.2007.4658602},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6662043,
author={Houssem, Abid and Philippe, Pernelle and Didier, Noterman and Pierre, Campagne Jean and Chokri, Benamar},
booktitle={2013 19th International Conference on Automation and Computing},
title={Integration approach of mechatronics system in PLM systems},
year={2013},
volume={},
number={},
pages={1-6},
abstract={The design of mechatronic systems requires the integration of data concerning numerous disciplines (mechanical, electric CAD, automatism, embarked software…). In this context, the PLM (Product Lifecycle Management) systems define a naturally relevant solution to manage those data. However, being built around the CAD, the structure of these systems does not allow a real integration in terms of re-use, reconfiguration or calculation of impact. This article proposes a frame of definition for the global integration of components into an application library (CAD, Automatic, HMI…) within a PLM including the mechatronic systems. This integration approach is based on the construction of a meta-model structured with multi-faceted components. The propositions were experimented in the context of a simple mechatronic system and implemented with the PLM Windchill system.},
keywords={Mechatronics;Business;Solid modeling;Design automation;Software;Unified modeling language;Assembly;PLM;Mechatronic;Integration;Meta-Model},
doi={},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{538316,
author={Sreekanth, U. and Govindaraj, T. and Mitchell, C.M. and McGinnis, L.F.},
booktitle={1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century},
title={POEMS - a process and object environment for manufacturing simulation},
year={1995},
volume={4},
number={},
pages={3426-3431 vol.4},
abstract={Existing research efforts in object-oriented manufacturing simulation are structured predominantly on the event-interaction simulation paradigm. An often effective simulation option for discrete-event systems modeling is a process interaction approach, as indicated by the success of commercial manufacturing simulation software that support this paradigm. The construction and use of an object-oriented modeling architecture structured on the process-interaction simulation paradigm is described in this paper. Through a real-world example, the creation and re-use of an object and process library for manufacturing entities is illustrated. The software is implemented in C++, using C++/CSIM and runs on UNIX workstations.},
keywords={Manufacturing processes;Virtual manufacturing;Object oriented modeling;Manufacturing systems;Discrete event simulation;Computational modeling;Computer integrated manufacturing;Analytical models;Computer simulation;Manufacturing industries},
doi={10.1109/ICSMC.1995.538316},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8750929,
author={Zhao, Ziang and Kang, Yunfan and Magdy, Amr and Cowger, Win and Gray, Andrew},
booktitle={2019 IEEE 35th International Conference on Data Engineering Workshops (ICDEW)},
title={A Data-Driven Approach for Tracking Human Litter in Modern Cities},
year={2019},
volume={},
number={},
pages={69-73},
abstract={In the recent years, human litter, such as food waste, diapers, construction materials, used motor oil, hypodermic needles, etc, is causing growing problems for the environment and quality of life in modern cities. Data about this waste has a significant importance in the field of environmental sciences due to its important use cases that span saving marine life, reducing the risk from natural hazards, community cleaning efforts, etc. In addition, such litter spreads several diseases in urban areas with high populations such as undeveloped neighborhoods in large modern cities. In this paper, we introduce a data-driven approach that enables environmental scientists and organizations to track, manage, and model human litter data at a large scale through smart technologies. We make a major on-going effort to collect and maintain this data worldwide from different sources through a community of environmental scientists and partner organizations. With the increasing volume of collected datasets, existing software packages, such as GIS software, do not scale to process, query, and visualize such data. To overcome this, we provide a scalable data management and visualization framework that digests datasets from different sources, with different formats, in a scalable backend that cleans, integrates, and unifies them in a structured form. On top of this backend, frontend applications are built to visualize litter data at multiple spatial levels, from continents and oceans to street level, to enable new opportunities for both environmental scientists and organizations to track, model, and clean up litter data. The framework is currently managing thirty real datasets and provide different interfaces for different kinds of users.},
keywords={Conferences;Data engineering;data management;visualization;data cleaning;litter data},
doi={10.1109/ICDEW.2019.00-33},
ISSN={2473-3490},
month={April},}
@INPROCEEDINGS{5757278,
author={Wegner, Jan Dirk and Inglada, Jordi and Tison, Celine},
booktitle={7th European Conference on Synthetic Aperture Radar},
title={Automatic Fusion of SAR and Optical Imagery based on Line Features},
year={2008},
volume={},
number={},
pages={1-4},
abstract={A generic image processing chain for the fusion of very high resolution optical and SAR imagery is developed. It is fast to operate and provides reasonably good accuracy. This method is capable of rapidly registering optical and SAR images based on extracted features rather than on pixels. Indeed, at very high resolution, pixel approaches fail on above ground structures. An approach on a higher semantic level is required. The fusion is generally applicable to many kinds of remote sensing imagery due to it being generic. Its goal is to enable feature based registration of unrectified optical and SAR images with sub metric resolution in urban areas. Important aspects of this fusion approach are the very high resolution of the images, the introduction of a classification, rapid processing and the modular construction of the entire processing chain based on the open source software library ORFEO Toolbox (OTB).},
keywords={Optical imaging;Optical filters;Optical sensors;Optical distortion;Feature extraction;Image resolution},
doi={},
ISSN={},
month={June},}
@ARTICLE{744533,
author={Zaman, M.H. and Carlen, E.T. and Mastrangelo, C.H.},
journal={IEEE Transactions on Semiconductor Manufacturing},
title={Automatic generation of thin film process flows. I. Basic algorithms},
year={1999},
volume={12},
number={1},
pages={116-128},
abstract={This paper is the first in a series of two papers describing the algorithms used in the development of MISTIC (Michigan synthesis tools for integrated circuits). MISTIC is a planar device process compiler that generates process flows for thin film devices from schematics of their structure. This software uses a laboratory specific database of process recipes to produce process flows for a specific set of laboratory resources (furnaces, etchers, lithography equipment, etc.) and generates process statistics that help to choose the most suitable process flow in a comparative manner. The process compiler is augmented by several auxiliary modules: a device builder, process viewer, and database editor thus forming a self-contained process design environment. This paper concentrates on the algorithms used to construct process flows from schematic device representations. The compiler algorithms first extract a directed graph representation of the device organization stored in the form of a restricted square boolean matrix. This matrix is used to generate linear ordered lists of device layers which serve as footprints for the construction of process flows. Process flows are then constructed from these lists through a series of conversions, expansions, and insertions of process steps.},
keywords={Transistors;Integrated circuit synthesis;Laboratories;Databases;Transmission line matrix methods;Thin film circuits;Thin film devices;Furnaces;Etching;Lithography},
doi={10.1109/66.744533},
ISSN={1558-2345},
month={Feb},}
@INPROCEEDINGS{238791,
author={Couvillion, J. and Freire, R. and Johnson, R. and Obal, W.D. and Qureshi, M.A. and Rai, M. and Sanders, W.H. and Tvedt, J.E.},
booktitle={Proceedings of the Fourth International Workshop on Petri Nets and Performance Models PNPM91},
title={Performability modeling with UltraSAN},
year={1991},
volume={},
number={},
pages={290-299},
abstract={Stochastic extensions to Petri nets have received growing attention during the past decade as a model for evaluating the performance, dependability, and performability of computer hardware, software, and networks. Their formal structure permits solution by analytic means in many cases. When this is not possible, they an facilitate the automatic generation of a simulation program to estimate system behavior. The paper describes an X-window based software tool for evaluating systems that are represented as stochastic activity networks, a variant of stochastic Petri nets. The tool, known as UltraSAN, incorporates the results of recent research to significantly reduce the size of the state space that is considered for analytic solution, as well as the number of event types that are considered in simulation. Throughout the paper, a simple local area network model is used to illustrate the concepts, user interface, and model construction and solution methods implemented in the package.<>},
keywords={Petri nets;Stochastic systems;Stochastic processes;Performance evaluation;Computer networks;Hardware;Software performance;Computational modeling;Software tools;State-space methods},
doi={10.1109/PNPM.1991.238791},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7353530,
author={Loianno, Giuseppe and Mulgaonkar, Yash and Brunner, Chris and Ahuja, Dheeraj and Ramanandan, Arvind and Chari, Murali and Diaz, Serafin and Kumar, Vijay},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title={Smartphones power flying robots},
year={2015},
volume={},
number={},
pages={1256-1263},
abstract={Consumer grade technology seen in cameras and phones has led to the price/performance ratio of sensors and processors falling dramatically over the last decade. In particular, most devices are packaged with a camera, a gyroscope, and an accelerometer, important sensors for aerial robotics. The low mass and small form factor make them particularly well suited for autonomous flight with small flying robots, especially in GPS-denied environments. In this work, we present the first fully autonomous smartphone-based quadrotor. All the computation, sensing and control runs on an off-the-shelf smartphone, with all the software functionality in a smartphone app.We show how quadrotors can be stabilized and controlled to achieve autonomous flight in indoor buildings with application to smart homes, search and rescue, construction and architecture. The work allows any consumer with a smartphone to autonomously drive a quadrotor robot platform, even without GPS, by downloading an app, and concurrently build 3-D maps.},
keywords={Vehicles;Smart phones;Cameras;Robot sensing systems;Software},
doi={10.1109/IROS.2015.7353530},
ISSN={},
month={Sep.},}
@ARTICLE{9131777,
author={Jiang, Jing and Qu, Linchi},
journal={IEEE Access},
title={Evolution and Emerging Trends of Sustainability in Manufacturing Based on Literature Visualization Analysis},
year={2020},
volume={8},
number={},
pages={121074-121088},
abstract={Sustainability is necessary for improving the quality of the manufacturing industry, which is an important pillar of the economy. The existing studies can hardly realize a comprehensive reflection of the research status of sustainability in manufacturing. Thus, this study adopts science mapping as the main research method for the systematic and accurate mastery of the overall research status, development track, research hotspots, and trends within this field. Moreover, this work provides a systematic review of related studies. This study examines 6,591 articles (1999-2019), including in core journals on Web of Science as research samples, and deeply explores core journal, core author, core country, discipline distribution, keyword co-occurrence, and literature co-citation via CiteSpace software. According to results, authoritative journals regarding sustainability in manufacturing place particular emphasis on theory, method, or application in different degrees. The number of published articles in the U.S.A. tops the ranking, and the influence of Chinese research institutes and scholars is continuously enhanced, where the main research fields are engineering, environmental science, and so on. Sustainable innovativeness in the manufacturing industry is generally at a high level. Moreover, research hotspots have evolved from traditional to modern and undergone three phases: theory construction-industry or enterprise technology construction-the emergence of new problems and application of new technologies. This study further analyzes the four clusters with citation bursts, namely, additive manufacturing, power consumption, green supply chain, and green information system in terms of grasping the knowledge structures and probing the classical research results. Future research should emphasize 3D printing design and application in the information era, the business model integrating the manufacturing industry and service industry, optimization of production modeling, and so on. This study aims to provide a systematic, comprehensive, dynamic, and objective review of the literature on sustainability in manufacturing to deepen and perfect the research in this field.},
keywords={Sustainable development;Manufacturing;Manufacturing industries;Market research;Production;Systematics;Manufacturing industry;sustainability;bibliometrics;science mapping;evolution;emerging trends},
doi={10.1109/ACCESS.2020.3006582},
ISSN={2169-3536},
month={},}
@ARTICLE{1276164,
author={Lemaitre, L. and McAndrew, C.},
journal={IEEE Circuits and Devices Magazine},
title={An open-source software tool for compact modeling applications},
year={2004},
volume={20},
number={2},
pages={6-41},
abstract={This column features an open-source software tool - called automatic device model synthesizer (ADMS) - that supports and simplifies compact model development, implementation, distribution, maintenance, and sharing. The intended audience is people in universities or CAD companies interested in contributing to the development and improvement of the tool. It should also help people whose main interest is using the tool to get a better understanding of how ADMS works internally. ADMS is available on SourceForge.net, one of the most popular open-source software development Web sites. It was developed at Motorola Inc., whose main interest is to support standardization of compact models.},
keywords={Open source software;Software tools;Application software;Add-drop multiplexers;XML;Mathematical model;Hardware design languages;Resistors;Software libraries;Data structures},
doi={10.1109/MCD.2004.1276164},
ISSN={1558-1888},
month={March},}
@ARTICLE{STD98111,
author={},
journal={IEEE Standard for Software Interface for Maintenance Information Collection and Analysis (SIMICA)},
title={IEEE Standard for Software Interface for Maintenance Information Collection and Analysis (SIMICA)},
year={},
volume={},
number={},
pages={},
abstract={This document provides an implementation-independent specification for a software interface to information systems containing data pertinent to the diagnosis and maintenance of complex systems consisting of hardware, software, or any combination thereof. These interfaces will support service definitions for creating application programming interfaces (API) for the access, exchange, and analysis of historical diagnostic and maintenance information. This will address the pervasive need of organizations to assess the effectiveness of diagnostics for complex systems throughout the product life cycle. The use of formal information models will facilitate exchanging historical maintenance information between information systems and analysis tools. The models will facilitate creating open system software architectures for maturing system diagnostics.},
keywords={},
doi={},
ISSN={},
month={},}
@INPROCEEDINGS{6093379,
author={},
booktitle={The 5th International Conference on New Trends in Information Science and Service Science},
title={[Front and back cover]},
year={2011},
volume={1},
number={},
pages={c1-c4},
abstract={The following topics are dealt with: IT service oriented academic programmes; software process improvement; storage cloud computing; virtual world health avatar application; GPU-attached multicore hybrid systems; discrete decision variables; service-oriented architecture; information service; social network systems; middleware architecture; accounting disclosure research; supply chain; device-oriented adaptive user interface; information disseminator; mobile e-healthcare system; shortest path algorithm; Zigbee wireless embedded network; rough set theory; neuralnetwork; information fusion methodology; information extraction; software safety integrity level; face verification; BitTorrent-like P2P systems; SOA quality rating; ubiquitous museum service environment; vehicular ad hoc networks; IR image processing; fast polynomial reconstruction attack; multi agent system; inventory management; customer-focused Internet business model; virtual team collaboration; academic libraries; higher education; genetic algorithm; hierarchical facilitylocation model; Web service choreography; customizable agile software quality assurance model; game theory analysis; knowledge discovery; online keyword advertisement; objects tracking; tacit knowledge management; Chinese construction industry; aspect oriented programming and bibliometric methodology.},
keywords={},
doi={},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5978560,
author={},
booktitle={2011 IEEE INTERNATIONAL CONFERENCE ON ELECTRO/INFORMATION TECHNOLOGY},
title={Table of contents},
year={2011},
volume={},
number={},
pages={1-7},
abstract={The following topics are dealt with: hybrid semi-blind digital image watermarking technique; face recognition;database schemas; intelligent agent based job search system; retinal image segmentation; intelligent information retrieval lifecycle architecture; illumination-reflectance; unified power quality conditioner; universal scalability law; network performance evaluation; super high voltage transmission line electric field; signal processing methods; cost reduction; pavement pothole detection; SOCKx; application layer network switching framework; automated pavement distress inspection; transmission simulator development; keyword spotting experiment; multipath routing; environmental sensor network data packaging; predictive multichannel feedback active noise control; 2-channel adaptive speech enhancement; permission-based security model; dielectric electroactive polymer energy harvesting system forward path design; sEMG based real-time embedded force control strategy; tree-type wireless acquisition network; aligned aluminum-doped zinc oxide nanorod arrays; conditioning circuit development; thin film transistors fabrication; robust license plate localization; collaborative learning; frequency domain symbol synchronization; agent based TDMA schedule; malicious messages identification; customer survey data processing; trace element detection; fuzzy rule extraction; component packaging footprint; wireless network coding scheme; secure data transmission; model-based software-defined radio; clinical decision support system; low power 6-T CNFET SRAM cell; intelligent extended clustering genetic algorithm; low power process monitoring circuit; distributed intrusion detection scheme; time frequency analysis techniques; collaborative product design; capsule image segmentation; reconfigurable architectures; 9T SRAM design; turbo codec performance evaluation; architectural design tool; application commerce; foetal electrocardiogram extraction; route anomaly detection; yellowjacket wast nest construction; free space laser communication; energy harvesting system; low power skin sensor; discrete cosine transform-based reconfigurable system design; energy aware adaptive clustering; automated electronic pen; and remote avian monitoring system.},
keywords={},
doi={10.1109/EIT.2011.5978560},
ISSN={2154-0373},
month={May},}
@ARTICLE{4310922,
author={Daneshdoost, M. and Shaat, R.},
journal={IEEE Power Engineering Review},
title={A PC Based Integrated Software for Power System Education},
year={1989},
volume={9},
number={8},
pages={70-70},
abstract={This paper presents the implementation of an integrated software package to run under PC-DOS for the analysis and design of Electric Power Networks. Graphics and windows are embedded in the user interface to form the basis of the interactive environment. System configuration is entered graphically, while system data is entered directly through tabular windows. A variety of analysis programs are provided in this environment such as different types of load flow solution techniques. In addition, this paper demonstrates a novel use of PC enhanced graphics capability to display the real and reactive power flows by means of animation. A modular interactive expert system is an element of this environment. Popular features such as help menus and icons selection menus are also included. The modular design of this environment permits the user to interface any custom made analysis package regardless of the computer language used. This environment proves to be useful for educational and research purposes. Recent work in CAE (Computer Aided Engineering) and CAI (Computer Assisted Instruction) areas show a clear trend toward more sophisticated user interfaces. One of the primary goals of any interactive environment is to enhance the bandwidth of communication between the user and the application program. Wider bandwidth of communications means that more information is conveyed to the user in a shorter period of time. The user interface is the aspect of the program that governs the communication between the user and the application.},
keywords={Software systems;Power systems;User interfaces;Graphics;Computer aided engineering;Computer aided instruction;Bandwidth;Power system analysis computing;Software packages;Load flow analysis},
doi={10.1109/MPER.1989.4310922},
ISSN={1558-1705},
month={Aug},}
@ARTICLE{1362580,
author={Leavitt, N.},
journal={Computer},
title={Are Web services finally ready to deliver?},
year={2004},
volume={37},
number={11},
pages={14-18},
abstract={Web services, in brief, are a framework of software technologies designed to support interoperable machine-to-machine interaction over a network. Companies on different systems can use Web services to exchange information online with business partners, customers, and suppliers. Various standards organizations and industry consortia are developing Web services specifications without a unifying authority. Organizations such as the World Wide Web Consortium (W3C), the Organization for the Advancement of Structured Information Standards (OASIS), the Liberty Alliance Project, and the Web Services Interoperability Organization (WS-I) have developed or reviewed numerous standards. A primary goal of Web services is to unlock a new generation of e-commerce applications.},
keywords={Web services;Simple object access protocol;Automation;Packaging;Guidelines;Production;Standards development;Software tools;Information security;Routing protocols},
doi={10.1109/MC.2004.199},
ISSN={1558-0814},
month={Nov},}