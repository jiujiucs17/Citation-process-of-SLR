@article{STILLERMAN2014784,
title = {Input/output plugin architecture for MDSplus},
journal = {Fusion Engineering and Design},
volume = {89},
number = {5},
pages = {784-786},
year = {2014},
note = {Proceedings of the 9th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2013.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920379613007163},
author = {Joshua Stillerman and Thomas Fredian and Gabriele Manduchi},
keywords = {Data management, Data acquisition, MDSplus},
abstract = {The first version of MDSplus was released in 1991 for VAX/VMS. Since that time the underlying file formats have remained constant. The software however has evolved, it was ported to unix, linux, Windows, and Macintosh. In 1997 a TCP based protocol, mdsip, was added to provide network access to MDSplus data. In 2011 a mechanism was added to allow protocol plugins to permit the use of other transport mechanisms such as ssh to access data users. This paper describes a similar design which permits the insertion of plugins to handle the reading and writing of MDSplus data at the data storage level. Tree paths become URIs which specify the protocol, host, and protocol specific information. The protocol is provided by a dynamically activated shared library that can provide any consistent subset of the data store access API, treeshr. The existing low level network protocol called mdsip, is activated by defining tree paths like “host::/directory”. Using the new plugin mechanism this is re-implemented as an instance of the general plugin that replaces the low level treeshr input/output routines. It is specified by using a path like “mdsip://host/directory”. This architecture will make it possible to adapt the MDSplus data organization and analysis tools to other underlying data storage. The first new application of this, after the existing network protocol is implemented, will be a plugin based on a key value store. Key value stores, can provide inexpensive scalable, redundant data storage. An example of this might be an Amazon G3 plugin which would let you specify a tree path such as “AG3://container” to access MDSplus data stored in the cloud.}
}
@article{RAMESH201819,
title = {MPI performance engineering with the MPI tool interface: The integration of MVAPICH and TAU},
journal = {Parallel Computing},
volume = {77},
pages = {19-37},
year = {2018},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819118301479},
author = {Srinivasan Ramesh and Aurèle Mahéo and Sameer Shende and Allen D. Malony and Hari Subramoni and Amit Ruhela and Dhabaleswar K. (DK) Panda},
keywords = {, Runtime introspection, Autotuning, Performance engineering, Performance recommendations, TAU, MVAPICH2, BEACON},
abstract = {The desire for high performance on scalable parallel systems is increasing the complexity and tunability of MPI implementations. The MPI Tools Information Interface (MPI_T) introduced as part of the MPI 3.0 standard provides an opportunity for performance tools and external software to introspect and understand MPI runtime behavior at a deeper level to detect scalability issues. The interface also provides a mechanism to fine-tune the performance of the MPI library dynamically at runtime. In this paper, we propose an infrastructure that extends existing components — TAU, MVAPICH2, and BEACON to take advantage of the MPI_T interface and offer runtime introspection, online monitoring, recommendation generation, and autotuning capabilities. We validate our design by developing optimizations for a combination of production and synthetic applications. Using our infrastructure, we implement an autotuning policy for AmberMD (a molecular dynamics package) that monitors and reduces the internal memory footprint of the MVAPICH2 MPI library without affecting performance. For applications such as MiniAMR whose collective communication is latency sensitive, our infrastructure is able to generate recommendations to enable hardware offloading of collectives supported by MVAPICH2. By implementing this recommendation, the MPI time for MiniAMR at 224 processes reduces by 15%.}
}
@article{SHATNAWI2020110748,
title = {ReSIde: Reusable service identification from software families},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110748},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110748},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301722},
author = {Anas Shatnawi and Abdelhak Seriai and Houari Sahraoui and Tewfik Ziadi and Abderrahmene Seriai},
keywords = {Software reuse, Service-oriented reengineering, Reverse engineering, Variability, Software families, Object-oriented source code},
abstract = {The clone-and-own approach becomes a common practice to quickly develop Software Product Variants (SPVs) that meet variability in user requirements. However, managing the reuse and maintenance of the cloned codes is a very hard task. Therefore, we aim to analyze SPVs to identify cloned codes and package them using a modern systematic reuse approach like Service-Oriented Architecture (SOA). The objective is to benefit from all the advantages of SOA when creating new SPVs. The development based on services in SOA supports the software reuse and maintenance better than the development based on individual classes in monolithic object-oriented software. Existing service identification approaches identify services based on the analysis of a single software product. These approaches are not able to analyze multiple SPVs to identify reusable services of cloned codes. Identifying services by analyzing several SPVs allows to increase the reusability of identified services. In this paper, we propose ReSIde (Reusable Service Identification): an automated approach that identifies reusable services from a set of object-oriented SPVs. This is based on analyzing the commonality and the variability between SPVs to identify the implementation of reusable functionalities corresponding to cloned codes that can be packaged as reusable services. To validate ReSIde, we have applied it on three product families of different sizes. The results show that the services identified based on the analysis of multiple product variants using ReSIde are more reusable than services identified based on the analysis of singular ones.}
}
@article{PORTILLO20171,
title = {MUESLI - a Material UnivErSal LIbrary},
journal = {Advances in Engineering Software},
volume = {105},
pages = {1-8},
year = {2017},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2017.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0965997816301430},
author = {David Portillo and Daniel {del Pozo} and Daniel Rodríguez-Galán and Javier Segurado and Ignacio Romero},
keywords = {Material modeling, Open source, Software library, Automatic testing},
abstract = {This article describes MUESLI, an open source library with constitutive models of continuum materials for solid, fluid, and thermal problems available at http://www.materials.imdea.org/Muesli. The library is object oriented, and includes the most commonly employed material models in Computational Mechanics. It is designed for easy extension, and includes classes for tensor manipulation and automatic testing. The library can be linked to existing codes, including commercial ones, for some of which specific interfaces are provided.}
}
@article{AMPATZOGLOU20112265,
title = {An empirical investigation on the reusability of design patterns and software packages},
journal = {Journal of Systems and Software},
volume = {84},
number = {12},
pages = {2265-2283},
year = {2011},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2011.06.047},
url = {https://www.sciencedirect.com/science/article/pii/S0164121211001592},
author = {Apostolos Ampatzoglou and Apostolos Kritikos and George Kakarontzas and Ioannis Stamelos},
keywords = {Design patterns, Design, Quality, Reusability, Empirical approach},
abstract = {Nowadays open-source software communities are thriving. Successful open-source projects are competitive and the amount of source code that is freely available offers great reuse opportunities to software developers. Thus, it is expected that several requirements can be implemented based on open source software reuse. Additionally, design patterns, i.e. well-known solution to common design problems, are introduced as elements of reuse. This study attempts to empirically investigate the reusability of design patterns, classes and software packages. Thus, the results can help developers to identify the most beneficial starting points for white box reuse, which is quite popular among open source communities. In order to achieve this goal we conducted a case study on one hundred (100) open source projects. More specifically, we identified 27,461 classes that participate in design patterns and compared the reusability of each of these classes with the reusability of the pattern and the package that this class belongs to. In more than 40% of the cases investigated, design pattern based class selection, offers the most reusable starting point for white-box reuse. However there are several cases when package based selection might be preferable. The results suggest that each pattern has different level of reusability.}
}
@article{HEINE2021117147,
title = {Design and dispatch optimization of packaged ice storage systems within a connected community},
journal = {Applied Energy},
volume = {298},
pages = {117147},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117147},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921005870},
author = {Karl Heine and Paulo Cesar Tabares-Velasco and Michael Deru},
keywords = {Connected community optimization, Cool thermal energy storage, Mixed-integer linear programming, Packaged ice storage, Unitary thermal storage systems, Energy storage, EnergyPlus},
abstract = {The traditional implementation of cool thermal energy storage (CTES) must be reimagined within the context of a dynamic grid and smart buildings operating as connected communities. As most buildings do not operate central chillers or connect to district cooling loops, this necessitates a broader use of packaged CTES. Our objective is to begin answering the question of how such packaged CTES should be implemented within a connected community. We do so by presenting a simulation–optimization workflow employing building energy modeling software and a mixed-integer linear program to design and dispatch a packaged CTES technology to achieve minimum total annual cost. We demonstrate this methodology on a seven-building case study using current utility rates and find that total annual cooling energy costs can be reduced by 17.8% compared to baseline, after accounting for the cost of storage. We perform three parametric sensitivity studies to evaluate modeling assumptions and obtain the prioritization of storage procurement as a function of annualized life-cycle cost of storage. We find that a community optimization approach provides significantly different results than individual building optimizations and provides greater savings compared to baseline.}
}
@article{CIZNICKI201490,
title = {Benchmarking JPEG 2000 implementations on modern CPU and GPU architectures},
journal = {Journal of Computational Science},
volume = {5},
number = {2},
pages = {90-98},
year = {2014},
note = {Empowering Science through Computing + BioInspired Computing},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2013.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877750313000410},
author = {Miłosz Ciżnicki and Michał Kierzynka and Piotr Kopta and Krzysztof Kurowski and Paweł Gepner},
keywords = {GPU, Multi-core CPU, JPEG 2000, Signal processing},
abstract = {The use of graphics hardware for non-graphics applications has become popular among many scientific programmers and researchers as we have observed a higher rate of theoretical performance increase than the CPUs in recent years. However, performance gains may be easily lost in the context of a specific parallel application due to various both hardware and software factors. JPEG 2000 is a complex standard for data compression and coding, that provides many advanced capabilities demanded by more specialized applications. There are several JPEG 2000 implementations that utilize emerging parallel architectures with the built-in support for parallelism at different levels. Unfortunately, many available implementations are only optimized for a certain parallel architecture or they do not take advantage of recent capabilities provided by modern hardware and low level APIs. Thus, the main aim of this paper is to present a comprehensive real performance analysis of JPEG 2000. It consists of a chain of data and compute intensive tasks that can be treated as good examples of software benchmarks for modern parallel hardware architectures. In this paper we compare achieved performance results of various JPEG 2000 implementations executed on selected architectures for different data sets to identify possible bottlenecks. We discuss also best practices and advices for parallel software development to help users to evaluate in advance and then select appropriate solutions to accelerate the execution of their applications.}
}
@article{DOROZ2014766,
title = {A million-bit multiplier architecture for fully homomorphic encryption},
journal = {Microprocessors and Microsystems},
volume = {38},
number = {8, Part A},
pages = {766-775},
year = {2014},
note = {2013 edition of the Euromicro Conference on Digital System Design (DSD 2013)},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2014.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0141933114000842},
author = {Yarkın Doröz and Erdinç Öztürk and Berk Sunar},
keywords = {Fully homomorphic encryption, Very-large number multiplication, Number theoretic transform},
abstract = {In this work we present a full and complete evaluation of a very large multiplication scheme in custom hardware. We designed a novel architecture to realize a million-bit multiplication scheme based on the Schönhage–Strassen Algorithm. We constructed our scheme using Number Theoretical Transform (NTT). The construction makes use of an innovative cache architecture along with processing elements customized to match the computation and access patterns of the NTT-based recursive multiplication algorithm. We realized our architecture with Verilog and using a 90nm TSMC library, we could get a maximum clock frequency of 666MHz. With this frequency, our architecture is able to compute the product of two million-bit integers in 7.74ms. Our data shows that the performance of our design matches that of previously reported software implementations on a high-end 3GHz Intel Xeon processor, while requiring only a tiny fraction of the area.1An earlier short conference version of this paper appeared in [41].1}
}
@article{AREND2022100341,
title = {MLPro 1.0 - Standardized reinforcement learning and game theory in Python},
journal = {Machine Learning with Applications},
volume = {9},
pages = {100341},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000482},
author = {Detlef Arend and Steve Yuwono and Mochammad Rizky Diprasetya and Andreas Schwung},
keywords = {Machine learning, Reinforcement learning, Game theory, Automation, Scientific software development, Python},
abstract = {Nowadays there are numerous powerful software packages available for most areas of machine learning (ML). These can be roughly divided into frameworks that solve detailed aspects of ML and those that pursue holistic approaches for one or two learning paradigms. For the implementation of own ML applications, several packages often have to be involved and integrated through individual coding. The latter aspect in particular makes it difficult for newcomers to get started. It also makes a comparison with other works difficult, if not impossible. Especially in the area of reinforcement learning (RL), there is a lack of frameworks that fully implement the current concepts up to multi-agents (MARL) and model-based agents (MBRL). For the related field of game theory (GT), there are hardly any packages available that aim to solve real-world applications. Here we would like to make a contribution and propose the new framework MLPro, which is designed for the holistic realization of hybrid ML applications across all learning paradigms. This is made possible by an additional base layer in which the fundamentals of ML (interaction, adaptation, training, hyperparameter optimization) are defined on an abstract level. In contrast, concrete learning paradigms are implemented in higher sub-frameworks that build on the conventions of this additional base layer. This ensures a high degree of standardization and functional recombinability. Proven concepts and algorithms of existing frameworks can still be used. The first version of MLPro includes sub-frameworks for RL and cooperative GT.}
}
@article{MOON2022107637,
title = {Software platform for high-fidelity-data-based artificial neural network modeling and process optimization in chemical engineering},
journal = {Computers & Chemical Engineering},
volume = {158},
pages = {107637},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107637},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421004142},
author = {Jiyoung Moon and Dela Quarme Gbadago and Gyuyeong Hwang and Dongjun Lee and Sungwon Hwang},
keywords = {Artificial neural network (ANN), Genetic algorithm (GA), Software, Data prediction, Chemical engineering},
abstract = {Artificial neural networks are revolutionizing the field of engineering because of their ability to model complex non-linear problems without explicit programming. Their applications in different areas, such as manufacturing and healthcare, have provided a new path away from traditional modeling techniques. Nonetheless, users of this technology, especially those without prior knowledge of neural networks, spend a considerable amount of time gaining a basic understanding of the use of technology. Traditional trial-and-error approaches are often employed in training these neural networks, which further increases the time spent in developing them. Owing to the laborious nature of the trial-and-error method, the optimal or best hyperparameters of a particular neural network may not be determined, thereby affecting the accuracy of the developed model. Hence, in this study, a software platform is presented to aid in the training and development of neural networks by using genetic algorithms (GAs) for optimizing the model's hyperparameters, such as the number of neurons, learning rate, and activation function. As an essential aspect of chemical engineering processes, design or operating-parameter optimization is also included in this software package, wherein the best (optimized) weights and biases from the neural network are saved and employed in another GA to optimize key process variables, such as temperature, and velocity, as required by the user. This dual-purpose provides a complete application of neural networks that are primarily encountered in many engineering disciplines. The software platform can also plot 3D contours, heat maps (correlation plots), and other line graphs. For the validation and generalization of the software, it was benchmarked against five cases presented by different authors across various chemical engineering fields. The prediction results obtained using the software package were higher than those presented in the published literature, demonstrating the superior performance of the software package.}
}
@article{AVALLONE201698,
title = {Design and implementation of WiMesh: A tool for the performance evaluation of multi-radio wireless mesh networks},
journal = {Journal of Network and Computer Applications},
volume = {63},
pages = {98-109},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2015.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516000436},
author = {S. Avallone and G. {Di Stasi}},
keywords = {Multi-radio wireless mesh networks, Channel assignment and routing algorithms, Network simulation},
abstract = {In this paper we present WiMesh, a software tool we developed during the last few years of research conducted in the field of multi-radio wireless mesh networks. WiMesh serves two main purposes: (i) to evaluate and compare different algorithms for the static configuration of a wireless mesh network (assignment of channels, transmission rate and power to the available network radios, explicit routing); (ii) to automatically setup and run packet level simulations (by using the ns-3 network simulator) based on the network configuration returned by such algorithms. WiMesh consists of a core library, three libraries dedicated to distinct functionalities of WiMesh and three corresponding utilities that allow us to easily conduct experiments. To ensure ease of use and flexibility, all such utilities accept as input an XML configuration file where various options and parameters can be specified. WiMesh is freely available to the research community as open source software, with the purpose of easing the development of new algorithms and the verification of their performances. In this paper, we first present the architecture of WiMesh and its features and capabilities by illustrating the design and the usage of each of the provided utilities. Then, to the benefit of those willing to implement their own solution within WiMesh or extend its functionalities, we illustrate the design of the WiMesh libraries. Finally, we report some of the results, which we were able to show in previous research work thanks to the use of WiMesh.}
}
@article{ZHU2020103810,
title = {Computer-aided mobility analysis of parallel mechanisms},
journal = {Mechanism and Machine Theory},
volume = {148},
pages = {103810},
year = {2020},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2020.103810},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X20300318},
author = {Xiaorong Zhu and Huiping Shen and Chengqi Wu and Damien Chablat and Tingli Yang},
keywords = {Mobility analysis, Degree of freedom, Position and orientation characteristic, Parallel mechanisms},
abstract = {To determine mobility properties is one of the most challenging issue in the analysis and synthesis of parallel manipulators (PMs). However, currently, the most used methods mainly rely on experiences and manual analysis, which led to inefficient implementation. The motivation of this paper is to present an automatic mobility analysis algorithm and software package for the researchers and designers with an effective and practical means. According to the topological design theory of PMs based on position and orientation characteristic (POC) equations, this paper proposes a set of computer algorithmic rules and procedures for automated mobility analysis of PMs in the most user-friendly and efficient way. Firstly, a complete digital information model for topological structures which has a mapping relationship with the POC of a PM is proposed. This model not only describes the dimension of the motion outputs, but also includes the mapping relationship between the output orientation and the axes of the kinematic joints. Secondly, algorithmic rules are established that convert the union and intersection operations of POC into binary logical operations and then the detailed algorithmic procedures for an automatic mobility analysis are presented. In what follows, a corresponding software for automatic mobility analysis is described. The software package is equipped with a GUI that facilitates the input and allows the visualization of the results. Finally, four typical examples are provided to show the effectiveness of the software package for most of parallel mechanisms (not including some paradoxical mechanisms).}
}
@incollection{REDDY201165,
title = {Chapter 3 - Patterns},
editor = {Martin Reddy},
booktitle = {API Design for C++},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {65-104},
year = {2011},
isbn = {978-0-12-385003-4},
doi = {https://doi.org/10.1016/B978-0-12-385003-4.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123850034000038},
author = {Martin Reddy},
abstract = {Publisher Summary
This chapter focuses on the techniques and principles of building high-quality APIs. It covers a few useful design patterns and idioms that relate to C++ API design. A design pattern is a general solution to a common software design problem. It concentrates on those design patterns that are of particular importance to the design of high-quality APIs and discusses their practical implementation in C++. It also covers C++ idioms that may not be considered true generic design patterns, but which are nevertheless important techniques for C++ API design. The techniques that are used specifically are: pimpl idiom, singleton and factory method, proxy, adapter, and facade, and observer. The chapter also details the API wrapping patterns and observer pattern. Writing a wrapper interface is a relatively common API design task and it's very common for objects to call methods in other objects.}
}
@article{SHARMA2021103963,
title = {Exploring the security landscape: NoC-based MPSoC to Cloud-of-Chips},
journal = {Microprocessors and Microsystems},
volume = {84},
pages = {103963},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103963},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121001411},
author = {Gaurav Sharma and Georgios Bousdras and Soultana Ellinidou and Olivier Markowitch and Jean-Michel Dricot and Dragomir Milojevic},
keywords = {Cloud-of-Chips, Security overview, Software-defined networking, Key management, MPSoC, Network-on-chip},
abstract = {In this paper, we present a detailed and systematic overview of communication security aspects of Multi-Processor Systems-on-Chip (MPSoC) and the emerging potential threats on the novel Cloud-of-Chips (CoC) paradigm. The CoC concept refers to highly scalable and composable systems, assembled not only at system design-time using RTL, like traditional SoC, but also at integrated circuit (IC) packaging time thanks to 3D-IC integration technology. Practical implementation of CoC systems needs to solve the problem of scalable, configurable and secure communication not only between different functional blocks in a single ICs, but also between different ICs in a single package, and between different packages on the same or different PCBs and even between different systems. To boost such extremely flexible communication infrastructure CoC system relies on Software-Defined Network-on-Chip (SDNoC) paradigm that combines design-time configurability of on-chip systems (NoC) and highly configurable communication of macroscopic systems (SDN). This study first explores security threats and existing solutions for traditional MPSoC platforms. Afterwards, we propose SDNoC as an alternative to MPSoC communication security, and we further extend our discussion to CoC systems to identify additional security concerns. Moreover, we present a comparison of SDNoC based approach over existing approaches and discuss its potential advantages.}
}
@article{AGHAYI2021110840,
title = {Crowdsourced Behavior-Driven Development},
journal = {Journal of Systems and Software},
volume = {171},
pages = {110840},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110840},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302302},
author = {Emad Aghayi and Thomas D. LaToza and Paurav Surendra and Seyedmeysam Abolghasemi},
keywords = {Microtask programming, Programming environments, Behavior-Driven Development, Crowdsourcing, Workflow, Microservices},
abstract = {Key to the effectiveness of crowdsourcing approaches for software engineering is workflow design, describing how complex work is organized into small, relatively independent microtasks. This paper, we introduce a Behavior-Driven Development (BDD) workflow for accomplishing programming work through self-contained microtasks, implemented as a preconfigured environment called CrowdMicroservices. In our approach, a client, acting on behalf of a software team, describes a microservice as a set of endpoints with paths, requests, and responses. A crowd then implements the endpoints, identifying individual endpoint behaviors that they test, implement, debug, create new functions, and interact with persistence APIs as needed. To evaluate our approach, we conducted a feasibility study in which a small crowd worked to implement a small ToDo microservice. The crowd created an implementation with only four defects, completing 350 microtasks and implementing 13 functions. We discuss the implications of these findings for incorporating crowdsourced programming contributions into traditional software projects.}
}
@article{SECKLER2021101296,
title = {AutoPas in ls1 mardyn: Massively parallel particle simulations with node-level auto-tuning},
journal = {Journal of Computational Science},
volume = {50},
pages = {101296},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101296},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320305901},
author = {Steffen Seckler and Fabio Gratl and Matthias Heinen and Jadran Vrabec and Hans-Joachim Bungartz and Philipp Neumann},
keywords = {AutoPas, ls1 mardyn, Molecular dynamics, Particle simulations, MPI, Auto-tuning},
abstract = {Due to computational cost, simulation software is confronted with the need to always use optimal building blocks — data structures, solver algorithms, parallelization schemes, and so forth — in terms of efficiency, while it typically needs to support a variety of hardware architectures. AutoPas implements the computationally most expensive molecular dynamics (MD) steps (e.g., force calculation) and chooses on-the-fly, i.e., at run time, the optimal combination of the previously mentioned building blocks. We detail decisions made in AutoPas to enable the interplay with MPI-parallel simulations and, to our knowledge, showcase the first MPI-parallel MD simulations that use dynamic tuning. We discuss the benefits of this approach for three simulation scenarios from process engineering, in which we obtain performance improvements of up to 50%, compared to the baseline performance of the highly optimized ls1 mardyn software.}
}
@article{RUSSEL2007111,
title = {A package for exact kinetic data structures and sweepline algorithms},
journal = {Computational Geometry},
volume = {38},
number = {1},
pages = {111-127},
year = {2007},
note = {Special Issue on CGAL},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2006.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S092577210700020X},
author = {Daniel Russel and Menelaos I. Karavelas and Leonidas J. Guibas},
abstract = {In this paper we present a package for implementing exact kinetic data structures built on objects which move along polynomial trajectories. We discuss how the package design was influenced by various considerations, including extensibility, support for multiple kinetic data structures, access to existing data structures and algorithms in CGAL, as well as debugging. Due to the similarity between the operations involved, the software can also be used to compute arrangements of polynomial objects using a sweepline approach. The package consists of three main parts, the kinetic data structure framework support code, an algebraic kernel which implements the set of algebraic operations required for kinetic data structure processing, and kinetic data structures for Delaunay triangulations in one and two dimensions, and Delaunay and regular triangulations in three dimensions. The models provided for the algebraic kernel support both exact operations and inexact approximations with heuristics to improve numerical stability.}
}
@article{ANZT2021101278,
title = {Crediting pull requests to open source research software as an academic contribution},
journal = {Journal of Computational Science},
volume = {49},
pages = {101278},
year = {2021},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101278},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320305743},
author = {Hartwig Anzt and Eileen Kuehn and Goran Flegar},
keywords = {Scientific excellence paradigms, Conference contributions, Scientific reputation, Community software development},
abstract = {Like any other scientific discipline, the High Performance Computing community suffers under the publish or perish paradigm. As a result, a significant portion of novel algorithm designs and hardware-optimized implementations never make it into production code but are instead abandoned once they served the purpose of yielding (another) publication. At the same time, community software packages driving scientific research lack the addition of new technology and hardware-specific implementations. This results in a very unsatisfying situation where researchers and software developers are working independently, and the traditional peer reviewing is reaching its capacity limits. A paradigm shift that accepts high-quality software pull requests to open source research software as conference contributions may create incentives to realize new and/or improved algorithms in community software ecosystems. In this paper, we propose to complement code reviews on pull requests to scientific open source software with scientific reviews, and allow the presentation and publication of high quality software contributions that present an academic improvement to the state-of-the-art at scientific conferences.}
}
@article{LATT2021334,
title = {Palabos: Parallel Lattice Boltzmann Solver},
journal = {Computers & Mathematics with Applications},
volume = {81},
pages = {334-350},
year = {2021},
note = {Development and Application of Open-source Software for Problems with Numerical PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2020.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0898122120301267},
author = {Jonas Latt and Orestis Malaspinas and Dimitrios Kontaxakis and Andrea Parmigiani and Daniel Lagrava and Federico Brogi and Mohamed Ben Belgacem and Yann Thorimbert and Sébastien Leclaire and Sha Li and Francesco Marson and Jonathan Lemus and Christos Kotsalos and Raphaël Conradin and Christophe Coreixas and Rémy Petkantchin and Franck Raynaud and Joël Beny and Bastien Chopard},
keywords = {Palabos, Lattice Boltzmann method, Open-source software, Computational Fluid Dynamics, High performance computing},
abstract = {We present the scope, concepts, data structures and application programming models of the open-source Lattice Boltzmann library Palabos. Palabos is a C++ software platform developed since 2010 for Computational Fluid Dynamics simulations and Lattice Boltzmann modeling, which specifically targets applications with complex, coupled physics. The software proposes a very broad modeling framework, capable of addressing a large number of applications of interest in the Lattice Boltzmann community, yet exhibits solid computational performance. The article describes the philosophy of this programming framework and lists the models already implemented. Finally, benchmark simulations are provided which serve as a proof of quality of the implemented core functionalities.}
}
@article{KANTERS2014721,
title = {Tools and methods used by architects for solar design},
journal = {Energy and Buildings},
volume = {68},
pages = {721-731},
year = {2014},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2012.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0378778812003040},
author = {Jouri Kanters and Miljana Horvat and Marie-Claude Dubois},
keywords = {Solar energy, Architecture, Design process, Design tools, Design methods, Early design stage, Barriers},
abstract = {Architects have a key role to play when it comes to the design of future low-energy (solar) buildings. Proper design tools and working methods could help architects in the design process. In order to identify barriers of existing tools and methods for solar design, needs of architects for improved tools, and to gain an insight into architects’ methods of working during the design process, an international survey was carried out within the framework of IEA-SHC Task 41-Solar Energy and Architecture, combined with semi-structured interviews. This paper presents an overview of main results of this study. Both the survey and interviews strongly indicate the need for further development of design tools for solar architecture, focusing on a user-friendly, visual tool that is easily interoperable within current modelling software packages, and which generates clear and meaningful results that are compatible with the existing work flow of the architect. Furthermore, the survey and interviews also indicated a strong awareness about solar aspects among respondents. However, this was combined with a limited use and knowledge of solar energy technologies, suggesting the need for further skill development amongst architects and tool development to accelerate the implementation of these technologies in future buildings and urban fabric.}
}
@article{PATALI2020104701,
title = {Efficient modular hybrid adders and Radix-4 booth multipliers for DSP applications},
journal = {Microelectronics Journal},
volume = {96},
pages = {104701},
year = {2020},
issn = {0026-2692},
doi = {https://doi.org/10.1016/j.mejo.2020.104701},
url = {https://www.sciencedirect.com/science/article/pii/S0026269219303799},
author = {Pramod Patali and Shahana {Thottathikkulam Kassim}},
keywords = {Adder, Concatenation, Critical path, Incrementation, Radix-4, Multiplier},
abstract = {Adders and multipliers are the fundamental elements of a signal processing architecture. Improve the speed of addition and multiplication operations while minimizing power consumption and area is the problem of interest of this paper. Two versions of modular hybrid adder structures are proposed. The adder structures are derived through the merging of improved carry skip, carry look ahead and ripple carry adder (RCA) concepts. The proposed adder version-1 has improved speed of operation while maintaining power consumption lower than that of RCA. The proposed adder version-2 achieves further improvement in speed through the addition of incrementation scheme at the cost of slight increase in hardware complexity. Two versions of radix-4 booth multipliers are proposed. Among the two versions, the booth multiplier version-1 has the highest speed and lowest power consumption and version-2 has the lowest area compared to most of the existing architectures. Synthesis result show that the delay of proposed multiplier version-1 is reduced by 20.74%, PDP by 45.62% and ADP by 32.26% in comparison with a typical low PDP 8*8 Booth multiplier while consuming 31.4% less power and 14.59% less area. Cadence software with gpdk 45 ​nm standard cell library is used for the design and implementation.}
}
@article{JORDAN2020102648,
title = {The allscale framework architecture},
journal = {Parallel Computing},
volume = {99},
pages = {102648},
year = {2020},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2020.102648},
url = {https://www.sciencedirect.com/science/article/pii/S0167819120300417},
author = {Herbert Jordan and Philipp Gschwandtner and Peter Thoman and Peter Zangerl and Alexander Hirsch and Thomas Fahringer and Thomas Heller and Dietmar Fey},
keywords = {Programming models, Parallel runtimes, Runtimes, High performance computing, Extreme scale, Parallel computing, Space weather simulation, Efficient computing, Development efficiency, Productivity},
abstract = {The tremendous challenge of developing applications efficiently utilizing the hardware provided by contemporary parallel systems of all scales is among the most limiting factors for the continuous growth of high performance computing. In this article, we present a novel architecture taking on this challenge by providing an infrastructure for the effective development of such applications. Our design combines the expressive power of modern C++, advanced compiler technology, and sophisticated runtime system solutions, with the goal of providing a clean separation of domain specific algorithms, resource management activities, and low-level hardware interactions — all required to be accounted for by high performance applications. The article covers the architecture design, its key aspects, and a first evaluation of the achievable performance of an application implemented based on the proposed infrastructure.}
}
@article{CUOMO2017228,
title = {A parallel PDE-based numerical algorithm for computing the Optical Flow in hybrid systems},
journal = {Journal of Computational Science},
volume = {22},
pages = {228-236},
year = {2017},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877750317303010},
author = {Salvatore Cuomo and Pasquale {De Michele} and Ardelio Galletti and Livia Marcellino},
keywords = {Optical Flow, Partial differential equations, High performance computing, Graphic processor units, Hybrid architectures},
abstract = {In this paper, we propose a fine-to-coarse parallelization strategy in order to exploit, in a case study, a parallel hybrid architecture. We consider the Optical Flow numerical problem, modelled by partial differential equations, and implement a parallel multilevel software. Our hybrid software solution is a smart combination between codes on Graphic Processor Units (GPUs) and standard scientific parallel computing libraries on a cluster. Numerical experiments, on real satellite image sequences coming from a large dataset in a big data scenario, together with application profiling, highlight good results in terms of performance for the proposed approach.}
}
@article{YAGHOOBI2019109078,
title = {PRISMS-Plasticity: An open-source crystal plasticity finite element software},
journal = {Computational Materials Science},
volume = {169},
pages = {109078},
year = {2019},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2019.109078},
url = {https://www.sciencedirect.com/science/article/pii/S0927025619303696},
author = {Mohammadreza Yaghoobi and Sriram Ganesan and Srihari Sundar and Aaditya Lakshmanan and Shiva Rudraraju and John E. Allison and Veera Sundararaghavan},
keywords = {Crystal plasticity finite element, Open source software, Parallel performance, Twinning},
abstract = {An open-source parallel 3-D crystal plasticity finite element (CPFE) software package PRISMS-Plasticity is presented here as a part of PRISMS integrated framework. A highly efficient rate-independent crystal plasticity algorithm is implemented along with developing its algorithmic tangent modulus. Additionally, a twin activation mechanism is incorporated into the framework based on an integration point sensitive scheme. The integration of the software as a part of the PRISMS framework is demonstrated. To do so, the integration of the PRISMS-Plasticity software with experimental characterization techniques such as electron backscatter diffraction (EBSD) and synchrotron X-ray diffraction using available open source software packages of DREAM.3D and Neper is elaborated. The integration of the PRISMS-Plasticity software with the information repository of Materials Commons is also presented. The parallel performance of the software is characterized which demonstrates that it scales well for large problems running on hundreds of processors. Various examples of polycrystalline metals with face-centered cubic (FCC), body-centered cubic (BCC), and hexagonal close-packed (HCP) crystals structures are presented to show the capability of the software to efficiently solve crystal plasticity boundary value problems, in addition to integration with preprocessing and postprocessing tools. PRISMS-Plasticity is an important activity within the broader PRISMS Center and future enhancements to PRISMS-Plasticity are planned and will be described.}
}
@article{JALVING2019134,
title = {Graph-based modeling and simulation of complex systems},
journal = {Computers & Chemical Engineering},
volume = {125},
pages = {134-154},
year = {2019},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418312687},
author = {Jordan Jalving and Yankai Cao and Victor M. Zavala},
keywords = {Graphs, Cyber-physical, Connectivity, Algebraic, Computing},
abstract = {We present graph-based modeling abstractions to represent cyber-physical dependencies arising in complex systems. Specifically, we propose an algebraic graph abstraction to capture physical connectivity in complex optimization models and a computing graph abstraction to capture communication connectivity in computing architectures. The proposed abstractions are scalable and are used as the backbone of a Julia-based software package that we call Plasmo.jl. We show how the algebraic graph abstraction facilitates the implementation, analysis, and decomposition of optimization problems and we show how the computing graph abstraction facilitates the implementation of optimization and control algorithms and their simulation in virtual environments that involve distributed, centralized, and hierarchical computing architectures.}
}
@article{SAHA2008245,
title = {Portable library development for reconfigurable computing systems: A case study},
journal = {Parallel Computing},
volume = {34},
number = {4},
pages = {245-260},
year = {2008},
note = {Reconfigurable Systems Summer Institute 2007},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2008.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S016781910800046X},
author = {Proshanta Saha and Esam El-Araby and Miaoqing Huang and Mohamed Taher and Sergio Lopez-Buedo and Tarek El-Ghazawi and Chang Shu and Kris Gaj and Alan Michalski and Duncan Buell},
keywords = {Reconfigurable computing, Portable libraries, FPGA, Hardware design methodologies},
abstract = {Portable libraries of highly-optimized hardware cores can significantly reduce the development time of reconfigurable computing applications. This paper presents the tradeoffs and challenges in the design of such libraries. A set of library development guidelines is provided, which has been validated with the RCLib case study. RCLib is a set of portable libraries with over 100 cores, targeting a wide range of applications. RCLib portability has been verified in three major High-Performance reconfigurable computing architectures: SRC6, Cray XD1 and SGI RC100. Compared to full-software implementations, applications using RCLib hardware acceleration cores show speedups ranging from one to four orders of magnitude.}
}
@article{GAUSTAD2014241,
title = {A scientific data processing framework for time series NetCDF data},
journal = {Environmental Modelling & Software},
volume = {60},
pages = {241-249},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214001704},
author = {Krista Gaustad and Tim Shippert and Brian Ermold and Sherman Beus and Jeff Daily and Atle Borsholm and Kevin Fox},
keywords = {Atmospheric science, Time-series, NetCDF, Scientific data analysis, Observation data, Scientific workflow, Data management},
abstract = {The Atmospheric Radiation Measurement (ARM) Data Integrator (ADI) is a framework designed to streamline the development of scientific algorithms that analyze, and models that use time-series NetCDF data. ADI automates the process of retrieving and preparing data for analysis, provides a modular, flexible framework that simplifies software development, and supports a data integration workflow. Algorithm and model input data, preprocessing, and output data specifications are defined through a graphical interface. ADI includes a library of software modules to support the workflow, and a source code generator that produces C, IDL®, and Python™ templates to jump start development. While developed for processing climate data, ADI can be applied to any time-series data. This paper discusses the ADI framework, and how ADI's capabilities can decrease the time and cost of implementing scientific algorithms allowing modelers and scientists to focus their efforts on their research rather than preparing and packaging data.}
}
@incollection{SCHMIDT2013323,
title = {Chapter 19 - Software Implementation},
editor = {Richard F. Schmidt},
booktitle = {Software Engineering},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {323-333},
year = {2013},
isbn = {978-0-12-407768-3},
doi = {https://doi.org/10.1016/B978-0-12-407768-3.00019-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124077683000197},
author = {Richard F. Schmidt},
keywords = {software technical data package (TDP), assembly, integration, and testing, acceptance testing, programmatic design, source code, programming, test readiness review (TRR), deployment readiness review (DRR)},
abstract = {This chapter provides a generic set of tasks for the various software organizations involved in software engineering during the software implementation stage of development. The software implementation stage involves the transformation of the software technical data package into one or more fabricated, integrated, and tested software configuration items that are ready for software acceptance testing. Software implementation includes the dry-run of the software acceptance testing. This dry-run exercise is intended to ensure that the acceptance testing procedures are effective and the software product performs according to software specifications. A dry-run provides a demonstration of the software product’s readiness for acceptance testing. Software deficiencies identified within the software implementation stage must be resolved. Those deficiencies that are determined to be the result of architectural flaws must be documented in architectural problem reports.}
}
@article{GUDWIN2020179,
title = {The TROCA Project: An autonomous transportation robot controlled by a cognitive architecture},
journal = {Cognitive Systems Research},
volume = {59},
pages = {179-197},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719304772},
author = {Ricardo Gudwin and Eric Rohmer and André Paraense and Eduardo Fróes and Wandemberg Gibaut and Ian Oliveira and Sender Rocha and Klaus Raizer and Aneta {Vulgarakis Feljan}},
keywords = {Cognitive architecture, Transportation robot, Dual-process theory, Dynamic subsumption, MECA},
abstract = {Autonomous mobile robots emerged as an important kind of transportation system in warehouses and factories. In this work, we present the use of MECA cognitive architecture in the development of an artificial mind for an autonomous robot responsible for multiple tasks, including transportation of packages along a factory floor, environment exploration, warehouse inventory, its internal energy management, self-monitoring and dealing with human operators and other robots. The present text provides a detailed specification for the architecture and its software implementation. Future work will present the simulation results under different configurations, together with a detailed analysis of the architecture performance and its generalization for autonomous robot control.}
}
@incollection{HOLMQVIST2016811,
title = {A Generic PAT Software Interface for On-Line Monitoring and Control of Chromatographic Separation Systems},
editor = {Zdravko Kravanja and Miloš Bogataj},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {38},
pages = {811-816},
year = {2016},
booktitle = {26th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63428-3.50140-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634283501405},
author = {Anders Holmqvist and Anton Sellberg},
keywords = {Chromatography, Process analytical technology, Software interface, On-line monitoring and control, Iterative learning control},
abstract = {This contribution presents a novel process analytical technology (PAT) software interface for online monitoring and control of commercial high-pressure liquid chromatography (HPLC) systems. The developed interface is an add-on to chromatography control software and uses industry-standard bidirectional communication protocols to link sensor technologies with the individual HPLC system components in an overall automation framework that facilitates data acquisition, central operation and control of all instruments. The interface is encoded in the Python™ scripting language and supports versatile data transfer to chromatography control software using either OPC (OLE for process control) or COM (component object model) technologies, which are both based on client/server architectures. By these means, the interface utilizes the flexibility of the high-level programming language for formulating optimal control strategies and enables (semantic) interoperability between the chromatography control software and user defined scripts as well as third-party scientific libraries and numerical packages. The advantages and applicability of the developed interface are highlighted through the implementation of a model-based iterative learning control strategy, in order to assure batch-to-batch repeatability, and open-loop optimal controlled elution trajectories on a commercial HPLC separation system. It is, however, noteworthy that the software interface is completely generic and constitutes a novel framework for implementing any PID control schemes as well as sequential optimal experimental design and model predictive control strategies.}
}
@article{SCHUPP2002797,
title = {Semantic and behavioral library transformations},
journal = {Information and Software Technology},
volume = {44},
number = {13},
pages = {797-810},
year = {2002},
note = {Special Issue on Source Code Analysis and Manipulation (SCAM)},
issn = {0950-5849},
doi = {https://doi.org/10.1016/S0950-5849(02)00122-2},
url = {https://www.sciencedirect.com/science/article/pii/S0950584902001222},
author = {Sibylle Schupp and Douglas Gregor and David Musser and Shin-Ming Liu},
keywords = {Active libraries, Extensible compilers, Concept-based transformations, C++ metaprogramming},
abstract = {While software methodology encourages the use of libraries and advocates architectures of layered libraries, in practice the composition of libraries is not always seamless and the combination of two well-designed libraries not necessarily well designed, since it could result in suboptimal call sequences, lost functionality, or avoidable overhead. In this paper we introduce Simplicissimus, a framework for rewrite-based source code transformations that allows for code replacement in a systematic and safe manner. We discuss the design and implementation of the framework and illustrate its functionality with applications in several areas.}
}
@article{LAKSHMI201379,
title = {VLSI architecture for parallel radix-4 CORDIC},
journal = {Microprocessors and Microsystems},
volume = {37},
number = {1},
pages = {79-86},
year = {2013},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2012.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0141933112001925},
author = {B. Lakshmi and A.S. Dhar},
keywords = {CORDIC algorithm, Parallel radix-4, Redundant arithmetic},
abstract = {COordinate Rotation DIgital Computer (CORDIC) algorithm is an iterative method for fast hardware implementation of the elementary functions such as trigonometric, inverse trigonometric, logarithm, exponential, multiplication and division functions in a simple and elegant way. This paper presents a regular and scalable VLSI architecture for the implementation of parallel radix-4 rotational CORDIC algorithm. Thorough comparison of the proposed architecture with the available architectures has been carried out to show the latency and the hardware improvement. Furthermore, the proposed architecture is coded for 16-bit precision using the VHDL language. The functionally simulated net list has been synthesized with 90nm CMOS technology library and the area-time measures are provided. This architecture is also implemented using Xilinx ISE7.1i software and a Virtex device.}
}
@article{AWAN2022106896,
title = {Quantum computing challenges in the software industry. A fuzzy AHP-based approach},
journal = {Information and Software Technology},
volume = {147},
pages = {106896},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106896},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000581},
author = {Usama Awan and Lea Hannola and Anushree Tandon and Raman Kumar Goyal and Amandeep Dhir},
keywords = {Fuzzy analytic hierarchy process (F-AHP), Software process automation, Multiple-criteria decision-making (MCDM), Quantum software requirement, Quantum computing},
abstract = {Context
The current technology revolution has posed unexpected challenges for the software industry. In recent years, the field of quantum computing (QC) technologies has continued to grow in influence and maturity, and it is now poised to revolutionise software engineering. However, the evaluation and prioritisation of QC challenges in the software industry remain unexplored, relatively under-identified and fragmented.
Objective
The purpose of this study is to identify, examine and prioritise the most critical challenges in the software industry by implementing a fuzzy analytic hierarchy process (F-AHP).
Method
First, to identify the key challenges, we conducted a systematic literature review by drawing data from the four relevant digital libraries and supplementing these efforts with a forward and backward snowballing search. Second, we followed the F-AHP approach to evaluate and rank the identified challenges, or barriers.
Results
The results show that the key barriers to QC adoption are the lack of technical expertise, information accuracy and organisational interest in adopting the new process. Another critical barrier is the lack of standards of secure communication techniques for implementing QC.
Conclusion
By applying F-AHP, we identified institutional barriers as the highest and organisational barriers as the second highest global weight ranked categories among the main QC challenges facing the software industry. We observed that the highest-ranked local barriers facing the software technology industry are the lack of resources for design and initiative while the lack of organisational interest in adopting the new process is the most significant organisational barrier. Our findings, which entail implications for both academicians and practitioners, reveal the emergent nature of QC research and the increasing need for interdisciplinary research to address the identified challenges.}
}
@article{CASANOVA2020162,
title = {Developing accurate and scalable simulators of production workflow management systems with WRENCH},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {162-175},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19317431},
author = {Henri Casanova and Rafael {Ferreira da Silva} and Ryan Tanaka and Suraj Pandey and Gautam Jethwani and William Koch and Spencer Albrecht and James Oeth and Frédéric Suter},
keywords = {Scientific workflows, Workflow management systems, Simulation, Distributed computing},
abstract = {Scientific workflows are used routinely in numerous scientific domains, and Workflow Management Systems (WMSs) have been developed to orchestrate and optimize workflow executions on distributed platforms. WMSs are complex software systems that interact with complex software infrastructures. Most WMS research and development activities rely on empirical experiments conducted with full-fledged software stacks on actual hardware platforms. These experiments, however, are limited to hardware and software infrastructures at hand and can be labor- and/or time-intensive. As a result, relying solely on real-world experiments impedes WMS research and development. An alternative is to conduct experiments in simulation. In this work we present WRENCH, a WMS simulation framework, whose objectives are (i) accurate and scalable simulations; and (ii) easy simulation software development. WRENCH achieves its first objective by building on the SimGrid framework. While SimGrid is recognized for the accuracy and scalability of its simulation models, it only provides low-level simulation abstractions and thus large software development efforts are required when implementing simulators of complex systems. WRENCH thus achieves its second objective by providing high-level and directly re-usable simulation abstractions on top of SimGrid. After describing and giving rationales for WRENCH’s software architecture and APIs, we present two case studies in which we apply WRENCH to simulate the Pegasus production WMS and the WorkQueue application execution framework. We report on ease of implementation, simulation accuracy, and simulation scalability so as to determine to which extent WRENCH achieves its objectives. We also draw both qualitative and quantitative comparisons with a previously proposed workflow simulator.}
}
@article{BISIGHINI2022103173,
title = {EndoBeams.jl: A Julia finite element package for beam-to-surface contact problems in cardiovascular mechanics},
journal = {Advances in Engineering Software},
volume = {171},
pages = {103173},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103173},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000849},
author = {Beatrice Bisighini and Miquel Aguirre and Baptiste Pierrat and David Perrin and Stéphane Avril},
keywords = {Julia, 3D Corotational beams, Contact, Finite Element, Signed Distance Field, Braided Stents},
abstract = {The increasing use of mini-invasive and endovascular surgical techniques is at the origin of the pressing need for computational models to support planning and training. Several implantable devices have a wire-like structure, which can be modelled using beam elements. Our objective is to create an efficient Finite Element (FE) modelling framework for such devices. For that, we developed the EndoBeams.jl package, written exclusively in Julia, for the numerical simulation of contact interactions between wire-like structures and rigid surfaces. The package is based on a 3D FE corotational formulation for frictional contact dynamics of beams. The rigid target surface is described implicitly using a signed distance field, predefined in a volumetric grid. Since the main objective behind this package is to find the best compromise between computational speed and code readability, the algorithm, originally in Matlab, was translated and optimised in Julia, a programming language designed to combine the performance of low-level languages with the productivity of high-level ones. To evaluate the robustness, a set of tests were conducted to compare the simulation results and computational time of EndoBeams.jl against literature data, the original Matlab code and the commercial software Abaqus. The tests proved the accuracy of the underlying beam-to-surface formulation and showed the drastic performance improvement of the Julia code with respect to the original one. EndoBeams.jl is also slightly faster than Abaqus. Finally, as a proof of concept in cardiovascular medicine, a further example is shown where the deployment of a braided stent is simulated within an idealised artery.}
}
@article{MARKER20131282,
title = {Code Generation and Optimization of Distributed-memory Dense Linear Algebra Kernels},
journal = {Procedia Computer Science},
volume = {18},
pages = {1282-1291},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.295},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913004389},
author = {Bryan Marker and Don Batory and Robert {van de Geijn}},
keywords = {program generation, dense linear algebra, high-performance software, distributed-memory computing},
abstract = {Design by Transformation (DxT) is an approach to software development that encodes domain-specific programs as graphs and expert design knowledge as graph transformations. The goal of DxT is to mechanize the generation of highly-optimized code. This paper demonstrates how DxT can be used to transform sequential specifications of an important set of Dense Linear Algebra (DLA) kernels, the level-3 Basic Linear Algebra Subprograms (BLAS3), into high-performing library routines targeting distributed-memory (cluster) architectures. Getting good BLAS3 performance for such platforms requires deep domain knowledge, so their implementations are manually coded by experts. Unfortunately, there are few such experts and developing the full variety of BLAS3 implementations takes a lot of repetitive effort. A prototype tool, DxTer, automates this tedious task. We explain how we build on previous work to represent loops and multiple loop-based algorithms in DxTer. Performance results on a BlueGene/P parallel supercomputer show that the generated code meets or beats implementations that are hand-coded by a human expert and outperforms the widely used ScaLAPACK library.}
}
@article{JOYEUX20101057,
title = {Managing plans: Integrating deliberation and reactive execution schemes},
journal = {Robotics and Autonomous Systems},
volume = {58},
number = {9},
pages = {1057-1066},
year = {2010},
note = {Hybrid Control for Autonomous Systems},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2010.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0921889010001090},
author = {Sylvain Joyeux and Frank Kirchner and Simon Lacroix},
keywords = {Architecture, Plan management, Plan libraries},
abstract = {In this paper, we argue that a shift is needed for plan-based architectures to support versatile, long-lived systems. Our hypothesis is that we need to integrate plan generation, plan analysis and plan adaptation within a wholesome framework that allow a seamless integration of planning and plan execution activities, i.e. that the architectural focus should move away from planning towards so-called plan management. We present, in this paper, a software framework for plan management, which is based on a novel plan representation. This representation has been designed so that the actual context of the execution is available at all times: what the robot is doing and–more importantly–why it is doing it. Our plan manager implementation is available as open source, and has already been used on three different live systems, on which it demonstrated its capabilities.}
}
@article{FARSCHTSCHI20121281,
title = {Conceptual Design of a Two-Level Server Architecture for MATLAB-Java Coupling},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {2},
pages = {1281-1284},
year = {2012},
note = {7th Vienna International Conference on Mathematical Modelling},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120215-3-AT-3016.00227},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016308606},
author = {Yousef Farschtschi and Marc Widemann and Kai Himstedt and Dietmar P.F. Möller},
keywords = {MATLAB, Java, Coupling, TCP Server, Modeling & Simulation},
abstract = {Abstract
A well-known MATLAB problem is the non-existent thread-safety in its API. Therefore we suggest the concept of a two-level server architecture to further improve the MATLAB Java coupling performance. A single master server is responsible for the distribution of the requests of the clients to several worker servers. This architecture allows a parallel execution of MATLAB requests. MATLAB as a script-file based software for high-performance-simulation has different interfaces as programming languages of the third generation like C or Java. These techniques will be presented shortly. In previous research activities, a MATLAB Java Server was implemented on the basis of these two techniques to increase the performance of MATLAB and MATLAB Simulink simulation models. What makes this so special is that the MATLAB environment is already running and the MATLAB startup phase is avoided. All incoming input data can now be run in the specific simulation model. The MATLAB environment does not allow multi threading, thus just one MATLAB environment can be run on a machine.}
}
@article{HEGEDUS2021105732,
title = {Program package MPGOS: Challenges and solutions during the integration of a large number of independent ODE systems using GPUs},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {97},
pages = {105732},
year = {2021},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2021.105732},
url = {https://www.sciencedirect.com/science/article/pii/S1007570421000435},
author = {Ferenc Hegedűs},
keywords = {Ordinary differential equations, Non-linear systems, GPU Programming, Massively parallel architecture},
abstract = {Challenges and efficient solution techniques during the integration of a large number of independent ordinary differential equations (ODEs) using the massively parallel architecture of graphics processing units (GPUs) are presented. One of the main difficulties is the minimisation of the memory transactions through the PCI-E bus between the host (CPU) and the device (GPU) required frequently, for instance, during the calculation of the Lyapunov exponent, winding number or maximum response diagram. The second difficulty is the minimisation of the slow global memory transactions and memory usage by exploiting the memory hierarchy of the GPU architecture. Finally, a good GPU solver has to treat the possible asynchronous features of the ODE systems efficiently; for instance, event detection occurring at distinct time instances or handling the orders of magnitude difference in the required number of time steps of the different ODE systems. The program package MPGOS (written in C++ and CUDA C software environments) can address the aforementioned issues easily via the addition of user-defined functions that must be implemented similarly to the right-hand side of the system; via the possibility of the definition of shared parameters common to all instances of the independent ODE systems; via user-programmable parameters to store only the desired properties of the trajectories; and via an easy was to overlap GPU and CPU computations. This paper focuses on the detailed description of the implementation strategies of the program package.}
}
@article{ALBA2006415,
title = {Efficient parallel LAN/WAN algorithms for optimization. The mallba project},
journal = {Parallel Computing},
volume = {32},
number = {5},
pages = {415-440},
year = {2006},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2006.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106000329},
author = {E. Alba and F. Almeida and M. Blesa and C. Cotta and M. Díaz and I. Dorta and J. Gabarró and C. León and G. Luque and J. Petit and C. Rodríguez and A. Rojas and F. Xhafa},
keywords = {Combinatorial optimization, Software engineering,  library, Exact techniques, Metaheuristics, Hybridization, Local and wide area implementations, Parallel algorithms},
abstract = {The mallba project tackles the resolution of combinatorial optimization problems using generic algorithmic skeletons implemented in C++. A skeleton in the mallba library implements an optimization method in one of the three families of generic optimization techniques offered: exact, heuristic and hybrid. Moreover, for each of those methods, mallba provides three different implementations: sequential, parallel for Local Area Networks, and parallel for Wide Area Networks. This paper introduces the architecture of the mallba library, details some of the implemented skeletons, and offers computational results for some classical optimization problems to show the viability of our library. Among other conclusions, we claim that the design used to develop the optimization techniques included in the library is generic and efficient at the same time.}
}
@article{FRIEDMANN200651,
title = {REUSABLE ARCHITECTURE AND TOOLS FOR TEAMS OF LIGHTWEIGHT HETEROGENEOUS ROBOTS},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {20},
pages = {51-56},
year = {2006},
note = {1st IFAC Workshop on Multivehicle Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20061002-2-BR-4906.00010},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015311861},
author = {Martin Friedmann and Jutta Kiener and Sebastian Petters and Dirk Thomas and Oskar {von Stryk}},
keywords = {Reusable robotic software, lightweight mobile autonomous robots, cooperation of heterogeneous robots, tools for debugging and monitoring, robot simulation},
abstract = {The software framework RoboFrame has been designed to meet the special requirements for teams of lightweight autonomous heterogeneous robot systems. Due to platform abstraction and modern object oriented design, it allows the reuse of components of common robot control software. It can also efficiently be implemented on new platforms and enables different control architectures for different tasks. For the exemplary application in autonomous robot soccer teams configurable and portable algorithms for vision, world modeling, behavior and motion control have been developed on top of the framework. For debugging, controlling and monitoring, an extendable graphical user interface and a generic simulator package have been implemented around the framework. Based on these instruments, different applications for homogeneous and heterogeneous robot teams can be realized in short time.}
}
@article{MONDAL2021100446,
title = {DarpanX: A python package for modeling X-ray reflectivity of multilayer mirrors},
journal = {Astronomy and Computing},
volume = {34},
pages = {100446},
year = {2021},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2020.100446},
url = {https://www.sciencedirect.com/science/article/pii/S2213133720301001},
author = {B. Mondal and S.V. Vadawale and N.P.S. Mithun and C.S. Vaishnava and N.K. Tiwari and S.K. Goyal and S.S. Panini and V. Navalkar and C. Karmakar and M.R. Patel and R.B. Upadhyay},
keywords = {X-ray astronomy, Instrumentation, X-ray optics, Multilayer mirrors},
abstract = {Multilayer X-ray mirrors consist of a coating of a large number of alternate layers of high Z and low Z materials with a typical thickness of 10–100  Å , on a suitable substrate. Such coatings play an important role in enhancing the reflectivity of X-ray mirrors by allowing reflections at angles much larger than the critical angle of X-ray reflection for the given materials. Coating with an equal thickness of each bilayer (constant period multilayers) enhances the reflectivity at discrete energies, satisfying Bragg condition for the given thickness. However, by systematically varying the bilayer thickness in the multilayer stack (depth graded multilayers), it is possible to design X-ray mirrors having enhanced reflectivity over a broad energy range. One of the most important applications of such a depth graded multilayer mirror is to realize hard X-ray telescopes for astronomical purposes. Design of such multilayer X-ray mirrors and their characterization with X-ray reflectivity measurements require appropriate software tools that can compute X-ray reflectivity for the given set of parameters and geometry. We have initiated the development of hard X-ray optics for future Indian X-ray astronomical missions, and in this context, we have developed a program, DarpanX, to calculate X-ray reflectivity for single and multilayer mirrors. It can be used as a stand-alone tool for designing multilayer mirrors with required characteristics. But more importantly, it has been implemented as a local model for the popular X-ray spectral fitting program, XSPEC, and thus can be used for accurate fitting of the experimentally measured X-ray reflectivity data. DarpanX is implemented as a Python 3 module, and an API is provided to access the underlying algorithms. Here we present details of DarpanX implementation and its validation for different types of multilayer structures. We also demonstrate the model fitting capability of DarpanX for experimental X-ray reflectivity measurements of single and multilayer samples.}
}
@article{DURO2021106851,
title = {Liger: A cross-platform open-source integrated optimization and decision-making environment},
journal = {Applied Soft Computing},
volume = {98},
pages = {106851},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106851},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620307894},
author = {João A. Duro and Yiming Yan and Ioannis Giagkiozis and Stefanos Giagkiozis and Shaul Salomon and Daniel C. Oara and Ambuj K. Sriwastava and Jacqui Morison and Claire M. Freeman and Robert J. Lygoe and Robin C. Purshouse and Peter J. Fleming},
keywords = {Software engineering, Multi-objective optimization, Multi-criteria decision-making, Evolutionary algorithms, Metaheuristics},
abstract = {Real-world optimization problems involving multiple conflicting objectives are commonly best solved using multi-objective optimization as this provides decision-makers with a family of trade-off solutions. However, the complexity of using multi-objective optimization algorithms often impedes the optimization process. Knowing which optimization algorithm is the most suitable for the given problem, or even which setup parameters to pick, requires someone to be an optimization specialist. The lack of supporting software that is readily available, easy to use and transparent can lead to increased design times and increased cost. To address these challenges, Liger is presented. Liger has been designed for ease of use in industry by non-specialists in optimization. The user interacts with Liger via a visual programming language to create an optimization workflow, enabling the user to solve an optimization problem. Liger contains a novel optimization library known as Tigon. The library utilizes the concept of design patterns to enable the composition of optimization algorithms by making use of simple reusable operator nodes. The library offers a varied range of multi-objective evolutionary algorithms which cover different paradigms in evolutionary computation; and supports a wide variety of problem types, including support for using more than one programming language at a time to implement the optimization model. Additionally, Liger functionality can be easily extended by plugins that provide access to state-of-the-art visualization tools and are responsible for managing the graphical user interface. Lastly, new user-driven interactive capabilities are shown to facilitate the decision-making process and are demonstrated on a control engineering optimization problem.}
}
@article{WIJAYARATHNA201954,
title = {Why Johnny can’t develop a secure application? A usability analysis of Java Secure Socket Extension API},
journal = {Computers & Security},
volume = {80},
pages = {54-73},
year = {2019},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818304887},
author = {Chamila Wijayarathna and Nalin Asanka Gamagedara Arachchilage},
keywords = {Security APIs, Transport Layer Security, Programmer experience, API usability, Java Secure Socket Extension},
abstract = {Lack of usability of security Application Programming Interfaces (APIs) is one of the main reasons for mistakes that programmers make that result in security vulnerabilities in software applications they develop. Especially, APIs that provide Transport Layer Security (TLS) related functionalities are sometimes too complex for programmers to learn and use. Therefore, applications are often diagnosed with vulnerable TLS implementations due to mistakes made by programmers. In this work, we evaluated the usability of Java Secure Socket Extension (JSSE) API to identify usability issues in it that persuade programmers to make mistakes while developing applications that would result in security vulnerabilities. We conducted a study with 11 programmers where each of them spent around 2 hours and attempted to develop a secure programming solution using JSSE API. From data we collected, we identified 59 usability issues that exist in JSSE API. Then we divided those usability issues into 15 cognitive dimensions and analyzed how those issues affected the experience of participant programmers. Results of our study provided useful insights about how TLS APIs and similar security APIs should be designed, developed and improved to provide a better experience for programmers who use them.}
}
@article{FENICLE2004343,
title = {A secure methodology for interchangeable services},
journal = {Information and Software Technology},
volume = {46},
number = {5},
pages = {343-349},
year = {2004},
note = {Special Issue on Software Engineering, Applications, Practices and Tools from the ACM Symposium on Applied Computing 2003},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2003.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584903002039},
author = {Brian Fenicle and Tim Wahls},
keywords = {E-commerce environments, Common Object Request Broker Architecture, Interoperability, Secure electronic transaction, Software licensing models},
abstract = {Computing today requires the use of many software packages, but only a few packages are used on a daily basis. This infrequent usage pattern often does not justify purchasing full licenses and therefore motivates a need for a more flexible way to use and pay for the usage of software. This paper describes a design philosophy in which similar services provide the same interface to clients. Services based on this design are interchangeable, allow payment per use, handle payment conveniently and securely, are platform independent, and frequently do not require local installation. Clients can therefore easily utilize resources based on application needs and services available at the time that the application is executing. An example implementation using this methodology is also discussed.}
}
@article{OUSSALAH2013105,
title = {A software architecture for Twitter collection, search and geolocation services},
journal = {Knowledge-Based Systems},
volume = {37},
pages = {105-120},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2012.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0950705112002055},
author = {M. Oussalah and F. Bhat and K. Challis and T. Schnier},
keywords = {Data mining, Tweet, Social network, Software architecture, Semantic analysis},
abstract = {The substantial increase of social networks and their combination with mobile devices make rigorous analysis of the outcomes of such system of paramount importance for intelligence gathering and decision making purposes. Since the introduction of Twitter system in 2006, tweeting emerged as an efficient open social network that attracted interest from various research/commercial and military communities. This paper investigates the current software architecture of Twitter system and put forward a new architecture dedicated for semantic and spatial analysis of Twitter data. Especially, Twitter Streaming API was used as a basis for tweet collection data stored in MySQL like database. While Lucene system together with WordNet lexical database linked to advanced natural language processing and PostGIS platform were used to ensure semantic and spatial analysis of the collected data. A functional diversity approach was implemented to enforce fault tolerance for the data collection part where its performances were evaluated through comparison with alternative approaches. The proposal enables the discovery of spatial patterns within geo-located Twitter and can provide the user or operator with useful unforeseen elements.}
}
@article{ANDRIANOV2021151,
title = {A brain-inspired cognitive support model for stress reduction based on an adaptive network model},
journal = {Cognitive Systems Research},
volume = {65},
pages = {151-166},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300838},
author = {Andrei Andrianov and S. Sahand Mohammadi Ziabari and Charlotte Gerritsen},
keywords = {Integrative model, Computational model, Temporal-causal network, Network-oriented modelling, Stress, Cognitive modelling},
abstract = {Stress is often seen as a negative factor which affects every individual’s life quality and decision making. To help avoid or deal with extreme emotions caused by an external stressor, a number of practices have been introduced. In the scope of this paper, we take three kinds of therapy into account: mindfulness, humor, and music therapy. This paper aims to see how various practices help people to cope with stress, using mathematical modelling. We present practical implementations in the form of client–server software, incorporating the computational model which describes therapy effects for overcoming stress based on quantitative neuropsychological research. The underlying network model simulates the elicitation of an extremely stressful emotion due to a strong stress-inducing event as an external stimulus, followed by a therapy practice simulation leading to a reduction of the stress level. Each simulation is based on user input and preferences, integrating a parameter tuning process; it fits a simulation for a particular user. The client–server architecture software which has been designed and developed completely fulfills this objective. It includes server part with embedded MATLAB interaction and API for client communication.}
}
@article{CHEN2009299,
title = {An architectural co-synthesis algorithm for energy-aware Network-on-Chip design},
journal = {Journal of Systems Architecture},
volume = {55},
number = {5},
pages = {299-309},
year = {2009},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2009.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1383762109000253},
author = {Yi-Jung Chen and Chia-Lin Yang and Yen-Sheng Chang},
keywords = {Network-on-Chip, Hardware–software co-synthesis, Energy-aware design},
abstract = {Network-on-Chip (NoC) has been proposed to overcome the complex on-chip communication problem of System-on-Chip (SoC) design in deep sub-micron. A complete NoC design contains exploration on both hardware and software architectures. The hardware architecture includes the selection of Processing Elements (PEs) with multiple types and their topology. The software architecture contains allocating tasks to PEs, scheduling of tasks and their communications. To find the best hardware design for the target tasks, both hardware and software architectures need to be considered simultaneously. Previous works on NoC design have concentrated on solving only one or two design parameters at a time. In this paper, we propose a hardware–software co-synthesis algorithm for a heterogeneous NoC architecture. The design goal is to minimize energy consumption while meeting the real-time requirements commonly seen in embedded applications. The proposed algorithm is based on Simulated-Annealing (SA). To compare the solution quality and efficiency of the proposed algorithm, we also implement the branch-and-bound and iterative algorithm to solve the hardware–software co-synthesis problem of a heterogeneous NoC. With the given synthetic task sets, the experimental results show that the proposed SA-based algorithm achieves near-optimal solution in a reasonable time, while the branch-and-bound algorithm takes a very long time to find the optimal solution, and the iterative algorithm fails to achieve good solution quality. When applying the co-synthesis algorithms to a real-world application with PE library that has little variation in PE performance and energy consumption, the iterative algorithm achieves solution quality comparable to that of the proposed SA-based algorithm.}
}
@incollection{WOLF2007383,
title = {Chapter 7 - Hardware and Software Co-design},
editor = {Wayne Wolf},
booktitle = {High-Performance Embedded Computing},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {383-432},
year = {2007},
isbn = {978-0-12-369485-0},
doi = {https://doi.org/10.1016/B978-012369485-0/50008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123694850500087},
author = {Wayne Wolf},
abstract = {Publisher Summary
The goal of hardware and software co-design is to search a very large design space to find reasonable system architectures for application-specific systems. To plausibly formulate this design space, let alone efficiently search it, typically some assumptions about the design are made. It may be working from a template, using a library of predesigned components, or using particular objective functions to drive a search. Even a few assumptions make the design problem much more tractable. Hardware/software co-design is not a pushbutton solution to embedded system design. The techniques described in earlier chapters of the book to implement many of the components of the embedded system identified by co-synthesis are still needed. Co-synthesis, however, can help organize the search for reasonable architectures. A variety of platforms can be used as the target for co-design, ranging from systems-on-chips to field programmable gate arrays (FPGAs). Hardware/software co-synthesis can start from either programs or task graphs. Platform-driven co-synthesis allocates processes onto predefined hardware architecture, tuning the parameters of that architecture during synthesis. Co-synthesis algorithms that do not rely on platforms must often make other assumptions—such as fixed-component libraries—to make their search spaces more reasonable. Reconfigurable systems complicate co-synthesis because scheduling and floor-planning interact. Hardware/software co-simulation links heterogeneous simulators together through a simulation bus to provide a uniform view of time across hardware and software execution.}
}
@article{KALUZ202017228,
title = {ELab: A Lightweight SCADA System for Control Engineering Research and Education},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17228-17233},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1757},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323636},
author = {Martin Kalúz and Ľuboš Čirka and Miroslav Fikar},
keywords = {Process Control, Education, Laboratories, SCADA, RESTful API, XBee, Microcontroller},
abstract = {This paper presents a lightweight SCADA system eLab that is based on open-source and affordable hardware/software technologies. The primary purpose of eLab is to provide an easy means for researchers and students to perform laboratory experiments, without requirements for extensive configuration on the side of the user. The architecture of the system consists of several functional parts. I/O nodes are hardware devices that directly connect and electrically control sensors and actuators of laboratory equipment. For this purpose, an implementer can either use a dedicated MCU, such as Arduino board, a single-board computer like Raspberry Pi, or any device with UART communication capabilities. The central part of eLab is a SCADA master, i.e., the computer that serves all the functionalities required by a SCADA system. The SCADA software is implemented in server-side JavaScript (Node.js). The communication between I/O nodes and SCADA master is served via XBee radio modules. The master computer acts as a communication gateway between I/O nodes and other parts of the system. The gateway provides a dedicated RESTful API that is used for the front-end connection to control software or HMI. Additionally, the system uses an internal database for configuration of experiments, tags, and data sessions. The eLab also provides a novel integration with DCore blockchain technology so that the users can store the data either in private or public blockchain network. The use of blockchain ensures the preservation, immutability, and verifiability of measured data.}
}
@article{VALCARCEL2010470,
title = {Real-time software for the COMPASS tokamak plasma control},
journal = {Fusion Engineering and Design},
volume = {85},
number = {3},
pages = {470-473},
year = {2010},
note = {Proceedings of the 7th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2010.03.049},
url = {https://www.sciencedirect.com/science/article/pii/S0920379610001201},
author = {D.F. Valcárcel and A.S. Duarte and A. Neto and I.S. Carvalho and B.B. Carvalho and H. Fernandes and J. Sousa and F. Sartori and F. Janky and P. Cahyna and M. Hron and R. Pánek},
keywords = {Real-time, ATCA, Data acquisition, Plasma control software},
abstract = {The COMPASS tokamak has started its operation recently in Prague and to meet the necessary operation parameters its real-time system, for data processing and control, must be designed for both flexibility and performance, allowing the easy integration of code from several developers and to guarantee the desired time cycle. For this purpose an Advanced Telecommunications Computing Architecture based real-time system has been deployed with a solution built on a multi-core x86 processor. It makes use of two software components: the BaseLib2 and the MARTe (Multithreaded Application Real-Time executor) real-time frameworks. The BaseLib2 framework is a generic real-time library with optimized objects for the implementation of real-time algorithms. This allowed to build a library of modules that process the acquired data and execute control algorithms. MARTe executes these modules in kernel space Real-Time Application Interface allowing to attain the required cycle time and a jitter of less than 1.5μs. MARTe configuration and data storage are accomplished through a Java hardware client that connects to the FireSignal control and data acquisition software. This article details the implementation of the real-time system for the COMPASS tokamak, in particular the organization of the control code, the design and implementation of the communications with the actuators and how MARTe integrates with the FireSignal software.}
}
@article{TANG2019153,
title = {Implementing railway vehicle dynamics simulation in general-purpose multibody simulation software packages},
journal = {Advances in Engineering Software},
volume = {131},
pages = {153-165},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818308421},
author = {Zhao Tang and Xiaolin Yuan and Xin Xie and Jie Jiang and Jianjun Zhang},
keywords = {Railway vehicle dynamics simulation, GPMBS software packages, Railway vehicles, Loose coupling method, Engineering software},
abstract = {In terms of simulation efficiency, modelling and visualization, general-purpose multibody simulation (GPMBS) software packages have a lot of inherent advantages compared with specialized railway vehicle dynamics simulation software packages. However, their application is limited due to the absence of specialized force elements and contact models, and thus cannot be directly employed for railway vehicle dynamics simulation. To deal with this issue, this paper proposes a novel framework to use GPMBS software packages to simulate the dynamic behavior of the railway vehicle. This framework contains a loose coupling method, a sub-mechanism feature and a top-down modelling method and allows users to take full advantage of the advanced functionalities of the GPMBS software packages to improve the efficiency of railway vehicle dynamics simulation. The feasibility of this framework is verified by integrating extensions for two representative GPMBS software packages. In addition, the two comparisons between the self-developed extensions (for LMS Virtual.Lab Motion and Unity3D) and the widely used specialized railway vehicle dynamics simulation software (Simpack and Universal Mechanism), further confirm the validity of the framework.}
}
@article{SZABOLICS20192343,
title = {New data acquisition control software developments for the simultaneous control of ten intelligent overview video cameras at W7-X},
journal = {Fusion Engineering and Design},
volume = {146},
pages = {2343-2347},
year = {2019},
note = {SI:SOFT-30},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2019.03.186},
url = {https://www.sciencedirect.com/science/article/pii/S0920379619305198},
author = {Tamás Szabolics and Gábor Cseh and Gábor Kocsis and Tamás Szepesi and Sándor Zoletnik and Christoph Biedermann and Ralf König and Aleix Puig Sitjes and Torsten Bluhm},
keywords = {Video diagnostics, Event Detection Intelligent Camera, EDICAM, Wendelstein 7-X, W7-X, Data acquisition and control software, Software development},
abstract = {In the past two campaigns of Wendelstein 7-X (W7-X) stellarator the overview video diagnostics played a key role in the daily experiments. The current software implementations went through numerous improvements and changes according to the continuously changing requirements. However, while the control software could handle all the needs, the changes reached a point where the redesign and re-implementation became necessary to ensure high reliability. In the next campaign of W7-X real-time data streaming will be implemented to W7-X’s Archive, which can be practically a limitless real-time data backup. Other improvements involve high-level interaction with the trigger-time-event (TTE) network, intense graphical user interface (GUI) improvements such as the grouping of cameras, visual display of measurement settings (e.g. exposure and readout timing). This new software package is arranged into two standalone pieces, similarly as in the previous version. One of the main upgrades is that the software will be able to send so-called event descriptions over the network during the measurement to a central event display, where events from several diagnostics will be visualized. This feature can provide essential information for machine operators, as they can visually monitor dangerous events inside the machine in real-time. This paper will present the detailed experiences, new requirements and detailed design, implementation and testing of this new software package.}
}
@article{SARRADJ201750,
title = {A Python framework for microphone array data processing},
journal = {Applied Acoustics},
volume = {116},
pages = {50-58},
year = {2017},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2016.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X16302808},
author = {Ennes Sarradj and Gert Herold},
keywords = {Microphone array, Beamforming, Python, Software},
abstract = {Acoular is an open source object-oriented Python package for microphone array data processing. It supports various methods for sound source characterization and mapping. The background of these methods, which rely on synchronously captured microphone signals, is shortly introduced, and the requirements for a software that implements these methods are discussed. The object-oriented design based on Python allows for easy-to-use scripting and graphical user interfaces, the practical combination with other data handling and scientific computing libraries, and the possibility to extend the software by implementing new processing methods with minimal effort. Built-in result caching and fast C++ based parallelized implementation of core routines is explained. Together with data handling procedures that can accommodate the huge amounts of measured data needed, this makes the application of Acoular to industrial-scale problems possible. Basic examples of Acoular use and extension are given.}
}
@article{BERTI2006110,
title = {GrAL—the grid algorithms library},
journal = {Future Generation Computer Systems},
volume = {22},
number = {1},
pages = {110-122},
year = {2006},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2003.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X0300205X},
author = {Guntram Berti},
keywords = {Mesh algorithms, Mesh data structures, PDE solution software, Generic programming, Software reuse},
abstract = {Dedicated library support for mesh-level geometry components, central to numerical PDE solution, is scarce. We claim that the situation is due to the inadequacy of traditional design techniques for complex and variable data representations typical for meshes. As a solution, we introduce an approach based on generic programming, implemented in the C++ library GrAL, whose algorithms are able to run on any mesh representation. We present the core design of GrAL and highlight some of its generic components. Finally, we discuss some practical issues of generic libraries, in particular efficiency and usability.}
}
@article{SANTAMARIAVAZQUEZ2023107357,
title = {MEDUSA©: A novel Python-based software ecosystem to accelerate brain-computer interface and cognitive neuroscience research},
journal = {Computer Methods and Programs in Biomedicine},
volume = {230},
pages = {107357},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107357},
url = {https://www.sciencedirect.com/science/article/pii/S016926072300024X},
author = {Eduardo Santamaría-Vázquez and Víctor Martínez-Cagigal and Diego Marcos-Martínez and Víctor Rodríguez-González and Sergio Pérez-Velasco and Selene Moreno-Calderón and Roberto Hornero},
keywords = {Brain-computer interfaces, Neurotechnology, Neuroscience, Electroencephalography},
abstract = {Background and objective: Neurotechnologies have great potential to transform our society in ways that are yet to be uncovered. The rate of development in this field has increased significantly in recent years, but there are still barriers that need to be overcome before bringing neurotechnologies to the general public. One of these barriers is the difficulty of performing experiments that require complex software, such as brain-computer interfaces (BCI) or cognitive neuroscience experiments. Current platforms have limitations in terms of functionality and flexibility to meet the needs of researchers, who often need to implement new experimentation settings. This work was aimed to propose a novel software ecosystem, called MEDUSA©, to overcome these limitations. Methods: We followed strict development practices to optimize MEDUSA© for research in BCI and cognitive neuroscience, making special emphasis in the modularity, flexibility and scalability of our solution. Moreover, it was implemented in Python, an open-source programming language that reduces the development cost by taking advantage from its high-level syntax and large number of community packages. Results: MEDUSA© provides a complete suite of signal processing functions, including several deep learning architectures or connectivity analysis, and ready-to-use BCI and neuroscience experiments, making it one of the most complete solutions nowadays. We also put special effort in providing tools to facilitate the development of custom experiments, which can be easily shared with the community through an app market available in our website to promote reproducibility. Conclusions: MEDUSA© is a novel software ecosystem for modern BCI and neurotechnology experimentation that provides state-of-the-art tools and encourages the participation of the community to make a difference for the progress of these fields. Visit the official website at https://www.medusabci.com/ to know more about this project.}
}
@article{ZAPALOWSKI2018125,
title = {The WGB method to recover implemented architectural rules},
journal = {Information and Software Technology},
volume = {103},
pages = {125-137},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301253},
author = {Vanius Zapalowski and Ingrid Nunes and Daltro José Nunes},
keywords = {Software architecture, Architectural rule, Source code dependency, Architecture recovery},
abstract = {Context: The identification of architectural rules, which specify allowed dependencies among architectural modules, is a key challenge in software architecture recovery. Existing approaches either retrieve a large set of rules, compromising their practical use, or are limited to supporting the understanding of such rules, which are manually recovered. Objective: To propose and evaluate a method to recover architectural rules, focusing on those implemented in the source code, which may differ from planned or conceptual rules. Method: We propose the WGB method, which analyzes dependencies among architectural modules as a graph, adding weights that correspond to the proposed module dependency strength (MDS) metric and identifies the set of implemented architectural rules by solving a mathematical optimization problem. We evaluated our method with a case study and an empirical study that compared rules extracted by the method with the conceptual architecture and source code dependencies of six systems. These comparisons considered efficiency and effectiveness of our method. Results: Regarding efficiency, our method took 45.55 s to analyze the largest system evaluated. Considering effectiveness, our method captured package dependencies as extracted rules with a reduction of 87.6%, on average, to represent this information. Using allowed architectural dependencies as a reference point (but not a gold standard), provided rules achieved 37.1% of precision and 37.8% of recall. Conclusion: Our empirical evaluation shows that the implemented architectural rules recovered by our method consist of abstract representations of (a large number of) module dependencies, providing a concise view of dependencies that can be inspected by developers to identify occurrences of architectural violations and undocumented rules.}
}
@article{KAMENSKY2019477,
title = {tIGAr: Automating isogeometric analysis with FEniCS},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {344},
pages = {477-498},
year = {2019},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0045782518304985},
author = {David Kamensky and Yuri Bazilevs},
keywords = {Isogeometric analysis, Software, , NURBS, T-splines, Bézier extraction},
abstract = {This paper introduces tIGAr, a library for using the open-source finite element (FE) automation software FEniCS to perform isogeometric analysis (IGA). The library uses a global variant of Bézier extraction to avoid modifying the finite element assembly procedures of FEniCS. This makes much of the convenient functionality of FEniCS directly available for IGA. General rational splines are implemented through an abstraction that sees only an extracted representation of the IGA function space. Through this abstraction, an enormous variety of spline spaces can be used for analysis, so long as a corresponding preprocessor is developed for each one, implementing a simple interface. As examples, we discuss preprocessors for B-splines specified analytically, non-uniform rational B-splines (NURBS) designed using the open-source software igakit, and T-splines designed using commercial software. We then demonstrate the implementation of solvers for several non-trivial partial differential equations that benefit from IGA. We also evaluate the parallel performance of tIGAr on a distributed-memory supercomputer. Finally, we outline possibilities for further development of IGA in FEniCS. Source code for tIGAr is continuously updated online at https://github.com/david-kamensky/tIGAr.}
}
@article{BIANCOF2013352,
title = {Fast Coding of LTE eNB L1 API Definition},
journal = {Procedia Technology},
volume = {7},
pages = {352-359},
year = {2013},
note = {3rd Iberoamerican Conference on Electronics Engineering and Computer Science, CIIECC 2013},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2013.04.044},
url = {https://www.sciencedirect.com/science/article/pii/S2212017313000455},
author = {José A. {Bianco F.} and Karlo G. Lenzi and Felipe A.P. {de Figueiredo}},
keywords = {FAPI, LTE, eNodeB, perl, FPGA, VHDL},
abstract = {Like other modern systems a LTE eNodeB has a software layer and a hardware layer. Both these layers shall be connected through a fast application programming interface (API) not to compromise the performance of the whole system. When there is no hard-processor available on hardware side and the whole API must be implemented in FPGA, the design process takes too much time. This work presents a systematic way to fast code Small Cell Forum LTE eNB L1 API Definition [1] by using perl scripts. Our results indicates a series of advantages over traditional methods like a code to script character relation of 7.79 to 1, a cleaner code to support and reliability. These results points out a design methodology that can be applied on several coding abstractions or languages achieving similar results as the ones founded here.}
}
@article{BEYRANVANDNEJAD2013424,
title = {A hardware/software platform for QoS bridging over multi-chip NoC-based systems},
journal = {Parallel Computing},
volume = {39},
number = {9},
pages = {424-441},
year = {2013},
note = {Novel On-Chip Parallel Architectures and Software Support},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2013.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167819113000550},
author = {Ashkan {Beyranvand Nejad} and Anca Molnos and Matias {Escudero Martinez} and Kees Goossens},
keywords = {Distributed SoCs, Bridge architecture, Networks-on-Chip, Quality of service, Prototype, Verification},
abstract = {Recent embedded systems integrate a growing number of intellectual property cores into increasingly large designs. Implementation, prototyping, and verification of such large systems has become very challenging. One of the reasons is that chips/FPGAs resources are limited and therefore it is not always possible to implement the whole design in the traditional system-on-a-chip solutions. The state-of-the-art is to partition such systems into smaller sub-systems to implement each on a separate chip. Consequently, it requires interconnecting separate chips/FPGAs. Since Networks-on-Chip (NoCs) have become common interconnection solutions in embedded designs, we propose to bridge NoC-based SoCs enabling a generic multi-chip systems interconnection. In this context, the contribution of this paper is threefold, (i) we explore the NoC protocol stack to determine the best layer for implementing the off-chip bridge, (ii) we propose a generic hardware architecture for the bridge, and (iii) we develop a new software architecture enabling seamless configuration and communication of multi-chip NoC-based SoCs. Finally, we demonstrate performance, i.e., bandwidth and latency, of the bridge in a multi-FPGA platform, while the bridge guarantees QoS of traffic. The synthesis results indicate the implementation area cost of the bridge is only 1% of Xilinx Virtex6 FPGA.}
}
@article{PATALI2020617,
title = {High throughput and energy efficient FIR filter architectures using retiming and two level pipelining},
journal = {Procedia Computer Science},
volume = {171},
pages = {617-626},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.067},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920310346},
author = {Pramod Patali and Shahana Thottathikkulam Kassim},
keywords = {Critical path, Digital synthesis, FIR filter, Latency, Pipelining, Parallel processing, Retiming, RTL compiler, Standard cell library},
abstract = {A methodology to improve the throughput and energy efficiency of finite impulse response (FIR) filters through the effective application of retiming and two-level pipelining is presented in this paper. Improving throughput and energy efficiency of the filter while minimizing latency and hardware complexity is a challenge to be addressed. Two-level pipelining bifurcate the multiplication and addition operations. Retiming is applied to break addition operation. The basic architecture of a 64-tap filter consists of 4-tap pipelined retiming delay generation sub filter (PRF), a 4-tap pipelined sub filter (PSF) and 7 numbers of 8-tap pipelined sub filters. The critical path delay (CPD), effective latency (ELT), power-delay product (PDP) and area-delay product (ADP) of the proposed filter-1 is improved by 35%, 33.97%, 38.06% and 29.67% in comparison with the recent high throughput retimed filter (HTF1) for a filter length of 64. Cadence software with gpdk 45nm standard cell library is used for the design and implementation. An ECG denoising low pass filter is developed using the proposed filter-1 and performed simulation using modelsim.}
}
@article{BRUNN2021149,
title = {Fast GPU 3D diffeomorphic image registration},
journal = {Journal of Parallel and Distributed Computing},
volume = {149},
pages = {149-162},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S074373152030407X},
author = {Malte Brunn and Naveen Himthani and George Biros and Miriam Mehl and Andreas Mang},
keywords = {GPU computing, Parallel optimization, Diffeomorphic image registration, Mixed-precision solver, Gauss–Newton–Krylov method},
abstract = {3D image registration is one of the most fundamental and computationally expensive operations in medical image analysis. Here, we present a mixed-precision, Gauss–Newton–Krylov solver for diffeomorphic registration of two images. Our work extends the publicly available CLAIRE library to GPU architectures. Despite the importance of image registration, only a few implementations of large deformation diffeomorphic registration packages support GPUs. Our contributions are new algorithms to significantly reduce the run time of the two main computational kernels in CLAIRE: calculation of derivatives and scattered-data interpolation. We deploy (i) highly-optimized, mixed-precision GPU-kernels for the evaluation of scattered-data interpolation, (ii) replace Fast-Fourier-Transform (FFT)-based first-order derivatives with optimized 8th-order finite differences, and (iii) compare with state-of-the-art CPU and GPU implementations. As a highlight, we demonstrate that we can register 2563 clinical images in less than 6 s on a single NVIDIA Tesla V100. This amounts to over 20× speed-up over the current version of CLAIRE and over 30× speed-up over existing GPU implementations.}
}
@article{SEINSTRA2002967,
title = {A software architecture for user transparent parallel image processing},
journal = {Parallel Computing},
volume = {28},
number = {7},
pages = {967-993},
year = {2002},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(02)00103-5},
url = {https://www.sciencedirect.com/science/article/pii/S0167819102001035},
author = {F.J. Seinstra and D. Koelma and J.M. Geusebroek},
keywords = {Data parallel image processing library, Parallelizable patterns, Abstract parallel image processing machine, Performance modeling, Homogeneous MIMD-style multicomputers},
abstract = {This paper describes a software architecture that allows image processing researchers to develop parallel applications in a transparent manner. The architecture's main component is an extensive library of data parallel low level image operations capable of running on homogeneous distributed memory MIMD-style multicomputers. Since the library has an application programming interface identical to that of an existing sequential library, all parallelism is completely hidden from the user. The first part of the paper discusses implementation aspects of the parallel library, and shows how sequential as well as parallel operations are implemented on the basis of so-called parallelizable patterns. A library built in this manner is easily maintainable, as extensive code redundancy is avoided. The second part of the paper describes the application of performance models to ensure efficiency of execution on all target platforms. Experiments show that for a realistic application performance predictions are highly accurate. These results indicate that the core of the architecture forms a powerful basis for automatic parallelization and optimization of a wide range of imaging software.}
}
@article{YAZDANPANAH2015130,
title = {Picos: A hardware runtime architecture support for OmpSs},
journal = {Future Generation Computer Systems},
volume = {53},
pages = {130-139},
year = {2015},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14002702},
author = {Fahimeh Yazdanpanah and Carlos Álvarez and Daniel Jiménez-González and Rosa M. Badia and Mateo Valero},
keywords = {Hardware implementation, Task scheduling, Dataflow execution, Parallel programming model, OmpSs, OpenMP},
abstract = {OmpSs is a programming model that provides a simple and powerful way of annotating sequential programs to exploit heterogeneity and task parallelism based on runtime data dependency analysis, dataflow scheduling and out-of-order task execution; it has greatly influenced Version 4.0 of the OpenMP standard. The current implementation of OmpSs achieves those capabilities with a pure-software runtime library: Nanos++. Therefore, although powerful and easy to use, the performance benefits of exploiting fine-grained (pico) task parallelism are limited by the software runtime overheads. To overcome this handicap we propose Picos, an implementation of the Task Superscalar (TSS) architecture that provides hardware support to the OmpSs programming model. Picos is a novel hardware dataflow-based task scheduler that dynamically analyzes inter-task dependencies and identifies task-level parallelism at run-time. In this paper, we describe the Picos Hardware Design and the latencies of the main functionality of its components, based on the synthesis of their VHDL design. We have implemented a full cycle-accurate simulator based on those latencies to perform a design exploration of the characteristics and number of its components in a reasonable amount of time. Finally, we present a comparison of the Picos and Nanos++ runtime performance scalability with a set of real benchmarks. With Picos, a programmer can achieve ideal scalability using aggressive parallel strategies with a large number of fine granularity tasks.}
}
@article{SOTIROPOULOS2020441,
title = {High performance topology optimization computing platform},
journal = {Procedia Manufacturing},
volume = {44},
pages = {441-448},
year = {2020},
note = {The 1st International Conference on Optimization-Driven Architectural Design (OPTARCH 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.272},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920308593},
author = {Stefanos Sotiropoulos and Georgios Kazakis and Nikos D. Lagaros},
keywords = {Topology optimization, SAP2000, HP-OCP, C#},
abstract = {One of the most challenging tasks in the construction industry nowadays, is to reduce the material demands and distribute, in the same time, the material among the structural system in the best possible way. Topology optimization is a design procedure that is increasingly used, to generate optimized forms of structures in several engineering fields. The current paper presents the Topology Optimization (TO) module of the High-Performance Optimization Computing Platform (HP-OCP) which focuses on civil engineering problems. More specifically the SIMP method [1] is implemented and the topology optimization problem is solved by using the OC algorithm. The HP-OCP is a platform which evaluates several objective functions, such as the volume of the structure, the compliance etc. and can solve constrained or unconstrained structural optimization problems. The above libraries are developed in C#. The core of the platform is created in such way that it can be integrated with any CAE program that has OAPI, XML or any other type of data exchange format. In the proposed work the structural analysis and design software SAP2000 is used. Theoretical aspects are discussed in order to implement the mathematical formulation in a commercial software. Basic and specific features are applied and representative examples are performed. One of the highlights of the proposed work is that the above module can be used for all kind of finite elements. Benchmark tests are presented with structures that are simulated by 2D plane-stress elements, 3D-solid elements and shell elements. Furthermore, it is independent of the type of the mesh, structured or unstructured, so both examples are presented. In the proposed work a powerful tool for both architects and civil engineers is introduced. The analysis and design of the structures are performed in SAP2000 software, in order to achieve a realistic result that could be a solution for a real-world structure.}
}
@article{DALY2022105514,
title = {Quo vadis, agent-based modelling tools?},
journal = {Environmental Modelling & Software},
volume = {157},
pages = {105514},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105514},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002146},
author = {Aisling J. Daly and Lander {De Visscher} and Jan M. Baetens and Bernard {De Baets}},
keywords = {Agent-based models, Simulation, Model analysis, Inference, Calibration},
abstract = {Agent-based models (ABMs) are an increasingly popular choice for simulating large systems of interacting components, and have been applied across a wide variety of natural and environmental systems. However, ABMs can be incredibly disparate and often opaque in their formulation, implementation, and analysis. This can impede critical assessment and re-implementation, and jeopardize the reproducibility and conclusions of ABM studies. In this review, we survey recent work towards standardization in ABM methodology in several aspects: model description and documentation, model implementation, and model analysis and inference. Based on a critical review of the literature, focused on ABMs of environmental and natural systems, we describe a recurrent trade-off between flexibility and standardization in ABM methodology. We find that standard protocols for model documentation are beginning to establish, although their uptake by the ABM community is inhibited by their sometimes excessive level of detail. We highlight how implementation options now exist at all points along a spectrum from ad hoc, ‘from scratch’ implementations, to specific software offering ‘off-the-shelf’ ABM implementations. We outline how the main focal points of ABM analysis (behavioural and inferential analysis) are facing similar issues with similar approaches. While this active development of ABM analysis techniques brings additional methods to our analysis toolbox, it does not contribute to the development of a standardized framework, since the performance and design of these methods tends to be highly problem-specific. We therefore recommend that agent-based modellers should consider multiple approaches simultaneously when analysing their model. Well-documented software packages, and critical comparative reviews of such, will be important facilitators in these advances. ABMs can additionally make better use of developments in other fields working with high-dimensional problems, such as Bayesian statistics and machine learning.}
}
@article{GLEADALL2021102109,
title = {FullControl GCode Designer: Open-source software for unconstrained design in additive manufacturing},
journal = {Additive Manufacturing},
volume = {46},
pages = {102109},
year = {2021},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102109},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421002748},
author = {Andrew Gleadall},
keywords = {DfAM, Calibration, Toolpath, Manufacturing plan, Slicer software},
abstract = {A new concept is presented for the design of additive manufacturing procedures, which is implemented in open-source software called FullControl GCode Designer. In this new design approach, the user defines every segment of the print-path along with all printing parameters, which may be related to geometric and non-geometric factors, at all points along the print-path. Machine control code (GCode) is directly generated by the software, without the need for any programming skills and without using computer-aided design (CAD), STL-files or slicing software. Excel is used as the front end for the software, which is written in Visual Basic. Case studies are used to demonstrate the broad range of structures that can be designed using the software, including: precisely controlled specimens for printer calibration, parametric specimens for hardware characterisation utilising hundreds of unique parameter combinations, novel mathematically defined lattice structures, and previously inconceivable 3D geometries that are impossible for traditional slicing software to achieve. The FullControl design approach enables unconstrained freedom to create nonplanar 3D print-paths and break free from traditional restrictions of layerwise print-path planning. It also allows nozzle movements to be carefully designed - both during extrusion and while travelling between disconnected extrusion volumes - to overcome inherent limitations of the printing process or to improve capabilities for challenging materials. An industrial case study shows how explicit print-path design improved printer reliability, production time, and print quality for a production run of over 1000 parts. FullControl GCode Designer offers a general framework for unconstrained design and is not limited to a particular type of structure or hardware; transferability to lasers and other manufacturing processes is discussed. Parametric design files use a few bytes or kilobytes of data to describe all details that are sent to the printer, which greatly improves shareability by eliminating any risk of errors being introduced during STL file conversion or due to different users having inconsistent slicer settings. Adjustable parameters allow GCode for revised designs to be produced instantly, instead of the laborious traditional routine using multiple software packages and file conversions. The FullControl design concept offers new opportunities for creative and high-precision use of additive manufacturing systems. It facilitates design for additive manufacturing (DfAM) at the smallest possible scale based on the fundamental nature of the process (i.e. assembly of individual extrusions). The software and source code are provided as supplementary data and ongoing updates to improve functionality and the user interface will be available at www.fullcontrolgcode.com.}
}
@article{VICTORIA2016126,
title = {liteITD a MATLAB Graphical User Interface (GUI) program for topology design of continuum structures},
journal = {Advances in Engineering Software},
volume = {100},
pages = {126-147},
year = {2016},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2016.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0965997816301995},
author = {Mariano Victoria and Osvaldo M. Querin and Concepción Díaz and Pascual Martí},
keywords = {MATLAB GUI, liteITD algorithm, Isolines, Topology design, Continuum structures},
abstract = {Over the past few decades, topology optimization has emerged as a powerful and useful tool for the design of structures, also exploiting the ever growing computational speed and power. The design process has also been affected by computers which changed the concept of form into the concept of formation and the emergence of digital design. Topology optimization can modify existing designs, incorporate explicit features into a design and generate completely new designs. This paper will show how topology optimization can be used as a digital tool. The liteITD (lite version of Isolines Topology Design) software package will be described with the purpose of providing a tool for topology design. The liteITD program solves the topology optimization of two-dimensional continuum structures using von Mises stress isolines under single or multiple loading conditions, with different material properties in tension and compression, and multiple materials. The liteITD program is fully implemented in the MATrix LABoratory (MATLAB) software environment of MathWorks under Windows operating system. GUIDE (Graphical User Interface Development Environment) was used to create a friendly Graphical User Interface (GUI). The usage of this application is directed to students mainly (educational purposes), although also to designers and engineers with experience. The liteITD program can be downloaded and used for free from the website: http://www.upct.es/goe/software/liteITD.php.}
}
@article{BERGAMASCO201728,
title = {WASS: An open-source pipeline for 3D stereo reconstruction of ocean waves},
journal = {Computers & Geosciences},
volume = {107},
pages = {28-36},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417304302},
author = {Filippo Bergamasco and Andrea Torsello and Mauro Sclavo and Francesco Barbariol and Alvise Benetazzo},
keywords = {Sea surface stereo reconstruction, Stereo reconstruction, 3D wave field, Camera calibration, Open-source software},
abstract = {Stereo 3D reconstruction of ocean waves is gaining more and more popularity in the oceanographic community and industry. Indeed, recent advances of both computer vision algorithms and computer processing power now allow the study of the spatio-temporal wave field with unprecedented accuracy, especially at small scales. Even if simple in theory, multiple details are difficult to be mastered for a practitioner, so that the implementation of a sea-waves 3D reconstruction pipeline is in general considered a complex task. For instance, camera calibration, reliable stereo feature matching and mean sea-plane estimation are all factors for which a well designed implementation can make the difference to obtain valuable results. For this reason, we believe that the open availability of a well tested software package that automates the reconstruction process from stereo images to a 3D point cloud would be a valuable addition for future researches in this area. We present WASS (http://www.dais.unive.it/wass), an Open-Source stereo processing pipeline for sea waves 3D reconstruction. Our tool completely automates all the steps required to estimate dense point clouds from stereo images. Namely, it computes the extrinsic parameters of the stereo rig so that no delicate calibration has to be performed on the field. It implements a fast 3D dense stereo reconstruction procedure based on the consolidated OpenCV library and, lastly, it includes set of filtering techniques both on the disparity map and the produced point cloud to remove the vast majority of erroneous points that can naturally arise while analyzing the optically complex nature of the water surface. In this paper, we describe the architecture of WASS and the internal algorithms involved. The pipeline workflow is shown step-by-step and demonstrated on real datasets acquired at sea.}
}
@article{WANG2022443,
title = {Face.evoLVe: A cross-platform library for high-performance face analytics},
journal = {Neurocomputing},
volume = {494},
pages = {443-445},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.118},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005057},
author = {Qingzhong Wang and Pengfei Zhang and Haoyi Xiong and Jian Zhao},
keywords = {Face analytics, Toolbox, Library, PyTorch, PaddlePaddle},
abstract = {We develop face.evoLVe—a comprehensive library that collects and implements a wide range of popular deep learning-based methods for face recognition. The motivation of the software is to lower the technical burdens in reproducing the existing methods for comparison, while users of our library could focus on developing advanced approaches more efficiently. More specifically, Face.evoLVe is well designed with an extensible framework under vibrantly evolving, so that new face recognition approaches can be easily plugged into our framework. The library is available at https://github.com/ZhaoJ9014/face.evoLVe. Face.evoLVe has been widely used for face analytics, receiving 2,700 stars and 683 forks and we have used Face.evoLVe to participate in a number of face recognition competitions and secured the first place.}
}
@article{TANG2012584,
title = {Design and Implementation of Reusable Components Using PowerBuilder},
journal = {Procedia Engineering},
volume = {29},
pages = {584-588},
year = {2012},
note = {2012 International Workshop on Information and Electronics Engineering},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2012.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877705812000185},
author = {Zhenjun Tang and Xianquan Zhang and Liyan Huang},
keywords = {Software reuse, Component, Object-oriented technology, PowerBuilder (PB), Datawindow},
abstract = {Component technology is a key technology of software reuse. This paper investigates PowerBuilder based technology of software reuse, especially the technology of component design. To build a reusable component, reusable elements in the application system are firstly extracted. The reusable components are then used to form a reusable component library. When designing application system, suitable components are selected from the reusable library and then instantiated. Software system is implemented by composing the instanced components under a reusable framework. Practical results show that the use of reusable components can improve the efficiency of software development.}
}
@article{VIDOTTO2022110468,
title = {Software defined radio for vector network analysis: Configuration, characterization and calibration},
journal = {Measurement},
volume = {189},
pages = {110468},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2021.110468},
url = {https://www.sciencedirect.com/science/article/pii/S0263224121013531},
author = {Mauro I. Vidotto and Francisco E. Veiras and Patricio A. Sorichetti},
keywords = {Vector network analysis, Software defined radio, Portable instruments, S-parameter characterization, Dielectric spectroscopy},
abstract = {A portable vector network analyzer operating from 30 MHz to 850 MHz, using a commercial low-cost software-defined radio is described. The system can be used for transmission (S21) or, using a directional bridge, reflection measurements (S11). A complete software package is developed in order to define and operate the system as an automatic analyzer including the correction of systematic errors. The analyzer has heterodyne receivers with an specific intermediate frequency, digitizer sampling rate and block size chosen to correctly process the signals. Measurements to characterize the transmitters and receivers on the board are shown. System errors are determined by comparison with laboratory equipment measurements and manufacturer data.}
}
@article{YAN2020102279,
title = {An open line architecture to enable white-box optical ecosystem},
journal = {Optical Fiber Technology},
volume = {58},
pages = {102279},
year = {2020},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2020.102279},
url = {https://www.sciencedirect.com/science/article/pii/S1068520020302698},
author = {Boyuan Yan and Andrea Campanella and Alessio Giorgetti and Yongli Zhao and Jie Zhang},
keywords = {Open optical network, Software-defined networking, Open line system, White box},
abstract = {The white-box optical ecosystem is considered as the future trend in transport networks, which includes the openness in both software and hardware. The openness in hardware decomposes the equipment into multiple independent components to reduce costs. And the openness in software including general YANG models make all components controllable, which enables fast innovation in the control plane of software-defined networking (SDN). In this paper, an open line architecture (OLA) is proposed to implement the openness and disaggregation to enable white-box optical ecosystem. OLA defines a transport application interface (TAPI) to provide a flexible control of the open line system. To build the physical network, OLA uses the optical equipment developed by different manufacturers that follows specific YANG specifications. And in the control plane, OLA develops a series of drivers and applications to control the calculation, creation, and deletion of optical connectivity service. Finally, the paper shows OLA in use as part of a testbed, which demonstrates the feasibility of building networks consisting of both open and disaggregated hardware.}
}
@article{KHALAFINEJAD20131017,
title = {Translation of Z specifications to executable code: Application to the database domain},
journal = {Information and Software Technology},
volume = {55},
number = {6},
pages = {1017-1044},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S095058491200242X},
author = {Saeed Khalafinejad and Seyed-Hassan Mirian-Hosseinabadi},
keywords = {Prototyping, Database, Formal methods, Software development, Graphical user interface},
abstract = {Context
It is well-known that the use of formal methods in the software development process results in high-quality software products. Having specified the software requirements in a formal notation, the question is how they can be transformed into an implementation. There is typically a mismatch between the specification and the implementation, known as the specification-implementation gap.
Objective
This paper introduces a set of translation functions to fill the specification-implementation gap in the domain of database applications. We only present the formal definition, not the implementation, of the translation functions.
Method
We chose Z, SQL and Delphi languages to illustrate our methodology. Because the mathematical foundation of Z has many properties in common with SQL, the translation functions from Z to SQL are derived easily. For the translation of Z to Delphi, we extend Delphi libraries to support Z mathematical structures such as sets and tuples. Then, based on these libraries, we derive the translation functions from Z to Delphi. Therefore, we establish a formal relationship between Z specifications and Delphi/SQL code. To prove the soundness of the translation from a Z abstract schema to the Delphi/SQL code, we define a Z design-level schema. We investigate the consistency of the Z abstract schema with the Z design-level schema by using Z refinement rules. Then, by the use of the laws of Morgan refinement calculus, we prove that the Delphi/SQL code refines the Z design-level schema.
Results
The proposed approach can be used to build the correct prototype of a database application from its specification. This prototype can be evolved, or may be used to validate the software requirements specification against user requirements.
Conclusion
Therefore, the work presented in this paper reduces the overall cost of the development of database applications because early validation reveals requirement errors sooner in the software development cycle.}
}
@article{SCHAFFER2021207,
title = {Process-Driven Approach within the Engineering Domain by Combining Business Process Model and Notation (BPMN) with Process Engines},
journal = {Procedia CIRP},
volume = {96},
pages = {207-212},
year = {2021},
note = {8th CIRP Global Web Conference – Flexible Mass Customisation (CIRPe 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.076},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121001037},
author = {Eike Schäffer and Volker Stiehl and Peter K. Schwab and Andreas Mayr and Josef Lierhammer and Jörg Franke},
keywords = {Process-Driven Approach, PDA, BPMN, engineering, process engine, architecture, orchestration, Industry 4.0},
abstract = {Digitization within the framework of Industry 4.0 is considered the biggest and fastest driver of change in history of manufacturing industry. While the size of a company is becoming less essential, the ability to adapt quickly to changing market conditions and new technologies is more important than ever. This trend particularly applies to the companies’ software landscapes, where individual sub-processes and services must be orchestrated, seamlessly integrated, and iteratively renewed according to the ever-increasing user requirements. However, inflexible, closed monolithic software applications as well as self-programmed stand-alone tools that are difficult to integrate are still predominant in the engineering domain. A complete reimplementation of existing, proprietary engineering tools and their integration into monolithic applications of large software providers is often not economically feasible, especially for small and medium-sized machinery and plant manufacturers. In this context, the so-called Process-Driven Approach (PDA) offers a sustainable and tool-neutral opportunity for process and tool orchestration, enabling an easy integration of individual software applications by consistent utilization of the separation of concerns principle. The PDA, originating from business informatics, is mainly based on the standardized and machine-executable visual modeling language Business Process Model and Notation (BPMN). Using the semantic enhancements found in version 2.0, BPMN is not just used to model the business processes but also to model and execute the integration processes between different systems. After the PDA has already been successfully applied to large-scale projects in business informatics, it is now being transferred to the engineering domain. As shown in this paper, PDA allows to orchestrate the different processes in engineering and to integrate the underlying software tools, such as e-mail or spreadsheet applications, engineering tools, or custom microservices, using standardized interfaces like REST API. In doing so, engineering processes can be made more transparent, monitored, and optimized by means of appropriate key figures. The concept is validated by a prototypical implementation of a minimum functional PDA architecture for the engineering domain.}
}
@article{CARRE2019102860,
title = {Exhaustive single bit fault analysis. A use case against Mbedtls and OpenSSL’s protection on ARM and Intel CPU},
journal = {Microprocessors and Microsystems},
volume = {71},
pages = {102860},
year = {2019},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2019.102860},
url = {https://www.sciencedirect.com/science/article/pii/S0141933118305143},
author = {Sébastien Carré and Matthieu Desjardins and Adrien Facon and Sylvain Guilley},
keywords = {Bellcore, Fault attack, OpenSSL, MBedTLS, ARM, Intel},
abstract = {Faults in software implementations target both data and instructions at different locations. Bellcore attack is a well-known fault attack that is able to break CRT-RSA. In response, cryptographic libraries such as OpenSSL are designed with protection. In this paper, new faults locations are shown on OpenSSL implementation of the CRT-RSA signature running on Intel and ARM processors. Among those faults, one restores the Bellcore attack on the OpenSSL library despite a protection and another is a complete new fault that exploits the OpenSSL protection to get RSA private key. Quite surprisingly, one of the exhibited faults is, ironically enough, made possible because of the existence of such protection. Mbedtls library is also analyzed in this paper. A way to find all exploitable faults on monobit flip fault model is also detailed.}
}
@article{SMOLJANOVIC2021111964,
title = {Finite strain numerical model for the nonlinear analysis of thin shells},
journal = {Engineering Structures},
volume = {234},
pages = {111964},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.111964},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621001140},
author = {Hrvoje Smoljanović and Ivan Balić and Boris Trogrlić and Nikolina Živaljić and Ante Munjiza},
keywords = {Finite strains, Geometric nonlinearity, Material nonlinearity, Numerical model, Thin shell structures, Three-noded rotation-free finite elements},
abstract = {The paper presents a new numerical model for analysis of thin shell structures taking into account geometrical and material nonlinearity, finite displacements, finite rotations and finite strains. The model is based on three-noded rotation-free triangular finite elements and is implemented in the open source Y-FDEM software package based on the Combined finite-discrete element method (FDEM). Material nonlinearity was implemented considering the von Mises flow condition, the Levi-Mises flow rule, and the Johnson-Cook constitutive law. The efficiency of the model was demonstrated on the benchmark problems by comparing numerical results with known analytical or other numerical results.}
}
@article{RUNGER201067,
title = {Fast recursive matrix multiplication for multi-core architectures},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {67-76},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000104},
author = {Gudula Rünger and Michael Schwind},
abstract = {In this article, we present a fast algorithm for matrix multiplication optimized for recent multicore architectures. The implementation exploits different methodologies from parallel programming, like recursive decomposition, efficient low-level implementations of basic blocks, software prefetching, and task scheduling resulting in a multilevel algorithm with adaptive features. Measurements on different systems and comparisons with GotoBLAS, Intel Math Kernel Library (IMKL), and AMD Core Math Library (AMCL) show that the matrix implementation presented has a very high efficiency.}
}
@article{VANBINSBERGEN2020100945,
title = {Purely functional GLL parsing},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100945},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100945},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300058},
author = {L. Thomas {van Binsbergen} and Elizabeth Scott and Adrian Johnstone},
keywords = {Top-down parsing, Generalised parsing, Parser combinators, Syntax descriptions, Functional programming},
abstract = {Generalised parsing has become increasingly important in the context of software language design and several compiler generators and language workbenches have adopted generalised parsing algorithms such as GLR and GLL. The original GLL parsing algorithms are described in low-level pseudo-code as the output of a parser generator. This paper explains GLL parsing differently, defining the FUN-GLL algorithm as a collection of pure, mathematical functions and focussing on the logic of the algorithm by omitting implementation details. In particular, the data structures are modelled by abstract sets and relations rather than specialised implementations. The description is further simplified by omitting lookahead and adopting the binary subtree representation of derivations to avoid the clerical overhead of graph construction. Conventional parser combinators inherit the drawbacks from the recursive descent algorithms they implement. Based on FUN-GLL, this paper defines generalised parser combinators that overcome these problems. The algorithm is described in the same notation and style as FUN-GLL and uses the same data structures. Both algorithms are explained as a generalisation of basic recursive descent algorithms. The generalised parser combinators of this paper have several advantages over combinator libraries that generate internal grammars. For example, with the generalised parser combinators it is possible to parse larger permutation phrases and to write parsers for languages that are not context-free. The ‘BNF combinator library’ is built around the generalised parser combinators. With the library, embedded and executable syntax specifications are written. The specifications contain semantic actions for interpreting programs and constructing syntax trees. The library takes advantage of Haskell’s type-system to type-check semantic actions and Haskell’s abstraction mechanism enables ‘reuse through abstraction’. The practicality of the library is demonstrated by running parsers obtained from the syntax descriptions of several software languages.}
}
@incollection{ALDINUCCI2004617,
title = {A framework for experimenting with structured parallel programming environment design},
editor = {G.R. Joubert and W.E. Nagel and F.J. Peters and W.V. Walter},
series = {Advances in Parallel Computing},
publisher = {North-Holland},
volume = {13},
pages = {617-624},
year = {2004},
booktitle = {Parallel Computing},
issn = {0927-5452},
doi = {https://doi.org/10.1016/S0927-5452(04)80077-7},
url = {https://www.sciencedirect.com/science/article/pii/S0927545204800777},
author = {M. Aldinucci and S. Campa and P. Ciullo and M. Coppola and M. Danelutto and P. Pesciullesi and R. Ravazzolo and M. Torquati and M. Vanneschi and C. Zoccolo},
abstract = {Publisher Summary
A software development system based on integrated skeleton technology (ASSIST) is a parallel programming environment aimed at providing programmers of complex parallel application with a suitable and effective programming tool. Being based on algorithmically skeletons and coordination languages technologies, the programming environment relieves the programmer from a number of cumbersome, error prone activities that are required when using traditional parallel programming environments. ASSIST has been specifically designed to be easily customizable in order to experiment different implementation techniques, solutions, algorithms or back-ends any time new features are required or new technologies become available. This chapter explains how the ASSIST programming environment can be used to experiment new implementation techniques, mechanisms, and solutions within the framework of structured parallel programming models. The ASSIST implementation structure is briefly outlined and some experiments performed aimed at extending the environment are discussed. Those experiments were aimed at modifying ASSIST environment in such a way that, it can be used to program GRID architectures, including existing libraries in the application code, target heterogeneous cluster architectures, etc.}
}
@article{DUYUN2022720,
title = {Simulation of the structural and force parameters of a robotic platform using co-simulation},
journal = {Procedia Computer Science},
volume = {213},
pages = {720-727},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.126},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922018257},
author = {Tatyana Duyun and Ivan Duyun and Larisa Rybak and Victoria Perevuznik},
keywords = {design optimization, parallel manipulators, virtual prototype, simulation modeling, kinematic characteristics, Stewart-Gough platform},
abstract = {The paper presents the methodology and results of modeling the operation of the parallel mechanism - a 6-DOF robotic platform of the Stewart-Gough type. MSC Adams and Matlab software packages were used as modeling tools, as well as a general-purpose, high-level object-oriented programming language, Python. The digital layout, which has the properties of a parameterized simulation model, is built in the MSC Adams software package. The Python programming language is used as an alternative to the Adams View internal command language for creating and iteratively modifying modeling objects in Adams. Mutual integration of Matlab Simulink and Adams View provides the implementation of joint simulation and allows to perform kinematic, dynamic and force analysis of the mechanism, taking into account its design features through the use of a virtual prototype. Simulink provides the ability to import of physical and mechanical parameters of a solid model from Adams. The proposed implementation of the Python-Adams interface in the form of special procedures and functions automates the execution of a computational experiment and allows solving the problem of optimizing structural elements and finding the optimal geometric design of the platform in accordance with the selected optimality criteria. This is implemented through conducting a series of experiments consisting in sequential multiple changes in geometric parameters, followed by simulation model and analysis of the results in order to find design options that meet the specified criteria. The technique was tested on the example of one of the design versions of the platform when moving along several characteristic types of a given trajectory. The results of a computational experiment are presented and an analysis of the force characteristics during movement along a given trajectory is carried out.}
}
@incollection{REDDY20111,
title = {Chapter 1 - Introduction},
editor = {Martin Reddy},
booktitle = {API Design for C++},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-19},
year = {2011},
isbn = {978-0-12-385003-4},
doi = {https://doi.org/10.1016/B978-0-12-385003-4.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123850034000014},
author = {Martin Reddy},
abstract = {Publisher Summary
This chapter defines an Application Programming Interface (API), which provides an abstraction for a problem and specifies how clients should interact with software components that implement a solution to that problem. API development is ubiquitous in modern software development. Its purpose is to provide a logical interface to the functionality of a component while also hiding any implementation details. It can be as small as a single function or involve hundreds of classes, methods, free functions, data types, enumerations, and constants. Its implementation can be proprietary or open source. The important underlying concept is that an API is a well-defined interface that provides a specific service to other pieces of software. An API is an interface designed for developers, in much the same way that a Graphical User Interface (GUI) is an interface designed for end users.}
}
@article{BUNTING2014216,
title = {The Remote Sensing and GIS Software Library (RSGISLib)},
journal = {Computers & Geosciences},
volume = {62},
pages = {216-226},
year = {2014},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413002288},
author = {Peter Bunting and Daniel Clewley and Richard M. Lucas and Sam Gillingham},
keywords = {Software, Remote sensing, GIS, Raster, Vector, Open source},
abstract = {Key to the successful application of remotely sensed data to real world problems is software that is capable of performing commonly used functions efficiently over large datasets, whilst being adaptable to new techniques. This paper presents an open source software library that was developed through research undertaken at Aberystwyth University for environmental remote sensing, particularly in relation to vegetation science. The software was designed to fill the gaps within existing software packages and to provide a platform to ease the implementation of new and innovative algorithms and data processing techniques. Users interact with the software through an XML script, where XML tags and attributes are used to parameterise the available commands, which have now grown to more than 300. A key feature of the XML interface is that command options are easily recognisable to the user because of their logical and descriptive names. Through the XML interface, processing chains and batch processing are supported. More recently a Python binding has been added to RSGISLib allowing individual XML commands to be called as Python functions. To date the Python binding has over 100 available functions, mainly concentrating on image utilities, segmentation, calibration and raster GIS. The software has been released under a GPL3 license and makes use of a number of other open source software libraries (e.g., GDAL/OGR), a user guide and the source code are available at http://www.rsgislib.org.}
}
@article{PELLERIN201793,
title = {RINGMesh: A programming library for developing mesh-based geomodeling applications},
journal = {Computers & Geosciences},
volume = {104},
pages = {93-100},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417302637},
author = {Jeanne Pellerin and Arnaud Botella and François Bonneau and Antoine Mazuyer and Benjamin Chauvin and Bruno Lévy and Guillaume Caumon},
keywords = {Structural model, Geology, BRep, Unstructured meshes, C++, Open-source},
abstract = {RINGMesh is a C++ open-source programming library for manipulating discretized geological models. It is designed to ease the development of applications and workflows that use discretized 3D models. It is neither a geomodeler, nor a meshing software. RINGMesh implements functionalities to read discretized surface-based or volumetric structural models and to check their validity. The models can be then exported in various file formats. RINGMesh provides data structures to represent geological structural models, either defined by their discretized boundary surfaces, and/or by discretized volumes. A programming interface allows to develop of new geomodeling methods, and to plug in external software. The goal of RINGMesh is to help researchers to focus on the implementation of their specific method rather than on tedious tasks common to many applications. The documented code is open-source and distributed under the modified BSD license. It is available at https://www.ring-team.org/index.php/software/ringmesh.}
}
@article{GIESEKE20171,
title = {bufferkdtree: A Python library for massive nearest neighbor queries on multi-many-core devices},
journal = {Knowledge-Based Systems},
volume = {120},
pages = {1-3},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117300011},
author = {Fabian Gieseke and Cosmin Oancea and Christian Igel},
keywords = {Python, Nearest neighbor queries, -d trees, OpenCL, GPUs},
abstract = {The bufferkdtree package is an open-source software that provides an efficient implementation for processing huge amounts of nearest neighbor queries in Euclidean spaces of moderate dimensionality. Its underlying implementation resorts to a variant of the classical k-d tree  data structure, called buffer k-d tree, which can be used to efficiently perform bulk nearest neighbor searches on modern many-core devices. The package, which is based on Python, C, and OpenCL, is made publicly available online at https://github.com/gieseke/bufferkdtree under the GPLv2 license.}
}
@article{REGULY2020104425,
title = {Productivity, performance, and portability for computational fluid dynamics applications},
journal = {Computers & Fluids},
volume = {199},
pages = {104425},
year = {2020},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2020.104425},
url = {https://www.sciencedirect.com/science/article/pii/S0045793020300013},
author = {István Z. Reguly and Gihan R. Mudalige},
keywords = {Review, Domain specific language, Performance, Portability, Productivity},
abstract = {Hardware trends over the last decade show increasing complexity and heterogeneity in high performance computing architectures, which presents developers of CFD applications with three key challenges; the need for achieving good performance, being able to utilise current and future hardware by being portable, and doing so in a productive manner. These three appear to contradict each other when using traditional programming approaches, but in recent years, several strategies such as template libraries and Domain Specific Languages have emerged as a potential solution; by giving up generality and focusing on a narrower domain of problems, all three can be achieved. This paper gives an overview of the state-of-the-art for delivering performance, portability, and productivity to CFD applications, ranging from high-level libraries that allow the symbolic description of PDEs to low-level techniques that target individual algorithmic patterns. We discuss advantages and challenges in using each approach, and review the performance benchmarking literature that compares implementations for hardware architectures and their programming methods, giving an overview of key applications and their comparative performance.}
}
@article{RAVELOMENDEZ2023102897,
title = {Kraken 2.0: A platform-agnostic and cross-device interaction testing tool},
journal = {Science of Computer Programming},
volume = {225},
pages = {102897},
year = {2023},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2022.102897},
url = {https://www.sciencedirect.com/science/article/pii/S0167642322001307},
author = {William Ravelo-Méndez and Camilo Escobar-Velásquez and Mario Linares-Vásquez},
keywords = {Signaling, Automated testing, Cross-device testing, Cross-application testing},
abstract = {Nowadays applications that require the interaction and collaboration of two or more users simultaneously via different devices such as web browsers and mobile phones are becoming more and more frequent in a time where cross-device and cross-play features are demanded by final users, thus, to ensure high quality of those products, software engineers are adopting test automation techniques that can optimize their quality assurance processes. In previous work we presented Kraken 2.0, a publicly available cross-device testing tool that completely reconstructed the Kraken 1.0 architecture and enables a tester to write, run, and validate test scenarios that involve the interaction of web and mobile devices in parallel by implementing test scripts created with the Gherkin syntax. In this paper, we provide more details about the Kraken 2.0 architecture and describe new features such as extraction of GUI snapshots and definition of test scripts in an automation API fashion.}
}
@article{XU1994449,
title = {ComPaSS: A Communication Package for Scalable Software Design},
journal = {Journal of Parallel and Distributed Computing},
volume = {22},
number = {3},
pages = {449-461},
year = {1994},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1994.1103},
url = {https://www.sciencedirect.com/science/article/pii/S0743731584711038},
author = {H. Xu and E.T. Kalns and P.K. Mckinley and L.M. Ni},
abstract = {In massively parallel computers (MPCs), efficient communication among processors is critical to performance. This paper describes the initial implementation of the ComPaSS communication library to support scalable software development in MPCs. ComPaSS provides high-level global communication operations for both data manipulation and process control, many of which are based upon a small set of low-level communication primitives. The low-level operations of the ComPaSS library are provably optimal for a class of architectures representative of many commercial scalable systems, in particular those using wormhole routing and n-dimensional mesh network topologies. This paper concentrates on the multicast and multireceive components of the ComPaSS library, which are fundamental to implementing efficient high-level data parallel operations. The design of the multicast and multireceive primitives is described and an example of a data parallel application utilizing ComPaSS multicast is given. The scalability of these primitives is discussed, and improvements in performance resulting from use of the library on a 64-node nCUBE-2 are presented.}
}
@article{DU2022109844,
title = {Numerical implementation for isogeometric analysis of thin-walled structures based on a Bézier extraction framework: nligaStruct},
journal = {Thin-Walled Structures},
volume = {180},
pages = {109844},
year = {2022},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2022.109844},
url = {https://www.sciencedirect.com/science/article/pii/S0263823122005195},
author = {Xiaoxiao Du and Gang Zhao and Ran Zhang and Wei Wang and Jiaming Yang},
keywords = {Isogeometric analysis, Plate and shell, T-spline, Bézier extraction, Linear and nonlinear, Hyperelasticity},
abstract = {We present an open Octave/Matlab package nligaStruct, for linear and nonlinear isogeometric analysis of thin-walled structures. nligaStruct is an extension of NLIGA in aspect of isogeometric structural analysis. A framework for Bézier extraction of T-spline and NURBS is embedded in the simulation pipeline. Both Reissner–Mindlin and Kirchhoff–Love assumptions are considered in the case of static bending and free vibration analysis of thin-walled structures (plates and shells). The convergence behaviors of these two assumptions for both plate and shell are thoroughly studied and compared. The discrete formulations for large deformation of Kirchhoff–Love shells considering geometrical and material nonlinearity are detailedly derived and carefully implemented. Three types of hyperelastic materials, including St. Venant–Kirchhoff, incompressible neo-Hookean and compressible neo-Hookean, are employed in the large deformation of Kirchhoff–Love shells. A series of popular benchmark examples and their geometrical models are demonstrated in the package. The numerical results are validated by comparing with analytical solutions and that obtained from commercial software. The postprocessing is self-contained and implemented by tessellation of Bézier elements. Finally, complete source codes and the data obtained from numerical examples are fully provided and free to access.}
}
@article{GRASA2019100003,
title = {IRATI: Open source RINA implementation for Linux},
journal = {Software Impacts},
volume = {1},
pages = {100003},
year = {2019},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2019.100003},
url = {https://www.sciencedirect.com/science/article/pii/S266596381930003X},
author = {Eduard Grasa and Miquel Tarzan and Leonardo Bergesio and Bernat Gastón and Vincenzo Maffione and Francesco Salvestrini and Sander Vrijders and Dimitri Staessens},
keywords = {Network architecture, RINA, Future internet experimentation, Network protocols, Network virtualization, Network programmability},
abstract = {IRATI is a open source implementation of RINA for OS/Linux systems that allows researchers and innovators to experiment with RINA networks. RINA is a new Internetwork architecture that supports without the need of extra mechanisms mobility, multi-homing and Quality of Service, provides a secure and configurable environment and allows for a seamless adoption. IRATI implements the core RINA protocols and multiple policies to customize such protocols to different environments. Policies can be developed via a Software Development Kit (SDK). IRATI provides an API for applications to natively use RINA IPC services, as well as multiple monitoring utilities and example applications.}
}
@incollection{LI2006747,
title = {7 - Socket API Extensions},
editor = {Qing Li and Tatuya Jinmei and Keiichi Shima},
booktitle = {IPv6 Core Protocols Implementation},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {747-907},
year = {2006},
series = {The Morgan Kaufmann Series in Networking},
issn = {18759351},
doi = {https://doi.org/10.1016/B978-012447751-3/50009-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124477513500096},
author = {Qing Li and Tatuya Jinmei and Keiichi Shima},
abstract = {Publisher Summary
Network applications use standard application programming interfaces (APIs) that are provided by the underlying operating system to access the available network services. The Berkeley Software Distribution (BSD) socket API is one of the most commonly deployed APIs that is supported by a wide variety of operating systems. Although the socket API has a flexible design that can accommodate various network protocols including Internet Protocol Version 6 (IPv6), additional extensions for providing useful and portable interfaces for IPv6 network programming are still necessary. The IETF standardized two sets of extensions: one is classified as the Basic Socket API Extensions defined in [RFC3493] and the other as the Advanced Socket API Extensions defined in [RFC3542]. The Basic Socket API provides standard definitions that represent IPv6 addresses, name and address conversion functions, and multicast-related interfaces. On the other hand, the Advanced Socket API defines interfaces for accessing special IPv6 packet information such as the IPv6 header and the extension headers. The Advanced Socket API is also used to extend the capability of IPv6 raw sockets. This chapter provides an overview of these API specifications along with sample code usage. It then explains the internal kernel implementation that realizes the services offered by the API sets. The chapter concludes with a discussion on the implementation of some interesting library functions defined in the Basic Socket API.}
}
@article{GARCIA2022135,
title = {An integrated open source CAT based on Skin Model Shapes},
journal = {Procedia CIRP},
volume = {114},
pages = {135-140},
year = {2022},
note = {17th CIRP Conference on Computer Aided Tolerancing (CAT2022)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122014457},
author = {Carlos Andres Restrepo Garcia and Denis Teissandier and Nabil Anwer and Vincent Delos and Yann Ledoux and Laurent Pierre},
keywords = {UML, Skin Model Shapes, Tolerance Analysis;Computer-aided tolerancing},
abstract = {The control of geometrical deviations and form variations throughout the product life-cycle is a fundamental task in geometric dimensioning and tolerancing. As product complexity increases, it has not only become necessary to rely on computers to process geometrical and non-geometrical information from early design stages but also to come up with more realistic shape representation. Most of computer-aided tolerancing (CAT) packages used nowadays are fully integrated to computer-aided design softwares like CATIA and SolidWorks and they allow to model 2D and 3D tolerances stack-up through worst-case or statistical models. These CAT systems are generally available as proprietary commercial software which can sometimes restrict their domain of application and slow the implementation of new paradigms like the Skin Model. The Skin Model is an abstract surface model that represents the interface between a workpiece and its environment whose implementation involves the modeling of finite instances of the Skin Model called Skin Model Shapes (SMS) that encompass different sources of deviations and constitute a non-ideal geometrical model. The aim of this work is to show the first phase of implementation of an integrated open source environment based on PolitoCAT and Salome to model Skin Model Shapes. An Unified Model Language (UML) based logical data model of the integrated system is presented, it is an extended version of current data models for geometric modeling that includes the objects and relationships to manage form variations at different design stages. The work carried out contributes to the conceptualization of Skin Model Shapes model and it constitutes a support on the implementation of SMS in an open source CAT. An example of this integration involving a Skin Model Shapes implementation is shown as an illustration of the functionality of the platform.}
}
@article{WANG2020190,
title = {Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost},
journal = {Pattern Recognition Letters},
volume = {136},
pages = {190-197},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520302129},
author = {Chen Wang and Chengyuan Deng and Suzhen Wang},
keywords = {Imbalanced classification, XGBoost, Python package},
abstract = {The paper presents Imbalance-XGBoost, a Python package that combines the powerful XGBoost software with weighted and focal losses to tackle binary label-imbalanced classification tasks. Though a small-scale program in terms of size, the package is, to the best of our knowledge, the first of its kind which provides an integrated implementation for the two loss functions on XGBoost and brings a general-purpose extension to XGBoost for label-imbalanced scenarios. In this paper, the design and usage of the package are discussed and illustrated with examples. Furthermore, as the first- and second-order derivatives of the loss functions are essential for the implementations, the algebraic derivation is discussed and it can be deemed as a separate contribution. The performances of the methods implemented in the package are extensively evaluated on Parkinson’s disease classification dataset, and multiple competitive performances are presented with the ROC and Precision-Recall (PR) curves. To further assert the superiority of the methods, the performances on four other benchmark datasets from the UCI machine learning repository are additionally reported. Given the scalable nature of XGBoost, the package has great potentials to be broadly applied to real-life binary classification tasks, which are usually of large-scale and label-imbalanced.}
}
@article{CARRERAS2023110529,
title = {Benchmark test for mode I fatigue-driven delamination in GFRP composite laminates: Experimental results and simulation with the inter-laminar damage model implemented in SAMCEF},
journal = {Composites Part B: Engineering},
volume = {253},
pages = {110529},
year = {2023},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2023.110529},
url = {https://www.sciencedirect.com/science/article/pii/S135983682300032X},
author = {L. Carreras and B.L.V. Bak and S.M. Jensen and C. Lequesne and H. Xiong and E. Lindgaard},
keywords = {GFRP laminates, Delamination, Fatigue, Cohesive zone model, Finite element method},
abstract = {Adopting effective and accurate numerical tools capable of predicting damage effects on the structure reduces design, certification, and maintenance costs. However, the tools to assess progressive delamination under high-cycle fatigue are rarely validated against realistic benchmark tests different from simple tests on coupon specimens that can be simplified to a 2D geometry. This work presents a benchmark test on a demonstrator specimen made of a non-crimp fabric laminated Glass Fiber Reinforced Polymer (GFRP) used in the wind energy industry. The case shows varying crack growth rates and crack front shape over the fatigue life, making it more representative of structures in service than coupon specimens. Moreover, the test is simulated with the first commercially available tool to assess progressive delamination under high-cycle fatigue loading based on a cohesive zone model approach. The method is implemented in the Simcenter Samcef 2021.2 software package dedicated to mechanical virtual prototyping. A characterization testing campaign on coupon specimens is carried out to obtain the material properties for the method. The numerical method can reproduce the experimental results on the demonstrator specimen regarding crack front shape evolution and crack front location versus the number of fatigue cycles.}
}
@article{BELLOTTI2001259,
title = {AutoGraL: a Java 2D graphics library for configurable automotive dashboards},
journal = {Computers & Graphics},
volume = {25},
number = {2},
pages = {259-268},
year = {2001},
issn = {0097-8493},
doi = {https://doi.org/10.1016/S0097-8493(00)00129-1},
url = {https://www.sciencedirect.com/science/article/pii/S0097849300001291},
author = {F. Bellotti and A. {De Gloria} and M. Risso and A. Villamaina},
keywords = {Application packages, Virtual device interfaces, Human factors, Animation, Real time and embedded systems, Performance of systems},
abstract = {Electronics and Information Technology are playing a role of growing importance in the automotive market, also for the integration of in-car information systems with the external world. The consequent increasing range of services provided to drivers and passengers requires the study and development of human–machine interfaces able to manage multimedia streams according to criteria of safety and simplicity of use. We have implemented a library of Java components that faithfully reproduce the typically analog look of current instrument clusters, introducing runtime configurability and a limited degree of interactivity on the user's part. Components have been designed to support integration into more complex multimedia systems. The paper presents the issues we tackled in the design and development of mission-critical software and the related solutions. We show the structure and the functionality of the library and the performance analysis of a sample program, proposing a system architecture and discussing problems related to the in-car deployment of Java dashboard applications.}
}
@article{ERCAN2016112,
title = {Design and implementation of a general software library for using NSGA-II with SWAT for multi-objective model calibration},
journal = {Environmental Modelling & Software},
volume = {84},
pages = {112-120},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2016.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815216302547},
author = {Mehmet B. Ercan and Jonathan L. Goodall},
keywords = {Multi-objective calibration, Genetic algorithms, Watershed modeling, SWAT, NSGA-II},
abstract = {Calibrating watershed-scale hydrologic models remains a critical but challenging step in the modeling process. The Soil and Water Assessment Tool (SWAT) is one example of a widely used watershed-scale hydrologic model that requires calibration. The calibration algorithms currently available to SWAT modelers through freely available and open source software, however, are limited and do not include many multi-objective genetic algorithms (MOGAs). The Non-Dominated Sorting Genetic Algorithm II (NSGA-II) has been shown to be an effective and efficient MOGA calibration algorithm for a wide variety of applications including for SWAT model calibration. Therefore, the objective of this study was to create an open source software library for multi-objective calibration of SWAT models using NSGA-II. The design and implementation of the library are presented, followed by a demonstration of the library through a test case for the Upper Neuse Watershed in North Carolina, USA using six objective functions in the model calibration.}
}
@incollection{SCHMIDT201877,
title = {Chapter 4 - C++11 Multithreading},
editor = {Bertil Schmidt and Jorge González-Domínguez and Christian Hundt and Moritz Schlarb},
booktitle = {Parallel Programming},
publisher = {Morgan Kaufmann},
pages = {77-133},
year = {2018},
isbn = {978-0-12-849890-3},
doi = {https://doi.org/10.1016/B978-0-12-849890-3.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128498903000046},
author = {Bertil Schmidt and Jorge González-Domínguez and Christian Hundt and Moritz Schlarb},
keywords = {Multithreading, C++, C++11, Thread spawning, Race condition, Promise, Future, Deadlock, Task parallelism, Asynchronous task, Static scheduling, Matrix vector multiplication, Thread distribution, Closure, False sharing, Cache line, Load balancing, Dynamic scheduling, Mutex, Condition variable},
abstract = {During the last decade, single-core performance of modern CPUs has been stagnating due to hard architectural limitations of silicon-based semiconductors. On the one hand, the historic shrinking of the manufacturing process of three orders-of-magnitude from a few micrometers to a few nanometers cannot proceed at arbitrary pace in the future. This can be explained by emerging delocalization effects of quantum mechanics which governs physics at the nanometer scale. On the other hand, the power dissipation of integrated circuits exhibits a quadratic dependency on the voltage and a linear dependency on the frequency. Thus, significant up-scaling of the CPU's frequency is prohibitive in terms of energy consumption. As a result, we cannot expect single-threaded programs to run faster on future hardware without considering parallelization over multiple processing units. The free lunch is over, so to speak. Future improvements in runtime have to be accomplished by using several processing units. Historically, there have been several C- and C++-based libraries that support multithreading over multiple CPU cores. POSIX Threads, or short PThreads, have been the predominant implementation over several decades in the Linux/UNIX world. Some Windows versions featured a POSIX-compatible layer which has been deprecated in newer Windows versions in favor of Microsoft's homebrew threading API. Intel's Threading Building Blocks (TBB) is another popular implementation. This heterogeneous software landscape made it difficult to write platform-portable code in C or C++. With the release of C++11 and its novel threading API it is finally possible to write platform-independent code in C++ that is supported by compilers from both the Linux/UNIX world and the Windows ecosystem without the need for third party libraries like Intel TBB. Thus, our approach to multithreading in this chapter is based on the modern C++11 and C++14 dialects of the C++ language.}
}
@article{ZEHNER201683,
title = {Rasterizing geological models for parallel finite difference simulation using seismic simulation as an example},
journal = {Computers & Geosciences},
volume = {86},
pages = {83-91},
year = {2016},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2015.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S009830041530073X},
author = {Björn Zehner and Olaf Hellwig and Maik Linke and Ines Görz and Stefan Buske},
keywords = {3D, Rasterization, Voxelization, Scan conversion, Seismic, Finite difference simulation, Parallel computation},
abstract = {3D geological underground models are often presented by vector data, such as triangulated networks representing boundaries of geological bodies and geological structures. Since models are to be used for numerical simulations based on the finite difference method, they have to be converted into a representation discretizing the full volume of the model into hexahedral cells. Often the simulations require a high grid resolution and are done using parallel computing. The storage of such a high-resolution raster model would require a large amount of storage space and it is difficult to create such a model using the standard geomodelling packages. Since the raster representation is only required for the calculation, but not for the geometry description, we present an algorithm and concept for rasterizing geological models on the fly for the use in finite difference codes that are parallelized by domain decomposition. As a proof of concept we implemented a rasterizer library and integrated it into seismic simulation software that is run as parallel code on a UNIX cluster using the Message Passing Interface. We can thus run the simulation with realistic and complicated surface-based geological models that are created using 3D geomodelling software, instead of using a simplified representation of the geological subsurface using mathematical functions or geometric primitives. We tested this set-up using an example model that we provide along with the implemented library.}
}
@article{GRAEFE1993141,
title = {Conceptual architecture of an object library for design, control and simulation of a manufacturing enterprise},
journal = {Control Engineering Practice},
volume = {1},
number = {1},
pages = {141-146},
year = {1993},
issn = {0967-0661},
doi = {https://doi.org/10.1016/0967-0661(93)92113-I},
url = {https://www.sciencedirect.com/science/article/pii/096706619392113I},
author = {U. Graefe and A. Pardasani and A.W. Chan},
keywords = {Manufacturing processes, object library, object oriented programming, simulation, Smalltalk},
abstract = {The object-oriented paradigm facilitates the creation of software objects which can be easily modified, extended, specialized and readily reused across applications. This concept is gaining wide acceptance in the design, control and simulation of manufacturing systems. Object-oriented concepts can be used to design and implement a software object library, the components of which can be easily assebled to develop decision support tools for use throughout the life cycle of a manufacturing enterprise. This paper presents a methodology for developing architectures of a reusable library, and includes an example of a browser to illustrate a mechanism for organizing the storage and location of objects in the library.}
}
@article{FEDER2022858,
title = {An approach for automatic generation of the URDF file of modular robots from modules designed using SolidWorks},
journal = {Procedia Computer Science},
volume = {200},
pages = {858-864},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.283},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002927},
author = {Maddalena Feder and Andrea Giusti and Renato Vidoni},
keywords = {Modular reconfigurable robots, Industry 4.0, Robot Operating System, URDF, Kinematics},
abstract = {Modular robot manipulators promise to enhance current automation practice by providing higher flexibility and quick recovery in case of failures with respect to fixed-structure robots. The configuration of the modular robot control for each new assembly, to account for the new system kinematics and dynamics, can be time consuming and requires robot modelling expertise. We propose an approach for automatically generating the Unified Robot Description Format (URDF) file of modular robot manipulators, starting from the kinematic and dynamic descriptions expressed following the URDF of the single modules they can be composed of. The approach has been implemented and numerically verified by exploiting off-the-shelf software tools from Robot Operating System (ROS) libraries.}
}