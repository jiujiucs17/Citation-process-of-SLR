@article{PASHCHENKO2022479,
title = {Microclimate Monitoring System Design for the Smart Grid Analysis and Constructive Parameters Estimation⋆},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {9},
pages = {479-484},
year = {2022},
note = {11th IFAC Symposium on Control of Power and Energy Systems CPES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.07.083},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322004682},
author = {Alexander F. Pashchenko and Yuriy M. Rassadin},
keywords = {Identification, Temperature Control, HVAC, grey-box model, thermal network},
abstract = {This paper describes a microclimate monitoring system consisting of a LoRaWAN network of wireless climate sensors, a data collector and analytical software. The system is a part of the ICS RAS SmartGrid Centre project for predicting building energy consumption. During the design phase, the authors considered the concept of comfort, which is involved in setting control objectives for HVAC plants. It was necessary to overcome some characteristics of the LoRaWAN protocol, such as floating data transmission period and limited intensity of sensor communication. These have been overcome by post-processing the data with Python software, using libraries numpy and scipy. The collected data was passed through an interpolation filter for synchronization, and the resulting data is freely available in dataset format on our website for all interested researchers. Additionally, weather data was collected using a local meteostation to be considered as external disturbances in analysis problems. This paper also considers an approach to passive identification of the thermal protection parameters of a building. The coronavirus lockdown period was chosen to assume the impact of visitors negligible. The parameters are supposed to be estimated by correlation analysis. The estimates obtained should be compared with the values calculated according to ISO and Russian construction standards for diagnostic reasons.}
}
@incollection{NELSON2008255,
title = {Chapter 12 - The JHDL Design and Debug System},
editor = {Scott Hauck and André Dehon},
booktitle = {Reconfigurable Computing},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {255-273},
year = {2008},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012370522-8.50017-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123705228500170},
author = {Brent Nelson and Brad Hutchings},
abstract = {Publisher Summary
JHDL is a CAD environment developed for the design, debug, and runtime control of configurable computing applications based on field-programmable gate array (FPGA) technology. The term JHDL refers to one of two things that includes the JHDL circuit design language itself or the JHDL CAD system. The JHDL language is a text-based design language for algorithmic construction of structured circuits that is embedded within the Java programming language. JHDL designs are created as Java programs that access JHDL libraries to generate circuits. Within the JHDL CAD environment, circuits can be simulated, net-listed, and downloaded to the reconfigurable computing platform for execution and testing. Additional CAD tools can be built on top of the JHDL infrastructure to support higher-level circuit construction, optimization, and debugging tasks. One of the most unique features of JHDL is its runtime environment, which provides a unified simulator/hardware debugger that can be used to debug and validate a circuit through either simulation or hardware execution and which contains many features normally found only in source-level software debuggers. JHDL is currently in use in a variety of research projects, from module generator systems to behavioral synthesis systems to microarchitectural simulation systems. By providing a framework for the construction, simulation, netlisting, and hardware debug of FPGA-based designs, JHDL allows researchers to focus on tasks other than recreating the infrastructure that JHDL provides.}
}
@article{PENG2020106650,
title = {A new method for interoperability and conformance checking of product manufacturing information},
journal = {Computers & Electrical Engineering},
volume = {85},
pages = {106650},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106650},
url = {https://www.sciencedirect.com/science/article/pii/S004579062030505X},
author = {Zhiguo Peng and Meifa Huang and Yanru Zhong and Leilei Chen and Guanghao Liu},
keywords = {PMI, Interoperability, Conformance checking, Ontology, SROIQ(D)},
abstract = {Product manufacturing information (PMI) is the most important information in the product digital model. Whether PMI can be clearly and unambiguous represented and exchanged between different computer-aided software (CAx) has become a topic of high interest in current research. To enhance the interoperability of PMI information among different CAx systems and offer framework software to check the PMI realization ability of different computer-aided design (CAD) software packages, this paper proposes a new method for interoperability and conformance checking by establishing a unified PMI ontology. Using web ontology language description logic (OWL2 DL) and semantic web rule language (SRWL), PMI can be described clearly and without ambiguity and can be read and interpreted automatically by the computer. In addition, the ontology also supplies the functions of consistency checking, knowledge inference, and semantic queries. The analysis shows that this ontology has great engineering significance in realization of PMI interoperability exchange and construction of framework software for PMI conformance checking.}
}
@article{PANFEROV20113003,
title = {Theory and Software Package for Simulation and Smart Control Design for Complex Flexible Aerospace Vehicles},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {3003-3008},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.02903},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016440711},
author = {Alexander I. Panferov and Alexander V. Nebylov and Sergey A. Brodsky},
keywords = {mathematical model, flexible vehicle, simulation, sloshing, local angle of attack, oscillations damping},
abstract = {Abstract
New results in the field of the automaton of the aerospace flight control system design are presented in this paper. The concept of a universal program for research of dynamic properties, calculation of comprehensive mathematical models, and simulation of the flight motion and synthesis of smart control laws for different types of the flexible aerospace constructions (launchers and missiles) is considered. The special attention to creation of mathematical models of different physical phenomenon of aerospace vehicles is given. The basis of the program is the structure, which allows carrying out of the analysis of dynamic characteristics, simulation, and visual information representation. The software package is supplied with the library of program modules. These modules are designed on the base of mathematical models of separate vehicle elements and different physical effects, such as flexibility, sloshing, sluggishness of engines, local angle of attack values, etc. Selection of different sets of the mathematical models and program modules allows investigating different constructions for known and prospective vehicles. The interface of the program enables changing parameters of the vehicle in a wide range, to simulate flight at any trajectory and to output results of the analysis in a digital or graphical form. For synthesis of control laws, provides the usage of the smart control approach.}
}
@article{SEMERARO199767,
title = {PVM implementation of the symmetric-Galerkin method},
journal = {Engineering Analysis with Boundary Elements},
volume = {19},
number = {1},
pages = {67-72},
year = {1997},
note = {High Performance Computing},
issn = {0955-7997},
doi = {https://doi.org/10.1016/S0955-7997(97)00026-X},
url = {https://www.sciencedirect.com/science/article/pii/S095579979700026X},
author = {B.D. Semeraro and L.J. Gray},
keywords = {Boundary element method, parallel processing, workstation cluster, proximity sensor, block linear algebra algorithms},
abstract = {We report on initial progress towards a parallel virtual machine (PVM) implementation of the symmetric-Galerkin boundary integral method. We take advantage of software packages specifically designed to solve linear algebra problems on distributed memory parallel computers. In particular we use linear algebra routines from the ScaLAPACK, PBLAS and BLACS libraries. These routines assume a block cyclic decomposition of the matrix operands. The decomposition of the operands and its impact on the construction of the coefficient matrix are described. Computational results for solving the two-dimensional Laplace equation are presented. This program is being used to simulate the performance of a proximity sensor used in robotics and other applications.}
}
@article{OMARSDOTTIR2016231,
title = {The Axiomatic Design of Chessmate: A Chess-playing Robot},
journal = {Procedia CIRP},
volume = {53},
pages = {231-236},
year = {2016},
note = {The 10th International Conference on Axiomatic Design (ICAD2016)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116307235},
author = {Freyja Yeatman Ómarsdóttir and Róbert Bjarnar Ólafsson and Joseph Timothy Foley},
keywords = {Axiomatic Design, chess, robotics},
abstract = {Successfully completing a project on time is often a difficult task especially when the project is not well defined. This paper demonstrates the application of Axiomatic Design principles to shape and direct a multi-disciplinary project from initial conception to the final tested product. This product is Chessmate: a small robot which plays chess on a physical board. This robot is intended as a telepresence mechanism or for players who are physically challenged. Verifiable requirements were developed at the beginning of the project based upon this top level goal. These Functional Requirements ensured that the team focused on the necessary capabilities of the end product even while working on electrical, mechanical, and software elements in parallel. Construction of a design matrix identified sources of coupling that would require additional effort to avoid delays. Coupling was reduced in software by careful Application Programming Interface (API) and abstraction development. Testing parameters were explicitly stated by the requirements enabling regular validation in both software and hardware. The result was a complete chess-playing system from start to finish in 12 weeks.}
}
@article{GARCIA198851,
title = {Specification, design and Modula-2 implementation of a low cost industrial control system},
journal = {Annual Review in Automatic Programming},
volume = {14},
pages = {51-56},
year = {1988},
note = {Real Time Programming 1988},
issn = {0066-4138},
doi = {https://doi.org/10.1016/0066-4138(88)90008-0},
url = {https://www.sciencedirect.com/science/article/pii/0066413888900080},
author = {D Garcia and H Lopez and J Tuya and A Diez},
keywords = {Software Engineering, Real time control, Modula-2 language, Concurrency, Computer-aided training},
abstract = {The purpose of this paper is to show the application of modern software engineering techniques to the construction of a small/medium size package for real time control. This package, denoted by SCM (acronym of “Sistema de Control Modular”), has been written in Modula-2 and presents all advantages offered by this high-level structured programming language. It has been implemented on a PC-AT compatible computer.}
}
@article{BENGHI2019102926,
title = {Automated verification for collaborative workflows in a Digital Plan of Work},
journal = {Automation in Construction},
volume = {107},
pages = {102926},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102926},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517310725},
author = {Claudio Benghi},
keywords = {Digital plan of work, Model verification, Model checking, Interdisciplinary collaboration, COBie},
abstract = {This paper presents the rationale, strategy and development behind the Verification components of the UK Digital Plan of Work (DPoW), a large-scale interdisciplinary collaboration platform implemented to enhance collaborative Building Information Modelling (BIM) workflows. Construction enterprises currently organise their information exchanges through documents that are not suitable for the automation of quality assurance processes. The DPoW addresses this issue with the introduction of a documented collaboration format and a suite of associated verification libraries, available at no cost as open source packages. The resulting infrastructure has the potential to automate the management of information exchanges in multi-disciplinary construction projects, but its uptake may be impeded by known issues that affect the dynamics of technology adoption. The verification infrastructure has been designed to reduce the impact of these issues by providing immediate paths to adoption and enabling three levels of process feedback for sustainable incremental improvements, starting from the current maturity level. The research followed the Design Science Methodology and is presented in this paper progressing from the definition of goals and objectives to the presentation of the developed solution, ending with a summary of known limitations and areas of further development.}
}
@article{HU201629,
title = {Improving interoperability between architectural and structural design models: An industry foundation classes-based approach with web-based tools},
journal = {Automation in Construction},
volume = {66},
pages = {29-42},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516300152},
author = {Zhen-Zhong Hu and Xiao-Yang Zhang and Heng-Wei Wang and Mohamad Kassem},
keywords = {BIM, Data Model, IFC, Model Conversion, Structural Analysis, WebGL},
abstract = {Medium and large construction projects typically involve multiple structural consultants who use a wide range of structural analysis applications. These applications and technologies have inadequate interoperability and there is still a dearth of investigations addressing interoperability issues in the structural engineering domain. This paper proposes a novel approach which combines an industry foundation classes (IFC)-based Unified Information Model with a number of algorithms to enhance the interoperability: (a) between architectural and structural models, and (b) among multiple structural analysis models (bidirectional conversion or round tripping). The proposed approach aims to achieve the conversion by overcoming the inconsistencies in data structures, representation logics and syntax used in different software applications. The approach was implemented in both Client Server (C/S) and Browser Server (B/S) environments to enable central and remote collaboration among geographically dispersed users. The platforms were tested in four large real-life projects. The testing involved four key scenarios: (a) the bidirectional conversion among four structural analysis tools; (b) the comparison of the conversion via the proposed approach with the conversion via direct links among the involved tools; (c) the direct export from an IFC-based architectural tool through the Application Program Interface (API), and (d) the conversion and visualization of structural analysis results. All these scenarios were successfully performed and tested in four significant case studies. In particular, the conversion among the four structural analysis applications (ETABS, SAP2000, ANSYS and MIDAS) was successfully tested for all possible conversion routes among the four applications in two of the case studies (i.e., Project A and Project B). The first four steps of natural mode shapes and their natural vibration periods were calculated and compared with the converted models. They were all achieved within a standard deviation of 0.1s and 0.2s in Project A and Project B, respectively, indicating an accurate conversion.}
}
@article{CARACCIOLO2021105759,
title = {Elliptic tori in FPU non-linear chains with a small number of nodes},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {97},
pages = {105759},
year = {2021},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2021.105759},
url = {https://www.sciencedirect.com/science/article/pii/S1007570421000708},
author = {Chiara Caracciolo and Ugo Locatelli},
keywords = {FPU problem, Lower dimensional invariant tori, KAM theory, Normal form methods, Perturbation theory for Hamiltonian systems},
abstract = {We revisit an algorithm constructing elliptic tori, that was originally designed for applications to planetary hamiltonian systems. The scheme is adapted to properly work with models of chains of N+1 particles interacting via anharmonic potentials, thus covering also the case of FPU chains. After having preliminarily settled the Hamiltonian in a suitable way, we perform a sequence of canonical transformations removing the undesired perturbative terms by an iterative procedure. This is done by using the Lie series approach, that is explicitly implemented in a programming code with the help of a software package, which is especially designed for computer algebra manipulations. In the cases of FPU chains with N=4,8, we successfully apply our new algorithm to the construction of elliptic tori for wide sets of the parameter ruling the size of the perturbation, i.e., the total energy of the system. Moreover, we explore the stability regions surrounding 1D elliptic tori. We compare our semi-analytical results with those provided by numerical explorations of the FPU-model dynamics, where the latter ones are obtained by using techniques based on the so called frequency analysis. We find that our procedure works up to values of the total energy that are of the same order of magnitude with respect to the maximal ones, for which elliptic tori are detected by numerical methods.}
}
@article{DOROZ2014766,
title = {A million-bit multiplier architecture for fully homomorphic encryption},
journal = {Microprocessors and Microsystems},
volume = {38},
number = {8, Part A},
pages = {766-775},
year = {2014},
note = {2013 edition of the Euromicro Conference on Digital System Design (DSD 2013)},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2014.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0141933114000842},
author = {Yarkın Doröz and Erdinç Öztürk and Berk Sunar},
keywords = {Fully homomorphic encryption, Very-large number multiplication, Number theoretic transform},
abstract = {In this work we present a full and complete evaluation of a very large multiplication scheme in custom hardware. We designed a novel architecture to realize a million-bit multiplication scheme based on the Schönhage–Strassen Algorithm. We constructed our scheme using Number Theoretical Transform (NTT). The construction makes use of an innovative cache architecture along with processing elements customized to match the computation and access patterns of the NTT-based recursive multiplication algorithm. We realized our architecture with Verilog and using a 90nm TSMC library, we could get a maximum clock frequency of 666MHz. With this frequency, our architecture is able to compute the product of two million-bit integers in 7.74ms. Our data shows that the performance of our design matches that of previously reported software implementations on a high-end 3GHz Intel Xeon processor, while requiring only a tiny fraction of the area.1An earlier short conference version of this paper appeared in [41].1}
}
@article{SARAKINOS200739,
title = {A software tool for generic parameterized aircraft design},
journal = {Advances in Engineering Software},
volume = {38},
number = {1},
pages = {39-49},
year = {2007},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2006.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0965997806000792},
author = {Sotirios S. Sarakinos and Ioannis M. Valakos and Ioannis K. Nikolos},
keywords = {Aircraft design, Surface generation, NURBS surfaces},
abstract = {In this work a surface generation software named Ge.P.A.S. (for generic parameterized aircraft surface) is presented, designed for the construction of aircraft aerodynamic surfaces. The surface generation procedure is parameterized and different aircraft configurations can be produced in an interactive way. A hierarchical structure of geometric parameters was adopted, resulting in easier manipulation of the shape and a scalable number of control parameters. Additionally, the geometric parameters may serve as design optimization variables in cooperation with an external optimizer. The surface generation is based on the use of NURBS curves and surfaces, which provide the ability to produce complicated geometries with a relative small number of design variables. Standard or user-defined airfoil sections can be used for the wing generation. The surface description is compatible with international input/output standards; IGES and STEP formats are supported for the output files. Consequently, Ge.P.A.S. can serve as a preprocessor for other software packages, which may be used in order to refine the geometry or to generate the grid for numerical simulations. The geometric algorithms, the software features and its basic characteristics are presented in this paper, along with a demonstration of its abilities in sample aircraft configurations.}
}
@article{HUGHES2021104980,
title = {A method to include reservoir operations in catchment hydrological models using SHETRAN},
journal = {Environmental Modelling & Software},
volume = {138},
pages = {104980},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.104980},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221000232},
author = {Daryl Hughes and Stephen Birkinshaw and Geoff Parkin},
keywords = {Reservoir management, Reservoir operations, Hydrology, SHETRAN, Climate change, Water resources},
abstract = {Reservoir construction and operation have significant impacts on catchment hydrology, flood risk and fluvial processes. However, few available hydrological modelling packages can simulate complex, dynamic, manually-operated reservoir control structures. We present SHETRAN-Reservoir, a physically-based spatially distributed modelling tool to simulate catchment hydrology, including reservoir operations. We also propose a method for deriving parsimonious reservoir operation rules from real-world observations. Application of SHETRAN-Reservoir to the Upper Cocker catchment in the Lake District National Park, UK, is shown to improve modelling of hydrological response. Modelling combined climate change and water resource management scenarios demonstrates the influence of operational control rules on hydrological impacts, especially during droughts. We discuss how SHETRAN-Reservoir can be applied to other reservoir-containing catchments to guide decisions concerning water resources, ecology and flood risk. We also discuss potential future software developments.}
}
@article{WANG20091,
title = {Semi-automated model matching using version difference},
journal = {Advanced Engineering Informatics},
volume = {23},
number = {1},
pages = {1-11},
year = {2009},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2008.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034608000360},
author = {Hongjun Wang and Burcu Akinci and James H. Garrett and Eric Nyberg and Kent A. Reed},
abstract = {Interoperability of software is a critical requirement in the architecture, engineering, and construction (AEC) industry, where a number of data exchange standards have been created to enable data exchange among different software packages. To be able to comply with existing data exchange standards, the software developers need to match their internal data schemas to the schema defined in a standard and vice versa. The process of matching two large scale data models is time consuming and cumbersome when performed manually, and becomes even more challenging when a source and/or a target model is being updated frequently to meet the ever expanding real world requirements. While several prior studies discussed the need for approaches toward automated or semi-automated schema matching, an approach that builds on existing matches between two models has rarely been studied. In this paper, we present a semi-automated approach for model matching. This approach leverages a given set of existing matching between two models and upgrades those matching when a new version of a target model is released. The paper describes in detail a list of upgrade patterns generated and validated through a prototype by matching a domain-specific data model to several recent releases of the industry foundation classes.}
}
@article{LADINES2020109455,
title = {BOPcat software package for the construction and testing of tight-binding models and bond-order potentials},
journal = {Computational Materials Science},
volume = {173},
pages = {109455},
year = {2020},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2019.109455},
url = {https://www.sciencedirect.com/science/article/pii/S0927025619307542},
author = {A.N. Ladines and T. Hammerschmidt and R. Drautz},
keywords = {Atomistic simulations, Tight-binding, Bond-order potentials},
abstract = {Atomistic models like tight-binding (TB), bond-order potentials (BOP) and classical potentials describe the interatomic interaction in terms of mathematical functions with parameters that need to be adjusted for a particular material. The procedures for constructing TB/BOP models differ from the ones for classical potentials. We developed the BOPcat software package as a modular python code for the construction and testing of TB/BOP parameterizations. It makes use of atomic energies, forces and stresses obtained by TB/BOP calculations with the BOPfox software package. It provides a graphical user interface and flexible control of raw reference data, of derived reference data like defect energies, of automated construction and testing protocols, and of parallel execution in queuing systems. We demonstrate the concepts and usage of the BOPcat software and illustrate its key capabilities by exemplary constructing and testing a parameterization of a magnetic BOP for Fe. We provide a parameterization protocol with a successively increasing set of reference data that leads to good transferability to a variety of properties of the ferromagnetic bcc groundstate and to crystal structures which were not part of the training set.}
}
@article{MULTEDO1999393,
title = {ASAAC Phase II programme progress on the definition of standards for the core processing architecture of future military aircraft},
journal = {Microprocessors and Microsystems},
volume = {23},
number = {7},
pages = {393-407},
year = {1999},
issn = {0141-9331},
doi = {https://doi.org/10.1016/S0141-9331(99)00050-2},
url = {https://www.sciencedirect.com/science/article/pii/S0141933199000502},
author = {G Multedo and D Jibb and G Angel},
keywords = {Allied standard avionics architecture council, Military core avionics, Construction of demonstrators},
abstract = {The Allied Standard Avionics Architecture Council (ASAAC) Phase II programme is sponsored by the Ministry of Defence of the UK, Germany and France through a Memorandum of Understanding (MOU), which provides for a two stage programme over five years to establish a complete set of standards for military core avionics for the end of this century. The standards will cover systems, software, networks, packaging and common functional modules, and will be validated by the construction of a set of demonstrators. The contract was let on 18th November 1997, and project teams were formed in the UK, France and Germany to perform the work. The current contract concerns Stage 1, i.e. the first 15 months of the ASAAC Phase II programme. Stage 1 is required to build on the results of the earlier ASAAC Phase I programme by firstly assessing and refining the architecture concepts and secondly by producing detailed specifications for Stage 2 demonstrations and outline standards for the architecture. The main objectives of this paper are, one year after the beginning of the ASAAC Phase II programme, to present as:•ASAAC Phase II programme objectives;•ASAAC Phase II contractual and industrial organisation;•Architecture concepts definition results of the Stage 1 work;•Demonstrations planned for Stage 2.}
}
@article{LETTNER2020351,
title = {An integrated approach for power transformer modeling and manufacturing},
journal = {Procedia Manufacturing},
volume = {42},
pages = {351-355},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.076},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306417},
author = {Christian Lettner and Michael Moser and Josef Pichler},
keywords = {cutting stock problem, manufacturing, optimization, machine learning, software engineering},
abstract = {Essential characteristics of smart factories, such as flexibility and resource efficiency, can be leveraged and improved by the power of machine learning and optimization techniques. For instance, the manufacturing process of a power transformer core constitutes a highly complex optimization problem. It involves creating a cost optimal slitting plan that meets all customer requirements and at the same time takes into account flexible and short-term constraints from production (e.g. current available metal bands in stock). As many of these constraints rely on forecasts, a learning system may provide the necessary predictions for these constraints. In addition, companies apply and maintain engineering software for a variety of tasks in construction, simulation, and interpretation of data. For instance, electrical engineers use a variety of tools to design an initial model of a power transformer according to customer requirements and constraints. Such tools often incorporate knowledge that serves as input for optimization and forecast models as described before. If these models are improved over time using external machine learning libraries, the newly developed models must find their way back into the implementation of engineering tools. Knowledge scattered across multiple software systems bears risk of being inconsistent. Furthermore, keeping knowledge consistent without a systematic approach is time-consuming and error-prone. In this paper, we describe an approach that leverages software engineering methods and tools and that supports knowledge transfer between software systems for optimization and modelling tasks. The approach follows the idea of a single source of knowledge together with transformation into different representations, as required by different engineering tasks. The proposed approach was introduced at an industrial site to improve the manufacturing process of power transformer cores.}
}
@article{GERVASI2004703,
title = {SIMBEX: a portal for the a priori simulation of crossed beam experiments},
journal = {Future Generation Computer Systems},
volume = {20},
number = {5},
pages = {703-715},
year = {2004},
note = {Computational Chemistry and Molecular Dynamics},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2003.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03002498},
author = {O. Gervasi and A. Laganà},
keywords = {Internet portal, Molecular simulator, Computing Grid, Reaction dynamics},
abstract = {The architecture and the computational kernels of Simulation of Crossed Molecular Beam Experiments, an Internet portal managing the simulation of elementary bimolecular processes as those occurring in crossed beam apparatuses, is discussed. The construction of this portal is our contribution to project 003/001 of the COST in Chemistry action D23 (METACHEM: Metalaboratories for complex computational applications in chemistry). The portal is specifically designed to support the collaborative efforts of various Computational Chemistry and Computer Science European laboratories aimed at building an a priori molecular simulator based on a Grid infrastructure. Such an environment makes use of free-software packages and was implemented using Web technologies.}
}
@article{LI2004555,
title = {A new approach to image-based realistic architecture modeling with featured solid library},
journal = {Automation in Construction},
volume = {13},
number = {5},
pages = {555-564},
year = {2004},
note = {Current IT Research and Development in the Construction Industry of China},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2004.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580504000238},
author = {Hua Li and Weiyu Wu},
keywords = {Image-based, Architecture modeling, Featured component library},
abstract = {To overcome restrictions of present modeling techniques, and to approach quick and convenient 3D architecture modeling, we present a realistic modeling system on the basis of a featured solid library. The system incorporates reference library techniques to build up realistic models from a single image. In our approach, parameterized solid components are mapped to 2D featured projection graphs on image plane, and matching features of all component graphs with the resource image results in 3D realistic scene, whether complex or irregular. Besides, a suite of texture toolkit is implemented to rectify flaws of texture mapping and to enhance modeling reality. Our approach works feasibly and robustly with single-vision images taken from arbitrary view angles, and the efficiency of our approach is proved in systematic modeling experiments. As our approach complemented image-based modeling techniques in construction CAD and digital city systems, it could be of popular use in relevant modeling software.}
}
@article{HUANG201558,
title = {Optimisation of site layout planning for multiple construction stages with safety considerations and requirements},
journal = {Automation in Construction},
volume = {53},
pages = {58-68},
year = {2015},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2015.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580515000394},
author = {C. Huang and C.K. Wong},
keywords = {Construction site layout planning, Multiple construction stage site layout planning, Binary-mixed-integer-linear programming (BMILP), Safety design constraints and requirements},
abstract = {This paper develops a mathematical formulation to model and optimise site facility locations inside a construction site for a construction project's different stages. The existence and dimensions of a construction project's available locations and site facilities can be varied across the construction stages. Without proper planning, unnecessary facility relocations may be required in each construction stage, resulting in a higher construction cost and longer construction time due to the need to dismantle and set up site facilities. Site layout plans should be optimised using a multiple-stage model to avoid unnecessary changes to facility settings across construction stages, improving efficiency. The proposed site layout problem with multiple construction stages is formulated as a binary-mixed-integer-linear programme, which can be solved by a standard branch-and-bound algorithm using the commercial software package LINGO. The mathematical objective function established in the solution process aims to minimise the total cost, which consists of the material transportation cost between the relevant site facilities and the dismantling, setup and relocation costs for all of the involved site facilities in each construction stage. Numerical examples using the proposed mathematical model to optimise different site layout settings for a construction site are given, including (1) a reference site layout plan using the conventional static single-stage approach, (2) a multiple-stage construction site layout plan and (3) a multiple-stage site layout plan with additional safety design considerations.}
}
@article{WAN201558,
title = {A coupled connector element for nonlinear static pushover analysis of precast concrete diaphragms},
journal = {Engineering Structures},
volume = {86},
pages = {58-71},
year = {2015},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2014.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0141029614007706},
author = {Ge Wan and Dichuan Zhang and Robert B. Fleischman and Clay J. Naito},
keywords = {Precast concrete, Floor diaphragms, Nonlinear analysis, Finite elements},
abstract = {This paper describes the formulation of a diaphragm connector element developed for use in two-dimensional finite element (2D-FE) modeling of precast concrete diaphragms. The connector elements, composed of assemblages of standard element types readily available in most FE software package libraries, are nonlinear, coupled for shear–tension interaction, enable friction mechanisms, and possess descending branch behavior. Element construction is based on data from full-scale tests of common precast diaphragm connectors. The 2D-FE models have been employed in nonlinear static “pushover” analysis of isolated floor diaphragms to determine diaphragm stiffness, strength, deformation capacity, and limit state sequence. The use of discrete elements to model the precast diaphragm connectors permits the direct evaluation of local force and deformation demands acting on these details. Further, the coupled formulation is adaptable to complex force histories and deformation patterns in the floor diaphragm, thereby permitting the element to respond in realistic fashion. The models, verified for accuracy using large scale testing, are providing crucial information on capacity and limit states for calibrating performance-based design factors for a new seismic design methodology for precast concrete diaphragms.}
}
@article{XIE2023112043,
title = {A Poisson-Nernst-Planck single ion channel model and its effective finite element solver},
journal = {Journal of Computational Physics},
volume = {481},
pages = {112043},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112043},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001389},
author = {Dexuan Xie and Zhen Chao},
keywords = {Poisson-Nernst-Planck equations, Finite element method, Single ion channel, Potassium channel, Electric current calculation},
abstract = {A single ion channel is a membrane protein with an ion selectivity filter that allows only a single species of ions (such as potassium ions) to pass through in the “open” state. Its selectivity filter also naturally separates a solvent domain into an intracellular domain and an extracellular domain. Such biological and geometrical characteristics of a single ion channel are novelly adopted in the construction of a new kind of dielectric continuum ion channel model, called the Poisson-Nernst-Planck single ion channel (PNPSIC) model, in this paper. An effective PNPSIC finite element solver is then developed and implemented as a software package workable for a single ion channel with a three-dimensional X-ray crystallographic molecular structure and a mixture of multiple ionic species. Numerical results for a potassium channel confirm the convergence and efficiency of the PNPSIC finite element solver and demonstrate the high performance of the software package. Moreover, the PNPSIC model is applied to the calculation of electric current and validated by biophysical experimental data.}
}
@article{MURPHY201389,
title = {Historic Building Information Modelling – Adding intelligence to laser and image based surveys of European classical architecture},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {76},
pages = {89-102},
year = {2013},
note = {Terrestrial 3D modelling},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2012.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271612002079},
author = {Maurice Murphy and Eugene McGovern and Sara Pavia},
keywords = {CAD, Cultural heritage, Modelling, Architecture, Building, Software},
abstract = {Historic Building Information Modelling (HBIM) is a novel prototype library of parametric objects, based on historic architectural data and a system of cross platform programmes for mapping parametric objects onto point cloud and image survey data. The HBIM process begins with remote collection of survey data using a terrestrial laser scanner combined with digital photo modelling. The next stage involves the design and construction of a parametric library of objects, which are based on the manuscripts ranging from Vitruvius to 18th century architectural pattern books. In building parametric objects, the problem of file format and exchange of data has been overcome within the BIM ArchiCAD software platform by using geometric descriptive language (GDL). The plotting of parametric objects onto the laser scan surveys as building components to create or form the entire building is the final stage in the reverse engineering process. The final HBIM product is the creation of full 3D models including detail behind the object’s surface concerning its methods of construction and material make-up. The resultant HBIM can automatically create cut sections, details and schedules in addition to the orthographic projections and 3D models (wire frame or textured) for both the analysis and conservation of historic objects, structures and environments.}
}
@article{BALATON2013335,
title = {Operator training simulator process model implementation of a batch processing unit in a packaged simulation software},
journal = {Computers & Chemical Engineering},
volume = {48},
pages = {335-344},
year = {2013},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2012.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098135412002839},
author = {M.G. Balaton and L. Nagy and F. Szeifert},
keywords = {Batch, Batch processing unit, OTS, UniSim Design, Monofluid thermoblock, Thermometer models},
abstract = {In chemical industry, especially in the case of continuous processes, operator training simulators (OTS) are becoming widely used. With the help of these systems several operation and safety issues can be analysed, and the operating staff of the plant can be trained for handling different plant failures. The main part of the OTS is the process model that replaces the real technology. Hence, in control development the simulated process variables are required to be reasonably accurate. The paper presents the structure of the process model of a batch processing unit in UniSim Design, different model constructions of a jacketed batch reactor and the identification of the parameters affecting its hydrodynamic and thermal behaviour. Construction of the process model is the first step in developing the OTS of the pilot plant located in the authors’ laboratory. It can be an effective tool in the development of model-based control systems.}
}
@article{HARUN199477,
title = {Visual simulation of construction projects on a microcomputer},
journal = {Simulation Practice and Theory},
volume = {2},
number = {2},
pages = {77-90},
year = {1994},
issn = {0928-4869},
doi = {https://doi.org/10.1016/0928-4869(94)90015-9},
url = {https://www.sciencedirect.com/science/article/pii/0928486994900159},
author = {Z.B. Harun and G. Singh},
keywords = {Animation, CAD, Construction, Logistics, PC, Projects, Simulation, Visualisation},
abstract = {Under the Microsoft's Windows wnvironment, Drafix CAD, MS Project, Visual Basic and Q+E Database/VB packages are integrated. This integrated software is able to show the step by step progress of the construction work in compressed time on the computer screen under the Visual Basic environment. The user is allowed to view the project in the bar-chart or network modes under the MS Project environment. This software opens up a new possibility of linking project planning with project design/visualisation in the microcomputer environment. The approach is demonstrated to be feasible and worthy of further development for the benefit of designers and project managers as well as site engineers.}
}
@article{ZAKHEM2020100324,
title = {Three-dimensional investigation of how newly constructed buildings supported on raft foundations affect pre-existing tunnels},
journal = {Transportation Geotechnics},
volume = {22},
pages = {100324},
year = {2020},
issn = {2214-3912},
doi = {https://doi.org/10.1016/j.trgeo.2020.100324},
url = {https://www.sciencedirect.com/science/article/pii/S2214391219304271},
author = {Anna Maria Zakhem and Hany {El Naggar}},
keywords = {Three-dimensional analyses, Shallow foundation, Existing tunnel, Small-strain stiffness, New user-defined concrete model, Exclusion zone},
abstract = {In densely populated areas, there is increasing construction of high-rise buildings adjacent to existing tunnels. The interactions involved are complex. Based on experience and field monitoring, many tunnel owners impose exclusion zones for construction close to their tunnels. This paper studies the effect of a newly constructed building supported by a raft foundation on an intact pre-existing tunnel. With the aid of PLAXIS software, a detailed three-dimensional finite element analysis is used to conduct a parametric study showing the interaction between the burial location of the pre-existing tunnel and the new raft foundation. The intact concrete lining is modelled by using the newly developed concrete model included in the PLAXIS user-defined library. The model considers the non-linearity of the material behaviour and the distinction between the concrete strength in tension and compression. The soil medium where the tunnel system is constructed is simulated by using a hardening soil model with small-strain stiffness, which accounts for increased stiffness at small strains. The construction of the tunnel is divided into several phases, where each phase is simulated with the advancement of the shield boring machine. Accordingly, new design guidelines can be developed for raft foundations in close proximity to pre-existing tunnels.}
}
@article{LI201640,
title = {Risk Caused by Construction of the Metro Shaft Adjacent to Building and its Control Measure},
journal = {Procedia Engineering},
volume = {165},
pages = {40-48},
year = {2016},
note = {15th International scientific conference “Underground Urbanisation as a Prerequisite for Sustainable Development” 12-15 September 2016, St. Petersburg, Russia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.11.733},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816340942},
author = {Liyun Li and Zhongwang Qiu and Yingying Dong and Xiuli Du},
keywords = {metro shaft, construction mechanics, numerical simulation, control measure},
abstract = {Abstract: Before the construction of Metro station, many accessory structures need to be constructed firstly, including shaft, Cross channel, and metro station pilot tunnel, etc. Many Engineering cases show that the risks of the circumstances around the Metro station caused by the excavation of these accessory structures are the major component of the metro construction risk. In this paper, the risk caused by construction of the metro shaft adjacent to building was studied, which located at the crossings of ziyou Road and Renmin Street, Changchun, China. Firstly, the major risk was introduced and the construction mechanic behavior was researched by using MIDAS/GTS finite element software package. Then, a control method was proposed to eliminate this risk, which was verified by using finite element software. Finally, the proposed measure was also validated with the result of construction monitoring for this project. This works focus on studying the excavation mechanics behavior of metro shaft and its influence on the around environment, and these conclusions are useful for providing a basis for similar project construction in the future.}
}
@article{TAHERSIMA201744,
title = {Finite element modeling of hydration heat in a concrete slab-on-grade floor with limestone blended cement},
journal = {Construction and Building Materials},
volume = {154},
pages = {44-50},
year = {2017},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2017.07.176},
url = {https://www.sciencedirect.com/science/article/pii/S0950061817315180},
author = {Mohammad Tahersima and Paul Tikalsky},
keywords = {Hydration heat, Limestone blended cement, Finite element model, Mass concrete},
abstract = {Delayed ettringite formation and thermal cracks are two main concerns of temperature rise in mass concrete structures. The mass concrete containing limestone blended cement and fly ash has different hydration heat that was not well predicted by commercially available software. The purpose of this study is to provide such data for designers or software developers. In this study, results of thermocouple measurement at the construction site are compared with a widely used commercial finite element model (FEM) and a widely used concrete design software package. Verification of the FEM with experimental data has been completed with cement ASTM C595 Type I limestone blended cement (IL) with a 25% replacement by mass of ASTM C618 Type C fly ash (FC). In this study, the same mixture design of construction site was used in modeling of hydration heat of full scale building. The heat of hydration for cement Type I-II and 25% of FC and cement Type IL are predicted to compare the results of hydration heat generated by different mix proportions. The FEM result has a much better compatibility with the construction site data with respect to the commercial concrete software results. The prediction accuracy of finite element results is about 15% more for the maximum temperature rise and 30% more for the peak time. Cement Type IL has greater hydration heat than cement Type I-II. Applying measured ambient temperature and calorimetry results are two main factors in precise prediction of hydration heat.}
}
@article{STACHTIARI201852,
title = {Early validation of system requirements and design through correctness-by-construction},
journal = {Journal of Systems and Software},
volume = {145},
pages = {52-78},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.053},
url = {https://www.sciencedirect.com/science/article/pii/S016412121830150X},
author = {Emmanouela Stachtiari and Anastasia Mavridou and Panagiotis Katsaros and Simon Bliudze and Joseph Sifakis},
keywords = {Rigorous system design, Requirements formalization, Model-based design, Correctness-by-construction},
abstract = {Early validation of requirements aims to reduce the need for the high-cost validation testing and corrective measures at late development stages. This work introduces a systematic process for the unambiguous specification of system requirements and the guided derivation of formal properties, which should be implied by the system ’s structure and behavior in conjunction with its external stimuli. This rigorous design takes place through the incremental construction of a model using the BIP (Behavior-Interaction-Priorities) component framework. It allows building complex designs by composing simpler reusable designs enforcing given properties. If some properties are neither enforced nor verified, the model is refined or certain requirements are revised. A validated model provides evidence of requirements’ consistency and design correctness. The process is semi-automated through a new tool and existing verification tools. Its effectiveness was evaluated on a set of requirements for the control software of the CubETH nanosatellite and an extract of software requirements for a Low Earth Orbit observation satellite. Our experience and obtained results helped in identifying open challenges for applying the method in industrial context. These challenges concern with the domain knowledge representation, the expressiveness of used specification languages, the library of reusable designs and scalability.}
}
@article{GARCES2021111004,
title = {Three decades of software reference architectures: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {179},
pages = {111004},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111004},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001011},
author = {Lina Garcés and Silverio Martínez-Fernández and Lucas Oliveira and Pedro Valle and Claudia Ayala and Xavier Franch and Elisa Yumi Nakagawa},
keywords = {Software architecture, Reference architecture, Systematic mapping, Secondary study},
abstract = {Software reference architectures have played an essential role in software systems development due to the possibility of knowledge reuse. Although increasingly adopted by industry, these architectures are not yet completely understood. This work presents a panorama on existing software reference architectures, characterizing them according to their context, goals, perspectives, application domains, design approaches, and maturity, as well as the industry involvement for their construction. For this, we planned and conducted a systematic mapping study. During last decade, the number of reference architectures in very diverse application domains has increased, resulting from efforts of industry, academia, and through their collaborations. Academic reference architectures are oriented to facilitate the reuse of architectural and domain knowledge. The industry has focused on architectures for standardization with certain maturity level. However, the great amount of architectures studied in this work have been designed without following a systematic process, and they lack the maturity to be used in real software projects. Further investigations can be oriented to gathering empirical evidences, from different sources than academic data libraries, that allow to understand how references architectures have been constructed, utilized, and maintained during the whole software life-cycle.}
}
@article{CHANDRA2017348,
title = {Building Information Modeling in the Architecture-engineering Construction Project in Surabaya},
journal = {Procedia Engineering},
volume = {171},
pages = {348-353},
year = {2017},
note = {The 3rd International Conference on Sustainable Civil Engineering Structures and Construction Materials - Sustainable Structures for Future Generations},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817303533},
author = {Herry Pintardi Chandra and Paulus Nugraha and Evan Sutanto Putra},
keywords = {Building Information Modeling, project.},
abstract = {In current practice, many digital models do not contain sufficient information from designers to contractors and operators. A great deal of literature has pointed to the importance of understanding the Building Information Modeling (BIM). BIM is a digital representation of the physical and functional characteristics of a building. In Architecture-Engineering-Construction, BIM is the development and use a computer software model to stimulate the construction and operation of a facility, to make decisions and to improve the process of delivering the facility. The aims of this paper are to explore the need of technological support to implement the site-linked of BIM, the benefits of BIM, and the challenges of BIM. The research was conducted through literature review of BIM. The data was collected from the questionnaire survey carried out to 26 valid responses within the main stakeholders of construction work in Surabaya. A questionnaire is prepared by incorporating the technological support, benefits, and challenges of BIM. The data was analyzed by using descriptive analysis including mean analysis with 5 Likert scale. The results of the research shows that the need of technological support to implement the BIM is parametric components (mean value of 4.46), the need of software package majority is prepared by contractor and construction management consultant, and the benefit of BIM is to reduce the construction cost (mean value of 4.6). In addition, the main challenge of BIM is different brand with the mean value of 4.27, in which of incompatibility of different brand (mean value of 4.31.}
}
@article{ERRIPRADEEP2021103667,
title = {Blockchain-aided information exchange records for design liability control and improved security},
journal = {Automation in Construction},
volume = {126},
pages = {103667},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103667},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001187},
author = {Abhinaw Sai {Erri Pradeep} and Tak Wing Yiu and Yang Zou and Robert Amor},
keywords = {Blockchain, Construction industry, Information exchange, Building Information Modelling (BIM), Record-keeping, Design liability, Information security, Smart contracts, Ethereum, Design science research, Prototype evaluation},
abstract = {With the recent advances in Information and Communication Technologies in the construction industry, information is exchanged digitally with little regard to the contracts that govern them. Although parties collaborating in project design are contracted to the client, they transact with each other when using BIM and other collaborative practices without any direct contractual relationship among themselves. This results in a lack of design liability control and an increase in claims and disputes. Further, the use of multiple software packages results in the exposure of data to third parties, data corruption and compromise in data privacy (using data for unintended purposes), data integrity (unauthorised access to sensitive data), and data longevity (loss of data post-handover). This study investigates blockchain technology (BCT) to address these issues using a design science research method. The current information exchange processes were mapped to identify the critical transactions that may benefit from record-keeping on the blockchain. Next, a prototype was designed to demonstrate and evaluate the proposed BCT integrated process models. Three key project processes, design review, design coordination and request for information; and two potential conflict scenarios during and post-construction were simulated as part of the evaluation. The prototype's implementation exhibits BCT's ability to record snapshots of individual design inputs to the overall project design and to enable a clear and long-term record of key exchange transactions. This improves the design liability control for contributing stakeholders and the auditability of the exchange records. Further, the proofs derived from such a system are independent of any third-party storage or subscription. Given the nature of records stored in a blockchain, the existence, integrity, and authenticity of information along with its associated metadata can be verified in the long-term as well. Therefore, BCT could be a supplementary technology that supports the existing information exchange systems.}
}
@article{WU2020103004,
title = {Incorporating multi-physics deterioration analysis in building information modeling for life-cycle management of durability performance},
journal = {Automation in Construction},
volume = {110},
pages = {103004},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.103004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519305618},
author = {Jie Wu and Michael D. Lepech},
keywords = {Building information modeling, Facility management, Durability, Life-cycle management, Sustainability},
abstract = {Extending the application of Building Information Modeling (BIM) tools into the operation and management phases of a building's life cycle is a significant advance in recent years. However, few studies have investigated or developed tools for the life-cycle management of durability performance of built structures leveraging BIM. This gap is partly due to (i) the lack of an accessible interface that connects BIM software with durability models of built structures, and (ii) no available durability models that are fundamental in nature and thus can integrate with other tools. To begin addressing this gap, this paper presents a framework that uses the Application Programming Interface (API) within BIM to integrate physics-based durability performance models, which extends the application of BIM into the post-construction phases by enabling more accurate predictions of life cycle performance. An application of the framework is presented for analysis and management of a reinforced concrete structure.}
}
@article{HARRISON1996417,
title = {Integrating multiple and diverse abstract knowledge types in real-time embedded systems},
journal = {Knowledge-Based Systems},
volume = {9},
number = {7},
pages = {417-434},
year = {1996},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(96)01052-0},
url = {https://www.sciencedirect.com/science/article/pii/S0950705196010520},
author = {Alan Harrison and Peter G. Thomas},
keywords = {Real-time embedded system, Knowledge-based techniques, Knowledge-based components, Abstract knowledge types, Blackboard architecture, Software engineering principles, Ada},
abstract = {Designers of large-scale real-time systems are increasingly turning to knowledge-based techniques in order to solve complex problems. This paper identifies three essential needs to support the implementation of these systems: first, the need to provide a variety of knowledge-based components that can be used to model the diverse expert domains being encountered; second, the need to provide the user with the means of creating multiple independent instances of the knowledge-based components; and third, the need to provide an integrating environment in which the knowledge-based instances may be controlled. This paper uses ideas derived from the concept of abstract data types and recommends the construction of a library of diverse knowledge-based components, called abstract knowledge types, and that multiple instances of the abstract knowledge types be integrated and controlled using a blackboard architecture. A prototype component library and a blackboard have been implemented in Ada in order to take advantage of a real-time language which supports software engineering principles through a well defined and enforced standard. The use of abstract knowledge types gives a uniform software engineered approach to the development and integration of both conventional and knowledge-based components.}
}
@article{HUANG2022104232,
title = {Multi-LOD BIM for underground metro station: Interoperability and design-to-design enhancement},
journal = {Tunnelling and Underground Space Technology},
volume = {119},
pages = {104232},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104232},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004235},
author = {M.Q. Huang and H.M. Zhu and J. Ninić and Q.B. Zhang},
keywords = {Building information modelling (BIM), Level of detail (LOD), Industry Foundation Classes (IFC), Numerical modelling, Interoperability},
abstract = {Underground metro stations as essential large-scale infrastructures that ease the traffic congestion on the overcrowded urban surface should be thoroughly designed, constructed and maintained. Building information modelling (BIM) has been increasingly employed for project design authoring, construction monitoring and operation management to promote digital engineering. Geotechnical design engaging numerical modelling techniques is an indispensable process for underground construction projects. These cross-disciplinary processes employing heterogeneous applications are yet to be coordinated in an efficient way, and thus improved interoperability can streamline the processes and optimise engineering design. This paper proposes an extension to the primary standard for openBIM data exchange – the Industry Foundation Classes (IFC) to facilitate wider adoption of BIM in underground infrastructure design, construction and management. Then a multiple level-of-detail (LOD) metro station BIM model is introduced to represent information at distinct levels of geometric and semantic richness required for varying application scenarios. Finally, we suggest a workflow underpinned by heuristic techniques, as an intermediate solution, to enhance the interoperability between a BIM authoring tool and a numerical modelling program for geotechnical analysis, and exemplify the workflow with the State Library Station of Melbourne. The proposed workflow offers an automated error-free, design-to-design solution, and hence enables the efficient exploration of design scenarios and construction optimisation before IFC is adopted by geotechnical modelling software.}
}
@article{AVNI201850,
title = {Synthesis from component libraries with costs},
journal = {Theoretical Computer Science},
volume = {712},
pages = {50-72},
year = {2018},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304397517308277},
author = {Guy Avni and Orna Kupferman},
keywords = {Synthesis, Component libraries, Cost-sharing games, Lattice automata},
abstract = {Synthesis is the automated construction of a system from its specification. In real life, hardware and software systems are rarely constructed from scratch. Rather, a system is typically constructed from a library of components. Lustig and Vardi formalized this intuition and studied LTL synthesis from component libraries. In real life, designers seek optimal systems. In this paper we add optimality considerations to the setting. We distinguish between quality considerations (for example, size – the smaller a system is, the better it is), and pricing (for example, the payment to the company who manufactured the component). We study the problem of designing systems with minimal quality-cost and price. A key point is that while the quality cost is individual – the choices of a designer are independent of choices made by other designers that use the same library, pricing gives rise to a resource-allocation game – designers that use the same component share its price, with the share being proportional to the number of uses (a component can be used several times in a design). We study both closed and open settings, and in both we solve the problem of finding an optimal design. In a setting with multiple designers, we also study the game-theoretic problems of the induced resource-allocation game.}
}
@article{HUFNAGEL1994375,
title = {Health care professional workstation: software system construction using DSSA scenario-based engineering process},
journal = {International Journal of Bio-Medical Computing},
volume = {34},
number = {1},
pages = {375-386},
year = {1994},
note = {The Health Care Professional Workstation},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(94)90038-8},
url = {https://www.sciencedirect.com/science/article/pii/0020710194900388},
author = {Stephen Hufnagel and Karan Harbison and John Silva and Erik Mettala},
keywords = {Scenario-based engineering process (SEP), Software engineering, Domain-specific software architectures (DSSA), Health care systems architecture, Workstations},
abstract = {This paper describes a new method for the evolutionary determination of user requirements and system specifications called scenario-based engineering process (SEP). Health care professional workstations are critical components of large scale health care system architectures. We suggest that domain-specific software architectures (DSSAs) be used to specify standard interfaces and protocols for reusable software components throughout those architectures, including workstations. We encourage the use of engineering principles and abstraction mechanisms. Engineering principles are flexible guidelines, adaptable to particular situations. Abstraction mechanisms are simplifications for management of complexity. We recommend object-oriented design principles, graphical structural specifications, and formal components' behavioral specifications. We give an ambulatory care scenario and associated models to demonstrate SEP. The scenario uses health care terminology and gives patients' and health care providers' system views. Our goal is to have a threefold benefit. (i) Scenario view abstractions provide consistent interdisciplinary communications. (ii) Hierarchical object-oriented structures provide useful abstractions for reuse, understandability, and long term evolution. (iii) SEP and health care DSSA integration into computer aided software engineering (CASE) environments. These environments should support rapid construction and certification of individualized systems, from reuse libraries.}
}
@article{LUCHETTA2015512,
title = {Control and data acquisition of the ITER full-scale ion source for the neutral beam test facility},
journal = {Fusion Engineering and Design},
volume = {96-97},
pages = {512-516},
year = {2015},
note = {Proceedings of the 28th Symposium On Fusion Technology (SOFT-28)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2015.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0920379615300302},
author = {Adriano Luchetta and Gabriele Manduchi and Cesare Taliercio and Francesco Paolucci and Filippo Sartori and Lennart Svensson and Carmelo Vincenzo Labate and Mauro Breda and Roberto Capobianco and Federico Molon and Modesto Moressa and Paola Simionato and Enrico Zampiva and Paolo Barbato and Sandro Polato},
keywords = {Control, Data acquisition, Neutral beam injectors, ITER, SPIDER},
abstract = {The neutral beam test facility, which is under construction in Padova, Italy, is developing the ITER full-scale ion source for the ITER heating neutral beam injectors, referred to as the SPIDER experiment, and the full-size prototype injector, referred to as MITICA. The SPIDER control and data acquisition system (CODAS) has been developed and its construction will start in 2014. Slow control and data acquisition will be based on the ITER CODAC core system software suite that has been designed to facilitate the integration of ITER plant systems with CODAC. Fast control and data acquisition will use solutions specific to the test facility, as the corresponding concepts are not ready-to-use in the ITER design. The ITER hardware catalog for fast control has been taken into consideration. The software development will be based on the integration of MDSplus and MARTe, two framework software packages that are well known in the fusion community, targeting data organization and fast real-time control, respectively. The paper revises the system requirements and the system design and shows the results already achieved in terms of system integration. In addition, the paper will report the experience in the usage of different cooperating software frameworks and in the integration of industrial procured plant systems.}
}
@article{WANG2015431,
title = {Computer program for directed structure topology optimization},
journal = {Acta Mechanica Solida Sinica},
volume = {28},
number = {4},
pages = {431-440},
year = {2015},
issn = {0894-9166},
doi = {https://doi.org/10.1016/S0894-9166(15)30028-8},
url = {https://www.sciencedirect.com/science/article/pii/S0894916615300288},
author = {Xianjie Wang and Xun'an Zhang and Kepeng Cheng},
keywords = {bi-directional evolutionary structural optimization (BESO), continuum structures, computer program development, improved algorithm, directed structure topology optimization, portion construction design},
abstract = {To compensate for the imperfection of traditional bi-directional evolutionary structural optimization, material interpolation scheme and sensitivity filter functions are introduced. A suitable filter can overcome the checkerboard and mesh-dependency. And the historical information on accurate elemental sensitivity numbers are used to keep the objective function converging steadily. Apart from rational intervals of the relevant important parameters, the concept of distinguishing between active and non-active elements design is proposed, which can be widely used for improving the function and artistry of structures directly, especially for a one whose accurate size is not given. Furthermore, user-friendly software packages are developed to enhance its accessibility for practicing engineers and architects. And to reduce the time cost for large time-consuming complex structure optimization, parallel computing is built-in in the MATLAB codes. The program is easy to use for engineers who may not be familiar with either FEA or structure optimization. And developers can make a deep research on the algorithm by changing the MATLAB codes. Several classical examples are given to show that the improved BESO method is superior for its handy and utility computer program software.}
}
@article{ELHADI201462,
title = {Enhancing the detection of metamorphic malware using call graphs},
journal = {Computers & Security},
volume = {46},
pages = {62-78},
year = {2014},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2014.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167404814001060},
author = {Ammar Ahmed E. Elhadi and Mohd Aizaini Maarof and Bazara I.A. Barry and Hentabli Hamza},
keywords = {Computer security, Malware, Malware detection, API call graph, API call graph construction algorithm, API call graph matching algorithm},
abstract = {Malware stands for malicious software. It is software that is designed with a harmful intent. A malware detector is a system that attempts to identify malware using Application Programming Interface (API) call graph technique and/or other techniques. API call graph techniques follow two main steps, namely, transformation of malware samples into an API call graph using API call graph construction algorithm, and matching the constructed graph against existing malware call graph samples using graph matching algorithm. A major issue facing malware API call graph construction algorithms is building a precise call graph from information collected about malware samples. On the other hand call graph matching is an NP-complete problem and is slow because of computational complexity. In this study, a malware detection system based on API call graph is proposed. In the proposed system, each malware sample is represented as an API call graph. API call graph construction algorithm is used to transform input malware samples into API call graph by integrating API calls and operating system resource to represent graph nodes. Moreover, the dependence between different types of nodes is identified and represented using graph edges. After that, graph matching algorithm is used to calculate similarity between the input sample and malware API call graph samples that are stored in a database. The graph matching algorithm is based on an enhanced graph edit distance algorithm that simplifies the computational complexity using a greedy approach to select best common subgraphs from the integrating API call graph with high similarity, which helps in terms of detecting metamorphic malware. Experimental results on 514 malware samples demonstrate that the proposed system has 98% accuracy and 0 false positive rates. Detailed comparisons against other detection methods have been carried out and significant improvement over them is shown.}
}
@article{CABALLE20102083,
title = {CLPL: Providing software infrastructure for the systematic and effective construction of complex collaborative learning systems},
journal = {Journal of Systems and Software},
volume = {83},
number = {11},
pages = {2083-2097},
year = {2010},
note = {Interplay between Usability Evaluation and Software Development},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2010.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0164121210001652},
author = {Santi Caballé and Fatos Xhafa},
keywords = {Software architecture and design, Software engineering methods, Software reuse, Component-based software engineering, Model-driven engineering, Service orientation, SOA, Computer-supported collaborative learning, E-learning, Software and systems education},
abstract = {Over the last decade, e-Learning and in particular Computer-Supported Collaborative Learning (CSCL) needs have been evolving accordingly with more and more demanding pedagogical and technological requirements. As a result, high customization and flexibility are a must in this context, meaning that collaborative learning practices need to be continuously adapted, adjusted, and personalized to each specific target learning group. These very demanding needs of the CSCL domain represent a great challenge for the research community on software development to satisfy. This contribution presents and evaluates a previous research effort in the form of a generic software infrastructure called Collaborative Learning Purpose Library (CLPL) with the aim of meeting the current and demanding needs found in the CSCL domain. To this end, we experiment with the CLPL in order to offer an advanced reuse-based service-oriented software engineering methodology for developing CSCL applications in an effective and timely fashion. A validation process is provided by reporting on the use of the CLPL platform as the primary resource for the Master's thesis courses at the Open University of Catalonia when developing complex software applications in the CSCL domain. The ultimate aim of the whole research is to yield effective CSCL software systems capable of supporting and enhancing the current on-line collaborative learning practices.}
}
@article{TYCH201219,
title = {A Matlab software framework for dynamic model emulation},
journal = {Environmental Modelling & Software},
volume = {34},
pages = {19-29},
year = {2012},
note = {Emulation techniques for the reduction and sensitivity analysis of complex environmental models},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2011.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815211001915},
author = {W. Tych and P.C. Young},
keywords = {Emulation, DACE, DBM, Dominant modes, Transfer function, Matlab},
abstract = {The paper describes a software framework for implementing the main stages of the Data Based Mechanistic (DBM) modelling approach to the reduced order emulation (meta-modelling) of large dynamic system computer models, within the Matlab software environment. The framework exploits routines in the CAPTAIN Toolbox to identify and estimate transfer function models that reflect the dominant modes of the dynamic behaviour in the large model. This allows for the ‘nominal emulation’ and validation of the large model for a single, specified set of parameters; as well as ‘stand-alone, full emulation’ based on the construction and validation of hyper-dimensional maps between a user-specified range of large model parameters and the parameters of the associated, low order transfer function models. The software framework uses the multivariable structure constructs available within Matlab™ to form a small library of routines that will become part of the Captain Toolbox. The library is formed around special data structures that facilitate multivariable operations and visualisations which both enhance the efficiency of the emulation modelling analysis and the modeller’s interaction with the process of emulation. The nature of the analysis is illustrated by a topical example concerned with the emulation of the OTIS computer simulation model for the transport and dispersion of solutes in a river system.}
}
@article{LEGLISE2001577,
title = {Computer-stimulated design: construction of a personal repertoire from scattered fragments},
journal = {Automation in Construction},
volume = {10},
number = {5},
pages = {577-588},
year = {2001},
note = {EuropIA '98 - Cyber Design},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(00)00069-8},
url = {https://www.sciencedirect.com/science/article/pii/S0926580500000698},
author = {Michel Léglise},
keywords = {Architectural design, Interpretation, Design learning},
abstract = {This paper describes some possibilities of creating and structuring a personal digital memory capable of facilitating architectural design and design learning. The raw materials of this memory are different representations that can be found on the Web. Having interpreted these representations, one is able to construct a meaningful memory, educated and personal, which can be called upon subsequently during the design phase, as long as one has a medium that can represent this memory and put it to good use. As a practical, effective application of this process, we will describe part of a configuration geared towards the learning of architectural design. This configuration is composed of various elements, precisely arranged in space and time in a set of interrelations and interactions. The design student is placed at the centre of the arrangement, from where he or she can call on a broad spectrum of possibilities from the Web as provider of image documents. When necessary, students can use specially developed software that allows them a verbal and pictorial interpretation stimulated during particular phases of the learning process. In this way, through pictorial material presented on the network, the students can build up a digital library appropriate to their own understanding of architecture and their own representation of the world. At this point, they can abandon the universe of digital documents and media and return to the world of materials and shapes in intensive design studio sessions, where slowly maturing ideas can at last find concrete form. Thus, we deal with the relationship between the public, shareable aspect of the documents, and the private aspect: the individual interpretation of these documents. In the same way, we show how, within the framework of the teaching programme that has been set up, and without interference, this relationship between public and private can be linked into a dimension of the work of learning which is at times personal, at times collective. The conclusion attempts to outline the issues raised by this sort of configuration, and to show how thoughtful use of computers and networks can stimulate and enrich design rather than just “aid” it, as is generally accepted.}
}
@article{BRODSKY2007371,
title = {SIMULATION AND CONTROL OF FLEXIBLE VEHICLES},
journal = {IFAC Proceedings Volumes},
volume = {40},
number = {7},
pages = {371-376},
year = {2007},
note = {17th IFAC Symposium on Automatic Control in Aerospace},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20070625-5-FR-2916.00064},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015332705},
author = {S.A. Brodsky and A.V. Nebylov and A.I. Panferov},
keywords = {simulation, control, flexible, vehicle dynamics, flight control, system synthesis},
abstract = {Possible approaches to the mathematical description of different types of flexible vehicles in view of oscillations of fluid in tanks and moving masses inside the vehicle are observed. Elastic bending of a body surface in interaction with a surrounding medium in a broad band of speed variation are taken into account. Problems of regulator's synthesis, damping of elastic oscillations, and also principles of construction of universal software for research of dynamic properties and simulations of elastic vehicles motion are considered. For implementation of suggested methods and algorithms the specialized program is designed. The software package is supplied with the program modules library. These modules are designed on the basis of mathematical models of the vehicle elements and control system, and also the significant physical phenomena such as flexibility, liquid oscillations, time lag of engines, local aerodynamic effects, etc. Functioning of the program is demonstrated and outcomes of calculations are presented.}
}
@article{BELOSTOTSKIY2015108,
title = {Verificated Techniques for the Numerical Simulation of Extreme Impacts on NPP Constructions},
journal = {Procedia Engineering},
volume = {111},
pages = {108-114},
year = {2015},
note = {XXIV R-S-P seminar, Theoretical Foundation of Civil Engineering (24RSP) (TFoCE 2015)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815013120},
author = {Alexandr M. Belostotskiy and Irina N. Afanasyeva and Sergey O. Petryashev and Nicolay O. Petryashev},
keywords = {Extreme loads, Nuclear power plants (NPP) structures, Computational fluid dynamic (CFD) methods, Finite element (FE) methods, Program packages, Examples of calculations.},
abstract = {The distinctive paper is devoted to advanced methods of analysis, choice of appropriate and correct methods and software application packages for numerical simulation of external loads and impacts on the basic structures of nuclear power plants (wind hurricane, tornado, plane crash, the impact of the shock wave, seismic and tsunami effect). The article gives examples of representative calculations.}
}
@article{CERASOLI2021110828,
title = {Advanced modeling of materials with PAOFLOW 2.0: New features and software design},
journal = {Computational Materials Science},
volume = {200},
pages = {110828},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2021.110828},
url = {https://www.sciencedirect.com/science/article/pii/S0927025621005486},
author = {Frank T. Cerasoli and Andrew R. Supka and Anooja Jayaraj and Marcio Costa and Ilaria Siloi and Jagoda Sławińska and Stefano Curtarolo and Marco Fornari and Davide Ceresoli and Marco {Buongiorno Nardelli}},
keywords = {DFT, Electronic structure,  tight-binding, High-throughput calculations},
abstract = {Recent research in materials science opens exciting perspectives to design novel quantum materials and devices, but it calls for quantitative predictions of properties which are not accessible in standard first principles packages. PAOFLOW, is a software tool that constructs tight-binding Hamiltonians from self-consistent electronic wavefunctions by projecting onto a set of atomic orbitals. The electronic structure provides numerous materials properties that otherwise would have to be calculated via phenomenological models. In this paper, we describe recent re-design of the code as well as the new features and improvements in performance. In particular, we have implemented symmetry operations for unfolding equivalent k-points, which drastically reduces the runtime requirements of first principles calculations, and we have provided internal routines of projections onto atomic orbitals enabling generation of real space atomic orbitals. Moreover, we have included models for non-constant relaxation time in electronic transport calculations, doubling the real space dimensions of the Hamiltonian as well as the construction of Hamiltonians directly from analytical models. Importantly, PAOFLOW has been now converted into a Python package, and is streamlined for use directly within other Python codes. The new object oriented design treats PAOFLOW’s computational routines as class methods, providing an API for explicit control of each calculation.}
}
@article{PEANSUPAP2015379,
title = {Identifying Issues of Change Leading to Cost Conflicts: Case Study in Cambodia},
journal = {Procedia Engineering},
volume = {123},
pages = {379-387},
year = {2015},
note = {Selected papers from Creative Construction Conference 2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.10.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815031446},
author = {Vachara Peansupap and Lakhena Cheang},
keywords = {Cambodia, change issues, contractor, cost conflict, owner},
abstract = {It is impossible to avoid changes in construction projects. Construction changes have minor and major effects on the success of a project, particularly on its cost. A detailed study of change is needed to alleviate such effects on project cost. This paper introduces the identification of specific change issues regarding costs that may lead to conflicts between the owner and the contractor of a project. Such identification would help project practitioners be aware of change issues and manage them effectively as well as open up alternative viewpoints of change by the two parties. To achieve the research objective, a 16-item questionnaire with a five-point Likert scale was conducted to collect the necessary information. The data were collected from respondents in Cambodian building construction projects. Responses from 53 professionals working for contractors and owners were analysed with the aid of software package, Statistical Package for Social Science (SPSS). The result of tests passed internal reliability of the scale and significant agreement among the respondents. The analysis of a one sample t-test showed that there were fifteen significant and important change issues. The five top priority change issues related to project costs are changes in scheduling made by the owner, project scope change made by the owner, change due to poor and incomplete design, change in design functions required by owner/client, and change in material specifications. The research findings would be beneficial to all project participants for gaining a deeper understanding of specific change issues so that change management can be carried out more efficiently. In addition, project stakeholders would be more perceptive about the consequences of any changes.}
}
@article{DODERO201731,
title = {Trade-off between interoperability and data collection performance when designing an architecture for learning analytics},
journal = {Future Generation Computer Systems},
volume = {68},
pages = {31-37},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.06.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16302813},
author = {Juan Manuel Dodero and Enrique Juan González-Conejero and Guillermo Gutiérrez-Herrera and Sonia Peinado and José Tomás Tocino and Iván Ruiz-Rube},
keywords = {Technology-Enhanced Learning (TEL), Virtual Learning Environment (VLE), Learning Analytics (LA), Experience API (xAPI), Learning Record Store (LRS)},
abstract = {The heterogeneity of external systems that can be connected in an e-learning environment can impose interoperability and performance requirements for recording and storing the learning data. Web-based protocols have been developed to improve e-learning systems’ interoperability and capability to perform meaningful analytics. The present paper describes a web-based learning environment aimed at training how to command and control unmanned autonomous vehicles, provided with analytic capabilities. It integrates an external web content management system and a simulation engine that present different performance requirements for recording all significant events that occur during the learning process. Its record store construction, based on standard interoperability protocols, is explored here from the performance viewpoint. The tests that were conducted to assess regular data stores used for learning analytics show that performance should not be overlooked when constructing and deploying learning analytics systems.}
}
@article{SCHMID1993765,
title = {Standardized Data Exchange between CACSD Systems},
journal = {IFAC Proceedings Volumes},
volume = {26},
number = {2, Part 2},
pages = {765-770},
year = {1993},
note = {12th Triennal Wold Congress of the International Federation of Automatic control. Volume 2 Robust Control, Design and Software, Sydney, Australia, 18-23 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)49048-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017490483},
author = {Chr. Schmid and R. Schumann},
keywords = {Standardization, data exchange, computer-aided design, data structures, data handling, computer software},
abstract = {With the increasing number of CACSD program packages the number of different data formats increases, which are used to store measurement data and design results in form of process models, control systems and simulation data. In order to exchange such data between different program packages, the ≪VDI/GMA Committee 5.6 CACSD Tools≫ has proposed a guideline, which describes a standard data format for CACSD data objects. The purpose of this paper is the presentation of this proposal. It comprises the discussion of the data objects within several object classes, the syntactical formulation according to DIN 66280 and application examples.}
}
@article{ZHOU2022104571,
title = {CloudFAS: Cloud-based building fire alarm system using Building Information Modelling},
journal = {Journal of Building Engineering},
volume = {53},
pages = {104571},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.104571},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222005848},
author = {Xiaoping Zhou and Haoran Li and Jia Wang and Jichao Zhao and Qingsheng Xie and Lei Li and Jiayin Liu and Jun Yu},
keywords = {Building information modelling (BIM), Building fire, Building fire alarm system (FAS), Natural language processing (NLP), Cloud computing},
abstract = {Building fires are a common urban disaster. The emergence of high-rise, large-scale and inner-complex buildings bring new challenges for fire safety and triggers new demand to upgrade traditional building fire alarm system (FAS). Different from current studies by deploying enormous smart fire sensors to replace FAS, this study addresses this issue from a novel perspective and proposes a cloud-based FAS using Building Information Modelling (BIM) on top of FAS, termed CloudFAS. Firstly, the system framework and the software architecture are designed. Secondly, two key technologies are presented to address two unresolved technical issues: private fire alarm data sharing and alignment of fire sensors with the BIM model. A cloud gateway for fire sensors is developed to address the first problem by capturing the fire alarm data from the fire alarm control unit through the IEEE 1824 standard. Noticing that the fire sensor locations are listed in a sensor installation spreadsheet using natural language, termed as sensor location table (SLT). A natural language processing (NLP)-based sensor-BIM alignment algorithm is proposed to automatically match fire sensors with the BIM model through SLT, which enables to display fire sensor statuses in proper places in the 3D BIM model. Finally, a concrete case study from the China Construction Library is presented, which verifies the effectiveness of our proposed CloudFAS. Our CloudFAS is built on top of traditional FAS. If the fire alarm control unit follows the IEEE 1824 standard and an SLT is available, then CloudFAS can upgrade the traditional FAS in existing buildings effortlessly with its BIM model. Moreover, the cloud gateway for fire sensors contributes to addressing the private data sharing problem using IEEE 1824 standard, and the NLP-based sensor-BIM alignment algorithm can promote the adoption of BIM in the building operation phase.}
}
@article{SHASH20051,
title = {Survey of Procedures Adopted by A/E Firms in Accounting for Design Variables in Early Cost Estimates},
journal = {Journal of King Saud University - Engineering Sciences},
volume = {18},
number = {1},
pages = {1-16},
year = {2005},
issn = {1018-3639},
doi = {https://doi.org/10.1016/S1018-3639(18)30819-5},
url = {https://www.sciencedirect.com/science/article/pii/S1018363918308195},
author = {Ali A. Shash and Doko Ibrahim},
keywords = {A/E firms, Design variables, Early cost estimates, Estimating techniques},
abstract = {The success or failure of a construction project depends on the reliability of the cost estimates prepared, especially those in the early phase of its development. The objectives of this study were to investigate the early cost estimating techniques and the procedures adopted by A/E firms in accounting for design variables in the early cost estimates they prepare for residential buildings. These were achieved through the administration of a structured questionnaire. Nineteen A/E firms, practicing in the Eastern Province of Saudi Arabia, participated in the survey. The survey results revealed that most of the A/E firms do not utilize specialized software packages in carrying out estimating services, any systematic procedures in accounting for design variables, or any models developed by construction researchers. The consequences of mal-assessment of the cost implications of design variables were also revealed by the study.}
}
@article{TSAI2010476,
title = {A three-stage framework for introducing a 4D tool in large consulting firms},
journal = {Advanced Engineering Informatics},
volume = {24},
number = {4},
pages = {476-489},
year = {2010},
note = {Construction Informatics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2010.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1474034610000261},
author = {Meng-Han Tsai and Shih-Chung Kang and Shang-Hsien Hsieh},
keywords = {4D tool, Usability test, Workflow},
abstract = {The increase in the use of 4D management tools in recent years within the construction industry has been phenomenal, partly due to the increasing support available in commercial software packages, and partly in response to a greater demand for efficient construction management. However, successfully implementing a 4D management tool in an engineering firm for use in actual projects remains a challenging task. This paper presents the authors’ experiences of implementing an in-house 4D management tool at a large engineering, procurement and construction (EPC) firm with a long history of design–build projects. A three-stage consulting framework of system evaluation, usability study, and management plan (SUM) was proposed and implemented for a firm of this size, which included three parts: (1) System evaluation: requirement analysis and performance evaluation of both hardware and software components of the 4D tool; (2) Usability study: usability tests and improvement of the 4D tool; and (3) Management plan: workflow re-engineering for the firm to be able to successfully implement and apply the 4D management tool to actual projects. We found that the SUM framework was able to effectively identify major problems when introducing a 4D tool to a large design–build project and helped to minimize its own impact on the firm’s business processes.}
}
@article{KHUZIN2021102030,
title = {Processes of structure formation and paste matrix hydration with multilayer carbon nanotubes additives},
journal = {Journal of Building Engineering},
volume = {35},
pages = {102030},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.102030},
url = {https://www.sciencedirect.com/science/article/pii/S2352710220336627},
author = {Airat Khuzin and Ruslan Ibragimov},
keywords = {Multilayer carbon nanotubes, Modification, Strength, Dispersion},
abstract = {The creation of high-strength materials with targeted structure formation and hydration to obtain building composites with a given set of properties represents a relevant area of the modern construction sector. This paper investigates promising nanomodifying additives i.e. multilayer carbon nanotubes along with an effective method to obtain a comprehensive nanomodifying additive through pre-ultrasonic dispersion of MWCNT globules of varying dispersive capacity in an isopropyl alcohol medium. The comprehensive nanomodifying additive obtained is characterized by high stability as well as a uniform distribution of carbon nanotubes in the additive. The Structure software package developed by the authors was used to study the structure of paste matrix at the mesastructural level. The modification of paste matrix by means of the developed comprehensive nanostructure additive reduces the crack length index to 113% with the crack length ratio and the crack shape ratio going down to 61% and 34%, respectively. The increase in the strength and durability of the compositions modified by the developed comprehensive nanomodifying additive is due to a more complete filling of the intergranular void with newly formed structures in the form of C-S-H (I), tobermorite and gel phases which enhance the degree of packing and density of the composite under examination. However, the share of nano - and micropores goes up by 1.21 and 1.31 times, respectively, while the share of meso - and macropores decreases by 1.51 and 5.25 times.}
}
@article{QIU201681,
title = {Understanding the API usage in Java},
journal = {Information and Software Technology},
volume = {73},
pages = {81-100},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300027},
author = {Dong Qiu and Bixin Li and Hareton Leung},
keywords = {API usage, Empirical study, Java},
abstract = {Context
Application Programming Interfaces (APIs) facilitate the use of programming languages. They define sets of rules and specifications for software programs to interact with. The design of language API is usually artistic, driven by aesthetic concerns and the intuitions of language architects. Despite recent studies on limited scope of API usage, there is a lack of comprehensive, quantitative analyses that explore and seek to understand how real-world source code uses language APIs.
Objective
This study aims to understand how APIs are employed in practical development and explore their potential applications based on the results of API usage analysis.
Method
We conduct a large-scale, comprehensive, empirical analysis of the actual usage of APIs on Java, a modern, mature, and widely-used programming language. Our corpus contains over 5000 open-source Java projects, totaling 150 million source lines of code (SLoC). We study the usage of both core (official) API library and third-party (unofficial) API libraries. We resolve project dependencies automatically, generate accurate resolved abstract syntax trees (ASTs), capture used API entities from over 1.5 million ASTs, and measure the usage based on our defined metrics: frequency, popularity and coverage.
Results
Our study provides detailed quantitative information and yield insight, particularly, (1) confirms the conventional wisdom that the usage of APIs obeys Zipf distribution; (2) demonstrates that core API is not fully used (many classes, methods and fields have never been used); (3) discovers that deprecated API entities (in which some were deprecated long ago) are still widely used; (4) evaluates that the use of current compact profiles is under-utilized; (5) identifies API library coldspots and hotspots.
Conclusions
Our findings are suggestive of potential applications across language API design, optimization and restriction, API education, library recommendation and compact profile construction.}
}
@article{GARCIA198851,
title = {Specification, design and modula-2 implementation of a low cost industrial control system},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {3},
pages = {51-56},
year = {1988},
note = {15th IFAC/IFIP Workshop on Real Time Programming 1988, Valencia, Spain, 25-27 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-036236-6.50012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080362366500126},
author = {D. Garcia and H. Lopez and J. Tuya and A. Diez},
keywords = {Software Engineering, Real time control, Modula-2 language, Concurrency, Computer-aided training},
abstract = {The purpose of this paper is to show the application of modern software engineering techniques to the construction of a small/medium size package for real time control. This package, denoted by SCM (acronym of “Sistema de Control Modular”), has been written in Modula-2 and presents all advantages offered by this high-level structured programming language. It has been implemented on a PC-AT compatible computer.}
}
@article{ARTRITH2016135,
title = {An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2},
journal = {Computational Materials Science},
volume = {114},
pages = {135-150},
year = {2016},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2015.11.047},
url = {https://www.sciencedirect.com/science/article/pii/S0927025615007806},
author = {Nongnuch Artrith and Alexander Urban},
keywords = {Machine learning, Artificial neural networks, Atomistic simulations, Titanium dioxide (TiO), Behler–Parrinello},
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler–Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (ænet) package. The construction and application of ANN potentials using ænet is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential’s capabilities for the prediction of the high-pressure phases columbite (α-PbO2 structure) and baddeleyite (ZrO2 structure).}
}
@article{ZHAI2010357,
title = {Ancient vernacular architecture: characteristics categorization and energy performance evaluation},
journal = {Energy and Buildings},
volume = {42},
number = {3},
pages = {357-365},
year = {2010},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2009.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378778809002400},
author = {Zhiqiang (John) Zhai and Jonathan M. Previtali},
keywords = {Ancient vernacular architecture, Building characteristics, Energy performance, Computer energy simulation},
abstract = {Building has significant impacts on the environment and natural resources. The emerging world energy and environment challenges demand a substantial revolution of building design philosophies, strategies, technologies, and construction methods. Vernacular architectures, built by people whose design decisions are influenced by traditions in their culture, have been gleaned through a long period of trial and error and the ingenuity of local builders who possess specific knowledge about their place on the planet, and thus are valuable in promoting climate-specific passive building technologies to modern buildings. This study introduced an approach to categorizing distinct vernacular regions and evaluating energy performance of ancient vernacular homes as well as identifying optimal constructions using vernacular building techniques. The research conducted an extensive computer energy modeling for a number of representative ancient vernacular architectural characteristics observed for different climatic regions. The vernacular test subjects were compared against those established according to the International Energy Conservation Code and those generated by the optimization software. The simulation results of the energy models suggest that considering traditions seen in ancient vernacular architecture as an approach to improving building energy performance is a worthwhile endeavor and a scientific guidance can help enhance the performance. The study indicates that, although many vernacular dwells exist in the world, it is challenging (but desired) to package vernacular architecture traditions and quantitative design knowledge to modern building designers. This project is the first part of a much larger project that intends to create a knowledge base of vernacular building traditions that will include information about not only the energy performance of traditional building techniques, but also address areas of cost, material availability and cultural traditions.}
}
@article{KIM2021100054,
title = {Open-source toolkit for end-to-end Korean speech recognition},
journal = {Software Impacts},
volume = {7},
pages = {100054},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100054},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000026},
author = {Soohwan Kim and Seyoung Bae and Cheolhwang Won},
keywords = {End-to-End (E2E), Korean, Automatic speech recognition (ASR), Open-source software, Speech processing},
abstract = {A modular and extensible end-to-end Korean automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch called as KoSpeech11This paper is written based on our technical report available at: https://arxiv.org/abs/2009.03092. was released as an opensource software. Several ASR open-source toolkits have been released, but all of them deal with non-Korean languages. For this reason, The purpose of KoSpeech is to provide a customizable training environment for Korean ASR researchers. Furthermore, KoSpeech provides more than fifty options. Researchers can conveniently customize various hyperparameters. Because of these advantages, KoSpeech can be a very useful toolkit for Korean ASR researchers and could be a guideline for those who research Korean speech recognition.}
}
@article{PANFEROV2010154,
title = {Theory and Software for Simulation of Complex Flexible Aerospace Vehicles and Smart Control},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {15},
pages = {154-159},
year = {2010},
note = {18th IFAC Symposium on Automatic Control in Aerospace},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100906-5-JP-2022.00027},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015318322},
author = {Alexander I. Panferov and Alexander V. Nebylov and Sergey A. Brodsky},
keywords = {mathematical model, smart control, flexible vehicle, simulation, bending, sloshing, local angle of attack, oscillations damping},
abstract = {Abstract
The concept of a universal program for research into dynamic properties, calculation of comprehensive mathematical models, simulation of the flight motion and synthesis of smart control laws for different types of the flexible aerospace constructions (launchers and missiles) is considered. The basis of the program is the structure, which allows carrying out of the analysis of dynamic characteristics, simulation, and visual information representation. The software package is supplied with the library of program modules. These modules are designed on the base of mathematical models of separate vehicle elements and different physical effects, such as flexibility, sloshing, sluggishness of engines, local angle of attack values, etc. Selection of different sets of the mathematical models and program modules allows investigating different constructions for known and prospective vehicles. The interface of the program enables changing parameters of the vehicle in a wide range, to simulate flight at any trajectory and to output results of the analysis in a digital or graphical form. For synthesis of control laws, provides the usage of the smart control approach.}
}
@article{SIMANAVICIENE201447,
title = {Assessing reliability of design, construction, and safety related decisions},
journal = {Automation in Construction},
volume = {39},
pages = {47-58},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2013.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0926580513002112},
author = {Ruta Simanaviciene and Rita Liaudanskiene and Leonas Ustinovichius},
keywords = {Multiple attribute decision making methods, Construction, Technological, Safety, Reliability, Sensitivity analysis, Error},
abstract = {Currently there is no approach which would help to comprehensively ensure occupational safety. Many scientists perform researches and calculations, create new methods related to safety and health, but most of them analyze separate aspects of safety in the field of construction. The authors of this paper present a new complex view on ensuring occupational safety and health during construction. The selection of safety solutions is performed based on complex evaluation of structure, technology and safety. In their previous works, the authors offered a new method for multiple attribute decision synthesis, SyMAD-3, which helps to choose an effective construction project alternative from multiple alternatives by assessing various construction, technological and occupational safety solutions, based on a set of quantitative attributes. However, the integration of these solutions may cause doubts, since decision making in construction is always associated with uncertainty. The investment projects in construction are characterized by the large accuracy variation (from 15 to 50%) of some attribute values. Although the SyMAD-3 method is mathematically grounded, it does not answer the question if the error of attribute values impacts the final decision and if this decision can be reliably assessed. In the present paper, the authors supplement the SyMAD-3 method with decision sensitivity analysis (SyMAD-3 with SA) to improve the reliability of the SyMAD-3 method and assess the reliability of the obtained decision. The SyMAD-3 with SA method allows us to choose an effective alternative of a construction project by assessing three stages of construction, based on a set of attributes given the error of their values, and determine the reliability of the final decision. The proposed method is implemented in a software package created by the authors with the aim of analyzing decisions and performing experimental calculations in the field of construction.}
}
@article{GUAN2020101707,
title = {Formalization of continuous Fourier transform in verifying applications for dependable cyber-physical systems},
journal = {Journal of Systems Architecture},
volume = {106},
pages = {101707},
year = {2020},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101707},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120300011},
author = {Yong Guan and Jie Zhang and Zhiping Shi and Yi Wang and Yongdong Li},
keywords = {Formal verification, Continuous Fourier transform, HOL4, Theorem proving, Cyber-physical systems},
abstract = {Continuous Fourier transform (CFT) is widely used and is often directly applied in cyber-physical systems (CPS) without checking its preconditions. This inevitably leads to unexpected defects and even errors. Thus, verification is necessary for the CFT-based engineering design to ensure a dependable cyber physical system. HOL4 (Higher Order Logic 4) is a formal theorem prover that prevails in software and hardware verification. However, there is no CFT theorem library in current HOL4. In this paper, the definition and some frequently used properties of CFT are formalized and verified in HOL4. Based on this, we formally model a basic theorem library of CFT. As a case study, the CFT library is employed to verify the frequency response of an RLC circuit, which is a critical application for dependable CPS. The formalization of the CFT and its properties and the construction of the formal CFT theorem library can effectively extend the function of the HOL4 system. The obtained formal results can be applied in various CFT-based cyber-physical systems.}
}
@article{KAISER2022387,
title = {Model-based automatic generation of digital twin models for the simulation of reconfigurable manufacturing systems for timber construction},
journal = {Procedia CIRP},
volume = {107},
pages = {387-392},
year = {2022},
note = {Leading manufacturing systems transformation – Proceedings of the 55th CIRP Conference on Manufacturing Systems 2022},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122002797},
author = {Benjamin Kaiser and Alexander Reichle and Alexander Verl},
keywords = {AutomationML, ROS, URDF, Software Defined Manufacturing, Reconfigurable Manufacturing System, Simulation, Timber Prefabrication, IntCDC},
abstract = {Reconfigurable manufacturing systems adapt to changing requirements and therefore offer a solution to the challenges associated with increasing product variety and shorter product life cycles. This is especially helpful in the prefabrication in timber construction, as components and requirements vary widely between projects. However, reconfigurations must be quick to keep downtimes short. The planning of reconfigurations is a complex problem where digital twins and simulation models of the manufacturing systems are necessary for simulation and validation. In our timber prefabrication use case, building components and requirements change from project to project, and the manufacturing system needs to reconfigure frequently. The building components and the configuration of the manufacturing system are designed simultaneously during a co-design process to ensure the integrity of the manufacturing process in a changing manufacturing environment. Hence, this may require many iterations of the planning process. Each iteration needs validation through a simulation. However, the manual generation of simulation models is time-consuming and error-prone and thus poses an obstacle for the digital planning phase. This paper addresses this issue and presents a model-based approach to generate simulation models for reconfigurable manufacturing systems. Our system and its components are modeled in automation markup language as this domain-specific language allows us to model not only kinematics and geometry but also behavior, skills, and interfaces to check the compatibility of components. We apply the method to our use case with the IntCDC wood prefabrication system. The proposed method builds a component graph for a configuration of the manufacturing system and generates the required packages for the simulation in Gazebo and ROS. To build simulation models automatically, we apply model-to-model and model-to-text transformation techniques. The proposed method is suitable for integration in the digital co-design workflow of IntCDC and allows fast iterations with continuous simulations of machine configurations throughout the planning process.}
}
@article{CASASOROZCO2021107408,
title = {PharmaPy: An object-oriented tool for the development of hybrid pharmaceutical flowsheets},
journal = {Computers & Chemical Engineering},
volume = {153},
pages = {107408},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107408},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421001861},
author = {Daniel Casas-Orozco and Daniel Laky and Vivian Wang and Mesfin Abdi and X. Feng and E. Wood and Carl Laird and Gintaras V. Reklaitis and Zoltan K. Nagy},
keywords = {Digital design, Flowsheet simulation, Pharmaceutical process simulator, Hybrid processes, Parameter estimation, Optimization, Python},
abstract = {Process design and optimization continue to provide computational challenges as the chemical engineering and process optimization communities seek to address more complex and larger scale applications. Software tools for digital design and flowsheet simulation are readily available for traditional chemical processing applications such as in commodity chemicals and hydrocarbon processing; however, tools for pharmaceutical manufacturing are much less well developed. This paper introduces, PharmaPy, a Python-based modelling platform for pharmaceutical manufacturing systems design and optimization. The versatility of the platform is demonstrated in simulation and optimization of both continuous and batch processes. The structure and features of a Python-based modeling platform, PharmaPy are presented. Illustrative examples are shown to highlight key features of the platform and framework.}
}
@article{ALMEIDA2014216,
title = {CAOVerif: An open-source deductive verification platform for cryptographic software implementations},
journal = {Science of Computer Programming},
volume = {91},
pages = {216-233},
year = {2014},
note = {Special Issue on Selected Contributions from the Open Source Software Certification (OpenCert) Workshops},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2012.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S016764231200189X},
author = {José Bacelar Almeida and Manuel Barbosa and Jean-Christophe Filliâtre and Jorge Sousa Pinto and Bárbara Vieira},
keywords = {Formal verification, Program verification, Cryptographic software, Deductive verification},
abstract = {CAO is a domain-specific imperative language for cryptography, offering a rich mathematical type system and crypto-oriented language constructions. We describe the design and implementation of a deductive verification platform for CAO and demonstrate that the development time of such a complex verification tool could be greatly reduced by building on the Jessie plug-in included in the Frama-C framework. We discuss the interesting challenges raised by the domain-specific characteristics of CAO, and describe how we tackle these problems in our design. We base our presentation on real-world examples of CAO code, extracted from the open-source code of the NaCl cryptographic library, and illustrate how various cryptography-relevant security properties can be verified.}
}
@article{SOTIROPOULOS2020441,
title = {High performance topology optimization computing platform},
journal = {Procedia Manufacturing},
volume = {44},
pages = {441-448},
year = {2020},
note = {The 1st International Conference on Optimization-Driven Architectural Design (OPTARCH 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.272},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920308593},
author = {Stefanos Sotiropoulos and Georgios Kazakis and Nikos D. Lagaros},
keywords = {Topology optimization, SAP2000, HP-OCP, C#},
abstract = {One of the most challenging tasks in the construction industry nowadays, is to reduce the material demands and distribute, in the same time, the material among the structural system in the best possible way. Topology optimization is a design procedure that is increasingly used, to generate optimized forms of structures in several engineering fields. The current paper presents the Topology Optimization (TO) module of the High-Performance Optimization Computing Platform (HP-OCP) which focuses on civil engineering problems. More specifically the SIMP method [1] is implemented and the topology optimization problem is solved by using the OC algorithm. The HP-OCP is a platform which evaluates several objective functions, such as the volume of the structure, the compliance etc. and can solve constrained or unconstrained structural optimization problems. The above libraries are developed in C#. The core of the platform is created in such way that it can be integrated with any CAE program that has OAPI, XML or any other type of data exchange format. In the proposed work the structural analysis and design software SAP2000 is used. Theoretical aspects are discussed in order to implement the mathematical formulation in a commercial software. Basic and specific features are applied and representative examples are performed. One of the highlights of the proposed work is that the above module can be used for all kind of finite elements. Benchmark tests are presented with structures that are simulated by 2D plane-stress elements, 3D-solid elements and shell elements. Furthermore, it is independent of the type of the mesh, structured or unstructured, so both examples are presented. In the proposed work a powerful tool for both architects and civil engineers is introduced. The analysis and design of the structures are performed in SAP2000 software, in order to achieve a realistic result that could be a solution for a real-world structure.}
}
@article{RIBEIRO2022105346,
title = {Eurocode-compliant topology optimisation of steel moment splice connections},
journal = {Journal of Building Engineering},
volume = {62},
pages = {105346},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2022.105346},
url = {https://www.sciencedirect.com/science/article/pii/S2352710222013523},
author = {Tiago Ribeiro and Luís Bernardo and Ricardo Carrazedo and Dario {De Domenico}},
keywords = {Topology optimisation, Eurocode 3, Steel moment connection, Bolted beam splice connection, Cover flange plate under tension, Engineering software, Steel design, TOSCA Structure},
abstract = {To address the meagre use of Topology Optimisation (TO) in the civil engineering industry, the case of bolted moment connections in steel construction has been investigated. Connections are essential in steel structures, contributing up to 25% of the global weight and concentrating the complexity in design and, therefore, the industry's added value. It has been found that compliance with code requirements and difficulties in employing advanced software packages in real, heterogeneous and complex connections are the two major deterrents to the widespread use of TO. Hence, a novel code-compliant methodology was proposed, applied to the cover flange plate under tension of a bolted beam splice moment connection and validated with non-linear Finite Element Analyses (NLFEA). It has been found that a 50% volume reduction is possible by employing linear elastic TO and establishing the non-exceedance of the steel ultimate stress as a criterion. NLFEA showed that if the original joint capacity is to be maintained, the maximum optimisation threshold reduces only 45% of the initial volume. Two critical conclusions are that linear elastic TO could not meet the safety needs and that the introduced validation stage with NLFEA is an essential step, highlighting the novelty and significance of the proposed method. Topologically optimised solutions showed a significant volume and cost reduction, meaningfully contributing to the steel construction decarbonisation goals and leading to joints with a more ductile behaviour.}
}
@article{VANAMERONGEN200387,
title = {Modelling of physical systems for the design and control of mechatronic systems},
journal = {Annual Reviews in Control},
volume = {27},
number = {1},
pages = {87-117},
year = {2003},
issn = {1367-5788},
doi = {https://doi.org/10.1016/S1367-5788(03)00010-5},
url = {https://www.sciencedirect.com/science/article/pii/S1367578803000105},
author = {Job {van Amerongen} and Peter Breedveld},
keywords = {Mechatronics, Physical systems, Design and control, Modelling and simulation, Bond graphs, Energy-based modelling},
abstract = {Mechatronic design requires that a mechanical system and its control system be designed as an integrated system. This contribution covers the background and tools for modelling and simulation of physical systems and their controllers, with parameters that are directly related to the real-world system. The theory will be illustrated with examples of typical mechatronic systems such as servo systems and a mobile robot. Hands-on experience is realised by means of exercises with the 20-sim software package (a demo version is freely available on the Internet). In mechatronics, where a controlled system has to be designed as a whole, it is advantageous that model structure and parameters are directly related to physical components. In addition, it is desired that (sub-)models be reusable. Common block-diagram- or equation-based simulation packages hardly support these features. The energy-based approach towards modelling of physical systems allows the construction of reusable and easily extendible models. This contribution starts with an overview of mechatronic design problems and the various ways to solve such problems. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission and a mobile robot. The energy-based approach towards modelling is treated in some detail. This will give the reader sufficient insight in order to exercise it with the aid of modelling and simulation software (20-sim). Such a tool allows high level input of models in the form of iconic diagrams, equations, block diagrams or bond graphs and supports efficient symbolic and numerical analysis as well as simulation and visualisation. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a process that can be controlled by block-diagram-based (digital) controllers. This contribution is based on object-oriented modelling: each object is determined by constitutive relations at the one hand and its interface, the power and signal ports to and from the outside world, at the other hand. Other realizations of an object may contain different or more detailed descriptions, but as long as the interface (number and type of ports) is identical, they can be exchanged in a straightforward manner. This allows top–down modelling as well as bottom–up modelling. Straightforward interconnection of (empty) submodels supports the actual decision process of modelling, not just model input and output manipulation. Empty submodel types may be filled with specific descriptions with various degrees of complexity (models can be polymorphic) to support evolutionary and iterative modelling and design approaches. Additionally, submodels may be constructed from other submodels in hierarchical structures. An introduction to the design of controllers based on these models is also given. Modelling and controller design as well as the use of 20-sim may be exercised in hands-on experience assignments, available at the Internet (http://www.ce.utwente.nl/IFACBrief/). A demonstration copy of 20-sim that allows the reader to use the ideas presented in this contribution may be downloaded from the Internet (http://www.20sim.com).}
}
@article{ZHOU2022103101,
title = {Library on-shelf book segmentation and recognition based on deep visual features},
journal = {Information Processing & Management},
volume = {59},
number = {6},
pages = {103101},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103101},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002023},
author = {Shuo Zhou and Tan Sun and Xue Xia and Ning Zhang and Bo Huang and Guojian Xian and Xiujuan Chai},
keywords = {Library information management, On-shelf book recognition, Book spine segmentation, Deep learning, Library robot},
abstract = {On-shelf book segmentation and recognition are crucial steps in library inventory management and daily operation. In this paper, a detailed investigation of related work is conducted. RFID and barcode-based solutions suffer from expensive hardware facilities and long-term maintenance. Digital Image processing and OCR techniques are flawed due to a lack of accuracy and robustness. On this basis, we propose a visual and non-character system utilizing deep learning methods to accomplish on-shelf book segmentation and recognition tasks. Firstly, book spine masks are extracted from the image of on-shelf books by instance segmentation model, followed by affine transformation to rectangle images. Secondly, a spine feature encoder is trained to learn the deep visual features of spine images. Finally, the book inventory search space is constructed and the similarity metric between spine visual representations is calculated to recognize the target book identity. To train the models we collect high-resolution datasets of 10k-level and develop a data annotation software accordingly. For validation, we design simulated scenarios of recognizing 3.6k IDs from 5.6k book spines and achieve a best top1 accuracy of 99.18% and top5 accuracy of 99.91%. Furthermore, we develop a prototype of a mobile library management robot with embedded edge intelligence. It can automatically perform on-shelf book image capturing, spine segmentation and recognition, and target book grasping workflow.}
}
@article{KUHNER20122232,
title = {Progress on standardization and automation in software development on W7X},
journal = {Fusion Engineering and Design},
volume = {87},
number = {12},
pages = {2232-2237},
year = {2012},
note = {Proceedings of the 8th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2012.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920379612003067},
author = {Georg Kühner and Torsten Bluhm and Peter Heimann and Christine Hennig and Hugo Kroiss and Jon Krom and Heike Laqua and Marc Lewerentz and Josef Maier and Jörg Schacht and Anett Spring and Andreas Werner and Manfred Zilker},
keywords = {W7-X, Software development, Quality management, Standardization, ISO/IEC 15504},
abstract = {For a complex experiment like W7X being subject to changes all along its projected lifetime the advantages of a formalized software development method have already been stated [1]. Quality standards like ISO/IEC-12207 provide a guideline for structuring of development work and improving process and product quality. A considerable number of tools has emerged supporting and automating parts of development work. On W7X progress has been made during the last years in exploiting the benefit of automation and management during software development:–Continuous build, integration and automated test of software artefacts.∘Syntax checks and code quality metrics.∘Documentation generation.∘Feedback for developers by temporal statistics.–Versioned repository for build products (libraries, executables).–Separate snapshot and release repositories and automatic deployment.–Semi-automatic provisioning of applications.–Feedback from testers and feature requests by ticket system. This toolset is working efficiently and allows the team to concentrate on development. The activity there is presently focused on increasing the quality of the existing software to become a dependable product. Testing of single functions and qualities must be simplified. So a restructuring is underway which relies more on small, individually testable components with standardized interfaces providing the capability to construct arbitrary function aggregates for dedicated tests of quality attributes as availability, reliability, performance. A further activity is on improving the development cycle. The use of release cycles has already provided favourable concentration of work and predictability of delivery times. However, the demand has risen, to react quickly on priority changes from W7X-project management. So a more agile development cycle is being prepared relying on smaller working packages, shorter release cycles and an associated release plan giving the software development responsible the possibility to react on a shorter time scale.}
}
@article{SAKAGUCHI2020102372,
title = {Program extraction for mutable arrays},
journal = {Science of Computer Programming},
volume = {191},
pages = {102372},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.102372},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319301650},
author = {Kazuhiko Sakaguchi},
keywords = {Interactive theorem proving, Formally verified software, The Coq proof assistant, Program extraction, Program transformation and optimization},
abstract = {We present a lightweight method to represent, verify, and extract efficient programs involving mutable arrays in the Coq proof assistant. Our method mainly consists of a library for handling mutable arrays and an improved extraction plugin. Our library provides a monadic domain specific language for modeling computations involving mutable arrays, a simple reasoning method based on the Ssreflect extension and the Mathematical Components library, and an extraction method to efficient OCaml programs using in-place updates. Our extraction plugin improves the performance of our extracted programs, or more appropriately, through the application of simple program transformations for purely functional programs, it reduces both construction and destruction costs of inductive and coinductive objects and function call costs. As concrete applications for our method, we provide efficient implementations, correctness proofs, and benchmarks of the union–find data structure and the quicksort algorithm.}
}
@article{WANG2023115141,
title = {Numerical study of steel plate shear walls with diverse construction configurations},
journal = {Engineering Structures},
volume = {274},
pages = {115141},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.115141},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622012172},
author = {Chen Wang and Li-yan Xu and Ling-han Song and Jian-sheng Fan},
keywords = {Numerical simulation, Steel plate shear wall, Constitutive model, Axial forces, Parameter analysis},
abstract = {Extensive numerical research on steel plate shear walls (SPSWs) has been conducted for decades to facilitate their engineering applications. However, these studies usually adopted built-in constitutive models in commercial finite element (FE) packages and focused on SPSWs with a particular type of construction configuration, thus yielding unsatisfactory accuracy and inadequate validation. In addition, the effect of axial forces on the frame columns, which commonly exist in practice, has rarely been investigated. In this paper, a sophisticated constitutive model for structural steel is introduced and further improved by incorporating the Bao and Wierzbicki ductile damage criterion to consider strength degradation under large deformations. Then, 12 SPSW specimens with diverse construction configurations are simulated with the developed model to fully validate its effectiveness. Compared with built-in models in commercial FE software, the developed model is more accurate and can describe complicated deformation patterns under various circumstances, including extreme loading cases. On these bases, a thorough parametric analysis of 35 specimens – combining 5 construction configurations with 7 axial force ratios – is performed to quantitatively investigate the effect of axial forces applied on the frame columns. The results indicate that the axial forces can counteract the tensile forces from the tension fields; but meanwhile can also aggravate the compressive burden of the compression-side column, which induces column yielding or buckling. When the axial force ratio increases, the adverse effect gradually prevails over the beneficial effect and notably decreases the loading capacities. Accordingly, a limit axial force ratio of 0.5 and a simple linear reduction factor when calculating the ultimate loading capacity under axial forces are suggested in engineering design.}
}
@article{PANFEROV2009373,
title = {Intelligent Control Systems Design for Flexible Aerospace Vehicles: Theoretical and Software Tools},
journal = {IFAC Proceedings Volumes},
volume = {42},
number = {19},
pages = {373-378},
year = {2009},
note = {2nd IFAC Conference on Intelligent Control Systems and Signal Processing},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20090921-3-TR-3005.00066},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015308636},
author = {Alexander I. Panferov and Sergey A. Brodsky and Alexander V. Nebylov},
keywords = {flight control, control design, simulation, control, flexible, vehicle dynamics, system synthesis},
abstract = {Abstract
Possible approaches to the mathematical description of the dynamic properties of aerospace vehicles in the processes of flight in atmosphere are observed. This description includes different mathematical models such as: models of the construction flexibility, sloshing of the fuel and oxidant, rigid body flight in the variable gravitational field with variable distributed mass of the vehicle, integral and distributed aerodynamic in the big diapason velocities, aeroflexibility, models of different types of the rocket engines and so on. In the basis of these mathematical models the library of the program units were designed. This library was included in the special program, which allows automating the processes of simulation of arbitrary axisymmetrical aerospace constructions. The program allows calculating the general mathematical models in different forms, to simplify these models and to use for design the systems of stabilization.}
}
@article{ERAZORAMIREZ2022105525,
title = {HydroLang: An open-source web-based programming framework for hydrological sciences},
journal = {Environmental Modelling & Software},
volume = {157},
pages = {105525},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105525},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002250},
author = {Carlos {Erazo Ramirez} and Yusuf Sermet and Frank Molkenthin and Ibrahim Demir},
keywords = {Scientific visualization, Hydrological analysis, Software libraries, Web frameworks, Neural networks},
abstract = {This paper introduces HydroLang, an open-source and integrated community-driven computational web framework for hydrology and water resources research and education. HydroLang employs client-side web technologies and standards to carry out various routines aimed at acquiring, managing, transforming, analyzing, and visualizing hydrological datasets. HydroLang consists of four major high-cohesion low-coupling modules: (1) retrieving, manipulating, and transforming raw hydrological data, (2) statistical operations, hydrological analysis, and model creation, (3) generating graphical and tabular data representations, and (4) mapping and geospatial data visualization. To demonstrate the framework's capabilities, portability, and interoperability, two detailed case studies (assessment of lumped models and construction of a rainfall disaggregation model) have been presented. HydroLang's unique modular architecture and open-source nature allow it to be easily tailored into any use case and web framework, and it encourages iterative enhancements with community involvement to establish the comprehensive next-generation hydrological software toolkit.}
}
@article{MONCHINGER2021530,
title = {Automated 3D scan based CAD-repositioning for design and verification in one-off construction},
journal = {Procedia CIRP},
volume = {100},
pages = {530-535},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.115},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121005874},
author = {Stephan Mönchinger and Robert Schröder and Rainer Stark},
keywords = {CAD-Repositioning, Virtual Design, Verification, Digital Mock-Up, 3D scanning},
abstract = {The presented engineering discipline one-off construction is characterized by a multiplicity of manual processes. As almost all modern product developments, the manufacture is based on the creation and consistent use of digital models. Quality of underlying data can vary greatly and it is not certain that digital models match the actual state of construction. This can result in the need for rework after production or installation. Especially challenging in the area of premium products, for which high quality, scarce materials are used and tight schedules are defined. If physical products are reworked, the corresponding digital models must be manually maintained. At present, attempts are being made to counteract these late adjustments by means of physical mock-ups or visual inspection of 3D scan data. Such scan data is used for automated adaptation of underlying digital models to the actual state of the physical construction. Existing Point Cloud Library functionalities were adapted and further algorithms were designed. The developed software backend was integrated into the existing software architecture. During the software development, great care was taken to ensure that the backend is based on open source content. The results show significant improvements of the data basis for the subsequent engineering activities. This will lead to a significant reduction of manual effort and rework, ensuring development cycles and even shorten delivery times. It reduces costs in the product creation process and sustainably strengthens confidence in digital models used. It has been shown that automation of design processes can have productivity-enhancing effects in one-off construction.}
}
@article{TUNCA2022752,
title = {Design of buckling restrained steel braces using application programming interface between simulation and discrete optimization},
journal = {Structures},
volume = {43},
pages = {752-766},
year = {2022},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2022.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352012422005719},
author = {Osman Tunca},
keywords = {Buckling restrained steel braces, Finite element analysis, Application programming interface, Metaheuristic algorithms, Black widow optimization algorithm, Harmony search algorithm},
abstract = {In this study, buckling restrained braces (BRBs) are optimized through application programming interface between simulation and discrete optimization. It is aimed to maximize the energy dissipation capacity of BRBs considering the American Institute of Steel Construction (ANSI/AISC 341). Unlike other studies, BRBs modeled in the finite element packaged software are directly linked to optimization algorithms. So, the geometric and material nonlinearities are also considered. Harmony Search Algorithm (HSA), which simulates the improvisation musician performances in finding pleasing harmony, and Black Widow Optimization Algorithm (BWOA), which imitates inimitable paring attitude of black widow spiders, are taken as the optimizer tools of this study. They encoded in Microsoft Visual Basic programming language. Initially, the algorithmic performances of the HSA and BWOA are compared and evaluated on two benchmark structural engineering design problems. Afterward, two different shaped BRBs are modeled in a finite element analysis (FEA) based software, namely ANSYS Workbench. Then, the obtained simulations are integrated with the HS and BWO algorithms throughout the application programming interface without identification of complex objective function and design constraints in formulations. These are directly calculated by ANSYS Workbench over very simple formulas. So, the attained optimum designs of BRBs are investigated with a new approach. Furthermore, in order to see the supreme algorithmic performances of the HSA and BWOA, all benchmark and BRB design problems are solved so-called well-established conventional standard Genetic Algorithm (sGA). Eventually, the proposed novel design methodology gives opportunity and eases to the designers since it provides convenience in solving complicated problems having nonlinear objective function and design constraints that are tiresome to encode.}
}
@article{SEGURAORTIZ2023106653,
title = {GENECI: A novel evolutionary machine learning consensus-based approach for the inference of gene regulatory networks},
journal = {Computers in Biology and Medicine},
volume = {155},
pages = {106653},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106653},
url = {https://www.sciencedirect.com/science/article/pii/S001048252300118X},
author = {Adrián Segura-Ortiz and José García-Nieto and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Gene regulatory networks, Differential expression, Machine learning, Evolutionary algorithms},
abstract = {Gene regulatory networks define the interactions between DNA products and other substances in cells. Increasing knowledge of these networks improves the level of detail with which the processes that trigger different diseases are described and fosters the development of new therapeutic targets. These networks are usually represented by graphs, and the primary sources for their correct construction are usually time series from differential expression data. The inference of networks from this data type has been approached differently in the literature. Mostly, computational learning techniques have been implemented, which have finally shown some specialization in specific datasets. For this reason, the need arises to create new and more robust strategies for reaching a consensus based on previous results to gain a particular capacity for generalization. This paper presents GENECI (GEne NEtwork Consensus Inference), an evolutionary machine learning approach that acts as an organizer for constructing ensembles to process the results of the main inference techniques reported in the literature and to optimize the consensus network derived from them, according to their confidence levels and topological characteristics. After its design, the proposal was confronted with datasets collected from academic benchmarks (DREAM challenges and IRMA network) to quantify its accuracy. Subsequently, it was applied to a real-world biological network of melanoma patients whose results could be contrasted with medical research collected in the literature. Finally, it has been proved that its ability to optimize the consensus of several networks leads to outstanding robustness and accuracy, gaining a certain generalization capacity after facing the inference of multiple datasets. The source code is hosted in a public repository at GitHub under MIT license: https://github.com/AdrianSeguraOrtiz/GENECI. Moreover, to facilitate its installation and use, the software associated with this implementation has been encapsulated in a python package available at PyPI: https://pypi.org/project/geneci/.}
}
@article{OVEREEM2022106890,
title = {API-m-FAMM: A focus area maturity model for API Management},
journal = {Information and Software Technology},
volume = {147},
pages = {106890},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106890},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000532},
author = {Michiel Overeem and Max Mathijssen and Slinger Jansen},
keywords = {API Management, Maturity model, Focus area maturity models},
abstract = {Context:
Organizations are increasingly connecting software applications using Application Programming Interfaces (APIs) to share data, services, functionality, and even complete business processes. However, the creation and management of APIs is non-trivial. Aspects such as traffic management, community engagement, documentation, and version management are often rushed afterthoughts.
Objective:
In this research, we present and evaluate a focus area maturity model for API Management (API-m-FAMM). A focus area maturity model can be used to establish the maturity level of an organization in a specific functional domain described through a number of areas. The API-m-FAMM addresses the areas Lifecycle Management, Security, Performance, Observability, Community, and Commercial.
Method:
The model is constructed using established methods for the design of a focus area maturity model. It is grounded in literature and practice, and was developed and evaluated through a systematic literature Review, eleven expert interviews, and five case studies at software producing organizations.
Result:
The model is described in detail, and its application is illustrated by six case studies.
Conclusions:
The evaluations are reported on, and show that the API-m-FAMM is an efficient tool for aiding organizations in gaining a better understanding of their current implementation of API management practices, and provides them with guidance towards higher levels of maturity. The detailed description of the construction of the API-m-FAMM gives researchers an example to further support the available methodologies, specifically how to combine design science research with these methodologies. Additionally, this study’s unique case study design shows that maturity models can be successfully deployed in practice with minimal involvement of researchers. The focus area maturity model for API Management is maintained on www.maturitymodels.org, allowing practitioners to benefit from its useful insights.}
}
@article{ABUNDENEBA2017128,
title = {Modeling and simulated design: A novel model and software of a solar-biomass hybrid dryer},
journal = {Computers & Chemical Engineering},
volume = {104},
pages = {128-140},
year = {2017},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098135417301540},
author = {Fabrice {Abunde Neba} and Yvette {Jiokap Nono}},
keywords = {Modeling, Simulated design, Solar-biomass dryer, Design software, Food products},
abstract = {Solar-biomass hybrid dryers are widely used as environmentally friendly alternatives to fossil fuel dryers for the preservation of agricultural food products. However, the design and optimization of hybrid dryers often involve construction of expensive prototype systems and time-consuming studies. This study presents a novel hybrid dryer model and a dryer design and simulation software, which can be used to improve hybrid dryer design efficiency. The methodologies adopted are those of functional analysis, mathematical modeling and virtual prototyping, with SolidWorks employed for CAD modeling, and Matlab for numerical simulations and implementation of mathematical models into a user interface. The software package has as main functions (a) dimensioning the hybrid dryer, which comprises a solar collector, combustion reactor and tunnel drying chamber, (b) cost analysis and financial appraisal, and (c) temperature dynamic simulation in the system. The novel software has been successfully employed to dimension a hybrid dryer for drying of green pepper.}
}
@article{VANBINSBERGEN2020100945,
title = {Purely functional GLL parsing},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100945},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100945},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300058},
author = {L. Thomas {van Binsbergen} and Elizabeth Scott and Adrian Johnstone},
keywords = {Top-down parsing, Generalised parsing, Parser combinators, Syntax descriptions, Functional programming},
abstract = {Generalised parsing has become increasingly important in the context of software language design and several compiler generators and language workbenches have adopted generalised parsing algorithms such as GLR and GLL. The original GLL parsing algorithms are described in low-level pseudo-code as the output of a parser generator. This paper explains GLL parsing differently, defining the FUN-GLL algorithm as a collection of pure, mathematical functions and focussing on the logic of the algorithm by omitting implementation details. In particular, the data structures are modelled by abstract sets and relations rather than specialised implementations. The description is further simplified by omitting lookahead and adopting the binary subtree representation of derivations to avoid the clerical overhead of graph construction. Conventional parser combinators inherit the drawbacks from the recursive descent algorithms they implement. Based on FUN-GLL, this paper defines generalised parser combinators that overcome these problems. The algorithm is described in the same notation and style as FUN-GLL and uses the same data structures. Both algorithms are explained as a generalisation of basic recursive descent algorithms. The generalised parser combinators of this paper have several advantages over combinator libraries that generate internal grammars. For example, with the generalised parser combinators it is possible to parse larger permutation phrases and to write parsers for languages that are not context-free. The ‘BNF combinator library’ is built around the generalised parser combinators. With the library, embedded and executable syntax specifications are written. The specifications contain semantic actions for interpreting programs and constructing syntax trees. The library takes advantage of Haskell’s type-system to type-check semantic actions and Haskell’s abstraction mechanism enables ‘reuse through abstraction’. The practicality of the library is demonstrated by running parsers obtained from the syntax descriptions of several software languages.}
}
@article{KIM2009S108,
title = {Lessons learned from the construction of a Korean software reference data set for digital forensics},
journal = {Digital Investigation},
volume = {6},
pages = {S108-S113},
year = {2009},
note = {The Proceedings of the Ninth Annual DFRWS Conference},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2009.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1742287609000450},
author = {Kibom Kim and Sangseo Park and Taejoo Chang and Cheolwon Lee and Sungjai Baek},
keywords = {Digital forensic, Reference data set (RDS), Hash data set, National software reference library (NSRL), Korean RDS (KRDS)},
abstract = {This paper analyzes the National Software Reference Library Reference Data Set (NSRL RDS) and constructs a Korean RDS (KRDS) based on it. The fact that NSRL RDS offers the largest amount of hash data sets has led to its widespread adoption. However, the effectiveness analysis of NSRL RDS indicates that there are both duplicate/obsolete data that can be eliminated and unused metadata that can be deleted. Moreover, language-specific software and domestic software that has been widely used for years have to be added. Bearing these issues in mind, we develop a strategy and model for both importing effective NSRL RDS and adding Korea-specific data sets. We then construct initial KRDS using proprietary software designed to manage the entire process of analysis and construction. Lessons learned during this work are believed to be useful for those who need to construct their own RDS (based on NSRL RDS or not) and later upgrade of the NSRL RDS.}
}
@article{GANESAN20091586,
title = {Architecture compliance checking at run-time},
journal = {Information and Software Technology},
volume = {51},
number = {11},
pages = {1586-1600},
year = {2009},
note = {Third IEEE International Workshop on Automation of Software Test (AST 2008) Eighth International Conference on Quality Software (QSIC 2008)},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909001025},
author = {Dharmalingam Ganesan and Thorsten Keuler and Yutaro Nishimura},
keywords = {Run-time monitoring, Architecture compliance checking, Hierarchical Colored Petri nets},
abstract = {In this paper, we report on our experiences with architecture compliance checking – the process of checking whether the planned or specified software architecture is obeyed by the running system – of an OSGi-based, dynamically evolving application in the office domain. To that end, we first show how to dynamically instrument a running system in the context of OSGi in order to collect run-time traces. Second, we explain how to bridge the abstraction gap between run-time traces and software architectures, through the construction of hierarchical Colored Petri nets (CP-nets). In addition, we demonstrate how to design reusable hierarchical CP-nets. In an industry example, we were able to extract views that helped us to identify a number of architecturally relevant issues (e.g., architectural style violations, behavior violations) that would not have been detected otherwise, and could have caused serious problems like system malfunctioning or unauthorized access to sensitive data. Finally, we package valuable experiences and lessons learned from this endeavor.}
}
@article{MAREK2015100,
title = {Introduction to dynamic program analysis with DiSL},
journal = {Science of Computer Programming},
volume = {98},
pages = {100-115},
year = {2015},
note = {Fifth issue of Experimental Software and Toolkits (EST): A special issue on Academics Modelling with Eclipse (ACME2012)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642314000070},
author = {Lukáš Marek and Yudi Zheng and Danilo Ansaloni and Lubomír Bulej and Aibek Sarimbekov and Walter Binder and Petr Tůma},
keywords = {Dynamic program analysis, Bytecode instrumentation, Aspect-oriented programming, Domain-specific languages, Java Virtual Machine},
abstract = {Dynamic program analysis (DPA) tools assist in many software engineering and development tasks, such as profiling, program comprehension, and performance model construction and calibration. On the Java platform, many DPA tools are implemented either using aspect-oriented programming (AOP), or rely on bytecode instrumentation to modify the base program code. The pointcut/advice model found in AOP enables rapid tool development, but does not allow expressing certain instrumentations due to limitations of mainstream AOP languages—developers thus use bytecode manipulation to gain more expressiveness and performance. However, while the existing bytecode manipulation libraries handle some low-level details, they still make tool development tedious and error-prone. Targeting this issue, we provide the first complete presentation of DiSL, an open-source instrumentation framework that reconciles the conciseness of the AOP pointcut/advice model and the expressiveness and performance achievable with bytecode manipulation libraries. Specifically, we extend our previous work to provide an overview of the DiSL architecture, advanced features, and the programming model. We also include case studies illustrating successful deployment of DiSL-based DPA tools.}
}
@article{CROOKES1989417,
title = {An environment for developing concurrent software for transputer-based image processing},
journal = {Microprocessing and Microprogramming},
volume = {27},
number = {1},
pages = {417-422},
year = {1989},
note = {Fifteenth EUROMICRO Symposium on Microprocessing and Microprogramming},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(89)90085-9},
url = {https://www.sciencedirect.com/science/article/pii/0165607489900859},
author = {D. Crookes and P.J. Morrow and B. Sharif and I. McClatchey},
abstract = {This paper outlines the design and construction of the QUE-TIP system - a programming environment for developing concurrent image processing software for execution on transputer-based architectures. It supports a disciplined engineering approach to system construction using a graphically-based building-block concept. The environment incorporates a library of algorithms and diagnostic tools which are programmed in a generalised, non configuration-specific style, so that they can be mapped on to a user-specified size of transputer network.}
}
@article{ODETTI2020108205,
title = {SWAMP, an Autonomous Surface Vehicle expressly designed for extremely shallow waters},
journal = {Ocean Engineering},
volume = {216},
pages = {108205},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.108205},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820311318},
author = {Angelo Odetti and Gabriele Bruzzone and Marco Altosole and Michele Viviani and Massimo Caccia},
keywords = {Autonomous Vehicles, ASV, USV, Propulsion, Pump-jet, Shallow water, Wetlands, Environmental monitoring},
abstract = {Wetlands, the geographic areas where water meets the earth, are ecosystems essential for life and an increasing number of conventions, directives and research projects recognise the necessity of protecting them. Nevertheless the number, quality and spacial resolution of surveys is modest due to the absence of expressly addressed tools. In this paper the design, construction and testing of the first prototype of an innovative class of Autonomous Surface Vehicles (ASVs) for the extremely shallow water and remote areas of wetlands is presented. SWAMP (Shallow Water Autonomous Multipurpose Platform) is a full y electric, modular, portable, lightweight, and highly-controllable ASV. It is a catamaran, equipped with four thrusters azimuth Pump-Jet thrusters that are flush with the hull and specifically designed for this vehicle. SWAMP is also characterised by small draft soft-foam, unsinkable hull structure with high modularity and a flexible hardware/software architecture. Each hull can be considered as a single vehicle, being equipped with a full navigation, guidance and control package, as well as propulsion, power and communication systems. The extreme modularity is also guaranteed thanks to the adoption of on-board Wi-Fi communication architecture: the two mono-hull vehicles are connected via Wi-Fi as well as all the modules aboard each of them.}
}
@article{LAI20171648,
title = {Integrating CAD with Abaqus: A practical isogeometric analysis software platform for industrial applications},
journal = {Computers & Mathematics with Applications},
volume = {74},
number = {7},
pages = {1648-1660},
year = {2017},
note = {High-Order Finite Element and Isogeometric Methods 2016},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2017.03.032},
url = {https://www.sciencedirect.com/science/article/pii/S0898122117302079},
author = {Yicong Lai and Yongjie Jessica Zhang and Lei Liu and Xiaodong Wei and Eugene Fang and Jim Lua},
keywords = {Isogeometric analysis, Design-through-analysis, T-spline construction, Rhino, Abaqus, Software platform},
abstract = {Isogeometric analysis (IGA) has been developed for more than a decade. However, the usage of IGA is by far limited mostly within academic community. The lack of automatic or semi-automatic software platform of IGA is one of the main bottlenecks that prevent IGA from wide applications in industry. In this paper, we present a comprehensive IGA software platform that allows IGA to be incorporated into existing commercial software such as Abaqus, heading one step further to bridge the gap between design and analysis. The proposed IGA software framework takes advantage of user-defined elements in Abaqus, linking with general. IGES files from commercial computer aided design packages, Rhino specific files and mesh data. The platform includes all the necessary modules of the design-through-analysis pipeline: pre-processing, surface and volumetric T-spline construction, analysis and post-processing. Several practical application problems are studied to demonstrate the capability of the proposed software platform.}
}
@article{PRAT1990225,
title = {Back-end manager: an interface between a knowledge-based front end and its application subsystems},
journal = {Knowledge-Based Systems},
volume = {3},
number = {4},
pages = {225-229},
year = {1990},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(90)90100-V},
url = {https://www.sciencedirect.com/science/article/pii/095070519090100V},
author = {Albert Prat and Jesus Lores and Paul Fletcher and Josep M. Catot},
keywords = {knowledge-based front end, interface Separability, back-end manager, user interface},
abstract = {Front Ends for Open and Closed User Systems (FOCUS) is an ESPRIT/2 (no. 2620) project aimed at designing tools and techniques for the construction of knowledge-based front ends (KBFEs) for open-user systems (reusable software components, libraries, etc) and closed-user systems (free-standing software, packages, etc). An important part of the project involves the establishment of an architecture for KBFEs and the specification of the KBFE/back-end interface. This paper describes the properties and related issues of such an interface, known as the back-end manager (BEM), and its relationship to the proposed KBFE architecture.}
}
@article{MALDONIS20191,
title = {StructOpt: A modular materials structure optimization suite incorporating experimental data and simulated energies},
journal = {Computational Materials Science},
volume = {160},
pages = {1-8},
year = {2019},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2018.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S0927025618308401},
author = {Jason J. Maldonis and Zhongnan Xu and Zhewen Song and Min Yu and Tam Mayeshiba and Dane Morgan and Paul M. Voyles},
keywords = {Multi-objective, Structure, Optimization, Genetic algorithm, Atomic, Simulation},
abstract = {StructOpt, an open-source structure optimization suite, applies genetic algorithm and particle swarm methods to obtain atomic structures that minimize an objective function. The objective function typically consists of the energy and the error between simulated and experimental data, which is typically applied to determine structures that minimize energy to the extent possible while also being fully consistent with available experimental data. We present example use cases including the structure of a metastable Pt nanoparticle determined from energetic and scanning transmission electron microscopy data, and the structure of an amorphous-nanocrystal composite determined from energetic and fluctuation electron microscopy data. StructOpt is modular in its construction and therefore is naturally extensible to include new materials simulation modules or new optimization methods, either written by the user or existing in other code packages. It uses the Message Passing Interface’s (MPI) dynamic process management functionality to allocate resources to computationally expensive codes on the fly, enabling StructOpt to take full advantage of the parallelization tools available in many scientific packages.}
}
@article{CASE1996333,
title = {Discourse model for collaborative design},
journal = {Computer-Aided Design},
volume = {28},
number = {5},
pages = {333-345},
year = {1996},
note = {Computer-Aided Concurrent Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/0010-4485(95)00053-4},
url = {https://www.sciencedirect.com/science/article/pii/0010448595000534},
author = {Michael P Case and Stephen C-Y Lu},
keywords = {agent, conflict, discourse design collaboration, concurrent engineering, blackboard architecture, KQML},
abstract = {A Discourse Model, including a structure and a process, is developed that provides software support for collaborative engineering design. The model shares characteristics of other design systems in the literature, including frames, constraints, semantic networks, and libraries of sharable design objects. It contributes a new model for conflict-aware agents, dynamic identification and dissemination of agent interest sets, a virtual workspace language, automatic detection of conflict, and a unique protocol for negotiation that ensures that interested agents have an opportunity to participate. The model is implementation independent and applicable to many research and commercial design environments currently available. An example scenario is provided in the architecture/engineering/construction domain that illustrates collaboration during the conceptual design of a fire station.}
}
@article{WOODSIDE2001107,
title = {Automated performance modeling of software generated by a design environment},
journal = {Performance Evaluation},
volume = {45},
number = {2},
pages = {107-123},
year = {2001},
note = {Performance Validation of Software Systems},
issn = {0166-5316},
doi = {https://doi.org/10.1016/S0166-5316(01)00033-5},
url = {https://www.sciencedirect.com/science/article/pii/S0166531601000335},
author = {Murray Woodside and Curtis Hrischuk and Bran Selic and Stefan Bayarov},
keywords = {Queuing model, Wideband approach, Layered queuing network model, Software performance, Software design modeling},
abstract = {Automation is needed to make performance modeling faster and more accessible to software designers. This paper describes a prototype tool that exploits recently developed techniques for automatic model construction from traces. The performance model is a layered queuing model and it is based on traces captured for certain selected scenarios which are determined to be important for performance. The prototype tool has been integrated with a commercial software design environment that generates code with heavy use of standard libraries. The execution costs of the libraries has also been captured and used in the automatic model creation. The approach not only automates building early models, but also gives models which can be maintained during development, using traces gathered from the implementations. The paper describes the tool, the process by which it is applied, the process of capturing and managing the execution cost data, and an example. To use the tool, the designer further defines the scenarios, the execution environment, and the workload intensity parameters. The designer can then experiment with different environments and workloads, or revise the design, to improve the performance.}
}
@article{RASMUSSEN2019102956,
title = {Managing interrelated project information in AEC Knowledge Graphs},
journal = {Automation in Construction},
volume = {108},
pages = {102956},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102956},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519300378},
author = {Mads Holten Rasmussen and Maxime Lefrançois and Pieter Pauwels and Christian Anker Hviid and Jan Karlshøj},
keywords = {Linked data, Building information modelling, Complex design, Ontology, Inference, Information exchange, BIM, AEC Knowledge Graph, Linked building data},
abstract = {In the architecture, engineering and construction (AEC) industry stakeholders from different companies and backgrounds collaborate in realising a common goal being some physical structure. The exact goal is typically not known from the beginning, and throughout all design stages, new decisions are made - similarly to other design industries [1]. As a result, the design must adapt and subsequent consequences follow. With working methods being predominantly document-centric, highly interrelated and rapidly changing design data in a complex network of decisions, requirements and product specifications is primarily captured in static documents. In this paper, we consider a purely data-driven approach based on semantic web technologies and an earlier proposed Ontology for Property Management (OPM). The main contribution of this work consists of extensions for OPM to account for new competency questions including the description of property reliability and the reasoning logic behind derived properties. The secondary contribution is the specification of a homogeneous way to generate parametric queries for managing an OPM-compliant AEC Knowledge Graph (AEC-KG). A software library for operating an OPM-compliant AEC-KG is further presented in the form of an OPM Query Generator (OPM-QG). The library generates SPARQL 1.1 queries to query and manipulate construction project Knowledge Graphs represented using OPM. The OPM ontology aligns with latest developments in the W3C Community Group on Linked Building Data and suggests an approach to working with design data in a distributed environment using separate graphs for explicit facts and for materialised, deduced data. Finally, we evaluate the suggested approach using an open-source software artefact developed using OPM and OPM-QG, demonstrated online with an actual building Knowledge Graph. The particular design task evaluated is performing heat loss calculations for spaces of a future building using an AEC-KG described using domain- and project specific extensions of the Building Topology Ontology (BOT) in combination with OPM. With this work, we demonstrate how a typical engineering task can be accomplished and managed in an evolving design environment, thereby providing the engineers with insights to support decision making as changes occur. The application uses a strict division between the client viewer and the actual data model holding design logic, and can easily be extended to support other design tasks.}
}
@article{SCHEUERMANN20207575,
title = {An Object-Oriented Library for Heat Transfer Modelling and Simulation in Open Cell Foams⁎⁎This work was supported by Deutsche Forschungsgemeinschaft (project number 317092854) and Agence Nationale de la Recherche (ID ANR-16-CE92-0028), project DFG-ANR INFIDHEM.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {7575-7580},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1354},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320317602},
author = {Tobias M. Scheuermann and Paul Kotyczka and Christian Martens and Haithem Louati and Bernhard Maschke and Marie-Line Zanota and Isabelle Pitault},
keywords = {Port-Hamiltonian systems, metallic foam, cell method, distributed parameter systems, discrete modeling, geometric discretization, process systems, simulation},
abstract = {Metallic open cell foams have multiple applications in industry, e.g. as catalyst supports in chemical processes. Their regular or heterogeneous microscopic structure determines the macroscopic thermodynamic and chemical properties. We present an object-oriented python library that generates state space models for simulation and control from the microscopic foam data, which can be imported from the image processing tool iMorph. The foam topology and the 3D geometric data are the basis for discrete modeling of the balance laws using the cell method. While the material structure imposes a primal chain complex to define discrete thermodynamic driving forces, the internal energy balance is evaluated on a second chain complex, which is constructed by topological duality. The heat exchange between the solid and the fluid phase is described based on the available surface data. We illustrate in detail the construction of the dual chain complexes, and we show how the structured discrete model directly maps to the software objects of the python code. As a test case, we present simulation results for a foam with a Kelvin cell structure, and compare them to a surrogate finite element model with homogeneous parameters.}
}
@article{MULAZZANI1986171,
title = {Dependability Prediction: Comparison of Tools and Techniques},
journal = {IFAC Proceedings Volumes},
volume = {19},
number = {11},
pages = {171-178},
year = {1986},
note = {5th IFAC Workshop on Safety of Computer Control Systems 1986 (SAFECOMP '86). Trends in Safe Real Time Computer Systems, Sarlat, France, 14-17 October, 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-034801-8.50032-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080348018500325},
author = {M. Mulazzani and K. Trivedi},
keywords = {Reliability theory, dependability analysis, markov models, hybrid models, software},
abstract = {Dependability measures (such as reliability, mean time to failure, availability) are important criteria for the design of computer-based applications, as well as for their validation. In this paper important techniques for dependability modeling are discussed, including methods of model construction and model solution. Some of the recent software packages for dependability analysis are compared: ADVISER, ARIES, CARE III, GRAMP, HARP, METFAC, SAVE, SHARPE, and SURF. The assumptions and properties of the software packages are analyzed with respect to a variety of different criteria including user interface, supported structures and principles, reconfiguration, repair, and solution method.}
}
@article{SOUZA2022114243,
title = {Shear capacity of prestressed hollow core slabs in flexible support using computational modelling},
journal = {Engineering Structures},
volume = {260},
pages = {114243},
year = {2022},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.114243},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622003728},
author = {Jefferson Rosa de Souza and Daniel de Lima Araújo},
keywords = {Shear capacity, Hollow core slabs, Flexible supports, Slim floor construction, Finite element analysis},
abstract = {The aim of the present investigation is to analyse the shear capacity of hollow core slabs supported on shallow beams, so-called slim floor construction, using a computational modelling performed with the DIANA software package. The finite element models were calibrated using two full-scale tests on a slim floor system and tests on single slabs on rigid supports reported in the literature. Besides, the numerical analyses were compared with analytical equations available to predict the shear capacity of hollow core slabs on both rigid and flexible supports. The non-linear three-dimensional finite element model agreed well with the results from full-scale tests on the slim floor system. A parametric study was performed to evaluate the influence of the infill at the ends of the hollow cores and the support stiffness of the steel beams on the shear capacity of the slabs. The numerical results confirm the influence of flexible supports on the shear capacity of hollow core slabs but reveal a marginal influence of the infill of the hollow cores. For hollow core slabs with oblong-shaped openings, the shear capacity of the slabs on the flexible supports was reduced when the hollow cores were filled. From the parametric study, design models available in the literature for hollow core slabs on flexible supports were tested and modified.}
}
@article{PAPAN20161308,
title = {Dynamic Response Analysis of the Ondrej Nepela Stadium Grandstands},
journal = {Procedia Engineering},
volume = {161},
pages = {1308-1315},
year = {2016},
note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning Symposium 2016, WMCAUS 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.594},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816328119},
author = {Daniel Papán and Veronika Valašková},
keywords = {dynamic analysis, custom shape, finite element method, natural frequency, dynamic response, Ondrej Nepela grandstand.},
abstract = {Behaviour of the spectators in the grandstands during a sports match may generate excessive vibration that can cause problems with the serviceability limit state. Construction of modern stadium due to architectural design and improving the material properties of the elements are made more efficient. This type of structural design brings many benefits. On the other hand, the size of its vibration, caused mainly by coordinated motion spectators, became of great values, which may exceed the limit values given in the respective standards. In this article we are dealing with modal analysis of structures Ondrej Nepela stadium in Bratislava and finding natural frequencies adjacent grandstands. Global support system and his description are based on the original design project documents from 1988 and subsequent project documentation from 2009. Complete reconstruction of the stadium in shape as it looks today, was in 2009. Ground plan dimensions of the hall are 86.0 x 102.8 meters and maximum theoretical amount at mid- range is 23.3 meters. The mods of natural vibration and natural frequencies were calculated by the computational model in computer program Scia Engineer with using the Lanczos mathematical method. Software package Scia Engineer uses the principle of finite element method (FEM) - numerical method for solving boundary value of continuum mechanics. FEM solves a system of differential equations transformed to a system of linear algebraic equations.}
}
@article{UGLIOTTI20163,
title = {BIM-based Energy Analysis Using Edilclima EC770 Plug-in, Case Study Archimede Library EEB Project},
journal = {Procedia Engineering},
volume = {161},
pages = {3-8},
year = {2016},
note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning Symposium 2016, WMCAUS 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.489},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816326972},
author = {Francesca Maria Ugliotti and Maurizio Dellosta and Anna Osello},
keywords = {building energy performance analysis, building information modelling, energy model, data exchange, existing buildings, EEB project.},
abstract = {In the recent years, energy efficiency issues as well as Building Information Modelling (BIM) have been the greatest and most challenging paradigms for the Architecture Engineering and Construction (AECO) industry in the context of Smart Cities. Digital models are used to analyse the existing building stock to promote a better management and retrofitting actions. Data modelling is the first step to pursue an integrated approach for the building lifecycle allowing simulations and analysis for different purposes through the interoperability process. This study aims to investigate the potential and the limitations of a BIM-based energy analysis by means of an Italian commonly used software for energy diagnosis and certificates, according to the quasi-steady method specified in the UNI/TS 11300-1:2008. The case study is a library with municipal offices in Settimo Torinese (Italy), which is the demonstrator of the ongoing Zero Energy Buildings in Smart Urban Districts (EEB) national cluster, which has the main scope to create a data management infrastructure able to integrate heterogeneous networks. The energy rate is evaluated from a simplified Revit architectural model, where the most significant components of the building in terms of energy are defined with a proper Level of Detail/Development (LOD) to easily set the energy model through the EC770 plug-in. In this way, the acquisition of geometrical data is allowed by the interoperability among software, speeding up the redundant preliminary phases of the simulation concerning the description of the building envelope.}
}
@article{AKHUKOV2023111832,
title = {MULTICOMP package for multilevel simulation of polymer nanocomposites},
journal = {Computational Materials Science},
volume = {216},
pages = {111832},
year = {2023},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2022.111832},
url = {https://www.sciencedirect.com/science/article/pii/S0927025622005432},
author = {Mikhail A. Akhukov and Vassily A. Chorkov and Alexey A. Gavrilov and Daria V. Guseva and Pavel G. Khalatur and Alexei R. Khokhlov and Andrey A. Kniznik and Pavel V. Komarov and Mike V. Okun and Boris V. Potapkin and Vladimir {Yu. Rudyak} and Denis B. Shirabaykin and Anton S. Skomorokhov and Sergey V. Trepalin},
keywords = {Multiscale simulations, Nanotechnology, Software package, Computer design, Polymer nanocomposites},
abstract = {We present a MULTICOMP package developed for multiscale modeling of polymer-based nanomaterials. The package implements GUI-based tools for the automatic construction of various types of polymer systems with the transition from the atomistic level of modeling to the mesolevel and from the mesolevel to the macrolevel. Using automated scripts, researchers can construct different kinds of nanomaterials, create flexible simulation schemas and define how data are transferred between engaged software modules and tools. The package makes it possible to analyze structural, thermophysical, mechanical properties, the cohesive energy density, the viscosity of polymer melts, and the diffusion of small molecules. Due to the client–server organization, the package can perform calculations on local and remote computing facilities and transfer the most time-consuming calculations to supercomputers. Examples of preparation and characterization of a cross-linked polymer matrix, a clay-based nanocomposite, and analyzing their properties illustrate MULTICOMP package operation.}
}
@incollection{NEUSTEIN20221,
title = {Chapter 1 - Introduction},
editor = {Amy Neustein and Nathaniel Christen},
booktitle = {Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care},
publisher = {Academic Press},
pages = {1-16},
year = {2022},
isbn = {978-0-323-85197-8},
doi = {https://doi.org/10.1016/B978-0-32-385197-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851978000052},
author = {Amy Neustein and Nathaniel Christen},
keywords = {conceptual spaces, data set, hypergraph, digital health ecosystem, code libraries, CDC, data curation},
abstract = {In the first chapter we present the scaffolding of the book, introducing novel approaches to data modeling, such as type theory, conceptual spaces, or graph database architectures. We posit that any database, data set, or information space should be engineered with the expectation that multiple (not fully isomorphic) software components will be interacting with that data; and that parts therein will be passed and shared between such components, implying that data should be structured to facilitate cross-component communication. We propose that most of the theoretical constructions summarized here vis-à-vis hypergraph or code models may be concretely instantiated through virtual machines, via which query evaluation engines may be implemented. We use the architecture of virtual machines within this category as an organizing motif for analyses. From a more practical or “applied” point of view, we will call attention in particular to biomedical research projects that synthesize information with variegated disciplinary provenance and diverse data profiles. While biomedical research is inherently interdisciplinary, new breakthroughs and new research methods and technologies have further accelerated the cross-disciplinary insights of research in several specific biomedical disciplines, yielding diagnostic, prognostic, and explanatory models that cut across biophysical scales (molecular, cellular, tissues, organs) and data acquisition modalities (proteomics, genomics, biopsies, image processing, lab assays—such as for biologic sample analysis—and so forth). In examining the literature where these integrative studies are described, it becomes clear that scientists often construct the software ecosystem powering their research in ad-hoc ways, piecing together diverse software components (sometimes standalone applications, sometimes code libraries, or some combination thereof) designed for specific disciplinary contexts. We argue in this book that the relatively informal trial-and-error approach to integrating multidisciplinary biomedical data can act as an impediment to research replication and the systematic evaluation of interdisciplinary research findings. This warrants engaging in a detailed review of data profiles, data modeling paradigms, and data integration techniques, so as to lay the foundation for a software ecosystem that can support the emerging paradigm of transparent and replicable research data and digital scientific resources, such as code libraries and publications.}
}
@article{LJUNGBERG2006814,
title = {Design and usability of a PDE solver framework for curvilinear coordinates},
journal = {Advances in Engineering Software},
volume = {37},
number = {12},
pages = {814-825},
year = {2006},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2006.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997806000469},
author = {Malin Ljungberg and Kurt Otto and Michael Thuné},
keywords = {Variability modeling, Object-oriented, Framework, PDE solver, Curvilinear, Feature},
abstract = {An object-oriented PDE solver framework is a library of software components for numerical solution of partial differential equations, where each component is an object or a group of objects. Given such a framework, the construction of a particular PDE solver consists in selecting and combining suitable components. The present paper is focused on tengo [Åhlander K, Otto K. Software design for finite difference schemes based on index notation. Future Generation Comput Syst 2006;22:102–9], an object-oriented PDE solver framework for finite difference methods on structured grids, using tensor abstractions for convenient representation of numerical operators. Here, the design of tengo is extended to address curvilinear coordinates. These extensions to the tengo object model are the result of applying object-oriented analysis and design combined with feature modeling. The framework was implemented in Fortran 90/95, using standard techniques for emulating object-oriented constructs in that language. The new parts of the framework were assessed with respect to programming effort and execution time. It is shown that the programming effort required for construction and modification of PDE solvers on curvilinear grids is significantly reduced through the introduction of the new framework components. Moreover, for the test case of an underwater acoustics computation, there was no significant difference in execution time between the framework based code and a special purpose Fortran 90 code for the same application.}
}
@article{PANFEROV201229,
title = {Software Package for Simulation and Control System Design for a Nonrigid Space Vehicle},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {1},
pages = {29-34},
year = {2012},
note = {1st IFAC Workshop on Embedded Guidance, Navigation and Control in Aerospace},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120213-3-IN-4034.00008},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015350151},
author = {Alexander I. Panferov and Alexander V. Nebylov and Sergey A. Brodsky},
keywords = {flexible vehicle, smart control approach, simulation, sloshing, local angle of attack, oscillations damping},
abstract = {New results in the field of automation of the space flight control system design are presented in this paper. The concept of a universal program for research of dynamic properties, calculation of comprehensive mathematical models, simulation of the flight motion and synthesis of smart control laws for different types of flexible aerospace constructions (launchers and missiles) is considered. Special attention is given to creation of mathematical models of different physical phenomena in aerospace vehicles. The basis of the program is a block that enables the analysis of dynamic characteristics, simulation, and the visual representation of information. The software package is supplied with a library of program modules. These modules are designed on the base of mathematical models of separate vehicle elements and different physical effects, such as flexibility, sloshing, sluggishness of engines, local angle of attack values, etc. Selection of different sets of mathematical models and program modules allows investigating different constructions for known and prospective vehicles. The interface of the program enables changing parameters of the vehicle in a wide range, simulate flight at any trajectory and output results of the analysis in a numerical or graphical form. For synthesis of control laws the smart control approach is used.}
}
@article{PANFEROV2013429,
title = {Complex Flexible Aerospace Vehicles Simulation and Control System Design},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {19},
pages = {429-434},
year = {2013},
note = {19th IFAC Symposium on Automatic Control in Aerospace},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130902-5-DE-2040.00115},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015363618},
author = {Alexander I. Panferov and Alexander V. Nebylov and Sergey A. Brodsky},
keywords = {mathematical model flexible vehicle simulation sloshing local angle of attack oscillations damping},
abstract = {New results in the field of the automaton of the aerospace flight control system design are presented in this paper. The concept of a universal program for research of dynamic properties, calculation of comprehensive mathematical models, and simulation of the flight motion and synthesis of smart control laws for different types of the flexible aerospace constructions (launchers and missiles) is considered. The special attention to creation of mathematical models of different physical phenomenon of aerospace vehicles is given. The basis of the program is the structure, which allows carrying out of the analysis of dynamic characteristics, simulation, and visual information representation. The software package is supplied with the library of program modules. These modules are designed on the base of mathematical models of separate vehicle elements and different physical effects, such as flexibility, sloshing, sluggishness of engines, local angle of attack values, etc. Selection of different sets of the mathematical models and program modules allows investigating different constructions for known and prospective vehicles. The interface of the program enables changing parameters of the vehicle in a wide range, to simulate flight at any trajectory and to output results of the analysis in a digital or graphical form. For synthesis of control laws, provides the usage of the smart control approach.}
}