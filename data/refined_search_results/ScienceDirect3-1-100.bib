@article{PRADHAN20202581,
title = {Solutions to Vulnerabilities and Threats in Software Defined Networking (SDN)},
journal = {Procedia Computer Science},
volume = {171},
pages = {2581-2589},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.280},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920312734},
author = {Aayush Pradhan and Rejo Mathew},
keywords = {Software Defined Networking (SDN), DDoS, packet sniffing, ARP spoofing, brute force, API exploitation},
abstract = {In today’s advancing world, there is an increase in the size and requirements of networks which can be a burden since moving around with switches is quite chaotic. SDN helps switches to be programmed and implemented independently. SDN is a way of providing programmability for network application development by separating the control plane from the data plane. Security of Software Defined Networking (SDN) is an open subject. Separating the control plane from the data plane opens up a number of security challenges such as a man-in-the-middle attack (MITM), a service denial (DoS), overload saturation attacks, etc. In this paper, we have focused on the overview of software defined network (SDN), it’s challenges, it’s issues, and their solutions. First, we have discussed about the architecture of SDN, followed by elaborating the threats and at the last, proposing some solutions to improvise the security.}
}
@article{DAVELLA2023102453,
title = {ROS-Industrial based robotic cell for Industry 4.0: Eye-in-hand stereo camera and visual servoing for flexible, fast, and accurate picking and hooking in the production line},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {80},
pages = {102453},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102453},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522001351},
author = {Salvatore D’Avella and Carlo Alberto Avizzano and Paolo Tripicchio},
keywords = {Flexible manufacturing, Visual servoing, Machine learning, ROS-Industrial, Industry 4.0},
abstract = {The fourth industrial revolution envisages the use of modern smart technologies to automate traditional manufacturing and industrial practices. However, industrial robots execute mostly pre-programmed jobs and are not able to face challenging tasks in unstructured environments. Industry 4.0 pushes for flexibility on target changes and autonomy. In line with the new principles of Industry 4.0, the proposed work describes an autonomous industrial cell that employs several smart technologies for loading jewelry pieces from a conveyor belt to a hooking frame built on purpose. The cell involves an industrial robot, a custom gripper, pneumatic and electric actuators with the aim of moving and opening the frame hooks, and a custom vision pipeline for detecting the feature of interest during the picking and hooking phases. The implemented pipeline makes use of a stereo camera pair mounted under the robot gripper and two fixed monocular cameras. The method employs HOG feature descriptors and machine learning algorithms for the detection. The software architecture is a component-based designed architecture that uses ROS as the underlying framework and ROS-Industrial packages to control the robot. The robot is controlled with position-based commands to reach intermediate positions in the workspace and with velocity command to implement a visual servoing control scheme that runs at 30 Hz and adjusts the robot position with the feedback of the vision during picking and hooking. The proposed visual servoing approach, thanks to the design of the stereo camera and choice of the optics, is able to perceive the features until the final movement phase, differently from most of the visual servoing employed in the literature that, due to the use of RGB-D camera or other vision apparatus, use an open control loop at a standoff distance. The presented work reaches an accuracy of 95% with a cycle time under 8 s.}
}
@article{COJEAN2022102902,
title = {Ginkgo—A math library designed for platform portability},
journal = {Parallel Computing},
volume = {111},
pages = {102902},
year = {2022},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2022.102902},
url = {https://www.sciencedirect.com/science/article/pii/S0167819122000096},
author = {Terry Cojean and Yu-Hsiang Mike Tsai and Hartwig Anzt},
keywords = {Porting to GPU accelerators, Platform Portability, Performance portability, AMD, NVIDIA, Intel},
abstract = {In an era of increasing computer system diversity, the portability of software from one system to another plays a central role. Software portability is important for the software developers as many software projects have a lifetime longer than a specific system, e.g., a supercomputer, and it is important for the domain scientists that realize their scientific application in a software framework and want to be able to run on one or another system. On a high level, there exist two approaches for realizing platform portability: (1) implementing software using a portability layer leveraging any technique which always generates specific kernels from another language or through an interface for running on different architectures; and (2) providing backends for different hardware architectures, with the backends typically differing in how and in which programming language functionality is realized due to using the language of choice for each hardware (e.g., CUDA kernels for NVIDIA GPUs, SYCL (DPC++) kernels to targeting Intel GPUs and other supported hardware, …). In practice, these two approaches can be combined in applications to leverage their respective strengths. In this paper, we present how we realize portability across different hardware architectures for the Ginkgo library by following the second strategy and the goal to not only port to new hardware architectures but also achieve good performance. We present the Ginkgo library design, separating algorithms from hardware-specific kernels forming the distinct hardware executors, and report our experience when adding execution backends for NVIDIA, AMD, and Intel GPUs. We also present the performance we achieve with this approach for distinct hardware backends.}
}
@article{BARBOSA2015639,
title = {A high performance hardware accelerator for dynamic texture segmentation},
journal = {Journal of Systems Architecture},
volume = {61},
number = {10},
pages = {639-645},
year = {2015},
note = {Special section on Architecture of Computing Systems edited by Editors: Wolfgang Karl, Erik Maehle, Kay Römer, Eduardo Tovar, Martin Danek Special section on Testing, Prototyping, and Debugging of Multi-Core Architectures edited by Editors: Frank Hannig & Andreas Herkersdorf Special section on Embedded Vision Architectures and Applications edited by Editors: Christophe Bobda, Walter Stechele, Ali Ahmadinia and Miaoqing Huang},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2015.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1383762115001022},
author = {João P.F. Barbosa and Antonyus P.A. Ferreira and Rodrigo C.F. Rocha and Erika S. Albuquerque and Josivan R. Reis and Djeefther S. Albuquerque and Edna N.S. Barros},
keywords = {Video segmentation, FFT, IFFT, FPGA, Hardware architecture, High performance system},
abstract = {Hardware accelerators such as general-purpose GPUs and FPGAs have been used as an alternative to conventional CPU architectures in scientific computing applications, and have achieved good speed-up results. Within this context, the present study presents a heterogeneous architecture for high-performance computing based on CPUs and FPGAs, which efficiently explores the maximum parallelism degree for processing video segmentation using the concept of dynamic textures. The video segmentation algorithm includes processing the 3-D FFT, calculating the phase spectrum and the 2-D IFFT operation. The performance of the proposed architecture based on CPU and FPGA is compared with the reference implementation of FFTW in CPU and with the cuFFT library in GPU. The performance report of the prototyped architecture in a single Stratix IV FPGA obtained an overall speedup of 37x over the FFTW software library.}
}
@article{BURES2021111065,
title = {Targeting uncertainty in smart CPS by confidence-based logic},
journal = {Journal of Systems and Software},
volume = {181},
pages = {111065},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111065},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100162X},
author = {Tomáš Bureš and Petr Hnětynka and František Plášil and Dominik Škoda and Jan Kofroň and Rima {Al Ali} and Ilias Gerostathopoulos},
keywords = {Software architecture, Adaptation, Uncertainty, Smart cyber–physical systems, Statistical testing},
abstract = {Since Smart Cyber–Physical Systems (sCPS) are complex and decentralized systems of dynamically cooperating components, architecture-based adaptation is of high importance in their design. In this context, a key challenge is that they typically operate in uncertain environments. Thus, an inherent requirement in sCPS design is the need to deal with the uncertainty of data coming from the environment. Existing approaches often rely on the fact that an adequate model of the environment and/or base probabilities or a prior distribution of data are available. In this paper, we present a specific logic (CB logic), which, based on statistical testing, allows specifying transition guards in architecture-based adaptation without requiring knowledge of the base probabilities or prior knowledge about the data distribution. Applicable in state machines’ transition guards in general, CB logic provides a number of operators over time series that simplify the filtering, resampling, and statistics-backed comparisons of time series, making the application of multiple statistical procedures easy for non-experts. The viability of our approach is illustrated on a running example and a case study demonstrating how CB logic simplifies adaptation triggers. Moreover, a library with a Java and C ++ implementation of CB logic’s key operators is available on GitHub.}
}
@article{REQUENAGARCIACRUZ2023105877,
title = {Comparative study of alternative equivalent frame approaches for the seismic assessment of masonry buildings in OpenSees},
journal = {Journal of Building Engineering},
volume = {66},
pages = {105877},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.105877},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223000566},
author = {M.V. Requena-Garcia-Cruz and S. Cattari and R. Bento and A. Morales-Esteban},
keywords = {Masonry structures, Equivalent frame models, OpenSees, Nonlinear analyses},
abstract = {The equivalent frame (EF) idealisation of masonry structures is widely used in engineering structures. Despite its simplifications, reliable numerical models can be produced to calculate the seismic behaviour of unreinforced masonry (URM) buildings. Different EF modelling approaches have been implemented in commercial software specifically conceived for performing nonlinear analyses on URM buildings (such as 3Muri, adopted in this study). Furthermore, the adoption of such an approach is also possible in general-purpose structural analyses software packages, such as OpenSees, through an ad hoc implementation of analysts. The aim of this paper is to compare various EF modelling approaches by adopting alternative nonlinear beam-elements in OpenSees belonging to the distributed and the lumped plasticity. To this aim, the responses of some benchmark cases study available in the literature from the “URM nonlinear modelling-benchmark project” within the context of ReLUIS projects have been adopted to preliminary test the reliability of the alternative approaches considered. In particular, they consist of some single panels and a trilith, for which the results of various software are already available. In the paper, the results obtained with OpenSees have been more in-depth compared with 3Muri, which is assumed as representative of a larger set of EF models adopted in engineering-practice (having already verified in previous works that it provides a reasonable scatter with other software package options). Then, the analyses have been extended as well to a 3D building representative of the neighbourhood of ‘El Plantinar’ in Seville. An accurate comparison has been carried out in terms of generalised forces, drifts and damage at an element and at a global scale. The results have shown that the method proposed in this manuscript allows using OpenSees to calculate masonry structures with the EF approach with a good agreement to other engineering-practice oriented tools. Thus, this outcome may constitute, in future research, the basis for exploiting the potential and versatility of OpenSees in accounting for other tricky phenomena: the soil-foundation-structure interaction.}
}
@article{JARRAY2022105505,
title = {SMETool: A web-based tool for soil moisture estimation based on Eo-Learn framework and Machine Learning methods},
journal = {Environmental Modelling & Software},
volume = {157},
pages = {105505},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105505},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002079},
author = {Noureddine Jarray and Ali {Ben Abbes} and Manel Rhif and Hanen Dhaou and Mohamed Ouessar and Imed Riadh Farah},
keywords = {Soil moisture estimation, Open source data, Web-based tool, Eo-learn, Machine learning, Sentinel-1A and 2A},
abstract = {Earth Observation (EO) technologies have played an increasingly important role in monitoring the Sustainable Development Goals (SDG). These technologies often combined with Machine Learning (ML) models provide efficient means for achieving the SDGs. The great progress of this combination is also demonstrated by the large number of software, web tools and packages that have been made available for free use. In this paper, we introduce a software architecture to facilitate the generation of EO information targeted towards soil moisture that derive several challenges regarding the facilitation of satellite data processing. Thus, this paper presents a web-based tool for Soil Moisture Estimation (SMETool), designed for the soil moisture estimation using Sentinel-1A and Sentinel-2A data based on Eo-learn library. SMETool implements several ML techniques such as (Artificial Neural Network (ANN), Random Forest (RF), Convolutional Neural Network (CNN), etc.). The SMETool could be very useful for decision makers in the region in assessing the effects of drought and desertification events. Experiments were carried out on two sites in Tunisia during the period from 2016 to 2017. Although the performance of the used models is very close, it is clear that CNN and RF outperformed other ML models. The achieved results reveal that the soil moisture, was highly correlated to the in-situ measurements with high Pearson’s correlation coefficient r (rRF=0.86, rANN=0.75, rXGBoost=0.79, rCNN=0.87) and low Root Mean Square Error (RMSE) (RMSERF = 1.09%, RMSEANN = 1.49%, RMSEXGBoost = 1.39%, RMSECNN = 1.12%), respectively.}
}
@article{GOEHNER2013167,
title = {LIBI: A framework for bootstrapping extreme scale software systems},
journal = {Parallel Computing},
volume = {39},
number = {3},
pages = {167-176},
year = {2013},
note = {High-performance Infrastructure for Scalable Tools},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2012.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819112000774},
author = {J.D. Goehner and D.C. Arnold and D.H. Ahn and G.L. Lee and B.R. {de Supinski} and M.P. LeGendre and B.P. Miller and M. Schulz},
keywords = {Infrastructure bootstrapping, Job launching, System software},
abstract = {As the sizes of high-end computing systems continue to grow to massive scales, efficient bootstrapping for distributed software infrastructures is becoming a greater challenge. Distributed software infrastructure bootstrapping is the procedure of instantiating all processes of the distributed system on the appropriate hardware nodes and disseminating to these processes the information that they need to complete the infrastructure’s start-up phase. In this paper, we describe the lightweight infrastructure-bootstrapping infrastructure (LIBI), both a bootstrapping API specification and a reference implementation. We describe a classification system for process launching mechanism and then present a performance evaluation of different process launching schemes based on our LIBI prototype.}
}
@article{LAGHI2023113380,
title = {Status of JADE, an open-source software for nuclear data libraries V&V},
journal = {Fusion Engineering and Design},
volume = {187},
pages = {113380},
year = {2023},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2022.113380},
url = {https://www.sciencedirect.com/science/article/pii/S0920379622003702},
author = {Davide Laghi and Marco Fabbri and Stefano {La Rovere} and Lorenzo Isolan and Raul Pampin and Alfredo Portone and Marco Sumini},
keywords = {JADE, Verification & validation, Nuclear data libraries, Automatization, Benchmark, Nuclear fusion},
abstract = {In the last couple of years, a combined effort between NIER, Università di Bologna and Fusion For Energy led to the development of JADE, a python-based open-source software for the Verification and Validation of nuclear data libraries. Nuclear data is fundamental for particle and radiation transport simulations which, in turn, are responsible for the evaluation of key quantities for fusion-related machines design such as nuclear heating, DPA, particles production and dose rates. The aim for the project is to offer standardization and automation to the V&V process of data libraries in order to speed up their release cycles and, at the same time, improve the quality of the data. JADE takes advantage of MCNP for the particles and radiation transport simulations and, even if it is potentially applicable to the whole nuclear industry, a particular focus on fusion applications is obtained through the selections of the default benchmarks that have been implemented. The code was recently made publicly available to the community and the status of its development is summarized in this work. The more important features and benchmarks (both computational and experimental) are described, together with a brief discussion on the major case studies where JADE has been used. Lastly, the current strength and limitations of the tool are evaluated and the foreseen future developments for the project are outlined.}
}
@article{CASTROLEON201598,
title = {Fault tolerance at system level based on RADIC architecture},
journal = {Journal of Parallel and Distributed Computing},
volume = {86},
pages = {98-111},
year = {2015},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731515001434},
author = {Marcela Castro-León and Hugo Meyer and Dolores Rexachs and Emilio Luque},
keywords = {Software fault tolerance, Resilience, RADIC, Message passing, Semi-coordinated checkpoint, Uncoordinated checkpoint, Socket},
abstract = {The increasing failure rate in High Performance Computing encourages the investigation of fault tolerance mechanisms to guarantee the execution of an application in spite of node faults. This paper presents an automatic and scalable fault tolerant model designed to be transparent for applications and for message passing libraries. The model consists of detecting failures in the communication socket caused by a faulty node. In those cases, the affected processes are recovered in a healthy node and the connections are reestablished without losing data. The Redundant Array of Distributed Independent Controllers architecture proposes a decentralized model for all the tasks required in a fault tolerance system: protection, detection, recovery and masking. Decentralized algorithms allow the application to scale, which is a key property for current HPC system. Three different rollback recovery protocols are defined and discussed with the aim of offering alternatives to reduce overhead when multicore systems are used. A prototype has been implemented to carry out an exhaustive experimental evaluation through Master/Worker and Single Program Multiple Data execution models. Multiple workloads and an increasing number of processes have been taken into account to compare the above mentioned protocols. The executions take place in two multicore Linux clusters with different socket communications libraries.}
}
@article{AREND2022100341,
title = {MLPro 1.0 - Standardized reinforcement learning and game theory in Python},
journal = {Machine Learning with Applications},
volume = {9},
pages = {100341},
year = {2022},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2022.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666827022000482},
author = {Detlef Arend and Steve Yuwono and Mochammad Rizky Diprasetya and Andreas Schwung},
keywords = {Machine learning, Reinforcement learning, Game theory, Automation, Scientific software development, Python},
abstract = {Nowadays there are numerous powerful software packages available for most areas of machine learning (ML). These can be roughly divided into frameworks that solve detailed aspects of ML and those that pursue holistic approaches for one or two learning paradigms. For the implementation of own ML applications, several packages often have to be involved and integrated through individual coding. The latter aspect in particular makes it difficult for newcomers to get started. It also makes a comparison with other works difficult, if not impossible. Especially in the area of reinforcement learning (RL), there is a lack of frameworks that fully implement the current concepts up to multi-agents (MARL) and model-based agents (MBRL). For the related field of game theory (GT), there are hardly any packages available that aim to solve real-world applications. Here we would like to make a contribution and propose the new framework MLPro, which is designed for the holistic realization of hybrid ML applications across all learning paradigms. This is made possible by an additional base layer in which the fundamentals of ML (interaction, adaptation, training, hyperparameter optimization) are defined on an abstract level. In contrast, concrete learning paradigms are implemented in higher sub-frameworks that build on the conventions of this additional base layer. This ensures a high degree of standardization and functional recombinability. Proven concepts and algorithms of existing frameworks can still be used. The first version of MLPro includes sub-frameworks for RL and cooperative GT.}
}
@article{GILES20131451,
title = {Designing OP2 for GPU architectures},
journal = {Journal of Parallel and Distributed Computing},
volume = {73},
number = {11},
pages = {1451-1460},
year = {2013},
note = {Novel architectures for high-performance computing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0743731512001694},
author = {M.B. Giles and G.R. Mudalige and B. Spencer and C. Bertolli and I. Reguly},
keywords = {Performance, GPU, CUDA, Unstructured mesh applications, Auto-tuning},
abstract = {OP2 is an “active” library framework for the solution of unstructured mesh applications. It aims to decouple the specification of a scientific application from its parallel implementation to achieve code longevity and near-optimal performance through re-targeting the back-end to different multi-core/many-core hardware. This paper presents the design of the current OP2 library for generating efficient code targeting contemporary GPU platforms. In this we focus on some of the software architecture design choices and low-level optimizations to maximize performance on NVIDIA’s Fermi architecture GPUs. The performance impact of these design choices is quantified on two NVIDIA GPUs (GTX560Ti, Tesla C2070) using the end-to-end performance of an industrial representative CFD application developed using the OP2 API. Results show that for each system, a number of key configuration parameters need to be set carefully in order to gain good performance. Utilizing a recently developed auto-tuning framework, we explore the effect of these parameters, their limitations and insights into optimizations for improved performance.}
}
@article{GRAHL2020111819,
title = {W7-X logbook REST API for processing experimental metadata and data enrichment at the Wendelstein 7-X stellarator},
journal = {Fusion Engineering and Design},
volume = {160},
pages = {111819},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111819},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620303677},
author = {M. Grahl and A. Spring and T. Andreeva and T. Bluhm and S. Bozhenkov and S. Dumke and J. Geiger and O. Grulke and M. Grün and A. Holtz and U. Höfel and H. Laqua and M. Lewerentz and H. Riemann and J. Schilling and A. {von Stechow} and J. Svensson and A. Winter},
keywords = {Experimental data, Contextual metadata, Data analysis, REST, Web services, Stellarators},
abstract = {Contextual metadata becomes an important factor for the work with research data, which is created in large amounts at fusion experiments. In its first operational phases, the stellarator experiment Wendelstein 7-X (W7-X) produces about 500 TB of experimental data in only a few weeks of scientific campaign. A new central logbook software was implemented for the processing of contextual metadata at W7-X. The bulk data of W7-X experiments can be enriched with additional information, such as comments and tags for categorizing. Since its introduction, many enhancements have been implemented based on user feedback. This was possible due to a flexible software architecture. By using the logbook REST API, users can integrate logbook requests into their own software and programmatically add new content. The logbook quickly became a crucial tool for W7-X operation and is now considered a central hub for all experiment-related information. This paper describes the implementation of the W7-X logbook and the experience with the integration of metadata via REST API.}
}
@article{PIGAZZINI2021110984,
title = {A study on correlations between architectural smells and design patterns},
journal = {Journal of Systems and Software},
volume = {178},
pages = {110984},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110984},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000819},
author = {Ilaria Pigazzini and Francesca Arcelli Fontana and Bartosz Walter},
keywords = {Architectural smells, Design patterns, Architectural smells and design patterns relationships},
abstract = {Design patterns are recommended solutions for typical software design problems, with an extensively studied and documented impact on various quality factors. Flaws in design at a higher levels of abstraction are manifested in architectural smells. Some of those smells, similarly to code smells, can reduce the expected advantages of design patterns or even prevent their proper implementation. In this paper we study if and how design patterns and architectural smells are related, and how this knowledge could be exploited in practice. We present an empirical study with an analysis of 16 design patterns and 3 architectural smells in 60 open source Java systems. We analyze their diffuseness and correlation, and we extract association rules that describe their presence and dependencies. We demonstrate that there exist relationships between architectural smells and design patterns, both at the class and package levels. Some smells appear falsely positive, as they result from conscious decisions made by programmers, while the application of some patterns can be a cause of certain smells. Our results provide evidence that design patterns and architectural smells are related and affect each other. With knowledge about the relationships, programmers can avoid the side effects of applying some design patterns.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{MACKIE20133,
title = {Dynamic analysis of structures on multicore computers – Achieving efficiency through object oriented design},
journal = {Advances in Engineering Software},
volume = {66},
pages = {3-9},
year = {2013},
note = {Civil-Comp Conference Special Issue},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813000331},
author = {R.I. Mackie},
keywords = {Component-oriented, Object-oriented, Eigenproblems, Transient analysis, Seismic analysis, Parallel computing, Distributed computing},
abstract = {The paper examines software design aspects of implementing parallel and distributed computing for transient structural problems. Overall design is achieved using object and component oriented methods. The ideas are implemented using .NET and the Task Parallel Library (TPL). Parallelisation and distribution is applied both to single problems, and to solving multiple problems. The use of object-oriented design means that the solvers and data are packaged together, and this helps facilitate distributed and parallel solution. Factory objects are used to provide the solvers, and interfaces are used to represent both the factory objects and solvers.}
}
@article{ALESSI2012966,
title = {High Performance Parallelization of COMPSYN on a Cluster of Multicore Processors with GPUs},
journal = {Procedia Computer Science},
volume = {9},
pages = {966-975},
year = {2012},
note = {Proceedings of the International Conference on Computational Science, ICCS 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.04.103},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912002244},
author = {Ferdinando Alessi and Annalisa Massini and Roberto Basili},
keywords = {GPU, CUDA, synthetic seismogram},
abstract = {In this work we propose a high performance parallelization of the software package COMPSYN, devoted to the production of syntethic seismograms, on a cluster of multicore processors with multiple GPUs. To design and implement the proposed high performance version, we started from a naıve parallel version of COMPSYN. The naıve version consists in a simple parallelization on both device side, obtained by exploiting CUDA, and host side, obtained by exploiting the MPI paradigm and OpenMP API. The proposed high performance version implements several practical techniques of CUDA programming and deeply exploits the GPU architecture, thus achieving a much better performance with respect to the naıve version. We compare the performance of the proposed high performance version and that of the naıve one with the performance of the version running on the cluster of multicore processors without invoking the GPUs. We obtain for the high performance GPU version a speedup of 25x over the version running on the cluster of multicore processors without GPUs against the 10x of the naıve version. Regarding the sequential version, we estimate about 380x the speedup of the high performance GPU version against the about 140x of the naıve version.}
}
@article{BALOS2021102836,
title = {Enabling GPU accelerated computing in the SUNDIALS time integration library},
journal = {Parallel Computing},
volume = {108},
pages = {102836},
year = {2021},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102836},
url = {https://www.sciencedirect.com/science/article/pii/S0167819121000831},
author = {Cody J. Balos and David J. Gardner and Carol S. Woodward and Daniel R. Reynolds},
keywords = {Exascale computing, GPU accelerators, Time integration, Numerical software},
abstract = {As part of the Exascale Computing Project (ECP), a recent focus of development efforts for the SUite of Nonlinear and DIfferential/ALgebraic equation Solvers (SUNDIALS) has been to enable GPU-accelerated time integration in scientific applications at extreme scales. This effort has resulted in several new GPU-enabled implementations of core SUNDIALS data structures, support for programming paradigms which are aware of the heterogeneous architectures, and the introduction of utilities to provide new points of flexibility. In this paper, we discuss our considerations, both internal and external, when designing these new features and present the features themselves. We also present performance results for several of the features on the Summit supercomputer and early access hardware for the Frontier supercomputer, which demonstrate negligible performance overhead resulting from the additional infrastructure and significant speedups when using both NVIDIA and AMD GPUs.}
}
@article{FREI2021100070,
title = {LocModFE: Locally modified finite elements for approximating interface problems in deal.II},
journal = {Software Impacts},
volume = {8},
pages = {100070},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100070},
url = {https://www.sciencedirect.com/science/article/pii/S266596382100018X},
author = {Stefan Frei and Thomas Richter and Thomas Wick},
keywords = {Locally modified finite elements, Fitted finite elements, Interface problems, , Deal.II},
abstract = {We describe the Software package LocModFE, which is an implementation of a locally modified finite element method for an accurate solution of interface problems. The code was originally developed in the finite element library Gascoigne 3d and has now been rewritten in the widerspread library deal.II. This makes the concept of locally modified finite elements accessible to many users all over the world. Applications range from simple Poisson interface problems over multi-phase flows to complex multi-physics problems, such as fluid–structure interactions. Being based on deal.II, it provides plenty of possibilities for future extensions, e.g., parallel computing, multigrid solvers or mesh adaptivity.}
}
@article{XIE2023112043,
title = {A Poisson-Nernst-Planck single ion channel model and its effective finite element solver},
journal = {Journal of Computational Physics},
volume = {481},
pages = {112043},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112043},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001389},
author = {Dexuan Xie and Zhen Chao},
keywords = {Poisson-Nernst-Planck equations, Finite element method, Single ion channel, Potassium channel, Electric current calculation},
abstract = {A single ion channel is a membrane protein with an ion selectivity filter that allows only a single species of ions (such as potassium ions) to pass through in the “open” state. Its selectivity filter also naturally separates a solvent domain into an intracellular domain and an extracellular domain. Such biological and geometrical characteristics of a single ion channel are novelly adopted in the construction of a new kind of dielectric continuum ion channel model, called the Poisson-Nernst-Planck single ion channel (PNPSIC) model, in this paper. An effective PNPSIC finite element solver is then developed and implemented as a software package workable for a single ion channel with a three-dimensional X-ray crystallographic molecular structure and a mixture of multiple ionic species. Numerical results for a potassium channel confirm the convergence and efficiency of the PNPSIC finite element solver and demonstrate the high performance of the software package. Moreover, the PNPSIC model is applied to the calculation of electric current and validated by biophysical experimental data.}
}
@article{YASSIN2020164747,
title = {Optical face detection and recognition system on low-end-low-cost Xilinx Zynq SoC},
journal = {Optik},
volume = {217},
pages = {164747},
year = {2020},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2020.164747},
url = {https://www.sciencedirect.com/science/article/pii/S0030402620305830},
author = {Kortli Yassin and Jridi Maher and Merzougui Mehrez and Atri Mohamed},
keywords = {Optical face detection and recognition system, VanderLugt correlator, Pose invariance, Low-end-low-cost Xilinx Zync SOC, Image processing},
abstract = {Optical face detection and recognition system (FDRS) is widely used in modern biometric security systems. In this paper, we implement a new model of FDRS to identify human faces under different poses. Particularly, we propose the Viola-Jones detector to locate and detect human faces and the VanderLugt correlator (VLC) technique for identification. Despite its extensive use, the VLC technique still requires design implementation efforts. This disadvantage is primarily due to the intensive computation required by the VLC technique. In our work, we propose three implementations (the first one is software and the last both are Hw/Sw co-design) on low-end-low-cost Xilinx Zync SOC that integrates both an ARM Cortex-A9 processor and an FPGA into a single device. In the software approach, we propose an implementation based on the ARM Cortex-A9 processor using OpenCV library. For the first Hw/Sw approach, we propose a co-design based on a hybrid ARM-FPGA platform, where the traditional hardware VLC architecture processed on FPGA. To respect real-time constraints, we developed an optimized hardware VLC architecture processed on FPGA to realize the second Hw/Sw co-design. A set of experiments has been considered using the PHPID database where the faces ranging from -30° to +30°. The experimental result showed that the optimized hardware VLC architecture provides a speedup of 4.4x compared to the non-optimized traditional hardware VLC architecture, and a speed-up of 39.22x compared to the software implementation. Finally, from a functional point-of-view, the recognition ratios obtained from both Hw/Sw co-designs are comparable to those obtained using software implementation.}
}
@article{MORELAND2021102834,
title = {Minimizing development costs for efficient many-core visualization using MCD3},
journal = {Parallel Computing},
volume = {108},
pages = {102834},
year = {2021},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102834},
url = {https://www.sciencedirect.com/science/article/pii/S016781912100082X},
author = {Kenneth Moreland and Robert Maynard and David Pugmire and Abhishek Yenpure and Allison Vacanti and Matthew Larsen and Hank Childs},
keywords = {Scientific visualization, Many-core architectures, Data-parallel primitives},
abstract = {Scientific visualization software increasingly needs to support many-core architectures. However, development time is a significant challenge due to the breadth and diversity of both visualization algorithms and architectures. With this work, we introduce a development environment for visualization algorithms on many-core devices that extends the traditional data-parallel primitive (DPP) approach with several existing constructs and an important new construct: meta-DPPs. We refer to our approach as MCD3 — Meta-DPPs, Convenience routines, Data management, DPPs, and Devices. The twin goals of MCD3 are to reduce developer time and to deliver efficient performance on many-core architectures, and our evaluation considers both of these goals. For development time, we study 57 algorithms implemented in the VTK-m software library and determine that MCD3 leads to significant savings. For efficient performance, we survey ten studies looking at individual algorithms and determine that the MCD3 hardware-agnostic approach leads to performance comparable to hardware-specific approaches: sometimes better, sometimes worse, and better in the aggregate. In total, we find that MCD3 is an effective approach for scientific visualization libraries to support many-core architectures.}
}
@article{HEYDAROV2023101302,
title = {Low-cost VIS/NIR range hand-held and portable photospectrometer and evaluation of machine learning algorithms for classification performance},
journal = {Engineering Science and Technology, an International Journal},
volume = {37},
pages = {101302},
year = {2023},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2022.101302},
url = {https://www.sciencedirect.com/science/article/pii/S2215098622002117},
author = {Saddam Heydarov and Musa Aydin and Cagri Faydaci and Suha Tuna and Sadullah Ozturk},
keywords = {Spectrometer, Classification, Data analysis, Machine learning, Implementation},
abstract = {In this study, the electronic design of a low-cost and portable spectrophotometer device capable of analyzing in the visible-near infrared region was established. The design of C#.NET-based user-friendly device control software and the development of machine learning algorithms for data classification as well as the comparison of the results were presented. When the spectrophotometer design and implementation studies are reviewed in the literature, two groups of subjects become prominent: (i) a new device fabrication, (ii) solution approaches to current problems by combining commercial portable spectrometer systems and devices with artificial intelligence applications. This work encompasses both groups, and a supportive approach has been followed on how to transform the theoretical knowledge into practice in device development and supportive software with the help of machine learning approaches from design to production. Three commercial spectral sensors, each with six photodiode arrays, were adopted in the spectrophotometer. Thus, 18 features belonging to each sample were acquired in the optical spectral region in the 410 nm to 940 nm band range. The spectral analyses were conducted with 9 different food types of powder or flake structures. A Support Vector Machines (SVM) and Convolutional Neural Network (CNN) approaches were employed for data classification. As a result, SVM and CNN achieved 97% and 95% accuracies, respectively. Moreover, we provided the spectral measurement data, the electronic circuit designs, the API files containing the artificial intelligence algorithms and the graphical user interface (GUI).}
}
@article{DURSO2019196,
title = {An integrated framework for the realistic simulation of multi-UAV applications},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {196-209},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618329161},
author = {Fabio D’Urso and Corrado Santoro and Federico Fausto Santoro},
keywords = {UAV, Flock, Swarm, Simulation framework, Autonomous UAV simulation, Multi-UAV, Network simulation, MANET, FANET, Internet-of-Things},
abstract = {This paper describes the software architecture of an integrated simulator for the realistic simulation of multi Unmanned aerial vehicle applications. The integrated simulator exploits some already existing tools to simulate a specific part of the overall Unmanned aerial vehicle hardware and software structure: a 3D visualization engine, a physics simulator, a flight control stack and a network simulator to handle communications among Unmanned aerial vehicles. These features are provided by the tools Gazebo, ArduCopterand ns-3 that, however, are not designed to work together in an integrated manner. The solution proposed in this paper is based on a software middleware that coordinates all of these tools, which may optionally be run on multiple interconnected computers, and lets them have a common notion of simulated time during the simulation; moreover, the middleware coordinates the activities of the High-level Logic, which is the software part that implements the strategy and control of the multi Unmanned aerial vehicle application. A Python API is provided to allow developers to write their Unmanned aerial vehicle application (cooperative missions, flocking, etc.) in such a way as to be first simulated and then run onto the real platform with no or few modifications.}
}
@article{BERNASCHI2020100041,
title = {BootCMatchG: An adaptive Algebraic MultiGrid linear solver for GPUs},
journal = {Software Impacts},
volume = {6},
pages = {100041},
year = {2020},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2020.100041},
url = {https://www.sciencedirect.com/science/article/pii/S2665963820300324},
author = {Massimo Bernaschi and Pasqua D’Ambra and Dario Pasquini},
keywords = {Adaptive AMG, GPU},
abstract = {Sparse solvers are one of the building blocks of any technology for reliable and high-performance scientific and engineering computing. In this paper we present a software package which implements an efficient multigrid sparse solver running on Graphics Processing Units. The package is a branch of a wider initiative of software development for sparse Linear Algebra computations on emergent HPC architectures involving a large research group working in many application projects over the last ten years.}
}
@article{VANDENBROECK2022102636,
title = {Flexible software protection},
journal = {Computers & Security},
volume = {116},
pages = {102636},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102636},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822000359},
author = {Jens {Van den Broeck} and Bart Coppens and Bjorn {De Sutter}},
keywords = {Man-at-the-end attacks, Obfuscation, Code reuse, Software protection, Flexibility, Stealth, Resilience, Opaque predicates},
abstract = {To counter software reverse engineering or tampering, software obfuscation tools can be used. However, such tools to a large degree hard-code how the obfuscations are deployed. They hence lack resilience and stealth in the face of many attacks. To counter this problem, we propose the novel concept of flexible obfuscators, which implement protections in terms of data structures and APIs already present in the application to be protected. The protections are hence tailored to the application in which they are deployed, making them less learnable and less distinguishable. In our research, we concretized the flexible protection concept for opaque predicates. We designed an interface to enable the reuse of existing data structures and APIs in injected opaque predicates, we analyzed their resilience and stealth, we implemented a proof-of-concept flexible obfuscator, and we evaluated it on a number of real-world use cases. This paper presents an in-depth motivation for our work, the design of the interface, an in-depth security analysis, and a feasibility report based on our experimental evaluation. The findings are that flexible opaque predicates indeed provide strong resilience and improved stealth, but also that their deployment is costly, and that they should hence be used sparsely to protect only the most security-sensitive code fragments that do not dominate performance. Flexible obfuscation therefor delivers an expensive but also more durable new weapon in the ever ongoing software protection arms race.}
}
@article{PYSCHNY2022328,
title = {Increased efficiency in virtual commissioning with automated model generation based on component libraries},
journal = {Procedia CIRP},
volume = {109},
pages = {328-333},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.258},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007077},
author = {Nicolas Pyschny and Ben Rudat and Eike Permin},
keywords = {Virtual Commissioning, Automated Model Generation, Engineering Automation},
abstract = {The use of Virtual Commissioning (VC) is becoming increasingly relevant for the engineering of automation systems. VC allows for validating concepts and design solutions in the early phases of product development, thereby improving communication among disciplines, reducing the costs for bug fixing and accelerating the overall engineering process, in particular the commissioning of plants and machines. However, as an additional software tool in engineering, the introduction of VC causes initial training and more importantly continuous extra modelling efforts. An approach to overcome this flaw is the automated generation of VC models based on existing engineering information from the system or domain level - ideally in tandem with the reuse of solution models from previous project. The latter can prove particularly beneficial in the context of modular building-block systems as often pursued in automation engineering. This paper presents a practical implementation of an automated VC model generation solution based on system structure and layout models combined with an object library containing additional model information of the applied automation components. The presented use case addresses VC simulation models of intralogistics warehousing systems where the layout plans provide information about type, position and dimension of predefined subsystems, modules or components which are then being analyzed and merged with additional model information. By means of a specific exporter the assembled model information is being filtered and transferred into the simulation environment via a standard API. Significant time savings could be demonstrated in this use case. Based on these results a generalized framework for more efficient interdisciplinary engineering processes based on automated data exchange as well as automated model transformation and generation has been derived and will be presented along with an outlook on future research activities.}
}
@article{CHERNAKOV2020111588,
title = {Framework for software development of laboratory equipment and setups integrated into large scale DAQ systems (LabBot)},
journal = {Fusion Engineering and Design},
volume = {156},
pages = {111588},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111588},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620301368},
author = {Alexandr Chernakov and Nikita Zhiltsov and Vasiliy Senitchenkov and Nikita Babinov and Alexander Bazhenov and Ivan Bukreev and Paul Chernakov and Anton Chernakov and Anastasia Chironova and Artem Dmitriev and Denis Elets and Nikita Ermakov and Igor Khodunov and Alexander Koval and Gleb Kurskiev and Andrei Litvinov and Alina Mitrofanova and Alexander Mokeev and Eugene Mukhin and Alexey Razdobarin and Dmitry Samsonov and Leonid Snigirev and Valeri Solovei and Ivan Tereschenko and Sergei Tolstyakov and Lidia Varshavchik and Paul Zatylkin},
keywords = {Data acquisition, Framework, Experiment automation, Firmware development, Scientific software},
abstract = {The LabBot Framework project is intended to implement control of experiment and data acquisition without writing special platform codes, while achieving industrial-grade results. The Framework is intended for fast and advanced development of controlling and data acquisition software for laboratory-scale experimental setups; more sophisticated software for middle-scale laboratory equipment; industrial-grade software for self-made commercial instrumentation developed by small- and middle-scale companies. The paper presents review of the Framework top-level architecture and its implementation addressing development of ITER Divertor Thomson Scattering (DTS) diagnostic. The first examples of the Framework implementation include the set-ups for vacuum heating testbench; RF cleaning of diagnostic mirrors as well as DTS equipment, which must be controlled by the ITER CODAC system.}
}
@article{CASTRO20133109,
title = {A language-independent approach to black-box testing using Erlang as test specification language},
journal = {Journal of Systems and Software},
volume = {86},
number = {12},
pages = {3109-3122},
year = {2013},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2013.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0164121213001714},
author = {Laura M. Castro and Miguel A. Francisco},
keywords = {Black-box testing, Functional testing, Test automation},
abstract = {Integration of reused, well-designed components and subsystems is a common practice in software development. Hence, testing integration interfaces is a key activity, and a whole range of technical challenges arise from the complexity and versatility of such components. In this paper, we present a methodology to fully test different implementations of a software component integration API. More precisely, we propose a black-box testing approach, based on the use of QuickCheck and inspired by the TTCN-3 test architecture, to specify and test the expected behavior of a component. We have used a real-world multimedia content management system as case study. This system offers the same integration API for different technologies: Java, Erlang and HTTP/XML. Using our method, we have tested all integration API implementations using the same test specification, increasing the confidence in its interoperability and reusability.}
}
@article{MERT2020103219,
title = {FPGA implementation of a run-time configurable NTT-based polynomial multiplication hardware},
journal = {Microprocessors and Microsystems},
volume = {78},
pages = {103219},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103219},
url = {https://www.sciencedirect.com/science/article/pii/S014193312030380X},
author = {Ahmet Can Mert and Erdinç Öztürk and Erkay Savaş},
keywords = {Number theoretic transform, Polynomial multiplication, Modular multiplier, SEAL},
abstract = {Multiplication of polynomials of large degrees is the predominant operation in lattice-based cryptosystems in terms of execution time. This motivates the study of its fast and efficient implementations in hardware. Also, applications such as those using homomorphic encryption need to operate with polynomials of different parameter sets. This calls for design of configurable hardware architectures that can support multiplication of polynomials of various degrees and coefficient sizes. In this work, we present the design and an FPGA implementation of a run-time configurable and highly parallelized NTT-based polynomial multiplication architecture, which proves to be effective as an accelerator for lattice-based cryptosystems. The proposed polynomial multiplier can also be used to perform Number Theoretic Transform (NTT) and Inverse NTT (INTT) operations. It supports 6 different parameter sets, which are used in lattice-based homomorphic encryption and/or post-quantum cryptosystems. We also present a hardware/software co-design framework, which provides high-speed communication between the CPU and the FPGA connected by PCIe standard interface provided by the RIFFA driver [1]. For proof of concept, the proposed polynomial multiplier is deployed in this framework to accelerate the decryption operation of Brakerski/Fan-Vercauteren (BFV) homomorphic encryption scheme implemented in Simple Encrypted Arithmetic Library (SEAL), by the Cryptography Research Group at Microsoft Research [2]. In the proposed framework, polynomial multiplication operation in the decryption of the BFV scheme is offloaded to the accelerator in the FPGA via PCIe bus while the rest of operations in the decryption are executed in software running on an off-the-shelf desktop computer. The hardware part of the proposed framework targets Xilinx Virtex-7 FPGA device and the proposed framework achieves the speedup of almost 7 ×  in latency for the offloaded operations compared to their pure software implementations, excluding I/O overhead.}
}
@article{ZHAO2021106619,
title = {Icon2Code: Recommending code implementations for Android GUI components},
journal = {Information and Software Technology},
volume = {138},
pages = {106619},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106619},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000926},
author = {Yanjie Zhao and Li Li and Xiaoyu Sun and Pei Liu and John Grundy},
keywords = {Android, App development, Collaborative filtering, Icon implementation, API recommendation},
abstract = {Context:
Event-driven programming plays a crucial role in implementing GUI-based software systems such as Android apps. However, such event-driven code is inherently challenging to design and implement correctly. Despite a significant amount of research to help developers efficiently implement such software, improved approaches are still needed to assist developers in better handling events and associated callback methods.
Objective:
This work aims at inventing an intelligent recommendation system for helping app developers efficiently and effectively implement Android GUI components.
Methods:
To achieve the aforementioned objective, we introduce in this work a novel approach called Icon2Code. Given an icon or UI widget provided by designers as input, Icon2Code first searches from a large-scale app database to locate similar icons used in existing popular apps. It then learns from the implementation of these similar apps and leverages a collaborative filtering model to select and recommend the most relevant APIs.
Results:
Our approach can achieve an 81% success rate when only five recommended APIs are considered, and a 94% success rate if twenty results are considered, based on ten-fold cross-validation with a large-scale dataset containing over 45,000 icons and their code implementations.
Conclusion:
It is feasible to automatically recommend code implementations for Android GUI components and Icon2Code is useful and effective in helping achieve such an objective.}
}
@article{MUMTAZ2021110885,
title = {A systematic mapping study on architectural smells detection},
journal = {Journal of Systems and Software},
volume = {173},
pages = {110885},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302752},
author = {Haris Mumtaz and Paramvir Singh and Kelly Blincoe},
keywords = {Architectural smells, Architectural debt, Antipatterns, Smell detection techniques, Systematic mapping study},
abstract = {The recognition of the need for high-quality software architecture is evident from the increasing trend in investigating architectural smells. Detection of architectural smells is paramount because they can seep through to design and implementation stages if left unidentified. Many architectural smells detection techniques and tools are proposed in the literature. The diversity in the detection techniques and tools suggests the need for their collective analysis to identify interesting aspects for practice and open research areas. To fulfill this, in this paper, we unify the knowledge about the detection of architectural smells through a systematic mapping study. We report on the existing detection techniques and tools for architectural smells to identify their limitations. We find there has been limited investigation of some architectural smells (e.g., micro-service smells); many architectural smells are not detected by tools yet; and there are limited empirical validations of techniques and tools. Based on our findings, we suggest several open research problems, including the need to (1) investigate undetected architectural smells (e.g., Java package smells), (2) improve the coverage of architectural smell detection across architecture styles (e.g., service-oriented and cloud), and (3) perform empirical validations of techniques and tools in industry across different languages and project domains.}
}
@article{OMAR20183921,
title = {Optimization of daylight utilization in energy saving application on the library in faculty of architecture, design and built environment, Beirut Arab University},
journal = {Alexandria Engineering Journal},
volume = {57},
number = {4},
pages = {3921-3930},
year = {2018},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1110016818301868},
author = {Osama Omar and Berta García-Fernández and Antonio Álvarez Fernández-Balbuena and Daniel Vázquez-Moliní},
keywords = {Daylight, Energy saving, Daylight factor, University building, Library},
abstract = {Considering that reading and research are the main functions of use in libraries of all educational facilities, proper lighting becomes a crucial factor in the overall success of a library design. In this framework, daylight is essential for both energy saving and improvement of the quality of life in newer buildings where visual tasks are more diverse, and technology poses new types of lighting requirements. Furthermore, emphasis on the importance and methods used to utilize energy will be implemented; provided by nature as the first step in achieving optimum energy saving and reducing our dependence on fossil fuels. Thus, this study will examine the conditions of indoor daylight and the library’s energy performance in the faculty of Architecture, Design and Built Environment, Beirut Arab University with various architectural elements including space depth, window size, external obstruction angle, and glazing visible transmittance. This is done by first analysing the existing situations of daylighting (using Autocad Ecotect software), the situation of the artificial lighting inside the space (using Dial DIALux software), and the behavior of the users throughout the day (using Hobo loggers). Then, the outcomes will be analyzed to specify the challenges, therefore providing solutions related to environmental, technological, and energy saving as well as sustainable and green building designs. As a result, daylight designs based on hollow prismatic light guides are proposed. These designs act as luminaires increasing guide efficiency and uniformity distribution of natural light into the library spaces. The proposed designs are configured and analyzed by ray-tracing simulations for achieving high illumination levels and uniform lighting in the working plane of the library.}
}
@article{SZWED2021107271,
title = {Classification and feature transformation with Fuzzy Cognitive Maps},
journal = {Applied Soft Computing},
volume = {105},
pages = {107271},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107271},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621001940},
author = {Piotr Szwed},
keywords = {Fuzzy Cognitive Maps, Classification, Feature transformation},
abstract = {Fuzzy Cognitive Maps (FCMs) are considered a soft computing technique combining elements of fuzzy logic and recurrent neural networks. They found multiple application in such domains as modeling of system behavior, prediction of time series, decision making and process control. Less attention, however, has been turned towards using them in pattern classification. In this work we propose an FCM based classifier with a fully connected map structure. In contrast to methods that expect reaching a steady system state during reasoning, we chose to execute a few FCM iterations (steps) before collecting output labels. Weights were learned with a gradient algorithm and logloss or cross-entropy were used as the cost function. Our primary goal was to verify, whether such design would result in a descent general purpose classifier, with performance comparable to off the shelf classical methods. As the preliminary results were promising, we investigated the hypothesis that the performance of d-step classifier can be attributed to a fact that in previous d−1 steps it transforms the feature space by grouping observations belonging to a given class, so that they became more compact and separable. To verify this hypothesis we calculated three clustering scores for the transformed feature space. We also evaluated performance of pipelines built from FCM-based data transformer followed by a classification algorithm. The standard statistical analyzes confirmed both the performance of FCM based classifier and its capability to improve data. The supporting prototype software was implemented in Python using TensorFlow library.}
}
@article{ROVERE2022773,
title = {Streamline 3D simulation model development for virtual commissioning with IEC61499},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {773-778},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.503},
url = {https://www.sciencedirect.com/science/article/pii/S240589632201802X},
author = {Diego Rovere and Marco Silvestri and Giovanni Dal Maso and Hilmo Dzafic and Paolo Pedrazzoli},
keywords = {Virtual Commissioning, Simulation, IEC61499, CPS, Distributed Automation},
abstract = {Current discrete manufacturing systems are characterized by an ever-increasing complexity, demanding for innovative solutions, capable to optimize performances while increasing the resilience and the capability to adapt to production modifications. With such a background, changing perspective to deal with distributed modular architectures of Cyber Physical Systems is mandatory, and the IEC 61499 standard, its object oriented, and event-based approaches promote this paradigm shift. The multi-disciplinary nature of the CPS entities and the possibility to exploit their digital counterparts, paves the way for the development of enhanced decision-support systems like the ones dedicated to Virtual Commissioning (VC). VC supports the automation developer in evaluating the impact of different management strategies, increasing the reliability of the final control applications, while reducing the amount of time to carry on physical tests on the real mechatronic system. However, creating a virtual commissioning model is still a complex and potentially expensive process that needs to be carried out by different professionals who must tightly cooperate to generate an effective playground for the automation testing. We propose a new approach to the design and develop virtual commissioning models, that, leveraging the synergies between modular simulation and IEC 61499 automation technologies, aims at improving the efficiency of the overall process of implementing 3D simulation digital twins for complex automated discrete manufacturing systems. The paper describes an open architecture, composed of reference data models and software API, and presents a proof-of-concept implementation of an integrated engineering platform of VC models.}
}
@article{WRIGHT2014754,
title = {The MPO API: A tool for recording scientific workflows},
journal = {Fusion Engineering and Design},
volume = {89},
number = {5},
pages = {754-757},
year = {2014},
note = {Proceedings of the 9th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2014.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0920379614000957},
author = {John C. Wright and Martin Greenwald and Joshua Stillerman and Gheni Abla and Bobby Chanthavong and Sean Flanagan and David Schissel and Xia Lee and Alex Romosan and Arie Shoshani},
keywords = {Metadata, Bigdata, Provenance, Workflow, Database},
abstract = {Data from large-scale experiments and extreme-scale computing is expensive to produce and may be used for high-consequence applications. The Metadata, Provenance and Ontology (MPO) project builds on previous work [M. Greenwald, Fusion Eng. Des. 87 (2012) 2205–2208] and is focused on providing documentation of workflows, data provenance and the ability to data-mine large sets of results. While there are important design and development aspects to the data structures and user interfaces, we concern ourselves in this paper with the application programming interface (API) – the set of functions that interface with the data server. Our approach for the data server is to follow the Representational State Transfer (RESTful) software architecture style for client–server communication. At its core, the API uses the POST and GET methods of the HTTP protocol to transfer workflow information in message bodies to targets specified in the URL to and from the database via a web server. Higher level API calls are built upon this core API. This design facilitates implementation on different platforms and in different languages and is robust to changes in the underlying technologies used. The command line client implementation can communicate with the data server from any machine with HTTP access.}
}
@article{FORTIN2022105476,
title = {A Web API for weather generation and pest development simulation in North America},
journal = {Environmental Modelling & Software},
volume = {157},
pages = {105476},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105476},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001803},
author = {Mathieu Fortin and Jean-François Lavoie and Jacques Régnière and Rémi Saint-Amant},
keywords = {Weather generator, Web API, Client–server architecture, Interoperability, Software maintenance, Software integration},
abstract = {Climate is an essential component of environmental models. Over the last two decades, many weather generators have been presented in the literature. Although their implementation into software has been of great help to environmental modellers, their lack of integration into modelling frameworks still represents a challenge for end users. In many cases, end users have to retrieve the climate variables by themselves in order to use an environmental model. In some other cases, the weather generator software is embedded into the modelling framework, but this increases the maintenance effort. In this paper, we present a different approach: the deployment of a weather generator as a Web API. A few application examples are provided to illustrate the benefits of this implementation. In summary, a Web API facilitates the integration into modelling frameworks, decreases the maintenance effort and avoid interoperability issues due to different programming languages.}
}
@article{PETCU20131417,
title = {Portable Cloud applications—From theory to practice},
journal = {Future Generation Computer Systems},
volume = {29},
number = {6},
pages = {1417-1430},
year = {2013},
note = {Including Special sections: High Performance Computing in the Cloud & Resource Discovery Mechanisms for P2P Systems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2012.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X12000210},
author = {Dana Petcu and Georgiana Macariu and Silviu Panica and Ciprian Crăciun},
keywords = {Cloud computing, Open-source API implementation, Application portability},
abstract = {The adoption of the Cloud computing concept and its market development are nowadays hindered by the problem of application, data and service portability between Clouds. Open application programming interfaces, standards and protocols, as well as their early integration in the software stack of the new technological offers, are the key elements towards a widely accepted solution and the basic requirements for the further development of Cloud applications. An approach for a new set of APIs for Cloud application development is discussed in this paper from the point of view of portability. The first available, proof-of-the-concept, prototype implementation of the proposed API is integrated in a new open-source deployable Cloudware, namely mOSAIC, designed to deal with multiple Cloud usage scenarios and providing further solutions for portability beyond the API.}
}
@article{CHIODI2022110787,
title = {General, robust, and efficient polyhedron intersection in the Interface Reconstruction Library},
journal = {Journal of Computational Physics},
volume = {449},
pages = {110787},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110787},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121006823},
author = {Robert Chiodi and Olivier Desjardins},
keywords = {Volume truncation, Volume integration, Volume distribution, Half-edge data structure, Non-convex polyhedra, Interface Reconstruction Library},
abstract = {Intersecting a polyhedron with a half-space or another polyhedron is required in various fields, including computer graphics, computer-aided design, and many methods used in computational physics. Often, the polyhedra involved are complicated non-convex objects, and intersecting them constitutes a significant computational cost. To address this, we present a general, geometrically robust, and efficient polyhedron intersection algorithm based on the half-edge data structure and implemented in the open-source Interface Reconstruction Library. We then employ this algorithm in a novel graph-based approach to distribute the volume of a polyhedron over a computational mesh. To demonstrate their performance, the two methods are tested on randomly generated configurations. Compared to the open-source packages VOFTools and R3D, the polyhedron intersection algorithm shows superior speed over the range of convex and non-convex polyhedra tested. The volume distribution method also proves to be discretely conservative, even for meshes containing degenerate faces.}
}
@article{WANG2014425,
title = {Design and implementation of five-axis transformation function in CNC system},
journal = {Chinese Journal of Aeronautics},
volume = {27},
number = {2},
pages = {425-437},
year = {2014},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2014.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1000936114000181},
author = {Feng Wang and Hu Lin and Liaomo Zheng and Lei Yang and Jinjin Feng and Han Zhang},
keywords = {3D cutter radius compensation, CNC software structure, Five-axis transformation function, Motion control, Rotation tool center point control},
abstract = {To implement five-axis functions in CNC system, based on domestic system Lan Tian series, an improved design method for the system software structure is proposed in this paper. The numerical control kernel of CNC system is divided into the task layer and the motion layer. A five-axis transformation unit is integrated into the motion layer. After classifying five-axis machines into different types and analyzing their geometry information, the five-axis kinematic library is designed according to the abstract factory pattern. Furthermore, by taking CA spindle-tilting machine as an example, the forward and the inverse kinematic transformations are deduced. Based on the new software architecture and the five-axis kinematic library, algorithms of RTCP (rotation tool center point control) and 3D radius compensation for end-milling are designed and realized. The milling results show that, with five-axis functions based on such software structure, the instructions with respect to the cutter’s position and orientation can be directly carried out in the CNC system.}
}
@article{VERDUZCO2022103189,
title = {CALRECOD — A software for Computed Aided Learning of REinforced COncrete structural Design},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103189},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103189},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000965},
author = {Luis Fernando Verduzco and Jaime Horta and Miguel A. Pérez Lara y Hernández and Juan Bosco Hernández},
keywords = {CALRECOD, Reinforced concrete structures, High education, Computed aided learning, Optimization methods},
abstract = {It is presented the development and implementation of a new computed aided learning MatLab Toolbox for the design of reinforced concrete structures named as CALRECOD for their abbreviation Computer Aided Learning of Reinforced Concrete Design. Such development emerges as the result of a series of research works in the Autonomous University of Queretaro with the main purpose of improving the way in which the design of reinforced concrete structures is taught in high education institutions. CALRECOD uses optimization methods and algorithms to aid students in their design interaction learning so that they are able to compare their own designs and what commercial software delivers with optimal ones given certain load conditions on the elements or structures. The software consists almost entirely of MatLab functions (.m files) and the ACI 318-19 code is taken as their main design reference to make it internationally useful, although in some cases the Mexican code NTC-17 specifications are used. Besides MatLab functions, the software consists as well of ANSYS SpaceClaim script functions (.scscript files) as an additional tool for the aid in the visualization of design results in a 3D space in the software ANSYS SpaceClaim. CALRECOD has proven to be versatile, flexible and of easy use with a huge potential to increase learning outcomes for students in high education programs related with the design of reinforced concrete structures as well as to enhance the creation of efficient interactive environments for researchers and academics focused on the development of new design and analysis methods for such structures. With their optimization design functions, a solid comparison platform of designs’ performance could be laid out, and with its extended function design packages for structural systems, reinforced concrete design courses could be enhanced in a great deal regarding their program content’s scope. The software can be found at: https://github.com/calrecod/CALRECOD.}
}
@article{SILVA2023120904,
title = {An IoT-based energy management system for AC microgrids with grid and security constraints},
journal = {Applied Energy},
volume = {337},
pages = {120904},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.120904},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923002684},
author = {Jéssica Alice A. Silva and Juan Camilo López and Cindy Paola Guzman and Nataly Bañol Arias and Marcos J. Rider and Luiz C.P. {da Silva}},
keywords = {Energy management system, Grid and security constraints, Internet-of-Things (IoT), Microgrids, Software-in-the-loop},
abstract = {This paper proposes an Internet-of-Things (IoT) based energy management system (EMS) for the optimal operation of unbalanced three-phase AC microgrids. The system utilizes a software architecture based on microservices, which includes a stochastic economic dispatch optimizer (EDO), a database, a web-based graphical user interface (GUI), and an application programming interface (API). The EDO uses a mixed-integer linear programming (MILP) model to ensure the day-ahead dispatch of the distributed energy resources (DERs) in the microgrid while adhering to grid constraints such as voltage, current, and power limits. Additionally, the optimization module takes into account security constraints for unplanned islanded operation, as well as stochastic scenarios of local demand and renewable generation. To assess the performance of the proposed IoT-based EMS, tests are conducted using a real-time simulator in a software-in-the-loop (SIL) experimental setup. Actual data from a microgrid located at the State University of Campinas (UNICAMP) in Brazil is utilized for the tests. The microgrid consisted of a photovoltaic (PV) system, a battery energy storage system (BESS), a thermal generation unit, and variable demands. Results indicated the effectiveness of the proposed IoT-based EMS in monitoring the operation of the microgrid and defining the optimal day-ahead dispatch of local DERs.}
}
@article{DUNNING202186,
title = {MATAR: A performance portability and productivity implementation of data-oriented design with Kokkos},
journal = {Journal of Parallel and Distributed Computing},
volume = {157},
pages = {86-104},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521000770},
author = {Daniel J. Dunning and Nathaniel R. Morgan and Jacob L. Moore and Eappen Nelluvelil and Tanya V. Tafolla and Robert W. Robey},
keywords = {Performance, Portability, Productivity, Memory efficiency, GPUs, Dense and sparse storage},
abstract = {There is a need for simple, fast, and memory-efficient multidimensional data structures for dense and sparse storage that arise with numerical methods and in software applications. The data structures must perform equally well across multiple computer architectures, including CPUs and GPUs. For this purpose, we developed MATAR, a C++ software library that allows for simple creation and use of intricate data structures that is also portable across disparate architectures using Kokkos. The performance aspect is achieved by forcing contiguous memory layout (or as close to contiguous as possible) for multidimensional and multi-size dense or sparse MATrix and ARray (hence, MATAR) types. Our results show that MATAR has the capability to improve memory utilization, performance, and programmer productivity in scientific computing. This is achieved by fitting more work into the available memory, minimizing memory loads required, and by loading memory in the most efficient order. This document describes the purpose of the work, the implementation of each of the data types, and the resulting performance both in some simple baseline test cases and in an application code.}
}
@article{FARQUHAR2023109489,
title = {Marconi-Rosenblatt Framework for Intelligent Networks (MR-iNet Gym): For Rapid Design and Implementation of Distributed Multi-agent Reinforcement Learning Solutions for Wireless Networks},
journal = {Computer Networks},
volume = {222},
pages = {109489},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109489},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005230},
author = {Collin Farquhar and Swatantra Kafle and Kian Hamedani and Anu Jagannath and Jithin Jagannath},
keywords = {Machine learning, Reinforcement learning, Wireless network, CDMA, Ns-3, OpenAI gym, LPD/I networks, Power and frequency control},
abstract = {We present the Marconi-Rosenblatt Framework for Intelligent Networks (MR-iNet Gym) an open-source architecture designed for accelerating research and development of novel reinforcement learning applied to distributed wireless networks. To ensure an end-to-end architecture, we leverage the existing work of ns3-gym, a software package that allows for using ns-3, a wireless network simulator, as an environment within the OpenAI Gym framework for RL. In addition to this, we have implemented the first known custom CDMA module for ns-3 as well as a framework for RL models with a core suite of implemented algorithms. The software framework capturing the interaction between wireless transceiver (agent) and RL decision engine has been designed to maximize the ease-of-use when testing different RL algorithms and models. In the rest of the paper, we describe these new software components and demonstrate some of the results and capabilities that can be achieved when used in conjunction with the existing open-source ecosystem.}
}
@article{KRISHNAN2021103196,
title = {OpenPATH: Application aware high-performance software-defined switching framework},
journal = {Journal of Network and Computer Applications},
volume = {193},
pages = {103196},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103196},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002022},
author = {Prabhakar Krishnan and Subhasri Duttagupta and Rajkumar Buyya},
keywords = {SDN, NFV, Service function chaining, Network function parallelism, Intent-based networking, Software switching},
abstract = {Currently, core networking architecture is facing disruptive developments, due to the emergence of SDN for control, NFV for services and so on. SDN promises more versatility in routing and managing traffic flows, while NFV represents a large shift in how network functions and services are built, deployed, and managed. We present OpenPATH (aPplication Aware software-defined swiTcHing framework)—A software-defined switching framework for NFV processing and orchestration of Network Functions (NFs) and steering the flows through service chains. Inspired by the potential benefits of encapsulating the application logic into the SDN dataplane, OpenPATH is built on the concept of a modular dataplane, which consists of two layers - switching fabric layer to control packet forwarding; and switch management layer, which inspects the incoming packets, steers the flows through a sequence of NFs and determines the next forward/drop action. The application logic of the NFs can be introduced and pushed to the dataplane at runtime and the framework offers fast packet processing and I/O functionalities to support NF parallelism in the Service Function Chaining (SFC) scenarios. OpenPATH is a modular framework for software switches and offers flexibility for programming run time functions depending on the dynamic behavior of the network traffic and cyberattacks. The architecture components are not hard-coded or rigidly implementations in conventional switches/bridges and standard OpenFlow based SDN stacks. The design allows the vendors, operators, or developers to configure policies at run time and deploy custom logic and NF (also series of NFs) through software programs embedded in the switching fabric. While the basic concept is similar to some pioneering works in this area, OpenPATH does not sacrifice portability, performance, or security for programmability. The OpenPATH as a programmable switching platform takes a different approach to meet most of the requirements of application-aware and intent-based networking. OpenPATH helps administrators to quickly configure network security services using a rich set of standard APIs, with simplified flow tables. The evaluation shows that our design can leverage complex states in the data plane without overloading the SDN controller. Compared to conventional SDN methods, this provides much greater versatility and precision. The key findings indicate that OpenPATH achieves lower cost for scaling, higher overall throughput, and reductions in latency for real-world service chains.}
}
@article{DEZSO201123,
title = {LEMON – an Open Source C++ Graph Template Library},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {264},
number = {5},
pages = {23-45},
year = {2011},
note = {Proceedings of the Second Workshop on Generative Technologies (WGT) 2010},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2011.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571066111000740},
author = {Balázs Dezső and Alpár Jüttner and Péter Kovács},
keywords = {C++, library, design, graph, network, template},
abstract = {This paper introduces LEMON, a generic open source C++ library providing easy-to-use and efficient implementations of graph and network algorithms and related data structures. The basic design concepts, features, and performance of LEMON are compared with similar software packages, namely BGL (Boost Graph Library) and LEDA. LEMON turned out to be a viable alternative to these widely used libraries, and our benchmarks show that it typically outperforms them in efficiency.}
}
@article{GODFREY201586,
title = {Understanding software artifact provenance},
journal = {Science of Computer Programming},
volume = {97},
pages = {86-90},
year = {2015},
note = {Special Issue on New Ideas and Emerging Results in Understanding Software},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2013.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167642313003055},
author = {Michael W. Godfrey},
keywords = {Software artifact provenance, Software analytics, Code cloning},
abstract = {In a well designed software system, units of related functionality are organized into modules and classes, which are in turn arranged into inheritance trees, package hierarchies, components, libraries, frameworks, and services. The trade-offs between simplicity versus flexibility and power are carefully considered, and interfaces are designed that expose the key functional properties of a component while hiding much of the complexity of the implementation details. However, over time the design integrity of a well-engineered system tends to decay as new features are added, as new quality attributes are emphasized, and as old architectural knowledge is lost when experienced development personnel shift to new jobs. Consequently, as developers and as users we often find ourselves looking at a piece of functionality or other design artifact and wondering, “Why is this here?” That is, we would like to examine the provenance of an artifact to understand its history and why it is where it is within the current design of the system. In this brief paper, we sketch some of the dimensions of the broad problem of extracting and reasoning about the provenance of software development artifacts. As a motivating example, we also describe some recent related work that uses hashing to quickly and accurately identify version information of embedded Java libraries.}
}
@article{CERASOLI2021110828,
title = {Advanced modeling of materials with PAOFLOW 2.0: New features and software design},
journal = {Computational Materials Science},
volume = {200},
pages = {110828},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2021.110828},
url = {https://www.sciencedirect.com/science/article/pii/S0927025621005486},
author = {Frank T. Cerasoli and Andrew R. Supka and Anooja Jayaraj and Marcio Costa and Ilaria Siloi and Jagoda Sławińska and Stefano Curtarolo and Marco Fornari and Davide Ceresoli and Marco {Buongiorno Nardelli}},
keywords = {DFT, Electronic structure,  tight-binding, High-throughput calculations},
abstract = {Recent research in materials science opens exciting perspectives to design novel quantum materials and devices, but it calls for quantitative predictions of properties which are not accessible in standard first principles packages. PAOFLOW, is a software tool that constructs tight-binding Hamiltonians from self-consistent electronic wavefunctions by projecting onto a set of atomic orbitals. The electronic structure provides numerous materials properties that otherwise would have to be calculated via phenomenological models. In this paper, we describe recent re-design of the code as well as the new features and improvements in performance. In particular, we have implemented symmetry operations for unfolding equivalent k-points, which drastically reduces the runtime requirements of first principles calculations, and we have provided internal routines of projections onto atomic orbitals enabling generation of real space atomic orbitals. Moreover, we have included models for non-constant relaxation time in electronic transport calculations, doubling the real space dimensions of the Hamiltonian as well as the construction of Hamiltonians directly from analytical models. Importantly, PAOFLOW has been now converted into a Python package, and is streamlined for use directly within other Python codes. The new object oriented design treats PAOFLOW’s computational routines as class methods, providing an API for explicit control of each calculation.}
}
@article{BOUZINIER2022104174,
title = {AnFiSA: An open-source computational platform for the analysis of sequencing data for rare genetic disease},
journal = {Journal of Biomedical Informatics},
volume = {133},
pages = {104174},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104174},
url = {https://www.sciencedirect.com/science/article/pii/S153204642200185X},
author = {M.A. Bouzinier and D. Etin and S.I. Trifonov and V.N. Evdokimova and V. Ulitin and J. Shen and A. Kokorev and A.A. Ghazani and Y. Chekaluk and Z. Albertyn and A. Giersch and C.C. Morton and F. Abraamyan and P.K. Bendapudi and S. Sunyaev and  {Undiagnosed Diseases Network} and  {Brigham Genomic Medicine} and  {SEQuencing a Baby for an Optimal Outcome} and  Quantori and J.B. Krier},
keywords = {Clinical genomics, Genome sequencing, Genome annotation, Genome filtering, Diagnostic clinical genomics, OLAP, Explainable models},
abstract = {Despite genomic sequencing rapidly transforming from being a bench-side tool to a routine procedure in a hospital, there is a noticeable lack of genomic analysis software that supports both clinical and research workflows as well as crowdsourcing. Furthermore, most existing software packages are not forward-compatible in regards to supporting ever-changing diagnostic rules adopted by the genetics community. Regular updates of genomics databases pose challenges for reproducible and traceable automated genetic diagnostics tools. Lastly, most of the software tools score low on explainability amongst clinicians. We have created a fully open-source variant curation tool, AnFiSA, with the intention to invite and accept contributions from clinicians, researchers, and professional software developers. The design of AnFiSA addresses the aforementioned issues via the following architectural principles: using a multidimensional database management system (DBMS) for genomic data to address reproducibility, curated decision trees adaptable to changing clinical rules, and a crowdsourcing-friendly interface to address difficult-to-diagnose cases. We discuss how we have chosen our technology stack and describe the design and implementation of the software. Finally, we show in detail how selected workflows can be implemented using the current version of AnFiSA by a medical geneticist.}
}
@article{COPPOCK2020103270,
title = {Hardware Root-of-Trust-based integrity for shared library function pointers in embedded systems},
journal = {Microprocessors and Microsystems},
volume = {79},
pages = {103270},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103270},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120304294},
author = {Patrick H. Coppock and Momen K. Yacoub and Bruce L. Qin and Alhad J. Daftardar and Zayd Tolaymat and Vincent J. Mooney},
keywords = {Computer security, Embedded software, Field-programmable gate arrays, Reconfigurable architectures, Runtime environment, Runtime library},
abstract = {Many security measures designed for large-scale compute hardware (e.g., workstations and servers) are not optimized for embedded systems. One such measure, Relocation Read-Only (RelRO), protects binary relocation sections from tampering after dynamic linking; however, RelRO depends on the presence of memory management hardware that not all embedded systems include. More generally, the latest hardware modifications to processor architectures may not always be available to provide security in small-scale embedded systems. In this paper, we propose another solution for relocation protection for use in embedded systems that have a field-programmable gate array (FPGA) on chip. Our solution prevents relocation section overwrites from diverting control flow as they would in an unprotected binary by implementing a hardware Root of Trust into which to store and out of which to retrieve shared library function pointers. We offer two system variations which provide designers with flexibility to choose a lighter or a more robust protection. We also demonstrate a proof of concept implemented on a popular FPGA development board and provide comparison with RelRO. Our work provides embedded system developers with a security measure like RelRO without requiring a customized memory management unit. Our work is useful on computer systems that include embedded reconfigurable logic on chip. The authors believe this is the first paper in which reconfigurable logic is used to provide security functionality previously implemented in a custom instruction set or other computer architecture modifications. Our work points toward a future where FPGA logic embedded on chip can be adapted to improve the security of software.}
}
@article{SCHMITT2023100666,
title = {sympy2c: From symbolic expressions to fast C/C++ functions and ODE solvers in Python},
journal = {Astronomy and Computing},
volume = {42},
pages = {100666},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2022.100666},
url = {https://www.sciencedirect.com/science/article/pii/S2213133722000804},
author = {U. Schmitt and B. Moser and C.S. Lorenz and A. Réfrégier},
keywords = {code generation, ODE solver, Python, Computer algebra},
abstract = {Computer algebra systems play an important role in science as they facilitate the development of new theoretical models. The resulting symbolic equations are often implemented in a compiled programming language in order to provide fast and portable codes for practical applications. We describe sympy2c, a new Python package designed to bridge the gap between the symbolic development and the numerical implementation of a theoretical model. sympy2c translates symbolic equations implemented in the SymPy Python package to C/C++ code that is optimized using symbolic transformations. The resulting functions can be conveniently used as an extension module in Python. sympy2c is used within the PyCosmo Python package to solve the Einstein–Boltzmann equations, a large system of ODEs describing the evolution of linear perturbations in the Universe. After reviewing the functionalities and usage of sympy2c, we describe its implementation and optimization strategies. This includes, in particular, a novel approach to generate optimized ODE solvers making use of the sparsity of the symbolic Jacobian matrix. We demonstrate its performance using the Einstein–Boltzmann equations as a test case. sympy2c is general and potentially useful for various areas of computational physics. sympy2c is publicly available at https://cosmology.ethz.ch/research/software-lab/sympy2c.html.}
}
@article{BAKSI2009161,
title = {Integrating MPI and deduplication engines: A software architecture roadmap},
journal = {International Journal of Medical Informatics},
volume = {78},
number = {3},
pages = {161-169},
year = {2009},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505608000853},
author = {Dibyendu Baksi},
keywords = {Model-driven architecture, Unique patient identifier, Vendor lock-in, Hospital information system, Interfaces},
abstract = {Objectives
: The objective of this paper is to clarify the major concepts related to architecture and design of patient identity management software systems so that an implementor looking to solve a specific integration problem in the context of a Master Patient Index (MPI) and a deduplication engine can address the relevant issues.
Methods
: The ideas presented are illustrated in the context of a reference use case from Integrating the Health Enterprise Patient Identifier Cross-referencing (IHE PIX) profile. Sound software engineering principles using the latest design paradigm of model driven architecture (MDA) are applied to define different views of the architecture.
Results
: The main contribution of the paper is a clear software architecture roadmap for implementors of patient identity management systems. Conceptual design in terms of static and dynamic views of the interfaces is provided as an example of platform independent model. This makes the roadmap applicable to any specific solutions of MPI, deduplication library or software platform.
Conclusions
: Stakeholders in need of integration of MPIs and deduplication engines can evaluate vendor specific solutions and software platform technologies in terms of fundamental concepts and can make informed decisions that preserve investment. This also allows freedom from vendor lock-in and the ability to kick-start integration efforts based on a solid architecture.}
}
@article{VENKATARAMANA2021153812,
title = {Implementation of radar digital receiver using intel integrated performance primitives},
journal = {AEU - International Journal of Electronics and Communications},
volume = {138},
pages = {153812},
year = {2021},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2021.153812},
url = {https://www.sciencedirect.com/science/article/pii/S1434841121002090},
author = {P Venkataramana and Suresh Mopuri and Gannera Mamatha},
keywords = {Radar digital receiver, Pulse compression, Tracking radar, Software defined radar},
abstract = {Radar Digital Receiver (RDR) requires to perform many computationally intensive operations in real-time. Therefore, RDRs have been implemented using DSP (Digital Signal Processor) or high performance FPGA (Field Programmable Gate Array) based customized hardware and software, and optimized for heavy math computation. These designs have a few limitations like long development cycle and high implementation cost due to customized hardware and proprietary software including development tools, Board Support Package and Operating System (OS). A new design is proposed to overcome these limitations by making use of low-cost COTS (Commercially Off The Shelf) hardware containing multi-core processors that are enabled to perform complex math efficiently, open source software and ready to use Intel’s IPP (Integrated Performance Primitives) library of functions. A prototype RDR for tracking application is developed and evaluated by integrating with an S-Band Radar and tracking various targets. The details of design, implementation and performance evaluation of the prototype RDR are presented in this paper.}
}
@incollection{PRIFTI20221321,
title = {A Capex Opex Simultaneous Robust Optimizer: Process Simulation-based Generalized Framework for Reliable Economic Estimations},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1321-1326},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50221-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790502216},
author = {Kristiano Prifti and Andrea Galeazzi and Massimo Barbieri and Flavio Manenti},
keywords = {Economics, Aspen HYSYS, Simulation, Optimization},
abstract = {During the design of a process, the most economically impactful choices are often taken with limited data and details available on the specifics of unit operations. Despite this fact, the effect some of these choices have on the final economic performance of the plant can be dramatic and far more influential than later decisions. The availability of fast and reliable preliminary cost estimations based on few key design parameters for the most common units can go a long way in improving flowsheet design. Due to the early design stage, these estimates don’t need to be very accurate since errors in the range of 50% are admissible as long as the estimation requires few input data. Automating the costs computation process and making the interface with the most common simulation packages standardized and easily accessible for users not accustomed to programming languages is very important in speeding up the cost evaluation of the plant and reducing the human resources tied to this task. These concepts stand at the core design of the CAPEX OPEX Robust Optimizer (CORO) developed in this work. Aspen HYSYS serves as the commercial simulation package to estimate the input variable of the economic libraries. Excel is used both as a GUI and as a data extraction tool from Aspen HYSYS due to its widespread diffusion in industry and versatility provided by Visual Basic for Applications. The CORO code is detached both from Excel and HYSYS and interacts only with a standardized xml data sheet to allow for in-house expansions to other simulation packages. The long-term development goal is a generalized CAPE-OPEN interface working with every commercial software that supports the interface. In the current CORO release, the interface can interact only with Aspen HYSYS. This paper will showcase the economic libraries implemented in CORO, the mathematical C++ optimization libraries, the overall structure of the tool, planned future expansions, and customization options.}
}
@article{LIU201266,
title = {An object-oriented serial implementation of a DSMC simulation package},
journal = {Computers & Fluids},
volume = {57},
pages = {66-75},
year = {2012},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2011.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045793011003707},
author = {Hongli Liu and Chunper Cai and Chun Zou},
keywords = {DSMC implementation, Hybrid grid, Object-Oriented Programming, Rarefied gas flow},
abstract = {This paper reports a scalar implementation of a multi-dimensional direct simulation Monte Carlo (DSMC) package named “Generalized Rarefied gAs Simulation Package” (GRASP). This implementation adopts a concept of simulation engine and it utilizes many Object-Oriented Programming features and software engineering design patterns. As a result, this implementation successfully resolves the problem of program functionality and interface conflictions for multi-dimensional DSMC implementations. The package has an open architecture which benefits further development and code maintenance. To reduce engineering time for three-dimensional simulations, one effective implementation is to adopt a hybrid grid scheme with a flexible data structure, which can automatically treat cubic cells adjacent to object surfaces. This package can utilize traditional structured, unstructured or hybrid grids to model multi-dimensional complex geometries and simulate rarefied non-equilibrium gas flows. Benchmark test cases indicate that this implementation has satisfactory accuracy for complex rarefied gas flow simulations.}
}
@article{FREDIAN2010568,
title = {MDSplus objects—Python implementation},
journal = {Fusion Engineering and Design},
volume = {85},
number = {3},
pages = {568-570},
year = {2010},
note = {Proceedings of the 7th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2010.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0920379610001705},
author = {T. Fredian and J. Stillerman and G. Manduchi},
keywords = {MDSplus, Data acquisition},
abstract = {MDSplus is a data acquisition and analysis software package used widely throughout the international fusion research community. During the past year, an important set of enhancements were designed under the project name of “MDSobjects” which would provide a common, powerful application programming interface (API) to MDSplus in programming languages with object-oriented capabilities. This paper will discuss the Python language implementation of this API and some of the capabilities that this implementation provides for data storage and retrieval using the MDSplus system. We have implemented a new MDSplus Python module which exposes the MDSplus objects features to the language. The internal MDSplus programming language, TDI, has also been enhanced to be able to invoke Python commands from the TDI language. Now that Python is aware of the complex data structures in MDSplus such as Signals, the language becomes a very good candidate for applications ranging from data acquisition device support to analysis and visualization.}
}
@article{NAG2021489,
title = {Prototyping operational autonomy for Space Traffic Management},
journal = {Acta Astronautica},
volume = {180},
pages = {489-506},
year = {2021},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2020.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S0094576520307323},
author = {Sreeja Nag and David D. Murakami and Nimesh A. Marker and Miles T. Lifson and Parimal H. Kopardekar},
keywords = {Space debris, Space traffic, Decision making under uncertainty, Trajectory optimization, Spacecraft management},
abstract = {Current state of the art in Space Traffic Management (STM) relies on a handful of providers for surveillance and collision prediction, and manual coordination between operators. Neither is scalable to support the expected 10x increase in active spacecraft population in less than 10 years, nor does it support automated maneuver planning. We present a software prototype of an STM architecture based on open Application Programming Interfaces (APIs), and drawing on insights from NASA's architecture for low-altitude Unmanned Aerial System Traffic Management. The STM architecture is designed to provide structure to the interactions between spacecraft operators, various regulatory bodies, and STM service suppliers, while maintaining flexibility of these interactions and the ability for new market participants to enter easily. Autonomy will be an indispensable part of the proposed architecture in enabling efficient data sharing, coordination between STM participants and safe flight operations (e.g. select spacecraft maneuvers to prevent impending conjunctions between multiple spacecraft). The STM prototype is based on modern micro-service architecture adhering to OpenAPI standards and deployed in industry-standard virtualized containers, facilitating easy communication between different participants or services. The system architecture is designed to facilitate adding and replacing services with minimal disruption. We have implemented some example participant services (e.g. a space situational awareness/SSA provider, a conjunction assessment supplier/CAS, an automated maneuver advisor/AMA) within the prototype. Different services, with creative algorithms folded into them, can fulfill similar functional roles within the STM architecture by flexibly connecting to it using pre-defined APIs and data models, thereby lowering the barrier to entry of new players in the STM marketplace. We demonstrate the STM prototype on a multiple conjunction scenario with multiple maneuverable spacecraft, where an example CAS and AMA can recommend optimal maneuvers to the spacecraft operators, based on a pre-defined reward function. Such tools can intelligently search the space of potential collision avoidance maneuvers with varying parameters like lead time and propellant usage, to optimize a customized reward function, and be implemented as a scheduling service within the STM architecture. The case study shows an example of autonomous maneuver planning using the API-based framework. As satellite populations and predicted conjunctions increase, an STM architecture can facilitate seamless information exchange related to collision prediction and mitigation among various service applications on different platforms and servers. The availability of such an STM network also opens up new research topics on satellite maneuver planning, scheduling and negotiation across disjoint entities.}
}
@article{ROVERA2022107111,
title = {Development of a REDCap-based workflow for high-volume relational data analysis on real-time data in a medical department using open source software},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107111},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107111},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722004928},
author = {Guido Rovera and Piero Fariselli and Désirée Deandreis},
keywords = {Database management systems, Data collection, Data analysis, Workflow},
abstract = {Background/Aim
The current availability of large volumes of clinical data has provided medical departments with the opportunity for large-scale analyses, but it has also brought forth the need for an effective strategy of data-storage and data-analysis that is both technically feasible and economically sustainable in the context of limited resources and manpower. Therefore, the aim of this study was to develop a widely-usable data-collection and data-analysis workflow that could be applied in medical departments to perform high-volume relational data analysis on real-time data.
Methods
A sample project, based on a research database on prostate-specific-membrane-antigen/positron-emission-tomography scans performed in prostate cancer patients at our department, was used to develop a new workflow for data-collection and data-analysis. A checklist of requirements for a successful data-collection/analysis strategy, based on shared clinical research experience, was used as reference standard. Software libraries were selected based on widespread availability, reliability, cost, and technical expertise of the research team (REDCap-v11.0.0 for collaborative data-collection, Python-v3.8.5 for data retrieval and SQLite-v3.31.1 for data storage). The primary objective of this study was to develop and implement a workflow to: a) easily store large volumes of structured data into a relational database, b) perform scripted analyses on relational data retrieved in real-time from the database. The secondary objective was to enhance the strategy cost-effectiveness by using open-source/cost-free software libraries.
Results
A fully working data strategy was developed and successfully applied to a sample research project. The REDCap platform provided a remote and secure method to collaboratively collect large volumes of standardized relational data, with low technical difficulty and role-based access-control. A Python software was coded to retrieve live data through the REDCap-API and persist them to an SQLite database, preserving data-relationships. The SQL-language enabled complex datasets retrieval, while Python allowed for scripted data computation and analysis. Only cost-free software libraries were used and the sample code was made available through a GitHub repository.
Conclusions
A REDCap-based data-collection and data-analysis workflow, suitable for high-volume relational data-analysis on live data, was developed and successfully implemented using open-source software.}
}
@article{CANTABONI2022372,
title = {Modelling and FE simulation of 3D printed Co-Cr Lattice Structures for biomedical applications},
journal = {Procedia CIRP},
volume = {110},
pages = {372-377},
year = {2022},
note = {V CIRP Conference on BioManufacturing},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122008496},
author = {F. Cantaboni and P. Ginestra and M. Tocci and A. Colpani and A. Avanzini and A. Pola and E. Ceretti},
keywords = {Lattice structure, Co-Cr-Mo, FEM analysis, stiffness matrix, unit cell.},
abstract = {The lattice structures are a particular type of structures made by the repetition of a unit cell and show great design opportunities. In addition, their structure is really close to some physiological tissues, which can allow their use to develop prostheses needed to the rehabilitation or replacement of a body part. However, their use is still limited, mainly due to the lack of methods to fully implement them during the production and to virtually predict all their mechanical properties. This problem is mostly caused by the computational effort and number of design parameters that the implementation of these materials in a Finite Element Modelling (FEM) analysis requires. Moreover, many common CAD software have a lack of materials libraries and geometry flexibility. In this work, samples with different lattice structures were manufactured by Laser Powder Bed Fusion technique using Co-Cr-Mo alloy. Compression tests were carried out to characterize their mechanical behavior. Subsequently, modelling and FE simulations were carried out to predict their mechanical response. In fact, a Finite Element Analysis allows to have a preview of final designed structure and to reduce the experimental tests otherwise needed to reach the final design, saving time and resources. Numerical simulations of the compression test were performed by FEM code Abaqus, in order to explore the possibilities and limitations of this approach for the study of lattice structures. Results of numerical simulations were compared with experimental data. Finally, NTopology software was also used to study the stiffness of the lattice structures according to the geometry of the investigated unit cells.}
}
@article{HARIHARAN2005311,
title = {Efficient parallel algorithms and software for compressed octrees with applications to hierarchical methods},
journal = {Parallel Computing},
volume = {31},
number = {3},
pages = {311-331},
year = {2005},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2004.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S016781910500027X},
author = {Bhanu Hariharan and Srinivas Aluru},
keywords = {Compressed octrees, Hierarchical methods, Parallel algorithms, Parallel software library, Parallel tree data structures},
abstract = {We describe the design and implementation of efficient parallel algorithms, and a software library for the parallel implementation of compressed octree data structures. Octrees are widely used in supporting hierarchical methods for scientific applications such as the N-body problem, molecular dynamics and smoothed particle hydrodynamics. The primary goal of our work is to identify and abstract the commonalities present in various hierarchical methods using octrees, design efficient parallel algorithms for them, and encapsulate them in a software library. We designed provably efficient parallel algorithms and implementation strategies that perform well irrespective of the spatial distribution of data in the computational domain. The library will enable rapid development of applications, allowing application developers to use efficient parallel algorithms developed for this purpose, without the necessity of having detailed knowledge of the algorithms or of implementing them. The software is developed in C using the Message Passing Interface (MPI). We report experimental results on an IBM xSeries parallel computer.}
}
@article{MINAFO20231939,
title = {Development of a software package with GUI for the evaluation of monolithicity factors of RC jacketed columns},
journal = {Structures},
volume = {48},
pages = {1939-1948},
year = {2023},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2023.01.061},
url = {https://www.sciencedirect.com/science/article/pii/S2352012423000620},
author = {Giovanni Minafò and Gaetano Camarda},
keywords = {RC jacketing, Monolithicity factors, Graphical User Interface},
abstract = {The design of reinforced concrete (RC) jacketing for retrofitting substandard columns is a common practice when approaching the structural rehabilitation design of existing structures. In this framework, international technical codes suggest to assume the hypothesis of monolithic section and finally affecting the deduced performances by some empirical factors, known in the literature as monolithicity factors. The assessment of these coefficients is a difficult task, and only some rough assumptions can be found in the modern codes. This paper presents the development of a software package able to perform the static non-linear analysis of RC jacketed columns and evaluating the actual value of the monolithicity factor. The code is developed in an integrated environment, taking advantage of finite element (FE) framework OpenSees and adopting Matlab for developing an user-friendly Graphical User Interface (GUI). The fibre-section approach is automatically implemented for modelling the flexural behaviour of the column and the effective bond between the old and new column in considered through a distribution of non-linear springs. Finally, the parametric analysis performed by the proposed tool allows to present the monolithicity factors as a function of the key parameters.}
}
@article{RATHOD20201027,
title = {Parameter Extraction of PSP MOSFET Model in Multi-core Zynq SoC Platform},
journal = {Procedia Computer Science},
volume = {171},
pages = {1027-1036},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.110},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920310863},
author = {Amit Rathod and Rajesh Thakker and A. Amalin Prince},
keywords = {Zynq SoC, System on Chip, Asymmetric Multi-Processing, Particle Swarm Optimization, PSP MOSFET model},
abstract = {Zynq System on Chip (SoC) is widely used architecture to accelerate the computational intensive application with its ARM Processing System (PS) and FPGA fabric - Programmable Logic (PL). In this paper, parameter extraction of PSP MOSFET model is studied as a computational intensive process and its function is accelerated by HW/SW co-design on Zynq SoC using its dual-core Asymmetric Multiprocessing (AMP) architecture. Particle Swarm Optimization (PSO) algorithm is being applied to extract PSP model parameters in order to accurately match the model generated response with measured characteristic. This paper presents the acceleration model for two major processes of parameter extraction: 1) Optimization algorithm (PSO), and 2) Model library (SiMKit) execution. First, the PSO algorithm is implemented as hardware on PL. Second, PSP MOSFET library SiMKit is implemented as a software on PS. Implementation results show 73.06% improvement in PSO execution time compared to ARM-based software implementation. Dual-core AMP configuration results into 28.54% better performance in SiMKit execution time compared to a single-core processor.}
}
@article{YU2004527,
title = {Ch OpenCV for interactive open architecture computer vision},
journal = {Advances in Engineering Software},
volume = {35},
number = {8},
pages = {527-536},
year = {2004},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2004.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0965997804000626},
author = {Qingcang Yu and Harry H Cheng and Wayne W Cheng and Xiaodong Zhou},
keywords = {C/C++ interpreter, Ch, Open CV, Computer vision, Image processing},
abstract = {In this paper, design and implementation of an interactive open architecture computer vision software package called Ch OpenCV is presented. Benefiting from both Ch and OpenCV, Ch OpenCV has many salient features. It is interactive, capable of interface with binary static or dynamical C/C++ libraries, integrated with advanced numerical features and embeddable. It is especially suitable for rapid prototyping, web-based applications, and teaching and learning about computer vision. Applications of Ch OpenCV including web-based image processing are illustrated with examples.}
}
@article{SAGHIR2023100455,
title = {An R package for percentile-based control charts: pbcc},
journal = {Software Impacts},
volume = {15},
pages = {100455},
year = {2023},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100455},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001397},
author = {Aamir Saghir and Zsolt T. Kosztyán},
keywords = {Statistical quality control, Control charts, Statistical design, Optimization},
abstract = {Although the percentile-based control charts are very effective with guaranteed in-control and out-of-control run lengths in statistical process control, they still lack algorithmic and (especially) software support. The proposed percentile-based control charts (pbcc) software tool fills this gap. This software is freely available and implements the recent work on individual as well as joint percentile-based control charts.}
}
@article{LIU2021103046,
title = {An application programming interface for multiscale shape-material modeling},
journal = {Advances in Engineering Software},
volume = {161},
pages = {103046},
year = {2021},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2021.103046},
url = {https://www.sciencedirect.com/science/article/pii/S0965997821000752},
author = {Xingchen Liu and Massimiliano Meneghin and Vadim Shapiro},
keywords = {Multiscale structure modeling, Multiscale queries, Architected materials, Homogenization},
abstract = {The application of multiscale structure in mechanical design become increasingly prevalent due to the rapid advancement of modern manufacturing technologies. Over the past few years, a multitude of methods, algorithms, and software have emerged to support the design and representation of such structures. An interoperable solution to connect and compose ad hoc approaches, therefore, could create the network effect to further the advancement of the field. In this paper, we proposed an Application Programming Interface (API) for multiscale structure modeling, which employs a query-based approach to provide interoperability between different shape and material property representations across different scales. The concept of neighborhood is used to link the adjacent scales such that the effective material property of the structure within a neighborhood on the fine-scale equals the material property on the coarse-scale. The use of intrinsic effective material properties enables the interchangeability between different families of fine-scale structures that support a common set of effective properties. The interoperability with downstream applications such as visualization and manufacturing planning is guaranteed through multiscale queries that extend but remains compatible with single scale queries. Such capability is demonstrated through a novel multiscale ray-marching algorithm. We discuss several usage scenarios and provide exemplary implementations of the core interfaces and functionalities of the proposed multiscale API.}
}
@article{BARTUSIAK2022105034,
title = {Open Process Automation: A standards-based, open, secure, interoperable process control architecture},
journal = {Control Engineering Practice},
volume = {121},
pages = {105034},
year = {2022},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2021.105034},
url = {https://www.sciencedirect.com/science/article/pii/S0967066121002902},
author = {R. Donald Bartusiak and Stephen Bitar and David L. DeBari and Bradley G. Houk and Dennis Stevens and Bridget Fitzpatrick and Patrick Sloan},
keywords = {Logical design, Physical design and implementation of embedded computer systems, Secure networked control systems, Systems interoperability, Internet-of-things and sensing enterprise},
abstract = {Open Process Automation is an industry standards initiative whose objective is to bring open architecture technology and business practices into the industrial control system industry. More than 100 organizations (operating companies, hardware and software suppliers, system integrators) are working in a managed Forum of The Open Group to define the Open Process Automation Standard (O-PASTM). Three versions of the O-PAS standard have been published. The paper provides an overview of the standard. The paper also describes results from two O-PAS prototyping projects by ExxonMobil and Lockheed Martin – a laboratory proof-of-concept and a hydrocarbon process-controlling prototype system. Finally, the paper relates Open Process Automation and two complementary initiatives by NAMUR, i.e., NAMUR Open Architecture (NOA) and Module Type Package (MTP).}
}
@article{WANG2021110100,
title = {Rapid generation of optimal generalized Monkhorst-Pack grids},
journal = {Computational Materials Science},
volume = {187},
pages = {110100},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2020.110100},
url = {https://www.sciencedirect.com/science/article/pii/S0927025620305917},
author = {Yunzhe Wang and Pandu Wisesa and Adarsh Balasubramanian and Shyam Dwaraknath and Tim Mueller},
keywords = {Brillouin zone, -points, Density functional theory, Crystalline materials, Symmetry-preserving superlattice},
abstract = {Computational modeling of the properties of crystalline materials has become an increasingly important aspect of materials research, consuming hundreds of millions of CPU-hours at scientific computing centers around the world each year, if not more. A routine operation in such calculations is the evaluation of integrals over the Brillouin zone. We have previously demonstrated that performing such integrals using generalized Monkhorst-Pack k-point grids can roughly double the speed of these calculations relative to the widely-used traditional Monkhorst-Pack grids. However the generation of optimal generalized Monkhorst-Pack grids is not implemented in most software packages due to the computational cost and difficulty of identifying the best grids. To address this problem, we present new algorithms that allow rapid generation of optimal generalized Monkhorst-Pack grids on the fly. We demonstrate that the grids generated by these algorithms are on average significantly more efficient than those generated using existing algorithms across a range of grid densities. For grids that correspond to a real-space supercell with at least 50 Å between lattice points, which is sufficient to converge density functional theory calculations within 1 meV/atom for nearly all materials, our algorithm finds optimized grids in an average of 0.19 s on a single processing core. To facilitate the widespread adoption of this approach, we present new open-source tools including a library designed for integration with third-party software packages.}
}
@article{GLIELMO2022100589,
title = {DADApy: Distance-based analysis of data-manifolds in Python},
journal = {Patterns},
volume = {3},
number = {10},
pages = {100589},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100589},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002070},
author = {Aldo Glielmo and Iuri Macocco and Diego Doimo and Matteo Carli and Claudio Zeni and Romina Wild and Maria d’Errico and Alex Rodriguez and Alessandro Laio},
keywords = {manifold analysis, intrinsic dimension, density estimation, density-based clustering, metric learning, feature selection},
abstract = {Summary
DADApy is a Python software package for analyzing and characterizing high-dimensional data manifolds. It provides methods for estimating the intrinsic dimension and the probability density, for performing density-based clustering, and for comparing different distance metrics. We review the main functionalities of the package and exemplify its usage in a synthetic dataset and in a real-world application. DADApy is freely available under the open-source Apache 2.0 license.}
}
@article{SERBAN2020908,
title = {Software reliability prediction using package level modularization metrics},
journal = {Procedia Computer Science},
volume = {176},
pages = {908-917},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319840},
author = {Camelia Şerban and Mohsin Shaikh},
keywords = {Software Package Metrics, Machine Learning, Reliability, Software Assessment},
abstract = {Reliability ensures architectural strength and error free operations of software systems. Software design and architectural measures have been studied as key indicators of faults in software systems. However, association between quantification of reliability prediction using design-level metrics has not been explored. This paper presents a novel approach of developing reliability metrics and it’s prediction using package level metrics. In particular, relevant fault severity information is empirically experimented with package level metrics in an effort-aware classification and ranking scenario. Results obtained hint significant view to predict the reliability of software systems using architectural level metrics. Therefore, the empirical analysis can guide development process to be design-focused and avoid accumulation of faults in implementation phase.}
}
@article{ASHRAF2023948,
title = {A Prototype of Supply Chain Traceability using Solana as blockchain and IoT},
journal = {Procedia Computer Science},
volume = {217},
pages = {948-959},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.292},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922023705},
author = {Mateen Ashraf and Cathal Heavey},
keywords = {Blockchain, Supply Chain, Traceability, IoT;Solana, Sigfox,LPWAN},
abstract = {The main objective of this article is to present a prototype that integrates blockchain and Internet-of-Things (IoT) devices to digitize information across a generic Supply Chain (SC). Blockchain bring many benefits to the operation of improving provenance and reliability, such as, digitizing the interaction among SC actors to allow implementation of smart contracts to support a secure and authenticated chain. In technical terms, blockchain is a decentralized database where information recording is secure and blockchain-based traceability in the SC can address shortcomings that exist in centralized solutions. Provenance could be achieved by implementing SC business logic within smart contracts using blockchains. This article presents an architecture of software that combines blockchain with IoT devices that allows traceability of a generic product from source to destination, through multi-echelon suppliers, logistics, manufacturers to the end-customer. The presented software architecture uses the Solana blockchain for the implementation of SC processes and business logic. This blockchain was selected from a review, reported in this article, of several widely used blockchain networks. It was selected mainly due to its speed and cost of transactions. Within the blockchain we store SC related data and events communicated over the internet and mobile application channels, in a Solana blockchain using Solana's native blockchain libraries. the IoT devices used are Sigfox cloud gateway, and Sensit that uses LPWAN (Low power wide area network) wireless telecommunication for data transfer. In our IoT device, the blockchain stores temperature, humidity, light, location, tilting, door opening, vibration, and magnetic field. The goal is to use existing technologies to develop a software architecture for a medium term objective of an implementable generic blockchain for SCs.}
}
@article{TIAN2020106289,
title = {BVDetector: A program slice-based binary code vulnerability intelligent detection system},
journal = {Information and Software Technology},
volume = {123},
pages = {106289},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106289},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300392},
author = {Junfeng Tian and Wenjing Xing and Zhen Li},
keywords = {Binary program, Vulnerability detection, Deep learning, Program slice, Library/API function call},
abstract = {Context
Software vulnerability detection is essential to ensure cybersecurity. Currently, most software is published in binary form, thus researchers can only detect vulnerabilities in these software by analysing binary programs. Although existing research approaches have made a substantial contribution to binary vulnerability detection, there are still many deficiencies, such as high false positive rate, detection with coarse granularity, and dependence on expert experience.
Objective
The goal of this study is to perform fine-grained intelligent detection on the vulnerabilities in binary programs. This leads us to propose a fine-grained representation of binary programs and introduce deep learning techniques to intelligently detect the vulnerabilities.
Method
We use program slices of library/API function calls to represent binary programs. Additionally, we design and construct a Binary Gated Recurrent Unit (BGRU) network model to intelligently learn vulnerability patterns and automatically detect vulnerabilities in binary programs.
Results
This approach yields the design and implementation of a program slice-based binary code vulnerability intelligent detection system called BVDetector. We show that BVDetector can effectively detect vulnerabilities related to library/API function calls in binary programs, which reduces the false positive rate and false negative rate of vulnerability detection.
Conclusion
This paper proposes a program slice-based binary code vulnerability intelligent detection system called BVDetector. The experimental results show that BVDetector can effectively reduce the false negative rate and false positive rate of binary vulnerability detection.}
}
@article{MENAKA2020103053,
title = {FPGA implementation of low power and high speed image edge detection algorithm},
journal = {Microprocessors and Microsystems},
volume = {75},
pages = {103053},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103053},
url = {https://www.sciencedirect.com/science/article/pii/S0141933119307495},
author = {Dr. R. Menaka and Dr. R. Janarthanan and Dr. K. Deeba},
keywords = {Sobel operator, Edges, Gradient, Threshold, Chip layout},
abstract = {Image processing is a vital task in data processing system for applications in medical fields, remote sensing, microscopic imaging etc., Algorithms for processing image exist except for real time system style, hardware implementation is most popular principally. This paper presents a design for Sobel filter based edge detection on Field Programmable Gate Array (FPGA) board. Hardware implementation of the Sobel edge detection algorithm is chosen because it presents an honest scope for similarity over software package. On the opposite hand, Sobel edge detection will work with less deterioration in high level of noise. Edges are primarily the noticeable variation of intensities in a picture. Edges facilitate to spot the placement of an object and also the boundary of a selected entity within the image. It conjointly helps in feature extraction and pattern recognition. Hence, edge detection is of nice importance in pc vision. The planned design for edge detection exploitation Sobel algorithm is designed using structural Verilog lipoprotein synthesized exploitation Cadence Genus and enforced using Cadence Innovus. The practicality of the planning is verified exploitation normal pictures by FPGA implementation. The proposed architecture reduce the power, delay and space complexity compare to three existing architectures.}
}
@incollection{BERNAL20221285,
title = {Advances in Generalized Disjunctive and Mixed- Integer Nonlinear Programming Algorithms and Software for Superstructure Optimization},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1285-1290},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50214-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596502141},
author = {David E. Bernal and Yunshan Liu and Michael L. Bynum and Carl D. Laird and John D. Siirola and Ignacio E. Grossmann},
keywords = {superstructure optimization, generalized disjunctive programming, MINLP},
abstract = {This manuscript presents the recent advances in Mixed-Integer Nonlinear Programming (MINLP) and Generalized Disjunctive Programming (GDP) with a particular scope for superstructure optimization within Process Systems Engineering (PSE). We present an environment of open-source software packages written in Python and based on the algebraic modeling language Pyomo. These packages include MindtPy, a solver for MINLP that implements decomposition algorithms for such problems, CORAMIN, a toolset for MINLP algorithms providing relaxation generators for nonlinear constraints, Pyomo.GDP, a modeling extension for Generalized Disjunctive Programming that allows users to represent their problem as a GDP natively, and GDPOpt, a collection of algorithms explicitly tailored for GDP problems. Combining these tools has allowed us to solve several problems relevant to PSE, which we have gathered in an easily installable and accessible library, GDPLib. We show two examples of these models and how the flexibility of modeling given by Pyomo.GDP allows for efficient solutions to these complex optimization problems. Finally, we show an example of integrating these tools with the framework IDAES PSE, leading to optimal process synthesis and conceptual design with advanced multi-scale PSE modeling systems.}
}
@article{ZUO2020102878,
title = {An API gateway design strategy optimized for persistence and coupling},
journal = {Advances in Engineering Software},
volume = {148},
pages = {102878},
year = {2020},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2020.102878},
url = {https://www.sciencedirect.com/science/article/pii/S0965997820304452},
author = {Xianyu Zuo and Yuehan Su and Qianqian Wang and Yi Xie},
keywords = {API gateway, Microservices, RPC, Coupling degree, Persistence},
abstract = {Microservices play a more and more important role in software development nowadays. Almost every programming language has its own microservices development framework. The characteristics of microservices make microservices have cross-platform compatibility issues and inconsistent call standards issues in the process of development and call microservices. The birth of API Gateway alleviates these problems to some extent. For small and medium-sized enterprises using today's popular API Gateways, it is difficult for them to get a balance between cost, performance and maintainability. This paper proposes a scheme to optimize the API Gateway. Firstly, the framework of API Gateway is optimized. Next, the coupling degree of API Gateway is optimized by reducing the coupling degree of core services and extended functions. In this way, the optimized Gateway can adapt to the plug-in mode, improve the user experience and reduce development costs. Then, the persistent design of the configuration information of the API Gateway is carried out, and the read-write optimization is carried out so that the optimized API Gateway not only has advantages in the configuration persistence, but also has further improved the I/O performance. Based on the optimized design, this paper implements a cross platform compatible API Gateway. Then it compares the performance of two popular API Gateway schemes through performance testing. The test results show that the optimized design of API Gateway achieves a new balance among cost, performance and maintainability. At the end of this paper, the work of this paper is summarized, and the next work is projected.}
}
@article{HUO2021459,
title = {juSFEM: A Julia-based open-source package of parallel Smoothed Finite Element Method (S-FEM) for elastic problems},
journal = {Computers & Mathematics with Applications},
volume = {81},
pages = {459-477},
year = {2021},
note = {Development and Application of Open-source Software for Problems with Numerical PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0898122120300523},
author = {Zenan Huo and Gang Mei and Nengxiong Xu},
keywords = {Smoothed Finite Element Method (S-FEM), Parallel algorithm, Julia language, Computational efficiency, Computational accuracy},
abstract = {The Smoothed Finite Element Method (S-FEM) proposed by Liu G.R. can achieve more accurate results than the conventional FEM. Currently, much commercial software and many open-source packages have been developed to analyze various science and engineering problems using the FEM. However, there is little work focusing on designing and developing software or packages for the S-FEM. In this paper, we design and implement an open-source package of the parallel S-FEM for elastic problems by utilizing the Julia language on multi-core CPU. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing. We term our package as juSFEM. To the best of the authors’ knowledge, juSFEM is the first package of parallel S-FEM developed with the Julia language. To verify the correctness and evaluate the efficiency of juSFEM, two groups of benchmark tests are conducted. The benchmark results show that (1) juSFEM can achieve accurate results when compared to commercial FEM software ABAQUS, and (2) juSFEM only requires 543 s to calculate the displacements of a 3D elastic cantilever beam model which is composed of approximately 2 million tetrahedral elements, while in contrast the commercial FEM software needs 930 s for the same calculation model; (3) the parallel juSFEM executed on the 24-core CPU is approximately 20× faster than the corresponding serial version. Moreover, the structure and function of juSFEM are easily modularized, and the code in juSFEM is clear and readable, which is convenient for further development.}
}
@article{OVEREEM2022106890,
title = {API-m-FAMM: A focus area maturity model for API Management},
journal = {Information and Software Technology},
volume = {147},
pages = {106890},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106890},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000532},
author = {Michiel Overeem and Max Mathijssen and Slinger Jansen},
keywords = {API Management, Maturity model, Focus area maturity models},
abstract = {Context:
Organizations are increasingly connecting software applications using Application Programming Interfaces (APIs) to share data, services, functionality, and even complete business processes. However, the creation and management of APIs is non-trivial. Aspects such as traffic management, community engagement, documentation, and version management are often rushed afterthoughts.
Objective:
In this research, we present and evaluate a focus area maturity model for API Management (API-m-FAMM). A focus area maturity model can be used to establish the maturity level of an organization in a specific functional domain described through a number of areas. The API-m-FAMM addresses the areas Lifecycle Management, Security, Performance, Observability, Community, and Commercial.
Method:
The model is constructed using established methods for the design of a focus area maturity model. It is grounded in literature and practice, and was developed and evaluated through a systematic literature Review, eleven expert interviews, and five case studies at software producing organizations.
Result:
The model is described in detail, and its application is illustrated by six case studies.
Conclusions:
The evaluations are reported on, and show that the API-m-FAMM is an efficient tool for aiding organizations in gaining a better understanding of their current implementation of API management practices, and provides them with guidance towards higher levels of maturity. The detailed description of the construction of the API-m-FAMM gives researchers an example to further support the available methodologies, specifically how to combine design science research with these methodologies. Additionally, this study’s unique case study design shows that maturity models can be successfully deployed in practice with minimal involvement of researchers. The focus area maturity model for API Management is maintained on www.maturitymodels.org, allowing practitioners to benefit from its useful insights.}
}
@article{GE2020103293,
title = {Development of a CPU/GPU portable software library for Lagrangian–Eulerian simulations of liquid sprays},
journal = {International Journal of Multiphase Flow},
volume = {128},
pages = {103293},
year = {2020},
issn = {0301-9322},
doi = {https://doi.org/10.1016/j.ijmultiphaseflow.2020.103293},
url = {https://www.sciencedirect.com/science/article/pii/S030193221930953X},
author = {Wenjun Ge and Ramanan Sankaran and Jacqueline H. Chen},
keywords = {Multiphase flow, High performance computing, GPU, Direct numerical simulation},
abstract = {The Lagrangian–Eulerian method is widely used for simulations of fuel sprays in turbulent combustion because of the advantage of treating the spray droplets as discrete points. One challenge of the Lagrangian–Eulerian method is the intense computational requirement when tracking the large number of Lagrangian particles needed for high fidelity. We have developed a performance-portable library, Grit, to track the Lagrangian particles in parallel on central processing unit (CPU) and graphics processing unit (GPU) accelerated high performance computing (HPC) architectures. Grit is a C++ library which employs Message Passing Interface (MPI) for distributed memory parallelism and Kokkos programming model for on-node shared memory parallelism with performance portability across different architectures of GPUs and multi-core/manycore CPUs. The parallel algorithms, key parallel kernels, and their performances on the pre-exascale supercomputer, Summit, are presented. Grit is coupled with a direct numerical simulation (DNS) solver, S3D, for multiphase simulations. A conservative formulation has been developed and implemented in Grit for phase coupling with thermodynamic consistency. The formulation separates the conservation of mass, momentum and energy from the physical models to prevent accidental violation of conservation laws due to inconsistent models. The formulation also enforces consistent definitions of enthalpies of the fuel for both phases and the latent heat of evaporation. Simulations of turbulent particle-laden flow and the evaporation of dilute turbulent spray jet are performed to verify the software implementation, and to demonstrate the scalability of Grit for large-scale multiphase simulations.}
}
@article{LI2022102872,
title = {DMalNet: Dynamic malware analysis based on API feature engineering and graph learning},
journal = {Computers & Security},
volume = {122},
pages = {102872},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102872},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002668},
author = {Ce Li and Zijun Cheng and He Zhu and Leiqi Wang and Qiujian Lv and Yan Wang and Ning Li and Degang Sun},
keywords = {Malware detection, Malware type classification, API Feature engineering, API Call graph, Deep learning, Graph neural network},
abstract = {Application Programming Interfaces (APIs) are widely considered a useful data source for dynamic malware analysis to understand the behavioral characteristics of malware. However, the accuracy of API-based malware analysis is limited for two reasons. (1) Existing solutions often only consider the API names while ignoring the API arguments, or cannot fully exploit the semantic information from different types of arguments. (2) The relationship between API calls is important to describe the software behavior but is difficult to capture. To overcome the above limitations, we propose DMalNet, a novel malware analysis framework for accurate malware detection and classification. Specifically, we first present a hybrid feature encoder to extract semantic features from API names and arguments. Then, we derive an API call graph from the API call sequence to convert the relationship between API calls into the structural information of the graph. Finally, we design a graph neural network to implement the malware detection and type classification. To evaluate our approach, we use datasets of over 20k benign and 18k malicious samples belonging to 8 malware types. DMalNet achieves 98.43% and 91.42% accuracy on malware detection and malware type classification, respectively. We also conduct ablation studies to assess the impact of API feature engineering and the graph learning model. Further experiments show that DMalNet can effectively detect malware.}
}
@article{GONG2023,
title = {Diversified and compatible web APIs recommendation based on game theory in IoT},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352864823000378},
author = {Wenwen Gong and Huiping Wu and Xiaokang Wang and Xuyun Zhang and Yawei Wang and Yifei Chen and Mohammad R. Khosravi},
keywords = {Internet of things, Web APIs recommendation, Game theory, Diversity and compatibility},
abstract = {With the ever-increasing popularity of Internet of Things (IoT), massive enterprises are attempting to encapsulate their developed outcomes into various lightweight Web Application Programming Interfaces (APIs) that can be accessible remotely. In this context, finding and writing a list of existing Web APIs that can collectively meet the functional needs of software developers has become a promising approach to economically and easily develop successful mobile applications. However, the number and diversity of candidate IoT Web APIs places an additional burden on application developers’ Web API selection decisions, as it is often a challenging task to simultaneously ensure the diversity and compatibility of the final set of Web APIs selected. Considering this challenge and latest successful applications of game theory in IoT, a Diversified and Compatible web APIs Recommendation approach, namely DivCAR, is put forward in this paper. First of all, to achieve API diversity, DivCAR employs random walk sampling technique on a pre-built “API-API” correlation graph to generate diverse “API-API” correlation subgraphs. Afterwards, with the diverse “API-API” correlation subgraphs, the compatible Web APIs recommendation problem is modeled as a minimum group Steiner tree search problem. A sorted set of multiple compatible and diverse Web APIs are returned to the application developer by solving the minimum group Steiner tree search problem. At last, a set of experiments are designed and implemented on a real dataset crawled from www.programmableweb.com. Experimental results validate the effectiveness and efficiency of our proposed DivCAR approach in balancing the Web APIs recommendation diversity and compatibility.}
}
@article{ARSENE20119011,
title = {Design patterns multiagents driven for software development},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {9011-9016},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.02864},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016450585},
author = {O. Arsene and I. Dumitrache},
keywords = {Software architecture, MultiAgent Systems, Software patterns},
abstract = {Abstract
The complexity of nowadays systems imposes development of a dynamic architecture based on modularization, dependency, processing and data access principles. The field of software design and development is rich in patterns. Software patterns can be seen as a library of code modules that is reused during development of programs. The paper propose a software pattern multiagent systems driven. There will be taken four main patterns: singleton, transfer object, abstract factory and data access object and will be implemented as software agents. Patterns have always participants who are related through the pattern structure. This leads to the observation that different patterns can be created by varying type and features of the participants. An entire family of patterns can be built from one pattern; thus, there will be many smaller building blocks, cooperating and computing toward one goal. This is the trigger for using software agents as blocks. Each project functionality will be decomposed into many tasks, which will be completed by software agents. Each task will be solved by software agents acting as software pattern blocks. The agents (software pattern implementation) will be automatically generated based on the specificity of the task. The implementation is done using JADE, a mature software agent technology platform. The paper is focused on the automation of the task – software pattern(s) mapping and implementation.}
}
@article{JANSEN2023209,
title = {The vendor-agnostic EMPAIA platform for integrating AI applications into digital pathology infrastructures},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {209-224},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003405},
author = {Christoph Jansen and Björn Lindequist and Klaus Strohmenger and Daniel Romberg and Tobias Küster and Nick Weiss and Michael Franz and Lars Ole Schwen and Theodore Evans and André Homeyer and Norman Zerbe},
keywords = {Computational pathology, Artificial intelligence, Applications, API, Interoperability, Digital platform},
abstract = {Automated image analysis and artificial intelligence (AI) are becoming increasingly common in digital pathology software. While various proprietary pathology systems exist, there are no fully vendor-agnostic integration approaches for AI apps. This makes it difficult for vendors of AI solutions to integrate their products into the multitude of non-standard software systems in pathology. The EMPAIA Consortium is developing an open and decentralized platform allowing AI-based apps of different vendors to be integrated with existing clinical IT infrastructures. For this purpose, we defined, analyzed, and prioritized relevant use cases and identified requirements for an open platform to support these use cases. We then designed the platform architecture described here to meet these requirements based on web technologies. For all platform services open source reference implementations are available, that are used by developers of AI apps as an integration target. Developers of compatible clinical systems can either use and integrate components of the reference implementation or directly implement the interfaces as per specification, allowing apps to run in their clinical environment. Pathology laboratories can use both on-premises and cloud deployments of the platform. Apps can be obtained via a central marketplace so that pathologists can use them in their daily workflow. An adoption of this platform will enable interoperability among different existing digital pathology software systems. This reduces integration efforts for software vendors, while users will benefit from a wider variety of tools and a quicker availability of new and innovative methods. Ultimately, the platform will reduce barriers to market entry for AI vendors and provide pathologists with access to advanced AI tools.}
}
@article{DELGADO20198,
title = {Real-time control architecture based on Xenomai using ROS packages for a service robot},
journal = {Journal of Systems and Software},
volume = {151},
pages = {8-19},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.052},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300160},
author = {Raimarius Delgado and Bum-Jae You and Byoung Wook Choi},
keywords = {Real-time control architecture, Robot operating system, Xenomai, Cross-domain datagram protocol, Service mobile robots},
abstract = {This paper proposes a real-time (RT) control architecture based on Xenomai, an RT embedded Linux, to control a service robot along with non-real-time (NRT) robot operating system (ROS) packages. Most software, including device drivers and ROS, are developed to operate under the standard Linux kernel that does not provide RT guarantees. Standard Linux system calls in an RT context stimulates mode switching, resulting in non-deterministic responses and stability problems such as priority inversion and kernel panic. This paper overcomes such issues through a communication interface between RT and NRT tasks, termed cross-domain datagram protocol. The proposed architecture supports priority-based scheduling of multiple tasks while exposing an interface compatible with the original ROS packages. Moreover, it enables standard device driver operation inside RT tasks without developing RT device drivers that requires significant amount of development time. Feasibility is proven by implementation on a Raspberry Pi 3, a low-cost open embedded hardware platform, and conducted various experiments to analyze its performance and applied it to a service robot using ROS navigation packages. The results indicate that the proposed architecture can effectively provide an RT environment without stability issues when utilizing ROS packages and standard device drivers.}
}
@article{ORTNERPICHLER2022101492,
title = {Integration of parametric modelling in web-based knowledge-based engineering applications},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101492},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101492},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100241X},
author = {Alexander Ortner-Pichler and Christian Landschützer},
keywords = {Knowledge-based engineering, KBE, Parametric modelling, Intralogistics, System planning, CAD},
abstract = {Knowledge-based Engineering (KBE) is a methodology to efficiently handle complex and iterative processes as well as simple recurring tasks in CAD-based product design. Even though Knowledge-based Engineering’s roots go back as far as the late 20th century, potential of attractive research is still given. As geometry handling in native full Knowledge-based Engineering approaches can still be a challenging and time-consuming task, approaches for utilizing the parametric modeling capabilities of high-level CAD packages were developed. With the rise of web-based technologies, bringing these utilizing approaches to the Web, seems to be the next step in the technological development of Knowledge-based Engineering applications. Even though KBE has found its way to the Web already in many different sectors, planning of large systems based on existing machinery and subsystems, is still at least one open field left. This article connects to a newly developed KBE methodology, tailored for the use within intralogistic system planning, but widely usable in other systems and proposes a software architecture to utilize CAD’s capabilities regarding geometry creation and manipulation by the parametric modelling method in a web-based environment. The architecture is especially designed for implementations of the Knowledge-based Engineering method described within the research context in the field of intralogistics system planning.}
}
@article{MANDUCHI2010564,
title = {A new object-oriented interface to MDSplus},
journal = {Fusion Engineering and Design},
volume = {85},
number = {3},
pages = {564-567},
year = {2010},
note = {Proceedings of the 7th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2010.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S092037961000027X},
author = {G. Manduchi and T. Fredian and J. Stillerman},
keywords = {Data acquisition, Object oriented languages, Data systems, UML},
abstract = {The MDSplus data acquisition and management software package is widely used in the international fusion research community. Its core Application Programming Interface (API) remained unchanged since the system was ported to a multiplatform environment in the late nineties. Originally written in C, the MDSplus API did not fully exploit several object-oriented features of the system that were included in the original architecture. In 2008 a project was initiated by the authors to provide the core MDSplus functionality with an object-oriented API. A generic, language-independent class structure has been defined and modeled in Uniform Modeling Language (UML). Based on this description the new API has been implemented so far in C++, Python, and Java. The new API provides data type management, allowing the full exploitation of the rich set of data types defined in MDSplus by means of composition of data object instances, and pulse file access, for writing and reading data objects as well as managing database components properties. The definition of a language-independent class organization allows the MDSplus object-oriented API to be consistent across all the object oriented languages that will be supported. Regardless of the language used, this approach provides a much more natural programming interaction with MDSplus.}
}
@article{SHAKIBA2022100445,
title = {Libra: A modular software library for quantum nonadiabatic dynamics},
journal = {Software Impacts},
volume = {14},
pages = {100445},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100445},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822001294},
author = {Mohammad Shakiba and Brendan Smith and Wei Li and Matthew Dutra and Amber Jain and Xiang Sun and Sophya Garashchuk and Alexey Akimov},
keywords = {Nonadiabatic dynamics, Quantum dynamics, Molecular dynamics, Trajectory surface hopping, Excited states, Solar energy materials},
abstract = {Libra is a versatile open-source software that implements a multitude of community-developed methods and computational workflows for nonadiabatic and quantum dynamics calculations, such as trajectory surface hopping or Ehrenfest dynamics, discrete variable representation or trajectory-guided wavepacket propagation schemes, and more. Through interfaces with electronic structure codes for excited state calculations, Libra enables modeling quantum nonadiabatic processes in extended atomistic systems. Libra can be used as a framework for systematic assessment of various nonadiabatic dynamics methods, a methodology prototyping library, and a fully-fledged suite for applied materials research. In this paper, we briefly overview the Libra package and its impact.}
}
@article{MA202030,
title = {Design and implementation of SVM OTPC searching based on Shared Dot Product Matrix},
journal = {Integration},
volume = {71},
pages = {30-37},
year = {2020},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167926019300288},
author = {Shang Ma and Wei Cao and Shengqiang Jiang and Jianhao Hu and Xin Lei and Xiongzhong Xiong},
keywords = {Support vector machine (SVM), Optimal training parameter combination (OTPC), Sharing dot product matrix, Hardware and software co-design, Field programmable gate array (FPGA)},
abstract = {In this paper, we proposed a FPGA implementation architecture for SVM classifier. The architecture is based on the proposed Shared Dot Product Matrix (SDPM) method which computes and stores the dot product of all training data before SVM searching process. We implemented the proposed method by software simulation and hardware implementation. The software simulation of SDPM method achieves twice the speed of LIBSVM, which is one of the most popular SVM implementation libraries. This acceleration mainly results from the reduction of repeat Kernel function calculation. Then the hardware software collaboration architecture for SDPM is also proposed in this paper. Results show that the proposed architecture achieves approximately 30 times faster searching speed compared with LIBSVM.}
}
@article{SHUKLA201714386,
title = {Software and Hardware Code Generation for Predictive Control Using Splitting Methods**This work has received funding from the People Programme (Marie Curie Actions) of the European Union’s Seventh Framework Programme (FP7/2007-2013) under REA grant agreement no 607957 (TEMPO).},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14386-14391},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2025},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317326642},
author = {Harsh A. Shukla and Bulat Khusainov and Eric C. Kerrigan and Colin N. Jones},
keywords = {Model predictive, optimization-based control, Hardware-software co-design, Embedded computer architectures, Hardware-in-the-loop simulation, Splitting methods},
abstract = {This paper presents SPLIT, a C code generation tool for Model Predictive Control (MPC) based on operator splitting methods. In contrast to existing code generation packages, SPLIT is capable of generating both software and hardware-oriented C code to allow quick prototyping of optimization algorithms on conventional CPUs and field-programmable gate arrays (FPGAs). A Matlab interface is provided for compatibility with existing commercial and open-source software packages. A numerical study compares software, hardware and heterogeneous implementations of splitting methods and investigates MPC design trade-offs. For the considered testcases the reported speedup of hardware implementations over software realizations is 3x to 11x.}
}
@article{WEIN200737,
title = {Advanced programming techniques applied to Cgal's arrangement package},
journal = {Computational Geometry},
volume = {38},
number = {1},
pages = {37-63},
year = {2007},
note = {Special Issue on CGAL},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2006.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925772107000077},
author = {Ron Wein and Efi Fogel and Baruch Zukerman and Dan Halperin},
keywords = {, Arrangements, Generic programming, Design patterns, Exact computation, Robustness},
abstract = {Arrangements of planar curves are fundamental structures in computational geometry. Recently, the arrangement package of Cgal, the Computational Geometry Algorithms Library, has been redesigned and re-implemented exploiting several advanced programming techniques. The resulting software package, which constructs and maintains planar arrangements, is easier to use, to extend, and to adapt to a variety of applications. It is more efficient space- and time-wise, and more robust. The implementation is complete in the sense that it handles degenerate input, and it produces exact results. In this paper we describe how various programming techniques were used to accomplish specific tasks within the context of computational geometry in general and arrangements in particular. These tasks are exemplified by several applications, whose robust implementation is based on the arrangement package. Together with a set of benchmarks they assured the successful application of the various programming techniques.}
}
@article{ABATE2013459,
title = {A modular package manager architecture},
journal = {Information and Software Technology},
volume = {55},
number = {2},
pages = {459-474},
year = {2013},
note = {Special Section: Component-Based Software Engineering (CBSE), 2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001851},
author = {Pietro Abate and Roberto {Di Cosmo} and Ralf Treinen and Stefano Zacchiroli},
keywords = {Software dependencies, Software repositories, Software components, Package manager, Open source},
abstract = {Context
The success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics.
Objective
We aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the user’s preferences.
Method
In this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of dependency solvers and backends.
Results
We have built a working prototype–called MPM–following the design advocated in this paper, and we show how it largely outperforms a variety of current package managers.
Conclusion
We argue that a modular architecture, allowing for delegating the task of constraint solving to external solvers, is the path that leads to the next generation of package managers that will deliver better results, offer more expressive preference languages, and be easily adaptable to new platforms.}
}
@article{WASEEM2021111061,
title = {Design, monitoring, and testing of microservices systems: The practitioners’ perspective},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111061},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111061},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001588},
author = {Muhammad Waseem and Peng Liang and Mojtaba Shahin and Amleto {Di Salle} and Gastón Márquez},
keywords = {Microservices architecture, Design, Monitoring, Testing, Industrial survey},
abstract = {Context:
Microservices Architecture (MSA) has received significant attention in the software industry. However, little empirical evidence exists on design, monitoring, and testing of microservices systems.
Objective:
This research aims to gain a deep understanding of how microservices systems are designed, monitored, and tested in the industry.
Methods:
A mixed-methods study was conducted with 106 survey responses and 6 interviews from microservices practitioners.
Results:
The main findings are: (1) a combination of domain-driven design and business capability is the most used strategy to decompose an application into microservices, (2) over half of the participants used architecture evaluation and architecture implementation when designing microservices systems, (3) API gateway and Backend for frontend patterns are the most used MSA patterns, (4) resource usage and load balancing as monitoring metrics, log management and exception tracking as monitoring practices are widely used, (5) unit and end-to-end testing are the most used testing strategies, and (6) the complexity of microservices systems poses challenges for their design, monitoring, and testing, for which there are no dedicated solutions.
Conclusions:
Our findings reveal that more research is needed to (1) deal with microservices complexity at the design level, (2) handle security in microservices systems, and (3) address the monitoring and testing challenges through dedicated solutions.}
}
@article{DARMADI2018120,
title = {Hypermedia Driven Application Programming Interface for Learning Object Management},
journal = {Procedia Computer Science},
volume = {135},
pages = {120-127},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.157},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314455},
author = {Herru Darmadi and Suryadiputra Liawatimena and Bahtiar Saleh Abbas and Agung Trisetyarso},
keywords = {hypermedia API, HTML5, learning object, content management},
abstract = {The research purpose is to create a hypermedia driven API for learning object management. The research focuses on conception, design, implementation and evaluation of the learning object management hypermedia API. The hypermedia API was designed based on learning object models, hypermedia concepts, and related studies. Learning object model can be represented and manipulated as a hypermedia resource. It contains page resources with text and links to multimedia elements. The hypermedia API was designed using UML and represented in HTML5 because of its rich affordance. The implementation and evaluation phase were conducted by testing the API using three different client’s software to prove the concept of hypermedia as the engine of application state for managing learning objects. The results show that the hypermedia API for learning object management is accessible, consistent, and discoverable by heterogonous client applications.}
}
@article{DEMARCO2020105667,
title = {Automatic modeling of aircraft external geometries for preliminary design workflows},
journal = {Aerospace Science and Technology},
volume = {98},
pages = {105667},
year = {2020},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2019.105667},
url = {https://www.sciencedirect.com/science/article/pii/S1270963819316943},
author = {Agostino {De Marco} and Mario {Di Stasio} and Pierluigi {Della Vecchia} and Vittorio Trifari and Fabrizio Nicolosi},
keywords = {Geometric modeling, Aircraft design, Design automation, Aerodynamic optimization},
abstract = {This article introduces a high-fidelity geometry definition methodology enabling Multidisciplinary Design, Analysis and Optimization (MDAO) of aircraft configurations. All definitions and functional features have been implemented within the JPAD software, a Java-based computing library for aircraft designers, which provides a dedicated geometric modeling module called JPADCAD. The geometric module, that comes as an application programming interface (API) built on top of the OpenCASCADE Technology solid modeling kernel, is conceived for the automatic production of parametric aircraft CAD geometries. The tool allows the definition of input geometries for low-fidelity as well as high-fidelity aerodynamic analyses, hence proves to be a key factor in the entire MDAO process, particularly in conceptual or preliminary design analysis workflows. The main goal of such a geometric library remains ease of use and support for automation to minimize unnecessary or repetitive human effort. The backbone of the presented methodology is the parametric definition of a generic commercial transport aircraft configuration that translates into software data structures and functionalities of CAD surface modelers. These aspects are discussed in the first part of the article. The second part presents a use case example of the geometric modeling API, where an automated aerodynamic analysis workflow is used to construct a prediction model for canard-wing configurations.}
}
@article{AKHUKOV2023111832,
title = {MULTICOMP package for multilevel simulation of polymer nanocomposites},
journal = {Computational Materials Science},
volume = {216},
pages = {111832},
year = {2023},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2022.111832},
url = {https://www.sciencedirect.com/science/article/pii/S0927025622005432},
author = {Mikhail A. Akhukov and Vassily A. Chorkov and Alexey A. Gavrilov and Daria V. Guseva and Pavel G. Khalatur and Alexei R. Khokhlov and Andrey A. Kniznik and Pavel V. Komarov and Mike V. Okun and Boris V. Potapkin and Vladimir {Yu. Rudyak} and Denis B. Shirabaykin and Anton S. Skomorokhov and Sergey V. Trepalin},
keywords = {Multiscale simulations, Nanotechnology, Software package, Computer design, Polymer nanocomposites},
abstract = {We present a MULTICOMP package developed for multiscale modeling of polymer-based nanomaterials. The package implements GUI-based tools for the automatic construction of various types of polymer systems with the transition from the atomistic level of modeling to the mesolevel and from the mesolevel to the macrolevel. Using automated scripts, researchers can construct different kinds of nanomaterials, create flexible simulation schemas and define how data are transferred between engaged software modules and tools. The package makes it possible to analyze structural, thermophysical, mechanical properties, the cohesive energy density, the viscosity of polymer melts, and the diffusion of small molecules. Due to the client–server organization, the package can perform calculations on local and remote computing facilities and transfer the most time-consuming calculations to supercomputers. Examples of preparation and characterization of a cross-linked polymer matrix, a clay-based nanocomposite, and analyzing their properties illustrate MULTICOMP package operation.}
}
@article{ALVES20175831,
title = {UltraDES - A Library for Modeling, Analysis and Control of Discrete Event Systems**This work was supported by Capes - Brazil, CNPq and FAPEMIG.},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {5831-5836},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.540},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317309084},
author = {Lucas V.R. Alves and Lucas R.R. Martins and Patrícia N. Pena},
keywords = {Discrete Event Systems, Supervisory Control Theory, Software Package},
abstract = {In this paper a library of functions and data structures for analysis and control of Discrete Event Systems based in the. NET Framework is proposed. The main objective is to create an environment for the implementation of algorithms for Discrete Event Systems, as well as the integration of these algorithms and codes in the fields of IT (Information Technology) and AT (Automation Technology). The data structure, and the functions implemented so far are presented. The performance of the current version of the library is evaluated.}
}
@article{KAMALOV2023100485,
title = {XyGen: Synthetic data generator for feature selection},
journal = {Software Impacts},
volume = {15},
pages = {100485},
year = {2023},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2023.100485},
url = {https://www.sciencedirect.com/science/article/pii/S2665963823000222},
author = {Firuz Kamalov and Said Elnaffar and Hana Sulieman and Aswani Kumar Cherukuri},
keywords = {Feature selection, Synthetic data, Machine learning, Data mining},
abstract = {Given the large number of feature selection algorithms, it has become imperative to have a uniform procedure for evaluating the performance of the algorithms. We propose a library of synthetic datasets designed specifically to test the effectiveness of feature selection algorithms. The datasets are inspired by applications in the field of electronics and have a range of characteristics to provide a variety of test scenarios. The software comes in the form of a Python library with standard interface for loading and generating datasets. Each dataset is implemented as a function that allows control of various parameters of the data.}
}
@incollection{NEUSTEIN20221,
title = {Chapter 1 - Introduction},
editor = {Amy Neustein and Nathaniel Christen},
booktitle = {Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care},
publisher = {Academic Press},
pages = {1-16},
year = {2022},
isbn = {978-0-323-85197-8},
doi = {https://doi.org/10.1016/B978-0-32-385197-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851978000052},
author = {Amy Neustein and Nathaniel Christen},
keywords = {conceptual spaces, data set, hypergraph, digital health ecosystem, code libraries, CDC, data curation},
abstract = {In the first chapter we present the scaffolding of the book, introducing novel approaches to data modeling, such as type theory, conceptual spaces, or graph database architectures. We posit that any database, data set, or information space should be engineered with the expectation that multiple (not fully isomorphic) software components will be interacting with that data; and that parts therein will be passed and shared between such components, implying that data should be structured to facilitate cross-component communication. We propose that most of the theoretical constructions summarized here vis-à-vis hypergraph or code models may be concretely instantiated through virtual machines, via which query evaluation engines may be implemented. We use the architecture of virtual machines within this category as an organizing motif for analyses. From a more practical or “applied” point of view, we will call attention in particular to biomedical research projects that synthesize information with variegated disciplinary provenance and diverse data profiles. While biomedical research is inherently interdisciplinary, new breakthroughs and new research methods and technologies have further accelerated the cross-disciplinary insights of research in several specific biomedical disciplines, yielding diagnostic, prognostic, and explanatory models that cut across biophysical scales (molecular, cellular, tissues, organs) and data acquisition modalities (proteomics, genomics, biopsies, image processing, lab assays—such as for biologic sample analysis—and so forth). In examining the literature where these integrative studies are described, it becomes clear that scientists often construct the software ecosystem powering their research in ad-hoc ways, piecing together diverse software components (sometimes standalone applications, sometimes code libraries, or some combination thereof) designed for specific disciplinary contexts. We argue in this book that the relatively informal trial-and-error approach to integrating multidisciplinary biomedical data can act as an impediment to research replication and the systematic evaluation of interdisciplinary research findings. This warrants engaging in a detailed review of data profiles, data modeling paradigms, and data integration techniques, so as to lay the foundation for a software ecosystem that can support the emerging paradigm of transparent and replicable research data and digital scientific resources, such as code libraries and publications.}
}
@article{PINTER201919,
title = {Polymorph segmentation representation for medical image computing},
journal = {Computer Methods and Programs in Biomedicine},
volume = {171},
pages = {19-26},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718313038},
author = {Csaba Pinter and Andras Lasso and Gabor Fichtinger},
keywords = {Segmentation, Software library, Open-source, 3D Slicer, Voxelization, DICOM},
abstract = {Background and objective
Segmentation is a ubiquitous operation in medical image computing. Various data representations can describe segmentation results, such as labelmap volumes or surface models. Conversions between them are often required, which typically include complex data processing steps. We identified four challenges related to managing multiple representations: conversion method selection, data provenance, data consistency, and coherence of in-memory objects.
Methods
A complex data container preserves identity and provenance of the contained representations and ensures data coherence. Conversions are executed automatically on-demand. A graph containing the implemented conversion algorithms determines each execution, ensuring consistency between various representations. The design and implementation of a software library are proposed, in order to provide a readily usable software tool to manage segmentation data in multiple data representations. A low-level core library called PolySeg implemented in the Visualization Toolkit (VTK) manages the data objects and conversions. It is used by a high-level application layer, which has been implemented in the medical image visualization and analysis platform 3D Slicer. The application layer provides advanced visualization, transformation, interoperability, and other functions.
Results
The core conversion algorithms comprising the graph were validated. Several applications were implemented based on the library, demonstrating advantages in terms of usability and ease of software development in each case. The Segment Editor application provides fast, comprehensive, and easy-to-use manual and semi-automatic segmentation workflows. Clinical applications for gel dosimetry, external beam planning, and MRI-ultrasound image fusion in brachytherapy were rapidly prototyped resulting robust applications that are already in use in clinical research. The conversion algorithms were found to be accurate and reliable using these applications.
Conclusions
A generic software library has been designed and developed for automatic management of multiple data formats in segmentation tasks. It enhances both user and developer experience, enabling fast and convenient manual workflows and quicker and more robust software prototyping. The software’s BSD-style open-source license allows complete freedom of use of the library.}
}
@article{MULLER2022790,
title = {The operating system of the neuromorphic BrainScaleS-1 system},
journal = {Neurocomputing},
volume = {501},
pages = {790-810},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.05.081},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222006646},
author = {Eric Müller and Sebastian Schmitt and Christian Mauch and Sebastian Billaudelle and Andreas Grübl and Maurice Güttler and Dan Husmann and Joscha Ilmberger and Sebastian Jeltsch and Jakob Kaiser and Johann Klähn and Mitja Kleider and Christoph Koke and José Montes and Paul Müller and Johannes Partzsch and Felix Passenberg and Hartmut Schmidt and Bernhard Vogginger and Jonas Weidner and Christian Mayr and Johannes Schemmel},
keywords = {Neuromorphic computing, Neuroscientific modeling, Hardware abstraction},
abstract = {BrainScaleS-1 is a wafer-scale mixed-signal accelerated neuromorphic system targeted for research in the fields of computational neuroscience and beyond-von-Neumann computing. Here we present the BrainScaleS Operating System (BrainScaleS OS): the software stack gives users the possibility to emulate networks described in the high-level network description language PyNN with minimal knowledge of the system, as well as expert usage facilitated by allowing access to the system at any depth of the stack. BrainScaleS OS has been used extensively in the commissioning and calibration of BrainScaleS-1 as well as in various neuromorphic experiments, e.g., rate-based deep learning, accelerated physical emulation of Bayesian inference, solving of SAT problems, and others. The tolerance to faults of individual components of the neuromorphic system is reflected in the mapping process based on information stored in an availability database. We evaluate the robustness and compensation mechanisms of the system and software stack. The software stack is designed with performance in mind, with its core implemented in C++ and most user-facing API wrapped automatically to Python. The implemented multi-FPGA orchestration allows for parallel configuration and synchronized experiments facilitating wafer-scale experiments. The initial configuration of a wafer-scale experiment with hundreds of neuromorphic ASICs is performed in a fraction of a minute. Subsequent experiments, that potentially change only a subset of parameters, can be executed with rates of typically 10Hz. The bandwidth from the host machine to the neuromorphic system is fully utilized starting from a quarter of the system’s FPGA count. Operation and development methodologies implemented for the BrainScaleS-1 neuromorphic architecture are presented and the individual components of BrainScaleS OS constituting the software stack for BrainScaleS-1 platform operation are detailed.}
}
@article{ALI2015167,
title = {The smart surface network: A bus-based approach to dense sensing},
journal = {Computer Networks},
volume = {83},
pages = {167-183},
year = {2015},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2015.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S138912861500095X},
author = {Farha N. Ali and Yvon Feaster and Jiannan Zhai and Jason O. Hallstrom},
keywords = {Bus-based communication, Dense sensing, Hardware/software co-design, Indoor sensing},
abstract = {We present the Smart Surface Network (SSN), a hardware and software platform designed for dense sensing. Sensor nodes connected to the SSN communicate using a serial bus integrated within a mountable physical surface. The hardware architecture and bus access and communication mechanisms are implemented in a self-stabilizing manner, providing robust handling of unannounced arrivals and departures of network devices. An associated API supports a peer-to-peer communication paradigm, providing access to the physical, data link, and application layers of the bus. In this paper, we describe the SSN hardware architecture and present the bus access and peer discovery algorithms. We also discuss the design of the API and describe experimental results characterizing the fairness of the bus algorithm, the efficiency of the peer discovery algorithm, and the performance of the SSN system under varying load conditions.}
}
@article{CHEN2022106522,
title = {swdpwr: A SAS macro and an R package for power calculations in stepped wedge cluster randomized trials},
journal = {Computer Methods and Programs in Biomedicine},
volume = {213},
pages = {106522},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106522},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721005964},
author = {Jiachen Chen and Xin Zhou and Fan Li and Donna Spiegelman},
keywords = {Sample size estimation, Cross-sectional designs, Cohort designs, Correlation structure, Generalized estimating equations, Generalized linear mixed models, R shiny},
abstract = {Background and objective:The stepped wedge cluster randomized trial is a study design increasingly used in a wide variety of settings, including public health intervention evaluations, clinical and health service research. Previous studies presenting power calculation methods for stepped wedge designs have focused on continuous outcomes and relied on normal approximations for binary outcomes. These approximations for binary outcomes may or may not be accurate, depending on whether or not the normal approximation to the binomial distribution is reasonable. Although not always accurate, such approximation methods have been widely used for binary outcomes. To improve the approximations for binary outcomes, two new methods for stepped wedge designs (SWDs) of binary outcomes have recently been published. However, these new methods have not been implemented in publicly available software. The objective of this paper is to present power calculation software for SWDs in various settings for both continuous and binary outcomes. Methods: We have developed a SAS macro %swdpwr, an R package swdpwr and a Shiny app for power calculations in SWDs. Different scenarios including cross-sectional and cohort designs, binary and continuous outcomes, marginal and conditional models, three link functions, with and without time effects under exchangeable, nested exchangeable and block exchangeable correlation structures are accommodated in this software. Unequal numbers of clusters per sequence are also allowed. Power calculations for a closed cohort employ a block exchangeable within-cluster correlation structure that accounts for three intracluster (intraclass) correlations: the within-period, between-period, and within-individual correlations. Cross-sectional cohorts allow for nested exchangeable or exchangeable correlation structures defined by the within-period and the between-period intracluster correlations only. Our software assumes a complete design and equal cluster-period sizes. While the methods accommodate correlation structures of constant within-period intracluster correlation coefficient (ICC) as well as a different within- and between-period ICC, it does not allow the between-period ICC to decay. Results: swdpwr provides an efficient tool to support investigators in the design and analysis of stepped wedge cluster randomized trials. swdpwr addresses the implementation gap between newly proposed methodology and their application to obtain more accurate power calculations in SWDs. Conclusions: In an effort to make computationally efficient (and non-simulation-based) power methods under both the cross-sectional and closed-cohort designs for continuous and binary outcomes more accessible, we have developed this user-friendly software. swdpwr is implemented under two platforms: SAS and R, satisfying the needs of investigators from various backgrounds. Additionally, the Shiny app enables users who are not able to use SAS or R to implement these methods online straightforwardly.}
}