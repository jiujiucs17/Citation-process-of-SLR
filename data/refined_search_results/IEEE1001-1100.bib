@INPROCEEDINGS{7160972,
author={Cenni, Fabio and Guillaume, Olivier and Diaz-Nava, Mario and Maehne, Torsten},
booktitle={2015 Symposium on Design, Test, Integration and Packaging of MEMS/MOEMS (DTIP)},
title={SystemC-AMS/MDVP-based modeling for the virtual prototyping of MEMS applications},
year={2015},
volume={},
number={},
pages={1-6},
abstract={This paper aims at providing guidelines to select the right modeling style for MEMS simulation in order to perform system analysis. This paper presents how an application with micro-electro-mechanical system (MEMS) sensors controlled by software can be modeled and simulated at high abstraction level. The overall modeling and simulation methodology lies on the extensions of SystemC named SystemC AMS/MDVP. In order to illustrate our approach, a case study is presented which integrate three-axis MEMS containing an accelerometer, a gyroscope, a magnetometer, and its associated CPU. A special focus is put on the MEMS gyroscope.},
keywords={Micromechanical devices;Registers;Sensors;Gyroscopes;Mathematical model;Data models;Integrated circuit modeling;Micro-Electro-Mechanical System (MEMS);Multi-Domain Virtual Prototyping (MDVP);virtual prototyping;software validation;dimensional analysis;Timed Data Flow (TDF);block diagram;bond graph;reduced-order modeling;finite element analysis;SystemC AMS;Transaction-Level Modeling (TLM)},
doi={10.1109/DTIP.2015.7160972},
ISSN={},
month={April},}
@INPROCEEDINGS{9284335,
author={Li, Cheng and Dakkak, Abdul and Xiong, Jinjun and Hwu, Wen-mei},
booktitle={2020 IEEE 13th International Conference on Cloud Computing (CLOUD)},
title={The Design and Implementation of a Scalable Deep Learning Benchmarking Platform},
year={2020},
volume={},
number={},
pages={414-425},
abstract={The current Deep Learning (DL) landscape is fast-paced and is rife with non-uniform models, hardware/software (HW/SW) stacks. Currently, there is no DL benchmarking platform to facilitate the evaluation and comparison of DL innovations, be it models, frameworks, libraries, or hardware. As a result, the current practice of evaluating the benefits of proposed DL innovations is both arduous and error-prone - stifling the adoption of the innovations. In this work, we first identify 10 design features that are desirable within a DL benchmarking platform. These features include: performing the evaluation in a consistent, reproducible, and scalable manner, being framework and hardware agnostic, supporting real-world benchmarking workloads, providing in-depth model execution inspection across the HW/SW stack levels, etc. We then propose MLModelScope, a DL benchmarking platform that realizes these 10 design objectives. MLModelScope introduces a specification to define DL model evaluations and provides a runtime to provision the evaluation workflow using the user-specified HW/SW stack. MLModelScope defines abstractions for frameworks and supports the board range of DL models and evaluation scenarios. We implement MLModelScope as an open-source project with support for all major frameworks and hardware architectures. Through MLModelScope's evaluation and automated analysis workflows, we perform a case-study analysis of 37 models across 4 systems and show how model, hardware, and framework selection affects model accuracy and performance under different benchmarking scenarios. We further demonstrate how MLModelScope's tracing capability gives a holistic view of model execution and helps pinpoint bottlenecks.},
keywords={Deep learning;Analytical models;Technological innovation;Cloud computing;Computational modeling;Benchmark testing;Hardware;Deep Learning;Benchmarking;Machine Learning;Software Engineering;Artificial Intelligence},
doi={10.1109/CLOUD49709.2020.00063},
ISSN={2159-6190},
month={Oct},}
@INPROCEEDINGS{5070822,
author={Bernardo, Claudio Goncalves and Montini, Denis Avila and Fernandes, Danilo Douradinho and da Silva, Daniela América and Dias, Luiz Alberto Vieira and da Cunha, Adilson Marques},
booktitle={2009 Sixth International Conference on Information Technology: New Generations},
title={Using GQM for Testing Design Patterns in Real-Time and Embedded Systems on a Software Production Line},
year={2009},
volume={},
number={},
pages={1397-1404},
abstract={This article describes a methodology named Causal Analysis and Resolution (CAR) based on Goals, Questions, and Metrics (GQM) principles. Indicators are defined based on metrics for a decision-making process. Its main contributions are the construction of an information process system model and a prototype, involving GQM approach, in a quantitative definition and qualitative metrics. The CAR methodology is a process area (PA) of the Capability Maturity Model Integrated (CMMi) for software development from Carnegie Mellon University. This PA was used to eliminate systematic error cases listed in a Technical Report (TR) generated by CAR. An information system model was created to allow the elimination of defects, errors, and failures in a design pattern named IO Manager, during the test phase, and before its publication in a components library. The prototype was created using Rational Rose RealTime (RRRT) with focus on verification tests. It provided a quality assessment to the IO Manager design pattern. The use of this methodology was based on GQM and CAR along with the information process system model. The developed prototype aimed to monitor errors on design pattern tests in real-time embedded system of a software production line.},
keywords={Software testing;System testing;Real time systems;Embedded system;Embedded software;Software systems;Production systems;Capability maturity model;Computer errors;Software prototyping;I-CASE-E;GQM;FI;Testing;and Design Patterns},
doi={10.1109/ITNG.2009.267},
ISSN={},
month={April},}
@INPROCEEDINGS{9349302,
author={Yudin, Oleksandr and Toliupa, Serhii and Korchenko, Oleksandr and Tereikovska, Liudmyla and Tereikovskyi, Ihor and Tereikovskyi, Oleh},
booktitle={2020 IEEE 2nd International Conference on Advanced Trends in Information Theory (ATIT)},
title={Determination of Signs of Information and Psychological Influence in the Tone of Sound Sequences},
year={2020},
volume={},
number={},
pages={276-280},
abstract={The article is devoted to the development of information technology for recognizing the destructive influences in the non-verbal content of the sound sequences of multimedia messages of Internet-oriented mass media. The necessity of determining the significant signs of informational and psychological influence in the tonality of the sound series is introduced. It was found that, first of all, such characteristics of music as pulsation, a binaural effect and changes in the main tone should be analyzed. It is proposed to determine the signs of information-psychological influence by analyzing the music used in multimedia messages, which destructive properties have been proven. The object of the study is the music of the song (`I want neither love nor honor'), which is used as a video background in the famous suicidal game (`Blue Whale'). To analyze the tonality of music on each quasi-stationary fragment, the time variation of the modulus, the real part, and also the imaginary part of the complex representation of the Fourier coefficient was calculated. During the calculations, the length of the quasi-stationary fragment was 1024 samples, which at a sampling rate of 44100 Hz corresponds to about 20 ms. As a result of the analysis, it was determined that significant signs of information and psychological influence are the low-frequency nature of the sound, the periodic nature of the amplitude-time indicators of the sound signal with a period of 5-12 seconds, the periodic nature of the amplitude-frequency indicators of the fundamental tone of the sound signal with a period of 5-7 seconds. To calculate and visualize these indicators, a software package specially developed in the MatLab 2018 environment was used. It is proposed to correlate the ways of further research with the development of a method for recognizing the presence of destructive influences in the non-verbal content of the sound sequences of multimedia messages distributed in Internet-oriented mass media.},
keywords={Time-frequency analysis;Visualization;Psychology;Entertainment industry;Media;Information technology;Task analysis;informational and psychological influence;tonality of sound sequences;quasi-stationary fragment;main tone},
doi={10.1109/ATIT50783.2020.9349302},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4100389,
author={Watson, Gregory R. and DeBardeleben, Nathan A.},
booktitle={2006 IEEE International Conference on Cluster Computing},
title={A Model-Based Framework for the Integration of Parallel Tools},
year={2006},
volume={},
number={},
pages={1-11},
abstract={A large number of tools are already available to aid in the development of parallel scientific applications, yet many developers are unaware they exist, do not have access to them, or find them too difficult to use. And, unlike the wider software development community where the use of integrated development environments is best practice, parallel software development languishes with the lowest common denominator of command-line tools and Emacs style editors. By harnessing the power and flexibility of the phenomenally successful Eclipse framework, we have developed a platform for the integration of parallel tools that aims to provide a robust, portable, and scalable parallel development environment for the development of high performance scientific computing applications. The Eclipse Parallel Tools Platform utilizes a model-view-controller design and a generic API architecture to support a wide range of parallel computing environments. The platform has been designed so that it is easily extensible, and will support the integration of existing and new parallel tools. In this paper we describe the architecture of the platform, provide details of an example implementation for a particular parallel runtime system, and show how other parallel tools can be integrated with the Eclipse Parallel Tools Platform},
keywords={Application software;Programming;Parallel processing;User interfaces;Computer architecture;National security;Laboratories;Computer aided software engineering;US Department of Energy;Open source software},
doi={10.1109/CLUSTR.2006.311883},
ISSN={2168-9253},
month={Sep.},}
@INPROCEEDINGS{9251924,
author={Opdebeeck, Ruben and Zerouali, Ahmed and Velázquez-Rodríguez, Camilo and Roover, Coen De},
booktitle={2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
title={Does Infrastructure as Code Adhere to Semantic Versioning? An Analysis of Ansible Role Evolution},
year={2020},
volume={},
number={},
pages={238-248},
abstract={Ansible, a popular Infrastructure-as-Code platform, provides reusable collections of tasks called roles. Roles are often contributed by third parties, and like general-purpose libraries, they evolve. As such, new releases of roles need to be tagged with version numbers, for which Ansible recommends adhering to the semantic versioning format. However, roles significantly differ from general-purpose libraries, and it is not yet known what constitutes a breaking change or the addition of a feature to a role. Consequently, this can cause confusion for clients of a role and new role contributors. To alleviate this issue, we perform an empirical study on semantic versioning in Ansible roles to uncover the types of changes that trigger certain types of version bumps. We collect a dataset of over 70000 version increments spanning upwards of 7800 Ansible roles. Moreover, we design a novel structural model for these roles, and implement a domainspecific structural change extraction algorithm to calculate structural difference metrics. Afterwards, we quantitatively investigate the state of semantic versioning in Ansible roles and identify the most commonly changed components. Then, using the structural difference metrics, we train a Random Forest classifier to predict applicable version bumps for Ansible role releases. Lastly, we confirm our empirical findings with a developer survey. Our observations show that although most Ansible role developers follow the semantic versioning format, it appears that they do not always consistently follow the same rules when selecting the version bump to apply.},
keywords={Measurement;Semantics;Prediction algorithms;Libraries;Classification algorithms;Task analysis;Guidelines;Ansible;Infrastructure as Code;Semantic Versioning;empirical study;mining software repositories},
doi={10.1109/SCAM51674.2020.00032},
ISSN={2470-6892},
month={Sep.},}
@INPROCEEDINGS{8085811,
author={Smirnov, Vladimir U. and Kos, Oksana I.},
booktitle={2017 International Conference "Quality Management,Transport and Information Security, Information Technologies" (IT&QM&IS)},
title={Program module for calculating the optimal interval of preventive substitutions},
year={2017},
volume={},
number={},
pages={282-283},
abstract={Railway transport is one of the main transport modes of the Russian Federation, which is strategically important for the country. Artificial constructions on the railways are complex and responsible elements of the track economy. The probability of their failure-free operation must be high enough. For the adoption of optimal solutions for managing the technical state of man-made structures, it is necessary to use probabilistic operating models, build algorithms for calculating reliability and development indices using these algorithms of the software package, which makes it possible to automate the process of controlling the operation of artificial constructions on railways.},
keywords={Bridges;Reliability engineering;Maintenance engineering;Rail transportation;Software reliability;Software packages;optimal interval;reliability;program module},
doi={10.1109/ITMQIS.2017.8085811},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{1000035,
author={Yau, S.S. and Xiaoyong Zhou},
booktitle={Proceedings of the Seventh IEEE International Workshop on Object-Oriented Real-Time Dependable Systems. (WORDS 2002)},
title={Schedulability in model-based software development for distributed real-time systems},
year={2002},
volume={},
number={},
pages={45-52},
abstract={Schedulability of distributed real-time system has been studied extensively. However, due to the discrepancies the reference model used in scheduling analysis and the object-oriented model, the results cannot be easily used for distributed real-time object-oriented software development. Even if object-oriented models can be used to conduct schedulability analysis successfully, there still lacks an effective approach to validate the implementation with the design. Model-based approach alleviates the discrepancies between models of different stages and the implementation by automatically transforming models at different granularity until the code generation for the implementation from the generated design. In this paper, an approach to incorporating schedulability analysis reference diagrams into the existing framework for model-based software development will be presented. It will improve the predictability of a distributed real-time system as well as increase the capability of model refinements and code generation using the new reference diagrams and schedulability analysis results to generate code for implementing scheduling and synchronization aspects of the distributed real-time system.},
keywords={Programming;Real time systems;Object oriented modeling;Processor scheduling;Timing;Runtime library;Unified modeling language;Computational modeling;Yarn;Computer science},
doi={10.1109/WORDS.2002.1000035},
ISSN={1530-1443},
month={Jan},}
@INPROCEEDINGS{7281791,
author={Kolek, L and Ibrahim, M. Yousef and Gunawan, I and Laribi, M. A. and Zegloul, S.},
booktitle={2015 IEEE 13th International Conference on Industrial Informatics (INDIN)},
title={Evaluation of control system reliability using combined dynamic fault trees and Markov models},
year={2015},
volume={},
number={},
pages={536-543},
abstract={In this paper, dynamic simulation methods for reliability evaluation of common industry-based control system architectures are investigated. Control system design often employs complex reliability structures in the forms of several levels of software and hardware redundancies, hot and cold standby systems. This is required in order to achieve certain plant availability and safety functions. Control system maintenance requires expert knowledge due to the complexity of troubleshooting steps involved with a hardware or software failures of a large system. Hence, it is crucial to understand the effect of recovery time on reliability and on overall availability in a critical control system. Dynamic Fault Tree Analysis (DFTA), Markov Chains and Reliability Block Diagrams (RBD) are presented and a block library is introduced for addressing the aforementioned modelling problems. In order to be able to evaluate dynamic fault trees and Markov Chains, Monte Carlo simulation has been used. An industry-based case study is presented, where critical failures of a redundant Programmable Logic Controller (PLC) system are identified by a Failure Mode and Effect Analysis (FMEA). The bottom up process of modelling control system reliability is discussed.},
keywords={Logic gates;Markov processes;Modeling;Maintenance engineering;Control systems;Reliability engineering;control system reliability;PLC;DFTA;FMEA;Markov model;Monte Carlo simulation;failure recovery},
doi={10.1109/INDIN.2015.7281791},
ISSN={2378-363X},
month={July},}
@INPROCEEDINGS{9286210,
author={Myers, Jeremy M. and Dunlavy, Daniel M. and Teranishi, Keita and Hollman, D.S.},
booktitle={2020 IEEE High Performance Extreme Computing Conference (HPEC)},
title={Parameter Sensitivity Analysis of the SparTen High Performance Sparse Tensor Decomposition Software},
year={2020},
volume={},
number={},
pages={1-7},
abstract={Tensor decomposition models play an increasingly important role in modern data science applications. One problem of particular interest is fitting a low-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has sparse structure and the tensor elements are nonnegative count data. SparTen is a high-performance C++ library which computes a low-rank decomposition using different solvers: a first-order quasi-Newton or a second-order damped Newton method, along with the appropriate choice of runtime parameters. Since default parameters in SparTen are tuned to experimental results in prior published work on a single real-world dataset conducted using MATLAB implementations of these methods, it remains unclear if the parameter defaults in SparTen are appropriate for general tensor data. Furthermore, it is unknown how sensitive algorithm convergence is to changes in the input parameter values. This report addresses these unresolved issues with large-scale experimentation on three benchmark tensor data sets. Experiments were conducted on several different CPU architectures and replicated with many initial states to establish generalized profiles of algorithm convergence behavior.},
keywords={Tensors;Handheld computers;Heuristic algorithms;Software algorithms;Linear programming;Software;Convergence;tensor decomposition;Poisson factorization;Kokkos;Newton optimization},
doi={10.1109/HPEC43674.2020.9286210},
ISSN={2643-1971},
month={Sep.},}
@INPROCEEDINGS{739734,
author={Saksena, M. and Ptak, A. and Freedman, P. and Rodziewicz, P.},
booktitle={Proceedings 19th IEEE Real-Time Systems Symposium (Cat. No.98CB36279)},
title={Schedulability analysis for automated implementations of real-time object-oriented models},
year={1998},
volume={},
number={},
pages={92-102},
abstract={The increasing complexity of real time software has led to a recent trend in the use of high level modeling languages for development of real time software. One representative example is the modeling language ROOM (real time object oriented modeling), which provides features such as object orientation, state machine description of behaviors, formal semantics for executability of models, and possibility of automated code generation. However these modeling languages largely ignore the timeliness aspect of real time systems, and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior. We consider schedulability analysis for automated implementations of ROOM models, based on the ObjecTime toolset. This work builds on results presented by M. Saksena (1997), where we developed some guidelines for the design and implementation of real time object oriented models. Using the guidelines, we have modified the run time system library provided by the ObjecTime toolset to make it amenable to schedulability analysis. Based on the modified toolset, we show how a ROOM model can be analyzed for schedulability, taking into account the implementation overheads and structure. The analysis is validated experimentally, first using simple periodic models, and then using a large case study of a train tilting system.},
keywords={Object oriented modeling;Real time systems;Control systems;Timing;Predictive models;Failure analysis;Guidelines;Embedded software;Automatic control;Runtime},
doi={10.1109/REAL.1998.739734},
ISSN={1052-8725},
month={Dec},}
@INPROCEEDINGS{6853202,
author={Vilanova, Lluís and Ben-Yehuda, Muli and Navarro, Nacho and Etsion, Yoav and Valero, Mateo},
booktitle={2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)},
title={CODOMs: Protecting software with Code-centric memory Domains},
year={2014},
volume={},
number={},
pages={469-480},
abstract={Today's complex software systems are neither secure nor reliable. The rudimentary software protection primitives provided by current hardware forces systems to run many distrusting software components (e.g., procedures, libraries, plugins, modules) in the same protection domain, or otherwise suffer degraded performance from address space switches. We present CODOMs (COde-centric memory DOMains), a novel architecture that can provide finer-grained isolation between software components with effectively zero run-time overhead, all at a fraction of the complexity of other approaches. An implementation of CODOMs in a cycle-accurate full-system x86 simulator demonstrates that with the right hardware support, finer-grained protection and run-time performance can peacefully coexist.},
keywords={Abstracts;Hardware;Kernel;Memory management},
doi={10.1109/ISCA.2014.6853202},
ISSN={1063-6897},
month={June},}
@INPROCEEDINGS{7363616,
author={Park, Sihyeong and Kim, Hyungshin and Kang, Soo-Yeong and Koo, Cheol Hea and Joe, Hyunwoo},
booktitle={2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing},
title={Lua-Based Virtual Machine Platform for Spacecraft On-Board Control Software},
year={2015},
volume={},
number={},
pages={44-51},
abstract={Mission critical embedded software for autonomous operation requires high development cost due to its long development cycle. One of the potential solutions for reducing the cost is to reuse the software developed at previous missions. Virtual machine platform such as JVM is a good example to provide code portability across various missions. Flight software in aerospace field is adopting this concept to improve reusability and eventually to reduce development cost. In this paper, we propose a Lua-based virtualization environment for spacecraft flight software. Flight software for spacecraft control consists of a few tasks that are highly autonomous. Lua is chosen as the script language for programming the control tasks. Though Lua was designed with simplicity and portability, it only supports multithreading with collaborative coroutines. To support preemptive multitasking, we implement time slicing coroutines as spacecraft control processes. New coroutine scheduler is devised and time slicing functionality is added into the scheduler. Scheduler locking and message passing with external flight software are also implemented. Instead of modifying the Lua interpreter, we have exploited the debug support APIs for our implementation. For evaluation, we have implemented the flight software virtualization environment on the flight computer. Accuracy of the time slicing scheduler is also analyzed.},
keywords={Software;Space vehicles;Computers;Engines;Virtual machining;Virtualization;Runtime;OBCP;Lua;spacecraft;virtual machine;mission critical embedded software;reusability},
doi={10.1109/EUC.2015.21},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8384660,
author={Krause, Christoph and Holzapfel, Florian},
booktitle={2018 4th International Conference on Control, Automation and Robotics (ICCAR)},
title={Implementing a multi-level finite state machine with MATLAB Simulink and Stateflow in the environment of high-integrity aircraft controller software},
year={2018},
volume={},
number={},
pages={147-151},
abstract={At the Technical University Munich the Institute of Flight System Dynamics uses different manned and unmanned aerial platform to demonstrate modern flight control and flight management functions. Those algorithms are designed, implemented and tested using MATLAB Simulink and Stateflow. The context of high-integrity aircraft controller software claims various limitations of the tools. The state machines used in the flight control software are developed using a special combination of MATLAB Simulink and Stateflow. That can greatly reduce complexity while simultaneously enhancing clarity and testing abilities. This paper contains a detailed explanation of the implementation scheme and structure developed at the Institute of Flight System Dynamics to design multi-level finite state machines in the context of high-integrity aircraft controller software.},
keywords={Aerospace control;Software packages;Aircraft;Matlab;System dynamics;Routing;state machine;stateflow;aircraft control;MATLAB;simulink},
doi={10.1109/ICCAR.2018.8384660},
ISSN={},
month={April},}
@INPROCEEDINGS{830318,
author={Elarde, J.V. and Brewster, G.B.},
booktitle={Conference Proceedings of the 2000 IEEE International Performance, Computing, and Communications Conference (Cat. No.00CH37086)},
title={Performance analysis of application response measurement (ARM) version 2.0 measurement agent software implementations},
year={2000},
volume={},
number={},
pages={190-198},
abstract={Effective distributed application performance management infrastructures are based upon the collection and management of key performance metrics including resource utilization, workload and service measurements. Without application oriented transaction service measurements, the viability and long term usability of the client/server paradigm for mission critical applications is in question. The Application Response Measurement (ARM) Application Programming Interface (API) addresses this requirement by enabling the measurement of transaction level metrics, notably response time, for distributed applications through application code instrumentation. Version 2.0 of the ARM API includes the ability to measure and correlate response time components on different processors (i.e., clients and sewers). ARM Agents encompass the software components responsible for interception and processing of the ARM API calls including support for response time measurement, collection and communication. This paper introduces alternatives for several important design aspects of ARM agent software that are not specified in the ARM standards documents. In particular, we consider the architecture of ARM Agent software, correlator generation and communication, and ARM data storage. We propose, implement and analyze the performance of several alternative designs. This work provides an assessment of ARM instrumentation impact on application response times as well as insights into the design issues involved. We first provide a general overview of the ARM 2.0 API and features. We then consider alternative ARM agent software designs. Next, we present measurements results for several design implementations that shed light on the overhead involved with instrumentation and the developed designs. Finally, we present conclusions and a description of future work.},
keywords={Performance analysis;Time measurement;Delay;Resource management;Application software;Instruments;Usability;Mission critical systems;Software measurement;Software standards},
doi={10.1109/PCCC.2000.830318},
ISSN={},
month={Feb},}
@INPROCEEDINGS{596832,
author={Di Martino, B. and Iannello, G. and Zima, H.P.},
booktitle={Proceedings of PDSE '97: 2nd International Workshop on Software Engineering for Parallel and Distributed Systems},
title={An automated algorithmic recognition technique to support parallel software development},
year={1997},
volume={},
number={},
pages={120-129},
abstract={Techniques for automatic program recognition, at the algorithmic level, could be of great interest in the area of program parallelization, because the selection of suitable parallelization strategies is driven by algorithmic features of the code. A technique for the specification and automatic recognition of algorithmic concepts is presented. Its flexibility and expressivity power for specifying the hierarchy, the constraints and the relationships among concepts allow it to deal with recognition of algorithmic concepts within optimized code, irregular computations, and in the presence of code sharing, delocalization, implementation variations and other problems related to program recognition in the context of the imperative languages typically used for scientific computation.},
keywords={Software algorithms;Programming;Libraries;Constraint optimization;Humans;Concrete;Data structures;Target recognition;Runtime;Character recognition},
doi={10.1109/PDSE.1997.596832},
ISSN={},
month={May},}
@INPROCEEDINGS{4440280,
author={Wittmer, W. and Colocho, W. and White, G.},
booktitle={2007 IEEE Particle Accelerator Conference (PAC)},
title={A parallel controls software approach for PEP II: AIDA & MATLAB middle layer},
year={2007},
volume={},
number={},
pages={566-568},
abstract={The controls software in use at PEP II (Stanford control program - SCP) had originally been developed in the eighties. It is very successful in routine operation but due to its internal structure it is difficult and time consuming to extend its functionality. This is problematic during machine development and when solving operational issues. Routinely, data has to be exported from the system, analyzed offline, and calculated settings have to be reimported. Since this is a manual process, it is time consuming and error-prone. Setting up automated processes, as is done for MIA (model independent analysis), is also time consuming and specific to each application. Recently, there has been a trend at light sources to use MATLAB [1] as the platform to control accelerators using a "MATLAB middle layer" [2] (MML), and so called channel access (CA) programs to communicate with the low level control system (LLCS). This has proven very successful, especially during machine development time and trouble shooting. A special CA code, named AIDA (Accelerator Independent Data Access [3]), was developed to handle the communication between MATLAB, modern software frameworks, and the SCP. The MML had to be adapted for implementation at PEP II. Colliders differ significantly in their designs compared to light sources, which poses a challenge. PEP II is the first collider at which this implementation is being done. We will report on this effort, which is still ongoing.},
keywords={MATLAB;Light sources;Packaging machines;Control systems;Level control;Performance evaluation;Mathematical model;Computer languages;Automatic control;Lighting control},
doi={10.1109/PAC.2007.4440280},
ISSN={2152-9582},
month={June},}
@INPROCEEDINGS{1515794,
author={Reves, X. and Marojevic, V. and Ferrus, R. and Gelonch, A.},
booktitle={International Conference on Field Programmable Logic and Applications, 2005.},
title={FPGA's middleware for software defined radio applications},
year={2005},
volume={},
number={},
pages={598-601},
abstract={The division in several layers of the implementation of systems is a solution adopted to avoid complexity, provide flexibility and improve portability and code reusability through different hardware. Middleware (intermediate layer between two other layers) implementations are based on the use of increasingly high-level languages and application programming interfaces (API). The field programmable gate arrays (FPGA) world can also apply this approach to produce building blocks independent from hardware platforms and devices. This paper presents details of the implementation of a middleware, called platform and hardware abstraction layer (P-HAL) when applied to FPGA devices. It was specially designed for radio applications and allows designing specific functions independently of the hardware context where they are applied, thus providing flexibility to the so-called software radios employing FPGA devices.},
keywords={Field programmable gate arrays;Middleware;Software radio;Application software;Hardware;Energy consumption;Design optimization;Manufacturing;Irrigation;High level languages},
doi={10.1109/FPL.2005.1515794},
ISSN={1946-1488},
month={Aug},}
@ARTICLE{9113456,
author={Pinto, Pedro and Bispo, João and Cardoso, João M. P. and Barbosa, Jorge G. and Gadioli, Davide and Palermo, Gianluca and Martinovič, Jan and Golasowski, Martin and Slaninová, Kateřina and Cmar, Radim and Silvano, Cristina},
journal={IEEE Transactions on Software Engineering},
title={Pegasus: Performance Engineering for Software Applications Targeting HPC Systems},
year={2022},
volume={48},
number={3},
pages={732-754},
abstract={Developing and optimizing software applications for high performance and energy efficiency is a very challenging task, even when considering a single target machine. For instance, optimizing for multicore-based computing systems requires in-depth knowledge about programming languages, application programming interfaces (APIs), compilers, performance tuning tools, and computer architecture and organization. Many of the tasks of performance engineering methodologies require manual efforts and the use of different tools not always part of an integrated toolchain. This paper presents Pegasus, a performance engineering approach supported by a framework that consists of a source-to-source compiler, controlled and guided by strategies programmed in a Domain-Specific Language, and an autotuner. Pegasus is a holistic and versatile approach spanning various decision layers composing the software stack, and exploiting the system capabilities and workloads effectively through the use of runtime autotuning. The Pegasus approach helps developers by automating tasks regarding the efficient implementation of software applications in multicore computing systems. These tasks focus on application analysis, profiling, code transformations, and the integration of runtime autotuning. Pegasus allows developers to program their strategies or to automatically apply existing strategies to software applications in order to ensure the compliance of non-functional requirements, such as performance and energy efficiency. We show how to apply Pegasus and demonstrate its applicability and effectiveness in a complex case study, which includes tasks from a smart navigation system.},
keywords={Task analysis;Software;Tools;Runtime;Tuning;Power demand;Libraries;Performance engineering;methodology;high-performance computing;domain-specific languages;source-to-source compilers},
doi={10.1109/TSE.2020.3001257},
ISSN={1939-3520},
month={March},}
@ARTICLE{32629,
author={Daneshdoost, M. and Shaat, R.},
journal={IEEE Transactions on Power Systems},
title={A PC based integrated software for power system education},
year={1989},
volume={4},
number={3},
pages={1285-1292},
abstract={The implementation of an integrated software package for the analysis and design of electric power networks, to run under PC-DOS, is discussed. Graphics and windows are embedded in the user interface to form the basis of the interactive environment. System configuration is entered graphically, whereas system data is entered directly through tabular windows. A variety of analysis programs are provided, such as different types of load-flow solution techniques. This work demonstrates a novel use of PC-enhanced graphics capability to display real and reactive power flows by means of animation. A modular interactive expert system is an element of this environment. Popular features such as help menus and icon-selection menus are included. The modular design of this environment permits the user to interface any custom-made analysis package regardless of the computer language used. The environment has proved to be useful for educational and research purposes.<>},
keywords={Software systems;Power systems;Graphics;Power system analysis computing;Software packages;User interfaces;Load flow analysis;Computer displays;Reactive power;Animation},
doi={10.1109/59.32629},
ISSN={1558-0679},
month={Aug},}
@INPROCEEDINGS{7040961,
author={Che, Shuai and Beckmann, Bradford M. and Reinhardt, Steven K.},
booktitle={2014 IEEE High Performance Extreme Computing Conference (HPEC)},
title={BelRed: Constructing GPGPU graph applications with software building blocks},
year={2014},
volume={},
number={},
pages={1-6},
abstract={Graph applications are common in scientific and enterprise computing. Recent research studies used graphics processing units (GPUs) to accelerate graph workloads. These applications tend to present characteristics that are challenging for single instruction multiple data (SIMD) computation. To achieve high performance, prior work studied individual graph problems, and designed device-specific algorithms and optimizations to achieve high performance. However, programmers have to expend significant manual effort, packing data and computation to make such solutions GPU-friendly. Usually, they are too complex for regular programmers, and the resultant implementations may not be portable nor perform well across platforms. To address these concerns, we present a library of software building blocks, BelRed1 which allows programmers to build GPGPU graph applications with ease. BelRed is based on the prior research of graph algorithms in linear algebra, and is implemented and optimized for the GPU platform. BelRed currently is built on top of the OpenCL framework. It consists of fundamental building blocks necessary for graph processing. This paper introduces the library and presents several case studies on how to leverage it for a variety of representative graph problems. We evaluate application performance on an AMD GPU and investigate optimization approaches to improve performance.},
keywords={Graphics processing units;Kernel;Vectors;Libraries;Data structures;Sparse matrices;Performance evaluation},
doi={10.1109/HPEC.2014.7040961},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6068335,
author={Scheller, Thomas and Kuhn, Eva},
booktitle={2011 37th EUROMICRO Conference on Software Engineering and Advanced Applications},
title={Measurable Concepts for the Usability of Software Components},
year={2011},
volume={},
number={},
pages={129-133},
abstract={While usability has proven to be an important software quality attribute, its application to APIs is still rather uncommon. Available methods for measuring software usability show significant disadvantages when applied to APIs, like the need for test users and experienced evaluators. This makes it difficult to evaluate the usability of software components, as well as to compare different software components. An API usability measurement method is needed that is both machine-computable and objective. This paper takes a first step in the direction of such a measure by identifying measurable concepts for the usability of software components, and validating these concepts against existing studies and guidelines for usability and API design.},
keywords={Usability;Complexity theory;Software measurement;Middleware},
doi={10.1109/SEAA.2011.28},
ISSN={2376-9505},
month={Aug},}
@ARTICLE{1458050,
author={Black, J.P. and Marshall, L.F. and Randell, B.},
journal={Proceedings of the IEEE},
title={The architecture of UNIX united},
year={1987},
volume={75},
number={5},
pages={709-718},
abstract={UNIX United is an architecture for a distributed system based on UNIX. As it is compatible with UNIX at the system call level, any program written for a normal UNIX system can be transparently extended to exploit the richer environment of UNIX United. As it relies on having a UNIX system beneath it, the implementation of UNIX United, called the Newcastle Connection, provides an interesting example of the construction of a very powerful distributed system with only a modicum of effort. A description of the basic semantics of UNIX United is followed by that of the architecture implied by the protocol between components in a UNIX United system, and of a software structure appropriate to the architecture and the protocol.},
keywords={Kernel;Computer architecture;Protocols;Councils;Radar;Computer science;Laboratories;Distributed computing;Investments;Software libraries},
doi={10.1109/PROC.1987.13783},
ISSN={1558-2256},
month={May},}
@INPROCEEDINGS{227407,
author={Scheuermann, P. and Hsiang-Lung Tung},
booktitle={[1992 Proceedings] Second International Workshop on Research Issues on Data Engineering: Transaction and Query Processing},
title={A deadlock checkpointing scheme for multidatabase systems},
year={1992},
volume={},
number={},
pages={184-191},
abstract={A multidatabase system (MDBS) is a software package that integrates a number of pre-existing, autonomous and heterogeneous local database systems (LDBS). Deadlock detection and resolution in MDBS is much more difficult than in traditional distributed database systems due to the autonomy requirement which implies that LDBSs cannot exchange any control information. The authors present an efficient periodic deadlock detection and resolution scheme for MDBS which allows for the concurrent execution of global transactions at multiple sites. The authors scheme employs a depth-first search of a bipartite graph called the transaction-block-at site graph (TBSG).<>},
keywords={System recovery;Checkpointing;Database systems;Transaction databases;Software packages;Control systems;Bipartite graph;Concurrency control;Computer architecture;Protocols},
doi={10.1109/RIDE.1992.227407},
ISSN={},
month={Feb},}
@INPROCEEDINGS{342678,
author={Lowry, M. and Philpot, A. and Pressburger, T. and Underwood, I.},
booktitle={Proceedings KBSE '94. Ninth Knowledge-Based Software Engineering Conference},
title={A formal approach to domain-oriented software design environments},
year={1994},
volume={},
number={},
pages={48-57},
abstract={This paper describes a formal approach to domain-oriented software design environments, based on declarative domain theories, formal specifications, and deductive program synthesis. A declarative domain theory defines the semantics of a domain-oriented specification language and its relationship to implementation-level subroutines. Formal specification development and reuse is made accessible to users through an intuitive graphical interface that guides them in creating diagrams denoting formal specifications. Deductive program synthesis ensures that specifications are correctly implemented. This approach has been implemented in AMPHION, a generic KBSE system that targets scientific subroutine libraries. AMPHION has been applied to the domain of solar system kinematics. AMPHION enables space scientists to develop, modify, and reuse specifications an order of magnitude more rapidly than manual program development. Program synthesis is efficient and completely automatic.<>},
keywords={Software design;Formal specifications;Algorithms;Solar system;Kinematics;Impedance;Explosions;Application software;Space technology;Artificial intelligence},
doi={10.1109/KBSE.1994.342678},
ISSN={1068-3062},
month={Sep.},}
@INPROCEEDINGS{6605950,
author={Kao, Chia Hung and Lin, Chun Cheng and Chen, Juei-Nan},
booktitle={2013 13th International Conference on Quality Software},
title={Performance Testing Framework for REST-Based Web Applications},
year={2013},
volume={},
number={},
pages={349-354},
abstract={Recently, enterprises, organizations, and software companies are building more and more web applications to provide their services over the Internet. In order to fulfill various requirements, the complexity of web applications nowadays is increasing dramatically. As a result, the performance characteristics of web applications, including response time, throughput, etc, become more critical than before and should be taken into careful consideration. If the response time of a web application is poor, users may lose their interests even the function of the web application is correct. Therefore, how to execute performance testing on a complex web application systematically and efficiently will be an important issue. In this paper, a performance testing framework for REST-based web applications is introduced. The performance testing framework aims to provide software testers with an integrated process from test cases design, test scripts generation, to test execution. Based on the test cases designed by software testers and the appropriate software artifacts preserved by the framework (e.g., API document), the framework generates the corresponding performance test scripts, which can be executed by specific performance test tools. This helps software testers to focus more in the design of performance test cases. In addition, effort needed to understand the design and implementation of the application and to learn the operation of testing tools decrease. Thus, the efficiency of performance testing can be highly facilitated.},
keywords={Software;Testing;Engines;Computer architecture;Complexity theory;Time factors;XML;Performance testing;web application;software testing},
doi={10.1109/QSIC.2013.32},
ISSN={2332-662X},
month={July},}
@INPROCEEDINGS{7087258,
author={Zappatore, Marco and Longo, Antonella and Bochicchio, Mario A.},
booktitle={Proceedings of 2015 12th International Conference on Remote Engineering and Virtual Instrumentation (REV)},
title={The bibliographic reference collection GRC2014 for the Online Laboratory Research community},
year={2015},
volume={},
number={},
pages={24-31},
abstract={Online Laboratories are becoming more and more appealing in many educational scenarios due to technology advancements and the rising of pedagogical interests. Therefore, the scientific community that orbits around them is growing as well. Proper qualitative and quantitative analyses are needed to profile publications and their authors, to highlight trends and most promising research areas, to identify leading authors, to drive the scientific community towards the right publication targets. Since no tools are currently available to achieve such purposes, the authors propose a data processing pipeline capable of producing a dataset of cleaned and normalized bibliographic references (namely, GRC2014). The description of both design and implementation phases of such a solution is provided in this paper, as well as a thorough evaluation of the dataset and its contents. In addition, reports that highlight core features of this scientific domain are provided, thus characterizing the proposed solution as a useful tool for researchers and authors.},
keywords={Communities;Pipelines;Data processing;Instruments;Cleaning;Libraries;Market research;Online Laboratories;Reporting;Bibliography;Reference Management Software},
doi={10.1109/REV.2015.7087258},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9993991,
author={Kingsley-Omoyibo, Queeneth and Chukwutoo, Ihueze Christopher},
booktitle={2022 International Conference on Smart Applications, Communications and Networking (SmartNets)},
title={Industry 4.0: A Comparative Analysis of Advanced Planning and Scheduling Manufacturing Soft wares for Implementation and Adoption in Connected Factories},
year={2022},
volume={},
number={},
pages={1-8},
abstract={The dire need to implement and adopt industry in order to stabilize supply chains, improve throughput in manufacturing processes to meet planning and scheduling needs using advanced planning and scheduling manufacturing software key enablers such as production sequencing, real time optimization, planning and scheduling, prompted this study. The aim of this study is to comparatively analyze and select from the advanced manufacturing software key enablers to be used to implement and adopt industry for a connected factory. The concurrent triangulation design was used for analyzing the data collected from industry survey questionnaires distributed. The internal consistency of the respondents from the in-depth interview was measured using reliability test results from cronbach alpha values for production scheduling software, real time optimization software, production planning software and production sequencing software were 0.99147, 0.98814, 0.97910 and 0.97095 respectively using the rule of thumb 0≥ 0.9 excellent which showed a good index of association. A total of 450 questionnaires were distributed, retrieved and found useable. Results from phase 1 and phase 2 of the concurrent triangulation model were merged and showed that 33.78% industries were of the opinion that optimizing supply network cost and implementing a complete order allocation using production scheduling software for Industry 4.0 will speedup implementation of connected factories, 34.22 % were in support of production planning, 17.33 % were in support of real time optimization while production sequencing and 14.67% supported production sequencing manufacturing software. From the results, optimized value of alpha at 0.98241 embracing the full package of the software used to implement and adopt industry 4.0for connected factories, will help to keep pace with industry-leading digitization practices when fully implemented and adopted.},
keywords={Industries;Sequential analysis;Job shop scheduling;Production planning;Software;Real-time systems;Production facilities;Industry;connected factories;manufacturing software;implementation;adoption;concurrent triangulation model},
doi={10.1109/SmartNets55823.2022.9993991},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8120800,
author={Paraschiv, Dragos and McDowall, Rory and Thurlow, Chad},
booktitle={2017 International Conference on ENERGY and ENVIRONMENT (CIEM)},
title={Energy efficient single detached dwellings beyond Ontario building code},
year={2017},
volume={},
number={},
pages={172-176},
abstract={This paper presents an analysis of advancing a code compliant single detached dwelling in Ontario, Canada to Passive House Standard energy efficiency level. The research was based on comparing energy models built with the Passive House Planning Package, and included thermal bridging calculations performed in the THERM software. For each energy model, only the effect of improved exterior insulation was isolated and analysed. Starting from the energy model built for code compliant construction, insulation thickness was then increased and air tightness improved until the energy model reached the Passive House energy efficiency level. Achieving the Passive House energy efficiency required 152 mm of exterior insulation and 420 mm of batt insulation in a double stud wall, due to poor internal volume to exterior surface area ratio for the single detached dwelling and rectangular building shape.},
keywords={Insulation;Buildings;Energy efficiency;Standards;Load modeling;Atmospheric modeling;Software;building insulation;energy efficiency;energy modelling;passive house;thermal bridging},
doi={10.1109/CIEM.2017.8120800},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8835346,
author={Erbsen, Andres and Philipoom, Jade and Gross, Jason and Sloan, Robert and Chlipala, Adam},
booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
title={Simple High-Level Code for Cryptographic Arithmetic - With Proofs, Without Compromises},
year={2019},
volume={},
number={},
pages={1202-1219},
abstract={We introduce a new approach for implementing cryptographic arithmetic in short high-level code with machine-checked proofs of functional correctness. We further demonstrate that simple partial evaluation is sufficient to transform into the fastest-known C code, breaking the decades-old pattern that the only fast implementations are those whose instruction-level steps were written out by hand. These techniques were used to build an elliptic-curve library that achieves competitive performance for 80 prime fields and multiple CPU architectures, showing that implementation and proof effort scales with the number and complexity of conceptually different algorithms, not their use cases. As one outcome, we present the first verified high-performance implementation of P-256, the most widely used elliptic curve. implementations from our library were included in BoringSSL to replace existing specialized code, for inclusion in several large deployments for Chrome, Android, and CloudFlare.},
keywords={Cryptography;Libraries;Computer bugs;Optimization;Pipelines;Google;Elliptic curves;Cryptography;Software-engineering;Formal-verification},
doi={10.1109/SP.2019.00005},
ISSN={2375-1207},
month={May},}
@INPROCEEDINGS{534159,
author={Deri, L.},
booktitle={Proceedings of IEEE International Workshop on System Management},
title={Surfin' network resources across the Web},
year={1996},
volume={},
number={},
pages={158-167},
abstract={Network management standards provide a basis for hiding differences between resources, along with a method of managing them in a consistent way. Frequently, network management tools are based on proprietary products and are often complex both to use and to install. The increasing popularity of the World Wide Web, with its established user interface and the ability to run on almost any platform. offers a new way to provide wide access to complex software applications. This paper describes the architecture and implementation of two new network management applications that are accessible through the Web and are targeted to run on powerful hosts as well as small mobile computers. The GDMO/ASN.1 (Guidelines for the Definition of Managed Objects using the Abstract Syntax Notation 1) Search Engine enables users to perform complex queries and to navigate OSI management documents exploiting the Web's hypertext facilities. Liaison is a Web-based browser for CMIP (Common Management Information Protocol) and SNMP (Simple Network Management Protocol) agents that is equipped with powerful tools, such as a directory and a metadata facility.},
keywords={Energy management;Resource management;Application software;Computer network management;Protocols;Web sites;User interfaces;Computer architecture;Service oriented architecture;Mobile computing},
doi={10.1109/IWSM.1996.534159},
ISSN={},
month={June},}
@INPROCEEDINGS{1193932,
author={Kim, K.H. and Seok-Joong Kang},
booktitle={The Sixth International Symposium on Autonomous Decentralized Systems, 2003. ISADS 2003.},
title={A GUI approach to programming of TMO frames and design of real-time distributed computing software},
year={2003},
volume={},
number={},
pages={53-60},
abstract={An advanced high-level approach for programming of real-time distributed computing applications, the TMO (time-triggered message-triggered object) programming and specification scheme, has been enabled without creating any new language or compiler. Instead, a middleware system named the TMO Support Middleware (TMOSM) and an API that wraps around the execution support services of TMOSM have been established. An approach enabling further reduction of the labor in TMO programming is to let the programmer use a GUI to build structural frames of application TMO networks. The supporting tool called the Visual Studio for TMO (ViSTMO) consists of a GUI part for interactive design of TMO-network structures and a part for automated generation of C++ TMO code-frameworks. The TMO scheme contains mechanisms enabling efficient design of autonomy-rich structures of application systems and ViSTMO provides GUIs for programming the use of those mechanisms among others. The recent expansion and refinement of the functionality of and the implementation techniques used in ViSTMO are discussed in this paper.},
keywords={Graphical user interfaces;Distributed computing;Programming profession;Middleware;Application software;Program processors;Kernel;Object oriented modeling;Software engineering;Communication system control},
doi={10.1109/ISADS.2003.1193932},
ISSN={},
month={April},}
@INPROCEEDINGS{687751,
author={Piglowski, D. and Penaflor, B.G. and Phillips, J.C.},
booktitle={17th IEEE/NPSS Symposium Fusion Engineering (Cat. No.97CH36131)},
title={Use of the accessware interface/database software in the neutral beam control systems used by the DIII-D tokamak},
year={1997},
volume={2},
number={},
pages={819-822 vol.2},
abstract={The complexities of monitoring and controling the various DIII-D tokamak systems have always required the aid of high-speed computer resources. Recent upgrades in computer hardware to the DIII-D central operations and neutral beam computer systems have forced a redesign of the corresponding software. These newer systems cannot make use of the antiquated computer platforms of the past. Entirely new software has been written/installed to replace the old. During the design and development many newer features have been added; especially in the realm of graphical user interfaces and database management. For most of the systems involved, this required the implementation of a third party software, including a crucial package written by AccessWare Inc. of Houston Texas.},
keywords={Databases;Control systems;NIST;Tokamaks;Hardware;Switches;Graphical user interfaces;Computer aided manufacturing;Data acquisition;CAMAC},
doi={10.1109/FUSION.1997.687751},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6550468,
author={Chen, Siqin and Huang, Junfei and Gong, Yunzhan},
booktitle={2013 27th International Conference on Advanced Information Networking and Applications Workshops},
title={Static Testing as a Service on Cloud},
year={2013},
volume={},
number={},
pages={638-642},
abstract={Static testing is a form of software testing where the software needn't be executed and can find valuable software defects in the early stage of software development cycle. In contrast to dynamic testing, static testing performs code walkthrough automatically instead of the traditional manual way. It is primarily syntax checking of the code and this kind of software testing tool can work alone without software deployment under some strict hardware environment, so it's very convenient and hardware-saving. As it might be waste of costs to own various static testing tools and employ large number of technicians trained for using them skillfully, many small or medium sized enterprises may choose a third-party testing group for the whole or most of the testing work and is quite common in industry. Since static testing does little harm to the testing environment in contrast to dynamic testing, it's more convenient and easier to build and maintain a public software testing platform offering static testing service supplied by static testing tool built-in which can have many static testing jobs running at one time for different customers who can make their access via Internet. In this way, customers can cut their cost on software testing and get testing result with high quality. This paper presents a cloud-based platform architecture offering static testing service which can be fetched via popular Internet browsers such as IE and Fire fox. According to the design, customers should compress the software to be tested and then send upload this package to the platform with the help of browsers and then start testing execution step by step through the web page and finally can download testing result from remote server. This architecture now has an implementation in use, which is built on Hadoop and HBase both deployed on cluster of servers whose operation system are Windows, using Mapreduce implementation built-in Hadoop to distribute testing tasks over different servers, and with a friendly user interface web site built up by Tomcat. The static testing tool offering static testing service on this platform is called Defect Testing System (DTS), which is a mature static testing tool working on ordinary Windows OS and is installed standalone usually, developed by Beijing University of Posts and Telecommunications (BUPT).},
keywords={Servers;Cloud computing;Software testing;Prototypes;Computer architecture;software testing;static testing;dynamic testing;cloud computing;Hadoop;Mapreduce},
doi={10.1109/WAINA.2013.257},
ISSN={},
month={March},}
@INPROCEEDINGS{7077141,
author={Sharma, Richa and Biswas, K. K.},
booktitle={2014 9th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)},
title={Automated generation of test cases from logical specification of software requirements},
year={2014},
volume={},
number={},
pages={1-8},
abstract={The quality of the delivered software relies on rigorous testing performed. However, designing good test cases is a challenging task. The challenges are multi-fold and test-cases design is often delayed towards the end of implementation phase. In this paper, we propose an approach to automatically generate test cases from the logical form of requirements specifications during early phases of software development. Our approach is based on courteous logic representation of requirements. The Knowledge stored in the courteous logic predicates is used to automatically generate the test cases. We evaluate the effectiveness of our generated test-cases through case-studies.},
keywords={Testing;Software;Unified modeling language;Libraries;Null value;Industries;Semantics;Test Cases;Logical Specification;Courteous Logic;Natural Language Processing},
doi={},
ISSN={},
month={April},}
@INPROCEEDINGS{713643,
author={Gazi, V. and Moore, M. and Passino, K.M.},
booktitle={Proceedings of the 1998 IEEE International Symposium on Intelligent Control (ISIC) held jointly with IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA) Intell},
title={Real-Time Control System software for intelligent system development: experiments and an educational program},
year={1998},
volume={},
number={},
pages={102-107},
abstract={The National Institute of Standards and Technology (NIST) has been developing the "Real-Time Control Systems (RCS)" software for more than two decades and using it for the design and implementation of complex intelligent control systems. Present applications of RCS include mining, manufacturing, robotics, and autonomous vehicles. In this paper we summarize the efforts at Ohio State University to develop an educational program for the NIST RCS that includes classroom instruction, a strong laboratory component (complete with small-scale RCS implementations), and a software handbook.},
keywords={Real time systems;Control systems;System software;Intelligent systems;NIST;Intelligent manufacturing systems;Intelligent robots;Competitive intelligence;Software systems;Intelligent control},
doi={10.1109/ISIC.1998.713643},
ISSN={2158-9860},
month={Sep.},}
@INPROCEEDINGS{7281944,
author={Lindgren, Per and Lindner, Marcus and Lindner, Andreas and Pereira, David and Pinho, Luís Miguel},
booktitle={2015 IEEE 13th International Conference on Industrial Informatics (INDIN)},
title={Well-formed control flow for critical sections in RTFM-core},
year={2015},
volume={},
number={},
pages={1438-1445},
abstract={The mainstream of embedded software development as of today is dominated by C programming. To aid the development, hardware abstractions, libraries, kernels and lightweight operating systems are commonplace. Such kernels and operating systems typically impose a thread based abstraction to concurrency. However, in general thread based programming is hard, plagued by race conditions and dead-locks. For this paper we take an alternative outset in terms of a language abstraction, RTFM-core, where the system is modelled directly in terms of tasks and resources. In compliance to the Stack Resource Policy (SRP) model, the language enforces (well-formed) LIFO nesting of claimed resources, thus SRP based analysis and scheduling can be readily applied. For the execution onto bare-metal single core architectures, the rtfm-core compiler performs SRP analysis on the model and render an executable that is deadlock free and (through RTFM-kernel primitives) exploits the underlying interrupt hardware for efficient scheduling. The RTFM-core language embeds C-code and links to C-object files and libraries, and is thus applicable to the mainstream of embedded development. However, while the language enforces well-formed resource management, control flow in the embedded C-code may violate the LIFO nesting requirement. In this paper we address this issue by lifting a subset of C into the RTFM-core language allowing arbitrary control flow at the model level. In this way well-formed LIFO nesting can be enforced, and models ensured to be correct by construction. We demonstrate the feasibility by means of a prototype implementation in the rtfm-core compiler. Additionally, we develop a set of running examples and show in detail how control flow is handled at compile time and during run-time execution.},
keywords={Switches;Programming;Hardware;Kernel;Concurrent computing;Synchronization;Libraries},
doi={10.1109/INDIN.2015.7281944},
ISSN={2378-363X},
month={July},}
@ARTICLE{683742,
author={Gannon, D. and Bramley, R. and Stuckey, T. and Villacis, J. and Balasubramanian, J. and Akman, E. and Breg, F. and Diwan, S. and Govindaraju, M.},
journal={IEEE Computational Science and Engineering},
title={Developing component architectures for distributed scientific problem solving},
year={1998},
volume={5},
number={2},
pages={50-63},
abstract={Component programming models offer rapid construction for complex distributed applications, without recompiling and relinking code. This survey of the theory and design of component based software illustrates their use and utility with a prototype system for manipulating and solving large, sparse systems of equations.},
keywords={Component architectures;Problem-solving;Packaging;Linear systems;Laboratories;Programming profession;Equations;Network servers;Telescopes;Linear algebra},
doi={10.1109/99.683742},
ISSN={1558-190X},
month={April},}
@INPROCEEDINGS{886955,
author={Wills, L. and Sander, S. and Kannan, S. and Kahn, A. and Prasad, J.V.R. and Schrage, D.},
booktitle={19th DASC. 19th Digital Avionics Systems Conference. Proceedings (Cat. No.00CH37126)},
title={An open control platform for reconfigurable, distributed, hierarchical control systems},
year={2000},
volume={1},
number={},
pages={4D2/1-4D2/8 vol.1},
abstract={Complex control systems for autonomous vehicles require integrating new control algorithms with a variety of different component technologies and resources. These components are often supported on different types of hardware platforms and operating systems and often must interact in a distributed environment (e.g., in communication with a groundstation, mothership, or other UAVs in a swarm). At the same time, the configuration and integration of components must be flexible enough to allow rapid online reconfiguration and adaptation to react to environmental changes and respond to unpredictable events during flight, such as avoiding a moving obstacle or recovering from vehicle equipment failures. This paper describes an open software architecture, called the open control platform, for integrating control technologies and resources. The specific driving application is supporting autonomous control of VTOL uninhabited autonomous vehicles.},
keywords={Communication system control;Remotely operated vehicles;Unmanned aerial vehicles;Control systems;Mobile robots;Distributed control;Hardware;Operating systems;Equipment failure;Software architecture},
doi={10.1109/DASC.2000.886955},
ISSN={},
month={Oct},}
@INPROCEEDINGS{740508,
author={Bagchi, S. and Whisnant, K. and Kalbarczyk, Z. and Iyer, R.K.},
booktitle={Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281)},
title={The Chameleon infrastructure for adaptive, software implemented fault tolerance},
year={1998},
volume={},
number={},
pages={261-267},
abstract={This paper presents Chameleon, an adaptive software infrastructure for supporting different levels of availability requirements in a heterogeneous networked environment. Chameleon provides dependability through the use of ARMORs-Adaptive, Reconfigurable, and Mobile Objects for Reliability. Three broad classes of ARMORs are defined: Managers, Daemons, and Common ARMORs. Key concepts that support adaptive fault tolerance include the construction of fault tolerance execution strategies from a comprehensive set of ARMORs, the creation of ARMORs from a library of reusable basic building blocks, the dynamic adaptation to changing fault tolerance requirements, and the ability to detect and recover from errors in applications and in ARMORs.},
keywords={Fault tolerance;Application software;Electrical capacitance tomography;Read only memory;Computer architecture;NASA;High performance computing;Buildings;Redundancy;Computer networks},
doi={10.1109/RELDIS.1998.740508},
ISSN={1060-9857},
month={Oct},}
@INPROCEEDINGS{1452602,
author={Jicheng Chen and Qingdong Yao and Peng Liu and Ce Chi},
booktitle={Proceedings 7th International Conference on Signal Processing, 2004. Proceedings. ICSP '04. 2004.},
title={MD16: DSP with some RISC features for embedded system},
year={2004},
volume={1},
number={},
pages={144-147 vol.1},
abstract={To achieve the high performance/cost ratio, the idea of combining the advantages of DSP and RISC in a single architecture is a solution for the booming and versatile embedded application systems. The existent methods that appear in many literatures all focus on the construction of mixed architecture with RISC basement. Contrary to the above, a new idea of constructing architecture with DSP basement and supplementing it with some RISC features is put up in this paper. It can exert the advantages of DSP architecture by instruction level parallelization and powerful memory access capability, and obtain RISC good high-level language support by the manners of local-homogenous register set and RISC-like pipeline. To accelerate the DSP design and optimize the system, integrated design system (IDS) is presented to complete the hardware/software co-design and co-validation. Application programs can also be rapidly developed based on the integrated development environment (IDF). The behavior level simulation of the DSP system has been completed. It is given as 150 MIPS under the condition of 0.18 /spl mu/m technology library. FPGA validation is also accomplished.},
keywords={Digital signal processing;Reduced instruction set computing;Embedded system;Costs;Computer architecture;High level languages;Registers;Pipelines;Acceleration;Design optimization},
doi={10.1109/ICOSP.2004.1452602},
ISSN={},
month={Aug},}
@INPROCEEDINGS{669920,
author={Hopfner, T. and Fischer, F. and Farber, G.},
booktitle={Proceedings of the First Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing},
title={NoWait-RPC: extending ONC RPC to a fully compatible message passing system},
year={1998},
volume={},
number={},
pages={250-254},
abstract={Locally and functionally distributed applications realized on different system architectures demand a universal, portable and intuitive software utility for interprocess communication. Remote Procedure Calling (RPC) fulfills these requirements but suffers unnecessarily strict synchronization and the danger of deadlocks with complex client/server relations. Using message passing to avoid the inherent problems of RPC, however requires additional knowledge and sometimes a complete structural redesign. This paper presents NoWait-RPC, an extended but fully compatible version of SUN's Open Network Computing Group's ONC RPC, which adds message passing capabilities to form an easy to use programming environment for robust system integration. NoWait-RPC was developed to resolve potentially arising deadlocks in an already RPC-based complex application. It is designed to be a plug-and-play substitute for ONC RPC and consists of the library and the extended protocol compiler nwrpcgen. Additionally, applications using the asynchronous (non-blocking) features of NoWait-RPC may experience a major speedup compared to ONC RPC through pipelining calls to different servers. It has successfully been employed in a research programme dealing with the development of autonomous mobile robots.},
keywords={Message passing;System recovery;Application software;Computer architecture;Communication system software;Network servers;Sun;Computer networks;Programming environments;Robustness},
doi={10.1109/IPPS.1998.669920},
ISSN={1063-7133},
month={March},}
@ARTICLE{5611490,
author={Doudalis, Ioannis and Clause, James and Venkataramani, Guru and Prvulovic, Milos and Orso, Alessandro},
journal={IEEE Transactions on Computers},
title={Effective and Efficient Memory Protection Using Dynamic Tainting},
year={2012},
volume={61},
number={1},
pages={87-100},
abstract={Programs written in languages allowing direct access to memory through pointers often contain memory-related faults, which cause nondeterministic failures and security vulnerabilities. We present a new dynamic tainting technique to detect illegal memory accesses. When memory is allocated, at runtime, we taint both the memory and the corresponding pointer using the same taint mark. Taint marks are then propagated and checked every time a memory address m is accessed through a pointer p; if the associated taint marks differ, an illegal access is reported. To allow always-on checking using a low overhead, hardware-assisted implementation, we make several key technical decisions. We use a configurable, low number of reusable taint marks instead of a unique mark for each allocated area of memory, reducing the performance overhead without losing the ability to target most memory-related faults. We also define the technique at the binary level, which helps handle applications using third-party libraries whose source code is unavailable. We created a software-only prototype of our technique and simulated a hardware-assisted implementation. Our results show that 1) it identifies a large class of memory-related faults, even when using only two unique taint marks, and 2) a hardware-assisted implementation can achieve performance overheads in single-digit percentages.},
keywords={Resource management;Hardware;Prototypes;Libraries;Computational modeling;Software;Runtime;Computer systems organization;hardware/software interfaces;processor architectures;monitors.},
doi={10.1109/TC.2010.215},
ISSN={1557-9956},
month={Jan},}
@ARTICLE{9345354,
author={Calvo-Fullana, Miguel and Mox, Daniel and Pyattaev, Alexander and Fink, Jonathan and Kumar, Vijay and Ribeiro, Alejandro},
journal={IEEE Robotics and Automation Letters},
title={ROS-NetSim: A Framework for the Integration of Robotic and Network Simulators},
year={2021},
volume={6},
number={2},
pages={1120-1127},
abstract={Multi-agent systems play an important role in modern robotics. Due to the nature of these systems, coordination among agents via communication is frequently necessary. Indeed, Perception-Action-Communication (PAC) loops, or Perception-Action loops closed over a communication channel, are a critical component of multi-robot systems. However, we lack appropriate tools for simulating PAC loops. To that end, in this letter, we introduce ROS-NetSim, a ROS package that acts as an interface between robotic and network simulators. With ROS-NetSim, we can attain high-fidelity representations of both robotic and network interactions by accurately simulating the PAC loop. Our proposed approach is lightweight, modular and adaptive. Furthermore, it can be used with many available network and physics simulators by making use of our proposed interface. In summary, ROS-NetSim is (i) Transparent to the ROS target application, (ii) Agnostic to the specific network and physics simulator being used, and (iii) Tunable in fidelity and complexity. As part of our contribution, we have made available an open-source implementation of ROS-NetSim to the community.},
keywords={Physics;Robot kinematics;Picture archiving and communication systems;Synchronization;Robot sensing systems;Communication channels;Wireless sensor networks;Multi-robot systems;networked robots;methods and tools for robot system design;software architecture for robotic and automation},
doi={10.1109/LRA.2021.3056347},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{5282266,
author={Tan, H.M. and Vowles, D.J. and Zivanovic, R.},
booktitle={2009 IEEE Bucharest PowerTech},
title={Implementation of the Enhanced Binary-SIME method for finding transient stability limits with PSS/E™},
year={2009},
volume={},
number={},
pages={1-8},
abstract={The Enhanced Binary-Single Machine Infinite bus Equivalent (Binary-SIME) method is an enhancement of the SIME method. It provides a robust and flexible approach to searching for the transient stability limits (TSLs) in a fully detailed model of a multi-machine power system. This paper describes the modular implementation of the Binary-SIME method with the PSS/Etrade time domain simulation package. Extension to incorporate alternative search approaches is facilitated by the modular architecture. The Binary-SIME search implementation is applied to the IEEE simplified model of the Australian power system to search for power transfer limits (PTLs) and critical clearing times (CCTs). The Binary-SIME method is compared with the binary search method in the search for TSLs.},
keywords={Power system transients;Power system modeling;Power system stability;Power system simulation;Search methods;Robust stability;Australia;Power system reliability;Power system security;Software algorithms;Power System Transient Stability;Reliability;Security Assessment;Simulation Software},
doi={10.1109/PTC.2009.5282266},
ISSN={},
month={June},}
@INPROCEEDINGS{9138974,
author={Ham, Tae Jun and Bruns-Smith, David and Sweeney, Brendan and Lee, Yejin and Seo, Seong Hoon and Song, U Gyeong and Oh, Young H. and Asanovic, Krste and Lee, Jae W. and Wills, Lisa Wu},
booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
title={Genesis: A Hardware Acceleration Framework for Genomic Data Analysis},
year={2020},
volume={},
number={},
pages={254-267},
abstract={In this paper, we describe our vision to accelerate algorithms in the domain of genomic data analysis by proposing a framework called Genesis (genome analysis) that contains an interface and an implementation of a system that processes genomic data efficiently. This framework can be deployed in the cloud and exploit the FPGAs-as-a-service paradigm to provide cost-efficient secondary DNA analysis. We propose conceptualizing genomic reads and associated read attributes as a very large relational database and using extended SQL as a domain-specific language to construct queries that form various data manipulation operations. To accelerate such queries, we design a Genesis hardware library which consists of primitive hardware modules that can be composed to construct a dataflow architecture specialized for those queries. As a proof of concept for the Genesis framework, we present the architecture and the hardware implementation of several genomic analysis stages in the secondary analysis pipeline corresponding to the best known software analysis toolkit, GATK4 workflow proposed by the Broad Institute. We walk through the construction of genomic data analysis operations using a sequence of SQL-style queries and show how Genesis hardware library modules can be utilized to construct the hardware pipelines designed to accelerate such queries. We exploit parallelism and data reuse by utilizing a dataflow architecture along with the use of on-chip scratchpads as well as non-blocking APIs to manage the accelerators, allowing concurrent execution of the accelerator and the host. Our accelerated system deployed on the cloud FPGA performs up to 19.3× better than GATK4 running on a commodity multi-core Xeon server and obtains up to 15× better cost savings. We believe that if a software algorithm can be mapped onto a hardware library to utilize the underlying accelerator(s) using an already-standardized software interface such as SQL, while allowing the efficient mapping of such interface to primitive hardware modules as we have demonstrated here, it will expedite the acceleration of domainspecific algorithms and allow the easy adaptation of algorithm changes.},
keywords={genome sequencing;genomic data analysis;hardware accelerator;FPGA;SQL},
doi={10.1109/ISCA45697.2020.00031},
ISSN={},
month={May},}
@INPROCEEDINGS{4017492,
author={Wenwei, Chen and Jinyi, Zhang and Jiao, Li and Xiaojun, Ren and Jiwei, Liu},
booktitle={2005 Conference on High Density Microsystem Design and Packaging and Component Failure Analysis},
title={Study On a Mixed Verification Strategy for IP-Based SoC Design},
year={2005},
volume={},
number={},
pages={1-4},
abstract={The demands for more powerful products and the huge capacity of today' s silicon technology move system-on-chip (SoC) designs from the leading-age to mainstream design practice. The one at the very top of the list of challenges to be solved for SoC design is verification. General agreement among many observers is that verification consumes at least 70 percent of whole design percent. SoC verification involves in multi-levels: IP level verification, chip level verification, and hardware/software (HW/SW) co-verification. The last one is the key point to the whole verification process, and some of EDA vendors have provided several EDA tools for HW/SW co-verification. In this paper, the author analyses the architecture of co-verification and the weakness of existing EDA tools, then presents a practical verification strategy based on FPGA, which is more flexible and convenient, and more efficient than traditional verification methods whose hardware and software verification are separate. An experimental result, VAD (video add data) SoC verification, is given as well finally},
keywords={System-on-a-chip;Electronic design automation and methodology;Hardware;Silicon;Application specific integrated circuits;Testing;Microelectronics;Research and development;Design engineering;Power engineering and energy;SoC;IP core;Verification platfrom;HW/SW co-verification},
doi={10.1109/HDP.2005.251451},
ISSN={},
month={June},}
@INPROCEEDINGS{9177553,
author={D’Alterio, Pasquale and Garibaldi, Jonathan M. and John, Robert I. and Wagner, Christian},
booktitle={2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
title={Juzzy Constrained: Software for Constrained Interval Type-2 Fuzzy Sets and Systems in Java},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Constrained interval type-2 (CIT2) fuzzy sets are a class of type-2 fuzzy sets that has been recently proposed as a way to extend type-1 membership functions to interval type-2 (IT2) while keeping a semantic connection between the IT2 fuzzy set and the concept it models. Recent work has shown how their mathematical properties can be used to design CIT2 fuzzy logic systems that are able to provide explanations for their outputs. Although the CIT2 representation can be a valuable alternative to the IT2 one, no software library for their implementation is available for the research community. The aim of this paper is to introduce a new Java library, Juzzy Constrained, that has been developed as an extension of the popular type-1 and type-2 Java toolkit Juzzy, adding support for CIT2 sets and systems. Throughout the paper, the main classes and the structure of the new library are described, together with a working example that illustrates how to build a CIT2 fuzzy system from scratch and how it can be used to produce explanations for the output.},
keywords={Fuzzy sets;Fuzzy logic;Java;Generators;Software libraries;Shape;Juzzy;Constrained Interval Type-2;XAI;type-2 fuzzy logic},
doi={10.1109/FUZZ48607.2020.9177553},
ISSN={1558-4739},
month={July},}
@INPROCEEDINGS{7774908,
author={Gao, Mingyu and Wang, Weihuan and Zeng, Yu and He, Zhiwei and Gao, Cong},
booktitle={2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC)},
title={A Rapier Loom HMI System Based on an Easy Cross-Platform GUI Software},
year={2016},
volume={},
number={},
pages={874-878},
abstract={The rapier loom HMI system is a kind of device which is used to send command to the loom, monitor the running status and store the loom data. Its main functions are providing a friendly human machine interface, setting the loom parameter, communicating in real time and storing the machine data necessary. This paper introduces a rapier loom HMI implementation scheme. It runs on the MCU of STM32F429IGT6. In the aspect of software design, this paper comes up with a cross-platform GUI which is easy developed and has good portability. The experimental results show that the HMI system can meet the industrial requirement on indicating the rapier loom and get feedback of the machine.},
keywords={Graphical user interfaces;Real-time systems;Operating systems;Libraries;Graphics;Color;embedded;real-time;rapier loom;cross-platform;GUI},
doi={10.1109/IMCCC.2016.101},
ISSN={},
month={July},}
@INPROCEEDINGS{4053003,
author={Clarke, D.A.W.},
booktitle={2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology},
title={Commercial Experience with Agent-Oriented Software Engineering},
year={2006},
volume={},
number={},
pages={730-736},
abstract={Although there have been a number of attempts at defining agent-oriented design methodologies, none of these have been proven successful in industrial use. Over a two year period, the author was involved in a variety of commercial projects and proofs-ofconcept using a product that offered an enterprise-quality implementation of BDI-agent principles. During that time, a methodology was developed to make the most effective use of the product. This paper describes the essential features of the methodology and records the results of its successful use in five selected projects in very different application domains.},
keywords={Software engineering;Java;Design methodology;Aerospace and Electronic Systems Society;Runtime environment;Packaging;Business;Australia;Computer industry;Context-aware services},
doi={10.1109/IAT.2006.48},
ISSN={},
month={Dec},}
@INPROCEEDINGS{493423,
author={Henninger, S.},
booktitle={Proceedings of IEEE 18th International Conference on Software Engineering},
title={Supporting the construction and evolution of component repositories},
year={1996},
volume={},
number={},
pages={279-288},
abstract={Repositories must be designed to meet the evolving and dynamic needs of software development organizations. Current software repository methods rely heavily on classification, which exacerbates acquisition and evolution problems by requiring costly classification and domain analysis efforts before a repository can be used effectively. This paper outlines an approach in which minimal initial structure is used to effectively find relevant software components while methods are employed to incrementally improve repository structures. The approach is demonstrated through PEEL, a tool to semi-automatically identify reusable components, and CodeFinder, a retrieval system that compensates for the lack of explicit knowledge structures through spreading activation retrieval and allows component representations to be incrementally improved while users are searching for information. The combination of these techniques yields a flexible software repository that minimizes up-front costs and improves its retrieval effectiveness as developers use it to find reusable software artifacts.},
keywords={Software reusability;Indexing;Information retrieval;Investments;Computer science;Design engineering;Programming;Costs;Software libraries;Relational databases},
doi={10.1109/ICSE.1996.493423},
ISSN={0270-5257},
month={March},}
@INPROCEEDINGS{4145047,
author={De Lucia, Andrea and Deufemia, Vincenzo and Gravino, Carmine and Risi, Michele},
booktitle={11th European Conference on Software Maintenance and Reengineering (CSMR'07)},
title={A Two Phase Approach to Design Pattern Recovery},
year={2007},
volume={},
number={},
pages={297-306},
abstract={In this paper we present a two phase approach to the recovery of structural design pattern. In the first phase, the design pattern instances are identified at a course-grained level by considering the design structure only and using a visual language parsing technique. Then, the identified candidate patterns are validated by a fine-grained source code analysis phase. The latter phase is an enhancement of a previous approach developed by the authors aiming at improving the results of precision and time performances. The retrieval effectiveness of the approach is assessed by applying the recovery technique on four software systems.},
keywords={Software systems;Pattern analysis;Data mining;Software libraries;Collaboration;Object oriented modeling;Documentation;Software performance;Performance analysis;Stress},
doi={10.1109/CSMR.2007.10},
ISSN={1534-5351},
month={March},}
@INPROCEEDINGS{6811248,
author={Jain, Adarsh and Sharma, Vinod and Amrutur, Bharadwaj},
booktitle={2014 Twentieth National Conference on Communications (NCC)},
title={Soft real time implementation of a Cognitive Radio testbed for frequency hopping primary satisfying QoS requirements},
year={2014},
volume={},
number={},
pages={1-6},
abstract={The Cognitive Radio (CR) is a promising technology which provides a novel way to subjugate the issue of spectrum underutilization caused due to the fixed spectrum assignment policies. In this paper we report the design and implementation of a soft-real time CR MAC, consisting of multiple secondary users, in a frequency hopping (FH) primary scenario. This MAC is capable of sensing the spectrum and dynamically allocating the available frequency bands to multiple CR users based on their QoS requirements. As the primary is continuously hopping, a method has also been implemented to detect the hop instant of the primary network. Synchronization usually requires real time support, however we have been able to achieve this with a soft-real time technique which enables a fully software implementation of CR MAC layer. We demonstrate the wireless transmission and reception of video over this CR testbed through opportunistic spectrum access. The experiments carried out use an open source software defined radio package called GNU Radio and a basic radio hardware component USRP.},
keywords={Sensors;Vectors;Delays;Streaming media;Bandwidth;Cognitive radio;Cognitive radio (CR);Frequency hopping (FH);GNU Radio;Opportunistic Spectrum Access;Python;Spectrum sensing;Universal Software Radio Peripheral (USRP)},
doi={10.1109/NCC.2014.6811248},
ISSN={},
month={Feb},}
@INPROCEEDINGS{9110373,
author={Vasilakos, Xenofon and Köksal, Berkay and Izaldi, Dwi Hartati and Nikaein, Navid and Schmidt, Robert and Ferdosian, Nasim and Sari, Riri Fitri and Cheng, Ray-Guang},
booktitle={NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium},
title={ElasticSDK: A Monitoring Software Development Kit for enabling Data-driven Management and Control in 5G},
year={2020},
volume={},
number={},
pages={1-7},
abstract={5G networks generate massive (quasi-) real-time data streams that different network apps can exploit to implement sophisticated single- or cross-domain control and management logic. This paper presents ElasticSDK, a Software Development Kit specially designed to abstract the development and chaining of such agile 5G monitoring apps for the control, management, and coordination of the underlying 5G network heterogeneous modules. Custom apps can collect, incrementally process and further expose flows in a flexible Pub/Sub fashion via appropriate SDK API calls, thus sharing both raw and complex data flows among themselves. Furthermore, the design of ElasticSDK allows respecting typical 5G data ownership and privacy models, as desired by the different 5G stakeholders ranging from physical infrastructure providers up to service providers over slicing. Finally, we provide two important contributions to the 5G open-source research community: (i) a RAN monitoring prototype implementation over the ElasticSearch and FlexRAN platforms that allows to demonstrate ElasticSDK app development and capturing hierarchical control features of typical SDN-enabled 5G architectures, and (ii) a first-ever publicly available dataset of realistic 5G RAN monitoring traces.},
keywords={5G mobile communication;Prototypes;Data models;Real-time systems;Distance measurement;Delays;Stakeholders;5G mobile communication;network monitoring;SDK},
doi={10.1109/NOMS47738.2020.9110373},
ISSN={2374-9709},
month={April},}
@INPROCEEDINGS{9904807,
author={Sudvarg, Marion and Gill, Chris},
booktitle={2022 IEEE 28th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)},
title={A Concurrency Framework for Priority-Aware Intercomponent Requests in CAmkES on seL4},
year={2022},
volume={},
number={},
pages={1-10},
abstract={Component-based design can encapsulate and isolate state and the operations on it, but timing semantics crosscut these boundaries when a real-time task’s control flow spans multiple components. Under priority-based scheduling, inter-component control flow should be coupled with priority information, so that task execution can be prioritized appropriately end-to-end. However, the CAmkES component architecture for the seL4 microkernel does not adequately support priority propagation across intercomponent requests: component interfaces are bound to threads that execute at fixed priorities provided at compile-time in the component specification. In this paper, we present a new library for CAmkES with a thread model that supports (1) multiple concurrent requests to the same component endpoint; (2) propagation and enforcement of priority metadata, such that those requests are appropriately prioritized; and (3) implementations of Non-Preemptive Critical Sections, the Immediate Priority Ceiling Protocol and the Priority Inheritance Protocol for components encapsulating critical sections of exclusive access to a shared resource. We measure overheads and blocking times for these new features and use existing theory to perform schedulability analysis. Evaluations on both Intel x86 and ARM platforms show that our new library allows CAmkES to provide suitable end-to-end timing for real-time systems.},
keywords={Concurrent computing;Component architectures;Protocols;Program processors;Semantics;Metadata;Real-time systems;real-time-systems;component-based-software-engineering;concurrency;shared-resource-access;priority-based-scheduling;component-frameworks},
doi={10.1109/RTCSA55878.2022.00007},
ISSN={2325-1301},
month={Aug},}
@INPROCEEDINGS{6558293,
author={Deshpande, C. V. and Shetye, A. S. and Ghuge, S. S. and Aghav, J. V.},
booktitle={2013 IEEE Conference on Information & Communication Technologies},
title={Towards a WebJDK: Extending openJDK 7 for client file system access over cloud},
year={2013},
volume={},
number={},
pages={1253-1258},
abstract={Deploying Java applications to the cloud with access rights to the local client's file-system involves modification of the Java application code to use non-standard APIs for client's filesystem access. With the steady migration of existing technologies towards a web-aware implementation, this paper presents the file access sub-system of a potential WebJDK. WebJDK is designed to provide an implementation of the OpenJDK that runs in the cloud and interacts directly with the client's browser. The WebJDK implementation and its sub-systems are based on the Caciocavallo project by OpenJDK. The proposed file access subsystem implementation leverages the File-System API provided by HTML5. This allows the use of standard Java APIs for accessing files of clients with variety of underlying native file systems.},
keywords={Java;Browsers;Servers;Graphics;Graphical user interfaces;HTML;Conferences;Softwares;Open source software;File sytems;Web services},
doi={10.1109/CICT.2013.6558293},
ISSN={},
month={April},}
@INPROCEEDINGS{7428495,
author={Gomez, Victor Manuel and Rengifo, Carlos Felipe},
booktitle={2015 IEEE Thirty Fifth Central American and Panama Convention (CONCAPAN XXXV)},
title={A software tool for generating patterns of energy consumption in residential customers},
year={2015},
volume={},
number={},
pages={1-6},
abstract={In most of the developing countries, economic losses arising from the theft of power can difficult the finances and the demand planning of marketers and lead to the collapse of the electrical system of a country. This has led considerable efforts in the search for solutions to detect frauds of energy based on the application of statistical algorithms for the analysis of load patterns. The lack of energy consumption patterns that, in advance, allow to know which users are licit and which are fraudulent is the main difficulty for the determination of the effectiveness in the use of these models. This paper presents the results obtained in the process of design and implementation of a software platform that allows generating consumption patterns corresponding to users within the groups previously mentioned. The MATLAB development environment is used to manage the application and the information about the consumption habits.},
keywords={MATLAB;Energy consumption;Silicon compounds;Electronic mail;Visualization;Manuals;Patterns;Power;Power consumption},
doi={10.1109/CONCAPAN.2015.7428495},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8576187,
author={Jackisch, Florian},
booktitle={2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)},
title={PHYSIM - A Physical Layer Simulation Software},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Modern communication systems use different physical layer (PHY) techniques with diverse performance characteristics. Each PHY configuration option introduces trade-offs for spectral efficiency, robustness, out-of-band emissions, peak-to-average power ratio, and implementation complexity. This paper introduces the cross-platform PHYSIM software framework. PHYSIM can compare established modulations like orthogonal frequency-division multiplexing (OFDM) with modern alternatives, e.g., filter bank multi-carrier, universal filtered multi-carrier, and filtered-OFDM. Additionally, PHYSIM can draw performance comparisons between new coding schemes like polar codes and more established methods like low-density parity-check codes and turbo codes. The PHYSIM graphical user interface (GUI) assists in the design of physical layer parameters. The GUI offers plots to compare the performance of PHY configurations in transmission channels. The plots display the transmitted spectra, the received constellation diagrams, as well as the modulation, bit, and frame error ratios. Hence, the PHYSIM simulation framework assists the designer of a physical layer system and helps to find PHY settings for a target application.},
keywords={Modulation;Physical layer;OFDM;Graphical user interfaces;Forward error correction;Parity check codes;Transmitters},
doi={10.1109/ICCE-Berlin.2018.8576187},
ISSN={2166-6822},
month={Sep.},}
@ARTICLE{526898,
author={Kai Hwang and Zhiwei Xu},
journal={IEEE Signal Processing Magazine},
title={Scalable parallel computers for real-time signal processing},
year={1996},
volume={13},
number={4},
pages={50-66},
abstract={We assess the state-of-the-art technology in massively parallel processors (MPPs) and their variations in different architectural platforms. Architectural and programming issues are identified in using MPPs for time-critical applications such as adaptive radar signal processing. We review the enabling technologies. These include high-performance CPU chips and system interconnects, distributed memory architectures, and various latency hiding mechanisms. We characterize the concept of scalability in three areas: resources, applications, and technology. Scalable performance attributes are analytically defined. Then we compare MPPs with symmetric multiprocessors (SMPs) and clusters of workstations (COWs). The purpose is to reveal their capabilities, limits, and effectiveness in signal processing. We evaluate the IBM SP2 at MHPCC, the Intel Paragon at SDSC, the Gray T3D at Gray Eagan Center, and the Gray T3E and ASCI TeraFLOP system proposed by Intel. On the software and programming side, we evaluate existing parallel programming environments, including the models, languages, compilers, software tools, and operating systems. Some guidelines for program parallelization are provided. We examine data-parallel, shared-variable, message-passing, and implicit programming models. Communication functions and their performance overhead are discussed. Available software tools and communication libraries are also introduced.},
keywords={Concurrent computing;Signal processing;Software tools;Radar signal processing;Parallel programming;Adaptive signal processing;Time factors;Application software;Central Processing Unit;Memory architecture},
doi={10.1109/79.526898},
ISSN={1558-0792},
month={July},}
@INPROCEEDINGS{7389672,
author={Chen, Pin-Jui and Chen, Yen-Wen},
booktitle={2015 International Carnahan Conference on Security Technology (ICCST)},
title={Implementation of SDN based network intrusion detection and prevention system},
year={2015},
volume={},
number={},
pages={141-146},
abstract={In recent years, the rise of software-defined networks (SDN) have made network control more flexible, easier to set up and manage, and have provided a stronger ability to adapt to the changing demands of application development and network conditions. The network becomes easier to maintain, but also achieves improved security as a result of SDN. The architecture of SDN is designed for Control Plane and Forwarding Plane separation and uses open APIs to realize programmable control. SDN allows for the importing of third-party applications to improve network service, or even provide a new network service. In this paper, we present a defense mechanism, which can find attack packets previously identified through the Sniffer function, and once the abnormal flow is found, the protection mechanism of the Firewall function will be activated. For the capture of the packets, available libraries will be used to determine the properties and contents of the malicious packet, and to anticipate any possible attacks. Through the prediction of all latent malicious behaviors, our new defense algorithm can prevent potential losses like system failures or crashes and reduce the risk of being attacked.},
keywords={Ports (Computers);Operating systems;Firewalls (computing);Control systems;Routing;Firewall;Packet Sniffer;Software Defined Networks;SDN;OpenFlow;Controller;Defense Mechanism},
doi={10.1109/CCST.2015.7389672},
ISSN={2153-0742},
month={Sep.},}
@INPROCEEDINGS{801197,
author={Malekpour, M. and Torres, W.},
booktitle={Proceedings of the 1999 IEEE International Conference on Control Applications (Cat. No.99CH36328)},
title={Characterization of a recoverable flight control computer system},
year={1999},
volume={2},
number={},
pages={1519-1524 vol. 2},
abstract={The design and development of a closed-loop system to study and evaluate the performance of the Honeywell Recoverable Computer System (RCS) in electromagnetic environments (EME) is presented. The development of a Windows-based software package to handle the time-critical communication of data and commands between the RCS and flight simulation code in real-time, while meeting the stringent hard deadlines is also submitted. The performance results of the RCS and characteristics of its upset recovery scheme while exercising flight control laws under ideal conditions as well as in the presence of electromagnetic fields are also discussed.},
keywords={Aerospace control;Protection;Aircraft;NASA;Aerospace electronics;Computer architecture;Aerospace simulation;Electromagnetic fields;Electromagnetic radiation;Prototypes},
doi={10.1109/CCA.1999.801197},
ISSN={},
month={Aug},}
@INPROCEEDINGS{4755907,
author={Wissink, Andrew and Shende, Sameer},
booktitle={2008 DoD HPCMP Users Group Conference},
title={Performance Evaluation of the Multi-language Helios Rotorcraft Simulation Software},
year={2008},
volume={},
number={},
pages={442-447},
abstract={This paper describes application of the TAU (Tuning and Analysis Utilities) performance systemreg within Helios, a code being developed for high-fidelity modeling of rotorcraft aero and structural dynamics. TAU provides information about both single processor and parallel scaling performance. Helios consists of multiple modules written in different languages - FORTRAN90, C, and C++ - that are integrated through a high-level, Python-based infrastructure. This loosely-connected implementation has the advantage that each module can be developed separately from one another, but as with any parallel code, a single poor-performing module can hinder the performance and scalability of the suite as a whole. Although Helios is still in the early stages of development, integration with TAU provides a way to investigate performance when the design can be most influenced. The paper discusses the different levels of instrumentation, from run-time at the Python level for a high-level description, to automatic compile-time instrumentation for a more detailed breakdown. In both cases, the instrumentation is automatic, requiring no manual insertion of timers on the part of the developer. We report how these tools were used to diagnose single processor performance and parallel scaling issues.},
keywords={Software performance;Instruments;Performance analysis;Computational fluid dynamics;Runtime;Analytical models;NASA;Packaging;Physics;Application software},
doi={10.1109/DoD.HPCMP.UGC.2008.44},
ISSN={},
month={July},}
@INPROCEEDINGS{5431717,
author={Piccioni, Marco and Orioly, Manuel and Meyer, Bertrand and Schneider, Teseo},
booktitle={2009 IEEE/ACM International Conference on Automated Software Engineering},
title={An IDE-based, Integrated Solution to Schema Evolution of Object-Oriented Software},
year={2009},
volume={},
number={},
pages={650-654},
abstract={With the wide support for serialization in object-oriented programming languages, persistent objects have become common place. Retrieving previously ¿persisted¿ objects from classes whose schema changed is however difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses this issues through an IDE-based approach that handles schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of any corrupt objects. This article describes the principles behind invariant-safe schema evolution,and the design and implementation of the ESCHER system.},
keywords={Java;Packaging;Object oriented modeling;Software engineering;Computer languages;Runtime;Robustness;versioning;refactoring;persistence;serialization},
doi={10.1109/ASE.2009.100},
ISSN={1938-4300},
month={Nov},}
@INPROCEEDINGS{1332101,
author={Mills, A.K. and Chappell, W.J.},
booktitle={IEEE Antennas and Propagation Society Symposium, 2004.},
title={On the use of commercial FEM electromagnetic software in an undergraduate curriculum},
year={2004},
volume={3},
number={},
pages={3365-3368 Vol.3},
abstract={We explore the use of finite element method software as an enabling tool for an undergraduate electromagnetic curriculum. The software is used in upper level undergraduate courses taken after the basic electromagnetics courses. The advantages and disadvantages discovered during the implementation of this software are discussed and the driving factors for the design of "virtual labs" are introduced.},
keywords={Visualization;Software packages;Buildings;Computational modeling;Milling machines;Nanotechnology;Application software;Circuits;Standards development;Software tools},
doi={10.1109/APS.2004.1332101},
ISSN={},
month={June},}
@INPROCEEDINGS{7344225,
author={Ureel, Leo C. and Wallace, Charles},
booktitle={2015 IEEE Frontiers in Education Conference (FIE)},
title={WebTA: Automated iterative critique of student programming assignments},
year={2015},
volume={},
number={},
pages={1-9},
abstract={We introduce an interactive tool called WebTA that facilitates learning through automatic critique of student source code. Our tool provides immediate feedback to students and gives them experience with test-driven development. Students receive the benefits of cognitive apprenticeship through the feedback they receive in the tool. This facilitates tight, productive cycles of inquiry, critique and learning. WebTA compiles each student submission and executes it over a series of shakedown tests. Immediate feedback is given concerning errors and warnings, coupled with suggestions for debugging. The tool performs a textual analysis of the students source code and critiques programming style based on standard programming guidelines. To encourage inquiry through test-driven development, edge-case coverage, and API compliance, students develop and submit their own tests to be evaluated by the software. We report on use of WebTA in one first-year programming course and one second-year data structures course. Lab and assignment scores have improved with WebTA, and student comments attest to the effectiveness of the tool. Preliminary results indicate students receive higher grades with WebTA. One area with mixed results is WebTAs analysis of student developed JUnit tests; this feature improved API compliance but reduced edge-case testing. With these successful initial results, we offer suggestions for future development.},
keywords={Software;Testing;Education;Java;Programming profession},
doi={10.1109/FIE.2015.7344225},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1592604,
author={Keahey, K. and Gannon, D.},
booktitle={SC '97: Proceedings of the 1997 ACM/IEEE Conference on Supercomputing},
title={PARDIS: CORBA-based Architecture for Application-Level Parallel Distributed Computation},
year={1997},
volume={},
number={},
pages={23-23},
abstract={We describe the architecture and programming abstractions of PARDIS, a system based on ideas underlying the Common Object Request Broker Architecture (CORBA). PARDIS provides a distributed environment in which objects representing data-parallel computation, as well as non-parallel objects present in parallel programs, can interact across platforms and software systems. Each of these objects represents a small, encapsulated application that can be used as a building block in the construction of distributed metaapplications. We will present examples of building such metaapplications with PARDIS, and show their performance in distributed systems combining the computational power of different multi-processor architectures.},
keywords={Computer architecture;Computer applications;Concurrent computing;Distributed computing;Application software;Programming profession;Software systems;Software packages;Yarn;Computer science},
doi={10.1145/509593.509616},
ISSN={},
month={Nov},}
@INPROCEEDINGS{5951998,
author={Muto, Yuko and Okano, Kozo and Kusumoto, Shinji},
booktitle={2011 IEEE Ninth International Symposium on Parallel and Distributed Processing with Applications Workshops},
title={A Visualization Technique for the Passage Rates of Unit Testing and Static Checking with Caller-Callee Relationships},
year={2011},
volume={},
number={},
pages={336-341},
abstract={Software visualization has attracted lots of attention. The techniques fall into two categories: visualization of software component relationships and visualization of software metrics. We propose a new hybrid method based on both of the two categories. The proposed method visualizes coincidence between specification and implementation from two aspects: static checking and ordinal testing by test suites. Each of the verification is performed in a method or function basis (unit testing). In our method, each ratio of the coincidence is shown by pie charts which represent classes of the target software. Whole software is represented in a weighted digraph structure. We have prototyped a tool implemented our proposed method. We have evaluated the availability of the proposed method by applying the tool to two kinds of software: Warehouse Management Program, and a telephone directory management program. As a result, we conclude that the proposed method shows informative results.},
keywords={Testing;Visualization;Software;Measurement;Java;Libraries;Computer bugs;unit testing;static checking;ESC/Java2;software quality;visualization},
doi={10.1109/ISPAW.2011.54},
ISSN={},
month={May},}
@INPROCEEDINGS{9652940,
author={Wang, Xinda and Wang, Shu and Feng, Pengbin and Sun, Kun and Jajodia, Sushil and Benchaaboun, Sanae and Geck, Frank},
booktitle={MILCOM 2021 - 2021 IEEE Military Communications Conference (MILCOM)},
title={PatchRNN: A Deep Learning-Based System for Security Patch Identification},
year={2021},
volume={},
number={},
pages={595-600},
abstract={With the increasing usage of open-source software (OSS) components, vulnerabilities embedded within them are propagated to a huge number of underlying applications. In practice, the timely application of security patches in downstream software is challenging. The main reason is that such patches do not explicitly indicate their security impacts in the documentation, which would be difficult to recognize for software maintainers and users. However, attackers can still identify these “secret” security patches by analyzing the source code and generate corresponding exploits to compromise not only unpatched versions of the current software, but also other similar software packages that may contain the same vulnerability due to code cloning or similar design/implementation logic. Therefore, it is critical to identify these secret security patches to enable timely fixes. To this end, we propose a deep learning-based defense system called PatchRNN to automatically identify secret security patches in OSS. Besides considering descriptive keywords in the commit message (i.e., at the text level), we leverage both syntactic and semantic features at the source-code level. To evaluate the performance of our system, we apply it on a large-scale real-world patch dataset and conduct a case study on a popular open-source web server software - NGINX. Experimental results show that the PatchRNN can successfully detect secret security patches with a low false positive rate.},
keywords={Military communication;Codes;Software packages;Semantics;Documentation;Syntactics;Software;security patch;open source software;deep learning;vulnerability mitigation},
doi={10.1109/MILCOM52596.2021.9652940},
ISSN={2155-7586},
month={Nov},}
@INPROCEEDINGS{4449392,
author={Sinha, P. and Stiehl, K. R. and Huo, E. S. and Oyebode, O. A. and Dokov, R. P. and Chin, S. J. and Price, R. E. and Larson, R. W.},
booktitle={OCEANS 2007},
title={Design of a Modular, Compact, Multi-Role Remotely Operated Vehicle for Sheltered Water Operations},
year={2007},
volume={},
number={},
pages={1-7},
abstract={Since its founding in 2003, the MIT ROV Team has been participating in the Marine Advanced Technology Education (MATE) Center's International ROV Competition. This year's challenges, in recognition of the International Polar Year, include operations in environments made to resemble those found in the Polar Regions. This includes working in currents, under ice sheets and in simulated open ocean environments. The three missions, each approximately 15 minutes long and in depths of up to 20 feet, include recovery and deployment of science packages and related support structures, the collection of faunal samples and repair and maintenance work at stations under water. In keeping with its vision to look beyond the competition and push the envelope, the MIT ROV Team built on the lessons learnt during the design, construction and operation of its fourth generation ROV, MTHR, to design MIT ROV 5.0, a compact, powerful, highly maneuverable and modular robot capable of not only participating in the competition, but also performing a variety of tasks in the open environment. MIT ROV 5.0 was primarily designed to be an exploration robot meant to operate in sheltered waters, that is, with currents below one knot, lack of powerful localized turbulent flow and the absence of highly corrosive materials. The major design requirements included ease of operation, including setup, maintenance, dives and recovery, modularity, to allow quick reconfiguration to suit a variety of missions, robustness, so that it would stand up to the rigors of the open environment, small size, for greater maneuverability and low cost, for ease of reproduction. We came up with a robot that was slightly bigger than MTHR with two modular payload bays with standard connectors that could accommodate mission-specific packages. The control box contains custom designed PC-104 size circuit boards that can support a number of different actuators and thrusters and adequate space for NiMH or LiON battery packs, which can both be used. Each board can be switched easily due to extensive use of standard sizes and connection interfaces. The frame uses high strength ABS plastic side plates with LEXAN cross-struts. The tether is a single strand Kevlar-jacketed fiber-optic cable with a Kevlar support string for added security. This tether can be replaced by a regular Ethernet or CAT-V cable is the need arises. Thus the vehicle is truly modular in every way. The MIT ROV 5.0 system as designed is very easy to use. The control software can be run off virtually any computer, using the keyboard or a standard USB joystick. The vehicle itself connects to the computer using a USB-serial interface. The entire system can be setup and ready to go in under five minutes. An on-board guidance system consisting of an inertial navigation unit and a magnetic compass provide heading, attitude and position data, as well as performing station-keeping functions, allowing effective operation in currents and low-visibility conditions. MIT ROV 5.0 can also carry several different types of sensors, such as temperature, pressure and salinity, not to mention a combination of infra-red and color cameras. Payloads currently being prototyped include precision deployment and recovery modules, a sample collection package, instrument bays and an articulated manipulator arm. This capability coupled with a production cost as low as $3,000 makes MIT ROV 5.0 an extremely valuable platform for exploration, research, education, environmental monitoring and small scale repair and recovery work.},
keywords={Remotely operated vehicles;Packaging;Costs;Payloads;Temperature sensors;Educational technology;Marine technology;Ice;Oceans;Modular construction},
doi={10.1109/OCEANS.2007.4449392},
ISSN={0197-7385},
month={Sep.},}
@INPROCEEDINGS{5642079,
author={Rintaluoma, Tero and Silvén, Olli},
booktitle={2010 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation},
title={SIMD performance in software based mobile video coding},
year={2010},
volume={},
number={},
pages={79-85},
abstract={Most video applications use specific application programming interfaces to achieve the desired functionalities. Implementing interface backends with hardware is often too expensive for low-end mobile devices, so most of the devices cope with highly optimized software implementations that employ special instruction sets. The most common approach is the utilization of SIMD processing units such as ARM NEON or Intel WMMX in mobile application processors. Fully utilizing the potential benefits of such instruction sets usually means tedious assembly coding even if vectorizing compilers have improved lately. In addition, low level APIs such as OpenMax DL have been made available to offer a standardized interface for accelerated codec functionalities. In this paper we present optimization methods and results from using a NEON instruction set and OpenMax DL API for MPEG-4 and H.264 video encoding and decoding. Although these technologies provide for significant speed-ups and reduce the burden of application designers, the serial bit stream processing bottleneck remains to be solved.},
keywords={Decoding;Optimization;Transform coding;Program processors;Encoding;Video codecs},
doi={10.1109/ICSAMOS.2010.5642079},
ISSN={},
month={July},}
@INPROCEEDINGS{8901574,
author={Tuveri, Nicola and Brumley, Billy Bob},
booktitle={2019 IEEE Cybersecurity Development (SecDev)},
title={Start Your ENGINEs: Dynamically Loadable Contemporary Crypto},
year={2019},
volume={},
number={},
pages={4-19},
abstract={Software ever-increasingly relies on building blocks implemented by security libraries, which provide access to evolving standards, protocols, and cryptographic primitives. These libraries are often subject to complex development models and long decision-making processes, which limit the ability of contributors to participate in the development process, hinder the deployment of scientific results and pose challenges for OS maintainers. In this paper, focusing on OpenSSL as a de-facto standard, we analyze these limits, their impact on the security of modern systems, and their significance for researchers. We propose the OpenSSL ENGINE API as a tool in a framework to overcome these limits, describing how it fits in the OpenSSL architecture, its features, and a technical review of its internals. We evaluate our methodology by instantiating libsuola, a new ENGINE providing support for emerging cryptographic standards such as X25519 and Ed25519 for currently deployed versions of OpenSSL, performing benchmarks to demonstrate the viability and benefits. The results confirm that the ENGINE API offers (1) an ideal architecture to address wide-ranging security concerns; (2) a valuable tool to enhance future research by easing testing and facilitating the dissemination of novel results in real-world systems; and (3) a means to bridge the gaps between research results and currently deployed systems.},
keywords={applied cryptography;public key cryptography;elliptic curve cryptography;software engineering;software implementation;OpenSSL},
doi={10.1109/SecDev.2019.00014},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{779092,
author={Blasband, D.},
booktitle={Proceedings Technology of Object-Oriented Languages and Systems. TOOLS 29 (Cat. No.PR00275)},
title={Reflexivity and meta-tools to manage your software real estate},
year={1999},
volume={},
number={},
pages={403-403},
abstract={Summary form only given, as follows. This tutorial describes tools and techniques for extracting information from large software code bases. Applications described include: * Reverse engineering, abstracting design information from the code. * Metrics for portability, maintainability, etc. * Checking how effectively tests exercise the code. * Change impact analysis. * Automated translations, between languages and libraries. * Checking compliance to industry or project guidelines. * Refactorings, that translate high-level design changes into existing code. Tools and techniques used cover a range from Perl to the use of reflexive language features. The emphasis will be put on how does OO technology contribute.},
keywords={Application software;Reverse engineering;Automatic testing;Radio access networks;Electrical capacitance tomography;Read only memory},
doi={10.1109/TOOLS.1999.779092},
ISSN={},
month={June},}
@INPROCEEDINGS{6559681,
author={Longhai Li and Xiaojun Tian and Dong, Zaili and Lianqing Liu and Tabata, Osamu and Li, Wen J.},
booktitle={The 8th Annual IEEE International Conference on Nano/Micro Engineered and Molecular Systems},
title={Manipulation of DNA origami nanotubes in liquid using a programmable tapping mode AFM},
year={2013},
volume={},
number={},
pages={56-59},
abstract={Deoxyribonucleic acid (DNA) origami [1] is expected to be a nanoscale functional block for Nano Electro Mechanical Systems (NEMS). It can be assembled on a substrate containing other MEMS components to realize a NEMS device in which nanostructures play an important role. We recently demonstrated a tapping mode atomic force microscopy (AFM) process that can manipulate DNA origami structures in liquid to desired positions with controlled orientations, which is a novel process that will eventually allow the constructions of complex nanostructures on substrate surfaces. The manipulation of DNA origami nanotubes with 6 nm in diameter and 400 nm in length placed on a mica substrate was executed by tapping mode AFM with 0–10 nm amplitude. The acting vertical force from the AFM tip to a DNA origami nanotube was calculated to be 25 – 30 nN numerically by using Simulink software (MathWorks). Experimental results shown that ∼80% samples can be successfully manipulated if the tapping mode AFM tip amplitude is 3–4 nm. 1},
keywords={DNA;Force;Nanotubes;Liquids;Substrates;Atomic force microscopy;AFM-Based nanomanipulation;Automated nanomanipulation;Nanomanipulation;Nano-robotics;DNA origami},
doi={10.1109/NEMS.2013.6559681},
ISSN={},
month={April},}
@INPROCEEDINGS{9240845,
author={Todorov, Oleh and Bialobrzheskyi, Olexii and Bondarenko, Serhii},
booktitle={2020 IEEE Problems of Automated Electrodrive. Theory and Practice (PAEP)},
title={Virtual Complex Prototype for Metering a Three-Phase Network Electric Power Quantity and Quality},
year={2020},
volume={},
number={},
pages={1-5},
abstract={To improve the electricity use and distribution situation, companies use electricity automated control systems and metering. With possible different methods of implementation with different technical content, it is also necessary to take into account that the whole complex consists of technical and software parts. Many problems that arise in enterprises are insufficient attention to the metering system of electric power components result. The metering system prototype is based on the IEEE 1459-2010 standard recommendations. Using the Labview graphical programming environment, based on this standard recommendations the metering system graphical interface structured construction was performed. This allowed us to implement a virtual complex prototype with the power indicators by group's differentiation, for each line and the network as a whole.},
keywords={Energy consumption;Software packages;Prototypes;Power distribution;Control systems;Standards;Programming environments;power quality;power;harmonic;metering;distribution;data},
doi={10.1109/PAEP49887.2020.9240845},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6607965,
author={Menon, M. and Dixon, T. and Tena, I.},
booktitle={2013 MTS/IEEE OCEANS - Bergen},
title={Resolving subsea navigation, tracking and positioning issues by utilising Smart ROV Control system software},
year={2013},
volume={},
number={},
pages={1-8},
abstract={The offshore industry is now moving towards deeper waters, challenging geographical areas and dynamic working environments. This is not only applicable to the offshore oil and gas market, but also marine renewables and more recently, the lucrative industry of subsea mining. These initiatives pull together the best practice and experience from established players in dredging, offshore oil and gas and subsea vehicle manufacture. The increasing demand for reliability, performance and lower costs has spurred the development of subsea vehicle design and control which challenges the norms. Original results are presented for a workclass remotely operated vehicle (WROV) system employing an autonomous control algorithm package.},
keywords={Vehicles;Control systems;Vehicle dynamics;Sensors;Sonar;Navigation;Acoustic measurements},
doi={10.1109/OCEANS-Bergen.2013.6607965},
ISSN={},
month={June},}
@INPROCEEDINGS{9325260,
author={Cantoro, Riccardo and Foti, Dario and Sartoni, Sandro and Reorda, Matteo Sonza and Anghel, Lorena and Portolan, Michele},
booktitle={2020 IEEE International Test Conference (ITC)},
title={New Perspectives on Core In-field Path Delay Test},
year={2020},
volume={},
number={},
pages={1-5},
abstract={Path Delay fault test currently exploits DfT-based techniques, mainly relying on scan chains, widely supported by commercial tools. However, functional testing may be a desirable choice in this context because it allows to catch faults at-speed with no hardware overhead and it can be used both for end-of-manufacturing tests and for in-field test. The purpose of this article is to compare the results that can be achieved with both approaches. This work is based on an open-source RISC-V-based processor core as benchmark device. Gathered results show that there is no correlation between stuck-at and path delay fault coverage, and provide guidelines for developing more effective functional test.},
keywords={Delays;Circuit faults;Tools;Clocks;Integrated circuit modeling;Built-in self-test;Analytical models;software-based self-test;software test library;on-line test;path-delay test;safety;scan test;LOC},
doi={10.1109/ITC44778.2020.9325260},
ISSN={2378-2250},
month={Nov},}
@INPROCEEDINGS{7976137,
author={Abdulhassan, Aladdin and Ahmadi, Mahmood},
booktitle={2017 Annual Conference on New Trends in Information & Communications Technology Applications (NTICT)},
title={Parallel many fields packet classification technique using R-tree},
year={2017},
volume={},
number={},
pages={274-279},
abstract={Network packet classification is important network kernel function to enables various network services such as Quality of Service (QoS), security and resource reservation. The OpenFlow protocol is the responsible of the packet classification, it uses a set of rules called ruleset, each rule in that rule set contain a set of matching fields. With the rapid growing of rulesets size and rule fields numbers in the modern networks, it became so difficult to classify incoming packets at reasonable speed using the classical packet classification techniques. Many modern software-based classification solutions have been proposed to accelerate packet classification in additional to hardware-based solutions. In general, to design packet classification algorithm, it is important to strike a balance between high throughput and low memory requirements. This paper proposes an R-tree based parallel many-field packet classification technique using multi-threading to classify packet into the flow it belongs to; Multi-threading has been employed to accelerate packet classification by introduce a parallelism packet classification scheme. Two main algorithms that represent the main two parts of the proposed technique have been described, first algorithm describes how to constructing R-tree from the corresponding ruleset, and the other algorithm describes the R-tree querying in parallel manner. The performance of the proposed technique has been evaluated with consideration of throughput, latency, and memory access by testing the proposed many-fields packet classification algorithms on Class-Bench rulesets. Experiment results showed that the proposed parallel query algorithm can classify packets with very good throughput, latency, and memory access compared with other many-fields packet classification solutions.},
keywords={Classification algorithms;IP networks;Electronics packaging;Throughput;Market research;Quality of service;Software Defined Networking;OpenFlow;Packet Classification;R-Tree;Multi-threading},
doi={10.1109/NTICT.2017.7976137},
ISSN={},
month={March},}
@INPROCEEDINGS{7273508,
author={Das, Anup and Walker, Matthew J. and Hansson, Andreas and Al-Hashimi, Bashir M. and Merrett, Geoff V.},
booktitle={2015 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)},
title={Hardware-software interaction for run-time power optimization: A case study of embedded Linux on multicore smartphones},
year={2015},
volume={},
number={},
pages={165-170},
abstract={Applications running on smartphones interact with the hardware and the system software differently, resulting in widely varying power consumption and hence thermal profiles. Typically, these smartphone platforms expose some hardware power control features to users, controlled through software governors such as cpufreq for dynamic voltage-frequency scaling (DVFS) and cpuquiet for dynamic core selection (DCS). Operating systems on these platforms manage these governors conservatively, independent of application's performance requirement. To address this, we propose an alternative approach, which uses reinforcement learning to explore the trade-off between power saving opportunities using DVFS and DCS and application's performance at run-time. The objective is to reduce power consumption, taking into consideration dynamic power, leakage power, and the inter-dependency between temperature and power. The reinforcement learning-based control is validated as a case-study on ARM A15-based nvidia's tegra smartphone through its implementation as a run-time manager (RTM). This RTM interfaces with different hardware performance counters and the embedded Linux Operating System through (1) the cpuquiet API to select cores at run-time; and (2) the cpufreq API to scale the frequency of active cores. Experiments with mobile and high performance applications demonstrate that the proposed approach achieves an average 22% (7-40%) power reduction compared to existing techniques.},
keywords={Power demand;Mathematical model;Hardware;Embedded systems;Temperature dependence;Temperature sensors;Prediction algorithms;Power reduction;temperature minimization;reinforcement learning;cpufreq;cpuquiet},
doi={10.1109/ISLPED.2015.7273508},
ISSN={},
month={July},}
@INPROCEEDINGS{9245290,
author={Šipek, M. and Muharemagić, D. and Mihaljević, B. and Radovan, A.},
booktitle={2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)},
title={Enhancing Performance of Cloud-based Software Applications with GraalVM and Quarkus},
year={2020},
volume={},
number={},
pages={1746-1751},
abstract={Increased complexity of network-based software solutions and the ever-rising number of concurrent users forced a shift of the IT industry to cloud computing. Conventional network software systems commonly based on monolithic application stack running on costly physical single-purpose servers are affected by significant problems of resource management, computing power distribution, and scalability. Such implementation is restricting applications to be reduced to smaller, independent services that can be more easily deployed, managed, and scaled dynamically; therefore, embellishing environmental uniformity across development, testing, and production. Current cloud-based infrastructure frequently runs on containers placed in Kubernetes or Docker-based cluster, and the system configuration is considerably different compared to the environment prevailed with common virtualizations. This paper discusses the usage of GraalVM, a polyglot high-performance virtual machine for JVM-based and other languages, combined with new Kubernetes native Java tailored stacked framework named Quarkus, formed from enhanced Java libraries. Moreover, our research explores GraalVM's creation of native images using Ahead-Of-Time (AOT) compilation and Quarkus' deployment to Kubernetes. Furthermore, we examined the architectures of given systems, various performance variables, and differing memory usage cases within our academic testing environment and presented the comparison results of selected performance measures with other traditional and contemporary solutions.},
keywords={Cloud computing;Java;Production;Tools;Virtual machining;Servers;Virtualization;GraalVM;Kubernetes;Quarkus;Cloud computing;native image},
doi={10.23919/MIPRO48935.2020.9245290},
ISSN={2623-8764},
month={Sep.},}
@INPROCEEDINGS{6418375,
author={Valcárcel, Daniel F. and Alves, Diogo and Neto, André and Reux, Cédric and Carvalho, Bernardo B. and Felton, Robert and Lomas, Peter J. and Sousa, Jorge and Zabeo, Luca},
booktitle={2012 18th IEEE-NPSS Real Time Conference},
title={Parallel task management library for MARTe},
year={2012},
volume={},
number={},
pages={1-7},
abstract={The Multithreaded Application Real-Time executor (MARTe) is a real-time framework with increasing popularity and support in the thermonuclear fusion community. It allows to run modular code in a multi-threaded environment leveraging on the current multi-core processor (CPU) technology. One application that relies on the MARTe framework is the JET tokamak WALL load limiter System (WALLS). It calculates and monitors the temperature on metal tiles, plasma facing components (PFCs) that can melt or flake if their temperature gets too high when exposed to power loads. One of the main time consuming tasks inWALLS is the calculation of thermal diffusion models in real-time. These models tend to be described by very large state-space models thus making them perfect candidates for parallelisation. MARTe's traditional approach for task parallelisation is to split the problem into several Real-Time Threads, each responsible for a self-contained sequential execution of an input-to-output chain. This is usually possible, but it might not always be practical for algorithmic or technical reasons. Also, it might not be easily scalable with an increase of the available number of CPU cores. The WorkLibrary introduces a "GPU-Iike approach" of splitting work among the available cores of modern CPUs that is (i) straightforward to use in an application, (ii) scalable with more available cores and all of this (iii) without code rewrite or recompilation. The first part of this article explains the motivation behind the library, its architecture and implementation. The second part presents a real application for WALLS, a parallel version of a large state-space model describing the 2D thermal diffusion on a JET tile.},
keywords={Load modeling;Rocks;Linux;Kernel;RNA;Real time software systems;Parallel processing;Multi-Core;MARTe},
doi={10.1109/RTC.2012.6418375},
ISSN={},
month={June},}
@INPROCEEDINGS{4419603,
author={LeBaron, Todd and Jacobsen, Craig},
booktitle={2007 Winter Simulation Conference},
title={The simulation power of automod},
year={2007},
volume={},
number={},
pages={210-218},
abstract={Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model objects in other models, decreasing the time required to build a model. In addition, recent enhancements to AutoMod's material handling template systems have increased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages.},
keywords={Decision making;Costs;Raw materials;Virtual manufacturing;Testing;Project management;Engineering management;Computer architecture;Materials handling;Software packages},
doi={10.1109/WSC.2007.4419603},
ISSN={1558-4305},
month={Dec},}
@INPROCEEDINGS{4724898,
author={Yeung, Jackson H.C. and Tsang, C.C. and Tsoi, K.H. and Kwan, Bill S.H. and Cheung, Chris C.C. and Chan, Anthony P.C. and Leong, Philip H.W.},
booktitle={2008 16th International Symposium on Field-Programmable Custom Computing Machines},
title={Map-reduce as a Programming Model for Custom Computing Machines},
year={2008},
volume={},
number={},
pages={149-159},
abstract={The map-reduce model requires users to express their problem in terms of a map function that processes single records in a stream, and a reduce function that merges all mapped outputs to produce a final result. By exposing structural similarity in this way, a number of key issues associated with the design of custom computing machines including parallelisation; design complexity; software-hardware partitioning; hardware-dependency, portability and scalability can be easily addressed. We present an implementation of a map-reduce library supporting parallel field programmable gate arrays (FPGAs) and graphics processing units (GPUs). Parallelisation due to pipelining, multiple data paths and concurrent execution of FPGA/GPU hardware is automatically achieved. Users first specify the map and reduce steps for the problem in ANSI Cand no knowledge of the underlying hardware or parallelisation is needed. The source code is then manually translated into a pipelined data path which, along with the map-reduce library, is compiled into appropriate binary configurations for the processing units. We describe our experience in developing a number of benchmark problems in signal processing, Monte Carlo simulation and scientific computing as well as report on the performance of FPGA, GPU and heterogeneous systems.},
keywords={Field programmable gate arrays;Hardware;Concurrent computing;Parallel processing;Graphics;Signal processing;Computer science;Scalability;Software libraries;Pipeline processing;reconfigurable computing;map reduce;hardware/software codesign},
doi={10.1109/FCCM.2008.19},
ISSN={},
month={April},}
@INPROCEEDINGS{4597228,
author={Ostrowski, David A.},
booktitle={2008 IEEE International Conference on Semantic Computing},
title={Ontology Refactoring},
year={2008},
volume={},
number={},
pages={476-479},
abstract={This paper presents a rule based approach to ontology refactoring. Our method supports large scale instance relationships for support of translation to an improved, functionally equivalent design. By generating new ontology versions on-the-fly we can verify potential updates to our requirements. An example of this technique is presented utilizing a subset of the OWL-DL specification through the implementation of the Jena API. Advantages of this approach include rapid prototyping, versioning support, querying and tool development to support the automated engineering of instance data.},
keywords={Ontologies;Semantic web;OWL;Conferences;Object recognition;Logic programming;Software;Ontology;Software Engineering;Semantic Web},
doi={10.1109/ICSC.2008.38},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7695201,
author={Gañán, David and Caballé, Santi and Clarisó, Robert and Conesa, Jordi},
booktitle={2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)},
title={A Prototype of an eLearning Platform in Support for Learning Analytics and Gamification},
year={2016},
volume={},
number={},
pages={362-369},
abstract={This paper presents the implementation and prototyping of an innovative web-based eLearning platform, featuring Learning Analytics and Gamification called ICT-FLAG. A previous contribution presented the analysis and design of the platform. Following the design, this paper implements the platform and reports on the first experiences of connecting and integrating the platform with a real eLearning tool through an API. The purpose of this connection is to provide the tool with learning analytics and gamification-based services. The research reported in this paper is currently undertaken within the research project "Enhancing ICT education through Formative assessment, Learning Analytics and Gamification" (ICT-FLAG) funded by the Spanish Government.},
keywords={Fats;Electronic learning;Context;Data visualization;Data models;Data collection;eLearning platform;software development framework;web technologies;software analysis and design},
doi={10.1109/INCoS.2016.33},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8102148,
author={Gomes, Rahul and Straub, Jeremy and Jones, Andrew and Morgan, John and Tipparach, Santipab and Sletten, Aaron and Kim, Keon Woo and Loegering, David and Feikema, Nick and Dayananda, Karanam and Miryala, Goutham and Gass, Andrew and Setterstrom, Kevin and Mischel, Jacob and Shipman, Dylan and Nazzaro, Colleen},
booktitle={2017 IEEE/AIAA 36th Digital Avionics Systems Conference (DASC)},
title={An interconnected network of UAS as a system-of-systems},
year={2017},
volume={},
number={},
pages={1-7},
abstract={Technological advancements and miniaturization have made it possible for Unmanned Aerial Systems (UAS) to perform a diverse range of tasks. UAS are being used in various applications ranging from remote sensing[1] to disaster response[2] to package delivery[3]. As drones rapidly fill the airspace, there are several threats that the system can encounter. These threats include mid-air collisions, loss of remote command connection, security breach of the drone's software system and critical damage to the hardware. To ensure the integrity of the drone in flight, an interconnected UAS architecture, having a fully functional system capable of responding to these situations, is required. This paper presents a system meeting these requirements. An interconnected UAS system of systems is proposed that includes systems for onboard GPS, obstacle avoidance central control and safety response. It focuses on the coordination level that the UAS systems will need to have amongst themselves. It proposes a control system to ensure effective monitoring of UAS. The UAS system architecture is comprised of eleven systems that are essential for a safe flight. In addition, the control system includes five different systems that are involved in the decision-making process. The importance and operation of these systems are discussed in detail. The proposed UAS software architecture is capable of performing autonomous self-control. It has decision making modules that can receive data from various sensors and integrate it to provide efficient route planning and data management. The software systems monitor the area surrounding the UAV to facilitate routing and decision making. The software architecture includes self-awareness to respond to situations such as adverse climatic conditions that impact the flight capabilities of the UAS. The proposed UAS system also includes hardware to monitor and update the list of obstacles in its path. This requires the UAS to be equipped with sensors for obstacle determination and a software system that accepts the inputs from these sensors, makes decisions and performs actions, promptly. An emergency response system is included to ensure that the drone will land safely in an appropriate location, if its systems are compromised. This response system is capable of deciding the severity of the situation and sends commands to the flight control system regarding the recovery steps that should be taken. The proposed system architecture incorporates cybersecurity in its design framework so that it is equipped to handle potential hackers that might try to gain access to onboard navigation controls and reroute the UAS for personal gain or another agenda. If the system senses a threat, it launches the emergency response system. The proposed UAS software architecture includes a maintenance and diagnostics system that coordinates and monitors drone activities. It performs functions ranging from monitoring the health of the hardware systems to uploading error reports to the central server. The system transmits error reports to the control unit which further processes the data to determine the source of the error and resolve the issue. In UAS systems, a computerized framework takes input from at least one sensor, and uses a pre-characterized set of guidelines to make decisions [4]. This paper aims to achieve an interconnected UAS system of systems that can address several issues and solutions related to flight autonomously.},
keywords={Poles and towers;Monitoring;Path planning;Meteorology;Temperature sensors;UAS;remote sensing;path planning;obstacle avoidance;autonomous navigation},
doi={10.1109/DASC.2017.8102148},
ISSN={2155-7209},
month={Sep.},}
@INPROCEEDINGS{1418230,
author={Baghai-Wadji, A.R. and Wagner, K.C.},
booktitle={IEEE Ultrasonics Symposium, 2004},
title={Universal functions for the analysis of electromagnetic interactions in SAW devices},
year={2004},
volume={3},
number={},
pages={2015-2018 Vol.3},
abstract={In spite of the availability of a large number of commercial software packages for the analysis of electromagnetic fields, SAW devices defy a satisfying usage of these packages. This defiance is primarily due to the strong anisotropy and the presence of a large number of geometrically complex thin metallic structures positioned on parallel interfaces. The method of moments (MoM) is particularly suitable for this class of problems, despite its deficiencies. To combat several problems in the MoM applications we have developed the method of Fast-MoM utilizing a number of universal functions. In this paper we illustrate the construction and processing of universal functions for the EM analysis in SAW devices, and point out strategies for accelerating the computations and enhancing the accuracy of the numerical results.},
keywords={Electromagnetic analysis;Electromagnetic devices;Surface acoustic wave devices;Software packages;Availability;Electromagnetic fields;Packaging;Anisotropic magnetoresistance;Moment methods;Acceleration},
doi={10.1109/ULTSYM.2004.1418230},
ISSN={1051-0117},
month={Aug},}
@ARTICLE{7582313,
author={Liang, Yu and Peng, Guojun and Luo, Yuan and Zhang, Huanguo},
journal={China Communications},
title={Mitigating ROP attacks via ARM-specific in-place instruction randomization},
year={2016},
volume={13},
number={9},
pages={208-226},
abstract={Defending against return-oriented programing (ROP) attacks is extremely challenging for modern operating systems. As the most popular mobile OS running on ARM, Android is even more vulnerable to ROP attacks due to its weak implementation of ASLR and the absence of effective control-flow integrity enforcement. In this paper, leveraging specific ARM features, an instruction randomization strategy to mitigate ROP attacks in Android even with the threat of single pointer leakage vulnerabilities is proposed. By popping out more registers in functions' epilogue instructions and reallocating registers in function scopes, branch targets in all (direct and indirect) branch instructions potential to be ROP gadgets are changed randomly. Without the knowledge of binaries' runtime instructions layout, adversary's repeated control flow transfer in ROP exploits will be subverted. Furthermore, this instruction randomization idea has been implemented in both Android Dalvik runtime and ART. Corresponding evaluations proved it is capable to introduce enough randomness for more than 99% discovered functions and thwart about 95% ROP gadgets in application's shared libraries and oat file compiled from Dalvik bytecode. Besides, evaluations on real-world exploits also confirmed its effectiveness on mitigating ROP attacks within acceptable performance overhead.},
keywords={Androids;Humanoid robots;Runtime;Open area test sites;Registers;Subspace constraints;Libraries;software security;ROP mitigation;instruction randomization;ARM architecture},
doi={10.1109/CC.2016.7582313},
ISSN={1673-5447},
month={Sep.},}
@INPROCEEDINGS{9044267,
author={Tran, Ha Manh and Le, Son Thanh and Nguyen, Sinh Van and Le, Hai-Duong},
booktitle={2019 International Conference on Advanced Computing and Applications (ACOMP)},
title={A Web-Based Management System for Software Defined Network Controllers},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Auto-reconfiguration and response mechanisms in IP networks are challenges that lead to enforcing the required policies in such a dynamic environment. Software-Defined Network (SDN) provides an economic architecture solution for service providers by reducing provisioning time from weeks to minutes, facilitating centralized management, allowing programmability and promoting innovation. SDN achieves this by separating the two features inside network devices, such as routers and switches, namely the control plane from the data plane. The paper presents an implementation of a web-based management system for SDN controllers. This implementation allows network operators to control and supervise quickly the virtual network functions (VNF) provided by SDN networks. The implementation not only takes advantages of open source software including Flask and Ryu frameworks, OpenvSwitch and Mininet but also supports for virtual network functions. The system emulates a network topology with simple switches and links and also acts as a management center to query commands from Ryu network application using REST API in order to monitor changes in network traffic.},
keywords={Control systems;Protocols;Network topology;Software;Computer architecture;IP networks;Topology;SDN;RyuFramework;SDNController;NFV},
doi={10.1109/ACOMP.2019.00008},
ISSN={2688-0202},
month={Nov},}
@ARTICLE{159800,
author={Dugan, J.B. and Bavuso, S.J. and Boyd, M.A.},
journal={IEEE Transactions on Reliability},
title={Dynamic fault-tree models for fault-tolerant computer systems},
year={1992},
volume={41},
number={3},
pages={363-377},
abstract={Reliability analysis of fault-tolerant computer systems for critical applications is complicated by several factors. Systems designed to achieve high levels of reliability frequently employ high levels of redundancy, dynamic redundancy management, and complex fault and error recovery techniques. This paper describes dynamic fault-tree modeling techniques for handling these difficulties. Three advanced fault-tolerant computer systems are described: a fault-tolerant parallel processor, a mission avionics system, and a fault-tolerant hypercube. Fault-tree models for their analysis are presented. HARP (Hybrid Automated Reliability Predictor) is a software package developed at Duke University and NASA Langley Research Center that can solve those fault-tree models.<>},
keywords={Fault tolerant systems;Redundancy;Application software;Computer errors;Concurrent computing;Aerospace electronics;Hypercubes;Software packages;NASA;Predictive models},
doi={10.1109/24.159800},
ISSN={1558-1721},
month={Sep.},}
@ARTICLE{8497058,
author={Zhang, Chen and Sun, Guangyu and Fang, Zhenman and Zhou, Peipei and Pan, Peichen and Cong, Jason},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
title={Caffeine: Toward Uniformed Representation and Acceleration for Deep Convolutional Neural Networks},
year={2019},
volume={38},
number={11},
pages={2072-2085},
abstract={With the recent advancement of multilayer convolutional neural networks (CNNs) and fully connected networks (FCNs), deep learning has achieved amazing success in many areas, especially in visual content understanding and classification. To improve the performance and energy efficiency of the computation-demanding CNN, the FPGA-based acceleration emerges as one of the most attractive alternatives. In this paper, we design and implement Caffeine, a hardware/software co-designed library to efficiently accelerate the entire CNN and FCN on FPGAs. First, we propose a uniformed convolutional matrix-multiplication representation for both computation-bound convolutional layers and communication-bound FCN layers. Based on this representation, we optimize the accelerator microarchitecture and maximize the underlying FPGA computing and bandwidth resource utilization based on a revised roofline model. Moreover, we design an automation flow to directly compile highlevel network definitions to the final FPGA accelerator. As a case study, we integrate Caffeine into the industry-standard software deep learning framework Caffe. We evaluate Caffeine and its integration with Caffe by implementing VGG16 and AlexNet networks on multiple FPGA platforms. Caffeine achieves a peak performance of 1460 giga fixed point operations per second on a medium-sized Xilinx KU060 FPGA board; to our knowledge, this is the best published result. It achieves more than 100× speedup on FCN layers over prior FPGA accelerators. An end-to-end evaluation with Caffe integration shows up to 29× and 150× performance and energy gains over Caffe on a 12-core Xeon server, and 5.7× better energy efficiency over the GPU implementation. Performance projections for a system with a high-end FPGA (Virtex7 690t) show even higher gains.},
keywords={Field programmable gate arrays;Acceleration;Graphics processing units;Engines;Kernel;Bandwidth;Machine learning;Caffe;CNN FPGA engine;convolutional neural network (CNN);deep learning;hardware/software co-design},
doi={10.1109/TCAD.2017.2785257},
ISSN={1937-4151},
month={Nov},}
@ARTICLE{5222335,
author={Bavuso, Salvatore J. and Dugan, Joanne Bechta and Trivedi, Kishor S. and Rothmann, Elizabeth M. and Smith, W. Earl},
journal={IEEE Transactions on Reliability},
title={Analysis of Typical Fault-Tolerant Architectures using HARP},
year={1987},
volume={R-36},
number={2},
pages={176-185},
abstract={HARP (the Hybrid Automated Reliability Predictor) is a software package that implements advanced reliability modeling techniques. We present an overview of some of the problems that arise in modeling highly reliable fault-tolerant systems; the overview is loosely divided into model construction and model solution problems. We then describe the HARP approach to these difficulties, which is facilitated by a technique called behavioral decomposition. The bulk of this paper presents examples of the dependability evaluation of some typical fault-tolerant systems, including a local-area network, two well-known fault-tolerant computer systems (C.mmp and SIFT), and an example of a flight control system. HARP has been used to solve very large models. A system consisting of 20 components distributed among 7 stages produced a Markov chain with 24 533 states and over 335 000 transitions (without coverage). Depending on the system used to run this example, the run time took anywhere from 4 to 8 hours. HARP is undergoing beta testing at approximately 20 sites. It is written in standard FORTRAN 77, consists of nearly 30000 lines of code and comments, and has been tested under several operating systems. The graphics interface (written in C) runs on an IBM PC AT, and produces text files that can be used to solve the system on the PC (for very small systems), or can be uploaded to a larger machine. HARP is accompanied by an Introduction and Guide for Users. For information on obtaining a copy of HARP, contact one of the authors.},
keywords={Fault tolerance;Fault tolerant systems;Computer architecture;Software packages;Predictive models;Local area networks;Computer networks;Aerospace control;Testing;Code standards;Fault tolerance;Availability;Behavioral decomposition;Markov chain;Fault tree;HARP},
doi={10.1109/TR.1987.5222335},
ISSN={1558-1721},
month={June},}
@INPROCEEDINGS{6546132,
author={Bisset, Keith R. and Deodhar, Suruchi and Makkapati, Hemanth and Marathe, Madhav V. and Stretz, Paula and Barrett, Christopher L.},
booktitle={2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
title={Simfrastructure: A Flexible and Adaptable Middleware Platform for Modeling and Analysis of Socially Coupled Systems},
year={2013},
volume={},
number={},
pages={506-513},
abstract={Socially coupled systems are comprised of interdependent social, organizational, economic, infrastructure and physical networks. Today's urban regions serve as an excellent example of such systems. People and institutions confront the implications of the increasing scale of information becoming available due to a combination of advances in pervasive computing, data acquisition systems as well as high performance computing. Integrated modeling and decision making environments are necessary to support planning, analysis and counter factual experiments to study these complex systems. Here, we describe SIMFRASTRUCTURE - a flexible coordination middleware that supports high performance computing oriented decision and analytics environments to study socially coupled systems. Simfrastructure provides a multiplexing mechanism by which simple and intuitive user-interfaces can be plugged in as front-end systems, and high-end computing resources can be plugged in as back-end systems for execution. This makes the computational complexity of the simulations completely transparent to the users. The decoupling of user interfaces and data repository from simulation execution allows users to access simulation results asynchronously and enables them to add new datasets and simulation models dynamically. Simfrastructure enables implementation of a simple yet powerful modeling environment with built-in analytics-as-a service platform, which provides seamless access to high end computational resources, through an intuitive interface for studying socially coupled systems. We illustrate the applicability of Simfrastructure in the context of an integrated modeling environment to study public health epidemiology.},
keywords={Computational modeling;Computer architecture;Data models;Libraries;Data transfer;Middleware;Analytical models;Socially coupled systems;middleware;public health epidemiology;distributed systems;software architecture},
doi={10.1109/CCGrid.2013.78},
ISSN={},
month={May},}
@INPROCEEDINGS{5175832,
author={Kraemer, Frank Alexander and Samset, Haldor and Braek, Rolv},
booktitle={2009 IEEE International Conference on Web Services},
title={An Automated Method for Web Service Orchestration Based on Reusable Building Blocks},
year={2009},
volume={},
number={},
pages={262-270},
abstract={We describe a complete and largely automated method for the development of systems from Web services, which comprises the encapsulation of services, as well as their composition, verification and subsequent implementation in a model-driven manner. The paper follows the steps of the method: In a first phase, we import WSDL descriptions automatically as UML~2.x activities and provide them as building blocks, with some optional, manual adaptations. In a second phase, these building blocks can be used to compose an application that orchestrates Web services. The building blocks have behavioral contracts that enable automated, incremental verification based on compositional model checking. We demonstrate the approach by a subscription-based service to receive SMS messages.},
keywords={Web services;Unified modeling language;Libraries;Encapsulation;Java;Telematics;Technological innovation;Contracts;Acoustical engineering;Computer architecture;Web Services;Model-Driven Architecture;UML;Software Engineering},
doi={10.1109/ICWS.2009.68},
ISSN={},
month={July},}
@ARTICLE{6773923,
author={Ramakrishnan, K. G. and Mitra, D.},
journal={The Bell System Technical Journal},
title={An overview of PANACEA, a software package for analyzing Markovian queueing networks},
year={1982},
volume={61},
number={10},
pages={2849-2872},
abstract={PANACEA is a software package that significantly extends the range of Markovian queueing networks that are computationally tractable. It solves multi-class closed, open, and mixed queueing networks. Based on an underlying theory of integral representations and asymptotic expansions, PANACEA solves queueing networks that are orders of magnitude larger than can be solved by other established algorithms. The package is finding widespread use in Bell Laboratories. It also has important software innovations. A flexible programming-language-like interface facilitates compact representation of large queueing networks. An out-of-core implemental strategy enables PANACEA to be ported to processors with modest memory. The modular structure of this software package, along with the automatic machine-generated parser, makes it easily extendable. This paper provides an overview of two basic versions of PANACEA, versions 1.0 and 1.1, which solve “closed” networks only. A description of its model language is given from the point of view of its capability to describe queueing networks in a compact, natural manner. The paper discusses the algorithms, together with their time and storage requirements, that are used in the implementation. Several numerical examples are given.},
keywords={},
doi={10.1002/j.1538-7305.1982.tb02281.x},
ISSN={0005-8580},
month={Dec},}
@INPROCEEDINGS{101464,
author={Peters, P. and Dearuz, R. and Sung, C.-T. and Wang, C. and Meandzija, B.},
booktitle={IEEE INFOCOM '89, Proceedings of the Eighth Annual Joint Conference of the IEEE Computer and Communications Societies},
title={On generalizations in networking software to encourage code portability},
year={1989},
volume={},
number={},
pages={261-267 vol.1},
abstract={The principal issues involved in porting networking software are discussed and solutions that have been used to implement a standard environment interface to encourage code portability are reported. This interface has been designed to provide a uniform environment to protocol drivers generated by the Archetype language compiler. The structure of the present implementation is outlined and issues relating to the environment interface are elaborated. The implementation provides both portable data representation and portable systems services.<>},
keywords={Intelligent networks;Hardware;Operating systems;Computer aided manufacturing;Libraries;Instruments;Code standards;Software standards;Access protocols;Design methodology},
doi={10.1109/INFCOM.1989.101464},
ISSN={},
month={April},}
@INPROCEEDINGS{9070945,
author={Boora, Shakuntla and Agarwal, S. K and sandhu, K.S},
booktitle={2020 7th International Conference on Signal Processing and Integrated Networks (SPIN)},
title={Implementation of Mamdani Type FIS and PI Based 3-Legged VSI Controller for an Asynchronous Generator (AG)},
year={2020},
volume={},
number={},
pages={733-738},
abstract={In this manuscript, PI and Mamdani type FIS controllers (one for ac generated voltage control and another for dc capacitor voltage control) are recommended to formulate template based algorithm for yielding reference supply currents. The recommended controller controls both reactive and active (kW) power parallel of an asynchronous generator (AG) for keeping the voltage and frequency at the desired level under varying loads. The designing part of 3legged VSI controller is also elaborated. In this manuscript, the simulated outcomes of the performance characteristics are accomplished using both PI and Mamdani type FIS based controller just to testify that the recommended controller is superior to the PI method and is well suited for power generation in isolation mode. Here, the whole electrical system and the controller are modeled using Simpower system and Fuzzy logic toolbox of MATLAB software package.},
keywords={Voltage control;Capacitors;Harmonic analysis;Frequency control;Generators;Fuzzy logic;Zirconium;Reactive power;Discrete PI controller;MATLAB SIMULINK Software;Control algorithm;Hysteresis controller;3-legged VSI (CC-VSI);Mamdani type FIS},
doi={10.1109/SPIN48934.2020.9070945},
ISSN={2688-769X},
month={Feb},}
@INPROCEEDINGS{968145,
author={den Bosch, A.O. and Santamaria, J.C.},
booktitle={MTS/IEEE Oceans 2001. An Ocean Odyssey. Conference Proceedings (IEEE Cat. No.01CH37295)},
title={Remote visualization and management tools for underwater operations},
year={2001},
volume={3},
number={},
pages={1953-1959 vol.3},
abstract={This paper introduces a new set of software tools that integrate near-real-time visualization with a publish and subscribe mechanism to achieve remote monitoring and control of dynamic objects in an underwater scene. The approach proposed in this paper involves the integration of existing technologies to produce a powerful and flexible solution to problems in which enhanced awareness of the situation can lead to improvements in performance. These tools can be easily adapted to the extensive and diverse set of situations encountered in underwater construction, underwater surveying and maritime navigation. Real-time data from instruments and positioning sensors is made available by using a publishing mechanism and a remote data server. Users with Internet or intranet access can subscribe to any real-time data field being published and receive updates every time the information changes. Underwater infrastructure information resulting from construction, maintenance and inspection is often stored in databases. The software libraries introduced in this paper also provide a full set of tools to store and retrieve information from such databases. The database access tools use the same publish and subscribe mechanism involved in real-time data acquisition. This similarity makes it very easy for developers to integrate historic and live data in a single monitoring application. The visualization tools presented in this paper enable developers to define virtual scenes that can display all the relevant elements associated with an underwater construction job, including complex structures and dynamic objects; i.e., ROVs, vessels, etc. The use of advance cueing techniques and multi-resolution rendering make it possible to achieve satisfactory interactive frame rates without sacrificing accuracy and realism. The concept of 3D indicator modules is also introduced. These powerful modules can be linked to live data using the subscribe tools and attached to any element in the environment including dynamic ones. This flexibility allows the users to monitor the data, not only as it changes, but also in the 3D location that makes the most sense. The three components of the tools being introduced in this paper (data acquisition, data distribution, and scene visualization) are fully described along with examples of applications that take full advantage of the integration of all three components.},
keywords={Layout;Visual databases;Data acquisition;Data visualization;Software tools;Remote monitoring;Navigation;Instruments;Publishing;Web server},
doi={10.1109/OCEANS.2001.968145},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9401464,
author={Wang, Hao and Cheung, Sen-Ching S.},
booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)},
title={Augmented Reality Circuit Learning},
year={2021},
volume={},
number={},
pages={1-5},
abstract={Building electronic circuits is one of the most common hands-on activities in learning STEM subjects. Beginning students often find it difficult to translate a circuit schematic into the construction of the physical circuit on a breadboard. In this paper, we describe an Augmented Reality circuit learning software that can provide students with step-by-step instructions by placing virtual circuit components on a physical breadboard. We propose a novel image processing pipeline that can robustly identify the planar structure of a breadboard, even in the presence of occluding circuit components on the breadboard. Using a commercially available library of 3D circuit component models, the estimated 3D structure of the breadboard allows us to render arbitrary circuit components on it in real time. Experimental results demonstrate that our algorithms are accurate and can produce realistic-looking virtual circuits.},
keywords={Solid modeling;Three-dimensional displays;Pipelines;Breadboard;Software;Real-time systems;Augmented reality;vision-based circuit inspection;augmented reality;circuit learning},
doi={10.1109/ISCAS51556.2021.9401464},
ISSN={2158-1525},
month={May},}