@INPROCEEDINGS{6513640,
author={Mushtaq, Hamid and Al-Ars, Zaid and Bertels, Koen},
booktitle={2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)},
title={Efficient software-based fault tolerance approach on multicore platforms},
year={2013},
volume={},
number={},
pages={921-926},
abstract={This paper describes a low overhead software-based fault tolerance approach for shared memory multicore systems. The scheme is implemented at user-space level and requires almost no changes to the original application. Redundant multithreaded processes are used to detect soft errors and recover from them. Our scheme makes sure that the execution of the redundant processes is identical even in the presence of non-determinism due to shared memory accesses. It provides a very low overhead mechanism to achieve this. Moreover it implements a fast error detection and recovery mechanism. The overhead incurred by our approach ranges from 0% to 18% for selected benchmarks. This is lower than comparable systems published in literature.},
keywords={Clocks;Instruction sets;Synchronization;Benchmark testing;Fault tolerant systems;Fault tolerance;Libraries},
doi={10.7873/DATE.2013.194},
ISSN={1530-1591},
month={March},}
@INPROCEEDINGS{1490370,
author={Drumea, A. and Popescu, C.},
booktitle={27th International Spring Seminar on Electronics Technology: Meeting the Challenges of Electronics Technology Progress, 2004.},
title={Finite state machines and their applications in software for industrial control},
year={2004},
volume={1},
number={},
pages={25-29 vol.1},
abstract={Finite state machines (FSMs) are a common presence in digital circuit design. However, they can be very useful also for the software developer. Actual operating systems and application software are event-based and communication issues play a big role; these fields can be more easily handled with software based on finite state machines - software that is simpler and easier to understand, debug and modify. Embedded systems' software can also benefit from state machines because of their efficient way of using the limited resources of the system. The paper presents some basic concepts of finite state machines, some typical applications, with focus on Web technologies (modem control, FTP - File Transfer Protocol, remote access via Telnet console) and some implementation issues - programming finite state machines in Delphi for Windows, in microcontroller assembly language and C. Latest trends are also analyzed - the hardware implementation of state machines in silicon, like the new Texas Instruments MSP430 series of low power microcontrollers. These electronic packages offer some features like reduced power consumption, a single chip solution for complex applications and high functional flexibility.},
keywords={Automata;Application software;Industrial control;Communication system software;Software debugging;Microcontrollers;Digital circuits;Operating systems;Embedded system;Embedded software},
doi={10.1109/ISSE.2004.1490370},
ISSN={},
month={May},}
@INPROCEEDINGS{289937,
author={Becker, T.},
booktitle={Proceedings of 2nd International Workshop on Configurable Distributed Systems},
title={Application-transparent fault tolerance in distributed systems},
year={1994},
volume={},
number={},
pages={36-45},
abstract={We present a new software architecture in which all concepts necessary to achieve fault tolerance can be added to an application automatically without any source code changes. As a case study, we consider the problem of providing a reliable service despite node failures by executing a group of replicated servers. Replica creation and management as well as failure detection and recovery are performed automatically by a separate fault tolerance layer (ft-layer) which is inserted between the server application and the operating system kernel. The layer is invisible for the application since it provides the same functional interface as the operating system kernel, thus making the fault tolerance property of the service completely transparent for the application. A major advantage of our architecture is that the layer encapsulates both fault tolerance mechanisms and policies. This allows for maximum flexibility in the choice of appropriate methods for fault tolerance without any changes in the application code.<>},
keywords={Fault tolerant systems;Fault tolerance;Testing;Operating systems;Programming profession;Libraries;Kernel;Joining processes;Computer science;Software architecture},
doi={10.1109/IWCDS.1994.289937},
ISSN={},
month={March},}
@ARTICLE{8385157,
author={Neumann, Andy and Laranjeiro, Nuno and Bernardino, Jorge},
journal={IEEE Transactions on Services Computing},
title={An Analysis of Public REST Web Service APIs},
year={2021},
volume={14},
number={4},
pages={957-970},
abstract={Businesses are increasingly deploying their services on the web, in the form of web applications, SOAP services, message-based services, and, more recently, REST services. Although the movement towards REST is widely recognized, there is not much concrete information regarding the technical features being used in the field, such as typical data formats, how HTTP verbs are being used, or typical URI structures, just to name a few. In this paper, we go through the Alexa.com top 4000 most popular sites to identify precisely 500 websites claiming to provide a REST web service API. We analyze these 500 APIs for key technical features, degree of compliance with REST architectural principles (e.g., resource addressability), and for adherence to best practices (e.g., API versioning). We observed several trends (e.g., widespread JSON support, software-generated documentation), but, at the same time, high diversity in services, including differences in adherence to best practices, with only 0.8 percent of services strictly complying with all REST principles. Our results can help practitioners evolve guidelines and standards for designing higher quality services and also understand deficiencies in currently deployed services. Researchers may also benefit from the identification of key research areas, contributing to the deployment of more reliable services.},
keywords={Best practices;Documentation;Standards;Simple object access protocol;XML;Analytical models;REST;RESTful;web services;API;HTTP;web;web services analysis},
doi={10.1109/TSC.2018.2847344},
ISSN={1939-1374},
month={July},}
@INPROCEEDINGS{537159,
author={Ju-Young L. Park and Hyeong-Ah Choi and Natawut Nupairoj and Ni, L.M.},
booktitle={Proceedings of the 1996 ICPP Workshop on Challenges for Parallel Processing},
title={Construction of optimal multicast trees based on the parameterized communication model},
year={1996},
volume={1},
number={},
pages={180-187 vol.1},
abstract={Many tree-based multicast algorithms have been proposed to provide an efficient software implementation on parallel platforms without hardware multicast support. These algorithms are either architecture-dependent (not portable) or architecture-independent (portable) but do not provide good performance when ported to different parallel platforms. Based on the LogP model, the proposed parameterized communication model can more accurately characterize the communication network of parallel platforms. The model encompasses a number of critical system parameters which can be easily measured on a given parallel platform. Based on the model, efficient methods to construct optimal multicast trees are proposed for both 1-port and /spl alpha/-port communication architectures. Experimental results conducted on the IBR/SP at Argonne National Laboratory are presented to compare the performance of the optimal multicast tree with two other known free-based multicast algorithms. We claim that our proposed multicast algorithms can be ported to different parallel platforms and provide a near-optimal performance as the truly machine-specific optimal performance is achievable only when the underlying detailed network characteristics are considered.},
keywords={Multicast algorithms;Parallel processing;Libraries;Computer science;Hardware;Parallel machines;Network topology;Multicast communication;Clustering algorithms;Communication networks},
doi={10.1109/ICPP.1996.537159},
ISSN={0190-3918},
month={Aug},}
@ARTICLE{7884929,
author={Gallegos-Canales, Luis M. and Favela-Contreras, Antonio and Ávila, Alfonso and Martínez-Chapa, Sergio O.},
journal={IEEE Transactions on Industrial Electronics},
title={Embedded Software Implementation of the SISO Adaptive Predictive Control Algorithm},
year={2017},
volume={64},
number={9},
pages={7229-7238},
abstract={Advancements in microelectronic technology provide the opportunity of implementing more complex and computation-heavy algorithms. One area that benefits from these advancements is the automation and control industry. Control techniques and algorithms, previously implemented in industrial and commercial personal computers, are being ported to embedded systems, taking advantage of their real-time and customization capabilities. This work introduces a software-based embedded implementation of a single-input single-output adaptive predictive control (APC) algorithm. The goal of the proposed implementation is to minimize execution time of the APC algorithm without affecting the precision of the results. The ZYBO Zynq-7000 development board was the selected platform for development and evaluation. Our proposed software-centric implementation relies on the development of libraries for matrix data storage and manipulation, and data structures to minimize data transactions. Experimental results included the comparison of APC implementations with different memory usage and data management approaches. An improvement on execution time was possible, reducing it nearly 50% from an initial implementation. Experimental results show no impact on the precision after comparing the implementation results to the results obtained using Scilab-based numerical computation.},
keywords={Predictive control;Prediction algorithms;Predictive models;Software algorithms;Industries;Embedded systems;Adaptive predictive control (APC);embedded systems;single-input single-output (SISO) systems;system-on-chip;ZYnq BOard (ZYBO)},
doi={10.1109/TIE.2017.2686333},
ISSN={1557-9948},
month={Sep.},}
@INPROCEEDINGS{1575904,
author={Wilson, T. and Maharaj, S. and Clark, R.G.},
booktitle={Third IEEE International Conference on Software Engineering and Formal Methods (SEFM'05)},
title={Omnibus verification policies: a flexible, configurable approach to assertion-based software verification},
year={2005},
volume={},
number={},
pages={150-159},
abstract={The three main assertion-based verification approaches are: design by contract (DBC), extended static checking (ESC) and verified design by contract (VDBC). Each approach offers a different balance between rigour and ease of use making them appropriate in different situations. Our goal is to explore the use of these approaches together in a flexible way, enabling an application to be broken down into sections with different reliability requirements and different verification approaches used in each section. We explain the benefits of using the approaches together, present a set of guidelines to avoid potential conflicts and give an overview of how the Omnibus IDE provides support for the full range of assertion-based verification approaches within a single tool.},
keywords={Java;Object oriented modeling;Mathematics;Software engineering;Runtime;Project management;HTML;Documentation;Testing;Packaging},
doi={10.1109/SEFM.2005.29},
ISSN={2160-7656},
month={Sep.},}
@INPROCEEDINGS{9632118,
author={Nasyrov, Rashit},
booktitle={2021 3rd International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency (SUMMA)},
title={Evaluating the Effectiveness of Software Development Processes Based on Probabilistic Causal Models},
year={2021},
volume={},
number={},
pages={880-884},
abstract={An Agile methodology for developing software Agile is considered from the point of view of the effectiveness of practical implementation. An approach to assessing the effectiveness of project activities, presented in the form of generalized scenarios, is described. Such scenarios are built on the idea of a causal relationship between the stages of a scenario. The implementation of the Agile approach in the form of a generalized scenario is considered. A method is proposed for assessing the effectiveness of activities for the implementation of scenarios, which can be considered as generalized design solutions based on invariant criteria. For this purpose, a mathematical apparatus based on Markov chains is used. To implement the methodology, software was implemented using modern tools as part of the Python language, the Qt graphical interface library and the NetworkX graph visualization library. An example of evaluating the effectiveness of the software development process within one sprint within the framework of Agile technology is considered.},
keywords={Analytical models;Visualization;Uncertainty;Markov processes;Tools;Probabilistic logic;Software;effectivness evaluation;Agile methodology;causal approach;sequence of actions;Python},
doi={10.1109/SUMMA53307.2021.9632118},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6926011,
author={Watanabe, Yoshikazu and Karino, Shuichi and Saida, Yoshinori and Morita, Gen and Iihoshi, Takahiro},
booktitle={2014 IEEE 19th Real-Time and Embedded Technology and Applications Symposium (RTAS)},
title={STCoS: Software-defined traffic control for smartphones},
year={2014},
volume={},
number={},
pages={297-308},
abstract={Applications can be freely developed and installed into smartphones. Furthermore, smartphones can utilize various networks like private WiFi networks. Such openness of smartphones means that their traffic is uncontrollable for both carriers and users. This creates various problems such as network overloads and security risks that degrade user experience. To address the issue of uncontrollability, we propose STCoS, a software-defined, i.e., programmable, traffic control scheme for smartphones. STCoS is designed to work standalone basis in a smartphone for ease of deployment. It is also designed to have a layered architecture with a well-defined API for improved programmability. STCoS directs traffic of smartphone applications to the appropriate network interface, monitors the traffic, and filters the traffic as needed. It enables control logic to be implemented in the form of a smartphone application. The programmability facilitates real-time traffic control on a moment-to-moment basis. Thus STCoS recovers controllability on smartphone traffic with simple programming. Control logic implementation in example programs, such as a sophisticated WiFi offloading program, demonstrated that STCoS can be used to easily program traffic control. Implementation of STCoS in an Android smartphone demonstrated that the overhead of STCoS is low enough for practical use.},
keywords={Smart phones;Traffic control;IEEE 802.11 Standards;Mobile communication;Mobile computing;IP networks;Databases;Mobile Network;WiFi;Offloading;Smartphone;OpenFlow},
doi={10.1109/RTAS.2014.6926011},
ISSN={1545-3421},
month={April},}
@INPROCEEDINGS{6187327,
author={Reid, W. Mark and Monaco, Christopher A.},
booktitle={2012 IEEE Aerospace Conference},
title={Flight software application framework simplifies development for RBSP spacecraft},
year={2012},
volume={},
number={},
pages={1-7},
abstract={With the trend in spacecraft flight software systems toward the use of message-based architectures, flight software systems are being decomposed into several discrete applications each with a relatively narrow focus. These applications, however, share several common requirements for initialization, command processing, parameter management and telemetry generation. Even with a single common design, if each of these functions were left up to individual application developers, there would be multiple implementations. Each of these implementations would require testing and maintenance, which increases the overall development and maintenance costs and also increases the potential for bugs. In lieu of leaving these functions up to each individual developer of the applications the Radiation Belt Storm Probes (RBSP) Flight Software development team has isolated the commonality across all of the flight software applications and created an application framework. This framework separates the software functions that are common to all applications and the software functions that give a particular application its unique personality. An application deployment tool was also created that allows a developer to create a new application using this framework and insert it into a flight software system in a matter of minutes. The use of an application framework and deployment tool speeds up software development by enabling the creation of an executable application that can receive commands and generate basic telemetry in minutes. This approach, through the separation of the common application code and specific application code allows all applications to use the same overall design while enabling the batch maintenance of the common functionality. This paper discusses the design of the RBSP application framework, deployment tools, the flight software maintenance model, as well as the impact on the flight software development cycle.},
keywords={Software;Space vehicles;Telemetry;Libraries;Instruments;Memory management;Programming},
doi={10.1109/AERO.2012.6187327},
ISSN={1095-323X},
month={March},}
@ARTICLE{5427033,
author={Yang, Xiaoyu and Bruin, Richard P. and Dove, Martin T. and Walkingshaw, Andrew and Mortimer-Jones, Thomas V. and Sinclair, Richard and Wilson, Dan J. and Milman, Victor and Donovan, Tim},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={A Service-Oriented Framework for Running Quantum Mechanical Simulations of Material Properties in a Grid Environment},
year={2010},
volume={40},
number={4},
pages={485-490},
abstract={In this paper, a service-oriented framework for running quantum mechanical simulation of material properties over Grids is proposed, and a prototype framework has been developed. The framework consists of portal and workflow systems, a set of class libraries and application programming interfaces, defined service specifications, schemas, and configuration files. The framework can be instantiated to submit specific quantum mechanical simulation (e.g., CASTEP) jobs to a Grid from a Web browser with the required tasks managed and coordinated by the workflow without human interaction. The paper details analysis, design, and implementation of a prototype framework. In the test case, the prototype framework is instantiated to submit a CASTEP simulation job to available Grid resources in order to calculate the equation of state of a material.},
keywords={Quantum mechanics;Material properties;Computational modeling;Humans;Quantum computing;Geoscience;Virtual prototyping;Portals;Software prototyping;Grid computing;e-Science;Grid computing;service-oriented architecture (SOA);software framework;Web service;workflow},
doi={10.1109/TSMCC.2010.2040826},
ISSN={1558-2442},
month={July},}
@ARTICLE{9007454,
author={Zhu, Yanchao and Liu, Yi and Zhang, Guozhen},
journal={IEEE Access},
title={FT-PBLAS: PBLAS-Based Fault-Tolerant Linear Algebra Computation on High-performance Computing Systems},
year={2020},
volume={8},
number={},
pages={42674-42688},
abstract={As high-performance computing (HPC) systems have scaled up, resilience has become a great challenge. To guarantee resilience, various kinds of hardware and software techniques have been proposed. However, among popular software fault-tolerant techniques, both the checkpoint-restart approach and the replication technique face challenges of scalability in the era of peta- and exa-scale systems due to their numerous processes. In this situation, algorithm-based approaches, or algorithm-based fault tolerance (ABFT) mechanisms, have become attractive because they are efficient and lightweight. Although the ABFT technique is algorithm-dependent, it is possible to implement it at a low level (e.g., in libraries for basic numerical algorithms) and make it application-independent. However, previous ABFT approaches have mainly aimed at achieving fault tolerance in integrated circuits (ICs) or at the architecture level and are therefore not suitable for HPC systems; e.g., they use checksums of rows and columns of matrices rather than checksums of blocks to detect errors. Furthermore, they cannot deal with errors caused by node failure, which are common in current HPC systems. To solve these problems, this paper proposes FT-PBLAS, a PBLAS-based library for fault-tolerant parallel linear algebra computations that can be regarded as a fault-tolerant version of the parallel basic linear algebra subprograms (PBLAS), because it provides a series of fault-tolerant versions of interfaces in PBLAS. To support the underlying error detection and recovery mechanisms in the library, we propose a block-checksum approach for non-fatal errors and a scheme for addressing node failure, respectively. We evaluate two fault-tolerant mechanisms and FT-PBLAS on HPC systems, and the experimental results demonstrate the performance of our library.},
keywords={Fault tolerance;Fault tolerant systems;Libraries;Linear algebra;Computational modeling;Software;Integrated circuits;Algorithm-based fault tolerance;HPC systems;node failure;matrix multiplication;linear algebra computations},
doi={10.1109/ACCESS.2020.2975832},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{213064,
author={Goedicke, M. and Schumann, H. and Cramer, J.},
booktitle={Proceedings of the Sixth International Workshop on Software Specification and Design},
title={On the specification of software components},
year={1991},
volume={},
number={},
pages={166-174},
abstract={The authors discuss how the notion of software component can be turned into a specification language concept. Such component description languages (CDL) would directly support reuse, flexible construction of large systems and team-work. First they analyze important requirements to CDLs and discuss the component support of different languages. Finally they discuss extensions necessary to cover also quantitative aspects in component descriptions.<>},
keywords={Computer science;Software systems;Production facilities;Specification languages;Standardization;Software engineering;Formal languages;Packaging},
doi={10.1109/IWSSD.1991.213064},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6998474,
author={Lin, Zhaowen and Zhang, Can and Ma, Yan and Li, Jian},
booktitle={2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing},
title={A Routing Framework in Software Defined Network Environment},
year={2014},
volume={},
number={},
pages={907-911},
abstract={In this paper we propose a new design and prototype implementation of a routing framework in Software Defined Network (SDN) environment. This framework features a logically centralized control plane by which users could easily manage and monitor their network. It exposes APIs to support various routing protocols and algorithms so as to integrate Open Flow network with homogeneous or heterogeneous networks. We also discuss the motivation, design considerations and potential use cases in this paper.},
keywords={Control systems;Routing;Routing protocols;Network topology;Topology;Ports (Computers);SDN;OpenFlow;Networking;Routing},
doi={10.1109/IIH-MSP.2014.228},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8396594,
author={Godart, Peter and Vieira, Peter and Merewether, Gene and Ubellacker, Wyatt},
booktitle={2018 IEEE Aerospace Conference},
title={Auto-generating real-time capable robotics control software for highly reconfigurable robot platforms},
year={2018},
volume={},
number={},
pages={1-9},
abstract={This paper presents a novel implementation of the CASAH (Control Autonomy for Sampling and Handling) robotics software system used in research and technology development testbeds at the NASA Jet Propulsion Laboratory. Our implementation divides control software into decoupled behavior, user-interface, and hardware-level bus modules. This decoupling at the module level is accomplished by auto-generating human-readable message types that are tailored to the exact hardware topology of whatever system is currently in use. These message types provide modules with a common framework for exchanging state information and relaying commands to devices while being agnostic to the communication protocol itself. We also detail how to structure behavior and bus modules to facilitate modularity and flexibility with third-party software. This software package has been used with success on multiple technology development testbeds at JPL, an example of which is given in this paper, and has proven to provide developers a lightweight and highly reconfigurable system for efficient debugging and practical code sharing.},
keywords={Robots;Software;Hardware;Arrays;Topology;Propulsion;Debugging},
doi={10.1109/AERO.2018.8396594},
ISSN={},
month={March},}
@INPROCEEDINGS{534412,
author={Pacios, L. and de la Pena, A. and Labrador, I. and Carrasco, R. and Lapayese, F.},
booktitle={Proceedings of 16th International Symposium on Fusion Engineering},
title={A versatile Timing System based on OS9 for the Spanish stellarator TJ-II},
year={1995},
volume={2},
number={},
pages={1074-1077 vol.2},
abstract={We describe the Timing System for the TJ-II Stellarator, which is presently under construction in Madrid (Spain), and which is expected to start operation in 1996. The Timing System is an essential element of the TJ-II, that has been commissioned, designed, and built to provide synchronization for all the subsystems, diagnostics and auxiliary heating systems of TJ-II. Its structure is both centralized and distributed. The PSK (phase shift keying) modulation technique is used to distribute simultaneously both timing and event information via fiber optic link. The system provides absolute timing references with a variable time resolution ranging from 500 ns to 1 ms, depending on the span time selected, but in all cases, with a precision of 500 ns. The system allow a wide variety of programmable operating modes and control features to be configured easily in a user friendly environment. These are used to generate the sequence of signals required during each experimental pulse. In addition a new set of libraries and C programs named TEMPO have been developed for the OS9 real time operating system, and a set of VME cards has been designed. Remote access via ethernet LAN and multi-user capabilities are also provided. The Timing System has been fully developed by the TJ-II Monitoring and Control Team at CIEMAT. This paper outlines the detailed design of the hardware and software and gives results from the test phase.},
keywords={Timing;Phase shift keying;Control systems;Heating;Phase modulation;Optical fibers;Optical variables control;Pulse generation;Signal generators;Libraries},
doi={10.1109/FUSION.1995.534412},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{5929981,
author={Carmel-Veilleux, Tennessee and Boland, Jean-François and Bois, Guy},
booktitle={2011 22nd IEEE International Symposium on Rapid System Prototyping},
title={A novel low-overhead flexible instrumentation framework for virtual platforms},
year={2011},
volume={},
number={},
pages={92-98},
abstract={Instrumentation methods for code profiling, tracing and semihosting on virtual platforms (VP) and instruction-set simulators (ISS) rely on function call and system call interception. To reduce instrumentation overhead that can affect program behavior and timing, we propose a novel low-overhead flexible instrumentation framework called Virtual Platform Instrumentation (VPI). The VPI framework uses a new table-based parameter-passing method that reduces the runtime overhead of instrumentation to only that of the interception. Furthermore, it provides a high-level interface to extend the functionality of any VP or ISS with debugging support, without changes to their source code. Our framework unifies the implementation of tracing, profiling and semihosting use cases, while at the same time reducing detrimental runtime overhead on the target as much as 90% compared to widely deployed traditional methods, without significant simulation time penalty.},
keywords={Instruments;Payloads;Runtime;Debugging;Registers;Libraries;Input variables;Computer simulation;Software debugging;Software prototyping;System-level design},
doi={10.1109/RSP.2011.5929981},
ISSN={2150-5519},
month={May},}
@INPROCEEDINGS{6968195,
author={Budihal, Ramachandra and Surendran, R. and Mahendravarman, N. and Jamadagni, H S},
booktitle={2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
title={ANRC hybrid test bed implementation and an End-to-End performance characterization of dynamic spectrum access},
year={2014},
volume={},
number={},
pages={1175-1182},
abstract={An abundance of spectrum access and sensing algorithms are available in the dynamic spectrum access (DSA) and cognitive radio (CR) literature. Often, however, the functionality and performance of such algorithms are validated against theoretical calculations using only simulations. Both the theoretical calculations and simulations come with their attendant sets of assumptions. For instance, designers of dynamic spectrum access algorithms often take spectrum sensing and rendezvous mechanisms between transmitter-receiver pairs for granted. Test bed designers, on the other hand, either customize so much of their design that it becomes difficult to replicate using commercial off the shelf (COTS) components or restrict themselves to simulation, emulation / hardware-in-loop (HIL), or pure hardware but not all three. Implementation studies on test beds sophisticated enough to combine the three aforementioned aspects, but at the same time can also be put together using COTS hardware and software packages are rare. In this paper we describe i) the implementation of a hybrid test bed using a previously proposed hardware agnostic system architecture ii) the implementation of DSA on this test bed, and iii) the realistic hardware and software-constrained performance of DSA. Snapshot energy detector (ED) and Cumulative Summation (CUSUM), a sequential change detection algorithm, are available for spectrum sensing and a two-way handshake mechanism in a dedicated control channel facilitates transmitter-receiver rendezvous.},
keywords={GSM;Programming;Sensors;Monitoring;Logic gates;Switches;Hardware;Test Bed;Hybrid;Cognitive Radio;Dynamic Spectrum Access;Spectrum Sensing;Snapshot Energy Detection;ED;Sequential Change Detection;CUSUM;GNU Radio;USRP;Software Defined Radio;SDR},
doi={10.1109/ICACCI.2014.6968195},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7208628,
author={Jagannath, Jithin and Saarinen, Hanne M. and Drozd, Andrew L.},
booktitle={2015 IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA)},
title={Framework for automatic signal classification techniques (FACT) for software defined radios},
year={2015},
volume={},
number={},
pages={1-7},
abstract={The objective of this work is to design and implement a novel framework for automatic signal classification techniques (FACT) for software defined radios (SDR) capable of classifying multiple signals simultaneously. The focus of this work is to create a modular classification framework to facilitate the testing and implementation of new classification methods. The framework is divided into three parts: (i) Sensor resource manager (SRM), which performs the initial signal detection, preprocessing and the delegation of secondary receivers to the corresponding signals of interest (SOIs); (ii) Modulation classifier block (MCB), which takes the received signal from SRM and performs the required modulation classification and (iv) Data library and statistical block contains all the templates required to perform classification, thresholds for signal detection and also known parameters of expected signals. To prove the feasibility of the framework, FACT is implemented and tested on a Universal Software Radio Peripheral (USRP) test bed using GNU radio signal processing toolkit. We evaluate the performance of signal detection based on the probability of detection (Pd) in varying signal-to-noise ratios (SNR). Additionally, the USRP based experiments demonstrate FACT operating as a single unit, preforming both blind detection and classification of multiple SOIs using different classification methods.},
keywords={Modulation;Signal detection;Signal to noise ratio;Receivers;Detectors;Libraries},
doi={10.1109/CISDA.2015.7208628},
ISSN={2329-6275},
month={May},}
@INPROCEEDINGS{51740,
author={Rodrigo, J.A. and Leon, G.},
booktitle={Second International Conference on Software Engineering for Real Time Systems, 1989.},
title={A layered architecture for simulating distributed operating systems},
year={1989},
volume={},
number={},
pages={151-154},
abstract={One of the main problems to be solved by the designers of policies and algorithms for distributed systems or multiprocessors is their test and evaluation. The solution adopted in the paper is based on the design and construction of a software test bed. DASS, a simulation system to test and design distributed algorithms, is presented. With it, the designer can work in an environment similar to a real network but with increased flexibility enabling change to the parameters or the network itself. The system is implemented, in C language, on a VAX/VMS as a concurrent program composed of a variable number of processes. The creation and synchronization system calls are borrowed from the C/UNIX library available on VMS.<>},
keywords={Simulation;Distributed computing;Multiprocessing;Operating systems;Software verification and validation},
doi={},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8593714,
author={Hoerger, Marcus and Kurniawati, Hanna and Elfes, Alberto},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title={A Software Framework for Planning Under Partial Observability},
year={2018},
volume={},
number={},
pages={1-9},
abstract={Planning under partial observability is both challenging and critical for reliable robot operation. The past decade has seen substantial advances in this domain: The mathematically principled approach for addressing such problems, namely the Partially Observable Markov Decision Process (POMDP), has started to become practical for various robotics tasks. Good approximate solutions for problems framed as POMDPs can now be computed on-line, with a few classes of problems being solved in near real-time. However, applications of these more recent advances are often hindered by the lack of easy-to-use software tools. Implementation of state of the art algorithms exist, but most (if not all)require the POMDP model to be hard-coded inside the program, increasing the difficulty of applying them. To alleviate this problem, we propose a software toolkit, called On-line POMDP Planning Toolkit (OPPT)(downloadable from http://robotics.itee.uq.edu.au/~oppt). By providing a well-defined and general abstract solver API, OPPT enables the user to quickly implement new POMDP solvers. Furthermore, OPPT provides an easy-to-use plug-in architecture with interfaces to the high-fidelity simulator Gazebo that, in conjunction with user-friendly configuration files, allows users to specify POMDP models of a standard class of robot motion planning under partial observability problems with no additional coding effort.},
keywords={Planning;Robot sensing systems;Computational modeling;Observability;Computer architecture;Standards},
doi={10.1109/IROS.2018.8593714},
ISSN={2153-0866},
month={Oct},}
@ARTICLE{8368225,
author={Chen, Weiqi and Chen, Han and Guan, Quansheng and Ji, Fei and Guo, Bingyi},
journal={IEEE Access},
title={Evolutionary Sleep Scheduling in Software-Defined Networks},
year={2018},
volume={6},
number={},
pages={29541-29550},
abstract={The redundant design of communication networks leads to under-utilization of idle devices, which have been reported to consume a significant portion of energy. Thus, it demands a sleep scheduling scheme to improve energy efficiency of communication networks. In this paper, we formulate the optimal sleep scheduling problem from the perspective of routing, which aggregates the traffic loads to fewer active devices by route selection and put the idle devices into sleep to save energy. We then design a genetic algorithm to find out near-optimal sleep scheduling solution, which facilitates the implementation in software-defined networks. Simulation results over network instants from the online database survivable network design library show that our proposed genetic sleep scheduling algorithm outperforms the existing schemes in saving energy.},
keywords={Routing;Energy consumption;Communication networks;Genetic algorithms;Bandwidth;Scheduling;Scheduling algorithms;Green networks;sleeping scheduling;energy efficiency;genetic algorithm},
doi={10.1109/ACCESS.2018.2841985},
ISSN={2169-3536},
month={},}
@ARTICLE{8654231,
author={},
journal={IEEE Std 1636-2009 (Upgraded to Full Use)},
title={IEEE Standard for Software Interface for Maintenance Information Collection and Analysis (SIMICA)},
year={2009},
volume={},
number={},
pages={1-37},
abstract={This document provides an implementation-independent specification for a software interface to information systems containing data pertinent to the diagnosis and maintenance of complex systems consisting of hardware, software, or any combination thereof. These interfaces will support service definitions for creating application programming interfaces (API) for the access, exchange, and analysis of historical diagnostic and maintenance information. This will address the pervasive need of organizations to assess the effectiveness of diagnostics for complex systems throughout the product life cycle. The use of formal information models will facilitate exchanging historical maintenance information between information systems and analysis tools. The models will facilitate creating open system software architectures for maturing system diagnostics.},
keywords={IEEE Standards;Data analysis;Information analysis;Software maintenance;XML;AI-ESTATE;Automated Test Markup Language (ATML);diagnostic maturation;IEEE 1636TM;Maintenance Action Information;maintenance data;Software Interface for Maintenance Information Collection and Analysis (SIMICA);Test Results and Session Information},
doi={10.1109/IEEESTD.2009.8654231},
ISSN={},
month={Aug},}
@INPROCEEDINGS{4641276,
author={Mourad, Azzam and Alhadidi, Dima and Debbabi, Mourad},
booktitle={2008 Sixth Annual Conference on Privacy, Security and Trust},
title={Cross-Language Weaving Approach Targeting Software Security Hardening},
year={2008},
volume={},
number={},
pages={87-98},
abstract={In this paper, we propose an approach for systematic security hardening of software based on aspect-oriented programming and Gimple language. We also present the first steps towards a formal specification for Gimple weaving together with the implementation methodology of the proposed weaving semantics. The primary contribution of this approach is providing the software architects with the capabilities to perform systematic security hardening by applying well-defined solutions and without the need to have expertise in the security solution domain. We explore the viability of our propositions by realizing the weaving semantics for Gimple by implementing it into the GCC compiler and applying our methodologies for systematic security hardening to develop a case study for securing the connections of client applications together with experimental results.},
keywords={Security;Software;Weaving;Programming;Computer languages;Libraries;Laboratories},
doi={10.1109/PST.2008.22},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9152636,
author={Murdock, Kit and Oswald, David and Garcia, Flavio D. and Van Bulck, Jo and Gruss, Daniel and Piessens, Frank},
booktitle={2020 IEEE Symposium on Security and Privacy (SP)},
title={Plundervolt: Software-based Fault Injection Attacks against Intel SGX},
year={2020},
volume={},
number={},
pages={1466-1482},
abstract={Dynamic frequency and voltage scaling features have been introduced to manage ever-growing heat and power consumption in modern processors. Design restrictions ensure frequency and voltage are adjusted as a pair, based on the current load, because for each frequency there is only a certain voltage range where the processor can operate correctly. For this purpose, many processors (including the widespread Intel Core series) expose privileged software interfaces to dynamically regulate processor frequency and operating voltage.In this paper, we demonstrate that these privileged interfaces can be reliably exploited to undermine the system's security. We present the Plundervolt attack, in which a privileged software adversary abuses an undocumented Intel Core voltage scaling interface to corrupt the integrity of Intel SGX enclave computations. Plundervolt carefully controls the processor's supply voltage during an enclave computation, inducing predictable faults within the processor package. Consequently, even Intel SGX's memory encryption/authentication technology cannot protect against Plundervolt. In multiple case studies, we show how the induced faults in enclave computations can be leveraged in real-world attacks to recover keys from cryptographic algorithms (including the AES-NI instruction set extension) or to induce memory safety vulnerabilities into bug-free enclave code. We finally discuss why mitigating Plundervolt is not trivial, requiring trusted computing base recovery through microcode updates or hardware changes.},
keywords={Program processors;Voltage control;Cryptography;Clocks;Regulators},
doi={10.1109/SP40000.2020.00057},
ISSN={2375-1207},
month={May},}
@INPROCEEDINGS{6628381,
author={Doröz, Yarkin and Öztürk, Erdinç and Sunar, Berk},
booktitle={2013 Euromicro Conference on Digital System Design},
title={Evaluating the Hardware Performance of a Million-Bit Multiplier},
year={2013},
volume={},
number={},
pages={955-962},
abstract={In this work we present the first full and complete evaluation of a very large multiplication scheme in custom hardware. We designed a novel architecture to realize a million-bit multiplication architecture based on the Schönhage-Strassen Algorithm and the Number Theoretical Transform (NTT). The construction makes use of an innovative cache architecture along with processing elements customized to match the computation and access patterns of the FFT-based recursive multiplication algorithm. When synthesized using a 90nm TSMC library operating at a frequency of 666 MHz, our architecture is able to compute the product of integers in excess of a million bits in 7.74 milliseconds. Estimates show that the performance of our design matches that of previously reported software implementations on a high-end 3 Ghz Intel Xeon processor, while requiring only a tiny fraction of the area.},
keywords={Computer architecture;Routing;Indexes;Hardware;Algorithm design and analysis;Transforms;Software algorithms;Number Theoretical Transform;FFT;large multiplier;homomorphic encryption},
doi={10.1109/DSD.2013.108},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{818791,
author={Quartel, D.A.C. and Van Sinderen, M.J. and Pires, L.F.},
booktitle={Proceedings 7th IEEE Workshop on Future Trends of Distributed Computing Systems},
title={A model-based approach to service creation},
year={1999},
volume={},
number={},
pages={102-110},
abstract={This paper presents a model-based approach to support service creation. In this approach, services are assumed to be created from (available) software components. The creation process may involve multiple design steps in which the requested service is repeatedly decomposed into more detailed functional parts, until these parts can be mapped onto software components. A modelling language is used to express and enable analysis of the resulting designs, in particular the behaviour aspects. Methods are needed to verify the correctness of each design step. A technique called behaviour refinement is introduced to assess the conformance relation between an abstract behaviour and a more concrete (detailed) behaviour. This technique is based on the application of abstraction rules to determine the abstraction of the concrete behaviour such that the obtained abstraction can be compared to the original abstract behaviour. The application of this refinement technique throughout the creation process enforces the correctness of the created service.},
keywords={Application software;Middleware;Telematics;Concrete;Information technology;Operating systems;Software libraries;Guidelines;Testing;Runtime},
doi={10.1109/FTDCS.1999.818791},
ISSN={1071-0485},
month={Dec},}
@INPROCEEDINGS{8929177,
author={Leung, Jonathan and Shen, Zhiqi and Miao, Chunyan},
booktitle={2019 IEEE International Conference on Agents (ICA)},
title={Goal-Oriented Modelling for Virtual Assistants},
year={2019},
volume={},
number={},
pages={73-76},
abstract={Virtual assistants are used in a wide variety of environments by different types of users. Giving users the ability to build and customize virtual assistants' skills and capabilities would enable them to create virtual assistants that can fit the needs of different scenarios. We propose a model for virtual assistants, based on Goal Net, with the aim of empowering users without programming experience to personalize and customize their virtual assistants. Goal Net separates the design of agent mental models from their low-level implementation. Developers contribute to a library of functions which can be used designers to develop functionality for their virtual assistants. The Multi- Agent Development Environment (MADE) is a graphical tool for creating Goal Net agents and allows users to easily deploy their agents for usage without the need to compile code. A case study is performed to show how Goal Net can be used to develop virtual assistant skills. The proposed model provides a foundation for future work, which would involve human computer interaction and natural language processing.},
keywords={Virtual assistants;Computational modeling;Tools;Programming;Software;Natural language processing;Libraries;Virtual assistant;Goal net;Agent-oriented software engineering;End-user development;Dialogue system},
doi={10.1109/AGENTS.2019.8929177},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7993401,
author={Gerstmayer, Florian and Hausladen, Jurgen and Kramer, Michael and Horauer, Martin},
booktitle={2017 12th IEEE International Symposium on Industrial Embedded Systems (SIES)},
title={Binary protection framework for embedded systems},
year={2017},
volume={},
number={},
pages={1-8},
abstract={Embedded systems empower many products and are used in a variety of applications ranging from smart homes to modern cars. Respective technologies enable new functional features and at the same time improve also non-functional aspects like environmental efficiency. Especially, their inter-connection and coupling with existing networks - in particular to the Internet - allow for an unprecedented boost. However, at the same time security concerns emerge since respective security breaches may have dire consequences ranging from malfunctions, theft, tampering of intellectual property up to threats of safety. This paper presents a generic protection framework for binary file images. The focus of the framework is on hindering reverse engineering and to ensure integrity of embedded systems software. It is designed to be applied in a post-development stage and can be used to add/improve security features of existing products in a number of ways. The concept, a proof-of-concept implementation as well as several key features, such as an in-memory library, a userland-exec implementation, and antidebugging & anti-tampering mechanisms are elaborated.},
keywords={Tools;Hardware;Encryption;Reverse engineering;Embedded systems;embedded software;security;software protection;intellectual property},
doi={10.1109/SIES.2017.7993401},
ISSN={2150-3117},
month={June},}
@INPROCEEDINGS{9793831,
author={Raglianti, Marco},
booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
title={Topology of the Documentation Landscape},
year={2022},
volume={},
number={},
pages={297-299},
abstract={Every software system (ideally) comes with one or more forms of documentation. Beside source code comments, other structured and unstructured sources (e.g., design documents, API references, wikis, usage examples, tutorials) constitute critical assets. Cloud-based repositories for collaborative development (e.g., GitHub, Bitbucket, GitLab) provide many functionalities to create, persist, and version documentation artifacts. On the other hand, the last decade has seen the rise of rich instant messaging clients used as global software community platforms (e.g., Slack, Discord). Although completely detached from a specific versioning system or development workflow, they allow developers to discuss implementation issues, report bugs, and, in general, interact with one another.We refer to this evolving heterogeneous collection of information sources and documentation artifacts as the documentation landscape. It is important to have tools to extract information from these sources and integrate them in a topological visualization, to ease comprehension of a software system. How can we automatically generate this topology? How can we link elements in the topology back to the source code they refer to?The goal of this PhD research is to automatically mine the documentation landscape of a system by disclosing pieces of information to aid, for example, in program maintenance tasks. We present our classification of possible documentation sources. The long term vision is to provide a domain model of the documentation landscape to build, visualize, and explore its instances for real software systems and evaluate the usefulness of the metaphor we propose.},
keywords={Codes;Collaboration;Documentation;Tutorials;Software systems;Topology;Data mining;software documentation;communication platforms;visualization},
doi={10.1145/3510454.3517068},
ISSN={2574-1926},
month={May},}
@INPROCEEDINGS{5629742,
author={Carneiro, Glauco de F. and Silva, Marcos and Mara, Leandra and Figueiredo, Eduardo and Sant'Anna, Claudio and Garcia, Alessandro and Mendonça, Manoel},
booktitle={2010 Brazilian Symposium on Software Engineering},
title={Identifying Code Smells with Multiple Concern Views},
year={2010},
volume={},
number={},
pages={128-137},
abstract={Code smells are anomalies often caused by the way concerns are realized in the source code. Their identification might depend on properties governing the structure of individual concerns and their inter-dependencies in the system implementation. Although code visualization tools are increasingly applied to support anomaly detection, they are mostly limited to represent modular structures, such as methods, classes and packages. This paper presents a multiple views approach that enriches four categories of code views with concern properties, namely: (i) concern’s package-class method structure, (ii) concern’s inheritance-wise structure, (iii)concern dependency, and (iv) concern dependency weight. An exploratory study was conducted to assess the extent to which visual views support code smell detection. Developers identified a set of well-known code smells on five versions of an open source system. Two important results came out of this study. First, the concern-driven views provided useful support to identify God Class and Divergent Change smells. Second, strategies for smell detection supported by the multiple concern views were uncovered.},
keywords={Visualization;Color;Spirals;Couplings;Complexity theory;Software;Tutorials;software visualization;code smells;concerns;program comprehension},
doi={10.1109/SBES.2010.21},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9355245,
author={Lass, Michael and Schade, Robert and Kühne, Thomas D. and Plessl, Christian},
booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
title={A Submatrix-Based Method for Approximate Matrix Function Evaluation in the Quantum Chemistry Code CP2K},
year={2020},
volume={},
number={},
pages={1-14},
abstract={Electronic structure calculations based on density-functional theory (DFT) represent a significant part of today's HPC workloads and pose high demands on high-performance computing resources. To perform these quantum-mechanical DFT calculations on complex large-scale systems, so-called linear scaling methods instead of conventional cubic scaling methods are required. In this work, we take up the idea of the submatrix method and apply it to the DFT computations in the software package CP2K. For that purpose, we transform the underlying numeric operations on distributed, large, sparse matrices into computations on local, much smaller and nearly dense matrices. This allows us to exploit the full floating-point performance of modern CPUs and to make use of dedicated accelerator hardware, where performance has been limited by memory bandwidth before. We demonstrate both functionality and performance of our implementation and show how it can be accelerated with GPUs and FPGAs.},
keywords={Tensors;Temperature;Discrete Fourier transforms;Transforms;Computational efficiency;Sparse matrices;Field programmable gate arrays;Accelerator architectures;Approximate computing;Approximation algorithms;Chemistry;Density functional theory;Linear algebra;Open source software;Parallel algorithms;Reconfigurable architectures;Scientific computing;Solid-state physics},
doi={10.1109/SC41405.2020.00084},
ISSN={},
month={Nov},}
@INPROCEEDINGS{972787,
author={Souter, A.L. and Pollock, L.L.},
booktitle={Proceedings IEEE International Conference on Software Maintenance. ICSM 2001},
title={Incremental call graph reanalysis for object-oriented software maintenance},
year={2001},
volume={},
number={},
pages={682-691},
abstract={A program's call graph is an essential underlying structure for performing the various interprocedural analyses used in software development tools for object-oriented software systems. For interactive software development tools and software maintenance activities, the call graph needs to remain fairly precise and be updated quickly in response to software changes. The paper presents incremental algorithms for updating a call graph that has been initially constructed using the Cartesian Product Algorithm, which computes a highly precise call graph in the presence of dynamically dispatched message sends. Templates are exploited to reduce unnecessary reanalysis as software component changes occur. The preliminary empirical results from our implementation within a Java environment are encouraging. Significant time savings were observed for the incremental algorithm in comparison with an exhaustive analysis, with no loss in precision.},
keywords={Software maintenance;Application software;Performance analysis;Java;Software tools;Information analysis;Software libraries;Data flow computing;Flow graphs;Algorithm design and analysis},
doi={10.1109/ICSM.2001.972787},
ISSN={1063-6773},
month={Nov},}
@ARTICLE{6464270,
author={Milicev, Dragan and Mijailovic, Zarko},
journal={IEEE Transactions on Software Engineering},
title={Capsule-Based User Interface Modeling for Large-Scale Applications},
year={2013},
volume={39},
number={9},
pages={1190-1207},
abstract={We present a novel approach to modeling and implementing user interfaces (UI) of large business applications. The approach is based on the concept of capsule, a profiled structured class from UML which models a simple UI component or a coherent UI fragment of logically and functionally coupled components or other fragments with a clear interface. Consequently, the same modeling concept of capsule with internal structure can be reapplied recursively at successively lower levels of detail within a model, starting from high architectural modeling levels down to the lowest levels of modeling simple UI components. The interface of capsules is defined in terms of pins, while the functional coupling of capsules is specified declaratively by simply wiring their pins. Pins and wires transport messages between capsules, ensuring strict encapsulation. The approach includes a method for formal coupling of capsules' behavior with the underlying object space that provides proper impedance matching between the UI and the business logic while preserving clear separation of concerns between them. We also briefly describe an implementation of a framework that supports the proposed method, including a rich library of ready-to-use capsules, and report on our experience in applying the approach in large-scale industrial systems.},
keywords={Unified modeling language;Business;Couplings;Complexity theory;Object oriented modeling;Buildings;User interfaces;Graphical user interface (GUI);Unified Modeling Language (UML);modeling;model-driven development;software architecture;business applications;data-centric applications;information systems},
doi={10.1109/TSE.2013.8},
ISSN={1939-3520},
month={Sep.},}
@INPROCEEDINGS{7849455,
author={Lee, Jaejin and Seo, Sangmin and Kim, Chihun and Kim, Junghyun and Chun, Posung and Sura, Zehra and Kim, Jungwon and Han, SangYong},
booktitle={2008 International Conference on Parallel Architectures and Compilation Techniques (PACT)},
title={COMIC: A coherent shared memory interface for cell BE},
year={2008},
volume={},
number={},
pages={303-314},
abstract={The Cell BE processor is a heterogeneous multicore that contains one PowerPC Processor Element (PPE) and eight Synergistic Processor Elements (SPEs). Each SPE has a small software-managed local store. Applications must explicitly control all DMA transfers of code and data between the SPE local stores and the main memory, and they must perform any coherence actions required for data transferred. The need for explicit memory management, together with the limited size of the SPE local stores, makes it challenging to program the Cell BE and achieve high performance. In this paper, we present the design and implementation of our COMIC runtime system and its programming model. It provides the program with an illusion of a globally shared memory, in which the PPE and each of the SPEs can access any shared data item, without the programmer having to worry about where the data is, or how to obtain it. COMIC is implemented entirely in software with the aid of user-level libraries provided by the Cell SDK. For each read or write operation in SPE code, a COMIC runtime function is inserted to check whether the data is available in its local store, and to automatically fetch it if it is not. We propose a memory consistency model and a programming model for COMIC, in which the management of synchronization and coherence is centralized in the PPE. To characterize the effectiveness of the COMIC runtime system, we evaluate it with twelve OpenMP benchmark applications on a Cell BE system and an SMP-like homogeneous multicore (Xeon).},
keywords={Microprocessors;Coherence;Programming;Software;Runtime;Multicore processing;Software Distributed Shared Memory;Software Shared Virtual Memory;Heterogeneous Multicores;OpenMP;Cell BE},
doi={},
ISSN={},
month={Oct},}
@ARTICLE{9955513,
author={Aljaafari, Fatimah K. and Menezes, Rafael and Manino, Edoardo and Shmarov, Fedor and Mustafa, Mustafa A. and Cordeiro, Lucas C.},
journal={IEEE Access},
title={Combining BMC and Fuzzing Techniques for Finding Software Vulnerabilities in Concurrent Programs},
year={2022},
volume={10},
number={},
pages={121365-121384},
abstract={Finding software vulnerabilities in concurrent programs is a challenging task due to the size of the state-space exploration, as the number of interleavings grows exponentially with the number of program threads and statements. We propose and evaluate EBF (Ensembles of Bounded Model Checking with Fuzzing) – a technique that combines Bounded Model Checking (BMC) and Gray-Box Fuzzing (GBF) to find software vulnerabilities in concurrent programs. Since there are no publicly-available GBF tools for concurrent code, we first propose OpenGBF – a new open-source concurrency-aware gray-box fuzzer that explores different thread schedules by instrumenting the code under test with random delays. Then, we build an ensemble of a BMC tool and OpenGBF in the following way. On the one hand, when the BMC tool in the ensemble returns a counterexample, we use it as a seed for OpenGBF, thus increasing the likelihood of executing paths guarded by complex mathematical expressions. On the other hand, we aggregate the outcomes of the BMC and GBF tools in the ensemble using a decision matrix, thus improving the accuracy of EBF. We evaluate EBF against state-of-the-art pure BMC tools and show that it can generate up to 14.9% more correct verification witnesses than the corresponding BMC tools alone. Furthermore, we demonstrate the efficacy of OpenGBF, by showing that it can find 24.2% of the vulnerabilities in our evaluation suite, while non-concurrency-aware GBF tools can only find 0.55%. Finally, thanks to our concurrency-aware OpenGBF, EBF detects a data race in the open-source wolfMqtt library and reproduces known bugs in several other real-world programs, which demonstrates its effectiveness in finding vulnerabilities in real-world software.},
keywords={Fuzzing;Instruction sets;Concurrent computing;Software engineering;Programming;Computer bugs;System recovery;Concurrency-aware gray-box fuzzer;bounded model checking;concurrent programs;instrumentation;LLVM pass},
doi={10.1109/ACCESS.2022.3223359},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{744908,
author={Fishman, G.S.},
booktitle={1998 Winter Simulation Conference. Proceedings (Cat. No.98CH36274)},
title={LABATCH.2: software for statistical analysis of simulation sample path data},
year={1998},
volume={1},
number={},
pages={131-139 vol.1},
abstract={LABATCH.2 is a collection of computer programs available in C, FORTRAN, and SIMSCRIPT II.5. It performs statistical analyses on sample sequences collected on strictly stationary stochastic processes. Designed to make its implementation easy for potential users, it may be invoked in-line or from a stored data file. For each sample sequence of length t, LABATCH.2 takes O(t) computing time and O(log/sub 2/t) space. For each series, LABATCH.2 provides an asymptotically valid confidence interval, based on the batch means method, for assessing how well its sample average approximates its true unknown mean. It also produces interim estimates of the variance of the sample average that enable a user to detect systematic error in the latest variance estimate, due to correlation between batches. It also allows a user to assess the extent to which the sample average is free of initial conditions. LABATCH.2 has an interactive option that displays interim results on screen. Based on these quantities, a user may instruct LABATCH.2 to continue execution until the next update or to terminate statistical analysis and write the final results to a file.},
keywords={Statistical analysis;Analytical models;Computational modeling;Displays;Stochastic processes;Algorithms;Operations research;Buildings;Process design;Data analysis},
doi={10.1109/WSC.1998.744908},
ISSN={},
month={Dec},}
@ARTICLE{5387929,
author={Mishelevich, D. J. and Van Slyke, D.},
journal={IBM Systems Journal},
title={Application development system: The software architecture of the IBM Health Care Support/DL/I-Patient Care System},
year={1980},
volume={19},
number={4},
pages={478-504},
abstract={By presenting an architectural overview of PCS, this paper has demonstrated how application development system criteria are met. Such criteria included (1) data independence, (2) logic independence, (3) ease of logic implementation (in PCS via the screen coding and that data collection list mechanism), (4) ease of CRT screen design and coding, (5) ease of printer format design and coding, (6) applicaiton and architecture extensibility (the edit and the Data Manager are examples of architectural extensions), (7) ability of user personnel (not data processing personnel) to perform criteria 3, 4, and 5, and (8) a human-engineered, user-friendly, production system (actually demonstrated by routine, daily use). An important sidelight is that there are commercially available packages that are PCS-based and allow transfer of technology to immediately enahnce productivity of the receiving groups. An example is the Patient Care System-Radiology Installed User Program developed at the Dalls County Hospital District. The application of these concepts has been successful, and again it must be stressed that they are not tied to a given application and thus are “industry-independent.”},
keywords={},
doi={10.1147/sj.194.0478},
ISSN={0018-8670},
month={},}
@INPROCEEDINGS{128129,
author={Zimran, E.},
booktitle={[1990] Proceedings. Rensselaer's Second International Conference on Computer Integrated Manufacturing},
title={Generic material handling system},
year={1990},
volume={},
number={},
pages={380-387},
abstract={An advanced conveyor-based material-handling system called ORTC (on request transport controller) is presented. ORTC consists of a MicroVAX computer under the VMS operating system and a set of distributed BITBUS microcontrollers (nodes) that directly control I/O (input/output) points on the conveyor. Each BITBUS node controls up to eight decision points (or junctions) on the conveyor. A hierarchical control architecture and a flexible and modular software package are implemented to execute the control strategy. The ORTC system is configurable to support many types of conveyor hardware and plant layouts.<>},
keywords={Materials handling;Hardware;Databases;Trademarks;Robustness;Costs;Software design;System recovery;Voice mail;Distributed control},
doi={10.1109/CIM.1990.128129},
ISSN={},
month={May},}
@INPROCEEDINGS{9643663,
author={Popoola, Saheed and Gray, Jeff},
booktitle={2021 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)},
title={Artifact Analysis of Smell Evolution and Maintenance Tasks in Simulink Models},
year={2021},
volume={},
number={},
pages={817-826},
abstract={Bad smells often indicate a possible concern in a software design that may present challenges related to comprehension and maintenance. As a system evolves through a series of changes and maintenance activities, the bad smells embedded in the system may also evolve with the potential for introducing additional new smells. Existing bad smells research often targets textual code-based implementations. We found very little research on bad smells in systems designed with graphical languages that are used often in industry. This paper presents our analysis on the evolution of four bad smells in 575 Simulink models across 31 open-source repositories. We conducted our analysis by creating a chain of model-driven tools that could assist with various analysis needs. Our first step was to extract the evolution history of Simulink models in GitHub. Next, we manually classified each version to a maintenance category (i.e., adaptive, preventive, corrective, or perfective). Then, we developed queries to detect instances of four selected bad smells. Finally, we analysed the evolution of each of the smells across the version history of the repositories, the relationships between the smells and the size of the models, and the impact of maintenance activities on the evolution of the identified bad smells. The results suggest that: 1) larger models tend to contain more types of smells, 2) an increase in the instances of smells is usually associated with an increase in model size, but an increase in model size does not necessarily imply an increase in the number of smells, 3) the majority of bad smells are introduced during the initial construction of the models, although a significant portion of the smells are introduced at later stages, and 4) adaptive maintenance tasks often lead to an increase in the number of smells in Simulink models, but corrective maintenance tasks often correlate with a decrease in the number of smells.},
keywords={Measurement;Analytical models;Adaptation models;Software packages;Maintenance engineering;Model driven engineering;History;Simulink;Bad Smells;Software Maintenance;GitHub},
doi={10.1109/MODELS-C53483.2021.00128},
ISSN={},
month={Oct},}
@INPROCEEDINGS{508084,
author={Dubnicki, C. and Iftode, L. and Felten, E.W. and Kai Li},
booktitle={Proceedings of International Conference on Parallel Processing},
title={Software support for virtual memory-mapped communication},
year={1996},
volume={},
number={},
pages={372-381},
abstract={Virtual memory-mapped communication (VMMC) is a communication model providing direct data transfer between the sender's and receiver's virtual address spaces. This model eliminates operating system involvement in communication, provides full protection, supports user-level buffer management and zero-copy protocols, and minimizes software communication overhead. This paper describes system software support for the model including its API, operating system support, and software architecture, for two network interfaces designed in the SHRIMP project. Our implementations and experiments show that the VMMC model can indeed expose the available hardware performance to user programs. On two Pentium PCs with our prototype network interface hardware over a network, we have achieved user-to-user latency of 4.8 /spl mu/sec and sustained bandwidth of 23 MB/s, which is close to the peak hardware bandwidth. Software communication overhead is only a few user-level instructions.},
keywords={Hardware;Operating systems;Network interfaces;Bandwidth;Protection;Protocols;Communication system software;System software;Software architecture;Personal communication networks},
doi={10.1109/IPPS.1996.508084},
ISSN={},
month={April},}
@ARTICLE{5892917,
author={Adamczewski-Musch, Jörn and Essel, Hans G. and Linev, Sergei},
journal={IEEE Transactions on Nuclear Science},
title={The DABC Framework Interface to Readout Hardware},
year={2011},
volume={58},
number={4},
pages={1728-1732},
abstract={The Data Acquisition Backbone Core (DABC) is a new GSI software framework to run a data acquisition with distributed event building on high performance Linux clusters. Experiment specific front-end electronics is to be integrated to the software by means of hardware interface plug-ins like Device and Transport classes. DABC offers elaborate mechanisms for multiprocessing, buffer management, and dataflow throttling. These are transparently available for all implemented plug-ins. Device plug-ins can link a DABC node to remote readout hardware via network connections like Ethernet. Other Device plug-ins can communicate on the Linux device driver level with custom boards directly inserted at the node. Besides delivering the data input, a DABC Device can also provide control access to the connected hardware. This functionality can be used for setting up, or monitoring the front-ends from the application via DABC parameters and commands. An implementation example is a multipurpose PCI Express Optical Receiver (PEXOR) board developed at GSI. This board features an FPGA and 4 optical links and may be used for various front-ends, depending on the FPGA programming. A kernel driver and the DABC Device plug-in for this board have been developed and tested. They are described here with some performance bench-mark results. As another example, DABC is applied for data taking during test beam times of the Compressed Baryonic Matter (CBM) experiment from 2008 to 2010. Here the front-end readout controller boards (ROC) were integrated to the DABC hardware interface, both for a UDP based Ethernet protocol, and for optical connections to a custom PCIe board.},
keywords={Hardware;Data acquisition;Driver circuits;Bandwidth;Libraries;Instruction sets;Data acquisition;device driver;object oriented programming;software architecture},
doi={10.1109/TNS.2011.2158112},
ISSN={1558-1578},
month={Aug},}
@INPROCEEDINGS{640170,
author={Nakatani, T. and Tamai, T. and Tomoeda, A. and Matsuda, H.},
booktitle={Proceedings of Joint 4th International Computer Science Conference and 4th Asia Pacific Software Engineering Conference},
title={Towards constructing a class evolution model},
year={1997},
volume={},
number={},
pages={131-138},
abstract={Software tends to change itself according to user requirements changes. In the object-oriented technology, it is important to find a way how classes evolve to cope with new user requirements or to obtain reusability. The authors propose that developers need to know the class evolution processes and to forecast their future evolution according to the system, growth. Class specifications and class structures change inevitably, and classes may also be replaced during the class evolution processes. They classify classes into three categories the boundary species, the domain species and the common species, and discuss their evolution processes.},
keywords={Object oriented modeling;Art;Productivity;Programming;Libraries;Volume measurement;Message passing;Size measurement;User interfaces;Graphical user interfaces},
doi={10.1109/APSEC.1997.640170},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5453556,
author={James, Mark L. and Shapiro, Andrew A. and Springer, Paul L. and Zima, Hans P.},
booktitle={2008 International Workshop on Innovative Architecture for Future Generation High-Performance Processors and Systems},
title={Introspection-Based Fault Tolerance for COTS-Based High-Capability Computation in Space},
year={2008},
volume={},
number={},
pages={74-83},
abstract={Future missions of deep space exploration face the challenge of designing, building,and operating progressively more capable autonomous spacecraft and planetary rovers. Given the communication latencies and bandwidth limitations for such missions, the need for increased autonomy becomes mandatory, along with the requirement for enhanced on-board computational capabilities while in deep space or time-critical situations. This will result in dramatic changes in the way missions will be conducted and supported by on-board computing systems. Specifically, the traditional approach of relying exclusively on radiation-hardened hardware and modular redundancy will not be able to deliver the required computational power. As a consequence, such systems are expected to include high-capability low-power components based on emerging Commercial-Off-The-Shelf (COTS) multi-core technology. This paper describes the design of a generic framework for introspection that supports runtime monitoring and analysis of program execution as well as a feedback-oriented recovery from faults. One of the first applications of this framework will be to provide flexible software fault tolerance matched to the requirements and properties of applications by exploiting knowledge that is either contained in an application knowledge base, provided by users, or automatically derived from specifications. A prototype implementation is currently in progress at the Jet Propulsion Laboratory, California Institute of Technology, targeting a cluster of Cell Broadband Engines.},
keywords={Fault tolerance;Application software;Space exploration;Space vehicles;Delay;Bandwidth;Space missions;Time factors;Hardware;Redundancy;space-borne computing;high-performance computing;fault tolerance;introspection},
doi={10.1109/IWIA.2008.11},
ISSN={1537-3223},
month={Jan},}
@INPROCEEDINGS{1247987,
author={Kuchynkova, H.},
booktitle={The IEEE Region 8 EUROCON 2003. Computer as a Tool.},
title={3D modelling software as a tool in teaching at heavy current electrical engineering},
year={2003},
volume={1},
number={},
pages={100-103 vol.1},
abstract={There is a tendency to enable a higher education for much more students in comparison with recent times. This concerns also technical universities. This aim calls for principally new modern methods of education, including creativity, invention and demonstration methods. One of the possibilities is to provide lectures with presentation of up to date computer aided design methods and their use in 3D technical documents preparation. The main goal for implementation of both new CAD packages and contemporary forms of technical documentation is to contribute to an independent creative ability of bachelor students.},
keywords={Software tools;Productivity;Computer science education;Documentation;Production systems;Computer aided manufacturing;Educational technology;Communications technology;Consumer electronics;Power engineering and energy},
doi={10.1109/EURCON.2003.1247987},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4031850,
author={Buckl, Christian and Knoll, Alois and Schrott, Gerhard},
booktitle={2006 International Conference on Software Engineering Advances (ICSEA'06)},
title={Template-Based Development of Fault-Tolerant Embedded Software},
year={2006},
volume={},
number={},
pages={65-65},
abstract={Currently there are different approaches to develop fault-tolerant embedded software: implementing the system from scratch or using libraries respectively specialized hardware. By implementing from scratch the developer has all options concerning system design, the used programming language and hardware. But on the other hand the implementation is errorprone and time- and cost-intensive. The usage of libraries or specialized hardware reduces the design possibilities, while increasing the quality of the developed system and accelerating the development. We present a new technique for developing fault-tolerant systems that combines the advantages of these approaches. We suggest the implementation of reusable templates that solve different aspects of fault-tolerant systems, for example temporal synchronization. In addition we introduce a code generator that realizes a mapping of these templates into application-dependent source code.},
keywords={Fault tolerance;Embedded software;Certification;Fault tolerant systems;Hardware;Software safety;Application software;Software libraries;Operating systems;Embedded system},
doi={10.1109/ICSEA.2006.261321},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1392641,
author={Bruno, G. and Katz, M.J. and Sacerdoti, F.D. and Papadopoulos, P.M.},
booktitle={2004 IEEE International Conference on Cluster Computing (IEEE Cat. No.04EX935)},
title={Rolls: modifying a standard system installer to support user-customizable cluster frontend appliances},
year={2004},
volume={},
number={},
pages={421-430},
abstract={The Rocks toolkit uses a graph-based framework to describe the configuration of all node types (termed appliances) that make up a complete cluster. With hundreds of deployed clusters, our turnkey systems approach has shown to be quite easily adapted to different hardware and logical node configurations. However, the Rocks architecture and implementation contains a significant asymmetry: the graph definition of all appliance types except the initial frontend can be modified and extended by the end-user before installation. However, frontends can be modified only afterward by hands-on system administration. To address this administrative discontinuity between nodes and frontends, we describe the design and implementation of Rolls. First and foremost, Rolls provide both the architecture and mechanisms that enable the end-user to incrementally and programmatically modify the graph description for all appliance types. New functionality can be added and any Rocks-supplied software component can be overwritten or removed simply by inserting the desired Roll CD(s) at installation time. This symmetric approach to cluster construction has allowed us to shrink the core of the Rocks implementation while increasing flexibility for the end-user. Rolls are optional, automatically configured, cluster-aware software systems. Current add-ons include: scheduling systems (SGE, PBS), grid support (based on NSF Middleware Initiative), database support (DB2), Condor, integrity checking (Tripwire) and the Intel compiler. Community-specific Rolls can be and are developed by groups outside of the Rocks core development group.},
keywords={Home appliances;Supercomputers;Hardware;Computer architecture;Distributed databases;Storage area networks;Software packages;File systems;Middleware;Open source software},
doi={10.1109/CLUSTR.2004.1392641},
ISSN={1552-5244},
month={Sep.},}
@ARTICLE{241545,
author={Kornbluh, K.},
journal={IEEE Spectrum},
title={Engineering software-seeing data in action},
year={1993},
volume={30},
number={11},
pages={60-64},
abstract={The latest developments in data analysis and visualization software, including extensive use of color, three-dimensional rotating graphics, representation of equations symbolically or graphically, animation, sound, and more links between data and graphs, are examined. The use of exploratory data analysis (EDA) techniques in the programs and the implementation of existing programs on different platforms, such as Windows/NT, are discussed. A listing of available data analysis and visualization software, in which platforms requirements, and enhancements are discussed, is presented.<>},
keywords={Data engineering;Data analysis;Data visualization;Graphics;Electronic design automation and methodology;Statistical analysis;Equations;Testing;Power engineering and energy;Humans},
doi={10.1109/6.241545},
ISSN={1939-9340},
month={Nov},}
@INPROCEEDINGS{514698,
author={Kontogiannis, K. and DeMori, R. and Bernstein, M. and Galler, M. and Merlo, E.},
booktitle={Proceedings of 2nd Working Conference on Reverse Engineering},
title={Pattern matching for design concept localization},
year={1995},
volume={},
number={},
pages={96-103},
abstract={The effective synergy of a number of different techniques is the key to the successful development of an efficient reverse engineering environment. Compiler technology, pattern matching techniques, visualization tools, and software repositories play an important role for the identification of procedural, data, and abstract-data-type related concepts in the source code. This paper describes a number of techniques used for the development of a distributed reverse engineering environments. Design recovery is investigated through code-to-code and abstract-descriptions-to-code pattern matching techniques used to locate code that may implement a particular plan or algorithm. The code-to-code matching uses dynamic programming techniques to locate similar code fragments and is targeted for large software systems (1MLOC). Patterns are specified either as source code or as a sequence of abstract statements written in an concept language developed for this purpose. Markov models are used to compute similarity measures between an abstract description and or code fragment in terms of the probability that a given abstract statement can generate a given code fragment. The abstract-description-to-code matcher is under implementation and early experiments show it is a promising technique.},
keywords={Pattern matching;Reverse engineering;Software systems;Computer science;Data visualization;Software tools;Algorithm design and analysis;Laboratories;Councils;Computer architecture},
doi={10.1109/WCRE.1995.514698},
ISSN={},
month={July},}
@ARTICLE{566163,
author={Islam, N.},
journal={Computer},
title={Customizing system software using OO frameworks},
year={1997},
volume={30},
number={2},
pages={69-78},
abstract={Today's applications have exploded in their diversity, but most operating systems are still general-purpose and inefficient. One of the benefits of using an OO approach is the ability to modify very small details of an operating system, which makes it easy to tailor the system to the application. My experience indicates that optimizing an operating system for the general case can result in mediocre performance for specialized applications, especially parallel applications. Therefore, I envision a customizable operating system built from components that will allow an optimal match between application behavior and hardware architecture. I propose an object-oriented operating system in which design frameworks support alternative implementations of key systems software services.},
keywords={System software;Concrete;Application software;Operating systems;Control systems;Explosions;Packaging;Collaborative software;Collaborative work;Optimal matching},
doi={10.1109/2.566163},
ISSN={1558-0814},
month={Feb},}
@ARTICLE{8725488,
author={Zandberg, Koen and Schleiser, Kaspar and Acosta, Francisco and Tschofenig, Hannes and Baccelli, Emmanuel},
journal={IEEE Access},
title={Secure Firmware Updates for Constrained IoT Devices Using Open Standards: A Reality Check},
year={2019},
volume={7},
number={},
pages={71907-71920},
abstract={While the IoT deployments multiply in a wide variety of verticals, the most IoT devices lack a built-in secure firmware update mechanism. Without such a mechanism, however, critical security vulnerabilities cannot be fixed, and the IoT devices can become a permanent liability, as demonstrated by recent large-scale attacks. In this paper, we survey open standards and open source libraries that provide useful building blocks for secure firmware updates for the constrained IoT devices–by which we mean low-power, microcontroller-based devices such as networked sensors/actuators with a small amount of memory, among other constraints. We design and implement a prototype that leverages these building blocks and assess the security properties of this prototype. We present experimental results including first experiments with SUIT, a new IETF standard for secure IoT firmware updates. We evaluate the performance of our implementation on a variety of commercial off-the-shelf constrained IoT devices. We conclude that it is possible to create a secure, standards-compliant firmware update solution that uses the state-of-the-art security for the IoT devices with less than 32 kB of RAM and 128 kB of flash memory.},
keywords={Standards;Software;Microprogramming;Internet of Things;Prototypes;Cryptography;Internet of Things;IoT;security;software update;firmware update;open standards;constrained device},
doi={10.1109/ACCESS.2019.2919760},
ISSN={2169-3536},
month={},}
@ARTICLE{8454435,
author={Chen, Deyan and Zhao, Hong},
journal={IEEE Access},
title={Research on the Method of Extracting Domain Knowledge From the Freebase RDF Dumps},
year={2018},
volume={6},
number={},
pages={50306-50322},
abstract={In the process of constructing a domain semantic knowledge base based on ontologies, reusing existing domain knowledge bases not only facilitates sharing, integration, and reuse of the domain semantic knowledge base but also can accelerate the construction of the domain semantic knowledge base. The open and fast growing Freebase database is a good data source, which can be reused to construct the domain semantic knowledge base. However, extracting domain knowledge from the Freebase Resource Description Framework (RDF) dumps faces many challenges. For example, the dump package is too large to read or load; the dump package contains a lot of unnecessary and redundant facts; some ill-formed triples may cause the load to fail, and so on. In response to these obstacles and the deficiencies of existing research, this paper proposes a method to extract domain knowledge quickly, accurately, and completely from the Freebase RDF dumps and describes the domain knowledge using the semantic constructs in ontology standard description languages. Taking extracting the ontology schema and instance data of the medicine domain, including the facts pointing to semantically related domains, as an example, the principle and implementation process of the method are explained in detail and the algorithms of the key processes are described. Finally, the method of this paper is evaluated, including the comparison and analysis of related methods with work objectives, software tools used, processing results, processing performance, accuracy, completeness, and reusability.},
keywords={Ontologies;Resource description framework;Semantics;Knowledge based systems;Data mining;Standards;Freebase;domain semantic knowledge base;ontology;semantic constructs;semantic models},
doi={10.1109/ACCESS.2018.2868516},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{1488820,
author={Ding Wei-long and Xiong Fan-lun and Cheng Zhi-jun},
booktitle={Third International Conference on Information Technology and Applications (ICITA'05)},
title={Study and implementation of the expert system for greenhouse tomato planting},
year={2005},
volume={1},
number={},
pages={325-329 vol.1},
abstract={In order to improve the decision-making capability of the classical expert system of agriculture, an integrated mechanism of the agricultural expert system and the virtual plant growth model simulated the plant's figure is proposed by employing the idea of software integration. Making use of the virtual plant growth model, the decision-making capability of classical expert system can be improved and the decision result can be vividly expressed. The total structure, the modules, the construction of knowledge database, and the built process of the virtual tomato growth model of the greenhouse tomato's expert system are discussed in detail. Finally, an example of the system's application is presented.},
keywords={Expert systems;Decision making;Databases;Engines;Educational institutions;Agriculture;Crops;Visualization;Fertilizers;Libraries},
doi={10.1109/ICITA.2005.264},
ISSN={},
month={July},}
@ARTICLE{4602681,
author={Ratanotayanon, Sukanya and Sim, Susan Elliott},
journal={IEEE Software},
title={Inventive Tool Use to Comprehend Big Code},
year={2008},
volume={25},
number={5},
pages={91-92},
abstract={Software developers often need to understand a large body of unfamiliar code with little or no documentation, no experts to consult, and little time to do it. A post appeared in January 2008 on Slashdot, a technology news Web site, asking for tools and techniques that could help. This article analyzes 301 often passionate and sometimes articulate responses to this query, including the themes and the associated tool recommendations. The most common suggestions were to use a code navigation tool, use a design recovery tool, use a debugger to step through the code, create a runtime trace, use problem-based learning, ask people for help, study the code from top down, and print out all the code. This analysis presents an intriguing snapshot of how software developers in industry go about comprehending big code.},
keywords={Programming profession;Runtime;Unified modeling language;Application software;Software libraries;Printing;Navigation;Testing;software maintenance;program comprehension;navigation and visualization tools},
doi={10.1109/MS.2008.118},
ISSN={1937-4194},
month={Sep.},}
@INPROCEEDINGS{1514033,
author={Nebylov, A.V. and Panferov, A.I. and Brodsky, S.A.},
booktitle={Proceedings. 2005 International Conference Physics and Control, 2005.},
title={Mathematical models, designing, analysis and synthesis of control systems of the complex flexible objects},
year={2005},
volume={},
number={},
pages={491-496},
abstract={Possible approaches to the mathematical description of different types of flexible objects in view of oscillations of liquid and driving weights inside object are observed. Elastic bending of a surface of object and interacting with a surround medium in a broad band of an variation of speed are taken into account. Problems of a synthesis of regulators for control of such objects, parrying of elastic vibrations, and also principles of construction of the universal software for research of dynamic properties and simulations of motion of elastic objects are considered. For implementation of offered methods and algorithms the specialized program is designed. The program is supplied with the program modules library. These modules are designed on the base of mathematical models of the object elements and control system, and also significant physical effects such as flexibility, liquid oscillations, time lag of engines, local aerodynamic effects, etc. Activity of the program is demonstrated and outcomes of calculations are resulted.},
keywords={Control system synthesis;Mathematical model;Control systems;Aerodynamics;Frequency;Vehicles;Engines;Regulators;Vibrations;Aerospace electronics},
doi={10.1109/PHYCON.2005.1514033},
ISSN={},
month={Aug},}
@ARTICLE{5776618,
author={},
journal={IEEE P1666/D3, May 2011},
title={IEEE Draft Standard for Standard SystemC(R) Language Reference Manual},
year={2011},
volume={},
number={},
pages={1-674},
abstract={SystemC(R) is defined in this standard. SystemC is an ANSI standard C++ class library for system and hardware design for use by designers and architects who need to address complex systems that are a hybrid between hardware and software. This standard provides a precise and complete definition of the SystemC class library so that a SystemC implementation can be developed with reference to this standard alone. The primary audiences for this standard are the implementors of the SystemC class library, the implementors of tools supporting the class library, and users of the class library.},
keywords={IEEE standards;Manuals;Software standards;Software systems;C++;computer languages;digital systems;discrete event simulation;electronic design automation;electronic systems;electronic system level;embedded software;fixed-point;hardware description language;hardware design;hardware verification;SystemC;system modeling;system-on-chip;transaction level},
doi={},
ISSN={},
month={May},}
@INPROCEEDINGS{8327007,
author={Yao, Fan and Doroslovacki, Milos and Venkataramani, Guru},
booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
title={Are Coherence Protocol States Vulnerable to Information Leakage?},
year={2018},
volume={},
number={},
pages={168-179},
abstract={Most commercial multi-core processors incorporate hardware coherence protocols to support efficient data transfers and updates between their constituent cores. While hardware coherence protocols provide immense benefits for application performance by removing the burden of software-based coherence, we note that understanding the security vulnerabilities posed by such oft-used, widely-adopted processor features is critical for secure processor designs in the future. In this paper, we demonstrate a new vulnerability exposed by cache coherence protocol states. We present novel insights into how adversaries could cleverly manipulate the coherence states on shared cache blocks, and construct covert timing channels to illegitimately communicate secrets to the spy. We demonstrate 6 different practical scenarios for covert timing channel construction. In contrast to prior works, we assume a broader adversary model where the trojan and spy can either exploit explicitly shared read-only physical pages (e.g., shared library code), or use memory deduplication feature to implicitly force create shared physical pages. We demonstrate how adversaries can manipulate combinations of coherence states and data placement in different caches to construct timing channels. We also explore how adversaries could exploit multiple caches and their associated coherence states to improve transmission bandwidth with symbols encoding multiple bits. Our experimental results on commercial systems show that the peak transmission bandwidths of these covert timing channels can vary between 700 to 1100 Kbits/sec. To the best of our knowledge, our study is the first to highlight the vulnerability of hardware cache coherence protocols to timing channels that can help computer architects to craft effective defenses against exploits on such critical processor features.},
keywords={Coherence;Trojan horses;Timing;Protocols;Hardware;Bandwidth;Security;coherence protocols;covert timing channels;information leakage;hardware security},
doi={10.1109/HPCA.2018.00024},
ISSN={2378-203X},
month={Feb},}
@INPROCEEDINGS{5747448,
author={Walters, John Paul and Kost, Robert and Singh, Karandeep and Jinwoo Suh and Crago, Stephen P.},
booktitle={2011 Aerospace Conference},
title={Software-based fault tolerance for the Maestro many-core processor},
year={2011},
volume={},
number={},
pages={1-12},
abstract={The current generation of radiation-hardened general-purpose processors, such as the RAD750, lag far behind their commercial counterparts in terms of performance. To combat this, a new many-core processor was designed that would allow space applications to leverage up to 49 general-purpose processing cores for high performance space applications. The Maestro processor, based on Tilera's TILE64 chip, is the result of this effort. Maestro is rad-hard by design, but there is still the possibility of both hardware and software errors.},
keywords={Libraries;Fault tolerance;Fault tolerant systems;Tiles;Heart beat;Instruction sets},
doi={10.1109/AERO.2011.5747448},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{8102115,
author={Sharma, Himanshu and Lüthcke, Frederick and Desai, Amay and Demirkiran, Ilteris},
booktitle={2017 IEEE/AIAA 36th Digital Avionics Systems Conference (DASC)},
title={Development of a ground station software for a vertical take-off/landing unmanned aerial vehicle},
year={2017},
volume={},
number={},
pages={1-8},
abstract={With the modernization and commercialization of the unmanned aerial vehicle (UAV) industry, new unmanned aircraft system (UAS) designs are being researched and developed for a variety of applications in the civil market. This project is for a vertical take-off/landing (VTOL) UAV which is being developed for new and cost-effective agricultural solutions. The proposed design entails setting up a new ground station facility for the UAV. This paper addresses the design, simulation, development and implementation of software for this VTOL UAV's transmitter and ground control station receiver.},
keywords={Unmanned aerial vehicles;Receivers;Software packages;Transceivers;Telemetry;Protocols;vertical take-off/landing (VTOL);unmanned aerial vehicle (UAV);ground station;receiver;telemetry;communication},
doi={10.1109/DASC.2017.8102115},
ISSN={2155-7209},
month={Sep.},}
@INPROCEEDINGS{344336,
author={Hong Zu and Ya-Dong Gui and Ni, L.M.},
booktitle={Supercomputing '94:Proceedings of the 1994 ACM/IEEE Conference on Supercomputing},
title={Optimal software multicast in wormhole-routed multistage networks},
year={1994},
volume={},
number={},
pages={703-712},
abstract={Multistage interconnection networks are a popular class of interconnection architecture for constructing scalable parallel computers (SPCs). The focus of the paper is on wormhole routed multistage networks supporting turnaround routing. Existing machines characterized by such a system model include the IBM SP-1, TMC CM-5, and Meiko CS-2. Efficient collective communication among processor nodes is critical to the performance of SPCs. A system level multicast service, in which the same message is delivered from a source node to an arbitrary number of destination nodes, is fundamental in supporting collective communication primitives including the application level broadcast, reduction, and barrier synchronization. The paper addresses how to efficiently implement multicast services in wormhole routed multistage networks, in the absence of hardware multicast support, by exploiting the properties of the switching technology. An optimal multicast algorithm is proposed. The results of implementations on a 64-node SP-1 show that the proposed algorithm significantly outperforms the application level broadcast primitives provided by currently existing collective communication libraries including the public domain MPI.<>},
keywords={Broadcasting;Multicast algorithms;Multiprocessor interconnection networks;Computer architecture;Computer networks;Concurrent computing;Routing;Hardware;Communication switching;Paper technology},
doi={10.1109/SUPERC.1994.344336},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7167418,
author={Rombach, Dieter and Jedlitschka, Andreas},
booktitle={2015 IEEE/ACM 3rd International Workshop on Conducting Empirical Studies in Industry},
title={The Maturation of Empirical Studies},
year={2015},
volume={},
number={},
pages={1-2},
abstract={Conducting empirical studies and transferring their results into industry in a design discipline such as software engineering is ambitious. This is due to contextual restrictions, representativeness as well as problems in aggregating results from individual studies towards guidelines for practitioners. Nevertheless, they are necessary, as scientific contributions need to be challengeable. Significant progress in areas such as measurement, controlled experiments, industrial case studies, empirical based modeling, and packaging knowledge have been made over the past 30 to 40 years. External visibility has been increased significantly by means of books, conferences & journals! Future challenges include attracting more industrial contributions to the existing body of knowledge, using quantitative & qualitative studies to create more trustful evidences, and aggregation of empirical results. These challenges require community efforts.},
keywords={Software engineering;Industries;Software;Communities;Inspection;Companies;Empirical Studies;industry;challenges},
doi={10.1109/CESI.2015.7},
ISSN={},
month={May},}
@INPROCEEDINGS{8588849,
author={Suciu, George and Dragu, Mihaela and Hussain, Ijaz and Iliescu, Ana-Maria and Orza, Oana and Mocanu, Cristian},
booktitle={2018 IEEE 16th International Conference on Embedded and Ubiquitous Computing (EUC)},
title={3D Modeling Using Parrot Bebop 2 FPV},
year={2018},
volume={},
number={},
pages={61-65},
abstract={It is expected that drones will play a major role in connecting the world in future. They will be delivering packages and merchandise, serving as mobile hotspots for broadband wireless access, will deal with surveillance purposes and security of smart cities. Drones, while can be used for the betterment of the community, can also be used by malevolent entities to gather physical and cyber-attacks, and threaten the society. However, we took a different approach using Parrot Bebop 2 by doing 3D modeling of our building. In this paper the main problems and the available solutions are addressed for the generation of 3D models from drone images. Close range photogrammetry has dealt for many years with manual or automatic image measurements for precise 3D modelling. Nowadays drones are also becoming a major source for scanning and snapping, but image-based modelling remains the most complete, portable, flexible and widely used approach. In this article we used 3D modeling as a process of establishing a mathematical representation of a 3-dimensional building by using the software Pix4D or Pix4D cloud. 3D models are now widely used on industrial scale and real estate dealers, architecture, construction, dealing with hazardous situations and product development using 3D models for visualizing, simulating and depiction graphic designs.},
keywords={Three-dimensional displays;Solid modeling;Drones;Buildings;Software;Security;Data models;Parrot Bebop 2 FPV, Pix4D cloud software, 3D modeling, 3D reconstruction, photogrammetry, UAV},
doi={10.1109/EUC.2018.00016},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7119342,
author={Nanjundappa, Mahesh and Shukla, Sandeep K.},
booktitle={Proceedings of the 2014 Forum on Specification and Design Languages (FDL)},
title={Verification of unit and dimensional consistencies in polychronous specifications},
year={2014},
volume={978-2-9530504-9-3},
number={},
pages={1-8},
abstract={Cyber physical systems are characterized by continuous interaction between digital control systems and physical systems. To design critical control software that is to be used in control systems, a modeldriven correct-by-construction approach is preferable. Modeling languages based on synchronous model of time - such as Simulink, State Chart, Esterel, Lustre etc., are often used for sequential software synthesis and languages with a polychronous timing model such as Signal, MRICDF (Multi-Rate Instantaneous Channel-connected Data Flow) etc., are often used for concurrent software synthesis. The interfaces of such software to the real world are through digital signals that are often sampled quantities of physical entities - such as velocity, acceleration, pressure etc. Standard type systems available in programming or modeling languages assign traditional data types such as float, real etc., to these signals. Modelers might mistakenly connect two signals with the same traditional data types but representing different physical entities leading to critical bugs in the synthesized software. Early detection of such mistakes require enhanced type system and type checking algorithms. In this work, we attempt to extend the type system of the polychronous modeling language MRICDF and propose type inference techniques that consider the physical dimensions and units of the signals along with the data types. We also propose an SMT (Satisfiability Modulo Theories) based verification approach that verifies type consistency and provides invariants under which the type consistency is upheld.},
keywords={Clocks;Inference algorithms;Calculus;Data models;Standards;Software packages;F.4.3 Formal Languages;F.3.3.e Type structure;Polychronous language, Model driven;Unit and Dimensions;Type checking;Union types},
doi={10.1109/FDL.2014.7119342},
ISSN={1636-9874},
month={Oct},}
@INPROCEEDINGS{8817116,
author={Li, Cheng and Dakkak, Abdul and Xiong, Jinjun and Hwu, Wen-mei},
booktitle={2019 IEEE World Congress on Services (SERVICES)},
title={MLModelScope: Evaluate and Introspect Cognitive Pipelines},
year={2019},
volume={2642-939X},
number={},
pages={335-338},
abstract={The current landscape of cognitive pipelines exercises many Machine Learning (ML) and Deep Learning (DL) building blocks. These ML and DL building blocks leverage non-uniform frameworks, models, and system stacks. Currently, there is no end-to-end tool that facilitates ML and DL building blocks evaluation and introspection within cognitive pipelines. Due to the absence of such tools, the current practice for evaluating and comparing the benefits of hardware or software innovations on end-to-end cognitive pipelines is both arduous and error-prone - stifling the rate of adoption of innovations. We propose MLModelScope: a hardware/software agnostic platform to facilitate evaluation and introspection of cognitive pipelines in the cloud or on the edge. We describe the design and implementation of MLModelScope and show how it provides a holistic view of the execution of components within cognitive pipelines. MLModelScope aids application developers in experimenting with and discovering cognitive models, data scientists in comparing and evaluating published algorithms, and system architects in optimizing system stacks for cognitive applications.},
keywords={Pipelines;Hardware;Tools;Libraries;Graphics processing units;Data models;Machine learning;Machine Learning;Deep Learning;Performance Profiling;AI Software},
doi={10.1109/SERVICES.2019.00093},
ISSN={2642-939X},
month={July},}
@INPROCEEDINGS{5384883,
author={Xinchun, Yin and Xiaobin, Shen and Fuchao, Yuan and Bing, Mao and Li, Xie},
booktitle={2009 International Forum on Computer Science-Technology and Applications},
title={A Cross-Platform Multifunctional Testbed for Vulnerability Attack},
year={2009},
volume={3},
number={},
pages={372-375},
abstract={Recently, the research on the detection and defense of malicious attacks are becoming the main subject of information security. Various tools and technologies of detecting and defense malicious attacks are proposed in an endless stream, tools detecting vulnerabilities as well. However, there is a lack of method to test and evaluate the correctness and validity of these technologies and tools. In this paper, we first analyzed the characteristics and disadvantages of existing tools and testbeds for vulnerability attack, and discussed popular software bug types, including buffer overflows and integer overflows. On that basis, proposed an enhanced testbed for vulnerability attack. Then introduced the components of the testbed and their design and implementation details. Finally, validated its correctness and availability by experiment.},
keywords={Software testing;Buffer overflow;Application software;Engines;Software libraries;Performance evaluation;Software tools;Computer applications;Educational institutions;Laboratories;Software Vulnerability;Evaluation and Test;Malicious Attacks;Vulnerability Detection},
doi={10.1109/IFCSTA.2009.329},
ISSN={},
month={Dec},}
@INPROCEEDINGS{1553682,
author={Tonella, P.},
booktitle={Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.},
title={Reverse engineering of object oriented code},
year={2005},
volume={},
number={},
pages={724-725},
abstract={During software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. For object oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. Design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all. This tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. The recovered diagrams, represented in UML (Unified Modeling Language), include class, object, interaction (collaboration and sequence), state and package diagrams. A unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. A single running example is referred all over the presentation. Trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.},
keywords={Reverse engineering;Unified modeling language;Programming profession;Collaboration;Packaging;Scattering;Algorithm design and analysis;Software engineering;Software maintenance;Object oriented programming},
doi={10.1109/ICSE.2005.1553682},
ISSN={1558-1225},
month={May},}
@INPROCEEDINGS{6687403,
author={Cao, Zhen and Verbrugge, Clark},
booktitle={2013 42nd International Conference on Parallel Processing},
title={Mixed Model Universal Software Thread-Level Speculation},
year={2013},
volume={},
number={},
pages={651-660},
abstract={Software approaches to Thread-Level Speculation (TLS) have been recently explored, bypassing the need for specialized hardware designs. These approaches, however, tend to focus on source or VM-level implementations aimed at specific language and runtime environments. In addition, previous software approaches tend to make use of a simple thread forking model, reducing their ability to extract substantial parallelism from tree-form recursion programs such as depth-first search and divide-and-conquer. This paper proposes a Mixed forking model Universal software-TLS (MUTLS) system to overcome these limitations. MUTLS is purely based on the LLVM intermediate representation (IR), a language and architecture independent IR that supports more than 10 source languages and target architectures by many projects. MUTLS maximizes parallel coverage by applying a mixed forking model that allows all threads to speculate, forming a tree of threads. We evaluate MUTLS using several C/C++ and Fortran benchmarks on a 64-core machine. On 3 computation intensive applications we achieve speedups of 30 to 50 and 20 to 50 for the C and Fortran versions, respectively. We also observe speedups of 2 to 7 for memory intensive applications. Our experiments indicate that a mixed model is preferable for parallelization of tree-form recursion applications over the simple forking models used by previous software-TLS approaches. Our work also demonstrates that actual speedup is achievable on existing, commodity multi-core processors while maintaining the flexibility of a highly generic implementation context.},
keywords={Parallel processing;Synchronization;Out of order;Hardware;Context;Libraries;Thread-Level Speculation;Parallelization;Forking Model},
doi={10.1109/ICPP.2013.79},
ISSN={2332-5690},
month={Oct},}
@ARTICLE{5446529,
author={Adamczewski-Musch, J. and Essel, H. G. and Kurz, N. and Linev, S.},
journal={IEEE Transactions on Nuclear Science},
title={Dataflow Engine in DAQ Backbone DABC},
year={2010},
volume={57},
number={2},
pages={614-617},
abstract={New experiments at FAIR require new concepts of data acquisition systems. So rather than building hardware trigger configurations with strict latency limitations, self-triggered electronic systems will be utilized. Their front-end components will be time synchronized and will provide data furnished with time stamps. Data streams from a multitude of such components will be forwarded via a powerful sorting network to an event building farm. The Data Acquisition Backbone Core (DABC) is designed as a general purpose software framework for the implementation of such data acquisition systems. It is written in C++. Recently, we made its first version available. In this paper we describe the mechanisms of the data flow engine of DABC.},
keywords={Engines;Data acquisition;Spine;Application software;Sorting;Buildings;Filtering;Personal communication networks;Ethernet networks;Hardware;Data acquisition;software packages},
doi={10.1109/TNS.2010.2042174},
ISSN={1558-1578},
month={April},}
@INPROCEEDINGS{7225262,
author={Thangali, Ashwin and Prasad, Harsha and Kethamakka, Sai and Demirdjian, David and Checka, Neal},
booktitle={2015 IEEE International Symposium on Technologies for Homeland Security (HST)},
title={AESOP: Adaptive Event detection SOftware using Programming by example},
year={2015},
volume={},
number={},
pages={1-7},
abstract={This paper presents AESOP, a software tool for automatic event detection in video. AESOP employs a supervised learning approach for constructing event models, given training examples from different event classes. A trajectory-based formulation is used for modeling events with an aim towards incorporating invariance to changes in the camera location and orientation parameters. The proposed formulation is designed to accommodate events that involve interactions between two or more entities over an extended period of time. AESOP's event models are formulated as HMMs to improve the event detection algorithm's robustness to noise in input data and to achieve computationally efficient algorithms for event model training and event detection. AESOP's performance is demonstrated on a wide range of different scenarios, including stationary camera surveillance and aerial video footage captured in land and maritime environments.},
keywords={Trajectory;Hidden Markov models;Target tracking;Training;Event detection;Computational modeling;Feature extraction},
doi={10.1109/THS.2015.7225262},
ISSN={},
month={April},}
@INPROCEEDINGS{7916110,
author={Danilova, Evgeniya and Kochegarov, Igor and Yurkov, Nikolay},
booktitle={2017 14th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM)},
title={The construction of information measuring system of defects detection in conductive patterns of printed circuit boards},
year={2017},
volume={},
number={},
pages={183-187},
abstract={Modern electronic media are complex systems with a hierarchical structure, one of which elements are printed circuit boards. Their reliability is laid both at the design stage, and the stage of technological control. The aim of the work is to improve the information-measuring PCB control system with the assessment of the impact of technological variations and external factors on the development of operational defects found. To solve this problem morphological image processing techniques, computer simulation, and, in particular, the finite element method and the methods of fuzzy logic are used. For the numerical solution software package ANSYS is applied. The concept of latent defect PCBs is offered. Improved facet classification of PCBs' defects is up-grated. A technique of detection of latent defects in printed circuit boards is described. A structural scheme of the information-measuring PCB control system is proposed in this article. Introduction of the concept latent defect allows you to extend the range of tolerance deviations of the printed track sizes. Methods of detection of defects, including analysis, defect type classification unit and mathematical modeling allows to determine the further development of a latent defect. The proposed structural scheme of information measurement system implements the proposed method.},
keywords={Printed circuits;Finite element analysis;Control systems;Standards;Integrated circuit modeling;Fatigue;Production;printed circuit boards;latent defects;external influencing factors;mathematical model;control technique;information-measuring system},
doi={10.1109/CADSM.2017.7916110},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8576249,
author={Dragojević, Marko and Stević, Stevan and Stupar, Goran and Živkov, Dušan},
booktitle={2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)},
title={Utilizing IoT Technologies for Remote Diagnostics of Next Generation Vehicles},
year={2018},
volume={},
number={},
pages={1-4},
abstract={One of the most notable extensions in next generation vehicles are related to connectivity mechanism. Remote access to the car allows users, manufacturers and service personnel to track, maintain and improve vehicles in the abundance of use cases. Implementation of remote access involves software technologies, which are prone to security threats. On the other hand, proliferation of Internet of Things (IoT) brings connectivity solutions that may be applied in automotive. In this paper, we present a design where existing IoT technology is utilized to enhance automotive middleware (Adaptive AUTOSAR) and enable remote monitoring and diagnostics services for vehicles. This way we give indication of feasibility of today's IoT in automotive context.},
keywords={Bridges;Automotive engineering;Runtime;Safety;Software packages;Time measurement;automotive;software;remote diagnostics;IoT;Adaptive AUTOSAR},
doi={10.1109/ICCE-Berlin.2018.8576249},
ISSN={2166-6822},
month={Sep.},}
@INPROCEEDINGS{4629223,
author={Han, Hyuck and Jung, Hyungsoo and Yeom, Heon Y. and Lee, Dong-Young},
booktitle={2007 IEEE International Conference on Cluster Computing},
title={Taste of AOP : Blending concerns in cluster computing software},
year={2007},
volume={},
number={},
pages={110-117},
abstract={Pioneering work on Aspect Oriented Programming (AOP) has not flourished enough to enrich the design of distributed systems with the refined AOP paradigm. The more generous perspective today is that a decade of growing research on AOP has brought the paradigm into many exciting areas. We investigate two case studies that cover time-honored issues, fault tolerant computing and parallel computing, in the cluster computing world using the AOP paradigm. Aspects that we define here are simple, intuitive and reusable. We believe that our implementation is very useful in developing other cluster computing software, and AOP can be a powerful method in modularizing source codes.},
keywords={Logic gates;Biological system modeling;Software;Programming;Fault tolerance;Fault tolerant systems;Libraries},
doi={10.1109/CLUSTR.2007.4629223},
ISSN={2168-9253},
month={Sep.},}
@ARTICLE{656779,
author={Loques, O. and Leite, J. and Carrera, E.V.},
journal={IEEE Concurrency},
title={P-RIO: a modular parallel-programming environment},
year={1998},
volume={6},
number={1},
pages={47-57},
abstract={The authors discuss the development of their Parallel Reconfigurable Interconnectable Object (P-RIO) environment, which offers a graphical programming tool, modular construction, high portability, a separate configuration language, and runtime support mechanisms for parallel programs. P-RIO provides a middleware-based environment for parallel and distributed programming. Its object-based, software construction methodology facilitates modularity and code reuse, allowing engineers and scientists with little training to build relatively complex programs.},
keywords={Programming profession;Parallel programming;LAN interconnection;Object oriented programming;Computer architecture;Libraries;Encapsulation;Parallel processing;Concurrent computing;Programming environments},
doi={10.1109/4434.656779},
ISSN={1558-0849},
month={Jan},}
@INPROCEEDINGS{5347121,
author={Lu, Caroline and Fabre, Jean-Charles and Killijian, Marc-Olivier},
booktitle={2009 IEEE Conference on Emerging Technologies & Factory Automation},
title={Robustness of modular multi-layered software in the automotive domain: a wrapping-based approach},
year={2009},
volume={},
number={},
pages={1-8},
abstract={New automotive modular multi-layered software organization particularly favors use and interoperability of components-off-the-shelf. However, the integration of software components is error-prone, if their coordination is not rigorously controlled. The risk of failure is increased with the possibility to multiplex software components with heterogeneous levels of criticality, observability. Most of dependability mechanisms, today, address locally errors within each component or report them to further diagnosis services. Instead, we consider a global wrapping-based approach to deal with multilevel properties to be checked on the complete multilayered system at runtime. In this paper, we introduce a framework to design robust software, from analysis to implementation issues, and we illustrate the methodology on simple case study.},
keywords={Robustness;Automotive engineering},
doi={10.1109/ETFA.2009.5347121},
ISSN={1946-0759},
month={Sep.},}
@INPROCEEDINGS{274509,
author={Wales, S.W.},
booktitle={IEE Colloquium on Communications Simulation and Modelling Techniques},
title={An object oriented approach to radio link simulation},
year={1993},
volume={},
number={},
pages={1/9 pp.-},
abstract={A software tool is described that allows a wide variety of digital radio links to be modelled at varying levels of detail. It comprises a library of some 300 C/sup ++/ classes residing within another tool that provides features applicable to a wide range of modelling problems such as parameter handling, graphics facilities and simulation control, as well as a basic graphical user interface. The structure of the class library and the construction of models are described. Examples of particular modelling problems show the benefits of the object oriented approach.<>},
keywords={Digital radio;Graphical user interfaces;Object oriented programming;Software tools;Communication system software},
doi={},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{538578,
author={Chanchio, K. and Xian-He Sun},
booktitle={Proceedings of the 1996 ICPP Workshop on Challenges for Parallel Processing},
title={MpPVM: a software system for non-dedicated heterogeneous computing},
year={1996},
volume={3},
number={},
pages={215-222 vol.3},
abstract={This paper presents the design and preliminary implementation of MpPVM, a software system that supports process migration for PVM application programs in a non-dedicated heterogeneous computing environment. New concepts of migration point as well as migration point analysis and necessary data analysis are introduced. A preliminary implementation of MpPVM and its experimental results are also presented, showing the correctness and promising performance of our process migration mechanism in the scalable non-dedicated heterogeneous computing environment.},
keywords={Software systems;Computer networks;High performance computing;Distributed computing;Parallel processing;Concurrent computing;Data analysis;Sun;Computer science;Application software},
doi={10.1109/ICPP.1996.538578},
ISSN={0190-3918},
month={Aug},}
@INPROCEEDINGS{7760325,
author={Bonnot, Justine and Nogues, Erwan and Menard, Daniel},
booktitle={2016 24th European Signal Processing Conference (EUSIPCO)},
title={New evaluation scheme for software function approximation with non-uniform segmentation},
year={2016},
volume={},
number={},
pages={632-636},
abstract={Modern applications embed complex mathematical processing based on composition of elementary functions. A good balance between approximation accuracy, and implementation cost, i.e. memory space requirement and computation time, is needed to design an efficient implementation. From this point of view, approaches working with polynomial approximation obtain results of a monitored accuracy with a moderate implementation cost. For software implementation in fixed-point processors, accurate results can be obtained if the segment on which the function is computed I is segmented accurately enough, to have an approximating polynomial on each segment. Non-uniform segmentation is required to limit the number of segments and then the implementation cost. The proposed recursive scheme exploits the trade-off between memory requirement and evaluation time. The method is illustrated with the function exp(-√(x)) on the segment [2-6; 25] and showed a mean speed-up ratio of 98.7 compared to the mathematical C standard library on the Digital Signal Processor C55x.},
keywords={Approximation algorithms;Hardware;Software;Memory management;Approximation error;Indexing},
doi={10.1109/EUSIPCO.2016.7760325},
ISSN={2076-1465},
month={Aug},}
@ARTICLE{9361200,
author={Testa, Andrea and Camisa, Andrea and Notarstefano, Giuseppe},
journal={IEEE Robotics and Automation Letters},
title={ChoiRbot: A ROS 2 Toolbox for Cooperative Robotics},
year={2021},
volume={6},
number={2},
pages={2714-2720},
abstract={In this letter, we introduce ChoiRbot, a toolbox for distributed cooperative robotics based on the novel Robot Operating System (ROS) 2. ChoiRbot provides a fully-functional toolset to execute complex distributed multi-robot tasks, either in simulation or experimentally, with a particular focus on networks of heterogeneous robots without a central coordinator. Thanks to its modular structure, ChoiRbot allows for a highly straight implementation of optimization-based distributed control schemes, such as distributed optimal control, model predictive control, task assignment, in which local computation and communication with neighboring robots are alternated. To this end, the toolbox provides functionalities for the solution of distributed optimization problems. The package can be also used to implement distributed feedback laws that do not need optimization features but do require the exchange of information among robots. The potential of the toolbox is illustrated with simulations and experiments on distributed robotics scenarios with mobile ground robots. The ChoiRbot toolbox is available at https://github.com/OPT4SMART/choirbot.},
keywords={Robot kinematics;Task analysis;Optimization;Computer architecture;Decentralized control;Python;Predictive control;Distributed robot systems;optimization and optimal control;software architecture for robotic and automation},
doi={10.1109/LRA.2021.3061366},
ISSN={2377-3766},
month={April},}
@INPROCEEDINGS{6095183,
author={Shakhimardanov, Azamat and Hochgeschwender, Nico and Reckhaus, Michael and Kraetzschmar, Gerhard K.},
booktitle={2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={Analysis of software connectors in robotics},
year={2011},
volume={},
number={},
pages={1030-1035},
abstract={Most of the recent robot software frameworks follow a component-oriented development approach. They allow developers to compose a distributed application from a set of interacting components. Though these frameworks provide rich functionality, often they fail to cope with non-functional aspects (e.g., network scalability, predictability of system behavior) involved in system design, especially in distributed settings. This research sets out to address aforementioned quality attributes by introducing a pragmatic model, Protocol Stack View (PSV), for the analysis of distributed robotic software. The model relies on the fact that a distributed software can be viewed in terms of three main elements: components, ports and connectors. It specifically focuses on structure and semantics of software connectors on the implementation level. To prove effectiveness and usefulness of PSV a set of experiments were conducted to analyze scalability and to determine the configurable elements that affect it. The experiments are based on the comparison of communication infrastructure provided by two existing software packages, namely ROS and ZeroMQ.},
keywords={Periodic structures;Software;Robots},
doi={10.1109/IROS.2011.6095183},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{8884933,
author={Langiu, Antonio and Boano, Carlo Alberto and Schuß, Markus and Römer, Kay},
booktitle={2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)},
title={UpKit: An Open-Source, Portable, and Lightweight Update Framework for Constrained IoT Devices},
year={2019},
volume={},
number={},
pages={2101-2112},
abstract={Updating the software running on constrained IoT devices such as low-power sensors and actuators in a secure and efficient way is an open problem. The limited computational, memory, and storage capabilities of these devices, together with their small energy budget, indeed, restrict the number of features that can be embedded into an update system and make it also difficult to build a generic and compact solution. As a result, existing update systems for constrained IoT devices are often not portable, do not perform a proper verification of the downloaded firmware, or focus only on a single phase of the update process, which exposes them to security threats and calls for new solutions. In this paper we present UpKit, a portable and lightweight software update framework for constrained IoT devices encompassing all phases of the update process: from the generation and signature of a new firmware, to the transmission of the latter to an IoT device, its verification and installation. UpKit employs a novel update architecture that is agnostic to how new firmware images are distributed and that introduces a double-signature process to guarantee the freshness of a new firmware. This, together with an additional verification step, allows also to reject invalid software at an early stage and to prevent an unnecessary reboot of the device. We keep UpKit's design modular and provide an open-source implementation for several operating systems, hardware platforms, as well as cryptographic libraries. We further include support for differential updates and flexible memory slots, which allows to significantly increase the efficiency of the update process. An experimental evaluation shows that UpKit can be used to efficiently update highly-constrained IoT devices, and that it has a comparable memory footprint to state-of-the-art solutions, despite the introduction of several features.},
keywords={Performance evaluation;Hardware;Cryptography;Servers;Libraries;Open source software;Constrained devices;Contiki;Internet of Things;Pull and Push approach;RIOT;Software Updates;Zephyr},
doi={10.1109/ICDCS.2019.00207},
ISSN={2575-8411},
month={July},}
@INPROCEEDINGS{7301003,
author={Harris, Daniel R.},
booktitle={2015 IEEE International Conference on Information Reuse and Integration},
title={Modeling Reusable and Interoperable Faceted Browsing Systems with Category Theory},
year={2015},
volume={},
number={},
pages={388-395},
abstract={Faceted browsing has become ubiquitous with modern digital libraries and online search engines, yet the process is still difficult to abstractly model in a manner that supports the development of interoperable and reusable interfaces. We propose category theory as a theoretical foundation for faceted browsing and demonstrate how the interactive process can be mathematically abstracted. Existing efforts in facet modeling are based upon set theory, formal concept analysis, and light-weight ontologies, but in many regards, they are implementations of faceted browsing rather than a specification of the basic, underlying structures and interactions. We will demonstrate that category theory allows us to specify faceted objects and study the relationships and interactions within a faceted browsing system. Implementations can then be constructed through a category-theoretic lens using these models, allowing abstract comparison and communication that naturally support interoperability and reuse.},
keywords={Taxonomy;Set theory;Yttrium;Drugs;Central nervous system;Ontologies;Lattices;Data models;interactive systems;software reusability;information architecture},
doi={10.1109/IRI.2015.65},
ISSN={},
month={Aug},}
@INPROCEEDINGS{4433344,
author={de Lima, Carlos H. M. and Stancanelli, Elvis M. G. and Rodrigues, Emanuel B. and Maciel, Jean M. da S. and Cavalcanti, Francisco R. P.},
booktitle={2006 International Telecommunications Symposium},
title={A software development framework based on C++ OOP language for link-level simulation tools},
year={2006},
volume={},
number={},
pages={597-602},
abstract={This article introduces a software development framework, which amasses Object-Oriented Programming (OOP) concepts and designed procedures, intended to systematize the implementation of link-level simulation tools. This development framework is fully implemented in C++ programming language providing modularity and reusability (improving the coding activity). This framework then constitutes remarkable tool for quickly creating link-level applications. Aiming to evaluate the applicability of this software structure, the conversational service class transmitting at 12.2 kbps and over the Wideband CDMA (WCDMA) Downlink (DL) Dedicated Channel (DCH) is addressed as a case-study and therefore potential benefits of utilizing such development library are identified. It is shown that the development framework provides flexibility for the software development of link-level simulators remarkably reducing the design, programming and testing stages.},
keywords={Object oriented modeling;Mathematical model;Multiaccess communication;Power system modeling;Object oriented programming;Computer languages;Application software;Downlink;Employment;Power system reliability;Development Framework;OOP;C++;IT++;WCDMA;DL;DCH},
doi={10.1109/ITS.2006.4433344},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7473605,
author={Cunha, João Bruno and Neto, Arthur Ayres},
booktitle={2015 IEEE/OES Acoustics in Underwater Geosciences Symposium (RIO Acoustics)},
title={MAPIRA — A support decision tool for marine pipeline risk assessment},
year={2015},
volume={},
number={},
pages={1-4},
abstract={The lack of a supporting decision tool in submarine pipeline launching activity inspired this work, which intends to provide a quantitative measure about the risks involved in this kind of job. This tool uses Graphs, Fuzzy logic, AHP (Analytic Hierarchy Process) and the help of many experienced professionals to achieve this goal. The Theory of Graphs (Euler, 1736) studies the relationship between objects in a specific mathematical set and has a fundamental unity which is composed by a structure of nodes that interconnect the objects through their edges, the so called Graph. Nowadays, the graphs are used to model real spatial distributions in a computational environment in order to solve routing problems, network flows and flowing streams of any kind. The Fuzzy Logic is able to deal with uncertainty better than Crispy logic, bringing more confidence in supporting decisions regarding subjective issues. Fuzzy Inference Systems are supporting decision making in many areas. This project uses graphs abstraction and Fuzzy Inference System in Submarine Engineering scope to aid in the marine pipelines' route selecting problem. The area where the pipe will be placed is surveyed with acoustic geophysical methods as bathymetry, side-scan sonar and sub-bottom profilers. These resultant maps are then interpreted using the risk classification developed in this work and the gridded area is related to a graph, where each graph node represents a grid node, and a graph edge represents the path between two adjacent nodes with a specific weight. The object of interest of this project is the method responsible for the weight input (risks magnitudes) into graph's edges based on a Fuzzy Inference System. The Fuzzy Inference System uses AHP and a compilation of many specialists' opinions. These opinions were collected from pipeline engineers with many years of experience. They went through an online questionnaire about several factors that represent risks to pipelines such as tectonic zones, faulting, subsurface gases, different types of ocean floor, and marine physiographic features, presence of garbage, areas susceptible to freezing and domains taken by other submarine facilities. Once each graph's edge has the weight given by the risk magnitude the safer path can be calculate by the shortest path algorithm. This work is based on a methodology which was applied in many Civil engineering construction projects. This new methodology coded in Java® programing language resulted in a Software called MAPIRA (Marine Pipeline Risk Assessment), a support decision tool that intends to aid marine pipeline launching professionals. It uses GeoTools® libraries (Since 2006 in OSGeo Project) and brings a way to solve the safer path problem to marine pipelines installation and providing a quantitative measure about the risks marine pipelines could face.},
keywords={Pipelines;Fuzzy logic;Geology;Risk management;Underwater vehicles;Geophysical measurements;Geologic measurements;risk;assessment;AHP;Fuzzy;Graphs;MAPIRA;marine;pipelines},
doi={10.1109/RIOAcoustics.2015.7473605},
ISSN={},
month={July},}
@INPROCEEDINGS{6732323,
author={Plumbridge, Gary and Audsley, Neil C.},
booktitle={2013 International Conference on Reconfigurable Computing and FPGAs (ReConFig)},
title={Programming FPGA based NoCs with Java},
year={2013},
volume={},
number={},
pages={1-6},
abstract={FPGAs enable NoC architecture experimentation, although to be effective they need to be supported by tools and frameworks for construction of the NoC and effective software programming of the NoC. In this paper, we focus upon effective programming of the NoC using Java, complementing previous work which proposes the Blueshell framework for NoC generation for FPGAs. The approach taken is called Network-Chi, providing a number of key extensions to the Chi Java compiler. This includes provision of a networking API within Java giving a mesh based abstraction for network communication, allowing the programmer to send Java objects to other nodes without consideration for the underlying hardware topology or protocols; and a region-based memory management API that enables the definition of transient allocation contexts that discard all objects allocated within them when they reach the end of execution. Results show the approach taken to be efficient and effective.},
keywords={Java;Program processors;Runtime;Programming;Resource management;Context;Standards},
doi={10.1109/ReConFig.2013.6732323},
ISSN={2325-6532},
month={Dec},}
@ARTICLE{9552864,
author={Qi, Yue and Zhang, Xinliang and Vaezi, Mojtaba},
journal={IEEE Access},
title={Over-the-Air Implementation of NOMA: New Experiments and Future Directions},
year={2021},
volume={9},
number={},
pages={135828-135844},
abstract={Non-orthogonal multiple access (NOMA) is widely recognized to increase the number of users and enhance the spectral efficiency in fifth-generation (5G) wireless networks and beyond. NOMA is still in the theoretical analysis and simulation phases and fewer experimental works are reported to date. In this paper, we design and implement NOMA in software-defined radio, and evaluate its performance. This includes the real-time realization of the key components of NOMA, i.e., superposition and successive interference cancellation. The main novelty of this paper is to introduce constructing superimposed signals with varying symbol rates to enlarge the achievable rate region of the experimented NOMA. By applying varying symbol rates, the set of possible transmission rate pairs enlarges and we can reach higher data rates compared to existing modulation and coding schemes (MCS). We also propose an algorithm to efficiently find the rate pairs. Simulations and experiments demonstrate that NOMA with a varying symbol rate not only can reach higher data rates than orthogonal schemes such as time division multiple access, but it can also outperform existing MCS-based methods which have a fixed symbol rate. The experiments also show that there is a noticeable gap between NOMA in theory and practice. In addition to the new NOMA experiments, we review the state-of-the-art in experimental NOMA. We also discuss several directions for future experiments that can help bridge the gap between theory and practice and bring NOMA to practical communication systems.},
keywords={NOMA;Silicon carbide;Downlink;Throughput;Spectral efficiency;MIMO communication;Interference cancellation;NOMA;superposition coding;successive interference cancellation;software-defined radio (SDR);USRP},
doi={10.1109/ACCESS.2021.3116613},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6147738,
author={Muller, Dan},
booktitle={Proceedings of the 2011 Winter Simulation Conference (WSC)},
title={Automod™ - providing simulation solutions for over 25 years},
year={2011},
volume={},
number={},
pages={39-51},
abstract={Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model components in other models, decreasing the time required to build a model. In addition, recent enhancements to AutoMod's material handling template systems have in-creased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages.},
keywords={Solid modeling;Load modeling;Animation;Materials;Statistical distributions;Process control},
doi={10.1109/WSC.2011.6147738},
ISSN={1558-4305},
month={Dec},}
@INPROCEEDINGS{4382141,
author={Chen, Hun-Chen and Yen, Jui-Cheng and Juan, Jui-Hsiang and Fan, Kuo-Tai and Wu, Shu-Meng},
booktitle={2007 IEEE International Symposium on Consumer Electronics},
title={A New Cryptography System and its IP Core Design for Multimedia Application},
year={2007},
volume={},
number={},
pages={1-7},
abstract={In this paper, we have proposed a new cryptography system which combines both the position permutation and the value transformation encryption methods. Three good features involve in this system: (1) High security evaluated with the measure of fractal dimension, (2) The content of encrypted image is sensitive to the initial key, and (3) This system can easily defense against the exhaustive search attack. Besides, for the requirement of real-time in multimedia system, we also proposed the high performance reconfigurable architecture for this system as well as the IP core generator software. The proposed IP core generator can be parameterized by the parameters of system-type, packet size, throughput and security to create the proper IP core for the applications. All the architectures generated from the IP core generator have been verified; except for the coding guideline checking, there exist 100% code coverage. According to the UMC 0.18 um cell library, we further verified all the configurations of architecture for speed, area and power consumption as well as delivering the essential scripts. The verifications of all the configurations, the throughput can be ranged between 1.59 and 2.25 Gbps with the hardware cost of 0.54 and 3.92 mm2. Compared with the existing designs, the proposed design possesses performance enough for most of multimedia system applications.},
keywords={Cryptography;Multimedia systems;Security;Throughput;Fractals;Real time systems;Reconfigurable architectures;Software performance;Application software;Computer architecture},
doi={10.1109/ISCE.2007.4382141},
ISSN={2159-1423},
month={June},}
@INPROCEEDINGS{6671858,
author={Stoica, Florin and Stoica, Laura Florentina},
booktitle={2013 21st International Conference on Software, Telecommunications and Computer Networks - (SoftCOM 2013)},
title={Building a new CTL model checker using Web services},
year={2013},
volume={},
number={},
pages={1-5},
abstract={This Computation Tree Logic (CTL) is widely used to capture compositions of reactive systems. Model checking is particularly well-suited for the automated verification of finite-state systems, both for software and for hardware. A CTL model checker tool allows designers to automatically verify that systems satisfy specifications expressed in the language of CTL logic. In this paper we present a new CTL model checker implemented in client-server paradigm. CTL Designer, the client tool, allows an interactive construction of the CTL models as state-transition graphs. Java and C# APIs are provided for programmatic construction of large models. The server part of our tool embeds the core of the CTL model checker and is published as a Web service. The performance evaluation in terms as speed and scalability was accomplished implementing an algorithm to find a winning strategy in the Tic-Tac-Toe game.},
keywords={Model checking;Computational modeling;Unified modeling language;Games;Data structures;Web services;Model Checking;CTL;Web services},
doi={10.1109/SoftCOM.2013.6671858},
ISSN={},
month={Sep.},}
@ARTICLE{8981941,
author={Shi, Yunbo and Wang, Yanlin and Feng, Hengzhen and Zhao, Rui and Cao, Huiliang and Liu, Jun},
journal={IEEE Access},
title={Design, Fabrication and Test of a Low Range Capacitive Accelerometer With Anti-Overload Characteristics},
year={2020},
volume={8},
number={},
pages={26085-26093},
abstract={Capacitive accelerometers have been widely used in the fields of mobile phones, automobiles, seismic monitoring and others because of its high sensitivity, good repeatability and high precision. This type of accelerometer usually has a low range. The sensitive structure of the sensor is too vulnerable to damage in high impact environments, so it basically has no ability to detect smaller signals after a relatively high acceleration. This paper presents a capacitive accelerometer which employs a four-terminal fixed structure. Acceleration is outputted by a differential capacitance formed between the mass and the upper and lower glass plates. With consideration of better anti-overload capability and small signal detection capability, structure optimization and anti-overload protection such as chamfer and protection measures have been carried out. According to the simulation results of the optimized structure by finite element analysis software, it has a maximum stress of 62 Mpa when a 20,000g shock is load, the maximum displacement within effective range (100g) is 13.7μm, and it also has a first-order frequency response of 7.477 kHz. And then, the process flow is designed and the device is fabricated. Static capacitance test and flip test are utilized to verify its static performance. The anti-overload capability of the device is tested and verified by Marshall Hammer impact experiments.},
keywords={Accelerometers;Stress;Sensitivity;Structural beams;Acceleration;Silicon;Capacitance;Microelectromechanical systems;capacitive sensors;sandwich structure;microfabrication;anti-overload},
doi={10.1109/ACCESS.2020.2969723},
ISSN={2169-3536},
month={},}
@ARTICLE{10002307,
author={Audrito, Giorgio and Terraneo, Federico and Fornaciari, William},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={FCPP+Miosix: Scaling Aggregate Programming to Embedded Systems},
year={2023},
volume={34},
number={3},
pages={869-880},
abstract={As the density of nodes capable of sensing, computing and actuation increases, it becomes increasingly useful to model an entire network of physical devices as a single, continuous space-time computing machine. The emergent behaviour of the whole software system is then induced by local computations deployed within each node and by the dynamics of the information diffusion. A relevant example of this distribution model is given by aggregate programming and its minimal set of functional constructs used to manipulate distributed data structures evolving over space and time, and resulting in robustness to changes. In this paper, we propose the first implementation of the aggregate computing paradigm targeting microcontrollers, by integrating FCPP, a C++ implementation of the paradigm, with Miosix, a modern operating system for microcontrollers with full C++ support. To the best of the author's knowledge, we are the first to present results on the effectiveness of FCPP in an embedded operating system setting as opposed to a simulation environment, thus considering tight memory and computational constraints and accounting for packet losses due to nonidealities of the radio channel. We implemented and tested on a network of WandStem nodes two benchmark applications: a network connectivity checker for network planning and preventive maintenance, and a decentralised contact tracing application. Additionally, we show that common problems in sensor networks such as neighbour discovery, construction of a graph of the network topology, coarse grain clock synchronisation as well as network monitoring and the collection of statistics (such as memory occupation data) can be easily performed thanks to the expressive semantics of aggregate programming.},
keywords={Aggregates;Operating systems;Programming;Microcontrollers;Embedded systems;Libraries;Java;Aggregate programming;distributed systems;embedded systems},
doi={10.1109/TPDS.2022.3232633},
ISSN={1558-2183},
month={March},}
@INPROCEEDINGS{1342103,
author={Huang Xuemei and Wang Yuechao and Tan Dalong and Zhao Mingyang and Meng Fanli},
booktitle={Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788)},
title={Study on integration and modeling of simulation platform of reconfigurable assembly line},
year={2004},
volume={3},
number={},
pages={2768-2772 Vol.3},
abstract={Agent-based and holonic manufacturing are considered as effective means for implementation of manufacturing system flexibility and reconfigurability. Before application to real manufacturing world, thorough evaluation is absolutely necessary. Simulation platform based on digital manufacturing provides effective approach on evaluation of reconfigurable assembly line based on cooperation and coordination mechanisms of multi-agent and holonic manufacturing methodologies. Production line digital manufacturing environment provides environment for design, engineering application and performance analysis for manufacturing system with multi equipments. Aiming at construction of this complex system, system integration based on manufacturing data, sub-system functional relation and software platform were studied. Tree-structure database support platform and whole digital model for manufacturing resource entities were analyzed.},
keywords={Virtual manufacturing;Automobile manufacture;Assembly systems;Manufacturing automation;Manufacturing systems;Mechanical engineering;Production systems;Design engineering;Application software;Performance analysis},
doi={10.1109/WCICA.2004.1342103},
ISSN={},
month={June},}
@INPROCEEDINGS{9007333,
author={Gavrilov, Dmitriy and Lazarenko, Lyubov and Zakirov, Emil},
booktitle={2019 International Conference on Artificial Intelligence: Applications and Innovations (IC-AIAI)},
title={AI Recognition in Skin Pathologies Detection},
year={2019},
volume={},
number={},
pages={54-542},
abstract={Skin cancer is the most common type of cancer [1]. Between different malignant skin pathology melanoma is the most fleeting and mortality. Despite the superficial location of pathologies, only half of patients seek medical assistance on the early stages[2]. Treatment on the early (epidermal) stage provides a significantly higher chance of recovery. To assist a wide range of people in the early skin cancer detection, a software package was developed. The software based on deep convolutional neural networks technology. This complex allows to classify normal and malignant pathology on the uploaded photos. In clinical practice doctors use the ABCDE symptom's complex. This complex characterizes the observation of pigment spot asymmetry, border irregularities, color unevenness, diameter, and evolution [3]. The machine learning approach involves the computer evaluating similar factors when processing multiple images of different skin formations. The paper presents an algorithm for classification of skin lesions into pathology and norm using convolutional neural network architecture Xception with prior images segmentation. The upper classifying layers were frozen and new ones were added to classify skin diseases in the pre-trained neural network Xception. As a result, the classification of benign and malignant skin tumors provided at least 89% accuracy. At the moment, the result of research work is designed in form of application software that allows to download the image of pigmented skin spots from the camera. It is available on https://skincheckup.online.},
keywords={Skin;Pathology;Melanoma;Training;Image color analysis;preventive care, artificial intelligence, CNN, neural networks, recommendation systems, medical information systems},
doi={10.1109/IC-AIAI48757.2019.00017},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6297557,
author={Teubl, Fernando and Kurashima, Celso and Cabral, Marcio and Zuffo, Marcelo},
booktitle={2012 14th Symposium on Virtual and Augmented Reality},
title={FastFusion: A Scalable Multi-projector System},
year={2012},
volume={},
number={},
pages={26-35},
abstract={Multi-projector systems offer both higher resolution and brightness by using a cluster of projectors, and it can provide better visual quality when compared to traditional systems using a single high performance projector. When we consider the high cost associated with high-end projectors, the use of multiple low cost projectors can reduce considerably the cost of such installation. This article presents the research and development of a scalable multi-projection system that enables the construction of virtual reality systems with a large number of projectors and graphics computers, and that is capable of achieving a high resolution display. We demonstrate the viability of such system with the development of a camera-based multi-projector system called FastFusion, which automatically calibrates casually aligned projectors to properly blend different projections. Our system software improves known algorithms in the literature for projector calibration and image blending. The main improvement is a more efficient distribution of the calibration process. In addition, since our library proposes a new architecture that is able to manage many projectors, it may lead to the development of Immersion Systems with retina resolution. FastFusion has been tested and validated by virtual reality applications. In this work, we analyze the visual performance of FastFusion in a CAVE system with three walls, eighteen projectors and nine computers.},
keywords={Cameras;Calibration;Libraries;Computers;Virtual reality;Software algorithms;Multi-projector System;Virtual Reality;Visual Computer},
doi={10.1109/SVR.2012.1},
ISSN={},
month={May},}
@INPROCEEDINGS{4792720,
author={Chen, M. and Saberi, Ali and Sannuti, Peddapullaiah and Shamash, Yacov},
booktitle={1992 American Control Conference},
title={Loop transfer recovery for general nonminimum phase discrete time systems - part 2: design},
year={1992},
volume={},
number={},
pages={3108-3112},
abstract={This part focuses on the design of controllers for the recovery of target loop transfer function or sensitivity and complimentary sensitivity functions for general nonminimum phase discrete time systems. For general systems, loop transfer recovery is not completely feasible although there exists considerable amount of freedom to shape the inevitable recovery error. Here the necessary design constraints and the available design freedom are reviewed. In view of the available freedom, possible specifications on the eigenstructure of the observer dynamic matrix are formulated. Three different types of controllers which are respectively based on prediction, current and reduced order estimators, are considered. For each one of such controllers, three different design techniques are developed. The first one is an eigen-structure assignment scheme, and the other two are optimisation based designs. Eigenstructure assignment method yields a controller design which achieves any chosen recovery error matrix among a set of admissible recovery error matrices. On the other hand, one of the optimisation based design methods leads to a controller that achieves a recovery error matrix having the infimum H∞ norm, while the other does the same except it achieves a recovery error matrix having the infimum H2 norm. All the developed design methods are implemented in a 'Matlab' software package.},
keywords={Discrete time systems;Hydrogen;Design methodology;Transfer functions;Error correction;Optimization methods;Computer science;Robust control;Tellurium;Adaptive control},
doi={10.23919/ACC.1992.4792720},
ISSN={},
month={June},}
@INPROCEEDINGS{6527679,
author={Kamal, C. and Ramkumar, S. and Hariharan, N.},
booktitle={2013 International Conference on Power, Energy and Control (ICPEC)},
title={Experimental verification and implementation of a isolated ZVT boost converter for high step-up electric traction},
year={2013},
volume={},
number={},
pages={346-349},
abstract={Lofty power and elevated step-up isolated dc-dc converters have been widely employed in the emerald power systems and especially for electric traction drives. In electronics engineering, a DC to DC converter is a circuit, which converts a source of direct current from one voltage to another. It is a class of power converter. In many DC-DC applications, output isolation may need to be implemented depending on the type of application to meet safety standards to provide impedance matching. This galvanic isolation is required to attain a flexible system reconfiguration. Active lamp boost converter with coupled-inductors is proposed for high step-up electric traction applications. The primary-parallel-secondary-series structure is employed in this project to knob the huge input current, maintain the high output voltage and enlarge the voltage gain. Purpose of the coupled-inductors is to reduce the voltage gain addition and lesser concern in switching. Clamp circuit is used to re circulate the energy. The voltage strain in rectifier is minimized and reverse-recovery problem is eliminated. The hardware unit utilizes embedded technology using PIC microcontroller 16F84A to give gate pulses to the MOSFET converter switches. In this proposed converter only three MOSFET's are used which reduces the number of devices and makes circuit simple in construction. Efficiency, size, and cost are the primary advantages of the proposed isolated ZVT boost converter when compared to other existing converters. Isolated ZVT boost converter employing series-parallel arrangement will have an efficiency of about 88-94%, whereas existing converters are usually 80 to 85% efficient. In the present work, a novel ZVT boost converter for electric traction drive application is developed has been designed in MATLAB/SIMULINK software packages. The simulation result for a 40-V-to-600-V converter is simulated in open circuit and closed loop system. Simulated disturbance is applied at a time period of 0.7 micro second and noted that the output voltage is maintained constant always. The hardware unit and experimental results of a 15V to 85-V ZVT converter is implemented and the results are verified.},
keywords={Switches;Inductors;Voltage control;Stress;Clamps;Logic gates;Density measurement;clamp Circuit;inductors;DC-DC converter;Microcontroller},
doi={10.1109/ICPEC.2013.6527679},
ISSN={},
month={Feb},}
@INPROCEEDINGS{8077806,
author={Weber, Sam and Coblenz, Michael and Myers, Brad and Aldrich, Jonathan and Sunshine, Joshua},
booktitle={2017 IEEE Cybersecurity Development (SecDev)},
title={Empirical Studies on the Security and Usability Impact of Immutability},
year={2017},
volume={},
number={},
pages={50-53},
abstract={Although it is well-known that API design has a large and long-term impact on security, the literature contains few substantial guidelines for practitioners on how to design APIs that improve security. Even fewer of those guidelines have been evaluated empirically. Security professionals have proposed that software engineers choose immutable APIs and architectures to enhance security. Unfortunately, prior empirical research argued that immutablity decreases API usability. This paper brings together the results from a number of previous papers that together aim to show that immutability, when carefully designed using usability as a first-class requirement, can have positive effects on both usability and security. We also make observations on study design in this field.},
keywords={Security;Usability;Java;Computer bugs;Interviews},
doi={10.1109/SecDev.2017.21},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6067754,
author={Yang, Yanlan and Ye, Hua and Fei, Shumin and Yang, Yanlan and Ye, Hua and Fei, Shumin},
booktitle={2011 International Conference on Electronics, Communications and Control (ICECC)},
title={Design of communication interface for M2M-based positioning and monitoring system},
year={2011},
volume={},
number={},
pages={2624-2627},
abstract={The idea of M2M platform proposes a completely new system architecture for positioning and monitoring applications with wider coverage and higher communication efficiency. With the comparison of the original communication mode to the M2M-based one, the communication interface is changed in the new communication solution for the application side accessing to the platform, which is finally designed into two parts according to the interface specification for M2M platform. The one is the local web service which provides methods for the platform to transmit data from the terminal side, and the other is the communication service software developed based on the SDK package from the M2M platform. This interface has been implemented into an actual application for the construction machinery monitoring, and the communication work between the terminals and the control center through M2M platform performs efficiently and reliably.},
keywords={Monitoring;Web services;Data communication;Global Positioning System;Ground penetrating radar;Protocols;Reliability;positioning and monitoring system;terminal;M2M;web service},
doi={10.1109/ICECC.2011.6067754},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8474119,
author={Chatzidimitriou, Athanasios and Papadimitriou, George and Gizopoulos, Dimitris},
booktitle={2018 IEEE 24th International Symposium on On-Line Testing And Robust System Design (IOLTS)},
title={HealthLog Monitor: A Flexible System-Monitoring Linux Service},
year={2018},
volume={},
number={},
pages={183-188},
abstract={Error monitoring is a critical procedure for most computing systems, varying from HPC to embedded systems domains. Several generic architectures have been proposed and employed in modern processors, offering the capability of hardware-level error detection. This critical information is required to isolate and/or mitigate failures. However, research has revealed many cases where indications of upcoming failures can be identified early and before the actual fail occurrence, known as symptoms. Such cases become more frequent as technology trends try to exploit the conservative worst-case voltage guardbands and push computing systems towards more aggressive and often hazardous regions. In this paper we present HealthLog monitor, a flexible system monitoring service that offers a generic abstraction layer to combine both error and symptom monitoring. HealthLog is capable of monitoring hardware measurements (performance, sensor and errors) as well as external health-related data, allowing combined symptom description and reaction features supported by an API. The scope of the monitor is to offer a universal standard for error reporting and system monitoring mechanisms in all system layers. The current version of HealthLog was developed and tested on AppliedMicro's X-Gene 2 micro-server, but it is a cross-platform solution as it does not depend on a specific architecture. This work demonstrates how platform events, software metrics and external peripheral mechanisms can be combined to deliver early warnings of upcoming failures and trigger evading reactions.},
keywords={Monitoring;Hardware;Computer architecture;Tools;Linux;Registers;Sockets;reliability;error handling;error reporting;protection mechanisms},
doi={10.1109/IOLTS.2018.8474119},
ISSN={1942-9401},
month={July},}
@INPROCEEDINGS{8122372,
author={Jin, Lan and Liang, Jie},
booktitle={2017 IEEE 3rd Information Technology and Mechatronics Engineering Conference (ITOEC)},
title={Modeling of vehicle administrative management system based on unified modeling language},
year={2017},
volume={},
number={},
pages={50-54},
abstract={Establishing a good model during the analysis and design phase in the process of software development, is the key for the correct implementation of system. UML (Unified Modeling Language, Unified Modeling Language) is the standard modeling language, graphically on the system analysis and design, Suitable for all stages of software life cycle. Combining vehicle administrative management system, flowing with the UML modeling steps, using the modeling tool Enterprise Architect 12 which supports UML 2.0, it has completed system requirements modeling through use case diagram, static modeling through class diagram and package diagram, dynamic modeling through sequence diagram, activity diagram and state diagram, physical modeling through component diagram and deployment diagram. It implemented vehicle management, goods purchasing, personnel management, training management, loan management of life, and employee performance management, and other functions. Practice proves that the system model is more clear, better team communication, simplified development process, enhanced development efficiency.},
keywords={Unified modeling language;Procurement;Personnel;Software;Vehicle dynamics;Maintenance engineering;Appraisal;software development process;modeling;unified modeling language;vehicle administrative management system;use case diagram},
doi={10.1109/ITOEC.2017.8122372},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7163046,
author={Beurdouche, Benjamin and Bhargavan, Karthikeyan and Delignat-Lavaud, Antoine and Fournet, Cédric and Kohlweiss, Markulf and Pironti, Alfredo and Strub, Pierre-Yves and Zinzindohoue, Jean Karim},
booktitle={2015 IEEE Symposium on Security and Privacy},
title={A Messy State of the Union: Taming the Composite State Machines of TLS},
year={2015},
volume={},
number={},
pages={535-552},
abstract={Implementations of the Transport Layer Security (TLS) protocol must handle a variety of protocol versions and extensions, authentication modes, and key exchange methods. Confusingly, each combination may prescribe a different message sequence between the client and the server. We address the problem of designing a robust composite state machine that correctly multiplexes between these different protocol modes. We systematically test popular open-source TLS implementations for state machine bugs and discover several critical security vulnerabilities that have lain hidden in these libraries for years, and have now finally been patched due to our disclosures. Several of these vulnerabilities, including the recently publicized FREAK flaw, enable a network attacker to break into TLS connections between authenticated clients and servers. We argue that state machine bugs stem from incorrect compositions of individually correct state machines. We present the first verified implementation of a composite TLS state machine in C that can be embedded into OpenSSL and accounts for all its supported cipher suites. Our attacks expose the need for the formal verification of core components in cryptographic protocol libraries, our implementation demonstrates that such mechanized proofs are within reach, even for mainstream TLS implementations.},
keywords={Servers;Protocols;Authentication;Libraries;Cryptography;Computer bugs;Transport Layer Security;cryptographic protocols;man-in-the-middle attacks;software verification;formal methods},
doi={10.1109/SP.2015.39},
ISSN={2375-1207},
month={May},}