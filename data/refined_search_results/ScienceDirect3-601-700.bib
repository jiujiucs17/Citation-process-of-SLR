@article{LYNN2017296,
title = {Voxel model surface offsetting for computer-aided manufacturing using virtualized high-performance computing},
journal = {Journal of Manufacturing Systems},
volume = {43},
pages = {296-304},
year = {2017},
note = {High Performance Computing and Data Analytics for Cyber Manufacturing},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2016.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612516300929},
author = {Roby Lynn and Didier Contis and Mohammad Hossain and Nuodi Huang and Tommy Tucker and Thomas Kurfess},
keywords = {Cloud manufacturing, Distributed manufacturing, Computer-aided manufacturing, Computer numerical control, Virtualization, High-performance computing, Voxel, GPGPU, CUDA},
abstract = {Curve and surface offsetting is a common operation performed on solid models when planning toolpaths for a machining operation. This operation is usually done in a computer-aided manufacturing (CAM) software package to define the path along which the center of a cutting tool will follow to create a given feature. The CAM software then translates the toolpath created from the offset into G-Code, which is the standard programming language of CNC machine tools. The toolpath planning process can be computationally intensive; thus, a powerful workstation is required to run the CAM software effectively. These standalone workstations can be inconvenient due to their cost and size. Many organizations have been turning to virtualization as an alternative to multiple standalone workstations; virtualization allows for many users to access desktop environments that are hosted from a single remote server. This has the benefit of isolating the user from both OS and hardware requirements for certain software, and also allows them to run the applications they need from anywhere. This research explores the emerging area of virtualized general purpose computation on graphics processing units (GPGPU); this technique is used to support the development of a voxelized CAM package that allows for rapid toolpath generation for complex parts. The surface offset performance is benchmarked on various local and virtualized platforms to evaluate the losses from virtualization. Results indicate a minor loss of performance between virtualized and local GPUs, which is to be expected due to the abstraction of hardware from a virtual machine. Additionally, the development of a GPU-sharing implementation using a server operating system is described and analyzed; results indicate that, as compared to single virtual machines, the GPU-sharing approach demonstrates higher computational efficiency with the addition of compute load to the GPU.}
}
@incollection{SHACHAM2014429,
title = {Considering parameter uncertainties in the design of safe processes},
editor = {Mario R. Eden and John D. Siirola and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {34},
pages = {429-434},
year = {2014},
booktitle = {Proceedings of the 8th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63433-7.50056-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634337500560},
author = {Mordechai Shacham and Neima Brauner},
keywords = {Process design, process safety, parameter uncertainty, bounded variables},
abstract = {A systematic procedure has been developed for process unit design in order to meet the safety requirements even with the “worst case” combination of physical property values and other process variables, which are bounded within uncertainty limits. The new procedure can be implemented in widely available, general purpose mathematical software packages. The use of the proposed procedure is demonstrated by applying it to an example involving an exothermic reaction which is carried out in a cooled, semibatch reactor in order to estimate the minimal over-design of the cooling system required in order to prevent temperature runaway due to uncertainty of some of the physical properties and reaction rate coefficients.}
}
@article{LANG199315,
title = {Scientific visualization in a supercomputer network at RUS},
journal = {Computers & Graphics},
volume = {17},
number = {1},
pages = {15-22},
year = {1993},
issn = {0097-8493},
doi = {https://doi.org/10.1016/0097-8493(93)90047-D},
url = {https://www.sciencedirect.com/science/article/pii/009784939390047D},
author = {Ulrich Lang and Ruth Lang and Roland Rühle},
abstract = {The implementation of RSYST, a scientific software application environment at the University of Stuttgart is described. The architecture of the software system matches well with the hardware configuration of the University Computer Center, and the environment aspires to transparently integrate users, who are distributed all over the country, into a consistent software and hardware environment. The visualization concept at RUS is explained, together with some distributed visualization packages implemented at RUS and our own visualization development activities. Finally, the aims and characteristics of a European sponsored project, PAGEIN, are explained, which fits perfectly into the line of developments at RUS. The goal of this project is to experiment with future cooperative working modes of aerospace scientists in a high speed distributed supercomputing environment. Project results will have an impact on the development of real future scientific application environments.}
}
@incollection{CEVOLI200377,
title = {Chapter Five - Midlevel Interface Library},
editor = {Paul Cevoli},
booktitle = {Embedded FreeBSD Cookbook},
publisher = {Newnes},
address = {Burlington},
pages = {77-101},
year = {2003},
series = {Embedded Technology},
isbn = {978-1-58995-004-7},
doi = {https://doi.org/10.1016/B978-158995004-7/50005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781589950047500055},
author = {Paul Cevoli},
abstract = {Publisher Summary
This chapter highlights a FreeBSD device driver that reads and writes the PCI-DIO24 hardware register, and services the controller interrupt. A shared library that uses the Digital Input-Output (DIO) device driver, and serves as the software interface for programming the PCI-DIO24 data acquisition board are explained. Providing an application interface library isolates the device programming details and operating system that specifies to the library and device driver. This design enables system engineer to interface with the PCI-DIO24 controller using a simple C interface, and minimizes the impact of changing the underlying hardware or operating system. Creating and using shared libraries, accessing FreeBSD device drivers, low-level system calls, PCI-DIO24 register definitions, and design and implementation of the digital IO interface shared library are also discussed. The DIO device driver, which resides in the kernel, with a shared library residing in the user space, is connected.}
}
@article{GARELLI2016117,
title = {Evaluation of a coupling interface for solving fluid–structure interaction problems},
journal = {European Journal of Mechanics - B/Fluids},
volume = {58},
pages = {117-126},
year = {2016},
issn = {0997-7546},
doi = {https://doi.org/10.1016/j.euromechflu.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0997754615300571},
author = {Luciano Garelli and Marco Schauer and Gustavo Ríos Rodriguez and Sabine C. Langer and Mario A. Storti},
keywords = {Scalability, FLUID–STRUCTURE Interaction, Partitioned coupling},
abstract = {This research work evaluates the performance of a Fluid–Structure Interaction (FSI) solver, which is created using a generic interface to couple two independent software packages. The basic idea is to combine the advantages of the two independent codes to create a powerful FSI solver for two and three dimensional FSI analysis using the concept of modular programming. A detailed description about the implementation of an interface to couple a three-field system involved in the analysis is given, and this developed interface can be generalized to others codes. Since solving complex FSI problems is very time consuming, the focus of this work is placed on the performance of the coupled solver, for which a FSI benchmark will be solved on a computer cluster in order to measure speed up and efficiency.}
}
@article{PEDDLE19951163,
title = {Mercury⊕: An evidential reasoning image classifier},
journal = {Computers & Geosciences},
volume = {21},
number = {10},
pages = {1163-1176},
year = {1995},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(95)00047-X},
url = {https://www.sciencedirect.com/science/article/pii/009830049500047X},
author = {Derek R. Peddle},
keywords = {Remote sensing, Dempster-Shafer theory, Multisource data, Yukon, Permafrost},
abstract = {MERCURY⊕ is a multisource evidential reasoning classification software system based on the Dempster-Shafer theory of evidence. The design and implementation of this software package is described for improving the classification and analysis of multisource digital image data necessary for addressing advanced environmental and geoscience applications. In the remote-sensing context, the approach provides a more appropriate framework for classifying modern, multisource, and ancillary data sets which may contain a large number of disparate variables with different statistical properties, scales of measurement, and levels of error which cannot be handled using conventional Bayesian approaches. The software uses a nonparametric, supervised approach to classification, and provides a more objective and flexible interface to the evidential reasoning framework using a frequency-based method for computing support values from training data. The MERCURY⊕ software package has been implemented efficiently in the C programming language, with extensive use made of dynamic memory allocation procedures and compound linked list and hash-table data structures to optimize the storage and retrieval of evidence in a Knowledge Look-up Table. The software is complete with a full user interface and runs under Unix, Ultrix, VAX/VMS, MS-DOS, and Apple Macintosh operating system. An example of classifying alpine land cover and permafrost active layer depth in northern Canada is presented to illustrate the use and application of these ideas.}
}
@article{DORLEANS2000543,
title = {Level Regulation of a Tank Using a Generalized Predictive Control},
journal = {IFAC Proceedings Volumes},
volume = {33},
number = {13},
pages = {543-548},
year = {2000},
note = {IFAC Conference on Control Systems Design (CSD 2000), Bratislava, Slovak Republic, 18-20 June 2000},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)37247-6},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017372476},
author = {Philippe Dorléans and Jean-françois Massieu and Gérard Villeneuve and Mohammed M’saad},
keywords = {Predictive Control, sensitivity functions, performance and robustness analysis, ARMA model, disturbance rejection, windup},
abstract = {A laboratory application of a predictive control approach is presented in this paper. The controller design process relies on the C.A.C.S.D. advanced software package SIMART. This package addresses the different process design steps from the performance specification to the real time implementation. The considered application consists in controlling the liquid level of a tank system. One of the interest in this process lies in the closed loop water distribution which induces a low frequency input disturbance. Another interest lies in the antiwindup system developed to compensate the actuator saturations. To this purpose, we design a predictive control law which has been successfully experimented.}
}
@incollection{19926-1,
title = {Chapter 6 - Common PALcode Architecture (I)},
editor = {Richard L. Sites},
booktitle = {Alpha Architecture Reference Manual},
publisher = {Digital Press},
address = {Boston},
pages = {6-1-6-7},
year = {1992},
series = {HP Technologies},
isbn = {978-1-55558-098-8},
doi = {https://doi.org/10.1016/B978-1-55558-098-8.50012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781555580988500129},
abstract = {Publisher Summary
This chapter discusses the common PALcode architecture. One of the goals of Alpha is that microcode will not be necessary for practical implementation. However, it is still desirable to provide an architected interface to these functions that will be consistent across the entire family of machines. The Privileged Architecture Library (PALcode) provides a mechanism to implement these functions without resorting to a microcoded machine. PALcode is written in standard machine code with some implementation-specific extensions to provide access to low-level hardware. This lets an Alpha implementation make various design trade-offs based on the hardware technology being used to implement the machine. The PALcode can abstract these differences and make them invisible to system software. PALcode uses the Alpha instruction set for most of its operations. A small number of additional functions are needed to implement the PALcode. There are five opcodes reserved to implement PALcode functions: (1) PALRESO, (2) PALRES1, (3) PALRES2, (4) PALRES3, and (5) PALRES4. These instructions produce an Illegal Instruction Trap if executed outside the PALcode environment.}
}
@article{SCHMID1979625,
title = {CAD of Adaptive Systems},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {7},
pages = {625-630},
year = {1979},
note = {IFAC Symposium on computer Aided Design of Control Systems, Zurich, Switzerland, 29-31 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65661-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017656611},
author = {Chr. Schmid},
keywords = {Computer aided design, computer software, adaptive control, adaptive systems},
abstract = {The design and realization of discrete adaptive control systems using a stability conception and only process input and output signals are discussed. It is shown that known methods are special cases, which can be obtained from a more general and systematic design based on hyperstability theory. This allows the development of a systematic CAD conception existing of theory, algorithms and software architecture. The realization on a process computer in a general purpose DDC-design and implementation package as an adaptive control software subsystem is sketched.}
}
@article{LATIFSHABGAHI2001167,
title = {Component-oriented voter model for dependable control applications},
journal = {Microprocessors and Microsystems},
volume = {25},
number = {3},
pages = {167-176},
year = {2001},
issn = {0141-9331},
doi = {https://doi.org/10.1016/S0141-9331(01)00109-0},
url = {https://www.sciencedirect.com/science/article/pii/S0141933101001090},
author = {G Latif-Shabgahi and J.M Bass and S Bennett},
keywords = {Component-orientated software, Fault-tolerance, Voting algorithms},
abstract = {In many industrial applications arbitration between redundant subsystems using voting algorithms is popular. Many voting strategies, implemented in hardware or software, have been proposed of which majority and median voters have been widely used in real applications. Component-oriented design and modeling is receiving increasing amounts of interest in the software engineering community. Detailed analysis of voters shows that they can also be considered as a combination of independent components, each performing a specific function. This article proposes a component-oriented model for voters. The model offers benefits such as reusability, flexibility, and extensibility to the system designer. Components and their families are introduced, categorised and simulated. The model is simulated and a library of simulated components is provided. The generality of the model not only supports the analysis of a large number of voter permutations but also facilitates system design and implementation phases. The article presents the experimental results of selected component-oriented voters including majority, median, and linear predictor voters within a Triple Modular Redundant, TMR, system for a wide range of error scenarios. The correctness of the voter model is also proved by comparing the experimental results of selected component-oriented voters with those of the corresponding directly implemented voters.}
}
@article{FAVRE2019102543,
title = {A comparative evaluation of three volume rendering libraries for the visualization of sheared thermal convection},
journal = {Parallel Computing},
volume = {88},
pages = {102543},
year = {2019},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819119301280},
author = {Jean M. Favre and Alexander Blass},
keywords = {Scientific visualization, High performance computing, Navier-Stokes solver, Direct numerical simulation, Computational fluid dynamics},
abstract = {Oceans play a big role in the nature of our planet, about 70% of our earth is covered by water [1]. Strong currents are transporting warm water around the world making life possible, and allowing us to harvest its power producing energy. Yet, oceans also carry a much more deadly side. Floods and tsunamis can easily annihilate whole cities and destroy life in seconds. The earth’s climate system is also very much linked to the currents in the ocean due to its large coverage of the earth’s surface, thus, gaining scientific insights into the mechanisms and effects through simulations is of high importance. Deep ocean currents can be simulated by means of wall-bounded turbulent flow simulations. To support these very large scale numerical simulations and enable the scientists to interpret their output, we deploy an interactive visualization framework to study sheared thermal convection. The visualizations are based on volume rendering of the temperature field. To address the needs of supercomputer users with different hardware and software resources, we evaluate different volume rendering implementations supported in the ParaView [2] environment: two GPU-based solutions with Kitware’s native volume mapper or NVIDIA’s IndeX library, and a CPU-only Intel OSPRay-based implementation.}
}
@article{CHIEN201529,
title = {Versioned Distributed Arrays for Resilience in Scientific Applications: Global View Resilience},
journal = {Procedia Computer Science},
volume = {51},
pages = {29-38},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.187},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915009953},
author = {A. Chien and P. Balaji and P. Beckman and N. Dun and A. Fang and H. Fujita and K. Iskra and Z. Rubenstein and Z. Zheng and R. Schreiber and J. Hammond and J. Dinan and I. Laguna and D. Richards and A. Dubey and B. {van Straalen} and M. Hoemmen and M. Heroux and K. Teranishi and A. Siegel},
keywords = {Resilience, Fault tolerance, Exascale, Scalable computing, Application-based fault tolerance},
abstract = {Exascale studies project reliability challenges for future high-performance computing (HPC) systems. We propose the Global View Resilience (GVR) system, a library that enables applications to add resilience in a portable, application-controlled fashion using versioned distributed arrays. We describe GVR's interfaces to distributed arrays, versioning, and cross-layer error recovery. Using several large applications (OpenMC, the preconditioned conjugate gradient solver PCG, ddcMD, and Chombo), we evaluate the programmer effort to add resilience. The required changes are small (<2% LOC), localized, and machine-independent, requiring no software architecture changes. We also measure the overhead of adding GVR versioning and show that generally overheads <2% are achieved. We conclude that GVR's interfaces and implementation are flexible and portable and create a gentle-slope path to tolerate growing error rates in future systems.}
}
@article{KELER1999105,
title = {Language and library support for practical PRAM programming1This work was partially supported by ESPRIT LTR Project no. 20244-ALCOM-IT.12This article is the full version of a short paper that appeared at the 5th Euromicro Workshop on Parallel and Distributed Processing, London, UK, 1997.2},
journal = {Parallel Computing},
volume = {25},
number = {2},
pages = {105-135},
year = {1999},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(98)00092-1},
url = {https://www.sciencedirect.com/science/article/pii/S0167819198000921},
author = {Christoph W. Keßler and Jesper Larsson Träff},
keywords = {Parallel programming languages, Parallel software libraries, Programming environments, Compilers, PRAM model},
abstract = {We investigate the well-known Parallel Random Access Machine (PRAM) model of parallel computation as a practical parallel programming model. The two components of this project are a general-purpose PRAM programming language, called Fork95, and a library, called PAD, of fundamental, efficiently implemented parallel algorithms and data structures. We outline the main features of Fork95 as they apply to the implementation of PAD, and describe the implementation of library procedures for prefix-sums and sorting. The Fork95 compiler generates code for the SB-PRAM, a hardware emulation of the PRAM, which is currently being completed at the University of Saarbrücken. Both language and library can immediately be used with this machine. The project is, however, of independent interest. The programming environment can help the algorithm designer to evaluate the practicality of new parallel algorithms, and can furthermore be used as a tool for teaching and communication of parallel algorithms.}
}
@article{AMRAEI2020110094,
title = {Bond characteristics between high/ultra-high strength steel and ultra-high modulus CFRP laminates},
journal = {Engineering Structures},
volume = {205},
pages = {110094},
year = {2020},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.110094},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619341094},
author = {Mohsen Amraei and Xiao-Ling Zhao and Timo Björk and Amin Heidarpour},
keywords = {Bond, Carbon fibre reinforced polymer (CFRP), High strength steel (HSS), Ultra-high strength steel (UHSS)},
abstract = {With the increasing applications of high and ultra-high strength steel (HSS/UHSS) in engineering structures, there is a need to address rehabilitation and strengthening of such steel grades using carbon fibre reinforced polymer (CFRP). The bond between HSS/UHSS and CFRP is vital to ensure the efficiency of the strengthening. The existing bond study was focused on mild steel as substrate, with very limited work on HSS up to the grade of S690. Since HSS/UHSSs are designed to undergo much higher loading in service, much higher shear stress is expected in the adhesive layer, leading to a higher chance of premature debonding. In this paper, the bond between HSS/UHSS plates and ultra-high modulus (UHM) CFRP laminates under static tensile loading is studied experimentally, numerically and theoretically. Both single-sided and double-sided schemes were adopted. The numerical simulation using LS-DYNA software package was implemented and a reasonable agreement with the experimental results is found. A theoretical bond model was developed to relate the bond strength to the imposed strain in the steel member outside the bonded region.}
}
@article{PRIETO1999543,
title = {An environment to develop parallel code for solving partial differential equations based-problems1This work has been supported by the Spanish research grants TIC 96-1071 and TIC IN96-0150 and the Human Capital Mobility Network CHRX-CT94-0459.1},
journal = {Journal of Systems Architecture},
volume = {45},
number = {6},
pages = {543-554},
year = {1999},
issn = {1383-7621},
doi = {https://doi.org/10.1016/S1383-7621(98)00022-8},
url = {https://www.sciencedirect.com/science/article/pii/S1383762198000228},
author = {M Prieto and I Martı́n and F Tirado},
keywords = {Problem-solving environments (PSE), Partial differential equations (PDEs), Parallel processing for scientific computing, Multigrid methods},
abstract = {Since the beginning of computing, the use of computers to simulate physical phenomena has been a driving force to advance the field of computing. Computational scientists demand more computational power and more facility to implement applications. The conventional library interface does not hide the complexity of the underlying parallel architectures and their programming paradigms from users. Our investigation group is currently implementing an environment for solving partial differential equations (PDEs). Using this high-level tool the user can easily develop parallel applications for solving PDEs based problems using multigrid techniques without knowledge on the underlying computer hardware or software. This paper provides the first steps towards the creation of the environment.}
}
@article{IWASHITA20172200,
title = {Software Framework for Parallel BEM Analyses with H-matrices Using MPI and OpenMP},
journal = {Procedia Computer Science},
volume = {108},
pages = {2200-2209},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.263},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917308967},
author = {Takeshi Iwashita and Akihiro Ida and Takeshi Mifune and Yasuhito Takahashi},
keywords = {Boundary element analysis, H-matrix, parallel processing, software framework},
abstract = {A software framework has been developed for use in parallel boundary element method (BEM) analyses. The framework program was parallelized in a hybrid parallel programming model, and both multiple processes and threads were used. Additionally, an H-matrix library for a distributed memory parallel computer was also developed to accelerate the analysis. In this paper, we describe the basic design concept for the framework and details of its implementation. The framework program, which was written with MPI functions and OpenMP directives, is mainly intended to reduce the user’s parallel programming costs. We also show the results of a sample analysis performed with approximately 60,000 unknowns. The numerical results verify the effectiveness of both the parallelization and the H-matrix method. In the test analysis, which was performed using a single core, the H-matrix version of the framework is 17-fold faster than the dense matrix version. The parallel framework program with the H-matrix attains an approximately 50-fold acceleration using 128 cores when compared with sequential computation.}
}
@article{FRISON201714399,
title = {A high-performance Riccati based solver for tree-structured quadratic programs**Support by the EU via ERC-HIGHWIND (259 166), ITN-TEMPO (607 957), and ITN-AWESCO (642 682), by DFG via the Research Unit FOR 2401, by Ministerium für Wissenschaft, Forschung und Kunst Baden-Wuerttemberg (Az: 22-7533.-30-20/9/3), and by Det Frie Forskningsrad (DFF - 6111-00398) is gratefully acknowledged.},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14399-14405},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2027},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317326666},
author = {Gianluca Frison and Dimitris Kouzoupis and Moritz Diehl and John Bagterp Jørgensen},
keywords = {Predictive control, Quadratic programming, Tree structures, Numerical methods},
abstract = {Robust multi-stage Model Predictive Control (MPC) is an increasingly popular approach to handle model uncertainties due to the simplicity of its problem formulation and other attractive properties. However, the exponential growth of the problem dimensions with respect to the robust horizon renders the online solution of such problems challenging and the development of tailored solvers crucial. In this paper, an interior point method is presented that can solve Quadratic Programs (QPs) arising in multi-stage MPC efficiently by means of a tree-structured Riccati recursion and a high-performance linear algebra library. A performance comparison with code-generated and general purpose sparse QP solvers shows that the computation times can be significantly reduced for all problem sizes that are practically relevant in embedded MPC applications. The presented implementation is freely available as part of the open-source software HPMPC.}
}
@article{LAZZARETTI2013133,
title = {Design Space Exploration of Embedded Systems for Intelligent Maintenance},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {7},
pages = {133-138},
year = {2013},
note = {11th IFAC Workshop on Intelligent Manufacturing Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130522-3-BR-4036.00089},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015356639},
author = {E. Lazzaretti and M. Zuccolotto and C.E. Pereira and R.V.B. Henriques},
keywords = {Intelligent Maintenance, Watchdog Agent™, FPGA, Embedded systems, Automatic code generation},
abstract = {With the development of intelligent maintenance techniques, the embedded systems that will be used with such algorithms will need increasingly to present more flexibility, combined with high processing speed and low power consumption. Within this context, model based programming associated with automatic platform-specific code generation capabilities are of great interest. This work performs a design space exploration of some algorithms commonly used on intelligent maintenance applications, in this case wavelet package energies and logistic regression, by analyzing the performance and required footprint of different implementations for intelligent maintenance algorithms when executed in hardware and software. Starting point for the comparison was the so called Watchdog Agent™ IMS system, which is currently available both in MATLAB™ and LabVIEW™ environments. Using available code generation tools distinct hardware and software versions are deployed and both the performance of the generated systems as well as some energy and memory metrics of the resources used in FPGA implementations are compared For the validation tests, vibration data collected from a test bench composed by an electric mechanical actuator was used and obtained results confirmed a great variability of the generated solutions in terms of the assessed metrics, clearly indicating that best solution may vary depending on the application requirements.}
}
@article{TOMCZUKPIROG20081845,
title = {Knowledge Based Approach to Project Prototyping},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {1845-1850},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00315},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016392205},
author = {Izabela Tomczuk-Piróg and Peter Nielsen and Wojciech Muszyński and Zbigniew Banaszak},
abstract = {Decision making supported by task-oriented software tools plays a pivotal role in modern enterprises, because commercially available ERP systems are unable to respond in an interactive online/real-time mode. It opens up for a new generation of decision support system (DSS) that enable a fast prototyping of production flows in multi-project environment as well as integrating approaches to project execution evaluation. In that context our goal is to provide a knowledge base approach allowing one to be independent of context or representation data as well as allowing for the design of an interactive and task-oriented DSS. The assumed knowledge base mode of specifying a production system leads to solving a logic-algebraic method (LAM) decision problem. The results obtained are implemented in a software package supporting project management in SMEs. Illustrative example of the ILOG-based software application is provided.}
}
@article{GODENA199383,
title = {Modula-2 based multitasking environment for a flexible biprocessor controller},
journal = {Microprocessors and Microsystems},
volume = {17},
number = {2},
pages = {83-91},
year = {1993},
issn = {0141-9331},
doi = {https://doi.org/10.1016/0141-9331(93)90075-I},
url = {https://www.sciencedirect.com/science/article/pii/014193319390075I},
author = {Giovanni Godena and Janko Petrovčič},
keywords = {real-time programming languages, Modula-2, software design microprocessor control},
abstract = {This paper presents the most important issues of a real-time software package developed for the industrial multiloop controller MMC-90. MMC-90 is a biprocessor system based on the concept of a two-level controller. On the lower level of complexity, the base control processor performs the basic control functions. On the higher level of complexity, the flexible control coprocessor executes complex functions on request from the control tasks on the base control processor. All the software on the control coprocessor is written in Modula-2. The corresponding multitasking environment was designed as a set of Modula-2 libraries at various horizontal and vertical decomposition levels. On the control coprocessor a preemptive, priority based scheduler as well as a high level interprocess communication and synchronization mechanism has been developed. Communication between the processors is realized by means of a fast bi-directional communication channel, which is implemented via dual-port RAM. The developed system opens the door to applications of modern control algorithms in industrial practice.}
}
@incollection{HOWE2004924,
title = { - Algebraic Manipulation of Scientific Datasets},
editor = {Mario A. Nascimento and M. Tamer Özsu and Donald Kossmann and Renée J. Miller and José A. Blakeley and Berni Schiefer},
booktitle = {Proceedings 2004 VLDB Conference},
publisher = {Morgan Kaufmann},
address = {St Louis},
pages = {924-935},
year = {2004},
isbn = {978-0-12-088469-8},
doi = {https://doi.org/10.1016/B978-012088469-8.50081-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780120884698500814},
author = {Bill Howe and David Maier},
abstract = {Publisher Summary
This chapter investigates algebraic processing strategies for large numeric datasets equipped with a possibly irregular grid structure. Such datasets arise in many areas—computational simulations, observation networks, medical imaging, and 2-D and 3-D rendering. Existing approaches for manipulating these datasets are incomplete—the performance of SQL queries for manipulating large numeric datasets is not competitive with specialized tools. Database extensions for processing multidimensional discrete data can only model regular, rectilinear grids. Visualization software libraries are designed to process gridded datasets efficiently, but no algebra has been developed to simplify their use and afford optimization. Further, these libraries are data dependent—physical changes to data representation or organization break user programs. The chapter presents algebra of grid-fields for manipulating both regular and irregular gridded datasets, algebraic optimization techniques, and an implementation backed by experimental results. The chapter compares these techniques to those of spatial databases and visualization software libraries, using real examples from an Environmental Observation and Forecasting System. Finally, the chapter concludes that this approach can express optimized plans inaccessible to other techniques, resulting in improved performance with reduced programming effort.}
}
@article{DORLEANS19998793,
title = {Diameter regulation of an optical fiber using a generalized predictive control approach},
journal = {IFAC Proceedings Volumes},
volume = {32},
number = {2},
pages = {8793-8797},
year = {1999},
note = {14th IFAC World Congress 1999, Beijing, Chia, 5-9 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)57500-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701757500X},
author = {Philippe Dorléans and Olivier Gehan and Eric Pigeon and Mohammed M'Saad and M. Hertz and M. Desalle},
keywords = {Predictive control, sensitivity functions, performances et robustness analysis, armax model, disturbance rejection, draw tower, optical fiber},
abstract = {In industrial application of a predictive control approach is presented in this paper. The controller design process relies on the C.A.C.S.D. advanced software package SIMART. This package addresses the different process design steps from the performance specification to the real time implementation. The considered application consists in controlling the diameter of an optical fiber produced in a draw tower at ALCATEL company. This control objective is motivated by the high production level of optical fibers in the expanding market of telecommunications. To this purpose, we design a predictive control law which was successfully experimented an a draw tower.}
}
@article{PUZYREV2019157,
title = {pyROM: A computational framework for reduced order modeling},
journal = {Journal of Computational Science},
volume = {30},
pages = {157-173},
year = {2019},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2018.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877750318307518},
author = {Vladimir Puzyrev and Mehdi Ghommem and Shiv Meka},
keywords = {Model reduction, Projection-based method, Open-source software, Proper orthogonal decomposition, Dynamic mode decomposition},
abstract = {Model reduction techniques reduce the overall complexity of dynamic systems and allow to speed up simulations of their behavior several orders of magnitude while retaining good accuracy. Despite being useful to obtain real-time simulations and apply control strategies, only few freely available software implementations of model reduction techniques have been reported in the literature. Furthermore, the use of these tools tends to be only for a limited range of dynamic problems, mostly related to fluid flows, and to deal with relatively small systems and datasets. In this paper, we build a portable, user-friendly, and open source computational framework, namely pyROM, implementing model reduction techniques in the Python programming language. This tool is designed to satisfy the needs of wide range of users to deploy model reduction for reproducing the dynamic response of high-dimensional models with good accuracy while achieving significant computational savings. The framework is designed in an object-oriented way to be easy to use and extend and employs visualization tools from various Python libraries such as Matplotlib, Mayavi, and Bokeh. Several numerical examples using modern spatial discretization methods such as the finite element method, the isogeometric analysis, the meshless point collocation method, and the generalized multiscale finite element method demonstrate the performance of the developed computational tool and the capabilities of model reduction methods to handle different engineering problems.}
}
@article{ZUPANCIC1998703,
title = {Extension software for real-time control system design and implementation with MATLAB-SIMULINK},
journal = {Simulation Practice and Theory},
volume = {6},
number = {8},
pages = {703-719},
year = {1998},
issn = {0928-4869},
doi = {https://doi.org/10.1016/S0928-4869(98)00012-3},
url = {https://www.sciencedirect.com/science/article/pii/S0928486998000123},
author = {Borut Zupančič},
keywords = {Real-time simulation, Real-time system, Hardware-in-the-loop, Control system, Computer-aided control system design},
abstract = {The paper deals with a unified environment for the design and implementation of control schemes. The widely used MATLAB-SIMULINK is used for control scheme description. The implementation hardware is from Mitsubishi PLC. The SIMULINK library was extended with target hardware blocks. After an off-line design procedure, which can be combined with hardware-in-the-loop experiments, the control scheme is translated into PLC code in three steps. The results show that the autocoding of small, low-cost industrial controllers can be efficiently realized by integrating widely used, inexpensive and commercially available tools.}
}
@article{GARCIA2020103377,
title = {A serious game for teaching the fundamentals of ISO/IEC/IEEE 29148 systems and software engineering – Lifecycle processes – Requirements engineering at undergraduate level},
journal = {Computer Standards & Interfaces},
volume = {67},
pages = {103377},
year = {2020},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.103377},
url = {https://www.sciencedirect.com/science/article/pii/S0920548919300819},
author = {Ivan García and Carla Pacheco and Andrés León and Jose A. Calvo-Manzano},
keywords = {ISO/IEC/IEEE 29148:2011, Requirements engineering processes, Serious games, Software process standards},
abstract = {In the context of software engineering education, there is a recurrent demand for new approaches and techniques that support the application and transfer of knowledge to real-life situations with the aim of encouraging a more active learning among students. In particular, serious games have recently become an important learning resource for teaching the fundamentals of software process standards at undergraduate level. However, poor effort has been made to create a serious game that supports the teaching of the ISO/IEC/IEEE 29148:2011 Systems and Software Engineering – Lifecycle Processes – Requirements Engineering, an international standard that specifies the required processes that are to be implemented by requirements engineering for systems and software products (including services) throughout the lifecycle. With this in mind, a serious game called “Requengin” has been developed to provide undergraduate students with an interactive learning environment to facilitate the introduction of ISO/IEC/IEEE 29148:2011. The main objective of the game is to strengthen the comprehension and application of the main processes of the standard and some related requirements engineering techniques. Requengin was designed to simulate an academic library where players must apply the requirements engineering processes with the aim of changing the traditional management system by a software system while they receive, at the same time, preliminary training in ISO/IEC/IEEE 29148:2011. The results obtained by empirical evaluation indicate that Requengin could potentially contribute to an improvement in students’ acquisition of knowledge about ISO/IEC/IEEE 29148:2011, while also improving levels of motivation.}
}
@article{KELLER20161131,
title = {Estimation of Screw-ball Differential on Vehicle Dynamics},
journal = {Procedia Engineering},
volume = {150},
pages = {1131-1136},
year = {2016},
note = {2nd International Conference on Industrial Engineering (ICIE-2016)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.225},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816315429},
author = {A.V. Keller and A.A. Shelepov and D.I. Istomin},
keywords = {the movement of the car model, simulation, limited slip differential, coefficient lock differential, road test.},
abstract = {The article dwells upon the nonlinear dynamic model of the vehicle 4×4 with a screw-ball differential. The car model is carried out in the software package LMS Imagine. Lab Amesim automated modeling. In this model, we implemented limited slip differentials as their use in cars is promising, enabling simultaneously to improve safety and vehicle patency. Full-scale test “evasive maneuver S=20 m” results of UAZ 3151 confirm the adequacy of the physical behavior of the model. The developed model can be used to evaluate the dynamic properties of the vehicle in different driving modes as well as for the design of new power distribution systems and evaluating drivability.}
}
@article{MAZHAR202114,
title = {On the meshfree particle methods for fluid-structure interaction problems},
journal = {Engineering Analysis with Boundary Elements},
volume = {124},
pages = {14-40},
year = {2021},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2020.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0955799720302885},
author = {Farrukh Mazhar and Ali Javed and Jing Tang Xing and Aamer Shahzad and Mohtashim Mansoor and Adnan Maqsood and Syed Irtiza Ali Shah and Kamran Asim},
keywords = {Meshfree particle methods, Meshless methods, Fluid-solid interaction, Fluid-structure interface, FSI coupling, Immersed and body-conforming meshes},
abstract = {This paper presents a review of recent progress made towards the applications of the meshfree particle methods (MPMs) for solving coupled fluid-structure interaction (FSI) problems. Meshfree methods are categorized based on their mathematical formulation and treatment of computational data points. The advantages and limitations of these methods, particularly related to FSI applications, have been identified. A detailed account of salient work related to the FSI problems involving complex geometries, viscous flows, and large structural deformations has been presented and the benchmark solutions are identified for future research. Compared to their mesh-based counterparts, MPMs are found better suited in negotiating moving boundaries and complex geometries, features that are the hallmark of FSI problems. However, the biggest challenge to their wider acceptability is their implementation and programming complexity, higher computational cost, and lack of commercial software packages. So far, meshfree methods have mostly been limited to applications, where conventional methods show limited performance. Owing to its promising growth potential, partitioned FSI is the prime emphasis of this paper. Various aspects of partitioned FSI have been identified and classified for meshfree FSI problems, which include problem formulation strategies, domains discretization approaches, solver coupling methodology, interface treatment, benchmark problems, computational load, and availability of commercial software. Furthermore, various challenges involved in employing MPMs for FSI have also been identified and discussed along with the state-of-the-art techniques used in meshfree methods and FSI applications, and a future way forward has been proposed. In essence, this paper is an effort to identify and classify key aspects of MPM applications for FSI and suggest potential avenues to explore the full potential of MPM capabilities for the solution of coupled problems.}
}
@article{BERGEL201216,
title = {Spy: A flexible code profiling framework},
journal = {Computer Languages, Systems & Structures},
volume = {38},
number = {1},
pages = {16-28},
year = {2012},
note = {SMALLTALKS 2010},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2011.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1477842411000327},
author = {Alexandre Bergel and Felipe Bañados and Romain Robbes and David Röthlisberger},
keywords = {Smalltalk, Profiling, Visualization},
abstract = {Code profiling is an essential activity to increase software quality. It is commonly employed in a wide variety of tasks, such as supporting program comprehension, determining execution bottlenecks, and assessing code coverage by unit tests. Spy is an innovative framework to easily build profilers and visualize profiling information. The profiling information is obtained by inserting dedicated code before or after method execution. The gathered profiling information is structured in line with the application structure in terms of packages, classes, and methods. Spy has been instantiated on four occasions so far. We created profilers dedicated to test coverage, time execution, type feedback, and profiling evolution across version. We also integrated Spy in the Pharo IDE. Spy has been implemented in the Pharo Smalltalk programming language and is available under the MIT license.}
}
@article{RECHARD20155747,
title = {Experimental Verification of Ecological Interface Prototype Issued by an Automated Generation Tool},
journal = {Procedia Manufacturing},
volume = {3},
pages = {5747-5754},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.817},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915008185},
author = {Julien Rechard and Thierry Morineau and Florine Léger and Alain Bignon and Djamal Kesraoui and Jean-Frédéric Bouillon and Pascal Berruet},
keywords = {ecological interface design, protyping, design, automatic generation},
abstract = {Vicente and Rasmussen have developed a method called Ecological Interface Design (EID) in the field of cognitive engineering .[1] In this context, we proposed a software tool named Anaxagore, that can assist the EID process for a rapid implementation of user interface prototypes. In a first stage, models based on standardized structure-functional diagram are computed .[2] In a second stage, these diagrams as inputs are transformed through a model-driven engineering process, involving a library of ecological graphical representations and an interface background. Then, an ecological interface prototype can be generated. Previously, Anaxagore was used to generate conventional synoptic representation .[3] To validate its EID extended version, an experimental protocol has been established. The experiment was conducted at the National Maritime College, with 14 naval officers randomly divided into two groups according to the used interface. After a preliminary training of participants to the use of the interfaces, four scenarios were simulated during the experiment. For each scenario, the performance was evaluated by a success score and by measuring the time to detect and understand failure. With the EID interface generated by Anaxagore, faster detection time and better diagnosis accuracy were observed. Anaxagore seems to constitute a response for assisting the rapid prototyping of ecological interface.}
}
@article{FADUL1992191,
title = {Stand-alone programmable controller for time-critical robotic systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {9},
number = {3},
pages = {191-199},
year = {1992},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(92)90023-Y},
url = {https://www.sciencedirect.com/science/article/pii/073658459290023Y},
author = {Faisal Fadul and Francisco Bas},
abstract = {This paper proposes and describes a new technique in the design and implementation of stand-alone programmable controllers which implement complex control algorithms for complex systems in general and time-critical systems in particular. Unlike other existing commercially available controllers, the proposed controller is capable of implementing all kinds of simple and complex control algorithm techniques such as PIDs, adaptive, optimal, etc. The controller is designed to handle timed events accurately due to the utilization of a new software approach based on Petri net technique as well as the use of an optimized simulation language designed for control and data acquisition applications. The most significant aspects of the proposed controller are its low cost, high speed, and easy implementation. Using the new approach, it is possible to effortlessly and efficiently simulate any controller algorithm and controlled plant, verify results if they meet pre-defined system specifications and then immediately generate and save codes on EPROMs to be placed on the controller board. The proposed controller system consists of a hardware portion and a software package. The software package is written in C and assembly languages and consists of four different programs. A prototype of the proposed controller was designed, constructed and successfully tested to implement various control algorithms. The data obtained suggest that the proposed technique will significantly aid engineers to simplify the task of implementing complex algorithms industry such as robotics.}
}
@article{LAMBERTI2003215,
title = {Move limits definition in structural optimization with sequential linear programming. Part II: Numerical examples},
journal = {Computers & Structures},
volume = {81},
number = {4},
pages = {215-238},
year = {2003},
issn = {0045-7949},
doi = {https://doi.org/10.1016/S0045-7949(02)00443-1},
url = {https://www.sciencedirect.com/science/article/pii/S0045794902004431},
author = {Luciano Lamberti and Carmine Pappalettere},
keywords = {SLP, Move limits, Linearization error, Trust region},
abstract = {A variety of numerical methods have been proposed in literature in purpose to deal with the complexity and non-linearity of structural optimization problems. In practical design, sequential linear programming (SLP) is very popular because of its inherent simplicity and because linear solvers (e.g. Simplex) are easily available. However, SLP performance is sensitive to the definition of proper move limits for the design variables which task itself often involves considerable heuristics. This research presents a new SLP algorithm (LESLP) that implements an advanced technique for defining the move limits. The linearization error sequential linear programming (LESLP) algorithm is formulated so to overcome the traditional limitations of the SLP method. In a companion paper [Comput. Struct. 81 (2003) 197] the basics of the LESLP formulation along with a guide to programming are provided. The new algorithm is successfully tested in weight minimisation problems of truss structures with up to hundreds of design variables and thousands of constraints: sizing and configuration problems are considered. Optimization problems of non-truss structures are also presented. The numerical efficiency, advantages and drawbacks of LESLP are discussed and compared to those of other SLP algorithms recently published or implemented in commercial software packages.}
}
@article{GIARDINO2012313,
title = {BOMBER: A tool for estimating water quality and bottom properties from remote sensing images},
journal = {Computers & Geosciences},
volume = {45},
pages = {313-318},
year = {2012},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2011.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0098300411004158},
author = {Claudia Giardino and Gabriele Candiani and Mariano Bresciani and Zhongping Lee and Stefano Gagliano and Monica Pepe},
keywords = {Remote sensing images, Water, Bio-optical modelling, Inversion},
abstract = {BOMBER (Bio-Optical Model Based tool for Estimating water quality and bottom properties from Remote sensing images) is a software package for simultaneous retrieval of the optical properties of water column and bottom from remotely sensed imagery, which makes use of bio-optical models for optically deep and optically shallow waters. Several menus allow the user to choose the model type, to specify the input and output files, and to set all of the variables involved in the model parameterization and inversion. The optimization technique allows the user to retrieve the maps of chlorophyll concentration, suspended particulate matter concentration, coloured dissolved organic matter absorption and, in case of shallow waters, bottom depth and distributions of up to three different types of substrate, defined by the user according to their albedo. The software requires input image data that must be atmospherically corrected to remote sensing reflectance values. For both deep and shallow water models, a map of the relative error involved in the inversion procedure is also given. The tool was originally intended to estimate water quality in lakes; however thanks to its general design, it can be applied to any other aquatic environments (e.g., coastal zones, estuaries, lagoons) for which remote sensing reflectance values are known. BOMBER is fully programmed in IDL (Interactive Data Language) and uses IDL widgets as graphical user interface. It runs as an add-on tool for the ENVI+IDL image processing software and is available on request.}
}
@article{MAGNI1997183,
title = {DT-Planner: an environment for managing dynamic decision problems},
journal = {Computer Methods and Programs in Biomedicine},
volume = {54},
number = {3},
pages = {183-200},
year = {1997},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(97)00044-8},
url = {https://www.sciencedirect.com/science/article/pii/S0169260797000448},
author = {Paolo Magni and Riccardo Bellazzi},
keywords = {Decision Markov processes, Graphical models, Dynamic programming, Decision-theoretic planning},
abstract = {The problem of formulating plans under uncertainty and coping with dynamic decision problems is a major task of both artificial intelligence and control theory applications in medicine. In this paper we will describe a software package, called DT-Planner, designed to represent and solve dynamic decision problems that can be modelled as Markov decision processes, by exploiting a novel graphical formalism, called influence view. An influence view is a directed acyclic graph that depicts the probabilistic relationships between the problem state variables in a generic time transition; additional variables, called event variables, may be added, in order to describe the conditional independencies between state variables. By using the specified conditional independence structure, an influence view may allow a parsimonious specification of a Markov decision process. DT-Planner lets the user specify and manage models through a user-friendly graphical interface, and implements efficient for policy determination algorithms. DT-Planner is written in C with Open InterfaceTM libraries and can be obtained, for non commercial use, via anonymous ftp without charge.}
}
@article{WENG2011398,
title = {Web-based post-processing visualization system for finite element analysis},
journal = {Advances in Engineering Software},
volume = {42},
number = {6},
pages = {398-407},
year = {2011},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2011.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0965997811000317},
author = {Wei-Chu Weng},
keywords = {Web system, OpenGL, Database, Finite element analysis, Post-processing, Structural static analysis},
abstract = {In this paper, we propose and implement a website of post-processing system for finite element analysis (WebDFEA). Finite element analysis is a computer-aided engineering tool and is popular for static/dynamic structure analysis. It includes three processing systems where post-processing system is to graphically demonstrate the analysis result of a structure model analyzed by finite element method. WebDFEA performs as a website. It is cross-platform because it can auto-detect a client computer platform and auto-download proper OpenGL API for drawing computer graphics. It can draw precise graphics on webpage which can be free controlled by the mouse as a manner in professional software. A database server is involved to store finite element model data and its analysis result. The graphic user interface (GUI) of WebDFEA is a flexible GUI comprising three parts: the switch buttons designed by HTML, the display board and the color bar both developed in Java. The three components are independent and cooperative with each other. They can be recombined without running errors for different purposes. A ship hull section with half a hatch is chosen as the study case to test WebDFEA website. Its finite element model comprises 11,442 triangle elements (shapes). The timeframe starting when WebDFEA is connected to the end when the model is demonstrated is acceptable.}
}
@incollection{LIESLEHTO1991211,
title = {AN EXPERT SYSTEM FOR THE MULTIVARIABLE CONTROLLER DESIGN},
editor = {R. DEVANATHAN},
booktitle = {Intelligent Tuning and Adaptive Control},
publisher = {Pergamon},
address = {Oxford},
pages = {211-216},
year = {1991},
series = {IFAC Symposia Series},
isbn = {978-0-08-040935-1},
doi = {https://doi.org/10.1016/B978-0-08-040935-1.50038-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080409351500387},
author = {J. Lieslehto and J.T. Tanttu and H.N. Koivo},
abstract = {In this paper a software package for the design of centralized and decentralized multivariable controllers is represented. The software package consists of numerical calculation programs and an expert system. The paper first describes the different subtasks of the multivariable controller design. Next a short descriptions of the design and analysis methods included in the software package is given. The description of the implementation of the expert system finishes the paper.}
}
@article{HARRIS1998243,
title = {A benchmark for automated roster generation algorithms},
journal = {International Journal of Industrial Ergonomics},
volume = {21},
number = {3},
pages = {243-247},
year = {1998},
note = {Shiftwork: Working towards solutions},
issn = {0169-8141},
doi = {https://doi.org/10.1016/S0169-8141(97)00043-7},
url = {https://www.sciencedirect.com/science/article/pii/S0169814197000437},
author = {Geoff Harris and Philip Bohle},
keywords = {Shift rosters, Software benchmarks},
abstract = {This paper describes a benchmark that enables objective comparison between the implementations of algorithms for automated shift roster generation. The benchmark consists of three computational tests that provide measures of correctness, efficiency and efficacy. The tests are designed to ensure that it is difficult to specifically fine tune an implementation to perform well on the benchmark. A recently developed implementation (Bohle and Harris, 1996) is used to provide run-time performance metrics for a variety of PC hardware configurations. Relevance to industry Software that effectively automates shift roster design has the potential to greatly reduce the financial and health costs incurred by inefficient, manual roster design. The benchmark described here provides organisations with an objective basis for evaluating the effectiveness of different rostering packages, including in-house and commercial applications.}
}
@article{GUO2015227,
title = {Developing a scalable hybrid MPI/OpenMP unstructured finite element model},
journal = {Computers & Fluids},
volume = {110},
pages = {227-234},
year = {2015},
note = {ParCFD 2013},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2014.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045793014003442},
author = {Xiaohu Guo and Michael Lange and Gerard Gorman and Lawrence Mitchell and Michèle Weiland},
keywords = {Fluidity-ICOM, OpenMP, MPI, FEM, Matrix assembly, Sparse linear solver, HYPRE, PETSc, SpMV},
abstract = {The trend of all modern computer architectures, and the path to exascale, is towards increasing numbers of lower power cores, with a decreasing memory to core ratio. This imposes a strong evolutionary pressure on algorithms and software to efficiently utilise all levels of parallelism available on a given platform while minimising data movement. Unstructured finite elements codes have long been effectively parallelised using domain decomposition methods, implemented using libraries such as the Message Passing Interface (MPI). However, there are many optimisation opportunities when threading is used for intra-node parallelisation for the latest multi-core/many-core platforms. The benefits include increased algorithmic freedom, reduced memory requirements, cache sharing, reduced number of partitions, less MPI communication and I/O overhead. In this paper, we report progress in implementing a hybrid OpenMP–MPI version of the unstructured finite element code Fluidity. For matrix assembly kernels, the OpenMP parallel algorithm uses graph colouring to identify independent sets of elements that can be assembled concurrently with no race conditions. In this phase there are no MPI overheads as each MPI process only assembles its own local part of the global matrix. We use an OpenMP threaded fork of PETSc to solve the resulting sparse linear systems of equations. We experiment with a range of preconditioners, including HYPRE which provides the algebraic multigrid preconditioner BoomerAMG where the smoother is also threaded. Since unstructured finite element codes are well known to be memory latency bound, particular attention is paid to ccNUMA architectures where data locality is particularly important to achieve good intra-node scaling characteristics. We also demonstrate that utilising non-blocking algorithms and libraries are critical to mixed-mode application so that it can achieve better parallel performance than the pure MPI version.}
}
@article{GABALDON199831,
title = {A software for the integrated design of wastewater treatment plants},
journal = {Environmental Modelling & Software},
volume = {13},
number = {1},
pages = {31-44},
year = {1998},
issn = {1364-8152},
doi = {https://doi.org/10.1016/S1364-8152(98)00002-4},
url = {https://www.sciencedirect.com/science/article/pii/S1364815298000024},
author = {Carmen Gabaldón and José Ferrer and Aurora Seco and Paula Marzal},
keywords = {wastewater treatment plant, design software, steady-state biological model},
abstract = {A software package has been developed for automated design of wastewater treatment plants. A user-friendly environment has been implemented to facilitate design tasks, allowing rapid evaluation of different alternatives as well as performing sensitivity analyses. Flexible treatment plant configurations can be established with preliminary, primary, biological and tertiary wastewater treatments, and sludge treatment units. A generalized steady-state model developed for biological processes is also included. The design process includes treatment units sizing, plant layout, hydraulic profile calculation and equipment assignment. The system capabilities for designing new plants and upgrading existing plants are illustrated through three examples.}
}
@article{JOUBERT2005133,
title = {Designing and implementing health data and information providers},
journal = {International Journal of Medical Informatics},
volume = {74},
number = {2},
pages = {133-140},
year = {2005},
note = {MIE 2003},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2004.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S1386505604001583},
author = {Michel Joubert and Jean-Charles Dufour and Sylvain Aymard and Laurent Falco and Marius Fieschi},
keywords = {Information systems, Information storage and retrieval, Computer communication networks, Databases, Databanks},
abstract = {Summary
Objectives:
To model and implement web portals providing access to certified and high-quality information in the domain of health.
Material and methods:
The Unified Medical Language System (UMLS) knowledge sources of the U.S. National Library of Medicine and principles of implementation resulting from the previous ARIANE project are described. The XML technology that allows files transformations by the means of XSLT is briefly presented.
Results:
The design and implementation of software modules that exploit knowledge sources, operate the translation of a user's query to selected information sources, and wrap obtained results are detailed. Querying documentary and factual medical databases are presented.
Discussion:
Current implementation and wrapping perspectives are discussed in terms of integration and interoperability of health information and data resources.}
}
@article{QAMAR2014184,
title = {Design and Implementation of Hybrid and Native Communication Devices for Java HPC},
journal = {Procedia Computer Science},
volume = {29},
pages = {184-197},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S187705091400194X},
author = {Bibrak Qamar and Ansar Javed and Mohsan Jameel and Aamir Shafi and Bryan Carpenter},
keywords = {High Performance Computing, Java MPI, MPJ Express, Hybrid MPI, Native MPI for Java.},
abstract = {MPJ Express is a messaging system that allows computational scientists to write and execute parallel Java applications on High Performance Computing (HPC) hardware. The software is capable of executing in two modes namely cluster and multicore modes. In the cluster mode, parallel applications execute in a typical cluster environment where multiple processing elements communicate with one another using a fast interconnect like Gigabit Ethernet or other proprietary networks like Myrinet and Infiniband. In this context, the MPJ Express library provides communication devices for Ethernet and Myrinet. In the multicore mode, the parallel Java application executes on a single system comprising of shared memory or multicore processors. In this paper, we extend the MPJ Express software to provide two new communication devices namely the native and hybrid device. The goal of the native communication device is to interface the MPJ Express software with native—typically written in C—MPI libraries. In this setting the bulk of messaging logic is offloaded to the underlying MPI library. This is attractive because MPJ Express can exploit latest features, like support for new interconnects and efficient collective communication algorithms of the native MPI library. The second device, called the hybrid device, is developed to allow efficient execution of parallel Java applications on clusters of shared memory or multicore processors. In this setting the MPJ Express runtime system runs a single multithreaded process on each node of the cluster—the number of threads in each process is equivalent to processing elements within a node. Our performance evaluation reveals that the native device allows MPJ Express to achieve comparable performance to native MPI libraries—for latency and bandwidth of point-to-point and collective communications—which is a significant gain in performance compared to existing communication devices. The hybrid communication device—without any modifications at application level—also helps parallel applications achieve better speedups and scalability. We witnessed comparative performance for various benchmarks—including NAS Parallel Benchmarks—with hybrid device as compared to}
}
@article{SREENATH1992121,
title = {A hybrid computation environment for multibody simulation},
journal = {Mathematics and Computers in Simulation},
volume = {34},
number = {2},
pages = {121-140},
year = {1992},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(92)90049-M},
url = {https://www.sciencedirect.com/science/article/pii/037847549290049M},
author = {N. Sreenath},
abstract = {A simulation architecture capable of generating the dynamical equations of a multibody system symbolically, automatically creating the computer code to simulate these equations numerically, run the simulation and display the results using animation and graphics is discussed. The power of object-oriented programming is used systematically to manipulate the symbolic, numeric and graphic modules and produce an effective tool for understanding the complicated motions of multibody systems. The architecture has been implemented in OOPSS (Object-Oriented Planar System Simulator) a software package written in a multilanguage (macsyma–fortran–lisp) environment. The package supports user interface capable of interactively modifying system parameters, change runtime initial conditions and introduce feedback control.}
}
@incollection{NOERGAARD2010255,
title = {Chapter 6 - Virtual Machines in Middleware},
editor = {Tammy Noergaard},
booktitle = {Demystifying Embedded Systems Middleware},
publisher = {Newnes},
address = {Burlington},
pages = {255-304},
year = {2010},
isbn = {978-0-7506-8455-2},
doi = {https://doi.org/10.1016/B978-0-7506-8455-2.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780750684552000066},
author = {Tammy Noergaard},
abstract = {Publisher Summary
This chapter introduces embedded virtual machines (VMs), and their function within an embedded device. It focuses on programming languages and the higher-level languages that introduce the requirement of a VM within an embedded system. It discusses the major components that make up most embedded VMs such as an execution engine, the garbage collector, and loader to name a few. It also addresses the detailed discussions of process management, memory management, and I/O system management relative to VMs and their architectural components. A powerful approach to understanding what a virtual machine (VM) is and how it works within an embedded system is by relating in theory to how an embedded operating system (OS) functions. Simply, a VM implemented as middleware software is a set of software libraries that provides an abstraction layer for software residing on top of the VM to be less dependent on hardware and underlying software. Like an OS, a VM can provide functionality that can perform everything from process management to memory management to IO system management depending on the specification it adheres to. What differentiates the inherent purpose of a VM in an embedded system versus that of an OS is introduced in the next section of this chapter, and is specifically related to the actual programming languages used for creating programs overlying a VM.}
}
@article{AFZAL2019718,
title = {Enabling IoT platforms for social IoT applications: Vision, feature mapping, and challenges},
journal = {Future Generation Computer Systems},
volume = {92},
pages = {718-731},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17312724},
author = {Bilal Afzal and Muhammad Umair and Ghalib {Asadullah Shah} and Ejaz Ahmed},
keywords = {Social IoT, Operating systems, Microcontroller architecture, Embedded systems, Resource-constrained devices},
abstract = {Social IoT (SIoT) is an emerging paradigm of IoT in which different IoT devices interact and establish relationships with each other to achieve a common goal. In essence, SIoT adapts a service-oriented architecture where heterogeneous IoT devices can offer or request autonomous services and collaborate on behalf of their owners. Operating Systems (OSs) are employed in IoT devices as they offer portability, threading support and access to development libraries; thus allowing easiness in IoT application development. Several OSs are available for IoT devices, but selecting an OS and hardware befitting for a particular IoT application is a critical task. In case of SIoT, the specific OS selection for hardware devices in various applications is even more challenging because of their collaborative nature. Existing surveys on OSs are mostly domain oriented and lack the discussion on hardware architectural features. As a consequence, it is infeasible for developers to choose best-suited OS for various hardware platforms which results in their underperformance in many application scenarios. This paper considers standard features of OS as well as hardware IoT platforms and provides an OS-to-hardware architectures features-mapping while exploring the unique requirements of SIoT applications. In doing so, resource-constrained IoT devices are particularly emphasized due to their memory constraints and power limitations. Further, a model OS architecture is proposed for devices in SIoT applications and associated open research challenges are identified. This research will benefit developers to best utilize IoT platform resources and to envisage an efficient OS for futuristic SIoT applications.}
}
@article{PONS201689,
title = {A comprehensive open package format for preservation and distribution of geospatial data and metadata},
journal = {Computers & Geosciences},
volume = {97},
pages = {89-97},
year = {2016},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416303533},
author = {X. Pons and J. Masó},
keywords = {Data standard, Internet GIS, Metadata, Data model, Preservation, Package, MMZX},
abstract = {The complexities of the intricate geospatial resources and formats make preservation and distribution of GIS data difficult even among experts. The proliferation of, for instance, KML, Internet map services, etc, reflects the need for sharing geodata but a comprehensive solution when having to deal with data and metadata of a certain complexity is not currently provided. Original geospatial data is usually divided into several parts to record its different aspects (spatial and thematic features, etc), plus additional files containing, metadata, symbolization specifications and tables, etc; these parts are encoded in different formats, both standard and proprietary. To simplify data access, software providers encourage the use of an additional element that we call generically “map project”, and this contains links to other parts (local or remote). Consequently, in order to distribute the data and metadata refereed by the map in a complete way, or to apply the Open Archival Information System (OAIS) standard to preserve it for the future, we need to face the multipart problem. This paper proposes a package allowing the distribution of real (comprehensive although diverse and complex) GIS data over the Internet and for data preservation. This proposal, complemented with the right tools, hides but keeps the multipart structure, so providing a simpler but professional user experience. Several packaging strategies are reviewed in the paper, and a solution based on ISO 29500-2 standard is chosen. The solution also considers the adoption of the recent Open Geospatial Consortium Web Services common standard (OGC OWS) context document as map part, and as a way for also combining data files with geospatial services. Finally, and by using adequate strategies, different GIS implementations can use several parts of the package and ignore the rest: a philosophy that has proven useful (e.g. in TIFF).}
}
@article{ARNALDSSON201437,
title = {Numerical scheme to simulate flow through anisotropic rocks in TOUGH2},
journal = {Computers & Geosciences},
volume = {65},
pages = {37-45},
year = {2014},
note = {TOUGH Symposium 2012},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413002239},
author = {Andri Arnaldsson and Jean-Claude Berthet and Snorri Kjaran and Sven Þ. Sigurðsson},
keywords = {TOUGH2, Anisotropy, Groundwater, Geothermal, Numerical, Modeling},
abstract = {A new numerical scheme for fully tensorial treatment of anisotropic flow within model layers (2D) has been designed and implemented into the TOUGH family of simulators. The new scheme has been rigorously tested against a simple theoretical solution to Darcy's law as well as a more complicated example solved by numerical software packages with anisotropic flow capabilities. In all cases, a good agreement with the new scheme has been found.}
}
@article{KOLB1993785,
title = {An Object Oriented Kernel for Control Systems Engineering},
journal = {IFAC Proceedings Volumes},
volume = {26},
number = {2, Part 2},
pages = {785-788},
year = {1993},
note = {12th Triennal Wold Congress of the International Federation of Automatic control. Volume 2 Robust Control, Design and Software, Sydney, Australia, 18-23 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)49052-5},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017490525},
author = {P. Kolb and C.A. Ganz and M. Rickli},
keywords = {CACSD, control systems engineering, software design, process Models, modelling},
abstract = {In the last decade, several computer aided control system design (CACSD) packages have been implemented. Although some of these packages use modern concepts such as abstract data typing and integrate different software tools for calculation and simulation, many of them are not practically successful. One problem which arises during a complete control system design is the vastly growing amount of resulting data. The user can be overwhelmed by such data-management responsibilities, misinterpreting dala or not remembering their origin. Being responsible for the management of data the user easily loses lrack of his work. He or she may misinterpret data, not remembering their origin. Another problem is that many software packages do not meet the user requirements. This paper describes a solution-based model of an object oriented kernel for control systems engineering. First we will give a summary of user expectations of this program. From this content model we will derive a programming model which meets the requirements of the object oriented program. Finally, we will demonstrate how the user can actually use this kernel.}
}
@article{BOONSTRA200638,
title = {Interpreting an ERP-implementation project from a stakeholder perspective},
journal = {International Journal of Project Management},
volume = {24},
number = {1},
pages = {38-52},
year = {2006},
issn = {0263-7863},
doi = {https://doi.org/10.1016/j.ijproman.2005.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0263786305000621},
author = {Albert Boonstra},
keywords = {ERP (enterprise resource planning system), Stakeholder analysis, Change, Information technology, Implementation},
abstract = {ERP-systems are software packages that enable the integration of transactions oriented data and business processes throughout an organisation. ERP-implementation projects can be viewed as processes of organisational change: many problems related to ERP-implementation are related to a misfit of the system with the characteristics of the organisation. This article uses the evidence of a case study to uncover some important dimensions of the organisational change issues related to ERP-projects. The study shows how ERP-implementation can impact the interests of stakeholders of the ERP-system and how these groups may react by influencing the course of events, for example by altering the design and implementation in ways that are more consistent with their interests. Understanding the possible impact of ERP on particular interests of stakeholders may help project managers and others to manage ERP-implementations more effectively.}
}
@article{PASHKEVICH2001269,
title = {Integrated Design of Robotic Welding Systems},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {17},
pages = {269-274},
year = {2001},
note = {10th IFAC Symposium on Information Control Problems in Manufacturing (INCOM 2001), Vienna, Austria, 20-22 September 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)33291-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017332913},
author = {A. Pashkevich and A. Dolgui and O. Zaikin and K. Semkin},
keywords = {redundant manipulators, inverse kinematic problem, positioning systems},
abstract = {The paper focuses on the integrated design of a redundant two-manipulator robotic system taking into account particularities of the arc welding technology. It have been proposed a novel formulation and closed-form solution of the inverse kinematic problem that deals with explicit definition of the weld joint orientation relative to the gravity. There have been carried out detailed investigation of singularities and uniqueness-existence topics. The presented results are implemented in a commercial software package and verified for real-life applications.}
}
@article{LIN2020110720,
title = {Adequacy evaluation of smoothed particle hydrodynamics methods for simulating the external-flooding scenario},
journal = {Nuclear Engineering and Design},
volume = {365},
pages = {110720},
year = {2020},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2020.110720},
url = {https://www.sciencedirect.com/science/article/pii/S0029549320302144},
author = {Linyu Lin and Niels Montanari and Steven Prescott and Ram Sampath and Han Bao and Nam Dinh},
keywords = {External flooding, CSAU/EMDAP, Validation, Scaling, SPH},
abstract = {In modern nuclear risk analysis for external-flooding scenarios, Computational Fluid Dynamics (CFD) tools are used to simulate the generation, propagation, and interactions of Nuclear Power Plants (NPPs) with the nuclear Systems, Structures, and Components (SSCs). Smoothed Particle Hydrodynamics (SPH), as a Lagrangian and mesh-free method, is one of the particle-based CFD methods. Since SPH methods can effectively handling large-scale fluid simulations with complex interfacial structures, SPH-based software has been used to simulate the impacts of external flood onto nuclear facilities, and the simulation results have been used to support nuclear safety analysis. However, previous risk analysis assumes that SPH methods and the corresponding simulation packages are applicable to the external-hazards risk analysis, and their simulation uncertainties do not affect the confidence of safety decision. Considering the high consequences to nuclear safety induced by simulation errors, a systematic and complete validation process is needed to evaluate the adequacy of SPH simulations in informing related safety decisions. In this study, a scoping-stage assessment is performed for SPH’s adequacy in simulating the real-scale external flooding scenarios, especially in predicting the surface-wave impacts on SSCs at NPP sites. To ensure the completeness and consistency, validation frameworks, Code Scalability Applicability and Uncertainty (CSAU), and its regulatory guide, Evaluation Model Development and Assessment Process (EMDAP) are followed to guide validation activities and to make final code adequacy assessment. First, an external-flooding scenario is designed, and SPH simulations are performed with an SPH-based software named Neutrino. A Phenomenon Identification and Ranking Table (PIRT) is created, and the surface-wave impacts are identified as one of the high-rank phenomena. At the same time, a performance measurement standard is created for measuring the code adequacy in informing safety decisions consistently and transparently. Next, numerical benchmarks are designed for assessing the code adequacy of SPH methods and corresponding software implementations on Neutrino. Next, code accuracy is evaluated by comparing simulation results from Neutrino against experimental measurements in each benchmark. Meanwhile, a scaling analysis is performed to determine a group of dimensionless number for characterizing important physics and to assess the applicability of validation database collected in reduced-scale facility to the prototypic scenario. Finally, results from all activities are brought together to make an adequacy decision. It is found that, based on the current evidence, SPH methods and associated Neutrino software can predict the unbroken surface-wave peak pressure onto stationary rigid with reasonable accuracy if the suggested sizes of particles are used. However, it is suggested by independent reviews that the validity of major assumptions in target applications need to be evaluated with large-scale experiments, and the relevancy of other phenomena like turbulence and air pockets need to be identified with more benchmarks. As for the SPH’s adequacy in predicting the impact forces on dynamic rigid, the available evidence is not sufficient to support the decisions.}
}
@article{LEE2016119,
title = {BasinVis 1.0: A MATLAB®-based program for sedimentary basin subsidence analysis and visualization},
journal = {Computers & Geosciences},
volume = {91},
pages = {119-127},
year = {2016},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2016.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416300899},
author = {Eun Young Lee and Johannes Novotny and Michael Wagreich},
keywords = {MATLAB, BasinVis, Subsidence, Backstripping, Decompaction, Visualization, Interpolation},
abstract = {Stratigraphic and structural mapping is important to understand the internal structure of sedimentary basins. Subsidence analysis provides significant insights for basin evolution. We designed a new software package to process and visualize stratigraphic setting and subsidence evolution of sedimentary basins from well data. BasinVis 1.0 is implemented in MATLAB®, a multi-paradigm numerical computing environment, and employs two numerical methods: interpolation and subsidence analysis. Five different interpolation methods (linear, natural, cubic spline, Kriging, and thin-plate spline) are provided in this program for surface modeling. The subsidence analysis consists of decompaction and backstripping techniques. BasinVis 1.0 incorporates five main processing steps; (1) setup (study area and stratigraphic units), (2) loading well data, (3) stratigraphic setting visualization, (4) subsidence parameter input, and (5) subsidence analysis and visualization. For in-depth analysis, our software provides cross-section and dip-slip fault backstripping tools. The graphical user interface guides users through the workflow and provides tools to analyze and export the results. Interpolation and subsidence results are cached to minimize redundant computations and improve the interactivity of the program. All 2D and 3D visualizations are created by using MATLAB plotting functions, which enables users to fine-tune the results using the full range of available plot options in MATLAB. We demonstrate all functions in a case study of Miocene sediment in the central Vienna Basin.}
}
@article{ERIKSSON2017348,
title = {Optimization and integration of hybrid renewable energy hydrogen fuel cell energy systems – A critical review},
journal = {Applied Energy},
volume = {202},
pages = {348-364},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.03.132},
url = {https://www.sciencedirect.com/science/article/pii/S0306261917306256},
author = {E.L.V. Eriksson and E.MacA. Gray},
keywords = {Renewable energy, Design methodology, Design constraints, Hydrogen, Electrolyser, Fuel cell, Hydrogen fuel cell energy systems, Integration, Microgrid, Optimisation, Genetic algorithm, System integration, Software tools},
abstract = {Electricity generation presents the biggest opportunity to lower CO2 emissions and it is foreseen that hydrogen energy technology will play an important role in realising the scenario to cap global warming at 2°C through replacement of fossil fuels with renewables. The transition to electric power for transport in battery- and fuel-cell-electric vehicles will further increase the need for low-carbon electricity generation. For a successful transition to a renewable energy economy the traditional approach of designing energy systems to meet only goals related to the technology (capacity, availability, reliability) and economics (return on investment, cost to the consumer) must evolve to take on a more holistic viewpoint and be able to take into account other goals addressing environmental and social considerations. This paper reviews approaches for integrating hydrogen energy technology into hybrid energy systems, emphasising electricity generation using a hydrogen fuel cell. Integration of energy storage, sizing methodologies, energy flow management and their associated optimization algorithms and software implementation are addressed. Few published case studies go beyond technical considerations. This reality is discussed in the light of available software packages. A four-dimensional multi-objective meta-heuristic function is proposed with weighting of technical, economic, environmental and socio-political factors to suit the design goals for the energy system.}
}
@article{HYNCICA2009194,
title = {Protocol Gateways for HART Sensors},
journal = {IFAC Proceedings Volumes},
volume = {42},
number = {1},
pages = {194-197},
year = {2009},
note = {9th IFAC Workshop on Programmable Devices and Embedded Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20090210-3-CZ-4002.00040},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016324703},
author = {Ondřej Hynĉica and Petr Fiedler and Zdeněk Bradáĉ and Pavel Kuĉera and Petr Honzík},
keywords = {wireless communication, gateway, HART, Bluetooth, Ethernet, ZigBee},
abstract = {The paper presents description of three different protocol gateways for interfacing HART pressure sensors. The gateways have been designed, programmed and realized as hardware prototypes. The first gateway is a wireless transducer between the HART interface of the sensor and a Bluetooth network. The second gateway connects the sensor to an Ethernet network with Modbus protocol. The third gateway is a wireless transducer for ZigBee networks. The gateways are based on embedded microcontrollers with the required interfaces. The software implementation is done in C using protocol libraries. Hardware and enclosure are designed for industrial applications. The gateways are adapted for use with HART pressure sensors.}
}
@article{SEPULVEDA20171071,
title = {Architecture, Languages, Compilation and Hardware support for Emerging ManYcore systems (ALCHEMY): Preface},
journal = {Procedia Computer Science},
volume = {108},
pages = {1071-1072},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.276},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917309286},
author = {Johanna Sepúlveda and Vania Marangozova-Martin and Jeronimo Castrillon},
keywords = {Manycore, code portability, high performance, usability, security, reliability},
abstract = {Manycore systems are one of the key enabler technologies for most of current computational paradigms, including Internet of things, data-centers on chip and big data processing. These paradigms are characterized by tight and demanding requirements such as code portability, dynamicity, high performance, usability, predictability, reliability, low power and security. This combination of requirements has led to heterogeneous manycore systems which are extremely challenging to design and to program. As a result, a large body of research has focused on development of languages, simulation environments and analysis tools that allow to model and predict the behavior of this type of systems from early design stages. ALCHEMY 2017 presents five research works addressing the challenges of code portability, high performance, usability, security and reliability in manycore systems. These are namely: 1. ”An OpenMP backend for the Sigma-C streaming language, addresses the software portability challenge of manycore architectures. It proposes an implementation of an OpenMP backend for the SigmaC language, a cycle-static data flow abstraction to program many-core embedded platforms. Its compilation scheme allows for utilization of future manycore embedded systems such as Kalray’s MPPA.2. A multi-level optimization strategy to improve the performance of the stencil computation combines manual vectorization, space tiling and stencil composition for achieving high performance of stencil kernels on manycore systems. The evaluation with three compilers (Intel, Clang and GCC) and two target multi-core platforms (Intel Broadwell and Ivybridge) reports better results compared to the state of the art.3. A Distributed Shared Memory Model and C++ Templated Meta-Programming Interface for the Epiphany RISC Array Processor addresses the usability challenge. It proposes techniques for data layout and parallel loop order abstraction as a parallel programming API targeting the Epiphany architecture. This results into a transparent distributed shared memory (DSM) model for Epiphany that eliminates the need to manage local data movement between cores.4. Towards Protected MPSoC Communication for Information Protection against a Malicious NoC deals with vulnerabilities on Network-on-Chip (NoC). The authors propose a security protocol which allows the secure communication among the cores of the system, even in the presence of Trojan insertions at the NoC whose aim is to modify and steal data.5. GPU-Accelerated Real-Time Path Planning and the Predictable Execution Model addresses the reliability challenge and tackles the important problem of ensuring reliable Worst Case Execution Time for Real-Time and Cyber Physical Systems. While considering heterogeneous (CPU/GPU), the idea is to separate memory and processor operations through Time-Division Multiplexing (TDM).}
}
@article{VACEV20151,
title = {Testing and finite element analysis of reinforced concrete column footings failing by punching shear},
journal = {Engineering Structures},
volume = {92},
pages = {1-14},
year = {2015},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2015.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0141029615001200},
author = {Todor Vacev and Zoran Bonić and Verka Prolović and Nebojša Davidović and Dragan Lukić},
keywords = {Reinforced concrete, Column footing, Punching, Test, FEA, ANSYS},
abstract = {Finite element analysis (FEA) and engineering software is increasingly used in modelling of different structures and in analyzing their behaviour. The subject of this paper was an analysis of behaviour of reinforced concrete column footing laid on deformable subgrade and loaded by concentrated load until failure. The modelling and 3D nonlinear analysis were implemented by applying finite element method (FEM) and using software package ANSYS 14.5. Field test data were used for calibration of the FE model and validation of the adopted parameters for all materials. The comparison of the field test results and FEA results showed good agreement, but also revealed some questions regarding FEA, and especially concrete crushing.}
}
@incollection{TOYOOKA1992115,
title = {OPEN ARCHITECTURE DIRECT DRIVE MANIPULATOR RESEARCH AND DEVELOPMENT PACKAGE},
editor = {N.A. KHEIR and G.F. FRANKLIN and M.J. RABINS},
booktitle = {Advances in Control Education 1991},
publisher = {Pergamon},
address = {Amsterdam},
pages = {115-121},
year = {1992},
series = {IFAC Symposia Series},
isbn = {978-0-08-040958-0},
doi = {https://doi.org/10.1016/B978-0-08-040958-0.50026-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080409580500269},
author = {M. Toyooka and M. Tomizuka and J. Butler},
abstract = {This paper describes the Direct Drive Manipulator Research and Development Package, which includes a two link direct drive manipulator, amplifiers, a multi-processing controls workstation, a mathematical model of the manipulator system, and software environment which is used for the development and execution of real-time control algorithms. The software provides a C-language based Robot Programming Language (RPL), control algorithm development environment, and a robot simulator. The package is based on an open architecture with full user programming capability. The user can enter the control loop at any level with the ability to modify or replace any part of the RPL, servo routine, inverse kinematics, or trajectory generation modules. The package provides a convenient workstation to develop a control algorithm, simulate, implement, and fine-tune the controller. The simulation is a unique part of this package being a “semi-active” simulation which uses a mathematical model of the mechanical system while using the actual control hardware that is used for control implementation. Mechanical nonlinearities such as dynamic coupling and Coulomb friction are taken into account. Since the actual controller is used, the simulation accounts for quantization, integer overflow, and controller dynamics. This package is ideal for an advanced robotics research program or a lab oriented graduate course in robotics or controls.}
}
@article{SIPOS1990279,
title = {TeamSo - A Software Development and Maintenance Support System},
journal = {IFAC Proceedings Volumes},
volume = {23},
number = {8, Part 4},
pages = {279-282},
year = {1990},
note = {11th IFAC World Congress on Automatic Control, Tallinn, 1990 - Volume 4, Tallinn, Finland},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51836-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017518364},
author = {F. Sipos and T. Sztanó and Cs. Demjén},
keywords = {Software management tools, maintenance of software, documentation support, program frame generation, data dictionary},
abstract = {A software tool - called TeamSo - has been designed and developed to support the teamwork in software development projects. TeamSo improves the organization and quality of work and makes some working phases in an automatic way. Increasing thereby the productivity of developers. TeamSo offers uniform designing concepts in form of a data dictionary, a project library containing the definitions of components of the target system, man-machine interface to handle the data in the project library, and automatic creation of document in different points of the software life cycle. Program-frames and data structures for the target system can be generated, improving the consistency of plans and program codes. According to the first experiences, the best efficiency of TeamSo can be reached if it is integrated into the development software environment. For this reason, it has been implemented under four different operating systems, and proved to be a powerful tool in complex software projects}
}
@article{LAZOS2003241,
title = {An integrated research tool for X-ray imaging simulation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {70},
number = {3},
pages = {241-251},
year = {2003},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(02)00015-9},
url = {https://www.sciencedirect.com/science/article/pii/S0169260702000159},
author = {D Lazos and K Bliznakova and Z Kolitsi and N Pallikarakis},
keywords = {Monte Carlo, Imaging, Simulation, Projection radiography},
abstract = {This paper presents a software simulation package of the entire X-ray projection radiography process including beam generation, absorber structure and composition, irradiation set up, radiation transport through the absorbing medium, image formation and dose calculation. Phantoms are created as composite objects from geometrical or voxelized primitives and can be subjected to simulated irradiation process. The acquired projection images represent the two-dimensional spatial distribution of the energy absorbed in the detector and are formed at any geometry, taking into account energy spectrum, beam geometry and detector response. This software tool is the evolution of a previously presented system, with new functionalities, user interface and an expanded range of applications. This has been achieved mainly by the use of combinatorial geometry for phantom design and the implementation of a Monte Carlo code for the simulation of the radiation interaction at the absorber and the detector.}
}
@article{SHAH1995765,
title = {User interfaces for mathematical programming based multipurpose plant optimisation systems},
journal = {Computers & Chemical Engineering},
volume = {19},
pages = {765-772},
year = {1995},
note = {European Symposium on Computer Aided Process Engineering\3-5},
issn = {0098-1354},
doi = {https://doi.org/10.1016/0098-1354(95)87127-6},
url = {https://www.sciencedirect.com/science/article/pii/0098135495871276},
author = {N. Shah and K. Kuriyan and L. Liberis and C.C. Pantelides and L.G. Papageorgiou and P. Riminucci},
keywords = {Production scheduling, plant design, graphical user-interfaces.},
abstract = {This paper is concerned with the design and implementation of an integrated software package for the optimisation of general multipurpose batch and semi-continuous plants. In particular, we describe a sophisticated graphical user interface that supports the rapid specification of scheduling, planning, design and retrofit problems in an error-free manner. These problems are posed graphically and solved under a unified framework based on the state-task network process representation and a general characterisation of all production resources, such as processing and storage equipment, utilities and manpower. This representation facilitates the specification of processing recipes of arbitrary complexity, involving material recycles, shared intermediates, batch splitting and mixing and multiple routes to the same end-product. The architecture employed shields the users from the complex mathematical models and solution procedures that are employed, freeing up their time for problem specification and results analysis.}
}
@article{DJOUIMAA2007779,
title = {Transonic turbine blade loading calculations using different turbulence models – effects of reflecting and non-reflecting boundary conditions},
journal = {Applied Thermal Engineering},
volume = {27},
number = {4},
pages = {779-787},
year = {2007},
note = {Energy: Production, Distribution and Conservation},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2006.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S1359431106003681},
author = {S. Djouimaa and L. Messaoudi and Paul W. Giel},
keywords = {Transonic gas turbine, Turbulent conditions, RBC and NRBC Effects},
abstract = {The objective of this study is to simulate the transonic gas turbine blade-to-blade compressible fluid flow. We are interested mainly in the determination of the pressure distribution around the blade. The particular blade architecture makes these simulations more complex due to the variety of phenomena induced by this flow. Our study is based on the experiment performed by Giel and colleagues. Tests were conducted in a linear cascade at the NASA Glenn Research Center. The test article was a turbine rotor with design flow turning of 136° and an axial chord of 12.7cm. Simulations were performed on an irregular quadratic structured grid with the FLUENT software package which solves the Navier–Stokes equations by using finite volume methods. Two-dimensional stationary numerical simulations were made under turbulent conditions allowing us to compare the characteristic flow effects of Reflecting Boundary Conditions (RBC) and Non-Reflecting Boundary Conditions (NRBC) newly implemented in FLUENT 6.0. Many simulations were made to compare different turbulence models: a one equation model (Spalart–Allmaras), several two-equation models (k–ε, RNG k–ε, Realizable k–ε, SST k–ω), and a Reynolds-stress model (RSM). Also examined were the effects of the inlet turbulence intensities (0.25% and 7%), the exit Mach numbers (1.0 and 1.3) and the inlet Reynolds numbers (0.5×106 and 1×106). The results obtained show a good correlation with the experiment.}
}
@article{SIMANAVICIENE201447,
title = {Assessing reliability of design, construction, and safety related decisions},
journal = {Automation in Construction},
volume = {39},
pages = {47-58},
year = {2014},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2013.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0926580513002112},
author = {Ruta Simanaviciene and Rita Liaudanskiene and Leonas Ustinovichius},
keywords = {Multiple attribute decision making methods, Construction, Technological, Safety, Reliability, Sensitivity analysis, Error},
abstract = {Currently there is no approach which would help to comprehensively ensure occupational safety. Many scientists perform researches and calculations, create new methods related to safety and health, but most of them analyze separate aspects of safety in the field of construction. The authors of this paper present a new complex view on ensuring occupational safety and health during construction. The selection of safety solutions is performed based on complex evaluation of structure, technology and safety. In their previous works, the authors offered a new method for multiple attribute decision synthesis, SyMAD-3, which helps to choose an effective construction project alternative from multiple alternatives by assessing various construction, technological and occupational safety solutions, based on a set of quantitative attributes. However, the integration of these solutions may cause doubts, since decision making in construction is always associated with uncertainty. The investment projects in construction are characterized by the large accuracy variation (from 15 to 50%) of some attribute values. Although the SyMAD-3 method is mathematically grounded, it does not answer the question if the error of attribute values impacts the final decision and if this decision can be reliably assessed. In the present paper, the authors supplement the SyMAD-3 method with decision sensitivity analysis (SyMAD-3 with SA) to improve the reliability of the SyMAD-3 method and assess the reliability of the obtained decision. The SyMAD-3 with SA method allows us to choose an effective alternative of a construction project by assessing three stages of construction, based on a set of attributes given the error of their values, and determine the reliability of the final decision. The proposed method is implemented in a software package created by the authors with the aim of analyzing decisions and performing experimental calculations in the field of construction.}
}
@article{BASUALDO1998277,
title = {Rigorous simulation of conventional and unconventional multicomponent batch distillation behavior for startup and production periods},
journal = {IFAC Proceedings Volumes},
volume = {31},
number = {11},
pages = {277-282},
year = {1998},
note = {5th IFAC Symposium on Dynamics and Control of Process Systems 1998 (DYCOPS 5), Corfu, Greece, 8-10 June 1998},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)44941-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701744941X},
author = {Marta S. Basualdo and Carlos A. Ruiz},
keywords = {rigorous model simulation, batch reactive distillation, design, operability, control},
abstract = {A comparative study applied to multicomponent batch distillation using, for the reactive cases, conventional and unconventional column configurations is presented. Many other works in the literature have shown similar studies but much of them did not compare with experimental data nor use a rigorous software package either. Here the program called (ctive istillation namic imulator) is used for rigorous simulation purposes. This package permits the solution of a wide range of distillation problems accounting for simultaneous design, operation and control analysis, for single and multiple duties. Four example problems, two of them involving a reactive batch distillation column for the ethyl acetate production and its inverse configuration respectively. This last case was specially developed for this work in order to study such a system starting from the experimental data of the first case. The other two examples correspond to conventional multicomponent batch distillation columns, one of them using an optimal reflux ratio profile and operation conditions. The data employed for three examples are taken from experimental cases published in the literature. In addition the predicted dynamic behavior of a batch distillation column for separating a ternary mixture, obtained by, is compared with the results given by two well-known commercial software in order to evaluate its performance for implementing them in an inferential control structure inspired in the extended Luenberger observer design philosophy.}
}
@article{BALLING198687,
title = {Methods for interfacing analysis software to optimization software},
journal = {Computers & Structures},
volume = {22},
number = {1},
pages = {87-98},
year = {1986},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(86)90088-X},
url = {https://www.sciencedirect.com/science/article/pii/004579498690088X},
author = {Richard J. Balling and Alan R. Parkinson and Joseph C. Free},
abstract = {Three methods are presented for interfacing analysis software to optimization software to create design software. These methods are referred to as the “conventional interface”, the “pro-gramming-free interface”, and the “generalized interface”. The latter two methods introduce new ideas which are attractive from the user's standpoint. The programming-free interface simplifies the interface process by eliminating the necessity for tlie user to modify the analysis source code. The generalized interface allows one to create a general-purpose design package from a general-purpose analysis package. Support for the methods has been implemented in a software package named OPTDES.BYU. Use of the methods with this package is illustrated with a simple example.}
}
@article{NORGAARD1997931,
title = {NNSYSID and NNCTRL - MATLAB Tools for System Identification and Control with Neural Networks},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {11},
pages = {931-936},
year = {1997},
note = {IFAC Symposium on System Identification (SYSID'97), Kitakyushu, Fukuoka, Japan, 8-11 July 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)42966-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017429661},
author = {M. Nørgaard and N.K. Poulsen and O. Ravn},
keywords = {Software tools, system identification, nonlinear models, adaptive control, nonlinear control systems, neural networks},
abstract = {Two MATLAB packages have been implemented: the Neural Network Based System Identification toolbox (NNSYSID) and the Neural Network Based Control System Design Toolkit (NNCTRL). The NNSYSID toolbox has been developed to assist identification of nonlinear dynamic systems and it offers the possibility to work with a number of different nonlinear model structures based on neural networks. The NNCTRL toolkit is an add-on to the NNSYSID toolbox and contains tools for design and simulation of control systems based on neural networks. This paper gives an overview of the contents of NNSYSID and NNCTRL.}
}
@article{LIU200551,
title = {A proactive approach towards always-on availability in broadband cable networks},
journal = {Computer Communications},
volume = {28},
number = {1},
pages = {51-64},
year = {2005},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2004.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0140366404003202},
author = {Yun Liu and Yue Ma and James J. Han and Haim Levendel and Kishor S. Trivedi},
keywords = {Availability, Cable modem termination system, Petri net, Preventive maintenance, Software rejuvenation, Stochastic reward net},
abstract = {In this paper, we propose a high availability design of a Cable Modem Termination System (CMTS) clusters system based on the software rejuvenation technique. This proactive system maintenance technique is aimed to reduce system outages and the associated downtime cost due to the ‘software aging’ phenomenon. Different rejuvenation policies are studied from the perspectives of design, implementations, and availability assessment. To evaluate these policies, stochastic reward net models are developed and solved by Stochastic Petri Net Package (SPNP). Numerical results show that the deployment of software rejuvenation in the system leads to significant improvement in capacity-oriented availability and reduction in downtime cost. The optimization of the rejuvenation interval in the time-based approach and the effect of the prediction coverage in the measurement-based approach are also studied in this paper.}
}
@article{SCHETTINO2012135,
title = {Magan: A new approach to the analysis and interpretation of marine magnetic anomalies},
journal = {Computers & Geosciences},
volume = {39},
pages = {135-144},
year = {2012},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2011.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0098300411002366},
author = {Antonio Schettino},
keywords = {Marine magnetic anomalies, Plate kinematics, Forward modeling, Sea-floor spreading},
abstract = {The identification of marine magnetic anomalies is an important phase of plate tectonic modeling, but is limited by the lack of professional software, either free or commercial, which may help in the accomplishment of this task, and by the practice of performing approximations that may prevent in some instances a correct interpretation of the magnetic data. Although basic forward-modeling and inversion algorithms that may be incorporated in the core of gravity or magnetic application software have been published since the late 1950s, most research groups have implemented their own tools independently from each other, and apart from a few cases such computer programs are not publicly accessible. Here a new methodology of analysis of marine magnetic data is described, which allows a quantitative correlation of magnetic anomalies from different profiles and a statistical determination of relative plate velocities. The method is implemented through a new free software package, Magan, available for the MS Windows environment. The program is especially designed to work with NGDC GEODAS ship-track and aeromagnetic data, but allows the import of any ASCII text file containing magnetic anomaly data. The basic forward-modeling algorithms included in the Magan core are based on well-known techniques of potential field geophysics, modified to take into account specific requirements of marine magnetic data analysis and plate tectonic modeling. Such a kernel is flanked by a friendly graphical user interface (GUI), which helps and speeds up the interpretation of the ship-track data. In particular, the program allows one to (1) draw and edit flow lines where magnetic data can be projected, (2) calculate more accurately modeled anomalies through the use of apparent polar wander paths and single block parameters, (3) generate age–distance and time–velocity graphs, and (4) generate crossing point files that can be subsequently used to build magnetic isochrons.}
}
@article{PIRRO2010444,
title = {UFOme: An ontology mapping system with strategy prediction capabilities},
journal = {Data & Knowledge Engineering},
volume = {69},
number = {5},
pages = {444-471},
year = {2010},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2009.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X09001840},
author = {Giuseppe Pirró and Domenico Talia},
keywords = {Ontology mapping tools, Ontology mapping, Automatic ontology mapping strategy prediction},
abstract = {Ontology mapping, or matching, aims at identifying correspondences among entities in different ontologies. Several strands of research come up with algorithms often combining multiple mapping strategies to improve the mapping accuracy. However, few approaches have systematically investigated the requirements of a mapping system both from the functional (i.e., the features that are required) and user point of view (i.e., how the user can exploit these features). This paper presents an ontology mapping software framework that has been designed and implemented to help users (both expert and non-expert) in designing and/or exploiting comprehensive mapping systems. It is based on a library of mapping modules implementing functions such as discovering mappings or evaluating mapping strategies. In particular, the strategy predictor module of the designed framework, for each specific mapping task, can “predict” mapping modules to be exploited and parameter values (e.g., weights and thresholds). The implemented system, called UFOme, assists users during the various phases of a mapping task execution by providing a user friendly ontology mapping environment. The UFOme implementation and its prediction capabilities and accuracy were evaluated on the Ontology Alignment Evaluation Initiative tests with encouraging results.}
}
@article{RANKY19842511,
title = {A Software Library for Designing and Controlling Flexible Manufacturing Systems},
journal = {IFAC Proceedings Volumes},
volume = {17},
number = {2},
pages = {2511-2516},
year = {1984},
note = {9th IFAC World Congress: A Bridge Between Control Science and Technology, Budapest, Hungary, 2-6 July 1984},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)61359-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017613594},
author = {P.G. Ránky},
keywords = {FMS Software, Flexible Manufacturing System, CAM, Computer Aided Manufacture, CIM, Computer Integrated Manufacturing, DNC control, pallet alignment error compensation, lot size analysis, FMS capacity planning, FMS data base},
abstract = {The FMS Software Library has been created by the author with the aim of providing modular software tools and turnkey programs for designing, implementing, controlling and maintaining FMS (Flexible Manufacturing Systems). The Library, capable of running on over thirty different micro-, and mini computers, contains user friendly programs for FMS project planning, DNC control, assembly line balancing, FMS crew size analysis, conventional and dynamic scheduling, part programming using a macro library, batch size analysis, FMS capacity planning, pallet alignment and positioning error calculation for machining centres, graphic simulation and design of robot arms, robot positioning and orientation error analysis and a variety of FMS data bases. The paper gives a general overview as well as discussing some of the packages in more detail.}
}
@article{CHAWLA2016712,
title = {Cloud-based automatic test data generation framework},
journal = {Journal of Computer and System Sciences},
volume = {82},
number = {5},
pages = {712-738},
year = {2016},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2015.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022000016000027},
author = {Priyanka Chawla and Inderveer Chana and Ajay Rana},
keywords = {Software testing, Cloud computing, MapReduce, Soft computing, Particle swarm optimization, Genetic algorithm, Pareto-optimal, Cloud-based testing},
abstract = {Designing test cases is one of the most crucial activities in software testing process. Manual test case design might result in inadequate testing outputs due to lack of expertise and/or skill requirements. This article delivers automatic test data generation framework by effectively utilizing soft computing technique with Apache Hadoop MapReduce as the parallelization framework. We have evaluated and analyzed statistically our proposed framework using real world open source libraries. The experimental results conducted on Hadoop cluster with ten nodes are effective and our framework significantly outperforms other existing cloud-based testing models.}
}
@article{DAMIAN20021567,
title = {The kinetic preprocessor KPP-a software environment for solving chemical kinetics},
journal = {Computers & Chemical Engineering},
volume = {26},
number = {11},
pages = {1567-1579},
year = {2002},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(02)00128-X},
url = {https://www.sciencedirect.com/science/article/pii/S009813540200128X},
author = {Valeriu Damian and Adrian Sandu and Mirela Damian and Florian Potra and Gregory R. Carmichael},
keywords = {Chemical kinetics, Automatic code generation, Sparsity, Numerical integration},
abstract = {The kinetic preprocessor (KPP) is a software tool that assists the computer simulation of chemical kinetic systems. The concentrations of a chemical system evolve in time according to the differential law of mass action kinetics. A computer simulation requires the implementation of the differential system and its numerical integration in time. KPP translates a specification of the chemical mechanism into fortran or c simulation code that implement the concentration time derivative function and its Jacobian, together with a suitable numerical integration scheme. Sparsity in Jacobian is carefully exploited in order to obtain computational efficiency. KPP incorporates a library with several widely used atmospheric chemistry mechanisms and users can add their own chemical mechanisms to the library. KPP also includes a comprehensive suite of stiff numerical integrators. The KPP development environment is designed in a modular fashion and allows for rapid prototyping of new chemical kinetic schemes as well as new numerical integration methods.}
}
@article{BAUML20087600,
title = {Approximate observer error linearization for nonlinear systems with input},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {7600-7605},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.01285},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016401680},
author = {Markus Bäuml and Joachim Deutscher},
keywords = {nonlinear observers, approximate error linearization, Galerkin method, multivariable Legendre polynomials, -approximation},
abstract = {This paper presents an approach to the design of nonlinear observers by approximate error linearization. It extends the results in Deutscher and Bäuml (2006) to systems with input applying Lyapunov's Auxiliary Theorem. By using a Galerkin approach on the basis of multivariable Legendre polynomials the L2-norm of the remaining nonlinearity in the resulting error dynamics can be made small on a specified multivariable interval in the state space. Linear matrix equations are derived for determining the corresponding change of coordinates and output injections. Consequently, the proposed design procedure can easily be implemented in a numerical software package. A dc motor with a boost converter as actuator demonstrates the properties of the proposed numerical observer design.}
}
@article{VAKILOROAYA20141,
title = {Thermo-economic optimization of condenser coil configuration for HVAC performance enhancement},
journal = {Energy and Buildings},
volume = {84},
pages = {1-12},
year = {2014},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2014.07.079},
url = {https://www.sciencedirect.com/science/article/pii/S0378778814006288},
author = {Vahid Vakiloroaya and Bijan Samali and Stephen Cuthbert and Kambiz Pishghadam and David Eager},
keywords = {Condenser coil, Cost and energy saving, Design optimization, HVAC, Thermo-economic analysis},
abstract = {The purpose of this study is to develop an optimization methodology for the detailed energy and cost effective design of a finned-tube condenser coil in order to enhance the system performance. Using this method, the frontal area of the condenser coil is maintained as constant, while other geometrical parameters of the thermal and economic performance of the system are varied and investigated. An existing air-cooled direct expansion (DX) rooftop package unit of a real-world commercial building is used for experimental data collection. First, the theoretical–empirical model for the system components is developed. Based on mathematical models and using collected data, a numerical algorithm is developed and embedded in a transient simulation tool. The integrated simulation tool is then validated by using the wide range of operating data obtained experimentally from the cooling plant during summer time. Furthermore, a mixed heuristic–deterministic optimization algorithm was implemented to determine the synthesis and design variables that influence the cost and energy efficiency of each configuration. Different new designs for condenser coil were then constructed to evaluate the potential of design improvements. Afterwards, the computer model was used to predict how changes in condenser coil geometry would affect the cost and energy consumption of the system.}
}
@article{BAYAR2019211,
title = {CRM2DIM: A SAS macro for implementing the dual-agent Bayesian continual reassessment method},
journal = {Computer Methods and Programs in Biomedicine},
volume = {176},
pages = {211-223},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.04.025},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718317619},
author = {Mohamed Amine Bayar and Anastasia Ivanova and Gwénaël {Le Teuff}},
keywords = {Phase I clinical trial, Dose-finding, Drug combinations, Continual reassessment method, CRM, SAS MCMC procedure},
abstract = {Background and objective: The continual reassessment method (CRM) is a model-based dose-finding design for single-agent phase I oncology trials. With the advance of targeted therapies in oncology, more and more phase I trials investigate drug combinations rather than a single agent in order to find one or more maximum tolerated dose combinations. Several designs have been proposed for such dose-finding trials but only a few software packages are available to implement them. One of the designs is the two-dimensional Bayesian CRM proposed by Wang and Ivanova. Our goal was to provide an easy-to-use program to implement this design. Methods: We developed a new SAS macro, CRM2DIM, for implementing this design. This macro can be used to run a phase I dose-finding trial for two-drug combination and to perform simulations. Results: We describe the program with its different features, including the possibility of running an initial design (start-up rule), the possibility of incorporating historical data, and the choice of using either a power or a logistic regression model with or without interaction term. We illustrate our program by presenting simulation results and by a hypothetical trial example. Conclusions: The CRM2DIM macro provides a SAS implementation of the two-dimensional Bayesian CRM for dual-agent phase I oncology trials. It is an easy-to-use program that includes many useful features and provides statisticians involved in the early phases of development a new tool for designing dual-agent phase I oncology trials.}
}
@article{CHOI20091322,
title = {Tunneling Analyst: A 3D GIS extension for rock mass classification and fault zone analysis in tunneling},
journal = {Computers & Geosciences},
volume = {35},
number = {6},
pages = {1322-1333},
year = {2009},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2008.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098300408001969},
author = {Yosoon Choi and Seo-Youn Yoon and Hyeong-Dong Park},
keywords = {3D GIS, Tunneling, Rock mass classification, Multiple indicator kriging, Spatial query function, Rock mechanics},
abstract = {In this study, an extension called Tunneling Analyst (TA) has been developed in ArcScene 3D GIS software, part of the ArcGIS software package. It dramatically extends the functionalities of ArcScene because it allows: (1) estimation of the 3D distribution of rock mass rating (RMR) values using borehole and geophysical exploration data, (2) the modeling of 3D discontinuity planes such as faults from field-based structural measurements, and (3) analysis of 3D intersections and 3D buffer zones between proposed tunnel alignments and some discontinuities. Because TA can handle and visualize both 2D and 3D geological data in a single GIS environment, the tedious tasks required for data conversion between various software packages can be reduced significantly. The application to the Daecheong tunneling project in Korea shows that TA could present a rational solution to evaluating the rock mass classes along a proposed tunnel alignment and can also provide specific 3D spatial query tools to support the tunnel design work. This paper describes the concept and details of the development and implementation of TA.}
}
@article{ZIYATDINOV2013596,
title = {A software tool for large-scale synthetic experiments based on polymeric sensor arrays},
journal = {Sensors and Actuators B: Chemical},
volume = {177},
pages = {596-604},
year = {2013},
issn = {0925-4005},
doi = {https://doi.org/10.1016/j.snb.2012.09.093},
url = {https://www.sciencedirect.com/science/article/pii/S0925400512010076},
author = {A. Ziyatdinov and E. {Fernández Diaz} and A. Chaudry and S. Marco and K. Persaud and A. Perera},
keywords = {Gas sensor array, Conducting polymers, Electronic nose, Sensor simulation, Synthetic dataset, Benchmark, Educational tool},
abstract = {This manuscript introduces a software tool that allows for the design of synthetic experiments in machine olfaction. The proposed software package includes both, a virtual sensor array that reproduces the diversity and response of a polymer array and tools for data generation. The synthetic array of sensors allows for the generation of chemosensor data with a variety of characteristics: unlimited number of sensors, support of multicomponent gas mixtures and full parametric control of the noise in the system. The artificial sensor array is inspired from a reference database of seventeen polymeric sensors with concentration profiles for three analytes. The main features in the sensor data, like sensitivity, diversity, drift and sensor noise, are captured by a set of models under simplified assumptions. The generator of sensor signals can be used in applications related to test and benchmarking of signal processing methods, neuromorphic simulations in machine olfaction and educational tools. The software is implemented in R language and can be freely accessed at: http://chemosensors.r-forge.r-project.org/.}
}
@article{ZHANG20181,
title = {APT-MCMC, a C++/Python implementation of Markov Chain Monte Carlo for parameter identification},
journal = {Computers & Chemical Engineering},
volume = {110},
pages = {1-12},
year = {2018},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S009813541730412X},
author = {Li Ang Zhang and Alisa Urbano and Gilles Clermont and David Swigon and Ipsita Banerjee and Robert S. Parker},
keywords = {MCMC, Simulation, Bayesian inference},
abstract = {The inverse problem associated with fitting parameters of an ordinary differential equation (ODE) system to data is nonlinear and multimodal, which is of great challenge to gradient-based optimizers. Markov Chain Monte Carlo (MCMC) techniques provide an alternative approach to solving these problems and can escape local minima by design. APT-MCMC was created to allow users to setup ODE simulations in Python and run as compiled C++ code. It combines affine-invariant ensemble of samplers and parallel tempering MCMC techniques to improve the simulation efficiency. Simulations use Bayesian inference to provide probability distributions of parameters, which enable analysis of multiple minima and parameter correlation. Benchmark tests result in a 20×–60× speedup but 14% increase in memory usage against emcee, a similar MCMC package in Python. Several MCMC hyperparameters were analyzed: number of temperatures, ensemble size, step size, and swap attempt frequency. Heuristic tuning guidelines are provided for setting these hyperparameters.}
}
@article{LEE2022108827,
title = {Application of domain-adaptive convolutional variational autoencoder for stress-state prediction},
journal = {Knowledge-Based Systems},
volume = {248},
pages = {108827},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108827},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122003938},
author = {Sang Min Lee and Sang-Youn Park and Byoung-Ho Choi},
keywords = {Unsupervised domain adaptation, Stress analysis, Four-point bending, Variational autoencoder, Deep learning, Convolutional neural network},
abstract = {Applying data-driven methods such as deep learning in material mechanics is challenging because producing a sufficiently large, labeled dataset is costly resource-wise. This paper outlines a new approach to overcoming this difficulty by transferring knowledge from a source domain of finite-element-analysis data to a target domain of real-world test-specimen images so that a model capable of accurate and robust predictions in both domains may be constructed. To achieve this transfer of knowledge, discrepancy-based unsupervised domain adaptation is adopted into a convolutional variational autoencoder structure. To evaluate the proposed approach, a four-point bending experiment was conducted on 6061 aluminum alloy and 316 stainless steel to produce 550 unlabeled target-domain data images. The same bending situation was analyzed using the finite-element method implemented in the commercial software package ABAQUS to produce 6000 labeled, source-domain data images. The proposed domain-adaptive convolutional variational autoencoder was trained using the maximum mean discrepancy method on the target- and the source-domain data. The predictions using the domain-adapted convolutional variational autoencoder were relatively more accurate than those using the model trained only on the source domain. It is expected that the proposed approach can address the scarcity of labeled data in various applications of material mechanics and provide a base technology for the development of various data-driven approaches.}
}
@incollection{NEWCOMB2010133,
title = {Chapter 6 - PowerBuilder/4GL Generator Modernization Pilot**© 2010. The Software Revolution, Inc. All rights reserved.},
editor = {William M. Ulrich and Philip H. Newcomb},
booktitle = {Information Systems Transformation},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {133-170},
year = {2010},
series = {The MK/OMG Press},
isbn = {978-0-12-374913-0},
doi = {https://doi.org/10.1016/B978-0-12-374913-0.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123749130000068},
author = {Philip H. Newcomb and Dru Henke and Jim LoVerde and William Ulrich and Luong Nguyen and Robert Couch},
abstract = {Publisher Summary
This chapter presents a case study, which describes a pilot modernization project at a major US company and includes a description of the modernization methods, technologies, and processes employed in the project. The software environment was written in PowerBuilder and COBOL, which was generated by a fourth generation language code generator (4GL). The entire environment was deployed on an IBM mainframe. The pilot project encompassed the investigation, planning, and a proof of technology test to move (or retire) applications from the technology stack using an iterative, phased approach that encompasses multiple methods of conversion. Although this was considered a pilot project, the intent from the beginning and throughout the work effort was to deploy a production-ready application at the end of the pilot phase and deactivate the corresponding mainframe software that was being migrated. The effort began in 2007 and various implementation tasks are continuing as of this writing. The overall effort involved an analysis of options and approaches, development of certain target architecture technologies to support the migration, onsite system preparation and packaging, offsite software conversion, onsite software validation and user acceptance testing, and final deployment. The pilot conversion of the 4GL code and PowerBuilder code was completed successfully for a small, self-contained application module and a subsequent follow-on expanded application undertaken in 2007 and 2008.}
}
@article{ALLOMBERT2014888,
title = {An Out-of-core GPU Approach for Accelerating Geostatistical Interpolation},
journal = {Procedia Computer Science},
volume = {29},
pages = {888-896},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.080},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914002579},
author = {Victor Allombert and David Michea and Fabrice Dupros and Christian Bellier and Bernard Bourgine and Hideo Aochi and Sylvain Jubertie},
keywords = {Geostatistics, Graphic processing units (GPU), Linear algebra, Out-of-core, Geological Data Management (GDM)},
abstract = {Geostatistical methods provide a powerful tool to understand the complexity of data arising from Earth sciences. Since the mid 70's, this numerical approach is widely used to understand the spatial variation of natural phenomena in various domains like Oil and Gas, Mining or Environmental Industries. Considering the huge amount of data available, standard implementations of these numerical methods are not efficient enough to tackle current challenges in geosciences. Moreover, most of the software packages available for geostatisticians are designed for a usage on a desktop computer due to the trial and error procedure used during the interpolation. The Geological Data Management (GDM) software package developed by the French geological survey (BRGM) is widely used to build reliable three-dimensional geological models that require a large amount of memory and computing resources. Considering the most time-consuming phase of kriging methodology, we introduce an efficient out-of-core algorithm that fully benefits from graphics cards acceleration on desktop computer. This way we are able to accelerate kriging on GPU with data 4 times bigger than a classical in-core GPU algorithm, with a limited loss of performances.}
}
@article{GONZALEZ201680,
title = {PARAVT: Parallel Voronoi tessellation code},
journal = {Astronomy and Computing},
volume = {17},
pages = {80-85},
year = {2016},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2016.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S2213133716300609},
author = {R.E. González},
keywords = {Methods: -body simulations, Large-scale structure of universe, Software and its engineering: Massively parallel systems},
abstract = {In this study, we present a new open source code for massive parallel computation of Voronoi tessellations (VT hereafter) in large data sets. The code is focused for astrophysical purposes where VT densities and neighbors are widely used. There are several serial Voronoi tessellation codes, however no open source and parallel implementations are available to handle the large number of particles/galaxies in current N-body simulations and sky surveys. Parallelization is implemented under MPI and VT using Qhull library. Domain decomposition takes into account consistent boundary computation between tasks, and includes periodic conditions. In addition, the code computes neighbors list, Voronoi density, Voronoi cell volume, density gradient for each particle, and densities on a regular grid. Code implementation and user guide are publicly available at https://github.com/regonzar/paravt.}
}
@article{HANREICH20002069,
title = {High resolution thermal simulation of electronic components},
journal = {Microelectronics Reliability},
volume = {40},
number = {12},
pages = {2069-2076},
year = {2000},
issn = {0026-2714},
doi = {https://doi.org/10.1016/S0026-2714(00)00019-6},
url = {https://www.sciencedirect.com/science/article/pii/S0026271400000196},
author = {G Hanreich and J Nicolics and L Musiejovsky},
abstract = {An efficient thermal management in electronic components is essential to minimize the influence of thermomechanically induced stress and thermal load. Frequently, thermal simulation tools are applied to reduce the number of experiments needed for thermal characterization of the semiconductor components. However, for using commercially available software packages, much effort is necessary for maintenance and for generating the thermal models. Moreover, the limitation of the node number does not allow a discretization sufficiently fine for more complex structures as in high lead count packages. In this paper, a new thermal simulation tool is presented, which allows one to create models in a very efficient way. The developed and implemented solver based on the alternating direction implicit method is efficiently processing the required high node number. Moreover, the developed thermal simulation tool is applied for the thermal characterization of a 176 lead quad flat pack (QFP-package) using a discretization with 320,000 nodes. Steady-state and transient thermal qualities of the package are investigated under boundary conditions as specified by the Joint Electronic Device Engineering Council (JEDEC). Further, results obtained by thermal simulation are compared with those established from experimental procedures. Conclusions of how this new tool can be used for thermal design optimization are derived.}
}
@incollection{STEWART2013563,
title = {Chapter 17 - Multicore Software Development for Embedded Systems: This Chapter draws on Material from the Multicore Programming Practices Guide (MPP) from the Multicore Association},
editor = {Robert Oshana and Mark Kraeling},
booktitle = {Software Engineering for Embedded Systems},
publisher = {Newnes},
address = {Oxford},
pages = {563-612},
year = {2013},
isbn = {978-0-12-415917-4},
doi = {https://doi.org/10.1016/B978-0-12-415917-4.00017-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159174000177},
author = {Dave Stewart and Max Domeika and Scott A. Hissam and Skip Hovsmith and James Ivers and Ross Dickson and Ian Lintault and Stephen Olsen and Hyunki Baik and François Bodin and Robert Oshana},
keywords = {Multicore Programming Practices (MPP), Multicore Association (MCA), serial performance, parallel decomposition, synchronization, threads, PPthreads, mutexes, granularity, parallelism, MCAPI, MRAPI},
abstract = {Multicore software development is growing in importance and applicability in many areas of embedded systems from automotive to networking, to wireless base stations. This chapter is a summary of key sections of the recently released Multicore Programming Practices (MPP) from the Multicore Association (MCA). The MPP standardized “best practices” guide is written specifically for engineers and engineering managers of companies considering or implementing a development project involving multicore processors and favoring use of existing multicore technology. There is an important need to better understand how today’s C/C++ code may be written to be “multicore ready”, and this was accomplished under the influence of the MPP working group. The guide will enable you to (a) produce higher-performing software; (b) reduce the bug rate due to multicore software issues; (c) develop portable multicore code which can be targeted at multiple platforms; (d) reduce the multicore programming learning curve and speed up development time; and (e) tie into the current structure and roadmap of the Multicore Association’s API infrastructure.}
}
@article{LEE2014608,
title = {Design and implementation of a standard framework for KSTAR control system},
journal = {Fusion Engineering and Design},
volume = {89},
number = {5},
pages = {608-613},
year = {2014},
note = {Proceedings of the 9th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2014.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0920379614000994},
author = {Woongryol Lee and Mikyung Park and Taegu Lee and Sangil Lee and Sangwon Yun and Jinseop Park and Kaprai Park},
keywords = {KSTAR, Standardization, EPICS, Embedded IOC, Real time controller},
abstract = {Standardization of control system is an important issue in KSTAR which is organized with various heterogeneous systems. Diverse control systems in KSTAR have been adopting new application software since 2010. Development of this software was launched for easy implementation of a data acquisition system but it is extended to as a Standard Framework (SFW) of control system in KSTAR. It is composed with a single library, database, template, and descriptor files. The SFW based controller has common factors. It has non-blocking control command method with a thread. The internal sequence handler makes it can be synchronized with KSTAR experiment. It also has a ring buffer pool mechanism for streaming input data handling. Recently, there are two important functional improvements in the framework. Processor embedded FPGA was proposed as a standard hardware platform for specific application. These are also manipulated by the SFW based embedded application. This approach gives single board system an ability of low level distributed control under the EPICS environments. We also developed a real time monitoring system as a real time network inspection tool in 2012 campaign using the SFW.}
}
@article{FELDMANN2019100331,
title = {LEO-Py: Estimating likelihoods for correlated, censored, and uncertain data with given marginal distributions},
journal = {Astronomy and Computing},
volume = {29},
pages = {100331},
year = {2019},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2019.100331},
url = {https://www.sciencedirect.com/science/article/pii/S2213133719300952},
author = {R. Feldmann},
keywords = {Statistical software, Multivariate statistics, Statistical computing, Galaxies: fundamental parameters, Methods: statistical},
abstract = {Data with uncertain, missing, censored, and correlated values are commonplace in many research fields including astronomy. Unfortunately, such data are often treated in an ad hoc way in the astronomical literature potentially resulting in inconsistent parameter estimates. Furthermore, in a realistic setting, the variables of interest or their errors may have non-normal distributions which complicates the modeling. I present a novel approach to compute the likelihood function for such data sets. This approach employs Gaussian copulas to decouple the correlation structure of variables and their marginal distributions resulting in a flexible method to compute likelihood functions of data in the presence of measurement uncertainty, censoring, and missing data. I demonstrate its use by determining the slope and intrinsic scatter of the star forming sequence of nearby galaxies from observational data. The outlined algorithm is implemented as the flexible, easy-to-use, open-source Python package LEO-Py.}
}
@article{HINOJOSA2017160,
title = {One-layer gradient-based MPC+RTO of a propylene/propane splitter},
journal = {Computers & Chemical Engineering},
volume = {106},
pages = {160-170},
year = {2017},
note = {ESCAPE-26},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0098135417302521},
author = {Aldo Ignacio Hinojosa and Antonio Ferramosca and Alejandro H. González and Darci Odloak},
keywords = {Economic model predictive control, Real time optimization, Dynamic simulation, Propylene production unit},
abstract = {Here, the implementation of the gradient-based Economic MPC (Model Predictive Control) in an industrial distillation system is studied. The approach is an alternative to overcome the conflict between the MPC and RTO (Real Time Optimization) layers in the conventional control structure. The study is based on the rigorous dynamic simulation software (SimSciDynsim®) that reproduces the real system very closely and is able to communicate with Matlab. The gradient of the economic function, is obtained through the sensitivity tool of the real-time optimization package (SimSciROMeo®). In order to study the pros and cons of the new strategy, a propylene distillation system is simulated with both, the proposed approach (one-layer MPC+RTO) and the conventional two-layer hierarchical structure of control and optimization. The results show that, for this particular system, from the performance, stability and disturbance rejection viewpoint, the proposed gradient-based extended control method is equivalent or better than the conventional approach.}
}
@article{ORHAN201270,
title = {Efficiency comparison of various design schemes for copper–chlorine (Cu–Cl) hydrogen production processes using Aspen Plus software},
journal = {Energy Conversion and Management},
volume = {63},
pages = {70-86},
year = {2012},
note = {10th International Conference on Sustainable Energy Technologies (SET 2011)},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2012.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0196890412001069},
author = {Mehmet F. Orhan and İbrahim Dinçer and Marc A. Rosen},
keywords = {Hydrogen production, Thermochemical water decomposition, Nuclear, Thermoeconomic analysis, Copper–chlorine cycle, Simulation, Aspen Plus, Design},
abstract = {In this study, simulation models are developed to analyze, design and optimize the Cu–Cl cycles using the Aspen PlusTM chemical process simulation package. Energy, exergy and yield effectiveness of the process, based on three, four and five-step cycles, is examined. The thermal efficiency of the five-step thermochemical process is calculated as 44%, of the four-step process is 43% and of the three-step process is 41%, based on the lower heating value of hydrogen. Sensitivity analyses are performed to study the effects of various operating parameters on the efficiency and yield. A parametric study is conducted and possible efficiency improvements are discussed. Furthermore, new system configurations for the Cu–Cl cycle are developed for performance improvement in this study. Various design schemes, based on three, four and five-step Cu–Cl cycles, are analyzed and compared. Also, recommendations for implementation of these new design schemes are presented in detail.}
}
@article{SONKOLY2020102785,
title = {Scalable edge cloud platforms for IoT services},
journal = {Journal of Network and Computer Applications},
volume = {170},
pages = {102785},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102785},
url = {https://www.sciencedirect.com/science/article/pii/S1084804520302599},
author = {Balázs Sonkoly and Dávid Haja and Balázs Németh and Márk Szalay and János Czentye and Róbert Szabó and Rehmat Ullah and Byung-Seo Kim and László Toka},
keywords = {Edge computing, Resource orchestration, SDN, NFV, IoT},
abstract = {Nowadays, online applications are moving to the cloud, and for delay-sensitive ones, the cloud is being extended with edge/fog domains. Emerging cloud platforms that tightly integrate compute and network resources enable novel services, such as versatile IoT (Internet of Things), augmented reality or Tactile Internet applications. Virtual infrastructure managers (VIMs), network controllers and upper-level orchestrators are in charge of managing these distributed resources. A key and challenging task of these orchestrators is to find the proper placement for software components of the services. As the basic variant of the related theoretical problem (Virtual Network Embedding) is known to be NP-hard, heuristic solutions and approximations can be addressed. In this paper, we propose two architecture options together with proof-of-concept prototypes and corresponding embedding algorithms, which enable the provisioning of delay-sensitive IoT applications. On the one hand, we extend the VIM itself with network-awareness, typically not available in today's VIMs. On the other hand, we propose a multi-layer orchestration system where an orchestrator is added on top of VIMs and network controllers to integrate different resource domains. We argue that the large-scale performance and feasibility of the proposals can only be evaluated with complete prototypes, including all relevant components. Therefore, we implemented fully-fledged solutions and conducted large-scale experiments to reveal the scalability characteristics of both approaches. We found that our VIM extension can be a valid option for single-provider setups encompassing even 100 edge domains (Points of Presence equipped with multiple servers) and serving a few hundreds of customers. Whereas, our multi-layer orchestration system showed better scaling characteristics in a wider range of scenarios at the cost of a more complex control plane including additional entities and novel APIs (Application Programming Interfaces).}
}
@article{PERKINS20072260,
title = {Universal fatigue life prediction equation for ceramic ball grid array (CBGA) packages},
journal = {Microelectronics Reliability},
volume = {47},
number = {12},
pages = {2260-2274},
year = {2007},
note = {Electronic system prognostics and health management},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2006.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0026271407000066},
author = {Andy Perkins and Suresh K. Sitaraman},
abstract = {A traditional approach to predicting solder joint fatigue life involves finite-element simulations in combination with experimental data to develop a Coffin–Manson type predictive equation. The finite-element simulations often require good understanding of finite-element modeling, physics-based failure models, and time-, temperature-, and direction-dependent material constitutive behavior. Also, such simulations are computationally expensive and time-consuming. Microelectronic package designers often do not have the time and the expertise to perform such simulations. The traditional solder joint fatigue predictive equations fall short of ideal because: (1) they are not applicable to others due to numerical modeling issues, (2) they require a mature understanding of mechanics, numerical modeling, and reliability theory, and (3) they are difficult to implement into the design process. This includes both design of an individual electronic component and selecting which type of existing component to include in an application. Therefore, this work develops universal predictive equations that are: (1) simple, quick, and accurate, (2) require only a basic understanding of reliability and mechanics, (3) require no special software; easy to implement in a spreadsheet or current reliability tools, (4) information rich in regards to design parameters, and (5) maximize available information from experimental tests and numerical models. Using experimental data and finite-element simulations as a basis, this work has developed a predictive equation for solder joint fatigue life in lead-containing ceramic ball grid array (CBGA) package. The developed equation has been validated with other experimental data with good success. Efforts are underway to develop similar equations for other packages and Pb-free CBGAs.}
}
@article{COLE201469,
title = {Reduced-order residential home modeling for model predictive control},
journal = {Energy and Buildings},
volume = {74},
pages = {69-77},
year = {2014},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2014.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0378778814000711},
author = {Wesley J. Cole and Kody M. Powell and Elaine T. Hale and Thomas F. Edgar},
keywords = {Building energy simulation, Model reduction, EnergyPlus, OpenStudio, Model predictive control, Precooling, Thermal energy storage},
abstract = {Building simulation software packages such as EnergyPlus are useful energy modeling tools. These software packages, however, are often not amenable to model-based control due to model complexity or difficulties connecting control algorithms with the software. We present a method for automatically generating input/output data from an EnergyPlus residential home model using the OpenStudio software suite. These input/output data are used to create a simple reduced-order model that can be evaluated in fractions of a second. The reduced-order model is implemented in a model predictive controller to minimize the home's electricity costs during summer months in Austin, Texas, USA. The controller optimally precools the home in the morning and turns down or off the air conditioning system in the afternoon. For this example, electricity prices were taken from actual market prices in the Austin area. The optimal precooling strategy given by the model predictive controller reduces peak energy consumption from the air conditioning unit by an average of 70% and reduces operating costs by 60%. Precooling, however, consumes more total energy versus not precooling. Reducing peak energy consumption by 1kWh results, on average, in an increase of 0.63kWh in overall energy consumption.}
}
@article{NOULARD20011299,
title = {A key for reusable parallel linear algebra software},
journal = {Parallel Computing},
volume = {27},
number = {10},
pages = {1299-1319},
year = {2001},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(01)00090-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167819101000904},
author = {Eric Noulard and Nahid Emad},
keywords = {OO design, Genericity, Parallel and sequential code reuse, Krylov subspace methods},
abstract = {We propose an object-oriented design which enables very good code reuse for both sequential and parallel linear algebra applications. A linear algebra class library called LAKe is implemented using our design method. We introduce a new reuse mechanism called matrix shape which enables us to derive the implementation of both the sequential and the parallel version of the iterative methods of Linear Algebra Kernels (LAKe). We show that polymorphism is insufficient to achieve our goal and that both genericity and polymorphism are needed. We propose a new design pattern as a part of the solution. Some numerical experiments validate our approach and show that efficiency is not sacrificed.}
}
@article{DAUBER2012822,
title = {Modelling liquefied-natural-gas processes using highly accurate property models},
journal = {Applied Energy},
volume = {97},
pages = {822-827},
year = {2012},
note = {Energy Solutions for a Sustainable World - Proceedings of the Third International Conference on Applied Energy, May 16-18, 2011 - Perugia, Italy},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2011.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0306261911007422},
author = {Florian Dauber and Roland Span},
keywords = {GERG-2008, Vapourisation, CAPE-OPEN, Liquefied natural gas, Liquefaction, Transport},
abstract = {Accurate simulations are important for efficient design and operation of a process. Therefore, a precise representation of thermophysical properties using an adequate property model is necessary. The GERG-2008 by Kunz and Wagner [1] is the new reference equation of state for natural gases consisting of up to 21 specific compounds. It describes the gas and liquid phase as well as the super-critical region and the vapour–liquid equilibrium. In order to model LNG processes with the highest accuracy available, software available for the new equation is implemented into various common simulation tools. To ensure stable and consistent simulations, the GERG-2008 Property Package has been developed, which meets the CAPE-OPEN standard. The influence of property models on the simulation of the most important processes of the LNG value chain is investigated. Results show the expected advantages in accuracy for simulations using the new property model.}
}
@article{OGNESS200185,
title = {A System To Facilitate Telematic Implementation},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {9},
pages = {85-90},
year = {2001},
note = {IFAC Conference on Telematics Applications in Automation and Robotics, Weingarten, Germany, 24-26 July 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41686-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017416867},
author = {John Ogness and Klaus Schilling and Hubert Roth},
keywords = {Software engineering, Telematics, Systems design, Laboratory education, Communication systems},
abstract = {In the interest of promoting online robotics education, this paper presents a system to allow research institutions to easily make robots available on the internet. The system is based on Java and provides a simple, yet complete package to handle network communication, user management, serial device, and identification activities. By providing such an extensive functional base, telematic projects can be quickly implemented and made available while leaving developers to focus on more important issues.}
}
@article{KOBAYASHI2003769,
title = {Business process integration as a solution to the implementation of supply chain management systems},
journal = {Information & Management},
volume = {40},
number = {8},
pages = {769-780},
year = {2003},
issn = {0378-7206},
doi = {https://doi.org/10.1016/S0378-7206(02)00102-7},
url = {https://www.sciencedirect.com/science/article/pii/S0378720602001027},
author = {Takashi Kobayashi and Masato Tamaki and Norihisa Komoda},
keywords = {Supply chain management, Business process, Workflow, Enterprise application integration, Information sharing, Design template, Integration adapter},
abstract = {In the domain of supply chain management (SCM), various software packages have been developed for planning business strategies. To solve the problem of system productivity in applying planning packages, we propose a solution concept, business process integration (BPI), which fuses workflow and enterprise application integration (EAI) technology. Two characteristic policies are included in BPI. The first is to design the minimum set of business processes for real-time information sharing with planning packages without changing other processes. The second is to integrate several systems with EAI technology and to manage their execution with a workflow tool. Based on these policies, we propose various design templates and integration adapters. Our evaluation shows that using BPI, a target system can be developed with less manpower, in less time, and with higher quality than previous methods.}
}
@article{ELBOUHARGANI2022100576,
title = {MAPPRAISER: A massively parallel map-making framework for multi-kilo pixel CMB experiments},
journal = {Astronomy and Computing},
volume = {39},
pages = {100576},
year = {2022},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2022.100576},
url = {https://www.sciencedirect.com/science/article/pii/S2213133722000208},
author = {H. {El Bouhargani} and A. Jamal and D. Beck and J. Errard and L. Grigori and R. Stompor},
keywords = {Numerical methods, Linear systems solvers, High performance computing, Cosmic microwave background, Data analysis, Map-making},
abstract = {Forthcoming cosmic microwave background (CMB) polarized anisotropy experiments have the potential to revolutionize our understanding of the Universe and fundamental physics. The sought-after, tale-telling signatures will be however distributed over voluminous data sets which these experiments will collect. These data sets will need to be efficiently processed and unwanted contributions due to astrophysical, environmental, and instrumental effects characterized and efficiently mitigated in order to uncover the signatures. This poses a significant challenge to data analysis methods, techniques, and software tools which will not only have to be able to cope with huge volumes of data but to do so with unprecedented precision driven by the demanding science goals posed for the new experiments. A keystone of efficient CMB data analysis is solvers of very large linear systems of equations. Such systems appear in very diverse contexts throughout CMB data analysis pipelines, however they typically display similar algebraic structures and can therefore be solved using similar numerical techniques. Linear systems arising in the so-called map-making problem are one of the most prominent and common ones. In this work we present a massively parallel, flexible and extensible framework, comprised of a numerical library, MIDAPACK, and a high level code, MAPPRAISER, which provide tools for solving efficiently such systems. The framework implements iterative solvers based on conjugate gradient techniques: enlarged and preconditioned using different preconditioners. We demonstrate the framework on simulated examples reflecting basic characteristics of the forthcoming data sets issued by ground-based and satellite-borne instruments, executing it on as many as 16,384 compute cores. The software is developed as an open source project freely available to the community at: https://github.com/B3Dcmb/midapack.}
}
@article{NEE1989649,
title = {CAE/CAD/CAM curricula implementation - experience at the National University of Singapore},
journal = {Computer-Aided Design},
volume = {21},
number = {10},
pages = {649-653},
year = {1989},
issn = {0010-4485},
doi = {https://doi.org/10.1016/0010-4485(89)90163-2},
url = {https://www.sciencedirect.com/science/article/pii/0010448589901632},
author = {A.Y.C. Nee and C.C. Hang},
keywords = {computer-aided design, courses},
abstract = {A step-by-step approach to implement CAE/CAD/CAM curricula for undergraduate engineering courses at the National University of Singapore is presented. As the student workload is already quite heavy, each course is examined in detail and the CAE/CAD/CAM contents are carefully blended with the existing course structure. Some students are also assigned CAE/CAD/CAM specific projects to allow them to develop expertise in developing new software packages and building interfaces to existing packages. Experience indicates that CAE/CAD/CAM programmes at the undergraduate level should be implemented in various phases. It is also necessary to consider the state of the local industries in the adoption of the CAE/CAD/CAM technology so that the effort by the university is well synchronized with industry.}
}
@article{KRUPPA20181,
title = {A genetic algorithm for simulating correlated binary data from biomedical research},
journal = {Computers in Biology and Medicine},
volume = {92},
pages = {1-8},
year = {2018},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2017.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0010482517303499},
author = {Jochen Kruppa and Bernd Lepenies and Klaus Jung},
keywords = {Correlated binary data, Genetic algorithm, High-dimensional data, Random number generation, Computer simulation},
abstract = {Correlated binary data arise in a large variety of biomedical research. In order to evaluate methods for their analysis, computer simulations of such data are often required. Existing methods can often not cover the full range of possible correlations between the variables or are not available as implemented software. We propose a genetic algorithm that approaches the desired correlation structure under a given marginal distribution. The procedure generates a large representative matrix from which the probabilities of individual observations can be derived or from which samples can be drawn directly. Our genetic algorithm is evaluated under different specified marginal frequencies and correlation structures, and is compared against two existing approaches. The evaluation checks the speed and precision of the approach as well as its suitability for generating also high-dimensional data. In an example of high-throughput glycan array data, we demonstrate the usability of our approach to simulate the power of global test procedures. An implementation of our own and two other methods were added to the R-package ‘RepeatedHighDim’. The presented algorithm is not restricted to certain correlation structures. In contrast to existing methods it is also evaluated for high-dimensional data.}
}
@incollection{KEHTARNAVAZ2005157,
title = {Chapter 8 - DSP Implementation Platform: TMS320C6x Architecture and Software Tools},
editor = {Nasser Kehtarnavaz and Namjin Kim},
booktitle = {Digital Signal Processing System-Level Design Using LabVIEW},
publisher = {Newnes},
address = {Burlington},
pages = {157-166},
year = {2005},
isbn = {978-0-7506-7914-5},
doi = {https://doi.org/10.1016/B978-075067914-5/50016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780750679145500167},
author = {Nasser Kehtarnavaz and Namjin Kim},
abstract = {Publisher Summary
The choice of a digital signal processing (DSP) processor to be used in a signal processing system is generally application dependent. There are many factors that influence this choice, including cost, performance, power consumption, ease-of-use, time-to-market, and integration capabilities. This chapter focuses on TMS320C6x DSP processor, its components, architecture, and software tools. The TMS320C6x family of processors is manufactured by Texas Instruments and was primarily built to deliver speed. They are designed for million instructions per second (MIPS) intensive applications such as third generation (3G) wireless and digital imaging. Several processor versions are available for TMS320C6x family, which differ in instruction cycle time, speed, power consumption, memory, peripherals, packaging, and cost. The generic C6x architecture includes the C6x central processing unit (CPU) consisting of eight functional units divided into two sides A and B. Each side has a .M unit used for multiplication operation, a .L unit used for logical and arithmetic operations, a .S unit used for branch, bit manipulation and arithmetic operations, and a .D unit used for loading, storing, and arithmetic operations. The peripherals on a typical C6x processor include external memory interface (EMIF), direct memory access (DMA), boot loader, multi-channel buffered serial port (McBSP), host port interface (HPI), timer, and power down unit. The C64x is a recent DSP core, a part of the C6x family, with higher MIPS power and operating at higher clock rates.}
}
@article{LEI2003351,
title = {3DVIEWNIX-AVS: a software package for the separate visualization of arteries and veins in CE-MRA images},
journal = {Computerized Medical Imaging and Graphics},
volume = {27},
number = {5},
pages = {351-362},
year = {2003},
issn = {0895-6111},
doi = {https://doi.org/10.1016/S0895-6111(03)00029-6},
url = {https://www.sciencedirect.com/science/article/pii/S0895611103000296},
author = {Tianhu Lei and Jayaram K. Udupa and Dewey Odhner and László G. Nyúl and Punam K. Saha},
keywords = {Image segmentation, Artery–vein separation, Fuzzy connectedness, Contrast enhanced magnetic resonance angiography, Visualization systems},
abstract = {Our earlier study developed a computerized method, based on fuzzy connected object delineation principles and algorithms, for artery and vein separation in contrast enhanced Magnetic Resonance Angiography (CE-MRA) images. This paper reports its current development—a software package—for routine clinical use. The software package, termed 3DVIEWNIX-AVS, consists of the following major operational parts: (1) converting data from DICOM3 to 3DVIEWNIX format, (2) previewing slices and creating VOI and MIP Shell, (3) segmenting vessel, (4) separating artery and vein, (5) shell rendering vascular structures and creating animations. This package has been applied to EPIX Medical Inc's CE-MRA data (AngioMark MS-325). One hundred and thirty-five original CE-MRA data sets (of 52 patients) from 6 hospitals have been processed. In all case studies, unified parameter settings produce correct artery–vein separation. The current package is running on a Pentium PC under Linux and the total computation time per study is about 3 min. The strengths of this software package are (1) minimal user interaction, (2) minimal anatomic knowledge requirements on human vascular system, (3) clinically required speed, (4) free entry to any operational stages, (5) reproducible, reliable, high quality of results, and (6) cost effective computer implementation. To date, it seems to be the only software package (using an image processing approach) available for artery and vein separation of the human vascular system for routine use in a clinical setting.}
}
@article{PORRITT2018260,
title = {Updates to FuncLab, a Matlab based GUI for handling receiver functions},
journal = {Computers & Geosciences},
volume = {111},
pages = {260-271},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417303953},
author = {Robert W. Porritt and Meghan S. Miller},
keywords = {Receiver functions, Matlab, Seismic imaging, Converted waves, Visualization tool},
abstract = {Receiver functions are a versatile tool commonly used in seismic imaging. Depending on how they are processed, they can be used to image discontinuity structure within the crust or mantle or they can be inverted for seismic velocity either directly or jointly with complementary datasets. However, modern studies generally require large datasets which can be challenging to handle; therefore, FuncLab was originally written as an interactive Matlab GUI to assist in handling these large datasets. This software uses a project database to allow interactive trace editing, data visualization, H-κ stacking for crustal thickness and Vp/Vs ratio, and common conversion point stacking while minimizing computational costs. Since its initial release, significant advances have been made in the implementation of web services and changes in the underlying Matlab platform have necessitated a significant revision to the software. Here, we present revisions to the software, including new features such as data downloading via irisFetch.m, receiver function calculations via processRFmatlab, on-the-fly cross-section tools, interface picking, and more. In the descriptions of the tools, we present its application to a test dataset in Michigan, Wisconsin, and neighboring areas following the passage of USArray Transportable Array. The software is made available online at https://robporritt.wordpress.com/software and the IRIS seismic software repository, https://seiscode.iris.washington.edu/.}
}
@article{LJUNGBERG2006814,
title = {Design and usability of a PDE solver framework for curvilinear coordinates},
journal = {Advances in Engineering Software},
volume = {37},
number = {12},
pages = {814-825},
year = {2006},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2006.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997806000469},
author = {Malin Ljungberg and Kurt Otto and Michael Thuné},
keywords = {Variability modeling, Object-oriented, Framework, PDE solver, Curvilinear, Feature},
abstract = {An object-oriented PDE solver framework is a library of software components for numerical solution of partial differential equations, where each component is an object or a group of objects. Given such a framework, the construction of a particular PDE solver consists in selecting and combining suitable components. The present paper is focused on tengo [Åhlander K, Otto K. Software design for finite difference schemes based on index notation. Future Generation Comput Syst 2006;22:102–9], an object-oriented PDE solver framework for finite difference methods on structured grids, using tensor abstractions for convenient representation of numerical operators. Here, the design of tengo is extended to address curvilinear coordinates. These extensions to the tengo object model are the result of applying object-oriented analysis and design combined with feature modeling. The framework was implemented in Fortran 90/95, using standard techniques for emulating object-oriented constructs in that language. The new parts of the framework were assessed with respect to programming effort and execution time. It is shown that the programming effort required for construction and modification of PDE solvers on curvilinear grids is significantly reduced through the introduction of the new framework components. Moreover, for the test case of an underwater acoustics computation, there was no significant difference in execution time between the framework based code and a special purpose Fortran 90 code for the same application.}
}
@article{MEHLHORN1998289,
title = {A computational basis for higher-dimensional computational geometry and applications},
journal = {Computational Geometry},
volume = {10},
number = {4},
pages = {289-303},
year = {1998},
note = {Special Issue on Applied Computational Geometry},
issn = {0925-7721},
doi = {https://doi.org/10.1016/S0925-7721(98)00011-X},
url = {https://www.sciencedirect.com/science/article/pii/S092577219800011X},
author = {K. Mehlhorn and M. Müller and S. Näher and S. Schirra and M. Seel and C. Uhrig and J. Ziegler},
keywords = {Software library, Implementation, Convex hull, Delaunay triangulation},
abstract = {In this paper we describe and discuss a kernel for higher-dimensional computational geometry and we present its application in the calculation of convex hulls and Delaunay triangulations. The kernel is available in form of a software library module programmed in C++ extending LEDA. We introduce the basic data types like points, vectors, directions, hyperplanes, segments, rays, lines, spheres, affine transformations, and operations connecting these types. The description consists of a motivation for the basic class layout as well as topics like layered software design, runtime correctness via checking routines and documentation issues. Finally we shortly describe the usage of the kernel in the application domain.}
}