@INPROCEEDINGS{8820948,
author={Zhang, Yongzhe and Hu, Zhenjiang},
booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
title={Composing Optimization Techniques for Vertex-Centric Graph Processing via Communication Channels},
year={2019},
volume={},
number={},
pages={428-438},
abstract={Pregel's vertex-centric model allows us to implement many interesting graph algorithms, where optimization plays an important role in making it practically useful. Although many optimizations have been developed for dealing with different performance issues, it is hard to compose them together to optimize complex algorithms, where we have to deal with multiple performance issues at the same time. In this paper, we propose a new approach to composing optimizations, by making use of the channel interface, as a replacement of Pregel's message passing and aggregator mechanism, which can better structure the communication in Pregel algorithms. We demonstrate that it is convenient to optimize a Pregel program by simply using a proper channel from the channel library or composing them to deal with multiple performance issues. We intensively evaluate the approach through many nontrivial examples. By adopting the channel interface, our system achieves an all-around performance gain for various graph algorithms. In particular, the composition of different optimizations makes the S-V algorithm 3.39x faster than the current best implementation.},
keywords={Optimization;Message passing;Standards;Programming;Switches;Software algorithms;Redundancy;distributed computing;performance evaluation;software architecture},
doi={10.1109/IPDPS.2019.00053},
ISSN={1530-2075},
month={May},}
@INPROCEEDINGS{6221453,
author={Mohamadi, SH. and Akbari, A.},
booktitle={2012 11th International Conference on Environment and Electrical Engineering},
title={A new method for monitoring of distribution transformers},
year={2012},
volume={},
number={},
pages={632-636},
abstract={Distribution transformers are one of the most important equipment in power network. Because of, the large number of transformers distributed over a wide area in power electric systems, the data acquisition and condition monitoring is a important issue. This paper presents design and implementation of a mobile embedded system and a novel software to monitor and diagnose condition of transformers, by record key operation indictors of a distribution transformer like load currents, transformer oil, ambient temperatures and voltage of three phases. The proposed on-line monitoring system integrates a Global Service Mobile (GSM) Modem, with stand alone single chip microcontroller and sensor packages. Data of operation condition of transformer receives in form of SMS (Short Message Service) and will be save in computer server. Using the suggested online monitoring system will help utility operators to keep transformers in service for longer of time.},
keywords={Oil insulation;Monitoring;Power transformer insulation;Software;GSM;Standards;Monitoring;Distribution Transformers;Modular Software;GSM Network},
doi={10.1109/EEEIC.2012.6221453},
ISSN={},
month={May},}
@INPROCEEDINGS{1592798,
author={Wissink, A.M. and Hornung, R.D. and Kohn, S.R. and Smith, S.S. and Elliott, N.},
booktitle={SC '01: Proceedings of the 2001 ACM/IEEE Conference on Supercomputing},
title={Large Scale Parallel Structured AMR Calculations Using the SAMRAI Framework},
year={2001},
volume={},
number={},
pages={22-22},
abstract={This paper discusses the design and performance of the parallel data communication infrastructure in SAMRAI, a software framework for structured adaptive mesh refinement (SAMR) multi-physics applications. We describe requirements of such applications and how SAMRAI abstractions manage complex data communication operations found in them. Parallel performance is characterized for two adaptive problems solving hyperbolic conservation laws on up to 512 processors of the IBM ASCI Blue Pacific system. Results reveal good scaling for numerical and data communication operations but poorer scaling in adaptive meshing and communication schedule construction phases of the calculations. We analyze the costs of these different operations, addressing key concerns for scaling SAMR computations to large numbers of processors, and discuss potential changes to improve our current implementation.},
keywords={Large-scale systems;Adaptive mesh refinement;Data communication;Grid computing;Government;Libraries;Application software;Costs;Scientific computing;Laboratories},
doi={10.1145/582034.582040},
ISSN={},
month={Nov},}
@INPROCEEDINGS{365789,
author={Sitaraman, M.},
booktitle={Proceedings of 1994 3rd International Conference on Software Reuse},
title={On tight performance specification of object-oriented software components},
year={1994},
volume={},
number={},
pages={149-156},
abstract={Most modern designs of a software component include two separate pieces: functionality specification and implementation. When the specification is formal, this separation permits verification of reusable software components to be modular-essential for verification to be local, scalable, and hence, practical. In this paper, we explain the role of a third piece-an implementation dependent, performance specification-for a component. Introduction of this piece permits performance (e.g., execution time bonus) specification to be expressive (tight) while leaving functionality specification fully abstract and verification to be modular.<>},
keywords={Software performance;Switches;Software reusability;Software maintenance;Electrical capacitance tomography;Statistics;Computer science;Software engineering;Libraries;Productivity},
doi={10.1109/ICSR.1994.365789},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8895406,
author={Nanni, Marcelo and Torres Bernardo, Raul and Dotta, Daniel},
booktitle={2019 IEEE PES Innovative Smart Grid Technologies Conference - Latin America (ISGT Latin America)},
title={Development of Wide-Area Control Scheme in Power System Toolbox},
year={2019},
volume={},
number={},
pages={1-6},
abstract={The electric power system generation is currently experiencing remarkable changes, since the increasing shares of interments energy sources. To tackle this issue, we propose the implementation of the smart grid feature and intermittent energy source model in the Power System toolbox. It has been implemented in this paper a remote PSS controller applying remote PMU measurements. To explore the small signal stability analysis and control features available in the proposed platform, the parameter of the PSS and were designed by the HIFOO package aim the system damping. Linear and time simulations are used to demonstrate the system performance of the proposed remote controller in the test system with 53% of wind penetration. The proposed controller shows satisfactory damping performance when the measurement time delay is not greater than 200 ms.},
keywords={Open Source Software;Power System Analysis;Smart grid;Wide-Area Control},
doi={10.1109/ISGT-LA.2019.8895406},
ISSN={2643-8798},
month={Sep.},}
@INPROCEEDINGS{6303016,
author={Santana, Fabiana S. and Stange, Renata L. and Saraiva, Antonio M. and Pinaya, Jorge L. D. and Becerra, Jorge L. R.},
booktitle={2012 IEEE 13th International Conference on Information Reuse & Integration (IRI)},
title={A complete RM-ODP case-study to integrate geospatial services and ecological niche modeling systems},
year={2012},
volume={},
number={},
pages={239-246},
abstract={Conservation and sustainable use of natural resources are relevant research areas for many different purposes, including biodiversity maintenance, global warming studies and sustainable development. An ecological niche model presents the geographic distribution of a species, considering the spatial, ecological and evolutionary perspectives. It also allows the definition of present, past and future scenarios for species distribution. The study of species distribution can be determinant for their management, so ecological niche modeling is a major current research trend. However, providing the resources for ecological niche modeling is a highly complex computational problem because it demands solutions with many integration, distribution and interoperability features. The Reference Model of Open Distributed Processing provided by the International Organization for Standardization, ISO RM-ODP, establishes five viewpoints to design a system: Enterprise, Computational, Information, Engineering, and Technology. Each viewpoint treats specifics constraints in order to describe a complex architectural solution, as the ones required for ecological niche modeling. The Open Geospatial Consortium, OGC, defined the OGC Reference Model, a guideline to provide the features for geospatial services, another major software requirement for ecological niche modeling. This work proposes an architectural solution for ecological niche modeling integrating the OGC and RM-ODP specifications in order to address all the constraints of the problem by the usage of viewpoints. The presented solution is already implemented and available on the Internet. Besides solving the problem itself, the solution is in the form of a complete RM-ODP case-study and may also be used as reference for further works in the ecological niche modeling research area.},
keywords={Biological system modeling;Computational modeling;Software packages;Geospatial analysis;Standards;Data models;RM-ODP;OGC;Geospatial Services;Sustainable Development;Ecological Niche Modeling},
doi={10.1109/IRI.2012.6303016},
ISSN={},
month={Aug},}
@INPROCEEDINGS{995278,
author={Cox, P.T. and Baoming Song},
booktitle={Proceedings IEEE Symposia on Human-Centric Computing Languages and Environments (Cat. No.01TH8587)},
title={A formal model for component-based software},
year={2001},
volume={},
number={},
pages={304-311},
abstract={In an effort to manage increasing complexity and to maximise the reuse of code, the software engineering community has recently put a considerable effort into the design and development of component-based software development systems and methodologies. The concept of building software from existing components arose by analogy with the way that hardware is now designed and built, using cheap, reliable standard "off-the-shelf" modules. Due to the analogy with wiring hardware components, the component-based software development is a natural candidate for visual expression. Various component software technologies have emerged as a result of this attention, but their evolution has been rather ad hoc. In fact, some systems are defined purely by their implementation with little or no precise definition. In an attempt to address this shortcoming, we propose a well-defined syntax and semantics for a component software model that captures the essential concepts.},
keywords={Software libraries;Algorithms;Hardware;Software maintenance;Programming;Wires;Java;User interfaces;Informatics;Government},
doi={10.1109/HCC.2001.995278},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7459451,
author={Piessens, Frank and Verbauwhede, Ingrid},
booktitle={2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)},
title={Software security: Vulnerabilities and countermeasures for two attacker models},
year={2016},
volume={},
number={},
pages={990-999},
abstract={History has shown that attacks against network-connected software based systems are common and dangerous. An important fraction of these attacks exploit implementation details of the software based system. These attacks - sometimes called low-level attacks - rely on characteristics of the hardware, compiler or operating system used to execute software programs to make these programs misbehave, or to extract sensitive information from them. With the increased Internet-connectivity of embedded devices, including industrial control systems, sensors as well as consumer devices, there is a substantial risk that similar attacks will target these devices. This tutorial paper explains the vulnerabilities, attacks and countermeasures relevant for low-level software security. The paper discusses software security for two different attacker models: the classic model of an attacker that can only interact with the program by providing input and reading output, and the more recent and challenging model of an attacker that controls part of the execution platform on which the software runs, for instance because the attacker has compromised the operating system, or some of the libraries that the software under attack relies on.},
keywords={Security;Safety;Registers;Load modeling;Operating systems;Computers},
doi={},
ISSN={1558-1101},
month={March},}
@INPROCEEDINGS{9041738,
author={Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Kowalik, Kacper and Ludäscher, Bertram and McPhillips, Timothy and Nabrzyski, Jarek and Stodden, Victoria and Taylor, Ian and Thelen, Thomas and Turk, Matthew J. and Willis, Craig},
booktitle={2019 15th International Conference on eScience (eScience)},
title={Application of BagIt-Serialized Research Object Bundles for Packaging and Re-Execution of Computational Analyses},
year={2019},
volume={},
number={},
pages={514-521},
abstract={In this paper we describe our experience adopting the Research Object Bundle (RO-Bundle) format with BagIt serialization (BagIt-RO) for the design and implementation of "tales" in the Whole Tale platform. A tale is an executable research object intended for the dissemination of computational scientific findings that captures information needed to facilitate understanding, transparency, and re-execution for review and computational reproducibility at the time of publication. We describe the Whole Tale platform and requirements that led to our adoption of BagIt-RO, specifics of our implementation, and discuss migrating to the emerging Research Object Crate (RO-Crate) standard.},
keywords={Reproducibility of Results, Standards, Packag ing, Interoperability, Software, Digital Preservation},
doi={10.1109/eScience.2019.00068},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8917469,
author={Cecotti, Marco and Kanchwala, Husain and Aouf, Nabil},
booktitle={2019 IEEE Intelligent Transportation Systems Conference (ITSC)},
title={Autonomous Navigation for Mobility Scooters: a Complete Framework Based on Open-Source Software},
year={2019},
volume={},
number={},
pages={3627-3632},
abstract={In recent years, there has been a growing demand for small vehicles targeted at users with mobility restrictions and designed to operate on pedestrian areas. The users of these vehicles are generally required to be in control for the entire duration of their journey, but a lot more people could benefit from them if some of the driving tasks could be automated. In this scenario, we set out to develop an autonomous mobility scooter, with the aim to understand the commercial feasibility of a similar product.This paper reports on the progress of this project, proposing a framework for autonomous navigation on pedestrian areas, and focusing in particular on the construction of suitable costmaps. The proposed framework is based on open-source software, including a library created by the authors for the generation of costmaps.},
keywords={Three-dimensional displays;Motorcycles;Sensors;Software;Laser radar;Global navigation satellite system;Autonomous robots},
doi={10.1109/ITSC.2019.8917469},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8110427,
author={Li, Qiong and Liang, Dong},
booktitle={2017 4th International Conference on Information Science and Control Engineering (ICISCE)},
title={Design of High Voltage Power Supply Control System of Stationary Digital Breast Tomosynthesis Based on NI CompactRIO},
year={2017},
volume={},
number={},
pages={947-951},
abstract={In the Stationary Digital Breast Tomosynthesis(s-DBT) system, the role of the focusing electrode is to obtain clearer scanned images by focusing the electrons emitted by the carbon nanotubes. As one part of the control system of the s-DBT, the real-time control system of the focusing electrode high voltage power supply is designed. The control system using the NI CompactRIO device and LabVIEW as platform. The hardware unit mainly includes NI CompactRIO 9068, analog output module NI 9263 and analog acquisition module NI 9201. The software design includes the allocation of I/O ports, the choice of CompactRIO development architecture, the design of the power supply control's flow chart and the generation of dynamic link library. Through the construction of the test platform, the control system's feasibility and reliability were tested, analysis of the experimental data shows that the control system's relative error of the high voltage power supply's voltage control is less than 0.5%.},
keywords={Voltage control;Control systems;Power supplies;Real-time systems;Nickel;Breast;Hardware;NI CompactRIO;high voltage power supply;real-time;control system},
doi={10.1109/ICISCE.2017.200},
ISSN={},
month={July},}
@INPROCEEDINGS{6096134,
author={Enriquez, Marco},
booktitle={2011 IEEE/AIAA 30th Digital Avionics Systems Conference},
title={A modular software framework supporting simulation-driven optimization techniques},
year={2011},
volume={},
number={},
pages={7E3-1-7E3-10},
abstract={Simulation-Driven Optimization (SDO) problems (also referred to as "optimal control" or "4D optimization") are optimization problems with a simulation constraint. In the past decade, SDO techniques have been established as a promising tool for aviation analysis. Given a parameter-dependent simulation model, SDO techniques can automatically determine the optimal parameters that yield the desired simulation behavior. SDO techniques have been applied to aviation problems such as flight trajectory optimization, air traffic flow design and safety analysis of auto-land systems. The algorithmic solution of an SDO problem requires communication between simulation code (e.g., the numerical solution of the equations of motion) and optimization code (e.g., the Newton method). Typically, multiple simulations must be performed to form the numerical derivative of the cost function we seek to minimize or maximize, which must then be passed to some optimization software. This paper introduces the "Time-Stepping for Optimization" software framework or TSOpt to aid solution of SDO problems. TSOpt orchestrates communication and data exchange between the optimization code and the simulation code. TSOpt also offers support for implementation variants of the adjoint state method, a numerically efficient way to form derivatives for SDO problems. Further, TSOpt is equipped with tests that help ensure the correct numerical solution of SDO problems. Besides a concrete C++ software package, TSOpt framework offers a software paradigm that can be used to solve SDO problems on any platform, and in any language. I demonstrate this claim by solving a exploring the effect of low-fidelity wind data for a trajectory-based optimization problem in MATLAB.},
keywords={Mathematical model;Numerical models;Optimization;Unified modeling language;Computational modeling;Atmospheric modeling;Differential equations},
doi={10.1109/DASC.2011.6096134},
ISSN={2155-7209},
month={Oct},}
@INBOOK{8042449,
author={Holland, Oliver and Bogucka, Hanna and Medeisis, Arturas},
booktitle={Opportunistic Spectrum Sharing and White Space Access: The Practical Reality},
title={A Dynamically Reconfigurable Software Radio Framework},
year={2015},
volume={},
number={},
pages={81-98},
abstract={A software radio is a wireless system in which many of the tasks for generating a signal for transmission or receiving and extracting data from a transmitted signal are carried out in software, often on general‐purpose processors such as those found in PCs or laptops. Here, a radio is any device that sends or receives data wirelessly (this could be anything from a digital TV receiver to a 3GPP LTE modem used in modern mobile phones). One of the major advantages of building a radio in software is the flexibility it provides. In software, it is very easy to dramatically change the properties of the radio (e.g., switching from a garage door opener to an 802.11 Wi‐fi access point or vice versa).Iris is a software radio framework designed to take advantage of this flexibility. In this chapter, we present Iris and examine the features of the framework designed to support the implementation of highly flexible software radio systems. We present two case studies showing how Iris has been used to develop specific systems and how these features of the framework have been used.},
keywords={Iris;OFDM;Engines;Iris recognition;Software radio;Receivers;Libraries},
doi={10.1002/9781119057246.ch4},
ISSN={},
publisher={Wiley},
isbn={9781119057055},
url={https://ieeexplore.ieee.org/document/8042449},}
@INPROCEEDINGS{9185419,
author={Vendome, Christopher and Rao, Dhananjai M. and Giabbanelli, Philippe J.},
booktitle={2020 Spring Simulation Conference (SpringSim)},
title={How do Modelers Code Artificial Societies? Investigating Practices and Quality of Netlogo Codes from Large Repositories},
year={2020},
volume={},
number={},
pages={1-12},
abstract={Many guidelines have been developed for simulations in general or for agent-based models which support artificial societies. When applying such guidelines to examine existing practices, assessment studies are limited by the artifacts released by modelers. Although code is the final product defining an artificial society, 90% of the code produced is not released hence previous assessments necessarily focused on higher-level items such as conceptual design or validation. We address this gap by collecting 338 artificial societies from two hosting platforms, CoMSES/OpenABM and GitHub. An innovation of our approach is the use of software engineering techniques to automatically examine the models with respect to items such as commenting the code, using libraries, or dividing code into functions. We found that developers of artificial societies code the decision-making of their agents from scratch in every model, despite the existence of several libraries that could be used as building blocks.},
keywords={Measurement;Biological system modeling;Computational modeling;Libraries;Software;Complexity theory;Software engineering;Agent-Based Model;GitHub;Model Quality;NetLogo;Software Repository Mining},
doi={10.22360/SpringSim.2020.HSAA.007},
ISSN={},
month={May},}
@ARTICLE{6693038,
author={Chung, Haeyong and Andrews, Christopher and North, Chris},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={A Survey of Software Frameworks for Cluster-Based Large High-Resolution Displays},
year={2014},
volume={20},
number={8},
pages={1158-1177},
abstract={Large high-resolution displays (LHRD) enable visualization of extremely large-scale data sets with high resolution, large physical size, scalable rendering performance, advanced interaction methods, and collaboration. Despite the advantages, applications for LHRD can be developed only by a select group of researchers and programmers, since its software implementation requires design and development paradigms different from typical desktop environments. It is critical for developers to understand and take advantage of appropriate software tools and methods for developing their LHRD applications. In this paper, we present a survey of the state-of-the-art software frameworks and applications for cluster-based LHRD, highlighting a three-aspect taxonomy. This survey can aid LHRD application and framework developers in choosing more suitable development techniques and software environments for new LHRD applications, and guide LHRD researchers to open needs in LHRD software frameworks.},
keywords={Rendering (computer graphics);Software;Data visualization;Synchronization;Visualization;Large high-resolution display;tiled displays;distributed rendering;parallel rendering;distributed applications;graphics api;large scale visualization;programming models;input devices and strategies},
doi={10.1109/TVCG.2013.272},
ISSN={1941-0506},
month={Aug},}
@INPROCEEDINGS{6836287,
author={Deshmukh, Meenakshi and Schwarz, René and Braukhane, Andy and López, Rosa París and Gerndt, Andreas},
booktitle={2014 IEEE Aerospace Conference},
title={Model linking to improve visibility and reusability of models during space system development},
year={2014},
volume={},
number={},
pages={1-11},
abstract={The development of space systems involves complex interdisciplinary systems engineering. To manage such complexity, simulation and calculation models are becoming an integral part of it, where different domain-specific models (power, thermal, structure, propulsion, communication, etc.) are developed using different tools. Every domain model contains valuable knowledge of a respective discipline. However, creating such models takes an ample amount of time and efforts. Therefore, a common management for these models is needed to preserve the knowledge and to reuse them in future space missions. The project Simulation Model Library (SimMoLib) at the German Aerospace Center (DLR) develops guidelines and best practices regarding model development, model documentation, validation and verification, as well as model reviews to establish a collection of reusable models. To efficiently catalog the models, an innovative software system is created to support collaborative development, submission, archiving, review, search, and utilization of models. In SimMoLib, a model linking concept has been developed and implemented to enhance the model search and their probable reuse. Along with regular keyword-based search, a direct and an indirect linking between the models in the library has been implemented. Therefore, the model linking increases the visibility and consequently promotes the reuse of single and interdependent models within the library. The paper further describes different types of model relationships, categories, hierarchical levels of model development, implementation and presentation of model linking in detail.},
keywords={Mathematical model;Joining processes;Libraries;Space missions;Adaptation models;Computational modeling},
doi={10.1109/AERO.2014.6836287},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{6862730,
author={Milinković, Stevan},
booktitle={2014 3rd Mediterranean Conference on Embedded Computing (MECO)},
title={Pure software-based speech recognition for OS-less embedded systems},
year={2014},
volume={},
number={},
pages={40-43},
abstract={This paper discusses the implementation of a pure software-based, low cost embedded speech recognition system. The applied algorithms are memory efficient, scalable and provide sufficient accuracy of recognition. Advantages of such a system include rapid prototyping, flexibility in design modifications, and being a library, ease of integration with other applications.},
keywords={Speech recognition;Speech;Feature extraction;Accuracy;Speech processing;Hardware;Acoustics;embedded speech recognition;LPC;DTW},
doi={10.1109/MECO.2014.6862730},
ISSN={2377-5475},
month={June},}
@INPROCEEDINGS{5960696,
author={Rossler, Carl W. and Ertin, Emre and Moses, Randolph L.},
booktitle={2011 IEEE RadarCon (RADAR)},
title={A software defined radar system for joint communication and sensing},
year={2011},
volume={},
number={},
pages={1050-1055},
abstract={In this paper, we consider the theory and implementation of a joint radar-communication system. A radar software library compatible with Texas Instruments (TI's) small form factor software defined radio (SDR) platform is developed which allows users to implement real-time adaptive radar algorithms in MATLAB without requiring detailed knowledge of the inner workings of the SDR. As an example design and demonstration of the capabilities of the radar-software library, a joint radar-communication system has been implemented. The system uses wideband digital communication signals to simultaneously interrogate a scene as a radar while communicating range-Doppler maps resulting from previous interrogations. This combined system eliminates the need for separate hardware for communicating recorded radar returns to a base station. We study digital communication waveforms under the performance criteria of a radar waveform.},
keywords={Radar;Software;Radio frequency;Joints;Field programmable gate arrays;Peak to average power ratio;Modulation},
doi={10.1109/RADAR.2011.5960696},
ISSN={2375-5318},
month={May},}
@INPROCEEDINGS{613225,
author={Blackburn, M. and Busser, R.D. and Fontaine, J.S.},
booktitle={Proceedings of COMPASS '97: 12th Annual Conference on Computer Assurance},
title={Automatic generation of test vectors for SCR-style specifications},
year={1997},
volume={},
number={},
pages={54-67},
abstract={This paper provides the basis for integrating the Software Cost Reduction (SCR) specification method with the T-VEC (Test VECtor) test vector generator and specification analysis system. The SCR model is mapped to the T-VEC model to support automatic test vector generation for SCR specifications. The T-VEC system generated test vectors for an example SCR specification that was translated into the T-VEC language. The relationships between the models and the resulting test vectors are described. Two general guidelines for the translation process were identified that are fundamental for testing specifications that use event operators and for structuring the specifications to provide tests for all specified requirements.},
keywords={Automatic testing;System testing;Thyristors;Software testing;FAA;Certification;Productivity;Costs;Guidelines;Packaging},
doi={10.1109/CMPASS.1997.613225},
ISSN={},
month={June},}
@INPROCEEDINGS{4557008,
author={Poliakov, Ivan and Mokhov, Andrey and Rafiev, Ashur and Sokolov, Danil and Yakovlev, Alex},
booktitle={2008 14th IEEE International Symposium on Asynchronous Circuits and Systems},
title={Automated Verification of Asynchronous Circuits Using Circuit Petri Nets},
year={2008},
volume={},
number={},
pages={161-170},
abstract={To detect problematic circuit behaviour, such as potential hazards and deadlocks, in a reasonable amount of time a technique is required which would avoid exhaustive exploration of the state space of the system. Many of the existing methods rely on symbolic traversal of the state space, with the use of binary decision diagrams (BDDs) and associated software packages. This paper presents an alternative approach of using a special type of Petri nets to represent circuits. An algorithm for automatic conversion of a circuit netlist into a behaviourally equivalent Petri net is proposed. Once the circuit Petri net is constructed and composed with the provided environment specification, the presence and reachability of troublesome states is verified by using methods based on finite prefixes of Petri net unfoldings. The shortest trace leading to a deadlock or a hazard in the circuit Petri net is mapped back onto the gate-level representation of the circuit, thus assisting a designer in solving the problem. The method has been automated and compared against previously existing circuit verification tools.},
keywords={Asynchronous circuits;Petri nets;Circuit simulation;Hazards;State-space methods;Formal verification;System recovery;Data structures;Boolean functions;Explosions;Petri net;verification;asynchronous circuit},
doi={10.1109/ASYNC.2008.18},
ISSN={1522-8681},
month={April},}
@INPROCEEDINGS{8595212,
author={Cassee, Nathan and Pinto, Gustavo and Castor, Fernando and Serebrenik, Alexander},
booktitle={2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)},
title={How Swift Developers Handle Errors},
year={2018},
volume={},
number={},
pages={292-302},
abstract={Swift is a new programming language developed by Apple as a replacement to Objective-C. It features a sophisticated error handling (EH) mechanism that provides the kind of separation of concerns afforded by exception handling mechanisms in other languages, while also including constructs to improve safety and maintainability. However, Swift also inherits a software development culture stemming from Objective-C being the de-facto standard programming language for Apple platforms for the last 15 years. It is, therefore, a priori unclear whether Swift developers embrace the novel EH mechanisms of the programming language or still rely on the old EH culture of Objective-C even working in Swift. In this paper, we study to what extent developers adhere to good practices exemplified by EH guidelines and tutorials, and what are the common bad EH practices particularly relevant to Swift code. Furthermore, we investigate whether perception of these practices differs between novices and experienced Swift developers. To answer these questions we employ a mixed-methods approach and combine 10 semi-structured interviews with Swift developers and quantitative analysis of 78,760 Swift 4 files extracted from 2,733 open-source GitHub repositories. Our findings indicate that there is ample opportunity to improve the way Swift developers use error handling mechanisms. For instance, some recommendations derived in this work are not well spread in the corpus of studied Swift projects. For example, generic catch handlers are common in Swift (even though it is not uncommon for them to share space with their counterparts: non empty catch handlers), custom, developerdefined error types are rare, and developers are mostly reactive when it comes to error handling, using Swift's constructs mostly to handle errors thrown by libraries, instead of throwing and handling application-specific errors.},
keywords={Software;Data mining;Interviews;Protocols;C# languages;Guidelines;Error handling;Swift;Language feature usage},
doi={},
ISSN={2574-3864},
month={May},}
@ARTICLE{5871663,
author={},
journal={ISO/IEC/IEEE 26512 First Edition, 2011-06-01},
title={IEEE/ISO/IEC Systems and software engineering -- Requirements for acquirers and suppliers of user documentation},
year={2011},
volume={},
number={},
pages={1-48},
abstract={ISO/IEC/IEEE 26512:2011 was developed to assist users of ISO/IEC 15288:2008 (IEEE Std 15288-2008) or ISO/IEC 12207:2008 (IEEE Std 12207 2008) to acquire or supply software user documentation as part of the software life cycle processes. It defines the documentation process from the acquirer's standpoint and the supplier's standpoint. ISO/IEC/IEEE 26512:2011 covers the requirements for information items used in the acquisition of user documentation products: the Acquisition Plan, Document Specification, Statement of Work, Request for Proposals, and the proposal. It provides an overview of the software user documentation and information management processes which may require acquisition and supply of software user documentation products and services. It addresses the preparation of requirements for software user documentation. These requirements are central to the user documentation specification and Statement of Work. It includes requirements for primary document outputs of the acquisition and supply process: the Request for Proposal and the Proposal for user documentation products and services. It also discusses the use of a Documentation Management Plan and a Document Plan as they arise in the acquisition and supply processes. ISO/IEC/IEEE 26512:2011 is independent of the software tools that may be used to produce documentation, and applies to both printed documentation and on-screen documentation. Much of its guidance is applicable to user documentation for systems including hardware as well as software.},
keywords={IEEE standards;Software engineering;Documentation;ISO standards;IEC standards;acquisition;information management;proposal;software user documentation;statement of work;supply},
doi={10.1109/IEEESTD.2011.5871663},
ISSN={},
month={June},}
@INPROCEEDINGS{6975379,
author={Segura, Angel Mora and Cuadrado, Jesus Sanchez and De Lara, Juan},
booktitle={2014 IEEE 18th International Enterprise Distributed Object Computing Conference Workshops and Demonstrations},
title={ODaaS: Towards the Model-Driven Engineering of Open Data Applications as Data Services},
year={2014},
volume={},
number={},
pages={335-339},
abstract={The Data-as-a-Service (DaaS, or Data Services) paradigm enables an on-demand, service-based access to data, relying on similar principles to Software-as-a-Service (SaaS). DaaS permits centralized data quality management, a uniform view and access to heterogeneous data, and enables exposing a richer, domain-specific data model to users. Within this context, we are witnessing a trend in institutions to make information public as open data. However, such information is normally released "as-is", in heterogeneous formats, requiring costly, ad-hoc pre-processing steps for cleansing and analysis of its underlying structure. This paper proposes an adaptation of the DaaS paradigm for the construction of open data applications. For this purpose, we introduce an architecture based on Model-Driven Engineering (MDE), consisting of (i) multi-level modelling for the description of domains, based on generic meta-models, (ii) a library of injectors to bring data on demand from heterogeneous sources into the MDE technical space, and (iii) a REST-based infrastructure to access the data services. This work presents the architecture of such framework and the first steps in its realization.},
keywords={Data models;Biological system modeling;Object oriented modeling;Semantics;Transportation;Proposals;Government;Model-driven engineering;Multi-level modelling;Data services;Open data;Open Data as a Service (ODaaS)},
doi={10.1109/EDOCW.2014.55},
ISSN={2325-6605},
month={Sep.},}
@ARTICLE{9767818,
author={Fang, Yu-Shen and Fang, Li-Chun},
journal={IEEE Access},
title={A Review of Chinese E-Commerce Research: 2001–2020},
year={2022},
volume={10},
number={},
pages={49015-49027},
abstract={Electronic commerce (EC) has become the most critical business activity worldwide. China has become the world’s second largest economy. This study reviewed EC studies in China because although research on EC has yielded numerous results, limited research have reviewed these papers. The data used in this study were obtained from the Web of Science database. A total of 1,982 journal articles published between 2001 and 2020 were collected. In addition to conducting an overall analysis on EC in China, this study referred to the Five-Year Plan for Economic and Social Development of the People’s Republic of China and divided the research period into 5-year stages. The BibExcel, UCINET, and SPSS software programs were used to conduct co-word analysis on the keywords in the papers for determining the knowledge structure and clustering of each stage and understanding the trend of leading research in the future. The results indicated that (a) Stages I–IV comprised 4, 3, 4, and 4 clusters, respectively. (b) Consumer’s personalized demands were considered in EC development activities ranging from the initial EC infrastructure construction to the integration of artificial-intelligence-related technology. (c) Topics regarding consumer behaviors were centered on Stages II and III, which indicated that the research on these topics was mature. (d) Stage IV explored the new research topic of integrating smart technology into the EC environment and indicated the characteristics of the e-market.},
keywords={Market research;Databases;Electronic commerce;Education;Statistical analysis;Libraries;Consumer behavior;Topic evolution;co-word analysis;research focus;e-service;cluster analysis},
doi={10.1109/ACCESS.2022.3172433},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7977842,
author={Ribeiro, Marcos F. O. and Vasconcelos, João A. and Teixeira, Douglas A.},
booktitle={2017 IEEE International Conference on Environment and Electrical Engineering and 2017 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe)},
title={Optimization of compact overhead lines of 138/230kV: Optimal selection and arrangement of cables and definition of the best transmission line tower topology},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Due to the problem of the growing necessity of transmitting large electric power packages in high voltage within large urban centers, the construction of compact transmission lines with multiple circuits (LTC) becomes interesting. In this context, this paper presents a new methodology for LTC's optimization implemented on software, which includes since the selection of the topology of the structure until the choice of the conductor and overhead ground wire cables and its optimal positioning. This methodology includes up to four circuits of 138kV and 230 kV in the same tower and makes use of evolutionary optimization techniques to many objectives and decision-making. The obtained results demonstrate the excellence of the proposed methodology.},
keywords={Conductors;Optimization;Poles and towers;Power cables;Linear programming;Minimization;compact transmission lines;evolutionary optimization;many-objective and decision-making models},
doi={10.1109/EEEIC.2017.7977842},
ISSN={},
month={June},}
@INPROCEEDINGS{5490300,
author={Constantinides, C. and Aristokleous, N. and Johnson, G. A. and Perperides, D.},
booktitle={2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro},
title={Static and dynamic cardiac modelling: Initial strides and results towards a quantitatively accurate mechanical heart model},
year={2010},
volume={},
number={},
pages={496-499},
abstract={Magnetic Resonance Imaging (MRI) has exhibited significant potential for quantifying cardiac function and dysfunction in the mouse. Recent advances in high-resolution cardiac MR imaging techniques have contributed to the development of acquisition approaches that allow fast and accurate description of anatomic structures, and accurate surface and finite element (FE) mesh model constructions for study of global mechanical function in normal and transgenic mice. This study presents work in progress for construction of quantitatively accurate three-dimensional (3D) and 4D dynamic surface and FE models of murine left ventricular (LV) muscle in C57BL/6J (n=10) mice. Constructed models are subsequently imported into commercial software packages for the solution of the constitutive equations that characterize mechanical function, including computation of the stress and strain fields. They are further used with solid-free form fabrication processes to construct model-based material renditions of the human and mouse hearts.},
keywords={Heart;Mice;Magnetic resonance imaging;High-resolution imaging;Finite element methods;Muscles;Software packages;Equations;Stress;Capacitive sensors;Magnetic Resonance Imaging;cardiovascular system;image processing;finite element methods;rapid prototyping},
doi={10.1109/ISBI.2010.5490300},
ISSN={1945-8452},
month={April},}
@INPROCEEDINGS{4637466,
author={Zhang, Lixia and Wang, Weiming and Li, Chuanhuang},
booktitle={2008 Second International Conference on Genetic and Evolutionary Computing},
title={Research and Implementation of LFB-Resources Registration Management Method},
year={2008},
volume={},
number={},
pages={372-376},
abstract={The next generation networks demand the router to be more open, flexible and programmable. As an important part of ForCES router software, an extensible LFB management and development model based on ForCES router software was illustrated, which enhances extensibility of the software. In this paper the ForCES router software structure was first discussed. Based on that, some of the key technologies were discussed for implementation of the model, including LFB resources construction of data, LFB registration function model and so on. This paper compares two methods about the LFB-resources registration. And according to a series of tests, a better method has been found. The implementation of the model was presented in the paper, which is testified to be usable in ForCES. And the impact on traffic flows and applications is evaluated.},
keywords={Software;XML;Iron;Protocols;Routing protocols;Libraries;Arrays;Forwarding and Control Element Separation;Logical Functional Block},
doi={10.1109/WGEC.2008.10},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{1630731,
author={Santoro, A. and Quaglia, F.},
booktitle={20th Workshop on Principles of Advanced and Distributed Simulation (PADS'06)},
title={Transparent Optimistic Synchronization in HLA via a Time-Management Converter},
year={2006},
volume={},
number={},
pages={193-200},
abstract={In this paper we present the design and implementation of a Time Management Converter (TiMaC) for HLA based simulation systems. TiMaC is a layer interposed in between the federate and the underlying RTI in order to map the conservative Time Management interface onto the optimistic one. In this way, TiMaC transparently supports optimistic execution for federates originally designed for the conservative approach, which is achieved without the need for developing any ad-hoc RTI system. TiMaC relies on a recently proposed software architecture for transparent treatment of checkpointing/recovery of the federate state, namely Magic State Manager (MASM), and implements a set of additional facilities required to support all the tasks associated with the mapping of conservative onto optimistic Time Management interfaces. The implementation has been tailored to the Georgia Tech B-RTI package, although the underlying design principles would allow it to be integrated with any RTI system. We also report an experimental study demonstrating the viability and effectiveness of our proposal in allowing conservative federates to be supported with highly increased run-time effectiveness in general contexts for what concerns the features of the underlying computing systems (e.g. LAN vsWAN based systems.},
keywords={Checkpointing;Computational modeling;Design optimization;Software architecture;Packaging;Proposals;Runtime;Middleware;Application software;Discrete event simulation},
doi={10.1109/PADS.2006.37},
ISSN={1087-4097},
month={May},}
@ARTICLE{9442734,
author={Adamczewski-Musch, Jörn and Kurz, Nikolaus},
journal={IEEE Transactions on Nuclear Science},
title={The GosipGUI Framework for Control and Benchmarking of Readout Electronics Front-Ends},
year={2021},
volume={68},
number={8},
pages={2074-2080},
abstract={The gigabit optical serial interface protocol (GOSIP) provides communication via optical fibers between multiple kinds of front-end electronics and the KINPEX PCIe receiver board located in the readout host PC. In recent years, a stack of device driver software has been developed to utilize this hardware for several scenarios of data acquisition (DAQ). On top of this driver foundation, several graphical user interfaces (GUIs) have been created. These GUIs are based on the Qt graphics libraries and are designed in a modular way: All common functionalities, like generic I/O with the front-ends, handling of configuration files, and window settings are treated by a framework class GosipGUI. In the Qt workspace of such GosipGUI frame, specific subclasses may implement additional windows dedicated to operating different GOSIP front-end modules. For each kind of front-end, the GUIs allow to monitor specific register contents, set up the working configuration, and interactively change parameters like sampling thresholds during DAQ. The latter is extremely useful when qualifying and tuning the front-ends in the electronics lab or detector cave. Moreover, some of these GosipGUI implementations have been equipped with features for mostly automatic testing of ASICs in a prototype mass production.},
keywords={Graphical user interfaces;Hardware;Data acquisition;Software;C++ languages;Control systems;C++;control system;data acquisition (DAQ);frontend electronics;graphical user interface (GUI);Qt;software framework},
doi={10.1109/TNS.2021.3084351},
ISSN={1558-1578},
month={Aug},}
@INPROCEEDINGS{8740449,
author={Wang, Fengyu and Xing, Changyuan and Tang, Xia and Zhang, Cheng},
booktitle={2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)},
title={The Design and Practice of the "Cloud Technology" Based Economic Management Experimental Teaching Platform},
year={2018},
volume={},
number={},
pages={1310-1316},
abstract={The 21st Century is an era of educational informationization, and it is required for economic management experimental teaching to advance with times. Due to the universal problems of economic management teaching such as large difficulties in resource integration and unequal utilization ratio as well as individual problems of the economic management training center of Yangtze Normal University such as decentralized resources of two frameworks of C/S and B/S on software and limited practical training field, it is imperative to conduct the design and practice of "cloud technology" based economic management experimental teaching platform. Mainly including logical structure, software construction and hardware construction, the construction of this platform will further promote the change on learning method and the wide integration of teaching resources, to effectively promote the innovation capabilities and application abilities of students of economic management, to make the students adapt to social and economic development.},
keywords={Economics;Cloud computing;Training;Technological innovation;Libraries;Business;cloud technology;economic management experimental teaching;design; practice},
doi={10.1109/ITOEC.2018.8740449},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8816786,
author={Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris},
booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
title={World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data},
year={2019},
volume={},
number={},
pages={143-154},
abstract={Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.},
keywords={Software;Tools;Data mining;Ecosystems;Task analysis;Prototypes;Databases;software mining, software supply chain, software ecosystem},
doi={10.1109/MSR.2019.00031},
ISSN={2574-3864},
month={May},}
@ARTICLE{20288,
author={Billingsley, F.C. and Johnson, J. and Greenberg, E. and MacMedan, M.},
journal={IEEE Transactions on Geoscience and Remote Sensing},
title={Facilitating information transfer in the Eos era},
year={1989},
volume={27},
number={2},
pages={117-124},
abstract={A simple interactive demonstration program has been written in C to allow a user to input data field descriptions as label format. This program generates a full RECFMT (record format) description and the complete transfer syntax description notation (TSDN) file. It is intended that this program be upgraded to operational quality and be made available to users to simplify the description and TSDN file construction task. The total set of capabilities, from the standard formatted data unit packaging of related files and consistent segment structures, through the type definition techniques and the call server, will constitute a unique tool for the systematic transfer of data. This software on each end may be independent, one end from the other. With it available, local software that will be needed to convert user files to and from the canonical interface will be appreciably simplified.<>},
keywords={Earth Observing System;Data systems;Space technology;NASA;Propulsion;Data structures;Senior members;Information systems;Object oriented programming},
doi={10.1109/36.20288},
ISSN={1558-0644},
month={March},}
@INPROCEEDINGS{5689789,
author={Feng, Guoqi and Cui, Dongliang},
booktitle={The 2nd International Conference on Information Science and Engineering},
title={Research of several key technologies of business component in IMS-DATE},
year={2010},
volume={},
number={},
pages={469-472},
abstract={Aerodynamic design of aero turbine engine is a complex process that requires diverse engineering disciplines to execute independent of each other in sequence, usually iteratively and interactively. It involves various scientific computing packages to provide a fast, accurate, and reliable approach to model engineering problems. The calculations of aerodynamics, thermodynamics and structure are repeated continuously by many kinds of engineering computation software. The characteristics require a modular design mode. Business component is introduced in integrated management system for aerodynamic design of aero turbine engine (IMS-DATE) to deal with those difficulties, which is defined as a combinational management module that comprises design object's attributes and corresponding processing logic, which can implement universal management of design resources and design data. Concepts of business component applied are defined, including its structure compositions and framework design. And several key technologies in implementation are illustrated, namely, hierarchical software registry, software encapsulation and parameter parsing.},
keywords={Heating;Temperature sensors;Programming;Protocols;Relays;Monitoring;business component;data management;version management;parameter parsing;software encapsulation},
doi={10.1109/ICISE.2010.5689789},
ISSN={2160-1291},
month={Dec},}
@ARTICLE{6313181,
author={Silverman, Barry G.},
journal={IEEE Transactions on Systems, Man, and Cybernetics},
title={Analogy in systems management: A theoretical inquiry},
year={1983},
volume={SMC-13},
number={6},
pages={1049-1089},
abstract={This theoretical analysis of the intuitive and diffuse characteristics of analogical reasoning processes is the first step in a research effort intended to lead to: understanding of common (and possibly costly) errors, pitfalls, travails, and problem-solving impediments; possible recommendations for improvements to organizational structures, control and coordination processes, and management information flows, and guidelines for a generalized analogical reasoning support framework (e.g., a handbook, a knowledge bank design, and/or even a software package/artificial intelligence program).},
keywords={Cognition;Humans;Guidelines;Process control;Psychology;Cybernetics;Educational institutions},
doi={10.1109/TSMC.1983.6313181},
ISSN={2168-2909},
month={Nov},}
@ARTICLE{9865203,
author={Chae, Keunhong and Park, Jungin and Kim, Yusung},
journal={IEEE Internet of Things Journal},
title={Rethinking Autocorrelation for Deep Spectrum Sensing in Cognitive Radio Networks},
year={2023},
volume={10},
number={1},
pages={31-41},
abstract={We design a novel learning-based spectrum sensing model. Under the insight that an autocorrelation curve yields richer information than a single sum of received signal powers for detecting the presence of a primary user, we propose a convolutional neural network-based deep learning model, called deep spectrum sensing (DSS), that receives an autocorrelation curve as input. Extensive simulation results show that our DSS model has a higher performance than existing deep-learning-based models that use raw signals or spectrograms as an input. Furthermore, DSS can be trained with much smaller amounts of data than the existing models, and is a lighter model compared with the existing models. Finally, we evaluate the effectiveness of the DSS implementation over a real testbed consisting of universal software radio peripheral and GNU radio packages. The experimental results are consistent with the simulation performance.},
keywords={Sensors;Autocorrelation;Detectors;Internet of Things;Signal to noise ratio;Matched filters;Data models;Autocorrelation;convolutional neural network;deep learning;spectrum sensing;universal software radio peripheral (USRP)},
doi={10.1109/JIOT.2022.3200968},
ISSN={2327-4662},
month={Jan},}
@ARTICLE{8352744,
author={Iqbal, Uzair and Ying Wah, Teh and Habib Ur Rehman, Muhammad and Mastoi, Qurat-Ul-Ain},
journal={IEEE Access},
title={Usage of Model Driven Environment for the Classification of ECG features: A Systematic Review},
year={2018},
volume={6},
number={},
pages={23120-23136},
abstract={Electrocardiography (ECG) constitutes a perfect and primary diagnostic tool for measuring the different morbidity conditions of the heart in the context of different heart diseases and arrhythmia. Various studies have proposed different techniques of classification of ECG features and defined the parametric structure of different features of ECG. This paper is primarily designed to provide a more accurate classification by inducting the concept of a model-driven environment (MDE). Such induction works on the basis of reusable factors that are fitted to the state-of-the-art parametric structure of the ECG features. Some issues and challenges related to the embedding process are highlighted in the form of research questions. The aim of this paper is to provide the solutions to these research questions. The literature review is completed in two phases. In the first phase, those articles are collected that have been published in IEEE Xplore, ACM Library, Science Direct, and Springer, from 2008 to 2017.The second phase as a part of the execution stage is completed at the three different levels of the rectification process by adhering to the Kitchen ham guideline. At the first level, articles are filtered according to title and abstract. At the second level, articles are filtered according to specific eligibility criteria, and at the last level, articles are selected based on the skills of different domain experts (authors) by checking the quality assessment parameters. The significance of MDE in the classification of different ECG features is reflected in their compatibility with the research questions. Furthermore, future directions are proposed that depict the significance of dependencies involvement in classification analysis of ECG. These future directions are identified based on the planning and execution of our operational investigation and our critical observation of the existing gaps between dependencies of features classification that is the major cause of cardiovascular diseases.},
keywords={Electrocardiography;Data models;Analytical models;Hidden Markov models;Feature extraction;Software;Systematics;Software metrics;classification algorithms;electrocardiography;model driven environment},
doi={10.1109/ACCESS.2018.2828882},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6037182,
author={Ji Qingbo and Geng Liqun},
booktitle={Proceedings of 2011 Cross Strait Quad-Regional Radio Science and Wireless Technology Conference},
title={The program development based on DM642 using Reference Framework and algorithm standard},
year={2011},
volume={2},
number={},
pages={1224-1227},
abstract={The Reference Framework for TMS320DSP and the TMS320DSP algorithm standard, as the eXpressDSP software key module, not only standardize the interface of algorithm and system software, but also put forward the rules and guidelines of the standard algorithm, thus the difficulty of the program developer is reduced and the researching and developing period of products is shortened. In this paper, OTSU algorithm is realized on the TMS320DM642EVM board based on RF5 and the TMS320DSP algorithm standard. The successful calling of OTSU algorithm not only confirms the realization of algorithm standard and the successful structures of RF5, but also proves that the method described in this paper can effectively improve the DM642 efficiency.},
keywords={Digital signal processing;Synchronization;Standards;Libraries;Software;Hardware;Gold;RF5;DSP/BIOS;Algorithm Standard;image segmentation},
doi={10.1109/CSQRWC.2011.6037182},
ISSN={},
month={July},}
@ARTICLE{7321794,
author={Bernardi, Paolo and Cantoro, Riccardo and De Luca, Sergio and Sánchez, Ernesto and Sansonetti, Alessandro},
journal={IEEE Transactions on Computers},
title={Development Flow for On-Line Core Self-Test of Automotive Microcontrollers},
year={2016},
volume={65},
number={3},
pages={744-754},
abstract={Software-Based Self-Test is an effective methodology for devising the online testing of Systems-on-Chip. In the automotive field, a set of test programs to be run during mission mode is also called Core Self-Test library. This paper introduces many new contributions: (1) it illustrates the several issues that need to be taken into account when generating test programs for on-line execution; (2) it proposed an overall development flow based on ordered generation of test programs that is minimizing the computational efforts; (3) it is providing guidelines for allowing the coexistence of the Core Self-Test library with the mission application while guaranteeing execution robustness. The proposed methodology has been experimented on a large industrial case study. The coverage level reached after one year of team work is over 87 percent of stuck-at fault coverage, and execution time is compliant with the ISO26262 specification. Experimental results suggest that alternative approaches may request excessive evaluation time thus making the generation flow unfeasible for large designs.},
keywords={Built-in self-test;Registers;Automotive engineering;Robustness;Standards;Interrupters;Microprocessors and microcomputers;Reliability and Testing;Software-Based Self-Test;Microprocessors and microcomputers;reliability and testing;software-based self-test},
doi={10.1109/TC.2015.2498546},
ISSN={1557-9956},
month={March},}
@INPROCEEDINGS{6830944,
author={Chu, Chih-Han and Wan, Menghsi and Yang, Yufan and Gao, Jerry and Deng, Lei},
booktitle={2014 IEEE 8th International Symposium on Service Oriented System Engineering},
title={Building On-demand Marketing SaaS for Crowdsourcing},
year={2014},
volume={},
number={},
pages={430-438},
abstract={The concept of crowdsourcing has been introduced for years and greatly accepted by companies and enterprises. As opposed to current marketing systems, this paper designs a new marketing system by merging the idea of crowdsourcing into marketing information system (MkIS). Our purpose is to develop a cloud-based MkIS which serves as a platform and provides info publishing/tracking via various media and crowdsourcing features. To ensure accessibility and cost-efficiency, the product will be deployed as Software as a Service (SaaS), which will be capable of supporting large number of customers. The implementation of this idea is to construct a prototype system which targets enterprise and crowdsourcing service providers. The outcome has two deliverables. One is application server, which hosts the service API for web clients. Another is database server, which stores and replicates application data. The work serves as one of the first attempts for an information system \flexible enough to adapt marketing, crowdsourcing, and other business workflows. Moreover, it shows the potential of the form of MkIS. This paper also presents the design and implementation of the system.},
keywords={Business;Media;Servers;Information systems;Facebook;Automation;crowdsourcing; marketing information system; MkIS; software as a service; SaaS; workflow},
doi={10.1109/SOSE.2014.63},
ISSN={},
month={April},}
@INPROCEEDINGS{5664025,
author={DeCew, Judson and Baldwin, Kenneth and Celikkol, Barbaros and Chambers, Michael and Fredriksson, David W. and Irish, Jim and Langan, Rich and Rice, Glenn and Swift, M. Robinson and Tsukrov, Igor},
booktitle={OCEANS 2010 MTS/IEEE SEATTLE},
title={Assessment of a submerged grid mooring in the Gulf of Maine},
year={2010},
volume={},
number={},
pages={1-9},
abstract={The University of New Hampshire (UNH) developed and maintained an offshore aquaculture test site in the Western Gulf of Maine, south of the Isles of Shoals in approximately 50 m of water. This site was designed to have a permanent moored grid to which prototype fish cages or surface buoys could be attached for testing new designs and the viability of the structure in the exposed Gulf of Maine. In 1999, the first moorings deployed consisted of twin single bay grids each capable of each securing one fish cage. These systems were maintained until 2003. To expand the biomass capacity of the site, the single bay moorings were recovered and a new four bay submerged grid mooring was deployed within the same foot print of the previous twin systems. This unique system operated as a working platform to test various structures, including surface and submersible fish cages, feeding buoys and other supporting equipment. In addition, the expanded capability allowed aquaculture fish studies to be conducted along with engineering and new cage/feeder testing. The 4 bays of the mooring system were located 15 meters below the surface. These bays were supported by nine flotation elements. The system was secured to the seafloor on the sides with twelve catenary mooring legs, consisting of Polysteel® line, 27.5 m of 52 mm chain and a 1 ton embedment anchor, and in the center, with a single vertical line to a 2 ton weight. To size the mooring gear, the UNH software package Aqua-FE was employed. This program can apply waves and currents to oceanic structures, predicting system motions and mooring component tensions. The submerged grid was designed to withstand 9 meter, 8.8 second waves with a 1 m/s collinear current, when securing four fish cages. During its seven year deployment, the site regularly experienced extreme weather events, most notably a storm with a 9 m significant wave height, 10 second dominate period in April 2007. The maximum currents at the site were observed during internal solitary wave events when 0.75 m/s currents with 25 minute periods and 8 m duration were observed. The mooring was recovered in 2010 after 7 years of continuous deployment without problems. The dominate maintenance requirement of the mooring was the cleaning once a year of excessive mussel growth on the flotation elements and grid lines. No problems of anchor dragging or failure of mooring components were documented during the deployment. Upon recovery, critical mooring components were inspected and documented, focusing on items with wear or other areas of interest. The mooring proved to be a reliable, stable working platform for a variety of prototype ocean projects, highlighting the importance of a sound engineering approach taken in the design process.},
keywords={Marine animals;Aquaculture;Storms;Gears;Numerical models;Ocean temperature},
doi={10.1109/OCEANS.2010.5664025},
ISSN={0197-7385},
month={Sep.},}
@INPROCEEDINGS{9755840,
author={Krutikov, Alexander K. and Meltsov, Vasily Y. and Strabykin, Dmitry A.},
booktitle={2022 Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)},
title={Evaluation the Efficienty of Forecasting Sports Events Using a Cascade of Artificial Neural Networks Based on FPGA},
year={2022},
volume={},
number={},
pages={355-360},
abstract={Well-known implementations of software systems for predicting sports events based on artificial neural networks do not provide sufficiently accurate results in individual sports. The main difficulty in this case is the fact that various types and formats of data act as source information: numeric, symbolic, interval. The authors propose an approach based on cascading of several types of neural networks. A specialized training sample is prepared for each module in accordance with the type of model and the selected cascade structure. The results of information processing on each layer serve as data sets for modules of subsequent tiers. Initially, the system was implemented using the MATLAB package. The experiments carried out confirm the effectiveness of cascading modules. To significantly increase the performance of the system, for example, when used in betting shops, a cascade of neural networks is implemented on the basis of Altera Cyclone III FPGA using Quartus CAD.},
keywords={Training;Solid modeling;Neurons;Artificial neural networks;Predictive models;Mathematical models;Forecasting;artificial intelligence;neural network;sports forecasting;training sampling;vector quantization;cascading;software system;FPGA},
doi={10.1109/ElConRus54750.2022.9755840},
ISSN={2376-6565},
month={Jan},}
@INPROCEEDINGS{9519384,
author={Li, Xigao and Azad, Babak Amin and Rahmati, Amir and Nikiforakis, Nick},
booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
title={Good Bot, Bad Bot: Characterizing Automated Browsing Activity},
year={2021},
volume={},
number={},
pages={1589-1605},
abstract={As the web keeps increasing in size, the number of vulnerable and poorly-managed websites increases commensurately. Attackers rely on armies of malicious bots to discover these vulnerable websites, compromising their servers, and exfiltrating sensitive user data. It is, therefore, crucial for the security of the web to understand the population and behavior of malicious bots.In this paper, we report on the design, implementation, and results of Aristaeus, a system for deploying large numbers of "honeysites", i.e., websites that exist for the sole purpose of attracting and recording bot traffic. Through a seven-month-long experiment with 100 dedicated honeysites, Aristaeus recorded 26.4 million requests sent by more than 287K unique IP addresses, with 76,396 of them belonging to clearly malicious bots. By analyzing the type of requests and payloads that these bots send, we discover that the average honeysite received more than 37K requests each month, with more than 50% of these requests attempting to brute-force credentials, fingerprint the deployed web applications, and exploit large numbers of different vulnerabilities. By comparing the declared identity of these bots with their TLS handshakes and HTTP headers, we uncover that more than 86.2% of bots are claiming to be Mozilla Firefox and Google Chrome, yet are built on simple HTTP libraries and command-line tools.},
keywords={Bot (Internet);Heuristic algorithms;Fingerprint recognition;Tools;Libraries;Software;Browsers;Malware-and-unwanted-software;web-security},
doi={10.1109/SP40001.2021.00079},
ISSN={2375-1207},
month={May},}
@ARTICLE{6790913,
author={Seo, Jinseok and Kim, Gerard Jounghyun},
journal={Presence},
title={Design for Presence: A Structured Approach to Virtual Reality System Design},
year={2002},
volume={11},
number={4},
pages={378-403},
abstract={The development and maintenance of a virtual reality (VR) system requires indepth knowledge and understanding in many different disciplines. Three major features that distinguish VR systems are real-time performance while maintaining acceptable realism and presence, objects with two clearly distinct yet inter-related aspects like geometry/structure and function/behavior, and the still experimental nature of multi-modal interaction design. Until now, little attention has been paid to methods and tools for the structured development of VR software that addresses these features. Many VR application development projects proceed by modeling needed objects on conventional CAD systems, then programming the system using simulation packages. Usually, these activities are carried out without much planning, which may be acceptable for only small-scale or noncritical demonstration systems. However, for VR to be taken seriously as a media technology, a structural approach to developing VR applications is required for the construction of large-scale VR worlds, and this will undoubtedly involve and require complex resource management, abstractions for basic system/object functionalities and interaction tasks, and integration and easy plug-ins of different input and output methods. In this paper, we assembled a comprehensive structured methodology for building VR systems, called CLEVR (Concurrent and LEvel by Level Development of VR System), which combines several conventional and new concepts. For instance, we employ concepts such as the simultaneous consideration of form, function, and behavior, hierarchical modeling and top-down creation of LODs (levels of detail), incremental execution and performance tuning, user task and interaction modeling, and compositional reuse of VR objects. The basic underlying modeling approach is to design VR objects (and the scenes they compose) hierarchically and incrementally, considering their realism, presence, behavioral correctness, performance, and even usability in a spiral manner. To support this modeling strategy, we developed a collection of computeraided tools called P-VoT (POSTECH-Virtual reality system development Tool). We demonstrate our approach by illustrating a step-by-step design of a virtual ship simulator using the CLEVR/P-VoT, and demonstrate the effectiveness of our method in terms of the quality (performance and correctness) of the resulting software and reduced effort in its development and maintenance.},
keywords={},
doi={10.1162/105474602760204291},
ISSN={1054-7460},
month={Aug},}
@INPROCEEDINGS{9343226,
author={Suri, Guga and Fu, Jianming and Zheng, Rui and Liu, Xinying},
booktitle={2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)},
title={Family Identification of AGE-Generated Android Malware Using Tree-Based Feature},
year={2020},
volume={},
number={},
pages={386-393},
abstract={Application Generation Engine(AGE) is a development tool that can automatically generate simple Android applications by utilizing some boilerplate codes. People with little software programming background could also develop Android applications by using this tool based on their requirements. The emergence of AGE dramatically improves the ease of developing essential software and lowers the level of programming skills required for app developers. However, it also provides easy access for attackers to quickly develop a large number of malicious applications, which will seriously affect the device and data security of regular users. Since AGE mainly generates applications based on some boilerplate codes, the code structures of malicious apps created by AGE have a high degree of similarity when these apps belong to the same family. Based on the assumption that the package directory structures of the software from the same family are also similar, we designed a novel feature construction method to describe the application. Using this method, we extracted features from the leaf nodes of the smali tree, while each smali tree corresponds to the smali directory of the application. Unlike traditional static feature extraction of applications, the tree-based feature proposed in this paper can effectively counteract problems such as code obfuscation or reflection cause it can adequately reflect the semantic features of the smali files. To prove the effectiveness of tree-based features, we also conducted some experiments based on a dataset provided by the enterprise. This dataset contains 1792 AGE-generated applications, and these applications belong to 17 malicious families. We demonstrated that the feature construction method proposed in this paper is usable and can be applied to machine learning classification algorithms for the identification of malicious applications.},
keywords={Semantics;Vegetation;Static analysis;Tools;Feature extraction;Malware;Reflection;AGE, Android Malware, tree-based feature, family identification},
doi={10.1109/TrustCom50675.2020.00060},
ISSN={2324-9013},
month={Dec},}
@INPROCEEDINGS{4089351,
author={Blumrich, M. and Chen, D. and -t. Chiu, G. L. and Cipolla, T. and Coteus, P. and Crumley, P. and Gara, A. and Giampapa, M.E. and Hall, S. and Haring, R.A. and Heidelberger, P. and Hoenicke, D. and Kopcsay, G.V. and Liebsch, T.A. and Mok, L. and Ohmacht, M. and Salapura, V. and Swetz, R. and Takken, T. and Vranas, P.},
booktitle={International Workshop on Innovative Architecture for Future Generation High Performance Processors and Systems (IWIA'06)},
title={A Holistic Approach to System Reliability in Blue Gene},
year={2006},
volume={},
number={},
pages={3-12},
abstract={Optimizing supercomputer performance requires a balance between objectives for processor performance, network performance, power delivery and cooling, cost and reliability. In particular, scaling a system to a large number of processors poses challenges for reliability, availability and serviceability. Given the power and thermal constraints of data centers, the BlueGene/L supercomputer has been designed with a focus on maximizing floating point operations per second per Watt (FLOPS/Watt). This results in a drastic reduction in FLOPS/m2 floor space and FLOPS/dollar, allowing for affordable scale-up. The BlueGene/L system has been scaled to a total of 65,536 compute nodes in 64 racks. A system approach was used to minimize power at all levels, from the processor to the cooling plant. A BlueGene/L compute node consists of a single ASIC and associated memory. The ASIC integrates all system functions including processors, the memory subsystem and communication, thereby minimizing chip count, interfaces, and power dissipation. As the number of components increases, even a low failure rate per-component leads to an unacceptable system failure rate. Additional mechanisms have to be deployed to achieve sufficient reliability at the system level. In particular, the data transfer volume in the communication networks of a massively parallel system poses significant challenges on bit error rates and recovery mechanisms in the communication links. Low power dissipation and high performance, along with reliability, availability and serviceability were prime considerations in BlueGene/L hardware architecture, system design, and packaging. A high-performance software stack, consisting of operating system services, compilers, libraries and middleware, completes the system, while enhancing reliability and data integrity},
keywords={Power system reliability;Supercomputers;Cooling;Availability;Application specific integrated circuits;Power dissipation;Telecommunication network reliability;Cost function;Communication networks;Bit error rate},
doi={10.1109/IWIAS.2006.22},
ISSN={1537-3223},
month={Jan},}
@INPROCEEDINGS{9248830,
author={Camargo, Renner Sartório and Mayor, Daniel Santamargarita and Fernandes, Lucas De Mingo and Miguel, Alvar Mayor and Peña, Emilio José Bueno and Encarnação, Lucas Frizera},
booktitle={2020 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe)},
title={Non-Isolated Cascaded Multilevel Converter Based on Three-Phase Cells},
year={2020},
volume={},
number={},
pages={131-135},
abstract={This work proposes a new non-isolated cascaded multilevel converter topology based on three-phase cells, presenting lower construction and implementation cost than the classic multilevel converters topologies. However, this topology presents several short-circuit states when used with classic PWM switching strategies, which can make this structure unfeasible and cause damage to its components. To solve this technical challenge inherent in the proposed structure, an algorithm based on graph theory was developed in order to identify all converter possible states, including the short circuit states. A STATCOM simulation results are presented in the Simulink MATLAB software using the proposed topology based on instantaneous power theory, where an Optimal Switching Vector Model Predictive Control (OSV-MPC) was applied in order to use only the safe switching states, and prove its viability.},
keywords={Multilevel converters;Software packages;Simulation;Switches;Automatic voltage control;Topology;Oscillators;Multilevel Converters;Cascaded H-Bridge;Short-Circuits;Model Predictive Control;STATCOM},
doi={10.1109/ISGT-Europe47291.2020.9248830},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7345362,
author={Banse, Christian and Rangarajan, Sathyanarayanan},
booktitle={2015 IEEE Trustcom/BigDataSE/ISPA},
title={A Secure Northbound Interface for SDN Applications},
year={2015},
volume={1},
number={},
pages={834-839},
abstract={Software-Defined Networking (SDN) promises to introduce flexibility and programmability into networks by offering a northbound interface (NBI) for developers to create SDN applications. However, current designs and implementations have several drawbacks, including the lack of extended security features. In this paper, we present a secure northbound interface, through which an SDN controller can offer network resources, such as statistics, flow information or topology data, via a REST-like API to registered SDN applications. A trust manager ensures that only authenticated and trusted applications can utilize the interface. Furthermore, a permission system allows for fine-grained authorization and access control to the aforementioned resources. We present a prototypical implementation of our interface and developed example applications using our interface, including an SDN management dashboard.},
keywords={Switches;Topology;Network topology;Access control;Protocols;Software-Defined Networking;SDN;network security;northbound interface;trust},
doi={10.1109/Trustcom.2015.454},
ISSN={},
month={Aug},}
@INPROCEEDINGS{1254369,
author={Satish Chandra Gupta and Nguyen, T.N. and Munson, E.V.},
booktitle={Tenth Asia-Pacific Software Engineering Conference, 2003.},
title={The Software Concordance: using a uniform document model to integrate program analysis and hypermedia},
year={2003},
volume={},
number={},
pages={164-173},
abstract={Since the source code is only one of many types of documents that must be maintained as a software system evolves, modern software development could be improved by better interoperability between source code and other software documents. The Software Concordance is a prototype programming environment that uses hypermedia services and a uniform document model to achieve this interoperability. The Software Concordance editor allows developers to enhance their inline documentation with multimedia objects and hyperlinks, while still supporting advanced program analysis including lexing, parsing and type checking. We motivate the need for environments like the Software Concordance, describe the design and implementation of its program editor, and discuss lessons learned while creating it. The system is based on a uniform, Web compatible document model for both program source code and nonprogram software documents and a corresponding API. Important technical problems addressed by this research include creating a persistent representation of program lexemes that the analysis suite considers to be ephemeral (such as keywords), providing a simple incremental parsing system, and embedding multimedia in source code without disrupting program analysis. Based on insight gained in the process of implementing the document API, a new approach to the design of a more suitable program analysis infrastructure is suggested.},
keywords={Software maintenance;Trademarks;System testing;Software systems;Java;Maintenance engineering;Software prototyping;Programming environments;Documentation;Multimedia systems},
doi={10.1109/APSEC.2003.1254369},
ISSN={},
month={Dec},}
@ARTICLE{7862715,
author={},
journal={ISO/IEC/IEEE FDIS P26512_D3, January 2017},
title={ISO/IEC/IEEE Draft International Standard - Systems and Software Engineering -- Requirements for Acquirers and Suppliers of Information for Users},
year={2017},
volume={},
number={},
pages={1-42},
abstract={},
keywords={IEEE Standards;IEC Standards;ISO Standards;Software engineering;System analysis and design},
doi={},
ISSN={},
month={Feb},}
@ARTICLE{6075277,
author={Jian, Lin and Crookes, J G and Pidd, M},
journal={Journal of Systems Engineering and Electronics},
title={Implementing a layered system for discrete computer simulation},
year={1999},
volume={10},
number={1},
pages={62-74},
abstract={A successful simulation still requires the user to have good simulation knowledge and well developed modeling skills despite a large number of simulation software products available to users. This paper presents the design principles and implementation of a layered modeling system known as General-Purpose user-defined Modeling System (GPMS) which provides the user with multiple accesses to build a simulation model at three different levels of knowledge and skills. It does this by purpose-designed GPMS simulation language, which is briefly described in this paper.},
keywords={Computational modeling;Graphical user interfaces;Libraries;Software;Automatic programming;Load modeling;Layered modeling system;Discrete simulation;Simulation software;User interface;Methodology},
doi={},
ISSN={1004-4132},
month={March},}
@INPROCEEDINGS{6004172,
author={Kuusijarvi, Jarkko and Stenudd, Sakari},
booktitle={2011 IEEE/IPSJ International Symposium on Applications and the Internet},
title={Developing Reusable Knowledge Processors for Smart Environments},
year={2011},
volume={},
number={},
pages={286-291},
abstract={Software reuse brings a lot of benefits, from which time and cost savings are the most important. As smart environments grow in number, there will be numerous software developers developing new applications for them. In order to benefit from the earlier verified designs and implementations, reusability has to be considered, both in developing and using reusable software artefacts. This paper discusses software reusability in the context of smart spaces and presents a model for building reusable software agents, which can be utilised by software developers during application development. In addition, example monitors and a monitor manager have been designed and implemented. The case example proved that the presented reusable monitors and monitor manager speed up the development process.},
keywords={Monitoring;Software;Ontologies;Programming;Unified modeling language;Context;Libraries;reusability;software development;component;smart space application},
doi={10.1109/SAINT.2011.55},
ISSN={},
month={July},}
@INPROCEEDINGS{548848,
author={Zhang, M.T. and Watson, R. and Lee, F.C. and Roudet, J. and Schanen, J.-L. and Clavel, E.},
booktitle={PESC Record. 27th Annual IEEE Power Electronics Specialists Conference},
title={Characterization and analysis of electromagnetic interference in a high frequency AC distributed power system},
year={1996},
volume={2},
number={},
pages={1956-1960 vol.2},
abstract={High frequency quasi-square AC distributed power systems simplify the complexity of DC distributed power systems at the expense of greater generated EMI due to high values of dV/dt and dI/dt displayed in the bus waveform characteristics. To quantify EMI effects, the bus structure of a 300 kHz quasi-square waveform AC distributed power system is analyzed using the partial element equivalent circuits (PEEC) method incorporated in the software package InCa. Simulation and experimental results are compared and design guidelines to minimize EMI effects in AC distributed power systems are given.},
keywords={Electromagnetic analysis;Electromagnetic interference;Power system analysis computing;Power system simulation;Frequency;AC generators;DC generators;Character generation;Distributed power generation;Power generation},
doi={10.1109/PESC.1996.548848},
ISSN={0275-9306},
month={June},}
@ARTICLE{7479532,
author={Park, Yongtae and Kuk, Seungho and Kang, Inhye and Kim, Hyogon},
journal={IEEE Transactions on Mobile Computing},
title={Overcoming IoT Language Barriers Using Smartphone SDRs},
year={2017},
volume={16},
number={3},
pages={816-828},
abstract={In the Internet of Things (IoT) era, smartphones are expected to frequently interact with IoT devices and even facilitate various IoT applications. Due to limited roles, energy constraints, etc., however, IoT devices may use mission-tailored or proprietary wireless protocols that smartphones do not speak natively. In this paper, we propose a novel approach to the wireless “language barrier” problem between the smartphones and IoT devices of the future. We first demonstrate that smartphones have become powerful enough to process software defined radio (SDR) for some known wireless protocols. Moreover, we show that the SDRs can be packaged as “apps” and be downloaded from app stores for OS-independent deployment. Second, we show different SDR protocols on the smartphone can concurrently run through a shared RF to serve multi-tasked applications on it as might happen in diversified IoT environments. For proof-of-concept, we implement a prototype architecture that has all the SDR logic and supporting middleware on an Android smartphone which uses a USRP as the simple RF-end. Finally, we demonstrate that IEEE 802.11p and IEEE 802.15.4 SDRs on a smartphone, respectively, communicate with a ZigBee sensor mote, a ZigBee smart lightbulb, and a commercial Wireless Access in Vehicular Environment (WAVE) device, concurrently.},
keywords={Protocols;Wireless communication;Wireless sensor networks;Radio frequency;IEEE 802.11 Standard;Baseband;Software;Internet-of-Things (IoT);software-defined radio (SDR);smartphone;implementation;IEEE 802.11p;IEEE 802.15.4},
doi={10.1109/TMC.2016.2570749},
ISSN={1558-0660},
month={March},}
@INPROCEEDINGS{7500883,
author={Lyke, James and Freden, Jerker and Ahlberg, Mikael and Bruhn, Fredrik and Preble, Jeff},
booktitle={2016 IEEE Aerospace Conference},
title={Architectural framework and toolflow concepts for rapidly composable wireless spacecraft},
year={2016},
volume={},
number={},
pages={1-17},
abstract={This paper provides a glimpse of work being done in a nearly decade-long joint US/Sweden and spacecraft research collaboration exploring rapid spacecraft design based on modular components whose simplicity and composability motivate the exploitation of modern design automation approaches and concepts that have been popularized in consumer and industrial online commerce. Modular systems are engineered to minimize tight couplings between components, with an aim of permitting interchangeability of elements and free composability to form many different possible system designs. Physical wiring often contributes to the complexity, and reducing or eliminating it aids in the objectives of modularity. In this paper, we consider an approach to create modular spacecraft, with a particular emphasis on cube satellites having a 6U form factor, based on a composition of “nearly wireless” elements. Since efficient power delivery remains problematic, our scheme permits the introduction of two terminals (analogous to residential wall outlets) for the sole purpose of power access. All other functions are delivered through a wireless network self organized based on a given collection of components necessary to form a particular spacecraft design. Panel structures can be prewired for power-only distribution, eliminating the need for custom wiring harnesses. In the proposed 6U Cubesat concept, a primary flat surface (~200mm × 300mm) substrate is the basis of a “dinner tray” convention. Modules are added to the pegboard-like “dinner tray” by plugging them in topside, forming a single unified planar interface for electrical, mechanical, and thermal integration. Electrical power blocks energize the substrate, processor modules provide wireless connection access points, and all other modules extract power from the strategically distributed contact points throughout the substrate. Once powered, these modules are networked through a “join and discovery” mechanism which provides a dynamically extensible application programming interface (API) expressed in the form of electronic data sheets. A sophisticated middleware layer (running on the same processing modules that provide the wireless “hotspots”) matches applications using a “brokerage” publish and subscribe mechanism. When the dependencies of each application is satisfied through the existence of suitable modules, the application is activated. The entire application suite is a hierarchy implemented as a direct acyclic graph (DAG) of these dependencies that when satisfied form a viable system (in this case a spacecraft) design. The implications of the method are profound in that it is possible to rapidly develop an virtually infinite variety of system designs given a sufficiently large collection of building block hardware and software applications. This paper describes a pushbutton toolflow (PBTF) motivated by concepts electronic design automation, only they are now extended to encompass satellite (or other system) designs. This pushbutton toolflow involves a configurator concept through which the user could negotiate (with relatively simple wizard like dialogs) a variety of viable spacecraft designs. The tool would access a multi-vendor “electronic store”, where prebuilt components and applications are marshaled into DAGs corresponding to buildable systems. The approach is analogous to a shopping cart metaphor, in which system cost and delivery times can be tracked based on the collection of dependencies formed by the selection of particular components need to satisfy design constraints. Documentation, including bill of materials, data sheets, and a full work breakdown structure can be produced as a byproduct. The toolflow itself could be extended to encompass automatic program generation and three-dimensional printing approaches to permit automated, customizable software and hardware generation (respectively), to complement the catalog of available components. The toolflow has other profound impacts, such as the ability to publish “recipes” (i.e., useful system representations) for use by other users, the ability to automatically coordinate communications (through a “space dialtone” concept) between orbiting platforms in ground station networks, and techniques that arrange for the construction and launch of these configurator produced spacecraft design by cooperating third-party networks.},
keywords={Space vehicles;Satellites;Wireless communication;Substrates;Orbits;Design automation;Wiring},
doi={10.1109/AERO.2016.7500883},
ISSN={},
month={March},}
@INPROCEEDINGS{8341684,
author={Titov, V. G. and Kuzmenkov, A. N. and Onishenko, G. B.},
booktitle={2018 17th International Ural Conference on AC Electric Drives (ACED)},
title={On viability of application of electromagnetic bearings for electric propulsion installations (EPI)},
year={2018},
volume={},
number={},
pages={1-4},
abstract={A three-loop control system of radial electromagnetic bearing (EMB) for electric propulsion installations (EPI) is proposed. In the open state the system is unstable and has the features which must be considered while synthesizing the regulator. The issues of creating a pulse system and different variants of constructing the inner current circuit are considered. The construction of the control system is carried out in accordance with the general principles of building systems of subordinate regulation. To calculate the parameters of regulators, the authors used the method of transfer functions and z-transformation method. It is shown that the best performance in speed and overshoot has a relay regulator operating in a sliding mode. In accordance with the mathematical model of EMB and its corresponding structural diagrams, simulation models are built in the Matlab Simulink software package. As a result of simulating a pulsed electromagnetic bearing control system, transient processes on control and disturbance impacts are obtained. The analysis of transient processes in simulation showed that electromagnetic bearings equipped with a three-loop control system have a high static rigidity and high operating speed.},
keywords={Rotors;Sensors;Transient analysis;Regulators;Electromagnets;electromagnetic bearing;three-loop control system;pulse control system;relay controller;z-transformation method},
doi={10.1109/ACED.2018.8341684},
ISSN={},
month={March},}
@INPROCEEDINGS{6116408,
author={Hahanov, Vladimir and Mischenko, Aleksandr and Chumachenko, Svetlana and Hahanova, Anna and Priymak, Alexey},
booktitle={2011 9th East-West Design & Test Symposium (EWDTS)},
title={Spam diagnosis infrastructure for individual cyberspace},
year={2011},
volume={},
number={},
pages={161-168},
abstract={The theory, methods and the architecture of parallel information's analysis is presented by the form of analytical, graph and table forms of associative relations for the search, recognition, diagnosis of destructive components and the decision making in n-dimensional vector cybernetic individual space. Vector -logical processes-models of actual oriented tasks are considered. They include the diagnostic of spam and the recovery of serviceability, the hardware-software components of computer systems and the decision quality is estimated by the interactions of non-arithmetic metrics of Boolean vectors. The concept of self-development information of computer ecosystem is offered. It repeats the evolution of the functionality of the person. Original processes-models of associative-logical information analysis are represented on the basis of high-speed multiprocessor in n-dimensional vector discrete space.},
keywords={Vectors;Cyberspace;Testing;Computers;Libraries;Electronic mail;Analytical models},
doi={10.1109/EWDTS.2011.6116408},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7881626,
author={Sukhoroslov, Oleg and Volkov, Sergey and Afanasiev, Alexander},
booktitle={2016 IEEE/ACM 9th International Conference on Utility and Cloud Computing (UCC)},
title={Program Autotuning as a Service: Opportunities and Challenges},
year={2016},
volume={},
number={},
pages={148-155},
abstract={Program autotuning is becoming an increasingly valuable tool for improving performance portability across diverse target architectures, exploring trade-offs between several criteria, or meeting quality of service requirements. Recent work on general autotuning frameworks enabled rapid development of domain-specific autotuners reusing common libraries of parameter types and search techniques. In this work we explore the use of such frameworks to develop general-purpose online services for program autotuning using the Software as a Service model. Beyond the common benefits of this model, the proposed approach opens up a number of unique opportunities, such as collecting performance data and utilizing it to improve further runs, or enabling remote online autotuning. However, the proposed autotuning as a service approach also brings in several challenges, such as accessing target systems, dealing with measurement latency, and supporting execution of user-provided code. This paper presents the first step towards implementing the proposed approach and addressing these challenges. We describe an implementation of generic autotuning service that can be used for tuning arbitrary programs on user-provided computing systems. The service is based on OpenTuner autotuning framework and runs on Everest platform that enables rapid development of computational web services. In contrast to OpenTuner, the service doesn't require installation of the framework, allows users to avoid writing code and supports efficient parallel execution of measurement tasks across multiple machines. The performance of the service is evaluated by using it for tuning synthetic and real programs.},
keywords={Cloud computing;Computer architecture;Software as a service;Tuning;program autotuning;software as a service;web services;distributed computing},
doi={},
ISSN={},
month={Dec},}
@INPROCEEDINGS{5278290,
author={Pauly, Tim and Higginbottom, Ian and Pederson, Hugh and Malzone, Chris and Corbett, John and Wilson, Matthew},
booktitle={OCEANS 2009-EUROPE},
title={Keeping pace with technology through the development of an intuitive data fusion, management, analysis & visualization software solution},
year={2009},
volume={},
number={},
pages={1-8},
abstract={A significant challenge facing researchers is the bridging of domains between the natural and physical sciences through the integration, analysis & visualization of multivariate environmental data from a variety of sources. Typically the temporal, visual and analytical components are not well handled within a single software application or by existing tools, and the final products are often difficult to share with collaborating researchers and managers. To address these issues, Myriax has launched Eonfusion; the next generation of geospatial analysis software which provides a state-of-the-art 4D-analysis environment that is intuitive and readily extensible. The software significantly enhances the ease with which scientists can integrate diverse data types (raster, vector and media) and share methods across disciplines. The powerful 4D-visualization interface also incorporates a range of analysis and plotting tools as well as a time-navigation capability that allow users to explore the temporal evolution of spatial structure and resolved relationships. Its revolutionary fusion capability allows data at different temporal or spatial scales to be reconciled, attributes merged and topological relationships identified. The software is customizable, through an integrated coding environment for algorithm implementation and an application programming interface (API), to facilitating the development of modules. It is ideal for both undertaking complex analyses and communicating syntheses. This paper introduces the concepts behind fusion of irregular vector data in Eonfusion. Multiple data sets, including modeled and sampled data, can be fused into a single set with shared coordinates, enabling the discovery of topological relationships between coincident data items. These relationships also allow data attributes to be directly compared and transferred between data sets. A corresponding process also allows raster data attributes to be directly compared with vector data attributes through a raster mapping form of fusion. Both the raster and vector data sets can be visualized together, and the attribute transfer results displayed and used for further analysis. Vector data is introduced to the application as sets of spatial features (e.g. points, lines or polygons). Attributes may vary from vertex to vertex across a spatial feature (vertex attributes, e.g. x, y, z, t), or may be constant across a feature (feature attributes). Fusion typically takes place in a space defined by a subset of common vertex attributes. The implementation is sufficiently general to use any such collection of attributes, for example temperature or mean volume backscatter. A simple yet powerful example is the fusion of video data with camera trajectory data for ground truthing acoustic data. This architecture enables user-positioned "probes" in scenes on the camera trajectory line to locate the corresponding video frames. Along track acoustic integration data can also be fused with the trajectory data, providing tools for seabed classification or combined net/acoustic studies. This example represents the simple application of a powerful set of tools for oceanographic and ecological studies. Eonfusion was developed with the support of leading researchers around the world to meet the demands of the latest marine research tools, technology and methodology. Eonfusion moves the technology horizon far beyond simultaneous 4D visualization of coincident data, to the elegant integration of data via fusion. The ability to fuse data from multiple sources in a versatile 4-dimensional visualization environment provides new opportunities for observation insights into marine ecosystems. It will ultimately yield the analysis methodologies of the future.},
keywords={Software development management;Technology management;Data analysis;Data visualization;Collaborative software;Application software;Cameras;Marine technology;Software tools;Collaborative tools},
doi={10.1109/OCEANSE.2009.5278290},
ISSN={},
month={May},}
@INPROCEEDINGS{6394172,
author={Devitt, Simon and Nemoto, Kae},
booktitle={2012 IEEE 21st Asian Test Symposium},
title={Programming a Topological Quantum Computer},
year={2012},
volume={},
number={},
pages={55-60},
abstract={Topological quantum computing has recently proven itself to be a powerful computational model when constructing viable architectures for large scale computation. The topological model is constructed from the foundation of a error correction code, required to correct for inevitable hardware faults that will exist for a large scale quantum device. It is also a measurement based model of quantum computation, meaning that the quantum hardware is responsible only for the construction of a large, computationally universal quantum state. This quantum state is then strategically consumed, allowing for the realisation of a fully error corrected quantum algorithm. The number of physical qubits needed by the quantum hardware and the amount of time required to implement an algorithm is dictated by the manner in which this universal quantum state is consumed. In this paper we examine the problem of algorithmic optimisation in the topological lattice and introduce the required elements that will be needed when designing a classical software package to compile and implement a large scale algorithm on a topological quantum computer.},
keywords={Lattices;Logic gates;Quantum computing;Hardware;Computational modeling;Computers;Computer architecture;Quantum Computing;Error Correction;Fault-tolerance;Classical Compiler},
doi={10.1109/ATS.2012.52},
ISSN={2377-5386},
month={Nov},}
@ARTICLE{8723039,
author={Dong, Jian and Chen, Bin and Ai, Chuan and Zhang, Pengfei and Qiu, Xiaogang and He, Lingnan},
journal={IEEE Access},
title={Data Driven Spatio-Info Network Modeling and Evolution With Population and Economy},
year={2019},
volume={7},
number={},
pages={77190-77199},
abstract={Spatial interaction is the process that individuals interact with each other at different geographical locations. It attracts much research interests for the increasing data and applications related to spatial interaction. In this paper, a method is proposed to construct the spatio-info network with the dataset from WeChat. The correlation between human factors and statistics characteristics of the network is analyzed and confirmed, and then, the gross domestic product (GDP) and demographics are integrated into gravity model to model the spatio-info network. The likelihood method is used to solving the parameters and evaluates the four models; it is found that the GDP-GDP-distance (GGD) and population-population-distance (PPD) are similar and much better than the other two models. Finally, topological characteristics and community structure of the evolution network are analyzed to evaluate the models. It is found that evolution networks of the two models are almost consistent to origin network, and PPD models are better. It is concluded that the gravity model and human factors can be used to model the spatio-info network. This paper can be used to predict the communication amount of different regions in online social media dynamically. Naturally, this will help the mobile communication infrastructure construction, especially for a new generation of technology, such as 5G, or for regions with poor infrastructure. In addition, it will also help the software service providers configure server and advertising resources.},
keywords={Urban areas;Social networking (online);IP networks;Gravity;Libraries;Data models;Analytical models;Spatio-info network;law of universal gravitation;GDP;demographics},
doi={10.1109/ACCESS.2019.2919256},
ISSN={2169-3536},
month={},}
@ARTICLE{9207972,
author={Papis, Bartosz and Grochowski, Konrad and Subzda, Kamil and Sijko, Kamil},
journal={IEEE Transactions on Software Engineering},
title={Experimental Evaluation of Test-Driven Development With Interns Working on a Real Industrial Project},
year={2022},
volume={48},
number={5},
pages={1644-1664},
abstract={Context: There is still little evidence on differences between Test-Driven Development and Test-Last Development, especially for real-world projects, so their impact on code/test quality is an ongoing research trend. An empirical comparison is presented, with 19 participants working on an industrial project developed for an energy market software company, implementing real-world requirements for one of the company's customers. Objective: Examine the impact of TDD and TLD on quality of the code and the tests. The aim is to evaluate if there is a significant difference in external code quality and test quality between these techniques. Method: The experiment is based on a randomized within-subjects block design, with participants working for three months on the same requirements using different techniques, changed from week to week, within three different competence blocks: Intermediate, Novice and Mixed. The resulting code was verified for process conformance. The participants developed only business logic and were separated from infrastructural concerns. A separate group of code repositories was used to work without unit tests, to verify that the requirements were not too easy for the participants. Also, it was analysed if there is any difference between the code created by shared efforts of developers with different competences and the code created by participants isolated in the competence blocks. The resulting implementations had LOC order of magnitude of 10k. Results: Statistically significant advantage of TDD in terms of external code quality (1.8 fewer bugs) and test quality (5 percentage points higher) than TLD. Additionally, TDD narrows the gap in code coverage between developers from different competence blocks. At the same time, TDD proved to have a considerable entry barrier and was hard to follow strictly, especially by Novices. Still, no significant difference w.r.t. code coverage has been observed between the Intermediate and the Novice developers - as opposed to TLD, which was easier to follow. Lastly, isolating the Intermediate developers from the Novices had significant impact on the code quality. Conclusion:TDD is a recommended technique for software projects with a long horizon or when it is critical to minimize the number of bugs and achieve high code coverage.},
keywords={Software;Companies;Testing;Writing;Programming;Market research;Empirical software engineering;iterative test last development;Test driven development},
doi={10.1109/TSE.2020.3027522},
ISSN={1939-3520},
month={May},}
@INPROCEEDINGS{9337036,
author={Thakre, M. P. and Sayali, S. Shinde and M, Jain A.},
booktitle={2020 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)},
title={Stability and Total Harmonic Distortion Analysis with Performance of Grid-Tied PV Systems},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The stability and Total Harmonic Distortion (THD) of the grid integration photovoltaic (PV) array was discussed and analyzed in this article using two separate maximum power point (MPPT) tracking algorithms. A PV array access to the grid by means of a VSI and a boost converter were applied in this work. The structures in the PSCAD environment were simulated, including MPPT methodologies for Incremental Conductivity (InC) and Perturbation & Observation (P&O). Software package is used in both normal and temporary environments to evaluate grid disorders. The results indicate that the InC MPPT procedure provides excellent response in normal condition than those of the P&O MPPT control system. That being said, in both temporary conditions the P&O MPPT algorithm offers more recovery time than the InC MPPT algorithm. The entire model postulates the architecture of a hybrid MPPT algorithm in such a way that it operates efficiently under both temporary and normal conditions. Power converter and inverter use both PWM and SPWM switching methodologies during this whole approach. Finally, an analysis of the overall THD of the PCC output current of the inverter is being used, compared with the limits of the THD values obtained by regulatory standards such as IEEE Std 519-1992.},
keywords={Total harmonic distortion;Perturbation methods;Stability criteria;Software algorithms;Power system stability;Transient analysis;Standards;PV system;Stability;distributed generation;total harmonic distortion;MPPT;Smart Grid (SG)},
doi={10.1109/ICPECTS49113.2020.9337036},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9460305,
author={Quang, Le Hong and Putov, V.V. and Sheludko, V.N. and Kuznetsov, A.A. and Chernyshev, M.A.},
booktitle={2021 10th Mediterranean Conference on Embedded Computing (MECO)},
title={Adaptive Robust Control of an Uncertain Multi-Degree-of-Freedom Elastically Deformable Electromechanical Plant with Adaptive Compensation for an unknown Disturbance},
year={2021},
volume={},
number={},
pages={1-6},
abstract={This paper considers the construction of adaptive robust control systems based on the state of multi-degree-of- freedom elastically deformable electromechanical plant with uncertain parameters and unknown deterministic disturbance considered as the output of a linear autonomous finite- dimensional model with unknown constant parameters. Within the framework of adaptive compensation of disturbances, an observer of external disturbances is constructed that is operational under conditions of parametric uncertainty of the plant. An adaptive robust control with an adaptive disturbance observer is developed with a simplified method of adaptive integrator backstepping, cumbersome calculations of analytical expressions for "net" derivatives of virtual controls are replaced at every step with their filtered alternatives, and robust algorithms for adjusting the parameters of adaptive control are regularized with the parameter projection method. The results of digital implementation of the constructed systems of adaptive robust control and adaptive compensation of disturbances done using authorized software MATLAB / Simulink are presented.},
keywords={Robust control;Adaptive systems;Backstepping;Uncertainty;Software packages;Software algorithms;Filtering algorithms;multi-degree-of-freedom elastically deformable electromechanical plant;adaptive robust control;block cascade structure;adaptive integrator backstepping method with setting functions;parameter projection algorithms;filtered alternatives of "net" derivatives of virtual (step-by-step) controls;adaptive compensation of unknown disturbance},
doi={10.1109/MECO52532.2021.9460305},
ISSN={2637-9511},
month={June},}
@ARTICLE{5598553,
author={Cole, Stijn and Belmans, Ronnie},
journal={IEEE Transactions on Power Systems},
title={MatDyn, A New Matlab-Based Toolbox for Power System Dynamic Simulation},
year={2011},
volume={26},
number={3},
pages={1129-1136},
abstract={In this paper, we present a new Matlab-based toolbox for power system analysis, called MatDyn. It is open-source software, and available for everyone to download. Its design philosophy is based on the well-known open-source Matlab toolbox MATPOWER, but its focus is transient stability analysis and time-domain simulation of power systems, instead of steady-state calculations. MatDyn's philosophy, design criteria, program structure, and implementation are discussed in detail. A trade-off is achieved between the flexibility of the program and readability of the code. MatDyn retains overall flexibility by, for instance, allowing user defined models, and custom integration methods. The software is validated by comparing its results with those obtained by the commercial grade power system analysis package, PSS/E. Despite the fact that MatDyn is fairly new, it has already been extensively used in research and education. This paper reports interesting results obtained with MatDyn in recent research that would be hard to obtain using commercial software.},
keywords={Mathematical model;Power system dynamics;Equations;Software;Generators;Differential equations;Power system dynamic stability;power system modeling;power system simulation;software},
doi={10.1109/TPWRS.2010.2071888},
ISSN={1558-0679},
month={Aug},}
@INPROCEEDINGS{8921364,
author={Xu, Qing and Xiong, Xin and Feng, Guo-Li and Guo, Ming-Jing and Wan, Luan},
booktitle={2019 International Conference on Virtual Reality and Intelligent Systems (ICVRIS)},
title={Design of Intelligent Campus Multimedia Interactive System Based on Internet of Things Technology},
year={2019},
volume={},
number={},
pages={223-226},
abstract={In order to perfect the construction of intelligent campus multimedia interactive information network and improve the ability of campus multimedia interaction and intelligent education information management, a design scheme of intelligent campus multimedia interactive system based on Internet of things technology is proposed. The system development environment is based on Multigen Creator3.2 and embedded PCI bus development environment, and the intelligent campus multimedia interactive infrastructure is constructed by using Internet of things and radio frequency identification (RFID) construction scheme. The virtual scene simulation and multimedia information interaction design of multimedia information platform, intelligent classroom, library interactive platform and student information management system are realized on the intelligent multimedia information processing terminal to meet the needs of multimedia interaction in intelligent campus. The hardware structure and software development of the intelligent campus multimedia interactive system are carried out. the test results show that the system has high information integration and strong multimedia information processing ability. The artificial intelligence is good, the system is stable and reliable.},
keywords={Interactive systems;Internet of Things;Multimedia communication;Information processing;Radiofrequency identification;Hardware;Internet of things;intelligent campus;multimedia interactive system;bus;network protocol},
doi={10.1109/ICVRIS.2019.00062},
ISSN={},
month={Sep.},}
@ARTICLE{9266587,
author={Wan, Jiafu and Li, Xiaomin and Dai, Hong-Ning and Kusiak, Andrew and Martínez-García, Miguel and Li, Di},
journal={Proceedings of the IEEE},
title={Artificial-Intelligence-Driven Customized Manufacturing Factory: Key Technologies, Applications, and Challenges},
year={2021},
volume={109},
number={4},
pages={377-398},
abstract={The traditional production paradigm of large batch production does not offer flexibility toward satisfying the requirements of individual customers. A new generation of smart factories is expected to support new multivariety and small-batch customized production modes. For this, artificial intelligence (AI) is enabling higher value-added manufacturing by accelerating the integration of manufacturing and information communication technologies, including computing, communication, and control. The characteristics of a customized smart factory are: self-perception, operations optimization, dynamic reconfiguration, and intelligent decision-making. The AI technologies will allow manufacturing systems to perceive the environment, adapt to the external needs, and extract the process knowledge, including business models, such as intelligent production, networked collaboration, and extended service models. This article focuses on the implementation of AI in customized manufacturing (CM). The architecture of an AI-driven customized smart factory is presented. Details of intelligent manufacturing devices, intelligent information interaction, and construction of a flexible manufacturing line are showcased. The state-of-the-art AI technologies of potential use in CM, that is, machine learning, multiagent systems, Internet of Things, big data, and cloud-edge computing, are surveyed. The AI-enabled technologies in a customized smart factory are validated with a case study of customized packaging. The experimental results have demonstrated that the AI-assisted CM offers the possibility of higher production flexibility and efficiency. Challenges and solutions related to AI in CM are also discussed.},
keywords={Artificial intelligence;Manufacturing;Smart manufacturing;Adaptation models;Production facilities;Heuristic algorithms;Computational modeling;Multi-agent systems;Collaboration;Decision making;Machine learning;Software defined networking;Artificial intelligence (AI);customized manufacturing (CM);Industry 4.0;smart factory;software-defined network},
doi={10.1109/JPROC.2020.3034808},
ISSN={1558-2256},
month={April},}
@INPROCEEDINGS{6385139,
author={Romano, Daniele and Raila, Paulius and Pinzger, Martin and Khomh, Foutse},
booktitle={2012 19th Working Conference on Reverse Engineering},
title={Analyzing the Impact of Antipatterns on Change-Proneness Using Fine-Grained Source Code Changes},
year={2012},
volume={},
number={},
pages={437-446},
abstract={Antipatterns are poor solutions to design and implementation problems which are claimed to make object oriented systems hard to maintain. Our recent studies showed that classes with antipatterns change more frequently than classes without antipatterns. In this paper, we detail these analyses by taking into account fine-grained source code changes (SCC) extracted from 16 Java open source systems. In particular we investigate: whether classes with antipatterns are more change-prone (in terms of SCC) than classes without, (2) whether the type of antipattern impacts the change-proneness of Java classes, and (3) whether certain types of changes are performed more frequently in classes affected by a certain antipattern. Our results show that: 1) the number of SCC performed in classes affected by antipatterns is statistically greater than the number of SCC performed in classes with no antipattern, 2) classes participating in the three antipatterns Complex Class, Spaghetti Code, and SwissArmyKnife are more change-prone than classes affected by other antipatterns, and 3) certain types of changes are more likely to be performed in classes affected by certain antipatterns, such as API changes are likely to be performed in classes affected by the Complex Class, Spaghetti Code, and SwissArmyKnife antipatterns.},
keywords={Java;Software systems;Algorithm design and analysis;Complexity theory;Software engineering;Educational institutions;Antipatterns;change-proneness;fine-grained source code changes;empirical software engineering},
doi={10.1109/WCRE.2012.53},
ISSN={2375-5369},
month={Oct},}
@ARTICLE{6649990,
author={Kim, Taeho and Hwang, Kyu-Serk and Oh, Moo-Hwan and Jang, Duck-Jong},
journal={IEEE Journal of Oceanic Engineering},
title={Development of an Autonomous Submersible Fish Cage System},
year={2014},
volume={39},
number={4},
pages={702-712},
abstract={A novel and fully automatic rigid fish cage system has recently been developed for deployment in the waters of Korea. The cage structure has 12 sides, incorporating a steel framework with a diameter and depth of 5.92 and 2.91 m, respectively. Attached to the steel framework is a housing for motor valves controlling variable ballast tanks, eight housings for two air compressors, a main control system, four batteries, a reserve air tank, four high air pressure tanks, 12 variable ballast tanks, and a seawater pump housing. The net of the fish cage is tightened across the frame to minimize volume reduction due to currents. The cage is outfitted with a control station located above the valve housing. With the control system, the buoyancy can be adjusted by utilizing compressed air stored in the four high air pressure tanks. The mechanical components of the ballast systems are operated by automated software that incorporates control and monitoring algorithms. The software initiates control of the ballasting components so the fish cage system can submerge if a preselected sea state occurs. The automatic control station incorporates a wind gauge, wireless communication printed circuit boards (PCBs), and a transmitting antenna. During operation, it monitors the wind speed, so the cage can be submerged before extreme sea states and then surfaced after the weather has passed and the conditions are considered safe. The control station also regulates the flow of air and seawater to and from the variable ballast tanks in response to the surface environmental conditions. Control can also be done remotely by a facility operator. In the development process, in situ tests were conducted to assess the performance of the submersion mechanism and the reliability of the automatic control system with a 1/4 size fish cage of similar construction. During the tests, the vertical position and inclination of the fish cage in the water column were measured. During the tests, the close/open states of the motor valves that control the 12 variable ballast tanks were also assessed during descent and ascent operations. The successful performance of the 1/4 size fish cage during the tests showed promise that such a system could possibly be used on a much larger scale to avoid the rigors of the environment in support of commercial level offshore aquaculture.},
keywords={Electronic ballasts;Underwater vehicles;Control systems;Wind speed;Offshore installations;Aquaculture;Aquaculture;automatic submersion;compressed air operation;fish cage},
doi={10.1109/JOE.2013.2276707},
ISSN={1558-1691},
month={Oct},}
@INPROCEEDINGS{6843740,
author={Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
booktitle={2014 ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS)},
title={WiP abstract: System-level integration of mobile multi-modal multi-sensor systems},
year={2014},
volume={},
number={},
pages={227-227},
abstract={Heterogeneous roaming sensor systems have gained significant importance in many domains of civil infrastructure performance inspection as they accelerate data collection and analysis. However, designing such systems is challenging due to the immense complexity in the heterogeneity and processing demands of the involved sensors. Unifying frameworks are needed to simplify development, deployment and operation of roaming sensors and computing units. To address the sensing needs, we propose SIROM3, a Scalable Intelligent Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for infrastructure performance monitoring to address the following challenges: 1. Scalability and expandability. It offers a scalable and expandable solution enabling diversity in sensing and the growth in processing platforms from sensors to control centers. 2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially distributed sensors. 3. Big data handling. Automatic collection, categorization, storage and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3 minimizes human interaction through full automation from data acquisition to visualization of the fused results. Illustrated in Fig. 1, SIROM3 realizes scalability and expandability in a system-level design approach encapsulating common functionality across hierarchical components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm defining services in both software and hardware architectures. Equipped with multiple RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS) operate as mobile agents attached to vehicles to provide distributed computing services regulated by Fleet Control and Management (FCM) center via communication network. A series of foundational services including the Precise Timing Protocol (PTP), GPS timing systems, Distance Measurement Instruments (DMI) through middleware services (CORBA) embedded in the RTE build the fusion foundations for data correlation and analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous data stream collected by versatile sensors. A GIS visualization module is integrated for visual analysis and monitoring. SIROM3 enables coordination and collaboration across sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy (i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop new algorithms and methodologies expanding CPS principles to civil infrastructure performance monitoring. In result, SIROM3 simplifies the development, construction and operation of roaming multi-modal multi-sensor systems. We demonstrate the efficiency of SIROM3 by automating the assessment of road surface conditions at the city scale. We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones, GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected, aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of C++ code for system integration. SIROM3 offers a unified solution for comprehensive roadway assessment and evaluation. The integrated management of big data (from collection to automated processing) is an ideal research platform for automated assessment of civil infrastructure performance.},
keywords={Data visualization;Vehicles;Monitoring;Big data;Abstracts;Sensor systems},
doi={10.1109/ICCPS.2014.6843740},
ISSN={},
month={April},}
@INPROCEEDINGS{8908950,
author={Wang, Li and Zhang, Lili and Zhang, Lingyu and Li, Min and Zhang, Haibo and Li, Kailong and Xiu, Weijie},
booktitle={2019 IEEE 8th Data Driven Control and Learning Systems Conference (DDCLS)},
title={On-line Simulation System of Urban Road Traffic Signal Control Based on Scene Driven},
year={2019},
volume={},
number={},
pages={1213-1218},
abstract={In this paper we present a scene-driven urban road traffic control online simulation system called SD-UTCS. With this system, managers and researchers can realize the construction and simulation of various types of traffic scenarios, the rapid development and optimization of new control strategies, and the application of effective control strategies to actual traffic management. Firstly, the parallel system concept is adopted to realize the deep integration of the real-time traffic control system and the real-time simulation system, and the visual hardware and software in the loop system is used for display. Secondly, the virtual scene base engine and the strategy agent engine are put forward in the system design. By designing a common control strategy API to achieve data and control strategy separation, help researchers focus on the control algorithm itself without paying attention to the detection data and basic data, and use the detection data and GIS data to complete the automatic calibration of simulation parameters to ensure traffic simulation accuracy. Finally, in order to verify the real-time and stability of the simulation system, simulations were carried out using different scale road networks in Shunyi District of Beijing and Weifang City of Shandong Province, which proved the advantages of the simulation subsystem in performance and simulation scale.},
keywords={Computational modeling;Real-time systems;Software;Roads;Cloud computing;Control systems;Hardware;traffic engineering;scene driven;traffic control;traffic simulation;hardware and software in the loop},
doi={10.1109/DDCLS.2019.8908950},
ISSN={},
month={May},}
@INPROCEEDINGS{1425870,
author={Lagin, L. and Bryant, R. and Carey, R. and Casavant, D. and Demaret, R. and Edwards, O. and Ferguson, W. and Krammen, J. and Larson, D. and Lee, A. and Ludwigsen, P. and Miller, M. and Moses, E. and Nyholm, R. and Reed, R. and Shelton, R. and Van Arsdall, P.J. and Wuest, C.},
booktitle={20th IEEE/NPSS Symposium onFusion Engineering, 2003.},
title={Status of the National Ignition Facility integrated computer control system},
year={2003},
volume={},
number={},
pages={27-30},
abstract={The National Ignition Facility (NIF), currently under construction at the Lawrence Livermore National Laboratory, is a stadium-sized facility containing a 192-beam, 1.8-megajoule, 500-terawatt, ultraviolet laser system together with a 10-meter diameter target chamber with room for nearly 100 experimental diagnostics. When completed, NIF will be the world's largest and most energetic laser experimental system, providing an international center to study inertial confinement fusion and the physics of matter at extreme energy densities and pressures. NIF's 192 energetic laser beams will compress fusion targets to conditions required for thermonuclear burn, liberating more energy than required to initiate the fusion reactions. Laser hardware is modularized into line replaceable units such as deformable mirrors, amplifiers, and multi-function sensor packages that are operated by the integrated computer control system (ICCS). ICCS is a layered architecture of 300 front-end processors attached to nearly 60,000 control points and coordinated by supervisor subsystems in the main control room. The functional subsystems beam control including automatic beam alignment and wavefront correction, laser pulse generation and pre-amplification, diagnostics, pulse power, and timing-implement automated shot control, archive data, and support the actions of fourteen operators at graphic consoles. Object-oriented software development uses a mixed language environment of Ada (for functional controls) and Java (for user interface and database backend). The ICCS distributed software framework uses CORBA to communicate between languages and processors. ICCS software is approximately 3/4 complete with over 750 thousand source lines of code having undergone off-line verification tests and deployed to the facility. NIF has entered the first phases of its laser commissioning program. NIF has now demonstrated the highest energy 1/spl omega/, 2/spl omega/, and 3/spl omega/ beamlines in the world. NIF's target experimental systems are also being installed in preparation for experiments to begin in late 2003. This talk will provide a detailed look at the status of the control system.},
keywords={Ignition;Control systems;Laser fusion;Laser theory;Automatic control;Laser beams;Structural beams;Optical control;Automatic generation control;Laboratories},
doi={10.1109/FUSION.2003.1425870},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9418117,
author={S, Rahul and Gupta, Navneet},
booktitle={2021 6th International Conference for Convergence in Technology (I2CT)},
title={Electromagnetic Modeling of Microstrip Patch Antennas: A Neural Network Approach},
year={2021},
volume={},
number={},
pages={1-5},
abstract={This paper aims to elucidate a design procedure that can help solve both the synthesis and analysis parts of an electromagnetic microstrip patch antenna problem. Unlike numerical conventional modeling methods or maybe even analytical methods, artificial neural network are way faster and easier to perform due to their robust correlation and high accuracy. They are also computationally inexpensive when compared to tradition analytical and conventional methods alike. Due to their brevity and highly flexible nature, they are easy to train and work on and can be expanded or modelled in different ways as per the use case owing to their plasticity. Conventional methods are not only time consuming put also require high computational capacity unlike the neural network perspective which has been proposed here. Hence, this paper gives a brief overview of the same and also includes a brief background and basic working of artificial neural networks, activation functions, Radial Basis functions, etc. from both the analysis and synthesis perspective from a RF designers viewpoint. Unlike previous soft computing neural network approaches like back propagation and regression techniques to achieve the same purpose, this paper uses a custom designed Radial Basis Network due to its proven history of computational inexpensiveness and high accuracy and validates its accuracy through the results shown. Circuit design and yield optimizations using a method of moments(MoM) based electromagnetic simulation software was also done to verify accuracy and to feed the ground truth to the network. This neural network was implemented in python due to its vast resource library and ease of implementation.},
keywords={Radio frequency;Analytical models;Computational modeling;Patch antennas;Software algorithms;Microstrip antennas;Artificial neural networks;Radial Basis Function(RBF);Artificial Neural Networks;Microstrip Patch Antenna;Computational Electromagnetics;Python;Optimization;Simulation;Method of moments based EM Simulation software},
doi={10.1109/I2CT51068.2021.9418117},
ISSN={},
month={April},}
@INPROCEEDINGS{7190519,
author={Jansson, Kim and Karvonen, Iris and Kettunen, Outi and Ollus, Martin and Galbusera, Chiara and Perini, Stefano and Tavola, Giacomo and Lampathaki, Fenareti and Panopoulos, Dimitrios},
booktitle={2015 IST-Africa Conference},
title={Assessing benefits and impact creation in IST research projects},
year={2015},
volume={},
number={},
pages={1-9},
abstract={This paper describes novel methodologies for assessing benefits and impact creation in IST related research projects. The paper first includes a Verification & Validation (V&V) method developed for evaluating of Future Internet software components implementation in a multi-sectorial environment. The proposed approach is based on best of breed V&V methodologies characterized by a structured assessment of the software quality at different levels and a reduced set of business indicators able to evaluate the business impact of the integrated solution. All of these are deployed via a configurable V&V Package based on a web interface. Second it includes a methodology for maximising post project impact creating through activity monitoring. The method used is based on impact waves and success tree approaches. Examples are given of practical implementation in the EU FP7, FI-PPP Project FITMAN. At the end of the paper the practical experiences and lessons learned from our work are also described.},
keywords={Business;Monitoring;Software;Manufacturing;Industries;Electronic mail;Economics;Impact creation;Software engineering;Verification and Validation;Manufacturing},
doi={10.1109/ISTAFRICA.2015.7190519},
ISSN={},
month={May},}
@ARTICLE{84218,
author={Couvillion, J.A. and Freire, R. and Johnson, R. and Obal, W.D. and Qureshi, M.A. and Rai, M. and Sanders, W.H. and Tvedt, J.E.},
journal={IEEE Software},
title={Performability modeling with UltraSAN},
year={1991},
volume={8},
number={5},
pages={69-80},
abstract={The utility of stochastic activity networks (SANs) for performability evaluation is discussed. UltraSAN, a new graphical, X-Windows-based software package that uses SANs, is described. UltraSAN incorporates three innovations: a class of SAN-level performability variables common to both analytical and simulation solution methods, methods that use the performability-variable choice and the SAN structure to greatly reduce the size of the stochastic process required for an analytical solution, and methods that use the performability-variable choice and the SAN structure to reduce the number of activities checked on each state change, thus speeding the simulation. The UltraSAN modeling framework, organization, and user interface are examined. Model construction and solution are described.<>},
keywords={Stochastic processes;Stochastic systems;Petri nets;Performance analysis;Performance evaluation;Time measurement;Analytical models;Storage area networks;Predictive models;System testing},
doi={10.1109/52.84218},
ISSN={1937-4194},
month={Sep.},}
@ARTICLE{9193994,
author={Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
journal={IEEE Transactions on Network and Service Management},
title={A Decentralized Framework for Serverless Edge Computing in the Internet of Things},
year={2021},
volume={18},
number={2},
pages={2166-2180},
abstract={Serverless computing is becoming widely adopted among cloud providers, thus making increasingly popular the Function-as-a-Service (FaaS) programming model, where the developers realize services by packaging sequences of stateless function calls. The current technologies are very well suited to data centers, but cannot provide equally good performance in decentralized environments, such as edge computing systems, which are expected to be typical for Internet of Things (IoT) applications. In this article, we fill this gap by proposing a framework for efficient dispatching of stateless tasks to in-network executors so as to minimize the response times while exhibiting short- and long-term fairness, also leveraging information from a virtualized network infrastructure when available. Our solution is shown to be simple enough to be installed on devices with limited computational capabilities, such as IoT gateways, especially when using a hierarchical forwarding extension. We evaluate the proposed platform by means of extensive emulation experiments with a prototype implementation in realistic conditions. The results show that it is able to smoothly adapt to the mobility of clients and to the variations of their service request patterns, while coping promptly with network congestion.},
keywords={Task analysis;Edge computing;Computer architecture;Peer-to-peer computing;Internet of Things;Computational modeling;Data centers;Internet of Things services;software-defined networking;overlay networks;computer simulation experiments},
doi={10.1109/TNSM.2020.3023305},
ISSN={1932-4537},
month={June},}
@INPROCEEDINGS{8827813,
author={Griesser, Andreas and Van Gool, Luc},
booktitle={IEE International Conference on Visual Information Engineering (VIE 2005)},
title={RTSyncNet — A flexible real-time synchronisation network for cluster based vision- and graphics-architectures},
year={2005},
volume={},
number={},
pages={1-8},
abstract={Cluster-based architectures are very popular in the construction of versatile computer vision and graphics applications. Hereby, computers are connected over a network to perform collaborative processing. Systems which include cameras demand for accurate synchronisation as well as low latencies for short-message data transfers. We present work which combines the requests for low-latency, low-cost synchronisation within a multi-camera-projector system for rapid 3D-scanning of human bodies under motion. Key to the setup is a flexible interface controller connected to each network computer. Data packages as well as trigger stimuli and graphics synchronisation signals are quickly distributed to the addressed client machines. A real-time software framework allows for very low latencies that are needed for high-speed parallel processing.},
keywords={Synchronization;Cameras;Graphics;Real-time systems;Linux;Three-dimensional displays;Switches;Low-Latency;Synchronisation Network;Multi-Camera-Projector;Hard Real-Time;Linux Cluster},
doi={10.1049/cp:20050088},
ISSN={},
month={April},}
@INBOOK{8343601,
author={Galin, Daniel},
booktitle={Software Quality: Concepts and Practice},
title={Software Testing},
year={2018},
volume={},
number={},
pages={255-317},
abstract={This chapter focuses on the operative characteristics of software testing. The software testing process includes determining software testing strategies and requirement‐driven software testing. The chapter explains the designing and implementation of the testing process. Automated test management software packages provide features applicable for manual testing together with automated testing. Shortened test duration, cost savings and improvements in test management and control processes are the factors that have motivated the development of test automation tools. An analysis of the characteristics of alpha and beta site tests reveals that in no case they should replace the formal software tests performed by the developer. Specialized software packages for code reviews and document reviews can perform a portion of the qualification tests by listing cases of nonconformity to coding standards, procedures, and work instructions.},
keywords={Software testing;Encoding;Quality assurance;Software systems;Software quality},
doi={10.1002/9781119134527.ch14},
ISSN={},
publisher={IEEE},
isbn={9781119134503},
url={https://ieeexplore.ieee.org/document/8343601},}
@INPROCEEDINGS{6080776,
author={Butler, Simon and Wermelinger, Michel and Yu, Yijun and Sharp, Helen},
booktitle={2011 27th IEEE International Conference on Software Maintenance (ICSM)},
title={Mining java class naming conventions},
year={2011},
volume={},
number={},
pages={93-102},
abstract={Class names represent the concepts implemented in object-oriented source code and are key elements in program comprehension and, thus, software maintenance. Programming conventions often state that class names should be noun-phrases, but there is little further guidance for developers on the composition of class names. Other researchers have observed that the majority of Java class identifier names are composed of one or more nouns preceded, optionally, by one or more adjectives. However, no detailed analysis of class identifier name structure has been undertaken that could be leveraged to support program comprehension activities. We investigate the lexical and syntactic composition of Java class identifier names in two ways. Firstly, as others have done for C function and Java method names, we identify conventional patterns found in the use of parts of speech. Secondly, we identify the origin of words used in class names within the name of any super class and implemented interfaces to identify patterns of class name construction related to inheritance. Through the analysis of 120,000 unique class names found in 60 open source projects we identify both common and project specific class naming conventions. We apply this knowledge in a case study of the mind-mapping tool Freemind to investigate whether class names that follow unconventional naming schemes are candidates for refactoring either a name refactoring that conforms to established naming conventions within the code base, or refactoring of the class that results in conventionally named classes.},
keywords={Java;Accuracy;Tagging;Programming;Software;Speech;Libraries;Java class naming;identifier names},
doi={10.1109/ICSM.2011.6080776},
ISSN={1063-6773},
month={Sep.},}
@INPROCEEDINGS{6846453,
author={Agarwal, Dinesh and Karamati, Sara and Puri, Satish and Prasad, Sushil K.},
booktitle={2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
title={Towards an MPI-Like Framework for the Azure Cloud Platform},
year={2014},
volume={},
number={},
pages={176-185},
abstract={Message Passing Interface (MPI) has been the predominant standardized system for writing parallel and distributed applications. However, while MPI has been the software system of choice for traditional parallel and distributed computing platforms such as large compute clusters and Grid, MPI is not the system of choice for cloud platforms. The primary reasons for this is the lack of low latency high bandwidth network capabilities of the cloud platforms and the inherent architectural differences from traditional compute clusters. Prior studies suggest that the message latency of cloud platforms could be as much as 35x slower than that of an infiniband-connected cluster [1] for popular MPI implementations. MPI-like environment on cloud platforms is desirable for a large class of applications that run for long time spans with varying computing needs, such as the modeling and analysis to predict swath of a hurricane. Such applications could benefit from cloud's resiliency and on-demand access for a robust and green solution. Interestingly, most of the cloud vendors provide APIs to access cloud resources in an efficient manner different than how an MPI implementation would avail of those resources. We have done extensive research to identify the pain-points for designing and implementing an MPI-like framework for cloud platforms. Our research has provided us with vital guidelines that we are sharing in this paper. We present the details of the key components required for such a framework along with our experience while implementing a preliminary MPI-like framework over Azure dubbed cloud MPI and evaluate its pros and cons. A large GIS application has been ported over cloud MPI to study its effectiveness and limitations.},
keywords={Cloud computing;Receivers;Synchronization;Standards;Communities;Buffer storage;Computer architecture;MPI over cloud;Polygonal overlay processing in GIS;Programming productivity;Porting MPI code to Azure cloud},
doi={10.1109/CCGrid.2014.100},
ISSN={},
month={May},}
@ARTICLE{804527,
author={Tilbury, D.M. and Messner, W.C.},
journal={IEEE Transactions on Education},
title={Control tutorials for software instruction over the World Wide Web},
year={1999},
volume={42},
number={4},
pages={237-246},
abstract={The World Wide Web (WWW) has given educators unprecedented opportunities to provide information to students both within their own classes and around the globe. This paper describes the development and implementation of a set of Web-based tutorials for MATLAB, a popular computation and visualization software package. The potential impact of the WWW on pedagogical objectives is discussed, and the tutorials are presented as a case study of Web-based software instruction. Their structure and implementation are described, and feedback and statistics from the first academic year of use are shown. The authors also describe their efforts at disseminating information on the tutorials to increase their use. The paper concludes with a discussion of future work planned for the tutorials and potential future applications of the concept of Web-based tutorials.},
keywords={Courseware},
doi={10.1109/13.804527},
ISSN={1557-9638},
month={Nov},}
@INPROCEEDINGS{4178956,
author={Waltenberger, W. and Moser, F.},
booktitle={2006 IEEE Nuclear Science Symposium Conference Record},
title={RAVE - an Open, Extensible, Detector-Independent Toolkit for Reconstruction of Interaction Vertices},
year={2006},
volume={1},
number={},
pages={104-109},
abstract={A toolkit is presented that takes a set of a particle detector's reconstructed tracks as its input and reconstructs interaction vertices as its output. It deals both with finding (pattern recognition) and with fitting (statistical estimation) of the interaction vertices. Its main design goals are ease of use, high integratability in existing software projects, extensibility, and generality. To this end the API is defined in a simple way. The various algorithms available in the toolkit, and optionally their parameter settings, are referenced by a simple string. This guarantees that the user code decouples completely from the toolkit internals. Also, maintaining backward compatibility should become a trivial task. The Rave Toolkit is complemented by a simple standalone framework, called "Vertigo". Implementation, verification, and performance analysis of reconstruction algorithms should thus be possible in a very fast and straightforward manner. Rave has its roots in the CMS vertex reconstruction community. The current algorithmic parts of the toolkit are source code compatible with the original CMS software, but contributions from "outside" are highly welcome. The toolkit is written in C++, but interfaces for other languages (Java, Python) have been implemented.},
keywords={Pattern recognition;Collision mitigation;Software tools;Java;Magnetic materials;Nuclear and plasma sciences;Particle tracking;Performance analysis;Reconstruction algorithms;Software algorithms;core software tools;reconstruction},
doi={10.1109/NSSMIC.2006.356117},
ISSN={1082-3654},
month={Oct},}
@ARTICLE{7290159,
author={},
journal={RDD 15:2007},
title={RDD 15:2007 - SMPTE Registered Disclosure Doc - Software Scripting Language for Pixel-Based Color Transformations},
year={2007},
volume={},
number={},
pages={1-63},
abstract={This document describes the Color Transformation Language, or CTL, and its reference implementation in the form of a CTL interpreter. CTL is a small software programming language that has been designed to serve as a building block for digital color management systems. CTL allows users to describe color transforms in a concise and unambiguous way by expressing them as software programs. Any digital color management system that supports CTL includes a CTL interpreter. In order to apply a given transform to an image, the color management system instructs the interpreter to load and run the CTL program that describes the transform. The original and the transformed image constitute the CTL program's input and output. — This document describes the syntax and semantics of CTL, a standard library of functions that can be called from within CTL programs, as well as a C++ programming interface that allows application programs to access and control the CTL interpreter. — The RDD itself is comprised of this document and the reference implementation of the CTL interpreter as described in Annex B. All other referenced documents, sample code and software utilities are provided for informative purposes only.},
keywords={Color;Transform},
doi={10.5594/SMPTE.RDD15.2007},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7542990,
author={Dimova, Tatyana and Marinova, Mariya and Aprahamian, Bohos},
booktitle={2016 19th International Symposium on Electrical Apparatus and Technologies (SIELA)},
title={Assessment of the influence of the magnetic filter type on the magnetic field of a separator type MCR-5},
year={2016},
volume={},
number={},
pages={1-4},
abstract={The magnetic separation is a process of great practical relevance. The specifics of the separated material determine the type of the used separator. The aim is to achieve the maximum degree of purification of the processed product. The article examines a construction of real magnetic separator type MCR 5 with a specific problem defined by the practice. The analysis was conducted using a specially developed mathematical models based on the finite elements method. Models of the separator's structure with ferrite material concentrators and gaps of non-magnetic material were examined as well as those with different orientation of the magnetic induction vector of the individual permanent magnets. The stationary magnetic field of the system is analyzed as a flat parallel system, taking into account the nonlinear properties of the materials, which are separated by the separating apparatus. Also are examined the influence of the thickness and the material of the outer body (housing) of the separator. The distribution of the field is modeled numerically using the finite elements method and the software package FEMM 4.2. The results allow to optimize the structure of the magnetic separator and to achieve the highest level of purification.},
keywords={Magnetic separation;Particle separators;Permanent magnets;Magnetic flux;Mathematical model;Magnetic forces;mathematical modeling;permanent magnets;separator;technological parameters},
doi={10.1109/SIELA.2016.7542990},
ISSN={},
month={May},}
@INPROCEEDINGS{9829817,
author={Gallenmüller, Sebastian and Hauser, Eric and Carle, Georg},
booktitle={2022 IFIP Networking Conference (IFIP Networking)},
title={Prototyping Prototyping Facilities: Developing and Bootstrapping Testbeds},
year={2022},
volume={},
number={},
pages={1-6},
abstract={The creation of prototypes is a convincing approach, demonstrating the feasibility of scientific ideas. Testbeds act as enablers for such prototypes, contributing the facilities to their construction. In this paper, we apply a prototype-driven approach to the development of the testbeds themselves. Thus, we select abstractions and APIs to modularize testbeds to allow a selective adaptation or substitution of specific components. To minimize costs, our approach aims to consolidate all components into a single system. Hence, we named it testbed on a single system (toast). The single-server approach demands the recreation of entire components in software such as networks or experiment nodes. Simultaneously, the softwarization of components enables flexible network topologies and scalability. At the same time, we try to keep the behavior and the performance as close to a hardware-based testbed as possible. Therefore, we rely heavily on hardware acceleration of IO using techniques such as single root IO virtualization (SR-IOV). A case study compares the accelerated IO of toast to a hardware-based testbed and a testbed without IO acceleration. We want to use toast as a training and teaching environment and a prototype facility for future research infrastructures.},
keywords={Training;Network topology;Scalability;Prototypes;Life estimation;Computer architecture;Software;Testbed;Nework Experiments;Virtualization;SR-IOV},
doi={10.23919/IFIPNetworking55013.2022.9829817},
ISSN={1861-2288},
month={June},}
@INPROCEEDINGS{6228374,
author={Tu, Shu-Ling and Lin, Yue-Der and Chiu, I-Chen and Lin, Yaw-Jen},
booktitle={2012 International Symposium on Computer, Consumer and Control},
title={A Performance Study of Multimedia Patient Education: A Case of Surgical Wound Care},
year={2012},
volume={},
number={},
pages={577-580},
abstract={Wound care skills are essential for patients in stable condition after surgery. A nurse must teach wound care skills and explain infection symptoms and signs to patients or their families before discharge to help wound healing. The purpose of this study is to investigate the influence of group teaching with multimedia materials on wound care skills of surgical patients. This study is based on Solomon four-group design to study four surgical wards in a medical teaching hospital. There were 121 surgical patients selected according to purposive sampling, and divided into two experimental groups and two control groups. Test groups received multimedia teaching compiled by the researcher, and control groups received traditional nursing instruction. Performance of one test group and one control group were measured with a structured questionnaire before teaching, and all patients were measured after teaching. Data were collected and analyzed by SPSS/PC software package. Performance of test group and the control group before and after nursing instruction showed statistically significant difference (p=0.00). Wound care skills progress among all the four groups are significantly different (p= 0.002). It is an important task of nursing staff to provide disease-related knowledge and guidance of patient care to reduce complications. This study finds that wound care nursing guidelines with multimedia materials really enhance the wound care skills of patients and family members. It is hoped that this study will be a reference for other health education programs.},
keywords={Wounds;Multimedia communication;Surgery;Materials;Educational institutions;nursing instruction;surgical wound care},
doi={10.1109/IS3C.2012.151},
ISSN={},
month={June},}
@ARTICLE{4711489,
author={},
journal={IEEE Unapproved Draft Std P1636/D1.7, Jul 2008},
title={IEEE Draft Trial-Use Standard for Software Interface for Maintenance Information Collection and Analysis},
year={2008},
volume={},
number={},
pages={},
abstract={This document provides an implementation independent specification for a software interface to information systems containing data pertinent to the diagnosis and maintenance of complex systems consisting of hardware, software, or any combination thereof. These interfaces will support service definitions for creating application programming interfaces (API) for the access, exchange, and analysis of historical diagnostic and maintenance information. This will address the pervasive need of organizations to assess the effectiveness of diagnostics for complex systems throughout the product lifecycle. The use of formal information models will facilitate exchanging historical maintenance information between information systems and analysis tools. The models will facilitate creating open system software architectures for maturing system diagnostics.},
keywords={},
doi={},
ISSN={},
month={},}
@ARTICLE{6860260,
author={Miller, Tim and Lu, Bin and Sterling, Leon and Beydoun, Ghassan and Taveter, Kuldar},
journal={IEEE Transactions on Software Engineering},
title={Requirements Elicitation and Specification Using the Agent Paradigm: The Case Study of an Aircraft Turnaround Simulator},
year={2014},
volume={40},
number={10},
pages={1007-1024},
abstract={In this paper, we describe research results arising from a technology transfer exercise on agent-oriented requirements engineering with an industry partner. We introduce two improvements to the state-of-the-art in agent-oriented requirements engineering, designed to mitigate two problems experienced by ourselves and our industry partner: (1) the lack of systematic methods for agent-oriented requirements elicitation and modelling; and (2) the lack of prescribed deliverables in agent-oriented requirements engineering. We discuss the application of our new approach to an aircraft turnaround simulator built in conjunction with our industry partner, and show how agent-oriented models can be derived and used to construct a complete requirements package. We evaluate this by having three independent people design and implement prototypes of the aircraft turnaround simulator, and comparing the three prototypes. Our evaluation indicates that our approach is effective at delivering correct, complete, and consistent requirements that satisfy the stakeholders, and can be used in a repeatable manner to produce designs and implementations. We discuss lessons learnt from applying this approach.},
keywords={Object oriented modeling;Aircraft;Atmospheric modeling;Software;Industries;Analytical models;Educational institutions;Agent-oriented software engineering;agent-oriented modelling;technology transfer},
doi={10.1109/TSE.2014.2339827},
ISSN={1939-3520},
month={Oct},}
@INPROCEEDINGS{465330,
author={Valetto, G. and Kaiser, G.E.},
booktitle={Proceedings Seventh International Workshop on Computer-Aided Software Engineering},
title={Enveloping sophisticated tools into computer-aided software engineering environments},
year={1995},
volume={},
number={},
pages={40-48},
abstract={We present a CASE-tool integration strategy based on enveloping pre-existing tools without source access or assuming an API or any other special capabilities on the part of the tool. This Black Box enveloping (or wrapping) idea has been around for a long time, but was previously restricted to relatively simple tools such as compilers. We describe the design and implementation of a new Black Box enveloping facility intended for sophisticated tools-often with graphical user interfaces-with particular concern for the emerging class of groupware applications.<>},
keywords={Computer aided software engineering;File systems;Software engineering;Wrapping;User interfaces;Collaborative software;Collaborative work;Code standards;Joining processes;Communication standards},
doi={10.1109/CASE.1995.465330},
ISSN={},
month={July},}
@INPROCEEDINGS{750039,
author={Smith, G. and Gough, J. and Szyperski, C.},
booktitle={Proceedings Technology of Object-Oriented Languages. TOOLS 28 (Cat. No.98TB100271)},
title={A case for meta-interworking: projecting CORBA meta-data into COM},
year={1998},
volume={},
number={},
pages={242-253},
abstract={The pressure to reduce the time and effort required to produce and update software components, together with the existence of multiple competing component worlds, has forced the introduction of interworking standards. Unfortunately the interworking standards fail to suitably address the need for access to meta information. In the light of meta-data being increasingly important for component environments, this inaccessibility of meta-data can lead to components being unusable by their intended clients. What exasperates the situation is that if access to meta-data repositories is provided using an implementation of the interworking standard, the information retrieved is incorrect. The paper describes these difficulties. To exemplify the solution, the construction of an adapter is described. This adapter provides access to the Common Object Request Broker Architectures' (CORBA) Interface Repository via the Interfaces of the Component Object Model's (COM) Type Library. This solution provides access to meta-data which is essential if the full benefit of interworking is to be realised.},
keywords={Computer aided software engineering;Bridges;Sun;Information technology;Information retrieval;Web and internet services;Software engineering;Production systems;Software systems;Java},
doi={10.1109/TOOLS.1998.750039},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9421847,
author={Liu, Xiaoyuan and Sun, Tianyang and Bu, Fanliang and Qin, Hang},
booktitle={2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)},
title={The Analysis on the Role of Social Network in the Field of Anti-Terrorism Take the "East Turkistan" Organization as an Example},
year={2020},
volume={},
number={},
pages={2282-2285},
abstract={In order to analyze the network structure of the “East Turkistan” organization and identify members with higher individual threats, so as to conduct a better attack on the “East Turkistan” organization. We use Gephi, a network analysis software, and collect open source information on three batches of 25 terrorists announced by the Ministry of Public Security and use networkX, a Python package to study the structure of complex networks, to conduct social network analysis(SNA) of the “East Turkistan” organization personnel. The “East Turkistan” organization presents a combination of scale-free network and hierarchical network in the topological structure. To manifest the analysis of social network methods' importance in combating terrorism, we simulate the effect of removing the core members of the “East Turkistan” organization. The construction of SNA requires a large deal of open source information. How to obtain intelligence is an important factor that restricts precision strikes. In addition, how to combine social network analysis with computer deep learning knowledge is also a new hot spot and direction in the field of anti-terrorism.},
keywords={Knowledge engineering;Deep learning;Social networking (online);Terrorism;Organizations;Complex networks;Software;Gephi;Social network analysis;East Turkistan;Anti-terrorism;Python},
doi={10.1109/ICMCCE51767.2020.00493},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9760551,
author={Reddy, Yerrababu Moukthika and Nadampalli, Mounika and Panigrahy, Asisa Kumar and Divyasree, Kunduri Surya and Jahnavi, A and Vignesh, N. Arun},
booktitle={2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP)},
title={Automated Facemask Detection and Monitoring of Body Temperature using IoT Enabled Smart Door},
year={2022},
volume={},
number={},
pages={1-8},
abstract={Living with the novel Coronavirus is becoming the new normal as nations around the globe resume. However, in order to stop the virus from spreading, we must isolate Covid-infected persons from the rest of the population.Fever is the most common symptom of coronavirus infection, according to the CDC [1], with up to 83 percent of symptomatic patients presenting indications of fever. Early symptom detection and good hygiene standards are therefore critical, particularly in situations where people come into random contact with one another. As a result, temperature checks and masks are now required in schools, colleges, offices, and other public spaces. However, manually monitoring each individual and measuring their respective body temperatures is a cumbersome task. Currently, most of the temperature checkups are done manually which can be inefficient, impractical, and riskybecause sometimes people checking manually may be reluctant to check every person’s temperature or sometimes allow people even if they violate the guidelines. Moreover, the person assigned to manually check will be at high risk as he is exposed to a lot of people. To solve these issues, we propose a project that reduces the growth of COVID-19 by monitoring the presence of a facial mask and measuring their temperature. The Face Mask Detection can be done using the TensorFlow software library, Mobilenet V2 architecture and OpenCV.A non-contact IR temperature sensor is used to monitor the individual’s body temperature. To avoid false positives, the system will be strengthened by training it with a variety of cases. Once the system detects a mask, it measures the body temperature of the person. If the temperature is within the normal range, sanitization is done,and the person is permitted entry through an IOT enabled smart door. However, if the system fails to detect a mask or the person’s temperature falls out of the predefined range, a buzzer rings and the door remains closed. Our model is intended to be effective in preventing the spread of this infectious disease.},
keywords={Temperature measurement;Temperature sensors;COVID-19;Training;Temperature distribution;Manuals;Temperature control;Facial Mask Detection;Covid 19;TensorFlow;Mobilenet V2;OpenCV;Raspberry pi;Temperature Detection;IOT.},
doi={10.1109/AISP53593.2022.9760551},
ISSN={2640-5768},
month={Feb},}
@ARTICLE{7263352,
author={Hansen, Jonas and Lucani, Daniel E. and Krigslund, Jeppe and Medard, Muriel and Fitzek, Frank H.P.},
journal={IEEE Communications Magazine},
title={Network coded software defined networking: enabling 5G transmission and storage networks},
year={2015},
volume={53},
number={9},
pages={100-107},
abstract={Software defined networking has garnered large attention due to its potential to virtualize services in the Internet, introducing flexibility in the buffering, scheduling, processing, and routing of data in network routers. SDN breaks the deadlock that has kept Internet network protocols stagnant for decades, while applications and physical links have evolved. This article advocates for the use of SDN to bring about 5G network services by incorporating network coding (NC) functionalities. The latter constitutes a major leap forward compared to the state-ofthe- art store and forward Internet paradigm. The inherent flexibility of both SDN and NC provides fertile ground to envision more efficient, robust, and secure networking designs, which may also incorporate content caching and storage, all of which are key challenges of the upcoming 5G networks. This article not only proposes the fundamentals of this intersection, but also supports it with key use cases and a thorough performance evaluation on an implementation that integrated the Kodo library (NC) into OpenFlow (SDN). Our results on singlehop, multihop, and multi-path scenarios show that gains of 3x to 11x are attainable over standard TCP and multi-path TCP.},
keywords={Encoding;Switches;Software radio;5G mobile communication;Network coding;Virtual machining},
doi={10.1109/MCOM.2015.7263352},
ISSN={1558-1896},
month={Sep.},}
@ARTICLE{5387276,
author={Snir, M. and Hochschild, P. and Frye, D. D. and Gildea, K. J.},
journal={IBM Systems Journal},
title={The communication software and parallel environment of the IBM SP2},
year={1995},
volume={34},
number={2},
pages={205-221},
abstract={This paper describes the software available on the IBM SP2™ for parallel program development and execution. It presents the rationale for the design of the Message-Passing Library used on the SP2, outlines its current implementation, and gives information on performance. In addition, the paper describes the programming environment and the program development tools available for developing and executing parallel codes.},
keywords={},
doi={10.1147/sj.342.0205},
ISSN={0018-8670},
month={},}
@INPROCEEDINGS{4117719,
author={Boer, Csaba Attila and de Bruin, Arie and Verbraeck, Alexander},
booktitle={Proceedings of the 2006 Winter Simulation Conference},
title={Distributed Simulation in Industry - A Survey Part 2 - Experts on Distributed Simulation},
year={2006},
volume={},
number={},
pages={1061-1068},
abstract={Distributed simulation is used very little in industry, especially when compared with the interest in distributed simulation from research and from the military domain. In order to answer the question why industry lags behind, the authors have carried out an extensive survey, using a questionnaire and interviews, with users, vendors, and developers of distributed simulation products, as well as with vendors of non-distributed simulation software. This paper reports on the second part of the survey, namely a series of open ended interviews. We report on the responses we obtained indicating the discrepancies between the different "worlds". A categorization of these responses is given using which it is possible to formulate clear guidelines for further developments of standards for distributed simulation},
keywords={Computational modeling;Computer simulation;Defense industry;Computer industry;Data analysis;Packaging;Mathematics;Computer science;Technology management;Engineering management},
doi={10.1109/WSC.2006.323195},
ISSN={1558-4305},
month={Dec},}
@INPROCEEDINGS{6829408,
author={Schanne, Stéphane and Le Provost, Hervé and Kestener, Pierre and Gros, Aleksandra and Cortial, Marin and Götz, Diego and Sizun, Patrick and Château, Frédéric and Cordier, Bertrand},
booktitle={2013 IEEE Nuclear Science Symposium and Medical Imaging Conference (2013 NSS/MIC)},
title={A Scientific Trigger Unit for space-based real-time gamma ray burst detection I - Scientific software model and simulations},
year={2013},
volume={},
number={},
pages={1-5},
abstract={The on-board Scientific Trigger Unit (UTS) is designed to detect Gamma Ray Bursts (GRBs) in real-time, using the data produced by the ECLAIRs camera, foreseen to equip the future French-Chinese satellite mission SVOM (Space-based Variable Objects Monitor). The UTS produces GRB alerts, sent to the ground for GRB follow-up observations, and requests the spacecraft slew to repoint its narrow field instruments onto the GRB afterglow. Because of the diversity of GRBs in duration and variability, two simultaneously running GRB trigger algorithms are implemented in the UTS, the so called Image Trigger performing systematic sky image reconstruction on time scales above 20 s, and the Count-Rate Trigger, selecting a time scale from 10 ms to 20 s showing an excess in count-rate over background estimate, prior to imaging the excess for localization on the sky. This paper describes both trigger algorithms and their implementation in a library, compiled for the Scientific Software Model (SSM) running on standard Linux machines, and which can also be cross-compiled for the Data Processing Model (DPM), in order to have the same algorithms running on both platforms. While the DPM permits to validate the hardware concept and benchmark the algorithms (see paper II), the SSM allows to optimize the algorithms and estimate the GRB trigger-rate of ECLAIRs/UTS. The result of running on the SSM a dynamic photon by photon simulation based on the BATSE GRB catalog is presented.},
keywords={Photonics;Detectors;Gamma-ray bursts;Telescopes;Earth;Signal to noise ratio;X-rays},
doi={10.1109/NSSMIC.2013.6829408},
ISSN={1082-3654},
month={Oct},}
@ARTICLE{5173471,
author={Arquero, Agueda and Alvarez, Marina and Martinez, Estibaliz},
journal={IEEE Latin America Transactions},
title={Decision Management making by AHP (Analytical Hierarchy Process) trought GIS data},
year={2009},
volume={7},
number={1},
pages={101-106},
abstract={In this work, we propose the use of the AHP (Analytical Hierarchy Process) as a mathematical tool to structure a multiple criteria problem like a visual pattern. The main objective pursued is to determine what is the optimal placing to build a urban construction to locate a university library. In this case, it has been applied for the Campus of Montegancedo of the Polytechnic University of Madrid (Spain), where the Faculty of Computer science is placed. The data come from a Geographical Information System (GIS). Three different profiles of standard users have been considered and they determine the management of the final decision to take, in order to carry out the study. The use of the commercial software Expert Choice facilitates efficiently this management.},
keywords={Geographic Information Systems;Management information systems;Pattern analysis;Libraries;Computer science;Information analysis;Decision making;Instruments;Silicon compounds;Resumes;Analytical Hierarchy Process (AHP);Expert Choice;Geographical Information System (GIS);Multiple criteria decision-making},
doi={10.1109/TLA.2009.5173471},
ISSN={1548-0992},
month={March},}
@INPROCEEDINGS{9174369,
author={Ayub, Muhammad and Knowles, Patrick and Phan, Brandon and Coffman, Joel},
booktitle={2020 IEEE Systems Security Symposium (SSS)},
title={The Cost of a Macaroon},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Macaroons are authorization credentials which have high efficiency and expressiveness that rivals public-key-based mechanisms like Simple Public Key Infrastructure (SPKI) / Simple Distributed Security Infrastructure (SDSI). Macaroons are simple and easy to deploy similar to web cookies. Additionally, they offer secure delegation via their chained hashed message authentication code (HMAC) construction which provides a construct for decentralized delegation. These characteristics are motivating factors for the adoption of macaroons; however, the modern web environment has changed since the initial concept was demonstrated on a local server and since that time there has been a broad adoption of cloud services and an expansion of mobile and embedded devices with the trend of Internet of Things (IoT).The approach utilized by macaroons has a much lower processing overhead and lower latency compared to other authorization methods used in the cloud. Our work investigates factors relating to the overhead costs of macaroon primitives. We implement a macaroons library in Python and replicate the measurements taken by Birgisson et al. in 2014. Then we evaluate the overhead of macaroon primitives and validate the prior findings regarding the cost of authorization primitives on a comparable computing platform. We also run tests with different payload sizes to assess if the prior results continue to hold true using a variety of common payload sizes. Our results provided insight into the trade offs of how efficient and expressive macaroons can be. We additionally evaluate the differences for the cost of authorization primitives across a variety of platforms to assess macaroons relative to the use of modern computer processors and modern software libraries, including ARM architecture processors to evaluate their use with IoT devices. Our results indicate that hardware and software advances have decreased the cost of using macaroons and they are efficient enough to use in low-power embedded platforms.},
keywords={},
doi={10.1109/SSS47320.2020.9174369},
ISSN={},
month={July},}
@INPROCEEDINGS{4839231,
author={Dechev, Damian and Pirkelbauer, Peter and Rouquette, Nicolas and Stroustrup, Bjarne},
booktitle={2009 16th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems},
title={Semantically Enhanced Containers for Concurrent Real-Time Systems},
year={2009},
volume={},
number={},
pages={48-57},
abstract={Future space missions, such as Mars Science Laboratory, are built upon computing platforms providing a high degree of autonomy and diverse functionality. The increased sophistication of robotic spacecraft has skyrocketed the complexity and cost of its software development and validation. The engineering of autonomous spacecraft software relies on the availability and application of advanced methods and tools that deliver safe concurrent synchronization as well as enable the validation of domain-specific semantic invariants. The software design and certification methodologies applied at NASA do not reach the level of detail of providing guidelines for the development of reliable concurrent software. To achieve effective and safe concurrent interactions as well as guarantee critical domain-specific properties in code, we introduce the notion of a Semantically Enhanced Container (SEC). A SEC is a data structure engineered to deliver the flexibility and usability of the popular ISO C++ Standard Template Library containers, while at the same time it is hand-crafted to guarantee domain-specific policies. We demonstrate the SEC proof-of-concept by presenting a shared nonblocking SEC vector. To eliminate the hazards of the ABA problem (a fundamental problem in lock-free programming), we introduce an innovative library for querying C++ semantic information. Our SEC design aims at providing an effective model for shared data access within the JPL's Mission Data System. Our test results show that the SEC vector delivers significant performance gains (a factor of 3 or more) in contrast to the application of nonblocking synchronization amended with the traditional ABA avoidance scheme.},
keywords={Containers;Real time systems;Space vehicles;ISO standards;Space missions;Mars;Laboratories;Orbital robotics;Costs;Programming;nonblocking synchronization;static analysis;concurrent real-time systems},
doi={10.1109/ECBS.2009.12},
ISSN={},
month={April},}
@INBOOK{9289988,
author={Cyganek, Boguslaw},
booktitle={Introduction to Programming with C++ for Engineers},
title={Delving into Object‐Oriented Programming},
year={2020},
volume={},
number={},
pages={227-348},
abstract={In this chapter, the authors delve into the domain of the object‐oriented (OO) approach to software design and object‐oriented programming (OOP), which helps with software system modeling. They delve into the OOP properties of C++, starting with OOP concepts and then examining code examples. OOP is based on the four principles: encapsulation, inheritance, polymorphism, and abstraction. The authors provide a closer look at the internal structure of a class. The construction portion of the class contains the special member functions: default constructor, parametric constructor, copy constructor, assignment operator, and destructor. A class's data and member functions can be assigned to the private or public groups. If not explicitly assigned, then by default, members of a class are private. The authors explain how to make another class, this time to represent complex numbers: two valued entities with their own arithmetic rules, which are used e.g. in electronics and physics.},
keywords={Software;C++ languages;Software design;Object oriented programming;Transform coding;Libraries;Encapsulation},
doi={10.1002/9781119431152.ch4},
ISSN={},
publisher={IEEE},
isbn={9781119431176},
url={https://ieeexplore.ieee.org/document/9289988},}
@INPROCEEDINGS{7062847,
author={Sutardja, Sehat},
booktitle={2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers},
title={1.2 The future of IC design innovation},
year={2015},
volume={},
number={},
pages={1-6},
abstract={One of the greatest achievements of human kind is undoubtedly its ability to build tiny machines that marry the functionalities of computers and wireless communication devices so cheaply that almost anyone in the world can afford them. Every one of us carries, at least, one such device in our pocket, yet we rarely think how in the world anyone could have created such a thing! Even the experts in our industry could not have predicted that this would have happened so quickly. What we have achieved in the course of the design and implementation of these sophisticated systems could be considered nothing short of a miracle, considering that billions of transistors have to work together flawlessly. (Well, sort of!) The reality is that some of these devices are now so complex that we need thousands of engineers to design, validate, and support them, including recovery from inefficiencies and bugs along the way. But, despite their ever-increasing complexity, the way we build these devices has not changed much over the past decades. Integrated-circuit (IC) design engineers blindly do what they are told - integrate as much functionality into a single device, believing that more is better! This more-the-better mentality is not surprising because we saw in the past that the more completely we integrate the cheaper things become. However, the cost and complexity of building billions of transistors on a single device is finally taking a toll on our engineers, which calls for new paradigm shifts in designing complex devices. If chip-design engineers had considered the financial optimization of the overall design process, they would have built things differently. They would have realized that certain functions are better grouped into highly specialized integrated circuits that easily and seamlessly talk to each other without compromising the overall system cost and performance. The key to making this happen is what I call the Lego-Block approach of designing integrated circuits. However, in order for the Lego-Block approach to materialize, we need to change the way we architect our devices. For example, we need to define a new chip-to-chip interconnect protocol, take advantage of multi-chip-module packaging and high-speed SerDes technology, redefine the memory hierarchy to take advantage of 3D solid-state memory instead of blindly increasing the DRAM size, repartition DRAM to serve different logical functions instead of building gigantic single-die DRAM to serve every function, change the way we build DRAM so that they are optimized more for performance and power efficiency instead of capacity, and redefine what should be done in hardware versus software. In short, we need to change our way of thinking, and be brave enough to reject common wisdom! If we fail to take action, soon we will no longer see cost savings. On the other hand, if we succeed, we will see life beyond the end of Moore's Law!},
keywords={Random access memory;System-on-chip;Bandwidth;Smart phones;Buildings;Integrated circuit interconnections},
doi={10.1109/ISSCC.2015.7062847},
ISSN={2376-8606},
month={Feb},}