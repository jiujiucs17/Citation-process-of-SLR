@article{KAHRO197633,
title = {A Control Computer Software Production System},
journal = {IFAC Proceedings Volumes},
volume = {9},
number = {2},
pages = {33-36},
year = {1976},
note = {1st IFAC/IFIP Symposium on Software For Computer Control, Tallinn, USSR, 25-28 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)67397-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701767397X},
author = {M. Kahro and M. Männisalu and E. Töugu},
abstract = {A programming system for a micro programmable control computer is implemented on a host computer with an alpha-numeric diaplay for interactive use. Extensive use of the host computer system software hss been made. The library and utilities of the host computer are for use in the system designed. Special attention is paid to debugging facilities. These are to be used for debugging all target computer programs including the operating system. The debugging may be performed both in batch and interactive mode using the display. The system is implemented in hard time limit using FORTRAN and assembler language and atructured programming technique.}
}
@article{DESANTIS20111318,
title = {Full-hexahedral structured meshing for image-based computational vascular modeling},
journal = {Medical Engineering & Physics},
volume = {33},
number = {10},
pages = {1318-1325},
year = {2011},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2011.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350453311001445},
author = {Gianluca {De Santis} and Matthieu {De Beule} and Koen {Van Canneyt} and Patrick Segers and Pascal Verdonck and Benedict Verhegghe},
keywords = {Image-based modeling, Hexahedral mesh, Centerline, Patient-specific, Finite Element Analysis, Computational Fluid Dynamics, pyFormex},
abstract = {Image-based computational modeling offers a virtual access to spatially and temporally high resolution flow and structural mechanical data in vivo. Due to inter-subject morphological variability, mesh generation represents a critical step in modeling the patient-specific geometry and is usually performed using unstructured tetrahedral meshing algorithms. Although hexahedral structured meshes are known to provide higher accuracy and reduce the computational costs both for Finite Element Analysis and Computational Fluid Dynamics, their application in computational cardiovascular studies is challenging due to the complex 3D and branching topology of vascular territories. In this study, we propose a robust procedure for structured mesh generation, tailoring the mesh structure to the subject-specific vessel topology. The proposed methodology is based on centerline-based synthetic descriptors (i.e. centerlines, radii and centerlines’ normals) which are used to solve the meshing problem following a bottom-up approach. First, topologically equivalent block-structures are placed inside and outside the lumen domain. Then, a projection operation is performed, returning a parametric volume mesh which fits the original triangulated model with sub-micrometric accuracy. Additionally, a three-layered arterial wall (resembling the intima, media and adventitia) is artificially generated, with the possibility of setting variable thickness (e.g. proximal-to-distal tapering) and material anisotropy (e.g. position-dependent collagen-fibers’ orientation). This new meshing procedure, implemented using open-source software packages only, is demonstrated on two challenging human cases, being an aortic arch and an abdominal aortic aneurysm. High-quality meshes are generated in both cases, according to shape-quality metrics. By increasing the computation accuracy, the developed meshing tool has the potential to further add “confidence” to the use of computational methods in vascular applications.}
}
@article{KARMAKAR201088,
title = {Development of expert system modeling based decision support system for swine manure management},
journal = {Computers and Electronics in Agriculture},
volume = {71},
number = {1},
pages = {88-95},
year = {2010},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2009.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0168169910000037},
author = {S. Karmakar and M. NKetia and C. Laguë and J. Agnew},
keywords = {Manure management, Systems engineering approach, Expert system, Decision support system (DSS), Decision criteria},
abstract = {Animal waste has always been considered as a resource for agricultural input as biofertilizer. However, the management is becoming more stringent due to environmental regulations. Livestock producers are faced with different manure management options that may be implemented into their operations. Given the expansion of the livestock industry, the implementation of environmental regulations, and the increasing importance of social and health issues, the selection of optimal manure management systems is becoming a strategically important task. Increasingly, integrated decision support systems (DSSs) are becoming necessary to assist decision makers in their evaluation of different manure management alternatives, like, liquid system, semi-solid system, solid system and bio-gas or bio-energy system based on combinations of different manure management sub-systems (collection, storage and application). To address this situation, a user-friendly computer program called Integrated Swine Manure Management (ISMM) is being developed for the Canadian Prairie provinces. Decision criteria including environmental, agronomic, social and health, greenhouse gas emission, and economic factors have been considered for the selection, design, and operation of the DSS. The expert system modeling is based on Visual Basic programming. Decision on adopting a particular combination of systems components is based on performance rating of the overall system. The program is interactive so that weighting factors for the different decision criteria can be varied to suit site-specific considerations. In this paper, the systems approach for development of an integrated liquid manure management system is discussed. Using a case study, sensitivity analysis of different combinations of management components is also reported for systems performance. The decision software compared satisfactorily with other available DSS packages.}
}
@article{CHOW2019543,
title = {Numerical wave basin using incompressible smoothed particle hydrodynamics (ISPH) on a single GPU with vertical cylinder test cases},
journal = {Computers & Fluids},
volume = {179},
pages = {543-562},
year = {2019},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2018.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0045793018309034},
author = {Alex D. Chow and Benedict D. Rogers and Steven J. Lind and Peter K. Stansby},
keywords = {Numerical wave basin, Breaking wave, Incompressible SPH, GPU, DualSPHysics},
abstract = {This paper presents the development of a numerical wave basin for violent hydrodynamics of free-surface waves interacting with vertical cylindrical structures using the incompressible smoothed particle hydrodynamics (ISPH) method accelerated with a graphics processing unit (GPU). ISPH has been implemented on a single GPU based on the open-source DualSPHysics software and using the ViennaCL linear algebra library for solving the pressure Poisson equation. Extensions to the code are presented for a numerical wave basin including development of the solid boundary condition, based on a mirror image particle and moving least squares interpolation approach, for improved accuracy at the free and solid surface interfaces during wave-structure impact. This is applied to focused waves, including breaking for which SPH is well suited, interacting with a cylindrical column to compare with experimental data. Convergence is demonstrated within the limit of 5 million particles for a single GPU. Acceptable agreement with experiment is achieved for force measurements, and variation of free-surface elevation around the column is shown to be significant particularly in larger waves with a correspondingly large force contribution from hydrostatic pressure. While breaking does not have a marked effect on total force, associated high particle velocities amplify local pressures considerably on the column near the water surface. The computer runtime is similar to that for volume-of-fluid and particle-in-cell solvers running on 8 and 80 processors respectively.}
}
@article{MARCOS1992529,
title = {An Integrated System for Supervisory Process Control using ADA},
journal = {IFAC Proceedings Volumes},
volume = {25},
number = {6},
pages = {529-533},
year = {1992},
note = {IFAC Symposium on Intelligent Components and Instruments for Control Applications (SICICA'92), Malaga, Spain, 20-22 May 1992},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)50961-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017509611},
author = {M. Marcos and F. Artaza and N. Iriondo},
keywords = {Supervisory control, direct digital control, computer control, servomechanisms, real time computer systems},
abstract = {The paper presents an integrated software package for supervisory process control using ADA language. This system performs process control using classical and heuristic controllers, supervision in order to detect abnormal situations and on-line monitoring. Using ADA language, the definition of the different processes (control, supervision and monitoring) can be easily implemented using the concurrent programming techniques inherent to the language. Specifically, a rule-based system which compensates for dead-zone phenomena in a position control servomechanism is developed. The supervisor module is designed to detect and avoid undesirable on-linear control phenomena such as steady state errors, limit cycling or excursions into unwanted zones. It constitutes an intelligent compensator for system input nonlinearities which may be generalized to other non-linearity types.}
}
@article{DYMSCHIZ1977431,
title = {Computer aided design of control algorithms based on identified process models},
journal = {IFAC Proceedings Volumes},
volume = {10},
number = {16},
pages = {431-438},
year = {1977},
note = {Preprints of the 5th IFAC/IFIP International Conference on Digital Computer Applications to Process Control, The Hague, The Netherlands, 14-17 June, 1977},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)69553-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017695533},
author = {E. Dymschiz and R. Isermann},
abstract = {A software package ‘CADCA’ was developed for the design of control algorithms and the closed loop control with process computers. Based on parametric process models, which e.g. can be obtained by on-line identification with another software package, different feedback control algorithms can be designed. By an interactive dialog between the operator and the process computer ten different feedback control algorithms (PID-, deadbeat and state controllers with observers) can be designed for linear single input/single output processes. The closed loop behaviour is simulated in the process computer and displayed. The control algorithms can be modified and redesigned upon request. A best suited control algorithm is selected and implemented so that closed loop operation can start. An additional package ‘CAFCA’ allows the design of feedforward control algorithms. Results are shown for the application of CADCA on an industrial size heat exchanger.}
}
@article{MAZHARI2017669,
title = {A novel frequency-domain approach for distributed harmonic analysis of multi-area interconnected power systems},
journal = {Electric Power Systems Research},
volume = {143},
pages = {669-681},
year = {2017},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2016.10.048},
url = {https://www.sciencedirect.com/science/article/pii/S037877961630445X},
author = {Seyed Mahdi Mazhari and Shahram {Montaser Kouhsari} and Abner Ramirez},
keywords = {Distributed computing, Frequency-domain (FD) analysis, Harmonic analysis, Large-change sensitivity (LCS), Power system simulation},
abstract = {This paper presents a novel frequency-domain approach for distributed harmonic analysis (DHA) of a multi-area interconnected electric power system within restructured environment. The proposed approach is based on a decentralized structure in which harmonic analysis of an area is independently conducted, even with limited available data, via local computing resources. The large-change sensitivity (LCS) concept is then applied in a secure platform to account for the effects of whole network to each single area of the interconnected power system. Frequency-dependent models of system elements accompanied by any existent harmonic assessment method can be utilized for harmonic penetration calculation. The proposed DHA approach is capable of finding exact values as those of the interconnected system using TCP/IP communication facility. Moreover, it allows operator of a utility to independently conduct DHA within a restructured power network. The developed method is implemented in an existing software package and applied to several networks including the IEEE 118-bus test system.}
}
@article{SCHMID1982101,
title = {Design of Digital State-Feedback Controllers Using a CAD Package},
journal = {IFAC Proceedings Volumes},
volume = {15},
number = {1},
pages = {101-108},
year = {1982},
note = {IFAC Symposium on Theory and Application of Digital Control, New Dehli, India, 5-7 January},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)63333-0},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017633330},
author = {Chr. Schmid and G. Juen},
keywords = {Computer-aided design, optimal control, direct digital control, sampled data systems, feedback, feedforward, multivariable control systems, state-space methods, steam generator},
abstract = {CAD methods for multivariable discrete-time optimal state-feedback control systems including integral output feedback and observers with known and/or unknown inputs are discussed. Depending on the performance index different combinations of feedback and feedforward controllers are obtained. The effects of measured and observed disturbances and set point changes are shown by simulation studies of a 14th order steam generator subsystem. The implementation of theory, algorithms and software is sketched.}
}
@article{WANG2008549,
title = {Parallel finite element analysis of high frequency vibrations of quartz crystal resonators on Linux cluster},
journal = {Acta Mechanica Solida Sinica},
volume = {21},
number = {6},
pages = {549-554},
year = {2008},
issn = {0894-9166},
doi = {https://doi.org/10.1007/s10338-008-0866-6},
url = {https://www.sciencedirect.com/science/article/pii/S0894916609602603},
author = {Ji Wang and Yu Wang and Wenke Hu and Wenhua Zhao and Jianke Du and Dejin Huang},
keywords = {plate, vibration, quartz, resonator, FEM, parallel computing},
abstract = {Quartz crystal resonators are typical piezoelectric acoustic wave devices for frequency control applications with mechanical vibration frequency at the radio-frequency (RF) range. Precise analyses of the vibration and deformation are generally required in the resonator design and improvement process. The considerations include the presence of electrodes, mountings, bias fields such as temperature, initial stresses, and acceleration. Naturally, the finite element method is the only effective tool for such a coupled problem with multi-physics nature. The main challenge is the extremely large size of resulted linear equations. For this reason, we have been employing the Mindlin plate equations to reduce the computational difficulty. In addition, we have to utilize the parallel computing techniques on Linux clusters, which are widely available for academic and industrial applications nowadays, to improve the computing efficiency. The general principle of our research is to use open source software components and public domain technology to reduce cost for developers and users on a Linux cluster. We start with a mesh generator specifically for quartz crystal resonators of rectangular and circular types, and the Mindlin plate equations are implemented for the finite element analysis. Computing techniques like parallel processing, sparse matrix handling, and the latest eigenvalue extraction package are integrated into the program. It is clear from our computation that the combination of these algorithms and methods on a cluster can meet the memory requirement and reduce computing time significantly.}
}
@incollection{ALFADALA20051279,
title = {A hierarchical approach to optimize LNG fractionation units},
editor = {Luis Puigjaner and Antonio Espuña},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {20},
pages = {1279-1284},
year = {2005},
booktitle = {European Symposium on Computer-Aided Process Engineering-15, 38th European Symposium of the Working Party on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(05)80055-0},
url = {https://www.sciencedirect.com/science/article/pii/S1570794605800550},
author = {Hassan E. Alfadala and Bilal M. Ahmad and Abdulla F. Warsame},
abstract = {The objective of this paper is to optimize the thermal performance of a fractionation unit within a liquefied natural gas (LNG) facility. Typical fractionation units in an LNG facility consisting of three distillation columns, namely de-ethanizer, de-propanizer and de-butanizer were used in this study. A hierarchical approach is developed to optimizing the system. In this approach, increasing levels of model complexity are used and various thermal targets are set and implemented. The column targeting tool available within the simulation package of Aspen Plus© software was used to optimize a fractionation unit in an LNG facility. First, integrated thermal analysis was used in identifying design targets for improvements in energy consumption and efficiency. The column targeting tool is used in the design of distillation columns by setting targets to reduce utility cost, improve energy efficiency, reduce capital investment and facilitate column debottlenecking. Starting from a short-cut distillation design calculation using the DSTWU method which is based on the well-known Fenske-Underwood-Gilliland correlations, the minimum and actual reflux ratios, minimum and actual number of stages, optimum feed location and condenser and reboiler duties were estimated. These estimates were used as starting points in the rigorous fractionation column design method RADFRAC available in Aspen Plus©. The column Grand Composite Curve (CGCC) for each column was generated to give an insight of the actual operation and guide the optimization process. Starting with appropriate feed placement, the CGCC will show the scope for reflux ratio modification by increasing the number of stages. Feed would be either preheated or precooled due to the availability of sharp enthalpy change in the condenser or reboiler side. Finally, the scope for a side condensing or side reboiling can be identified from the area beneath or above the CGCC pinch point.}
}
@article{LEE20062,
title = {Ontology management for large-scale enterprise systems},
journal = {Electronic Commerce Research and Applications},
volume = {5},
number = {1},
pages = {2-15},
year = {2006},
note = {International Workshop on Data Engineering Issues in E-Commerce (DEEC 2005)},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1567422305000827},
author = {Juhnyoung Lee and Richard Goodwin},
keywords = {Semantic web, Ontology, Management system, Inference},
abstract = {Semantic markup languages such as RDF (Resource Description Framework) [Resource Description Framework (RDF), http://www.w3.org/RDF/] and OWL (Web Ontology Language) [Web Ontology Language (OWL), http://www.w3.org/2004/OWL/] are increasingly being used to externalize meta-data or ontologies about data, software and services in a declarative form. Such externalized descriptions in ontological format are used for purposes ranging from search and retrieval to information integration and to service composition [Resource Description Framework (RDF): Projects and Applications, http://w3c.org/RDF/#projects, Web Ontology Language (OWL): Tools, Projects and Applications, http://w3c.org/2004/OWL/#projects]. Ontologies could significantly reduce the costs of deploying, integrating and maintaining enterprise systems. The barrier to more wide-spread use of ontologies for such applications is the lack of support in the currently available middleware stacks used in enterprise computing. This paper presents our work on developing an enterprise-scale ontology management system that will provide APIs and query languages, and scalability and performance that enterprise applications demand. We describe the design and implementation of the management system that programmatically supports the ontology needs of enterprise applications in a similar way a database management system supports the data needs of applications. In addition, we present a novel approach to representing ontologies in relational database tables to address the scalability and performance issues. The state of the art ontology management systems are either memory-based or use ad hoc solutions for persisting data, and so provide limited scalability.}
}
@article{SUN2016146,
title = {ROAR: A QoS-oriented modeling framework for automated cloud resource allocation and optimization},
journal = {Journal of Systems and Software},
volume = {116},
pages = {146-161},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215001715},
author = {Yu Sun and Jules White and Sean Eade and Douglas C. Schmidt},
keywords = {Cloud computing, Resource optimization, Load testing and benchmarking},
abstract = {Cloud computing offers a fast, easy and cost-effective way to configure and allocate computing resources for web applications, such as consoles for smart grid applications, medical records systems, and security management platforms. Although a diverse collection of cloud resources (e.g., servers) is available, choosing the most optimized and cost-effective set of cloud resources for a given web application and set of quality of service (QoS) goals is not a straightforward task. Optimizing cloud resource allocation is a critical task for offering web applications using a software as a service model in the cloud, where minimizing operational cost while ensuring QoS goals are met is critical to meeting customer demands and maximizing profit. Manual load testing with different sets of cloud resources, followed by comparison of test results to QoS goals is tedious and inaccurate due to the limitations of the load testing tools, challenges characterizing resource utilization, significant manual test orchestration effort, and challenges identifying resource bottlenecks. This paper introduces our work using a modeling framework – ROAR (Resource Optimization, Allocation and Recommendation System) to simplify, optimize, and automate cloud resource allocation decisions to meet QoS goals for web applications, including complex multi-tier application distributed in different server groups. ROAR uses a domain-specific language to describe the configuration of the web application, the APIs to benchmark and the expected QoS requirements (e.g., throughput and latency), and the resource optimization engine uses model-based analysis and code generation to automatically deploy and load test the application in multiple resource configurations in order to derive a cost-optimal resource configuration that meets the QoS goals.}
}
@article{HOUSTIS1989497,
title = {Parallel (‖) ELLPACK: an expert system for parallel processing of partial differential equations},
journal = {Mathematics and Computers in Simulation},
volume = {31},
number = {4},
pages = {497-507},
year = {1989},
note = {Special Double Issue},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(89)90143-2},
url = {https://www.sciencedirect.com/science/article/pii/0378475489901432},
author = {E.N. Houstis and J.R. Rice and T.S. Papatheodorou},
abstract = {We are developing an expert system environment for solving elliptic partial differential equations (PDEs) defined on two and three dimensional domains on MIMD type parallel machines. According to its design objectives, it will provide a uniform programming environment for implementing parallel MIMD PDE solution solvers, an automatic partitioning and allocation of the PDE computation, a very high level problem specification language, an interactive high level environment for grid selection, a domain partitioning and mapping facility, a uniform environment for obtaining software engineering measurements and a graphical display of solution output. The ‖ELLPACK is implemented on a hardware facility consisting of a graphics workstation supporting X11 window system and connected to NCUBE and SEQUENT machines through a wide bandwidth local network. The software infrastructure includes (i) a PDE problem oriented language processor, (ii) a geometry processing tool which is capable of generating fixed meshes and domain decompositions automatically and interactively, (iii) an algorithm mapper facility for partitioning and mapping the underlying PDE computation and (iv) an expert front end that selects the discretization mesh, the parallel algorithm⧸machine pair and its configuration. In order to support the solution of elliptic PDEs on fixed meshes, we are building a library of finite difference and element discretization methods and direct⧸iterative solvers for the NCUBE and SEQUENT machines.}
}
@article{CORMEN1997571,
title = {Early experiences in evaluating the parallel disk model with the ViC∗ implementation},
journal = {Parallel Computing},
volume = {23},
number = {4},
pages = {571-600},
year = {1997},
note = {Parallel I/O},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(97)00014-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167819197000148},
author = {Thomas H. Cormen and Melissa Hirschl},
keywords = {Parallelism, Data distribution, File systems, Parallel disk model, Input/output},
abstract = {Although several algorithms have been developed for the Parallel Disk Model (PDM), few have been implemented. Consequently, little has been known about the accuracy of the PDM in measuring I/O time and total running time to perform an out-of-core computation. This paper analyzes timing results on multiple-disk platforms for two PDM algorithms, out-of-core radix sort and BMMC permutations, to determine the strengths and weaknesses of the PDM. The results indicate the following. First, good PDM algorithms are usually not I/O bound. Second, of the four PDM parameters, one (problem size) is a good indicator of I/O time and running time, one (memory size) is a good indicator of I/O time but not necessarily running time, and the other two (block size and number of disks) do not necessarily indicate either I/O or running time. Third, because PDM algorithms tend not to be I/O bound, using asynchronous I/O can reduce I/O wait times significantly. The software interface to the PDM is part of the ViC ∗ run-time library. The interface is a set of wrappers that are designed to be both efficient and portable across several underlying file systems and target machines.}
}
@article{KAMEL197963,
title = {Optimum design of finite element software subject to core restrictions},
journal = {Computers & Structures},
volume = {10},
number = {1},
pages = {63-80},
year = {1979},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(79)90074-9},
url = {https://www.sciencedirect.com/science/article/pii/0045794979900749},
author = {H.A. Kamel and M.W. McCabe and P.G. DeShazo},
abstract = {Writing general purpose application code for mini-computers and time-sharing systems requires a carefully chosen compromise between program performance and conservation of computer resources. This report addresses itself to the design and implementation of such systems with special reference to the GIFTS 4 finite element package. The methods employed to design and implement the system are described. The stiffness assembly module of GIFTS is chosen as an illustration, and efforts aimed at speeding it up are described in detail, thus providing a representative case history. Results of similar efforts, involving other GIFTS modules, are presented. General guidelines are drawn, which should be of help to those attempting similar objectives. The paper is intended for engineering software designers, rather than for computer scientists, for whom some of the contents may seem obvious.}
}
@incollection{HANTUCH1980285,
title = {AUTOMATIC SYNTHESIS OF A PACKAGED PROGRAM FOR REAL TIME TECHNOLOGICAL PROCESS CONTROL},
editor = {M.A. CUENOD},
booktitle = {Computer Aided Design of Control Systems},
publisher = {Pergamon},
pages = {285-298},
year = {1980},
isbn = {978-0-08-024488-4},
doi = {https://doi.org/10.1016/B978-0-08-024488-4.50049-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244884500498},
author = {I. Hantuch},
abstract = {This contribution refers to certain problems bearing upon the formation of a system generator programing medium of an automatic synthesis for the modular construction of a packaged program for real time technological process control. The package is made up of standard subroutines-functional modules, carrying out the basic control functions. The user defines the task in the problem-oriented “graf language” designed by an analysis of project documentation for automated systems of technological process control. From the viewpoint of software, this is an establishment of a communication and synchronization system, implemented by standard connectors and starting subroutines, operating over the central database and by a system of tables for recording required items of information on the structure of the control package.}
}
@article{FOLINO201087,
title = {A grid portal for solving geoscience problems using distributed knowledge discovery services},
journal = {Future Generation Computer Systems},
volume = {26},
number = {1},
pages = {87-96},
year = {2010},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2009.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X09001058},
author = {Gianluigi Folino and Agostino Forestiero and Giuseppe Papuzzo and Giandomenico Spezzano},
keywords = {Grid computing, Geoscience applications, Grid services, Knowledge discovery services, Spatial data mining, Swarm intelligence},
abstract = {This paper describes our research effort to employ Grid technologies to enable the development of geoscience applications by integrating workflow technologies with data mining resources and a portal framework in unique work environment called MOSÈ. Using MOSÈ, a user can easily compose and execute geo-workflows for analyzing and managing natural disasters such as landslides, earthquakes, floods, wildfires, etc. MOSÈ is designed to be applicable both for the implementation of response strategies when emergencies occur and for disaster prevention. It takes advantage of the standardized resource access and workflow support for loosely coupled software components provided by web/grid services technologies. The integration of workflows with data mining services significantly improves data analysis. Geospatial data management and mining are critical areas of modern-day geosciences research. An important challenge for geospatial information mining is the distributed nature of the data. MOSÈ provides knowledge discovery services based on the WEKA data mining library and novel distributed data mining algorithms for spatial data analysis. A P2P bio-inspired algorithm for distributed spatial clustering as an example of distributed knowledge discovery service for intensive data analysis is presented. A real case application for the analysis of landslide hazard areas in the Campania Region near the Sarno area shows the advantages of using the portal.}
}
@article{TUWAIR2015262,
title = {Evaluation of sandwich panels with various polyurethane foam-cores and ribs},
journal = {Composites Part B: Engineering},
volume = {79},
pages = {262-276},
year = {2015},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2015.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S1359836815002619},
author = {Hesham Tuwair and Matthew Hopkins and Jeffery Volz and Mohamed A. ElGawady and Mohaned Mohamed and K. Chandrashekhara and Victor Birman},
keywords = {A. Foams, A. Glass fibres, C. Finite element analysis (FEA), C. Analytical modeling},
abstract = {The objective of this study was to evaluate three potential core alternatives for glass fiber reinforced polymer (GFRP) foam-core sandwich panels. The proposed system could reduce the initial production costs and the manufacturing difficulties while improving the system performance. Three different polyurethane foam configurations were considered for the inner core, and the most suitable system was recommended for further prototyping. These configurations consisted of high-density polyurethane foam (Type 1), a bidirectional gridwork of thin, interconnecting, GFRP webs that is in-filled with low-density polyurethane foam (Type 2), and trapezoidal-shaped, low-density polyurethane foam utilizing GFRP web layers (Type 3). The facings of the three cores consisted of three plies of bidirectional E-glass woven fabric within a compatible polyurethane resin. Several types of small-scale experimental investigations were conducted. The results from this study indicated that the Types 1 and 2 cores were very weak and flexible making their implementation in bridge deck panels less practical. The Type 3 core possessed a higher strength and stiffness than the other two types. Therefore, this type is recommended for the proposed sandwich system to serve as a candidate for further development. Additionally, a finite element model (FEM) was developed using software package ABAQUS for the Type 3 system to further investigate its structural behavior. This model was successfully compared to experimental data indicating its suitability for parametric analysis of panels and their design.}
}
@article{YEO2016398,
title = {Development of an automated modeler of environment and energy geographic information (E-GIS) for ecofriendly city planning},
journal = {Automation in Construction},
volume = {71},
pages = {398-413},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516301637},
author = {In-Ae Yeo and Jurng-Jae Yee},
keywords = {Climate change, Low-carbon green city, Urban planning support system (PSS), Environment and energy planning, Existing and planned city, Data integration, Automation},
abstract = {In this study, an Energy Integrated Support System (EnerISS) Modeler (or simply, Modeler), an automated module of a knowledge-based urban planning support system, was developed to support the strategic technology implementation of environmentally friendly local energy planning. Modeler was developed as a tool that integrates various formats of the planning data into a unified format during the planning stage. It automates the three-dimensional (3D) modeling of the integrated urban space and presents visual information to support decision-making processes. The system architecture of Modeler includes the formation of building polygons, textures for classified land cover shape, topography, and 3D urban space. Modeler was developed on a native-based software (S/W) platform. It was created with a single document frame (SDF) from the Windows Microsoft Foundation Class (MFC) Library and is driven by a 3D-based engine that can be used by a GIS client. Modeler was validated for a domestic city in the urban planning stage on a test bed platform. Because of the expansion of a ubiquitous environment and increasing social network services (SNS), the ability to port the Modeler to a web-based platform should be investigated, thus enabling easier access for decision makers.}
}
@article{KUMAR1989164,
title = {Developing strategies and philosophies early for successful project implementation},
journal = {International Journal of Project Management},
volume = {7},
number = {3},
pages = {164-171},
year = {1989},
issn = {0263-7863},
doi = {https://doi.org/10.1016/0263-7863(89)90035-5},
url = {https://www.sciencedirect.com/science/article/pii/0263786389900355},
author = {D Kumar},
keywords = {project management, turnkey projects, engineering contractor, database software, computeraided design, project strategy, project philosophy},
abstract = {In today's competitive environment, project managers are faced with ever increasing demands for compressed schedules, minimum costs, quality construction and safe plant. There is also an increased tendency to award work on a turnkey basis in view of the individual responsibility entailed. The engineering contractor is required to face very large risks in terms of finance and reputation, yet cannot expect handsome profits because of cut-throat competition and market conditions. The revolution in microcomputer technology and the development of advanced database software packages and computeraided design have greatly enhanced management capabilities, but at the same time these techniques require large investments. Under these circumstances, the single most important factor for achieving success is the early development of major project strategies and philosophies. In this paper, an attempt is made to cover various issues related to project strategies and philosophies. An unorthodox approach is adopted by taking a sample project and gradually developing project strategies and philosophies with side discussions on major issues of importance.}
}
@article{LOPEZALMANSA2012108,
title = {A numerical model of the structural behavior of buckling-restrained braces},
journal = {Engineering Structures},
volume = {41},
pages = {108-117},
year = {2012},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2012.03.045},
url = {https://www.sciencedirect.com/science/article/pii/S0141029612001708},
author = {F. López-Almansa and J.C. Castro-Medina and S. Oller},
keywords = {Energy dissipators, Buckling-restrained braces, Passive control, Numerical simulation, Damage model, Plasticity},
abstract = {This work presents a numerical model of the cyclic structural behavior of dissipative buckling-restrained braces, commonly used as an alternative to classical concentric braces for seismic protection of building frames and other structures. Such devices are usually composed of a slender steel core embedded in a stockiest casing that is intended to prevent its buckling when it is under compression. The casing is made either of mortar or steel, and a sliding interface is interposed between the core and the casing to prevent excessive shear stress transfer. The behavior of the steel core is described by a damage and plasticity model; the behavior of the mortar casing is described by an isotropic damage model and the sliding behavior of the interface is described by a contact penalty model. These three models are implemented in the Abaqus software package following an explicit formulation. The ability of this algorithm to reproduce the cyclic behavior of buckling-restrained braces is verified in a number of representative yet simple situations. The accuracy of the proposed model is checked by comparison with experimental results; a satisfactory agreement is obtained. Preliminary conclusions are issued and further research needs are identified.}
}
@article{DENHAM1983667,
title = {A Robust Computational Approach to Control System Analysis and Design},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {19},
pages = {667-672},
year = {1983},
note = {2nd IFAC Symposium on computer Aided Design of Multivariable Technological Systems, West Lafayette, USA, 15-17 September 1982},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)61751-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017617518},
author = {M.J. Denham and C.J. Benson and T.W.C. Williams},
keywords = {computer aided design, control system analysis, control system synthesis, multivariable control systems, numerical methods, computer software},
abstract = {For many years the theory of control system analysis and design has been developed with little consideration for the reliable numerical computation of a solution to the analysis or design problem. The growing industrial interest in the use of recently developed multivariable control system design methods on modern, complex systems has exposed and highlighted the computational deficiencies of these methods and the need for a reformulation of the problems in a way which will directly result in efficient and numerically sound computational algorithms for their solution. Many of the results of recent research in numerical linear algebra have direct relevance to this, and this paper demonstrates how the theory underlying modern control system design methods can be represented in a way which uses these results. The implementation of the resulting algorithms in software and the design of a subroutine library for control engineering (SLICE) to contain the algorithms is also described. Some computational results are given to demonstrate the performance of the algorithms.}
}
@article{JAMSA1987167,
title = {Experiences in Control System Design of Phosphate Ore Grinding in Siilinjărvi Concentrator},
journal = {IFAC Proceedings Volumes},
volume = {20},
number = {8},
pages = {167-174},
year = {1987},
note = {5th IFAC Symposium on Automation and Mining, Mineral and Metal Processing 1986, Tokyo, Japan, 24-29 August, 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)59087-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017590874},
author = {S.L. Jämsä and J. Aaltonen},
keywords = {Multivariable control systems, Inverse Nyquist Array method, Computer-aided control system design, Phosphate ore grinding},
abstract = {The multivariable control that was designed for the grinding circuit in the Siilinjärvi concentrator can control important variables like the final particle size and the density of the cyclone feed. The controller was designed for the plant using an interactive CACSD software package which included the use of the inverse Nyquist array method. The control algorithms designed were implemented in the process control computer at Siilinjärvi and the correlation between the simulated results and the full scale tests in short term experiments was good. Changes in the setpoint of either of the controlled output variables produced satisfactory responses, while at the same time the other controlled output remained close to the designed constant value. The multivariable controller designed has not yet been taken into continuous use at Siilinjärvi. The large variation of mineral components in the ore has caused difficulties in the operation of the particle size analyser. The potential use of the PSM analyser for on-line control in the Siilinjärvi grinding process is discussed in the paper.}
}
@article{LAURIA19974,
title = {MPI-FM: High Performance MPI on Workstation Clusters},
journal = {Journal of Parallel and Distributed Computing},
volume = {40},
number = {1},
pages = {4-18},
year = {1997},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.1264},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596912642},
author = {Mario Lauria and Andrew Chien},
abstract = {Despite the emergence of high speed LANs, the communication performance available to applications on workstation clusters still falls short of that available on MPPs. A new generation of efficient messaging layers is needed to take advantage of the hardware performance and to deliver it to the application level. Communication software is the key element in bridging the communication performance gap separating MPPs and workstation clusters. MPI-FM is a high performance implementation of Message Passing Interface (MPI) for networks of workstations connected with a Myrinet network, built on top of the Fast Messages (FM) library. Based on the FM version 1.1 released in Fall 1995, MPI-FM achieves a minimum one-way latency of 19 μs and a peak bandwidth of 17.3 Mbyte/s with common MPI send and receive function calls. A direct comparison using published performance figures shows that MPI-FM running on SPARCstation 20 workstations connected with a relatively inexpensive Myrinet network outperforms the MPI implementations available on the IBM SP2 and the Cray T3D, both in latency and in bandwidth, for messages up to 2 kbyte in size. We describe the critical performance issues found in building a high level messaging library (MPI) on top of a low level messaging layer (FM), and the design solutions we adopted for them. One such issue was the direct and efficient support of common operations like adding and removing a header. Another was the exchange of critical information between the layers, like the location of the destination buffer. These two optimizations are both shown to be necessary, and their combination sufficient to achieve the aforementioned level of performance. The performance contribution of each of these optimizations is examined in some detail. These results delineate a new design approach for low level communication layers in which a closer integration with the upper layer and an appropriate balance of the communication pipeline stages are the key elements for high performance.}
}
@article{BURNETT199849,
title = {An ellipsoidal acoustic infinite element},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {164},
number = {1},
pages = {49-76},
year = {1998},
note = {Exterior Problems of Wave Propagation},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(98)00046-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782598000462},
author = {David S. Burnett and Richard L. Holford},
abstract = {In previous papers the authors presented new 3-D time-harmonic prolate and oblate spheroidal acoustic infinite elements, based on new prolate and oblate spheroidal multipole expansions, for modeling acoustic fields in unbounded domains. Here, we present the development of an ellipsoidal infinite element, which is the logical generalization of those elements. This development is also based on a new multipole expansion as well as a new system of ellipsoidal coordinates. The element stiffness, radiation-damping and mass matrices are developed and presented in sufficient detail to enable their software implementation. Both the new coordinate system and the new element include the previous spheroidal coordinate systems and spheroidal elements as limiting cases. Therefore, all the previously reported performance data for the spheroidal elements apply to this ellipsoidal element when used in spheroidal form. Since the three axes of an ellipsoid can be chosen independently, an ellipsoid can circumscribe any structural shape at least as closely, and generally more closely, than a prolate or oblate spheroid. The resulting reduction in size of the finite computational domain will result in even greater computational speeds than those already reported for the spheroidal elements. The element may be used to model problems in free-space (4π steradians), half-space (2π), quarter-space (π) or eighth-space (π2). Since this infinite element provides maximum computational efficiency for structures of all shapes, software element libraries would need only this one element for all problems in unbounded domains.}
}
@article{ZHANG2022105416,
title = {Simulation of variable-rate manure application under different application scenarios},
journal = {Soil and Tillage Research},
volume = {221},
pages = {105416},
year = {2022},
issn = {0167-1987},
doi = {https://doi.org/10.1016/j.still.2022.105416},
url = {https://www.sciencedirect.com/science/article/pii/S0167198722001027},
author = {Jian Zhang and Angela Guerrero and Abdul M. Mouazen},
keywords = {Variable rate manure application, Software package, Simulation, Uniform rate manure application},
abstract = {Although the variable-rate manure application (VRMA) offers a promising solution to overcome the shortcomings of the conventional uniform-rate (UR) application, effectively incorporating this technique into precision agriculture solutions presents a substantial challenge. This study entailed developing a quantitative analysis tool to calculate manure consumption, evaluate environmental risks, and compare different treatment schemes under different implementation scenarios. Firstly, a simulator was designed and developed in the LabVIEW development environment with MATLAB. Next, this tool was used to compare traditional UR application with VRMA, by which manure application rate was varied according to soil phosphorus (P) maps measured with an on-line visible and near infrared soil sensor, or according to data-fusion (DF)-based management zone (MZ) maps, developed by fusion of on-line measured soil properties and crop normalized difference vegetation index (NDVI). In the DF case, simulations included Robin Hood (RH) and Kings approaches, which meant to apply the largest manure application rate at the poorest fertility zones, or in the richest fertility zones, respectively. The implementation scenarios included both map-based (MB) without accounting for on-line manure sensing and map-sensor-based (MSB) with on-line manure sensing in three commercial fields in Flanders, Belgium. The results revealed that the P-based-VRMA scheme consumed 1–13% less nitrogen (N) and P2O5 in the case of ignoring legislative limits, compared to the UR and this was true for the RH-VRMA scheme in all three fields and for the Kings-VRMA scheme in two out of three fields. This was because the P-based maps have larger rich fertility areas (e.g., large P concentration) than the MZ maps that both the RH and Kings approaches used. Furthermore, imposing the “MAP6” legislation limits caused all VRMA schemes to consume less fertilizer than would be the case without restrictions, reducing environmental risks due to decreasing the amount of applied N and P2O5. According to the simulation results, the MSB scenario saved 6–9% manure compared to the MB scenario in the P-based-VRMA scheme when the mean value of real-time sensed P2O5 in the applied manure was greater than the nominal values measured in the laboratory. Opposite results were observed in the UR, RH-, and Kings-VRMA schemes. The MSB was more expensive than the MB by 5–7%, when the mean N content of manure used for MSB was lower than that for MB. We concluded that the manure consumed in VRMA depends on treatment schemes, measured manure quality, imposing legislative limits, and the proportion of rich, medium, and poor fertility areas in a field. It is essential to impose the legislative limits to prevent over-applications of N and P2O5, while further agronomy study should evaluate the impact of these limits on crop responses.}
}
@article{OZELL1995295,
title = {Analysis and visualization tools in CFD, part I: A configurable data extraction environment},
journal = {Finite Elements in Analysis and Design},
volume = {19},
number = {4},
pages = {295-307},
year = {1995},
note = {Graphics and Visualization for Finite Element Modeling},
issn = {0168-874X},
doi = {https://doi.org/10.1016/0168-874X(94)00069-R},
url = {https://www.sciencedirect.com/science/article/pii/0168874X9400069R},
author = {B. Ozell and R. Camarero and A. Garon and F. Guibault},
abstract = {The objective is to present new ideas for the implementation of visualization and analysis environments. This is carried out with a software interface allowing the design and configuration of project-specific environments built around a core library that performs all the data extraction and manipulation. This includes processing of solutions for rendering as well as for extraction of flow features for active control on the solution procedure. While the first function is highly interactive, the second is usually a batch oriented process coupled to the numerical algorithm. To achieve this goal, it is necessary to interpret solution data in a generic sense. The grid, the primary variables of the problem, and the derived variables are all considered as a number of simple scalar (discrete) fields defined on the domain. Basic entities in the library then provide building blocks to create images for rendering purposes or new fields as required in the grid adaption control loop. A data analysis language has been created. Its entities and grammar are oriented towards the description of both the rendering and the analysis processes. This customized environment is saved in an user configuration file, with an easily understandable syntax, which is loaded at execution time, hence providing the proper configuration for each application.}
}
@article{LOPEZFERNANDEZ20181,
title = {S2P: A software tool to quickly carry out reproducible biomedical research projects involving 2D-gel and MALDI-TOF MS protein data},
journal = {Computer Methods and Programs in Biomedicine},
volume = {155},
pages = {1-9},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2017.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717310702},
author = {Hugo López-Fernández and José E. Araújo and Susana Jorge and Daniel Glez-Peña and Miguel Reboiro-Jato and Hugo M. Santos and Florentino Fdez-Riverola and José L. Capelo},
keywords = {Protein identification, Data processing, 2D-gel, MALDI-TOF-MS, LC-MS/MS, emPAI},
abstract = {Background and objective
2D-gel electrophoresis is widely used in combination with MALDI-TOF mass spectrometry in order to analyze the proteome of biological samples. For instance, it can be used to discover proteins that are differentially expressed between two groups (e.g. two disease conditions, case vs. control, etc.) thus obtaining a set of potential biomarkers. This procedure requires a great deal of data processing in order to prepare data for analysis or to merge and integrate data from different sources. This kind of work is usually done manually (e.g. copying and pasting data into spreadsheet files), which is highly time consuming and distracts the researcher from other important, core tasks. Moreover, engaging in a repetitive process in a non-automated, handling-based manner is prone to error, thus threatening reliability and reproducibility. The objective of this paper is to present S2P, an open source software to overcome these drawbacks.
Methods
S2P is implemented in Java on top of the AIBench framework, and relies on well-established open source libraries to accomplish different tasks.
Results
S2P is an AIBench based desktop multiplatform application, specifically aimed to process 2D-gel and MALDI-mass spectrometry protein identification-based data in a computer-aided, reproducible manner. Different case studies are presented in order to show the usefulness of S2P.
Conclusions
S2P is open source and free to all users at http://www.sing-group.org/s2p. Through its user-friendly GUI interface, S2P dramatically reduces the time that researchers need to invest in order to prepare data for analysis.}
}
@article{DIMIROVSKI1989257,
title = {Expert System for Recognition and Typical Identification of Dynamic Process Models},
journal = {IFAC Proceedings Volumes},
volume = {22},
number = {6},
pages = {257-262},
year = {1989},
note = {IFAC Symposium on Advanced Information Processing in Automatic Control (AIPAC'89), Nancy, France, 3-5 July 1989},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54382-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017543827},
author = {G.M. Dimirovski and B.L. Crvenkovski and D.M. Joskovski},
abstract = {Expert systems (ES) for support In Inglnerlng application are attracting extensive research Investments with respect to all resources recently. ES for support In pattern recognition, model selection and Identification, skilled value judgement and decision making In large scale computerized systems for supervision, control and management of complex tehnologilcal objects are to be come Indlspenclble. They have to be based on both Artical Intelligence (Al) and Systems Engineering (SE) techniques. There has been designed and Implemented with us an Al system, with the nessacery Interactive package for a personal computer, called ESTIO and aimed at process behaviour recognition and typical Identification using certain classes of well posed models In the role of a knowledge - base. It has been used for Identification purposes by people with basic knowledge In systems and control, and no knowledge In Al and software engineering}
}
@incollection{SCHMID1986565,
title = {A WORKSTATION CONCEPT FOR COMPUTER AIDED ANALYSIS AND DESIGN OF CONTROL SYSTEMS},
editor = {M. PAUL},
booktitle = {Digital Computer Applications to Process Control},
publisher = {Pergamon},
address = {Oxford},
pages = {565-568},
year = {1986},
series = {IFAC Symposia Series},
isbn = {978-0-08-032554-5},
doi = {https://doi.org/10.1016/B978-0-08-032554-5.50084-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325545500847},
author = {C. Schmid},
abstract = {Abstract
The KEDDC software package for computer-aided analysis and control systems design, developed at Ruhr-University, Bochum, F.R.G. is presently being prepared for a supermicro-based workstation implementation. A bit map display plays a key role for the design of a concurrent user environment, where mixed alphanumerics and graphics are used to handle a comprehensive system of control tools in a natural way for control engineers.}
}
@article{JOHAN1992113,
title = {A data parallel finite element method for computational fluid dynamics on the Connection Machine system},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {99},
number = {1},
pages = {113-134},
year = {1992},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(92)90124-3},
url = {https://www.sciencedirect.com/science/article/pii/0045782592901243},
author = {Zdeněk Johan and Thomas J.R. Hughes and Kapil K. Mathur and S.Lennart Johnsson},
abstract = {A finite element method for computational fluid dynamics has been implemented on the Connection Machine systems CM-2 and CM-200. An implicit iterative solution strategy, based on the preconditioned matrix-free GMRES algorithm, is employed. Parallel data structures built on both nodal and elemental sets are used to achieve maximum parallelization. Communication primitives provided through the Connection Machine Scientific Software Library substantially improved the overall performance of the program. Computations of three-dimensional compressible flows using unstructured meshes having close to one million elements, such as a complete airplane, demonstrate that the Connection Machine systems are suitable for these applications. Performance comparisons are also carried out with the vector computers Cray Y-MP and Convex C-1.}
}
@article{BUSCHKE1987471,
title = {Computer aided evalution of simulation results, the Simueva program package},
journal = {Microprocessing and Microprogramming},
volume = {21},
number = {1},
pages = {471-478},
year = {1987},
note = {Microcomputers: Usage, Methods and Structures},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(87)90079-2},
url = {https://www.sciencedirect.com/science/article/pii/0165607487900792},
author = {Rainer Buschke and Klaus Lagemann},
abstract = {This paper describes the development, current state and next steps of the SIMUEVA project. The project aims at exploration and implementation of tools assisting the user in efficient and reliable evaluation of simulation results. A short introduction discusses reasons for such tools with respect to the current CAD software for simulation of integrated circuits. After specifying several levels of assistance, the already implemented levels of SIMUEVA are described focusing on user interaction and solutions for implementation. This includes data models, storage and access, techniques for the extraction of relevant simulation results and timing informations as parts of the first level. A second level of assistance is made up by automatic comparison of simulation results. This includes comparison to a predefined description of the expected simulation results using a special language called EXPRESS as well as the comparison of different simulation results. The paper ends with some issues on required circuit structure data and retracing information for the third level of assistance, which will be implemented during this year. For a more general description of the project refer to [1].}
}
@article{TRUFIAKOV1976193,
title = {Two-Level Computer-Aided System Software For Fatigue Tests Of Welded Structures},
journal = {IFAC Proceedings Volumes},
volume = {9},
number = {2},
pages = {193-196},
year = {1976},
note = {1st IFAC/IFIP Symposium on Software For Computer Control, Tallinn, USSR, 25-28 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)67424-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701767424X},
author = {V.I. Trufiakov and V.I. Dvoretsky and B.N. Malinovsky and V.P. Soloviov and N.I. Alishov},
abstract = {This report describes an external algorithm of operation as well as composition and structure of a two-level computer-aided system software for fatigue tests of welded structures. The software of the system is capable for orientation toward different configurations of testing equipment and peripherals and is invariant to some extent, toward a variation of experiment methods. The foundation of the software system is a subsystem of test control and a subsystem of processing and generalizing test results. Operation efficiency of both basic subsystems is provided with a coordinator program. An efficient experimenter-system interaction at different levels of the system and at different stages of the experiment is reached due to the presence of advanced languages resources. The availability of a library of performed test results which is permanently being completed, gives an additional effect of decreasing the investment for implementation of fatigue tests. A part of the software system under consideration is being put into experimental operation and the other one is at the stage of implementation.}
}
@incollection{HOGG1986249,
title = {COMPUTER AIDED CONTROL SYSTEM DESIGN FOR POWER GENERATION PLANT},
editor = {P. MARTIN LARSEN and N.E. HANSEN},
booktitle = {Computer Aided Design in Control and Engineering Systems},
publisher = {Pergamon},
pages = {249-254},
year = {1986},
isbn = {978-0-08-032557-6},
doi = {https://doi.org/10.1016/B978-0-08-032557-6.50050-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325576500509},
author = {B.W. Hogg and E. Swidenbank and D.G. Morrell},
abstract = {The paper describes the development of a computer-aided modelling and control-system-design facility for power generation plant. This has been fully implemented on a laboratory model turbogenerator, and extensive tests have been conducted on full-scale plant. The system is based on a dedicated PDP 11–23 microcomputer, with background processing on a VAX 11–780 minicomputer, using specially-written software packages. Results obtained both in the laboratory and from site tests are presented.}
}
@incollection{VANCASTEREN1983423,
title = {VALIDATION OF THE IN-ORBIT CHECKOUT OF THE IRAS GYROSCOPES USING COMPUTER SIMULATIONS},
editor = {P.TH.L.M. {VAN WOERKOM}},
booktitle = {Automatic Control in Space 1982},
publisher = {Pergamon},
pages = {423-432},
year = {1983},
isbn = {978-0-08-029328-8},
doi = {https://doi.org/10.1016/B978-0-08-029328-8.50049-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080293288500499},
author = {J.F.M. {van Casteren} and R.M. {van Doom}},
abstract = {Abstract
The IRAS gyroscope package has to be calibrated shortly after launch in order to satisfy accuracy requirements. Validation of the designed in-flight calibration approach and the method of test evaluation as well as verification of the corresponding obtainable accuracy has to take place beforehand. Furthermore, to achieve a rapid test turnaround, (desk top) computer software was developed and fill-in formats were designed, both of which have to be checked on their adequacy. Computer simulation is one of the tools to accomplish these tasks. The implementation of a realistic mathematical gyroscope sensor model and a satellite model in the truth-domain is essential in this respect. The gyroscope sensor is shortly described, its mathematical model and its implementation in a modular software simulation package is elaborated on. The simulation results and subsequent test evaluation indicate that the gyro checkout procedure is adequate, and that the above mentioned tasks are performed successfully. The obtained gyro calibration accuracy corresponds to the results of an earlier performed accuracy analysis.}
}
@article{WASER2018116,
title = {Fast and experimentally validated model of a latent thermal energy storage device for system level simulations},
journal = {Applied Energy},
volume = {231},
pages = {116-126},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.09.061},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918313692},
author = {R. Waser and F. Ghani and S. Maranda and T.S. O'Donovan and P. Schuetz and M. Zaglio and J. Worlitschek},
keywords = {Latent thermal energy storage, Numerical model, Thermal characterisation, Phase change material, Energy system simulations},
abstract = {Latent storages utilising phase change materials (PCM) to store thermal energy offer a considerably higher energy density at a nearly constant temperature level in comparison to sensible storage systems. Despite this advantage, only a few latent storage technologies have been integrated successfully to the market. This may be due several engineering challenges and in particular the lack of a computationally fast and accurate mathematical model to facilitate the optimal incorporation of latent heat storages into an energy system. The presented study fills this gap and proposes a new, fast and experimentally validated mathematical modelling approach for latent heat storage units. The numerical model proposed combines high accuracy, low computational effort and numerical stability. The validation was performed with two different commercial latent storage units supplied by Sunamp Ltd. with a nominal phase change temperatures of 34 °C and of 58 °C. Both units use a salt hydrate based phase change material in combination with a fin-tube heat exchanger. The proposed model may be used for both fast system level performance investigations as well as latent storage design for a given application. It may therefore be implemented in commercial software packages such as TRNSYS [1] or Simulink [2].}
}
@article{WIGREN2006159,
title = {Recursive prediction error identification and scaling of non-linear state space models using a restricted black box parameterization},
journal = {Automatica},
volume = {42},
number = {1},
pages = {159-168},
year = {2006},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2005.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S000510980500316X},
author = {Torbjörn Wigren},
keywords = {Identification algorithms, Non-linear systems, Prediction error methods, Recursive, Sampling, State-space models},
abstract = {A recursive prediction error algorithm for identification of systems described by non-linear ordinary differential equation (ODE) models is presented. The model is an ODE model, parameterized with coefficients of a multi-variable polynomial that describes one component of the right-hand side function of the ODE. This avoids over-parameterization problems. The selected model can also handle systems with more complicated right-hand side structure, by identification of a local input–output equivalent system in the coordinate system of the selected state variables. A novel technique based on scaling of the sampling period is proposed. The technique can improve the conditioning of the identification problem, thereby enhancing the chances of convergence to the correct minimum of the criterion. The algorithm is applied to live data from a system consisting of two cascaded tanks, with promising results. A MATLAB software package, which implements the proposed algorithm and a set of support scripts, can be freely downloaded from http://www.it.uu.se/research/reports/.}
}
@article{BAZILIAN200257,
title = {Modelling of a photovoltaic heat recovery system and its role in a design decision support tool for building professionals},
journal = {Renewable Energy},
volume = {27},
number = {1},
pages = {57-68},
year = {2002},
issn = {0960-1481},
doi = {https://doi.org/10.1016/S0960-1481(01)00165-3},
url = {https://www.sciencedirect.com/science/article/pii/S0960148101001653},
author = {Morgan D. Bazilian and Deo Prasad},
abstract = {A numerical model has been created to simulate the performance of a residential-scale building integrated photovoltaic (BiPV) cogeneration system. The investigation examines the combined heat and power system in the context of heat transfer. The PV cogeneration system will be based on existing BiPV roofing technology with the addition of a modular heat recovery unit that can be used in new or renovation construction schemes. The convection of the air behind the panels will serve to cool the PV panels while providing a heat source for the residence. This model was created in the Engineering Equation Solver software package (EES), from a series of highly coupled non-linear partial differential equations that are solved iteratively. The model's ability to utilize climatic data to simulate annual performance of the system will be presented along with a comparison to experimental data. A graphical front-end has been added to the model in order to facilitate its use as a predictive tool for building professionals. It will thus become a decision support tool used in identifying areas for implementation of a PV cogen system.}
}
@article{SCHISSEL2000105,
title = {Enhanced computational infrastructure for data analysis at the DIII–D National Fusion Facility},
journal = {Fusion Engineering and Design},
volume = {48},
number = {1},
pages = {105-111},
year = {2000},
issn = {0920-3796},
doi = {https://doi.org/10.1016/S0920-3796(00)00141-1},
url = {https://www.sciencedirect.com/science/article/pii/S0920379600001411},
author = {D.P Schissel and Q Peng and J Schachter and T.B Terpstra and T.A Casper and J Freeman and R Jong and K.M Keith and B.B McHarg and W.H Meyer and C.T Parker},
keywords = {Infrastructure, Data analysis, DIII–D},
abstract = {Recently a number of enhancements to the computer hardware infrastructure have been implemented at the DIII–D National Fusion Facility. Utilizing these improvements to the hardware infrastructure, software enhancements are focusing on streamlined analysis, automation, and graphical user interface (GUI) systems to enlarge the user base. The adoption of the load balancing software package LSF Suite by Platform Computing has dramatically increased the availability of CPU cycles and the efficiency of their use. Streamlined analysis has been aided by the adoption of the MDSplus system to provide a unified interface to analyzed DIII–D data. The majority of MDSplus data is made available in between pulses giving the researcher critical information before setting up the next pulse. Work on data viewing and analysis tools focuses on efficient GUI design with object–oriented programming (OOP) for maximum code flexibility. Work to enhance the computational infrastructure at DIII–D has included a significant effort to aid the remote collaborator since the DIII–D National Team consists of scientists from nine national laboratories, 19 foreign laboratories, 16 universities, and five industrial partnerships. As a result of this work, DIII–D data is available on a 24×7 basis from a set of viewing and analysis tools that can be run on either the collaborators’ or DIII–D's computer systems. Additionally, a web based data and code documentation system has been created to aid the novice and expert user alike.}
}
@article{PARSONS1988127,
title = {CTRL-C and ACSL Used in the Teaching of Control Engineering},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {6},
pages = {127-131},
year = {1988},
note = {IFAC Symposium on Trends in Control and Measurement Education, Swansea, UK, 11-13 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53851-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017538513},
author = {J.T. Parsons and P.R Warner and A.S. White},
keywords = {Teaching, Control, Simulation, Computers, Computer software, Pole placement, Root loci, Heat exchangers, Vibration control},
abstract = {In the new Finniston B.Eng. courses greater emphasis is placed on innovation anddesign and this has led to a reappraisal of teaching methods, highlighting synthesis rather than analysis in problem solving. Although much of the analytical material remains in the courses, computer software packages are used to free the students from repetitious and tedious mathematical work in order to concentrate on the study and design of systems that bear a closer resemblance to those that might be met in practice. CTRL-C and ACSL are sophisticated computer packages which permit the modelling of complex systems using both classical and modern techniques. These have been installed on a VAX 785 minicomputer. Student centred learning is achieved by implementing models/problems in the computer's public domain so that the students may have access to the programs and work through at their own pace. Links between the lecture and practice are enhanced by setting up workstations in the laboratory. The students have adopted the scheme most enthusiastically and are inhibited only by the number of interactive terminals that can adequately be accommodated at any one time by the VAX computer. The assessment of the student's performance is matched to the change in the learning process and caters for the role which CTRL-C and ACSL play in developing engineering competence. The method is cost effective, a feature that must in the future play an ever increasing role in the running of engineering courses.}
}
@incollection{NACSA199471,
title = {STEPS TOWARDS REAL-TIME CONTROL USING KNOWLEDGE BASED SIMULATION OF FLEXIBLE MANUFACTURING SYSTEMS},
editor = {K.-E. ARZEN},
booktitle = {Computer Software Structures Integrating Ai/kbs Systems in Process Control},
publisher = {Pergamon},
address = {Oxford},
pages = {71-74},
year = {1994},
series = {IFAC Postprint Volume},
isbn = {978-0-08-042360-9},
doi = {https://doi.org/10.1016/B978-0-08-042360-9.50014-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080423609500142},
author = {J. NACSA and G.L. KOVÁCS},
abstract = {Computer aided simulation can assist both in the design and operation of flexible manufacturing systems (FMS). A proper simulation model of a given FMS could be the best tool to validate it and to evaluate its performance. The control of the system can be solved by separating the control mechanism from the real devices of the FMS with a communication interface. In this case it is possible to use the simulated system for investigations instead of the real system. Such simulation systems can be built up using expert system shells. In this paper an FMS simulation system (SSQA) will be introduced with the hybrid application of expert systems and a traditional simulation software. The system has some evaluation, scheduling and quality control power as well, according to the implemented advisory systems that communicate with the simulation package.}
}
@article{BAJAJ2000391,
title = {Compression-Based 3D Texture Mapping for Real-Time Rendering},
journal = {Graphical Models},
volume = {62},
number = {6},
pages = {391-410},
year = {2000},
issn = {1524-0703},
doi = {https://doi.org/10.1006/gmod.2000.0532},
url = {https://www.sciencedirect.com/science/article/pii/S1524070300905320},
author = {Chandrajit Bajaj and Insung Ihm and Sanghun Park},
abstract = {While 2D texture mapping is one of the most effective of the rendering techniques that make 3D objects appear visually interesting, it often suffers from visual artifacts produced when 2D image patterns are wrapped onto the surfaces of objects with arbitrary shapes. On the other hand, 3D texture mapping generates highly natural visual effects in which objects appear carved from lumps of materials rather than laminated with thin sheets as in 2D texture mapping. Storing 3D texture images in a table for fast mapping computations, instead of evaluating procedures on the fly, however, has been considered impractical due to the extremely high memory requirement. In this paper, we present a new effective method for 3D texture mapping designed for real-time rendering of polygonal models. Our scheme attempts to resolve the potential texture memory problem by compressing 3D textures using a wavelet-based encoding method. The experimental results on various nontrivial 3D textures and polygonal models show that high compression rates are achieved with few visual artifacts in the rendered images and a small impact on rendering time. The simplicity of our compression-based scheme will make it easy to implement practical 3D texture mapping in software/hardware rendering systems including real-time 3D graphics APIs such as OpenGL and Direct3D.}
}
@incollection{BLANDFORD1981165,
title = {CAL SOFTWARE DESIGN FOR TRANSFERABILITY},
editor = {PR SMITH},
booktitle = {Computer Assisted Learning},
publisher = {Pergamon},
address = {Amsterdam},
pages = {165-174},
year = {1981},
series = {Computers and Education},
isbn = {978-0-08-028111-7},
doi = {https://doi.org/10.1016/B978-0-08-028111-7.50030-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080281117500301},
author = {C. BLANDFORD},
abstract = {The successful transfer of CAL software is often thwarted by differences between the computer system on which the software was developed and that on which it is required to be implemented. Facilities which CAL programs may exploit are usually those most vulnerable to change between different computer systems. Drawing upon experience gained in the design, programming, and subsequent transfer of CAL programs in engineering, examples are given of features which may cause problems during implementation and alternatives or improvements are suggested. FORTRAN and GINO-F are proposed as machine and device independent standards, for implementation language and graphics subroutine library, respectively. Good programming practices are described with a view to obtaining readable and easily maintained programs of a sound quality. Importance is placed on modularity in design as the key to writing transportable programs. The idea that every module should conceal some design decisions from all other modules is described with reference to modules that have proved to be particularly useful or advisable. This technique enables a design to be produced in which machine dependencies and difficult design decisions may be confined to a handful of routines. This makes the program more adaptable and gives the teacher and programmer greater freedom to change the implementation, allowing the program to move to any computer system where similar functions can be implemented.}
}
@article{MAHLKE2019100289,
title = {The ssos pipeline: Identification of Solar System objects in astronomical images},
journal = {Astronomy and Computing},
volume = {28},
pages = {100289},
year = {2019},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2019.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2213133719300071},
author = {M. Mahlke and E. Solano and H. Bouy and B. Carry and G.A. {Verdoes Kleijn} and E. Bertin},
keywords = {Solar system objects, Asteroids, Imaging data, Pipeline, Planetary formation, Data archives},
abstract = {Observatories and satellites around the globe produce tremendous amounts of imaging data to study many different astrophysical phenomena. The serendipitous observations of Solar System objects are a fortunate by-product which have often been neglected due to the lack of a simple yet efficient identification algorithm. Meanwhile, the determination of the orbit, chemical composition, and physical properties such as rotation period and 3D-shape of Solar System objects requires a large number of astrometry and multi-band photometry observations. Such observations are hidden in current and future astrophysical archives, and a method to harvest these goldmines is needed. This article presents an easy-to-implement, light-weight software package which detects bodies of the Solar System in astronomical images and measures their astrometry and photometry. The ssos pipeline is versatile, allowing for application to all kinds of observatory imaging products. The sole principle requirement is that the images observe overlapping areas of the sky within a reasonable time range. Both known and unknown Solar System objects are recovered, from fast-moving near-Earth asteroids to slow objects in the distant Kuiper belt. The high-level pipeline design and two test applications are described here, highlighting the versatility of the algorithm with both narrow-field pointed and wide-field survey observations. In the first study, 2,828 detections of 204 SSOs are recovered from publicly available images of the GTC OSIRIS Broad Band DR1 (Cortés-Contreras, in preparation). The false-positive ratio of SSO detections ranges from 0%-23% depending on the pipeline setup. The second test study utilizes the images of the first data release of J-PLUS, a 12-band optical survey. 4,606 SSO candidates are recovered, with a false-positive ratio of (2.0± 0.2)%. A stricter pipeline parameter setup recovers 3,696 candidates with a sample contamination below 0.68%.}
}
@article{SHEPPARD1983190,
title = {A decision support system for urban pavement management},
journal = {Advances in Engineering Software (1978)},
volume = {5},
number = {4},
pages = {190-195},
year = {1983},
issn = {0141-1195},
doi = {https://doi.org/10.1016/0141-1195(83)90046-3},
url = {https://www.sciencedirect.com/science/article/pii/0141119583900463},
author = {Sallie Sheppard and Leland Blank},
abstract = {The design, development, and use of a Street Inventory and Management System (SIMS) is described. SIMS maintains a database of street information used in supporting decisions for street maintenance project formulation in a large metropolitan city. The software, implemented using SAS, the Statistical Analysis System software package, performs a variety of functions such as preparation of data collection forms, error-checking of collected data, processing and updating of data, and preparation of management reports. The involvement of SIMS users in its development resulted in an acceptance of its ability to provide information useful to them in their day-to-day decision making.}
}
@article{HRUBY199269,
title = {Application Layer for Bitbus Based Low Cost Technological Network},
journal = {IFAC Proceedings Volumes},
volume = {25},
number = {25},
pages = {69-74},
year = {1992},
note = {3rd IFAC Symposium on Low Cost Automation 1992: Techniques, Components and Instruments, Applications, Vienna, Austria9-11 September 1992},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)49581-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017495814},
author = {D. Hrubý and K. Fabian and J. Kalina},
keywords = {Fieldbus, Industrial networks, Distributed control, Microprocesor control, Computer communication},
abstract = {Effective use of automation resources requires that they are interconnected by a communication system. To interconnected field devices such as sensors, actuators and intelligent process controllers; Fieldbus is used. The main idea of Fieldbus is to replace point to point links from each sensor or actuator to it's controller by a serial communication system. In this article we present the field bus from the application layer point of view. Experimental testbed of system consists of some sensors, actuators and PC machines. As a communication network Intel's Bitbus was chosen. Distributed system is based on the architecture of standard host PC with Master Bitbus controller and many remote slaves Bitbus modules (max 250). Each slave module represents microcontroller i8344 with on board memory RAM, EPROM, serial communication interface RS-485 and 8-bit PC-BUS extension which allows controlling of many standard I/O industrial PC cards. PC operates as an extension of a master node. When compared to other commercial field buses, it is seen that Bitbus is at least near enough the I EC standards requirements. Practical experience performed on experimental system have shown that commercial Bitbus software library is cumbersome and is not well suited for hard real time applications. Therefore new software modules were implemented in order to fulfill distributed real-time constraints. The flexibility of the microcontroller i8344 allows to implement specialized Logical Link Control sublayer and a Application layer.}
}
@article{MO1998729,
title = {Petri net modelling and design of task oriented messaging system for robot control},
journal = {Computers & Industrial Engineering},
volume = {34},
number = {4},
pages = {729-742},
year = {1998},
issn = {0360-8352},
doi = {https://doi.org/10.1016/S0360-8352(98)00100-4},
url = {https://www.sciencedirect.com/science/article/pii/S0360835298001004},
author = {John P.T Mo and Bryan C.K Tang},
keywords = {Flexible robot control, Messaging system, Manufacturing Message Specification (MMS), Petri net modelling, Virtual manufacturing device, Object windows},
abstract = {Manufacturing Message Specification (MMS) is an international standard for shop floor machine control. It defines a set of conceptual schema and an interactive software object known as Virtual Manufacturing Device (VMD). Many systems have been established using this protocol but very few formal methods have been used to build such systems. This paper addresses the problems of the design and analysis of a network-based task oriented messaging system for flexible robot task control using a Petri net. The information and message transfer processes of the MMS systems were analysed. The modelling methodology allows a top down approach by which the net model is decomposed into fine details with clear identification of components which can be realised directly from the model. This approach is illustrated in this paper by a Windows-based robot control prototype system implemented from the Petri net model. The prototype was built using Object Windows Library and the NetBIOS session layer protocol on a PC network.}
}
@article{BUTLER2005415,
title = {Integrating simulation models with a view to optimal control of urban wastewater systems},
journal = {Environmental Modelling & Software},
volume = {20},
number = {4},
pages = {415-426},
year = {2005},
note = {Vulnerability of Water Quality in Intensively Developing Urban Watersheds},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000568},
author = {David Butler and Manfred Schütze},
keywords = {Control, Integrated modeling, Optimization, Real-time control, Urban wastewater system, Wastewater treatment plant},
abstract = {Rising expectations concerning levels of service, protection of the environment and enhanced sustainability are putting increasing demands on the urban wastewater system. The conventional approach to design and operation of the system, considering each component separately, cannot provide the gains in performance required. However, with recent gains in the understanding and modeling of the system it is now possible to represent the system as a whole and to optimize its performance. This paper describes the development and benefits of integrated modelling of the operation and control of urban wastewater systems and describes the development of a simulation package SYNOPSIS. SYNOPSIS encompasses sub-models of the sewer system, treatment plant and river, based on commonly accepted modeling approaches. The sub-models are linked in a parallel way allowing interactions between these sub-systems to be fully considered. This feature, used in concert with a control module implemented in the software, proves to be of particular importance for the development and analysis of integrated real-time control strategies. The model is applied to a case study site where it is clearly shown that river quality can be significantly improved by implementing an integrated control strategy as compared with the conventional passive or local control scenario. It is also shown how the application of conventional criteria (e.g. overflow volumes, discharged pollutant loads) can result in misleading conclusions when assessing the performance of the urban wastewater system under various scenarios. The paper argues that it is no longer necessary to use such simplified criteria, but using the software and techniques outlined, it is now feasible to operate complete urban wastewater systems to maximize river water quality directly. Furthermore, simulation results indicate the clear potential of integrated control even for catchments where local control provides hardly any benefits.}
}
@article{GARCIASANZ1997535,
title = {A reduced model of central heating systems as a realistic scenario for analyzing control strategies},
journal = {Applied Mathematical Modelling},
volume = {21},
number = {9},
pages = {535-545},
year = {1997},
issn = {0307-904X},
doi = {https://doi.org/10.1016/S0307-904X(97)00082-6},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X97000826},
author = {M. García-Sanz},
keywords = {heating systems modelling, HVAC simulation, test-bed for controllers},
abstract = {The thermal dynamic behavior of a building with a central heating system under several climatic conditions is a complex process to control, due to the large time delays, the time-variable parameters, and the plant uncertainties involved. In addition, there are many practical problems for the building occupants and financial losses when new controllers are installed in order to test them. For these reasons this paper presents a reduced computer model of central heating systems as a realistic test-bed for analyzing control strategies. The model is based on real data from an existing central heating system (see Ref. 4). As a result the simulation of this process lets us analyze the performance behavior and reliability of hierarchical control structures with advanced controllers in one of their natural scenarios of application with a realistic and flexible system. SIMULINK and MATLAB software packages have been used for this implementation.}
}
@article{PETERS1989391,
title = {Online searching using speech as a man/machine interface},
journal = {Information Processing & Management},
volume = {25},
number = {4},
pages = {391-406},
year = {1989},
issn = {0306-4573},
doi = {https://doi.org/10.1016/0306-4573(89)90067-8},
url = {https://www.sciencedirect.com/science/article/pii/0306457389900678},
author = {B.F. Peters and G. Philip and F.J. Smith and D. Crookes},
abstract = {The work described in this article is part of a two-year research program to investigate and implement a voice interface for the British Library Blaise Online Information Retrieval System. Preliminary work consisted of the evaluation of an existing voice-accessed document database system to gain insights into the problems of voice interface design for online searching. Following that, a study was made to determine the syntax rules of the Blaise query language. Using this information, the new interface has been designed and software implementation of its core achieved. The main lessons learned from the evaluation of the existing speech interface are: (a) take full advantage of the syntax of the query language to limit the difficulty of the speech recognition process; and (2) avoid antagonizing the user by providing full control of the configuration of the interface, enabling varying degrees of audio reinforcement of visually presented data. Other considerations suggested the use of PROLOG as the most suitable development language for such an interface. Results of the evaluation show that the use of currently available speech recognition and synthesis hardware, along with intelligent software, can provide an interface well suited to the needs of online information retrieval systems.}
}
@article{CHU2004209,
title = {Roam, a seamless application framework},
journal = {Journal of Systems and Software},
volume = {69},
number = {3},
pages = {209-226},
year = {2004},
note = {Ubiquitous Computing},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(03)00052-9},
url = {https://www.sciencedirect.com/science/article/pii/S0164121203000529},
author = {Hao-hua Chu and Henry Song and Candy Wong and Shoji Kurakake and Masaji Katagiri},
abstract = {One of the biggest challenges in future application development is device heterogeneity. In the future, we expect to see a rich variety of computing devices that can run applications. These devices have different capabilities in processors, memory, networking, screen sizes, input methods, and software libraries. We also expect that future users are likely to own many types of devices. Depending on users’ changing situations and environments, they may choose to switch from one type of device to another that brings the best combination of application functionality and device mobility (size, weight, etc.). Based on this scenario, we have designed and implemented a seamless application framework called the Roam system that can both assist developers to build multi-platform applications that can run on heterogeneous devices and allow a user to move/migrate a running application among heterogeneous devices in an effortless manner. The Roam system is based on partitioning of an application into components and it automatically selects the most appropriate adaptation strategy at the component level for a target platform. To evaluate our system, we have created several multi-platform Roam applications including a Chess game, a Connect4 game, and a shopping aid application. We also provide measurements on application performance and describe our experience with application development in the Roam system. Our experience shows that it is relatively easy to port existing applications to the Roam system and runtime application migration latency is within a few seconds and acceptable to most non-real-time applications.}
}
@article{KARPPINEN1984135,
title = {The Large-Scale Research Simulator NORS for Man-Machine Systems Development},
journal = {IFAC Proceedings Volumes},
volume = {17},
number = {3},
pages = {135-142},
year = {1984},
note = {IFAC Workshop on Modelling and Control of Electric Power Plants, Como, Italy, 22-23 September},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)64167-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017641673},
author = {J. Karppinen and E. Stokke},
keywords = {Real-time simulator, man-machine systems, computer-aided modeling, nuclear plants, minicomputers},
abstract = {The use of real-time simulotors for control system development is a well established application area. With the growing number of large-scale training simulators their use for other purposes than training such as human factors studies and control room improvements is becoming a common practice. For the development and evaluation of highly sophisticated operator support systems the use of large-scale plant simuI a tors as test facilities is becoming a practical necessity. These development trends are leading to the introduction of a new type of real-time simulators. the dedicated large-scale research simulators. The NORS (NOkia Research Simulator) is of this new type. It was implemented during 1983 Halden Reactor Project's Man-Machine Systems Laboratory (HAMMLAB). The HAMM-LAB research programme on advanced computer and colour display based control rooms covers the establishment of experimental techniques and methods for the new facility basic Man-Machine Systems research with development of concepts and theories experiments on information presentation and exercise of control and experimental validation of advanced computerized operator support systems. The NORS simulator was designed to effectively support the needs of the versatile research programme. The design goal was to combine the simulation scope and quality of large-scale training simulators with a flexible software and hardware structure readily adaptable to various experimental conditions. Novel features of the simulator software include extensive use of computer aided modelling and generic modelling packages for simulation of plant systems and instrumentation, highly modular structure and comprehensive use of a data base and software development system.}
}
@article{BOEGL20041,
title = {Knowledge acquisition in the fuzzy knowledge representation framework of a medical consultation system},
journal = {Artificial Intelligence in Medicine},
volume = {30},
number = {1},
pages = {1-26},
year = {2004},
issn = {0933-3657},
doi = {https://doi.org/10.1016/S0933-3657(02)00073-8},
url = {https://www.sciencedirect.com/science/article/pii/S0933365702000738},
author = {Karl Boegl and Klaus-Peter Adlassnig and Yoichi Hayashi and Thomas E. Rothenfluh and Harald Leitich},
keywords = {Fuzzy set theory, Knowledge acquisition, Knowledge representation, Knowledge-based systems, Medical consultation system},
abstract = {This paper describes the fuzzy knowledge representation framework of the medical computer consultation system MedFrame/CADIAG-IV as well as the specific knowledge acquisition techniques that have been developed to support the definition of knowledge concepts and inference rules. As in its predecessor system CADIAG-II, fuzzy medical knowledge bases are used to model the uncertainty and the vagueness of medical concepts and fuzzy logic reasoning mechanisms provide the basic inference processes. The elicitation and acquisition of medical knowledge from domain experts has often been described as the most difficult and time-consuming task in knowledge-based system development in medicine. It comes as no surprise that this is even more so when unfamiliar representations like fuzzy membership functions are to be acquired. From previous projects we have learned that a user-centered approach is mandatory in complex and ill-defined knowledge domains such as internal medicine. This paper describes the knowledge acquisition framework that has been developed in order to make easier and more accessible the three main tasks of: (a) defining medical concepts; (b) providing appropriate interpretations for patient data; and (c) constructing inferential knowledge in a fuzzy knowledge representation framework. Special emphasis is laid on the motivations for some system design and data modeling decisions. The theoretical framework has been implemented in a software package, the Knowledge Base Builder Toolkit. The conception and the design of this system reflect the need for a user-centered, intuitive, and easy-to-handle tool. First results gained from pilot studies have shown that our approach can be successfully implemented in the context of a complex fuzzy theoretical framework. As a result, this critical aspect of knowledge-based system development can be accomplished more easily.}
}
@article{DUNLAVY20071588,
title = {QCS: A system for querying, clustering and summarizing documents},
journal = {Information Processing & Management},
volume = {43},
number = {6},
pages = {1588-1605},
year = {2007},
note = {Text Summarization},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2007.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0306457307000246},
author = {Daniel M. Dunlavy and Dianne P. O’Leary and John M. Conroy and Judith D. Schlesinger},
keywords = {Information retrieval, Latent semantic indexing, Clustering, Summarization, Text processing, Sentence trimming},
abstract = {Information retrieval systems consist of many complicated components. Research and development of such systems is often hampered by the difficulty in evaluating how each particular component would behave across multiple systems. We present a novel integrated information retrieval system—the Query, Cluster, Summarize (QCS) system—which is portable, modular, and permits experimentation with different instantiations of each of the constituent text analysis components. Most importantly, the combination of the three types of methods in the QCS design improves retrievals by providing users more focused information organized by topic. We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences (DUC) as measured by the best known automatic metric for summarization system evaluation, ROUGE. Although the DUC data and evaluations were originally designed to test multidocument summarization, we developed a framework to extend it to the task of evaluation for each of the three components: query, clustering, and summarization. Under this framework, we then demonstrate that the QCS system (end-to-end) achieves performance as good as or better than the best summarization engines. Given a query, QCS retrieves relevant documents, separates the retrieved documents into topic clusters, and creates a single summary for each cluster. In the current implementation, Latent Semantic Indexing is used for retrieval, generalized spherical k-means is used for the document clustering, and a method coupling sentence “trimming” and a hidden Markov model, followed by a pivoted QR decomposition, is used to create a single extract summary for each cluster. The user interface is designed to provide access to detailed information in a compact and useful format. Our system demonstrates the feasibility of assembling an effective IR system from existing software libraries, the usefulness of the modularity of the design, and the value of this particular combination of modules.}
}
@article{MOON1990697,
title = {3D database searching and de novo construction methods in molecular design},
journal = {Tetrahedron Computer Methodology},
volume = {3},
number = {6, Part C},
pages = {697-711},
year = {1990},
note = {Three-dimensional chemical structure handling},
issn = {0898-5529},
doi = {https://doi.org/10.1016/0898-5529(90)90168-8},
url = {https://www.sciencedirect.com/science/article/pii/0898552990901688},
author = {Joseph B. Moon and W.Jeffrey Howe},
keywords = {3D database search, Molecular design,  molecular construction, Peptide design, Drug design},
abstract = {Computer-based lead finding algorithms which attempt to design, on a de novo basis, ligands that will complement a known receptor site cavity face some major problems in terms of a combinatorial design space and the synthesizability of the designed molecules. On the other hand, typical 3D database search methods provide a different set of challenges. Both of these approaches are ultimately pointed toward the same goal and can be used together productively. In this article we describe advances in both areas: we first describe extensions to our de novo ligand design software which combines (a) a tree-based conformational search over a library of fragments, and (b) a form of simulated annealing which allows designed ligands to crawl around the binding site even as their structures are changing. In the second part, we then discuss an implementation of the database approach which allows users to formulate 3D substructure, superstructure, or similarity queries based upon demonstrated or hypothetical requirements for activity. Finally, we draw the two approaches together with an example of current research interest, showing how one method can feed the other.}
}
@article{VOINOV1999473,
title = {Patuxent landscape model: integrated ecological economic modeling of a watershed},
journal = {Environmental Modelling & Software},
volume = {14},
number = {5},
pages = {473-491},
year = {1999},
note = {Binding Environmental Sciences and Artificial Intelligence},
issn = {1364-8152},
doi = {https://doi.org/10.1016/S1364-8152(98)00092-9},
url = {https://www.sciencedirect.com/science/article/pii/S1364815298000929},
author = {Alexey Voinov and Robert Costanza and Lisa Wainger and Roelof Boumans and Ferdinando Villa and Thomas Maxwell and Helena Voinov},
keywords = {Landscape modeling, scaling, dynamic spatial modeling, land use change},
abstract = {The Patuxent Landscape Model (PLM) is designed to simulate fundamental ecological processes on the watershed scale, in interaction with an economic component that predicts the land use patterns. The paper focuses on the ecological component of the PLM and describes how the spatial and structural rescaling can be instrumental for calibration of complex spatially distributed models. The PLM is based on a modified General Ecosystem Model (GEM) that is replicated across a grid of cells that compose the rasterized landscape. Different habitats and land use types translate into different parameter sets to be fed into GEM. Cells are linked by horizontal fluxes of material and information, driven mostly by the hydrologic flows. This approach provides additional flexibility in scaling up and down over a range of spatial resolutions and is essential to track the land use change patterns generated by the economic component. Structural modularity is another important feature that is implemented in the general purpose software packages (Spatial Modeling Environment and Collaborative Modeling Environment), that the PLM employs.}
}
@article{GEN1986220,
title = {Interactive multiple objective linear programming system implemented on a microcomputer},
journal = {Computers & Industrial Engineering},
volume = {11},
number = {1},
pages = {220-224},
year = {1986},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(86)90082-3},
url = {https://www.sciencedirect.com/science/article/pii/0360835286900823},
author = {Mitsuo Gen and Kenichi Ida},
abstract = {Linear programming is one of the most widely used Operations Research/Management Science techniques. Recently, multiple objective decision making has been well established as a practical approach to seek a satisfactory solution to a decision making problem. Much attention has been focused on a microcomputer as an economical management tool. In this paper we propose an interactive goal attainment method using the eigenvector algorithm for solving a multiple objective linear programming problem interactively on microcomputers. In the software package Micro-LPS based on the method proposed, we design a conversational and user-friendly system in which the user commands are involved.}
}
@incollection{SHAH1983141,
title = {MATRIXx: A DATA ANALYSIS, SYSTEM IDENTIFICATION, CONTROL DESIGN AND SIMULATION PROGRAM},
editor = {G.G. LEININGER},
booktitle = {Computer Aided Design of Multivariable Technological Systems},
publisher = {Pergamon},
pages = {141-146},
year = {1983},
isbn = {978-0-08-029357-8},
doi = {https://doi.org/10.1016/B978-0-08-029357-8.50024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008029357850024X},
author = {S. Shah and R. Walker and C. Gregory},
abstract = {MATRIXX is an interactive software system to perform a complete cycle of steps starting from data analysis to system identification, control design, simulation and evaluation. It is built on a user-friendly interpreter incorporating powerful matrix operations. The package offers four major features: 1) powerful interpreter, simple command structure, good graphics capability with most “bookkeeping” chores handled by the software, 2) state-of-the-art numerical algorithms, which allow solutions to high order problems, 3) user transparent file management with uniform and consistent format, and 4) efficient implementation with a stack to require minimum memory and computation resources.}
}
@article{DEMAGISTRIS2013170,
title = {Dynamic control of DHM for ergonomic assessments},
journal = {International Journal of Industrial Ergonomics},
volume = {43},
number = {2},
pages = {170-180},
year = {2013},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169814113000048},
author = {Giovanni {De Magistris} and Alain Micaelli and Paul Evrard and Claude Andriot and Jonathan Savin and Clarisse Gaudez and Jacques Marsot},
keywords = {Digital human model, Dynamic control, Ergonomic analysis, Virtual reality},
abstract = {Physical risk factors assessment is usually conducted by analysing postures and forces implemented by the operator during a work-task performance. A basic analysis can rely on questionnaires and video analysis, but more accurate comprehensive analysis generally requires complex expensive instrumentation, which may hamper movement task performance. In recent years, it has become possible to study the ergonomic aspects of a workstation from the initial design process, by using digital human model (DHM) software packages such as Pro/ENGINEER Manikin, JACK, RAMSIS or CATIA-DELMIA Human. However, a number of limitations concerning the use of DHM have been identified, for example biomechanical approximations, static calculation, description of the probable future situation or statistical data on human performance characteristics. Furthermore, the most common DHM used in the design process are controlled through inverse kinematic techniques, which may not be suitable for all situations to be simulated. A dynamic DHM automatically controlled in force and acceleration would therefore be an important contribution to analysing ergonomic aspects, especially when it comes to movement, applied forces and joint torques evaluation. Such a DHM would fill the gap between measurements made on the operator performing the task and simulations made using a static DHM. In this paper, we introduce the principles of a new autonomous dynamic DHM, then describe an application and validation case based on an industrial assembly task adapted and implemented in the laboratory. An ergonomic assessment of both the real task and the simulation was conducted based on analysing the operator/manikin's joint angles and applied force in accordance with machinery safety standards (Standard NF EN ISO 1005-1 to 5 and OCcupational Repetitive Actions (OCRA) index). Given minimum description parameters of the task and subject, our DHM provides a simulation whose ergonomic assessment agrees with experimental evaluation.
Relevance to Industry
A new autonomous dynamic DHM was developed to study the ergonomic aspects of a workstation. When designing a new work-task, our DHM requires minimal information for a simulation and changing the subject's anthropometry and the scenario does not require new trajectory specification nor additional tuning.}
}
@article{KRATZER1979149,
title = {Design and Implementation of Process Control Software Under Realistic Environment Conditions},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {3},
pages = {149-153},
year = {1979},
note = {2nd IFAC/IFIP Symposium on Software for Computer Control, Prague, Czechoslovakia, 11-15 June},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65790-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017657902},
author = {G. Kratzer},
keywords = {software engineering, process control software, simulation, real time, computer aided design, control algorithm design, state space methods},
abstract = {A software package is described which offers a systematic approach to development of industrial process control software. The procedure of con trol program implementation splits up in 3 basic steps or phases. Phase 1 covers control algorithm design and offline simulation. During phase 2 the real control programs are installed on process computer using results and experience of step one. Programs are thoroughly tested and evaluated in real time environment. The interplay of control software performance on the whole and its control behaviour can be investigated in closed loop simulation. Within phase 3 control software is tested on a dual computer system consisting of one computer simulating and a second computer controlling the plant. Both computers are coupled by realistic process interface equipment and are working asynchronously. Plant simulation and control software operate in parallel and in real-time. Phase 3 simulation supports final tuning and adapting of control algorithms and parameters. Concerning software engineering aspects an overall test and performance evaluation of the control system can be done. The system establishes a software and control engineering tool which makes control software development less costly and more reliable. Especially the transfer to real plant will become less risky after testing under realistic environment conditions.}
}
@article{STEVENSON198435,
title = {BIOBELL — A simulation system for biochemistry and biophysics},
journal = {Computers in Biology and Medicine},
volume = {14},
number = {1},
pages = {35-46},
year = {1984},
issn = {0010-4825},
doi = {https://doi.org/10.1016/0010-4825(84)90018-0},
url = {https://www.sciencedirect.com/science/article/pii/0010482584900180},
author = {D.E. Stevenson and D.D. Warner and T.R. Brown},
keywords = {Biochemical simulation, Chemical kinetics, Nuclear magnetic resonance, Production language, LISP, GASP, Stiff differential equations, Hybrid simulation},
abstract = {The BIOBELL simulation system is designed to simulate biochemical kinetics and nuclear magnetic resonance phenomena. The system uses portable software from the PORT Library and GASP system. The user language is illustrated. The implementation of the portable compiler-compiler and portable stiff ordinary differential equation solver are discussed. Guidelines for similar projects are given.}
}
@article{MILANI2010918,
title = {Approximate limit analysis of full scale FRP-reinforced masonry buildings through a 3D homogenized FE package},
journal = {Composite Structures},
volume = {92},
number = {4},
pages = {918-935},
year = {2010},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2009.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0263822309003778},
author = {Gabriele Milani and Enrico Milani and Antonio Tralli},
keywords = {Masonry, FRP strengthening, Limit analysis, 3D finite elements, Homogenization},
abstract = {A 3D homogenized FE limit analysis software for the numerical prediction of collapse loads and failure mechanisms of entire masonry buildings reinforced with FRP strips is presented. In particular, a two steps approach is adopted: in step I, masonry homogenized failure surfaces are obtained through an admissible kinematic FE approach in the representative element of volume (REV), constituted by a brick interconnected with its six neighbors with finite thickness mortar joints. 8-Noded rigid infinitely resistant parallelepiped elements interconnected with interfaces with frictional behavior and limited tensile and compressive strength are utilized to model the REV. A simple linear programming problem in few variables is obtained, suitable to recover numerically masonry failure surfaces when loaded in- and out-of-plane. In step II, homogenized failure surfaces are implemented in the novel FE kinematic limit analysis software for an inexpensive evaluation of collapse loads of entire buildings. Delamination is considered in the model imposing to FRP–masonry interfaces a limited resistance in agreement with Italian code CNR-DT-200. 6-Noded rigid infinitely resistant 3D wedge-shaped elements are used to model homogenized masonry, whereas FRP strips are modeled by means of triangular 3-noded rigid elements. A two story masonry building reinforced in various ways with FRP strips and experimentally tested at Georgia Tech under quasi-static horizontal loads is analyzed to assess numerical results. Good agreement is found both in presence and absence of reinforcement, meaning that the procedure proposed may be used by practitioners for a reliable evaluation of collapse loads and failure mechanisms of complex 3D strengthened masonry structures.}
}
@article{EICHENLAUB2021105911,
title = {Bayesian parameter estimation in the oral minimal model of glucose dynamics from non-fasting conditions using a new function of glucose appearance},
journal = {Computer Methods and Programs in Biomedicine},
volume = {200},
pages = {105911},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105911},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720317442},
author = {Manuel M. Eichenlaub and John G. Hattersley and Mary C. Gannon and Frank Q. Nuttall and Natasha A. Khovanova},
keywords = {Oral minimal model, Variational Bayesian analysis, Glucose appearance, Insulin sensitivity},
abstract = {Background and objective
The oral minimal model (OMM) of glucose dynamics is a prominent method for assessing postprandial glucose metabolism. The model yields estimates of insulin sensitivity and the meal-related appearance of glucose from insulin and glucose data after an oral glucose challenge. Despite its success, the OMM approach has several weaknesses that this paper addresses.
Methods
A novel procedure introducing three methodological adaptations to the OMM approach is proposed. These are: (1) the use of a fully Bayesian and efficient method for parameter estimation, (2) the model identification from non-fasting conditions using a generalised model formulation and (3) the introduction of a novel function to represent the meal-related glucose appearance based on two superimposed components utilising a modified structure of the log-normal distribution. The proposed modelling procedure is applied to glucose and insulin data from subjects with normal glucose tolerance consuming three consecutive meals in intervals of four hours.
Results
It is shown that the glucose effectiveness parameter of the OMM is, contrary to previous results, structurally globally identifiable. In comparison to results from existing studies that use the conventional identification procedure, the proposed approach yields an equivalent level of model fit and a similar precision of insulin sensitivity estimates. Furthermore, the new procedure shows no deterioration of model fit when data from non-fasting conditions are used. In comparison to the conventional, piecewise linear function of glucose appearance, the novel log-normally based function provides an improved model fit in the first 30 min of the response and thus a more realistic estimation of glucose appearance during this period. The identification procedure is implemented in freely accesible MATLAB and Python software packages.
Conclusions
We propose an improved and freely available method for the identification of the OMM which could become the future standardard for the oral minimal modelling method of glucose dynamics.}
}
@article{GRAY1998213,
title = {Design and Implementation of MP, a Protocol for Efficient Exchange of Mathematical Expressions},
journal = {Journal of Symbolic Computation},
volume = {25},
number = {2},
pages = {213-237},
year = {1998},
issn = {0747-7171},
doi = {https://doi.org/10.1006/jsco.1997.0173},
url = {https://www.sciencedirect.com/science/article/pii/S0747717197901735},
author = {S. Gray and N. Kajler and P.S. Wang},
abstract = {The Multi Project is an ongoing research effort at Kent State University aimed at providing an environment for distributed scientific computing. An integral part of this environment is the Multi Protocol (MP) which is designed to support efficient communication of mathematical data between scientifically-oriented software tools. MP exchanges data in the form of linearized annotated syntax trees. Syntax trees provide a simple, flexible and tool-independent way to represent and exchange data, and annotations provide a powerful and generic expressive facility for transmitting additional information. At a level above the data exchange protocol, dictionaries provide definitions for operators and constants, providing shared semantics across heterogeneous packages. A clear distinction between MP-defined and user-defined entities is enforced. Binary encodings are used for efficiency. Commonly used values and blocks of homogeneous data are further optimized. The protocol is independent of the underlying communication paradigm and can support parallel computation, distributed problem-solving environments, and the coupling of tools for specific applications.}
}
@article{NACSA199488,
title = {Steps Towards Real-Time Control using Knowledge Based Simulation of Flexible Manufacturing Systems},
journal = {IFAC Proceedings Volumes},
volume = {27},
number = {10},
pages = {88-91},
year = {1994},
note = {2nd IFAC Workshop on Computer Software Structures Integrating AI/KBS Systems in Process Control, Lund, Sweden, August 10-12, 1994},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)45852-6},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017458526},
author = {J. Nacsa and G.L. Kovács},
keywords = {Expert systems, Flexible manufacturing, Simulation},
abstract = {Computer aided simulation can assist both in the design and operation of flexible manufacturing systems (FMS). A proper simulation model of a given FMS could be the best tool to validate it and to evaluate its performance. The control of the system can be solved by separating the control mechanism from the real devices of the FMS with a communication interface. In this case it is possible to use the simulated system for investigations instead of the real system. Such simulation systems can be built up using expert system shells. In this paper an FMS simulation system (SSQA) will be introduced with the hybrid application of expert systems and a traditional simulation software. The system has Some evaluation. scheduling and quality control power as well. according to the implemented advisory systems that communicate with the simulation package.}
}
@incollection{LIN1989403,
title = {COMPUTER AIDED DESIGN AND CONTROL FOR A RAW MILL IN CEMENT MANUFACTORY},
editor = {CHEN ZHEN-YU},
booktitle = {Computer Aided Design in Control Systems 1988},
publisher = {Pergamon},
address = {Oxford},
pages = {403-407},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035738-6},
doi = {https://doi.org/10.1016/B978-0-08-035738-6.50069-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357386500690},
author = {P. Lin and P. Prevot and J.P. Barbier},
abstract = {A multilevel software package which can be used to aid the design of controllers and the examination of controller's efficiency is presented in this paper. With a man-machine interface, one can easily implement the different modes of control strategy, such as optimal control with multicriterion and self-adaptive control. We apply this software tool to a raw mill process in a cement manufactory. After modeling, identifying and simulating the process behaviour, a self-adaptive control system with different algorithms of optimal control is developped for chemical composition of raw materials by using correction products. In addition, a heuristical adaptive supervisor is designed for adjusting the parameters used in the control policy according to process performance.}
}
@article{PEERSMAN201650,
title = {iCOP: Live forensics to reveal previously unknown criminal media on P2P networks},
journal = {Digital Investigation},
volume = {18},
pages = {50-64},
year = {2016},
issn = {1742-2876},
doi = {https://doi.org/10.1016/j.diin.2016.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1742287616300779},
author = {Claudia Peersman and Christian Schulze and Awais Rashid and Margaret Brennan and Carl Fischer},
keywords = {Computer crime, Peer-to-peer computing, Image classification, Text analysis, Forensic triage},
abstract = {The increasing levels of criminal media being shared in peer-to-peer (P2P) networks pose a significant challenge to law enforcement agencies. One of the main priorities for P2P investigators is to identify cases where a user is actively engaged in the production of child sexual abuse (CSA) media – they can be indicators of recent or on-going child abuse. Although a number of P2P monitoring tools exist to detect paedophile activity in such networks, they typically rely on hash value databases of known CSA media. As a result, these tools are not able to adequately triage the thousands of results they retrieve, nor can they identify new child abuse media that are being released on to a network. In this paper, we present a new intelligent forensics approach that incorporates the advantages of artificial intelligence and machine learning theory to automatically flag new/previously unseen CSA media to investigators. Additionally, the research was extensively discussed with law enforcement cybercrime specialists from different European countries and Interpol. The approach has been implemented into the iCOP toolkit, a software package that is designed to perform live forensic analysis on a P2P network environment. In addition, the system offers secondary features, such as showing on-line sharers of known CSA files and the ability to see other files shared by the same GUID or other IP addresses used by the same P2P client. Finally, our evaluation on real CSA case data shows high degrees of accuracy, while hands-on trials with law enforcement officers demonstrate the toolkit's complementarity to extant investigative workflows.}
}
@article{OMOMULE2020105301,
title = {Fuzzy prediction and pattern analysis of poultry egg production},
journal = {Computers and Electronics in Agriculture},
volume = {171},
pages = {105301},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105301},
url = {https://www.sciencedirect.com/science/article/pii/S0168169919322811},
author = {Taiwo Gabriel Omomule and Olusola Olajide Ajayi and Adebola Okunola Orogun},
keywords = {Egg production, Fuzzy prediction, Fuzzy inference systems, Pattern analysis, Decision analysis},
abstract = {Egg production is geared towards large scale production of eggs for profit maximization and consumption. But poultry farmers are faced with challenges that affect the quantity and quality of egg production during the production life cycle such as quality of feed intake, management issues, genetic factors of the breed birds, age at egg laying period, presence of diseases, molting flock, housing and others. Several mechanical solutions have been suggested to implement the production cycle but with very high cost implication which calls for the need of soft computing methods to model the egg production process. However, modelling egg production curve is a complex task because a typical egg production curve (from week 20 to week 72) is characterized by nonlinear probabilities and imprecise knowledge in the egg production process. This paper proposes an optimal fuzzy predictive model for poultry egg production. The motivation stems from analyzing the non-linearity and imprecision in egg production which is best handled by fuzzy logic as compared to other soft computing techniques. Data samples were collected on the age of chicks, quantity of feeds, quality of feeds, body weight of chicks and total egg production as input into the proposed system. The first stage of implementation is carried out in the Statistical Package for Social Science (SPSS) software environment to perform correlation analysis while the last two stages are carried out using Fuzzy Toolbox in MATLAB based on its capability to model and simulate complex systems with robust and interpretable predictions. Gaussian membership function was used in the fuzzification process while Mamdami structure was adopted as the fuzzy inference system (FIS). Model performance evaluation showed that the proposed system performed excellently by achieving best prediction accuracy 100% Pred(30) with an approximate mean magnitude relative error (MMRE) of 0.11744 as compared to the results in the reported literature. The method has proven to be a cost-effective and simple approach to modelling through the use of samples and parameters from poultry house.}
}
@article{BEEBE1989385,
title = {〈PLOT79〉: A comprehensive portable Fortran scientific line graphics system, as applied to biomedical research},
journal = {Computers in Biology and Medicine},
volume = {19},
number = {6},
pages = {385-402},
year = {1989},
issn = {0010-4825},
doi = {https://doi.org/10.1016/0010-4825(89)90074-7},
url = {https://www.sciencedirect.com/science/article/pii/0010482589900747},
author = {Nelson H.F. Beebe and R.P.C. Rodgers},
keywords = {Biomedical graphics, Computer graphics, Computer modeling, CORE proposal, Fortran, GKS, PHIGS, Program portability, SIGGRAPH, Software engineering, Structured computer programming, UNIX},
abstract = {Scientific results are often most succinctly presented in graphical form. We describe a system for computer-generated scientific line graphics known as 〈PLOT79〉, named to commemorate the SIGGRAPH CORE graphics standard proposal of 1979. 〈PLOT79〉 is a widely used and actively evolving graphics system, written primarily in SFTRAN3, a structured procedural computer language wich can be translated readily into Fortran. The package embodies concepts of sound software engineering, having been designed from the outset to be portable, maintainable and hardware-independent; much of the effort required to implement the system was directed toward the development of software engineering tools to ensure these goals. A modular design strategy has allowed a wide variety of graphics output devices to be supported. 〈PLOT79〉 has been installed under numerous operating systems, and software tools provided by UNIX have allowed particularly efficient installation and use of the system. Access to 〈PLOT79〉 is available through three avenues: (1) linking 〈PLOT79〉 routines with a user-written high-level program; (2) use of pre-written high-level applications programs which perform certain frequently-required tasks such as the plotting of simple two or three-dimensional data; or (3) the use of an interactive graphics command parser known as slides. 〈PLOT79〉 has proven popular among workers in the physical sciences and engineering both for its easy availability, openness (all source code is provided), and powerful capability. The system presents an equally important (though lesser known) resource for biomedical research, as demonstrated by examples from ongoing biomedical research projects. It also provides a focus for discussion of the practical limitations inherent in existing graphics standards and programming languages.}
}
@incollection{SCHMID1979251,
title = {KEDDC, A GENERAL PURPOSE CAD SOFTWARE SYSTEM FOR APPLICATION IN CONTROL ENGINEERING},
editor = {M. NOVAK},
booktitle = {Software for Computer Control},
publisher = {Pergamon},
pages = {251-255},
year = {1979},
isbn = {978-0-08-024448-8},
doi = {https://doi.org/10.1016/B978-0-08-024448-8.50049-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244488500496},
author = {Chr. Schmid and H. Unbehauen},
abstract = {KEDDC is a software package especially designed for process computers to cover a wide range of control engineering tasks. The system involves dynamical system identification, controller design and testing by digital simulation using a broad variety of modern as well as classical methods. Finally the KEDDC system includes implementation and on-line realization of control algorithms, so that any control scheme designed with this software package can be applied and tested on real processes.}
}
@incollection{CHOQUET1996561,
title = { - Towards modular CFD using the cerfacs parallel utilities},
editor = {A. Ecer and J. Periaux and N. Satdfuka and S. Taylor},
booktitle = {Parallel Computational Fluid Dynamics 1995},
publisher = {North-Holland},
address = {Amsterdam},
pages = {561-568},
year = {1996},
isbn = {978-0-444-82322-9},
doi = {https://doi.org/10.1016/B978-044482322-9/50123-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444823229501237},
author = {Rémi Choquet and Fabio Guerinoni and Michael Rudgyard},
abstract = {Publisher Summary
This chapter describes a modular, parallel computational fluid dynamics (CFD) code. It demonstrates the basic tenet of the original code, which is its capability to adapt to different message passing parallel systems. The code is tested on dedicated workstations and scalable multiprocessors. It can be described as a parallel unstructured/structured, two and three dimensional Navier-Stokes solver. It's supporting parallel routines CERFACS parallel utilities (CPULib) consist of message-passing implementations of generic utilities that are required by many numerical schemes in CFD. The library is built upon the integrated parallel macros (IPM), which allow using message passing software. The chapter gives a detailed description of the multiprocessor on which part of the code is tested. It discusses the numerical schemes and the presentation of test problems, and provides the performances and comparisons for two-dimensional and large three-dimensional problems. It also sketches the structure of the implicit library and its interaction with CPULib.}
}
@article{MARTINEZ198853,
title = {Waste-Water Treatment Test Plant Control: Computer Aided Modern Control Teaching},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {6},
pages = {53-59},
year = {1988},
note = {IFAC Symposium on Trends in Control and Measurement Education, Swansea, UK, 11-13 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53838-0},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017538380},
author = {M. Martinez and J. Salt and P. Albertos and F. Morant},
keywords = {Education, Educational aids, Teaching, Temperature control, pH control, Level control, Digital control, Computer aided design, Adaptive control, Analog computer control},
abstract = {An experimental work on the control design for waste-water treatment test-plant control computer aided modern control teaching has been conceived. The work presents three sucesive stages: First, the student must identify the process with the instrumentation available at the control laboratory. He shall use a personal computer with A/D and D/A converters connected to the process as well as a software package suitable for identification. In a second step, the student must analyze the process as well as design a digital regulator to get the desired behaviour of the global process according to the requeriments previously established by the teacher. In the last step, each student or group of them must verify their design in the test plant and must explain the conclusions about the practical results they have obtained. Our work has been prepared on the test plant in such a way that different control schemes can be implemented.}
}
@article{WINGARD1992205,
title = {Enabling Use of Engineering Terminology in Product Models and User Interfaces of CAD/CAM Systems},
journal = {CIRP Annals},
volume = {41},
number = {1},
pages = {205-208},
year = {1992},
issn = {0007-8506},
doi = {https://doi.org/10.1016/S0007-8506(07)61186-X},
url = {https://www.sciencedirect.com/science/article/pii/S000785060761186X},
author = {Lars Wingård and Per Carleberg and Torsten Kjellberg and Gunnar Sohlenius},
keywords = {features, product modelling, interactive design systems},
abstract = {Summary
This paper presents an approach to introducing engineering terminology m product models and user interfaces of CADCAM system. It describes how we have utilized form features and engineering elements in order to represent engineering knowledge more naturally in product models. This gives us a method for realizing user interfaces in which an Application specific and user adapted engineering terminology is used. A CADCAM Framework for integration of general software packages into a distributed CADCAM system is also suggested. Finally, some aspects of the suggested approach. collected from a limited test implementation and from discussions with an industrial partner are discussed.}
}
@incollection{SIIROLA2009139,
title = {Current Trends in Parallel Computation and the Implications for Modeling and Optimization},
editor = {Rita Maria {de Brito Alves} and Caludio Augusto Oller {do Nascimento} and Evaristo Chalbaud Biscaia},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {27},
pages = {139-141},
year = {2009},
booktitle = {10th International Symposium on Process Systems Engineering: Part A},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(09)70244-5},
url = {https://www.sciencedirect.com/science/article/pii/S1570794609702445},
author = {John D. Siirola},
keywords = {parallel computing, optimization, discrete event simulation},
abstract = {Extended Abstract
Process Systems Engineering (PSE) is built on the application of computational tools to the solution of physical engineering problems. Over the course of its nearly five decade history, advances in PSE have relied roughly equally on advancements in desktop computing technology and developments of new tools and approaches for representing and solving problems (Westerberg, 2004). Just as desktop computing development over that period focused on increasing the net serial instruction rate, tool development in PSE has emphasized creating faster general-purpose serial algorithms. However, in recent years the increase in net serial instruction rate has slowed dramatically, with processors first reaching an effective upper limit for clock speed and now approaching apparent limits for microarchitecture efficiency. Current trends in desktop processor development suggest that future performance gains will occur primarily through exploitation of parallelism. For PSE to continue to leverage the “free” advancements from desktop computing technology in the future, the PSE toolset will need to embrace the use of parallelization. Unfortunately, “parallelization” is more than just identifying multiple things to do at once. Parallel algorithm design has two fundamental challenges: first, to match the characteristics of the parallelizable problem workload to the capabilities of the hardware platform, and second to properly balance parallel computation with the overhead of communication and synchronization on that platform. The performance of any parallel algorithm is thus a strong function of how well the characteristics of the problem and algorithm match those of the hardware platform on which it will run. This has led to a proliferation of highly specialized parallel hardware platforms, each designed around specific problems or problem classes. While every platform has its own unique characteristics, we can group current approaches into six basic classes: symmetric multiprocessing (SMP), networks of workstations (NOW), massively parallel processing (MPP), specialized coprocessors, multi-threaded shared memory, and hybrids that combine components of the first five classes. Perhaps the most familiar of these is the SMP architecture, which forms the bulk of current the desktop and workstation market. These systems have multiple processing units (processors and/or cores) controlled by a single operating system image and sharing a single common shared memory space. While SMP systems provide only a modest level of parallelism (typically 2-16 processing units), the existence of shared memory and full-featured processing units makes them perhaps the most straightforward development platform. A challenge of SMP platforms is the discrepancy between the speed of the processor and the memory system: both latency and overall memory bandwidth limitations can lead to processors idling waiting for data. Clusters, a generic term for coordinated groups of independent computers (nodes) connected with high-speed networks, provide the opportunity for a radically different level of parallelism, with the largest clusters having over 25,000 nodes and 100,000 processing units. The challenge with clusters is memory is distributed across independent nodes. Communication and coordination among nodes must be explicitly managed and occurs over a relatively high latency network interconnect. Efficient use of this architecture requires applications that decompose into pseudo-independent components that run with high computation to communication ratios. The level to which systems utilize commodity components distinguishes the two main types of cluster architectures, with NOW nodes running commodity network interconnects and operating systems and MPP nodes using specialized or proprietary network layers or microkernels. Specialized coprocessors, including graphics processing units (GPU) and the Cell Broadband Engine (Cell), are gaining popularity as scientific computing platforms. These platforms employ non-general purpose dependent processing units to speed fine-grained, repetitive processing. Architecturally, they are reminiscent of vector computing, combining very fast access to a small amount of local memory with processing elements implementing either a single-instruction-multiple-data (SIMD) (GPU) or a pipelined (Cell) model. As application developers must explicitly manage both parallelism on the coprocessor and the movement of data to and from the coprocessor memory space, these architectures can be some of the most challenging to program. Finally, multi-threaded shared memory (MTSM) systems represent a fundamental departure from traditional distributed memory systems like NOW and MPP. Instead of a collection of independent nodes and memory spaces, an MTSM system runs a single system image across all nodes, combining all node memory into a single coherent shared memory space. To a developer, the MTSM appears to be a single very large SMP. However, unlike a SMP that uses caches to reduce the latency of a memory access, the MTSM tolerates latency by using a large number of concurrent threads. While this architecture lends itself to problems that are not readily decomposable, effective utilization of MTSM systems requires applications to run hundreds – or thousands – of concurrent threads. The proliferation of specialized parallel computing architectures presents several significant challenges for developers of parallel modeling and optimization applications. Foremost is the challenge of selecting the “appropriate” platform to target when developing the application. While it is clear that architectural characteristics can significantly affect the performance of an algorithm, relatively few rules or heuristics exist for selecting a platform based solely on application characteristics. A contributing challenge is that different architectures employ fundamentally different programming paradigms, libraries, and tools. Knowledge and experience on one platform does not necessarily translate to other platforms. This also complicates the process of directly comparing platform performance, as applications are rarely portable: software designed for one platform rarely compiles on another without modification, and the modifications may require a redesign of the fundamental parallelization approach. A final challenge is effectively communicating parallel results. While the relatively homogenous environment of serial desktop computing facilitated extremely terse descriptions of a test platform, often limited to processor make and clock speed, reporting results for parallel architectures must include not only processor information, but depending on the architecture, also include operating system, network interconnect, coprocessor make, model, and interconnect, and node configuration. There are numerous examples of algorithms and applications designed explicitly to leverage specific architectural features of parallel systems. While by no means comprehensive, three current representative efforts are the development of parallel branch and bound algorithms, distributed collaborative optimization algorithms, and multithreaded parallel discrete event simulation. PICO, the Parallel Integer and Combinatorial Optimizer (Eckstein, et al., 2001), is a scalable parallel mixed-integer linear optimizer. Designed explicitly for cluster environments (both NOW and MPP), PICO leverages the synergy between the inherently decomposable branch and bound tree search and the independent nature of the nodes within a cluster by distributing the independent sub-problems for the tree search across the nodes of the cluster. In contrast, agent-based collaborative optimization (Siirola, et al., 2004, 2007) matches traditionally non-decomposable nonlinear programming algorithms to high-latency clusters (e.g. NOWs or Grids) by replicating serial search algorithms intact and unmodified across the independent nodes of the cluster. The system then enforces collaboration through sharing intermediate “solutions” to the common problem. This creates a decomposable artificial meta-algorithm with a high computation to communication ratio that can scale efficiently on large, high latency, low bandwidth cluster environments. For modeling applications, efficiently parallelizing discrete event simulations has presented a longstanding challenge, with several decades of study and literature (Perumalla, 2006). The central challenge in parallelizing discrete event simulations on traditional distributed memory clusters is efficiently synchronizing the simulation time across the processing nodes during a simulation. A promising alternative approach leverages the Cray XMT (formerly called Eldorado; Feo, et al. 2005). The XMT implements an MTSM architecture and provides a single shared memory space across all nodes, greatly simplifying the time synchronization challenge. Further, the fine-grained parallelism provided by the architecture opens new opportunities for additional parallelism beyond simple event parallelization, for example, parallelizing the event queue management. While these three examples are a small subset of current parallel algorithm design, they demonstrate the impact that parallel architectures have had and will continue to have on future developments for modeling and optimization in PSE.}
}
@article{VERARDI1988115,
title = {An Integrated Approach to Control Analysis and Design},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {2},
pages = {115-118},
year = {1988},
note = {5th IFAC/IFIP Symposium on Software for Computer Control 1988 (SOCOCO '88), Johannesburg, S Africa, 26-28 April},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53963-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017539634},
author = {F. Verardi},
keywords = {Process Control, Control System Analysis},
abstract = {The last few years have seen an increasing trend in the use of computer-based technology for automation and process-control problems. Modern computer-based control systems must be user-friendly, flexible and easily configurable. The use of mass-storage devices with this technology permits the storage of copious quantities of historical plant data. This facility presents the control engineer with an ideal mechanism for performing process-analysis, simulation and control system design. This paper outlines an integrated system which permits an inexperienced user to engineer a computer-based process-control system within a few man days, while at the same time providing the control engineer with access to a set of sophisticated analysis and design tools. The computer system comprises a real-time database, software for communication with front-end input/output equipment, a configurable man-machine interface and a comprehensive logging subsystem for the long-term storage of plant data. A remote access facility allows networking to foreign intelligent devices such as IBM PC's and provides secure access to the real-time database and historical logfiles. Formatted transfer of logfile data to the foreign computers allows the use of proprietary software packages for process study, modelling, system identification and multivariable control system design. This networking facility reduces the necessity for higher level control functions such as optimisation to be implemented in the central process computer.}
}
@article{RAKOTOZAFY2011687,
title = {Real-time digital simulation of power electronics systems with Neutral Point Piloted multilevel inverter using FPGA},
journal = {Electric Power Systems Research},
volume = {81},
number = {2},
pages = {687-698},
year = {2011},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2010.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0378779610002695},
author = {Mamianja Rakotozafy and Philippe Poure and Shahrokh Saadate and Cédric Bordas and Loïc Leclere},
keywords = {Field Programmable Gate array, Power electronics, Real-time simulation, Neutral Point Piloted three-level converter, Power system simulation},
abstract = {Most of actual real time simulation platforms have practically about ten microseconds as minimum calculation time step, mainly due to computation limits such as processing speed, architecture adequacy and modeling complexities. Therefore, simulation of fast switching converters’ instantaneous models requires smaller computing time step. The approach presented in this paper proposes an answer to such limited modeling accuracies and computational bandwidth of the currently available digital simulators.As an example, the authors present a low cost, flexible and high performance FPGA-based real-time digital simulator for a complete complex power system with Neutral Point Piloted (NPP) three-level inverter. The proposed real-time simulator can model accurately and efficiently the complete power system, reducing costs, physical space and avoiding any damage to the actual equipment in the case of any dysfunction of the digital controller prototype. The converter model is computed at a small fixed time step as low as 100ns. Such a computation time step allows high precision account of the gating signals and thus avoids averaging methods and event compensations. Moreover, a novel high performance model of the NPP three-level inverter has also been proposed for FPGA implementation. The proposed FPGA-based simulator models the environment of the NPP converter: the dc link, the RLE load and the digital controller and gating signals. FPGA-based real time simulation results are presented and compared with offline results obtained using PLECS software. They validate the efficiency and accuracy of the modeling for the proposed high performance FPGA-based real-time simulation approach. This paper also introduces new potential FPGA-based applications such as low cost real time simulator for power systems by developing a library of flexible and portable models for power converters, electrical machines and drives.}
}
@article{CARRAVILLA199543,
title = {Hierarchical production planning in a make-to-order company: A case study},
journal = {European Journal of Operational Research},
volume = {86},
number = {1},
pages = {43-56},
year = {1995},
note = {EURO Summer Institute Hierarchical Planning},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(95)00060-4},
url = {https://www.sciencedirect.com/science/article/pii/0377221795000604},
author = {Maria Antónia Carravilla and Jorge Pinho {de Sousa}},
keywords = {Hierarchical production planning, Make-To-Order manufacturing, Decision Support Systems, Heuristics},
abstract = {In this paper, we analyse a complex production planning problem in a Make-To-Order company, involving quoting due dates, along with production scheduling, some plant layout decisions and line balancing issues. A general framework has been developed that identifies the main levels for decision making, and tackles these problems in a hierarchical fashion, the resulting interactions being carefully taken into consideration. This framework, based on considering discretised planning horizons, is implemented on a Decision Support System developed around an interface specially designed to involve the various participants in the planning process. In practice, the system is composed by a package of software products used as add-ons to the main management information system of the company. A comprehensive and detailed description of the approach, techniques used and developed algorithms is presented. The experience in using a first prototype of the system shows this may become a highly valuable computer tool in production planning, layout design and assignment of orders.}
}
@article{ANDREADIS1997397,
title = {Developing a multistack ISO-SR/Z39.50 application gateway},
journal = {Computer Standards & Interfaces},
volume = {18},
number = {5},
pages = {397-415},
year = {1997},
issn = {0920-5489},
doi = {https://doi.org/10.1016/S0920-5489(96)01014-8},
url = {https://www.sciencedirect.com/science/article/pii/S0920548996010148},
author = {Dimitrios Andreadis and Ahmed Patel},
keywords = {ISO-SR (Search and Retrieve), Z39.50, Gateway, Multistack, ISODE, Libraries, MARC},
abstract = {The unprecedented growth of computer networks, in particular the Internet, has led to the creation of new methods for libraries to provide services to their users. One of the most important services which can be enhanced and delivered in new ways is access to the library catalogue. The development of standardised communication interfaces to the actual database and search engine, i.e. search and retrieve protocols, has made it possible for users with appropriate client software to access any server supporting the standard interface. Two standards have been developed for this purpose: the ISO Search and Retrieve (SR) and the ANSI Z39.50. Although the two standards are very similar they have been developed and implemented in different environments, thus causing difficulties for users trying to achieve universal access to all types of servers. Incompatibilities can arise at two levels: the underlying network technology and the search and retrieve protocols themselves. Z39.50 is usually implemented directly over TCP/IP while ISO-SR is layered over a full OSI stack. In addition, the existence of a variety of different query types and bibliographic record formats limits potential functionality. A gateway offering communication between different stacks and conversion of different data formats would be a valuable sen/ice as the number of networked databases increases. This paper consolidates the experience gained from the design and the implementation of an application gateway which allows ISO-SR and Z39.50 systems to interoperate transparently on top of TCP/IP and OSI communication stacks. It also provides a basic background to the Search and Retrieve service offered by the two protocols in question.}
}
@article{TAUSWORTHE1979181,
title = {The work breakdown structure in software project management},
journal = {Journal of Systems and Software},
volume = {1},
pages = {181-186},
year = {1979},
issn = {0164-1212},
doi = {https://doi.org/10.1016/0164-1212(79)90018-9},
url = {https://www.sciencedirect.com/science/article/pii/0164121279900189},
author = {Robert C. Tausworthe},
abstract = {The work breakdown structure (WBS) is a vehicle for breaking an engineering project down into subproject, tasks, subtasks, work packages, and so on. It is an important planning tool which links objectives with resources and activities in a logical framework. It becomes an important status monitor during the actual implementation as the completions of subtasks are measured against the project plan. Whereas the WBS has been widely used in many other engineering applications, it has seemingly only rarely been formally applied to software projects, for various reasons. Recent successes with software project WBSs, however, have clearly indicated that the technique can be applied and have shown the benefits of such a tool in management of these projects. This paper advocates and summarizes the use of the WBS in software implementation projects. It also identifies some of the problems people have had generating software WBSs, and the need for standard checklists of items to be included.}
}
@article{WEIN200766,
title = {The visibility–Voronoi complex and its applications},
journal = {Computational Geometry},
volume = {36},
number = {1},
pages = {66-87},
year = {2007},
note = {Special Issue on the 21st European Workshop on Computational Geometry},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2005.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925772106000496},
author = {Ron Wein and Jur P. {van den Berg} and Dan Halperin},
abstract = {We introduce a new type of diagram called the VV(c)-diagram (the visibility–Voronoi diagram for clearance c), which is a hybrid between the visibility graph and the Voronoi diagram of polygons in the plane. It evolves from the visibility graph to the Voronoi diagram as the parameter c grows from 0 to ∞. This diagram can be used for planning natural-looking paths for a robot translating amidst polygonal obstacles in the plane. A natural-looking path is short, smooth, and keeps—where possible—an amount of clearance c from the obstacles. The VV(c)-diagram contains such paths. We also propose an algorithm that is capable of preprocessing a scene of configuration-space polygonal obstacles and constructs a data structure called the VV-complex. The VV-complex can be used to efficiently plan motion paths for any start and goal configuration and any clearance value c, without having to explicitly construct the VV(c)-diagram for that c-value. The preprocessing time is O(n2logn), where n is the total number of obstacle vertices, and the data structure can be queried directly for any c-value by merely performing a Dijkstra search. We have implemented a Cgal-based software package for computing the VV(c)-diagram in an exact manner for a given clearance value and used it to plan natural-looking paths in various applications.}
}
@article{DYMSCHIZ1979245,
title = {A Process Computer Program Package for Interactive Computer Aided Design of Multivariable Control Systems},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {3},
pages = {245-250},
year = {1979},
note = {2nd IFAC/IFIP Symposium on Software for Computer Control, Prague, Czechoslovakia, 11-15 June},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65809-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017658099},
author = {E. Dymschiz},
keywords = {Computer aided design, discrete-time systems, multivariable systems, direct digital control, optimal control, state-space methods},
abstract = {A Software package is presented allowing the computer aided design of multivariable control algorithms. Based on discrete-time parametric process models, design methods for two basic feedback control algorithms are described. By means of an interactive dialog between the operator and the computer, control algorithms are designed and implemented on-line to carry out the direct digital control of an analog simulated multivariable process.}
}
@incollection{RADENSKI1998105,
title = {Parallel Probabilistic Computations on a Cluster of Workstations},
editor = {E.H. D'Hollander and F.J. Peters and G.R. Joubert and U. Trottenberg and R. Völpel},
series = {Advances in Parallel Computing},
publisher = {North-Holland},
volume = {12},
pages = {105-112},
year = {1998},
booktitle = {Parallel Computing},
issn = {0927-5452},
doi = {https://doi.org/10.1016/S0927-5452(98)80011-7},
url = {https://www.sciencedirect.com/science/article/pii/S0927545298800117},
author = {A. Radenski and A. Vann and B. Norris},
abstract = {Probabilistic algorithms are computationally intensive approximate methods for solving intractable problems. Probabilistic algorithms are excellent candidates for cluster computations because they require little communication and synchronization. It is possible to specify a common parallel control structure as a generic algorithm for probabilistic cluster computations. Such a generic parallel algorithm can be glued together with domain-specific sequential algorithms in order to derive approximate parallel solutions for different intractable problems. In this paper we propose a generic algorithm for probabilistic computations on a cluster of workstations. We use this generic algorithm to derive specific parallel algorithms for two discrete optimization problems: the knapsack problem and the traveling salesperson problem. We implement the algorithms on clusters of Sun Ultra SPARC-1 workstations using PVM, the parallel virtual machine software package. Finally, we measure the parallel efficiency of the cluster implementation.}
}
@article{BARRAUD1985139,
title = {SIRENA+: A Versatile Interactive System for Simulation, Identification and Control Design},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {8},
pages = {139-144},
year = {1985},
note = {3rd IFAC/IFIP Symposium on Computer Aided Design in Control and Engineering Systems: Advanced Tools for Modern Technology, Lyngby, Denmark, 31 July-2 August 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60357-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017603574},
author = {A. Barraud and P. Laporte and S. Gentil},
keywords = {Computer aided design, Control system analysis, Identification, Modelling},
abstract = {This paper deals with an interactive system for Simulation, identification and control of linear systems described by state-space or transfer-functions, in a continuous, discrete or sampled representation. Based upon a university prototype, SIRENA + has now reached an industrial quality level. Written in FORTRAN-77, this system has been implemented on a large variety of computers. Robustness has been ensured by a high quality mathematical software developed from and in a similar way as some well known libraries (Harwell, Nats projects, Matlab, …). Its modular structure (700 routines / 60 000 lines) greatly enhances its maintenability and evolution . The system is user friendly, with an on-line help and error checking, files saving options, and online graphical features. Commands are prompted adaptively according to the working context and to the user skillness. SIRENA + performs all the steps necessary for control design Simulation, identification and control.}
}
@incollection{DYMSCHIZ1979245,
title = {A PROCESS COMPUTER PROGRAM PACKAGE FOR INTERACTIVE COMPUTER AIDED DESIGN OF MULTIVARIABLE CONTROL SYSTEMS},
editor = {M. NOVAK},
booktitle = {Software for Computer Control},
publisher = {Pergamon},
pages = {245-250},
year = {1979},
isbn = {978-0-08-024448-8},
doi = {https://doi.org/10.1016/B978-0-08-024448-8.50048-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244488500484},
author = {E. Dymschiz},
abstract = {A software package is presented allowing the computer aided design of multivariable control algorithms. Based on discrete-time parametric process models, design methods for two basic feedback control algorithms are described. By means of an interactive dialog between the operator and the computer, control algorithms are designed and implemented on-line to carry out the direct digital control of an analog simulated multivariable process.}
}
@article{FORDE1990355,
title = {Object-oriented finite element analysis},
journal = {Computers & Structures},
volume = {34},
number = {3},
pages = {355-374},
year = {1990},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(90)90261-Y},
url = {https://www.sciencedirect.com/science/article/pii/004579499090261Y},
author = {Bruce W.R. Forde and Ricardo O. Foschi and Siegfried F. Stiemer},
abstract = {This paper describes the problems with conventional finite element analysis software and the potential solutions offered by object-oriented programs. It introduces the basic concepts of object-oriented programming and of expandable applications. Finite element fundamentals are explained using a new perspective leading to the implementation of an object-oriented numerical analysis program. Objects, classes, methods, and inheritance are illustrated using a graphical representation. Implementation-independent descriptions are given for the internal operation of a generic class system, followed by some details regarding the design of object-oriented programs using class libraries from an expandable application framework. A theoretical foundation is laid for the implementation of an object-oriented program which uses isoparametric elements for the numerical analysis of two-dimensional linear problems in solid and structural mechanics. Class descriptions are given in parallel to a step by step formulation of the analysis solution. The analysis program is portrayed as an assembly of classes which control and organize, the solution process. The design, implementation, operation, validation, and maintenance of this program is compared to that of an equivalent procedural program to identify the advantages of the object-oriented approach. Practical applications of object-oriented finite element analysis are discussed with particular reference to the use of knowledge-based expert systems.}
}
@article{SCHMID19791087,
title = {Identification and CAD of Adaptive Systems Using the KEDDC Package},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {8},
pages = {1087-1094},
year = {1979},
note = {5th IFAC Symposium on Identification and System Parameter Estimation, Darmstadt, Germany, 24-28 September},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65530-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017655307},
author = {Chr. Schmid and H. Unbehauen},
keywords = {Computer aided design, computer software, identification, control system synthesis, direct digital control, adaptive control, adaptive systems},
abstract = {KEDDC is a portable software frame especially designed to cover a wide range of control engineering tasks. At present is contains about 560 modules for process identification, controller design and testing by digital simulation using a broad variety of modern as well as classical methods. Special emphasis is given to the combination of process identification and controller design and to a systematic conception of adaptive control. In addition to that, implementation and on-line realization of control algorithms are included.}
}
@article{BUNKS198973,
title = {Optimal Control of Constrained Mechanical Systems},
journal = {IFAC Proceedings Volumes},
volume = {22},
number = {2},
pages = {73-78},
year = {1989},
note = {8th IFAC Workshop on Control Applications of Nonlinear Programming and Optimization 1989, Paris, France, 7-9 June 1989},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-037869-5.50016-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080378695500169},
author = {C. Bunks and R. Nikoukhah},
keywords = {Optimal control, computer-aided design, maximum principle, multivariable control systems, non-linear control systems, torque control},
abstract = {The equations of motion for a constrained mechanical (or multi-body) system can be obtained from the Hamiltonian description of the dynamics which leads to the Euler-Lagrange's equation. The result, in general, is a set of mixed differential and algebraic equations involving the generalized coordinates of the system, their derivatives, input forces and torques, and generalized constraint forces and torques (Lagrange multipliers associated with the constraints). It is possible, in principle, to reduce this set of differential- algebraic equations to a set of strictly differential equations by eliminating the generalized constraint forces and torques. In practice, however, the complexity of these algebraic manipulations is prohibitive. Consequently, the problem of optimal open-loop control for these systems cannot be treated by standard optimization packages which generally require that the time evolution of the system be described by differential or difference equations. In this paper, we obtain a procedure for calculating the optimal open-loop control for constrained mechanical systems. This procedure can be implemented using standard integration and optimization software. We present several examples of optimal controls for constrained mechanical systems which have been computed using Basile, a specialized software package for control and signal processing developed at INRIA.}
}
@incollection{KULCZYCKI1986271,
title = {COMPUTER-AIDED DESIGN OF INDUSTRIAL ELECTRICAL DISTRIBUTION NETWORKS},
editor = {P. MARTIN LARSEN and N.E. HANSEN},
booktitle = {Computer Aided Design in Control and Engineering Systems},
publisher = {Pergamon},
pages = {271-276},
year = {1986},
isbn = {978-0-08-032557-6},
doi = {https://doi.org/10.1016/B978-0-08-032557-6.50054-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325576500546},
author = {J. Kulczycki and M. Zabicki},
abstract = {The paper presents general-purpose computer-aided design system for the determination of the optimal design of the industrial, electrical distribution system. A cost-effective structure for such a network is often a multilevel, hierarchical structure with one or more voltage levels and several kinds of equipment. It consists of main supply point, transformer substations and/or switchboards together with supplying lines (which all together form a “backbone network”) and cable feeders connecting demand points to backbone network (local networks). A distribution planning model is formulated using graph theory and discrete programming. Decisions can be taken on two levels - the first concerns backbone network, the second specifies assignments of demand points to particular elements of the backbone. The model is rather general - backbone can be any of loopless structure. The problem structure typically leads to heuristic solutions of an iterative nature. A kind of such method is implemented in a general-purpose software package letting on automatic design of industrial electrical distribution systems.}
}
@article{MEFRAZKHAN20142165,
title = {Covariance-guided One-Class Support Vector Machine},
journal = {Pattern Recognition},
volume = {47},
number = {6},
pages = {2165-2177},
year = {2014},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2014.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0031320314000077},
author = {Naimul {Mefraz Khan} and Riadh Ksantini and Imran {Shafiq Ahmad} and Ling Guan},
keywords = {Covariance, Support Vector Machine, One-class classification, Outlier detection},
abstract = {In one-class classification, the low variance directions in the training data carry crucial information to build a good model of the target class. Boundary-based methods like One-Class Support Vector Machine (OSVM) preferentially separates the data from outliers along the large variance directions. On the other hand, retaining only the low variance directions can result in sacrificing some initial properties of the original data and is not desirable, specially in case of limited training samples. This paper introduces a Covariance-guided One-Class Support Vector Machine (COSVM) classification method which emphasizes the low variance projectional directions of the training data without compromising any important characteristics. COSVM improves upon the OSVM method by controlling the direction of the separating hyperplane through incorporation of the estimated covariance matrix from the training data. Our proposed method is a convex optimization problem resulting in one global optimum solution which can be solved efficiently with the help of existing numerical methods. The method also keeps the principal structure of the OSVM method intact, and can be implemented easily with the existing OSVM libraries. Comparative experimental results with contemporary one-class classifiers on numerous artificial and benchmark datasets demonstrate that our method results in significantly better classification performance.}
}
@article{MAGNENATTHALMANN1983107,
title = {Transferring a macro program to a micro machine},
journal = {Microprocessors and Microsystems},
volume = {7},
number = {3},
pages = {107-110},
year = {1983},
issn = {0141-9331},
doi = {https://doi.org/10.1016/0141-9331(83)90001-7},
url = {https://www.sciencedirect.com/science/article/pii/0141933183900017},
author = {Nadia Magnenat-Thalmann and Alain Choquette and Daniel Thalmann},
keywords = {microsystems, graphics, PASCAL},
abstract = {A powerful graphical PASCAL extension has transferred from a large computer to a microcomputer. The graphical extension, called M1RA-2D, allows the user to program computer graphics applications in a modern, structured and efficient language. The problems encountered were mainly due to the limits of a microcomputer-based system and to the size of the project. Problems involved memory space, documentation, low-power peripherals, PASCAL restrictions and hardware robustness and quality. Finally, M1RA-2D is compared with two other graphics software packages implemented on microcomputers: Applegraphics and an implementation of a 2D, level 2 Core System.}
}
@article{FOWLER199561,
title = {Porting a three-dimensional semiconductor device modelling program to the Intel iPSC/860 hypercube},
journal = {Future Generation Computer Systems},
volume = {11},
number = {1},
pages = {61-70},
year = {1995},
note = {BECAUSE Workshop, Part II},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(94)00048-J},
url = {https://www.sciencedirect.com/science/article/pii/0167739X9400048J},
author = {R.F. Fowler and B.W. Henderson and C. Greenough},
keywords = {Parallel processing, iPSC/860, Semiconductor device simulation, CGS, ICCG, Mesh partitioning},
abstract = {EVEREST is a three-dimensional device modelling package used to study the electrical behaviour of semiconductor devices. A set of partial differential equations describing the current flow within the device is solved using a mixed mesh of tetrahedral and hexahedral elements. The highly non-linear nature of the equations requires that a flexible solution strategy be used to make the software robust and efficient. Within the solution process large sparse non-symmetric linear systems are solved using iterative solvers such as CGS with preconditioning. Run times of many hours or even days are required for realistic devices on current workstations. EVEREST consists of over 100 000 lines of Fortran and was developed for scalar architectures. The main aim of this work was to investigate how such an application could be adapted to run on the iPSC/860. A review of some of the possible parallelisation techniques is made. Most techniques for the parallel implementation of such mesh based calculations on MIMD machines involve a partitioning of the mesh. An extension of an algorithm due to Farhat has been implemented in a pre-processor to partition the device meshes used by EVEREST. Aspects of load balancing and the need to minimise the number of interface nodes in the decomposition are discussed. Some initial results using the partitioned mesh to perform the system matrix assembly are presented. Methods that could be used to implement a parallel preconditioned CGS solver on the iPSC/860 are reviewed and some performance estimates made using the results of certain BBS tests on this machine.}
}
@article{CUNO1985325,
title = {Application of a Multivariable Robust Controller Design Method to Hard-Coal Preparation},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {8},
pages = {325-330},
year = {1985},
note = {3rd IFAC/IFIP Symposium on Computer Aided Design in Control and Engineering Systems: Advanced Tools for Modern Technology, Lyngby, Denmark, 31 July-2 August 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60390-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017603902},
author = {B. Cuno and W. Neddermeyer},
keywords = {Multivariable robust controller design, parameter uncertainty, vector performance criterion, CAD},
abstract = {The paper describes an application of a robust controller design method for calculating the parameters of a controller for a multivariable hard-coal preparation process. The parameters are optimized via a sequential design method in the frequency domain, being based on a combination of the Horowitz/Sidi design (1972), Mayne's (1973) sequential design and Steinhauser/Kreisselrooier's (1979) vector performance criterion method. The design was carried out using a relatively sophisticated CAD-program. The paper shows how the parameters are calculated with this CAD-method and discusses the simulated control results. The simulation is compared with the measured results at the plant after implementation of the designed controller on a process computer AEG 80/30 with a real time software control package ARSI**ARSI is a trademark of AEG.}
}
@article{ALBRECHT199227,
title = {Mobile robotics: A paradigm for complex system engineering},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {9},
number = {1},
pages = {27-33},
year = {1992},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(92)90016-Y},
url = {https://www.sciencedirect.com/science/article/pii/073658459290016Y},
author = {Robert W. Albrecht},
abstract = {Mobile robotics is being used as an educational vehicle for teaching the principles of complex system engineering. A mobile robot is a complex system involving mechanical hardware, a suite of transducers, computer control of actuators and sensors, and a hierarchical control system. The operation of an autonomous mobile robot incorporates attributes of artificial intelligence with interfaces via radio links from base computers to on-board computers. A general plan supported by hardware, software shells, senior/graduate courses and graduate research is in place that facilitates the evolution of autonomous mobile robots capable of navigating and performing prototypic tasks in semi-structured environments. A series of projects is designed to enhance the repertoire of skills of the robots to robustly accomplish a range of tasks from simple to complex. The skills are built within a structure that is designed to be cumulative. The result is forecast to be systematic improvement in a generalized knowledge base that is identified as the key to implementation of semi-autonomous mobile robots in semi-structured environments. A university is an educational institution. This means that students are temporary. Students are always on a learning curve and they complete courses and degrees and leave the institution. One important component for achieving the goal of a continually evolving mobile robotic vehicle in the university environment is to document incremental student progress and package this information so that future students can rapidly assimilate previous progress and make a contribution to the continuing evolution of the mobile robotics program. This is accomplished through a set of modules called “robo-tutors”. Robo-tutors are tutorials for students that are written in an expert system shell and provide automated instruction concerning various aspects of the mobile robots program.}
}
@article{ERRICO20091642,
title = {Energy saving in a crude distillation unit by a preflash implementation},
journal = {Applied Thermal Engineering},
volume = {29},
number = {8},
pages = {1642-1647},
year = {2009},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2008.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1359431108003177},
author = {Massimiliano Errico and Giuseppe Tola and Michele Mascia},
keywords = {Preflash drum, Preflash column, Energy saving, Crude distillation unit},
abstract = {After the 70s energy crisis the revamping of plants designed before this date is very attractive for improving energy recovery and lowering operation costs. A typical case is the oil refinery plant where an intensive usage of energy takes place and is a promising case for the application of energy saving solutions. In this work we focused our attention to an industrial crude oil distillation unit, evaluating the possibility to modify the feed conditions by installing a preflash drum or a preflash plate column. Real data plant were collected to obtain a reliable simulation of the unit by means of the software package Aspen Plus 13.0. To characterize the crude oil fed the TBP curve was used. The results obtained were compared with the plant data in terms of flow rate and product quality utilizing the ASTM D-86 curves and a good agreement was obtained. According to the specialized literature the preflash drum/column was placed at the end of the pre-heat train, just before the column furnace. The furnace is the bottleneck of the plant and with both the preflash devices it is possible to lower its energy consumption. However the energy reduction is associated to the decrease of one kind of distillates (light or middle). The choice of the best preflash device was made according to the production asset of the plant.}
}
@article{PATNAIK198293,
title = {Implementation of an interactive relational graphics database},
journal = {Computers & Graphics},
volume = {6},
number = {3},
pages = {93-96},
year = {1982},
issn = {0097-8493},
doi = {https://doi.org/10.1016/0097-8493(82)90001-2},
url = {https://www.sciencedirect.com/science/article/pii/0097849382900012},
author = {L.M. Patnaik and N. Ramesh},
abstract = {The benefits that accrue from the use of design database include (i) reduced costs of preparing data for application programs and of producing the final specification, and (ii) possibility of later usage of data stored in the database for other applications related to Computer Aided Engineering (CAE). An INTEractive Relational GRAphics Database (INTERGRAD) based on relational models has been developed to create, store, retrieve and update the data related to two dimensional drawings. INTERGRAD provides two languages, Picture Definition Language (PDL) and Picture Manipulation Language (PML). The software package has been implemented on a PDP 11/35 system under the RSX-11M version 3.1 operating system and uses the graphics facility consisting of a VT-11 graphics terminal, the DECgraphic 11 software and an input device, a lightpen.}
}
@article{CHEN199891,
title = {An integrated graphical user interface (GUI) for concurrent engineering design of mechanical parts},
journal = {Computer Integrated Manufacturing Systems},
volume = {11},
number = {1},
pages = {91-112},
year = {1998},
issn = {0951-5240},
doi = {https://doi.org/10.1016/S0951-5240(98)00016-0},
url = {https://www.sciencedirect.com/science/article/pii/S0951524098000160},
author = {Kun-Hur Chen and Shi-Jie (Gary) Chen and Li Lin and S.Wesley Changchien},
keywords = {concurrent engineering, graphical user interface (GUI), feature-based design, design for manufacturing (DFM), design for assembly (DFA), knowledge-based systems},
abstract = {Due to increasing competition in the manufacturing industry, the search for shorter product development and production cycles and lower cost has led to the emergence of concurrent engineering. Concurrent engineering is the practice whereby the design needs to simultaneously consider various downstream activities throughout the entire product life cycle, in addition to meeting the products' functions. This calls for an integrated design environment that will enable the engineers to evaluate multiple constraints from manufacturing, assembly, services, etc. at the design stage. This research develops a prototype for such an integrated graphical user interface (GUI), the Integrated Concurrent Engineering Design for Mechanical Parts (ICEDMP), in concurrent engineering. Using ICEDMP, the user can access product data from different domains of computerized tools in one software system and evaluate the design of mechanical products based on criteria of Design for Manufacturing (DFM) and Design for Assembly (DFA). Modules of ICEDMP include (1) a web-based on-line user's guide, (2) a part library, (3) a design guidelines checklist, (4) a part modeler linked to CAD (Pro/Engineer), (5) a part feature converter using Practical Extraction and Report Language (PERL), and (6) a knowledge-based design critique system. Parts created by Pro/Engineering CAD system are represented by features using the PERL feature converter. The design's consequential impact on manufacture and assembly is then evaluated by the knowledge-based design critique system implemented by CLIPS. Two design examples are presented to demonstrate the effectiveness of the ICEDMP system. Using ICEDMP, the user is able to evaluate the designs using DFM and DFA criteria and obtain suggestions to improve the design in one integrated software environment. This will result in fewer costly design changes and thus reduce product development time and production cost.}
}
@article{BALLARD20153,
title = {Reconstructing Householder vectors from Tall-Skinny QR},
journal = {Journal of Parallel and Distributed Computing},
volume = {85},
pages = {3-31},
year = {2015},
note = {IPDPS 2014 Selected Papers on Numerical and Combinatorial Algorithms},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S074373151500101X},
author = {G. Ballard and J. Demmel and L. Grigori and M. Jacquelin and N. Knight and H.D. Nguyen},
keywords = {QR decomposition, Dense linear algebra, Communication-avoiding algorithms},
abstract = {The Tall-Skinny QR (TSQR) algorithm is more communication efficient than the standard Householder algorithm for QR decomposition of matrices with many more rows than columns. However, TSQR produces a different representation of the orthogonal factor and therefore requires more software development to support the new representation. Further, implicitly applying the orthogonal factor to the trailing matrix in the context of factoring a square matrix is more complicated and costly than with the Householder representation. We show how to perform TSQR and then reconstruct the Householder vector representation with the same asymptotic communication efficiency and little extra computational cost. We demonstrate the high performance and numerical stability of this algorithm both theoretically and empirically. The new Householder reconstruction algorithm allows us to design more efficient parallel QR algorithms, with significantly lower latency cost compared to Householder QR and lower bandwidth and latency costs compared with Communication-Avoiding QR (CAQR) algorithm. Experiments on supercomputers demonstrate the benefits of the communication cost improvements: in particular, our experiments show substantial improvements over tuned library implementations for tall-and-skinny matrices. We also provide algorithmic improvements to the Householder QR and CAQR algorithms, and we investigate several alternatives to the Householder reconstruction algorithm that sacrifice guarantees on numerical stability in some cases in order to obtain higher performance.}
}
@article{JAIN2016328,
title = {Dispatching strategies for managing uncertainties in automated manufacturing systems},
journal = {European Journal of Operational Research},
volume = {248},
number = {1},
pages = {328-341},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.06.060},
url = {https://www.sciencedirect.com/science/article/pii/S0377221715005937},
author = {S. Jain and W.J. Foley},
keywords = {Operations, Scheduling/sequencing, Rescheduling, Simulation, Experiments},
abstract = {Manufacturers in the western world need to exploit and perfect all their strengths to reduce the flight of manufacturing to global outsourcing destinations. Use of automated manufacturing systems (AMSs) is one such strength that needs to be improved to perfection. One area for improvement is the management of uncertainties on the production floor. This paper explores strategies for modifying detailed event list schedules following the occurrence of an interruption. Advanced planning and scheduling (APS) software packages provide a detailed advance plan of production events. However, the execution of this advance plan is disrupted by a myriad of unanticipated interruptions, such as machine breakdowns, yield variations, and hot jobs. The alternatives available to respond to such interruptions can be classified in four groups: regenerating the complete schedule using APS, switching to dispatching mode, modifying the existing schedule, and continuing to follow the schedule and letting the production system gradually absorb the impact of the interruption. Regeneration of the complete schedule using APS requires a large computation effort, may result in large changes in the schedule, and hence is not recommended. This paper reports on an experimental study for evaluating 10 strategies for responding to machine failures in AMSs that broadly fall in the latter three groups. The strategies are evaluated using simulation under an experimental design with manufacturing scenario, load level, severity and duration of interruptions as factors. The results are analyzed to understand the strengths and weaknesses of the considered strategies and develop recommendations.}
}
@incollection{DAVIDOVICIU1979101,
title = {SOFTWARE PACKAGE FOR PROCESS CONTROL WITH MICROPROCESSORS},
editor = {M. NOVAK},
booktitle = {Software for Computer Control},
publisher = {Pergamon},
pages = {101-104},
year = {1979},
isbn = {978-0-08-024448-8},
doi = {https://doi.org/10.1016/B978-0-08-024448-8.50019-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244488500198},
author = {A. Davidoviciu and D. Mihalca and P. Rˆdulescu and L. Iacob and A. Petrovici and D. Strujan and R. Hie and D. Voicu},
abstract = {Some problems regarding the microprocessors and their software are considered. Further, there are presented the principles for writting software for micros and several appropriate definitions. As the main part of the paper the general structure of a real-time monitor for the Romanian FELIX M-18 microcomputer, built up around the INTEL 8080 microprocessor is described. There are discused the scheduler, the system directives, the drivers, the interrupt handling routines, the man-machine communication and the SCAN and CONTROL routines for process control. Finally, there are presented some considerations regarding software package realization, implementation and performances.}
}
@article{BECKERS1996449,
title = {Parallel processing of chemical information in a local area network—III. Using genetic algorithms for conformational analysis of biomacromolecules},
journal = {Computers & Chemistry},
volume = {20},
number = {4},
pages = {449-457},
year = {1996},
issn = {0097-8485},
doi = {https://doi.org/10.1016/0097-8485(95)00086-0},
url = {https://www.sciencedirect.com/science/article/pii/0097848595000860},
author = {M.L.M. Beckers and E.P.P.A. Derks and W.J. Melssen and L.M.C. Buydens},
abstract = {Multi-dimensional nuclear magnetic resonance experiments are an excellent means of revealing the three-dimensional structure of biomacromolecules in solution. However, the search space in the conformational analysis of biomacromolecules, using multi-dimensional NMR data, is huge and complex. This calls for global optimization techniques with good sampling properties. This paper describes a genetic algorithm that optimizes the fit between (simulated) experimental two-dimensional Nuclear Overhauser Effect spectra and the corresponding calculated spectra for trial structures. This is a very computational intensive procedure. Speed-up of performance is achieved by parallelizing the algorithm, i.e. creating small subpopulations of trial structures, each of which can be processed on different processors. Good sampling behavior is obtained by initializing each subpopulation with its own random seed and the introduction of a migration operator. The latter replaces the best performing individual from one subpopulation with the worst performing individual from another subpopulation after a predetermined number of generations. A parallel genetic algorithm for the conformational analysis of nucleic acids is developed using the software package HYDRA. It is demonstrated that, for the data sets used in the study, a considerable reduction in computation time is obtained for the parallel genetic algorithm as compared to a sequential implementation, while the same optimal solutions are found.}
}