@article{GRAY19991279,
title = {Developing technologies for broad-network concurrent computing},
journal = {Journal of Systems Architecture},
volume = {45},
number = {15},
pages = {1279-1291},
year = {1999},
issn = {1383-7621},
doi = {https://doi.org/10.1016/S1383-7621(98)00068-X},
url = {https://www.sciencedirect.com/science/article/pii/S138376219800068X},
author = {Paul A Gray and Vaidy S Sunderam},
keywords = {Distributed virtual machine, Distributed computing, Java, Native methods, Heterogeneous cluster computing},
abstract = {Recent developments in networking infrastructures, computer workstation capabilities, software tools, and programming languages have motivated new approaches to broad-network concurrent computing. This paper describes extensions to concurrent computing which blend new and evolving technologies to extend users' access to resources beyond their local network. The result is a concurrent programming environment which can dynamically extend over network and file system boundaries to envelope additional resources, to enable multiple-user collaborative programming, and to achieve a more optimal process mapping. Additional aspects of the derivative environment feature extended portability and support for the accessing of legacy codes and packages. This paper describes the advantages of such a design and how they have been implemented in the environment termed “IceT”.}
}
@article{HABIEB2019109281,
title = {Hybrid seismic base isolation of a historical masonry church using unbonded fiber reinforced elastomeric isolators and shape memory alloy wires},
journal = {Engineering Structures},
volume = {196},
pages = {109281},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.109281},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619304584},
author = {Ahmad Basshofi Habieb and Marco Valente and Gabriele Milani},
keywords = {UFREI, SMA wire, FE model, Abaqus user element, Masonry church, Non-linear dynamic analysis},
abstract = {One of the most promising devices for seismic base isolation of structures is the Unbonded Fiber Reinforced Elastomeric Isolator (UFREI) due to its low manufacturing cost and horizontal stiffness. This paper investigates the possibility of combining UFREIs and shape memory alloy (SMA) wires to increase the energy dissipation capacity of the isolation system for the seismic protection of a historical masonry church. Detailed 3D finite element (FE) analyses are performed to characterize the response of UFREIs under cyclic displacements. The behavior of SMA is simulated through a thermomechanical constitutive model implemented in a user-defined material (UMAT) subroutine available in the software package Abaqus. To reduce the computational effort in non-linear dynamic analyses of large isolated structures, an Abaqus user element (UEL) is developed to represent the 3D behavior of the isolation system proposed in this study. Non-linear dynamic time history analyses are then carried out to evaluate the seismic response of a historical masonry church in different configurations (fixed-base model and model equipped with different base isolation systems) for moderate and severe seismic intensity levels. Numerical results show that the damage observed in the masonry church can be considerably reduced through the insertion of UFREIs. The utilization of SMA wires with a specific pre-strain significantly increases the energy dissipation capacity of the base isolation system and decreases the horizontal displacements of the masonry church.}
}
@article{KUMAR1989883,
title = {Finite element design sensitivity analysis and its integration with numerical optimization techniques for structural design},
journal = {Computers & Structures},
volume = {32},
number = {3},
pages = {883-897},
year = {1989},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(89)90372-6},
url = {https://www.sciencedirect.com/science/article/pii/0045794989903726},
author = {V. Kumar and S.-J. Lee and M.D. German},
abstract = {This paper presents the development of a structural design optimization methodology and a software system DESIGN-OPT by integrating numerical optimization techniques, finite element methods, and pre- and post-processing CAE tools. Specifically, the commercially available codes COPES/ADS and ADINA are employed for numerical optimization and finite element analysis, respectively; and software packages like MOVIE.BYU, PLOT 10 and SUPERTAB are used for pre- and post-processing purposes. The finite-difference and semi-analytical (or implicit differentiation) approaches of design sensitivity analysis were explored with the ADINA code for static as well as dynamic problems; and a comparison of the two approaches is made in terms of computational efficiency, solution accuracy and the ease of software implementation. A broad range of element types (truss, beam, plate and 2D/3D continuum) are considered, and the effects of centrifugal and thermal loadings are included. A number of interface programs are developed to integrate various computer codes: for example, OPT-AN and AN-OPT processors transfer data between the optimizer and the analyzer. A variety of design problems related to both size and shape optimization are presented as illustrative examples. Finally, some remarks are made for further research on the subject, such as the role of geometric modeling and automatic meshing for shape optimization.}
}
@article{PROTHERO1985185,
title = {Topper, a software package in FORTRAN for scaling studies},
journal = {International Journal of Bio-Medical Computing},
volume = {17},
number = {3},
pages = {185-191},
year = {1985},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(85)90021-2},
url = {https://www.sciencedirect.com/science/article/pii/0020710185900212},
author = {J.D. Prothero and J.W. Prothero},
abstract = {Scaling studies are concerned with the differences in structure, functions, behaviour and life-history associated with differences in size in broadly similar organisms. These studies make extensive use of bivariate regression analysis. Often it is of interest to carry out such analyses on many different subpopulations within a given sample population. The programs described here allow these analyses to be carried out efficiently. The output gives a summary of taxonomic parameters in addition to the usual statistical parameters, such as standard deviations and mean percent deviation. Some improvements in the software which are currently being implemented are discussed briefly.}
}
@article{LEE20131338,
title = {New design approach of FPGA based control system and implementation result in KSTAR},
journal = {Fusion Engineering and Design},
volume = {88},
number = {6},
pages = {1338-1341},
year = {2013},
note = {Proceedings of the 27th Symposium On Fusion Technology (SOFT-27); Liège, Belgium, September 24-28, 2012},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2013.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S0920379613001580},
author = {Woongryol Lee and Mikyung Park and Seongheon Seo and Hyungshin Kim},
keywords = {KSTAR, FPGA, FMC, Reflectometry, Data acquisition system},
abstract = {The frequency modulation (FM) reflectometer in KSTAR is under commissioning to measure the electron density profile which focused on edge area of plasma. This diagnostic system generated order of tens MHz output signal from an intermediate frequency (IF) amplifier after a radio frequency (RF) mixer. An arbitrary waveform generator is also required for triggering a fast frequency sweep source. We have developed a data acquisition system with multi-functions which composes several digitizers and waveform generator for KSTAR reflectometer. Along with this typical hardware system, we have newly developed a field programmable gate array (FPGA) based signal controller. This customized board adapts FPGA Mezzanine Card (FMC) as a daughter board within single package of 3U Compact PCI form factor. We have been evaluating Experimental Physics and industrial control system (EPICS) input output controller (IOC) over an embedded PowerPC processor on several customized hardware. Using previous result of investigation, this new processor embedded digitizer supports full function of EPICS including KSTAR standard software framework.}
}
@article{WANG2010230,
title = {Efficient registration of optical and IR images for automatic plant water stress assessment},
journal = {Computers and Electronics in Agriculture},
volume = {74},
number = {2},
pages = {230-237},
year = {2010},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2010.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0168169910001481},
author = {Xuezhi Wang and Weiping Yang and Ashley Wheaton and Nicola Cooley and Bill Moran},
keywords = {Thermal and optical image registration, Crop water stress index, Canopy temperature estimation, Plant water status, Automatic controlled irrigation},
abstract = {Automatic registration of optical and IR images is a crucial step towards constructing an automated irrigation control system where plant water information is sensed via thermal imaging. The scene of the IR image is assumed to be completely included in the optical image and the alignment between the common scene in the two images may involve translation and rotation by a small angle, though a small scale difference may also be present. This automatic registration of data from two quite different, non-rigid imaging regimes presents several challenges, which cannot be overcome using common image processing techniques. In this paper, a fully automatic image registration algorithm for the alignment of optical and IR image pairs is described, where Pearson's cross-correlation between a pair of images serves as the similarity measure. A computationally efficient algorithm is designed and packaged as a software application. This work provides an intervention free process for extracting plant water stress information which can be fed into an automated irrigation scheduling program. The proposed algorithm is justified by the comparison of its registration performance with that of other potential algorithm techniques using several experimental data collections. Our results demonstrate the effectiveness of the proposed algorithm and efficiency of its application to the registration of IR and optical images.}
}
@article{BODI199275,
title = {A Rexx-controlled developing environment for implementing intuitive user interfaces (IUI)},
journal = {Computer Methods and Programs in Biomedicine},
volume = {37},
number = {2},
pages = {75-84},
year = {1992},
issn = {0169-2607},
doi = {https://doi.org/10.1016/0169-2607(92)90088-O},
url = {https://www.sciencedirect.com/science/article/pii/016926079290088O},
author = {Richard A. Bödi and Theodor W. Kaulich},
keywords = {Intuitive user interface, Software integration, Rexx programming language, Source-code generators, CASE},
abstract = {In [our companion paper] intuitive user interfaces as well as a method for developing and programming such interfaces have been described. The present article will improve and simplify this mode of action. The main tool for this undertaking is the programming and script language Rexx. Using Rexx, all software components described in [the companion paper] are linked to form a developing environment, which acts like an integrated software package. Moreover, with the aid of Rexx, source-code and data structure generation is greatly extended, further decreasing the expenditure of programming intuitive user interfaces. This paper first gives a brief introduction to Rexx. A description of the Rexx-controlled developing environment then follows. Finally, there are elucidated the details of the question of how each software component is linked to the environment using Rexx.}
}
@article{KODHELAJ201980,
title = {Designing and deploying a business process for product recovery and repair at a servicing organization: A case study and framework proposal},
journal = {Computers in Industry},
volume = {105},
pages = {80-98},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518300885},
author = {Iva Kodhelaj and Claudia-Melania Chituc and Erik Beunders and Dave Janssen},
keywords = {Business process, Product recovery and repair, Harvesting parts of medical equipment, Enterprise information system, Case study, Servicing organization},
abstract = {The environmental concerns and the benefits associated with product recovery have influenced companies to focus on the remanufacturing and recycling of discarded products. Although associated with numerous benefits, this practice is still not largely applied in the service business. The purpose of this article is to present a framework for designing and deploying a business process for product recovery and repair at a servicing organization. The proposed framework comprises seven elements (Organization, Quality & Regulations, Information Technology, Purchasing, Finance, Processes, and Implementation), and their sequence corresponds to the actual steps towards designing and deploying a harvesting business process. The proposed framework was empirically validated for specific scenarios at a servicing organization in the Netherlands, illustrating how the business process for harvesting parts of medical equipment is enabled through SAP ERP transactions, while addressing elicited requirements. The tests executed in the SAP MBQ software environment showed that the proposed solution is supported in a real business environment, illustrating its practical relevance. This framework can easily be applied in different sectors. The work pursued has a scientific contribution and practical use in industry, providing a unique niche for researchers, practitioners, information technology managers and developers to make significant contributions, serving as a guiding tool, and supporting them in understanding the practical implications of deploying a harvesting business process and systematically managing it.}
}
@article{OTTE2016371,
title = {A toolbox using the stochastic optimization algorithm MIPT and ChemCAD for the systematic process retrofit of complex chemical processes},
journal = {Computers & Chemical Engineering},
volume = {84},
pages = {371-381},
year = {2016},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2015.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0098135415002847},
author = {Daniel Otte and Hilke-Marie Lorenz and Jens-Uwe Repke},
keywords = {Optimization, Molecular-Inspired Parallel Tempering, Multicomponent separation process, Retrofit, Stochastic algorithm, ChemCAD™},
abstract = {Global optimization techniques using powerful algorithms have led to a wide range of applications to increase the efficiency of chemical processes. Nevertheless, the performance for optimization of process models is limited by a certain complexity, especially accounting for existing processes (retrofit). Due to the great combinatorial diversity of possible alternatives a systematic approach is essential. The local integration of modifications in the overall process leads to changes in internal streams. Therefore new operating points have to be found and resulting effects on the plant performance have to be evaluated. An optimization framework for the purpose of retrofitting using a rigorous process simulation tool is proposed to fulfill this task. Here, flowsheet simulation software packages are offering a high performance for the prediction of new operation points for following units, and for units affected by recycle streams. An optimization approach using flowsheet simulation software and the stochastic optimization algorithm Molecular-Inspired Parallel Tempering (MIPT) implemented in the programming software Matlab™ is presented. Both of these programs are linked via OPC (OLE for process control), a standard communication platform. The toolbox provides a quick evaluation of the process by searching for the global optimum. The MIPT algorithm is suitable for large optimization problems and can handle constraints and infeasibilities. The usage of a rigorous process simulator is providing a high accuracy of the thermodynamic results which is necessary to evaluate the influence of the new process design. Furthermore, a simulation model of the industrial plant can directly be used for the optimization. A complex multicomponent separation process with recycle streams is used to demonstrate the advantages of the proposed toolbox. To simplify the user input a graphical user interface was programmed. The results of a sensitivity analysis and the optimization for different feed compositions are presented.}
}
@article{VIKAL201019,
title = {Perk Station—Percutaneous surgery training and performance measurement platform},
journal = {Computerized Medical Imaging and Graphics},
volume = {34},
number = {1},
pages = {19-32},
year = {2010},
note = {Image-Guided Surgical Planning and Therapy},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2009.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0895611109000603},
author = {Siddharth Vikal and Paweena U-Thainual and John A. Carrino and Iulian Iordachita and Gregory S. Fischer and Gabor Fichtinger},
keywords = {Image guidance, Needle placement, Augmented reality, Surgical training},
abstract = {Motivation
Image-guided percutaneous (through the skin) needle-based surgery has become part of routine clinical practice in performing procedures such as biopsies, injections and therapeutic implants. A novice physician typically performs needle interventions under the supervision of a senior physician; a slow and inherently subjective training process that lacks objective, quantitative assessment of the surgical skill and performance. Shortening the learning curve and increasing procedural consistency are important factors in assuring high-quality medical care.
Methods
This paper describes a laboratory validation system, called Perk Station, for standardized training and performance measurement under different assistance techniques for needle-based surgical guidance systems. The initial goal of the Perk Station is to assess and compare different techniques: 2D image overlay, biplane laser guide, laser protractor and conventional freehand. The main focus of this manuscript is the planning and guidance software system developed on the 3D Slicer platform, a free, open source software package designed for visualization and analysis of medical image data.
Results
The prototype Perk Station has been successfully developed, the associated needle insertion phantoms were built, and the graphical user interface was fully implemented. The system was inaugurated in undergraduate teaching and a wide array of outreach activities. Initial results, experiences, ongoing activities and future plans are reported.}
}
@article{LIN1988403,
title = {Computer Aided Design and Control for a Raw Mill in Cement Manufactory},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {8},
pages = {403-407},
year = {1988},
note = {4th IFAC Symposium on computer aided Design in Control Systems 1988, Beijing, PRC, 23-25 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54986-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017549861},
author = {P. Lin and P. Prevot and J.P. Barbier},
keywords = {Computer-aided design, self-adaptive control, optimal control, heuristic adjustment, cement industry, raw mill},
abstract = {A multilevel software package which can be used to aid the design of controllers and the examination of controller’s efficiency is presented in this paper. With a man-machine interface, one can easily implement the different modes of control strategy, such as optimal control with multicriterion and self-adaptive control. We apply this software tool to a raw mill process in a cement manufactory. After modeling, identifying and simulating the process behaviour, a self-adaptive control system with different algorithms of optimal control is developped for chemical composition of raw materials by using correction products. In addition, a heuristical adaptive supervisor is designed for adjusting the parameters used in the control policy according to process performance.}
}
@article{ZHENG20161025,
title = {J-TEXT WebScope: An efficient data access and visualization system for long pulse fusion experiment},
journal = {Fusion Engineering and Design},
volume = {112},
pages = {1025-1028},
year = {2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2016.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0920379616304070},
author = {Wei Zheng and Kuanhong Wan and Zhi Chen and Feiran Hu and Qiang Liu},
keywords = {Data access and visualization, Long pulse, WebScope, RESTful, MDSplus, J-TEXT},
abstract = {Fusion research is an international collaboration work. To enable researchers across the world to visualize and analyze the experiment data, a web based data access and visualization tool is quite important [1]. Now, a new WebScope based on RIA (Rich Internet Application) is designed and implemented to meet these requirements. On the browser side, a fluent and intuitive interface is provided for researchers at J-TEXT laboratory and collaborators from all over the world to view experiment data and related metadata. The fusion experiments will feature long pulse and high sampling rate in the future. The data access and visualization system in this work has adopted segment and scale concept. Large data samples are re-sampled in different scales and then split into segments for instant response. It allows users to view extremely large data on the web browser efficiently, without worrying about the limitation on the size of the data. The HTML5 and JavaScript based web front-end can provide intuitive and fluent user experience. On the server side, a RESTful (Representational State Transfer) web API, which is based on ASP.NET MVC (Model View Controller), allows users to access the data and its metadata through HTTP (HyperText Transfer Protocol). An interface to the database has been designed to decouple the data access and visualization system from the data storage. It can be applied upon any data storage system like MDSplus or JTEXTDB, and this system is very easy to deploy as well. A WebScope with data provider both for MDSplus and JTEXTDB is implemented and deployed. The WebScope has been integrated into J-TEXT LogBook, an experimental recording system for J-TEXT. It is now serving J-TEXT daily experiment. In the future more databases as the data source will be supported and it will support visualization of more data types besides waveform.}
}
@article{FRITZ1990253,
title = {Practical Experience in Software Engineering with ADA: Building an Operation Guidance; Recovery and Maintenance Planning System},
journal = {IFAC Proceedings Volumes},
volume = {23},
number = {8, Part 4},
pages = {253-262},
year = {1990},
note = {11th IFAC World Congress on Automatic Control, Tallinn, 1990 - Volume 4, Tallinn, Finland},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51832-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017518327},
author = {W. Fritz and V.H. Haase and R. Kalcher},
keywords = {Software Engineering, Decision Support System, Intelligent Control, Real Time Programming, ADA, ORACLE, GKS},
abstract = {In the development of an operation guidance, recovery and maintenance planning system for a large telecommunication network handling both data and voice the use of commercial standard software has been chosen. A network analysis system, capable to model several thousands of nodes and links together with a decision support system for the operator is being developped in an ADA environment on VAX/VMS machines, data storage is organized around an ORACLE database, and man-machine communication is almost exclusively done in graphics using GKS. It will be shown how the standard packages have been integrated to guarantee both minimum development effort for the project, and fast real time response. Various compatibility problems and their solutions will be discussed, as well as criteria which standard software to choose in which cases. Practical experiences and benefits using ADA in a small team of three to four software engineers both as a design and implementation language combined with rapid prototyping as a development methodology will be presented.}
}
@article{DINARDO2013215,
title = {MATLAB-implemented estimation procedure for model-based assessment of hepatic insulin degradation from standard intravenous glucose tolerance test data},
journal = {Computer Methods and Programs in Biomedicine},
volume = {110},
number = {2},
pages = {215-225},
year = {2013},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2012.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169260712002490},
author = {Francesco {Di Nardo} and Michele Mengoni and Micaela Morettini},
keywords = {C-peptide and insulin minimal model, Indexes of β-cell responsiveness, Optimization algorithm, Glucose–insulin regulatory system},
abstract = {Present study provides a novel MATLAB-based parameter estimation procedure for individual assessment of hepatic insulin degradation (HID) process from standard frequently-sampled intravenous glucose tolerance test (FSIGTT) data. Direct access to the source code, offered by MATLAB, enabled us to design an optimization procedure based on the alternating use of Gauss-Newton's and Levenberg-Marquardt's algorithms, which assures the full convergence of the process and the containment of computational time. Reliability was tested by direct comparison with the application, in eighteen non-diabetic subjects, of well-known kinetic analysis software package SAAM II, and by application on different data. Agreement between MATLAB and SAAM II was warranted by intraclass correlation coefficients ≥0.73; no significant differences between corresponding mean parameter estimates and prediction of HID rate; and consistent residual analysis. Moreover, MATLAB optimization procedure resulted in a significant 51% reduction of CV% for the worst-estimated parameter by SAAM II and in maintaining all model-parameter CV% <20%. In conclusion, our MATLAB-based procedure was suggested as a suitable tool for the individual assessment of HID process.}
}
@article{ZIEMKE2011583,
title = {An integrated development framework for rapid development of platform-independent and reusable satellite on-board software},
journal = {Acta Astronautica},
volume = {69},
number = {7},
pages = {583-594},
year = {2011},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2011.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0094576511001202},
author = {Claas Ziemke and Toshinori Kuwahara and Ivan Kossev},
keywords = {On-board software, Reusable, Platform-independent, System simulation, Open-source},
abstract = {Even in the field of small satellites, the on-board data handling subsystem has become complex and powerful. With the introduction of powerful CPUs and the availability of considerable amounts of memory on-board a small satellite it has become possible to utilize the flexibility and power of contemporary platform-independent real-time operating systems. Especially the non-commercial sector such like university institutes and community projects such as AMSAT or SSETI are characterized by the inherent lack of financial as well as manpower resources. The opportunity to utilize such real-time operating systems will contribute significantly to achieve a successful mission. Nevertheless the on-board software of a satellite is much more than just an operating system. It has to fulfill a multitude of functional requirements such as: Telecommand interpretation and execution, execution of control loops, generation of telemetry data and frames, failure detection isolation and recovery, the communication with peripherals and so on. Most of the aforementioned tasks are of generic nature and have to be conducted on any satellite with only minor modifications. A general set of functional requirements as well as a protocol for communication is defined in the SA ECSS-E-70-41A standard “Telemetry and telecommand packet utilization”. This standard not only defines the communication protocol of the satellite–ground link but also defines a set of so called services which have to be available on-board of every compliant satellite and which are of generic nature. In this paper, a platform-independent and reusable framework is described which is implementing not only the ECSS-E-70-41A standard but also functionalities for interprocess communication, scheduling and a multitude of tasks commonly performed on-board of a satellite. By making use of the capabilities of the high-level programming language C/C++, the powerful open source library BOOST, the real-time operating system RTEMS and finally by providing generic functionalities compliant to the ECSS-E-70-41A standard the proposed framework can provide a great boost in productivity. Together with open source tools such like the GNU tool-chain, Eclipse SDK, the simulation framework OpenSimKit, the emulator QEMU, the proposed on-board software framework forms an integrated development framework. It is possible to design, code and build the on-board software together with the operating system and then run it on a simulated satellite for performance analysis and debugging purposes. This makes it possible to rapidly develop and deploy a full-fledged satellite on-board software with minimal cost and in a limited time frame.}
}
@article{HODOROG201399,
title = {A regularization approach for estimating the type of a plane curve singularity},
journal = {Theoretical Computer Science},
volume = {479},
pages = {99-119},
year = {2013},
note = {Symbolic-Numerical Algorithms},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2012.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0304397512009474},
author = {Mădălina Hodorog and Josef Schicho},
keywords = {Plane curve singularity, Local topological type, Ill-posed problem, Regularization, Symbolic–numeric algorithm, Link of a singularity, Alexander polynomial, Delta-invariant, Genus},
abstract = {We address the algebraic problem of analyzing the local topology of each singularity of a plane complex algebraic curve defined by a squarefree polynomial with both exact (i.e. integers or rationals) and inexact data (i.e. numerical values). For the inexact data, we associate a positive real number that measures the noise in the coefficients. This problem is ill-posed in the sense that tiny changes in the input produce huge changes in the output. We design a regularization method for estimating the local topological type of each singularity of a plane complex algebraic curve. Our regularization method consists of the following: (i) a symbolic–numeric algorithm that computes the approximate local topological type of each singularity; (ii) and a parameter choice rule, i.e. a function in the noise level. We prove that the symbolic–numeric algorithm together with the parameter choice rule computes an approximate solution, which satisfies the convergence for noisy data property. We implement our algorithm in a new software package called GENOM3CK written in the Axel free algebraic geometric modeler and in the Mathemagix free computer algebra system. For our purpose, both of these systems provide modern graphical capabilities, and algebraic and geometric tools for exact and inexact input data.}
}
@article{ESPINO201554,
title = {Transient large strain contact modelling: A comparison of contact techniques for simultaneous fluid–structure interaction},
journal = {European Journal of Mechanics - B/Fluids},
volume = {51},
pages = {54-60},
year = {2015},
issn = {0997-7546},
doi = {https://doi.org/10.1016/j.euromechflu.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0997754615000163},
author = {Daniel M. Espino and Duncan E.T. Shepherd and David W.L. Hukins},
keywords = {Fluid–Structure Interaction, Hertzian contact, Large strain, Multi-physics modelling},
abstract = {Contact between two deformable structures, driven by applied fluid-pressure, is compared for an existing pseudo-transient contact method (the default in the Comsol Multi-physics v3.3 software package) and a new transient method. Application of the new method enables time-dependent and simultaneous Fluid–Structure Interaction (FSI) simulations to be solved. The new method is based on Hertzian contact. It enables truly transient simulations, unlike the default contact method. Both the default and new methods were implemented using a moving Arbitrary-Lagrange–Euler mesh, along with velocity constraints and Lagrange Multipliers to enable simultaneous FSI simulations. The comparison was based on a simple two-dimensional model developed to help understand the opening of a heart valve. The results from the new method were consistent with the steady-state solutions achieved using the default contact method. However, some minor differences in fluid dynamics, structural deformation and contact pressure predicted were obtained. The new contact method developed for FSI simulations enables transient analysis, in contrast to the default contact method that enables steady state solutions only.}
}
@article{QIN201770,
title = {Development of a GIS-based integrated framework for coastal seiches monitoring and forecasting: A North Jiangsu shoal case study},
journal = {Computers & Geosciences},
volume = {103},
pages = {70-79},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416308160},
author = {Rufu Qin and Liangzhao Lin},
keywords = {Coastal seiches, GIS, Visualization, Web-based platform, In situ observations, Model predictions},
abstract = {Coastal seiches have become an increasingly important issue in coastal science and present many challenges, particularly when attempting to provide warning services. This paper presents the methodologies, techniques and integrated services adopted for the design and implementation of a Seiches Monitoring and Forecasting Integration Framework (SMAF-IF). The SMAF-IF is an integrated system with different types of sensors and numerical models and incorporates the Geographic Information System (GIS) and web techniques, which focuses on coastal seiche events detection and early warning in the North Jiangsu shoal, China. The in situ sensors perform automatic and continuous monitoring of the marine environment status and the numerical models provide the meteorological and physical oceanographic parameter estimates. A model outputs processing software was developed in C# language using ArcGIS Engine functions, which provides the capabilities of automatically generating visualization maps and warning information. Leveraging the ArcGIS Flex API and ASP.NET web services, a web based GIS framework was designed to facilitate quasi real-time data access, interactive visualization and analysis, and provision of early warning services for end users. The integrated framework proposed in this study enables decision-makers and the publics to quickly response to emergency coastal seiche events and allows an easy adaptation to other regional and scientific domains related to real-time monitoring and forecasting.}
}
@article{VERWATERLUKSZO1994291,
title = {A novel model-based approach to optimization and control design of batch processes},
journal = {Journal of Process Control},
volume = {4},
number = {4},
pages = {291-295},
year = {1994},
issn = {0959-1524},
doi = {https://doi.org/10.1016/0959-1524(94)80048-0},
url = {https://www.sciencedirect.com/science/article/pii/0959152494800480},
author = {Zofia Verwater-Lukszo and Ger Otten},
keywords = {batch processes, experiment design, modelling},
abstract = {Traditionally, many industrial batch processes have been operated according to rigid recipies, in spite of the fact that production would yield more profit or a better product if they were efficiently adapted to changes in quality and cost of the used and/or produced products, process and scheduling conditions, and so on. In this paper a approach, called the flexible recipe approach, is given, which transforms the common rather static recipes into recipes that can be easily improved and used for systematic and efficient production adaptation at the start of a batch and during the processing. To be able to use this approach in an industrial environment a practical implementation is made in the software package FRIS. A fermentation process chosen as an example shows the methods and gives an indication of the expected profit.}
}
@article{FOSTER199670,
title = {The Nexus Approach to Integrating Multithreading and Communication},
journal = {Journal of Parallel and Distributed Computing},
volume = {37},
number = {1},
pages = {70-82},
year = {1996},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.0108},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596901082},
author = {Ian Foster and Carl Kesselman and Steven Tuecke},
abstract = {Lightweight threads have an important role to play in parallel systems: they can be used to exploit shared-memory parallelism, to mask communication and I/O latencies, to implement remote memory access, and to support task-parallel and irregular applications. In this paper, we address the question of how to integrate threads and communication in high-performance distributed-memory systems. We propose an approach based on global pointer and remote service request mechanisms, and explain how these mechanisms support dynamic communication structures, asynchronous messaging, dynamic thread creation and destruction, and a global memory model via interprocessor references. We also explain how these mechanisms can be implemented in various environments. Our global pointer and remote service request mechanisms have been incorporated in a runtime system called Nexus that is used as a compiler target for parallel languages and as a substrate for higher-level communication libraries. We report the results of performance studies conducted using a Nexus implementation; these results indicate that Nexus mechanisms can be implemented efficiently on commodity hardware and software systems.}
}
@article{DAVIDOVICIU1979101,
title = {Software Package for Process Control with Microprocessors},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {3},
pages = {101-104},
year = {1979},
note = {2nd IFAC/IFIP Symposium on Software for Computer Control, Prague, Czechoslovakia, 11-15 June},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65780-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701765780X},
author = {A. Davidoviciu and D. Mihalca and P. Rădulescu and L. Iacob and A. Petrovici and D. Strujan and R. Ilie and D. Voicu},
keywords = {Microprocessors, microcomputer software, real-time monitor, process control},
abstract = {Some problems regarding the microprocessors and their software are considered. Further, there are presented the principles for writting software for micros and several appropriate definitions. As the main part of the paper the general structure of a real-time monitor for the Romanian FELIX M-18 microcomputer, built up around the INTEL 8080 mi croprocessor is described. There are discused the scheduler, the system directives, the drivers, the interrupt handling routines, the man-machine communication and the SCAN and CONTROL routines for process control. Finally, there are pre sented some considerations regarding software package realization, implementation and performances.}
}
@article{NIAN2018195,
title = {A cohesive zone model incorporating a Coulomb friction law for fiber-reinforced composites},
journal = {Composites Science and Technology},
volume = {157},
pages = {195-201},
year = {2018},
issn = {0266-3538},
doi = {https://doi.org/10.1016/j.compscitech.2018.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S0266353817330191},
author = {Guodong Nian and Qiyang Li and Qiang Xu and Shaoxing Qu},
keywords = {Polymer-matrix composites (PMCs), Debonding, Friction/frictional sliding, Finite element analysis (FEA), Microbond test/ microdroplet test},
abstract = {A cohesive zone model (CZM) combining interfacial debonding, frictional sliding, and coupling between decohesion and friction is developed. The proposed interface model forms by incorporating a Coulomb friction law into the bilinear traction-separation law, and only one additional parameter is introduced compared to the traditional CZM. To verify this model, microbond test is carried out using an in-house developed tester. The interface model has been implemented into a commercial software package ABAQUS as a user-defined element. An axisymmetric finite element model with geometry and boundary conditions identical to the physical test has been used to simulate interfacial debonding and frictional sliding. The parameters for the interface model are determined by comparing the results of experiment and simulation. Once the parameters have been obtained for one test, the interface model can be used without further modification to predict the results of other experiments. The present interface model gives excellent quantitative predictions for the results of microbond test. Moreover, dimensional analysis has been adopted to study the relationship between the interfacial behavior and various parameters including the interfacial properties and the geometry of the structure. Dimensional considerations introduce a characteristic length, and the interfacial shear strength (IFSS) monotonically increases with the ratio of the characteristic length to the embedded length and is asymptotic to a horizontal line.}
}
@article{VANCASTEREN1983423,
title = {Validation of the In-Orbit Checkout of the IRAS Gyroscopes using Computer Simulations},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {11},
pages = {423-432},
year = {1983},
note = {9th IFAC/ESA Symposium on Automatic Control in Space 1982, Noordwijkerhout, The Netherlands, 5-9 July 1982},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)62226-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017622262},
author = {J.F.M. {van Casteren} and R.M. {van Doorn}},
keywords = {In-orbit checkout, computer simulations, gyroscopes, attitude control, Infra-Red Astronomical Satellite},
abstract = {The IRAS gyroscope package has to be calibrated shortly after launch in order to satisfy accuracy requirements. Validation of the designed in-flight calibration approach and the method of test evaluation as well as verification of the corresponding obtainable accuracy has to take place beforehand. Furthermore, to achieve a rapid test turnaround, (desk top) computer software was developed and fill-in formats were designed, both of which have to be checked on their adequacy. Computer simulation is one of the tools to accomplish these tasks. The implementation of a realistic mathematical gyroscope sensor model and a satellite model in the truth-domain is essential in this respect. The gyroscope sensor is shortly described, its mathematical model and its implementation in a modular software simulation package is elaborated on. The simulation results and subsequent test evaluation indicate that the gyro checkout procedure is adequate, and that the above mentioned tasks are performed successfully. The obtained gyro calibration accuracy corresponds to the results of an earlier performed accuracy analysis.}
}
@article{HASTINGS1997239,
title = {Engineering the development of optical fiber sensors for adverse environments},
journal = {Nuclear Engineering and Design},
volume = {167},
number = {3},
pages = {239-249},
year = {1997},
issn = {0029-5493},
doi = {https://doi.org/10.1016/S0029-5493(96)01307-6},
url = {https://www.sciencedirect.com/science/article/pii/S0029549396013076},
author = {Mardi C. Hastings and Bornain Chiu and David W. Nippa},
abstract = {Optical fiber sensors and measurement systems must be engineered to meet tough environmental requirements necessary for applications outside the laboratory. No generalized computer-aided tools exist to help advance the development, design and use of these systems in the field. Computer-aided design tools currently being developed are described. Structural finite element analyses are coupled with opto-elastic analyses of all-fiber interferometers and serial microbend sensors for the distributed measurement of various physical quantities. The combined analyses are parameterized and implemented on personal computers or workstations for use as design and development tools to determine the performance of different sensor configurations in various environments. Potentially, these computer-aided tools could be used for failure diagnosis and redesigning of existing optical fiber sensors. Performances predicted by the computer simulations are verified with experimental data and numerical analyses from the literature. The long-term goal is to develop user-friendly software packages for sensor manufacturers and end-users.}
}
@article{KULCZYCKI1985271,
title = {Computer-Aided Design of Industrial Electrical Distribution Networks},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {8},
pages = {271-276},
year = {1985},
note = {3rd IFAC/IFIP Symposium on Computer Aided Design in Control and Engineering Systems: Advanced Tools for Modern Technology, Lyngby, Denmark, 31 July-2 August 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60380-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701760380X},
author = {J. Kulczycki and M. Zabicki},
keywords = {Computer-aided design, modelling, heuristic programming, optimisation, power distribution},
abstract = {The paper presents general-purpose computer-aided design system for the determination of the optimal design of the industrial, electrical distribution system. A cost-effective structure for such a network io often a multilovol, hierarchical structure with one or more voltage levels and several kinds of equipment. It consists of main supply point, transformer substations and/or switchboards together with supplying lines (which all together form a “backbone network“) and cable feeders connecting demand points to backbone network (local networks). A distribution planning model is formulated using graph theory and discrete programming. Decisions can be taken on two levels - the first concerns backbone network, the second specifies assignments of demand points to particular elements of the backbone. The model is rather general - backbone can be any of loopless structure. The problem structure typically leads to heuristic solutions of an iterative nature. A kind of such method is implemented in a general-purpose software package letting on automatic design of industrial electrical distribution systems.}
}
@article{JAYARAM2006283,
title = {Introducing quantitative analysis methods into virtual environments for real-time and continuous ergonomic evaluations},
journal = {Computers in Industry},
volume = {57},
number = {3},
pages = {283-296},
year = {2006},
note = {Advanced Computer Support of Engineering and Service Processes of Virtual Enterprises},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2005.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361506000182},
author = {Uma Jayaram and Sankar Jayaram and Imtiyaz Shaikh and YoungJun Kim and Craig Palmer},
keywords = {Virtual environment, Ergonomics, Upper limb assessment, Virtual assembly, Software integration},
abstract = {This paper presents our work on methods to link virtual environments (VE) and quantitative ergonomic analysis tools in real time for occupational ergonomic studies. We pursued two distinct approaches: (a) create methods to integrate the VE with commercially available ergonomic analysis tools for a synergistic use of functionalities and capabilities; (b) create a built-in ergonomic analysis module in the VE. The first approach provides the use of established, off-the shelf tools integrated with the VE to create a hybrid application. This integration is performed through the use of APIs provided by the software vendor and existing Internet and communications technologies. The commercial ergonomics tool and the VE run concurrently and integrate their capabilities. The second approach provides the capability to do ergonomic evaluations in a self-contained VE application. In this method, the required ergonomics calculations are built into the VE. Each approach has its own distinct advantages. The use of a commercially available ergonomics tool integrated with a VE provides significant more capability and should be used where detailed and complex ergonomics evaluations are required. However, the process of integration in this approach is more difficult and time consuming. The self-contained VE application is more suited for simple ergonomic evaluations or in cases where the ergonomics algorithms are readily accessible and easily implemented. The two integration strategies are methodically explained and demonstrated using case studies conducted with industry partners. This integrated capability facilitates integration of ergonomic issues early in the design and planning phases of workplace layouts. It provides functionality beyond the capabilities of current commercial off-the-shelf (COTS) solutions. In addition, it contributes to a new trend in the integration of different technology fields for synergistic use in industry.}
}
@article{HELD200195,
title = {VRONI: An engineering approach to the reliable and efficient computation of Voronoi diagrams of points and line segments},
journal = {Computational Geometry},
volume = {18},
number = {2},
pages = {95-123},
year = {2001},
issn = {0925-7721},
doi = {https://doi.org/10.1016/S0925-7721(01)00003-7},
url = {https://www.sciencedirect.com/science/article/pii/S0925772101000037},
author = {Martin Held},
keywords = {Voronoi diagram, Topology-oriented algorithm, Reliability, Robustness, Experimental analysis},
abstract = {We discuss the design and implementation of a topology-oriented algorithm for the computation of Voronoi diagrams of points and line segments in the two-dimensional Euclidean space. The main focus of our work was on designing and engineering an algorithm that is completely reliable and fast in practice. The algorithm was implemented in ANSI C, using standard floating-point arithmetic. In addition to Sugihara and Iri's topology-oriented approach, it is based on a very careful implementation of the numerical computations required, an automatic relaxation of epsilon thresholds, and a multi-level recovery process combined with “desperate mode”. The resulting code, named vroni , was tested extensively on real-world data and turned out to be reliable. CPU-time statistics document that it is always faster than other popular Voronoi codes. In our computing environment, vroni needs about 0.01nlog2n milliseconds to compute the Voronoi diagram of n line segments, and this formula holds for a wide variety of synthetic and real-world data. In particular, its CPU-time consumption is hardly affected by the actual distribution of the input data. Vroni also features a function for computing offset curves, and it has been successfully tested within and integrated into several industrial software packages.}
}
@article{AHLANDER20021007,
title = {Einstein summation for multidimensional arrays},
journal = {Computers & Mathematics with Applications},
volume = {44},
number = {8},
pages = {1007-1017},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)00210-9},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102002109},
author = {K. Åhlander},
keywords = {Index notation, Mathematical software, Domain specific language, Tensor calculus},
abstract = {One of the most common data structures, at least in scientific computing, is the multidimensional array. Some numerical algorithms may conveniently be expressed as a generalized matrix multiplication, which computes a multidimensional array from two other multidimensional arrays. By adopting index notation with the Einstein summation convention, an elegant tool for expressing generalized matrix multiplications is obtained. Index notation is the succinct and compact notation primarily used in tensor calculus. In this paper, we develop computer support for index notation as a domain specific language. Grammar and semantics are proposed, yielding an unambiguous interpretation algorithm. An object-oriented implementation of a C++ library that supports index notation is described. A key advantage with computer support of index notation is that the notational gap between a mathematical index notation algorithm and its implementation in a computer language is avoided. This facilitates program construction as well as program understanding. Program examples that demonstrate the close resemblance between code and the original mathematical formulation are presented.}
}
@article{GERMAN199569,
title = {TimeNET: a toolkit for evaluating non-Markovian stochastic Petri nets},
journal = {Performance Evaluation},
volume = {24},
number = {1},
pages = {69-87},
year = {1995},
note = {Performance Modeling Tools},
issn = {0166-5316},
doi = {https://doi.org/10.1016/0166-5316(95)00010-U},
url = {https://www.sciencedirect.com/science/article/pii/016653169500010U},
author = {Reinhard German and Christian Kelling and Armin Zimmermann and Günter Hommel},
keywords = {Performance and dependability modeling tool, Analysis and simulation of stochastic Petri nets, Graphical user interface},
abstract = {This paper describes TimeNET (Timed Net Evaluation Tool), a software package for the modeling and evaluation of stochastic Petri nets with non-exponentially distributed firing times. TimeNET has been developed at the Technische Universität Berlin in several research projects. A graphical user interface is provided for the model specification and specialized analysis and simulation components are used for the automated model evaluation. The implementation of the analysis and simulation components is based on recent research results. Both the general structure and the underlying algorithms of TimeNET are described. An example illustrates the modeling and evaluation process using TimeNET.}
}
@article{BARRY1998217,
title = {Parallel adaptive mesh refinement techniques for plasticity problems},
journal = {Advances in Engineering Software},
volume = {29},
number = {3},
pages = {217-225},
year = {1998},
issn = {0965-9978},
doi = {https://doi.org/10.1016/S0965-9978(98)00040-4},
url = {https://www.sciencedirect.com/science/article/pii/S0965997898000404},
author = {William J. Barry and Mark T. Jones and Paul E. Plassmann},
keywords = {Adaptive h-refinement, Small-strain plasticity, SUMAA3d},
abstract = {Accurately modeling the nonlinear properties of materials can be computationally expensive. Parallel computing offers an attractive way for solving such problems. However, the efficient use of these systems requires the vertical integration of a number of very different software components. To investigate the practicality of solving large-scale, nonlinear problems on parallel computers, we explore the solution of two- and three-dimensional, small-strain plasticity problems. We consider a finite-element formulation of the problem with adaptive refinement of an unstructured mesh to accurately model plastic transition zones. We present a framework for the parallel implementation of such complex algorithms. This framework, using libraries from the SUMAA3d project, allows a user to build a parallel finite-element application without writing any parallel code. To demonstrate the effectiveness of this approach on widely varying parallel architectures, we present experimental results from an IBM SP parallel computer and an ATM-connected network of Sun UltraSparc workstations. The results detail the parallel performance of the computational phases of the application during the process while the material is incrementally loaded.}
}
@article{JIN20171,
title = {Gap metering for active traffic control at freeway merging sections},
journal = {Journal of Intelligent Transportation Systems},
volume = {21},
number = {1},
pages = {1-11},
year = {2017},
issn = {1547-2450},
doi = {https://doi.org/10.1080/15472450.2016.1157021},
url = {https://www.sciencedirect.com/science/article/pii/S1547245022003243},
author = {Peter J. Jin and Jie Fang and Xiaowen Jiang and Michael DeGaspari and C. Michael Walton},
keywords = {active traffic management, gap metering, merging control, ramp control, ramp metering},
abstract = {ABSTRACT
Freeway merging sections are critical segments that can recurrently activate peak-hour traffic congestion. This article proposes a novel vehicular gap control method as a new Active Traffic Management (ATM) strategy to be added to the existing Intelligent Transportation System (ITS) toolboxes for freeway merge control. The proposed strategy, “Gap Metering,” can be considered a non-stopping mainline version of ramp metering. It utilizes signals advising mainline through vehicles to yield sufficient gaps for merging vehicles. Detailed system design and control methods are proposed and implemented in VISSIM (please spell out the abbreviation of VISSIM for this first instance), a microsimulation software package. Different driver behavior sets with different standstill headway values are created to allow switching between gap-metered vehicles and regular vehicles. We evaluate the proposed system through two VISSIM models built and calibrated, respectively, for both the I-894 corridor in Milwaukee, WI, and the Riverside Drive segment on I-35 northbound in Austin, TX. Both corridors experience severe morning peak-hour congestion. We use the I-894 corridor for testing the system design parameters and use the I-35 corridor to conduct a comparison with the ramp metering strategies. The I-894 results indicate an average of 10–20% network delay reduction among all scenarios. We then tested the scenario on the I-35 corridor and compared with the ALINEAR ramp metering. Gap metering strategies alone or combined with ramp metering can, respectively, reduce 17% and 27% more total delay than ramp metering only control at 20% compliance rate.}
}
@article{PATTISON2000339,
title = {ACE: a bond graph modelling and development tool for control system design and implementation},
journal = {Mechatronics},
volume = {10},
number = {3},
pages = {339-351},
year = {2000},
issn = {0957-4158},
doi = {https://doi.org/10.1016/S0957-4158(99)00048-3},
url = {https://www.sciencedirect.com/science/article/pii/S0957415899000483},
author = {L Pattison and J.R Hewit},
abstract = {A software tool based on bond graphs is presented together with a hardware-in-the-loop experimental rig. These are designed to be used as aids in the planning, simulation, implementation and testing of control systems containing multiple actuators with sensory feedback. The windows-based, object-orientated (OO) development tool ACE (Adaptive Control Environment) enables the user to build up a graphical representation of the system by selecting objects from a library of system elements and/or mini systems. Each of these objects, although viewed as a graphical drawing of the selected element on this level, contains data representing an acausal bond graph. As the model is built up containing several of these objects, ACE has a view of the overall interconnecting acausal bond graph of the system. For hardware-in-the-loop simulation, ACE divides the core system model into three parts, representing:•the hardware-in-the-loop objects — that is the hardware under test (e.g. an electrical motor and sensors);•the controller — whose function it is to control the afore mentioned hardware;•an applied loading. It is from the last two of these sub-systems that control code may be generated for operation and testing on a specially developed DAP (Data Acquisition Processor) controlled test-rig.}
}
@article{KERBOEUF2005241,
title = {Encapsulation and behavioral inheritance in a synchronous model of computation for embedded system services adaptation},
journal = {The Journal of Logic and Algebraic Programming},
volume = {63},
number = {2},
pages = {241-269},
year = {2005},
note = {Special Issue on Process Algebra and System Architecture},
issn = {1567-8326},
doi = {https://doi.org/10.1016/j.jlap.2004.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1567832604000384},
author = {Mickaël Kerbœuf and Jean-Pierre Talpin},
keywords = {Synchronous programming, Process calculi, Encapsulation, Reuse and behavioral inheritance.},
abstract = {Because it encourages the incremental development of software and the reuse of code by abstracting away implementation details, object orientation is an intuitive and sensible way to conceive large software out of existing application components and libraries. In practice, however, object-orientation is most of the time applied and used with sequentiality in mind. This practice may sometimes be conceptually inadequate for, eg., control-dominated reactive system components. We address this issue by proposing a process calculus that melts the paradigm of synchronous programming to key object-oriented features: encapsulation and behavioral inheritance with overriding by means of specific algebraic concurrency combinators. This framework provides support for the reuse of components and, more specifically, for the adaptation of embedded systems with new services. Cast in the context of a strict interpretation of the synchronous hypothesis, the proposed model supports a static interpretation of inheritance: overriding is resolved at compile-time (or link-time) and inheritance combinators are translated into primitive synchronous ones. This compilation technique puts object-orientation to work in a syntax-oriented model of synchronous concurrency that naturally supports the incremental, refinement-based design of concurrent systems starting from encapsulated and reused application components. The benefits of our approach are illustrated by a concrete and practical example: the adaptation of services to a plain old telephone service specification.}
}
@article{NIEWIADOMSKASZYNKIEWICZ1992217,
title = {FC-VS: a decision support system for flood control in multireservoir systems},
journal = {Environmental Software},
volume = {7},
number = {4},
pages = {217-228},
year = {1992},
note = {3rd International Software Exhibition for Environmental Science and Engineering},
issn = {0266-9838},
doi = {https://doi.org/10.1016/0266-9838(92)90005-O},
url = {https://www.sciencedirect.com/science/article/pii/026698389290005O},
author = {Ewa Niewiadomska-Szynkiewicz and Andrzej Karbowski and Krzysztof Malinowski},
keywords = {hierarchical (two-level) control structure, flood-control, coordinating parameters, water reservoir management, optimal control, real-time decision support system},
abstract = {The paper presents a software package FC-VS (Flood Control - Vistula System) designed as a real-time Decision Support System for the control centre of multireservoir water system in Southern Poland. This is an implementation of a two-level control structure for the optimal flood operation of river - reservoir systems. The natural hierarchy of the decision units, i.e., the control centre and operators of the reservoirs is preserved. The FC-VS system helps the central dispatcher by proposing him the values of special coordinating parameters for modifying release decisions of local reservoirs operators. The aim of the structure is to minimize damages created by a flood wave passing through a river-basin. FC-VS can be also used for training purposes using historical data. Some simulation results in the final part of the article.}
}
@article{GABR2015181,
title = {Consolidity analysis for fully fuzzy functions, matrices, probability and statistics},
journal = {Ain Shams Engineering Journal},
volume = {6},
number = {1},
pages = {181-197},
year = {2015},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2014.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2090447914001294},
author = {Walaa Ibrahim Gabr},
keywords = {Intelligent systems, System consolidity theory, Fuzzy theory and systems, Fuzzy mathematical functions, Fuzzy optimization problems, Fuzzy probability and statistics},
abstract = {The paper presents a comprehensive review of the know-how for developing the systems consolidity theory for modeling, analysis, optimization and design in fully fuzzy environment. The solving of systems consolidity theory included its development for handling new functions of different dimensionalities, fuzzy analytic geometry, fuzzy vector analysis, functions of fuzzy complex variables, ordinary differentiation of fuzzy functions and partial fraction of fuzzy polynomials. On the other hand, the handling of fuzzy matrices covered determinants of fuzzy matrices, the eigenvalues of fuzzy matrices, and solving least-squares fuzzy linear equations. The approach demonstrated to be also applicable in a systematic way in handling new fuzzy probabilistic and statistical problems. This included extending the conventional probabilistic and statistical analysis for handling fuzzy random data. Application also covered the consolidity of fuzzy optimization problems. Various numerical examples solved have demonstrated that the new consolidity concept is highly effective in solving in a compact form the propagation of fuzziness in linear, nonlinear, multivariable and dynamic problems with different types of complexities. Finally, it is demonstrated that the implementation of the suggested fuzzy mathematics can be easily embedded within normal mathematics through building special fuzzy functions library inside the computational Matlab Toolbox or using other similar software languages.}
}
@article{TUPOV2017199,
title = {The Choice of Turbulence Models for Steam Jets},
journal = {Procedia Engineering},
volume = {176},
pages = {199-206},
year = {2017},
note = {Proceedings of the 3rd International Conference on Dynamics and Vibroacoustics of Machines (DVM2016) June 29–July 01, 2016 Samara, Russia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.02.289},
url = {https://www.sciencedirect.com/science/article/pii/S187770581730797X},
author = {V.B. Tupov and A.A. Taratorin},
keywords = {noise, mathematical simulation, steam jet, turbulence models, LES method},
abstract = {The noise from the steam dumping is a serious problem for many industries. For example, the popping of pressure relief valves of power boilers causes a temporal sound level overshoot over the admissible one up to 30-40 dBA in a radius of several kilometers. Mathematical modeling is an important tool for studying the outflow steam jets. Knowing processes of steam efflux into the atmosphere allows us correctly to calculate noise levels at a distance from them on the one hand and on the other hand to realize measures for sound attenuation. The study of mechanisms of noise generation by jets had begun in the end of 1940s and had been generally related to investigations of air jets and exhaust gas jets of aircrafts. The choice of turbulence models for steam jets has been studied insufficiently until recently. The significant subject of the steam jet simulation is the choice of the turbulence model. There are about a dozen mathematical models of turbulence, which are implemented in different software packages. In the case of simulation of turbulent flows with transient large scale vortex structures, it is ineffective and impossible to solve Reynolds averaged Navier-Stokes equations closed using semi empirical turbulence model or by means of the direct numerical simulation (DNS). Simulation of three dimensional and nonstationary vortices is carried out using a Large Eddy Simulation (LES) method in which the idea of the formal mathematical differentiation of coarse and fine structures by means of either operation, for example the filtration operation, is recommended. It is noted that, based on the LES method, it is possible to compute the coherent vortex structures, and the applicability of the model for prediction of noise generated by steam jets. LES model describes noise generation processes from steam jet more correct.}
}
@article{BISWAL2017165,
title = {Finite element model updating of concrete structures based on imprecise probability},
journal = {Mechanical Systems and Signal Processing},
volume = {94},
pages = {165-179},
year = {2017},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2017.02.042},
url = {https://www.sciencedirect.com/science/article/pii/S0888327017301140},
author = {S. Biswal and A. Ramaswamy},
keywords = {Imprecise probability, Finite element model updating, Markov chain Monte Carlo, Probability boxes, Bounded measurements},
abstract = {Imprecise probability based methods are developed in this study for the parameter estimation, in finite element model updating for concrete structures, when the measurements are imprecisely defined. Bayesian analysis using Metropolis Hastings algorithm for parameter estimation is generalized to incorporate the imprecision present in the prior distribution, in the likelihood function, and in the measured responses. Three different cases are considered (i) imprecision is present in the prior distribution and in the measurements only, (ii) imprecision is present in the parameters of the finite element model and in the measurement only, and (iii) imprecision is present in the prior distribution, in the parameters of the finite element model, and in the measurements. Procedures are also developed for integrating the imprecision in the parameters of the finite element model, in the finite element software Abaqus. The proposed methods are then verified against reinforced concrete beams and prestressed concrete beams tested in our laboratory as part of this study.}
}
@article{BRUCE1996882,
title = {Application of a multivariable simplex data analysis system for welding engineers},
journal = {Journal of Materials Processing Technology},
volume = {56},
number = {1},
pages = {882-890},
year = {1996},
note = {International Conference on Advances in Material and Processing Technologies},
issn = {0924-0136},
doi = {https://doi.org/10.1016/0924-0136(95)01900-6},
url = {https://www.sciencedirect.com/science/article/pii/0924013695019006},
author = {N. Bruce and S.G.R. Brown and J.D. Parker},
abstract = {This paper describes the development and application of a user-friendly PC based software system (SNAP - Simplex based Numerical Analysis Package). The software is statistically based, employing the ‘Downhill Simplex Method’, and can implement both the Ordinary Least Squares (OLS) method as well as more recently developed Robust techniques. The use of Robust statistics means fewer assumptions concerning the distributional nature of scatter in data need be made and departures from the standard Gaussian distribution can be accommodated. The package also incorporates visual aids which allow the user to examine the structure of experimental data and thus fit the most appropriate mathematical expressions. Various plotting facilities and confidence interval routines are also provided. The benefit of using the SNAP system are described with reference to a re-evaluation of published experimental data. Specifically, the relationships between welding residual stress ( longitudinal and transverse ), weld groove area and heat input variables are examined and quantified for arc welds produced in a hot rolled structural steel. The advantages gained through use of the SNAP system are demonstrated by the development of a greatly improved quantitative description of the relationship between residual stress and other measured variables as compared to the original study.}
}
@article{ALTING1986317,
title = {Integration of Engineering Functions/Disciplines in CIM},
journal = {CIRP Annals},
volume = {35},
number = {1},
pages = {317-320},
year = {1986},
issn = {0007-8506},
doi = {https://doi.org/10.1016/S0007-8506(07)61897-6},
url = {https://www.sciencedirect.com/science/article/pii/S0007850607618976},
author = {Leo Alting and T. Wanheim},
abstract = {In the last few years many of the hardware and software elements necessary to establish CIM-systems (Computer Integrated Manufacturinq)have been developed but still many problems have to be solved especially concerning integration of engineering functions/disciplines into CAD/CAM and CIM in a broader context. The present paper describes the development of a number of computer aided engineering application modules (solutions) which can provide both integration and support of important mechanical design and manufacturing activities. Following subjects will be discussed: integration of engineering functions into CAD/CAM including software solutions, classification and coding, logics for selecting materials, processes etc. integrated in design, generative process planning, automated part family design and manufacture and CIM-miniature laboratory. The philosophy in developing these engineering integration and support modules has been to focus on the logics and decision making concepts so that the user easily can implement his own logics/decision structure and data without spending tine on tedious programming. Further integration of these modules to other software packages including CAD/CAM systems plays a very important role. Only by integration and utilization of common (eventually local) databases the full advantage of the new technology can be expected.}
}
@article{SALIM20122174,
title = {A user configurable data acquisition and signal processing system for high-rate, high channel count applications},
journal = {Fusion Engineering and Design},
volume = {87},
number = {12},
pages = {2174-2177},
year = {2012},
note = {Proceedings of the 8th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2012.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S0920379612002505},
author = {Arwa Salim and Louise Crockett and John McLean and Peter Milne},
keywords = {Data acquisition, DSP, FPGAs, Reconfigurable},
abstract = {Real-time signal processing in plasma fusion experiments is required for control and for data reduction as plasma pulse times grow longer. The development time and cost for these high-rate, multichannel signal processing systems can be significant. This paper proposes a new digital signal processing (DSP) platform for the data acquisition system that will allow users to easily customize real-time signal processing systems to meet their individual requirements. The D-TACQ reconfigurable user in-line DSP (DRUID) system carries out the signal processing tasks in hardware co-processors (CPs) implemented in an FPGA, with an embedded microprocessor (μP) for control. In the fully developed platform, users will be able to choose co-processors from a library and configure programmable parameters through the μP to meet their requirements. The DRUID system is implemented on a Spartan 6 FPGA, on the new rear transition module (RTM-T), a field upgrade to existing D-TACQ digitizers. As proof of concept, a multiply-accumulate (MAC) co-processor has been developed, which can be configured as a digital chopper-integrator for long pulse magnetic fusion devices. The DRUID platform allows users to set options for the integrator, such as the number of masking samples. Results from the digital integrator are presented for a data acquisition system with 96 channels simultaneously acquiring data at 500kSamples/s per channel.}
}
@article{AMAIOUA201813,
title = {Efficient solution of quadratically constrained quadratic subproblems within the mesh adaptive direct search algorithm},
journal = {European Journal of Operational Research},
volume = {268},
number = {1},
pages = {13-24},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.10.058},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717309876},
author = {Nadir Amaioua and Charles Audet and Andrew R. Conn and Sébastien {Le Digabel}},
keywords = {Nonlinear programming, Derivative-free optimization, Quadratic programming, Trust-region subproblem, Mesh adaptive direct search},
abstract = {The mesh adaptive direct search algorithm (MADS) is an iterative method for constrained blackbox optimization problems. One of the optional MADS features is a versatile search step in which quadratic models are built leading to a series of quadratically constrained quadratic subproblems. This work explores different algorithms that exploit the structure of the quadratic models: the first one applies an l1-exact penalty function, the second uses an augmented Lagrangian and the third one combines the former two, resulting in a new algorithm. It is notable that this latter approach is uniquely suitable for quadratically constrained quadratic problems. These methods are implemented within the NOMAD software package and their impact are assessed through computational experiments on 65 analytical test problems and 4 simulation-based engineering applications.}
}
@article{SALZWEDEL1983673,
title = {Computer Aided Robustness Analysis of Dynamic Systems},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {19},
pages = {673-679},
year = {1983},
note = {2nd IFAC Symposium on computer Aided Design of Multivariable Technological Systems, West Lafayette, USA, 15-17 September 1982},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)61752-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701761752X},
author = {H. Salzwedel},
keywords = {Robustness analysis, computer-aided system design, software for control system design, interactive graphics},
abstract = {The robustness analysis of multivariable dynamic systems is discussed. A computer package for robustness analysis of mulivariable dynamic systems, called ROBUST, is described. The programs have been written in FORTRAN. They compute different robustness measures for controlled dynamic systems, and display them graphically. These measures have been proven to be very useful to the control system designer in analyzing the robustness properties of multivariable systems. The programs have been implemented on a VAX 11-780 computer and are used together with an interactive graphics package called VGRAF.}
}
@article{DEMURJIAN199378,
title = {Programming versus databases in the object-oriented paradigm},
journal = {Information and Software Technology},
volume = {35},
number = {2},
pages = {78-88},
year = {1993},
issn = {0950-5849},
doi = {https://doi.org/10.1016/S0950-5849(05)80002-3},
url = {https://www.sciencedirect.com/science/article/pii/S0950584905800023},
author = {SA Demurjian and GM Beshers and TC Ting},
keywords = {object-oriented paradigm, programming languages, database systems},
abstract = {The object-oriented paradigm has come to the forefront of the research community in the software engineering, programming language, and database research areas. Moreover, the paradigm appears capable of supporting advanced applications such as software development environments (SDEs) that require both programming ability and persistency via a database system. However, there exists a disparity between the programming and database approaches to the object-oriented paradigm. The paper examines and discusses this disparity between the two approaches for the purpose of formulating an understanding of their commonalities and differences. This understanding has been instrumental in supporting work involving the prototyping of SDEs using the object-oriented paradigm, an examination of the techniques required to evolve a class library for persistency, and the proposal of a software architecture and functionality of a persistent programming language system. Thus, it is believed that the work presented in this paper can serve as a framework for researchers and practitioners whose efforts include the aforementioned or other, related areas. From a content perspective, this paper provides a comparative analysis between the concepts of programming and databases for the object-oriented paradigm, through a detailed presentation of system-level and model-level considerations. Both philosophical concepts and implementation pragmatics are investigated. A practical examination of the C++ programming language versus the Opal data language has been conducted, revealing many valuable insights of systems and application details and issues. Features of both approaches are also analysed and illustrated.}
}
@article{LIN2017761,
title = {Stiffness matrix for the analysis and design of partial-interaction composite beams},
journal = {Construction and Building Materials},
volume = {156},
pages = {761-772},
year = {2017},
issn = {0950-0618},
doi = {https://doi.org/10.1016/j.conbuildmat.2017.08.154},
url = {https://www.sciencedirect.com/science/article/pii/S0950061817317622},
author = {Jian-Ping Lin and Guannan Wang and Guangjian Bao and Rongqiao Xu},
keywords = {Composite beam, Partial interaction, Finite element analysis, Explicitly-expressed stiffness matrix, Interfacial slip, Shear deformation, Particle swarm optimization},
abstract = {Compared to the classical Rayleigh-Ritz method and other analytical solutions, finite element (FE) method is more efficient and capable in calculating the deformations and stress states of partial-interaction composite beams, as well as manipulating the material and geometrical parameters for better engineering designs. Stiffness matrix of composite beams considering the interlayer slips is derived based on the kinematic assumptions of the Timoshenko’s beam theory by taking into account of the transverse shear deformations. A detailed derivation is elaborated to obtain the local stiffness matrix for a composite beam element, while the higher-order interpolation functions are adopted for the displacement fields (deflection, rotation, and interlayer slip). Then a finite element program is developed by assembling the local stiffness matrices and applying corresponding equivalent nodal stresses. Several numerical results are presented and compared against the analytical solutions available in the literature to demonstrate the accuracy of the proposed FE stiffness matrix. Finally, a design procedure by connecting particle swarm optimization technique with the present FE analysis is created to reduce the deformations of simply supported composite beams while the quantity of shear connectors remains the same, to prove the superior simulation capacity and efficiency of the derived FE stiffness matrix with other techniques. Compared to the analytical methods, the proposed finite element is more convenient and applicable in the analysis of partial-interaction composite beams under more complicated loading and boundary conditions. In the meantime, the explicitly expressed local stiffness matrix can be easily implemented into other commercial software packages asa subroutine for both professional and personal engineering designs and calculations.}
}
@article{LEMER1982255,
title = {OVIDE: A Software Package for Verifying and Validating Petri Nets},
journal = {IFAC Proceedings Volumes},
volume = {15},
number = {7},
pages = {255-260},
year = {1982},
note = {3rd IFAC/IFIP Symposium on Software for Computer Control 1982, Madrid, Spain, 5-8 October},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)62830-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017628301},
author = {E. {Le Mer}},
keywords = {Computer-aided system design, System analysis, Packaging, Modelling, Petri-nets},
abstract = {Managing with the increasing complexity of large scale systems, especIally real-time ones, become more and more difficult without the aid of a formal and powerful design methodology; the latter must be supported by automatic tools dealing with the resulting models in order to verify and validate these models. We introduce such a package based upon Petri-nets theory by which we can ensure the correctness of the specifications before the implementation phasis, mainly, concerning parallelism, synchronization, concurrency; OVIDE is an interactive graphic tool allowing different kinds of analysis: set of reachable markings, reductions, invariants,...}
}
@article{BELOKON2002481,
title = {New schemes for the finite-element dynamic analysis of piezoelectric devices},
journal = {Journal of Applied Mathematics and Mechanics},
volume = {66},
number = {3},
pages = {481-490},
year = {2002},
issn = {0021-8928},
doi = {https://doi.org/10.1016/S0021-8928(02)00058-8},
url = {https://www.sciencedirect.com/science/article/pii/S0021892802000588},
author = {A.V. Belokon' and A.V. Nasedkin and A.N. Solov'yev},
abstract = {New finite-element schemes are proposed for investigating harmonic and non-stationary problems for composite elastic and piezoelectric media. These schemes develop the techniques for the finite-element analysis of piezoelectric structures based on symmetric and partitioned matrix algorithms. In order to take account of attenuation in piezoelectric media, a new model is used which extends the Kelvin model for viscoelastic media. It is shown that this model enables the system of finite-element equations to be split into separate scalar equations. The Newmark scheme in a convenient formulation, which does not explicitly use the velocities and accelerations of the nodal degrees of freedom, is employed for the direct integration with respect to time of the finite-element equations of non-stationary problems. The results of numerical experiments are presented which illustrate the effectiveness of the proposed techniques and their implementation in the ACELAN finite-element software package.}
}
@article{HAWLEY1994237,
title = {Implementation of a PC-based integrated control system for children},
journal = {Medical Engineering & Physics},
volume = {16},
number = {3},
pages = {237-242},
year = {1994},
issn = {1350-4533},
doi = {https://doi.org/10.1016/1350-4533(94)90043-4},
url = {https://www.sciencedirect.com/science/article/pii/1350453394900434},
author = {M.S. Hawley and P.A. Cudd and A.D. Cherry},
keywords = {Physical disability, children, integrated control systems, computers},
abstract = {Children with severe physical disabilities often require a range of technological devices to and them with the fundamental tasks carried out in everyday life. These task areas may include mobility, communication, environment control and computer access for education. The most efficient, and often the most cost-effective, way of providing for these requirements is by the provision of a single, integrated system tailored to the individual's needs and abilities. BASIS, the Barnsley And Sheffield Integrated System, is an integrated control system based on a wheelchair-mounted personal computer (PC) and currently implements the following modules: wheelchair control; a spoken communication package; control of home equipment via infra-red; and a facility to run third-party software. The system presents a consistent and uniform environment to the user for selection and activation of these modules and can be controlled by one to four switches (scanning or directed access), a touch screen, a head pointer or a mouse. Safety considerations have played a central part in the design of the system. BASIS can be tailored to the user's requirements by editing a text file in software and by connecting the appropriate hardware modules to the serial and parallel ports. The graphical user interface has been designed for use by children and employs pictorial representation (icons) of available functions on a colour screen. These icons can be chosen from clip-art or drawn on computer drawing package. It is also possible to scan in and utilize drawings or photographs. The computer-based integrated system offers significant advantages over systems previously reported, which were based on logic-gate circuitry with feedback to the user via a LED array.}
}
@article{YAO2017148,
title = {A WebGIS-based decision support system for locust prevention and control in China},
journal = {Computers and Electronics in Agriculture},
volume = {140},
pages = {148-158},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916311814},
author = {Xiaochuang Yao and Dehai Zhu and Wenju Yun and Fan Peng and Lin Li},
keywords = {WebGIS, DSS, Geo-processing service, Real-time data synchronization, Locust population estimation},
abstract = {Locust swarms are destructive agricultural and biological disasters in China. The green prevention and control (GPC, such as ecological regulation and physical control) of locusts is a comprehensive and complex process, especially in information technology. In this study, a web-based decision support system (DSS) integrated with geographic information system (GIS) is developed to prevent and control locusts efficiently, accurately, and rapidly. The locust prevention and control DSS (LPCDSS) is developed to assist farmers and local government agencies in Chinese provinces with high incidence of locust by providing spatial decision-making information. LPCDSS offers online access to county, city, provincial, and national level data queries and is capable of storing, spatial analyzing, and displaying geographically referenced information of locust data. The system can also provide the real-time tracking of global positioning system (GPS) location, as well as goods scheduling of locust plagues prevention. Six types of web service, real-time data synchronization model, and locust population estimation model are developed and implemented to improve the decision-making usability and feasibility of LPCDSS by adopting a three-layer system architecture. The system is developed by using several programming languages, libraries, and software components. As a result, this system has been running successfully for several years and has improved efficiency of the locust prevention and control in China with high efficiency and great accuracy. The approaches and methodologies presented in this paper can serve as a reference for those who are interested in developing integrated pest control system applications.}
}
@article{WU1995427,
title = {A Distributed Real-time Image Processing System},
journal = {Real-Time Imaging},
volume = {1},
number = {6},
pages = {427-435},
year = {1995},
issn = {1077-2014},
doi = {https://doi.org/10.1006/rtim.1995.1044},
url = {https://www.sciencedirect.com/science/article/pii/S1077201485710443},
author = {D.M. Wu and L. Guan},
abstract = {This paper presents the design and implementation of a distributed, real-time image processing system. In this system, an IBM-PC personal computer is used as the front end to a remote host computer via the Internet. Using standard TCP/IP networking protocols, the software can be configured to accommodate high performance remote devices such as transputer networks, sun SPARC servers and super-computers. The access to the powerful remote computer enables the system to complete complex image processing tasks in real-time. During processing, the image is transferred to the remote machine for processing and then transferred back to the PC for display. The system can serve as a prototype for a full-feature image processing and analysis package, as well as a programming platform for the research and development of new image processing algorithms.}
}
@article{BECHE2006421,
title = {Conception of optical integrated circuits on polymers},
journal = {Microelectronics Journal},
volume = {37},
number = {5},
pages = {421-427},
year = {2006},
issn = {0026-2692},
doi = {https://doi.org/10.1016/j.mejo.2005.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0026269205002788},
author = {B. Bêche and N. Pelletier and E. Gaviot and R. Hierle and A. Goullet and J.P. Landesman and J. Zyss},
keywords = {Microtechnologies, Microphotolithography, Spin on glass (SOG), SU-8, Integrated optical circuits},
abstract = {The authors present a successful design, realisation and characterisation of single-mode TE00–TM00 rib optical waveguides composed of SU-8 polymer. For the simulation, a generic software package that provides an interactive and graphical environment for analysis by polarised Semi-Vectorial Finite Difference (SVFD) method of all kinds of integrated optical waveguides, such as buried channel, raised strip, rib, embedded, or ridge waveguides, has been implemented and tested. In this method we have taken into account the terms due to the interface between each layer. After realisation of various single mode optical waveguides on SU-8 polymer and Spin on Glass (SOG) like straight, S-bends, Y-junctions, Mach-Zehnder (MZ) interferometers, the linear absorption coefficient of energy αTE–TM of such rib waveguides have been measured and estimated, respectively, near 0.32 and 0.46cm−1 for both optical modes TE00 and TM00 on Si/SiO2/SU-8 structures. These values yield optical losses of 1.36 and 2.01dB/cm. Optical losses ascribed to Si/SiO2/SOG/SU-8 microstructures have been evaluated to 2.33 and 2.95dB/cm for both polarisations. Hence, as a crucial step for designing polymer components devoted to microsensors applications (pressure, heat transfert), the SU-8 polymer appears as a promising candidate for integrated optics with low optical losses.}
}
@article{GAUTHIER19961,
title = {SAGE: An object-oriented framework for the construction of farm decision support systems},
journal = {Computers and Electronics in Agriculture},
volume = {16},
number = {1},
pages = {1-20},
year = {1996},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(96)00018-X},
url = {https://www.sciencedirect.com/science/article/pii/S016816999600018X},
author = {Laurent Gauthier and Thierry Néel},
keywords = {Farm management, Decision support systems, Smalltalk, Object-oriented software, Object-oriented databases},
abstract = {In agriculture as in other domains, there exists a need for multifaceted and comprehensive decision support frameworks enabling the integration and use of different types of knowledge and information processing tools. The object-oriented paradigm provides a foundation for the construction of such general decision support frameworks. The objective of the described project was to build an object-oriented framework (called SAGE) for knowledge management and decision support in the area of agro-ecosystem management. The Smalltalk object-oriented programming system was the basic technology used to build the SAGE system. A Smalltalk-based object-oriented database management system was also used to provide persistence for Smalltalk objects. The result of the design and implementation effort is a library of Smalltalk classes that constitutes a framework onto which developers can build systems to represent agroecosystems and support the management of these systems. These classes are described and their design and implementation issues are discussed.}
}
@article{DBOUK2016610,
title = {A DF-IBM/NSCD coupling framework to simulate immersed particle interactions},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {309},
pages = {610-624},
year = {2016},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2016.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0045782516303590},
author = {T. Dbouk and F. Perales and F. Babik and R. Mozul},
keywords = {Immersed granular flows, Fluid/structure interaction, Direct-forcing, Immersed boundary method, Non-smooth contact dynamics},
abstract = {Immersed granular flows are present widely in different domains under different forms (at various scales) such as in nature (rivers, muds, atmosphere, blood...), and in many industrial applications (detergents, cosmetics, etc...). Studying such flows properly requires one to represent well the physics behind their dynamics: the fluid/solid interactions (FSI), the solid/solid interactions (SSI) and the coupling mechanisms at various scales. In this work, a new coupling framework to simulate immersed granular flows has been developed. The FSI has been modeled using a direct-forcing immersed boundary method (DF-IBM) and implemented in the parallelized “PELICANS” C++ library. In this DF-IBM, all the mathematical equations, including the direct-forcing term, are discretized, both in space and time, and solved iteratively via a finite-volume and projection methods on Eulerian Grids. A sharp-edge interface, that can be smoothed, is used to represent the fluid/solid transition. The modeling of the multiple SSI at the grain’s scale is based on the Non-Smooth Contact Dynamics (NSCD) approach developed in the “LMGC90” open-source library. The coupling of the two softwares “PELICANS” and “LMGC90”, called Xper, provides an efficient framework to simulate and study dense immersed granular flows by taking into account, both advanced contact laws between grains, and hydrodynamic interactions. We address in this paper the effects of imposing a fluid-ring numerically (or fluid-mesh-cells) around two settling solid disks on modifying their dynamics. The DF-IBM approach implemented in Xper is validated, on a 2D flow over a stationary rigid cylinder benchmark, and on the settling of a rigid buoyant sphere in an incompressible laminar fluid at different Reynolds numbers. The numerical results are in good agreement with experimental and numerical data from the literature.}
}
@article{NAGY19911079,
title = {Computer-Aided Model Structure Selection},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {3},
pages = {1079-1084},
year = {1991},
note = {9th IFAC/IFORS Symposium on Identification and System Parameter Estimation 1991, Budapest, Hungary, 8-12 July 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)52493-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017524933},
author = {P.A.J. Nagy and L. Ljung},
keywords = {Artificial intelligence, expert systems, knowledge engineering, models, system identification},
abstract = {Typically the most difficult problem that faces a user of a software package for system identification is to select an appropriate model structure. In this contribution we discuss several tools for computer support of this decision. They range from advanced bookkeeping of models that have been produced so far to intelligent advice to the user about the quality of the models and suggestions about further structures which should be tested. We also describe how physical knowledge in form of tentative bond-graphs for the system can be used and integrated in the system identification process. All these tools are implemented in a programming environment in which MATLAB, MACSYMA, Common Lisp, and YAPS have been integrated.}
}
@article{KOCAARSLAN2007174,
title = {An adaptive control application in a large thermal combined power plant},
journal = {Energy Conversion and Management},
volume = {48},
number = {1},
pages = {174-183},
year = {2007},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2006.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0196890406001518},
author = {İlhan Kocaarslan and Ertuğrul Çam},
keywords = {Energy application, Power plant, Real-time application, Model reference adaptive control},
abstract = {In this paper, an adaptive controller was applied to a 765MW large thermal power plant to decrease operating costs, increase quality of generated electricity and satisfy environmental concerns. Since power plants may present several operating problems such as disturbances and severe effects at operating points, design of their controllers needs to be carried out adequately. For these reasons, first, a reduced mathematical model was developed under Computer Aided Analysis and Design Package for Control, (CADACS), so that the results of the experimental model have briefly been discussed. Second, conventional PID and adaptive controllers were designed and implemented under the real-time environment of the CADACS software. Additionally, the design of the adaptive model-reference and conventional PID controllers used in the power plant for real-time control were theoretically presented. All processes were realized in real-time. Due to safety restrictions, a direct connection to the sensors and actuators of the plant was not allowed. Insted a coupling to the control system was realized. This offers, in addition, the usage of the supervisiory functions of an industrial process computer system. Application of the controllers indicated that the proposed adaptive controller has better performances for rise and settling times of electrical power, and enthalpy outputs than the conventional PID controller does.}
}
@article{JAMES2014160,
title = {Sequential digital elevation models of active lava flows from ground-based stereo time-lapse imagery},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {97},
pages = {160-170},
year = {2014},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2014.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S092427161400210X},
author = {M.R. James and S. Robson},
keywords = {Photogrammetry, Volcanoes, Sequences, Terrestrial, Stereoscopic, DEM/DTM},
abstract = {We describe a framework for deriving sequences of digital elevation models (DEMs) for the analysis of active lava flows using oblique stereo-pair time-lapse imagery. A photo-based technique was favoured over laser-based alternatives due to low equipment cost, high portability and capability for network expansion, with images of advancing flows captured by digital SLR cameras over durations of up to several hours. However, under typical field scale scenarios, relative camera orientations cannot be rigidly maintained (e.g. through the use of a stereo bar), preventing the use of standard stereo time-lapse processing software. Thus, we trial semi-automated DEM-sequence workflows capable of handling the small camera motions, variable image quality and restricted photogrammetric control that result from the practicalities of data collection at remote and hazardous sites. The image processing workflows implemented either link separate close-range photogrammetry and traditional stereo-matching software, or are integrated in a single software package based on structure-from-motion (SfM). We apply these techniques in contrasting case studies from Kilauea volcano, Hawaii and Mount Etna, Sicily, which differ in scale, duration and image texture. On Kilauea, the advance direction of thin fluid lava lobes was difficult to forecast, preventing good distribution of control. Consequently, volume changes calculated through the different workflows differed by ∼10% for DEMs (over ∼30m2) that were captured once a minute for 37min. On Mt. Etna, more predictable advance (∼3mh−1 for ∼3h) of a thicker, more viscous lava allowed robust control to be deployed and volumetric change results were generally within 5% (over ∼500m2). Overall, the integrated SfM software was more straightforward to use and, under favourable conditions, produced results comparable to those from the close-range photogrammetry pipeline. However, under conditions with limited options for photogrammetric control, error in SfM-based surfaces may be difficult to detect.}
}
@article{ISERMANN198225,
title = {On the Development and Implementation of Parameter-Adaptive Controllers},
journal = {IFAC Proceedings Volumes},
volume = {15},
number = {7},
pages = {25-49},
year = {1982},
note = {3rd IFAC/IFIP Symposium on Software for Computer Control 1982, Madrid, Spain, 5-8 October},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)62799-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701762799X},
author = {R. Isermann and K.-H. Lachmann},
keywords = {Adaptive control, digital control, linearized systems, stability, convergence, supervision, software, microcomputers},
abstract = {The design of parameter-adaptive control algorithms which are based on proper parameter estimation and linear control algorithms is considered for single-input single-output processes. The resulting adaptive control algorithms work well if some pre-conditions for the stability and convergence are satisfied. In practice these pre-conditions may be violated. Therefore a supervision level is introduced which monitors faulty functions and takes appropriate actions. Further improvements are obtained by designing a “parameter-interface” between the parameter estimation and the controller design. Various steps for the application are considered and software packages and the implementation on microcomputer controllers are described.}
}
@article{GUAN2013109,
title = {Process virtualization of large-scale lidar data in a cloud computing environment},
journal = {Computers & Geosciences},
volume = {60},
pages = {109-116},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413002021},
author = {Haiyan Guan and Jonathan Li and Liang Zhong and Yu Yongtao and Michael Chapman},
keywords = {Process virtualization, Lidar, Condor, Computing environment},
abstract = {Light detection and ranging (lidar) technologies have proven to be the most powerful tools to collect, within a short time, three-dimensional (3-D) point clouds with high-density, high-accuracy and significantly detailed surface information pertaining to terrain and objects. However, in terms of feature extraction and 3-D reconstruction in a computer-aided drawing (CAD) format, most of the existing stand-alone lidar data processing software packages are unable to process a large volume of lidar data in an effective and efficient fashion. To break this technical bottleneck, through the design of a Condor-based process virtualization platform, we presented in this paper a novel strategy that uses network-related computational resources to process, manage, and distribute vast quantities of lidar data in a cloud computing environment. Three extensive experiments with and without a cloud computing environment were compared. The experiment results demonstrated that the proposed process virtualization approach is promisingly applicable and effective in the management of large-scale lidar point clouds.}
}
@article{KWOK1991701,
title = {A combined fuzzy and classical PID controller},
journal = {Microprocessing and Microprogramming},
volume = {32},
number = {1},
pages = {701-708},
year = {1991},
note = {Euromicro symposium on microprocessing and microprogramming},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(91)90424-R},
url = {https://www.sciencedirect.com/science/article/pii/016560749190424R},
author = {D.P. Kwok and P. Wang and C.K. Li},
abstract = {This paper presents a new kind of process controller which combines the functional elements of a fuzzy logic controller and that of a classical PID controller. The control algorithm of such a controller is formulated by means of the fuzzy set theory and the classical control theory. The properties of this controller are investigated in terms of The on-line implementation of thic controller is realized on a PC286/287 by means of the nonlinearity and robustness. The off-line design of this controller is developed using linguistic phase plane analysis and classical design heuristics on a HP9000/370 computer. real-time TUTSIM software package. The plant investigation includes a simulated model and an experimental heating process.}
}
@article{BUCHNER198591,
title = {A hierarchical graphics interface for control system programming},
journal = {Annual Review in Automatic Programming},
volume = {13},
pages = {91-92},
year = {1985},
note = {13th IFAC/IFIP Workshop on Real Time Programming},
issn = {0066-4138},
doi = {https://doi.org/10.1016/0066-4138(85)90449-5},
url = {https://www.sciencedirect.com/science/article/pii/0066413885904495},
author = {M Buchner},
keywords = {Computer-aided system design, computer control, computer graphics, control system synthesis, programming languages},
abstract = {There are many control system analysis and design software packages which are currently commercially available. While a large number of these use graphics for the presentation of data, i.e., the results of the simulation and/or computer analysis, to an analyst, the input of the model structure is normally handled through a text definition with strict syntax rules. A graphics interface will be presented for a control system programming language which allows the analyst or designer to input, in an interactive manner, a control system as a graphical block diagram. The interface permits a hierarchical structure to be defined for the control system thereby providing a useful framework for large complex system control problems. Further, the interface provides a standard form to interact with a variety of software analysis systems. The graphics interface is implemented with Pascal as the base language and ACM CORE Standard routines for all graphics interactions on an IBM XT.}
}
@article{BASSHAM1984279,
title = {Dietary analysis with the aid of a microcomputer},
journal = {Journal of Microcomputer Applications},
volume = {7},
number = {3},
pages = {279-289},
year = {1984},
issn = {0745-7138},
doi = {https://doi.org/10.1016/0745-7138(84)90060-5},
url = {https://www.sciencedirect.com/science/article/pii/0745713884900605},
author = {S. Bassham and L.R. Fletcher and R.H.J. Stanton},
abstract = {This paper describes the design, development, implementation and use of a microcomputer software package to assist hospital and other dietitians with a variety of arithmetical tasks relating to analysis of their patients' diets in terms of nutrient content. Although these tasks are computationally simple, the calculations are laborious and time-consuming if done by hand. Indeed, the arithmetical drudgery involved in manual computation can have a direct and adverse effect on the patient care provided by dietitians. Conversely, the availability of this software has made it possible for dietitians to provide much higher quality information in some clinical procedures relating to diet. The main technical problem in providing this software is the large quantity of data contained in the United Kingdom standard food tables which form the database of the package. A no less significant, though non-technical, problem has been the lack of awareness amongst dietitians of the scope for information technology in their professional work.}
}
@article{ALANIS2015240,
title = {Real-time discrete neural control applied to a Linear Induction Motor},
journal = {Neurocomputing},
volume = {164},
pages = {240-251},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.02.065},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215002519},
author = {Alma Y. Alanis and Jorge D. Rios and Jorge Rivera and Nancy Arana-Daniel and Carlos Lopez-Franco},
keywords = {Linear Induction Motor, Recurrent High Order Neural Network, Extended Kalman Filter, Discrete-time sliding modes, Lyapunov},
abstract = {This work presents a real-time discrete nonlinear neural identifier based on a Recurrent High Order Neural Network (RHONN) trained online with an Extended Kalman Filter (EKF) based algorithm applied to a Linear Induction Motor (LIM). For the obtained neural model, a discrete-time sliding mode control law is designed for trajectory tracking of velocity and flux magnitude. The stability analysis is also included, based on the Lyapunov approach. This work is implemented in real-time by using MATLAB®,11MATLAB is a registered trademark of The MathWorks, Inc. a dSPACE®22dSPACE is a registered trademark of DSPACE GmbH. DS1104 controller board and its software RTI libraries and ControlDesk®,33ControlDesk is a registered trademark of DSPACE GmbH. respectively, to control a Linear Induction Motor Lab-Volt®44LAB-Volt is a registered trademark of Lab-Volt Systems, Inc. 8228.}
}
@article{BRANDIMARTE198923,
title = {Outline of an Environment for the Specification and Prototyping of Scheduling and Control Systems for Manufacturing},
journal = {IFAC Proceedings Volumes},
volume = {22},
number = {14},
pages = {23-29},
year = {1989},
note = {IFAC/CIRP/IFIP/IFORS Workshop on Decisional Structures in Automated Manufacturing, Genova, Italy, 18-21 September},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54321-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017543219},
author = {P. Brandimarte and G. Menga and M. Morisio},
abstract = {This paper presents the outline of an environment for the specification and prototyping of manufacturing systems software, which is under development in the framework of a research project sponsored by Italian National Research Council. The environment is heavily influenced, both at the user and at the implementation level, by the object oriented programing paradigm, which is the basis to propose a unified view to represent the hierarchical structure of commonly used reference models. The results are a library of classes to express concurrency and to support an extended client server paradigm, and a new design methodology. Scheduling is another key issue in the prototyping of manufacturing systems, and best results are obtained with knowledge based, heuristic approaches: the object oriented programming paradigm allows to smoothly integrate an expert systems development tool and a simulation environment.}
}
@article{KECELI2019363,
title = {Automated computational thermochemistry for butane oxidation: A prelude to predictive automated combustion kinetics},
journal = {Proceedings of the Combustion Institute},
volume = {37},
number = {1},
pages = {363-371},
year = {2019},
issn = {1540-7489},
doi = {https://doi.org/10.1016/j.proci.2018.07.113},
url = {https://www.sciencedirect.com/science/article/pii/S1540748918305315},
author = {Murat Keçeli and Sarah N. Elliott and Yi-Pei Li and Matthew S. Johnson and Carlo Cavallotti and Yuri Georgievskii and William H. Green and Matteo Pelucchi and Justin M. Wozniak and Ahren W. Jasper and Stephen J. Klippenstein},
keywords = {Computational thermochemistry, Butane oxidation,  thermochemistry, Workflow},
abstract = {Large-scale implementation of high level computational theoretical chemical kinetics offers the prospect for dramatically improving the fidelity of combustion chemical modeling. As a first step toward this goal, we developed a procedure for automatically generating the thermochemical data for combustion of an arbitrary fuel. The procedure begins by producing a list of combustion relevant species from a specification of the fuel and combustion conditions of interest. Then, for each element in the list of species, the procedure determines an internal coordinate z-matrix description of its structure, the optimal torsional configuration via Monte Carlo sampling, key rovibrational properties for that optimal geometry (including anharmonic corrections from torsional mappings and/or vibrational perturbation theory), and high level estimates of the electronic and zero-point energies via arbitrarily defined composite methods. This dataset is then converted first to partition functions, then to thermodynamic properties, and finally to NASA polynomial representations of the data. The end product is an automatically generated database of electronic structure results and thermochemical data including representations in a format appropriate for combustion simulations. The utility and functioning of this predictive automated computational thermochemistry (PACT) software package is illustrated through application to the automated generation of thermochemical data for the combustion of n-butane. Butane is chosen for this demonstration as its species list is of reasonably manageable size for debugging level computations, while still presenting most of the key challenges that need to be surmounted in the consideration of larger fuels. Furthermore, its low temperature chemistry is representative of that occurring with larger alkanes.}
}
@article{HUSSEIN2017965,
title = {Hardware implementation of antenna array system for maximum SLL reduction},
journal = {Engineering Science and Technology, an International Journal},
volume = {20},
number = {3},
pages = {965-972},
year = {2017},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2016.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S2215098616308989},
author = {Amr H. Hussein and Mohamed A. Metawe'e and Haythem H. Abdullah},
keywords = {Antenna array, Beamforming, Power divider, Side lobe level},
abstract = {Side lobe level (SLL) reduction has a great importance in recent communication systems. It is considered as one of the most important applications of digital beamforming since it reduces the effect of interference arriving outside the main lobe. This interference reduction increases the capacity of the communication systems. In this paper, the hardware implementation of an antenna array system for SLL reduction is introduced using microstrip technology. The proposed antenna array system consists of two main parts, the antenna array, and its feeding network. Power dividers play a vital role in various radio frequency and communication applications. A power divider can be utilized as a feeding network of an antenna array. For the synthesis of a radiation pattern, an unequal-split power divider is required. A new design for a four ports unequal circular sector power divider and its application to antenna array SLL reduction is introduced. The amplitude and phase of the signals emerging from each power divider branch are adjusted using stub and inset matching techniques. These matching techniques are used to adjust the branches impedances according to the desired power ratio. The design of the antenna array and the power divider are made using the software package CST MICROWAVE STUDIO. The power divider is realized on Rogers R03010 substrate with dielectric constant εr=10.2, loss tangent of 0.0035, and height h=1.28mm. In addition, a design for ultra-wide band (UWB) antenna element and array are introduced. The antenna elements and the array are realized on the FR4 (lossy) substrate with dielectric constant εr=4.5, loss tangent of 0.025, and height h=1.5mm. The fabrication is done using thin film technology and photolithographic technique. The experimental measurements are done using the vector network analyzer (VNA HP8719Es). Good agreement is found between the measurements and the simulation results.}
}
@article{YEARWORTH2013151,
title = {The uses of qualitative data in multimethodology: Developing causal loop diagrams during the coding process},
journal = {European Journal of Operational Research},
volume = {231},
number = {1},
pages = {151-161},
year = {2013},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2013.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S037722171300386X},
author = {Mike Yearworth and Leroy White},
keywords = {Multimethodology, Paradigm crossing, Qualitative data analysis, Causal loop diagrams (CLDs), Computer aided qualitative data analysis software (CAQDAS), Problem structuring methods (PSMs)},
abstract = {In this research note we describe a method for exploring the creation of causal loop diagrams (CLDs) from the coding trees developed through a grounded theory approach and using computer aided qualitative data analysis software (CAQDAS). The theoretical background to the approach is multimethodology, in line with Minger’s description of paradigm crossing and is appropriately situated within the Appreciate and Analyse phases of PSM intervention. The practical use of this method has been explored and three case studies are presented from the domains of organisational change and entrepreneurial studies. The value of this method is twofold; (i) it has the potential to improve dynamic sensibility in the process of qualitative data analysis, and (ii) it can provide a more rigorous approach to developing CLDs in the formation stage of system dynamics modelling. We propose that the further development of this method requires its implementation within CAQDAS packages so that CLD creation, as a precursor to full system dynamics modelling, is contemporaneous with coding and consistent with a bridging strategy of paradigm crossing.}
}
@article{BARUA1993533,
title = {EXSHOF: An artificial intelligence approach to high-order filter synthesis},
journal = {Engineering Applications of Artificial Intelligence},
volume = {6},
number = {6},
pages = {533-547},
year = {1993},
issn = {0952-1976},
doi = {https://doi.org/10.1016/0952-1976(93)90050-8},
url = {https://www.sciencedirect.com/science/article/pii/0952197693900508},
author = {Alok Barua and Kedar A. Choudhary},
keywords = {Artificial intelligence, expert system, knowledge engineering, software engineering, active filter, network synthesis, circuit, system},
abstract = {A two-tier expert-system-based synthesis of high-order filters is developed. Expert-system techniques have been used for choosing a particular biquad structure and for selection of the high-order filter structure as well. The data inputs will be either filter specifications or transfer functions and the output will be a detailed filter circuit with all its element values. EXSHOF (Expert System based Synthesis of High Order Filter) is totally menu-driven and highly interactive. It assumes little knowledge of computers or filters from the user. The complete package is implemented in Turbo PASCAL and Turbo PROLOG languages.}
}
@article{LENG201979,
title = {Numerical simulation of an immersed rotating structure in fluid for hemodynamic applications},
journal = {Journal of Computational Science},
volume = {30},
pages = {79-89},
year = {2019},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2018.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877750318309979},
author = {Wei Leng and Chen-Song Zhang and Pengtao Sun and Bin Gao and Jinchao Xu},
keywords = {Artificial heart pump, Fluid-structure interactions, Monolithic method, Arbitrary Lagrangian–Eulerian formulation, Mixed finite element},
abstract = {In this paper, numerical simulation of a hemodynamic fluid-structure interaction (FSI) problem with an immersed rotating structure is carried out. A dynamic FSI problem involving a rotational elastic solid, which is modeled by the incompressible shear stress transport (SST) k–ω turbulence model in the fluid domain and by a co-rotational linearized St. Venant–Kirchhoff model in the structure domain, is studied and applied to a type of artificial heart pump. A monolithic arbitrary Lagrangian–Eulerian mixed finite element method, which is modified to adapt to the interaction between fluid and an immersed rotating structure, is employed to discretize the coupled FSI system. The Newton's linearization and the streamline-upwind/Petrov–Galerkin (SUPG) stabilization are employed to overcome strong nonlinearity and dominant convection effects, respectively. Numerical validations are preformed and compared with a commercial CFD software. This paper is an extension to our recent conference paper [1].}
}
@article{JAFFE1986133,
title = {A computer program for the interpretation of exercise tolerance tests},
journal = {Computer Methods and Programs in Biomedicine},
volume = {23},
number = {2},
pages = {133-143},
year = {1986},
issn = {0169-2607},
doi = {https://doi.org/10.1016/0169-2607(86)90108-2},
url = {https://www.sciencedirect.com/science/article/pii/0169260786901082},
author = {Michael B. Jaffe},
keywords = {Exercise testing, Exercise interpretation, Predictive equations, Computer-assited evaluation},
abstract = {A software package has been developed to interpret exercise test data recorded by a metabolic measurement cart (MMC). This package reduces and interprets the exercise test data relative to population and patient specific criteria. The data flow for the data reduction and report printing is described. The data structures used in this processing are illustrated. The application of structured programming, a real-time executive and an easy-to-read language — such as Pascal — is illustrated for the implementation of a medical instrument. Issues of prediction and interpretation relative to exercise testing are discussed.}
}
@article{MICHAIL2019492,
title = {Parallelization of large-scale drug–protein binding experiments},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {492-502},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.065},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1830058X},
author = {Dimitrios Michail and Antonios Makris and Iraklis Varlamis and Mark Sawyer},
keywords = {Molecular dynamics simulation, Drug–protein structural similarity, High performance computing},
abstract = {The pharmaceutical industry invests billions of dollars on a yearly basis for new drug research. Part of this research is focused on the repositioning of established drugs to new disease indications and is based on “drug promiscuity”, or in plain words, on the ability of certain drugs to bind multiple proteins. The increased cost of wet-lab experiments makes the in-silico alternatives a promising solution. In order to find similar protein targets for an existing drug, it is necessary to analyse the protein and drug structures and find potential similarities. The latter is a highly demanding in computational resources task. However, algorithmic advances in conjunction with increased computational resources can leverage this task and increase the success rate of drug discovery with significantly smaller cost. The current work proposes several algorithms that implement the protein similarity task in a parallel high-performance computing environment, solve several load imbalance and memory management issues and take maximum advantage of the available resources. The proposed optimizations achieve better memory and CPU balancing and faster execution times. Several parts of the previously linear processing pipeline, which used different software packages, have been re-engineered in order to improve process parallelization. Experimental results, on a high-performance computing environment with up to 1024 cores and 2048GB of memory, demonstrate the effectiveness of our approach, which scales well to large amounts of protein pairs.}
}
@article{WAHNSCHAFFT1991565,
title = {Split: A separation process designer},
journal = {Computers & Chemical Engineering},
volume = {15},
number = {8},
pages = {565-581},
year = {1991},
issn = {0098-1354},
doi = {https://doi.org/10.1016/0098-1354(91)80012-K},
url = {https://www.sciencedirect.com/science/article/pii/009813549180012K},
author = {O.M. Wahnschafft and T.P. Jurain and A.W. Westerberg},
abstract = {The development of chemical processes is a complex design problem, commonly performed as an evolution among synthesis, analysis and evaluation steps. Experience and skilled use of existing tools for process analysis and optimization allow designers to focus on the development of relatively few promising flowsheets. However, there is still a need for software systems supporting this activity. The blackboard-based integrated software environment SPLIT (“Separation Process Layout by Invention and Testing”) is conceived to automate the design of separation systems, while allowing the user the option to guide the solution process. The system can handle problem specifications at various levels of detail, using any available knowledge to restrict the number of alternative designs investigated. Resembling the control architecture AKORN-D proposed by Lien (Ph.D. Thesis, Univ. of Trondheim, Norway, 1988), SPLIT combines data-driven and goal-oriented strategies to provide a framework for opportunistic reasoning. Representations of the problem and the current state of the solution are embedded in the blackboard as several semantic hierarchies, some of which are dynamically created. Knowledge sources (KS) of two types screen the common memory blackboard for data patterns indicating their ability to be executed. Domain KS implement procedural knowledge about physical properties, the technologies available to perform required tasks, etc. Control KS are used to guide the problem solving behavior of the system; they are integrated in the same recognition/bidding cycle, but when selected by the scheduling function, only change foci of attention on tasks, partial solutions, or in general any object on the blackboard. Using a distributed problem solving framework (Cardozo, Ph.D. Thesis, Carnegie Mellon University, Pittsburgh, U.S.A., 1987), the blackboard system is being integrated with existing analysis/optimization tools. This framework is used within several domain knowledge sources to invoke such external programs as the flowsheeting program Aspen Plus (Aspen Tech, Aspen User Guide to Release 8.2 and 8.3, Cambridge, MA, 1988) or the optimization package DICOPT + + (Viswanathan and Grossmann, Computers chem. Engng, 14, 769–782, 1990) (a mixed-integer nonlinear programming package) for the analysis and optimization of partial and complete flowsheets. The focus of this paper is the way SPLIT addresses the presynthesis problem, i.e. the generation of a reasonable set of alternative separation processes, which may be incorporated in a superstructure in order to determine the best solution. The system first decomposes separation problems initially described through specifications of feed and product streams into explicitly represented binary split tasks. A classification of the components according to a criterion such as molecular structure is used to identify abstract tasks. This abstraction aids in the selection of technologies that can accomplish the necessary separations. Combined with novel representations of separation and mixing functionality, this approach enables the current prototype version of SPLIT to systematically develop promising flowsheet alternatives, including complex processes with recycles.}
}
@article{ZHOU2007346,
title = {CAD–PACS integration tool kit based on DICOM secondary capture, structured report and IHE workflow profiles},
journal = {Computerized Medical Imaging and Graphics},
volume = {31},
number = {4},
pages = {346-352},
year = {2007},
note = {Computer-aided Diagnosis (CAD) and Image-guided Decision Support},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2007.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0895611107000225},
author = {Zheng Zhou and Brent J. Liu and Anh H. Le},
keywords = {CAD, PACS, System integration, DICOM, IHE},
abstract = {Computer aided diagnosis/detection (CAD) goes beyond subjective visual assessment of clinical images providing quantitative computer analysis of the image content, and can greatly improve clinical diagnostic outcome. Many CAD applications, including commercial and research CAD, have been developed with no ability to integrate the CAD results with a clinical picture archiving and communication system (PACS). This has hindered the extensive use of CAD for maximum benefit within a clinical environment. In this paper, we present a CAD–PACS integration toolkit that integrates CAD results with a clinical PACS. The toolkit is a software package with two versions: DICOM (digital imaging and communications in medicine)-SC (secondary capture) and DICOM–IHE (Integrating the Healthcare Enterprise). The former uses the DICOM secondary capture object model to convert the screen shot of the CAD results to a DICOM image file for PACS workstations to display, while the latter converts the CAD results to a DICOM structured report (SR) based on IHE Workflow Profiles. The DICOM-SC method is simple and easy to be implemented without ability for further data mining of CAD results, while the DICOM–IHE can be used for data mining of CAD results in the future but more complicated to implement than the DICOM-SC method.}
}
@article{GENTIL1990803,
title = {SEXI: An expert identification package},
journal = {Automatica},
volume = {26},
number = {4},
pages = {803-809},
year = {1990},
issn = {0005-1098},
doi = {https://doi.org/10.1016/0005-1098(90)90056-N},
url = {https://www.sciencedirect.com/science/article/pii/000510989090056N},
author = {S. Gentil and A.Y. Barraud and K. Szafnicki},
keywords = {Artificial intelligence, expert systems, identification, modelling, parameter estimation},
abstract = {This paper deals with an identification package. In the first part, the various classical off-line parameter estimation methods implemented are described. A graphic editor allows one to prepare the input-output data easily, which ensures a correct estimation. However, good methods and robust algorithms are not sufficient to guarantee a successful use of identification in industry. Another necessary component is the know-how of the person carrying it out. In the second part, the expert system SEXI (“Système EXpert en Identification”) is considered, which uses the software mentioned above so as to determine the model structure. SEXI behaves as a supervisor: it selects and runs the appropriate numerical module to obtain the quantitative results necessary for its reasoning and it iterates this process until a relevant model is found.}
}
@incollection{OLZAK2010377,
title = {Chapter 15 - Desktop virtualization},
editor = {Thomas Olzak and Jason Boomer and Robert M. Keefer and James Sabovik},
booktitle = {Microsoft Virtualization},
publisher = {Syngress},
address = {Boston},
pages = {377-441},
year = {2010},
isbn = {978-1-59749-431-1},
doi = {https://doi.org/10.1016/B978-1-59749-431-1.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781597494311000151},
author = {Thomas Olzak and Jason Boomer and Robert M. Keefer and James Sabovik},
abstract = {Publisher Summary
Microsoft virtualization is a collection of solutions designed to protect and enhance personal and enterprise computing. Microsoft's enterprise desktop virtualization solution consists of two parts that include Virtual PC and Microsoft Enterprise Desktop Virtualization (MED-V). Virtual PC segregates desktop operating environments while MED-V enables easy implementation of images. In addition to basic virtualization capabilities, Microsoft also provides backward compatibility by including Windows XP with certain Virtual PC licenses. The installation of Virtual PC will automatically launch the console and the New Machine Wizard. MED-V is the engine behind the management of an enterprise virtual desktop environment. It empowers an administrator to control the deployment and management of Virtual PC images. One must be a Microsoft Software Assurance customer to take advantage of this functionality, as it is part of the Microsoft Desktop Optimization Pack (MDOP). Installation of the MED-V client requires local admin rights on the target machine. The client and management application for MED-V are installed from the same MDOP 2009 media from which the server components are installed. Once created, a MED-V image can be wrapped up in a MED-V package and uploaded to an image repository for web-based distribution. One can also deploy a package via other distribution tools, such as Systems Management Server (SMS) or System Center Configuration Manager (SCCM).}
}
@article{SMITH1993507,
title = {FFT — fRISCy fourier transforms?},
journal = {Microprocessors and Microsystems},
volume = {17},
number = {9},
pages = {507-521},
year = {1993},
issn = {0141-9331},
doi = {https://doi.org/10.1016/S0141-9331(09)91002-X},
url = {https://www.sciencedirect.com/science/article/pii/S014193310991002X},
author = {MR Smith},
keywords = {FFT, digital signal processing, RISC, Am29050 processor},
abstract = {This is an applications tutorial oriented towards the practical use of the discrete Fourier transform (DFT) implemented via the fast Fourier transform (FFT) algorithm. The DFT plays an important role in many areas of digital signal processing, including linear filtering, convolution and spectral analysis. The first part of the article is a practical industrial example and takes the reader through the thought process an engineer might take as DFT familiarity is gained. If standard software packages did not provide the necessary performance, the engineer would need to port the application to specialized hardware. The second part of the tutorial discusses the theoretical concepts behind the FFT algorithm and a processor architecture suitable for high speed FFT handling. Rather than examining the standard digital signal processors (DSP) in this situation, the final section looks at how the reduced instruction set (RISC) processors perform. The Advanced Micro Devices scalar Am29050 and the super-scalar Intel i860 processors are detailed, Comparison of the DSP and RISC processors is given showing that the more generalized RISC chips do well, although changes in certain aspects of the RISC architecture would provide for considerable improvements in performance.}
}
@article{LEIPZIG2021100322,
title = {The role of metadata in reproducible computational research},
journal = {Patterns},
volume = {2},
number = {9},
pages = {100322},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100322},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001707},
author = {Jeremy Leipzig and Daniel Nüst and Charles Tapley Hoyt and Karthik Ram and Jane Greenberg},
keywords = {reproducible research, reproducible computational research, RCR, reproducibility, replicability, metadata, provenance, workflows, pipelines, ontologies, notebooks, containers, software dependencies, semantic, FAIR},
abstract = {Summary
Reproducible computational research (RCR) is the keystone of the scientific method for in silico analyses, packaging the transformation of raw data to published results. In addition to its role in research integrity, improving the reproducibility of scientific studies can accelerate evaluation and reuse. This potential and wide support for the FAIR principles have motivated interest in metadata standards supporting reproducibility. Metadata provide context and provenance to raw data and methods and are essential to both discovery and validation. Despite this shared connection with scientific data, few studies have explicitly described how metadata enable reproducible computational research. This review employs a functional content analysis to identify metadata standards that support reproducibility across an analytic stack consisting of input data, tools, notebooks, pipelines, and publications. Our review provides background context, explores gaps, and discovers component trends of embeddedness and methodology weight from which we derive recommendations for future work.}
}
@article{CHI2001144,
title = {Automatic proxy-based watermarking for WWW},
journal = {Computer Communications},
volume = {24},
number = {2},
pages = {144-154},
year = {2001},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(00)00309-1},
url = {https://www.sciencedirect.com/science/article/pii/S0140366400003091},
author = {C.-H Chi and Y Lin and J Deng and X Li and T.-S Chua},
keywords = {Internet, Digital libraries, Copyright, Watermarking, Proxy server, Content transformation},
abstract = {With the appearance of digital libraries and information archive centers on the Internet, copyright issues are becoming very concerning. Unlike hardcopies, digital data can be modified and distributed very easily. Watermarking is the solution to this copyright problem. By embedding a secure, identifiable mark on the digital data, ownership and content integrity can be ensured. Currently, the watermarking process is usually done manually and off-line. This makes updating of watermarking techniques and watermark logo content difficult. Enforcement of watermarking policies within an organization for providing content on the Internet is also not easy. Furthermore, installing watermarking software on each workstation of an organization is not very cost effective. In this paper, we propose a novel approach to address the copyright issues of digital data on the Internet. The basic idea is to move the watermarking process from the content developers to a reverse proxy server that is usually close to the organization's gateway. Data will be watermarked only when it is retrieved through this proxy. The main advantages of this approach are that: (i) it is cost effective as being independent of the amount of data on the web server and also independent of the number of content developers in the organization, (ii) it makes updates and maintenance tasks easy, being an one-for-all solution, and (iii) the reverse proxy effectively enforces the watermarking guideline. Both design considerations and implementation details of watermarking in a fully functional, Squid-based reverse proxy server are presented and its performance is analyzed. All results show that automatic watermarking in a reverse proxy server is a practical, cost efficient and enforcement effective solution for handling copyright issues in Internet digital libraries.}
}
@incollection{LAVOIE1983233,
title = {INTERACTIVE COMPUTER GRAPHICS FOR NETWORK ANALYSIS AND DESIGN},
editor = {G.G. LEININGER},
booktitle = {Computer Aided Design of Multivariable Technological Systems},
publisher = {Pergamon},
pages = {233-241},
year = {1983},
isbn = {978-0-08-029357-8},
doi = {https://doi.org/10.1016/B978-0-08-029357-8.50039-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080293578500391},
author = {L. Lavoie and H.H. Hoang},
abstract = {ABSTRACT.
A software package was developed and implemented to be used as graphical display tools in the analysis and design of different kinds of networks, e.g., transportation, communication, and power networks. It is essentially a specialized data base management system, where databases represent network structures and characteristics. The data definition and manipulation languages were defined to permit the creation and update of topological structures, physical attributes, and performance measures of networks, as well as the graphical display of these data in a convenient manner. The software has been designed to provide flexible, easy-to-use and transportable computer-aided design tools for network analysis and modelling.}
}
@incollection{SHAH1994423,
title = {CHAPTER 18 - A Testbed For Rapid Prototyping Of Feature Based Applications},
editor = {Jami J. Shah and Martti Mäntylä and Dana S. Nau},
series = {Manufacturing Research and Technology},
publisher = {Elsevier},
volume = {20},
pages = {423-453},
year = {1994},
booktitle = {Advances in Feature Based Manufacturing},
issn = {1572-4417},
doi = {https://doi.org/10.1016/B978-0-444-81600-9.50023-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444816009500237},
author = {Jami J. Shah and Mary T. Rogers},
abstract = {Abstract
The A.S.U. Features Testbed provides an infrastructure for rapid prototyping of feature based applications. The system has an open architecture in which users can define feature libraries for modeling their products and build knowledge bases for a variety of manufacturing applications. The Testbed is organized into two shells: one for feature based product definition, the other for feature based product evaluation or manufacturing planning. The product modeler allows one to integrate features, dimensions, tolerances, assembly data, geometry, topology, and design rules into a unified product description. The applications shell facilitates the creation of knowledge bases and reasoning procedures desired for the applications. In this chapter, we give a brief overview of the system and then discuss several applications that have been implemented. These include GT coding for machining, machinability evaluation, and composite panel forming. The Testbed can be used for two different purposes: (1) Organizations that want to evaluate feature based technology in conjunction with knowledge based applications can quickly do so in this integrated system. (2) feature related techniques and algorithms can be evaluated by software developers. Customization of the Testbed does not require any programming or re-compiling because the system is data-driven.}
}
@article{NAGAOSA2017414,
title = {Turbulence model-free approach for predictions of air flow dynamics and heat transfer in a fin-and-tube exchanger},
journal = {Energy Conversion and Management},
volume = {142},
pages = {414-425},
year = {2017},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2017.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S0196890417302741},
author = {Ryuichi S. Nagaosa},
keywords = {Heat exchanger, Computational fluid dynamics (CFD), Pressure drop, Heat transfer coefficient, Flow transition, Open-source software},
abstract = {The purpose of this study is to quantify performances of a plain fin-and-tube heat exchanger based on a computational fluid dynamics (CFD) technique. This study employs a novel turbulence model-free approach to obtain exact numerical predictions and to avoid numerical uncertainties caused by the models. It applies three open-source software packages for conducting a series of CFD processes. The present CFD approach implements a total of 6.7×106 meshes to discretize the governing equations with the aid of viscous-layer meshes for fine-scale resolution, especially near the two flat-plate fins. The inlet air flow velocity is varied from 0.25 to 8.0ms-1, and the numerical results are compared with the laboratory measurements conducted under an equivalent experimental condition. The results of air flow visualizations show that the air flows can be categorized into three flow regimes, (1) a steady-state laminar flow, (2) an unsteady flow with periodic fluctuations, and (3) a turbulent flow with random fluctuations. This study identifies the critical Reynolds numbers for the transitions from (1) to (2) at ReD≈4000, and from (2) to (3) at ReD≈6000. The present numerical work also predicts the pressure drops and the heat transfer coefficients within an acceptable margin of errors. The fact demonstrates the potential usefulness and suitability of the present numerical approach for practical thermal engineering problems. It is concluded that the present simulation technique is beneficial to introduce for advanced design and optimization of heat transfer equipment with minimized numerical uncertainties.}
}
@article{LAPANE201139,
title = {Perceptions of e-prescribing efficiencies and inefficiencies in ambulatory care},
journal = {International Journal of Medical Informatics},
volume = {80},
number = {1},
pages = {39-46},
year = {2011},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2010.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S1386505610002108},
author = {Kate L. Lapane and Rochelle K. Rosen and Catherine Dubé},
keywords = {Ambulatory care, Electronic prescribing, E-prescribing, Efficiency},
abstract = {Introduction
Recent studies have demonstrated that e-prescribing takes longer than handwriting. Additional studies documenting the perceived efficiencies realized from e-prescribing from those who have implemented electronic prescribing are warranted.
Methods
We used a mixed method study design. We report on qualitative date from 64 focus groups with clinicians and office staff from six US states. Participants used one of six e-prescribing software packages. Qualitative data from the focus groups (276 participants) were coded and analyzed using NVivo software. Quantitative data regarding perceived efficiencies were extracted from a survey of 157 clinicians using e-prescribing.
Results
Perceptions of e-prescribing included 64% reporting e-prescribing as very efficient. The next closest method was computer generated fax and prescriptions in which ∼25% rated the method as very efficient. Improvements in workflow and record keeping were noted. Perceived efficiencies were realized by decreased errors, availability of formularies at the point of prescribing and refill processing. Perceived inefficiencies noted included the need for dual systems owing to regulations preventing e-prescribing of scheduled medications as well as those introduced with incorrect information on formularies, pharmacy used, and warnings.
Discussion
Overwhelmingly, clinicians and their staff confirmed the perceived efficiencies realized with the adoption of e-prescribing. Perceived efficiencies were realized in knowing formularies, processing refills, and decreasing errors. Opportunities to improve efficiencies could be realized by assuring correct information in the system.}
}
@article{SIGRIMIS200021,
title = {Synergism of high and low level systems for the efficient management of greenhouses},
journal = {Computers and Electronics in Agriculture},
volume = {29},
number = {1},
pages = {21-39},
year = {2000},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(00)00134-4},
url = {https://www.sciencedirect.com/science/article/pii/S0168169900001344},
author = {N.A. Sigrimis and K.G. Arvanitis and G.D. Pasgianos},
keywords = {Computer control, Fuzzy systems, Knowledge based control, Proportional-integral-derivative (PID) control, Smith predictor},
abstract = {The advantages of using artificial intelligence (AI) decision support tools in synergism with low level process controllers or schedulers are investigated in this paper. The development of a modern control and management system for greenhouses used recent advances in software design, and development tools, to provide an open system for rapid program development. To effectively integrate expert system applications in a control and management system, an environment was built that supports all required interfaces between AI applications and the greenhouse management system (GMS). This environment incorporates a native fuzzy knowledge based system (KBS) and a number of procedural control functions, in the GMS, that can effectively interact. The programmable logic controller (PLC) houses all well-known control function blocks, in library form, callable to implement various control loop designs. Functions that have not been foreseen in the PLC control library can be instantly implemented using the open KBS system. The innovative addition of integral initial conditions on a proportional-integral-derivative (PID) controller, for repetitive load switching applications, is an example, demonstrated in this paper. The usefulness of other control blocks such as a self-adjusting Smith predictor is also tested for a real application of a mixing process with long dead time. Synergism of fuzzy decisions and fuzzy controllers, at the supervisory level, with low level process regulators provide adaptive systems, which can optimize both long-term objectives and the short time dynamic responses.}
}
@article{MOROZOV2006123,
title = {The effect of filament-winding mosaic patterns on the strength of thin-walled composite shells},
journal = {Composite Structures},
volume = {76},
number = {1},
pages = {123-129},
year = {2006},
note = {Fifteenth International Conference on Composite Materials},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2006.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0263822306002583},
author = {E.V. Morozov},
keywords = {Composite shells, Filament-winding pattern, Stress analysis, Manufacturing effects},
abstract = {The paper deals with the modelling and stress analysis of filament-wound composite shells. Existing approaches to the structural analysis of composite shells are usually based on the implementation of mechanics of composite laminates. However, there are some manufacturing effects, specific for the filament-winding process, which could significantly affect the stress and strain fields in the thin-walled composite shells. One of such effects is related to the filament-winding mosaic pattern of the composite layer and usually is not considered in the general stress analysis procedures, including existing finite element software packages. The paper presents results of the modelling and analysis of composite cylindrical shells with different types of filament-winding mosaic patterns. The differences in the numerically predicted stress values caused by the mosaic pattern effect are demonstrated using particular examples. As shown, the actual level of stresses in the thin-walled filament wound composite shells could be underestimated in some cases by the stress analysis based on the conventional mechanics of laminated structures.}
}
@incollection{GOUCEM1989137,
title = {AN EXTENSION TO A COMMERCIAL CACSD PACKAGE FOR USE IN A BASIC CONTROL COURSE},
editor = {D.A. LINKENS and D.P. ATHERTON},
booktitle = {Trends in Control and Measurement Education},
publisher = {Pergamon},
address = {Oxford},
pages = {137-142},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035736-2},
doi = {https://doi.org/10.1016/B978-0-08-035736-2.50031-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357362500311},
author = {A. Goucem and D.P. Atherton},
abstract = {The paper describes the implementation of procedures in CTRL-C which make it immediately usable for single control problems by an undergraduate with no knowledge of the software. The student is required to execute two procedures, PLANT and CONTROL, and then respond appropriately to simple menus. This provides the student with an easy entry to powerful control system design software which he can use in more depth later in his education or professional career.}
}
@incollection{SIXIN1989397,
title = {AN APPLIED SOFTWARE PACKAGE FOR SYSTEM IDENTIFICATION AND ITS INDUSTRIAL PRACTICE},
editor = {CHEN ZHEN-YU},
booktitle = {Computer Aided Design in Control Systems 1988},
publisher = {Pergamon},
address = {Oxford},
pages = {397-402},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035738-6},
doi = {https://doi.org/10.1016/B978-0-08-035738-6.50068-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357386500689},
author = {Xu Sixin and Song Wenzhong and Huang Dong},
abstract = {In this paper, an applied software packag for identifying MIMO system is introduce. It is suitable for implementation on microcomputers. The package contains six algorithms of parameter estimation and five methods of determining model structure. A new model structure test method viz. model response comparison method and several transformation algorithms in recusive form from input-output difference equation into other model forms are included in the package. The modelling procedure involves two steps:(1), looking for a middle model with high order which dynamic is approximate to true object; (2), reducing model order to obtain a suitable parameteric model. The package is equipped for a special microcomputer to identify the dynamics. The identification experiments for a 350 MW boiler-turbine-generator(BTG) set by using the special microcomputer have been carried out, and the data are processed off-line by the package. Then the more overall mathematical models of the BTG set are obtained in the form of transfer functions.}
}
@article{DEWEY2001373,
title = {Visual modeling and design of microelectromechanical system transducers},
journal = {Microelectronics Journal},
volume = {32},
number = {4},
pages = {373-381},
year = {2001},
issn = {0026-2692},
doi = {https://doi.org/10.1016/S0026-2692(99)00090-7},
url = {https://www.sciencedirect.com/science/article/pii/S0026269299000907},
author = {A. Dewey and V. Srinivasan and E. Icoz},
keywords = {Microelectromechanical system, Transducers, Visual modeling, VHDL-AMS, Design capture},
abstract = {Microelectromechanical systems (MEMS) integrate miniaturized mechanical structures with electronics to extend the benefits of planar integrated circuit technology to a broader class of systems, involving sensors, actuators, filters, resonators, switches, and wave guides. The mechanical structures, such as beams, plates, groves, and diaphragms, implement transduction between energy domains, passive implementations of discrete electrical devices, and conduction paths for electromagnetic radiation [F. Frank, J. Staller, The merging of micromachining and microelectronics, Third International Forum on ASIC and Transducer Technology, Alberta, Canada, May 1990, pp. 53–60, R. Howe, Silicon micromechanics: sensors and actuators in a chip, IEEE Spectrum, July (1990) 29–35]. To realize the potential and growth of microelectromechanical systems (MEMS) technology, many new design and manufacturing challenges must be addressed. The close proximity of the integration of mechanical and electrical domains within the small dimensions associated with very large scale integration (VLSI) presents new energy-coupling issues. The behavior of the overall system is not the simple concatenation of separate mechanical and electrical behaviors, but the simultaneous combination of the mechanical and electrical behaviors. New modeling, analysis, and design techniques are required to address both mechanics and electronics. In this paper, we address initial design capture and system conceptualization of MEMS transducers based on visual modeling and design. Visual modeling frames the task of generating hardware description language (analog and digital) component models in a manner similar to the task of generating software programming language applications. A domain is created using relevant artifacts and the artifacts are rendered to highlight key design aspects. The artifacts may be directly manipulated in controlled ways to alter design aspects—a process we refer to as design-by-direct-manipulation. To facilitate the application of visual modeling and design for microelectromechanical transducers, artifacts, renderings, and associated design aspects need to be largely predefined. This requirement leads to a structured topological design strategy wherein microelectromechanical foundry cell libraries are utilized. Microelectromechanical system transducer design becomes a process of exploring candidate cells (topologies), varying key aspects of the transduction for each topology, and determining which topology best satisfies design requirements. Design renderings and aspects emphasize a circuit level of abstraction. Coupled-energy MEMS characterizations are presented based on branch constitutive relations and an overall system of simultaneous differential and algebraic equations (DAE). The resulting design methodology is called Visual Integrated-Microelectromechanical VHDL-AMS Interactive Design (VIVID).}
}
@article{HANTUCH1979285,
title = {Automatic Synthesis of a Packaged Program for Real Time Technological Process Control},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {7},
pages = {285-298},
year = {1979},
note = {IFAC Symposium on computer Aided Design of Control Systems, Zurich, Switzerland, 29-31 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65611-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017656118},
author = {I. Hantuch},
keywords = {Control system synthesis, control engineering computer applications, programing languages, real time applications},
abstract = {This contribution refers to certain problems bearing upon the formation of a system generator programing medium of an automatic synthesis for the modular construction of a packaged program for real time technological process control. The package is made up of standard subroutines-functional modules, carrying out the basic control functions. The user defines the task in the problem-oriented "graf language" designed by an analysis of project documentation for automated systems of technological process control. From the viewpoint of software, this is an establishment of a communication and synchronization system, implemented by standard connectors and starting subroutines, operating over the central database and by a system of tables for recording required items of information on the structure of the control package.}
}
@article{ALLENSHER1990241,
title = {A pyramid programming environment on the connection machine},
journal = {Pattern Recognition Letters},
volume = {11},
number = {4},
pages = {241-245},
year = {1990},
issn = {0167-8655},
doi = {https://doi.org/10.1016/0167-8655(90)90062-7},
url = {https://www.sciencedirect.com/science/article/pii/0167865590900627},
author = {C {Allen Sher} and Azriel Rosenfeld},
keywords = {Pyramids, software packages, connection machine, Hough transform},
abstract = {A pyramid programming environment on the Connection Machine is presented. The mapping between the Connection Machine and pyramid structures is based on a scheme called Shuffled 2D Gray Code. A pyramid Hough transform, based on computing the distances between line or edge segments and enforcing merge and select strategies among them, is implemented using this programming environment.}
}
@article{HOLIK2019333,
title = {Optimization of an organic Rankine cycle constrained by the application of compact heat exchangers},
journal = {Energy Conversion and Management},
volume = {188},
pages = {333-345},
year = {2019},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2019.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0196890419303346},
author = {Mario Holik and Marija Živić and Zdravko Virag and Antun Barac},
keywords = {Waste heat recovery, Heavy-duty truck engine, Organic Rankine cycle, Compact heat exchanger sizing, Multi-objective optimization},
abstract = {Recovering waste heat from the exhaust gases of heavy-duty diesel truck engines by using the organic Rankine cycle can reduce both fuel consumption and greenhouse gas emissions. The design of such systems is constrained by the space available for installation; thus, an optimization procedure is required. In this research work, a two-objective optimization, based on the maximization of power output and minimization of the total heat exchanger surface area, is implemented in the Engineering Equation Solver software package. Application of the developed procedure is illustrated in a case with an exhaust gas temperature of 394.2 °C and mass flow rate of 0.306 kg/s. Configurations both with and without a recuperator are analyzed, and four working fluids (water, ethanol, toluene, and hexamethyldisiloxane) are compared. Of these fluids, ethanol could be recommended as the best when considering the organic Rankine cycle power output and the total area/volume of heat exchangers. For a total surface area of 10 m2 in the case without the recuperator, the power output values for ethanol, water, and toluene are 17.6 kW, 17.2 kW, and 15.8 kW, respectively; with the recuperator, the power output is slightly higher, but the total area of heat exchangers is considerably larger. Again, the best working fluid is ethanol: for a total surface area of heat exchangers of 24 m2, the power output values for ethanol, toluene, and hexamethyldisiloxane are 18 kW, 17.1 kW, and 15.8 kW, respectively. The presented procedure is useful both for the conceptual design of a complete waste heat recovery unit and for the optimization of all heat exchangers.}
}
@article{CARLUCCI2015378,
title = {Multi-objective optimization of a nearly zero-energy building based on thermal and visual discomfort minimization using a non-dominated sorting genetic algorithm (NSGA-II)},
journal = {Energy and Buildings},
volume = {104},
pages = {378-394},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2015.06.064},
url = {https://www.sciencedirect.com/science/article/pii/S0378778815301080},
author = {Salvatore Carlucci and Giulio Cattarin and Francesco Causone and Lorenzo Pagliano},
keywords = {Simulation-based optimization, Zero energy buildings, Genetic algorithm, NSGA-II, Multi-objective optimization, Thermal comfort, Visual comfort, , , },
abstract = {Multi-objective optimization methods provide a valid support to buildings’ design. They aim at identifying the most promising building variants on the basis of diverse and potentially contrasting needs. However, optimization has been mainly used to optimize the energy performance of buildings, giving secondary importance to thermal comfort and usually neglecting visual comfort and the indoor air quality. The present study addresses the design of a detached net zero-energy house located in Southern Italy to minimize thermal and visual discomfort. The optimization problem admits four objective functions (thermal discomfort during winter and summer and visual discomfort due to glare and an inappropriate quantity of daylight) and uses the non-dominated sorting genetic algorithm, implemented in the GenOpt optimization engine through the Java genetic algorithms package, to instruct the EnergyPlus simulation engine. The simulation outcome is a four-dimensional solution set. The building variants of the Pareto frontier adopt diverse and non-intuitive design alternatives. To derive good design practices, two-dimensional projections of the solution set were also analyzed. Finally, in cases of complex optimization problems with many objective functions, optimization techniques are recommended to effectively explore the large number of available building variants in a relatively short time and, hence, identify viable non-intuitive solutions.}
}
@article{CHANG1996191,
title = {Development of Columbia's video on demand testbed},
journal = {Signal Processing: Image Communication},
volume = {8},
number = {3},
pages = {191-207},
year = {1996},
issn = {0923-5965},
doi = {https://doi.org/10.1016/0923-5965(95)00047-X},
url = {https://www.sciencedirect.com/science/article/pii/092359659500047X},
author = {Shih-Fu Chang and Alexandros Eleftheriadis and Dimitris Anastassiou},
keywords = {Video on demand, Interactive video, Video interoperability, Video servers, MPEG-2 video over ATM},
abstract = {This paper describes our progress in developing an advanced video-on-demand (VOD) testbed, which will accommodate various multimedia research and applications such as Electronic News on Demand, Columbia's Video Course Network, and Digital Libraries. Two different prototypes have been completed. The first generation of the testbed was based on a constant-bit-rate (CBR) video server utilizing Ethernet delivery. Contents were encoded and stored as MPEG-2 audio/video elementary streams. Software encoders/decoders were used in content generation and playback. The second generation of the testbed was enhanced with the capability of transmitting true MPEG-2 transport streams over the campus ATM network as well as the wide-area NYNET ATM network. A real-time video pump and a distributed application control protocol (MPEG-2's DSM-CC) have been incorporated. Hardware decoders and set-tops are being incorporated to test wide-area video interoperability. Our VOD testbed also provides an advanced platform for implementing proof-of-concept prototypes of related research. Our current research focus covers video transmission with heterogeneous quality-of-service (QoS) provision, video storage architecture design, content-based video indexing and browsing, multi-resolution (MR) video coding, efficient manipulation of compressed video, and advanced user interfaces. An important aim is to enhance interoperability. Accommodation of practical multimedia applications and interoperability testing with external VOD systems are currently being undertaken.}
}
@article{TAN2020112694,
title = {Direct FE2 for concurrent multilevel modelling of heterogeneous structures},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {360},
pages = {112694},
year = {2020},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2019.112694},
url = {https://www.sciencedirect.com/science/article/pii/S0045782519305791},
author = {Vincent Beng Chye Tan and Karthikayen Raju and Heow Pueh Lee},
keywords = {FE method, Multi-scale finite element computation, Multi-level FEM, Multi-point constraints (MPCs), Heterogeneous structures, Composites},
abstract = {FE2 methods can be used for the analysis of heterogeneous materials at the continuum macroscale while simultaneously accounting for the microstructural details. FE2 analyses typically comprise two levels of Finite Element (FE) simulations that are performed concurrently. At the macroscale level, the entire heterogeneous material or structure is discretized into homogenized continuum finite elements. Homogenized constitutive relations are not required for the macroscale calculations. Instead they are obtained from microscale level FE simulations on representative volume elements (RVE) of the material where the different phases of the heterogeneous material are explicitly modelled. The paper presents how the two levels of simulations can be collapsed into one by combining the equations governing both levels of FE analyses. The result is a single system of equations in terms of only the microscale level degrees of freedom, d̃. The equations take the familiar form of Kd̃=f̃. It is shown that K comprises the stiffness contributions from the RVE meshes scaled by an amount that is dependent on the relative sizes of the macroscale finite element and the RVE mesh, and the geometry of and choice of shape functions for the macroscale finite element. The derived force vector, f̃, is a direct outcome of the usual kinematic relations used to bridge the nodal displacements across the macroscale and microscale. We also show how this Direct FE2 can be carried out as a single simulation on any commercial FE software that supports multipoint constraints (MPC). The Direct FE2models are shown to give similar results to full FE meshes of heterogeneities throughout the entire domain with significantly less degrees of freedom. We further demonstrate that in-built capabilities of the commercial codes are naturally available with Direct FE2 through examples involving large deformation, plasticity and viscoelasticity.}
}
@incollection{DEWAELE198817,
title = {Lily: A Software Package for Image Processing},
editor = {Edzard S. GELSEMA and Laveen N. KANAL},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {7},
pages = {17-33},
year = {1988},
booktitle = {Pattern Recognition and Artificial Intelligence},
issn = {0923-0459},
doi = {https://doi.org/10.1016/B978-0-444-87137-4.50007-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444871374500072},
author = {P. Dewaele and D. {van den Oudenhoven} and J. Vandeneede and R. Bartels and P. Wambacq and A. Oosterlinck},
abstract = {In this paper we will present LILY, a software package for image processing. LILY stands for Leuven Image processing LibrarY and has been developed with funding from a large number of Belgian industrial companies who are all interested in visual inspection problems. Therefore, the algorithms in the package are mainly concerned with this application field, although also other algorithms are incorporated. All procedures have been written in Pascal and Fortran on a VAX running VMS, with current work being done to convert the package to the C-language. Along with the programmed algorithms come a large number of support functions and procedures to facilitate the development of image processing programs. The package thus presents itself to the user as a large toolbox from which the appropriate tools must be taken to accomplish a certain task. The available procedures fall in one of the following categories: segmentation, coding, filtering, feature extraction, classification, texture analysis, relaxation, and pyramidal structures. In the first part of the paper, an overview will be given of the algorithms that are present in all these different classes. Syntactical conventions, documentation, maintenance and development tools will be discussed also. The second part of the paper is devoted to some specific problems that have been solved as a testcase for the algorithms. First Laws' procedure for texture segmentation is implemented and applied to the detection of defects in textiles. The procedure involves filtering the image with one or more suitably chosen masks, squaring the obtained values, computing the energy as a texture feature and classifying the resulting values. Each of these steps corresponds more or less to a different module of the package. A second industrial problem of defect inspection in unexposed radiographic film has been approached using three alternative techniques: one dimensional convolution filtering, Fourier domain filtering and polynomial regression.}
}
@article{CHLAMTAC1986145,
title = {A concurrent network simulator for automated protocol development and performance evaluation},
journal = {Microprocessing and Microprogramming},
volume = {18},
number = {1},
pages = {145-152},
year = {1986},
note = {Microarchitectures, Developments and Applications},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(86)90038-4},
url = {https://www.sciencedirect.com/science/article/pii/0165607486900384},
author = {Imrich Chlamtac},
abstract = {In this paper we describe the CONSIP modeling and simulation environment constructed for aiding in automated development of protocols and in network performance evaluation. CONSIP provides concurrent execution of processes tailored to the process scheduling philosophy of protocols. It accepts the protocol prototypes as input in the way they are specified for implementation and verification purposes and then executes protocol replication at different stations as requested. The user of the CONSIP package does not therefore have to “translate” the modeled real system into a given conceptual modeling framework. Instead the network model is generated by virtually copying the network specification. In this way the level of simulation validity and verification is increased significantly by saving the need for translation at both ends: at simulation model generation and at the implementation phase of the simulated protocols. The package is modular, allowing different protocols layers to be modeled independently, thus emulating the software design approach found in the real world communication systems. From considerations of execution efficiency multiple protocols layers can be lumped in the model by being described, possibly analytically, as virtual servers of the adjoining protocols. Lastly, CONSIP provides a multi-level user dialogue ranging from interactive network specification at the highest level, to event by event trace at its lowest level. In summary theefore the package thus provides a user friendly and efficient environment for automated software development of network protocols as well as system performance evaluation.}
}
@incollection{SHIVAKUMAR2015101,
title = {3 - Optimizing Performance of Enterprise Web Application},
editor = {Shailesh Kumar Shivakumar},
booktitle = {Architecting High Performing, Scalable and Available Enterprise Web Applications},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {101-141},
year = {2015},
isbn = {978-0-12-802258-0},
doi = {https://doi.org/10.1016/B978-0-12-802258-0.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128022580000032},
author = {Shailesh Kumar Shivakumar},
keywords = {Performance modeling, performance-based design, bottleneck analysis, continuous monitoring, performance governance, performance modeling, workload modeling, performance patterns, performance design guidelines, caching, distributed and parallel computing, lightweight design, asynchronous and on-demand data request, batching, standards-based technology, continuous and iterative build and testing, HTML 5 optimization, responsive web design (RWD), smart asset proxy, semantic progressive loading, rapid rendering framework techniques, content chunking strategy, monitoring, web analytics-based monitoring},
abstract = {Fast and responsive web pages are critical to success factors of a website. An optimized web provides the competitive advantage and increases the overall user experience. It has direct impact on business drivers such as site traffic, conversion ratio, and user satisfaction index. With the proliferation of ubiquitous devices and mobile platforms, the speed of the web is more relevant than ever. Web performance optimization (WPO) is mainly related to making the web components, such as web pages, perform faster and render in the most optimal way on the user device so as to provide a positive user experience. Web components include web pages, widgets, client-side components, JavaScript libraries, static assets, and others. Normally, for a web application, the web page forms the final component in the entire delivery chain. This chapter elaborates a complete strategy for WPO. The chapter takes a 360° approach in analyzing various dimensions of WPO, including performance optimization principles at each of the project lifecycle phases, common pitfalls in page development, caching strategy, monitoring and maintenance strategy, infrastructure guidelines, and fine-tuning existing web pages. The chapter also elaborates performance metrics, performance governance framework, and performance techniques for the entire ecosystem. Finally, we see the tools that can be leveraged for implementing the strategies discussed. The optimization techniques explained in this chapter are drawn from various real-world performance engineering programs where the strategy was successfully implemented to meet the challenging performance SLAs across geographies. Performance engineering is a multilayer exercise involving software components at various layers in an n-tier architecture. Equally, hardware also plays a key role in ensuring optimal performance for the application. This chapter predominantly focuses on the web tier wherein the optimization techniques for presentation components are discussed in detail. There is a brief section related to performance best practices that can be implemented at other layers.}
}
@article{MARIMUTHU2013131,
title = {Development of efficient finite elements for structural integrity analysis of solid rocket motor propellant grains},
journal = {International Journal of Pressure Vessels and Piping},
volume = {111-112},
pages = {131-145},
year = {2013},
issn = {0308-0161},
doi = {https://doi.org/10.1016/j.ijpvp.2013.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0308016113001063},
author = {R. Marimuthu and B. {Nageswara Rao}},
keywords = {Rocket motors, Incompressible materials, Visco-elastic, Herrmann formulation, Solid propellant grains},
abstract = {Solid propellant rocket motors (SRM) are regularly used in the satellite launch vehicles which consist of mainly three different structural materials viz., solid propellant, liner, and casing materials. It is essential to assess the structural integrity of solid propellant grains under the specified gravity, thermal and pressure loading conditions. For this purpose finite elements developed following the Herrmann formulation are: twenty node brick element (BH20), eight node quadrilateral plane strain element (PH8) and, eight node axi-symmetric solid of revolution element (AH8). The time-dependent nature of the solid propellant grains is taken into account utilizing the direct inverse method of Schepary to specify the effective Young's modulus and Poisson's ratio. The developed elements are tested considering various problems prior to implementation in the in-house software package (viz., Finite Element Analysis of STructures, FEAST). Several SRM configurations are analyzed to assess the structural integrity under different loading conditions. Finite element analysis results are found to be in good agreement with those obtained earlier from MARC software.}
}
@article{STANEK2016169,
title = {A new approach to configurable primary data collection},
journal = {Computer Methods and Programs in Biomedicine},
volume = {133},
pages = {169-181},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715301772},
author = {J. Stanek and E. Babkin and M. Zubov},
keywords = {Genomic data, Variety, Extraction–transformation–loading, Customization, Manual entry optimization},
abstract = {Background and objectives
The formats, semantics and operational rules of data processing tasks in genomics (and health in general) are highly divergent and can rapidly change. In such an environment, the problem of consistent transformation and loading of heterogeneous input data to various target repositories becomes a critical success factor. The objective of the project was to design a new conceptual approach to configurable data transformation, de-identification, and submission of health and genomic data sets. Main motivation was to facilitate automated or human-driven data uploading, as well as consolidation of heterogeneous sources in large genomic or health projects.
Methods
Modern methods of on-demand specialization of generic software components were applied. For specification of input–output data and required data collection activities, we propose a simple data model of flat tables as well as a domain-oriented graphical interface and portable representation of transformations in XML. Using such methods, the prototype of the Configurable Data Collection System (CDCS) was implemented in Java programming language with Swing graphical interfaces. The core logic of transformations was implemented as a library of reusable plugins.
Results
The solution is implemented as a software prototype for a configurable service-oriented system for semi-automatic data collection, transformation, sanitization and safe uploading to heterogeneous data repositories—CDCS. To address the dynamic nature of data schemas and data collection processes, the CDCS prototype facilitates interactive, user-driven configuration of the data collection process and extends basic functionality with a wide range of third-party plugins. Notably, our solution also allows for the reduction of manual data entry for data originally missing in the output data sets.
Conclusions
First experiments and feedback from domain experts confirm the prototype is flexible, configurable and extensible; runs well on data owner's systems; and is not dependent on vendor's standards.}
}
@article{PARIS2017163,
title = {Recent developments to evaluate global explosion loading on complex systems},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {46},
pages = {163-176},
year = {2017},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2017.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950423017301225},
author = {Laurent Paris and Anais Dubois},
keywords = {Explosion, Design, Risk, CFD, FLACS, Global loading, Blast load cases (BLC)},
abstract = {During the engineering phase, a “Design Explosion loads Specification” is often developed by the safety discipline in order to provide the necessary explosion response inputs to other engineering disciplines for each individual item part of a safety critical system. This includes the specific targets, the associated performance criteria and the corresponding design explosion loads. This is an efficient way to manage explosion in design for each individual item composing a safety critical system but when combination of items need to be addressed, for instance global loading on complex items (e.g. modules, critical pipework or packages), this approach may result in an overly conservative design if the maximum explosion loads on each item are summed simultaneously. Indeed each component may experience variability in loading time due to the propagation of blast wave during the explosion event. In the opposite, only considering explosion loads on each individual item successively may be not safe enough. An alternative methodology based on Computational Fluid Dynamics (CFD) FLACS® software simulations is presented in the article in order to get more adequate global blast loads for design verification, in particular taking into account potential shielding effects, group effects of elements. It focuses on the development of dedicated blast load cases for the design in order to address both internal and external explosion events related to complex items such as whole onshore units or offshore modules on floating facilities. This method will be favorably implemented on generic typical systems in order to develop blast loads cases combination rule sets on future projects. This will contribute to enhance blast design approaches and promote opportunities for further optimization.}
}
@article{TOMEK1986336,
title = {Assembler-monitor package to teach assembly language},
journal = {Microprocessors and Microsystems},
volume = {10},
number = {6},
pages = {336-339},
year = {1986},
issn = {0141-9331},
doi = {https://doi.org/10.1016/0141-9331(86)90274-7},
url = {https://www.sciencedirect.com/science/article/pii/0141933186902747},
author = {Ivan Tomek and Peter Steele},
keywords = {microprocessors, assembly language, simulator},
abstract = {The paper describes a software package designed to make the study of assembly language programming easier. The package, called AMS, consists of a set of programs that have been implemented for CP/M and MS-DOS systems. The main parts of each package are a text editor, a code generator, and a simulator. The simulator, which is the essential component of the package, simultaneously displays the source code being executed, memory and register contents, and timing information. In its single-step mode the simulator highlights the currently executing instruction both in source form and in hexadecimal code, and updates the screen after each individual instruction. AMS is related to debugging tools; therefore a comparison between AMS and two recently announced MS-DOS debuggers is given.}
}
@article{SALOGNI2010464,
title = {Modeling of solid oxide fuel cells for dynamic simulations of integrated systems},
journal = {Applied Thermal Engineering},
volume = {30},
number = {5},
pages = {464-477},
year = {2010},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2009.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1359431109002981},
author = {A. Salogni and P. Colonna},
keywords = {SOFC, Dynamic modeling, Causal approach, Acausal software, Integrated systems, Lumped parameters},
abstract = {Solid oxide fuel cell (SOFC) stacks are at the core of complex and efficient energy conversion systems for distributed power generation. Such systems are currently in various stages of development. These power plants of the future feature complicated configurations, because the fuel cell demands for a complex balance of plant. Moreover, proposed SOFC-based systems for stationary applications are often connected to additional components and subsystems, such as a gasifier with its gas-cleaning section, a gas turbine, and a heat recovery system for thermal cogeneration or additional power production. For the simplest SOFC configurations, and more so for complex integrated systems, the dynamic operation of the power plant is challenging, especially because the fluctuating electrical load of distributed energy systems demand for reliable transient operation. Issues related to dynamic operation must be studied in the early design stage and simulation results can be used to optimize the system configuration, taking into account transient behavior. This paper presents the development and the validation of a non-linear dynamic lumped-parameters model of a SOFC stack suitable for integration into models of complex power plants. Particular emphasis is placed on the systematic approach to model development. The model is implemented using the open-source Modelica language, which allows for a high degree of flexibility and modularity, the main features of the model herein presented. The SOFC stack model will be incorporated into ThermoPower, a freely distributed library of reusable software components for the modeling of thermo-hydraulic processes and power plants.}
}
@article{GENIUS2013455,
title = {Space optimal solution for data reordering in streaming applications on NoC based MPSoC},
journal = {Journal of Systems Architecture},
volume = {59},
number = {7},
pages = {455-467},
year = {2013},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2013.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762113000507},
author = {Daniela Genius and Alix {Munier Kordon} and Khouloud {Zine el Abidine}},
keywords = {Kahn process network, Stream processing, Hardware/software co-design},
abstract = {Many streaming applications feature coarse grain task farm or pipeline parallelism and can be modeled as a set of parallel threads. Performance requirements can often only be met by mapping the application onto a Multi Processor System-on-Chip (MPSoC). To avoid contention, hierarchical interconnection networks, where the central interconnect is a network-on-chip, are employed. In such a clustered MPSoC, the memory access latency varies strongly depending on the location of data, and is the principal cause of out-of-order arrival of data items. We present an algorithm which re-establishes the order of data items on the output side. If their earliness or lateness exceeds a limit previously fixed by experimentation, they are dropped, otherwise stored in a buffer. Write operations to this buffer are random access, whereas read operations are in FIFO order. Our algorithm guarantees that no data is removed from the buffer before it has been read, and, for a given throughput, minimum buffer size. The algorithm was implemented within the output co-processors for three application case studies and validated on a simulation platform based on the SoCLib library.}
}