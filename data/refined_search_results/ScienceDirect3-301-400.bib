@article{MOROZOV20061403,
title = {A generalized web service model for geophysical data processing and modeling},
journal = {Computers & Geosciences},
volume = {32},
number = {9},
pages = {1403-1410},
year = {2006},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2005.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0098300406000033},
author = {Igor Morozov and Brian Reilkoff and Glenn Chubak},
keywords = {Web service, Geophysics, Seismology, Processing},
abstract = {A web service model for geophysical data manipulation, analysis and modeling based on a generalized data processing system was implemented. The service is not limited to any specific data type or operation and allows the user to combine ∼190 tools of the existing package, and new codes easily includable. It allows remote execution of complex processing flows completely designed and controlled by remote clients who are presented with mirror images of the server processing environment. Clients are also able to upload their processing flows to the server, thereby building a knowledge base of processing expertise shared by the community. Flows in this knowledge base are currently represented by a hierarchy of automatically generated interactive web forms. These flows can be accessed and the resulting data retrieved by either using a web browser or through API calls from within the clients’ applications. The server administrator is thus relieved of the need for development of any content-specific data access mechanisms. The underlying processing system is fully developed and includes a graphical user interface, parallel processing capabilities, on-line documentation, on-line software distribution service and automatic code updates. Currently, the service is installed on the University of Saskatchewan seismology web server (http://seisweb.usask.ca/SIA/ps.php) and maintains a library of processing examples (http://seisweb.usask.ca/temp/examples) including a number of useful web tools (such as UTM coordinate transformations, calculation of travel times of seismic waves in a global Earth model and generation of color palettes). Important potential applications of this web service model for building intelligent data queries, processing and modeling of global seismological data are also discussed.}
}
@article{SHAHDAD2015881,
title = {Applying TRIZ to Graphic Design using Genetic Algorithms},
journal = {Procedia Engineering},
volume = {131},
pages = {881-891},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.399},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815042836},
author = {Mir Abubakr Shahdad and Paul Filmore},
abstract = {A CAI (Computer Aided Innovation) tool called AEGIS (Accelerated Evolutionary Graphic Interface System) is being developed and applied to graphic design. The tool, which uses TRIZ ‘guided’ algorithms, is being tested with graphic design experts. The AEGIS processes initial inputs (initial design ideas) and apply TRIZ based algorithms to them and outputs ‘innovative designs’. These designs may further be processed using manual techniques to achieve the final outputs desired by designers. This paper discusses version 6.6 of AEGIS. In this version, Genetic Algorithms (GA's) are being utilised. Genetic algorithms are one of the best ways to solve a problem for which little is known. They are a very general algorithm and so will work well in any search space. All you need to know is what you need the solution to be able to do well, and a genetic algorithm will often be able to create a high quality solution. Genetic algorithms use the principles of selection and evolution to produce several solutions to a given problem. This paper considers the various entities and structures used and developed to implement Genetic Algorithms through which TRIZ guided effects and mutations are applied here to graphic design elements. It attempts to code TRIZ Principles based mutations (function/method parameters) into genes. This paper discusses the detailed implementation of Genetic Algorithms and hence the translation and implementation of TRIZ Principles to Genetic Algorithm structure to obtain generations of phenotypes (Images). A complete input or output image in Graphic Design is generally considered to consist of four main components: the background, the logo (branding), the text on the packaging and the extra effect (extra logo or design). TRIZ ‘algorithms’ are applied to these components individually and then these components are recombined. This is achieved by implementing these components as layers. GA's are inspired by nature and evolution. Each organism is considered to be made of cells which in turn consist of chromosomes; chromosomes are made of genes (which are functional blocks of DNA). Each gene controls a particular property of an aspect or behaviour or part of the organism. The different possibilities which a property can have are called ‘Alleles’. Genes have a particular place in a chromosome (this is called ‘locus’). This paper discusses the detailed implementation of Genetic Algorithms and hence the translation and implementation of TRIZ Principles to Genetic Algorithm structure to obtain generations of phenotypes (Images). The first GA enabled version of AEGIS discussed here has three chromosomes[1]. Outputs are shown applied to fonts for the chromosomes implemented. A brief comparison of the time taken to produce designs using AEGIS compared to professional packages points to AEGIS producing large time savings. The outputs from the software i.e., designs, are promising, and have to some extent motivated professional graphic designers to accept that this tool has benefits in aiding them to produce more innovative designs or at least speed up this process. Some ‘design’ examples using GA's are shown in this paper. In the future a further 14 chromosomes are being implemented in AEGIS.}
}
@article{XIE2022111556,
title = {An efficient finite element iterative method for solving a nonuniform size modified Poisson-Boltzmann ion channel model},
journal = {Journal of Computational Physics},
volume = {470},
pages = {111556},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2022.111556},
url = {https://www.sciencedirect.com/science/article/pii/S0021999122006180},
author = {Dexuan Xie},
keywords = {Poisson-Boltzmann equation, Finite element method, Ion channel model, Electrostatics, Ion size effects, Membrane charges},
abstract = {In this paper, a nonuniform size modified Poisson-Boltzmann ion channel (nuSMPBIC) model is presented as a nonlinear system of an electrostatic potential and multiple ionic concentrations. It mixes nonlinear algebraic equations with a Poisson boundary value problem involving Dirichlet-Neumann mixed boundary value conditions and a membrane surface charge density to reflect the effects of ion sizes and membrane charges on electrostatics and ionic concentrations. To overcome the difficulties of strong singularities and exponential nonlinearities, it is split into three submodels with a solution of Model 1 collecting all the singular points and Models 2 and 3 much easier to solve numerically than the original nuSMPBIC model. A damped two-block iterative method is then presented to solve Model 3, along with a novel modified Newton iterative scheme for solving each related nonlinear algebraic system. To this end, an effective nuSMPBIC finite element solver is derived and then implemented as a program package that works for an ion channel protein with a three-dimensional molecular structure and a mixture of multiple ionic species. Numerical results for a voltage-dependent anion channel (VDAC) in a mixture of four ionic species demonstrate a fast convergence rate of the damped two-block iterative method, the high performance of the software package, and the importance of considering nonuniform ion sizes. Moreover, the nuSMPBIC model is validated by the anion selectivity property of VDAC.}
}
@article{HENRY2021100092,
title = {Gym-ANM: Open-source software to leverage reinforcement learning for power system management in research and education},
journal = {Software Impacts},
volume = {9},
pages = {100092},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100092},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000348},
author = {Robin Henry and Damien Ernst},
keywords = {Gym-ANM, Reinforcement learning, Active network management, Distribution networks, Renewable energy},
abstract = {Gym-ANM is a Python package that facilitates the design of reinforcement learning (RL) environments that model active network management (ANM) tasks in electricity networks. Here, we describe how to implement new environments and how to write code to interact with pre-existing ones. We also provide an overview of ANM6-Easy, an environment designed to highlight common ANM challenges. Finally, we discuss the potential impact of Gym-ANM on the scientific community, both in terms of research and education. We hope this package will facilitate collaboration between the power system and RL communities in the search for algorithms to control future energy systems.}
}
@article{VALERA2001527,
title = {Real-Time Robot Control Implementation with Matlab/Simulink},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {9},
pages = {527-532},
year = {2001},
note = {IFAC Conference on Telematics Applications in Automation and Robotics, Weingarten, Germany, 24-26 July 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41762-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017417629},
author = {A. Valera and M. Vallés and J. Tornero},
keywords = {Robot control, Nonlinear control, Real-time systems, Computer controlled systems, Industrial robots, Simulation},
abstract = {This presents a hardware platform and a programming environment for the design, analysis and implementation of industrial robots. The platform is compound with two industrial robots and a distributed computation software architecture based on PCs. For the programming environment a specific library of nonlinear robot controllers has been implemented and tested. The environment is developed over MATLAB/Simulink which provides modeling, simulating and the analyzing facilities for testing robot control strategies. The automatic generation of program code is carried out by the Real-Time Workshop and the Real-Time Windows Target toolboxes also provided by MATLAB. These toolboxes allow obtaining an implementation of the controllers over several Operating Systems. Finally, two industrial robotics have been used as physical set-up for testing the algorithms and the proposed architecture, as well as the facilities of the program environment.}
}
@incollection{KALAKUL2017979,
title = {The Chemical Product Simulator – ProCAPD},
editor = {Antonio Espuña and Moisès Graells and Luis Puigjaner},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {40},
pages = {979-984},
year = {2017},
booktitle = {27th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63965-3.50165-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639653501653},
author = {Sawitree Kalakul and Mario R. Eden and Rafiqul Gani},
keywords = {Chemical product design, Product-process simulator, blended products, formulations, single molecular products},
abstract = {In this paper, a chemical product design simulator called ProCAPD is presented. ProCAPD works in the same way as a chemical process simulator, that is, it helps to verify product design decisions and generates information that can be used to make design decisions. Like the contents of the process simulator, the product simulator needs a database of chemicals and properties, a library of models, numerical routines to solve mathematical problems as well as various calculation options. Also, like the process simulator, the product simulator comes with a user-interface to describe the problems and to obtain the simulation results. In order to make the chemical product simulator versatile and applicable for a wide range of problems, it includes a suite of databases (chemicals, solvents, active ingredients, aroma, color-agents and many more); a library of models (properties, product performance, etc.); calculation tools (product attributes, blend compositions, environmental impact, etc.); design templates (single molecules, blends, formulations, emulsions, devices); and, design-simulation-analysis functions. All these capabilities are based on the prototype tool developed by Kalakul et al. (2017). This paper highlights the software architecture, the implemented computer aided methods-tools, whose scope-significance are illustrated through new chemical product design-evaluation applications.}
}
@article{BENGHI2019102926,
title = {Automated verification for collaborative workflows in a Digital Plan of Work},
journal = {Automation in Construction},
volume = {107},
pages = {102926},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102926},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517310725},
author = {Claudio Benghi},
keywords = {Digital plan of work, Model verification, Model checking, Interdisciplinary collaboration, COBie},
abstract = {This paper presents the rationale, strategy and development behind the Verification components of the UK Digital Plan of Work (DPoW), a large-scale interdisciplinary collaboration platform implemented to enhance collaborative Building Information Modelling (BIM) workflows. Construction enterprises currently organise their information exchanges through documents that are not suitable for the automation of quality assurance processes. The DPoW addresses this issue with the introduction of a documented collaboration format and a suite of associated verification libraries, available at no cost as open source packages. The resulting infrastructure has the potential to automate the management of information exchanges in multi-disciplinary construction projects, but its uptake may be impeded by known issues that affect the dynamics of technology adoption. The verification infrastructure has been designed to reduce the impact of these issues by providing immediate paths to adoption and enabling three levels of process feedback for sustainable incremental improvements, starting from the current maturity level. The research followed the Design Science Methodology and is presented in this paper progressing from the definition of goals and objectives to the presentation of the developed solution, ending with a summary of known limitations and areas of further development.}
}
@incollection{NETZER2015503,
title = {Chapter 28 - Knowledge Discovery in Proteomic Mass Spectrometry Data},
editor = {Quoc Nam Tran and Hamid Arabnia},
booktitle = {Emerging Trends in Computational Biology, Bioinformatics, and Systems Biology},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {503-519},
year = {2015},
series = {Emerging Trends in Computer Science and Applied Computing},
isbn = {978-0-12-802508-6},
doi = {https://doi.org/10.1016/B978-0-12-802508-6.00028-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025086000284},
author = {Michael Netzer and Michael Handler and Bernhard Pfeifer and Andreas Dander and Christian Baumgartner},
keywords = {Proteomics, mass spectrometry, data preprocessing, feature selection, biomarker identification, computational workflow},
abstract = {High-throughput technologies such as mass spectrometry produce large amounts of data that require sophisticated computational methods to preprocess and identify highly discriminatory features (biomarker candidates) from these data. In this chapter, a computational workflow for the search and identification of biomarker candidates using mass spectrometry data is presented. First, preprocessing steps necessary to transform raw spectra into comparable data sets are described, followed by a novel three-step feature selection approach that combines the advantages of efficient filter and effective wrapper techniques. The proposed workflow has been integrated into the Knowledge Discovery in Databases (KD3) Designer tool, our self-designed and cost-free software package. One of the main advantages of this tool is its straightforward design, which visualizes processing steps that can be easily connected to workflows. Due to its modular software architecture, new algorithms can be readily implemented into the system. This analysis strategy was evaluated using an example mass spectrometry data set.}
}
@article{LANDI2021100228,
title = {reval: A Python package to determine best clustering solutions with stability-based relative clustering validation},
journal = {Patterns},
volume = {2},
number = {4},
pages = {100228},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100228},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000428},
author = {Isotta Landi and Veronica Mandelli and Michael V. Lombardo},
keywords = {stability-based relative validation, clustering, unsupervised learning, clustering replicability},
abstract = {Summary
Determining the best partition for a dataset can be a challenging task because of the lack of a priori information within an unsupervised learning framework and the absence of a unique clustering validation approach to evaluate clustering solutions. Here we present reval: a Python package that leverages stability-based relative clustering validation methods to select best clustering solutions as the ones that replicate, via supervised learning, on unseen subsets of data. The implementation of relative validation methods can contribute to the theory of clustering by fostering new approaches for the investigation of clustering results in different situations and for different data distributions. This work aims at contributing to this effort by implementing a package that works with multiple clustering and classification algorithms, hence allowing both the automation of the labeling process and the assessment of the stability of different clustering mechanisms.}
}
@article{LI201912,
title = {Acycle: Time-series analysis software for paleoclimate research and education},
journal = {Computers & Geosciences},
volume = {127},
pages = {12-22},
year = {2019},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0098300418308926},
author = {Mingsong Li and Linda Hinnov and Lee Kump},
keywords = {Data processing, Geology, Cyclostratigraphy, Stratigraphy, Sedimentology, Algorithms},
abstract = {Recognition and interpretation of paleoclimate signals in sedimentary proxy datasets are time consuming and subjective. Acycle is a comprehensive and easy-to-use software package for time series analysis in paleoclimate research and education. It is designed to speed paleoclimate time series analysis, especially cyclostratigraphy, and to provide objective methods for estimating astrochronology. Acycle provides for detrending with multiple options to track and remove secular trends. A selection of power spectral analysis methodologies is offered for the detection of periodic signals. Many of the functions are specific to cyclostratigraphy and astrochronology that are not found in standard statistical packages. A specialized function is provided to assess the astronomical (Milankovitch) forcing of paleoclimate series and search for the most likely sedimentation rate by evaluating the correlation coefficient between power spectra of an astronomical solution and sedimentary proxy data. Sedimentary noise modeling (for past sea-level changes) is also provided in Acycle. As an example, Acycle is applied to a sedimentary proxy series from the cyclostratigraphy of the Paleocene-Eocene thermal maximum (PETM) in Core BH9/05 from the Paleogene Central Basin, Svalbard. Acycle detects significant astronomical forcing in the proxy series and relatively stable sedimentation rates during and after the PETM. Acycle runs in the MATLAB environment or as stand-alone software on Windows and Macintosh OS X, and is open-source software.}
}
@article{MOLLAZADE2017597,
title = {LightScatter: A comprehensive software package for non-destructive monitoring of horti-food products by monochromatic imaging-based spatially-resolved light scattering technology},
journal = {Computers and Electronics in Agriculture},
volume = {142},
pages = {597-606},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917310219},
author = {Kaveh Mollazade and Arman Arefi},
keywords = {Backscattering, Farrell, Prediction, Quality inspection, Real-time},
abstract = {The horticulture and food industry are faced with a range of control measures to supply good quality and safe products to the market. Over the past four decades, a number of techniques have been developed to measure such control criteria in a non-destructive manner. This paper presents a new comprehensive software developed with MATLAB to help scientists and experts to easily and quickly implement light scattering imaging technology, as a non-invasive tool, in the horticulture and food industry. LightScatter is designed as a user friendly software so that interaction with it is done via a coding-free modern graphical user interface (GUI). This software package is equipped with various advanced tools for image acquisition, image pre-processing, feature extraction, and prediction/classification model generation. LightScatter also provides a powerful tool as a standalone application for use of light scattering imaging in the real-time mode. The latter can be activated manually by the user or automatically by an external trigger signal. The software response can be displayed on the monitor screen or transmitted over a serial port for further analysis or for activating electro-mechanical actuators. By the use of this software, light scattering imaging technology can be implemented in both the stationary mode, like a laboratory monitoring tool, and in the dynamic stop and go measuring mode, like sorting/grading machines. An updated LightScatter version is made publicly available and can be obtained upon request by sending an e-mail to the corresponding author.}
}
@article{FARRUSSENG200952,
title = {Virtual screening of materials using neuro-genetic approach: Concepts and implementation},
journal = {Computational Materials Science},
volume = {45},
number = {1},
pages = {52-59},
year = {2009},
note = {Selected papers from the E-MRS 2007 Fall Meeting Symposium G: Genetic Algorithms in Materials Science and Engineering},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2008.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S0927025608003078},
author = {David Farrusseng and Frederic Clerc and Claude Mirodatos and Ricco Rakotomalala},
keywords = {Combinatorial chemistry, High throughput screening, Library design, Genetic algorithm, Iterative optimization, Knowledge discovery, Catalysis},
abstract = {We describe the concepts and an implementation of a quantitative structure activity relationship methodology for the discovery of solid materials. The paper focuses on virtual screening methodologies. We have investigated the integration of artificial neural network with genetic algorithm in order to accelerate the search of optima. The learning model monitors the optimization process while it is updated in the course of the screening. The optimization is performed on the basis of catalytic data collected in a large high throughput experimentation campaign. The implementation is carried out in a free software (OptiCat).}
}
@article{GIULIANI201839,
title = {π-BEM: A flexible parallel implementation for adaptive, geometry aware, and high order boundary element methods},
journal = {Advances in Engineering Software},
volume = {121},
pages = {39-58},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818300371},
author = {Nicola Giuliani and Andrea Mola and Luca Heltai},
keywords = {BEM, Fast multiple method, High order elements, Local refinement, MPI, Multi-threaded, OpenSOURCE, CAD},
abstract = {Many physical phenomena can be modelled using boundary integral equations, and discretised using the boundary element method (BEM). Such models only require the discretisation of the boundary of the domain, making the setup of the simulation straightforward and lowering the number of degrees of freedom. However, while many parallel efficient libraries are available for the Finite Element Method (FEM), the implementation of scalable BEM solvers still poses many challenges. We present the open source framework π-BEM (where π stands for parallel): a novel boundary element method solver, combining distributed and shared memory paradigms to achieve high scalability. π-BEM exploits high performance libraries and graph partitioning tools to deliver a parallel solver employing automatic domain decomposition, high order elements, local refinement capabilities, and exact geometry-adaptivity (using CAD files). A preliminary fast multipole accelerator is included in the implementation. Every aspect of the library is modular and easily extendible by the community. We discuss the internal structure of the code, and present some examples to demonstrate the reliability and scalability of our implementation.}
}
@article{HU2021104378,
title = {Implementation and validation of true material constitutive model for accurate modeling of thick-walled cylinder swage autofrettage},
journal = {International Journal of Pressure Vessels and Piping},
volume = {191},
pages = {104378},
year = {2021},
issn = {0308-0161},
doi = {https://doi.org/10.1016/j.ijpvp.2021.104378},
url = {https://www.sciencedirect.com/science/article/pii/S0308016121000764},
author = {Zhong Hu and Anthony P. Parker},
keywords = {Constitutive behavior, User-programmable function, USERMAT, Swage autofrettage, Thick-walled cylinder, Residual stress, Neutron diffraction},
abstract = {The fatigue life prediction and associated fail-safe design of autofrettaged tubes require accurate stress modeling of each specific high-strength thick-walled cylinder during and after the autofrettage process, which depends largely on the elastoplastic behavior of the original steel tubes. However, the complex material behavior is dominated by the Bauschinger effect. Modeling swage autofrettage of such steel tubes remains problematic. This is a crucial issue that has not been resolved by any available modeling software. Therefore, the modeling and design of the autofrettage process for current and future materials depends upon extending the capability of current software. This research aims to develop a user-programmable function (UPF), USERMAT, to realize a user-defined material model that features true material constitutive behavior with the corresponding algorithms for accurate stress analysis of high strength thick-walled cylinders during the autofrettage process. The material constitutive model to be implemented can be formulated with high accuracy via an available material characterization, which can adapt to both nonlinear and linear strain hardening during the initial tensile loading, while the elastic modulus can be reduced during reversed-loading. It also incorporates the Bauschinger effect as a function of the prior tensile plastic strain during the nonlinear compressive reversed-loading phase and fits the experimental data of the true material model. Swage autofrettaged thick-walled cylinder case studies were reported. The results for residual radial and hoop stress and for elastic strain components were compared with independent, available neutron diffraction measurements, and they are in good agreement. Developing and integrating such a UPF into a standard finite element analysis software package is potentially the most significant unsolved fundamental modeling issue relating to re-yielding, fracture and fatigue of modern autofrettaged cylinders and pressure vessels. It can not only provide a fundamental understanding of the deformation mechanism and stress development within the high strength steel tubes during the autofrettage process, but also provide guidance for the design and optimization of the autofrettage manufacturing processes and of material selection for high intensity pressure vessels, a potential market of billions of dollars.}
}
@article{CARVALHO201874,
title = {On the implementation of dynamic software product lines: An exploratory study},
journal = {Journal of Systems and Software},
volume = {136},
pages = {74-100},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302601},
author = {Michelle Larissa Luciano Carvalho and Matheus Lessa Gonçalves {da Silva} and Gecynalda Soares da Silva Gomes and Alcemir Rodrigues Santos and Ivan do Carmo Machado and Magno Luã de Jesus Souza and Eduardo Santana {de Almeida}},
keywords = {Dynamic software product lines, Variability mechanisms, Software evolution, Evidence-based software engineering},
abstract = {Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.}
}
@article{VOLANSCHI2012609,
title = {Pattern matching for the masses using custom notations},
journal = {Science of Computer Programming},
volume = {77},
number = {5},
pages = {609-635},
year = {2012},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2011.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167642311002243},
author = {Nic Volanschi},
keywords = {Pattern matching, Data notations, Customization},
abstract = {For many programmers, the notion of “pattern matching” evokes nothing more than regular expressions for matching unstructured text, or technologies such as XPath to match semi-structured data in XML. This common perception of pattern matching is partly due to the success of regular expressions and XPath, which are supported in many popular programming languages today, either as standard libraries or as part of the language. But it is also due to the fact that many programmers never used another elegant form of pattern matching—on structured data, i.e., the native data structures of a programming language. This form of matching is common in functional or logic languages used in the research, but unfortunately much less used in the software industry. It is indeed very surprising that none of the popular languages in use today support, in their standard form, a nearly general form of structured data matching, decades after this technology has been discovered and continuously improved. This paper shows that programmers do not have to wait for next generation languages to integrate pattern matching, neither need they use non-standard pre-processors, thereby losing some advantages that are most important in an industrial setting: official support, compatibility, standardization, etc. Instead, pattern matching of native data in custom notations can be implemented as a minimalist library in popular object languages. Thus, some of the comfortable existing notations from logic languages can be reused, existing standard notations for structured data such as JSON (JavaScript Object Notation) can be smoothly extended to support pattern matching, and new notations can be designed. As in most library implementations of regular expressions, custom notation patterns are simply represented as strings. They can be used in two different modes: interpreted and compiled. This paper presents two open-source implementations of custom matching notations, for Java and JavaScript, exhibiting a reasonable overhead compared to other forms of pattern matching.}
}
@article{PANJAPORNPON20081569,
title = {Differential-geometric model-based control (DGMBC): A software package for controller design},
journal = {Computers & Chemical Engineering},
volume = {32},
number = {7},
pages = {1569-1588},
year = {2008},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2007.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0098135407002001},
author = {Chanin Panjapornpon and Masoud Soroush and Warren D. Seider},
keywords = {Controller design software, Model-based control, Input–output linearization, Feedback linearization, Numerical simulation, Computer-aided design software},
abstract = {This paper presents a new software package, called differential-geometric model-based control (DGMBC), which carries out symbolic manipulations to automatically generate differential-geometric, model-based controllers and subsequently tests the designed controller. This prototype software was developed to simplify the industrial implementation and testing of differential-geometric, model-based controllers on lumped-parameter processes. DGMBC has a user-friendly interface that allows a user to enter process model equations and parameters easily. The user interface was developed using Visual Basic and linked to MATHEMATICA using MathLink. The user enters the process model (set of ordinary differential equations), and the software generates an analytical model-based controller, if an analytical solution exists. The resulting analytical model-based controller (set of ordinary differential and algebraic equations) can be in FORTRAN, C, or MATLAB format. DGMBC can also simulate the closed-loop process responses. The application and implementation of DGMBC 1.0 are shown using three chemical and biochemical process examples with varying levels of complexity. An analytical model-based controller is designed for each of the processes, and simulation results showing the closed-loop process responses are presented.}
}
@article{GERVASI2004703,
title = {SIMBEX: a portal for the a priori simulation of crossed beam experiments},
journal = {Future Generation Computer Systems},
volume = {20},
number = {5},
pages = {703-715},
year = {2004},
note = {Computational Chemistry and Molecular Dynamics},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2003.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03002498},
author = {O. Gervasi and A. Laganà},
keywords = {Internet portal, Molecular simulator, Computing Grid, Reaction dynamics},
abstract = {The architecture and the computational kernels of Simulation of Crossed Molecular Beam Experiments, an Internet portal managing the simulation of elementary bimolecular processes as those occurring in crossed beam apparatuses, is discussed. The construction of this portal is our contribution to project 003/001 of the COST in Chemistry action D23 (METACHEM: Metalaboratories for complex computational applications in chemistry). The portal is specifically designed to support the collaborative efforts of various Computational Chemistry and Computer Science European laboratories aimed at building an a priori molecular simulator based on a Grid infrastructure. Such an environment makes use of free-software packages and was implemented using Web technologies.}
}
@article{LI2004555,
title = {A new approach to image-based realistic architecture modeling with featured solid library},
journal = {Automation in Construction},
volume = {13},
number = {5},
pages = {555-564},
year = {2004},
note = {Current IT Research and Development in the Construction Industry of China},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2004.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0926580504000238},
author = {Hua Li and Weiyu Wu},
keywords = {Image-based, Architecture modeling, Featured component library},
abstract = {To overcome restrictions of present modeling techniques, and to approach quick and convenient 3D architecture modeling, we present a realistic modeling system on the basis of a featured solid library. The system incorporates reference library techniques to build up realistic models from a single image. In our approach, parameterized solid components are mapped to 2D featured projection graphs on image plane, and matching features of all component graphs with the resource image results in 3D realistic scene, whether complex or irregular. Besides, a suite of texture toolkit is implemented to rectify flaws of texture mapping and to enhance modeling reality. Our approach works feasibly and robustly with single-vision images taken from arbitrary view angles, and the efficiency of our approach is proved in systematic modeling experiments. As our approach complemented image-based modeling techniques in construction CAD and digital city systems, it could be of popular use in relevant modeling software.}
}
@article{MULLER2001369,
title = {Towards integration of clinical decision support in commercial hospital information systems using distributed, reusable software and knowledge components},
journal = {International Journal of Medical Informatics},
volume = {64},
number = {2},
pages = {369-377},
year = {2001},
issn = {1386-5056},
doi = {https://doi.org/10.1016/S1386-5056(01)00218-0},
url = {https://www.sciencedirect.com/science/article/pii/S1386505601002180},
author = {Marcel Lucas Müller and Thomas Ganslandt and Hans Peter Eich and Konrad Lang and Christian Ohmann and Hans-Ulrich Prokosch},
keywords = {Hospital information systems, Decision support systems, Clinical decision making, Computer-assisted expert systems, Medical informatics applications, Computers},
abstract = {Problem: Clinicians' acceptance of clinical decision support depends on its workflow-oriented, context-sensitive accessibility and availability at the point of care, integrated into the Electronic Patient Record (EPR). Commercially available Hospital Information Systems (HIS) often focus on administrative tasks and mostly do not provide additional knowledge based functionality. Their traditionally monolithic and closed software architecture encumbers integration of and interaction with external software modules. Our aim was to develop methods and interfaces to integrate knowledge sources into two different commercial hospital information systems to provide the best decision support possible within the context of available patient data. Methods: An existing, proven standalone scoring system for acute abdominal pain was supplemented by a communication interface. In both HIS we defined data entry forms and developed individual and reusable mechanisms for data exchange with external software modules. We designed an additional knowledge support frontend which controls data exchange between HIS and the knowledge modules. Finally, we added guidelines and algorithms to the knowledge library. Results: Despite some major drawbacks which resulted mainly from the HIS' closed software architectures we showed exemplary, how external knowledge support can be integrated almost seamlessly into different commercial HIS. This paper describes the prototypical design and current implementation and discusses our experiences.}
}
@article{SAFEEA2020133,
title = {Model-based hardware in the loop control of collaborative robots},
journal = {Procedia Manufacturing},
volume = {51},
pages = {133-139},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920318758},
author = {Mohammad Safeea and Pedro Neto and Richard Béarée},
keywords = {Collaborative Robots, Model-based design, Simulink, Hardware in the loop control},
abstract = {Simulation and model-based design software packages are widely used in many engineering disciplines. When it comes to robotics those tools are very important for robot design, simulation and the development of control algorithms before the implementation on the real robot. Simulink by MathWorks® is an advanced model-based design tool. It is popular in education, industry and research. In addition, Simulink supports several hardware components, facilitating a rapid deployment of the developed programs on the target hardware. In this study, the SimulinkIIWA interface for controlling KUKA iiwa robots from Simulink is presented and compared to the KUKA Sunrise Toolbox (KST). This interface is based on the User Datagram Protocol (UDP) and allows graphical real-time control of iiwa robots from Simulink without a need for writing any code. The interface supports different robot control modes, at the joints level and at the end-effector (EEF) level. Example applications are also provided showing the flexibility and the ease of use of the proposed interface.}
}
@incollection{HARRINGTON2009181,
title = {Chapter 10 - Using CASE Tools for Database Design},
editor = {Jan L. Harrington},
booktitle = {Relational Database Design (Third Edition)},
publisher = {Morgan Kaufmann},
edition = {Third Edition},
address = {Boston},
pages = {181-196},
year = {2009},
isbn = {978-0-12-374730-3},
doi = {https://doi.org/10.1016/B978-0-12-374730-3.00012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123747303000127},
author = {Jan L. Harrington},
abstract = {Publisher Summary
A computer-aided software engineering (CASE) tool is a software package that provides support for the design and implementation of information systems. It can document a database design and provide invaluable help in maintaining the consistency of a design. By integrating many of the techniques used to document a system design—including the data dictionary, data flows, and entity relationships—CASE software can increase the consistency and accuracy of a database design. It can also ease the task of creating the diagrams that accompany a system design. There are many CASE tools on the market. The actual look of the diagrams is specific to each particular package. However, it should always be kept in mind that although some current CASE tools can verify the integrity of a data model, they cannot design the database. There is no software in the world that can examine a database environment and identify the entities, attributes, and relationships that should be represented in a database. The model created with CASE software is therefore only as good as the analysis of the database environment provided by the people using the tool.}
}
@article{BALATON2013335,
title = {Operator training simulator process model implementation of a batch processing unit in a packaged simulation software},
journal = {Computers & Chemical Engineering},
volume = {48},
pages = {335-344},
year = {2013},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2012.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098135412002839},
author = {M.G. Balaton and L. Nagy and F. Szeifert},
keywords = {Batch, Batch processing unit, OTS, UniSim Design, Monofluid thermoblock, Thermometer models},
abstract = {In chemical industry, especially in the case of continuous processes, operator training simulators (OTS) are becoming widely used. With the help of these systems several operation and safety issues can be analysed, and the operating staff of the plant can be trained for handling different plant failures. The main part of the OTS is the process model that replaces the real technology. Hence, in control development the simulated process variables are required to be reasonably accurate. The paper presents the structure of the process model of a batch processing unit in UniSim Design, different model constructions of a jacketed batch reactor and the identification of the parameters affecting its hydrodynamic and thermal behaviour. Construction of the process model is the first step in developing the OTS of the pilot plant located in the authors’ laboratory. It can be an effective tool in the development of model-based control systems.}
}
@article{MOSSES201939,
title = {Software meta-language engineering and CBS},
journal = {Journal of Computer Languages},
volume = {50},
pages = {39-48},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.jvlc.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X18302209},
author = {Peter D. Mosses},
keywords = {Semantics of programming languages, Meta-languages, Modularity},
abstract = {The SLE conference series is devoted to the engineering principles of software languages: their design, their implementation, and their evolution. This paper is about the role of language specification in SLE. A precise specification of a software language needs to be written in a formal meta-language, and it needs to co-evolve with the specified language. Moreover, different software languages often have features in common, which should provide opportunities for reuse of parts of language specifications. Support for co-evolution and reuse in a meta-language requires careful engineering of its design. The author has been involved in the development of several meta-languages for semantic specification, including action semantics and modular variants of structural operational semantics (MSOS, I-MSOS). This led to the PLanCompS project, and to the design of its meta-language, CBS, for component-based semantics. CBS comes together with an extensible library of reusable components called ‘funcons’, corresponding to fundamental programming constructs. The main aim of CBS is to optimise co-evolution and reuse of specifications during language development, and to make specification of language semantics almost as straightforward as context-free syntax specification. The paper discusses the engineering of a selection of previous meta-languages, assessing how well they support co-evolution and reuse. It then gives an introduction to CBS, and illustrates significant features. It also considers whether other current meta-languages might also be used to define an extensible library of funcons for use in component-based semantics.}
}
@article{BURGUENO201982,
title = {Specifying quantities in software models},
journal = {Information and Software Technology},
volume = {113},
pages = {82-97},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301156},
author = {Loli Burgueño and Tanja Mayerhofer and Manuel Wimmer and Antonio Vallecillo},
keywords = {Model-based engineering, Modeling physical quantities, Measurement uncertainty, Dimensions, Units},
abstract = {Context
An essential requirement for the design and development of any engineering application that deals with real-world physical systems is the formal representation and processing of physical quantities, comprising both measurement uncertainty and units. Although solutions exist for several programming languages and simulation frameworks, this problem has not yet been fully solved for software models.
Objective
This paper shows how both measurement uncertainty and units can be effectively incorporated into software models, becoming part of their basic type systems.
Method
We introduce the main concepts and mechanisms needed for representing and handling physical quantities in software models. More precisely, we describe an extension of basic type Real, called Quantity, and a set of operations defined for the values of that type, together with a ready-to-use library of dimensions and units, which can be added to any modeling project.
Results
We show how our approach permits modelers to safely represent and operate with physical quantities, statically ensuring type- and unit-safe assignments and operations, prior to any simulation of the system or implementation in any programming language.
Conclusion
Our approach improves the expressiveness and type-safety of software models with respect to measurement uncertainty and units of physical quantities, and its effective use in modeling projects of physical systems.}
}
@article{REZENDE2015311,
title = {BR-Sensor: An On-line Data-driven Soft Sensor of Downhole Pressure∗∗Final support from Petrobas S.A. is acknowledged.},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {6},
pages = {311-316},
year = {2015},
note = {2nd IFAC Workshop on Automatic Control in Offshore Oil and Gas Production OOGP 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.08.049},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315009167},
author = {Edson F.A. Rezende and Alex F. Teixeira and Eduardo M.A.M. Mendes},
keywords = {Softsensors, object-oriented, software-design, Kalman-Filters, dynamic-link-library},
abstract = {In this work an object-oriented approach for implementing soft sensor components, encapsulated as dynamic-link-library (DLL), is proposed. A variety of models used for the estimation of a specific unmeasured process variable and implemented as DLL components is integrated to the soft sensor main component by simply following the interface described in this paper. In order to check the performance of the proposed approach a computational analysis is carried out. Although the main objective is to deploy the soft sensors into the supervisory the main ideas laid out here can be extended to other Windows-based solutions.}
}
@article{NOLTING2019391,
title = {Dynamic self-reconfiguration of a MIPS-based soft-core processor architecture},
journal = {Journal of Parallel and Distributed Computing},
volume = {133},
pages = {391-406},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517302678},
author = {Stephan Nolting and Guillermo Payá-Vayá and Florian Giesemann and Holger Blume and Sebastian Niemann and Christian Müller-Schloer},
keywords = {Reconfigurable computing, Soft-core processor, Self-optimizing system, Dynamic partial reconfiguration},
abstract = {The rising demands for computational performance are a permanent trend in our increasingly digital world. Consistently addressing this trend poses a challenge for every embedded processor system. This paper proposes the use of reconfigurable processor architectures to increase “on demand” processing performance while running a specific target application. The reconfiguration is used to interchange specialized co-processors attached to a static soft-core processor during run-time. Different self-optimization software–hardware substitution mechanisms, inspired by the field of organic computing, are implemented and evaluated using two different synthetic benchmarks and an exemplary application from the field of parallel robotics. An efficient self-optimization can be reached by combining a speed-up-based replacement strategy for scheduling the reconfigurable co-processors and a least mean square optimization algorithm without requiring any a-priori application profiling. For a reduced number of reconfigurable co-processors, the results show that the proposed software–hardware reconfiguration strategy provides, in general, better trade-offs between the required hardware resources and performance improvement when compared to the equivalent soft-core processor with the same number of static co-processors.}
}
@article{STEELE2001407,
title = {Resource modeling for the integration of the manufacturing enterprise},
journal = {Journal of Manufacturing Systems},
volume = {19},
number = {6},
pages = {407-427},
year = {2001},
issn = {0278-6125},
doi = {https://doi.org/10.1016/S0278-6125(01)80012-1},
url = {https://www.sciencedirect.com/science/article/pii/S0278612501800121},
author = {Jay Steele and Young-Jun Son and Richard A. Wysk},
keywords = {Information Technology, Manufacturing Systems, Computer Control},
abstract = {Researchers are beginning to develop taxonomies of different knowledge domains in order to specify the requirements of engineering functions in various manufacturing enterprises. These interrelated functions include product design, process planning, capacity planning, production costing, quality control, acquisition or reconfiguration of resources, planning and scheduling of shop activities, and execution of shop activities. These taxonomies and functions are not typically integrated in today's manufacturing enterprise. This results in inefficient manual transfer of knowledge between domains and the unavailability of critical information required for decision making. Object-oriented design methodologies are useful for modeling diverse information and behavior. Furthermore, planning, analysis, and control of resources such as machine tools, fixtures, and tooling increasingly dominate the engineering functions. This paper demonstrates how to ingrate these functions with an object-oriented resource model that links information from different knowledge domains. These functions are implemented using different software packages that can easily access the common resource data because the data are embedded in the resource class structure. This resource model is based on software objects that have a one-to-one correspondence with physical objects. This resource model is illustrated using object-oriented software, but the model may also be applied to distributed object and agent architectures.}
}
@article{CROWL1997209,
title = {Configuration techniques for a validated plant},
journal = {ISA Transactions},
volume = {36},
number = {3},
pages = {209-218},
year = {1997},
issn = {0019-0578},
doi = {https://doi.org/10.1016/S0019-0578(97)00019-0},
url = {https://www.sciencedirect.com/science/article/pii/S0019057897000190},
author = {Thomas E. Crowl and Leslie C. Minnich},
keywords = {ISA S88.01, IEC 1131-3, Batch, Validation, Reusable modules, Configuration, Unit-relative},
abstract = {A major challenge in automating a validated plant is to maximize quality while optimizing the time and effort spent developing and validating the configuration. One method to achieve this optimization is to organize the detailed design and configuration effort into small pieces which can be managed more efficiently. If designed properly, the configuration can be constructed implementing reusable modules of code that are developed individually. As the reusable modules are completed, they are tested and stored in a software library under change control. ISA S88.01 models define a structure of modular entities that can be used to organize the design and configuration effort. The IEC 1131-3 configuration languages fit well into the S88.01 structures and provide a means to create, library, and reuse the change-controlled individual software modules. This paper focuses on the techniques available in IEC 1131-3 to implement generic reusable software modules for S88.01 control modules, phases and units.}
}
@article{SOTIRIADIS2016180,
title = {An inter-cloud bridge system for heterogeneous cloud platforms},
journal = {Future Generation Computer Systems},
volume = {54},
pages = {180-194},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15000400},
author = {Stelios Sotiriadis and Nik Bessis},
keywords = {Cloud computing, Inter-cloud, OpenStack, FI-LAB, Heterogeneous inter-cloud, Cloud interoperability},
abstract = {Over the years, more cloud computing systems have been developed providing flexible interfaces for inter-cloud interaction. This work approaches the concept of inter-cloud by utilizing APIs, open source specifications and exposed interfaces from cloud platforms such as OpenStack, OpenNebula and others. Despite other works in the area of inter-cloud, that are mainly resource management-centric, we focus on designing and developing a service-centric architecture. We implement an inter-cloud bridge system that is elastic, easy to be upgraded and managed. We develop a prototype composed not only from heterogeneous cloud platforms but also from independent cloud services. These are developed by different cloud service providers and offered as open source Software as a Service (SaaS). The proposed Inter-Cloud Mediation Service uses Future Internet SaaS such as a Context Broker for registrations and subscriptions to services and a Complex Event Processing engine for event management. We present an experimental analysis to show interactions with various heterogeneous cloud platforms and we evaluate the performance of inter-cloud services separately and as a whole.}
}
@article{TOUMAZI2018163,
title = {dfpk: An R-package for Bayesian dose-finding designs using pharmacokinetics (PK) for phase I clinical trials},
journal = {Computer Methods and Programs in Biomedicine},
volume = {157},
pages = {163-177},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717308349},
author = {A. Toumazi and E. Comets and C. Alberti and T. Friede and F. Lentz and N. Stallard and S. Zohar and M. Ursino},
keywords = {Dose-finding, Maximum tolerated dose, Pharmacokinetics, Phase I clinical trials, R package},
abstract = {Background and objective
Dose-finding, aiming at finding the maximum tolerated dose, and pharmacokinetics studies are the first in human studies in the development process of a new pharmacological treatment. In the literature, to date only few attempts have been made to combine pharmacokinetics and dose-finding and to our knowledge no software implementation is generally available. In previous papers, we proposed several Bayesian adaptive pharmacokinetics-based dose-finding designs in small populations. The objective of this work is to implement these dose-finding methods in an R package, called dfpk.
Methods
All methods were developed in a sequential Bayesian setting and Bayesian parameter estimation is carried out using the rstan package. All available pharmacokinetics and toxicity data are used to suggest the dose of the next cohort with a constraint regarding the probability of toxicity. Stopping rules are also considered for each method. The ggplot2 package is used to create summary plots of toxicities or concentration curves.
Results
For all implemented methods, dfpk provides a function (nextDose) to estimate the probability of efficacy and to suggest the dose to give to the next cohort, and a function to run trial simulations to design a trial (nsim). The sim.data function generates at each dose the toxicity value related to a pharmacokinetic measure of exposure, the AUC, with an underlying pharmacokinetic one compartmental model with linear absorption. It is included as an example since similar data-frames can be generated directly by the user and passed to nsim.
Conclusion
The developed user-friendly R package dfpk, available on the CRAN repository, supports the design of innovative dose-finding studies using PK information.}
}
@article{RABE20131,
title = {A scalable module system},
journal = {Information and Computation},
volume = {230},
pages = {1-54},
year = {2013},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2013.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0890540113000631},
author = {Florian Rabe and Michael Kohlhase},
abstract = {Symbolic and logic computation systems ranging from computer algebra systems to theorem provers are finding their way into science, technology, mathematics and engineering. But such systems rely on explicitly or implicitly represented mathematical knowledge that needs to be managed to use such systems effectively. While mathematical knowledge management (MKM) “in the small” is well-studied, scaling up to large, highly interconnected corpora remains difficult. We hold that in order to realize MKM “in the large”, we need representation languages and software architectures that are designed systematically with large-scale processing in mind. Therefore, we have designed and implemented the Mmt language – a module system for mathematical theories. Mmt is designed as the simplest possible language that combines a module system, a foundationally uncommitted formal semantics, and web-scalable implementations. Due to a careful choice of representational primitives, Mmt allows us to integrate existing representation languages for formal mathematical knowledge in a simple, scalable formalism. In particular, Mmt abstracts from the underlying mathematical and logical foundations so that it can serve as a standardized representation format for a formal digital library. Moreover, Mmt systematically separates logic-dependent and logic-independent concerns so that it can serve as an interface layer between computation systems and MKM systems.}
}
@article{CHANDRA2017348,
title = {Building Information Modeling in the Architecture-engineering Construction Project in Surabaya},
journal = {Procedia Engineering},
volume = {171},
pages = {348-353},
year = {2017},
note = {The 3rd International Conference on Sustainable Civil Engineering Structures and Construction Materials - Sustainable Structures for Future Generations},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.343},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817303533},
author = {Herry Pintardi Chandra and Paulus Nugraha and Evan Sutanto Putra},
keywords = {Building Information Modeling, project.},
abstract = {In current practice, many digital models do not contain sufficient information from designers to contractors and operators. A great deal of literature has pointed to the importance of understanding the Building Information Modeling (BIM). BIM is a digital representation of the physical and functional characteristics of a building. In Architecture-Engineering-Construction, BIM is the development and use a computer software model to stimulate the construction and operation of a facility, to make decisions and to improve the process of delivering the facility. The aims of this paper are to explore the need of technological support to implement the site-linked of BIM, the benefits of BIM, and the challenges of BIM. The research was conducted through literature review of BIM. The data was collected from the questionnaire survey carried out to 26 valid responses within the main stakeholders of construction work in Surabaya. A questionnaire is prepared by incorporating the technological support, benefits, and challenges of BIM. The data was analyzed by using descriptive analysis including mean analysis with 5 Likert scale. The results of the research shows that the need of technological support to implement the BIM is parametric components (mean value of 4.46), the need of software package majority is prepared by contractor and construction management consultant, and the benefit of BIM is to reduce the construction cost (mean value of 4.6). In addition, the main challenge of BIM is different brand with the mean value of 4.27, in which of incompatibility of different brand (mean value of 4.31.}
}
@article{LAU201728,
title = {Context-aware RAON middleware for opportunistic network},
journal = {Pervasive and Mobile Computing},
volume = {41},
pages = {28-45},
year = {2017},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119216303558},
author = {G. Lau and M. Al-Sabah and M. Jaseemuddin and H. Razavi and M. Bhuiyan},
keywords = {Middleware, Ad hoc network, Unstructured P2P overlay network, Cooperative caching, Context-aware, Resource-aware, Modified biased random walk, D2D communication, 5G networks},
abstract = {Application development and deployment on Mobile Ad Hoc Networks (MANET) is a major challenge in the widespread use of MANET. The increasing D2D communication in 5G networks has renewed interest in an effective middleware design for MANET where application developers face various challenges such as unstable connectivity, high error rate, mobility induced disruption and disconnection, and limited battery power. We find that unstructured overlay network provides a good abstraction to facilitate application development and deployment on MANET. In this paper, we present the design of a middleware that builds a Resource-Aware Overlay Network (RAON), which is an unstructured overlay network of nodes engaged in the application that employs a query–reply mechanism for resource discovery. RAON is enhanced with features such as proactive neighbor replacement, congestion-aware data download and cooperative caching. Simulation results show that these features are effective in reducing query delay, improving data availability, and balancing node power consumption with protocol performance. We also present the middleware software design that offers the API based on node and path abstractions to applications. The middleware implements a generic context framework for acquiring device and user context. We discuss the implementation of application-level multicast and credit-based file-sharing applications using the middleware API. The middleware is implemented in Java J2ME on Android, which is tested in an ad hoc network of Nexus 7 devices running OLSR.}
}
@incollection{REID2004251,
title = {10 - Programming for Scalability},
editor = {Fiach Reid},
booktitle = {Network programming in .NET},
publisher = {Digital Press},
address = {Burlington},
pages = {251-273},
year = {2004},
isbn = {978-1-55558-315-6},
doi = {https://doi.org/10.1016/B978-155558315-6/50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781555583156500116},
author = {Fiach Reid},
abstract = {Publisher Summary
Scalability problems generally start appearing once a product has rolled out into full-scale production. The goal of a scalable system is that it must be available for use at all times and remain highly responsive regardless of how many people use the system. Scalability, with respect to software architectures, has also come to mean extensibility and modularity. This simply means that when a software system needs to scale upward in complexity, it does not need to be overhauled with each addition. At this stage in the life cycle, making modifications to the software becomes a logistical nightmare. Any changes to the software will necessarily have to be backward compatible with older versions of the product. Many software packages now include an autoupdater that accommodates post deployment updates; however, the best solution is to address scalability issues at the design phase, rather than ending up with a dozen versions of a product and the server downtime caused by implementing updates.}
}
@article{LIM2017121,
title = {Phxnlme: An R package that facilitates pharmacometric workflow of Phoenix NLME analyses},
journal = {Computer Methods and Programs in Biomedicine},
volume = {140},
pages = {121-129},
year = {2017},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716305818},
author = {Chay Ngee Lim and Shuang Liang and Kevin Feng and Jason Chittenden and Ana Henry and Samer Mouksassi and Angela K. Birnbaum},
keywords = {Model diagnostics, R package, Pharmacometrics, Phxnlme, Phoenix NLME},
abstract = {Background and objective
Pharmacometric analyses are integral components of the drug development process, and Phoenix NLME is one of the popular software used to conduct such analyses. To address current limitations with model diagnostic graphics and efficiency of the workflow for this software, we developed an R package, Phxnlme, to facilitate its workflow and provide improved graphical diagnostics.
Methods
Phxnlme was designed to provide functionality for the major tasks that are usually performed in pharmacometric analyses (i.e. nonlinear mixed effects modeling, basic model diagnostics, visual predictive checks and bootstrap). Various estimation methods for modeling using the R package are made available through the Phoenix NLME engine. The Phxnlme R package utilizes other packages such as ggplot2 and lattice to produce the graphical output, and various features were included to allow customizability of the output. Interactive features for some plots were also added using the manipulate R package.
Results
Phxnlme provides enhanced capabilities for nonlinear mixed effects modeling that can be accessed using the phxnlme() command. Output from the model can be graphed to assess the adequacy of model fits and further explore relationships in the data using various functions included in this R package, such as phxplot() and phxvpc.plot(). Bootstraps, stratified up to three variables, can also be performed to obtain confidence intervals around the model estimates. With the use of an R interface, different R projects can be created to allow multi-tasking, which addresses the current limitation of the Phoenix NLME desktop software. In addition, there is a wide selection of diagnostic and exploratory plots in the Phxnlme package, with improvements in the customizability of plots, compared to Phoenix NLME.
Conclusions
The Phxnlme package is a flexible tool that allows implementation of the analytical workflow of Phoenix NLME with R, with features for greater overall efficiency and improved customizable graphics. Phxnlme is freely available for download on the CRAN repository (https://cran.r-project.org/web/packages/Phxnlme/).}
}
@article{RITSCHEL20173542,
title = {A Thermodynamic Library for Simulation and Optimization of Dynamic Processes**This project is funded partly by: 1) Innovation Fund Denmark in the CITIES project (1305-00027B) and in the OPTION project (63-2013-3), 2) the interreg project Smart Cities Accelerator (10606 SCA), and 3) EUDP 64013-0558 in the IEA Annex for energy efficient process control.},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {3542-3547},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.951},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317314234},
author = {Tobias K.S. Ritschel and Jozsef Gaspar and John Bagterp Jørgensen},
keywords = {Thermodynamic library, Process simulation, Dynamic optimization, Vapor compression cycle, Vapor-liquid equilibrium, Flash separation},
abstract = {Process system tools, such as simulation and optimization of dynamic systems, are widely used in the process industries for development of operational strategies and control for process systems. These tools rely on thermodynamic models and many thermodynamic models have been developed for different compounds and mixtures. However, rigorous thermodynamic models are generally computationally intensive and not available as open-source libraries for process simulation and optimization. In this paper, we describe the application of a novel open-source rigorous thermodynamic library, ThermoLib, which is designed for dynamic simulation and optimization of vapor-liquid processes. ThermoLib is implemented in Matlab and C and uses cubic equations of state to compute vapor and liquid phase thermodynamic properties. The novelty of ThermoLib is that it provides analytical first and second order derivatives. These derivatives are needed for efficient dynamic simulation and optimization. The analytical derivatives improve the computational performance by a factor between 12 and 35 as compared to finite difference approximations. We present two examples that use ThermoLib routines in their implementations: (1) simulation of a vapor-compression cycle, and (2) optimal control of an isoenergetic-isochoric flash separation process. The ThermoLib software used in this paper is distributed as open-source software at www.psetools.org.}
}
@article{FTRONCOSO2021110167,
title = {ClasSOMfier: A neural network for cluster analysis and detection of lattice defects},
journal = {Computational Materials Science},
volume = {188},
pages = {110167},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2020.110167},
url = {https://www.sciencedirect.com/science/article/pii/S0927025620306583},
author = {Javier {F. Troncoso}},
keywords = {Neural network, Kohonen network, Cluster analysis},
abstract = {ClasSOMfier is a software package to classify atoms into a given number of disconnected groups (or clusters) and detect lattice defects, such as vacancies, interstitials, dislocations, voids and grain boundaries. Each cluster is formed by atoms whose atomic environment can be described by a common pattern. Unlike many methods available in the literature, where these patterns are given in advance and are associated with known lattice structures (i.e. fcc, bcc or hcp), this code implements a Kohonen network, which is based on unsupervised learning and where no information about the atomic environment has to be given in advance. ClasSOMfier accelerates the application of machine learning for cluster analysis by providing an efficient and fast code in Fortran with a user-friendly interface in Python.}
}
@article{IWAMOTO20031509,
title = {Receiving message prediction method},
journal = {Parallel Computing},
volume = {29},
number = {11},
pages = {1509-1538},
year = {2003},
note = {Parallel and distributed scientific and engineering computing},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2003.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167819103001443},
author = {Yoshiyuki Iwamoto and Koichi Suga and Kanemitsu Ootsu and Takashi Yokota and Takanobu Baba},
keywords = {Receiving message prediction, Message passing interface, Speculative execution, NAS parallel benchmark, Message communication, Workstation cluster},
abstract = {This paper proposes and evaluates the Receiving Message Prediction Method for high performance message passing. In this method, a node in the idle state predicts the next message reception, and speculatively executes the message reception and user processes. This method is independent of underlying computer architecture and message passing libraries. We propose the algorithms for the message prediction, and evaluate them from the viewpoint of the success ratio and speed-ups. We use the NAS parallel benchmark programs as typical parallel applications running on two different types of parallel platforms, i.e., a workstation cluster and a shared memory multiprocessor. The experimental results show that the method can be applied to various platforms. The method can also be implemented just by changing the software inside their message passing libraries without any support from the underlying system software or hardware. This mean that we do not require any change of application software that uses the libraries. The application of the method to the message passing interface libraries achieves a speed-up of 6.8% for the NAS Parallel Benchmarks, and the static and dynamic selection of prediction methods based on profiling results improve the performance.}
}
@article{DATTOMA201297,
title = {A parametric–associative modelling of aeronautical concepts for structural optimisation},
journal = {Advances in Engineering Software},
volume = {50},
pages = {97-109},
year = {2012},
note = {CIVIL-COMP},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0965997812000385},
author = {V. Dattoma and M. {De Giorgi} and S. Giancane and P. Manco and A.E. Morabito},
keywords = {Design conceptualisation, Aeronautic structures, Wireframe primitives, Quad-mapped mesh, CAD/CAE tool, Parametric-associative modelling},
abstract = {In this work, a scheme of representation for aircraft structural concepts is identified. Based on this scheme, a parametric–associative geometrical modelling of the aeronautic structure, consisting in a quad-mapped mesh, is proposed. The mesh generation is based on a hierarchical scheme ensuring the one-to-one correspondence between mesh elements belonging to adjacent primitives. The automatic propagation of modifications is efficiently implemented according to well-defined schemes of dependence thanks to which the modifications involve only the concerned instances. This scheme is implemented in an original software, called MeshFEM and developed using C++, Matlab and the VTK library for 3D graphic visualisation.}
}
@incollection{ENGEL2013251,
title = {Chapter 11 - High-Performance Digital-to-Analog Converters for Wireless Infrastructure},
editor = {Gabriele Manganaro and Domine Leenaerts},
booktitle = {Advances in Analog and RF IC Design for Wireless Communication Systems},
publisher = {Academic Press},
address = {Oxford},
pages = {251-269},
year = {2013},
isbn = {978-0-12-398326-8},
doi = {https://doi.org/10.1016/B978-0-12-398326-8.00011-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012398326800011X},
author = {Gil Engel and Gabriele Manganaro},
keywords = {digital-to-analog converter, DAC, RF DAC, RF synthesis, transmitter},
abstract = {This chapter discusses the practical application of RF digital-to-analog converters (RF DACs) to communication systems such as cable distribution, wireless communications infrastructure (WIFR) base stations, wireless backhaul, and other such systems. The key specifications that are driving the development of RF DAC technology are reviewed, as are some common radio architectures used to implement those systems. Challenges associated with the design of RF DACs are described, and some trade-offs and possible solutions are discussed. Design considerations of the package and the printed circuit board (PCB) design are reviewed. Measured results of an RF DAC suitable for cable head-end transmitters are presented. The features and performance of RF DACs provide an enabling solution for “Software Defined Radio” (SDR) systems targeted toward multi-carrier, multi-band, multi-standard radio transmitters.}
}
@article{MIRHAIDARI2022108433,
title = {Nonlinear effects of bolted flange connections in aeroengine casing assemblies},
journal = {Mechanical Systems and Signal Processing},
volume = {166},
pages = {108433},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108433},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021007810},
author = {Seyed-Ehsan Mir-Haidari and Kamran Behdinan},
keywords = {Aeroengine, Nonlinear analysis, Bolted flange connection, Lump model, Finite element analysis, Nonlinear experiment, Computational efficiency},
abstract = {Aeroengine manufacturers face fundamental limitations when correlating experimental dynamic analysis with finite element method models. The source of inconsistency between model predictions and experimental test data can be attributed to nonlinear behaviour at the bolted flange interface. The two main sources of nonlinearities in aeroengine casing assemblies are damping dissipation and the nonlinear effects of boundary conditions. In this research, a novel and robust analytical formulation is proposed for implementation in FE analysis that accurately captures and represents the nonlinear dynamic characteristics of bolted flange connections. The proposed nonlinear analytical lump model has demonstrated significant accuracy and precision in capturing the nonlinear dynamic characteristics of bolted flange connections with spigots under various loading conditions. The proposed model clearly represents and characterizes the nonlinear phenomena of peak amplitude damping and frequency shift. It is also computationally efficient, making the model feasible for implementation when performing nonlinear analyses of large structural assemblies such as full aeroengine models. Moreover, the proposed analytical lump model is universal, permitting its implementation in various structures with different material properties and geometries. The validity and accuracy of the proposed model has been verified using nonlinear experimental test data for an aeroengine casing assembly.}
}
@article{MARULLI201635,
title = {CosmoBolognaLib: C++ libraries for cosmological calculations},
journal = {Astronomy and Computing},
volume = {14},
pages = {35-42},
year = {2016},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2213133716300014},
author = {F. Marulli and A. Veropalumbo and M. Moresco},
keywords = {Cosmology: theory, Cosmology: observations, Cosmology: large-scale structure of Universe, Methods: numerical, Methods: statistical},
abstract = {We present the CosmoBolognaLib, a large set of Open Source C++ numerical libraries for cosmological calculations. CosmoBolognaLib is a living project aimed at defining a common numerical environment for cosmological investigations of the large-scale structure of the Universe. In particular, one of the primary focuses of this software is to help in handling astronomical catalogues, both real and simulated, measuring one-point, two-point and three-point statistics in configuration space, and performing cosmological analyses. In this paper, we discuss the main features of this software, providing an overview of all the available C++ classes implemented up to now. Both the CosmoBolognaLib and their associated doxygen documentation can be freely downloaded at https://github.com/federicomarulli/CosmoBolognaLib. We provide also some examples to explain how these libraries can be included in either C++ or Python codes.}
}
@article{ASHOUR2000161,
title = {An FPGA implementation guide for some different types of serial–parallel multiplier structures},
journal = {Microelectronics Journal},
volume = {31},
number = {3},
pages = {161-168},
year = {2000},
issn = {0026-2692},
doi = {https://doi.org/10.1016/S0026-2692(99)00110-X},
url = {https://www.sciencedirect.com/science/article/pii/S002626929900110X},
author = {M.A Ashour and H.I Saleh},
keywords = {Digital signal processing (DSP), Serial–parallel multiplier, Field programmable gate array (FPGA)},
abstract = {The multiplier is one of the most important components in the computing and reconfigurable computing systems, especially in the field of digital signal processing (DSP). Hence, in this paper, a performance evaluation and comparison (efficient area and moderate speed) for different serial–parallel multiplier structures have been carried out for the case of their implementation by one of the programmable logic devices, such as a field programmable gate array (FPGA). The implementation of these structures for 8-bit parallel operands has been executed by utilizing the XC4010E chip and Foundation software package V1.3 from Xilinx. The implementation results illustrate the progress in the design area, saving and speeding up the design performance.}
}
@article{DRUZGALSKI2020101169,
title = {Process optimization of complex geometries using feed forward control for laser powder bed fusion additive manufacturing},
journal = {Additive Manufacturing},
volume = {34},
pages = {101169},
year = {2020},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2020.101169},
url = {https://www.sciencedirect.com/science/article/pii/S2214860420305418},
author = {C.L. Druzgalski and A. Ashby and G. Guss and W.E. King and T.T. Roehling and M.J. Matthews},
keywords = {Powder bed fusion, 3D printing, Additive manufacturing, DMLS, DMLM, Optimization, Control},
abstract = {Additive manufacturing (AM) enables the fabrication of complex designs that are difficult to create by other means. Metal parts manufactured by laser powder bed fusion (LPBF) can incorporate intricate design features and demonstrate desirable mechanical properties. However, printing a part that is qualified for its intended application often involves reprinting and discarding many parts to eliminate defects, improve dimensional accuracy, and increase repeatability. The process of iteratively converging on the appropriate build parameters increases the time and cost of creating functional LPBF manufactured parts. This paper describes a fast, scalable method for part-scale process optimization of arbitrary geometries. The computational approach uses feature extraction to identify scan vectors in need of parameter adaptation and applies results from simulation-based feed forward control models. This method provides a framework to quickly optimize complex parts through the targeted application of models with a range of fidelity and by automating the transfer of optimization strategies to new part designs. The computational approach and algorithmic framework are described, a software package is implemented, the method is applied to parts with complex features, and parts are printed on a customized open architecture LPBF machine.}
}
@article{DOVALLESIMOES1996421,
title = {Hardware implementation of RAM neural networks},
journal = {Pattern Recognition Letters},
volume = {17},
number = {4},
pages = {421-429},
year = {1996},
note = {Neural Networks for Computer Vision Applications},
issn = {0167-8655},
doi = {https://doi.org/10.1016/0167-8655(95)00137-9},
url = {https://www.sciencedirect.com/science/article/pii/0167865595001379},
author = {Eduardo {do Valle Simões} and Luís Felipe Uebel and Dante Augusto {Couto Barone}},
keywords = {Boolean neural networks, Character recognition, Fast prototyping, Image classification, Neural network hardware implementation},
abstract = {This work describes an alternative technique for hardware and software implementation of RAM-based Boolean neural networks, which describes neurons using the VHDL language. An example of application consisting of the classification problem of the British mail scanned address is attended with a RAM architecture presenting (340 x 12)-input neurons. The weights of each neuron are represented by its truth table and described using simple logic gates (AND, OR, and NOT), aiming to make possible the network logic minimisation and its hardware implementation by the ALTERA MAX + PLUS II fast prototyping package (Altera, 1992). The developed software tool allows the specification and training of the network. Then, its VHDL description is generated to be interpreted and minimised by the ALTERA EPLD design system. If it is not necessary to have high-speed processing or if pre-processing phases are needed, the ANN can be implemented in software. The software strategy makes use of the direct translation of the VHDL description into a simplified C language code. Once the user has specified and taught the network, this approach makes possible automatic prototyping of RAM neural networks in software and hardware.}
}
@incollection{PERRIN20171,
title = {1 - Specification of Shared Objects},
editor = {Matthieu Perrin},
booktitle = {Distributed Systems},
publisher = {Elsevier},
pages = {1-22},
year = {2017},
isbn = {978-1-78548-226-7},
doi = {https://doi.org/10.1016/B978-1-78548-226-7.50001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548226750001X},
author = {Matthieu Perrin},
keywords = {Abstract data types, ADT composition communications, Asynchronous processes, Concurrent history, Consistency criteria, Modelization, Sequential specifications, Shared objects},
abstract = {Abstract:
When a project manager designs the general architecture of an application, one of his main concerns is to divide the workload into distinct logical entities, software building blocks, thus enabling them to be implemented by different developers. Each software building block is then separately specified to mitigate the risks of conflict during the final integration. The specification is thus the communication vector between two developers: the designer who designs the object and the user who uses it in his/her own program. When the object in question is part of a library (and all the more when the source code is proprietary), this is the only way to describe the object. A good specification should be written from the user’s perspective and for the user. We will now describe the three main qualities that a specification must have.}
}
@article{MARTY19889,
title = {Software integration},
journal = {Computer Standards & Interfaces},
volume = {8},
number = {1},
pages = {9-14},
year = {1988},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(88)90070-0},
url = {https://www.sciencedirect.com/science/article/pii/0920548988900700},
author = {Rudolf Marty},
keywords = {Software engineering, System interfaces, System services, Portability, Adaptability, Software integration},
abstract = {The typical computer user makes use of various computer services such as text processing, software development, scientific computing, electronic mailing, etc. In the classical computing environment, the individual services were implemented by individual software packages that could hardly be combined into an integrated environment. The more advanced the computer hardware got and the more high-level services became available, the more came the demand for integrated software. This paper addresses the question of how to design and implement software such that it may be bound into an integrated computer based service environment. We begin the discussion by reviewing some important steps in the evolution of software integration. Later, we focus on monolythic integrated software packages, which are typically large and bulky. We call this kind of integration closed integration. In the following chapter we describe a way of building integrated systems by letting the user combine several individual pieces of software into an integrated computing environment individually. This way of constructing an integrated service is based on relatively small but integratable software packages and is called open integration. We conclude by proposing an open model for a layered system interface architecture designed to enhance the integratability of software.}
}
@article{BUNTING2013197,
title = {Sorted pulse data (SPD) library. Part I: A generic file format for LiDAR data from pulsed laser systems in terrestrial environments},
journal = {Computers & Geosciences},
volume = {56},
pages = {197-206},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413000332},
author = {Peter Bunting and John Armston and Richard M. Lucas and Daniel Clewley},
keywords = {LiDAR, Storage, Pulse, Waveform, Sorted, SPD},
abstract = {The management and spatial-temporal integration of LiDAR data from different sensors and platforms has been impeded by a lack of generic open source tools and standards. This paper presents a new generic file format description (sorted pulse data; SPD) for the storage and processing of airborne and terrestrial LiDAR data. The format is designed specifically to support both traditional discrete return and waveform data, using a pulse (rather than point) based data model. The SPD format also supports 2D spatial indexing of the pulses, where pulses can be referenced using cartesian, spherical, polar or scan geometry coordinate systems and projections. These indexes can be used to significantly speed up data processing whilst allowing the data to be appropriately projected and are particularly useful when analysing and interpreting TLS data. The format is defined within a HDF5 file, which provides a number of benefits including broad support across a wide range of platforms and architectures and support for file compression. An implementation of the format is available within the open source sorted pulse data software library (SPDLib; http://www.spdlib.org).}
}
@article{MINH2022103206,
title = {Structural damage identification in thin-shell structures using a new technique combining finite element model updating and improved Cuckoo search algorithm},
journal = {Advances in Engineering Software},
volume = {173},
pages = {103206},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103206},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822001120},
author = {Hoang-Le Minh and Thanh Sang-To and Magd {Abdel Wahab} and Thanh Cuong-Le},
keywords = {Structural damage identification, Damaged thin-shell structures, Cuckoo search algorithm, finite element model updating, SAP2000-OAPI},
abstract = {In this paper, the powerful advantages of Open Application Programming Interface (OAPI) in SAP2000 software, known as an excellent tool to solve structural engineering problems, are exploited to establish a new technique of finite element (FE) model updating. This technique is based on the development of a successful sub-program, which can link programming language of MATLAB with SAP2000 through the OAPI library. Therefore, the two-way data exchange between MATLAB and SAP2000 can be continuously secured, and the process of modification of the initial FE model in SAP2000 is implemented by coding instead of using the standard point-and-click procedure in SAP2000′s interface. The damage identification process in structures is then done using an optimization algorithm, named a modified version Cuckoo search (CS) algorithm, and called new balance of Cuckoo search (NB-CS). The key factor in NB-CS is to create a balance between two values: the best global and the worst global. To achieve this balance, NB-CS proposes two main contributions. The first is based on the modification of the concept of a random walk whose step lengths obey Lévy distribution. Whereas, the second is to establish a new balance vector, which is presented based on the global optimal and the worse optimal exploited at each iteration. This vector is combined with each step length to establish a new movement strategy, which is more flexible and more intelligent than that of the original CS. To examine NB-CS's reliability, the first 23 classical benchmark functions are selected to illustrate the convergence rate and level of accuracy of NB-CS compared to the original CS. To authenticate the efficiency of the proposed method, three models, including square shell structure, parabolic domes, and partial sphere shell structure, are investigated for different damage cases. NB-CS is then employed to minimize the objective function with design variables related to the extent and location of damage in thin-shell elements in the structures. The statistical results show the feasible performance of NB-CS in a widespread class of optimization problems and its great potential for application to structural damage identification.}
}
@article{BHONSALE2018719,
title = {Pomodoro: A Novel Toolkit for Dynamic (MultiObjective) Optimization, and Model Based Control and Estimation⁎⁎All the authors are with KU Leuven - Department of Chemical Engineering, BioTeC & OPTEC, Gebroeders de Smetstraat 1, B-9000,Ghent, Belgium.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {2},
pages = {719-724},
year = {2018},
note = {9th Vienna International Conference on Mathematical Modelling},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.03.122},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318301265},
author = {Satyajeet Bhonsale and Dries Telen and Dominique Vercammen and Mattia Vallerio and Jan Hufkens and Philippe Nimmegeers and Filip Logist and Jan Van Impe},
keywords = {Optimal Control, Multi Objective Dynamic Optimization, Model Predictive Control, State, Parameter Estimation},
abstract = {This paper presents the software package Pomodoro which includes a collection of algorithms and tools for dynamic optimization. It also introduces the framework for multiobjective problems, model predictive control and state estimation. Pomodoro is implemented in Python and utilizes CasADi as a backbone to formulate the problem. It uses orthogonal collocation technique to solve the dynamic optimization problems and efficient third-party solvers are employed to solve the resulting nonlinear programs. The design of the software and its main modules are discussed and the user-friendliness of the software is demonstrated with the help of tutorial examples for each problem class. Lastly, the advantages and limitations of this software are discussed.}
}
@article{SAINI2022107152,
title = {A biophysically guided constitutive law of the musculotendon-complex: modelling and numerical implementation in Abaqus},
journal = {Computer Methods and Programs in Biomedicine},
volume = {226},
pages = {107152},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.107152},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722005338},
author = {Harnoor Saini and Oliver Röhrle},
keywords = {Skeletal muscle, Continuum mechanics, Motor-units, Constitutive relation, Abaqus, Finite element},
abstract = {Background and Objective: Many biomedical, clinical, and industrial applications may benefit from musculoskeletal simulations. Three-dimensional macroscopic muscle models (3D models) can more accurately represent muscle architecture than their 1D (line-segment) counterparts. Nevertheless, 3D models remain underutilised in academic, clinical, and commercial environments. Among the reasons for this is a lack of modelling and simulation standardisation, verification, and validation. Here, we strive towards a solution by providing an open-access, characterised, constitutive relation (CR) for 3D musculotendon models. Methods: The musculotendon complex is modelled following the state-of-the-art active stress approach and is treated as hyperelastic, transversely isotropic, and nearly incompressible. Furthermore, force-length and -velocity relationships are incorporated, and muscle activation is derived from motor-unit information. The CR was implemented within the commercial finite-element software package Abaqus as a user-subroutine. A masticatory system model with left and right masseters was used to demonstrate active and passive movement. Results: The CR was characterised by various experimental data sets and was able to capture a wide variety of passive and active behaviours. Furthermore, the masticatory simulations revealed that joint movement was sensitive to the muscle’s in-fibre passive response. Conclusions: This user-material provides a “plug and play” template for 3D neuro-musculoskeletal finite element modelling. We hope that this reduces modelling effort, fosters exchange, and contributes to the standardisation of such models.}
}
@article{YAN2021351,
title = {Nektar++: Design and implementation of an implicit, spectral/hp element, compressible flow solver using a Jacobian-free Newton Krylov approach},
journal = {Computers & Mathematics with Applications},
volume = {81},
pages = {351-372},
year = {2021},
note = {Development and Application of Open-source Software for Problems with Numerical PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2020.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0898122120301073},
author = {Zhen-Guo Yan and Yu Pan and Giacomo Castiglioni and Koen Hillewaert and Joaquim Peiró and David Moxey and Spencer J. Sherwin},
keywords = {Nektar++, Implicit time integration, Spectral/ element, Discontinuous Galerkin, Jacobian-free Newton Krylov},
abstract = {At high Reynolds numbers the use of explicit in time compressible flow simulations with spectral/hp element discretization can become significantly limited by time step. To alleviate this limitation we extend the capability of the spectral/hp element open-source software framework, Nektar++, to include an implicit discontinuous Galerkin compressible flow solver. The integration in time is carried out by a singly diagonally implicit Runge–Kutta method. The non-linear system arising from the implicit time integration is iteratively solved by the Jacobian-free Newton Krylov (JFNK) method. A favorable feature of the JFNK approach is its extensive use of the explicit operators available from the previous explicit in time implementation. The functionalities of different building blocks of the implicit solver are analyzed from the point of view of software design and placed in appropriate hierarchical levels in the C++ libraries. In the detailed implementation, the contributions of different parts of the solver to computational cost, memory consumption and programming complexity are also analyzed. A combination of analytical and numerical methods is adopted to simplify the programming complexity in forming the preconditioning matrix. The solver is verified and tested using cases such as manufactured compressible Poiseuille flow, Taylor–Green vortex, turbulent flow over a circular cylinder at Re=3900 and shock wave boundary-layer interaction. The results show that the implicit solver can speed-up the simulations while maintaining good simulation accuracy.}
}
@article{SOJKA2011366,
title = {Modular software architecture for flexible reservation mechanisms on heterogeneous resources},
journal = {Journal of Systems Architecture},
volume = {57},
number = {4},
pages = {366-382},
year = {2011},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2011.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1383762111000282},
author = {Michal Sojka and Pavel Píša and Dario Faggioli and Tommaso Cucinotta and Fabio Checconi and Zdeněk Hanzálek and Giuseppe Lipari},
keywords = {Real-time, Operating systems, Embedded systems, Distributed systems, Middleware},
abstract = {Management, allocation and scheduling of heterogeneous resources for complex distributed real-time applications is a challenging problem. Timing constraints of applications may be fulfilled by the proper use of real-time scheduling policies, admission control and enforcement of timing constraints. However, it is not easy to design basic infrastructure services that allow for easy access to the allocation of multiple heterogeneous resources in a distributed environment. In this paper, we present a middleware for providing distributed soft real-time applications with a uniform API for reserving heterogeneous resources with real-time scheduling capabilities in a distributed environment. The architecture relies on standard POSIX OS facilities, such as time management and standard TCP/IP networking services, and it is designed around CORBA, in order to facilitate modularity, flexibility and portability of the applications using it. However, real-time scheduling is supported by proper extensions at the kernel-level, plugged within the framework by means of dedicated resource managers. Our current implementation on Linux supports the reservation of the CPU, disk and network bandwidth. However, additional resource managers supporting alternative real-time schedulers for these resources, as well as additional types of resources, may be easily added. We present experimental results gathered on both synthetic applications and a real multimedia video streaming case study, showing the advantages deriving from the use of the proposed middleware. Finally, overhead figures are reported, showing the sustainability of the approach for a wide class of complex, distributed, soft real-time applications.}
}
@article{KAHOUI2000161,
title = {Deciding Hopf Bifurcations by Quantifier Elimination in a Software-component Architecture},
journal = {Journal of Symbolic Computation},
volume = {30},
number = {2},
pages = {161-179},
year = {2000},
issn = {0747-7171},
doi = {https://doi.org/10.1006/jsco.1999.0353},
url = {https://www.sciencedirect.com/science/article/pii/S074771719990353X},
author = {M’hammed El Kahoui and Andreas Weber},
abstract = {In this paper we give a semi-algebraic description of Hopf bifurcation fixed points for a given parameterized polynomial vector field. The description is carried out by use of the Hurwitz determinants, and produces a first-order formula which is transformed into a quantifier-free formula by the use of usual-quantifier elimination algorithms. We apply techniques from the theory of sub-resultant sequences and of Gröbner bases to come up with efficient reductions, which lead to quantifier elimination questions that can often be handled by existing quantifier elimination packages. We could implement the algorithms for the conditions on Hopf bifurcations by combining the computer algebra system Maple with packages for quantifier elimination using a Java-based component architecture recently developed by the second author. In addition to some textbook examples we applied our software system to an example discussed in a recent research paper.}
}
@article{BONE2022111204,
title = {AutoMapper: A python tool for accelerating the polymer bonding workflow in LAMMPS},
journal = {Computational Materials Science},
volume = {205},
pages = {111204},
year = {2022},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2022.111204},
url = {https://www.sciencedirect.com/science/article/pii/S0927025622000179},
author = {Matthew A. Bone and Brendan J. Howlin and Ian Hamerton and Terence Macquart},
keywords = {Molecular dynamics, LAMMPS, Polymer simulation, Open source software},
abstract = {Polymeric materials modelling has the potential to rapidly accelerate the discovery of new materials due to the comparative ease of simulations compared to laboratory testing campaigns. High quality molecular dynamics simulation software, such as LAMMPS, are able to facilitate the transition from empirical to digitised chemistry. However, in order to fully benefit from the speed of simulations, tools need to be developed to automate the preprocessing stages required for modelling. AutoMapper is an open-source Python application that automates the generation of files required to use REACTER, a powerful polymer bonding package implemented within LAMMPS. To automate this process, the authors developed an iterative path search algorithm based on chemical graph theory to accurately map pre- and post-reaction polymerisation structures, and hence eliminating the bulk of the human effort previously required to run a simulation. AutoMapper requires minimal user input, is force field independent, and has shown marvellous performance on a wide range of polymerisation types.}
}
@article{MILO20112088,
title = {A fast, GPU based, dictionary attack to OpenPGP secret keyrings},
journal = {Journal of Systems and Software},
volume = {84},
number = {12},
pages = {2088-2096},
year = {2011},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2011.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0164121211001270},
author = {Fabrizio Milo and Massimo Bernaschi and Mauro Bisson},
keywords = {Cryptanalysis, Graphics Processing Units, CUDA, OpenPGP},
abstract = {We describe the implementation, based on the Compute Unified Device Architecture (CUDA) for Graphics Processing Units (GPU), of a novel and very effective approach to quickly test passphrases used to protect private keyrings of OpenPGP cryptosystems. Our combination of algorithm and implementation, reduces the time required to test a set of possible passphrases by three-orders of magnitude with respect to an attack based on the procedure described in the OpenPGP standard and implemented by software packages like GnuPG, and a tenfold speed up if compared to our highly tuned CPU implementation. Our solution can be considered a replacement and an extension of pgpcrack, a utility used in the past for attacking PGP. The optimizations described can be applied to other cryptosystems and confirm that the GPU architecture is also very effective for running applications that make extensive (if not exclusive) use of integer operations.}
}
@article{VALENCIA2021106378,
title = {A CAD-free methodology for volume and mass properties computation of 3-D lifting surfaces and wing-box structures},
journal = {Aerospace Science and Technology},
volume = {108},
pages = {106378},
year = {2021},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2020.106378},
url = {https://www.sciencedirect.com/science/article/pii/S1270963820310609},
author = {Esteban Valencia and Victor Alulema and Victor Hidalgo and Dario Rodriguez},
keywords = {Aircraft design, 3-d modeling, CAD-free, Volume and mass computation, Lifting surface CAD/CAE, Geometry discretization},
abstract = {The geometry, volume, and mass properties (GVM) of lifting surfaces and wing box structures play an important role in aircraft design and optimization. Commercial Computer-Aided-Design packages can be employed to determine these features; however, they are difficult to embed in multidisciplinary frameworks because of their limited scripting capabilities and their black-box structure. In this context, the present work introduces an open-source, fully scriptable, high fidelity, and low computational demanding methodology to compute the volume and mass properties of lifting surfaces and wingbox structures using their three-dimensional geometric definition. NURBS modeling is employed to generate the 3-D geometry and derive the vertex representation of the lifting surface. The volume computation is based on the Divergence Theorem and employs a structured triangulation approach, tailored for lifting surfaces and their cross sections. The mass properties (i.e. center of mass and moments of inertia) are calculated as a system of mass particles. For this, a mass distribution model, based on the thickness and chord distribution throughout the lifting surface, has been elaborated. The methodology for volume computation and the mass distribution model has been validated analytically using a simple polytope that resembles to a lifting surface. The convergence and the robustness of the methodology has been evaluated for a straight tapered lifting surface. The results indicate a maximum error, compared with a commercial CAD software, of 0.3% and 0.5% for the volume and mass properties computation, respectively. In addition, to assess its suitability for complex lifting surfaces, the NASA Common Research Model aircraft (NASA-CRM) has been used as case of study. To summarize, the main contribution of this work lies on the development of an open-source and fully scriptable methodology, which can be easily implemented in any computational environment for aircraft design and optimization.}
}
@article{OESTERLEIN2016165,
title = {Analysis and visualization of intracardiac electrograms in diagnosis and research: Concept and application of KaPAVIE},
journal = {Computer Methods and Programs in Biomedicine},
volume = {127},
pages = {165-173},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2015.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715003302},
author = {Tobias Georg Oesterlein and Jochen Schmid and Silvio Bauer and Amir Jadidi and Claus Schmitt and Olaf Dössel and Armin Luik},
keywords = {Intracardiac electrogram, Electroanatomical mapping, Visualization, Atrial fibrillation, Medical imaging, Diagnostic software},
abstract = {Background and objective
Progress in biomedical engineering has improved the hardware available for diagnosis and treatment of cardiac arrhythmias. But although huge amounts of intracardiac electrograms (EGMs) can be acquired during electrophysiological examinations, there is still a lack of software aiding diagnosis. The development of novel algorithms for the automated analysis of EGMs has proven difficult, due to the highly interdisciplinary nature of this task and hampered data access in clinical systems. Thus we developed a software platform, which allows rapid implementation of new algorithms, verification of their functionality and suitable visualization for discussion in the clinical environment.
Methods
A software for visualization was developed in Qt5 and C++ utilizing the class library of VTK. The algorithms for signal analysis were implemented in MATLAB. Clinical data for analysis was exported from electroanatomical mapping systems.
Results
The visualization software KaPAVIE (Karlsruhe Platform for Analysis and Visualization of Intracardiac Electrograms) was implemented and tested on several clinical datasets. Both common and novel algorithms were implemented which address important clinical questions in diagnosis of different arrhythmias. It proved useful in discussions with clinicians due to its interactive and user-friendly design. Time after export from the clinical mapping system to visualization is below 5min.
Conclusion
KaPAVIE22See http://www.ibt.kit.edu/hardundsoftware.php. is a powerful platform for the development of novel algorithms in the clinical environment. Simultaneous and interactive visualization of measured EGM data and the results of analysis will aid diagnosis and help understanding the underlying mechanisms of complex arrhythmias like atrial fibrillation.}
}
@article{KANG2019107786,
title = {Multi-lattice inner structures for high-strength and light-weight in metal selective laser melting process},
journal = {Materials & Design},
volume = {175},
pages = {107786},
year = {2019},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2019.107786},
url = {https://www.sciencedirect.com/science/article/pii/S0264127519302230},
author = {Dongseok Kang and Sanghu Park and Yong Son and Simo Yeon and Sang Hoon Kim and Ilyong Kim},
keywords = {Lattice structure, Lightweight design, Additive manufacturing, Topology optimization, Selective laser melting (SLM), Sandwich panel},
abstract = {Due to the development of additive manufacturing (AM), lattice structure which cannot be fabricated by the conventional manufacturing process or have shape restriction has attracted much attention. We propose a new lightweight design method using two types of lattice structures considering the manufacturability in the metal selective laser melting (SLM) and structural characteristics. Firstly, the specific procedure for the proposed design method is presented. In order to apply the two lattice structures, relative density criterion is derived by fabricating experiments using metal SLM process and analyzing geometry according to relative density. The optimal relative density distribution is calculated by performing the topology optimization with minimum relative density using a commercial software package. This proposed method is computationally and experimentally validated by a three-point bending test. Simultaneously, the same procedure is applied to uniform lattice for comparison with the proposed method. This proposed design has a 46% increase in stiffness, a relative flexural rigidity of 35% compared to the solid material, and has a deformation mode different from the uniform lattice. This design sets the standard for using two lattice structures and gives a new perspective on lightweight design with lattice structures.}
}
@article{RASTKAR201779,
title = {A meshfree approach for homogenization of mechanical properties of heterogeneous materials},
journal = {Engineering Analysis with Boundary Elements},
volume = {75},
pages = {79-88},
year = {2017},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0955799716303812},
author = {Siavash Rastkar and Maryam Zahedi and Igor Korolev and Arvind Agarwal},
keywords = {Meshfree, Meshless, Asymptotic homogenization, Solution Structure Method, Periodic boundary condition},
abstract = {In this paper, asymptotic homogenization and meshfree Solution Structure Method (SSM) are combined to develop a hybrid homogenization technique. This hybrid method makes it possible to capture accurate geometric information of material microstructure, directly from micrographs or Computed Tomography (CT) scans, and offers a completely automated numerical procedure. Homogenization methods often employ FEA to incorporate realistic geometry of the material's microstructure. However, generating a finite element mesh from images or 3D voxel data could be tedious, error-prone, and expensive. Also, in many practical situations, considerable manual modifications are often required. On the other hand, the SSM uses implicit mathematical functions to represent the geometric model. It could be implemented using different types of basis functions, either on a non-conforming structural grid or cloud of points. Adaptive numerical and geometric algorithms assure good geometric flexibility of SSM in handling complex structures. Furthermore, to accommodate material homogenization equations, the SSM is extended so it can provide the exact satisfaction of periodic boundary conditions without using any spatial meshes. To validate the developed method, the architecture of a computer software package is designed that provides an automated computational pipeline for material homogenization. Numerical examples are provided to evaluate the developed platform against other methods and previously published data.}
}
@article{MUSTAFA200067,
title = {Changing class behaviors at run-time in MRP systems},
journal = {Journal of Systems and Software},
volume = {55},
number = {1},
pages = {67-71},
year = {2000},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(00)00048-0},
url = {https://www.sciencedirect.com/science/article/pii/S0164121200000480},
author = {Yousif Mustafa and Olugnenga Mejabi},
keywords = {Behavior, Reuse, Run-time, Flexibility, Class},
abstract = {This paper presents an architecture that can be used to develop and maintain class behaviors in object-oriented material requirements planning (MRP) systems at run-time. The architecture provides this capability through a user-interface, and does not require knowledge of any programming language. The architecture is based on the concept of software reuse, it utilizes a library of fine-grained pre-compiled objects to develop and maintain class behaviors. The architecture is a dynamic-object application builder implemented on top of an object-oriented run-time environment.}
}
@incollection{SZECSI200671,
title = { - Implementing manufacturing feature based design in CAD/CAM},
editor = {D.T. Pham and E.E. Eldukhri and A.J. Soroka},
booktitle = {Intelligent Production Machines and Systems},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {71-76},
year = {2006},
isbn = {978-0-08-045157-2},
doi = {https://doi.org/10.1016/B978-008045157-2/50019-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080451572500195},
author = {T. Szecsi},
abstract = {Publisher Summary
The chapter outlines the development of a new design system that allows the composition of a design from manufacturable features. The aim of the project is to develop a software system that enables designers to appreciate manufacturing process capabilities and limitations during the design phase. The system is linked to a commercial computer-aided design/computer-aided manufacturing (CAD/CAM) package Pro/Engineer that supports parametric feature-based design. The system contains several modules: hierarchical design-for-manufacture rule system, manufacturing feature library, manufacturing feature-based design module, designer advisory module, manufacturing feature recognition module, and design analysis module. This system allows the designer to compose a design using manufacturable entities. The inserted features are validated from functionality and manufacturing point of view. The functionality rules ensure that the design is consistent with the functionality requirements. The implementation of this system enables early detection of manufacturing problems during the design phase. Solutions that are likely to cause manufacturing problems are rejected, or alternative solutions are suggested. This leads to reduced manufacturing costs.}
}
@article{CETINCEVIZ2012461,
title = {Design and implementation of an Internet based effective controlling and monitoring system with wireless fieldbus communications technologies for process automation—An experimental study},
journal = {ISA Transactions},
volume = {51},
number = {3},
pages = {461-470},
year = {2012},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2012.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S001905781200002X},
author = {Yucel Cetinceviz and Ramazan Bayindir},
keywords = {PLC, Internet based control, Industrial wireless LAN, Profinet},
abstract = {The network requirements of control systems in industrial applications increase day by day. The Internet based control system and various fieldbus systems have been designed in order to meet these requirements. This paper describes an Internet based control system with wireless fieldbus communication designed for distributed processes. The system was implemented as an experimental setup in a laboratory. In industrial facilities, the process control layer and the distance connection of the distributed control devices in the lowest levels of the industrial production environment are provided with fieldbus networks. In this paper, the Internet based control system that will be able to meet the system requirements with a new-generation communication structure, which is called wired/wireless hybrid system, has been designed on field level and carried out to cover all sectors of distributed automation, from process control, to distributed input/output (I/O). The system has been accomplished by hardware structure with a programmable logic controller (PLC), a communication processor (CP) module, two industrial wireless modules and a distributed I/O module, Motor Protection Package (MPP) and software structure with WinCC flexible program used for the screen of Scada (Supervisory Control And Data Acquisition), SIMATIC MANAGER package program (“STEP7”) used for the hardware and network configuration and also for downloading control program to PLC.}
}
@article{HIRSCHFELD200555,
title = {Reflective Designs — An Overview},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {127},
number = {3},
pages = {55-58},
year = {2005},
note = {Proceedings of the Workshop on Software Evolution through Transformations: Model-based vs. Implementation-level Solutions (SETra 2004)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2004.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105001386},
author = {Robert Hirschfeld and Ralf Lämmel},
keywords = {Reflective Designs, Runtime Adaptation, Design Elements, Design Operators, Design Patterns, Reflection, Method-Call Interception, Meta-Programming, Aspect-Oriented Programming, Dynamic Weaving, Dynamic Composition, AspectS, Squeak, Smalltalk, Metaobject Protocol},
abstract = {We render runtime system adaptations by design-level concepts such that running systems can be adapted and examined at a higher level of abstraction. The overall idea is to express design decisions as applications of design operators to be carried out at runtime. Design operators can implement design patterns for use at runtime. Applications of design operators are made explicit as design elements in the running system such that they can be traced, reconfigured, and made undone. Our approach enables Reflective Designs: on one side, design operators employ reflection to perform runtime adaptations; on the other side, design elements provide an additional reflection protocol to examine and configure performed adaptations. Our approach helps understanding the development and the maintenance of the class of software systems that cannot tolerate downtime or frequent shutdown-revise-startup cycles. We have accumulated a class library for programming with Reflective Designs in Squeak/Smalltalk. This library employs reflection and dynamic aspect-oriented programming. We have also implemented tool support for navigating in a system that is adapted continuously at runtime. Note: This extended abstract summarises our full paper [Hirschfeld, R. and R. Lämmel, Reflective Designs, IEE Proceedings Software (2004), Special Issue on Reusable Software Libraries. To appear. Available at http://homepages.cwi.nl/~ralf/rd/].}
}
@article{ALMEIDA2014216,
title = {CAOVerif: An open-source deductive verification platform for cryptographic software implementations},
journal = {Science of Computer Programming},
volume = {91},
pages = {216-233},
year = {2014},
note = {Special Issue on Selected Contributions from the Open Source Software Certification (OpenCert) Workshops},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2012.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S016764231200189X},
author = {José Bacelar Almeida and Manuel Barbosa and Jean-Christophe Filliâtre and Jorge Sousa Pinto and Bárbara Vieira},
keywords = {Formal verification, Program verification, Cryptographic software, Deductive verification},
abstract = {CAO is a domain-specific imperative language for cryptography, offering a rich mathematical type system and crypto-oriented language constructions. We describe the design and implementation of a deductive verification platform for CAO and demonstrate that the development time of such a complex verification tool could be greatly reduced by building on the Jessie plug-in included in the Frama-C framework. We discuss the interesting challenges raised by the domain-specific characteristics of CAO, and describe how we tackle these problems in our design. We base our presentation on real-world examples of CAO code, extracted from the open-source code of the NaCl cryptographic library, and illustrate how various cryptography-relevant security properties can be verified.}
}
@article{TACHWALI2009607,
title = {Configurable symbol synchronizers for software-defined radio applications},
journal = {Journal of Network and Computer Applications},
volume = {32},
number = {3},
pages = {607-615},
year = {2009},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2008.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1084804508000829},
author = {Y. Tachwali and W.J. Barnes and H. Refai},
keywords = {Configurable, Symbol timing recovery, Synchronization, SDR},
abstract = {In many synchronous receivers, symbol timing synchronization is achieved through implementation of an analog phase locked loop (PLL). A phase detector and voltage-controlled oscillator drive a reference signal to be in phase with the received training sequence. Due to the quick phase convergence this option is attractive; however, limitations in pre-packaged hardware make this approach infeasible at times. Changes in the received symbol rate in software radio applications can further complicate the hardware implementation by requiring additional control signals to alter the frequency of the reference signal. This paper examines a configurable symbol synchronizer for software-defined radio (SDR) architecture with a predefined RF front end. In this scenario, we implement a typical method for digital phase locking and make it adaptable to different data rates. A pre-synchronization step is used to provide a reasonable initial estimate for the received symbol period for lower, over-sampled data rates. This decreases the synchronization time while maintaining a constant sampling period at the ADC. It also maintains the down-conversion stage at the receiver. The paper shows the feasibility of this architecture to support wide range of symbol rates.}
}
@article{OLIVEIRA20053,
title = {Modelling the GSM Handover Protocol in CommUnity},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {141},
number = {3},
pages = {3-25},
year = {2005},
note = {Proceedings of the Second International Workshop on Formal Foundations of Embedded Software and Component-based Software Architectures (FESCA 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105051637},
author = {Cristóvão Oliveira and Michel Wermelinger and José Luiz Fiadeiro and Antónia Lopes},
keywords = {(Software) Architecture, composition, (dynamic) configuration, connector, coordination, distribution, mobility, superposition},
abstract = {CommUnity is a formal approach to software architecture. It has a precise, yet intuitive mathematical semantics based on category theory. It supports, at the methodological level, a clear separation between computation, coordination, and distribution (including mobility). It provides a simple state-based language for describing component behaviour that is inspired by Unity and Interacting Processes. It also addresses composition as a first class concern and accounts for the emergence of global system properties from interconnections. This paper describes the approach and available tool support by modelling essential aspects of the GSM handover protocol. We also sketch a framework that we are implementing for the distributed execution of such specifications using Klava, a Java library for mobile agent systems based on tuple spaces.}
}
@article{SAKAGUCHI2020102372,
title = {Program extraction for mutable arrays},
journal = {Science of Computer Programming},
volume = {191},
pages = {102372},
year = {2020},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.102372},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319301650},
author = {Kazuhiko Sakaguchi},
keywords = {Interactive theorem proving, Formally verified software, The Coq proof assistant, Program extraction, Program transformation and optimization},
abstract = {We present a lightweight method to represent, verify, and extract efficient programs involving mutable arrays in the Coq proof assistant. Our method mainly consists of a library for handling mutable arrays and an improved extraction plugin. Our library provides a monadic domain specific language for modeling computations involving mutable arrays, a simple reasoning method based on the Ssreflect extension and the Mathematical Components library, and an extraction method to efficient OCaml programs using in-place updates. Our extraction plugin improves the performance of our extracted programs, or more appropriately, through the application of simple program transformations for purely functional programs, it reduces both construction and destruction costs of inductive and coinductive objects and function call costs. As concrete applications for our method, we provide efficient implementations, correctness proofs, and benchmarks of the union–find data structure and the quicksort algorithm.}
}
@article{FRASER200597,
title = {The reengineering of a software system for glaucoma analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {79},
number = {2},
pages = {97-109},
year = {2005},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2005.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260705000167},
author = {Ryan George Fraser and Jocelyn Armarego and Kanagasingam Yogesan},
keywords = {Reengineering, User-centred design, Software engineering, Glaucoma},
abstract = {Summary
Glaucoma is a destructive eye disease that causes blindness in individuals displaying little or no symptoms. There is no cure as yet though there are treatments that can arrest its effects or slow its development. The earlier the disease is detected, the more likely the treatment will be successful; however early detection of the disease can be difficult. This highlights the importance of ophthalmologists having access to tools that can assist in accurately diagnosing glaucoma and other retinal diseases as early as possible. The stereo optic disc analyser (SODA) software package is a tool intended to be used by ophthalmologists, to aid in the accurate detection of retinal diseases. SODA will use stereoscopy and three-dimensional image analysis to assist in accurately detecting changes in the retina, caused by diseases such as glaucoma. This paper will focus on the reengineering and redesign of the SODA software package to overcome the shortcomings inherent in its prototype implementation and develop a package that can be commercialised. Software Engineering principles and the software development lifecycle, along with principles of object-orientation and usability, have been used to establish a framework for SODA, improve its accuracy, enhance its usability and to redevelop the product into an implementation that can later be commercialised.}
}
@article{FUNKE2005179,
title = {Structural filtering: a paradigm for efficient and exact geometric programs},
journal = {Computational Geometry},
volume = {31},
number = {3},
pages = {179-194},
year = {2005},
note = {11th Canadian Conference on Computational Geometry},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2004.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0925772104001348},
author = {Stefan Funke and Kurt Mehlhorn and Stefan Näher},
keywords = {Computational geometry, Exact computation, Software design, Sorting},
abstract = {We introduce a new and simple filtering technique that can be used in the implementation of geometric algorithms called “structural filtering”. Using this filtering technique we gain about 20% when compared to predicate-filtered implementations. Of theoretical interest are some results regarding the robustness of sorting algorithms against erroneous comparisons. There is software support for the concept of structural filtering in LEDA (Library of Efficient Data Types and Algorithms, http://www.mpi-sb.mpg.de/LEDA/leda.html) and CGAL (Computational Geometry Algorithms Library, http://www.cgal.org).}
}
@article{ECHEVERRIA2011535,
title = {Customizing floating-point units for FPGAs: Area-performance-standard trade-offs},
journal = {Microprocessors and Microsystems},
volume = {35},
number = {6},
pages = {535-546},
year = {2011},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0141933111000597},
author = {Pedro Echeverría and Marisa López-Vallejo},
keywords = {Floating-point arithmetic, FPGAs, Library of operators, High performance},
abstract = {The high integration density of current nanometer technologies allows the implementation of complex floating-point applications in a single FPGA. In this work the intrinsic complexity of floating-point operators is addressed targeting configurable devices and making design decisions providing the most suitable performance-standard compliance trade-offs. A set of floating-point libraries composed of adder/subtracter, multiplier, divisor, square root, exponential, logarithm and power function are presented. Each library has been designed taking into account special characteristics of current FPGAs, and with this purpose we have adapted the IEEE floating-point standard (software-oriented) to a custom FPGA-oriented format. Extended experimental results validate the design decisions made and prove the usefulness of reducing the format complexity.}
}
@article{MANRIQUERODRIGUEZ2012155,
title = {Developing a drug library for smart pumps in a pediatric intensive care unit},
journal = {Artificial Intelligence in Medicine},
volume = {54},
number = {3},
pages = {155-161},
year = {2012},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0933365711001679},
author = {Silvia Manrique-Rodríguez and Amelia Sánchez-Galindo and Cecilia M. Fernández-Llamazares and Jesús López-Herce and Isabel García-López and Ángel Carrillo-Álvarez and María Sanjuro-Sáez},
keywords = {Drug library, Intravenous administration, Smart pumps, Pediatric critical care},
abstract = {Background
The most serious medication errors occur during intravenous administration. The potential consequences are more serious the more critical and younger the patient. Smart pumps can help to prevent infusion-related programming errors, thanks to associated dose-limiting software known as “drug library”. Drug libraries alert the user if pre-determined high dosage limits are exceeded or if entry is below pre-determined low dosage limits.
Objective
To describe the process for developing a specific drug library for a pediatric intensive care unit (PICU) and the key factors for preventing programming errors.
Methods and materials
The study was performed by a multidisciplinary team consisting of a clinical pharmacist, a PICU pediatrician, and the chief nurse of the unit. The process of developing the drug library lasted seven months. A literature review was carried out to determine standard concentrations and accurate limits for intravenous administration of high-risk drugs. Alaris® syringe pumps and Guardrails® CQI v4.1 Event Reporter software were used.
Results
Several manufacturers offer smart pump technology. Users should be aware of differences in features, such as definition of parameters and associations between them, definition of safety limits, organization of the drug library, and data use. Our infusion pump technology covered 108 drugs. Compliance with the drug library was 85% and nurses’ acceptance of the drug library was high as 94% would recommend implementation of this technology in other units. After nine months of implementation, several potentially harmful infusion-related programming errors were intercepted.
Conclusions
Drug libraries are specifically designed for a particular hospital unit, and may condition the success in implementing this technology. Implementation of smart pumps proved effective in intercepting infusion-related programming errors after nine months of implementation in a PICU.}
}
@article{REGULY201999,
title = {Improving resilience of scientific software through a domain-specific approach},
journal = {Journal of Parallel and Distributed Computing},
volume = {128},
pages = {99-114},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519300917},
author = {I.Z. Reguly and G.R. Mudalige and M.B. Giles and S. Maheswaran},
keywords = {Domain specific language, High performance computing, Checkpointing, Resilience, Parallel I/O},
abstract = {In this paper we present research on improving the resilience of the execution of scientific software, an increasingly important concern in High Performance Computing (HPC). We build on an existing high-level abstraction framework, the Oxford Parallel library for Structured meshes (OPS), developed for the solution of multi-block structured mesh-based applications, and implement an algorithm in the library to carry out checkpointing automatically, without the intervention of the user. The target applications are a hydrodynamics benchmark application from the Mantevo Suite, CloverLeaf 3D, the sparse linear solver proxy application TeaLeaf, and the OpenSBLI compressible Navier–Stokes direct numerical simulation (DNS) solver. We present (1) the basic algorithm that OPS relies on to determine the optimal checkpoint in terms of size and location, (2) improvements that supply additional information to improve the decision, (3) techniques that reduce the cost of writing the checkpoints to non-volatile storage, (4) a performance analysis of the developed techniques on a single workstation and on several supercomputers, including ORNL’s Titan. Our results demonstrate the utility of the high-level abstractions approach in automating the checkpointing process and show that performance is comparable to, or better than the reference in all cases.}
}
@article{SAHIN2009984,
title = {Development of remote control and monitoring of web-based distributed OPC system},
journal = {Computer Standards & Interfaces},
volume = {31},
number = {5},
pages = {984-993},
year = {2009},
note = {Specification, Standards and Information Management for Distributed Systems},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2008.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0920548908001487},
author = {Cihan Şahin and Emine Doğru Bolat},
keywords = {Distributed OPC (DOPC), Web technologies, Delphi},
abstract = {This paper focuses on implementing remote control and monitoring of a web-based distributed OLE for Process Control (OPC) system. Remote monitoring and control of OPC-based systems realized at N different local control points on the Internet are achieved by using a distributed OPC (DOPC) architecture. In this proposed architecture, every local control center can control and monitor every other control point. DOPC architecture is developed by using an OPC standard created for the automation industry. This OPC was developed as an industrial standard using Microsoft's Object Linking and Embedding (OLE) technology. OPC enables different control devices to communicate with each other by exchanging data. This proposed architecture permits different OPC-based process control architectures from a wide range of fields to communicate with each other. Thus, OPC-based local control systems located in different places can communicate with each other on the Internet without using bus architecture. The basic functions of the proposed system were implemented using the Delphi software package. Local control architectures developed at N points can communicate with each other and a remote control point on a dynamic web page constructed using Active Server Pages (ASP). Consequently, the proposed DOPC architecture allows the user to control and monitor local systems from any location with internet access, and to implement data exchange between N OPC points.}
}
@article{MILLER2019120,
title = {WeldANA: Welding decision support tool for conceptual design},
journal = {Journal of Manufacturing Systems},
volume = {51},
pages = {120-131},
year = {2019},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518301742},
author = {Simon W. Miller and Daniel A. Finke and Michael Kupinski and Christopher B. Ligetti},
keywords = {Welding, Manufacturability, Conceptual design, Decision support},
abstract = {This paper describes a methodology and reference implementation created to aid design engineers in the early phases of design for a new discrete product. The decision support tool is used to understand design decisions and their relation to manufacturability considerations, in particular, the weldability of a design. As such, we have defined four metrics (partability, setup/orientation, accessibility, and preparation) that encapsulate good structural weldment design. Those metrics have been codified in a software tool that interacts directly with a commercial CAD package as part of a manufacturability analysis decision support tool suite that also includes machining and casting assessment capabilities. This paper focuses on weldment manufacturability analysis, WeldANA. Several common use cases are used to demonstrate the efficacy and efficiency of the approach for use in the early stages of the design process.}
}
@article{OECHSLE20211716,
title = {Methodology for Assessing, Evaluating and Selecting an Integration and Migration Strategy for Industry 4.0 in SME},
journal = {Procedia CIRP},
volume = {104},
pages = {1716-1721},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.289},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011872},
author = {Oliver Oechsle and Tom Drews and Paul Molenda},
keywords = {Industry 4.0, Industry 4.0 methodology, measures for integration, migration of Industry 4.0},
abstract = {The paper at hand presents a methodology to assess, evaluate and select a package of measures that represents an appropriate integration and migration strategy for the implementation of Industry 4.0 tools. The methodology is based on a developed interaction and restriction matrix which improves the decision support process of small and medium sized enterprises (SME). The comparison of different strategies is assisted by a software tool. This structured decision process reduces the risk of misinvestment, enhances the competitiveness and raises the Industry 4.0 maturity level of SME. For a broad application area, the methodology was applied and validated in several SME.}
}
@article{MELONI201789,
title = {Real-Time neural signal decoding on heterogeneous MPSocs based on VLIW ASIPs},
journal = {Journal of Systems Architecture},
volume = {76},
pages = {89-101},
year = {2017},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1383762116302041},
author = {Paolo Meloni and Claudio Rubattu and Giuseppe Tuveri and Danilo Pani and Luigi Raffo and Francesca Palumbo},
keywords = {Neural signal decoding, MPSoCS, ASIPs, Design space exploration, Low power},
abstract = {An important research problem, at the basis of the development of embedded systems for neuroprosthetic applications, is the development of algorithms and platforms able to extract the patient’s motion intention by decoding the information encoded in neural signals. At the state of the art, no portable and reliable integrated solutions implementing such a decoding task have been identified. To this aim, in this paper, we investigate the possibility of using the MPSoC paradigm in this application domain. We perform a design space exploration that compares different custom MPSoC embedded architectures, implementing two versions of a on-line neural signal decoding algorithm, respectively targeting decoding of single and multiple acquisition channels. Each considered design points features a different application configuration, with a specific partitioning and mapping of parallel software tasks, executed on customized VLIW ASIP processing cores. Experimental results, obtained by means of FPGA-based prototyping and post-floorplanning power evaluation on a 40nm technology library, assess the performance and hardware-related costs of the considered configurations. The reported power figures demonstrate the usability of the MPSoC paradigm within the processing of bio-electrical signals and show the benefits achievable by the exploitation of the instruction-level parallelism within tasks.}
}
@incollection{GANNEY2020131,
title = {Chapter 9 - Software engineering},
editor = {Azzam Taktak and Paul S. Ganney and David Long and Richard G. Axell},
booktitle = {Clinical Engineering (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {131-168},
year = {2020},
isbn = {978-0-08-102694-6},
doi = {https://doi.org/10.1016/B978-0-08-102694-6.00009-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026946000097},
author = {Paul S. Ganney and Sandhya Pisharody and Edwin Claridge},
keywords = {Software development, Software management, Operating system, Language selection, Programming, Software lifecycle, Procedural programming, Object-oriented programming, Functional programming, Real-time programming, Embedded programming, Software validation, Software verification, Software quality assurance, Database, Spreadsheet, Flat-file, Normal form, SQL, Typical applications, Image processing, Finite element analysis, Artificial intelligence, Expert systems, Equipment management database, Device tracking},
abstract = {This section looks at software, firstly describing common operating systems before looking at techniques for selecting the appropriate programming language for a project or task. We then move on to examine the common features in any programming language: variables and commands/operators before describing the software lifecycle: both the steps within it and models of implementation. We then return to programming to describe several types of programming: procedural, object-oriented, functional, real-time and embedded. We then investigate software quality through validation and verification before concluding the section with an examination of databases, both simple and structured, including normalisation and the manipulation of data using Structured Query Language. We then describe some typical applications (plus some sample packages) within clinical computing: image processing, finite element analysis, artificial intelligence, expert systems, equipment management databases and device tracking systems.}
}
@article{MICHAELBROWN2012186,
title = {An Evaluation of Molecular Dynamics Performance on the Hybrid Cray XK6 Supercomputer},
journal = {Procedia Computer Science},
volume = {9},
pages = {186-195},
year = {2012},
note = {Proceedings of the International Conference on Computational Science, ICCS 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S187705091200141X},
author = {W. {Michael Brown} and Trung D. Nguyen and Miguel Fuentes-Cabrera and Jason D. Fowlkes and Philip D. Rack and Mark Berger and Arthur S. Bland},
keywords = {Molecular Dynamics, Materials, GPU, Supercomputer, Embedded Atom Method},
abstract = {Formanyyears,thedrivetowards computationalphysics studiesthatmatchthesizeand time-scalesofexperiment has been fueled by increases in processor and interconnect performance that could be exploited with relatively little modiﬁcation to existing codes. Engineering and electrical power constraints have disrupted this trend, requiring more drastic changes to both hardware and software solutions. Here, we present details of the Cray XK6 architecture that achieves increased performance with the use of GPU accelerators. We review software development efforts in the LAMMPS molecular dynamics package that have been implemented in order to utilize hybrid high performance computers. We present benchmark results for solid-state, biological, and mesoscopic systems and discuss some challenges for utilizinghybrid systems. We present some earlyworkin improving application performance on the XK6 and performance results for the simulation of liquid copper nanostructures with the embedded atom method.}
}
@article{BRICCOLA201960,
title = {Analysis of 3D linear elastic masonry-like structures through the API of a finite element software},
journal = {Advances in Engineering Software},
volume = {133},
pages = {60-75},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818311517},
author = {Deborah Briccola and Matteo Bruggi},
keywords = {Energy-based methods, Linear elastic no-tension material, Historical masonry structures, Masonry domes, Masonry vaults, Masonry walls},
abstract = {A new numerical procedure is presented to perform the analysis of three-dimensional linear elastic no-tension structures exploiting the application programming interface of a general purpose finite element analysis software. Masonry is replaced by an equivalent orthotropic material with spatially varying elastic properties and negligible stiffness in the case of cracking strain. A non-incremental algorithm is implemented to define the distribution of the equivalent material, minimizing the strain energy so as to achieve a compression-only state of stress for any given compatible load. Applications are shown for masonry-like solids of general shape visualizing load paths in walls subject to dead loads and out-of-plane live loads, circular domes under self-weight and a groin vault acted upon by both vertical and horizontal seismic loading.}
}
@article{CHENG201892,
title = {Aimsgb: An algorithm and open-source python library to generate periodic grain boundary structures},
journal = {Computational Materials Science},
volume = {155},
pages = {92-103},
year = {2018},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2018.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0927025618305536},
author = {Jianli Cheng and Jian Luo and Kesong Yang},
keywords = {Grain boundary, Coincidence site lattice, Open-source, Software, O-lattice},
abstract = {An algorithm implemented in an open-source python library was developed for building periodic grain boundary models in a universal fashion. The software framework, aimsgb, aims to generate tilt and twist grain boundaries from an input cubic or non-cubic crystal structure for ab-initio and classical atomistic simulation. It can output a coincidence site lattice (CSL) grain boundary for a cubic input structure and a non-CSL grain boundary for a non-cubic input structure. This framework has two useful features: (i) it can calculate all the CSL matrices for generating CSL from a given Sigma (Σ) value and rotation axis, allowing the users to build the specific CSL and grain boundary models; (ii) it provides a convenient command line tool to enable high-throughput generation of tilt and twist grain boundaries by assigning an input crystal structure, Σ value, rotation axis, and grain boundary plane. The developed algorithm in the open-source python library is expected to facilitate studies of grain boundary in materials science. The software framework is available on the website: aimsgb.org.}
}
@article{HABIB2017253,
title = {Attribute driven process architecture for additive manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {44},
pages = {253-265},
year = {2017},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0736584516301715},
author = {Md Ahasan Habib and Bashir Khoda},
keywords = {Contour plurality, Optimal deposition direction, Part Attributable Motion (PAM), Build time, Additive manufacturing},
abstract = {In additive manufacturing (AM) process, the manufacturing attributes are highly dependent upon the execution of hierarchical plan. Among them, material deposition plan can frequently interrupt the AM process due to tool-path changes, tool start-stop and non-deposition time, which can be challenging during free-form part fabrication. In this paper, the layer geometries for both model and support structure are analyzed to identify the features that create change in deposition modality. First, the overhanging points on the part surface are identified using the normal vector direction of the model surface. A k-th nearest point algorithm is implemented to generate the 3d boundary support contour which is used to construct the support structure. Both model and support structures are sliced and contours are evaluated. The layer contour, plurality, concavity, number of contours, geometric shape, size and interior islands are considered to generate an AM deposition model. The proposed model is solved for minimizing the change in deposition modality by maximizing the continuity and connectivity in the material deposition plan. Both continuity and connectivity algorithms are implemented for model and support structure for free-form object. The proposed algorithm provides the optimum deposition direction that results in minimum number of tool-path segments and their connectivity while minimizing contour plurality effect. This information is stored as a generic digital file format named Part Attributable Motion (PAM). A common application program interface (API) platform is also proposed in this paper, which can access the PAM and generate machine readable file for different existing 3D printers. The proposed research is implemented on three free-form objects with complex geometry and parts are fabricated. Also, the build time is evaluated and the results are compared with the available 3d printing software.}
}
@article{TCHAGNAKOUANOU201868,
title = {An optimal big data workflow for biomedical image analysis},
journal = {Informatics in Medicine Unlocked},
volume = {11},
pages = {68-74},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300844},
author = {Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda},
keywords = {Biomedical images, Big data, Artificial intelligence, Machine learning, Hadoop/spark},
abstract = {Background and objective
In the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and/or architectures based on big data technologies, for a complete processing of biomedical image data.
Method
This paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.
Results
Two architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.
Conclusions
The proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.}
}
@article{LOPEZIBANEZ201643,
title = {The irace package: Iterated racing for automatic algorithm configuration},
journal = {Operations Research Perspectives},
volume = {3},
pages = {43-58},
year = {2016},
issn = {2214-7160},
doi = {https://doi.org/10.1016/j.orp.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214716015300270},
author = {Manuel López-Ibáñez and Jérémie Dubois-Lacoste and Leslie {Pérez Cáceres} and Mauro Birattari and Thomas Stützle},
keywords = {Automatic algorithm configuration, Racing, Parameter tuning},
abstract = {Modern optimization algorithms typically require the setting of a large number of parameters to optimize their performance. The immediate goal of automatic algorithm configuration is to find, automatically, the best parameter settings of an optimizer. Ultimately, automatic algorithm configuration has the potential to lead to new design paradigms for optimization software. The irace package is a software package that implements a number of automatic configuration procedures. In particular, it offers iterated racing procedures, which have been used successfully to automatically configure various state-of-the-art algorithms. The iterated racing procedures implemented in irace include the iterated F-race algorithm and several extensions and improvements over it. In this paper, we describe the rationale underlying the iterated racing procedures and introduce a number of recent extensions. Among these, we introduce a restart mechanism to avoid premature convergence, the use of truncated sampling distributions to handle correctly parameter bounds, and an elitist racing procedure for ensuring that the best configurations returned are also those evaluated in the highest number of training instances. We experimentally evaluate the most recent version of irace and demonstrate with a number of example applications the use and potential of irace, in particular, and automatic algorithm configuration, in general.}
}
@article{THIELE2010972,
title = {NetLogo meets R: Linking agent-based models with a toolbox for their analysis},
journal = {Environmental Modelling & Software},
volume = {25},
number = {8},
pages = {972-974},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2010.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815210000514},
author = {Jan C. Thiele and Volker Grimm},
keywords = {NetLogo, Gnu R, Model analysis, Agent-based modelling, Individual-based modelling, Modelling software},
abstract = {NetLogo is a software platform for agent-based modelling that is increasingly used in ecological and environmental modelling. So far, for comprehensive analyses of agent-based models (ABMs) implemented in NetLogo, results needed to be written to files and evaluated by using external software, for example R. Ideally, however, it would be possible to call any R function from within a NetLogo program. This would allow sophisticated interactive statistical analysis of model structure and dynamics, using R functions and packages for generating certain statistical distributions and experimental design, and for implementing complex descriptive submodels within ABMs. Here we present an R extension of NetLogo. It consists of only nine new NetLogo primitives for sending data between NetLogo and R and for calling R functions (six additional primitives for debugging). We demonstrate the usage of the R extension with three short examples.}
}
@article{KIM1996187,
title = {UWICL: A Multi-Layered Parallel Image Computing Library for Single-Chip Multiprocessor-based Time-Critical Systems},
journal = {Real-Time Imaging},
volume = {2},
number = {3},
pages = {187-199},
year = {1996},
issn = {1077-2014},
doi = {https://doi.org/10.1006/rtim.1996.0019},
url = {https://www.sciencedirect.com/science/article/pii/S1077201496900194},
author = {Jihong Kim and Yongmin Kim},
abstract = {Many software libraries have been created to support the commonly used primitive operations needed in image processing, image analysis and image understanding. Generally, these libraries are based on thesingle-layeredApplication Program Interface (API). While a single-layered API provides the useful abstraction level to interact with the library and hides unnecessary implementation details from the user, it does not produce an efficient program when a new algorithm is implemented by assembling the selected existing library routines. The composed program suffers from the inefficient data movement and additional loop control overhead. Furthermore, when a system employs a highly integrated processor such as a single-chip multiprocessor, the single-layered API prevents the user from fully utilizing the resources available in the system. In this article, we describe the University of Washington Image Computing Library (UWICL), themulti-layerdhigh-performance parallel image computing library for Texas Instruments TMS320C80 Multimedia Video Processor (MVP)-based time-critical systems. Our goal in designing the UWICL is to provide the TMS320C80 user community with efficient and flexible image computing library routines. The UWICL provides three levels of APIs to the programmers under the multi-layered organization, the MVP-level API, the DSP-level API, and APIs for data flow and processing cores. By optimizing the processing core functions, we have achieved high performance in the individual function level, and by allowing the sub-primitive library routine composition, we can achieve efficient image processing application development, avoiding most problems encountered in using the single-layered library routines. The performance of the multi-layered organizationvs.the single-layered one is analysed and compared using the Canny's edge detection algorithm as an example. The balanced composition based on the multi-layered organization outperforms the single-layered composition by 14 to 41% depending on the system's memory bandwidth available. As an adjunct to the UWICL, we have also developed an integrated MVP performance monitor (MPM). The MPM can identify the performance bottleneck of the TMS320C80 applications and can be used in optimization by enabling the user to select the most efficient library composition level in building the application with the UWICL. In order to provide the overall performance evaluation model of the MVP, the simple MVP functional model has also been defined in the MPM. For the image thresholding operation, the difference between the measured execution time and the analysis prediction is less than 2%. The design and implementation of the MPM, and the applicability and usefulness of the MPM and MVP performance model are described in this article.}
}
@article{RINEAU2007100,
title = {A generic software design for Delaunay refinement meshing},
journal = {Computational Geometry},
volume = {38},
number = {1},
pages = {100-110},
year = {2007},
note = {Special Issue on CGAL},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2006.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0925772107000193},
author = {Laurent Rineau and Mariette Yvinec},
keywords = {Mesh generation, Delaunay refinement, Generic programming},
abstract = {This paper describes a generic software designed to implement meshing algorithms based on the Delaunay refinement paradigm. Such a meshing algorithm is generally described through a set of rules guiding the refinement of mesh elements. The central item of the software design is a generic class, called a mesher level, that is able to handle one of the rules guiding the refinement process. Several instantiations of the mesher level class can be stacked and tied together to implement the whole refinement process. As shown in this paper, the design is flexible enough to implement all currently known mesh generation algorithms based on Delaunay refinement. In particular it can be used to generate meshes approximating smooth or piecewise smooth surfaces, as well as to mesh three dimensional domains bounded by such surfaces. It also adapts to algorithms handling small input angles and various refinement criteria. This design highly simplifies the task of implementing Delaunay refinement meshing algorithms. It has been used to implemented several meshing algorithms in the Cgal library.}
}
@article{GUILLARD2015131,
title = {A Decision Support System to design modified atmosphere packaging for fresh produce based on a bipolar flexible querying approach},
journal = {Computers and Electronics in Agriculture},
volume = {111},
pages = {131-139},
year = {2015},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2014.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0168169914003202},
author = {Valérie Guillard and Patrice Buche and Sébastien Destercke and Nouredine Tamani and Madalina Croitoru and Luc Menut and Carole Guillaume and Nathalie Gontard},
keywords = {MAP modeling, Multi-criteria querying, Decision Support System, Knowledge engineering, Respiring product},
abstract = {To design new packaging for fresh food, stakeholders of the food chain express their needs and requirements, according to some goals and objectives. These requirements can be gathered into two groups: (i) fresh food related characteristics and (ii) packaging intrinsic characteristics. Modified Atmosphere Packaging (MAP) is an efficient way to delay senescence and spoilage and thus to extend the very short shelf life of respiring products such as fresh fruits and vegetables. Consequently, packaging O2/CO2 permeabilities must fit the requirements of fresh fruits and vegetable as predicted by virtual MAP simulating tools. Beyond gas permeabilities, the choice of a packaging material for fresh produce includes numerous other factors such as the cost, availability, potential contaminants of raw materials, process ability, and waste management constraints. For instance, the user may have the following multi-criteria query for his/her product asking for a packaging with optimal gas permeabilities that guarantee product quality and optionally a transparent packaging material made from renewable resources with a cost for raw material less than 3€/kg. To help stakeholders taking a rational decision based on the expressed needs, a new multi-criteria Decision Support System (DSS) for designing biodegradable packaging for fresh produce has been built. In this paper we present the functional specification, the software architecture and the implementation of the developed tool. This tool includes (i) a MAP simulation module combining mass transfer models and respiration of the food, (ii) a multi-criteria flexible querying module which handles imprecise, uncertain and missing data stored in the database. We detail its operational functioning through a real life case study to determine the most satisfactory materials for apricots packaging.}
}
@article{ENKOVAARA201117,
title = {GPAW - massively parallel electronic structure calculations with Python-based software},
journal = {Procedia Computer Science},
volume = {4},
pages = {17-25},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911000615},
author = {Jussi Enkovaara and Nichols A. Romero and Sameer Shende and Jens J. Mortensen},
keywords = {Python, Numpy, MPI, Density-functional theory},
abstract = {Electronic structure calculations are a widely used tool in materials science and large consumer of supercomputing resources. Traditionally, the software packages for these kind of simulations have been implemented in compiled languages, where Fortran in its different versions has been the most popular choice. While dynamic, interpreted languages, such as Python, can increase the effciency of programmer, they cannot compete directly with the raw performance of compiled languages. However, by using an interpreted language together with a compiled language, it is possible to have most of the productivity enhancing features together with a good numerical performance. We have used this approach in implementing an electronic structure simulation software GPAW using the combination of Python and C programming languages. While the chosen approach works well in standard workstations and Unix environments, massively parallel supercomputing systems can present some challenges in porting, debugging and profiling the software. In this paper we describe some details of the implementation and discuss the advantages and challenges of the combined Python/C approach. We show that despite the challenges it is possible to obtain good numerical performance and good parallel scalability with Python based software.}
}
@article{FILLERON2012113,
title = {Designing group sequential randomized clinical trials with time to event end points using a R function},
journal = {Computer Methods and Programs in Biomedicine},
volume = {108},
number = {1},
pages = {113-128},
year = {2012},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169260712000405},
author = {Thomas Filleron and Jocelyn Gal and Andrew Kramar},
keywords = {Randomized clinical trial, Time to event endpoint, Superiority, Non-inferiority, Sample size, R function, Group sequential},
abstract = {A major and difficult task is the design of clinical trials with a time to event endpoint. In fact, it is necessary to compute the number of events and in a second step the required number of patients. Several commercial software packages are available for computing sample size in clinical trials with sequential designs and time to event endpoints, but there are a few R functions implemented. The purpose of this paper is to describe features and use of the R function. plansurvct.func, which is an add-on function to the package gsDesign which permits in one run of the program to calculate the number of events, and required sample size but also boundaries and corresponding p-values for a group sequential design. The use of the function plansurvct.func is illustrated by several examples and validated using East software.}
}
@article{SHIEH1996215,
title = {An object-oriented approach to develop software fault-tolerant mechanisms for parallel programming systems},
journal = {Journal of Systems and Software},
volume = {32},
number = {3},
pages = {215-225},
year = {1996},
issn = {0164-1212},
doi = {https://doi.org/10.1016/0164-1212(95)00126-3},
url = {https://www.sciencedirect.com/science/article/pii/0164121295001263},
author = {Ce-Kuen Shieh and Su-Cheong Mac and Tzu-Chiang Chang and Chung-Ming Lai},
abstract = {Some parallel programming systems are libraries that allow programmers to write thread-based parallel programs with existing sequential languages. Basically, parallel programs are hard to debug and much more complex than sequential programs which causes design faults to possibly reside in the parallel programs. This paper is aimed to design and implement a software fault-tolerant mechanism in an object-oriented approach for the existing parallel programming systems. With these software fault-tolerant objects, programmers can write their reliable parallel programs on these parallel programming systems. Recover Block, N-Version Programming, and Conversation software fault tolerant mechanisms are chosen to support. All these mechanisms are implemented and grouped into a separate software layer which resides on the top of the parallel programming system, used to monitor the behavior of applications, detect software faults, and recover and restart programs. Parallel programming systems are responsible for managing concurrent threads and for providing fault-tolerant mechanisms with necessary concurrent facilities. This layered system architecture makes these software fault-tolerant mechanisms portable, extensible, and lighter overhead. We have originally implemented the above software fault-tolerant objects based on Presto in C++. These objects have also been ported to C-Thread of Mach and LWP of SUN OS.}
}
@article{MEN2022100201,
title = {PyTorch-based implementation of label-aware graph representation for multi-class trajectory prediction},
journal = {Software Impacts},
volume = {11},
pages = {100201},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100201},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000889},
author = {Qianhui Men and Hubert P.H. Shum},
keywords = {Trajectory prediction, Graph convolution network, Multi-class prediction, Traffic analysis, Human motion understanding},
abstract = {Trajectory Prediction under diverse patterns has attracted increasing attention in multiple real-world applications ranging from urban traffic analysis to human motion understanding, among which graph convolution network (GCN) is frequently adopted with its superior ability in modeling the complex trajectory interactions among multiple humans. In this work, we propose a python package by enhancing GCN with class label information of the trajectory, such that we can explicitly model not only human trajectories but also that of other road users such as vehicles. This is done by integrating a label-embedded graph with the existing graph structure in the standard graph convolution layer. The flexibility and the portability of the package also allow researchers to employ it under more general multi-class sequential prediction tasks.}
}
@article{HENNIG2016984,
title = {ArchiveDB—Scientific and technical data archive for Wendelstein 7-X},
journal = {Fusion Engineering and Design},
volume = {112},
pages = {984-990},
year = {2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2016.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0920379616303830},
author = {Christine Hennig and Josef Maier and Martin Grün and Jon Krom and Torsten Blum and Michael Grahl and Peter Heimann and Heike Riemann and Heike Laqua and Marc Lewerentz and Anett Spring and Andreas Werner},
keywords = {Continuous operation, Data archive, Data stream management system, Big data, Lambda architecture, CoDaC, Java},
abstract = {ArchiveDB is the data archive for all scientific and technical data collected at the Wendelstein 7-X project. It is a distributed system allowing continuous data archival. ArchiveDB has demanding requirements regarding performance efficiency (storage performance of 30GB/s during experiments, expected storage amount of 1.4PB/year), reliability (availability of 364days/year), maintainability (testability) and portability (including change of hardware and software). Data acquisition with continuous operation and high time resolutions (up to nanoseconds scale) for physics data is supported as well as long-term recording up to 24h/7days for operational data (∼1Hz rate). Moreover, all results of data analysis are stored in the archive. Another challenge, uniform retrieval of measured and analyzed data, allowing time and structure information as selection criteria, is mastered as well. The key concepts of data storage and retrieval are: (1) partitioning of incoming data in groups and stream, (2) chunking of data in boxes of manageable size covering a finite time period, and (3) indexing of data using absolute time as ordering and indexing criteria. Continuous operation of the ArchiveDB software and hardware for various systems and components relevant to Wendelstein 7-X has been done successfully for several years, thus, showing that the key requirements are satisfied. The overall data amount so far has reached 7 Terabyte over 9 years of data taking. Round-the-clock operation of the archive is in place since 5 years. Initial plasma operation OP1.1 of Wendelstein 7-X has been supported with no downtime during the whole experimental campaign. The paper describes the software engineering concepts that have been used, consolidated and refined over the years of continuing productive ArchiveDB use and development. Changes in the underlying techniques, e.g. a change of the data store, have been encapsulated via an Application Programming Interface (API). This API unifies different implementations and is also suitable for data migration.}
}
@article{GESING2018544,
title = {Gathering requirements for advancing simulations in HPC infrastructures via science gateways},
journal = {Future Generation Computer Systems},
volume = {82},
pages = {544-554},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.02.042},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17303011},
author = {Sandra Gesing and Rion Dooley and Marlon Pierce and Jens Krüger and Richard Grunzke and Sonja Herres-Pawlis and Alexander Hoffmann},
keywords = {e-Sciences, Internet and web computing, Large scale scientific computing, Applied modeling and simulation, Large scale systems for computational sciences, Cloud computing and applications},
abstract = {Compute-intensive simulations are often based on complex scientific theories and necessitate high-performance computing (HPC) infrastructures to deliver results in reasonable time. While domain researchers are experts in their field and apply sophisticated theoretical models in computational simulations, they are not necessarily also HPC experts or IT specialists in general. Thus, they appreciate easy-to-use solutions tailored to their research, which hide the complex underlying computing and data infrastructures. Science gateways form such end-to-end solutions and their development for compute-intensive simulations necessitates expertise to connect HPC research infrastructures including grid and cloud infrastructures to support with the efficient access to such resources. HPC experts and IT specialists fulfilling this task may have only rudimentary knowledge about the research domain of a simulation. Thus, it is crucial that they gather the requirements of a research use case, which they aim to support efficiently via a science gateway. In the last 10 years quite a few web development frameworks, science gateway frameworks and APIs with different foci and strengths have evolved to support the developers of science gateways in implementing an intuitive solution for a target research domain. The selection of a suitable technology for a specific use case is essential and helps reducing the effort in implementing the science gateway by re-using existing software or frameworks. Thus, a solution for a user community can be provided more efficiently. This paper introduces the general architecture of science gateways, goes into detail for criteria to design science gateways efficiently and gives examples of mature science gateways and science gateway frameworks.}
}
@article{KHAN2015389,
title = {Development of a graph method for preliminary design of borehole ground-coupled heat exchanger in North Louisiana},
journal = {Energy and Buildings},
volume = {92},
pages = {389-397},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2015.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0378778815000390},
author = {Md Adnan Khan and Jay X. Wang},
keywords = {Graph method, Geothermal energy, Borehole, GLD 2012, GLHEPRO, Louisiana},
abstract = {Heating and cooling of a building is a major energy demand and a source of CO2 emission in the atmosphere. Using geothermal energy in the form of borehole ground coupled heat exchanger (GHX) can be an effective and economical alternative. To make the GHX design quick and easy to do in its preliminary stage, a simple and easily implemented method is needed. This paper synthesizes relations between the GHX spacing, diameter, soil thermal conductivity and peak heating/cooling energy output for a typical building foundation in Louisiana by proposing a graph method. One set of design graphs were developed for northern Louisiana, where subsurface soils are mainly stiff clays and dense sands. The GHX boreholes were designed without any tedious calculation by using these graphs. The GHX solutions from the developed graph-method were reliable and accurate, and agreed fairly well with the results from the industry-popular software packages GLD 2012 and GLHEPRO for a house cooling example that was selected in Ruston, Louisiana.}
}
@article{CENNI2016179,
title = {The reliability and validity of a clinical 3D freehand ultrasound system},
journal = {Computer Methods and Programs in Biomedicine},
volume = {136},
pages = {179-187},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169260716304874},
author = {Francesco Cenni and Davide Monari and Kaat Desloovere and Erwin Aertbeliën and Simon-Henri Schless and Herman Bruyninckx},
keywords = {3D freehand ultrasonography, Volume and length measurements, Reliability and validity analysis, Open source software},
abstract = {Background and Objective
Acquiring large anatomical volumes in a feasible manner is useful for clinical decision-making. A relatively new technique called 3D freehand ultrasonography is capable of this by combining a conventional 2D ultrasonography system. Currently, a thorough analysis of this technique is lacking, as the analyses are dependent on the software implementation details and the choice of measurement systems. Therefore this study starts by making this implementation available under the form of an open-source software library to perform 3D freehand ultrasonography. Following that, reliability and validity analyses of extracting volumes and lengths will be carried out using two independent motion-tracking systems.
Methods
A PC-based ultrasonography device and two optical motion-tracking systems were used for data acquisition. An in-house software library called Py3DFreeHandUS was developed to reconstruct (off-line) the corresponding data into one 3D data set. Reliability and validity analyses of the entire experimental set-up were performed by estimating the volumes and lengths of ground truth objects. Ten water-filled balloons and six cross-wires were used. Repeat measurements were also performed by two experienced operators.
Results
The software library Py3DFreeHandUS is available online, along with the relevant documentation. The reliability analyses showed high intra- and inter-operator intra-class correlation coefficient results for both volumes and lengths. The accuracy analysis revealed a discrepancy in all cases of around 3%, which corresponded to 3 ml and 1 mm for volume and length measurements, respectively. Similar results were found for both of the motion-tracking systems.
Conclusions
The undertaken analyses for estimating volume and lengths acquired with 3D freehand ultrasonography demonstrated reliable design measurements and suitable performance for applications that do not require sub-mm and -ml accuracy.}
}
@article{KRUPIARZ20061071,
title = {File-based data processing on MESSENGER},
journal = {Acta Astronautica},
volume = {59},
number = {8},
pages = {1071-1078},
year = {2006},
note = {Selected Proceedings of the Fifth IAA International Conference on Low Cost Planetary Missions},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2005.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0094576505002687},
author = {Christopher J. Krupiarz and David A. Artis and Andrew B. Calloway and Constantine M. Frangos and Brian K. Heggestad and Douglas B. Holland and William C. Stratton},
abstract = {As part of the system software, MESSENGER will be using a file-based system for the transfer and processing of instrument and spacecraft data. The flow of files within the MESSENGER software architecture begins with the receipt of science data by the main processor and the creation of files containing these data by the flight software. The files are then autonomously selected for downlink via a priority-based algorithm, packaged for transmittal via the CCSDS File Delivery Protocol (CFDP), and radiated to the ground. The ground software reconstructs the files via its implementation of CFDP, performs further processing on the file, and sends it to the operations data archive and the Science Operations Center. The MESSENGER spacecraft operations team manages the overall handling of these files through interaction with both the flight and ground systems.}
}
@incollection{YANG20011121,
title = {An open thermodynamics server for integrated process engineering environments},
editor = {Rafiqul Gani and Sten Bay Jørgensen},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {9},
pages = {1121-1126},
year = {2001},
booktitle = {European Symposium on Computer Aided Process Engineering - 11},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(01)80180-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794601801802},
author = {A.-D Yang and L {von Wedel} and W Marquardt},
abstract = {Publisher Summary
Computer-aided process engineering (CAPE) practice has shown that thermodynamics software tools play a critical role in the process engineering lifecycle, including process modeling, simulation, optimization, and process design. To provide CAPE applications unified and improved thermodynamics services, a component-based open thermodynamics server (OpenThermo) has been developed. OpenThermo is able to carry out open and closed-form physical property calculations and to supply and process symbolic thermodynamic model equations. These services are provided based on the integration of individual thermodynamics resources, such as physical property databases and physical property packages. Such functionality is achieved with a layered software architecture following a resource/service-based classification of the software components that comprise the thermodynamics sever. So far, the major functionality of the service components has been implemented, and several resource components have been connected to OpenThermo.}
}
@article{TERUZZI2019103,
title = {Parallel implementation of a data assimilation scheme for operational oceanography: The case of the MedBFM model system},
journal = {Computers & Geosciences},
volume = {124},
pages = {103-114},
year = {2019},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0098300418302334},
author = {A. Teruzzi and P. {Di Cerbo} and G. Cossarini and E. Pascolo and S. Salon},
abstract = {The MedBFM model system provides forecasts and reanalysis of the Mediterranean Sea biogeochemistry for the European Copernicus Services. The system integrates model and observations through a 3D variational assimilation scheme, whose performance capitalizes on present HPC systems and ensures compliance with the service requirements. Domain decomposition with message passing paradigm was implemented to parallelize the assimilation code and maximize performance and scalability. In particular, the parallelization of the horizontal recursive filtering algorithm was implemented using a dynamic sliced domain decomposition. Moreover, the efficient parallel solver of the PETSc/TAO library was adopted for optimizing the cost function minimization on which the variational assimilation is based. Considering the complex domain decomposition of the Mediterranean Sea and the high variability of the data input, a modern and scalable software application for variational assimilation schemes that exploits the performance of modern HPC architectures on the whole node was developed.}
}