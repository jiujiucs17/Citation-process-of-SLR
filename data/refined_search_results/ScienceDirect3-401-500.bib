@article{ZHAN2005279,
title = {Parallel Fortran-MPI software for numerical inversion of the Laplace transform and its application to oscillatory water levels in groundwater environments},
journal = {Environmental Modelling & Software},
volume = {20},
number = {3},
pages = {279-284},
year = {2005},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204001288},
author = {Xiaoyong Zhan},
keywords = {Groundwater environment, Oscillatory water levels, Numerical inversion of the Laplace transform, Fortran, Message Passing Interface (MPI)},
abstract = {A parallel Fortran-MPI (Message Passing Interface) software for numerical inversion of the Laplace transform based on a Fourier series method is developed to meet the need of solving intensive computational problems involving oscillatory water level's response to hydraulic tests in a groundwater environment. The software is a parallel version of ACM (The Association for Computing Machinery) Transactions on Mathematical Software (TOMS) Algorithm 796. Running 38 test examples indicated that implementation of MPI techniques with distributed memory architecture speedups the processing and improves the efficiency. Applications to oscillatory water levels in a well during aquifer tests are presented to illustrate how this package can be applied to solve complicated environmental problems involved in differential and integral equations. The package is free and is easy to use for people with little or no previous experience in using MPI but who wish to get off to a quick start in parallel computing.}
}
@article{GIUSTO2004245,
title = {Rapid design exploration of safety-critical distributed automotive applications via virtual integration platforms},
journal = {Journal of Systems and Software},
volume = {70},
number = {3},
pages = {245-262},
year = {2004},
note = {Rapid system prototyping},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(03)00072-4},
url = {https://www.sciencedirect.com/science/article/pii/S0164121203000724},
author = {Paolo Giusto and Thilo Demmeler},
abstract = {Modern automotive applications such as Drive-by-Wire are implemented over distributed architectures where electronic control units (ECU’s) communicate via broadcast buses. In this paper, we present the concept of virtual integration platform for automotive applications. The platform provides the basis for early analysis and validation of distributed applications, therefore enhancing the current model based development process techniques that are applied to one ECU at a time. The virtual prototype includes both functional and performance (time) models of the application software, scheduling policies, and the bus communication protocols. As a result, since design errors can be found earlier in the design process before the different sub-systems are integrated in the car, savings in both production and development costs can be achieved. The virtual integration platform concept is supported by an integrated IP-based tool environment for authoring, integration, and validation. First, a model of the distributed application is built by composing models of HW and SW components. The models can be either authored or imported from different tools. Functional simulation of the overall distributed control algorithm can be carried out first. Then, the mapping phase can take place: sub-functions of the control algorithm are mapped to architectural resources (CPUs), and zero-time communication links between the sub-functions are mapped to bus protocol delay models. Changing mappings, parameters such as task priorities, and bus schedule enables the exploration of alternative implementations. The validation is carried out by simulating the resulted virtual prototype of the distributed control algorithm running on the ECU network. The design environment shortens design turn-around time by supporting (semi)-automatic configuration of the architecture model (e.g. frame packaging, redundancy level, communication matrix, bus and RTOS scheduling, etc.).}
}
@article{DERANGO2021133,
title = {OpenCAL system extension and application to the three-dimensional Richards equation for unsaturated flow},
journal = {Computers & Mathematics with Applications},
volume = {81},
pages = {133-158},
year = {2021},
note = {Development and Application of Open-source Software for Problems with Numerical PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2020.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0898122120302248},
author = {Alessio {De Rango} and Luca Furnari and Andrea Giordano and Alfonso Senatore and Donato D’Ambrosio and William Spataro and Salvatore Straface and Giuseppe Mendicino},
keywords = {OpenCAL, Extended cellular automata, Richards equation, Heterogeneous computing, Domain-specific language, Quantization},
abstract = {OpenCAL is a scientific software library specifically developed for the simulation of 2D and 3D complex dynamical systems on parallel computational devices. It is written in C/C++ and relies on OpenMP/OpenCL and MPI for parallel execution on multi-/many-core devices and clusters of computers, respectively. The library provides the Extended Cellular Automata paradigm as a high-level formalism for modeling complex systems on structured computational grids. As a consequence, it can be used as a parallel explicit solver of ordinary and partial differential equations through classical methods, including finite-difference and finite-volume. Here the latest version of the library is described, introducing the MPI infrastructure over the 3D OpenCL and 2D/3D OpenMP components. The implementation of a three-dimensional unsaturated flow model based on a direct discrete formulation of the Richards’ equation is also shown, corresponding to a finite-difference scheme. Computational performances have been assessed on both a scientific workstation equipped with a dual Intel Xeon socket and three Nvidia GPUs, and a 16 nodes cluster with a fast interconnection network. The OpenCAL embedded quantization optimization is also discussed and exploited to drastically reduce computing time.}
}
@article{OSORIOMURILLO201598,
title = {Software framework for inverse modeling and uncertainty characterization},
journal = {Environmental Modelling & Software},
volume = {66},
pages = {98-109},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000201},
author = {Carlos A. Osorio-Murillo and Matthew W. Over and Heather Savoy and Daniel P. Ames and Yoram Rubin},
keywords = {Modeling frameworks, Method of Anchored Distributions, Inverse modeling, Spatial random fields, Model integration},
abstract = {Estimation of spatial random fields (SRFs) is required for predicting groundwater flow, subsurface contaminant movement, and other areas of environmental and earth sciences modeling. This paper presents an inverse modeling framework called MAD# for characterizing SRFs, which is an implementation of the Bayesian inverse modeling technique Method of Anchored Distributions (MAD). MAD# allows modelers to “wrap” simulation models using an extensible driver architecture that exposes model parameters to the inversion engine. MAD# is implemented in an open source software package with the goal of lowering the barrier to using inverse modeling in education, research, and resource management. MAD# includes an intentionally simple user interface for simulation configuration, external software integration, spatial domain and model output visualization, and evaluation of model convergence. Four test cases are presented demonstrating the novel functionality of this framework to apply inversion in order to calibrate the model parameters characterizing a groundwater aquifer.}
}
@article{SEMERARO199767,
title = {PVM implementation of the symmetric-Galerkin method},
journal = {Engineering Analysis with Boundary Elements},
volume = {19},
number = {1},
pages = {67-72},
year = {1997},
note = {High Performance Computing},
issn = {0955-7997},
doi = {https://doi.org/10.1016/S0955-7997(97)00026-X},
url = {https://www.sciencedirect.com/science/article/pii/S095579979700026X},
author = {B.D. Semeraro and L.J. Gray},
keywords = {Boundary element method, parallel processing, workstation cluster, proximity sensor, block linear algebra algorithms},
abstract = {We report on initial progress towards a parallel virtual machine (PVM) implementation of the symmetric-Galerkin boundary integral method. We take advantage of software packages specifically designed to solve linear algebra problems on distributed memory parallel computers. In particular we use linear algebra routines from the ScaLAPACK, PBLAS and BLACS libraries. These routines assume a block cyclic decomposition of the matrix operands. The decomposition of the operands and its impact on the construction of the coefficient matrix are described. Computational results for solving the two-dimensional Laplace equation are presented. This program is being used to simulate the performance of a proximity sensor used in robotics and other applications.}
}
@article{XIE2018207,
title = {Measuring the Impact of Nonignorable Missingness Using the R Package isni},
journal = {Computer Methods and Programs in Biomedicine},
volume = {164},
pages = {207-220},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718305029},
author = {Hui Xie and Weihua Gao and Baodong Xing and Daniel F. Heitjan and Donald Hedeker and Chengbo Yuan},
keywords = {Analytical reliability, Data quality, Missing data, Missing not at random, Multivariate normal, Selection model},
abstract = {Background and Objective: The popular assumption of ignorability simplifies analyses with incomplete data, but if it is not satisfied, results may be incorrect. Therefore it is necessary to assess the sensitivity of empirical findings to this assumption. We have created a user-friendly and freely available software program to conduct such analyses. Method: One can evaluate the dependence of inferences on the assumption of ignorability by measuring their sensitivity to its violation. One tool for such an analysis is the index of local sensitivity to nonignorability (ISNI), which evaluates the rate of change of parameter estimates to the assumed degree of nonignorability in the neighborhood of an ignorable model. Computation of ISNI avoids the need to estimate a nonignorable model or to posit a specific magnitude of nonignorability. Our new R package, named isni, implements ISNI analysis for some common data structures and corresponding statistical models. Result: The isni package computes ISNI in the generalized linear model for independent data, and in the marginal multivariate Gaussian model and the linear mixed model for longitudinal/clustered data. It allows for arbitrary patterns of missingness caused by dropout and/or intermittent missingness. Examples illustrate its use and features. Conclusions: The R package isni enables a systematic and efficient sensitivity analysis that informs evaluations of reliability and validity of empirical findings from incomplete data.}
}
@article{SIAD201636,
title = {A new framework for implementing identity-based cryptosystems},
journal = {Journal of Systems and Software},
volume = {118},
pages = {36-48},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.04.059},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300346},
author = {A. Siad and M. Amara},
keywords = {Cryptographic protocols, Distributed protocols, Software implementation},
abstract = {Identity-Based Encryption (IBE) suffers from the problem of trust in the Private Key Generator (PKG), which translates into the ability of the PKG to produce and distribute multiple private keys or multiple copies of a single key without knowledge of the genuine user. This problem makes the deployment of these systems limited to areas where trust in the PKG must have a high level. Many works addressed this problem and proposed a wide range of key generation protocols which grew from simple protocols run between user and PKG to complex and interactive protocols involving distributed computations. However, the implementation of such complex protocols requires much programming efforts and the few existing tools and libraries deal with special case protocols. In this paper, we present the first complete, efficient and modular framework, composed of a set of libraries, which brings together the most known techniques of private-key generation for identity-based cryptosystems. Our framework aims at providing robust tools designed in a modular and reusable manner to allow developers to implement the latest results coming from theoretical cryptography.}
}
@article{HU201629,
title = {Improving interoperability between architectural and structural design models: An industry foundation classes-based approach with web-based tools},
journal = {Automation in Construction},
volume = {66},
pages = {29-42},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516300152},
author = {Zhen-Zhong Hu and Xiao-Yang Zhang and Heng-Wei Wang and Mohamad Kassem},
keywords = {BIM, Data Model, IFC, Model Conversion, Structural Analysis, WebGL},
abstract = {Medium and large construction projects typically involve multiple structural consultants who use a wide range of structural analysis applications. These applications and technologies have inadequate interoperability and there is still a dearth of investigations addressing interoperability issues in the structural engineering domain. This paper proposes a novel approach which combines an industry foundation classes (IFC)-based Unified Information Model with a number of algorithms to enhance the interoperability: (a) between architectural and structural models, and (b) among multiple structural analysis models (bidirectional conversion or round tripping). The proposed approach aims to achieve the conversion by overcoming the inconsistencies in data structures, representation logics and syntax used in different software applications. The approach was implemented in both Client Server (C/S) and Browser Server (B/S) environments to enable central and remote collaboration among geographically dispersed users. The platforms were tested in four large real-life projects. The testing involved four key scenarios: (a) the bidirectional conversion among four structural analysis tools; (b) the comparison of the conversion via the proposed approach with the conversion via direct links among the involved tools; (c) the direct export from an IFC-based architectural tool through the Application Program Interface (API), and (d) the conversion and visualization of structural analysis results. All these scenarios were successfully performed and tested in four significant case studies. In particular, the conversion among the four structural analysis applications (ETABS, SAP2000, ANSYS and MIDAS) was successfully tested for all possible conversion routes among the four applications in two of the case studies (i.e., Project A and Project B). The first four steps of natural mode shapes and their natural vibration periods were calculated and compared with the converted models. They were all achieved within a standard deviation of 0.1s and 0.2s in Project A and Project B, respectively, indicating an accurate conversion.}
}
@incollection{TAYLOR2013167,
title = {Chapter 16 - Preparing the System Security Plan},
editor = {Laura P. Taylor},
booktitle = {FISMA Compliance Handbook},
publisher = {Syngress},
address = {Boston},
pages = {167-199},
year = {2013},
isbn = {978-0-12-405871-2},
doi = {https://doi.org/10.1016/B978-0-12-405871-2.00016-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058712000166},
author = {Laura P. Taylor},
keywords = {System Security Plan, SSP, SP 800-53, SP 800-18, System boundaries, Security controls, Security requirements, Security Assessment Report, Interconnections, ISSO appointment letter},
abstract = {The System Security Plan is the most important document in the Security Package. IT sums up the system description, system boundary, architecture, and security control in one document. All systems that are general support systems or major applications must have a System Security Plan. When describing system boundaries, keep in mind that your description should be inclusive of and should discuss the systems you described in your Hardware and Software Inventory. The system delineated by the boundary definition should be consistent with what comes under the purview of the system owner. In the System Security Plan, you describe how all security controls are implemented.}
}
@article{ANDREWS20111171,
title = {An open software environment for hydrological model assessment and development},
journal = {Environmental Modelling & Software},
volume = {26},
number = {10},
pages = {1171-1185},
year = {2011},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2011.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364815211001046},
author = {F.T. Andrews and B.F.W. Croke and A.J. Jakeman},
keywords = {Model evaluation, Hydrological models, Modelling frameworks, Unit hydrograph, Event separation, R},
abstract = {The hydromad (Hydrological Model Assessment and Development) package provides a set of functions which work together to construct, manipulate, analyse and compare hydrological models. The class of hydrological models considered are dynamic, spatially-aggregated conceptual or statistical models. The package functions are designed to fit seamlessly into the R system, and builds on its powerful data manipulation and analysis capabilities. The framework used in the package encourages a separation of model components based on Unit Hydrograph theory; many published models are consistent with this and implementations of several are included. For comparative assessment, model performance can be analysed over time and with respect to covariates to reveal systematic biases. Support has been built in for event-based analysis of data and assessment of model performance. Fit statistics can be defined by choices of (1) temporal scale and aggregation function; (2) weighting and transformation; and (3) reference model. One can define new Soil Moisture Accounting models, routing models, calibration methods, objective functions, and evaluation statistics, while retaining as much of the default framework as is useful. And as the package code is available under a free software licence, one always has the freedom to adapt it as required. Use of the software is demonstrated in a case study of the Queanbeyan River catchment in South-East Australia.}
}
@incollection{OSIS2017239,
title = {Chapter 11 - Components and Deployment Design},
editor = {Janis Osis and Uldis Donins},
booktitle = {Topological UML Modeling},
publisher = {Elsevier},
address = {Boston},
pages = {239-245},
year = {2017},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-805476-5},
doi = {https://doi.org/10.1016/B978-0-12-805476-5.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054765000113},
author = {Janis Osis and Uldis Donins},
keywords = {Components, interfaces, nodes, relationships, nonfunctional requirements, component diagram, deployment diagram},
abstract = {Components and deployment design is an activity within Topological UML modeling, it follows the structuring logical layout of software design activity and concludes the Topological UML modeling process. Components are designed in accordance with packages and nonfunctional requirements while deployment is planned for the designed components in accordance with nonfunctional requirements. The components design is depicted by using component diagram showing the internal parts, connectors, and ports that implement a component. Component represents a modular part of a system that encapsulates its contents. It defines its behavior in terms of provided and required interfaces. The planned deployment is reflected by using deployment diagram which commonly is used to specify how the components of a system are distributed across the infrastructure and how they are related together. To model such a view deployment diagrams use just two kinds of elements—nodes (i.e., components of a system or the infrastructure artifacts) and relationships that link nodes together.}
}
@article{SARIMBEKOV2014344,
title = {Dynamic program analysis—Reconciling developer productivity and tool performance},
journal = {Science of Computer Programming},
volume = {95},
pages = {344-358},
year = {2014},
note = {Special Section: ACM SAC-SVT 2013 + Bytecode 2013},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2014.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167642314001543},
author = {Aibek Sarimbekov and Yudi Zheng and Danilo Ansaloni and Lubomír Bulej and Lukáš Marek and Walter Binder and Petr Tůma and Zhengwei Qi},
keywords = {Dynamic program analysis, Bytecode instrumentation, Development productivity, Controlled experiment},
abstract = {Dynamic program analysis tools serve many important software engineering tasks such as profiling, debugging, testing, program comprehension, and reverse engineering. Many dynamic analysis tools rely on program instrumentation and are implemented using low-level instrumentation libraries, resulting in tedious and error-prone tool development. Targeting this issue, we have created the Domain-Specific Language for Instrumentation (DiSL), which offers high-level programming abstractions especially designed for instrumentation-based dynamic analysis. When designing DiSL, our goal was to boost the productivity of tool developers targeting the Java Virtual Machine, without impairing the performance of the resulting tools. In this paper we assess whether DiSL meets this goal. First, we perform a controlled experiment to measure tool development time and correctness of the developed tools, comparing DiSL with a prevailing, state-of-the-art instrumentation library. Second, we recast 10 open-source software engineering tools in DiSL and compare source code metrics and performance with the original implementations. Our studies show that DiSL significantly improves developer productivity, enables concise tool implementations, and does not have any negative impact on tool performance.}
}
@article{PALYANOV20161,
title = {Application of smoothed particle hydrodynamics to modeling mechanisms of biological tissue},
journal = {Advances in Engineering Software},
volume = {98},
pages = {1-11},
year = {2016},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2016.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0965997816300618},
author = {Andrey Palyanov and Sergey Khayrulin and Stephen D. Larson},
keywords = {Computational fluid dynamics, PCISPH, Biomechanics, Elastic matter, Liquid impermeable elastic membranes, Contractile matter, High-performance parallel computing, Open science},
abstract = {A prerequisite for simulating the biophysics of complex biological tissues and whole organisms are computational descriptions of biological matter that are flexible and can interface with materials of different viscosities, such as liquid. The landscape of software that is easily available to do such work is limited and lacks essential features necessary for combining elastic matter with simulations of liquids. Here we present an open source software package called Sibernetic, designed for the physical simulation of biomechanical matter (membranes, elastic matter, contractile matter) and environments (liquids, solids and elastic matter with variable physical properties). At its core, Sibernetic is built as an extension to Predictive–Corrective Incompressible Smoothed Particle Hydrodynamics (PCISPH). Sibernetic is built on top of OpenCL, making it possible to run simulations on CPUs or GPUs, and has 3D visualization support built on top of OpenGL. Several test examples of the software running and reproducing physical experiments, as well as performance benchmarks, are presented and future directions are discussed.}
}
@article{CHEN2005331,
title = {Interpretive OpenGL for computer graphics},
journal = {Computers & Graphics},
volume = {29},
number = {3},
pages = {331-339},
year = {2005},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2005.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0097849305000415},
author = {Bo Chen and Harry H. Cheng},
keywords = {Methodology and techniques—interaction techniques, Graphics utilities—software support, Graphics systems—distributed/network graphics},
abstract = {OpenGL is the industry-leading, cross-platform graphics application programming interface (API), and the only major API with support for virtually all operating systems. Many languages, such as Fortran, Java, Tcl/Tk, and Python, have OpenGL bindings to take advantage of OpenGL visualization power. In this article, we present Ch OpenGL Toolkit, a truly platform-independent Ch binding to OpenGL for computer graphics. Ch is an embeddable C/C++ interpreter for cross-platform scripting, shell programming, numerical computing, and embedded scripting. Ch extends C with salient numerical and plotting features. Like some mathematical software packages, such as MATLAB, Ch has built-in support for two and three-dimensional graphical plotting, computational arrays for vector and matrix computation, and linear system analysis with advanced numerical analysis functions based on LAPACK. Ch OpenGL Toolkit allows OpenGL application developers to write applications in a cross-platform environment, and all of the OpenGL application source code can readily run on different platforms without compilation and linking processes. In addition, the syntax of Ch OpenGL Toolkit is identical to C interface to OpenGL. Ch OpenGL Toolkit saves OpenGL programmers’ energies for solving problems without struggling with mastering new language syntax. Ch OpenGL Toolkit is embeddable. Embedded Ch OpenGL graphics engine enables graphical application developers or users to dynamically generate and manipulate graphics at run-time. The truly platform independent, scriptable, and embeddable features of Ch OpenGL Toolkit make it a good candidate for rapid prototyping, mobile graphics applications, Web-based applications, and classroom interactive presentation. The design issues of Ch OpenGL Toolkit and its potential applications are presented in the article. A methodology that can be used to implement a Web-based visualization system based on Ch OpenGL and Ch CGI is also introduced. The method described in the article can be easily followed to create a Web-based visualization system at low cost and with minimal effort. The software packages Ch and Ch CGI Toolkit are freely available and can be downloaded from the Internet.}
}
@incollection{LIU2014301,
title = {Chapter 11 - Modeling Techniques},
editor = {G.R. Liu and S.S. Quek},
booktitle = {The Finite Element Method (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
address = {Oxford},
pages = {301-345},
year = {2014},
isbn = {978-0-08-098356-1},
doi = {https://doi.org/10.1016/B978-0-08-098356-1.00011-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080983561000114},
author = {G.R. Liu and S.S. Quek},
keywords = {modeling techniques, FEM models, finite element analysis, reliability, accuracy, FEM software, practical modeling techniques},
abstract = {In this chapter, various modeling techniques for creating FEM models will be introduced. These techniques are necessary when carrying out finite element analysis to help to ensure the reliability and accuracy of the results obtained. FEM software packages are very often used as a routine tool in the design process. The primary objective of this chapter is to present practical modeling techniques that ensure proper implementations of the FEM theory, and to avoid unnecessary mistakes in creating an FEM model when using a commercial package.}
}
@article{DELASEN1986219,
title = {Computer-aided design for a real-time acceleration of the convergence of adaptive control algorithms},
journal = {Computer-Aided Design},
volume = {18},
number = {4},
pages = {219-223},
year = {1986},
issn = {0010-4485},
doi = {https://doi.org/10.1016/0010-4485(86)90133-8},
url = {https://www.sciencedirect.com/science/article/pii/0010448586901338},
author = {M. {de la Sen}},
keywords = {adaptive control, acceleration of sequences, adaptation transients},
abstract = {This paper presents a technique for the computer-aided design of modified adaptive controllers for improving the transients in adaptive systems. It is based on the adaptation of well known parallel tools from numerical analysis to this problem. The software package has a modular structure. Each module has a specific task corresponding to the hardware design. In structing the modules, both the expected hardware implementation facilities, and ease of programming are taken into account. Also, a basic library of functions and routines has been designed.}
}
@article{AMELI20072161,
title = {Integrated distributed energy evaluation software (IDEAS): Simulation of a micro-turbine based CHP system},
journal = {Applied Thermal Engineering},
volume = {27},
number = {13},
pages = {2161-2165},
year = {2007},
note = {Heat Powered Cycles – 04},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2005.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S135943110500267X},
author = {S.M. Ameli and B. Agnew and I. Potts},
keywords = {CHP, Combined heat power, Micro-turbine, IPS-Pro, Fuel cell, Renewable},
abstract = {IDEAS was defined in University of Newcastle upon Tyne in United Kingdom as a joint academic-industry project, in order to develop a comprehensive software package for designing, optimizing and monitoring of distributed energy systems based on micro-turbine, fuel cell and internal combustion engine driven systems using fossil fuels and renewables. The IDEAS idea shaped in pursue of a series of actual projects at Research Centre for Innovation and Design (RCID) to suggest the most energy consumption optimized and environmentally friendly distributed CHP system for some new residential complex as well as some academic PhD thesis at Mechanical and Systems engineering department to optimize the performance of gas turbine and micro-turbine driven CHP system for generating power, cooling and heating. Increasing number of CHP equipment suppliers and consultant engineering companies through UK and Europe and approved inspiring short-term and long-term policies of EU for implementing the CHP systems, fascinating successful experiences in some EU countries like Denmark in CHP implementation and lack of a comprehensive European based software package for designing and optimizing of these systems, all supported the idea of developing the IDEAS. IDEAS tends to be a complement to tens of small CHP systems related European software, each has been developed with limited data and a European version of similar powerful non-European (mostly American) software packages. To gain an overview of the difficulties, musts, domain of the package abilities, required financial, human resources and information and also for a better presentation of the idea to absorb supports from both academic and industry possible partners, a micro-turbine driven CHP system was designed for a 71 dwelling residential complex in north of England. This paper presents the works which have been done and yielded results about the requirements of developing IDEAS.}
}
@article{MURAT2023108958,
title = {Research and simulation of magnetically controlled shunt MCRs using Matlab Simulink},
journal = {Electric Power Systems Research},
volume = {214},
pages = {108958},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108958},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622010070},
author = {Aibek Murat and Sergey Sokolov and Irina Sokolova and Lyazzat Uteshkaliyeva},
keywords = {Reactive power compensation, Shunt MCR, Three-phase three-limb-core type, Matlab Simulink, FACTS devices},
abstract = {The paper presents the history of the creation of controlled reactors with DC bias, describes the principle of operation and the main types of designs of controlled reactors with DC bias. The basic principles of regulation, compensation of higher harmonics and the possibility of manufacturing controlled reactors based on standard transformers are shown. An adequate mathematical model of a three-limb-core magnetically controllable reactor (MCR) has been created in the Matlab Simulink software package. Different magnetic analyzes can be made by changing core structures and changing the winding connections. The adequacy of the model has been verified by experimental studies on physical models of the MCR. Сurrent harmonics were analyzed in the entire range of MCR control. The operability of an MCR made of a three-limb magnetic circuit has been proven. Moreover, mathematical model can solve the problem that the previous PSCAD model cannot accurately simulate iron core size optimization and provide guarantee for further research in the future.}
}
@article{WANG201456,
title = {A CAD/CAE integrated framework for structural design optimization using sequential approximation optimization},
journal = {Advances in Engineering Software},
volume = {76},
pages = {56-68},
year = {2014},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2014.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S096599781400091X},
author = {Donghui Wang and Fan Hu and Zhenyu Ma and Zeping Wu and Weihua Zhang},
keywords = {Sequential approximation optimization, CAD/CAE integration, Integrated framework, Surrogate model, Adaptive sampling strategy, Structural design},
abstract = {This paper presents an open and integrated framework that performs the structural design optimization by associating the improved sequential approximation optimization (SAO) algorithm with the CAD/CAE integration technique. In the improved SAO algorithm, a new estimate of the width of Gaussian kernel functions is proposed to enhance the surrogate models for SAO. Based on the improved surrogate models, an adaptive sampling strategy is developed to balance the exploration/exploitation in the sampling process, which better balances between the competence to locate the global optimum and the computation efficiency in the optimization process. Fewer function evaluations are required to seek the optimum, which is of great significance for computation-intensive structural optimization problems. Moreover, based on scripting program languages and Application Programming Interfaces (APIs), integration between commercial CAD and CAE software packages is implemented to expand the applications of the SAO algorithm in mechanical practices. Two benchmark tests from simple to complex, from low-dimension to moderate-dimension were performed to validate the efficacy of the proposed framework. Results show that the proposed approach facilitates the structural optimization process and reduces the computing cost immensely compared to other approaches.}
}
@article{VATANKHAH201919,
title = {IGUG: A MATLAB package for 3D inversion of gravity data using graph theory},
journal = {Computers & Geosciences},
volume = {128},
pages = {19-29},
year = {2019},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0098300418309221},
author = {Saeed Vatankhah and Vahid Ebrahimzadeh Ardestani and Susan Soodmand Niri and Rosemary Anne Renaut and Hojjat Kabirzadeh},
keywords = {Gravity, 3D inversion, Graph theory, Equidistance function, Mobrun},
abstract = {We present an open source MATLAB package, IGUG, for 3D inversion of gravity data. The algorithm implemented in this package is based on methodology that was introduced by Bijani et al. (2015). A homogeneous subsurface body is modeled by an ensemble of simple point masses. The model parameters are the Cartesian coordinates of the point masses and their total mass. The set of point masses, assumed to each have the same mass, is associated to the vertices of a weighted complete graph in which the weights are computed by the Euclidean pairwise distances separating vertices. Kruskal's algorithm is used to solve the minimum spanning tree (MST) problem for the graph, yielding the reconstruction of the skeleton of the body described by the model parameters. The algorithm is stabilized using an equidistance function that restricts the spatial distribution of point masses and favors a homogeneous distribution for the subsurface structure. The non-linear global objective function for the model parameters comprises the data misfit term and the equidistance stabilization function. A regularization parameter λ is introduced to balance the two terms of the objective function, and reasonable physically-relevant bound constraints are imposed on the model parameters. A genetic algorithm is used to minimize the bound constrained objective function for a fixed λ, subject to the bound constraints. A new diagnostic approach is presented for determining a suitable choice for λ, requiring a limited number of solutions for a small set of λ. This contrasts the use of the L-curve which was suggested for estimating a suitable λ in Bijani et al. (2015). Simulations for synthetic examples demonstrate the efficiency and effectiveness of the implementation of the algorithm. It is verified that the constraints on the model parameters are not restrictive, even with less realistic bounds acceptable approximations of the body are still obtained. Included in the package is the script GMD.m which is used for generating synthetic data and for putting measurement data in the format required for the inversion implemented within IGUG.m. The script Diagnostic_Results.m is included within IGUG.m for analyzing and visualizing the results, but can also be used as a standalone script given import of prior results. The software can be used to verify the simulations and the analysis of real data that is presented here. The real data set uses gravity data from the Mobrun ore body, north east of Noranda, Quebec, Canada.}
}
@article{KONSTANTINOVIC1986257,
title = {A graphics software package for interactive graphics terminal},
journal = {Computers & Graphics},
volume = {10},
number = {3},
pages = {257-261},
year = {1986},
issn = {0097-8493},
doi = {https://doi.org/10.1016/0097-8493(86)90010-5},
url = {https://www.sciencedirect.com/science/article/pii/0097849386900105},
author = {Zoran Konstantinović and Ljiljana B. Damnjanović},
abstract = {GSP, a graphics software package that supports a high resolution interactive raster graphics terminal, is described. The software is primarily designed to provide portable elements for supporting interactive graphics terminals in various hardware configurations. The graphics library, which contains 2-D graphics functions based on Graphical Kernel System as well as some 3-D graphics functions, has been implemented. The Graphics Software Package includes Graphics System Support software as the main component in order to realize the following: one level of segmentations, image transformation, viewing in 2-D and 3-D, planar geometric projections and clipping. It also uses Basic System Support to provide facilities for the efficient control of the graphical peripherals, such as joysticks, tablets and light pens. Most of the graphics software has been written in C language and is designed to be almost entirely machine independent.}
}
@article{CORNUBERT199587,
title = {Benchmark of application software kernels on the SUPERNODE SN1000 using the 3P PARLIB},
journal = {Future Generation Computer Systems},
volume = {11},
number = {1},
pages = {87-109},
year = {1995},
note = {BECAUSE Workshop, Part II},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(94)00050-O},
url = {https://www.sciencedirect.com/science/article/pii/0167739X9400050O},
author = {R. Cornubert and G. Gruez and P. Steinfeld and E. Znaty},
keywords = {BECAUSE Project, Benchmarking, Distributed Memory, MIMD Parallel Architectures, Data Parallel Programming, Data Mapping, Domain Decomposition Method (DDM)},
abstract = {This article presents the benchmarking by BERTIN (F) of the SUPERNODE SN1000 parallel architecture from PARSYS within the framework of the BECAUSE Project. This evaluation of a Distributed Memory parallel architecture was realised by means of the BECAUSE Benchmark Set (BBS). The very strong idea was to specify parallelisation methodologies and to develop parallel software which are machine independent and as such portable. This approach was possible and realistic since the principle of parallelism which is involved is the Data Parallel Programming. As a consequence, the hardware features of the target architecture are transparent to the industrial user and are managed through a communication library called 3P PARLIB. In this paper, principles of parallelisation which were used are presented. Practical implementation of these parallelisation principles is illustrated with various significant Test Programs from the BBS. The corresponding results are presented. Specifications for the 3P PARLIB (Portable parallel programming library) are also given.}
}
@article{TOYOOKA1991115,
title = {Open Architecture Direct Drive Manipulator Research and Development Package},
journal = {IFAC Proceedings Volumes},
volume = {25},
number = {12},
pages = {115-121},
year = {1991},
note = {IFAC Symposium on Advances in Control Education, Boston, MA, USA, 24-25 June 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)50099-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017500993},
author = {M. Toyooka and M. Tomizuka and J. Butler},
keywords = {Computer control, computer simulation, software development, programmable controllers, robot manipulator},
abstract = {This paper describes the Direct Drive Manipulator Research and Development Package, which includes a two link direct drive manipulator, amplifiers, a multi-processing controls workstation, a mathematical model of the manipulator system, and software environment which is used for the development and execution of real-time control algorithms. The software provides a C-language based Robot Programming Language (RPL), control algorithm development environment, and a robot simulator. The package is based on an open architecture with full user programming capability. The user can enter the control loop at any level with the ability to modify or replace any part of the RPL, servo routine, inverse kinematics, or trajectory generation modules. The package provides a convenient workstation to develop a control algorithm, simulate, implement, and fine-tune the controller. The simulation is a unique part of this package being a “semi-active” simulation which uses a mathematical model of the mechanical system while using the actual control hardware that is used for control implementation. Mechanical nonlinearities such as dynamic coupling and Coulomb friction are taken into account. Since the actual controller is used, the simulation accounts for quantization, integer overflow, and controller dynamics. This package is ideal for an advanced robotics research program or a lab oriented graduate course in robotics or controls.}
}
@article{DUMAS2016235,
title = {Recursion based parallelization of exact dense linear algebra routines for Gaussian elimination},
journal = {Parallel Computing},
volume = {57},
pages = {235-249},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2015.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819115001222},
author = {Jean-Guillaume Dumas and Thierry Gautier and Clément Pernet and Jean-Louis Roch and Ziad Sultan},
keywords = {PLUQ decomposition, Parallel shared memory computation, Finite field, Dataflow task dependencies, NUMA architecture, Rank deficiency},
abstract = {We present block algorithms and their implementation for the parallelization of sub-cubic Gaussian elimination on shared memory architectures. Contrarily to the classical cubic algorithms in parallel numerical linear algebra, we focus here on recursive algorithms and coarse grain parallelization. Indeed, sub-cubic matrix arithmetic can only be achieved through recursive algorithms making coarse grain block algorithms perform more efficiently than fine grain ones. This work is motivated by the design and implementation of dense linear algebra over a finite field, where fast matrix multiplication is used extensively and where costly modular reductions also advocate for coarse grain block decomposition. We incrementally build efficient kernels, for matrix multiplication first, then triangular system solving, on top of which a recursive PLUQ decomposition algorithm is built. We study the parallelization of these kernels using several algorithmic variants: either iterative or recursive and using different splitting strategies. Experiments show that recursive adaptive methods for matrix multiplication, hybrid recursive–iterative methods for triangular system solve and tile recursive versions of the PLUQ decomposition, together with various data mapping policies, provide the best performance on a 32 cores NUMA architecture. Overall, we show that the overhead of modular reductions is more than compensated by the fast linear algebra algorithms and that exact dense linear algebra matches the performance of full rank reference numerical software even in the presence of rank deficiencies.}
}
@article{KALVIAINEN1996889,
title = {Houghtool — A software package for the use of the Hough transform},
journal = {Pattern Recognition Letters},
volume = {17},
number = {8},
pages = {889-897},
year = {1996},
issn = {0167-8655},
doi = {https://doi.org/10.1016/0167-8655(96)00049-9},
url = {https://www.sciencedirect.com/science/article/pii/0167865596000499},
author = {Heikki Kälviäinen and Petri Hirvonen and Erkki Oja},
keywords = {Randomized Hough transform, Curve detection, Shape detection, Software package, Computer vision},
abstract = {The Hough Transform (HT) is a popular method for detecting curve segments in an image. A software package, the Houghtool, is proposed to calculate the HT. Several methods for line detection are implemented in the package. These include new HT techniques such as probabilistic approaches which use random sampling of the input points. The aim of the Houghtool is to provide general tools for the design of new algorithms and to provide solutions to specific applications. The software design is flexible to allow user extension of the tools provided. A graphical user interface, the XHoughtool, is provided for the visualization of the HT calculation.}
}
@article{MONGA2015720,
title = {Real-time simulation of dynamic vehicle models using a high-performance reconfigurable platform},
journal = {Microprocessors and Microsystems},
volume = {39},
number = {8},
pages = {720-740},
year = {2015},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2015.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0141933115001477},
author = {Madhu Monga and Daniel Roggow and Manoj Karkee and Song Sun and Lakshmi Kiran Tondehal and Brian Steward and Atul Kelkar and Joseph Zambreno},
keywords = {FPGA, Real-time simulation, Hardware-in-the-loop, Non-linear functions, Hardware acceleration},
abstract = {With the increase in the complexity of models and lack of flexibility offered by the analog computers, coupled with the advancements in digital hardware, the simulation industry has subsequently moved to digital computers and increased usage of programming languages such as C, C++, and MATLAB. However, the reduced time-step required to simulate complex and fast systems imposes a tighter constraint on the time within which the computations have to be performed. The sequential execution of these computations fails to cope with the real-time constraints which further restrict the usefulness of Real-Time Simulation (RTS) in a Virtual Reality (VR) environment. In this paper, we present a methodology for the design and implementation of RTS algorithms, based on the use of Field-Programmable Gate Array (FPGA) technology. We apply our methodology to an 8th order steering valve subsystem of a vehicle with relatively low response time requirements and use the FPGA technology to improve the response time of this model. Our methodology utilizes traditional hardware/software co-design approaches to generate a heterogeneous architecture for an FPGA-based simulator by porting the computationally complex regions to hardware. The hardware design was optimized such that it efficiently utilizes the parallel nature of FPGAs and pipelines the independent operations. Further enhancement was made by building a hardware component library of custom accelerators for common non-linear functions. The library also stores the information about resource utilization, cycle count, and the relative error with different bit-width combinations for these components, which is further used to evaluate different partitioning approaches. In this paper, we illustrate the partitioning of a hardware-based simulator design across dual FPGAs, initiate RTS using a system input from a Hardware-in-the-Loop (HIL) framework, and use these simulation results from our FPGA-based platform to perform response analysis. The total simulation time, which includes the time required to receive the system input over a socket (without HIL), software initialization, hardware computation, and transfer of simulation results back over a socket, shows a speedup of 2 × as compared to a similar setup with no hardware acceleration. The correctness of the simulation output from the hardware has also been validated with the simulated results from the software-only design.}
}
@article{CLARKE2019200,
title = {Pre-conditioning strategies to accelerate the convergence of iterative methods in multiphase flow simulations},
journal = {Mathematics and Computers in Simulation},
volume = {165},
pages = {200-222},
year = {2019},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0378475419301053},
author = {Lauren Elizabeth Clarke and Gautham Krishnamoorthy},
keywords = {CFD, PETSc, MFiX, BiCGSTAB, Multiphase flow},
abstract = {A computational bottleneck during the solution to multiphase formulations of the incompressible Navier–Stokes equations is often during the implicit solution of the pressure-correction equation that results from operator-splitting methods. Since density is a coefficient in the pressure-correction equation, large variations or discontinuities among the phase densities greatly increase the condition number of the pressure-correction matrix and retard the convergence of iterative methods employed in its solution. To alleviate this shortcoming, the open-source multiphase code MFiX is interfaced with the linear solver library PETSc. Through an appropriate mapping of matrix and vector data structures between the two software packages, an access to a suite of robust, scalable, solver options in PETSc is obtained. Verification of the implementation is demonstrated through predictions that are identical to those obtained from MFiX’s native solvers for a class of single-phase and multiphase flow problems. For a low Reynolds number, flow over a cylinder case, applying Right Side Block Jacobi Preconditioning to the BiCGSTAB iterative solver in PETSc was faster than MFiX’s native solver. This speed-up increased with higher mesh resolution and for higher-order spatial discretizations. In a fluidized bed simulation, this solver–preconditioner combination resulted in a 25% decrease in solve time compared to MFiX’s native solver.}
}
@article{EGHTESAD2022111348,
title = {Coupling of a multi-GPU accelerated elasto-visco-plastic fast Fourier transform constitutive model with the implicit finite element method},
journal = {Computational Materials Science},
volume = {208},
pages = {111348},
year = {2022},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2022.111348},
url = {https://www.sciencedirect.com/science/article/pii/S092702562200132X},
author = {Adnan Eghtesad and Kai Germaschewski and Marko Knezevic},
keywords = {Crystal plasticity, Finite element method, Microstructures, Parallel computing, FE-GPU-EVPCUFFT},
abstract = {This paper presents an implementation of the elasto-visco-plastic fast Fourier transform (EVPFFT) crystal plasticity model in the implicit finite element (FE) method of Abaqus standard through a user material (UMAT) subroutine to provide a constitutive relationship between stress and strain at FE integration points. To facilitate the implicit coupling ensuring fast convergence rates, an analytical Jacobian is derived. The constitutive response at every integration point is obtained by the full-field homogenization over an explicit microstructural cell. The implementation is a parallel computing approach involving multi-core central processing units (CPUs) and graphics processing units (GPUs) for computationally efficient simulations of large plastic deformation of metallic components with arbitrary geometry and loading boundary conditions. To this end, the EVPFFT solver takes advantages of GPU acceleration utilizing Nvidia’s high performance computing software development kit (SDK) compiler and compute unified device architecture (CUDA) FFT libraries, while the FE solver leverages the message passing interface (MPI) for parallelism across CPUs. The high-performance hybrid CPU-GPU multi-level framework is referred to as FE-GPU-EVPCUFFT. Simulations of simple compression of Cu and large strain cyclic reversals of dual phase (DP) 590 have been used to benchmark the accuracy of the implementation in predicting the mechanical response and texture evolution. Subsequently, two applications are presented to illustrate the potential and utility of the multi-level simulation strategy: 4-point bending of textured Zr bars, in which the model captures the shape variations as a consequence of texture with respect to the bending plane and another bending of DP1180, in which the model reveals details of spatial micromechanical fields.}
}
@article{HOYO201599,
title = {Teaching Control Engineering Concepts using Open Source tools on a Raspberry Pi board**This work has been partially funded by the following projects: DPI2014- 55932-C2-1-R and DPI2014-56364-C2-1-R (financed by the Spanish Ministry of Science and Innovation and EU- ERDF funds)},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {29},
pages = {99-104},
year = {2015},
note = {IFAC Workshop on Internet Based Control Education IBCE15},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.11.220},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315024787},
author = {Ángeles Hoyo and José Luis Guzmán and José Carlos Moreno and Manuel Berenguel},
keywords = {Programming, control, communications, Raspberry Pi, Python},
abstract = {This paper presents the idea about how to combine a set of software and hardware resources available in literature to be used as support to control engineering education. The available tools allow to mix topics related to programming, communications, operating systems, and control theory. The well-known Raspberry Pi board is used as platform to exploit the di_erent proposed concepts. SciPy, Matplotlib, and NumPy libraries, which are Python-based open-source libraries for scienti_c computing and graphical representation, are used to perform Matlab-like simulations and to implement classical control loops. On the other hand, virtual processes developed in Easy Java Simulations are adapted to be controlled through the network from a controller implemented on the Raspberry Pi with Python. This option is very useful from a teaching point of view since time-based, networked-based, or event-based control approaches can be easily introduced on this proposed architecture. Furthermore, once students know how to implement control loops on the Raspberry Pi using Python, external real processes can be easily controlled by using the GPIO interface available in the this electronic board. Then, a project based on these tools and ideas is motivated and presented in this paper to control a two-tank level process © Copyright IFAC 2015.}
}
@article{LIN19961866,
title = {Interactive Optimization-Based Design Software for Rotorcraft Flight Control Systems},
journal = {IFAC Proceedings Volumes},
volume = {29},
number = {1},
pages = {1866-1871},
year = {1996},
note = {13th World Congress of IFAC, 1996, San Francisco USA, 30 June - 5 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)57942-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017579422},
author = {Chujen Lin and Mark B. Tischler and William S. Levine},
keywords = {Design, Helicopter control, Software, Computer-aided control system design, Optimization},
abstract = {A specialized software package, known as GIFCORCODE, has been developed to aid the designer of rotorcraft flight control systems. GIFCORCODE is based on CONSOL-OPTCAD (C-O), a software package that implements a design methodology based on multicriterion, parametric optimization. GIFCORCODE includes SIMULINK® and MATLAB® files for evaluating a design's performance, defined by the Airworthiness Design Standard for military rotorcraft (ADS-33C). GIFCORCODE also includes a generalized user interface that allows the designer to interact with C-O by means of pull-down menus and graphical displays. GIFCORCODE has been used in the design of flight control systems for the UH-60A RASCAL helicopter.}
}
@article{ISUPOV202025,
title = {Design and implementation of multiple-precision BLAS Level 1 functions for graphics processing units},
journal = {Journal of Parallel and Distributed Computing},
volume = {140},
pages = {25-36},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519303302},
author = {Konstantin Isupov and Vladimir Knyazkov and Alexander Kuvaev},
keywords = {Multiple-precision arithmetic, Residue number system, Parallel processing, High-performance computing, BLAS},
abstract = {Basic Linear Algebra Subprograms (BLAS) are the building blocks for various numerical algorithms and are widely used in scientific computations. However, some linear algebra applications need more precision than the standard double precision available in most existing BLAS libraries. In this paper, we implement and evaluate multiple-precision scalar and vector BLAS functions on graphics processing units (GPUs). We use the residue number system (RNS) to represent arbitrary length floating-point numbers. The non-positional nature of RNS enables parallelism in multiple-precision arithmetic and makes RNS a good tool for high-performance computing applications. We first present new data-parallel algorithms for multiplying and adding RNS-based floating-point representations. Next, we suggest algorithms for multiple-precision vectors specially designed for parallel computations on GPUs. Using these algorithms, we develop and evaluate four GPU-accelerated multiple-precision BLAS functions, ASUM, DOT, SCAL, and AXPY. It is shown through experiments that in many cases, the implemented functions achieve significantly better performance compared to existing multiple-precision software for CPU and GPU.}
}
@article{GRIGORIEV2014517,
title = {Research and Development of a Cross-platform CNC Kernel for Multi-axis Machine Tool},
journal = {Procedia CIRP},
volume = {14},
pages = {517-522},
year = {2014},
note = {6th CIRP International Conference on High Performance Cutting, HPC2014},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2014.03.051},
url = {https://www.sciencedirect.com/science/article/pii/S2212827114001942},
author = {Sergej N. Grigoriev and Georgi M. Martinov},
keywords = {Cross platform CNC kernel, Portable real-time software, Multi-protocol control system ;},
abstract = {The demand for cross-platform solutions for real-time control systems is determined by the necessity to reduce the development cycle time, to provide broad opportunities of configuring the CNC system for multi-axis machine tools and to simplify the implementation of application solutions for shop floor. The paper proposes an approach of building a portable CNC kernel based on platform independent libraries. Open architecture CNC system offers levels of abstraction in the kernel for implementing various HMIs, accepting different part program language versions and using different fieldbuses. An example of adapting cross-platform CNC kernel for multi-channel and multi-axis machine tool is illustrated.}
}
@article{MORELL202253,
title = {Dynamic and adaptive fault-tolerant asynchronous federated learning using volunteer edge devices},
journal = {Future Generation Computer Systems},
volume = {133},
pages = {53-67},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.02.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000735},
author = {José Ángel Morell and Enrique Alba},
keywords = {Federated learning, Edge computing, Internet browser, Distributed computing, Volunteer computing, Deep learning},
abstract = {The number of devices, from smartphones to IoT hardware, interconnected via the Internet is growing all the time. These devices produce a large amount of data that cannot be analyzed in any data center or stored in the cloud, and it might be private or sensitive, thus precluding existing classic approaches. However, studying these data and gaining insights from them is still of great relevance to science and society. Recently, two related paradigms try to address the above problems. On the one hand, edge computing (EC) suggests to increase processing on edge devices. On the other hand, federated learning (FL) deals with training a shared machine learning (ML) model in a distributed (non-centralized) manner while keeping private data locally on edge devices. The combination of both is known as federated edge learning (FEEL). In this work, we propose an algorithm for FEEL that adapts to asynchronous clients joining and leaving the computation. Our research focuses on adapting the learning when the number of volunteers is low and may even drop to zero. We propose, implement, and evaluate a new software platform for this purpose. We then evaluate its results on problems relevant to FEEL. The proposed decentralized and adaptive system architecture for asynchronous learning allows volunteer users to yield their device resources and local data to train a shared ML model. The platform dynamically self-adapts to variations in the number of collaborating heterogeneous devices due to unexpected disconnections (i.e., volunteers can join and leave at any time). Thus, we conduct comprehensive empirical analysis in a static configuration and highly dynamic and changing scenarios. The public open-source platform enables interoperability between volunteers connected using web browsers and Python processes. We show that our platform adapts well to the changing environment getting a numerical accuracy similar to today’s configurations using a given number of homogeneous (hardware and software) computers as a static platform for learning. We demonstrate the fault-tolerance of the platform in self-recovering from unexpected disconnections of volunteer devices. We then prove that EC, coupled with FL, can lead to scientific tools that can be practical involving real users for final competitive numerical results in real problems for science and society.}
}
@article{OVATMAN20132754,
title = {Model-based cache-aware dispatching of object-oriented software for multicore systems},
journal = {Journal of Systems and Software},
volume = {86},
number = {11},
pages = {2754-2770},
year = {2013},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2013.06.025},
url = {https://www.sciencedirect.com/science/article/pii/S0164121213001441},
author = {Tolga Ovatman and Feza Buzluca},
keywords = {Model-based scheduling, Object-oriented design for multicore systems, Cache-aware object dispatching},
abstract = {In recent years, processor technology has evolved towards multicore processors, which include multiple processing units (cores) in a single package. Those cores, having their own private caches, often share a higher level cache memory dedicated to each processor die. This multi-level cache hierarchy in multicore processors raises the importance of cache utilization problem. Assigning parallel-running software components with common data to processor cores that do not share a common cache increases the number of cache misses. In this paper we present a novel approach that uses model-based information to guide the OS scheduler in assigning appropriate core affinities to software objects at run-time. We build graph models of software and cache hierarchies of processors and devise a graph matcher algorithm that provides mapping between these two graphs. Using this mapping we obtain candidate core sets that each software object can be affiliated with at run-time. These affiliations are determined based on the idea that software components that have the potential to share common data at run-time should run on cores that share a common cache. We also develop an object dispatcher algorithm that keeps track of object affiliations at run-time and dispatches objects by using the information from the compile-time graph matcher. We apply our approach on design pattern implementations and two different application program running on servers using CFS scheduling. Our results show that cache-aware dispatching based on information obtained from software model, decreases number of cache misses significantly and improves CFS’ scheduling performance.}
}
@article{HOFFMAN1994471,
title = {Vince: Vendor independent (and architecture flexible) network control},
journal = {Computer Networks and ISDN Systems},
volume = {27},
number = {3},
pages = {471-478},
year = {1994},
note = {Annual Conference of the Internet Society/5th Joint European Networking Conference},
issn = {0169-7552},
doi = {https://doi.org/10.1016/0169-7552(94)90121-X},
url = {https://www.sciencedirect.com/science/article/pii/016975529490121X},
author = {Eric Hoffman and Allison Mankin and Maryann Perez and S.J. Marsh},
abstract = {This paper discusses experience gained while implementing Vince, a publicly available software package for the development of protocols in ATM networks. Vince provides a simple set of general abstractions which are extremely useful for protocol development and experimentation.}
}
@article{HUFFEL1997199,
title = {NICONET: Network for Performant Numerical Software Development in Control Engineering},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {4},
pages = {199-204},
year = {1997},
note = {7th IFAC Symposium on Computer Aided Control Systems Design (CACSD '97), Gent, Belgium, 28-30 April},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)43636-6},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017436366},
author = {Sabine Van Huffel and Ad J.W {van den Boom}},
keywords = {Computer-aided control systems design, numerical algorithms, software tools, software performance, control algorithms, system theory},
abstract = {Robust and performant numerical software for control systems analysis and design, such as the SLICOT and RASP libraries, is an essential ingredient in modern computer aided control systems design. To avoid duplicating the implementation efforts and ensure a wider dissemination, the originators of the SLICOT and RASP libraries have agreed to combine their libraries and make the joint library freely available. To extend the scope of cooperation, a thematic network for numerics in control NICONET is set up. This paper motivates the need for such a network and describes the objectives, benefits and main network activities.}
}
@article{PATLAKAS2018216,
title = {Automatic code compliance with multi-dimensional data fitting in a BIM context},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {216-231},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618300880},
author = {P. Patlakas and A. Livingstone and R. Hairstans and G. Neighbour},
abstract = {BIM-based tools can contribute to addressing some of the challenges faced by structural engineering practitioners. A BIM-based framework for the development of components that deliver Automatic Code Compliance (ACC) is presented. The structural design problems that such components solve are categorised as simple, where ACC can be implemented directly, or complex, where more advanced approaches are needed. The mathematical process of Multi-Dimensional Data Fitting (MDDF) is introduced in order for the latter, enabling the compression of complex engineering calculations to a single equation that can be easily implemented into a BIM software engineering package. Proof-of-concept examples are given for both cases: offsite-manufactured structural joists are utilised as a non-recursive example, implementing the results obtained in the manufacturer’s literature; the axial capacity of metal fasteners in axially loaded timber-to-timber connections are utilised as an example of recursive problems. The MDDF analysis and its implementation in a BIM package of those problems are presented. Finally, the concept is generalised for non-structural aspects at a framework level, and the challenges, implications, and prospects of ACC in a BIM context are discussed.}
}
@article{APROVITOLA20152688,
title = {SParC-LES: Enabling large eddy simulations with parallel sparse matrix computation tools},
journal = {Computers & Mathematics with Applications},
volume = {70},
number = {11},
pages = {2688-2700},
year = {2015},
note = {Numerical Methods for Scientific Computations and Advanced Applications},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2015.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0898122115003144},
author = {Andrea Aprovitola and Pasqua D’Ambra and Filippo M. Denaro and Daniela {di Serafino} and Salvatore Filippone},
keywords = {Sparse matrix computations, Parallel software libraries, Large eddy simulation, Turbulent channel flows},
abstract = {We discuss the design and development of a parallel code for Large Eddy Simulation (LES) by exploiting libraries for sparse matrix computations. We formulate a numerical procedure for the LES of turbulent channel flows, based on an approximate projection method, in terms of linear algebra operators involving sparse matrices and vectors. Then we implement the procedure using general-purpose linear algebra libraries as building blocks. This approach allows to pursue goals such as modularity, accuracy and robustness, as well as easy and fast exploitation of parallelism, with a relatively low coding effort. The parallel LES code developed in this work, named SParC-LES (Sparse Parallel Computation-based LES), exploits two parallel libraries: PSBLAS, providing basic sparse matrix operators and Krylov solvers, and MLD2P4, providing a suite of algebraic multilevel Schwarz preconditioners. Numerical experiments, concerning the simulation by SParC-LES of a turbulent flow in a plane channel, confirm that the LES code can achieve a satisfactory parallel performance. This supports our opinion that the software design methodology used to build SParC-LES yields a very good tradeoff between the exploitation of the computational power of parallel computers and the amount of coding effort.}
}
@article{JIN2009312,
title = {Platform-independent MB-based AVS video standard implementation},
journal = {Signal Processing: Image Communication},
volume = {24},
number = {4},
pages = {312-323},
year = {2009},
note = {Special Issue on AVS and its Application},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2008.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0923596509000022},
author = {Xin Jin and Songnan Li and King Ngi Ngan},
keywords = {AVS video standard, MB-based architecture, Frame-based architecture, Embedded video codec implementation, Video coding},
abstract = {AVS1-P2 is the newest video standard of Audio Video coding Standard (AVS) workgroup of China, which provides close performance to H.264/AVC main profile with lower complexity. In this paper, a platform-independent software package with macroblock-based (MB-based) architecture is proposed to facilitate AVS video standard implementation on embedded system. Compared with the frame-based architecture, which is commonly utilized for PC platform oriented video applications, the MB-based decoder performs all of the decoding processes, except the high-level syntax parsing, in a set of MB-based buffers with adequate size for saving the information of the current MB and the neighboring reference MBs to minimize the on-chip memory and to save the time consumed in on-chip/off-chip data transfer. By modifying the data flow and decoding hierarchy, simulating the data transfer between the on-chip memory and the off-chip memory, and modularizing the buffer definition and management for low-level decoding kernels, the MB-based system architecture provides over 80% reduction in on-chip memory compared to the frame-based architecture when decoding 720p sequences. The storage complexity is also analyzed by referencing the performance evaluation of the MB-based decoder. The MB-based decoder implementation provides an efficient reference to facilitate development of AVS applications on embedded system. The complexity analysis provides rough storage complexity requirements for AVS video standard implementation and optimization.}
}
@article{QIU201952,
title = {A software tool for assessing the performance of and implementing water distribution system solution methods},
journal = {Environmental Modelling & Software},
volume = {112},
pages = {52-69},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S136481521830416X},
author = {Mengning Qiu and Bradley Alexander and Angus R. Simpson and Sylvan Elhay},
keywords = {Water distribution system, C++ toolkit, Object-oriented design, Forest-core partitioning algorithm, Reformulated Co-Tree flows method, Global gradient algorithm, Open source software},
abstract = {WDSLib is an extensible simulation toolkit for the steady-state analysis of a water distribution system. It includes a range of solution methods: the forest-core partitioning algorithm, the global gradient algorithm, the reformulated co-tree flows method, and also combinations of these methods. WDSLib has been created using a modularized object-oriented design and implemented in the C++ programming language, and has been validated against a reference MATLAB implementation. WDSLib has been designed: (i) to avoid unnecessary computations by hoisting each of the modules to its appropriate level of repetition, (ii) to perform the computations independently of measurement units using scaled variables, (iii) to accurately report the execution time of all the modules in that it is possible to produce a timing model to parameterize multiple simulation times (such as in an optimization using a genetic algorithm) from a series of sampling simulation runs and (iv) to guard against numerical failures. Two example applications, a once-off simulation and a network optimization design application simulation, are presented. This toolkit can be used (i) to implement, test and compare different solution methods, (ii) to focus the research on the most time-consuming parts of a solution method and (iii) to guide the choice of solution method when multiple simulation runs are required.}
}
@article{YANG20082290,
title = {A multi-agent system to facilitate component-based process modeling and design},
journal = {Computers & Chemical Engineering},
volume = {32},
number = {10},
pages = {2290-2305},
year = {2008},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2007.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S009813540700289X},
author = {A. Yang and B. Braunschweig and E.S. Fraga and Z. Guessoum and W. Marquardt and O. Nadjemi and D. Paen and D. Piñol and P. Roux and S. Sama and M. Serra and I. Stalker},
keywords = {Multi-agent system, Process modeling, Matchmaking, Ontology, Component-based software},
abstract = {Component-based software technology and software interfaces standardization initiatives, such as CAPE–OPEN, have made it possible to model chemical processes and to perform model-based engineering tasks by combining components of process modeling software from different sources, hence providing the potential of exploiting the “best of breed” offered by the CAPE community. In this context, software component libraries, possibly located on a local computer, on the intranet of an organization, or on the Internet, have to be searched to find the most suitable components for a particular engineering task at hand to be integrated into the engineers’ computing environment. This paper proposes to address this issue through a multi-agent software system which facilitates the engineers to find and to integrate software components and aims at reducing the engineers’ effort to the minimum. Within this system, a directory facilitator serves as the “yellow pages” such that an undetermined set of software component libraries located anywhere may be registered with the system. A matchmaker is used to match the specification of a desired software component with the potential candidates in the relevant libraries. The integration of a matching component into the computing environment is handled by an integration manager. A prototype of such a system, called COGents, has been developed employing an existing multi-agent platform. The ontology OntoCAPE defines the chemical engineering and modeling concepts required for specifying desired software components and for characterizing existing ones. OntoCAPE also provides a shared semantic basis for communication between the software agents. Details of the implementation of COGents are presented and the re-usability of the parts of the COGents system is discussed. Three successful demonstrative applications of COGents are reported, each dealing with different types of tasks, specifically flowsheeting, detailed modeling and process design.}
}
@article{DEVRIES1992315,
title = {The implementation of TSS},
journal = {Computers & Security},
volume = {11},
number = {4},
pages = {315-325},
year = {1992},
issn = {0167-4048},
doi = {https://doi.org/10.1016/0167-4048(92)90171-M},
url = {https://www.sciencedirect.com/science/article/pii/016740489290171M},
author = {T.P. {de Vries}},
keywords = {Network security, Workstation security, TSS},
abstract = {The IBM Transaction Security System (TSS, product family 475 x) is an integrated hardware and software implementation of common cryptographic functions. Together with the software support package SECIWS, TSS is designed to improve security at PC workstations, communication channels between workstations and between workstations and host processors. In September 1990, IBM announced a security extension of its System Application Architecture (SAA). One part of this extension is the Common Cryptographic Architecture (CCA). CCA has the objective of standardizing prime cryptographic functions such as: confidentiality, integrity, authenticity, person verification and related key management. All new IBM cryptographic products will support CCA or do so already. This article describes the objectives and implementation of the IBM Transaction Security System with regard to ensuring the security of a stand-alone workstation. The capacity of TSS to safeguard LAN communication as well as between a host and a workstation are beyond the scope of this publication.}
}
@article{HUANG19971,
title = {Recent progress in multiblock hybrid structured and unstructured mesh generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {150},
number = {1},
pages = {1-24},
year = {1997},
note = {Symposium on Advances in Computational Mechanics},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(97)00105-9},
url = {https://www.sciencedirect.com/science/article/pii/S0045782597001059},
author = {Chung-Yuan Huang},
abstract = {One of the bottlenecks in applying numerical methods to solve engineering problems is how to efficiently build-up the computational model for a given complex geometry. Huang and Oden have successfully integrated most of existing grid generation techniques and topological concepts into one grid generation package, GAMMA2D, for the automatic generation of linear quadrilateral finite elements. The present work focuses on the development and applications of a numerical software package, HOMESH, which extends the design philosophy of GAMMA2D to integrate multiblock, structured/unstructured mesh generation techniques for the automatic generation of high-order finite elements. The latest function improvements of HOMESH include: the implementation of a two-dimensional hyperbolic grid generator, the capabilities for the generation of hybrid 3-node triangular and 4-node quadrilateral linear elements, hybrid 6-node triangular and 8-node/9-node quadrilateral quadratic elements. In addition, various sweeping methods for the three-dimensional mesh generation are incorporated in HOMESH. These newly implemented grid generation methods greatly enhance the mesh design capability. To verify the usefulness of the mesh generated by HOMESH, commercial computational fluid dynamics (CFD) software packages are applied to test the data exchangeability.}
}
@article{BRACCO1999425,
title = {Data analysis software on the FTU experiment and its recent developments},
journal = {Fusion Engineering and Design},
volume = {43},
number = {3},
pages = {425-432},
year = {1999},
issn = {0920-3796},
doi = {https://doi.org/10.1016/S0920-3796(98)00414-1},
url = {https://www.sciencedirect.com/science/article/pii/S0920379698004141},
author = {G Bracco and G Buceti and A Imparato and M Panella and S Podda and G.B Righetti and O Tudisco and V Zanza},
keywords = {FTU experiment, Data analysis, Program},
abstract = {A general purpose data analysis and display program, named SHOW, has been developed over the years as the answer to the needs of the experimentalists working on FTU, a high magnetic field tokamak devoted to the study of plasma behaviour both with ohmic and additional heating. The description of the main characteristics of the SHOW program can be seen as a summary of many of the facilities required by the experimental physicists in their data analysis activity. Some of these facilities rely heavily on the program architecture, that derives from the awareness that only a very flexible structure permits an easy adaptation to the changing requirements of the data analysis activity. The program has been developed in FORTRAN on the IBM mainframe (running MVS-ESA) where the main FTU databases are located. The GDDM package has been used to implement the interactive user interface, based on panels, and the graphic display facilities, that include 2- and 3-D plot sections. The program provides utilities for the evaluation of derived quantities (as integrals, derivatives, smoothing filters) and for the time series analysis. A spread-sheet section permits the analysis of tables of data. A quantitative analysis of the utilisation of the SHOW program is also presented, during a period of >3 years. In the last year a preliminary version of the program has been ported to the UNIX environment, on a DEC Alpha workstation, and the user interface has been realised making use only of libraries available for free. A client/server software has been developed so that the IBM mainframe can still be used as a file server.}
}
@article{BINDER2015140,
title = {Estimation of Flow Rate and Viscosity in a Well with an Electric Submersible Pump using Moving Horizon Estimation∗∗This work is funded by the Research Council of Norway and Statoil through the PETROMAKS project No. 215684: Enabling High-Performance Safety-Critical Offshore and Subsea Automatic Control Systems Using Embedded Optimization (emOpt)},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {6},
pages = {140-146},
year = {2015},
note = {2nd IFAC Workshop on Automatic Control in Offshore Oil and Gas Production OOGP 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315008897},
author = {Benjamin J.T. Binder and Alexey Pavlov and Tor A. Johansen},
keywords = {Petroleum Production, Electric Submersible Pumps, Estimators, Moving Horizon Estimation, Flow Rate Allocation, Production Optimization},
abstract = {A Moving Horizon Estimator (MHE) is designed for a petroleum production well with an Electric Submersible Pump (ESP) installed for artificial lift. The focus is on estimating the flow rate from the well, the viscosity of the produced fluid, and the productivity index of the well. The software package ACADO is used to implement a Moving Horizon Estimator using a third-order nonlinear model. Simulation results show that the implemented estimator is able to estimate the desired variable and parameters. The resulting C-code solver is very fast, admitting real-time implementation.}
}
@article{STEFANSIC2002211,
title = {Design and implementation of a PC-based image-guided surgical system},
journal = {Computer Methods and Programs in Biomedicine},
volume = {69},
number = {3},
pages = {211-224},
year = {2002},
issn = {0169-2607},
doi = {https://doi.org/10.1016/S0169-2607(01)00192-4},
url = {https://www.sciencedirect.com/science/article/pii/S0169260701001924},
author = {James D Stefansic and W.Andrew Bass and Steven L Hartmann and Ryan A Beasley and Tuhin K Sinha and David M Cash and Alan J Herline and Robert L Galloway},
keywords = {Image-guided surgery, Computer assisted surgery, Windows programming, Image display, Image registration},
abstract = {In interactive, image-guided surgery, current physical space position in the operating room is displayed on various sets of medical images used for surgical navigation. We have developed a PC-based surgical guidance system (ORION) which synchronously displays surgical position on up to four image sets and updates them in real time. There are three essential components which must be developed for this system: (1) accurately tracked instruments; (2) accurate registration techniques to map physical space to image space; and (3) methods to display and update the image sets on a computer monitor. For each of these components, we have developed a set of dynamic link libraries in MS Visual C++ 6.0 supporting various hardware tools and software techniques. Surgical instruments are tracked in physical space using an active optical tracking system. Several of the different registration algorithms were developed with a library of robust math kernel functions, and the accuracy of all registration techniques was thoroughly investigated. Our display was developed using the Win32 API for windows management and tomographic visualization, a frame grabber for live video capture, and OpenGL for visualization of surface renderings. We have begun to use this current implementation of our system for several surgical procedures, including open and minimally invasive liver surgery.}
}
@article{ROSIAK2021111070,
title = {Custom-tailored clone detection for IEC 61131-3 programming languages},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111070},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111070},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001679},
author = {Kamil Rosiak and Alexander Schlie and Lukas Linsbauer and Birgit Vogel-Heuser and Ina Schaefer},
keywords = {Clone detection, Variability mining, IEC 61131-3, Reverse engineering},
abstract = {Automated production systems (aPS) are highly customized systems that consist of hardware and software. Such aPS are controlled by a programmable logic controller (PLC), often in accordance with the IEC 61131-3 standard that divides system implementation into so-called program organization units (POUs) as the smallest software unit and is comprised of multiple textual (Structured Text (ST)) and graphical (Function Block Diagram (FBD), Ladder Diagram (LD), and Sequential Function Chart(SFC)) programming languages that can be arbitrarily nested. A common practice during the development of such systems is reusing implementation artifacts by copying, pasting, and then modifying code. This approach is referred to as code cloning. It is used on a fine-granular level where a POU is cloned within a system variant. It is also applied on the coarse-granular system level, where the entire system is cloned and adapted to create a system variant, for example for another customer. This ad hoc practice for the development of variants is commonly referred to as clone-and-own. It allows the fast development of variants to meet varying customer requirements or altered regulatory guidelines. However, clone-and-own is a non-sustainable approach and does not scale with an increasing number of variants. It has a detrimental effect on the overall quality of a software system, such as the propagation of bugs to other variants, which harms maintenance. In order to support the effective development and maintenance of such systems, a detailed code clone analysis is required. On the one hand, an analysis of code clones within a variant (i.e., clone detection in the classical sense) supports experts in refactoring respective code into library components. On the other hand, an analysis of commonalities and differences between cloned variants (i.e., variability analysis) supports the maintenance and further reuse and facilitates the migration of variants into a software productline (SPL). In this paper, we present an approach for the automated detection of code clones within variants (intra variant clone detection) and between variants (inter variant clone detection) of IEC61131-3 control software with arbitrary nesting of both textual and graphical languages. We provide an implementation of the approach in the variability analysis toolkit (VAT) as a freely available prototype for the analysis of IEC 61131-3 programs. For the evaluation, we developed a meta-model-based mutation framework to measure our approach’s precision and recall. Besides, we evaluated our approach using the Pick and Place Unit (PPU) and Extended Pick and Place Unit (xPPU) scenarios. Results show the usefulness of intra and inter clone detection in the domain of automated production systems.}
}
@article{XU20071185,
title = {The model for optimal design of robot vision systems based on kinematic error correction},
journal = {Image and Vision Computing},
volume = {25},
number = {7},
pages = {1185-1193},
year = {2007},
note = {Computer Vision Applications},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2006.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0262885606002733},
author = {Kanglin Xu and George F. Luger},
keywords = {Active vision system, Differential transformation, Pose estimation, Design for manufacturing (DFM)},
abstract = {An active vision system is a robot device for controlling the optics and mechanical structure of cameras based on visual information to simplify the processing for computer vision. In this paper, we present a kinematic model for the optimal design of such active vision systems. We first build a generic kinematic model for robot structure error analysis using a Denavit–Hartenberg transformation matrix, differential changes for this transformation matrix and link parameters. We then extend it to analyze an active vision system using algorithms for estimating depth using stereo cameras. This model is generic and is suitable for analysis of any active vision system. Since we can employ it to analyze errors based on variations of link parameters when we use an active vision system to estimating depth, we can combine it with a cost-tolerance model to implement an optimal design for active vision systems. In this way, we can not only save manufacturing cost and implement design for manufacturing (DFM) but reduce or avoid calibration work for an active vision system. Our algorithm also works for a binocular head and on even more complex tasks. Based on our approach, we have created a software tool that functions as a C++ class library. We also demonstrate how to use this software model to analyze a real system TRICLOPS, which is a significant proof of concept.}
}
@article{TULVAN20131239,
title = {3D graphics coding in a reconfigurable environment},
journal = {Signal Processing: Image Communication},
volume = {28},
number = {10},
pages = {1239-1254},
year = {2013},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2013.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0923596513001276},
author = {Christian Tulvan and Marius Preda},
keywords = {MPEG reconfigurable media coding, Dataflow process network, Media tool library, 3D graphics, Compression, Reconfigurable decoders},
abstract = {The main objective of this paper is to introduce the concept of Reconfigurable Graphic Coding and its validation under the form of a Functional Units (FU) library. The heterogeneity of data for 3D graphics objects representation requires the adaptability of the compression schemas to various types of content. While such adaptation can be relatively easy to support in software implementations, the same is much more difficult to implement in hardware. Although compression schemas inherently share the same data processing chain, the components forming it may vary with respect to the number and type of components to encode, data range and correlation type. Based on the analysis of the state of the art on 3D graphics compression approaches, we propose a set of processing units. We show how this set can be configured/connected into a network, including hardware networks, to obtain reference decoders. Moreover, the network can be reconfigured at runtime, based on information that is provided with the encoded object. This modular concept of functional units, allows optimized management of computation (such as identification of parallelizable functions or functions that are suitable for acceleration) relative to the hardware architecture (CPU, GPU, FPGA, etc.). In the four decoders presented, at least half of the FUs are being reused at least once. The results were performed by generating and compiling C code from RVC-CAL code and comparing the results with the MPEG reference software implementation. The FUs described in this paper were standardized by MPEG as part of the ISO/IEC 23001-4.}
}
@article{LIN20161,
title = {High-performance IPv6 address lookup in GPU-accelerated software routers},
journal = {Journal of Network and Computer Applications},
volume = {74},
pages = {1-10},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301588},
author = {Feng Lin and Gang Wang and Junhai Zhou and Shiwen Zhang and Xin Yao},
keywords = {IPv6, IP lookup, Longest prefix matching, Graphic process unit, Bloom filter},
abstract = {Due to the ever-increasing physical link speed, routing table size and internet traffic, modern routers have been the major bottleneck to process packages with a high throughout. As the most time-consumption task of routers, designing efficient IP lookup schemes for IPv6 face new challenges. In this paper, we design parallel bloom filter for IPv6, and implement it on Graphics Processing Unit to develop a novel GPU-accelerated software router, GRv6. Moreover, we design two schemes to support dynamic prefix update, i.e., dynamic prefix insert scheme and dynamic prefix delete scheme. To evaluate the performance of GRv6, we implement it with NVIDIA GeForce GTX 580 and utilize 5 real-life IPv6 routing tables to demonstrate that the IP lookup engine could achieve 60Gbps for static routing tables, and 40Gbps for dynamic routing tables with 3000 updates per second.}
}
@article{XIE2020109915,
title = {A finite element iterative solver for a PNP ion channel model with Neumann boundary condition and membrane surface charge},
journal = {Journal of Computational Physics},
volume = {423},
pages = {109915},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109915},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120306896},
author = {Dexuan Xie and Zhen Chao},
keywords = {Poisson-Nernst-Planck equations, Finite element method, Ion channel, Electrostatics, Neumann boundary condition, Membrane surface charge},
abstract = {In this paper, an effective finite element iterative algorithm is presented for solving a Poisson-Nernst-Planck ion channel (PNPic) model with Neumann boundary value condition and a membrane surface charge density. It is constructed by a solution decomposition scheme to avoid singularity problems caused by atomic charges, an alternating block iterative scheme to sharply reduce computation complexity and computer memory requirement, and a Slotboom variable transformation scheme to significantly enhance numerical stability, as well as a modified Newton iterative scheme to efficiently solve each related nonlinear finite element equation. This PNPic finite element solver is then implemented as a software package that works for an ion channel protein with a crystallographic structure in a mixture solution of multiple ionic species. Furthermore, a finite element scheme is presented to compute a volume integral of a potential/concentration function over a block of a solvent region. This work can greatly improve the accuracy of a visualization tool for depicting the distribution pattern of a three-dimensional potential/concentration function across membrane in a simple two-dimensional curve. Numerical results for a mouse voltage-dependent anion-channel isoform (mVDAC1) in a solution of up to four ionic species are reported. They demonstrate the convergence of the PNPic iterative solver, the performance of the software package, and the valuable usage of the visualization tool in the comparison study of different potential and concentration functions. They also validate that this PNPic model can well retain the anion selectivity property of mVDAC1.}
}
@article{PAPACHRISTOU1991303,
title = {Microcontrol architectures with sequencing firmware and modular microcode development tools},
journal = {Microprocessing and Microprogramming},
volume = {29},
number = {5},
pages = {303-328},
year = {1991},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(91)90005-E},
url = {https://www.sciencedirect.com/science/article/pii/016560749190005E},
author = {Christos A Papachristou and Satnamsingh B Gambhir},
keywords = {Microprogramming, Firmware engineering, Microcontrol architecture, Microprogram simulation, PLA structures, Microcode development, CAD tools},
abstract = {The objective of this research is to develop a design methodology for microprogramming architectures with supporting firmware and development tools. Two PLA-based microcontrol architectures are proposed that are suitable for modular microprogramming. The first scheme consists of a PLA sequence store, a microcode ROM and an address processor. This structure has the capability of complex microsequencing such as multiway branching, microsubroutines, nested microlooping and the like. To alleviate the pin-limitation problem, a bit-slice approach is taken in the second scheme which allows for easy microcontrol expandability and compaction of the sequence store. Firmware support for the microcontrollers is provided by such control constructs as if-then-else, while-do and the like, which are available at the microlevel. Several firmware design tools have been developed and incorporated into a software package, MMDS, a Modular Microprogram Development System. MMDS includes the following tools: (i) a microsequencer and microcode assembler, (ii) a PLA code formatter, (iii) a functional-level simulator of the microarchitectures. The firmware migration of a binary search tree algorithm on a target machine, using MMDS, is described as a test example. To evaluate the scheme, experimentation was conducted using MMDS, samples of several functions and several data sets. The results demonstrated: (1) orders of magnitude time savings achieved by this method versus implementations in software; (2) the storage space efficiency of this scheme in comparison to conventional microprogram implementations.}
}
@article{PIROTTE2019102866,
title = {Balancing elliptic curve coprocessors from bottom to top},
journal = {Microprocessors and Microsystems},
volume = {71},
pages = {102866},
year = {2019},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2019.102866},
url = {https://www.sciencedirect.com/science/article/pii/S0141933118305003},
author = {Niels Pirotte and Jo Vliegen and Lejla Batina and Nele Mentens},
keywords = {ASIC, Weierstrass curve, Complete addition formulas, Elliptic curve cryptography (ECC), Side-channel analysis (SCA), Simple power analysis (SPA), Differential power analysis (DPA), Montgomery ladder},
abstract = {In 2016, Renes et al. were the first to propose complete addition formulas for Elliptic Curve Cryptography (ECC) on Weierstrass curves. With these formulas, the same set of equations can be used for point addition and point doubling, which makes software and hardware implementations less vulnerable to side-channel (SCA) attacks. Further, all inputs are valid, so there is no need for conditional statements handling special cases such as the point at infinity. This paper presents the first ASIC design of the complete addition formulas of Renes et al. Each computation layer in the design is balanced, from the field arithmetic to the point multiplication. The design explores two datapaths: a full-width Montgomery Multiplier ALU (MMALU) with a built-in adder and a serialized version of the MMALU. The interface sizes of the MMALU are optimized through an exploration of the design parameters. The register file size is minimized through an optimal scheduling of the modular operations. The top-level point multiplication is implemented using the Montgomery ladder algorithm, with the additional option of randomizing the execution order of the point operations as a countermeasure against SCA attacks. The implementation results after synthesis are generated using the open source NANGATE45 library.}
}
@article{OH201255,
title = {Practical finite element based simulations of nondestructive evaluation methods for concrete},
journal = {Computers & Structures},
volume = {98-99},
pages = {55-65},
year = {2012},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2012.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794912000326},
author = {Taekeun Oh and John S. Popovics and Suyun Ham and Sung Woo Shin},
keywords = {Finite element simulation, Wave propagation, Non-reflecting boundary, Concrete, Air-coupled NDE},
abstract = {This paper describes the development of practical, efficient and accurate dynamic finite element (FE) based simulations of ultrasonic nondestructive evaluation (NDE) methods for concrete structures. We propose a technique to eliminate transient ultrasonic wave reflections from the boundary of a small simulation model in order to simulate a larger structure in a computationally efficient manner. We also propose a coupled solid–fluid (concrete–air) model to simulate contactless air coupled sensing configuration. The proposed techniques are implemented within a commercially available FE analysis software package, although the presented results and findings are intended to be universal to all FE simulations. The simulation results compare favorably to experimental data, an analytical solution, and another computational solution.}
}
@article{CAMACHO2018235,
title = {PAF: A software tool to estimate free-geometry extended bodies of anomalous pressure from surface deformation data},
journal = {Computers & Geosciences},
volume = {111},
pages = {235-243},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417303588},
author = {A.G. Camacho and J. Fernández and F. Cannavò},
keywords = {Software, Surface deformation, Pressure sources, Volcano monitoring, Data inversion, Geodetic modeling},
abstract = {We present a software package to carry out inversions of surface deformation data (any combination of InSAR, GPS, and terrestrial data, e.g., EDM, levelling) as produced by 3D free-geometry extended bodies with anomalous pressure changes. The anomalous structures are described as an aggregation of elementary cells (whose effects are estimated as coming from point sources) in an elastic half space. The linear inverse problem (considering some simple regularization conditions) is solved by means of an exploratory approach. This software represents the open implementation of a previously published methodology (Camacho et al., 2011). It can be freely used with large data sets (e.g. InSAR data sets) or with data coming from small control networks (e.g. GPS monitoring data), mainly in volcanic areas, to estimate the expected pressure bodies representing magmatic intrusions. Here, the software is applied to some real test cases.}
}
@article{PAULAVICIUS2020106609,
title = {BASBL: Branch-And-Sandwich BiLevel solver. Implementation and computational study with the BASBLib test set},
journal = {Computers & Chemical Engineering},
volume = {132},
pages = {106609},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2019.106609},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419307252},
author = {R. Paulavičius and J. Gao and P.-M. Kleniati and C.S. Adjiman},
keywords = {Nonconvex bilevel programming, Branch-and-Sandwich algorithm, Optimization software,  solver,  toolkit, },
abstract = {We describe BASBL, our implementation of the deterministic global optimization algorithm Branch-and-Sandwich for a general class of nonconvex/nonlinear bilevel problems, within the open-source MINOTAUR framework. The solver incorporates the original Branch-and-Sandwich algorithm and modifications proposed in (Paulavičius and Adjiman, J. Glob. Opt., 2019, Submitted). We also introduce BASBLib, an extensive online library of bilevel benchmark problems collected from the literature and designed to enable contributions from the bilevel optimization community. We use the problems in the current release of BASBLib to analyze the performance of BASBL using different algorithmic options and we identify a set of default options that provide good overall performance. Finally, we demonstrate the application of BASBL to a set of flexibility index problems including linear and nonlinear constraints.}
}
@incollection{YIN2022291,
title = {Chapter 11 - Recent development toward future evolution: A powerful tool for virtual experiments},
editor = {Huiming Yin and Gan Song and Liangliang Zhang and Chunlin Wu},
booktitle = {The Inclusion-Based Boundary Element Method (iBEM)},
publisher = {Academic Press},
pages = {291-296},
year = {2022},
isbn = {978-0-12-819384-6},
doi = {https://doi.org/10.1016/B978-0-12-819384-6.00019-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012819384600019X},
author = {Huiming Yin and Gan Song and Liangliang Zhang and Chunlin Wu},
keywords = {virtual material testing and design, image-aided design and testing, nondestructive testing and evaluation},
abstract = {This chapter concludes the book with some active research topics and visions for future research directions. Based on the teaching notes and PhD dissertations, the book provides a practical and comprehensive approach to solve three types of problems (elastic, thermal, and Stokes flows) for general heterogeneous material systems and composite structures. The iBEM software package is available for research purposes. The ongoing and future research topics include adaptively forming inclusion for multiscale/multiphysical modeling and simulation, parallel computing, and optimization for high-speed large-scale calculation, extending the concept of eigenstrain for nonlinear material behavior with damaging and degradation, and implementation of iBEM for emerging engineering technologies. The applications of iBEM to fracture analysis, elastoplastic analysis, and the size effects of nanocomposites are discussed.}
}
@article{NOEI2016135,
title = {EXAF: A search engine for sample applications of object-oriented framework-provided concepts},
journal = {Information and Software Technology},
volume = {75},
pages = {135-147},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300532},
author = {Ehsan Noei and Abbas Heydarnoori},
keywords = {Object-oriented application frameworks, Framework-provided concepts, Sample applications, Frameworks comprehension, Code search engines},
abstract = {Context: Object-oriented application frameworks, such as Java Swing, provide reusable code and design for implementing domain-specific concepts, such as Context Menu, in software applications. Hence, use of such frameworks not only can decrease the time and the cost of developing new software applications, but also can increase their maintainability. However, the main problems of using object-oriented application frameworks are their large and complex APIs, and often incomplete user manuals. To mitigate these problems, developers often try to learn how to implement their desired concepts from available sample applications. Nonetheless, this introduces another hard and time-consuming challenge which is finding proper sample applications. Objective: To address this difficulty, we introduce EXAF (EXample Applications Finder) that helps developers find sample applications which implement their desired framework-provided concepts. Method: The majority of existing framework comprehension approaches can only help programmers to get familiar with the usage of particular fine-grained API elements of the desired framework such as its classes and methods. Nevertheless, our approach is able to find sample applications that implement a particular framework-provided concept. To this end, EXAF benefits from the Latent Semantic Indexing (LSI) information retrieval technique. We evaluated the approach using 24 concepts on top of the Microsoft .Net, Qt, and Java Swing frameworks. Results: Based on our evaluations, the precision of EXAF is more than 79%. Besides, it can find some sample applications that could not be found by common code search engines such as the ones which are used in SourceForge and Google Code. Conclusions: The results of our evaluations indicate that EXAF is effective in practice, and yields better search results because it considers various artifacts of a project like user reviews and bug reports.}
}
@article{CHANG20071362,
title = {The design and implementation of an application program interface for securing XML documents},
journal = {Journal of Systems and Software},
volume = {80},
number = {8},
pages = {1362-1374},
year = {2007},
note = {The Impact of Barry Boehm’s Work on Software Engineering Education and Training},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2006.10.051},
url = {https://www.sciencedirect.com/science/article/pii/S0164121206003360},
author = {Tao-Ku Chang and Gwan-Hwan Hwang},
keywords = {XML, Security, Encryption, Decryption, Digital signature, Document security language},
abstract = {The encryption and signature standards proposed by W3C specifying the format for encrypted XML documents are important advances towards XML security [Eastlake, Donald, Reagle, Joseph, Imamura, Takeshi, Dillaway, Blair, Simon, Ed, 2002. XML Encryption Syntax and Processing. W3C Recommendation 10 December 2002 <http://www.w3.org/TR/xmlenc-core/>, Eastlake, Donald, Reagle, Joseph, Solo, David, Bartel, Mark, Boyer, John, Fox, Barb, LaMacchia, Brian, Simon, Ed, 2002. XML-Signature Syntax and Processing W3C Recommendation, 12 February 2002. <http://www.w3.org/TR/xmldsig-core/>]. Related works include the proposal of a specification language that allows a programmer to describe the security details of XML documents [Hwang, Gwan-Hwan, Chang, Tao-Ku, 2004. An operational model and language support for securing XML documents. Computers & Security 23(6), 498–529, Hwang, Gwan-Hwan, Chang, Tao-Ku, 2001. Document security language (DSL) and an efficient automatic securing tool for XML documents. International Conference on Internet Computing, Las Vegas, Nevada, USA, 24–28 June 2001, pp. 393–399]. Despite the success of these works, we consider them to be insufficient from the viewpoint of software engineering. In this paper, we employ some real examples to demonstrate that it is necessary to design an appropriate API for the securing system of subtree encryption for XML documents. The goal is to increase productivity and reduce the cost of maintaining this kind of software, for which we propose a document security language (DSL) API. We describe the implementation of the DSL API, and use experimental results to demonstrate its practicality.}
}
@article{DIRUSCIO2014438,
title = {Simulating upgrades of complex systems: The case of Free and Open Source Software},
journal = {Information and Software Technology},
volume = {56},
number = {4},
pages = {438-462},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000172},
author = {Davide {Di Ruscio} and Patrizio Pelliccione},
keywords = {Evolution of FOSS systems, Simulation, Model-driven engineering, Linux distributions},
abstract = {Context
The upgrade of complex systems is intrinsically difficult and requires techniques, algorithms, and methods which are both expressive and computationally feasible in order to be used in practice. In the case of FOSS (Free and Open Source Software) systems, many upgrade errors cannot be discovered by current upgrade managers and then a system upgrade can potentially lead the system to an inconsistent and incoherent state.
Objective
The objective of this paper is to propose an approach to simulate the upgrade of complex systems in order to predict errors before they affect the real system.
Method
The approach promotes the use of model-driven engineering techniques to simulate the upgrade of complex systems. The basic idea is to have a model-based description of the system to be upgraded and to make use of model transformations to perform the upgrade on a source model so to obtain a target model representing the state of the upgraded system.
Results
We provide an implementation of the simulator, which is tailored to FOSS systems. The architecture of the simulator is distribution independent so that it can be easily instantiated to specific distributions. The simulator takes into account also pre and post-installation scripts that equip each distribution package. This feature is extremely important since maintainer scripts are full-fledged programs that are run with system administration rights.
Conclusions
The paper shows the kind of errors the simulator is able to predict before upgrading the real system, and how the approach improves the state of the art of package managers while integrated in real Linux distribution installations.}
}
@article{LIESLEHTO1991211,
title = {An Expert System for the Multivariable Controller Design},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {1},
pages = {211-216},
year = {1991},
note = {IFAC Symposium on Intelligent Tuning and Adaptive Control, Singapore, 15-17 January 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51321-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701751321X},
author = {J. Lieslehto and J.T. Tanttu and H.N. Koivo},
keywords = {Expert systems, multivariable control systems, PID control},
abstract = {In this paper a software package for the design of centralized and decentralized multivariable controllers is represented. The software package consists of numerical calculation programs and an expert system. The paper first describes the different subtasks of the multivariable controller design. Next a short descriptions of the design and analysis methods included in the software package is given. The description of the implementation of the expert system finishes the paper.}
}
@article{PANELLA20131257,
title = {ITER CODAC Core System at FTU: State of the art and new perspectives},
journal = {Fusion Engineering and Design},
volume = {88},
number = {6},
pages = {1257-1262},
year = {2013},
note = {Proceedings of the 27th Symposium On Fusion Technology (SOFT-27); Liège, Belgium, September 24-28, 2012},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2013.02.097},
url = {https://www.sciencedirect.com/science/article/pii/S0920379613002123},
author = {Maurizio Panella and Cristina Centioli and Franck {Di Maio} and Mathieu Napolitano and Mikel Rojo and Marco Vellucci and Vincenzo Vitale and Anders Wallander},
keywords = {Slow control, Fast control, CODAC Core System, EPICS, FTU},
abstract = {Recently the slow control of the MFG1 motor flywheel generator powering FTU toroidal magnet has been redesigned within the framework of the ITER CODAC I&C architecture [1] using the software package CODAC Core System (CCS). In this paper the progress made towards the final commissioning will be detailed, focusing on the system integration in the FTU control system supervisor through a software bridge and a multithreaded channel access interface to the Plant System Host, emphasizing the problems found so far and their solutions. The development and implementation of the alarm handler, the logging system and the data archiving system will also be illustrated, as well as the relevant monitoring and visualization interfaces developed on the Mini-CODAC node with the standard EPICS tools BEAST, BOY and BEAUTY. Furthermore, the tests run on FTU that finally led to a successful commissioning will be thoroughly discussed. Due to the satisfactory outcome of the project, and taking advantage of the release of the new CCS v 3.0 introducing the management of fast controllers, an account on a possible application of the CODAC ITER fast control architecture at FTU will also be given, taking the FTU real time feedback system as the first test case, to be further extended to newly implemented fast controllers.}
}
@article{KARIM2003721,
title = {Simulation and video software development for soil consolidation testing},
journal = {Advances in Engineering Software},
volume = {34},
number = {11},
pages = {721-728},
year = {2003},
issn = {0965-9978},
doi = {https://doi.org/10.1016/S0965-9978(03)00098-X},
url = {https://www.sciencedirect.com/science/article/pii/S096599780300098X},
author = {U.F.A Karim},
keywords = {Soil, Consolidation test, Simulation, Multi-media, Courseware, Video, Geotechnical, Software},
abstract = {The development techniques and file structures of CTM, a novel multi-media (computer simulation and video) package on consolidation and laboratory consolidation testing, are presented in this paper. A courseware tool called Authorware proved to be versatile for building the package and the paper shows in details how the file structure of CTM is implemented within Authorware. In the audio–visual parts of the package background information, animations on consolidation theory and practice as well as standard and large-scale model consolidation tests, are shown. Running the package in a multi-user examination-type setting proved to be effective for learning and control of the learning process. Content-oriented multiple-choice questions on the theoretical and professional aspects of the test are posed, and users response is evaluated and reported on log files. Student users have responded positively to this development indicating minor future improvements.}
}
@article{LEUNG2007480,
title = {Program entanglement, feature interaction and the feature language extensions},
journal = {Computer Networks},
volume = {51},
number = {2},
pages = {480-495},
year = {2007},
note = {Feature Interaction},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2006.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389128606002180},
author = {Wu-Hon F. Leung},
keywords = {Feature interaction, Program entanglement, Programming language, Feature interaction resolution, Reusable programs, Exception handling, Inheritance},
abstract = {One of the most difficult tasks in software development is that the programmer must implement a feature going through a laborious and error prone process of modifying the programs of other features. The programs of the different features entangle in the same reusable program units of the programming language, making them also difficult to be verified, maintained and reused. We show that if (C1) the features interact, (C2) they are executed by the same process and (C3) they are implemented in a programming language that requires the programmer to specify execution flows, program entanglement is inevitable and the problem cannot be solved by software design alone. Applications with interacting features are common including those that require exception handling. The feature language extensions (FLX) is a set of programming language constructs designed to enable the programmer to develop interacting features as separate and reusable program modules even though the features interact. The programmer uses FLX to specify non-procedural program units, organize the program units into reusable features and integrate features into executable feature packages. He develops a feature based on a model instead of the code of other features. FLX supports an automatic procedure to detect the interaction condition among features; the programmer then resolve the interaction in a feature package without changing feature code. FLX features and feature packages are reusable; the programmer may package different combinations of them and resolve their interactions differently to meet different user needs. An FLX to Java compiler has been implemented; our experience of using it has been very positive.}
}
@article{KRISHNAN2009466,
title = {SOAs for scientific applications: Experiences and challenges},
journal = {Future Generation Computer Systems},
volume = {25},
number = {4},
pages = {466-473},
year = {2009},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2008.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X08001386},
author = {Sriram Krishnan and Karan Bhatia},
keywords = {SOA, Grid, Cyberinfrastructures, eScience},
abstract = {Over the past several years, with the advent of the Open Grid Services Architecture (OGSA) [I. Foster, C. Kesselman, J. Nick, S. Tuecke, Grid Services for Distributed System Integration, Computer 35 (6) (2002)] and the Web Services Resource Framework (WSRF) [K. Czajkowski, et al., WS-resource framework. http://www-106.ibm.com/developerworks/library/ws-resource/ws-wsrf.pdf, 2004. [25]], Service-oriented Architectures (SOA) and Web service technologies have been embraced in the field of scientific and Grid computing. These new principles promise to help make scientific infrastructures simpler to use, more cost effective to implement, and easier to maintain. However, understanding how to leverage these developments to actually design and build a system remains more of an art than a science. In this paper, we present some positions learned through experience, that provide guidance in leveraging SOA technologies to build scientific infrastructures. In addition, we present the technical challenges that need to be addressed in building an SOA, and as a case study, we present the SOA that we have designed for the National Biomedical Computation Resource (NBCR) [The National Biomedical Computation Resource (NBCR). http://nbcr.net/] community. We discuss how we have addressed these technical challenges, and present the overall architecture, the individual software toolkits developed, the client interfaces, and the usage scenarios. We hope that our experiences prove to be useful in building similar infrastructures for other scientific applications.}
}
@article{SOFFER2005639,
title = {Aligning an ERP system with enterprise requirements: An object-process based approach},
journal = {Computers in Industry},
volume = {56},
number = {6},
pages = {639-662},
year = {2005},
note = {Current trends in ERP implementations and utilisation},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2005.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361505000771},
author = {Pnina Soffer and Boaz Golany and Dov Dori},
keywords = {Enterprise Resource Planning, Requirement-driven, Similarity, Business process design},
abstract = {One of the main problems in ERP implementation projects is how to align an off-the-shelf software package with the business processes of the enterprise implementing it. The paper proposes a requirement-driven approach, which benefits from reusing the business process design without being restricted by predefined solutions and criteria. The approach applies an iterative alignment process, which employs an algorithm that matches a model of the enterprise requirements with a model of the ERP system capabilities. The algorithm identifies possible matches between the two models and evaluates the gaps between them despite differences in their completeness and detail level. It provides the enterprise with a set of feasible combinations of requirements that can be satisfied by the ERP system as a basis for making implementation decisions. We use Object-Process Methodology (OPM) to model both the ERP system and the enterprise requirements, and utilize the pair of resulting OPM models as input for the matching algorithm. The alignment algorithm has been tested in an experimental study, whose encouraging results demonstrate the ability of the approach to provide a satisfactory solution to the problem of aligning an ERP software package with an enterprise business model.}
}
@article{LOPES2023112021,
title = {Simulation toolkit for digital material characterization of large image-based microstructures},
journal = {Computational Materials Science},
volume = {219},
pages = {112021},
year = {2023},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112021},
url = {https://www.sciencedirect.com/science/article/pii/S0927025623000150},
author = {Pedro C.F. Lopes and Rafael S. Vianna and Victor W. Sapucaia and Federico Semeraro and Ricardo Leiderman and André M.B. Pereira},
keywords = {Image-based modeling, Homogenization, Effective properties, Conductivity, Elasticity, Permeability, Micro-CT, FEM},
abstract = {In this paper, an efficient image-based simulation toolkit for material characterization is presented, which is scalable to work from personal computers to workstations. The effective thermal conductivity, elasticity, and permeability are evaluated employing a computational homogenization framework based on the Finite Element Method (FEM). Two complementary open-source packages are presented: one developed in Python, which can convert digital images into voxel meshes (pyTomoviewer); the other developed in Julia, that can run numerical simulations to compute effective material properties (chpack). Also, a CUDA C version of chpack is provided (chfem_gpu). They were designed to deal with large multi-phase models, so strategies were devised to minimize their memory footprint, while avoiding a high toll on execution time. The voxel-based approach significantly simplifies the FEM meshes and allows efficient matrix-free implementations. In that sense, to handle large linear systems of equations, the element-by-element (EBE) technique is adopted, in conjunction with a low-memory implementation of the Preconditioned Conjugate Gradient (PCG) method. The code was thoroughly tested on an artificial geometry made of a square array of cylinders, for which analytical solutions exist, as well as on a real micro-tomographic reconstruction of FiberFormTM, a carbon preform commonly used in thermal protection systems.}
}
@incollection{PYEATT201699,
title = {Chapter 5 - Structured Programming},
editor = {Larry D. Pyeatt},
booktitle = {Modern Assembly Language Programming with the ARM Processor},
publisher = {Newnes},
pages = {99-135},
year = {2016},
isbn = {978-0-12-803698-3},
doi = {https://doi.org/10.1016/B978-0-12-803698-3.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012803698300005X},
author = {Larry D. Pyeatt},
keywords = {Structured programming, Sequencing, Selection, Iteration, Loop, Subroutine, Function, Recursion, Struct, Aggregate data, Array},
abstract = {This chapter first introduces the structured programming concepts and describes the principles of good software design. It then shows how the language elements covered in the previous three chapters are used to create the elements required by structured programming, giving comparative examples of these elements in C and assembly language. It covers programming elements for sequencing, selection, and iteration. Then it covers in greater detail how to access the standard C library functions from assembly language, and how to access assembly language functions from C. It then explains how automatic variables are allocated, and covers writing recursive functions in assembly language. Finally, it explains the implementation of C structs and shows how they can be accessed from assembly language, then covers arrays in the same way.}
}
@article{FIALKO2014190,
title = {Iterative methods for solving large-scale problems of structural mechanics using multi-core computers},
journal = {Archives of Civil and Mechanical Engineering},
volume = {14},
number = {1},
pages = {190-203},
year = {2014},
issn = {1644-9665},
doi = {https://doi.org/10.1016/j.acme.2013.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1644966513000666},
author = {S.Yu. Fialko},
keywords = {Finite element method, Preconditioned conjugate gradient method, Sparse matrices, Multi-core computers, Multistory buildings},
abstract = {The paper studies the conjugate gradient method for solving systems of linear algebraic equations with symmetric sparse matrices that arise when the finite-element method is applied to the problems of structural mechanics. The main focus is on designing effective preconditioning and parallelizing the method for multi-core desktop computers. Preconditioning is based on the incomplete Cholesky “by value” factorization method and implemented based on the technique of sparse matrices, which allows increasing convergence considerably without a significant increase of the computer's resources. Parallelization is implemented for the incomplete factorization as well as for iterative process stages. The method is integrated into the SCAD software package (www.scadsoft.com). The paper includes a discussion of the results of calculations done with direct and iterative methods for large-scale design models of tall buildings, originally from the SCAD Soft11SCAD Soft (www.scadsoft.com)—IT company, developer of the SCAD FEA software, one of the most popular software used in the CIS countries for structural analysis and design, certified according to the regional norms. problem collection.}
}
@article{BENAVICES2015307,
title = {Networking Control Education by the use of Software Defined Networking Tools and Techniques},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {29},
pages = {307-312},
year = {2015},
note = {IFAC Workshop on Internet Based Control Education IBCE15},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.11.253},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315025112},
author = {C. Benavices and I. García and H. Alaiz and A. Alonso and J.M. Alija},
keywords = {Internet-based education, learning environment, software-defined networking},
abstract = {Software Defined Networking (SDN) is an emerging field of study within computer networks research area that will, in a near future, play an important role in the design and development of network infrastructures. The main idea behind SDN is to separate the control and the data planes that are nowadays coupled in the traditional network devices (routers, switches, etc). This separation of the control plane from the electronic that performs the actual packet switching tasks allows the former to be implemented in a physical different device and also permits to build the control strategies in a vendor and platform independent fashion, what helps to unify the different operating systems that today make it difficult to configure a complex network. The data plane (packet switching) is performed in physical devices but the control plane is implemented entirely by software, in devices that are called controllers. Controllers communicate control schemes to the packet switching devices by means of different protocols and APIs and can also communicate and interact with higher level applications that can be developed for different tasks: from easing the management of control schemas to monitoring the network status or, as is the case in this paper, for educational purposes. The aim of the work described in this paper is to show the possibilities of using software applications embedded in SDN controllers as an Internet-based educational tools in the field of computer network control.}
}
@article{HANAFIAH201514,
title = {A Web-Based Chinese Chess Xiang Qi using n-tier Architecture Model},
journal = {Procedia Computer Science},
volume = {59},
pages = {14-18},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.332},
url = {https://www.sciencedirect.com/science/article/pii/S187705091501861X},
author = {Novita Hanafiah and Jaka Hartanto and Yulyani Arifin and Hengky Frans and Winardi Cristian},
keywords = {ID card, character, recognition, OCR, template, matching ;},
abstract = {Classic game has its attraction so that it is not forgotten easily along the time, for instance Chinese chess called Xiang Qi. The current technology development makes the classic games have to adapt by converting the style of play, such as online version of the classic game. N-tier architecture provides a model for software developers to create an application which has a high flexibility and reusability. By dividing the application into some tiers, developers have options to modify or add a particular layer, instead of changing the whole application. The implementation of n-tier can be done by supporting with internet. The combination of Xiang Qi with online game technology based on web is expected to be successful in attracting the attention of gamers/players. The paper presents a re-packaging of conventional games into an online game application which built using n-tier developing approach that can manage accessibility and interactivity of the games.}
}
@article{COLLINS198325,
title = {An interactive design program for modular building elevations using an apple II microcomputer},
journal = {Computer-Aided Design},
volume = {15},
number = {1},
pages = {25-32},
year = {1983},
issn = {0010-4485},
doi = {https://doi.org/10.1016/S0010-4485(83)80047-5},
url = {https://www.sciencedirect.com/science/article/pii/S0010448583800475},
author = {Donald W. Collins},
keywords = {microcomputer, architecture, drafting},
abstract = {This paper presents the history of MODBED (modular building elevation design), an architectural elevation design software package, describes its function and discusses the advantages of implementing similar software on new desktop computers (microcomputers) in answer to the design needs of small and medium-sized architectural and engineering offices. Local ‘smart terminals’ open the way to distributive processing and computer networking. These advancements can enhance decision making in the early stages of design and make it more cost effective.}
}
@article{NG2022112026,
title = {A three-dimensional fluid-structure interaction model based on SPH and lattice-spring method for simulating complex hydroelastic problems},
journal = {Ocean Engineering},
volume = {260},
pages = {112026},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.112026},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822013579},
author = {K.C. Ng and W.C. Low and Hailong Chen and A. Tafuni and A. Nakayama},
keywords = {Smoothed particle hydrodynamics (SPH), Fluid structure interaction (FSI), DualSPHysics, Volume compensated particle method (VCPM), Lattice particle method (LPM), Lattice spring model (LSM)},
abstract = {The present work revolves around the development of a 3D particle-based Fluid-Structure Interaction (FSI) solver to simulate hydroelastic problems that involve free surface. The three-dimensional Volume-Compensated Particle Method (VCPM) for modelling deformable solid bodies is developed within the open-source SPH software package DualSPHysics. Complex 3D FSI problems are readily simulated within a reasonable time frame thanks to the parallel scalability of DualSPHysics on both CPU and GPU. The Sequential Staggered (SS) scheme paired with a multiple time-stepping procedure is implemented in DualSPHysics for coupling the SPH and VCPM models. It is found that the SPH-VCPM method is computationally more efficient than the previously reported SPH-TLSPH method. Extensive validations have been performed based on some very recent 3D experimental setups that involve violent free surface and complex structural dynamics. Findings from this research highlight the capability of the 3D SPH-VCPM model to reproduce some of the physical observations that were not captured by previous 2D studies. Some preliminary 3D FSI results involving solid fracture are also demonstrated.}
}
@article{CRISTIANI1991101,
title = {The PRIST-2 development environment: Architecture and implementation},
journal = {International Journal of Bio-Medical Computing},
volume = {28},
number = {1},
pages = {101-116},
year = {1991},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(91)90030-I},
url = {https://www.sciencedirect.com/science/article/pii/002071019190030I},
author = {P. Cristiani and G. Costa and S. Pazzi},
keywords = {Computer aided software engineering, 4th Generation languages, Relational data base management system, Graphic user interface, Hospital information system},
abstract = {The PRIST-2 system has been designed as an interactive and high-productivity tool for the rapid prototyping and development of medical applications. Three major issues were addressed in this research project which derived from the evolution of a previous 4th generation software package, called PRIST (patient record information system tool): a high transportability on different hardware and operating systems, a conversational and interactive user-interface and user-independence from the Relational Data Base Management System (RDBMS). Although we developed PRIST-2 on the top of the ORACLE RDBMS, it does not depend on SQL commercial products because the ORACLE features have been directly used only for SQL relational data base management. The application design methodology implemented in the system architecture allows an interactive and formal description of the application constraints in terms of the semantic data model rather than in terms of the data structure. The translation of the conceptual constraints into SQL tables is performed by several pre-defined routines. In the PC based release (MS/DOS, OS/2, Xenix operating systems), the Graphic-User Interface (GUI) has been developed using Microsoft Windows Software Development Kit. The UNIX release will use a GUI developed on top of the X-Windows environment}
}
@article{SEGURAORTIZ2023106653,
title = {GENECI: A novel evolutionary machine learning consensus-based approach for the inference of gene regulatory networks},
journal = {Computers in Biology and Medicine},
volume = {155},
pages = {106653},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.106653},
url = {https://www.sciencedirect.com/science/article/pii/S001048252300118X},
author = {Adrián Segura-Ortiz and José García-Nieto and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Gene regulatory networks, Differential expression, Machine learning, Evolutionary algorithms},
abstract = {Gene regulatory networks define the interactions between DNA products and other substances in cells. Increasing knowledge of these networks improves the level of detail with which the processes that trigger different diseases are described and fosters the development of new therapeutic targets. These networks are usually represented by graphs, and the primary sources for their correct construction are usually time series from differential expression data. The inference of networks from this data type has been approached differently in the literature. Mostly, computational learning techniques have been implemented, which have finally shown some specialization in specific datasets. For this reason, the need arises to create new and more robust strategies for reaching a consensus based on previous results to gain a particular capacity for generalization. This paper presents GENECI (GEne NEtwork Consensus Inference), an evolutionary machine learning approach that acts as an organizer for constructing ensembles to process the results of the main inference techniques reported in the literature and to optimize the consensus network derived from them, according to their confidence levels and topological characteristics. After its design, the proposal was confronted with datasets collected from academic benchmarks (DREAM challenges and IRMA network) to quantify its accuracy. Subsequently, it was applied to a real-world biological network of melanoma patients whose results could be contrasted with medical research collected in the literature. Finally, it has been proved that its ability to optimize the consensus of several networks leads to outstanding robustness and accuracy, gaining a certain generalization capacity after facing the inference of multiple datasets. The source code is hosted in a public repository at GitHub under MIT license: https://github.com/AdrianSeguraOrtiz/GENECI. Moreover, to facilitate its installation and use, the software associated with this implementation has been encapsulated in a python package available at PyPI: https://pypi.org/project/geneci/.}
}
@article{ABUNDENEBA2017128,
title = {Modeling and simulated design: A novel model and software of a solar-biomass hybrid dryer},
journal = {Computers & Chemical Engineering},
volume = {104},
pages = {128-140},
year = {2017},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098135417301540},
author = {Fabrice {Abunde Neba} and Yvette {Jiokap Nono}},
keywords = {Modeling, Simulated design, Solar-biomass dryer, Design software, Food products},
abstract = {Solar-biomass hybrid dryers are widely used as environmentally friendly alternatives to fossil fuel dryers for the preservation of agricultural food products. However, the design and optimization of hybrid dryers often involve construction of expensive prototype systems and time-consuming studies. This study presents a novel hybrid dryer model and a dryer design and simulation software, which can be used to improve hybrid dryer design efficiency. The methodologies adopted are those of functional analysis, mathematical modeling and virtual prototyping, with SolidWorks employed for CAD modeling, and Matlab for numerical simulations and implementation of mathematical models into a user interface. The software package has as main functions (a) dimensioning the hybrid dryer, which comprises a solar collector, combustion reactor and tunnel drying chamber, (b) cost analysis and financial appraisal, and (c) temperature dynamic simulation in the system. The novel software has been successfully employed to dimension a hybrid dryer for drying of green pepper.}
}
@article{HERT200716,
title = {An adaptable and extensible geometry kernel},
journal = {Computational Geometry},
volume = {38},
number = {1},
pages = {16-36},
year = {2007},
note = {Special Issue on CGAL},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2006.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0925772107000065},
author = {Susan Hert and Michael Hoffmann and Lutz Kettner and Sylvain Pion and Michael Seel},
keywords = {Computational geometry, Software library, Generic programming},
abstract = {Geometric algorithms are based on geometric objects such as points, lines and circles. The term kernel refers to a collection of representations for constant-size geometric objects and operations on these representations. This paper describes how such a geometry kernel can be designed and implemented in C++, having special emphasis on adaptability, extensibility and efficiency. We achieve these goals following the generic programming paradigm and using templates as our tools. These ideas are realized and tested in Cgal, the Computational Geometry Algorithms Library, see http://www.cgal.org/.}
}
@article{JVOGWELL1990269,
title = {The design of open structure engineering databases},
journal = {Microprocessing and Microprogramming},
volume = {28},
number = {1},
pages = {269-273},
year = {1990},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(90)90187-E},
url = {https://www.sciencedirect.com/science/article/pii/016560749090187E},
author = { {J. Vogwell} and S.J. Culley and J. Armour},
abstract = {Databases are beginning to be used in mechanical engineering for aiding a designer select, analyse and validate the most appropriate catalogue component for an application. The format of the data and the way in which it is used varies significantly from more conventional database applications. This paper describes how databases have been designed and implemented in a commercial micro computer software package and describes how the user interface has influenced the software structure.}
}
@incollection{ELGUINDI201269,
title = {3 - Electronic resource management systems: implementation and transformation},
editor = {Anne C. Elguindi and Kari Schmidt},
booktitle = {Electronic Resource Management},
publisher = {Chandos Publishing},
pages = {69-107},
year = {2012},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-668-5},
doi = {https://doi.org/10.1016/B978-1-84334-668-5.50003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781843346685500031},
author = {Anne C. Elguindi and Kari Schmidt},
keywords = {electronic resource management systems, resource management, library systems, next-gen ILS, discovery systems, OpenURL resolvers, knowledge bases, data management, integrated library system, web-scale management},
abstract = {Abstract:
As electronic resource management evolved it became clear it had data and workflow requirements that could not be sufficiently handled within the ILS. The ERMS brought a needed centralization and structure to these new requirements, but it still lacks the functionality and flexibility that many libraries need. The future is a reunification of library systems, with the ERMS acting as transitional software between the ILS and a new kind of resource management system that serves all formats.}
}
@article{SOLOMONIK20143176,
title = {A massively parallel tensor contraction framework for coupled-cluster computations},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {12},
pages = {3176-3190},
year = {2014},
note = {Domain-Specific Languages and High-Level Frameworks for High-Performance Computing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S074373151400104X},
author = {Edgar Solomonik and Devin Matthews and Jeff R. Hammond and John F. Stanton and James Demmel},
keywords = {Coupled-cluster, Tensor contractions, Matrix multiplication, Topology-aware mapping, Communication-avoiding algorithms},
abstract = {Precise calculation of molecular electronic wavefunctions by methods such as coupled-cluster requires the computation of tensor contractions, the cost of which has polynomial computational scaling with respect to the system and basis set sizes. Each contraction may be executed via matrix multiplication on a properly ordered and structured tensor. However, data transpositions are often needed to reorder the tensors for each contraction. Writing and optimizing distributed-memory kernels for each transposition and contraction is tedious since the number of contractions scales combinatorially with the number of tensor indices. We present a distributed-memory numerical library (Cyclops Tensor Framework (CTF)) that automatically manages tensor blocking and redistribution to perform any user-specified contractions. CTF serves as the distributed-memory contraction engine in Aquarius, a new program designed for high-accuracy and massively-parallel quantum chemical computations. Aquarius implements a range of coupled-cluster and related methods such as CCSD and CCSDT by writing the equations on top of a C++ templated domain-specific language. This DSL calls CTF directly to manage the data and perform the contractions. Our CCSD and CCSDT implementations achieve high parallel scalability on the BlueGene/Q and Cray XC30 supercomputer architectures showing that accurate electronic structure calculations can be effectively carried out on top of general distributed-memory tensor primitives.}
}
@article{DIAZ2010987,
title = {A computer code for finite element analysis and design of post-tensioned voided slab bridge decks with orthotropic behaviour},
journal = {Advances in Engineering Software},
volume = {41},
number = {7},
pages = {987-999},
year = {2010},
note = {Advances in Structural Optimization},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2010.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997810000505},
author = {J. Díaz and S. Hernández and A. Fontán and L. Romera},
keywords = {Bridge design, Voided slab, Concrete deck, Finite element analysis, Orthotropic plate},
abstract = {This paper presents a computer software which allows the generation of a complete structural model of a concrete bridge with a voided slab deck, a common design for medium span bridges. The code implements the orthotropic plate paradigm and provides a graphical user interface, which allows both preprocessing and post-processing, linked to a commercial finite element package. A description of the code is presented, along with the formulation of the orthotropic plate. Verification and comparison examples demonstrate the performance and features of the software and also the applicability of the formulation.}
}
@article{HEO2015237,
title = {PAPIRUS, a parallel computing framework for sensitivity analysis, uncertainty propagation, and estimation of parameter distribution},
journal = {Nuclear Engineering and Design},
volume = {292},
pages = {237-247},
year = {2015},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0029549315002721},
author = {Jaeseok Heo and Kyung Doo Kim},
abstract = {This paper introduces a statistical data analysis toolkit, PAPIRUS, designed to perform the model calibration, uncertainty propagation, Chi-square linearity test, and sensitivity analysis for both linear and nonlinear problems. The PAPIRUS was developed by implementing multiple packages of methodologies, and building an interface between an engineering simulation code and the statistical analysis algorithms. A parallel computing framework is implemented in the PAPIRUS with multiple computing resources and proper communications between the server and the clients of each processor. It was shown that even though a large amount of data is considered for the engineering calculation, the distributions of the model parameters and the calculation results can be quantified accurately with significant reductions in computational effort. A general description about the PAPIRUS with a graphical user interface is presented in Section 2. Sections 2.1 Data assimilation – deterministic, 2.2 Data assimilation – probabilistic, 2.3 Uncertainty propagation, 2.4 Chi-square linearity test, 2.5 Sensitivity analysis present the methodologies of data assimilation, uncertainty propagation, Chi-square linearity test, and sensitivity analysis implemented in the toolkit with some results obtained by each module of the software. Parallel computing algorithms adopted in the framework to solve multiple computational problems simultaneously are also summarized in the paper.}
}
@article{SHEN201366,
title = {Study on generation and sharing of on-demand global seamless data—Taking MODIS NDVI as an example},
journal = {Computers & Geosciences},
volume = {54},
pages = {66-74},
year = {2013},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2012.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0098300412003883},
author = {Dayong Shen and Meixia Deng and Liping Di and Weiguo Han and Chunming Peng and Ali Levent Yagci and Genong Yu and Zeqiang Chen},
keywords = {BigTIFF, GDAL, WMS, WCS, On-demand, NDVI, MODIS},
abstract = {By applying advanced Geospatial Data Abstraction Library (GDAL) and BigTIFF technology in a Geographical Information System (GIS) with Service Oriented Architecture (SOA), this study has derived global datasets using tile-based input data and implemented Virtual Web Map Service (VWMS) and Virtual Web Coverage Service (VWCS) to provide software tools for visualization and acquisition of global data. Taking MODIS Normalized Difference Vegetation Index (NDVI) as an example, this study proves the feasibility, efficiency and features of the proposed approach.}
}
@article{SANTOS2008809,
title = {A Web services-based framework for building componentized digital libraries},
journal = {Journal of Systems and Software},
volume = {81},
number = {5},
pages = {809-822},
year = {2008},
note = {Software Process and Product Measurement},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2007.07.029},
url = {https://www.sciencedirect.com/science/article/pii/S0164121207001835},
author = {Rodrygo L.T. Santos and Pablo A. Roberto and Marcos André Gonçalves and Alberto H.F. Laender},
keywords = {Digital libraries, Web services, Component-based software development, Software evaluation},
abstract = {We present a new Web services-based framework for building componentized digital libraries (DLs). We particularly demonstrate how traditional RDBMS technology can be easily deployed to support several common digital library services. Configuration and customization of the framework to build specialized systems is supported by a wizard-like tool which is based on a generic metamodel for DLs. Such a tool implements a workflow process that segments the DL design tasks into well-defined steps and drives the designer along these steps. Both the framework and the configuration tool are evaluated in terms of several performance and usability criteria. Our experimental evaluation demonstrates the feasibility and superior performance of our framework, as well as the effectiveness of the wizard tool for setting up DLs.}
}
@article{PARTHASARATHY201619,
title = {Efficiency analysis of ERP packages—A customization perspective},
journal = {Computers in Industry},
volume = {82},
pages = {19-27},
year = {2016},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2016.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361516300690},
author = {Sudhaman Parthasarathy and Srinarayan Sharma},
keywords = {Customization, Data Envelopment Analysis (DEA), Efficiency, Enterprise Resource Planning (ERP), Software productivity},
abstract = {Enterprise Resource Planning (ERP) packages developed by ERP vendors are designed not only to standardize the existing business processes of the implementing organization, but also to bring in some of the best practices of the industry. There are many past studies on the assessment of the efficiency of standard ERP projects; however, to date no research has been conducted to assess the efficiency of ERP packages from the point of view of customization. Assessing the efficiency of customized ERP packages is vital for benchmarking best customization practices. In this study we examine the efficiency of customized ERP packages using Data Envelopment Analysis (DEA). We also examine the relationship between the degree of customization of ERP packages and their efficiency. Data was collected from an IT vendor who had deployed ERP package in 12 educational institutions. The results suggest that customization adversely affects the efficiency of ERP packages. We also discuss the implications of the results for research and practice.}
}
@article{JULVEZ2012806,
title = {SimHPN: A MATLAB toolbox for simulation, analysis and design with hybrid Petri nets},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {6},
number = {2},
pages = {806-817},
year = {2012},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2011.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X11000586},
author = {Jorge Júlvez and Cristian Mahulea and Carlos-Renato Vázquez},
keywords = {Hybrid Petri nets, Software tools, Hybrid systems},
abstract = {This paper presents a MATLAB embedded package for hybrid Petri nets called SimHPN. It offers a collection of tools devoted to simulation, analysis and synthesis of dynamical systems modeled by hybrid Petri nets. The package supports several server semantics for the firing of both, discrete and continuous, types of transitions. Besides providing different simulation options, SimHPN offers the possibility of computing steady state throughput bounds for continuous nets. For such a class of nets, optimal control and observability algorithms are also implemented. The package is fully integrated in MATLAB which allows the creation of powerful algebraic, statistical and graphical instruments that exploit the routines available in MATLAB.}
}
@article{SIKORA2007304,
title = {PARALLEL AND DISTRIBUTED SIMULATION OF FRAME RELAY NETWORKS},
journal = {IFAC Proceedings Volumes},
volume = {40},
number = {9},
pages = {304-309},
year = {2007},
note = {11th IFAC Symposium on Large Scale Complex Systems Theory and Applications},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20070723-3-PL-2917.00049},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015327312},
author = {Andrzej Sikora and Ewa Niewiadomska-Szynkiewicz},
keywords = {computer simulation, communication protocols, distributed simulation, simulators, software performance, synchronization},
abstract = {This paper deals with the description of a software framework FR/ASimJava for large scale Frame Relay networks simulation and its application to the exampled networks simulation. FR/ASimJava is implemented based on ASimJava – a Java-based software package that can be used to develop parallel and distributed simulators of complex real-life systems. The focus is on scalability and efficiency of large scale networks simulation. We describe the functionality, design and performance of FR/ASimJava software. The case studies – simulations of six networks operating under Frame Relay are provided to illustrate the effectiveness of the presented software tool.}
}
@article{WOODSIDE2001107,
title = {Automated performance modeling of software generated by a design environment},
journal = {Performance Evaluation},
volume = {45},
number = {2},
pages = {107-123},
year = {2001},
note = {Performance Validation of Software Systems},
issn = {0166-5316},
doi = {https://doi.org/10.1016/S0166-5316(01)00033-5},
url = {https://www.sciencedirect.com/science/article/pii/S0166531601000335},
author = {Murray Woodside and Curtis Hrischuk and Bran Selic and Stefan Bayarov},
keywords = {Queuing model, Wideband approach, Layered queuing network model, Software performance, Software design modeling},
abstract = {Automation is needed to make performance modeling faster and more accessible to software designers. This paper describes a prototype tool that exploits recently developed techniques for automatic model construction from traces. The performance model is a layered queuing model and it is based on traces captured for certain selected scenarios which are determined to be important for performance. The prototype tool has been integrated with a commercial software design environment that generates code with heavy use of standard libraries. The execution costs of the libraries has also been captured and used in the automatic model creation. The approach not only automates building early models, but also gives models which can be maintained during development, using traces gathered from the implementations. The paper describes the tool, the process by which it is applied, the process of capturing and managing the execution cost data, and an example. To use the tool, the designer further defines the scenarios, the execution environment, and the workload intensity parameters. The designer can then experiment with different environments and workloads, or revise the design, to improve the performance.}
}
@article{SULYOK201950,
title = {Locality optimized unstructured mesh algorithms on GPUs},
journal = {Journal of Parallel and Distributed Computing},
volume = {134},
pages = {50-64},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519301698},
author = {András Attila Sulyok and Gábor Dániel Balogh and István Z Reguly and Gihan R Mudalige},
keywords = {Finite volume, Finite element, Race condition, GPU},
abstract = {Unstructured-mesh based numerical algorithms such as finite volume and finite element algorithms form an important class of applications for many scientific and engineering domains. The key difficulty in achieving higher performance from these applications is the indirect accesses that lead to data-races when parallelized. Current methods for handling such data-races lead to reduced parallelism and suboptimal performance. Particularly on modern many-core architectures, such as GPUs, that has increasing core/thread counts, reducing data movement and exploiting memory locality is vital for gaining good performance. In this work we present novel locality-exploiting optimizations for the efficient execution of unstructured-mesh algorithms on GPUs. Building on a two-layered coloring strategy for handling data races, we introduce novel reordering and partitioning techniques to further improve efficient execution. The new optimizations are then applied to several well established unstructured-mesh applications, investigating their performance on NVIDIA’s latest P100 and V100 GPUs. We demonstrate significant speedups (1.1–1.75×) compared to the state-of-the-art. A range of performance metrics are benchmarked including runtime, memory transactions, achieved bandwidth performance, GPU occupancy and data reuse factors and are used to understand and explain the key factors impacting performance. The optimized algorithms are implemented as an open-source software library and we illustrate its use for improving performance of existing or new unstructured-mesh applications.}
}
@article{ZHANG2021112748,
title = {A practical bond-based peridynamic modeling of reinforced concrete structures},
journal = {Engineering Structures},
volume = {244},
pages = {112748},
year = {2021},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.112748},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621008981},
author = {Ning Zhang and Quan Gu and Surong Huang and Xin Xue and Shaofan Li},
keywords = {Reinforced concrete, Bond-based peridynamics, ASI bond-slip model, OpenSees},
abstract = {Peridynamics has been increasingly used for the study of damage and failure behaviors of reinforced concrete (RC) structures, e.g., cracking, fragmentation, and interface debonding due to its strong capacity in analyzing discontinuous problems. This paper presents a practical bond-based peridynamics (BPD) modeling for simulating complicated nonlinear behaviors of RC structures. A novel coupled axial-shear interaction (ASI) bond-slip model is developed to simulate the relative slip and interface damages between the concrete and steel, while the complicated behaviors of concrete and steel are simulated using one-dimensional nonlinear material constitutive models. The BPD framework and the ASI algorithm are implemented in an open source finite element software OpenSees, allowing the advantage of its abundant nonlinear material libraries. The BPD modeling is verified by three application examples, i.e., a uniaxial tension and compression test of concrete specimens, a plain round steel bar pull-out test and a pushover analysis of an RC column, and cracking behaviors are analyzed. The results demonstrate the strong capacity of the enhanced BPD modeling in simulating the damage behavior for RC structures, e.g., the strength deterioration, stiffness degradation and pinch effect in the stress–strain response, as well as the crack developments and the bond-slips behaviors.}
}
@article{TAMMINGA2014788,
title = {Open Traffic: A Toolbox for Traffic Research},
journal = {Procedia Computer Science},
volume = {32},
pages = {788-795},
year = {2014},
note = {The 5th International Conference on Ambient Systems, Networks and Technologies (ANT-2014), the 4th International Conference on Sustainable Energy Information Technology (SEIT-2014)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.492},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914006929},
author = {G. Tamminga and P. Knoppers and J.W.C. {van Lint}},
keywords = {transport model, simulation, open source},
abstract = {Open Traffic is an open source software project that provides a transport modeling software environment. While most transport model packages offer ready-to-use modules for end-users, Open Traffic provides open access to a modelling environment for the (further) development of methods and algorithms and enables the sharing, distribution and further development of the implied knowledge. The Open Traffic platform is designed as a modular system which enables users to utilize existing modules and extend the system with new ones. The system supports the development of multi modal and multi scale models by providing a collection of objects that enable the creation of a transport infrastructure and its environment at multiple levels of detail. The definition of the geographical objects aligns to the principles of CityGML, an open standard for geo data that is internationally accepted by the Open Geospatial Consortium. Additional utilities such as a graphical editor and visualizer, in addition with facilities to import data from external sources like Open Streetmap and Esri shape files, enable users to quickly create and demonstrate their use-cases. In this article we present the high level architecture of Open Traffic, its current status, and a first application with the implementation of the micro simulation model MOTUS. Also, the possibilities and requirements to adhere Open Traffic to agent based modelling approaches are explored.}
}
@article{ABRAMOV20041303,
title = {Telescoping in the context of symbolic summation in Maple},
journal = {Journal of Symbolic Computation},
volume = {38},
number = {4},
pages = {1303-1326},
year = {2004},
note = {Symbolic Computation in Algebra and Geometry},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2003.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0747717104000446},
author = {S.A. Abramov and J.J. Carette and K.O. Geddes and H.Q. Le},
keywords = {Symbolic summation, Software design, Telescoping sums, Maple, Zeilberger’s algorithm, Closed form, Hypergeometric terms},
abstract = {This paper is an exposition of different methods for computing closed forms of definite sums. The focus is on recently-developed results on computing closed forms of definite sums of hypergeometric terms. A design and an implementation of a software package which incorporates these methods into the computer algebra system Maple are described in detail.}
}
@article{XHONNEUX2018177,
title = {The enhanced fuel management models of the HTR code package (HCP) module SHUFLE},
journal = {Nuclear Engineering and Design},
volume = {329},
pages = {177-191},
year = {2018},
note = {The Best of HTR 2016: International Topical Meeting on High Temperature Reactor Technology},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2017.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0029549317305642},
author = {A. Xhonneux and D. Lambertz and H.-J. Allelein},
abstract = {The HTR Code Package (HCP) has been under development since 2010 at Forschungszentrum Jülich and at Aachen University. This HCP combines the capabilities of individual legacy codes developed during the German HTR programme in a highly integrated manner and contains new features. Within the HCP, all aspects related to the fuel management are handled by SHUFLE (Software for Handling Universal Fuel Elements). In principle the module is designed to handle all kinds of fuel elements. The module depicts the pebble flow in such a way, that both the accuracy as well as calculation time constraints given by a multi-physics code package such as the HCP are met. SHUFLE offers two different fuel management models for pebble-bed reactors, the ‘flow tube model’ and the ‘seepage model’. Within this paper, the flow tube model is discussed in detail. Especially aspects such as the development, implementation, application and testing of this model are reviewed.}
}
@article{LLOYD20111240,
title = {Environmental modeling framework invasiveness: Analysis and implications},
journal = {Environmental Modelling & Software},
volume = {26},
number = {10},
pages = {1240-1250},
year = {2011},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2011.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364815211000867},
author = {W. Lloyd and O. David and J.C. Ascough and K.W. Rojas and J.R. Carlson and G.H. Leavesley and P. Krause and T.R. Green and L.R. Ahuja},
keywords = {Component-based modeling, Environmental modeling frameworks, Invasiveness, Frameworks, Software metrics},
abstract = {Environmental modeling frameworks support scientific model development by providing model developers with domain specific software libraries which are used to aid model implementation. This paper presents an investigation on the framework invasiveness of environmental modeling frameworks. Invasiveness, similar to object-oriented coupling, is defined as the quantity of dependencies between model code and a modeling framework. We investigated relationships between invasiveness and the quality of modeling code, and also the utility of using a lightweight framework design approach in an environmental modeling framework. Five metrics to measure framework invasiveness were proposed and applied to measure dependencies between model and framework code of several implementations of Thornthwaite and the Precipitation-Runoff Modeling System (PRMS), two well-known hydrological models. Framework invasiveness measures were compared with existing common software metrics including size (lines of code), cyclomatic complexity, and object-oriented coupling. Models with lower framework invasiveness tended to be smaller, less complex, and have less coupling. In addition, the lightweight framework implementations of the Thornthwaite and PRMS models were less invasive than the traditional framework model implementations. Our results show that model implementations with higher degrees of framework invasiveness also had structural characteristics which previously have been shown to predict poor maintainability, a non-functional code quality attribute of concern. We conclude that using a framework with a lightweight framework design shows promise in helping to improve the quality of model code and that the lightweight framework design approach merits further attention by environmental modeling framework developers.}
}
@article{WANG2014786,
title = {Insights from developing a multidisciplinary design and analysis environment},
journal = {Computers in Industry},
volume = {65},
number = {4},
pages = {786-795},
year = {2014},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2014.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0166361514000591},
author = {Chengen Wang},
keywords = {Multidisciplinary design and analysis, Application and information integration, Collaborative design, Parametric geometric modeling},
abstract = {This paper presents technical insights gained from developing a multidisciplinary design and analysis (MDA) environment, which proficiently integrates, coordinates and controls disciplinary software packages, data sources, and human factors. The MDA environment cost-effectively boosters global optimization of product design problems by means of integrating and coordinating engineering resources, implementing optimization approaches, and reconciling contradicting disciplinary objectives. This paper begins with depicting a multidisciplinary view of a generic complex engineering system, which sets the basic tone of developments of the environment. Subsequently, this paper proceeds with descriptions of information techniques or software utilities that constitute core competencies of the MDA environment. Firstly, software components based integration techniques are implemented to enhance interoperability among heterogeneous engineering applications and data sources. The integration techniques have viably overcome engineering inefficiencies, system incongruences, and information inconsistencies caused by ‘automation islands’ problems. Secondly, project and workflow management utilities are developed to support collaborative design, which allows better utilization of engineering resources and effective coordination. Thirdly, tailored geometry modeling techniques are implemented to enable expeditious representations of shape variations in congruence with outcomes of multiphysics analyses and simulations. Fourthly, optimization strategies, sensitivity analysis, surrogate models and searching algorithms are coded to enable global engineering optimization. Finally, this paper concludes with insights gained from developments of the MDA infrastructure.}
}
@article{SACCHI2015175,
title = {JTSA: An open source framework for time series abstractions},
journal = {Computer Methods and Programs in Biomedicine},
volume = {121},
number = {3},
pages = {175-188},
year = {2015},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2015.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715001492},
author = {Lucia Sacchi and Davide Capozzi and Riccardo Bellazzi and Cristiana Larizza},
keywords = {Temporal abstractions, Time series analysis, Data analysis workflow, Temporal pattern discovery, Biomedical data mining software tool},
abstract = {Background and objective
The evaluation of the clinical status of a patient is frequently based on the temporal evolution of some parameters, making the detection of temporal patterns a priority in data analysis. Temporal abstraction (TA) is a methodology widely used in medical reasoning for summarizing and abstracting longitudinal data.
Methods
This paper describes JTSA (Java Time Series Abstractor), a framework including a library of algorithms for time series preprocessing and abstraction and an engine to execute a workflow for temporal data processing. The JTSA framework is grounded on a comprehensive ontology that models temporal data processing both from the data storage and the abstraction computation perspective. The JTSA framework is designed to allow users to build their own analysis workflows by combining different algorithms. Thanks to the modular structure of a workflow, simple to highly complex patterns can be detected. The JTSA framework has been developed in Java 1.7 and is distributed under GPL as a jar file.
Results
JTSA provides: a collection of algorithms to perform temporal abstraction and preprocessing of time series, a framework for defining and executing data analysis workflows based on these algorithms, and a GUI for workflow prototyping and testing. The whole JTSA project relies on a formal model of the data types and of the algorithms included in the library. This model is the basis for the design and implementation of the software application. Taking into account this formalized structure, the user can easily extend the JTSA framework by adding new algorithms. Results are shown in the context of the EU project MOSAIC to extract relevant patterns from data coming related to the long term monitoring of diabetic patients.
Conclusions
The proof that JTSA is a versatile tool to be adapted to different needs is given by its possible uses, both as a standalone tool for data summarization and as a module to be embedded into other architectures to select specific phenotypes based on TAs in a large dataset.}
}
@article{AISSOU2015615,
title = {Modeling and control of hybrid photovoltaic wind power system with battery storage},
journal = {Energy Conversion and Management},
volume = {89},
pages = {615-625},
year = {2015},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2014.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0196890414009157},
author = {S. Aissou and D. Rekioua and N. Mezzai and T. Rekioua and S. Bacha},
keywords = {Photovoltaic power systems, Wind power system, Modeling, Storage, Control, LabVIEW Software, Data acquisition},
abstract = {In this paper, the model and the control of hybrid power system is presented. It comprises wind and photovoltaic sources with battery storage supplying a load via an inverter. First, the design and the identification of the hybrid power system components has been made, then the proposed system is modeled and simulated under Matlab/Simulink Package. Finally, the power control of the hybrid system is introduced, by using LabVIEW Software. The proposed control strategy has been experimentally implemented and practical results are presented to show the effectiveness of the proposed hybrid system.}
}
@incollection{KALAKUL20151415,
title = {VPPD Lab -The Chemical Product Simulator},
editor = {Krist V. Gernaey and Jakob K. Huusom and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {37},
pages = {1415-1420},
year = {2015},
booktitle = {12th International Symposium on Process Systems Engineering and 25th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63577-8.50081-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444635778500814},
author = {Sawitree Kalakul and Rehan Hussain and Nimir Elbashir and Rafiqul Gani},
keywords = {Chemical product design, Blended product, Jet-fuels},
abstract = {In this paper, the development of a systematic model-based framework for product design, implemented in the new product design software called VPPD-Lab is presented. This framework employs its in-house knowledge-based system to design and evaluate chemical products. The built-in libraries of product performance models and product-chemical property models are used to evaluate different classes of product. The product classes are single molecular structure chemicals (lipids, solvents, aroma, etc.), blended products (gasoline, jet-fuels, lubricants, etc.), and emulsified product (hand wash, detergent, etc.). It has interface to identify workflow/data-flow for the inter-related activities between knowledge-based system and model-based calculation procedures to systematically, efficiently and robustly solve various types of product design-analysis problems. The application of the software is highlighted for the case study of tailor made design of jet-fuels. VPPD-Lab works in the same way as a typical process simulator. It enhances the future development of chemical product design.}
}
@article{ATTARAN2018229,
title = {A novel, simple 3D/2D outflow boundary model for blood flow simulations in compliant arteries},
journal = {Computers & Fluids},
volume = {174},
pages = {229-240},
year = {2018},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0045793018304419},
author = {Seyed Hamidreza Attaran and Hanieh Niroomand-oscuii and Farzan Ghalichi},
keywords = {Outflow boundary, Fluid-structure interaction, Blood flow simulation, Numerical modeling},
abstract = {Outflow boundary condition is one of the most challenging issues in the simulation of blood flow in arteries. It has a great impact on the solution within computational domain. Our aim in the present study is to develop a new outflow boundary model which can be easily implemented in commercially available software packages and capable of capturing the three main features of downstream: resistance, compliance and wave reflection. The new boundary model is an extension of a 2D/3D elastic tube to the end of the main computational domain which is partially filled with porous material. Wave reflection characteristics of our model are investigated by applying it to the end of a straight vessel and an axisymmetric model of an aorta. The sensibility of the solution to model parameters is also studied. Pressure, velocity, and wave intensity curves resulted from numerical simulations, are presented. All main characteristics of pressure and flow waves were observed in results of simulations with our boundary model. Computed pressures were in agreement with those previously published in the literature. It is shown that this model provides accurate results while it avoids complexities of other models such as Windkessel and structured tree.}
}
@article{LYNN20161183,
title = {Toward Rapid Manufacturability Analysis Tools for Engineering Design Education},
journal = {Procedia Manufacturing},
volume = {5},
pages = {1183-1196},
year = {2016},
note = {44th North American Manufacturing Research Conference, NAMRC 44, June 27-July 1, 2016, Blacksburg, Virginia, United States},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2016.08.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978916301056},
author = {Roby Lynn and Christopher Saldana and Thomas Kurfess and Nithin Reddy and Timothy Simpson and Kathryn Jablokow and Tommy Tucker and Saish Tedia and Christopher Williams},
keywords = {GPGPU, High Performance Computing, Design for Manufacturability, Computer-Aided Manufacturing, Additive Manufacturing, Subtractive Manufacturing, Manufacturability Education, Manufacturability Analysis, Distributed Cybermanufacturing},
abstract = {Engineering students are often unaware of manufacturing challenges that are introduced during the design process. Students will sometimes design parts that are either very difficult or impossible to manufacture, because they are unaware of the intricacies and limitations of various manufacturing processes. Design for manufacturability (DFM) education must be improved to address these issues, and this work is a vision for implementation of a rapid method for facilitating DFM education in terms of subtractive and additive manufacturing processes. The goal is to teach students about how their designs impact ease and cost of manufacturing, in addition to giving them knowledge and intuition to fluidly move between both additive and subtractive manufacturing mindsets. This work describes use of a commercial high-performance computing (HPC)-accelerated parallelized trajectory planning software package called SculptPrint, which enables students to visualize the subtractive manufacturability of the parts they design. While SculptPrint is currently limited to subtractive manufacturability analysis, this work also describes the future development of a manufacturability analysis tool for Additive Manufacturing (AM). Analysis is performed on a set of sample parts for both subtractive and additive manufacturing. The results demonstrate the effectiveness of advanced manufacturability tools in manufacturing process selection with consideration of manufacturing time, cost, and complexity. A distributed architecture is also examined that will allow students to perform manufacturability analysis without physical access to HPC hardware.}
}