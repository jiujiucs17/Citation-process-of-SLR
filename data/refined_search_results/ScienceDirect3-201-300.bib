@article{XIONG2021100122,
title = {A digital framework for metrological information},
journal = {Measurement: Sensors},
volume = {18},
pages = {100122},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100122},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421000854},
author = {XingChuang Xiong and Yiwei Zhu and Jinyuan Li and Yuning Duan and Xiang Fang},
keywords = {Metrology for digital transformation (M4DT), Digital calibration certificate (DCC), Digital-SI, Digital framework, Digitalization of metrological information (DMI)},
abstract = {A digital-system structure is very important for developing Metrology for Digital Transformation (M4DT) at NMIs. A plan for the digital transformation in metrology at the National Institute of Metrology (NIM), China is proposed. The plan includes the digitalization of metrology standards, measuring instruments, metrological information, and metrological activities. Among them, the Digitalization of Metrological Information (DMI) is the key as a link between digital components and hence its development is a high priority. In order to realize a comprehensive DMI, a multi-layered, modular digital framework for structured information is designed, including a traceable chain of metadata models for metrological information, conversion and verification software libraries, and a supporting digital infrastructure. It provides an option for NMIs to implement digital transformation.}
}
@article{RODRIGUEZ201246,
title = {Assessing the SALSA architecture for developing agent-based ambient computing applications},
journal = {Science of Computer Programming},
volume = {77},
number = {1},
pages = {46-65},
year = {2012},
note = {System and Software Solution Oriented Architectures},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2010.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642310002261},
author = {Marcela D. Rodríguez and Jesús Favela},
keywords = {Ubiquitous computing, Autonomous agents, Middleware, Evaluation},
abstract = {We have proposed the use of autonomous agents for coping with some of the challenges of creating ambient computing systems. The motivation of this research is that ambient computing environments are characterized by the distribution, reactivity, collaboration and adaptation of their artifacts, which are also characteristics attributed to software agents. To assist developers in creating the software entities of an ambient computing environment, the Simple Agent Library for Smart Ambients (SALSA) was created. The SALSA middleware and architecture enables the creation of autonomous agents reactive to the context of the ambient computing environment. SALSA agents can represent users, resources, or wrap complex system functionality of the environment. The aim of this paper is to provide evidence that SALSA facilitates the implementation of ambient computing services through autonomous agents. Unfortunately, the literature on Ubicomp development frameworks has, aside from a few exceptions, not reported experimental evaluation of their usability. The evaluations of Ubicomp development platforms have not addressed this issue since their evaluations have been mostly focused on performance and to prove feasibility. We present the results of an empirical evaluation conducted to assess the use of SALSA. This study included in-lab programming experiments and design exercises to evaluate the programming facilities provided by SALSA agents. Even though for some of the evaluation participants the use of autonomous agents as an abstraction for the development of ubiquitous computing systems was not innate, the evaluation results demonstrate that the execution model of SALSA and its facilities to implement Ubicomp systems are comprehensible.}
}
@article{PURTILO199539,
title = {Extracting program structure for packaging in a component-based environment},
journal = {Computer Languages},
volume = {21},
number = {1},
pages = {39-48},
year = {1995},
issn = {0096-0551},
doi = {https://doi.org/10.1016/0096-0551(94)00013-G},
url = {https://www.sciencedirect.com/science/article/pii/009605519400013G},
author = {James M. Purtilo and Thomas M. Swiss and White L. Elizabeth},
keywords = {Interface description languages, Software packaging, Integration and interconnection},
abstract = {The development of a large-scale software application naturally lends itself to a top-down development process where the problem is divided into smaller problems that are dispersed among multiple people or teams. Once these sub-problems have been solved, the software components or modules implementing these solutions are integrated into the whole for a final solution. In a component-based environment, large applications are constructed by combining software components in a variety of ways. In such an environment, the components may be implemented in different languages and be distributed across different machines. Alternately, these modules may be different parts of a program running in a single address space. These options (and the many options in between) require different techniques for integration. Software packaging is an important method for assisting in the process of assembling large programs from separate pieces, especially over a heterogeneous environment (an environment consisting of different machines, architecture, languages or operating systems). A software packager is a tool that takes a description of the modules of a program, the available connection methods, and the final desired connection geometry, and creates the integration methods to create the application.}
}
@article{PORTER2004256,
title = {Architectures for integrating legacy information systems with modern bar code technology},
journal = {Journal of Manufacturing Systems},
volume = {23},
number = {3},
pages = {256-265},
year = {2004},
issn = {0278-6125},
doi = {https://doi.org/10.1016/S0278-6125(04)80038-4},
url = {https://www.sciencedirect.com/science/article/pii/S0278612504800384},
author = {J. David Porter and Richard E. Billo and Robert Rucker},
keywords = {Bar Codes, Legacy Systems, Material Tracking, Systems Integration, Web Services, Wireless Communications, XML},
abstract = {This work reviews alternative solutions for retrofitting existing legacy manufacturing information systems with modern bar code tracking hardware and software. Technologies are divided into one-tier architecturs, which include the use of screen scrapers and terminal emulation software; two-tier architectures, which include the use of standard and custom application program interfaces (API); and three-tier architectures, which discuss the application of standards such as CORBA, DCOM, XML messaging, and application protocols for seamless integration with legacy systems. A case study is provided that describes the use of a two-tier API product for implementing a wireless bar code data capture system with a proprietary warehouse management system. Concluding remarks discuss the importance of emerging three-tier architectures and wireless bar code technologies for business-to-business e-commerce applications. The approaches presented here are consonant with the modern view of considering legacy systems as also providing Web services via multiple channels serving multiple categories of clients.}
}
@article{RAO1993343,
title = {Confess—an architecture-independent concurrent finite element package},
journal = {Computers & Structures},
volume = {47},
number = {2},
pages = {343-348},
year = {1993},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(93)90385-Q},
url = {https://www.sciencedirect.com/science/article/pii/004579499390385Q},
author = {A.Rama Mohan Rao and K. Loganathan and N.V. Raman},
abstract = {Several concurrent algorithms for finite element analysis are available at present. Most research effort has been directed towards the development of concurrent algorithms (software) that will run as efficiently as possible on a particular hardware configuration. However, an important issue such as portability of algorithms across several parallel architectures has unfortunately not been given enough thought. In this paper, the development and implementation of a package called CONFESS (CONcurrent Finite Element Static analysis of Structures) is described. This concurrent finite element package is developed keeping software (or algorithmic) portability across different parallel architectures as the main objective. The basic strategy in this approach involves dividing the finite element mesh into a number of submeshes, solving all the submeshes concurrently by all the available processors. The processors all need to cooperate for a trivial amount of computation (interface nodal solution). Algorithmic details of the software package developed and issues related to portability are presented.}
}
@article{LIU2022114662,
title = {An efficient geometry-adaptive mesh refinement framework and its application in the immersed boundary lattice Boltzmann method},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {392},
pages = {114662},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114662},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522000573},
author = {Zhengliang Liu and Fang-Bao Tian and Xingya Feng},
keywords = {Adaptive mesh refinement, Hash table, Immersed boundary method, Lattice Boltzmann method},
abstract = {This work presents an adaptive mesh refinement (AMR) method. This AMR is based on a pointless representation of octrees, i.e. hash table. An individual hash table is used for each level of refinement to avoid conflicts of key values. Cases for two complex geometries are presented to analyse the performance of the AMR framework with different data structures and strategies. Then, the immersed boundary-lattice Boltzmann method (IB-LBM) is implemented as an example to evaluate the performance of the proposed AMR framework in computational fluid dynamics (CFD) applications. The integrated solver is validated and its performance is analysed through several cases. Considerable reductions of more than 65% in computational time are achieved when the hash table is adopted instead of the binary tree. Moreover, hash table offers more benefits to 3D cases with larger numbers of nodes. The AMR framework presented in this study is simple to implement with the standard C++ libraries and time-efficient for computational fluid dynamics (CFD) applications, which is released as open-source software and can be used by researchers with their CFD solvers.}
}
@article{ROBINSON2008481,
title = {Composing software services in the pervasive computing environment: Languages or APIs?},
journal = {Pervasive and Mobile Computing},
volume = {4},
number = {4},
pages = {481-505},
year = {2008},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2008.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574119208000035},
author = {Jon Robinson and Ian Wakeman and Dan Chalmers},
keywords = {Pervasive computing, Programming languages, Performance evaluation},
abstract = {The pervasive computing environment will be composed of heterogeneous services. In this work, we have explored how a domain specific language for service composition can be implemented to capture the common design patterns for service composition, yet still retain a comparable performance to other systems written in mainstream languages such as Java. In particular, we have proposed the use of the method delegation design pattern, the resolution of service bindings through the use of dynamically adjustable characteristics and the late binding of services as key features in simplifying the service composition task. These are realised through the Scooby language, and the approach is compared to the use of APIs to define adaptable services.}
}
@article{WARD201860,
title = {Matminer: An open source toolkit for materials data mining},
journal = {Computational Materials Science},
volume = {152},
pages = {60-69},
year = {2018},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0927025618303252},
author = {Logan Ward and Alexander Dunn and Alireza Faghaninia and Nils E.R. Zimmermann and Saurabh Bajaj and Qi Wang and Joseph Montoya and Jiming Chen and Kyle Bystrom and Maxwell Dylla and Kyle Chard and Mark Asta and Kristin A. Persson and G. Jeffrey Snyder and Ian Foster and Anubhav Jain},
keywords = {Data mining, Open source software, Machine learning, Materials informatics},
abstract = {As materials data sets grow in size and scope, the role of data mining and statistical learning methods to analyze these materials data sets and build predictive models is becoming more important. This manuscript introduces matminer, an open-source, Python-based software platform to facilitate data-driven methods of analyzing and predicting materials properties. Matminer provides modules for retrieving large data sets from external databases such as the Materials Project, Citrination, Materials Data Facility, and Materials Platform for Data Science. It also provides implementations for an extensive library of feature extraction routines developed by the materials community, with 47 featurization classes that can generate thousands of individual descriptors and combine them into mathematical functions. Finally, matminer provides a visualization module for producing interactive, shareable plots. These functions are designed in a way that integrates closely with machine learning and data analysis packages already developed and in use by the Python data science community. We explain the structure and logic of matminer, provide a description of its various modules, and showcase several examples of how matminer can be used to collect data, reproduce data mining studies reported in the literature, and test new methodologies.}
}
@incollection{PARKER2017361,
title = {Chapter 27 - Implementation Using Digital Signal Processors},
editor = {Michael Parker},
booktitle = {Digital Signal Processing 101 (Second Edition)},
publisher = {Newnes},
edition = {Second Edition},
pages = {361-369},
year = {2017},
isbn = {978-0-12-811453-7},
doi = {https://doi.org/10.1016/B978-0-12-811453-7.00027-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128114537000275},
author = {Michael Parker},
keywords = {Accumulators, Barrel shifters, Circular buffers, Data address generators, Digital signal processing, DSP processors, Ethernet, Fixed point, Floating point, Interrupt service routines},
abstract = {The chapter speaks about the definition of digital signal processor. It is a special form of microprocessor that is optimized to perform digital signal processing (DSP) operations. Often called “DSPs” due to the function they perform, they are designed to perform DSP functions in an efficient method, using conventional software programming design and verification techniques discussed in the chapter. Owing to the special features and parallelism added for DSP operations, conventional programming languages such as C do not always produce the most optimal results, and rely on pre-optimized library calls for common algorithms and functions. Although advanced compilers developed by the DSP processor manufacturers do try to interpret the programmer's intent and map the DSP processor features in the most effective manner. For this reason, a vendor-specific language, known as assembly, is instead used to code the most DSP-intensive portions of the software. But the writing in assembly code does require knowledge of the DSP processor hardware architecture and how to optimally write code that takes advantage of the DSP processor architecture features.}
}
@article{LIU2007612,
title = {The modular multisensory DLR-HIT-Hand},
journal = {Mechanism and Machine Theory},
volume = {42},
number = {5},
pages = {612-625},
year = {2007},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2006.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X0600098X},
author = {H. Liu and P. Meusel and N. Seitz and B. Willberg and G. Hirzinger and M.H. Jin and Y.W. Liu and R. Wei and Z.W. Xie},
keywords = {Dexterous robot hand, DSP, FPGA, Modular},
abstract = {The paper presents hardware and software architecture of the new developed compact multisensory DLR-HIT hand. The hand has four identical fingers and an extra degree of freedom for palm. In each finger there is a Field Programmable Gate Array (FPGA) for data collection, brushless DC motors control and communication with palm’s FPGA by Point-to-Point Serial Communication (PPSeCo). The kernel of the hardware system is a PCI-based high speed floating-point Digital Signal Processor (DSP) for data processing, and FPGA for high speed (up to 25Mbps) real-time serial communication with the palm’s FPGA. In order to achieve high modularity and reliability of the hand, a fully mechatronic integration and analog signals in situ digitalization philosophy are implemented to minimize the dimension, number of the cables (five cables including power supply) and protect data communication from outside disturbances. Furthermore, according to the hardware structure of the hand, a hierarchical software structure has been established to perform all data processing and the control of the hand. It provides basic API functions and skills to access all hardware resources for data acquisition, computation and tele-operation. With the nice design of the hand’s envelop, the hand looks more like humanoid.}
}
@article{CARBONE201892,
title = {PySE: Software for extracting sources from radio images},
journal = {Astronomy and Computing},
volume = {23},
pages = {92-102},
year = {2018},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2213133716300671},
author = {D. Carbone and H. Garsden and H. Spreeuw and J.D. Swinbank and A.J. {van der Horst} and A. Rowlinson and J.W. Broderick and E. Rol and C. Law and G. Molenaar and R.A.M.J. Wijers},
keywords = {Astronomical transients, Techniques, Image processing, Methods, Data analysis},
abstract = {PySE is a Python software package for finding and measuring sources in radio telescope images. The software was designed to detect sources in the LOFAR telescope images, but can be used with images from other radio telescopes as well. We introduce the LOFAR Telescope, the context within which PySE was developed, the design of PySE, and describe how it is used. Detailed experiments on the validation and testing of PySE are then presented, along with results of performance testing. We discuss some of the current issues with the algorithms implemented in PySE and their interaction with LOFAR images, concluding with the current status of PySE and its future development.}
}
@article{KLUKOWSKA2013424,
title = {SNARK09 – A software package for reconstruction of 2D images from 1D projections},
journal = {Computer Methods and Programs in Biomedicine},
volume = {110},
number = {3},
pages = {424-440},
year = {2013},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169260713000060},
author = {Joanna Klukowska and Ran Davidi and Gabor T. Herman},
keywords = {SNARK09, Reconstruction from projections, Software package, Evaluation, Simulation, Computerized tomography},
abstract = {The problem of reconstruction of slices and volumes from 1D and 2D projections has arisen in a large number of scientific fields (including computerized tomography, electron microscopy, X-ray microscopy, radiology, radio astronomy and holography). Many different methods (algorithms) have been suggested for its solution. In this paper we present a software package, SNARK09, for reconstruction of 2D images from their 1D projections. In the area of image reconstruction, researchers often desire to compare two or more reconstruction techniques and assess their relative merits. SNARK09 provides a uniform framework to implement algorithms and evaluate their performance. It has been designed to treat both parallel and divergent projection geometries and can either create test data (with or without noise) for use by reconstruction algorithms or use data collected by another software or a physical device. A number of frequently-used classical reconstruction algorithms are incorporated. The package provides a means for easy incorporation of new algorithms for their testing, comparison and evaluation. It comes with tools for statistical analysis of the results and ten worked examples.}
}
@article{SHARMA20181,
title = {OpCloudSec: Open cloud software defined wireless network security for the Internet of Things},
journal = {Computer Communications},
volume = {122},
pages = {1-8},
year = {2018},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S014036641730378X},
author = {Pradip Kumar Sharma and Saurabh Singh and Jong Hyuk Park},
keywords = {Internet-of-Things, Software Defined Networking, Security, Deep learning},
abstract = {Cutting-edge cloud frameworks will require a paradigm shift in regards to how they are built and managed. Traditional management and control platforms face significant challenges in terms of security, reliability, and flexibility that these cutting-edge frameworks must deal with. On the other hand, Distributed Denial of Service (DDoS) attacks have become a weapon of choice for cyber-terrorists, cyber-extortionists, and hackers. Recently, the simplicity of programmability in Software-Defined Networking (SDN) makes it a good platform for the implementation of various initiatives that includes decentralized network management, dynamic topology changes, and application deployment in a multi-tenant data center environment. Motivated by the capabilities of SDN, we are proposing a mitigation architecture for security attacks that incorporates a highly programmable monitoring network so as to make it possible to identify attacks. It has a flexible control structure to quickly define the reaction of attacks and particular side, and we show how SDN can be used as a key application in the cloud IoT. We evaluated the performance of our proposed architecture and compared it with the existing models to obtain various performance measures. The results of our evaluation show that our OpCloudSec architecture model can efficiently and effectively meet the security challenges created by the new network paradigm.}
}
@article{GIANCARLO2015207,
title = {ValWorkBench: An open source Java library for cluster validation, with applications to microarray data analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {118},
number = {2},
pages = {207-217},
year = {2015},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2014.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169260714003939},
author = {R. Giancarlo and D. Scaturro and F. Utro},
keywords = {Microarray cluster analysis, Bioinformatics software, Pattern discovery in bioinformatics and biomedicine},
abstract = {The prediction of the number of clusters in a dataset, in particular microarrays, is a fundamental task in biological data analysis, usually performed via validation measures. Unfortunately, it has received very little attention and in fact there is a growing need for software tools/libraries dedicated to it. Here we present ValWorkBench, a software library consisting of eleven well known validation measures, together with novel heuristic approximations for some of them. The main objective of this paper is to provide the interested researcher with the full software documentation of an open source cluster validation platform having the main features of being easily extendible in a homogeneous way and of offering software components that can be readily re-used. Consequently, the focus of the presentation is on the architecture of the library, since it provides an essential map that can be used to access the full software documentation, which is available at the supplementary material website [1]. The mentioned main features of ValWorkBench are also discussed and exemplified, with emphasis on software abstraction design and re-usability. A comparison with existing cluster validation software libraries, mainly in terms of the mentioned features, is also offered. It suggests that ValWorkBench is a much needed contribution to the microarray software development/algorithm engineering community. For completeness, it is important to mention that previous accurate algorithmic experimental analysis of the relative merits of each of the implemented measures [19], [23], [25], carried out specifically on microarray data, gives useful insights on the effectiveness of ValWorkBench for cluster validation to researchers in the microarray community interested in its use for the mentioned task.}
}
@article{ESSERS2014385,
title = {Evaluating a Prototype Approach to Validating a DDS-based System Architecture for Automated Manufacturing Environments},
journal = {Procedia CIRP},
volume = {25},
pages = {385-392},
year = {2014},
note = {8th International Conference on Digital Enterprise Technology - DET 2014 Disruptive Innovation in Manufacturing Engineering towards the 4th Industrial Revolution},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2014.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S2212827114010841},
author = {M.S. Essers and T.H.J. Vaneker},
keywords = {Data Distribution Service (DDS), Industrial Robots, Manufacturing},
abstract = {Data Distribution Services (DDS) are emerging as communication systems in manufacturing environments. One of the key features of a DDS based system is the ability to regain performance levels after the introduction or removal of a DDS participant. In implementing a DDS participant to an existing system, message transport speed and message latency is often sacrificed due to protection problems in OEM software. Validity and suitability for integration of OpenDDS specifically, a manufacturing system is evaluated by defining two implementation scenarios; a flexible approach with a dedicated DDS participant application, and a high speed approach integrating the OpenDDS API directly in the target application. The system is validated by monitoring performance, efficiency and robustness in use and implementation. This result is part of a system architecture, developed for project Smart Industrial Robotics (SInBot), that focuses on maximizing the efficient use of mobile industrial robots during medium sized production runs. This modular system architecture is based on distributed intelligence and decentralized control to enable online reconfiguration of industrial robots in manufacturing facilities.}
}
@article{BRONNIMANN2006111,
title = {The design of the Boost interval arithmetic library},
journal = {Theoretical Computer Science},
volume = {351},
number = {1},
pages = {111-118},
year = {2006},
note = {Real Numbers and Computers},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2005.09.062},
url = {https://www.sciencedirect.com/science/article/pii/S0304397505006110},
author = {Hervé Brönnimann and Guillaume Melquiond and Sylvain Pion},
keywords = {Interval arithmetic, Software library, Generic programming, Policy-based design, Robust computations, Floating-point filter},
abstract = {We present the design of the Boost interval arithmetic library, a C++ library designed to handle mathematical intervals efficiently and in a generic way. Interval computations are an essential tool for reliable computing. Increasingly a number of mathematical proofs have relied on global optimization problems solved using branch-and-bound algorithms with interval computations; it is therefore extremely important to have a mathematically correct implementation of interval arithmetic. Various implementations exist with diverse semantics. Our design is unique in that it uses policies to specify three independent variable behaviors: rounding, checking, and comparisons. As a result, with the proper policies, our interval library is able to emulate almost any of the specialized libraries available for interval arithmetic, without any loss of performance nor sacrificing the ease of use. This library is openly available at www.boost.org.}
}
@article{ALANAZI2021110945,
title = {Facilitating program comprehension with call graph multilevel hierarchical abstractions},
journal = {Journal of Systems and Software},
volume = {176},
pages = {110945},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110945},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100042X},
author = {Rakan Alanazi and Gharib Gharibi and Yugyung Lee},
keywords = {Program comprehension, Static analysis, Static call graphs, Machine learning, Hierarchical clustering},
abstract = {Program comprehension is a fundamental prerequisite for software maintenance and evolution. In order to understand a software structure, developers often read its codebase or documentation—if available and not outdated. Both approaches are tedious, time-consuming, and inefficient. Recent methods and tools have emerged to facilitate program comprehension, such as static call graphs, which depict the structure of the software system as a directed graph. However, the usage of call graphs still faces two main challenges: (1) large call graphs can be difficult to understand, and (2) they are limited to a single level of granularity, such as function calls. In this paper, we introduce a coarsening technique to create multi-level, hierarchical representations of the call graph. Specifically, we propose a hierarchical clustering approach of the execution paths to visualize the call graph at different granularity levels and for different software units, including packages, classes, and functions. Our overarching goal is to assist software developers in understanding the software system from a high-level of abstraction to the low-level of implementation with the ability to focus on particular parts of the system individually. To validate our approach and tool support, we conducted a user study of 18 software engineers from more than 11 industries who carried out several tasks using our system and then answered a survey. The results demonstrate that our approach is feasible to automatically construct multi-level abstractions of the call graph and hierarchically cluster them into meaningful abstractions. A video demo of the tool is available at https://rakanalanazi.github.io/CodEx/.}
}
@article{TAREKIBNZIAD201596,
title = {On kernel acceleration of electromagnetic solvers via hardware emulation},
journal = {Computers & Electrical Engineering},
volume = {47},
pages = {96-113},
year = {2015},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2015.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0045790615003031},
author = {M. {Tarek Ibn Ziad} and Mohamed Hossam and Mohamad A. Masoud and Mohamed Nagy and Hesham A. Adel and Yousra Alkabani and M. Watheq El-Kharashi and Khaled Salah and Mohamed AbdelSalam},
keywords = {Electromagnetic simulations, Emulation in hardware, Finite element method, Gaussian Elimination method, Jacobi iterative method, Metamaterials},
abstract = {Finding new techniques to accelerate electromagnetic (EM) simulations has become a necessity nowadays due to its frequent usage in industry. As they are mainly based on domain discretization, EM simulations require solving enormous systems of linear equations simultaneously. Available software-based solutions do not scale well with the increasing number of equations to be solved. As a result, hardware accelerators have been utilized to speed up the process. We introduce using hardware emulation as an efficient solution for EM simulation core solvers. Two different scalable architectures are implemented to accelerate the solver part of an EM simulator based on the Gaussian Elimination and the Jacobi iterative methods. Results show that the performance gap between presented solutions and software-based ones increases as the number of equations increases. For example, solving 2,002,000 equations using our Clustered Jacobi design in single floating-point precision achieved a speed-up of 100.88x and 35.24x over pure software implementations represented by MATLAB and the ALGLIB C++ package, respectively.}
}
@article{HAN1998453,
title = {Modeler-independent feature recognition in a distributed environment},
journal = {Computer-Aided Design},
volume = {30},
number = {6},
pages = {453-463},
year = {1998},
issn = {0010-4485},
doi = {https://doi.org/10.1016/S0010-4485(97)00097-3},
url = {https://www.sciencedirect.com/science/article/pii/S0010448597000973},
author = {J.H. Han and A.A.G Requicha},
abstract = {Solid modelers and other CAD/CAM subsystems are moving to distributed heterogeneous computing environments, so as to support design and manufacturing processes that are temporally and spatially distributed. Communication and collaboration among the software components of such distributed systems require protocols for accessing remote objects. This paper discusses an approach that provides transparent access to diverse solid modelers in a distributed environment. A solid modeler is augmented with a software wrapper, called an adaptor, so as to provide a uniform application programming interface (API). Applications interact with the uniform API and need not concern themselves with the specifics of the modeling systems used. API calls are implemented in a client-server architecture, in which a modeler and its adaptor function as a geometry server, and various applications communicate with the server through remote procedure calls (RPCs). A few adaptors have been implemented at the University of Southern California's Programmable Automation Laboratory, and have been used routinely for several years. This paper discusses adaptor design problems and our approach to their solutions. It illustrates the application of our methods through an example that involves the incremental recognition of machinable features in a distributed environment. This environment includes a geometry server, a simple feature-based design system, a state-of-the-art feature recognizer, and a graphics renderer, all running as separate processes in different machines. To our knowledge, this is the first documented effort in which a complex application such as feature recognition is capable of running, unmodified, on top of modelers based on constructive solid geometry or on boundary representations, which are fundamentally different.}
}
@article{FREDIAN2008317,
title = {MDSplus extensions for long pulse experiments},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {317-320},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0920379607004267},
author = {T. Fredian and J. Stillerman and G. Manduchi},
keywords = {MDSplus, Data acquisition, Data analysis, Long pulse},
abstract = {MDSplus is a data acquisition and analysis software package used widely throughout the international fusion research community. It was originally designed for use on pulsed experiments where experiment measurements are typically acquired by external measurement devices with local memory and then gathered by the data system after the pulse has completed. Today, more and more fusion research devices are being constructed which have very long pulse lengths. It is no longer acceptable to wait until the pulse completes before acquiring the measurements and making them available to the researchers. To explore the possibility of adapting MDSplus for use on experiments with long pulses, we have designed and implemented some prototype extensions to make MDSplus more suitable for those types of experiments. These extensions may be applicable in other areas such as data handling for fusion modeling codes. To implement these extensions, an additional API was developed to enable applications to store data incrementally in a node. The data is stored in an indexed linked list of data segments in the MDSplus data file enabling efficient retrieval of subsets of the data within a specified time interval.}
}
@article{MCKENNA2022104016,
title = {Machine learning based predictive model for the analysis of sequence activity relationships using protein spectra and protein descriptors},
journal = {Journal of Biomedical Informatics},
volume = {128},
pages = {104016},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104016},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000326},
author = {Adam Mckenna and Sandhya Dubey},
keywords = {Digital Signal Processing, Directed Evolution, Machine Learning, Protein Spectra, Physiochemical Descriptors},
abstract = {Accurately establishing the connection between a protein sequence and its function remains a focal point within the field of protein engineering, especially in the context of predicting the effects of mutations. From this, there has been a continued drive to build accurate and reliable predictive models via machine learning that allow for the virtual screening of many protein mutant sequences, measuring the relationship between sequence and ‘fitness’ or ‘activity’, commonly known as a Sequence-Activity-Relationship (SAR). An important preliminary stage in the building of these predictive models is the encoding of the chosen sequences. Evaluated in this work is a plethora of encoding strategies using the Amino Acid Index database, where the indices are transformed into their spectral form via Digital Signal Processing (DSP) techniques, as well as numerous protein structural and physiochemical descriptors. The encoding strategies are explored on a dataset curated to measure the thermostability of various mutants from a recombination library, designed from parental cytochrome P450s. In this work it was concluded that the implementation of protein spectra in concatenation with protein descriptors, together with the Partial Least Squares Regression (PLS) algorithm, gave the most noteworthy increase in the quality of the predictive models (as described in Encoding Strategy C), highlighting their utility in identifying an SAR. The accompanying software produced for this paper is termed pySAR (Python Sequence-Activity-Relationship), which allows for a user to find the optimal arrangement of structural and or physiochemical properties to encode their specific mutant library dataset; the source code is available at: https://github.com/amckenna41/pySAR.}
}
@article{MUKHERJEE2021107217,
title = {A PMU-Based Control Scheme for Islanded Operation and Re-synchronization of DER},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {133},
pages = {107217},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107217},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521004567},
author = {Biswarup Mukherjee and Marcelo {de Castro Fernandes} and Luigi Vanfretti},
keywords = {Power systems modeling, PMU, Controlled islanding, Island operation, Re-synchronization, Power grid, DER, Modelica, OpenIPSL},
abstract = {This article proposes a novel synchrophasor-based control scheme enabling controlled islanding, islanded operation and automatic re-synchronization of a distributed energy resource (DER) in a distribution network. The performance of the proposed control system is studied using a test power system model. The DER controller uses a centralized architecture, receiving phasor measurement unit (PMU) measurements from both the transmission and distribution grids. In simulation, the frequency control function inside the proposed controller uses frequency estimates calculated using a new formula that exploits the bus voltage in rectangular form (real and imaginary values). The performance of the proposed frequency computation method is studied and compared with the conventional washout filter (WF) approach used by most power system software tools. The study also discusses why unwrapped bus angles are necessary to perform the automatic re-synchronization process. The performance of the proposed controller is evaluated using both deterministic and stochastic load models, allowing the assessment of variability in distribution grids. The implementation of the proposed control scheme and the simulation of the test system is carried out leveraging rich features of Modelica language and the Open-Instance Power System Library (OpenIPSL).}
}
@article{CHANG2017195,
title = {A cybernetics Social Cloud},
journal = {Journal of Systems and Software},
volume = {124},
pages = {195-211},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215002939},
author = {Victor Chang},
keywords = {SocialMedia API, Data visualization, Big Data cybernetics},
abstract = {This paper proposes a Social Cloud, which presents the system design, development and analysis. The technology is based on the BOINC open source software, our hybrid Cloud, Facebook Graph API and our development in a new Facebook API, SocialMedia. The creation of SocialMedia API with its four functions can ensure a smooth delivery of Big Data processing in the Social Cloud, with four selected examples provided. The proposed solution is focused on processing the contacts who click like or comment on the author's posts. Outputs result in visualization with their core syntax being demonstrated. Four functions in the SocialMedia API have evaluation test and each client-server API processing can be completed efficiently and effectively within 1.36 s. We demonstrate large scale simulations involved with 50,000 simulations and all the execution time can be completed within 70,000 s. Cybernetics functions are created to ensure that 100% job completion rate for Big Data processing. Results support our case for Big Data processing on Social Cloud with no costs involved. All the steps involved have closely followed system design, implementation, experiments and validation for Cybernetics to ensure a high quality of outputs and services at all times. This offers a unique contribution for Cybernetics to meet Big Data research challenges.}
}
@article{XU200812733,
title = {Structure Improvement of an XY Flexure Micromanipulator for Micro/Nano Scale Manipulation},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {12733-12738},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.02154},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016410207},
author = {Qingsong Xu and Yangmin Li},
abstract = {An effort made towards the performance improvement for an XY micromanipulator featuring parallel architecture and flexure hinges has been presented in this paper. Through the implementation of four steps evolution of the original structure, a new manipulator with decoupled motion is finally obtained, which owns an enlarged workspace and eliminated stiffening and buckling phenomena. The merits of the new manipulator have been illustrated via the finite element analysis performed via ANSYS software package. It is believed that the generated flexure micromanipulator can be adopted in the applications involving manipulation of objects in micro- and nano-meter scales.}
}
@article{SKALDEBO2022409,
title = {Dynamic Bayesian Networks for Reduced Uncertainty in Underwater Operations},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {31},
pages = {409-414},
year = {2022},
note = {14th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.462},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322025083},
author = {Martin Skaldebø and Ingrid Schjølberg},
keywords = {Bayesian belief network, online risk assessment, dynamic risk models, underwater intervention},
abstract = {This paper presents a novel framework for modelling dynamic Bayesian belief networks (BBNs) for online risk assessment in underwater operations. Existing frameworks spans from commercial software with restricted code access to non-profit open source frameworks. Frameworks with restricted code access often provides general user interfaces and visualization tools, while open source frameworks provides access to code for developers. The model presented in this paper pursues a best of both worlds scenario, where the model implementation should be uncomplicated while also providing visualization and verification to provide the user with a clear perception of the implemented model. The presented method is an expansion of the Bayesian model of the pomegranate python library, and simplifies the procedure of building, verifying and utilizing BBN models. The method is applied to a conceptual design of an underwater scenario case study with a model for an underwater vehicle manipulator system.}
}
@article{YOON201688,
title = {Provisioning of power event APIs as a mobile OS facility},
journal = {Journal of Systems Architecture},
volume = {71},
pages = {88-101},
year = {2016},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762116301552},
author = {Chanmin Yoon and Seokjun Lee and Rhan Ha and Hojung Cha},
keywords = {Power management, Mobile operating system, Power event, Monitoring},
abstract = {Monitoring various hardware and software events for energy consumption is essential for energy management in mobile devices. However, current mobile operating systems (OS) lack monitoring functionality and do not provide sufficient information of this kind. In this paper, we propose PEMOS (Power Events Monitor for Mobile Operating Systems), a framework for power event APIs for mobile devices, that provides a wide spectrum of energy-related information, enabling in-depth analysis of energy problems. PEMOS provides a set of well-defined APIs as a mobile OS facility, defining various energy-related system events as power events. These are classified into system events and application events, encompassing extensive and fine-grained power-related events. Benefits of PEMOS include extensive coverage of power events, high portability across various platforms, and efficient API implementation. The framework structure is portable across multiple devices, and the standard ioctl-based API implementation enables the same operations on different devices without system modification. We implemented PEMOS on the Android platform to evaluate its efficacy and usefulness. The experimental results and case studies confirm that PEMOS is effective and useful for a range of energy management systems, with minimal overhead.}
}
@article{SZABOLICS2017995,
title = {Software development for the simultaneous control of ten intelligent overview video cameras at W7-X},
journal = {Fusion Engineering and Design},
volume = {123},
pages = {995-1000},
year = {2017},
note = {Proceedings of the 29th Symposium on Fusion Technology (SOFT-29) Prague, Czech Republic, September 5-9, 2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2017.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S0920379617301667},
author = {Tamás Szabolics and Gábor Cseh and Gábor Kocsis and Tamás Szepesi and Sándor Zoletnik and Christoph Biedermann and Ralf König},
keywords = {EDICAM, Event detection intelligent camera, Software, Video diagnostics, W7-X},
abstract = {In the past few years a ten channel video diagnostics system was developed, built and installed for Wendestein 7-X stellarator (W7-X). The system is based on EDICAM (Event Detection and Intelligent Camera) CMOS cameras (400 fps @ 1.3 Mpixel). In the first W7-X experimental campaign (OP1.1) the video diagnostic system was not integrated into the central control and data acquisition system of W7-X, therefore the development of a user friendly interface was necessary to fulfill the following complex requirements: manage the configuration of all cameras (each with four independent region of interests), the measurement cycle and the real-time storage of the collected data to SSDs, and it has to provide a live video stream of all the ten cameras in the control room. The full bandwidth movie stream yields a data rate of approximately 1GB/s for a single camera, therefore downsampled data was used for the live view. The software also has to cope with the huge amount of data when W7-X will operate for up-to 30min. The software package is organized into two stand-alone pieces: VIDACS (Video Diagnostics Data Acquisition and Control Software) being the user interface and EDIDAQ (EDICAM Data Acquisition Software) controlling the individual EDICAMs. This paper will present the detailed design, implementation, testing and the first operation experiences of this software package.}
}
@article{HUANG201487,
title = {Network Hypervisors: Enhancing SDN Infrastructure},
journal = {Computer Communications},
volume = {46},
pages = {87-96},
year = {2014},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140366414000449},
author = {Shufeng Huang and James Griffioen and Kenneth L. Calvert},
keywords = {HyperNet, Network Hypervisor, SDN},
abstract = {Software-Defined Networking (SDN) has been widely recognized as a promising way to deploy new services and protocols in future networks. The ability to “program” the network enables applications to create innovative new services inside the network itself. However, current SDN programmability comes with downsides that could hinder its adoption and deployment. First, in order to offer complete control, today’s SDN networks provide low-level API’s on which almost any type of service can be written. Because the starting point is a set of low-level API calls, implementing high-level complex services needed by future network applications becomes a challenging task. Second, the set of emerging SDN technologies that are beginning to appear have little in common with one another, making it difficult to set up a flow that traverses multiple SDN technologies/providers. In this paper we propose a new way to set up SDN networks spanning multiple SDN providers. The key to our approach is a Network Hypervisor service. The Network Hypervisor offers high-level abstractions and APIs that greatly simplify the task of creating complex SDN network services. Moreover, the Network Hypervisor is capable of internetworking various SDN providers together under a single interface/abstraction so that applications can establish end-to-end flows without the need to see, or deal with, the differences between SDN providers.}
}
@article{PALOMBA2021102148,
title = {Dynamic modelling of Adsorption systems: a comprehensive calibrateddataset for heat pump and storage applications},
journal = {Journal of Energy Storage},
volume = {33},
pages = {102148},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2020.102148},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X20319745},
author = {V. Palomba and S. Nowak and B. Dawoud and A. Frazzica},
abstract = {The growing efforts for the development of clean and efficient energy systems require the use of a multi-disciplinary approach and the integration of multiple generation appliances. Among the fields that can be considered enabling technologies, adsorption systems for air conditioning and thermal energy storages, are constantly increasing their maturity. However, for a proper design and integration of such systems, there is the need for a simulation framework that is reliable and computationally convenient. In the present paper, the implementation of a dynamic model for adsorption systems is presented, which includes different components (adsorber, phase changer, sorption materials) and is structured as a library. Modelica language and the commercial software Dymola® are used for the analysis. Data for different heat exchangers and working pairs are calibrated using experimental results and the calibrated model is subsequently used for the design of an adsorber based on a plate heat exchanger for thermal energy storage applications. The results proved that the model is fast and can reproduce experimental results with good accuracy, thus being a useful tool for the design and optimization of the different components of sorption systems.}
}
@article{BADER2007720,
title = {High performance combinatorial algorithm design on the Cell Broadband Engine processor},
journal = {Parallel Computing},
volume = {33},
number = {10},
pages = {720-740},
year = {2007},
note = {High-Performance Computing Using Accelerators},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2007.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167819107001068},
author = {David A. Bader and Virat Agarwal and Kamesh Madduri and Seunghwa Kang},
keywords = {Combinatorial algorithms, Cell Broadband Engine processor, Multicore, List ranking, zlib, Parallel algorithms, Graph algorithms, Performance, Novel architectures},
abstract = {The Sony–Toshiba–IBM Cell Broadband Engine (Cell/B.E.) is a heterogeneous multicore architecture that consists of a traditional microprocessor (PPE) with eight SIMD co-processing units (SPEs) integrated on-chip. While the Cell/B.E. processor is architected for multimedia applications with regular processing requirements, we are interested in its performance on problems with non-uniform memory access patterns. In this article, we present two case studies to illustrate the design and implementation of parallel combinatorial algorithms on Cell/B.E.: we discuss list ranking, a fundamental kernel for graph problems, and zlib, a data compression and decompression library. List ranking is a particularly challenging problem to parallelize on current cache-based and distributed memory architectures due to its low computational intensity and irregular memory access patterns. To tolerate memory latency on the Cell/B.E. processor, we decompose work into several independent tasks and coordinate computation using the novel idea of Software-Managed threads (SM-Threads). We apply this generic SPE work-partitioning technique to efficiently implement list ranking, and demonstrate substantial speedup in comparison to traditional cache-based microprocessors. For instance, on a 3.2GHz IBM QS20 Cell/B.E. blade, for a random linked list of 1 million nodes, we achieve an overall speedup of 8.34 over a PPE-only implementation. Our second case study, zlib, is a data compression/decompression library that is extensively used in both scientific as well as general purpose computing. The core kernels in the zlib library are the LZ77 longest subsequence matching algorithm and Huffman data encoding. We design efficient parallel algorithms for these combinatorial kernels, and exploit concurrency at multiple levels on the Cell/B.E. processor. We also present a Cell/B.E. optimized implementation of gzip, a popular file-compression application based on the zlib library. For our Cell/B.E. implementation of gzip, we achieve an average speedup of 2.9 in compression over current workstations.}
}
@article{BISTAK202017240,
title = {Remote Control Laboratory for Three-Tank Hydraulic System Using Matlab, Websockets and JavaScript⁎⁎This work has been partially supported by the grants APVV SK-IL-RD-18-0008 and VEGA 1/0745/19.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17240-17245},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1766},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323740},
author = {Pavol Bisták},
keywords = {hydraulic system, remote, virtual laboratory, Matlab/Simulink, WebSocket, 3D visualization, nonlinear control system},
abstract = {This work aims to introduce a new architecture for building virtual and remote laboratories where the building blocks are represented by the Matlab/Simulink computing and simulation software, WebSocket communication technology and a front-end application created in JavaScript programming language. Matlab does not have direct support for WebSockets, but the implementation of the MatlabWebSocket library on the Matlab server has allowed connection through WebSockets that has been accepted with the client side realized in JavaScript. Additionally to the interactivity that is heavily supported by JavaScript, the remote laboratory has been visualized on the client side in 3D by implementation of the Three.js JavaScript library. From the control point of view the new remote laboratory enables to compare nonlinear feedback control with dynamical feedforward control respecting input saturation where in both cases a nonlinear disturbance observer can be used. WebSocket communication technology and the corresponding client interface in the form of a web application create possibilities for the presented remote laboratory to run from the Internet browser and no dedicated application is needed as it was in previous Matlab based laboratories what can be considered as a main contribution.}
}
@article{LASTRADIAZ201797,
title = {HESML: A scalable ontology-based semantic similarity measures library with a set of reproducible experiments and a replication dataset},
journal = {Information Systems},
volume = {66},
pages = {97-118},
year = {2017},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916303246},
author = {Juan J. Lastra-Díaz and Ana García-Serrano and Montserrat Batet and Miriam Fernández and Fernando Chirigati},
keywords = {HESML, PosetHERep, Semantic measures library, Ontology-based semantic similarity measures, Intrinsic and corpus-based Information Content models, Reproducible experiments on word similarity, WNSimRep v1 dataset, ReproZip, WordNet-based semantic similarity measures},
abstract = {This work is a detailed companion reproducibility paper of the methods and experiments proposed by Lastra-Díaz and García-Serrano in (2015, 2016) [56–58], which introduces the following contributions: (1) a new and efficient representation model for taxonomies, called PosetHERep, which is an adaptation of the half-edge data structure commonly used to represent discrete manifolds and planar graphs; (2) a new Java software library called the Half-Edge Semantic Measures Library (HESML) based on PosetHERep, which implements most ontology-based semantic similarity measures and Information Content (IC) models reported in the literature; (3) a set of reproducible experiments on word similarity based on HESML and ReproZip with the aim of exactly reproducing the experimental surveys in the three aforementioned works; (4) a replication framework and dataset, called WNSimRep v1, whose aim is to assist the exact replication of most methods reported in the literature; and finally, (5) a set of scalability and performance benchmarks for semantic measures libraries. PosetHERep and HESML are motivated by several drawbacks in the current semantic measures libraries, especially the performance and scalability, as well as the evaluation of new methods and the replication of most previous methods. The reproducible experiments introduced herein are encouraged by the lack of a set of large, self-contained and easily reproducible experiments with the aim of replicating and confirming previously reported results. Likewise, the WNSimRep v1 dataset is motivated by the discovery of several contradictory results and difficulties in reproducing previously reported methods and experiments. PosetHERep proposes a memory-efficient representation for taxonomies which linearly scales with the size of the taxonomy and provides an efficient implementation of most taxonomy-based algorithms used by the semantic measures and IC models, whilst HESML provides an open framework to aid research into the area by providing a simpler and more efficient software architecture than the current software libraries. Finally, we prove the outperformance of HESML on the state-of-the-art libraries, as well as the possibility of significantly improving their performance and scalability without caching using PosetHERep.}
}
@article{FRASCA2017397,
title = {COSNet: An R package for label prediction in unbalanced biological networks},
journal = {Neurocomputing},
volume = {237},
pages = {397-400},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.096},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216000485},
author = {Marco Frasca and Giorgio Valentini},
keywords = {Biological network, Label imbalance, Node label prediction, R package, Protein function prediction},
abstract = {Several problems in computational biology and medicine are modeled as learning problems in graphs, where nodes represent the biological entities to be studied, e.g. proteins, and connections different kinds of relationships among them, e.g. protein–protein interactions. In this context, classes are usually characterized by a high imbalance, i.e. positive examples for a class are much less than those negative. Although several works studied this problem, no graph-based software designed to explicitly take into account the label imbalance in biological networks is available. We propose COSNet, an R package to serve this purpose. COSNet deals with the label imbalance problem by implementing a novel parametric model of Hopfield Network (HN), whose output levels and activation thresholds of neurons are parameters to be automatically learnt. Due to the quasi-linear time complexity, COSNet nicely scales when the number of instances is large, and application examples to challenging problems in biomedicine show the efficiency and the accuracy of the proposed library.}
}
@article{MONTOYA2011541,
title = {Comparative analysis of power variables in high performance embedded and x86 architectures using GNU/Linux},
journal = {Computers & Electrical Engineering},
volume = {37},
number = {4},
pages = {541-549},
year = {2011},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2011.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0045790611000577},
author = {F.G. Montoya and A. Alcayde and P. Sánchez and C. Gil and M.G. Montoya and J. Gómez},
abstract = {In this work a comparative analysis of typical power variables is made using several hardware architectures and GNU/Linux software. Voltage and current data are simulated for an industrial device, comparing the performance of x86 and ARM in order to demonstrate the technical feasibility of using embedded hardware to manage high volumes of interesting data in the study of power quality systems. Voltage, current, active power, reactive power and harmonic distortion (both voltage and current) were obtained with simulated data provided by MATLAB and using Discrete Fourier Transform (DFT) implementations like Fast Fourier Transform (FFT) and libraries like FFTW and KISS FFT. All the software used in our work was open source, running Linux behind them. Results show the feasibility of using high performance embedded systems to develop advanced tasks in analyzing power signals.}
}
@article{ZAUNER1993879,
title = {A Modular CIM-Concept for Small And Medium Sized Companies},
journal = {IFAC Proceedings Volumes},
volume = {26},
number = {2, Part 5},
pages = {879-882},
year = {1993},
note = {12th Triennal Wold Congress of the International Federation of Automatic control. Volume 5 Associated Technologies and Recent Developments, Sydney, Australia, 18-23 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)48400-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701748400X},
author = {M. Zauner and J. Hölzl and P. Kopacek and G. Kronreif},
keywords = {CIM-concepts, CIM, client/server architecture, systems-engineering, data base systems, hierarchical intelligent control},
abstract = {Modem software architecture concepts offer various possibilities for the forward-looking design of open and modular computer integrated manfacturing systems. Additional, the very flexible organisational structures of small and medium sized companies can be implemented considering restrictions that are caused by commercial and technical influences. Therefore a common, client/server based CIM-concept was developed. This concept faces the implementation of highly integrated CIM-modules als well as the support of existing software packages. Both, the usage of PC's as client-workstations additional to UNIX-data base servers and a layer-based, modular software concept follow the way of an easy enhancement of this CIM solution.}
}
@article{CAO2004389,
title = {The design and implementation of a runtime system for graph-oriented parallel and distributed programming},
journal = {Journal of Systems and Software},
volume = {72},
number = {3},
pages = {389-399},
year = {2004},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(03)00099-2},
url = {https://www.sciencedirect.com/science/article/pii/S0164121203000992},
author = {J. Cao and Y. Liu and Li Xie and B. Mao and K. Zhang},
abstract = {Graph has been widely used in modeling, specification, and design of parallel and distributed systems. Many parallel and distributed programs can be expressed as a collection of parallel functional modules whose relationships can be defined by a graph. Often, the basic functions of communication and coordination of the parallel modules are expressed in terms of the underlying graph. Furthermore, parallel/distributed graph algorithms are used to realize various control functions. To facilitate the implementation of these algorithms, it is desirable to have an integrated approach that provides direct support for efficient operations on graphs. We have proposed a graph-oriented programming model, called GOP, which aims at providing high-level abstractions for configuring and programming cooperative parallel processes. GOP enables the programmer to configure the logical structure of a distributed program by using a logical graph and to write the program using communications and synchronization primitives based on the logical structure. In this paper, we describe the design and implementation of a portable run-time system for the GOP framework. The runtime system provides an interface with a library of programming primitives to the low-level facilities required to support graph-oriented communications and synchronization. The implementation is on top of the parallel virtual machine in a local area network of Sun workstations. We focus our discussion on the following four aspects: the software architecture, including the structure of runtime system and interfaces between user programs and the runtime kernel; graph representation; implementation of graph operations; and performance of the run-time in terms of the implementation of graph-oriented communications.}
}
@article{ACOSTAQUINONEZ2021101897,
title = {HOSVD prototype based on modular SW libraries running on a high-performance CPU+GPU platform},
journal = {Journal of Systems Architecture},
volume = {113},
pages = {101897},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2020.101897},
url = {https://www.sciencedirect.com/science/article/pii/S1383762120301685},
author = {R.I. Acosta-Quiñonez and D. Torres-Roman and R. Rodriguez-Avila},
keywords = {Digital signal processing, Heterogeneous computing platform, GPU, Parallel, },
abstract = {Efficient prototyping is an invaluable resource for modern enterprises and research centers. An efficient prototyping tool exhibits high throughput while maintaining flexibility, and reduces design and validation efforts, resulting in low time-to-market and high competitiveness. This paper presents a modular implementation of high-performance software (SW) libraries running on a Heterogeneous Computing Platform (HCP) based on CPU+GPU. The proposed SW libraries enable a fast and easy comparison of a prototype under different implementation criteria and maintain a high throughput and reusability due to their modular definition. These features accelerate the prototyping task by removing the overhead of designing and validating ad-hoc implementations. The novelty and benefits of this proposal are presented by prototyping and analysis of the multilinear SVD or Higher-Order SVD (HOSVD), an important, widely-used, and computationally demanding tensor decomposition. The mean square error (MSE), processing time, and speedup of this case study show its high performance, while modularity maintains flexibility. The HOSVD prototype reaches a maximum speedup of 17× that of one of the most important implementations in the state of the art.}
}
@article{CASTRO2015751,
title = {Data archiving system implementation in ITER's CODAC Core System},
journal = {Fusion Engineering and Design},
volume = {96-97},
pages = {751-755},
year = {2015},
note = {Proceedings of the 28th Symposium On Fusion Technology (SOFT-28)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2015.06.076},
url = {https://www.sciencedirect.com/science/article/pii/S0920379615301058},
author = {R. Castro and L. Abadie and Y. Makushok and M. Ruiz and D. Sanz and J. Vega and J. Faig and G. Román-Pérez and S. Simrock and P. Makijarvi},
keywords = {Data archiving, HDF5, Data acquisition, Fast control},
abstract = {The aim of this work is to present the implementation of data archiving in ITER's CODAC Core System software. This first approach provides a client side API and server side software allowing the creation of a simplified version of ITERDB data archiving software, and implements all required elements to complete data archiving flow from data acquisition until its persistent storage technology. The client side includes all necessary components that run on devices that acquire or produce data, distributing and streaming to configure remote archiving servers. The server side comprises an archiving service that stores into HDF5 files all received data. The archiving solution aims at storing data coming for the data acquisition system, the conventional control and also processed/simulated data.}
}
@article{WALTER199673,
title = {Plant engineering: Modeling and design of topological coupling aspects in a computer aided environment},
journal = {Computers in Industry},
volume = {28},
number = {2},
pages = {73-80},
year = {1996},
issn = {0166-3615},
doi = {https://doi.org/10.1016/0166-3615(95)00060-7},
url = {https://www.sciencedirect.com/science/article/pii/0166361595000607},
author = {Cláudio Walter and JoséPalazzo M. {de Oliveira}},
keywords = {Product modeling, Plant engineering},
abstract = {When designing a plant for continuous or semi-continuous processes, such as paper pulp or petrochemical production systems, engineers face very complex tasks, which are only in part supported by software tools. Engineering is an expensive task in itself, but the consequences of bad engineering are even worse, resulting, for instance, in the loss of a couple of production weeks. Information systems supporting the engineering activities play a critical role in minimizing errors and delays, thus maximizing the quality and the economic return of the plant. Computer-supported tasks require the concurrent participation of multiple designers. The design task complexity is compounded by its interdisciplinary character: the engineering system has to assure design consistency between teams with different specialties that must work as freely as possible. This paper analyses the problem, proposes the rcad software architecture and describes partial implementation results. The architecture, which is based on a simple yet powerful conceptual model, combines low-cost commercially available packages with relatively little custom-written software. Its features are nowadays either supported by very expensive or very specialized integrated packages, or separately performed by largely disintegrated database managers, drafting systems and other software.}
}
@article{PTICINA2015240,
title = {Software Solution for Planning and Conducting a Transport Survey},
journal = {Procedia Computer Science},
volume = {77},
pages = {240-248},
year = {2015},
note = {ICTE in regional Development 2015 Valmiera, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.381},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915038910},
author = {Irina Pticina and Eftihia Nathanail and Sergey Kibish},
keywords = {Transport survey, Traffic count, Software solution, Mobile application},
abstract = {In cases when there is no ability to use different intrusive methods for the counting of traffic volume usually a non-intrusive method is used – manual observation using tally sheets. This method of data collection has the following drawbacks: expenses for the decoding of handwritten symbols from the tally sheets; expenses for data input into the database; observers can be irresponsible, etc. In the article a software solution which helps to solve some of the mentioned drawbacks is presented. The requirements for the software solution and software's architecture are presented. The solution consists of three main components: 1) web application – space for project manager for a project configuration, managing observation areas, observers and data exporting; 2) mobile application – application from which an observer can send data about traffic flow volume the system; 3) API – application program interface for communication between the mobile application and the server. The software solution has been implemented and tested for a real problem solution. The paper presents the results of testing, the advantages/disadvantages of the offered solution and directions of future development.}
}
@article{SUDARSAN201048,
title = {Design and performance of a scheduling framework for resizable parallel applications},
journal = {Parallel Computing},
volume = {36},
number = {1},
pages = {48-64},
year = {2010},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2009.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S016781910900132X},
author = {Rajesh Sudarsan and Calvin J. Ribbens},
keywords = {Scheduling malleable applications, Run-time systems, Malleability in distributed-memory systems, Dynamic scheduling, High performance computing, Dynamic resizing, Data redistribution, Application resizing framework},
abstract = {This paper describes the design and initial implementation of a software framework for exploiting resizability in distributed-memory parallel applications. By “resizable” we mean the ability at run-time to expand or contract the number of processes participating in a parallel application. The ReSHAPE framework described here includes a cluster scheduler, a library supporting data redistribution and process remapping, and an application programming interface (API) which allows applications to interact with the scheduler and resizing library with only minor code modifications. Parallel applications executed using the ReSHAPE framework can expand to take advantage of additional free processors or contract to accommodate a high priority application without being suspended. Experimental results show that the ReSHAPE framework can significantly improve individual job turn-around time and overall system throughput, even with very simple application scheduling policies. In addition, the framework serves as a convenient platform for research into much more sophisticated cluster scheduling policies and methods.}
}
@incollection{JALVING20191063,
title = {Recent Advances in Graph-Based Abstractions for Modeling and Simulating Complex Systems},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {1063-1068},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50178-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343501788},
author = {Jordan Jalving and Victor Zavala},
keywords = {cyber-physical, structured modeling, complex systems, decomposition},
abstract = {Current graph-based approaches for modeling, simulation and optimization of complex cyber-physical systems have motivated the development of new graph-based abstractions. We propose an algebraic graph to represent physical connectivity in complex optimization models and a computing graph to capture computational aspects of cyber/control architectures. The algebraic graph facilitates the analysis and decomposition of optimization problems and the computing graph enables the simulation of optimization and control algorithms in virtual distributed environments. The proposed abstractions are implemented in a Julia software package called Plasmo.jl.}
}
@article{GOIRI2021110310,
title = {MultiShifter: Software to generate structural models of extended two-dimensional defects in 3D and 2D crystals},
journal = {Computational Materials Science},
volume = {191},
pages = {110310},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2021.110310},
url = {https://www.sciencedirect.com/science/article/pii/S0927025621000355},
author = {Jon Gabriel Goiri and Anton {Van der Ven}},
keywords = {Software, Gamma surfaces, Twisted bilayers, Ab initio, Multishifter, First principles, Cohesive zone, Open source, Stacking fault, Twistronics, Casm},
abstract = {Extended defects in crystals, such as dislocations, stacking faults and grain boundaries, play a crucial role in determining a wide variety of materials properties. Extended defects can also lead to novel electronic properties in two-dimensional materials, as demonstrated by recent discoveries of emergent electronic phenomena in twisted graphene bilayers. This paper describes several approaches to construct crystallographic models of two-dimensional extended defects in crystals for first-principles electronic structure calculations, including (i) crystallographic models to parameterize generalized cohesive zone models for fracture studies and meso-scale models of dislocations and (ii) crystallographic models of twisted bilayers. The approaches are implemented in an open source software package called MultiShifter.}
}
@article{BRODSKY2007371,
title = {SIMULATION AND CONTROL OF FLEXIBLE VEHICLES},
journal = {IFAC Proceedings Volumes},
volume = {40},
number = {7},
pages = {371-376},
year = {2007},
note = {17th IFAC Symposium on Automatic Control in Aerospace},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20070625-5-FR-2916.00064},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015332705},
author = {S.A. Brodsky and A.V. Nebylov and A.I. Panferov},
keywords = {simulation, control, flexible, vehicle dynamics, flight control, system synthesis},
abstract = {Possible approaches to the mathematical description of different types of flexible vehicles in view of oscillations of fluid in tanks and moving masses inside the vehicle are observed. Elastic bending of a body surface in interaction with a surrounding medium in a broad band of speed variation are taken into account. Problems of regulator's synthesis, damping of elastic oscillations, and also principles of construction of universal software for research of dynamic properties and simulations of elastic vehicles motion are considered. For implementation of suggested methods and algorithms the specialized program is designed. The software package is supplied with the program modules library. These modules are designed on the basis of mathematical models of the vehicle elements and control system, and also the significant physical phenomena such as flexibility, liquid oscillations, time lag of engines, local aerodynamic effects, etc. Functioning of the program is demonstrated and outcomes of calculations are presented.}
}
@article{PULKA2006389,
title = {Automated selection of embedded functions for SoC models based on IPs library},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {21},
pages = {389-394},
year = {2006},
note = {IFAC Workshop on Programmable Devices and Embedded Systems PDeS 2006, Brno, Czech Republic, February 14-16, 2006},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)30217-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017302173},
author = {Andrzej Pulka},
keywords = {Emhedded systems, Design VLSI, Models, Systems design, Hardware, Architccturcs, Dccomposition, Componcnts, Optimization},
abstract = {The presented Paper deals with the design and modeling of modern Systems on Chip (SoC) In this contribution the author proposes an approach to thc problcm of the partitioning and selection the functional tasks between hardware and software components. The issue is presented on an example of the selected system platform and functions implcmented for the digital signal processing. The searching algorithm - SMOG - implemented in Prolog (LPA Prolog) is formulated and descrihed. The structure nf the lPs library is discussed. Some results and conclusions summarized the entire work.}
}
@article{BELAOUCHA20102075,
title = {FADAlib: an open source C++ library for fuzzy array dataflow analysis},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {2075-2084},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.232},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910002334},
author = {Marouane Belaoucha and Denis Barthou and Adrien Eliche and Sid-Ahmed-Ali Touati},
abstract = {Ubiquitous multicore architectures require that many levels of parallelism have to be found in codes. Dependence analysis is the main approach in compilers for the detection of parallelism. It enables vectorisation and automatic parallelisation, among many other optimising transformations, and is therefore of crucial importance for optimising compilers. This paper presents new open source software, FADAlib, performing an instance-wise dataflow analysis for scalar and array references. The software is a C++ implementation of the Fuzzy Array Dataflow Analysis (FADA) method. This method can be applied on codes with irregular control such as while-loops, if-then-else or non-regular array accesses, and computes exact instance-wise dataflow analysis on regular codes. As far as we know, FADAlib is the first released open source C++ implementation of instance-wise data flow dependence handling larger classes of programs. In addition, the library is technically independent from an existing compiler; It can be plugged in many of them; this article shows an example of a successful integration inside gcc/GRAPHITE. We give details concerning the library implementation and then report some initial results with gcc and possible use for trace scheduling on irregular codes.}
}
@article{CHEN2004401,
title = {A reactive system architecture for building fault-tolerant distributed applications},
journal = {Journal of Systems and Software},
volume = {72},
number = {3},
pages = {401-415},
year = {2004},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(03)00098-0},
url = {https://www.sciencedirect.com/science/article/pii/S0164121203000980},
author = {Changgui Chen and Weijia Jia and Wanlei Zhou},
keywords = {Fault-tolerant computing, Reactive systems, Group communication, Distributed applications, Software development},
abstract = {Most fault-tolerant application programs cannot cope with constant changes in their environments and user requirements because they embed policies and mechanisms together so that if the policies or mechanisms are changed the whole programs have to be changed as well. This paper presents a reactive system approach to overcoming this limitation. The reactive system concepts are an attractive paradigm for system design, development and maintenance because they separate policies from mechanisms. In the paper we propose a generic reactive system architecture and use group communication primitives to model it. We then implement it as a generic package which can be applied in any distributed applications. The system performance shows that it can be used in a distributed environment effectively.}
}
@article{BILANCIA2021103001,
title = {An Overview of Procedures and Tools for Designing Nonstandard Beam-Based Compliant Mechanisms},
journal = {Computer-Aided Design},
volume = {134},
pages = {103001},
year = {2021},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.103001},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521000129},
author = {Pietro Bilancia and Giovanni Berselli},
keywords = {Compliant Mechanisms, Virtual Prototyping, Cross-axis flexural pivot, Design methods, CAD/CAE software framework, Shape optimization},
abstract = {Beam-based Compliant Mechanisms (CMs) are increasingly studied and implemented in precision engineering. Straight beams with uniform cross section are the basic modules in several design concepts, which can be deemed as standard CMs. Their behavioral analysis can be addressed with a large variety of techniques, including the Euler–Bernoulli beam theory, the Pseudo-Rigid Body (PRB) method, the beam constraint model and the discretization-based methods. This variety is unquestionably reduced when considering nonstandard CMs, namely design problems involving special geometries, such as curve/spline beams, variable section beams, nontrivial shapes and contact pairs. The 3D Finite Element Analysis (FEA) provides accurate results but its high computational cost makes it inappropriate for optimization purposes. This work compares the potentialities of computationally efficient modeling techniques (1D FEA, PRB method and chained-beam constraint model), focusing on their applicability in nonstandard planar problems. The cross-axis flexural pivot is used as a benchmark in this research due to its high configurable behavior and wide range of applications. In parallel, as an attempt to provide an easy-to-use environment for CM analysis and design, a multi-purpose tool comprising Matlab and a set of modern Computer-Aided Design/Engineering packages is presented. The framework can implement different solvers depending on the adopted behavioral models. Summary tables are reported to guide the designers in the selection of the most appropriate technique and software framework. Lastly, efficient design procedures that allow to configure nonstandard beam-based CMs with prescribed behavior are examined with two design examples.}
}
@article{ZAPLETAL2017157,
title = {Boundary element quadrature schemes for multi- and many-core architectures},
journal = {Computers & Mathematics with Applications},
volume = {74},
number = {1},
pages = {157-173},
year = {2017},
note = {5th European Seminar on Computing ESCO 2016},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2017.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0898122117300482},
author = {Jan Zapletal and Michal Merta and Lukáš Malý},
keywords = {Boundary element method, Quadrature, SIMD, Vectorization, Intel Xeon Phi, Many-core architecture},
abstract = {In the paper we study the performance of the regularized boundary element quadrature routines implemented in the BEM4I library developed by the authors. Apart from the results obtained on the classical multi-core architecture represented by the Intel Xeon processors we concentrate on the portability of the code to the many-core family Intel Xeon Phi. Contrary to the GP-GPU programming accelerating many scientific codes, the standard x86 architecture of the Xeon Phi processors allows to reuse the already existing multi-core implementation. Although in many cases a simple recompilation would lead to an inefficient utilization of the Xeon Phi, the effort invested in the optimization usually leads to a better performance on the multi-core Xeon processors as well. This makes the Xeon Phi an interesting platform for scientists developing a software library aimed at both modern portable PCs and high performance computing environments. Here we focus at the manually vectorized assembly of the local element contributions and the parallel assembly of the global matrices on shared memory systems. Due to the quadratic complexity of the standard assembly we also present an assembly sparsified by the adaptive cross approximation based on the same acceleration techniques. The numerical results performed on the Xeon multi-core processor and two generations of the Xeon Phi many-core platform validate the proposed implementation and highlight the importance of vectorization necessary to exploit the features of modern hardware.}
}
@article{GEVELER2011113,
title = {A simulation suite for Lattice-Boltzmann based real-time CFD applications exploiting multi-level parallelism on modern multi- and many-core architectures},
journal = {Journal of Computational Science},
volume = {2},
number = {2},
pages = {113-123},
year = {2011},
note = {Simulation Software for Supercomputers},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2011.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877750311000159},
author = {Markus Geveler and Dirk Ribbrock and Sven Mallach and Dominik Göddeke},
keywords = {HPC software development multi- and many-core architectures, Real-time simulationm, Lattice-Boltzmann methods fluid-structure interaction},
abstract = {We present a software approach to hardware-oriented numerics which builds upon an augmented, previously published set of open-source libraries facilitating portable code development and optimisation on a wide range of modern computer architectures. In order to maximise efficiency, we exploit all levels of parallelism, including vectorisation within CPU cores, the Cell BE and GPUs, shared memory thread-level parallelism between cores, and parallelism between heterogeneous distributed memory resources in clusters. To evaluate and validate our approach, we implement a collection of modular building blocks for the easy and fast assembly and development of CFD applications based on the shallow water equations: We combine the Lattice-Boltzmann method with fluid-structure interaction techniques in order to achieve real-time simulations targeting interactive virtual environments. Our results demonstrate that recent multi-core CPUs outperform the Cell BE, while GPUs are significantly faster than conventional multi-threaded SSE code. In addition, we verify good scalability properties of our application on small clusters.}
}
@article{SABETAMAL2022105044,
title = {Coupled hydro-mechanical modelling of unsaturated soils; numerical implementation and application to large deformation problems},
journal = {Computers and Geotechnics},
volume = {152},
pages = {105044},
year = {2022},
issn = {0266-352X},
doi = {https://doi.org/10.1016/j.compgeo.2022.105044},
url = {https://www.sciencedirect.com/science/article/pii/S0266352X22003810},
author = {Hassan Sabetamal and Daichao Sheng and John P. Carter},
keywords = {Coupled hydro-mechanical analysis, Unsaturated constitutive modelling, Large deformations, Soil-structure interaction, Mesh optimisations},
abstract = {This paper presents coupled hydro-mechanical modelling of unsaturated soil problems by incorporating some advanced numerical and constitutive models in a general-purpose commercial software package, Abaqus. Two different strategies for the interpretation of unsaturated soil behaviour with respect to the soil volume change are considered and the relevant constitutive models are implemented through user-defined subroutines. The first approach that is commonly used in geomechanics considers suction as an additional variable and uses a constitutive model in the space of effective stress–suction assuming that soil compressibility is a function of suction. The second approach treats the constitutive model in the space of effective stress and degree of saturation assuming that the soil compressibility is a function of the degree of saturation. The hydro-mechanical behaviour regarding the change in the degree of saturation caused by both suction and net stress changes is also considered, together with the effect of hydraulic hysteresis. Validation of the implemented algorithms is presented through several benchmark problems. Finally, the application and utility of the implemented procedures are illustrated by simulations of two challenging problems of unsaturated geomechanics, including a slope failure due to seepage and rainfall infiltration as well as cone penetration tests in unsaturated soil. A suitable mesh optimisation scheme is also incorporated to handle finite deformations.}
}
@article{ARTRITH2016135,
title = {An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for TiO2},
journal = {Computational Materials Science},
volume = {114},
pages = {135-150},
year = {2016},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2015.11.047},
url = {https://www.sciencedirect.com/science/article/pii/S0927025615007806},
author = {Nongnuch Artrith and Alexander Urban},
keywords = {Machine learning, Artificial neural networks, Atomistic simulations, Titanium dioxide (TiO), Behler–Parrinello},
abstract = {Machine learning interpolation of atomic potential energy surfaces enables the nearly automatic construction of highly accurate atomic interaction potentials. Here we discuss the Behler–Parrinello approach that is based on artificial neural networks (ANNs) and detail the implementation of the method in the free and open-source atomic energy network (ænet) package. The construction and application of ANN potentials using ænet is demonstrated at the example of titanium dioxide (TiO2), an industrially relevant and well-studied material. We show that the accuracy of lattice parameters, energies, and bulk moduli predicted by the resulting TiO2 ANN potential is excellent for the reference phases that were used in its construction (rutile, anatase, and brookite) and examine the potential’s capabilities for the prediction of the high-pressure phases columbite (α-PbO2 structure) and baddeleyite (ZrO2 structure).}
}
@article{WEINHARDT2015543,
title = {SAccO: An implementation platform for scalable FPGA accelerators},
journal = {Microprocessors and Microsystems},
volume = {39},
number = {7},
pages = {543-552},
year = {2015},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2015.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0141933115000290},
author = {Markus Weinhardt and Bernhard Lang and Frank M. Thiesing and Alexander Krieger and Thomas Kinder},
keywords = {Reconfigurable computing, Hardware acceleration, FPGA, Coprocessor, Implementation platform, Hardware/software codesign},
abstract = {This paper presents SAccO (Scalable Accelerator platform Osnabrück), a novel framework for implementing data-intensive applications using scalable and portable reconfigurable hardware accelerators. Instead of using expensive “reconfigurable supercomputers”, SAccO is based on standard PCs and PCI-Express extension cards featuring Field-Programmable Gate Arrays (FPGAs) and memory. In our framework, we exploit task-level parallelism by manually partitioning applications into several parallel processes using the SAccO communication API for data streams. This also allows pure software implementations on PCs without FPGA cards. If an FPGA accelerator is present, the same API calls transfer data between the PC’s CPU and the FPGA. Then, the processes implemented in hardware can exploit instruction-level and pipelining parallelism as well. Furthermore, SAccO components follow a set of hardware implementation rules which enable portable and scalable designs. Device specific hardware wrappers hide the FPGA’s and board’s idiosyncrasies from the application developer. SAccO also comprises a new method to automatically select a task’s optimal degree of parallelism on an FPGA for a given hardware platform, i.e. to generate a hardware design which uses the available communication bandwidth between the PC and the FPGA optimally. Experimental results show the feasibility of our approach.}
}
@article{LAKSHMI20111032,
title = {VLSI architecture for low latency radix-4 CORDIC},
journal = {Computers & Electrical Engineering},
volume = {37},
number = {6},
pages = {1032-1042},
year = {2011},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2011.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0045790611001133},
author = {B. Lakshmi and A.S. Dhar},
abstract = {The CORDIC algorithm, originally proposed using nonredundant radix-2 arithmetic, has been refined in terms of throughput and latency with the introduction of redundant arithmetic and higher radix techniques. In this paper, we propose a pipelined architecture using signed digit arithmetic for the VLSI efficient implementation of rotational radix-4 CORDIC algorithm, eliminating z path completely. A detailed comparison of the proposed architecture with the available radix-2 architectures shows the latency and hardware improvement. The proposed architecture achieves latency improvement over the previously proposed radix-4 architecture with a relatively small hardware overhead. The proposed architecture for 16-bit precision was implemented using VHDL and extensive simulations have been performed to validate the results. The functionally simulated net list has been synthesized for 16-bit precision with 90nm CMOS technology library and the area-time measures are provided. This architecture was also implemented using Xilinx ISE9.1 software and a Virtex device.}
}
@article{ALDINI2011282,
title = {Component-oriented verification of noninterference},
journal = {Journal of Systems Architecture},
volume = {57},
number = {3},
pages = {282-293},
year = {2011},
note = {Special Issue on Security and Dependability Assurance of Software Architectures},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2010.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1383762110000639},
author = {Alessandro Aldini and Marco Bernardo},
keywords = {Component-based software systems, Noninterference analysis, Architectural description languages, Process algebra, Equivalence checking},
abstract = {Component-based software engineering often relies on libraries of trusted components that are combined to build dependable and secure software systems. Resource dependences, constraint conflicts, and information flow interferences arising from component combination that may violate security requirements can be revealed by means of the noninterference approach to information flow analysis. However, the security of large component-based systems may be hard to assess in an efficient and systematic way. In this paper, we propose a component-oriented formulation of noninterference that enables compositional security verification driven by system topology. This is realized by implementing scalable noninterference checks in the formal framework of a process algebraic architectural description language equipped with equivalence checking techniques.}
}
@article{HOSSEINI2022114324,
title = {3D strain gradient elasticity: Variational formulations, isogeometric analysis and model peculiarities},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {389},
pages = {114324},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.114324},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521006149},
author = {S.B. Hosseini and J. Niiranen},
keywords = {Strain gradient elasticity, Coercivity, Continuity, Isogeometric analysis, Size effect, Homogenization},
abstract = {This article investigates the theoretical and numerical analysis as well as applications of the three-dimensional theory of first strain gradient elasticity. The corresponding continuous and discrete variational formulations are established with error estimates stemming from continuity and coercivity within a Sobolev space framework. An implementation of the corresponding isogeometric Ritz–Galerkin method is provided within the open-source software package GeoPDEs. A thorough numerical convergence analysis is accomplished for confirming the theoretical error estimates and for verifying the software implementation. Lastly, a set of model comparisons is presented for revealing and demonstrating some essential model peculiarities: (1) the 1D Timoshenko beam model is essentially closer to the 3D model than the corresponding Euler–Bernoulli beam model; (2) the 3D model and the 1D beam models agree on the strong size effect typical for microstructural and microarchitectural beam structures; (3) stress singularities of reentrant corners disappear in strain gradient elasticity. The computational homogenization methodologies applied in the examples for microarchitectural beams are shown to possess disadvantages that future research should focus on.}
}
@article{OTT1998334,
title = {An architecture for adaptive QoS and its application to multimedia systems design},
journal = {Computer Communications},
volume = {21},
number = {4},
pages = {334-349},
year = {1998},
note = {Quality of Services in Distributed Systems},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(97)00167-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140366497001679},
author = {M. Ott and G. Michelitsch and D. Reininger and G. Welling},
keywords = {multimedia distributed systems, quality of service, variable bit-rate video, graphical user interface},
abstract = {We describe a prototype implementation of a distributed multimedia system that generalizes the concept of QoS to all layers of its software architecture. Each layer deals with QoS at its appropriate level of abstraction using a generic API for communicating QoS parameters and values to layers above and below. The aggregation of these parameters and values is called a service contract. This architecture provides a hierarchical framework to design adaptive multimedia systems. Furthermore, the API allows for reporting of contract violations as well as dynamic renegotiation of the contract terms. A proof-of-concept multimedia system was built to evaluate the proposed architecture. Key components of this system are: a graphical user interface that dynamically requests the quality expected by the user to lower level components, a dynamic network service that efficiently matches network resources to user requirements and a processor scheduler which schedules tasks according to their execution requirements. Our experience with this system showed that the proposed architecture is an efficient framework for building adaptive multimedia systems.}
}
@article{FIROUZI2017255,
title = {Interpreting and implementing IEC 61850-90-5 Routed-Sampled Value and Routed-GOOSE protocols for IEEE C37.118.2 compliant wide-area synchrophasor data transfer},
journal = {Electric Power Systems Research},
volume = {144},
pages = {255-267},
year = {2017},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378779616305193},
author = {Seyed Reza Firouzi and Luigi Vanfretti and Albert Ruiz-Alvarez and Hossein Hooshyar and Farhan Mahmood},
keywords = {IEC 61850-90-5, IEEE C37.118.2, PDC, PMU, Routed-GOOSE, Routed-Sampled Value, WAMPAC},
abstract = {Flexibility and adaptability requirements of future electric power grids for integrating distributed energy resources (DERs) call for the development of wide-area monitoring, protection and control (WAMPAC) applications, utilizing synchrophasor measurements provided by the phasor measurement units (PMUs). IEEE C37.118 is the most utilized protocol for real-time exchange of synchronized phasor measurement data. In order to fulfill some gaps not addressed in IEEE C37.118, and also to harmonize with the IEC 61850 power utility automation standard, the IEC 61850-90-5 technical report has been developed. IEC TR 61850-90-5 introduces a mechanism for transfer of digital states and time synchronized phasor measurement data over wide-area networks between PMUs, phasor data concentrators (PDCs) and WAMPAC applications in the context of IEC 61850. This work interprets the IEEE C37.118.2 and IEC 61850-90-5 Routed-Sampled Value and Routed-GOOSE protocols and describes the design and implementation of a library named Khorjin with the functionality of (1) an IEEE C37.118.2 to IEC 61850-90-5 gateway and protocol converter and (2) an IEC 61850-90-5 subscriber and traffic parser. The main contribution of this work is the development of Khorjin library using only standard C libraries (i.e. independent from any operating system). This is allowing the use of the library in different platforms. The design requirements and functionality of the Khorjin library has been tested in the KTH SmarTS Lab real-time hardware-in-the-loop (HIL) simulation environment to assess its conformance to the functional requirements of IEEE C37.118.2 and IEC 61850-90-5 standards.}
}
@article{AN19941,
title = {Associative memory neural networks: Adaptive modelling theory, software implementations and graphical user interface},
journal = {Engineering Applications of Artificial Intelligence},
volume = {7},
number = {1},
pages = {1-21},
year = {1994},
issn = {0952-1976},
doi = {https://doi.org/10.1016/0952-1976(94)90038-8},
url = {https://www.sciencedirect.com/science/article/pii/0952197694900388},
author = {P.E. An and M. Brown and C.J. Harris and A.J. Lawrence and C.G. Moore},
keywords = {Associative memory, neural networks, fuzzy logic, CMAC, B-spline, non-linear modelling, parallel implementation, graphical user interface},
abstract = {This paper describes in a unified mathematical framework a class of associative memory neural networks (AMN), that have very fast learning rates, local generalisation, parallel implementation, and guaranteed convergence to the mean squared error, making them appropriate for applications such as intelligent control and on-line modelling of nonlinear dynamical processes. The class of AMN considered include the Albus CMAC, B-spline neural network and classes of fuzzy logic networks. Appropriate instantaneous learning rules are derived and applied to a bench mark nonlinear time series prediction problem. For practical implementation, a network software library and graphical user interface (GUI) is introduced for these networks. The data structure is modular, allowing a natural implementation on a parallel machine. The GUI provides a front end, for high-level procedures, allowing the networks to be designed, trained and analysed within a common environment with a minimum of user effort. The software library is readily integrable into industrial packages such as MATLAB.}
}
@article{BUTLER2021104680,
title = {PDAL: An open source library for the processing and analysis of point clouds},
journal = {Computers & Geosciences},
volume = {148},
pages = {104680},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104680},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420306518},
author = {Howard Butler and Bradley Chambers and Preston Hartzell and Craig Glennie},
keywords = {Point clouds, Lidar, Open source software, Geospatial, Iterative closest point},
abstract = {As large point cloud datasets become ubiquitous in the Earth science community, open source libraries and software dedicated to manipulating these data are valuable tools for geospatial scientists and practitioners. We highlight an open source library called the Point Data Abstraction Library, more commonly referred to by its acronym: PDAL. PDAL provides a standalone application for point cloud processing, a C++ library for development of new point cloud applications, and support for Python, MATLAB, Julia, and Java languages. Central to PDAL are the concepts of stages, which implement core capabilities for reading, writing, and filtering point cloud data, and pipelines, which are end-to-end workflows composed of sequential stages for transforming point clouds. We review the motivation for PDAL’s genesis, describe its general structure and functionality, detail several options for conveniently accessing PDAL’s functionality, and provide an example that uses PDAL’s Python extension to estimate earthquake surface deformation from pre- and post-event airborne laser scanning point cloud data using an iterative closest point algorithm.}
}
@article{FUJITA20191,
title = {Efficient implementation of MPI-3 RMA over openFabrics interfaces},
journal = {Parallel Computing},
volume = {87},
pages = {1-10},
year = {2019},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167819118303843},
author = {Hajime Fujita and Chongxiao Cao and Sayantan Sur and Charles Archer and Erik Paulson and Maria Garzaran},
keywords = {Message Passing Interface, MPI, Remote Memory Access (RMA), One-sided Communications, MPICH-OFI, OpenFabrics Intefaces (OFI)},
abstract = {The Message Passing Interface (MPI) standard supports Remote Memory Access (RMA) operations, where a process can read or write memory of another process without requiring the target process to be involved in the communication. This enables new more efficient programming models. This paper describes the RMA design and implementation in MPICH-OFI, an MPICH-based open source implementation of the MPI standard that uses the OpenFabrics Interfaces* (OFI*) to communicate with the underlying network fabric. MPICH-OFI is based on a new communication layer called CH4, which was designed to achieve high performance by minimizing the runtime software overhead and by having an internal API that is well aligned with MPI functions. MPICH-OFI uses the OpenFabrics Interfaces (OFI), a lightweight communication framework to support modern high-speed interconnects. Thanks to CH4 and OFI, MPICH-OFI achieves low latency and high bandwidth for RMA operations. Our experimental results using microbenchmarks show that MPICH-OFI achieves more than 3x better put/get latency and bandwidth than MPICH CH3, 10% better latency than Open MPI and MVAPICH2, and more than 1.7x bandwidth than MVAPICH2 for small messages ( ≤  4KB), on Intel® Omni-Path Architecture.}
}
@article{WANG2022103577,
title = {A monolithic one-velocity-field optimal control formulation for fluid–structure interaction problems with large solid deformation},
journal = {Journal of Fluids and Structures},
volume = {111},
pages = {103577},
year = {2022},
issn = {0889-9746},
doi = {https://doi.org/10.1016/j.jfluidstructs.2022.103577},
url = {https://www.sciencedirect.com/science/article/pii/S0889974622000470},
author = {Yongxing Wang},
keywords = {Fluid-structure interaction, Optimal control, Piecewise control, Monolithic method, One-velocity method},
abstract = {In this article, we formulate a monolithic optimal control method for general time-dependent Fluid–Structure Interaction (FSI) systems with large solid deformation: we consider a displacement-tracking type of objective with a constraint of the solid velocity, and tackle the time-dependent control problems by a piecewise-in-time control method; we cope with the large solid displacement using a one-velocity fictitious domain method, and solve the fully-coupled FSI and the corresponding adjoint equations in a monolithic manner. The proposed method is implemented in open-source software package FreeFEM++ and assessed by three numerical experiments, in the aspects of stability of the numerical scheme for different regularisation parameters, and efficiency of reducing the objective function with control of the solid velocity.}
}
@article{BONFE20131608,
title = {Design patterns for model-based automation software design and implementation},
journal = {Control Engineering Practice},
volume = {21},
number = {11},
pages = {1608-1619},
year = {2013},
note = {Advanced Software Engineering in Industrial Automation (INCOM’09)},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2012.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0967066112000810},
author = {Marcello Bonfè and Cesare Fantuzzi and Cristian Secchi},
keywords = {Industry automation, Programmable logic controllers, Object modeling techniques, Object-oriented programming},
abstract = {The paper presents the application of object-oriented modeling techniques to control software development for complex manufacturing systems, with particular focus on case studies taken from the packaging industry and design patterns that can be abstracted from such case studies. The proposed methodology for control software modeling and implementation is based on a practical approach refined on the basis of on-the-field experience and interactions with control engineers involved in the development projects. The final objective of the paper is to review and analyze patterns for the solution of design and implementation issues that typically arise in the considered application domain.}
}
@article{ALI20102322,
title = {Development of Java based RFID application programmable interface for heterogeneous RFID system},
journal = {Journal of Systems and Software},
volume = {83},
number = {11},
pages = {2322-2331},
year = {2010},
note = {Interplay between Usability Evaluation and Software Development},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2010.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0164121210001998},
author = {Mohammed F.M. Ali and Mohammed I. Younis and Kamal Z. Zamli and Widad Ismail},
keywords = {RFID, Interoperability, API, Sequence diagram, Tracking, Monitoring, Multithreading},
abstract = {Developing RFID based applications is a painstakingly difficult endeavor. The difficulties include non-standard software and hardware peripherals from vendors, interoperability problems between different operating systems as well as lack of expertise in terms of low-level programming for RFID (i.e. steep learning curve). In order to address these difficulties, a reusable RFIDTM API (RFID Tracking & Monitoring Application Programmable Interface) for heterogeneous RFID system has been designed and implemented. The API has been successfully employed in a number of application prototypes including tracking of inventories as well as human/object tracking and tagging. Here, the module has been tested on a number of different types and configuration of active and passive readers including that LF and UHF Readers.}
}
@article{LEE2022181,
title = {gShare: A centralized GPU memory management framework to enable GPU memory sharing for containers},
journal = {Future Generation Computer Systems},
volume = {130},
pages = {181-192},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004970},
author = {Munkyu Lee and Hyunho Ahn and Cheol-Ho Hong and Dimitrios S. Nikolopoulos},
keywords = {Containers, GPU memory, Virtualization},
abstract = {Owing to low overhead and rapid deployment, containers are increasingly becoming an attractive system software platform for deep learning and high performance computing (HPC) applications that leverage GPUs. Unfortunately, existing container software does not concern how each container allocates GPU memory. Therefore, if a certain container consumes the majority of GPU memory, other containers may not run their workloads because of insufficient memory. This paper presents gShare, a centralized GPU memory management framework to enable GPU memory sharing for containers. As with a modern operating system, gShare allocates the entire GPU memory inside the framework and manages the memory with sophisticated memory allocators. gShare is then able to enforce the GPU memory limit of each container by mediating the memory allocation calls. To achieve its objective, gShare introduces the API remoting components, the mediator, and the three-level memory allocator, which enable lightweight and efficient GPU memory management. Our prototype implementation achieves near-native performance with secure isolation and little memory waste in popular deep learning and HPC workloads.}
}
@article{BENNETT2021104999,
title = {A modelling framework and R-package for evaluating system performance under hydroclimate variability and change},
journal = {Environmental Modelling & Software},
volume = {139},
pages = {104999},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.104999},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221000426},
author = {Bree Bennett and Anjana Devanand and Sam Culley and Seth Westra and Danlu Guo and Holger R. Maier},
keywords = {Climate change impact assessment, Scenario-neutral, Bottom-up, Stochastic rainfall, Inverse approach, R-Package},
abstract = {Effective implementation of a scenario-neutral climate impact assessment relies on the integration of many modelling components at multiple stages: from the generation of appropriate climate boundary conditions that can be used to rigorously ‘stress-test’ a system, to the simulation, visualisation and interpretation of resulting system performance. For systems with complex temporal dynamics, the generation of climate forcing time series combined with the significant simulation and visualisation demands represent a barrier to the wide-scale adoption of scenario-neutral approaches. A unified five-step framework and supporting R-package, foreSIGHT (Systems Insights from the Generation of Hydroclimate Time series), are introduced that enable the application of a scenario-neutral climate impact assessment from appropriate time series generation to the analysis of system performance. The software can be applied to compare both current system performance and selected alternative system management or design options and its application is demonstrated for a case study.}
}
@article{KOK2021100103,
title = {Implementation of and experimental software for active selection of classification features},
journal = {Software Impacts},
volume = {9},
pages = {100103},
year = {2021},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2021.100103},
url = {https://www.sciencedirect.com/science/article/pii/S2665963821000397},
author = {Thomas T. Kok and Georg Krempl and Hugo G. Schnack},
keywords = {Active learning, Active feature acquisition, Active selection of classification features, Machine learning experiment evaluation framework},
abstract = {In some machine learning applications, obtaining data on the most predictive features is costly, but other features are readily available. Recently, first active learning approaches for this Actively Selecting Classification Features problem (ASCF) have been proposed. In this paper, we introduce a Python package that provides a framework for ASCF, including implementations of a supervised and an unsupervised selection approach, as well as a framework for performing experimental evaluations. This framework has been used in recent publications in the context of neuroimaging research on mental disorders, where its usefulness has been demonstrated in a simulated study design with MRI data.}
}
@article{GOSWAMI2002669,
title = {From Design Patterns to Parallel Architectural Skeletons},
journal = {Journal of Parallel and Distributed Computing},
volume = {62},
number = {4},
pages = {669-695},
year = {2002},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.2001.1809},
url = {https://www.sciencedirect.com/science/article/pii/S074373150191809X},
author = {Dhrubajyoti Goswami and Ajit Singh and Bruno R. Preiss},
keywords = {design patterns in parallel computing, parallel programming environments, skeleton-based parallel programming, high-performance computing models, software tools for parallel programming},
abstract = {The concept of design patterns has been extensively studied and applied in the context of object-oriented software design. Similar ideas are being explored in other areas of computing as well. Over the past several years, researchers have been experimenting with the feasibility of employing design-patterns related concepts in the parallel computing domain. In the past, several pattern-based systems have been developed with the intention to facilitate faster parallel application development through the use of preimplemented and reusable components that are based on frequently used parallel computing design patterns. However, most of these systems face several serious limitations such as limited flexibility, zero extensibility, and the ad hoc nature of their components. Lack of flexibility in a parallel programming system limits a programmer to using only the high-level components provided by the system. Lack of extensibility here refers to the fact that most of the existing pattern-based parallel programming systems come with a set of prebuilt patterns integrated into the system. However, the system provides no obvious way of increasing the repertoire of patterns when need arises. Also, most of these systems do not offer any generic view of a parallel computing pattern, a fact which may be at the root of several of their shortcomings. This research proposes a generic (i.e., pattern- and application-independent) model for realizing and using parallel design patterns. The term “parallel architectural skeleton” is used to represent the set of generic attributes associated with a pattern. The Parallel Architectural Skeleton Model (PASM) is based on the message-passing paradigm, which makes it suitable for a LAN of workstations and PCs. The model is flexible as it allows the intermixing of high-level patterns with low-level message-passing primitives. An object-oriented and library-based implementation of the model has been completed using C++and MPI, without necessitating any language extension. The generic model and the library-based implementation allow new patterns to be defined and included into the system. The skeleton-library serves as a framework for the systematic, hierarchical development of network-oriented parallel applications.}
}
@article{KOSTOPANAGIOTIS2015209,
title = {Low cost CPU–GPGPU parallel computing in real-world structural engineering},
journal = {Journal of Building Engineering},
volume = {4},
pages = {209-222},
year = {2015},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S2352710215300322},
author = {Christos Kostopanagiotis and Markos Kopanos and Dennis Ioakim and Kyriakos Perros and Nikos D. Lagaros},
keywords = {Parallel computing, CPU and GPGPU computing, RC and masonry structures, non-linear static analyses, direct time-history analyses},
abstract = {Parallel computing has matured from a narrow academic environment, where researchers developed finite element computing tools for performing their studies, to become the basis in modern analysis and design procedures. Several software applications in recent years have made these computing environments accessible to professional engineers, researchers and students outside the computer science research community. These software applications, mainly focused on aerospace, aeronautical, mechanical and naval structural systems, have incorporated the parallel computing component as an additional capability of the finite element software package. On the other hand though, there is not a computational approach for dealing with real-world civil engineering structures such as buildings, bridges or more complex civil engineering structures taking advantage of the high-performance capabilities of contemporary personal computers. Furthermore, graphic processing units (GPUs) have lately been used as high performance co-processors for demanding computational tasks. In modern structural analysis software packages, a hybrid computing model can be implemented, according to which the GPU can be used as a co-processor in order to offload the system's central processing unit (CPU), and therefore increase the overall computational efficiency. SCADA Pro is a structural analysis and design technical software package which implements out-of-core direct and iterative sparse parallel solvers in order to provide high performance computing capabilities in real-world structural engineering. In the present work, real-world structural systems are analysed using SCADA Pro's high performance computing capabilities. The results and computational efficiency are discussed and compared among different solvers, single and multi-threaded CPU and GPU computations.}
}
@article{MANZOOR2010696,
title = {QUIET: A Methodology for Autonomous Software Deployment using Mobile Agents},
journal = {Journal of Network and Computer Applications},
volume = {33},
number = {6},
pages = {696-706},
year = {2010},
note = {Advances on Agent-based Network Management},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2010.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S1084804510000500},
author = {Umar Manzoor and Samia Nefti},
keywords = {Autonomous network installation, Unattended network installation, Agent based installation, Silent installation, Mass installation, Mobile agent, Software deployment using agents},
abstract = {Every software setup has an installation wizard that helps the user to install/un-install the software on PCs. Typically user interaction is required and the process cannot proceed without user input. Silent Unattended Installation Package Manager (SUIPM) automates the process of software installation/un-installation and can be used to deploy any software silently without user interaction. In this paper, we have proposed A Methodology for Autonomous Software Deployment using Mobile Agents, which deploys silent unattended installation/un-installation packages efficiently and smartly on networks without user interaction or intervention, suitable for network of networks, commonly known as CAN (campus area network). The system once initialized is fully autonomous and deployment of the software(s) is performed efficiently and autonomously with the help of mobile agents. We have evaluated this architecture on the university campus having 7 laboratories equipped with 20–300 PCs in various laboratories. Results are very promising and support the implementation of the solution.}
}
@article{KIM201834,
title = {Efficient design optimization of complex system through an integrated interface using symbolic computation},
journal = {Advances in Engineering Software},
volume = {126},
pages = {34-45},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818304083},
author = {Hansu Kim and Shinyu Kim and Taekyun Kim and Tae Hee Lee and Namhee Ryu and Kihan Kwon and Seungjae Min},
keywords = {Integrated interface, Symbolic computation, Complex system, Analysis of variance, Design of experiments, Kriging surrogate model},
abstract = {A complex system is composed of several subsystems and numerous lower level components. It is inefficient to design the complex system at the system level by using high-fidelity models. A system level approach using a language-based model is required to design such a highly complex system before implementing a detailed design. However, as commercial process integration and design optimization software packages focus on detailed design, which uses the high-fidelity models, it is difficult to perform the design optimization of a highly complex system, such as a combat vehicle. Moreover, as commercial optimization software packages use numerical computation, the gradient calculation cost can increase, and the matrix computation process can be inefficient in terms of optimization time. Therefore, in this study, an integrated interface for efficient design optimization was developed to focus on concept design using a MODELICA language-based model. Additionally, the design variable screening by using analysis of variance, surrogate modeling through sequential design of experiments, and symbolic computation were used to solve the aforementioned problems. These were applied to the design optimization of the combat vehicle system to demonstrate the effectiveness of the integrated interface and symbolic computation. In conclusion, a concept design utilizing a MODELICA language-based model was achieved, and the optimization time achieved by symbolic computation was largely reduced in comparison to the optimization time achieved by numerical computation.}
}
@article{DOWERS2000471,
title = {Towards a framework for high-performance geocomputation: handling vector-topology within a distributed service environment},
journal = {Computers, Environment and Urban Systems},
volume = {24},
number = {5},
pages = {471-486},
year = {2000},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(00)00011-9},
url = {https://www.sciencedirect.com/science/article/pii/S0198971500000119},
author = {S. Dowers and B.M. Gittings and M.J. Mineter},
keywords = {High-performance computing, Vector topology, Open GIS, Software libraries, Service architectures, Algorithms},
abstract = {This paper lays out a framework, based on the emerging Open GIS standards, which will allow the integration of parallel computing technology such that it becomes a viable component of a new generation of geographical information system (GIS) software. The significant costs of parallel re-implementation have thus far acted as a major disincentive to software vendors taking advantage of parallel technology to solve performance problems. These problems will be thrown into sharp focus by the demands of web-based geographical information services. Designs for a series of software libraries, which are subject to a prototype implementation involving the use of a sophisticated data format (Neutral Transfer Format Level 4), are examined with a view to re-implementation making use of the Open GIS Abstract Specification Model. A range of services are envisaged, which can provide functions at various levels from data retrieval, spatial analysis and map generation to specialist environmental models, which are made available over the Internet. Parallelism is seen as an important route for accelerating individual transactions. These services can equally be based on large specialised parallel servers or a co-operating set of under-used workstations. The implementation strategy involves insulating standard serial algorithms from parallelism through support libraries. These libraries handle, for example, the decomposition of the data, thus effectively encapsulating the parallelism within one component of the software and allowing the creation of high-performance software components which are compatible with the Open GIS service architecture.}
}
@incollection{FERGUSON2003227,
title = {CHAPTER 8 - Library systems development},
editor = {Stuart Ferguson and Rodney Hebels},
booktitle = {Computers for Librarians (Third Edition)},
publisher = {Chandos Publishing},
edition = {Third Edition},
pages = {227-246},
year = {2003},
series = {Topics in Australasian Library and Information Studies},
isbn = {978-1-876938-60-4},
doi = {https://doi.org/10.1016/B978-1-876938-60-4.50014-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781876938604500148},
author = {Stuart Ferguson and Rodney Hebels},
keywords = {Inhouse development, Turnkey system, Consortium, Systems life cycle, Systems analysis, Feasibility study, Functional requirements, Request for proposals, Ergonomics, Smart barcodes},
abstract = {Publisher Summary
This chapter examines the development of computerized library systems. It builds on previous chapters, which dealt first with computer applications in libraries and then with the supporting technology. The chapter opens with an outline of the main options for systems development facing library managers. Where practicable, the emphasis is on a top-down approach, which takes as its starting point the data-processing and information needs of the library and not simply the availability of the technology. The chapter introduces the traditional systems analysis and design approach, before focusing on the way in which libraries generally go about acquiring new systems, which is to purchase prewritten software packages. Specific aspects of the systems development are examined, including project management, writing specifications, selecting systems, implementing systems, and the final systems evaluation.}
}
@article{OBERLOIER2022100050,
title = {Particle Swarm Optimization of Printing Parameters for Open-source TIG-based Metal 3D Printing},
journal = {Chinese Journal of Mechanical Engineering: Additive Manufacturing Frontiers},
volume = {1},
number = {4},
pages = {100050},
year = {2022},
issn = {2772-6657},
doi = {https://doi.org/10.1016/j.cjmeam.2022.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2772665722000344},
author = {Shane Oberloier and Wilson J Holmes and Luke A Reich and Joshua M Pearce},
keywords = {Metal 3D printing, Additive manufacturing, Tungsten inert gas welding, TIG welding, Particle swarm optimization, RepRap},
abstract = {Proprietary metal 3D printing is still relegated to relatively expensive systems that have been constructed over years of expensive trial-and-error to obtain optimum 3D printing settings. Low-cost open-source metal 3D printers can potentially democratize metal additive manufacturing; however, significant resources are required to redevelop optimal printing parameters for each metal on new machines. In this study, the particle swam optimization (PSO) experimenter, a free and open-source software package, is utilized to obtain the optimal printing parameters for a tungsten inert gas-based metal open source 3D printer. The software is a graphical user interface implementation of the PSO method and is designed specifically for hardware-in-loop testing. It uses the input of experimental variables and their respective ranges, and then proposes iterations for experiments. A custom fitness function is defined to characterize the experimental results and provide feedback to the algorithm for low-cost metal additive manufacturing. Four separate trials are performed to determine the optimal parameters for 3D printing. First, an experiment is designed to deposit and optimize the parameters for a single line. Second, the parameters for a single-layer plane is optimized experimentally. Third, the optimal printing parameters for a cube is determined experimentally. Fourth, the line optimization experiment is revised and reconducted using different shield gas parameters. The results and limitations are presented and discussed in the context of expanding wire arc additive manufacturing to more systems and material classes for distributed digital manufacturing.}
}
@article{TOPA201769,
title = {Modelling ecology and evolution of Foraminifera in the agent-oriented distributed platform},
journal = {Journal of Computational Science},
volume = {18},
pages = {69-84},
year = {2017},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877750316301168},
author = {Paweł Topa and Łukasz Faber and Jarosław Tyszka and Maciej Komosinski},
keywords = {Agent-based modelling, Artificial Life, Multi-agent systems, Foraminifera},
abstract = {We present a new software platform called eVolutus for simulating evolution of living organisms. We choose Foraminifera as model organisms that represent a group of single-cellular, mainly marine, organisms that construct well fossilisable protective shells. They have lived on Earth for more than 540 million years and have left an extraordinary fossil record that is excellent for testing palaeoecological and evolutionary hypotheses. We use the AgE platform, which is a lightweight agent-oriented platform supporting distributed computation. The paper presents the general architecture of this modelling environment as well as more detailed descriptions of the implemented rules and applied solutions. The utility of this software is demonstrated by presenting the configuration and results of sample experiments.}
}
@incollection{HAGINO2005311,
title = {F - IPv4-Mapped Address API Considered Harmful: Draft-cmetz-v6ops-v4mapped-api-harmful-00.txt},
editor = {Jun-ichiro itojun Hagino},
booktitle = {IPv6 Network Programming},
publisher = {Digital Press},
address = {Burlington},
pages = {311-316},
year = {2005},
isbn = {978-1-55558-318-7},
doi = {https://doi.org/10.1016/B978-155558318-7/50012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781555583187500125},
author = {Jun-ichiro itojun Hagino},
abstract = {Publisher Summary
The IPv6 Addressing Architecture defines the “IPv4-mapped IPv6 address.” This representation is used in the IPv6 basic API to denote IPv4 addresses using AF_INET6 sockets. This chapter discusses security issues with the use of IPv4-mapped address. A recommended and alternate solution is provided in the chapter. The use of IPv4-mapped addresses on AF INET6 sockets degrades portability, complicates IPv6 systems, and is likely to create security problems. This chapter discusses each issue in turn. It also proposes to resolve these problems by recommending deprecation of this mechanism. The drawbacks due to IPv4 mapped address support are degraded portability, increased implementation complexity, and access control complexity. The recommended solution is to deprecate RFC2553 section 3.7. By doing so, IPv6 implementations will be greatly simplified both in the system software and in all IPv6 application software. An alternate solution is to expand RFC2553 section 3.7 to fully define the behavior of AF INET6 sockets using IPv4-mapped addresses.}
}
@article{LENG201968,
title = {A lightweight policy enforcement system for resource protection and management in the SDN-based cloud},
journal = {Computer Networks},
volume = {161},
pages = {68-81},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618314129},
author = {Xue Leng and Kaiyu Hou and Yan Chen and Kai Bu and Libin Song and You Li},
keywords = {Software-defined networking, SDN-based cloud, Network management, Access control},
abstract = {SDN-based cloud adopts Software-defined Networking (SDN) to provide network services to the cloud, which allows more flexibility in network management. Meanwhile, the SDN controller provides users and administrators with various APIs to access and manage network resources. However, unauthorized requests, which are either sent from unregistered users or containing malicious operations, cannot be completely defended. Moreover, the correctness of network configuration in the SDN-based cloud cannot be guaranteed. In this paper, we propose SDNKeeper, a generic and fine-grained policy enforcement system for the SDN-based cloud, which can defend against unauthorized attacks and avoid network resource misconfiguration. Besides, a policy language is designed for administrators to define policies based on the attributes of the requester, resource, and environment. These policies will take effect when there are requests accessing the SDN controller via Northbound Interface (NBI). Specifically, SDNKeeper can block unauthorized network access requests outside the controller to protect the resources inside. Compared to other traditional policy-based access control systems, SDNKeeper is application-transparent and lightweight, which makes it easy to implement, deploy, and reconfigure at runtime. Based on the correctness proof of system design and the prototype implementation and evaluation, we conclude that SDNKeeper achieves accurate and efficient access control with insignificant throughput degradation and computational overhead.}
}
@article{MARCHESI2020100002,
title = {ABCDE—agile block chain DApp engineering},
journal = {Blockchain: Research and Applications},
volume = {1},
number = {1},
pages = {100002},
year = {2020},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2020.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2096720920300026},
author = {Lodovica Marchesi and Michele Marchesi and Roberto Tonelli},
keywords = {Blockchain, Smart contracts, Blockchain-oriented software engineering, UML, DApp design},
abstract = {Blockchain software development is becoming more and more important for any modern software developer and IT startup. Nonetheless, blockchain software production still lacks a disciplined, organized and mature development process, as demonstrated by the many and (in)famous failures and frauds occurred in recent years. In this paper we present ABCDE, a complete method addressing blockchain software development. The method considers the software integration among the blockchain components—smart contracts, libraries, data structures—and the out-of-chain components, such as web or mobile applications, which all together constitute a complete DApp system. We advocate for ABCDE the use of agile practices, because these are suited to develop systems whose requirements are not completely understood since the beginning, or tend to change, as it is the case of most blockchain-based applications. ABCDE is based on Scrum, and is therefore iterative and incremental. From Scrum, we kept the requirement gathering with user stories, the iterative-incremental approach, the key roles, and the meetings. The main difference with Scrum is the separation of development activities in two flows—one for smart contracts and the other for out-of-chain software interacting with the blockchain—each performed iteratively, with integration activities every 2–3 iterations. ABCDE makes explicit the activities that must be performed to design, develop, test and integrate smart contracts and out-of-chain software, and documents the smart contracts using formal diagrams to help development, security assessment, and maintenance. A diagram derived from UML class diagram helps to effectively model the data structure of smart contracts, whereas the exchange of messages between the entities of the system is modeled using a modified UML sequence diagram. The proposed method has also specific activities for security assessment and gas optimization, through systematic use of patterns and checklists. ABCDE focuses on Ethereum blockchain and its Solidity language, but preserves generality and with proper modifications might be applied to any blockchain software project. ABCDE method is described in detail, and an example is given to show how to concretely implement the various development steps.}
}
@article{ADAMS2021201,
title = {Evolving PDC curriculum and tools: A study in responding to technological change},
journal = {Journal of Parallel and Distributed Computing},
volume = {157},
pages = {201-219},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001490},
author = {Joel C. Adams},
keywords = {Beowulf cluster, Education, HPC, PDC, Supercomputing},
abstract = {Much has changed about parallel and distributed computing (PDC) since the author began teaching the topic in the late 1990s. This paper reviews some of the key changes to the field and describes their impacts on his work as a PDC educator. Such changes include: the availability of free implementations of the message passing interface (MPI) for distributed-memory multiprocessors; the development of the Beowulf cluster; the advent of multicore architectures; the development of free multithreading languages and libraries such as OpenMP; the availability of (relatively) inexpensive manycore accelerator devices (e.g., GPUs); the availability of free software platforms like CUDA, OpenACC, OpenCL, and OpenMP for using accelerators; the development of inexpensive single board computers (SBCs) like the Raspberry Pi, and other changes. The paper details the evolution of PDC education at the author's institution in response to these changes, including curriculum changes, seven different Beowulf cluster designs, and the development of pedagogical tools and techniques specifically for PDC education. The paper also surveys many of the hardware and software infrastructure options available to PDC educators, provides a strategy for choosing among them, and provides practical advice for PDC pedagogy. Through these discussions, the reader may see how much PDC education has changed over the past two decades, identify some areas of PDC that have remained stable during this same time period, and so gain new insight into how to efficiently invest one's time as a PDC educator.}
}
@article{BECKER2018237,
title = {BRAM-based function reuse for multi-core architectures in FPGAs},
journal = {Microprocessors and Microsystems},
volume = {63},
pages = {237-248},
year = {2018},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S014193311830156X},
author = {Pedro H. Exenberger Becker and Anderson L. Sartor and Marcelo Brandalero and Antonio C. {Schneider Beck}},
keywords = {Function-reuse, Soft-processors, Multi-core architectures, FPGA},
abstract = {Modern processors contain several specific hardware modules and multiple cores to ensure performance for a wide range of applications. In this context, FPGAs are frequently used as the implementation platform, since they offer architecture customization and fast time-to-market. However, many of them may not have the needed resources to implement all the necessary features, because of costs or complexity of the system to be implemented. When some needed functionalities do not fit in the target, they must be mapped into the much slower software domain. In this work, we exploit the fact these designs usually underuse their available BRAMs and propose a low-cost hardware-based function reuse mechanism for FPGAs, recovering some of the performance lost from the software part of applications that could not be implemented in hardware logic, with minimal impact on LUT usage. This is achieved by saving the inputs and outputs of the most frequently executed functions in a BRAM-based reuse table, so the next function executions with the same arguments can be skipped. This mechanism supports both precise and approximate modes and is evaluated with a 4-issue VLIW processor implemented in HDL, also considering a multi-core environment. Precise reuse, in single and multi-core scenarios, is assessed by running applications that use a software library to emulate floating point operations. Approximate reuse is evaluated over a single-core image-processing application that tolerates a certain level of error. Our scheme achieves 1.39 ×  geomean speedup in the precise single-core, while the multi-core case demonstrates application improvements from 1.25 ×  to 1.9 ×  when we start sharing the reuse table. In the approximate scenario, we achieve 1.52 ×  speedup with less than 10% error.}
}
@article{KAMENSKY2021634,
title = {Open-source immersogeometric analysis of fluid–structure interaction using FEniCS and tIGAr},
journal = {Computers & Mathematics with Applications},
volume = {81},
pages = {634-648},
year = {2021},
note = {Development and Application of Open-source Software for Problems with Numerical PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2020.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0898122120300481},
author = {David Kamensky},
keywords = {Isogeometric analysis, Fluid–structure interaction, Immersed boundary, Open-source software, FEniCS, Heart valve},
abstract = {We recently developed the open-source library tIGAr, which extends the FEniCS finite element automation framework to isogeometric analysis. The present contribution demonstrates the utility of tIGAr in complex problems by applying it to immersogeometric fluid–structure interaction (FSI) analysis. This application is implemented as the new open-source library CouDALFISh (Coupling, via Dynamic Augmented Lagrangian, of Fluids with Immersed Shells, pronounced “cuttlefish”), which uses the dynamic augmented Lagrangian (DAL) method to couple fluid and shell structure subproblems. The DAL method was introduced previously, over a series of papers largely focused on heart valve FSI, but an open-source implementation making extensive use of automation to compile numerical routines from high-level mathematical descriptions brings newfound transparency and reproducibility to these earlier developments on immersogeometric FSI analysis. The portions of CouDALFISh that do not use code generation also illustrate how a framework like FEniCS remains useful even when some functionality is outside the scope of its standard workflow. This paper summarizes the workings of CouDALFISh and documents a variety of benchmarks demonstrating its accuracy. Although the implementation emphasizes transparency and extensibility over performance, it is nonetheless demonstrated to be sufficient to simulate 3D FSI of an idealized aortic heart valve. Source code will be maintained at https://github.com/david-kamensky/CouDALFISh.}
}
@article{ZHU2021104751,
title = {A general approach to seismic inversion with automatic differentiation},
journal = {Computers & Geosciences},
volume = {151},
pages = {104751},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104751},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421000595},
author = {Weiqiang Zhu and Kailai Xu and Eric Darve and Gregory C. Beroza},
keywords = {Computational methods, Algorithms, Inverse problems, Parallel and highperformance computing, Software engineering},
abstract = {Imaging Earth structure or seismic sources from seismic data involves minimizing a target misfit function, and is commonly solved through gradient-based optimization. The adjoint-state method has been developed to compute the gradient efficiently; however, its implementation can be time-consuming and difficult. We develop a general seismic inversion framework to calculate gradients using reverse-mode automatic differentiation. The central idea is that adjoint-state methods and reverse-mode automatic differentiation are mathematically equivalent. The mapping between numerical PDE simulation and deep learning allows us to build a seismic inverse modeling library, ADSeismic, based on deep learning frameworks, which supports high performance reverse-mode automatic differentiation on CPUs and GPUs. We demonstrate the performance of ADSeismic on inverse problems related to velocity model estimation, rupture imaging, earthquake location, and source time function retrieval. ADSeismic has the potential to solve a wide variety of inverse modeling applications within a unified framework.}
}
@article{DZAFIC2022107866,
title = {Fast multi-phase gain matrix computation for real time distribution system state estimation},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {138},
pages = {107866},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107866},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521010802},
author = {Izudin Džafić and Rabih A. Jabr},
keywords = {Least squares approximation, Power distribution, Power system simulation, Single instruction multiple data, Sparse matrices, State estimation},
abstract = {Efficient distribution system state estimation (DSSE) is becoming increasingly important in modern grids. Building the gain matrix is carried out at every iteration of the weighted least squares (WLS) DSSE algorithm that employs the normal equations approach, and the computation of this matrix occupies a substantial portion of the total execution time. This paper presents a two-phase algorithm for the efficient gain matrix calculation suitable for modern CPU architectures. The preparation phase creates the necessary structures for effective data streaming and code execution. The gain matrix is calculated in the second phase using the prepared data and single instruction multiple data (SIMD) intrinsics. The Jacobian measurement matrix is organized as a sparse matrix of small dense blocks with double-precision complex values. The implementation exploits memory-aligned data blocks that require fewer CPU instructions and provide higher memory access speeds, in addition to modern C++ features for utilizing the instruction pipelining technique fully. Numerical results are reported on sizeable distribution networks having up to 3124 multi-phase nodes and 9537 three-phase nodes. They show that the proposed approach for computing the gain matrix is up to three times faster than the sparse matrix–matrix multiplication function in the SuiteSparse package, which makes the procedure useful for the development of real-time DSSE software.}
}
@article{ANJUM2001579,
title = {CitiTime: a system for rapid creation of portable next-generation telephony services},
journal = {Computer Networks},
volume = {35},
number = {5},
pages = {579-595},
year = {2001},
note = {Intelligent Networks and Internet Convergence},
issn = {1389-1286},
doi = {https://doi.org/10.1016/S1389-1286(00)00195-X},
url = {https://www.sciencedirect.com/science/article/pii/S138912860000195X},
author = {Farooq Anjum and Francesco Caruso and Ravi Jain and Paolo Missier and Adalberto Zordan},
keywords = {Next generation networks, Open programmable networks, Computer telephony integration, Java call control, Voice over IP},
abstract = {We present the architecture, design and experimental research prototype implementation of CitiTime, an open system architecture for the rapid development of advanced next-generation telephony services that overcomes some of the limitations of the current closed PSTN architecture and service model. CitiTime allows communication sessions to be set up over the PSTN, the Internet, or a combination of both. Services can be provided by multiple cooperating distributed service providers, some of whom may use third-party software components which can be “plugged in” or even dynamically downloaded from the network as needed. This allows advanced services to be deployed and delivered to users rapidly, a crucial requirement in the increasingly competitive telecommunications services marketplace. CitiTime is built upon an object-oriented call model called Citi Call Control (CCC) which we have defined as a small set of extensions to the standard Java Telephony API (JTAPI) call model. JTAPI is designed primarily for centralized, single provider, call center type applications. Our extensions provide support for multiple, distributed providers as well as advanced services. CCC hides details of underlying call-state management, protocols and hardware from applications. The CitiTime prototype software is currently operational in our laboratory. We briefly describe its current implementation as well as future work to address issues such as fault tolerance.}
}
@article{WADA1997S89,
title = {iGES — An intelligent graphical engineering system},
journal = {Computers & Chemical Engineering},
volume = {21},
pages = {S89-S94},
year = {1997},
note = {Supplement to Computers and Chemical Engineering},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(97)87484-4},
url = {https://www.sciencedirect.com/science/article/pii/S0098135497874844},
author = {T. Wada and Y. Nakada and P. Morrow and M.L. Lu},
abstract = {To support plant operators to operate and manage the plant for better quality, safer, and less cost, this paper describes the design and implementation of an intelligent Graphical Engineering System — iGES. First, three types of functions, data management, problem solving and utilities, are designed to support the whole plant operation. Then, a distributed and integrated software system architecture is described. Within this architecture, a central platform manages the functional distributed components, in terms of data transfer and communication and interfacing. Finally, the implementation issues are discussed in two phases including technical and business decisions, current status and next step plans. Based on a few commercial packages, the first phase system has been implemented and installed into a project site for evaluation.}
}
@article{LAMMIE2022124,
title = {MemTorch: An Open-source Simulation Framework for Memristive Deep Learning Systems},
journal = {Neurocomputing},
volume = {485},
pages = {124-133},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222002053},
author = {Corey Lammie and Wei Xiang and Bernabé Linares-Barranco and Mostafa {Rahimi Azghadi}},
keywords = {Memristors, RRAM, Non-ideal device characteristics, Deep learning, Simulation framework},
abstract = {Memristive devices have shown great promise to facilitate the acceleration and improve the power efficiency of Deep Learning (DL) systems. Crossbar architectures constructed using these Resistive Random-Access Memory (RRAM) devices can be used to efficiently implement various in-memory computing operations, such as Multiply Accumulate (MAC) and unrolled-convolutions, which are used extensively in Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs). However, memristive devices face concerns of aging and non-idealities, which limit the accuracy, reliability, and robustness of Memristive Deep Learning Systems (MDLSs), that should be considered prior to circuit-level realization. This Original Software Publication(OSP) presents MemTorch, an open-source11https://github.com/coreylammie/MemTorch framework for customized large-scale memristive Deep Learning (DL) simulations, with a refined focus on the co-simulation of device non-idealities. MemTorch also facilitates co-modelling of key crossbar peripheral circuitry. MemTorch adopts a modernized software engineering methodology and integrates directly with the well-known PyTorch Machine Learning (ML) library.}
}
@incollection{VANDAELE2015569,
title = {pyIDEAS: an Open Source Python Package for Model Analysis},
editor = {Krist V. Gernaey and Jakob K. Huusom and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {37},
pages = {569-574},
year = {2015},
booktitle = {12th International Symposium on Process Systems Engineering and 25th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63578-5.50090-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444635785500906},
author = {Timothy {Van Daele} and Stijn {Van Hoey} and Ingmar Nopens},
keywords = {model analysis, optimal experimental design, open source, Python},
abstract = {Mathematical models are used in many scientific areas such as enzyme kinetics and process engineering. They can be used for process analysis and optimization. However, a model is always a simplified representation of the real process and predictions always come with uncertainty. Therefore, the model building process should be performed thoroughly addressing calibration and validation procedures. Specific modeling tools (e.g. sensitivity analysis, optimization algorithms, experimental design techniques,…) to derive additional information (e.g. importance of parameters, estimate parameter uncertainty,…) are at hand and available in existing software. First, implementing these algorithms is time-consuming and often suboptimal in efficiency. Second, existing software is in many cases closed-source and not flexible in use. In both cases this results in the unavailability of the programmed algorithms in the corresponding articles making use of them. Therefore it is hard to validate the published findings and in some cases even impossible to reproduce the presented results. To address this problem the scientific community needs a certain critical mass of ‘off-the-shelf’ algorithms to perform model analyses which are available to the modeling community. To improve overall quality and reliability, such kind of code library should be open source and well documented. We hereby present pyIDEAS, an open source Python package to thoroughly but swiftly analyze systems represented by a set of (possibly mixed) differential and algebraic equations. The pyIDEAS package allows performing a model analysis in a straightforward and fast way. pyIDEAS provides a well-structured and logic framework which allows non-programmers to perform some model analysis and more advanced users to extend or adapt current functionality to their own requirements.}
}
@article{CUOMO2015170,
title = {Toward a Multi-level Parallel Framework on GPU Cluster with PetSC-CUDA for PDE-based Optical Flow Computation},
journal = {Procedia Computer Science},
volume = {51},
pages = {170-179},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.220},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915010285},
author = {S. Cuomo and A. Galletti and G. Giunta and L. Marcellino},
keywords = {Optical flow, Numerical solution of PDEs, Scientific computing libraries, GP-GPU, Mulitilevel parallel computing},
abstract = {In this work we present a multi-level parallel framework for the Optical Flow computation on a GPUs cluster, equipped with a scientific computing middleware (the PetSc library). Starting from a flow-driven isotropic method, which models the optical flow problem through a parabolic partial differential equation (PDE), we have designed a parallel algorithm and its software implementation that is suitable for heterogeneous computing environments (multiprocessor, single GPU and cluster of GPUs). The proposed software has been tested on real SAR images sequences. Numerical experiments highlight the performance of the proposed software framework, which can reach a gain of about 95% with respect to the sequential implementation.}
}
@article{VERSCHUEREN2018374,
title = {Towards a modular software package for embedded optimization},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {20},
pages = {374-380},
year = {2018},
note = {6th IFAC Conference on Nonlinear Model Predictive Control NMPC 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.062},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318327204},
author = {Robin Verschueren and Gianluca Frison and Dimitris Kouzoupis and Niels {van Duijkeren} and Andrea Zanelli and Rien Quirynen and Moritz Diehl},
keywords = {Nonlinear Model Predictive Control, Embedded Optimization, Software},
abstract = {In this paper we present acados, a new software package for model predictive control. It provides a collection of embedded optimization algorithms written in C, with a strong focus on computational efficiency. Its modular structure makes it useful for rapid prototyping, i.e. designing a control algorithm by putting together different algorithmic components that are readily connected and interchanged. The usefulness of the software is demonstrated with a closed-loop simulation experiment of an inverted pendulum, which shows acados attaining sub-millisecond computation times per iteration. Furthermore, we showcase a new algorithmic idea in the context of embedded nonlinear model predictive control (NMPC), namely sequential convex quadratic programming (SCQP), along with an efficient implementation of it.}
}
@article{CASE1996333,
title = {Discourse model for collaborative design},
journal = {Computer-Aided Design},
volume = {28},
number = {5},
pages = {333-345},
year = {1996},
note = {Computer-Aided Concurrent Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/0010-4485(95)00053-4},
url = {https://www.sciencedirect.com/science/article/pii/0010448595000534},
author = {Michael P Case and Stephen C-Y Lu},
keywords = {agent, conflict, discourse design collaboration, concurrent engineering, blackboard architecture, KQML},
abstract = {A Discourse Model, including a structure and a process, is developed that provides software support for collaborative engineering design. The model shares characteristics of other design systems in the literature, including frames, constraints, semantic networks, and libraries of sharable design objects. It contributes a new model for conflict-aware agents, dynamic identification and dissemination of agent interest sets, a virtual workspace language, automatic detection of conflict, and a unique protocol for negotiation that ensures that interested agents have an opportunity to participate. The model is implementation independent and applicable to many research and commercial design environments currently available. An example scenario is provided in the architecture/engineering/construction domain that illustrates collaboration during the conceptual design of a fire station.}
}
@article{HWANGBO2020106910,
title = {Design of control framework based on deep reinforcement learning and Monte-Carlo sampling in downstream separation},
journal = {Computers & Chemical Engineering},
volume = {140},
pages = {106910},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106910},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419310750},
author = {Soonho Hwangbo and Gürkan Sin},
keywords = {Liquid-liquid extraction column, Deep reinforcement learning, Monte-Carlo sampling, Control system, API production, Biopharmaceuticals},
abstract = {This paper proposes a systematic framework to develop deep reinforcement learning (RL)-based algorithms for control system of downstream separation in biopharmaceutical process as follows. First, a simulation model as a digital twin is built and Monte-Carlo sampling generates substantial amounts of samples considering disturbances. Second, the deep RL-based control system is designed and the optimization subject to sample datasets is conducted. The methodology is implemented in a prototype software and relevant codes are shared by Mendeley Data. The proposed model is successfully applied to control the liquid-liquid extraction column for the recovery of fusidic acid as part of downstream processing. The resulting deep RL algorithm provides an operation performance with a better API recovery yield (32 % higher than open loop operation) and lower deviations (23 % lower than open loop operation) against disturbances.}
}
@article{GONZALEZ2012247,
title = {Virtualization of reconfigurable coprocessors in HPRC systems with multicore architecture},
journal = {Journal of Systems Architecture},
volume = {58},
number = {6},
pages = {247-256},
year = {2012},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2012.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1383762112000197},
author = {Ivan Gonzalez and Sergio Lopez-Buedo and Gustavo Sutter and Diego Sanchez-Roman and Francisco J. Gomez-Arribas and Javier Aracil},
keywords = {High Performance Reconfigurable Computing, Coprocessor virtualization, Multicore programming, Reconfigurable hardware},
abstract = {HPRC (High-Performance Reconfigurable Computing) systems include multicore processors and reconfigurable devices acting as custom coprocessors. Due to economic constraints, the number of reconfigurable devices is usually smaller than the number of processor cores, thus preventing that a 1:1 mapping between cores and coprocessors could be achieved. This paper presents a solution to this problem, based on the virtualization of reconfigurable coprocessors. A Virtual Coprocessor Monitor (VCM) has been devised for the XtremeData XD2000i In-Socket Accelerator, and a thread-safe API is available for user applications to communicate with the VCM. Two reference applications, an IDEA cipher and an Euler CFD solver, have been implemented in order to validate the proposed architecture and execution model. Results show that the benefits arising from coprocessor virtualization outperform its overhead, specially when code has a significant software weight.}
}
@article{YIGIT2018216,
title = {A simulation-based optimization method for designing energy efficient buildings},
journal = {Energy and Buildings},
volume = {178},
pages = {216-227},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2018.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817341798},
author = {Sadik Yigit and Beliz Ozorhon},
keywords = {Energy simulation, Optimization, Genetic algorithm, Heat balance method},
abstract = {Buildings have a remarkable impact on the environment, therefore finding efficient design configurations satisfying conflicting criteria such as, economic and environmental performance has become an important task. The aim of this study is to propose a methodology to aid designers in satisfying the requirements of government regulations and green building certification programs, while optimizing the energy consumption and maintaining the thermal comfort. In this context, a software package combining a tailor-made thermal simulation software and Matlab Optimtool is developed to implement proposed methodology. The developed software package offers an effective method to perform large number of simulations to find optimal building configuration. The software package was tested on a reference building that represents a typical residential building in Turkey. The testing process is conducted on a five story building located in Istanbul and building information such as building size, location and occupation schedule were used within the developed software. Based on an extensive market search for building materials, cost functions for each parameter were developed as well as a database required for optimization process. A genetic algorithm optimization technique was utilized to minimize the objective function and find the optimal building configuration for the selected building. Development of a simulation-based optimization method fulfills the need for a tool that assists designers to find better design alternatives at the conceptual design stage. The software package requires least amount of data input for energy simulation process to improve usability. A tailor-made energy simulation module was developed to significantly reduce optimization time period. Besides, instead of coupling two separate software packages, performing the energy simulation and optimization processes on a single platform (MATLAB) reduces the time required to find optimal design and eliminates compatibility issues. Developing simulation-based optimization software on a single platform increased the flexibility and user-friendliness of the software. The effectiveness of the approach for finding optimal building configuration is demonstrated in the presented test cases.}
}
@article{PENGO2021104368,
title = {High performance and energy efficient sobel edge detection},
journal = {Microprocessors and Microsystems},
volume = {87},
pages = {104368},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104368},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121005202},
author = {Thaufig Peng-o and Panyayot Chaikan},
keywords = {Sobel, Edge detection, AVX, OpenMP, Image processing, Multi-core},
abstract = {Sobel edge detection is widely used in computer vision and image processing but its processing time becomes a serious problem in real-time environments, especially when an image is very large. Instead of utilizing a hardware-accelerated approach, we propose a purely software-based method which is simpler and cheaper. Our algorithm reduces the number of arithmetic operations and data loads, so that processing speed is increased and energy consumption reduced. The processing time is further reduced by the use of AVX intrinsics and OpenMP directives which distribute the workload among the AVX engines in a multi-core architecture. Our algorithm reduces the number of arithmetic operations by 22.73% compared to that of the state-of-the-art Sobel (SOAS) algorithm, while the number of data loads are reduced by 43.75% compared to SOAS. Performance and energy consumption comparisons between our algorithm and SOAS, as well as with the Sobel functions offered by the OpenCV and IPP libraries are investigated, and the results demonstrate that a multi-core version of our algorithm, implemented by AVX intrinsics, is on average 3.20, 9.34, and 13.99 times faster than IPP, SOAS, and OpenCV respectively. Also, it consumes an average of 2.91, 8.43, and 11.21 times less energy than IPP, SOAS, and OpenCV. Our algorithm, utilizing software modifications alone, benefits from both shorter development time and reduced cost compared to hardware approaches relying on an FPGA, ASIC, or GPU, making it more suitable for resource-constrained environments.}
}
@incollection{ELHAMRA2022415,
title = {Modeling of Phosphates Slurry Pipelines Through Dynamic Non-Newtonian Fluid Model with Modelica},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {415-420},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50070-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790500709},
author = {Fatima Ez-Zahra {El Hamra} and Radouan Boukharfane and Saad Benjelloun and Ahmed Ja and Jean-Michel Ghidaglia},
keywords = {Modelica, Bingham, finite volume, Non-Newtonian},
abstract = {The main objective of this work is to develop a one dimensional model to simulate the flow of Non-Newtonian fluids under Modelica software. We built a new library of components that provide the simulation of Newtonian and Non-Newtonian fluids flows. Within the pipe model, a new first order scheme of finite volume method to resolve Navier-Stokes equations for incompressible fluids with varying density is implemented. To introduce pressure drops, a Bingham plastic model is considered to represent the rheological behavior of the Non-Newtonian fluids. With the new library components, we perform a first numerical simulations of water flow in a pipe network of extended length. The computational cost of the performed simulation with the the Modelica Standard Library components reduced by a significant orders of magnitude compared to the previous implementation, while the dynamic results are not affected. Then we simulate the flow of the phosphate slurry through the real Pipeline network design and compare the results with industrial data.}
}
@article{RASTOGI2017677,
title = {Development of a prototype work-cell for validation of ITER remote handling control system standards},
journal = {Fusion Engineering and Design},
volume = {124},
pages = {677-681},
year = {2017},
note = {Proceedings of the 29th Symposium on Fusion Technology (SOFT-29) Prague, Czech Republic, September 5-9, 2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2016.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0920379616307359},
author = {Naveen Rastogi and Vamshi Krishna and Pramit Dutta and Manoah Stephen and Krishan {Kumar Gotewal} and David Hamilton and J.K Mukherjee},
keywords = {Remote handling, RH core system, ITER, OROCOS, Virtual reality, Robotics},
abstract = {An integrated control system architecture has been defined for the implementation of ITER Remote Handling (RH) equipment systems. The RH Core System (RHCS) is a standard software platform used for the development of ITER RH equipment controller applications to facilitate integration the overall control system. The RH core system is packaged using several open source tools and libraries including OROCOS, KDL, GSL, Eigen, Eclipse, maven, etc. It emphasizes the usage of OROCOS real-time tool chains in building individual control system components that are highly configurable and interactive. The communication between the control room and the embedded control applications is achieved using a standard Controller Interface Protocol defined by ITER. Prototyping work has been carried out for the development of individual sub-systems including RH equipment controller, viewing system, virtual reality monitoring system and RH plant controller built on the RH core system using the standard network communication protocols of ITER. All the individual sub-systems have been integrated into a prototype work cell and successfully tested on a COTS robot manipulator. This paper presents the design & implementation of the prototype work-cell and concludes by suggesting recommendations for the next version of the ITER RH core system.}
}
@article{CAO2004725,
title = {A framework for architecting and high-level programming support of CORBA applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {64},
number = {6},
pages = {725-739},
year = {2004},
note = {YJPDC Special Issue on Middleware},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2003.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731503002004},
author = {Jiannong Cao and Min Cao and Alvin S.T. Chan and Gengfeng Wu and Sajal K. Das},
keywords = {CORBA, Middleware, Distributed software, Software architecture, Dynamic reconfiguration, Graph-oriented programming},
abstract = {In this paper, we present a framework, called OrbGOP, to support the architecting and high-level programming of CORBA-based distributed applications. OrbGOP makes two contributions to the development of CORBA applications: (1) it provides higher-level abstractions for programming CORBA applications and frees the programmer from the underlying irrelevant details; (2) it facilitates the architecture description and dynamic reconfiguration of CORBA applications. OrbGOP is based on the graph-oriented programming (GOP) model, where the configuration of a distributed program is described as a logical graph separated from the programming of the constituent components of the program. Component interactions and dynamic reconfiguration are implemented by executing a set of operations that are defined over the graph. OrbGOP extends the application of GOP to the CORBA environment and provides more powerful support for distributed software architecture. Through a sample example, we show that OrbGOP provides a reflective, architectural approach to high-level programming support for the development of CORBA-based distributed applications. The system architecture, the design of runtime support and functional library support, as well as the preliminary evaluation of a working prototype of OrbGOP are also presented.}
}
@article{MARTINEZSANCHEZ2022106693,
title = {Statistical spatial analysis for cryo-electron tomography},
journal = {Computer Methods and Programs in Biomedicine},
volume = {218},
pages = {106693},
year = {2022},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2022.106693},
url = {https://www.sciencedirect.com/science/article/pii/S0169260722000785},
author = {Antonio Martinez-Sanchez and Wolfgang Baumeister and Vladan Lučić},
keywords = {Cryo-electron tomography, Statistical spatial analysis, Point pattern analysis, Macromolecular organization},
abstract = {Cryo-electron tomography (cryo-ET) is uniquely suited to precisely localize macromolecular complexes in situ, that is in a close-to-native state within their cellular compartments, in three-dimensions at high resolution. Point pattern analysis (PPA) allows quantitative characterization of the spatial organization of particles. However, current implementations of PPA functions are not suitable for applications to cryo-ET data because they do not consider the real, typically irregular 3D shape of cellular compartments and molecular complexes. Here, we designed and implemented first and the second-order, uni- and bivariate PPA functions in a Python package for statistical spatial analysis of particles located in three dimensional regions of arbitrary shape, such as those encountered in cellular cryo-ET imaging (PyOrg). To validate the implemented functions, we applied them to specially designed synthetic datasets. This allowed us to find the algorithmic solutions that provide the best accuracy and computational performance, and to evaluate the precision of the implemented functions. Applications to experimental data showed that despite the higher computational demand, the use of the second-order functions is advantageous to the first-order ones, because they allow characterization of the particle organization and statistical inference over a range of distance scales, as well as the comparative analysis between experimental groups comprising multiple tomograms. Altogether, PyOrg is a versatile, precise, and efficient open-source software for reliable quantitative characterization of macromolecular organization within cellular compartments imaged in situ by cryo-ET, as well as to other 3D imaging systems where real-size particles are located within regions possessing complex geometry.}
}
@article{GONNET2010313,
title = {Using piecewise polynomials for faster potential function evaluation},
journal = {Journal of Computational Physics},
volume = {229},
number = {2},
pages = {313-324},
year = {2010},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2009.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0021999109005208},
author = {Pedro Gonnet},
keywords = {Molecular dynamics, Potential function, Piecewise polynomial interpolation, Horner scheme, Chebyshev polynomials},
abstract = {In many molecular dynamics simulation software packages and hardware implementations, piecewise polynomials are used to represent and compute pairwise potential functions efficiently. In this paper, we present three modifications applicable to most interpolations to increase their accuracy. The increased accuracy reduces the amount of data that needs to be stored for each interaction potential, making such interpolations more suitable for architectures with limited memory and/or cache or hardware implementations.}
}
@article{KHOMH2018151,
title = {Understanding the impact of cloud patterns on performance and energy consumption},
journal = {Journal of Systems and Software},
volume = {141},
pages = {151-170},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300621},
author = {Foutse Khomh and S. Amirhossein Abtahizadeh},
keywords = {Cloud patterns, Energy consumption, Performance optimization, Energy efficiency},
abstract = {Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application.}
}