@inproceedings{10.1145/3457913.3457927,
author = {Zhang, Yinyuan and Zhang, Yang and Wu, Yiwen and Lu, Yao and Wang, Tao and Mao, Xinjun},
title = {Exploring the Dependency Network of Docker Containers: Structure, Diversity, and Relationship},
year = {2021},
isbn = {9781450388191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457913.3457927},
doi = {10.1145/3457913.3457927},
abstract = {Container technologies are being widely used in large scale production cloud environments, of which Docker has become the de-facto industry standard. As a key step, containers need to define their dependent base image, which makes complex dependencies exist in a large number of containers. Prior studies have shown that references between software packages could form technical dependencies, thus forming a dependency network. However, little is known about the details of docker container dependency networks. In this paper, we perform an empirical study on the dependency network of docker containers from more than 120,000 dockerfiles. We construct the container dependency network and analyze its network structure. Further, we focus on the Top-100 dominant containers and investigate their subnetworks, including diversity and relationships. Our findings help to characterize and understand the container dependencies in the docker community and motivate the need for developing container dependency management tools.},
booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
pages = {199–208},
numpages = {10},
keywords = {Network analysis, Dependency network, Docker container},
location = {Singapore, Singapore},
series = {Internetware '20}
}

@inproceedings{10.1145/3196398.3196428,
author = {Cassee, Nathan and Pinto, Gustavo and Castor, Fernando and Serebrenik, Alexander},
title = {How Swift Developers Handle Errors},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196428},
doi = {10.1145/3196398.3196428},
abstract = {Swift is a new programming language developed by Apple as a replacement to Objective-C. It features a sophisticated error handling (EH) mechanism that provides the kind of separation of concerns afforded by exception handling mechanisms in other languages, while also including constructs to improve safety and maintainability. However, Swift also inherits a software development culture stemming from Objective-C being the de-facto standard programming language for Apple platforms for the last 15 years. It is, therefore, a priori unclear whether Swift developers embrace the novel EH mechanisms of the programming language or still rely on the old EH culture of Objective-C even working in Swift.In this paper, we study to what extent developers adhere to good practices exemplified by EH guidelines and tutorials, and what are the common bad EH practices particularly relevant to Swift code. Furthermore, we investigate whether perception of these practices differs between novices and experienced Swift developers.To answer these questions we employ a mixed-methods approach and combine 10 semi-structured interviews with Swift developers and quantitative analysis of 78,760 Swift 4 files extracted from 2,733 open-source GitHub repositories. Our findings indicate that there is ample opportunity to improve the way Swift developers use error handling mechanisms. For instance, some recommendations derived in this work are not well spread in the corpus of studied Swift projects. For example, generic catch handlers are common in Swift (even though it is not uncommon for them to share space with their counterparts: non empty catch handlers), custom, developerdefined error types are rare, and developers are mostly reactive when it comes to error handling, using Swift's constructs mostly to handle errors thrown by libraries, instead of throwing and handling application-specific errors.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {292–302},
numpages = {11},
keywords = {swift, error handling, language feature usage},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2063384.2063431,
author = {Tan, Guangming and Li, Linchuan and Triechle, Sean and Phillips, Everett and Bao, Yungang and Sun, Ninghui},
title = {Fast Implementation of DGEMM on Fermi GPU},
year = {2011},
isbn = {9781450307710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063384.2063431},
doi = {10.1145/2063384.2063431},
abstract = {In this paper we present a thorough experience on tuning double-precision matrix-matrix multiplication (DGEM-M) on the Fermi GPU architecture. We choose an optimal algorithm with blocking in both shared memory and registers to satisfy the constraints of the Fermi memory hierarchy. Our optimization strategy is further guided by a performance modeling based on micro-architecture benchmarks. Our optimizations include software pipelining, use of vector memory operations, and instruction scheduling. Our best CUDA algorithm achieves comparable performance with the latest CUBLAS library. We further improve upon this with an implementation in the native machine language, leading to 20% increase in performance. That is, the achieved peak performance (efficiency) is improved from 302Gflop/s (58%) to 362Gflop/s (70%).},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {35},
numpages = {11},
keywords = {CUDA, high performance computing, GPU, matrix-matrix multiplication},
location = {Seattle, Washington},
series = {SC '11}
}

@inproceedings{10.1145/3383583.3398496,
author = {Fox, Edward A.},
title = {How Should One Explore the Digital Library of the Future?},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398496},
doi = {10.1145/3383583.3398496},
abstract = {Motivated by the Foreword of Licklider's "Libraries of the Future" (dedicated to Vannevar Bush), this keynote focuses on users, exploration, and future directions of the digital library (DL) field, which moves toward procognitive systems. Many different digital library "users," each a member of a Society, engage in a diversity of Scenarios, often involving some aspect of exploration, usually of the DL content Streams. Services -- e.g., searching, browsing, recommending, and visualizing -- help those users leverage knowledge Structures and Spatial representations. Following on the final sentence of Licklider's book, we "call for a formal base plus an overlay of experience, " leading to a new way to build better DLs. Licklider said we seek "the facts, concepts, principles, and ideas that lie behind the visible and tangible aspects of documents," to help us acquire and use knowledge. Put simply: "The console of the procognitive system will have two special buttons, a silver one labeled 'Where am I" and a gold one labeled 'What should I do next?' "How can we build and use this?For more than 55 years, researchers have applied artificial intelligence (AI), natural language processing (NLP), representations (data, information, knowledge), question-answering, databases, human-computer interaction, and other techniques described by Licklider, to these challenges. We have a vast range of hardware and software services available, but without a more formal approach, will not enable adaptive self-organization and tailored exploration.The 5S framework can help us build, apply, and improve digital libraries to facilitate exploration, through a formal approach that will simplify such efforts, making them extensible through both human and computing agents. For example, to more easily build DLs, we propose collaboratively building knowledge graphs -- involving both User eXperience designers, subject matter experts, and developers -- that specify connections to services and workflows, enabling DL operation atop a workflow engine. User exploration, additional help by UX designers, recommendations of adaptations of existing workflows, and AI-based optimizations and solutions to new problems, will all expand the knowledge graph to ensure new and more helpful assistance.When this is accomplished, we must teach and learn about this next generation of digital libraries, further developing suitable curriculum and educational modules, that rest upon a solid theoretical foundation, helping spread understanding of key concepts and best practices.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {1–2},
numpages = {2},
keywords = {5S, AI, NLP, HCI, adaptive self-organization, societies, licklider, spaces, procognitive, scenarios, streams, structures},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3299902.3311064,
author = {Gupta, Amit},
title = {Advances in Adaptable Computing},
year = {2019},
isbn = {9781450362535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299902.3311064},
doi = {10.1145/3299902.3311064},
abstract = {Recent technical challenges have forced the industry to explore options beyond the conventional "one size fits all" CPU scalar processing solution. Very large vector processing (DSP, GPU) solves some problems, but it runs into traditional scaling challenges due to inflexible, inefficient memory bandwidth usage. Traditional FPGA solutions provide programmable memory hierarchy, but the traditional hardware development flow has been a barrier to broad, high-volume adoption in application spaces like the Data Center market. Recent technical challenges in the semiconductor process prevent scaling of the traditional "one size fits all" CPU scalar compute engine. Changes in semiconductor process frequency scaling with the end of Dennard scaling have forced the standard computing elements to become increasingly multicore [Ref 1]. As a result, the semiconductor industry is exploring alternate domain-specific architectures, including ones previously relegated to specific extreme performance segments such as vector-based processing (DSPs, GPUs) and fully parallel programmable hardware (FPGAs). The question becomes: Which architecture is best for which task? Scalar processing elements (e.g., CPUs) are very efficient at complex algorithms with diverse decision trees and a broad set of libraries but are limited in performance scaling. Vector processing elements (e.g., DSPs, GPUs) are more efficient at a narrower set of parallelizable compute functions, but they experience latency and efficiency penalties because of inflexible memory hierarchy. Programmable logic (e.g., FPGAs) can be precisely customized to a particular compute function, which makes them best at latency-critical real-time applications (e.g., automotive driver-assist, ADAS) and irregular data structures (e.g., genomic sequencing), but algorithmic changes have traditionally taken hours to compile versus minutes.The solution combines all three paradigms with a new tool flow that offers a variety of different abstractions from framework to C to RTL-level coding into an adaptive compute acceleration platform (ACAP). This new category of devices, Xilinx's Versal? ACAPs, allows users to customize their own domain-specific architecture (DSA) from these three programmable paradigms.Xilinx is introducing a revolutionary new heterogeneous compute architecture, the adaptive compute acceleration platform (ACAP), which delivers the best of all three worlds-world-class vector and scalar processing elements tightly coupled to next-generation programmable logic (PL), all tied together with a high-bandwidth network-on-chip (NoC), which provides memory-mapped access to all three processing element types. This tightly coupled hybrid architecture allows more dramatic customization and performance increase than any one implementation alone.Such a dramatic increase in performance necessitates a similarly dramatic improvement in tools focusing on ease of use. ACAPs are specifically designed to work out of the box with no RTL flow required. ACAPs are natively software programmable, enabling C-based and framework-based design flows. The devices have an integrated shell that comprises a cache-coherent host interface (PCIe® or CCIX technology) with integrated DMA, a NoC, and integrated memory controllers, eliminating the requirement for RTL work.The new ACAP architecture also yields a dramatic improvement in ease of use. It provides a fully integrated, memory-mapped platform for programming through a unified toolchain. The Xilinx toolchain supports multiple entry methods for every type of developer. For example, certain applications (such as AI machine learning inference) can be coded at the framework level (e.g., Caffe, TensorFlow); others can be coded in C using pre-optimized libraries (e.g., filters for 5G radio). Traditional hardware evelopers can still port their existing RTL to ACAP via the traditional RTL entry flow.This talk reviews the needs driving the change from the traditional CPU-based compute model, explores the alternative options, and unveils the Xilinx Versal ACAP, the industry's first heterogeneous compute platform.},
booktitle = {Proceedings of the 2019 International Symposium on Physical Design},
pages = {37–38},
numpages = {2},
keywords = {acap, adas, dsa, adaptive compute acceleration platform, ai},
location = {San Francisco, CA, USA},
series = {ISPD '19}
}

@inproceedings{10.1145/2312005.2312014,
author = {Liu, Yujie and Diestelhorst, Stephan and Spear, Michael},
title = {Delegation and Nesting in Best-Effort Hardware Transactional Memory},
year = {2012},
isbn = {9781450312134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2312005.2312014},
doi = {10.1145/2312005.2312014},
abstract = {The guiding design principle behind best-effort hardware transactional memory (BEHTM) is simplicity of implementation and verification. Only minimal modifications to the base processor architecture are allowed, thereby reducing the burden of verification and long-term support. In exchange, the hardware can support only relatively simple multiword atomic operations, and must fall back to a software run-time for any operation that exceeds the abilities of the hardware.This paper demonstrates that BEHTM simplicity does not prohibit advanced and complex transactional behaviors. We exploit support for immediate non-transactional stores in the AMD Advanced Synchronization Facility to build a mechanism for communication among transactions. While our system allows arbitrary communication patterns, we focus on a design point where each transaction communicates with a system-wide manager thread. The API for the manager thread allows BEHTM transactions to delegate unsafe operations (such as system calls) to helper threads, and also enables the creation of nested parallel transactions. This paper also explores which forms of nesting are possible, and identifies constraints on nesting that are a consequence of how BEHTM is designed.},
booktitle = {Proceedings of the Twenty-Fourth Annual ACM Symposium on Parallelism in Algorithms and Architectures},
pages = {38–47},
numpages = {10},
keywords = {allocation, transactional memory, nesting, synchronization},
location = {Pittsburgh, Pennsylvania, USA},
series = {SPAA '12}
}

@inproceedings{10.1145/97426.97997,
author = {Carlson, Patricia A.},
title = {Artificial Neural Networks as Cognitive Tools for Professional Writing},
year = {1990},
isbn = {0897914147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/97426.97997},
doi = {10.1145/97426.97997},
abstract = {Computers are cognitive tools — they extend the capabilities of the human mind. Paper and pencil are also cognitive tools — they enhance human memory by acting as a permanent record, and they mediate the formation of thought by serving as a scratchpad or rehearsal device. However, there is a qualitative difference between these cognitive tools: the computer as a writing environment can become an active participant in the process while paper and pencil must remain passive instruments. Because this fundamental difference seems obvious to me, I'm surprised that most computer-aided writing (CAW) software available today is based on a model derived from writing with traditional tools.Such computer tools as spelling checkers, “style” checkers (actually, they check usage) and outliners, have been around in one form or another for a long time now. Yet they have not really had much of an impact. Most of these packages share three major drawbacks: their analysis is based on statistical measurements of simple surface features of writing; they provide “after-the-fact” profiling; and, in general, they treat all text as equal.As Shoshana Zuboff points out (In the Age of the Smart Machine), the computer — unlike the tools of the Industrial Revolution — not only automates, it also informates. When it comes to text technology, we've done a great deal to automate the process — as illustrated by word processing and desktop publishing. But we're only starting to use the computer's capacity to informate the process.A quick review of four categories of CAW tools indicates the state-of-the-art.Statistical Text Analysis: I'll use Bell Lab's Writer's Workbenchtm to cover a broad category of software which makes use of patterns to analyze prose. The Workbench is a collection of small programs to measure surface features of writing. For example, the user can find out readability level, average sentence length, word length, percentage of sentence types, percentage of passive-voice verbs, jargon, wordiness, sexist language, spelling errors, and improper usage. Other programs are intended to analyze rhetoric structures. For example, a program displays only the first and last sentence in each paragraph, with the idea that this representation will allow the user to check for logical transitions.Though the package has been around for some time now, it never really caught on. Its UNIX requirement and relatively high cost lessened the likelihood of widespread use. Additionally, the forty-or-more programs are all discrete — meaning that a truly cumulative analysis of a passage, taking into account the interaction of stylistic features, isn't possible. And, as a third limitation, because the analysis works at such a low level in the composing process, writers sometimes become obsessed with the accidents (surface errors) of their prose rather than the essence (strengthening the logic and content).Prewriting: Planning what to say and how to say it takes time. Professional writers have developed strategies for making this “prewriting” stage of composing more efficient. The journalist's “who, what, when, where, and why” litany is an example of a heuristic intended to help focus thoughts. There are many such heuristics, some dating back to Aristotle.Typically, software in this category automates an established strategy. The earliest invention software was modeled on Joseph Weizenbaum's ELIZA. Even today, implementation frequently takes the form of a dialogue, with the program asking significant questions and making appropriate comments on the writer's responses. The writer then uses the recorded information as the raw materials for the paper.Two drawbacks show up in current systems. First, many strategies when used by professional writers are comparable to a rule-of-thumb; thus, the effectiveness and appropriateness of a heuristic varies with the task. They lose most of their spontaneity and flexibility when automated. Second, the dialogue metaphor — which attributes a personality to the computer — is an embarrassing affectation in most programs.Outliners: This category has generated more commercial interest than the other three. Though frequently called “idea processors,” the label seems more honorific than earned. Most use a top-down (general-to-specific) knowledge representation as their bases. In other words, the writer is encouraged to find hierarchical relationships in her raw material by filling in an open-ended tree-structure. On some systems, levels can be hidden, thus focusing attention and reducing the cognitive load inherent in the writing process.Writing Environments: The more interesting of these programs are still in their infancy, and can be represented by WE (Writing Environment developed at the University of North Carolina—Chapel Hill) and CICILE (developed at the Center for Applied Cognitive Science in Toronto, Ontario). In something like the Writer's Workbenchtm, analytical programs are separate entities, and the writer is free to pick and choose among them. On the other hand, the suite of tools in a “writing environment” is integrated and part of a rigorously structured cognitive model of the writing process. In essence, a well-designed writing environment orchestrates the writing process by emulating stages of thinking. Few, if any, writing environments include AI applications as we normally define them (e.g. expert systems). Nevertheless, because the whole system supports and guides the activities of thinking, these knowledge-making habitats should be characterized as “intelligent.”I like the concept, but I am just a bit uneasy with the implementation. First, all of the examples I am aware of are theory-laden and exit as heavily-funded projects at large research universities or government-sponsored laboratories. In fact, these systems seem to be testbeds for doing high-powered research on the writing process more than practical tools for professionals. Second, because of the amount of “scaffolding” each system provides for the writer, they seem more appropriate as a learning environment. In short, they are more tutors than tools. And third, their heavy commitment to a definitive cognitive model of writing seems to ignore what historians of technology have taught us: new tools engender new habits of mind, and the tool — over time — can change the nature of the task.In summary then, word processing precipitated interest in computer-aided writing (CAW). Once text could be represented as bits and bytes, computational software for analyzing prose patterns became feasible. My objection is that the patterns used are too fine-grained and that the evaluation is too rigorous to qualify as a comfortable cognitive tool. I have never used a CAW product that, eventually, didn't pinch and constrain my writing process by being distractingly intrusive, nit-pickingly atomistic, or down-right tedious and misleading in the advice it returned.I view writing as one manifestation of the controlled creativity we call design. The cognitive activities of design take place in a cyclical rather than linear fashion. First, we decompose or partition the task into its components to get an idea of what we are trying to do. Then, we work on the pieces for a while, step back to compare interim results with higher-level goals, consolidate gains, jettison unrealistic expectations or excessive constraints, reorder plans, and move back to working on the pieces again. The cycle takes place over and over during the writing session. Good writers excel where poor writers fail because of this flexibility, this ability to move smoothly between top-down and bottom-up strategies. To my mind, a good cognitive tool for composing has two functions: (1) to serve as a peripheral brain that helps with the cognitive overload inherent in a complex task, and (2) to act as an expert associate that provides counsel in the iterative, “prototype and feedback” process of design.},
booktitle = {Proceedings of the 8th Annual International Conference on Systems Documentation},
pages = {95–110},
numpages = {16},
location = {Little Rock, Arkansas, USA},
series = {SIGDOC '90}
}

@article{10.1145/97435.97997,
author = {Carlson, Patricia A.},
title = {Artificial Neural Networks as Cognitive Tools for Professional Writing},
year = {1990},
issue_date = {1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {0731-1001},
url = {https://doi.org/10.1145/97435.97997},
doi = {10.1145/97435.97997},
abstract = {Computers are cognitive tools — they extend the capabilities of the human mind. Paper and pencil are also cognitive tools — they enhance human memory by acting as a permanent record, and they mediate the formation of thought by serving as a scratchpad or rehearsal device. However, there is a qualitative difference between these cognitive tools: the computer as a writing environment can become an active participant in the process while paper and pencil must remain passive instruments. Because this fundamental difference seems obvious to me, I'm surprised that most computer-aided writing (CAW) software available today is based on a model derived from writing with traditional tools.Such computer tools as spelling checkers, “style” checkers (actually, they check usage) and outliners, have been around in one form or another for a long time now. Yet they have not really had much of an impact. Most of these packages share three major drawbacks: their analysis is based on statistical measurements of simple surface features of writing; they provide “after-the-fact” profiling; and, in general, they treat all text as equal.As Shoshana Zuboff points out (In the Age of the Smart Machine), the computer — unlike the tools of the Industrial Revolution — not only automates, it also informates. When it comes to text technology, we've done a great deal to automate the process — as illustrated by word processing and desktop publishing. But we're only starting to use the computer's capacity to informate the process.A quick review of four categories of CAW tools indicates the state-of-the-art.Statistical Text Analysis: I'll use Bell Lab's Writer's Workbenchtm to cover a broad category of software which makes use of patterns to analyze prose. The Workbench is a collection of small programs to measure surface features of writing. For example, the user can find out readability level, average sentence length, word length, percentage of sentence types, percentage of passive-voice verbs, jargon, wordiness, sexist language, spelling errors, and improper usage. Other programs are intended to analyze rhetoric structures. For example, a program displays only the first and last sentence in each paragraph, with the idea that this representation will allow the user to check for logical transitions.Though the package has been around for some time now, it never really caught on. Its UNIX requirement and relatively high cost lessened the likelihood of widespread use. Additionally, the forty-or-more programs are all discrete — meaning that a truly cumulative analysis of a passage, taking into account the interaction of stylistic features, isn't possible. And, as a third limitation, because the analysis works at such a low level in the composing process, writers sometimes become obsessed with the accidents (surface errors) of their prose rather than the essence (strengthening the logic and content).Prewriting: Planning what to say and how to say it takes time. Professional writers have developed strategies for making this “prewriting” stage of composing more efficient. The journalist's “who, what, when, where, and why” litany is an example of a heuristic intended to help focus thoughts. There are many such heuristics, some dating back to Aristotle.Typically, software in this category automates an established strategy. The earliest invention software was modeled on Joseph Weizenbaum's ELIZA. Even today, implementation frequently takes the form of a dialogue, with the program asking significant questions and making appropriate comments on the writer's responses. The writer then uses the recorded information as the raw materials for the paper.Two drawbacks show up in current systems. First, many strategies when used by professional writers are comparable to a rule-of-thumb; thus, the effectiveness and appropriateness of a heuristic varies with the task. They lose most of their spontaneity and flexibility when automated. Second, the dialogue metaphor — which attributes a personality to the computer — is an embarrassing affectation in most programs.Outliners: This category has generated more commercial interest than the other three. Though frequently called “idea processors,” the label seems more honorific than earned. Most use a top-down (general-to-specific) knowledge representation as their bases. In other words, the writer is encouraged to find hierarchical relationships in her raw material by filling in an open-ended tree-structure. On some systems, levels can be hidden, thus focusing attention and reducing the cognitive load inherent in the writing process.Writing Environments: The more interesting of these programs are still in their infancy, and can be represented by WE (Writing Environment developed at the University of North Carolina—Chapel Hill) and CICILE (developed at the Center for Applied Cognitive Science in Toronto, Ontario). In something like the Writer's Workbenchtm, analytical programs are separate entities, and the writer is free to pick and choose among them. On the other hand, the suite of tools in a “writing environment” is integrated and part of a rigorously structured cognitive model of the writing process. In essence, a well-designed writing environment orchestrates the writing process by emulating stages of thinking. Few, if any, writing environments include AI applications as we normally define them (e.g. expert systems). Nevertheless, because the whole system supports and guides the activities of thinking, these knowledge-making habitats should be characterized as “intelligent.”I like the concept, but I am just a bit uneasy with the implementation. First, all of the examples I am aware of are theory-laden and exit as heavily-funded projects at large research universities or government-sponsored laboratories. In fact, these systems seem to be testbeds for doing high-powered research on the writing process more than practical tools for professionals. Second, because of the amount of “scaffolding” each system provides for the writer, they seem more appropriate as a learning environment. In short, they are more tutors than tools. And third, their heavy commitment to a definitive cognitive model of writing seems to ignore what historians of technology have taught us: new tools engender new habits of mind, and the tool — over time — can change the nature of the task.In summary then, word processing precipitated interest in computer-aided writing (CAW). Once text could be represented as bits and bytes, computational software for analyzing prose patterns became feasible. My objection is that the patterns used are too fine-grained and that the evaluation is too rigorous to qualify as a comfortable cognitive tool. I have never used a CAW product that, eventually, didn't pinch and constrain my writing process by being distractingly intrusive, nit-pickingly atomistic, or down-right tedious and misleading in the advice it returned.I view writing as one manifestation of the controlled creativity we call design. The cognitive activities of design take place in a cyclical rather than linear fashion. First, we decompose or partition the task into its components to get an idea of what we are trying to do. Then, we work on the pieces for a while, step back to compare interim results with higher-level goals, consolidate gains, jettison unrealistic expectations or excessive constraints, reorder plans, and move back to working on the pieces again. The cycle takes place over and over during the writing session. Good writers excel where poor writers fail because of this flexibility, this ability to move smoothly between top-down and bottom-up strategies. To my mind, a good cognitive tool for composing has two functions: (1) to serve as a peripheral brain that helps with the cognitive overload inherent in a complex task, and (2) to act as an expert associate that provides counsel in the iterative, “prototype and feedback” process of design.},
journal = {SIGDOC Asterisk J. Comput. Doc.},
month = {sep},
pages = {95–110},
numpages = {16}
}

@article{10.5555/2038772.2038806,
author = {Gudmundsen, Dee and Olivieri, Lisa and Sarawagi, Namita},
title = {Reducing the Learning Curve in an Introductory Programming Course},
year = {2012},
issue_date = {January 2012},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {27},
number = {3},
issn = {1937-4771},
abstract = {Learning the syntax of complex programming languages, such as C++ or Java, while simultaneously trying to understand the logic of basic programming constructs continues to be a challenge and a discouraging factor for many students enrolled in introductory programming courses. Even after completing a course that focuses on the logic of programming via a game-like environment, the learning curve to transition to an "industrial strength" programming language is often steep.Visual Logic© (www.visuallogic.org), an interactive, graphical software tool, introduces students to basic programming concepts in an almost syntax-free environment. The software enables novice programmers to develop and test programs in the form of interactive, executable flowcharts that provide students with immediate feedback. The graphical flowchart symbols students use to build their programs provide a visual perspective to abstract concepts including but not limited to branching, looping, arrays, pre-defined functions, IO, and procedures. There is a full Logo-like graphics package that allows students to create graphic programs. The simplicity of the Visual Logic program and its straightforward correlation with the essential structures of most programming languages make the transition to an "industrial" programming language a smooth one. The analysis, algorithm development, and documentation skills students acquire through flowcharting can also be applied to other languages in future courses.This workshop examines the simplicity, power and versatility of using Visual Logic to teach programming concepts, with different approaches in introductory courses: (i) as an entire course in algorithmic thinking, (ii) as the first half of an introductory programming course transitioning to Python, (iii) simultaneously using Visual Logic to illustrate concepts and brainstorm solutions as students learn Java syntax.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {158–159},
numpages = {2}
}

@article{10.1145/2390209.2390210,
author = {Tyson, Gareth and Mauthe, Andreas and Kaune, Sebastian and Grace, Paul and Taweel, Adel and Plagemann, Thomas},
title = {Juno: A Middleware Platform for Supporting Delivery-Centric Applications},
year = {2012},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/2390209.2390210},
doi = {10.1145/2390209.2390210},
abstract = {This article proposes a new delivery-centric abstraction which extends the existing content-centric networking API. A delivery-centric abstraction allows applications to generate content requests agnostic to location or protocol, with the additional ability to stipulate high-level requirements regarding such things as performance, security, and resource consumption. Fulfilling these requirements, however, is complex as often the ability of a provider to satisfy requirements will vary between different consumers and over time. Therefore, we argue that it is vital to manage this variance to ensure an application fulfils its needs. To this end, we present the Juno middleware, which implements delivery-centric support using a reconfigurable software architecture to: (i) discover multiple sources of an item of content; (ii) model each source’s ability to provide the content; then (iii) adapt to interact with the source(s) that can best fulfil the application’s requirements. Juno therefore utilizes existing providers in a backwards compatible way, supporting immediate deployment. This article evaluates Juno using Emulab to validate its ability to adapt to its environment.},
journal = {ACM Trans. Internet Technol.},
month = {dec},
articleno = {4},
numpages = {28},
keywords = {content-centric, content delivery, Middleware}
}

@article{10.1145/2541228.2555302,
author = {Totoni, Ehsan and Dikmen, Mert and Garzar\'{a}n, Mar\'{\i}a Jes\'{u}s},
title = {Easy, Fast, and Energy-Efficient Object Detection on Heterogeneous on-Chip Architectures},
year = {2013},
issue_date = {December 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2541228.2555302},
doi = {10.1145/2541228.2555302},
abstract = {We optimize a visual object detection application (that uses Vision Video Library kernels) and show that OpenCL is a unified programming paradigm that can provide high performance when running on the Ivy Bridge heterogeneous on-chip architecture. We evaluate different mapping techniques and show that running each kernel where it fits the best and using software pipelining can provide 1.91 times higher performance and 42% better energy efficiency. We also show how to trade accuracy for energy at runtime. Overall, our application can perform accurate object detection at 40 frames per second (fps) in an energy-efficient manner.},
journal = {ACM Trans. Archit. Code Optim.},
month = {dec},
articleno = {45},
numpages = {25},
keywords = {portable (mobile) devices, Energy efficiency, OpenCL, SIMD, heterogeneous on-chip architectures}
}

@inproceedings{10.1145/2875913.2875938,
author = {Liu, Jiwei and Mao, Xinjun},
title = {Towards Realisation of Evolvable Runtime Variability in Internet-Based Service Systems via Dynamical Software Update},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875938},
doi = {10.1145/2875913.2875938},
abstract = {Today's Internet-based service systems tend to run in open environments and try to satisfy varying requirements. In the context, the changes of requirements and environments can emerge at any point of their life cycle, and their runtime variability is supposed to be evolvable to deal with the changes. In other words, the number, type or attribute of variability elements (variation points and their associated variants) in the systems is regarded to be changeable, especially at runtime. How to realise evolvable runtime variability in software systems is a challenge in the community of software engineering. Software architecture is supposed to support updating software without interrupting service if runtime variability is to be changed. Besides, the relevant mechanisms have to be provided to achieve dynamic update. Thus, we propose a dynamically reconfigurable reference architecture to construct the systems with evolvable software variability. The resultant system established using our approach consists of variability units and their containers. Variability units embody the variability elements that can be changed through local relay, i.e., starting a new version of variability unit to take place of older ones. Containers are mainly used to carry out the dynamic updating for variability units and realise the functionality invocation among different variability units. They can also be added, removed or replaced without shutting down the entire system. We analyse the requirements and scenarios of evolvable runtime variability in the case of Personal Data Resource Network, and further show the effectiveness and applicability of our approach by constructing the system using our class library and solving the issues proposed in the case.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {229–238},
numpages = {10},
keywords = {Dynamic Software Update, Internet-based Service Systems, Software Architecture, Evolvable Runtime Variability},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/2677832.2677838,
author = {Liu, Jiwei and Mao, Xinjun},
title = {Towards Realisation of Evolvable Runtime Variability in Internet-Based Service Systems via Dynamical Software Update},
year = {2014},
isbn = {9781450333030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2677832.2677838},
doi = {10.1145/2677832.2677838},
abstract = {Today’s Internet-based service systems tend to run in open environments and try to satisfy varying requirements. In the context, the changes of requirements and environments can emerge at any point of their life cycle, and their runtime variability is supposed to be evolvable to deal with the changes. In other words, the number, type or attribute of variability elements (variation points and their associated variants) in the systems is regarded to be changeable, especially at runtime. How to realise evolvable runtime variability in software systems is a challenge in the community of software engineering. Software architecture is supposed to support updating software without interrupting service if runtime variability is to be changed. Besides, the relevant mechanisms have to be provided to achieve dynamic update. Thus, we propose a dynamically reconfigurable reference architecture to construct the systems with evolvable software variability. The resultant system established using our approach consists of variability units and their containers. Variability units embody the variability elements that can be changed through local relay, i.e., starting a new version of variability unit to take place of older ones. Containers are mainly used to carry out the dynamic updating for variability units and realise the functionality invocation among different variability units. They can also be added, removed or replaced without shutting down the entire system. We analyse the requirements and scenarios of evolvable runtime variability in the case of Personal Data Resource Network, and further show the effectiveness and applicability of our approach by constructing the system using our class library and solving the issues proposed in the case.},
booktitle = {Proceedings of the 6th Asia-Pacific Symposium on Internetware on Internetware},
pages = {97–106},
numpages = {10},
keywords = {Software Architecture, Evolvable Runtime Variability, Internet-based Service Systems, Dynamic Software Update},
location = {Hong Kong, China},
series = {INTERNETWARE 2014}
}

@inproceedings{10.5555/381473.381540,
author = {Fiorvanti, F. and Migliarese, G. and Nesi, P.},
title = {Reengineering Analysis of Object-Oriented Systems via Duplication Analysis},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {All software systems, no matter how they are designed, are subject to continuous evolution and maintenance activities to eliminate defects and extend their functionalities. This is particularly true for object-oriented systems where we may develop different software systems using the same internal library or framework. These systems may evolve in quite different directions in order to cover different functionalities. Typically there is the need to analyze their evolution in order to redefine the library or framework boundaries. This is a typical problem of software reengineering analysis. In this paper we describe metrics, based on duplication analysis, that contribute to the process of reengineering analysis of object-oriented systems. These metrics are the basic elements of a reengineering analysis method and tool. A duplication analysis at file, class and method levels have been performed. A structural analysis using metrics that capture similarities in class structure has been also exploited. In order to identify the best approach for the reengineering analysis of object-oriented systems a comparison between the two approaches is described. In this paper a case study based on real cases is presented, in which the results obtained by using a reengineering process with and without the analysis tool is described. The purpose of this study is to discover which method is the most powerful and which time reduction can be obtained by its use.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {577–586},
numpages = {10},
keywords = {metrics, reengineering, clones detection, doce duplication, object-oriented},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@article{10.1145/1926367.1926370,
author = {Morisita, Hirokazu and Inakagata, Kenta and Osana, Yasunori and Fujita, Naoyuki and Amano, Hideharu},
title = {Implementation and Evaluation of an Arithmetic Pipeline on FLOPS-2D: Multi-FPGA System},
year = {2011},
issue_date = {September 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0163-5964},
url = {https://doi.org/10.1145/1926367.1926370},
doi = {10.1145/1926367.1926370},
abstract = {UPACS (Unified Platform for Aerospace Computational Simulation) is one of the practical CFD (Computational Fluid Dynamics) packages supporting various selectability. A custom machine for efficient execution of MUSCL; a core functions of UPACS is implemented on FLOPS-2D (Flexibly Linkable Object for Programmable System); multi-FPGA reconfigurable system. The deep and complicated pipeline structure generated from MUSCL dataflow is divided and optimized into two FPGA boards by using a tuning tool called RER. With optimization of the order of operations and pipeline structure, about 60% utilization of the pipeline is achieved even by using serial links between two boards. The execution time is 6.16-23.19 times faster than that of the software on 2.66 GHz Intel Core 2 Duo processor.},
journal = {SIGARCH Comput. Archit. News},
month = {jan},
pages = {8–13},
numpages = {6},
keywords = {pipeline optimization, computational fluid dynamics, HPC with FPGAs}
}

@inproceedings{10.5555/800013.809502,
author = {Friel, Patricia and Sheppard, Sallie},
title = {Implications of the Ada R Environment for Simulation Studies},
year = {1984},
publisher = {IEEE Press},
abstract = {Ada packages to support event and process oriented simulation have been developed at Texas A&amp;M University. These packages include standard facilities as queue handling, random number generation, automatic statistics collection and simulation control. This paper provides an overview of the system facilities followed by an evaluation of the strengths and weaknesses of Ada as an implementation language for simulation software. Based on our experience Ada has appeal as a general purpose language which can be integrated into larger systems. It is portable among various computer architectures. It provides a rich base of constructs in which to implement models, and offers promise in terms of execution on parallel architectures.},
booktitle = {Proceedings of the 16th Conference on Winter Simulation},
pages = {476–489},
numpages = {14},
location = {Dallas, TX},
series = {WSC '84}
}

@article{10.1145/1102958.1102960,
author = {Friel, Patricia and Sheppard, Sallie},
title = {Implications of the ADA® Environment for Simulation Studies},
year = {1985},
issue_date = {April 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {0163-6103},
url = {https://doi.org/10.1145/1102958.1102960},
doi = {10.1145/1102958.1102960},
abstract = {Ada packages to support event and process oriented simulation have been developed at Texas A&amp;M University. These packages include standard Facilities such as queue handling, random number generation, automatic statistics collection and simulation control. This paper provides an overview of the system facilities followed by an evaluation of the strengths and weaknesses of Ada as an implementation language for simulation software. Based on our experience Ada has appeal as a general purpose language which can be integrated into larger systems. It is portable among various computer architectures, it provides a rich base of constructs in which to implement models, and offers promise in terms of execution on parallel architectures.},
journal = {SIGSIM Simul. Dig.},
month = {apr},
pages = {14–26},
numpages = {13}
}

@article{10.1145/3466818,
author = {Zhang, Weijia and Li, Jiuyong and Liu, Lin},
title = {A Unified Survey of Treatment Effect Heterogeneity Modelling and Uplift Modelling},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3466818},
doi = {10.1145/3466818},
abstract = {A central question in many fields of scientific research is to determine how an outcome is affected by an action, i.e., to estimate the causal effect or treatment effect of an action. In recent years, in areas such as personalised healthcare, sociology, and online marketing, a need has emerged to estimate heterogeneous treatment effects with respect to individuals of different characteristics. To meet this need, two major approaches have been taken: treatment effect heterogeneity modelling and uplifting modelling. Researchers and practitioners in different communities have developed algorithms based on these approaches to estimate the heterogeneous treatment effects. In this article, we present a unified view of these two seemingly disconnected yet closely related approaches under the potential outcome framework. We provide a structured survey of existing methods following either of the two approaches, emphasising their inherent connections and using unified notation to facilitate comparisons. We also review the main applications of the surveyed methods in personalised marketing, personalised medicine, and sociology. Finally, we summarise and discuss the available software packages and source codes in terms of their coverage of different methods and applicability to different datasets, and we provide general guidelines for method selection.},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {162},
numpages = {36},
keywords = {uplift modelling, conditional average treatment effect, individual treatment effect, Treatment effect heterogeneity modelling}
}

@inproceedings{10.5555/2020619.2020651,
author = {Maki, Kevin J. and Lee, Donghee and Piro, Dominic J. and Collette, Matthew},
title = {Hydroelastic Impact of Stern Structure Using CFD and FEA},
year = {2010},
publisher = {Society for Modeling &amp; Simulation International},
address = {Vista, CA},
abstract = {This paper presents the results of the structural response due to slamming in the stern region of the Joint High Speed Sealift (JHSS) vessel. A slamming event is identified by performing rigid-body large-amplitude seakeeping simulations using the LAMP program. The event is characterized by the relative motion between the free-surface and hull at a specific point near the stern of the vessel. The structure is developed using ABS and other conventional design guidelines, and it is analyzed using the commercial FEA software ABAQUS. The stress of the fluid on the structure is obtained by using a computational fluid dynamics (CFD) solver built on the open source CFD Library OpenFOAM. The fluid stress is applied to the modal model of the structure using a robust and flexible grid-matching routine. Finally, the structural response is determined in the time domain by solving the system of modal equations of motion.},
booktitle = {Proceedings of the 2010 Conference on Grand Challenges in Modeling &amp; Simulation},
pages = {231–238},
numpages = {8},
keywords = {hydroelasticity, impact, joint high speed sealift (JHSS), slamming},
location = {Ottawa, Ontario, Canada},
series = {GCMS '10}
}

@inproceedings{10.1145/1735813.1735817,
author = {Franssen, Michael and van den Brand, Mark},
title = {Design of a Proof Repository Architecture},
year = {2009},
isbn = {9781605589541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1735813.1735817},
doi = {10.1145/1735813.1735817},
abstract = {In this paper, we introduce a proof repository architecture to build a library of proofs for first-order theorems constructed by several theorem provers. The architecture is not fixed as such, but is configured by the user. It consists of three types of components, that allow us to connect theorem provers, store proofs and manage the connections between them. These components allow for many setups, like a local database of theorems, an interconnected series of databases of systems, interconnecting many theorem provers, using a theorem prover in a client-server architecture, Software As Service etc.},
booktitle = {Proceedings of the 1st Workshop on Modules and Libraries for Proof Assistants},
pages = {19–23},
numpages = {5},
keywords = {theorem database, repository, theorem proving, component architecture},
location = {Montreal, Canada},
series = {MLPA '09}
}

@article{10.1145/988655.988656,
author = {McInroy, John W. and Capowski, Joseph J.},
title = {A Graphics Subroutine Package for the Neuroscience Display Processor},
year = {1977},
issue_date = {Spring 1977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {0097-8930},
url = {https://doi.org/10.1145/988655.988656},
doi = {10.1145/988655.988656},
abstract = {The Neuroscience Display Processor (NDP) has been developed at the University of North Carolina for displaying two- and three-dimensional structures, and two-dimensional graphs. To make easy the displaying of graphs and other drawings on a refresh CRT or on a digital plotter, a subroutine package has been produced for the scientist programming in FORTRAN. Various levels of access (implemented as levels in a hierarchy of subroutines) provide increasing flexibility at the cost of increasing programming complexity. At the highest level, and with the least effort (i.e., with a single subroutine call), a programmer may display a graph accompanied by coordinate axes with scales, a title, and axis labels. The programmer may optionally display two such graphs on the CRT simulataneously. He may also display several graphs on the same set of coordinate axes. Graphs may be displayed without coordinate axes, etc., to facilitate the drawing of many superimposed graphs on hard copy.An NDP daslgn assumption that the general-purpose computer refreshes the CRT has far-reaching effects on the software design. That assumption, space and time constraints, the use of FORTRAN, and the provision of various options appropriate for the area of application all result in the programmer's being provided with a single subroutine to be called inside a refresh loop, with many switches as parameters.Where the needs of the application are not net by the capabilities aentloned above, the programmer may replace the single subroutine call by the use of several subroutines at lower levels as building blocks, with which to construct his own graphs or other drawings, thereby sacrificing some ease of programming for increased flexibility. The programmer nay call a subroutine to display a real number or an integer at a specified spot on the screen. He may display a character string. At a still lower level, the programmer say call for the display of a single character. At the lowest level of FORTRAN call, the programmer may specify that either polygons or series of disconnected lines or dots be drawn, nay indicate whether they are to be drawn on the CRT or on the plotter, and may load the rotation-and-translation matrix of the NDP.},
journal = {SIGGRAPH Comput. Graph.},
month = {apr},
pages = {1–12},
numpages = {12}
}

@inproceedings{10.1145/3357141.3357599,
author = {Costa, Ana Claudia L. A. I. and Colanzi, Thelma E. and Marcolino, Anderson S. and Barbosa, Ellen F.},
title = {Microservice-Oriented Product Line Architecture Design: An Exploratory Study},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357599},
doi = {10.1145/3357141.3357599},
abstract = {Microservice has been successfully employed in software industry [1, 11], as they provide modularization and easy management of small and autonomous services, high availability, scalability and short time-to-market. A recent study on microservices shows that most studies generate specific solutions, which emphasize the need for fundamental research, proposals of reusable practices and works that focus on providing information to ease communication between architects and stakeholders [11] -- software architecture can be a powerful tool for this regard. The definition and documentation of the software product line architecture (PLA) is an important activity, especially for inception and extraction of microservice-oriented PLA, because they involve decisions about how to design customizable microservices, how to arrange the communication between microsservices and APIs, etc. In this work, it is proposed a metamodel for the specification of microsserve-oriented PLA design in order to assist the developer in carrying out such an activity. The proposed metamodel was validated in an exploratory study, in which a new PLA was designed through the instantiation of the proposed metamodel and a product was configured from the designed PLA. Both the metamodel and its instantiation were evaluated in a survey involving software developers. The results indicate that the metamodel addresses the structural needs of microservice-oriented architectures. The main contributions of this work are (i) to assist the software product line developers in the specification and documentation of microservice-oriented PLA design and (ii) the lessons about the industrial practice learned from the surveys that are useful to enhance the proposed metamodel.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {113–122},
numpages = {10},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/1366230.1366234,
author = {Crawford, Catherine H. and Henning, Paul and Kistler, Michael and Wright, Cornell},
title = {Accelerating Computing with the Cell Broadband Engine Processor},
year = {2008},
isbn = {9781605580777},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1366230.1366234},
doi = {10.1145/1366230.1366234},
abstract = {In this paper, we describe our approach to utilizing the compute power of the Cell Broadband Engine™ (Cell/B.E.)1 processor as an accelerator for computationally intensive portions of high performance computing applications. We call this approach "hybrid programming" because it distributes application execution across heterogeneous processors. IBM developed a hardware implementation and software infrastructure that enables this hybrid computing model as part of the Roadrunner project for Los Alamos National Laboratory (LANL). In the hybrid programming model, a process running on a host processor, such as an x86_64 architecture processor, creates an accelerator process on an accelerator processor, such as the IBM® PowerXCell™8i2. The PowerXCell8i is a new implementation of the Cell Broadband Engine architecture. The host process then schedules compute intensive operations onto the accelerator process. The host and accelerator process can continue execution concurrently and synchronize when needed to transfer results or schedule new accelerator computation. We describe the Data Communication and Synchronization (DaCS) Library and Accelerated Library Framework (ALF) which are designed to allow applications to create new applications and adapt existing applications to exploit hybrid computing platforms. We also describe our experience in using such frameworks to construct hybrid versions of the familiar Linpack benchmark and an implicit Monte Carlo radiation transport application named Milagro. Performance measurements on prototype hardware are presented that show the performance improvements achieved to date, along with projections of the expected performance on the final Roadrunner system.},
booktitle = {Proceedings of the 5th Conference on Computing Frontiers},
pages = {3–12},
numpages = {10},
keywords = {hybrid programming models, accelerators},
location = {Ischia, Italy},
series = {CF '08}
}

@inproceedings{10.1145/3409334.3452049,
author = {Le, Bach and Troendle, David and Jang, Byunghyun},
title = {Detecting Fabric Density and Weft Distortion in Woven Fabrics Using the Discrete Fourier Transform},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452049},
doi = {10.1145/3409334.3452049},
abstract = {Fabric density and distortion offer important information on fabric attributes and quality during the manufacturing process. However, most current procedures require human effort, which is often inefficient, time-consuming, and imprecise. In this paper, we propose to use an automatic method using the 2D Fast Fourier Transform (2D-FFT) to count the number of yarns and determine the angle rotation of weft yarns in fabric images. First, we explain the mathematical background of Fourier Transform and 2D-FFT. Then, we use a customized and optimized software package to apply a 2D-FFT to extract image magnitude, phase, and power spectrum. We apply the inverse 2D Fast Fourier Transform (2D-iFFT) on selected frequencies corresponding to periodic structures - basic weave patterns - to reconstruct the original image and extract warp and weft yarns separately. Finally, we use a local adaptive threshold process to convert reconstructed images into binary images for the counting and calculating process. For the weft rotation, we apply a mathematical calculation on the frequency domain to collect the angular distribution and then figure out the major rotation of weft yarns. Our experiments show that the proposed method is highly accurate and capable of inspecting different patterns of fabric. We also observe that the processing time of our proposal method is practical and time-efficient.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {108–113},
numpages = {6},
keywords = {weft distortion, computer vision, fabric density, FFT},
location = {Virtual Event, USA},
series = {ACM SE '21}
}

@inproceedings{10.1145/2740908.2742027,
author = {Ekaputra, Fajar J. and Sabou, Marta and Serral, Estefan\'{\i}a and Biffl, Stefan},
title = {Collaborative Exchange of Systematic Literature Review Results: The Case of Empirical Software Engineering},
year = {2015},
isbn = {9781450334730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2740908.2742027},
doi = {10.1145/2740908.2742027},
abstract = {Complementary to managing bibliographic information as done by digital libraries, the management of concrete research objects (e.g., experimental workflows, design patterns) is a pre-requisite to foster collaboration and reuse of research results. In this paper we describe the case of the Empirical Software Engineering domain, where researchers use systematic literature reviews (SLRs) to conduct and report on literature studies. Given their structured nature, the outputs of such SLR processes are a special and complex type of research object. Since performing SLRs is a time consuming process, it is highly desirable to enable sharing and reuse of the complex knowledge structures produced through SLRs. This would enable, for example, conducting new studies that build on the findings of previous studies. To support collaborative features necessary for multiple research groups to share and reuse each other's work, we hereby propose a solution approach that is inspired by software engineering best-practices and is implemented using Semantic Web technologies.},
booktitle = {Proceedings of the 24th International Conference on World Wide Web},
pages = {1055–1056},
numpages = {2},
keywords = {slr, emse, research publication, collaboration},
location = {Florence, Italy},
series = {WWW '15 Companion}
}

@inproceedings{10.1109/ISCA52012.2021.00013,
author = {Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and O, Seongil and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung},
title = {Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00013},
doi = {10.1109/ISCA52012.2021.00013},
abstract = {Emerging applications such as deep neural network demand high off-chip memory bandwidth. However, under stringent physical constraints of chip packages and system boards, it becomes very expensive to further increase the bandwidth of off-chip memory. Besides, transferring data across the memory hierarchy constitutes a large fraction of total energy consumption of systems, and the fraction has steadily increased with the stagnant technology scaling and poor data reuse characteristics of such emerging applications. To cost-effectively increase the bandwidth and energy efficiency, researchers began to reconsider the past processing-in-memory (PIM) architectures and advance them further, especially exploiting recent integration technologies such as 2.5D/3D stacking. Albeit the recent advances, no major memory manufacturer has developed even a proof-of-concept silicon yet, not to mention a product. This is because the past PIM architectures often require changes in host processors and/or application code which memory manufacturers cannot easily govern. In this paper, elegantly tackling the aforementioned challenges, we propose an innovative yet practical PIM architecture. To demonstrate its practicality and effectiveness at the system level, we implement it with a 20nm DRAM technology, integrate it with an unmodified commercial processor, develop the necessary software stack, and run existing applications without changing their source code. Our evaluation at the system level shows that our PIM improves the performance of memory-bound neural network kernels and applications by 11.2X and 3.5X, respectively. Atop the performance improvement, PIM also reduces the energy per bit transfer by 3.5X, and the overall energy efficiency of the system running the applications by 3.2X.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {43–56},
numpages = {14},
keywords = {neural network, DRAM, processing in memory, accelerator},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1145/2797433.2797474,
author = {Alves, Carina Frota and Bosch, Jan and Jansen, Slinger and Knodel, Jens and Lungu, Mircea and Mens, Tom},
title = {Welcome from the WEA-IWSECO Chairs},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797474},
doi = {10.1145/2797433.2797474},
abstract = {Software ecosystems are becoming increasingly pervasive, leading to new research, management, and engineering challenges. Opening product architectures and platforms for third parties, designing closed innovation networks among partners through APIs and SDKs, coordinating multiple platforms, or participative engineering across organizational borders are all variants of ecosystems that come with their own opportunities and threats at the same time.The 3rd International Workshop on Ecosystem Architectures (WEA 2015) is co-organized with the 7th Workshop on Software Ecosystems (IWSECO 2015). IWSECO and WEA aim to further increase the body of knowledge on software ecosystems by providing a forum to exchange ideas and discuss the most recent innovations, trends and experiences in the field. They aim to build and shape the community of leading practitioners and research experts by providing forums for the exchange of research results and industrial practice in software ecosystems.WEA-IWSECO 2015 is a venue for both practitioners and researchers to discuss problems, solutions and lessons learned related to software development in the context of software ecosystems. The technical program covers a wide range of current research in both, business and social perspective and the technology and architecture perspective of software ecosystems.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {39},
numpages = {2},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1109/ICCPS.2014.6843740,
author = {Zhang, Jiaxing and Qiu, Hanjiao and Shamsabadi, Salar Shahini and Birken, Ralf and Schirner, Gunar},
title = {WiP Abstract: System-Level Integration of Mobile Multi-Modal Multi-Sensor Systems},
year = {2014},
isbn = {9781479949304},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCPS.2014.6843740},
doi = {10.1109/ICCPS.2014.6843740},
abstract = {Heterogeneous roaming sensor systems have gained significant importance in many domains of civil infrastructure performance inspection as they accelerate data collection and analysis. However, designing such systems is challenging due to the immense complexity in the heterogeneity and processing demands of the involved sensors. Unifying frameworks are needed to simplify development, deployment and operation of roaming sensors and computing units.To address the sensing needs, we propose SIROM3, a Scalable Intelligent Roaming Multi-Modal Multi-Sensor framework. SIROM3 incorporates a CPS approach for infrastructure performance monitoring to address the following challenges: 1. Scalability and expandability. It offers a scalable and expandable solution enabling diversity in sensing and the growth in processing platforms from sensors to control centers. 2. Fusion foundations. SIROM3 enables fusion of data collected by logically and geo-spatially distributed sensors. 3. Big data handling. Automatic collection, categorization, storage and manipulation of heterogeneous large volume of data streams. 4. Automation. SIROM3 minimizes human interaction through full automation from data acquisition to visualization of the fused results.Illustrated in Fig. 1, SIROM3 realizes scalability and expandability in a system-level design approach encapsulating common functionality across hierarchical components in a Run-Time Environment (RTE). The RTE deploys a layered design paradigm defining services in both software and hardware architectures. Equipped with multiple RTE-enabled Multi-Sensor Aggregators (MSA), an array of Roaming Sensor Systems (RSS) operate as mobile agents attached to vehicles to provide distributed computing services regulated by Fleet Control and Management (FCM) center via communication network. A series of foundational services including the Precise Timing Protocol (PTP), GPS timing systems, Distance Measurement Instruments (DMI) through middleware services (CORBA) embedded in the RTE build the fusion foundations for data correlation and analysis. A Heterogeneous Stream File-system Overlay (HSFO) alleviates the big data challenge. It facilitates storing, processing, categorizing and fusing large heterogeneous data stream collected by versatile sensors. A GIS visualization module is integrated for visual analysis and monitoring.SIROM3 enables coordination and collaboration across sensors, MSAs and RSSes, which produce high volume of heterogeneous data stored in HSFO. To fuse the data efficiently, SIROM3 contains an expandable plugin system (part of RTE) for rapid algorithm prototyping using data streams in the architectural hierarchy (i.e. from MSAs to FCM) via the HSFO API. This makes an ideal test-bed to develop new algorithms and methodologies expanding CPS principles to civil infrastructure performance monitoring. In result, SIROM3 simplifies the development, construction and operation of roaming multi-modal multi-sensor systems.We demonstrate the efficiency of SIROM3 by automating the assessment of road surface conditions at the city scale. We realized an RSS with 6 MSAs and 30 heterogeneous sensors, including radars, microphones, GPS and cameras, all deployed onto a van sponsored by the VOTERS (Versatile Onboard Traffic Embedded Roaming Sensors) project. Over 20 terabytes of data have been collected, aggregated, fused, analyzed and geo-spatially visualized using SIROM3 for studying the pavement conditions of the city of Brockton, MA covering 300 miles. The expandability of SIROM3 is shown by adding a millimeter-wave radar needing less than 50 lines of C++ code for system integration. SIROM3 offers a unified solution for comprehensive roadway assessment and evaluation. The integrated management of big data (from collection to automated processing) is an ideal research platform for automated assessment of civil infrastructure performance.},
booktitle = {ICCPS '14: ACM/IEEE 5th International Conference on Cyber-Physical Systems (with CPS Week 2014)},
pages = {227},
numpages = {1},
location = {Berlin, Germany},
series = {ICCPS '14}
}

@inproceedings{10.1145/2695664.2695895,
author = {Constantinou, Eleni and Stamelos, Ioannis},
title = {Architectural Stability and Evolution Measurement for Software Reuse},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695895},
doi = {10.1145/2695664.2695895},
abstract = {Software reuse has been established as a development practice due to several benefits like development cost reduction. However, successful reuse depends on several factors, including high level attributes of the reused software. Architectural stability is an important factor for software reuse, either during the reusable asset selection or library upgrades. In this paper, we introduce two sets of metrics that measure the architectural stability and the evolution of software projects in the context of software reuse. The first set of architectural stability metrics measures the degree of consistency between consecutive versions of the same system and considers the common architectural elements. The second set of architectural evolution metrics quantifies the architectural evolution between consecutive versions of the same system and considers the newly introduced architectural elements, as well as how they interact with the remaining elements of the system. Finally, we present the application of the proposed metrics to two categories of software projects: (1) projects developed for reuse and (2) projects that were not originally intended for reuse.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1580–1585},
numpages = {6},
keywords = {architecture stability, measurement, metrics, architecture evolution},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/996566.996623,
author = {Shenoy, Narendra V. and Kawa, Jamil and Camposano, Raul},
title = {Design Automation for Mask Programmable Fabrics},
year = {2004},
isbn = {1581138288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/996566.996623},
doi = {10.1145/996566.996623},
abstract = {Programmable circuit design has played an important role in improving design productivity over the last few decades. By imposing structure on the design, efficient automation of synthesis, placement and routing is possible. We focus on a class of programmable circuits known as mask programmable circuits. In this paper, we describe key issues in design and tool methodology that need to be addressed in creating a programmable fabric. We construct an efficient design flow that can explore different logic and routing architectures. The main advantage of our work is that we tailor tools designed for standard cell design, that are readily available in the market, to work on a programmable fabric. Our flow requires some additional software capability. A special router that understands programmable routing constructs to complete connections is described. In addition, a tool that packs logic efficiently after synthesis is also presented.},
booktitle = {Proceedings of the 41st Annual Design Automation Conference},
pages = {192–197},
numpages = {6},
keywords = {mask programmable fabrics, integrated circuits},
location = {San Diego, CA, USA},
series = {DAC '04}
}

@inproceedings{10.1145/3365438.3410965,
author = {Wood, Sophie and Matragkas, Nicholas and Kolovos, Dimitris and Paige, Richard and Gerasimou, Simos},
title = {Supporting Robotic Software Migration Using Static Analysis and Model-Driven Engineering},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410965},
doi = {10.1145/3365438.3410965},
abstract = {The wide use of robotic systems contributed to developing robotic software highly coupled to the hardware platform running the robotic system. Due to increased maintenance cost or changing business priorities, the robotic hardware is infrequently upgraded, thus increasing the risk for technology stagnation. Reducing this risk entails migrating the system and its software to a new hardware platform. Conventional software engineering practices such as complete re-development and code-based migration, albeit useful in mitigating these obsolescence issues, they are time-consuming and overly expensive. Our RoboSMi model-driven approach supports the migration of the software controlling a robotic system between hardware platforms. First, RoboSMi executes static analysis on the robotic software of the source hardware platform to identify platform-dependent and platform-agnostic software constructs. By analysing a model that expresses the architecture of robotic components on the target platform, RoboSMi establishes the hardware configuration of those components and suggests software libraries for each component whose execution will enable the robotic software to control the components. Finally, RoboSMi through code-generation produces software for the target platform and indicates areas that require manual intervention by robotic engineers to complete the migration. We evaluate the applicability of RoboSMi and analyse the level of automation and performance provided from its use by migrating two robotic systems deployed for an environmental monitoring and a line following mission from a Propeller Activity Board to an Arduino Uno.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {154–164},
numpages = {11},
keywords = {robotic systems, static analysis, software migration, model-driven engineering},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3544902.3546246,
author = {Cui, Di and Li, Xingyu and Liu, Feiyang and Wang, Siqi and Dai, Jie and Wang, Lu and Li, Qingshan},
title = {Towards Demystifying the Impact of Dependency Structures on Bug Locations in Deep Learning Libraries},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546246},
doi = {10.1145/3544902.3546246},
abstract = {Background: Many safety-critical industrial applications have turned to deep learning systems as a fundamental component. Most of these systems rely on deep learning libraries, and bugs of such libraries can have irreparable consequences. Aims: Over the years, dependency structure has shown to be a practical indicator of software quality, widely used in numerous bug prediction techniques. The problem is that when analyzing bugs in deep learning libraries, researchers are unclear whether dependency structures still have a high correlation and which forms of dependency structures perform the best. Method: In this paper, we present a systematic investigation of the above question and implement a dependency structure-centric bug analysis tool: Depend4BL, capturing the interaction between dependency structures and bug locations in deep learning libraries. Results: We employ Depend4BL to analyze the top 5 open-source deep learning libraries on Github in terms of stars and forks, with 279,788 revision commits and 8,715 bug fixes. The results demonstrate the significant differences among syntactic, history, and semantic structures, and their vastly different impacts on bug locations. Their combinations have the potential to further improve bug prediction for deep learning libraries. Conclusions: In summary, our work provides a new perspective regarding to the correlation between dependency structures and bug locations in deep learning libraries. We release a large set of benchmarks and a prototype toolkit to automatically detect various forms of dependency structures for deep learning libraries. Our study also unveils useful findings based on quantitative and qualitative analysis that benefit bug prediction techniques for deep learning libraries.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {249–260},
numpages = {12},
keywords = {Bug Prediction., Dependency Structure, Deep Learning System},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1145/325737.325788,
author = {Farrell, Robert and Fairweather, Peter and Breimer, Eric},
title = {A Task-Based Architecture for Application-Aware Adjuncts},
year = {2000},
isbn = {1581131348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/325737.325788},
doi = {10.1145/325737.325788},
abstract = {Users of complex applications need advice, assistance, and feedback while they work. We are experimenting with “adjunct” user agents that are aware of the history of interaction surrounding the accomplishment of a task. This paper describes an architectural framework for constructing these agents. Using this framework, we have implemented a critiquing system that can give task-oriented critiques to trainees while they use operating system tools and software applications. Our approach is generic, widely applicable, and works directly with off-the-shelf software packages.},
booktitle = {Proceedings of the 5th International Conference on Intelligent User Interfaces},
pages = {82–85},
numpages = {4},
keywords = {architecture, task model, plan recognition, agent, adjunct, graphical user interface, event, critic},
location = {New Orleans, Louisiana, USA},
series = {IUI '00}
}

@inproceedings{10.1145/940071.940109,
author = {Xie, Fei and Browne, James C.},
title = {Verified Systems by Composition from Verified Components},
year = {2003},
isbn = {1581137435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/940071.940109},
doi = {10.1145/940071.940109},
abstract = {This paper presents an approach to integration of model checking into component-based development of software systems. This approach assists in development of highly reliable component-based software systems and reduces the complexity of verifying these systems by utilizing their compositional structures. Temporal properties of a software component are specified, verified, and packaged with the component. Selection of a component for reuse considers not only its functionality but also its temporal properties. When a component is composed from other components, a property of the component is verified on an abstraction of the component. The abstraction is constructed from environment assumptions of the component and verified properties of its sub-components. A general component model that enables component verification is defined. Component verification is discussed in the context of the instantiation of the general component model on an Asynchronous Interleaving Message-passing computation model. This approach has been applied to improve reliability of instances of TinyOS, a component-based run-time system for networked sensors. A case study on TinyOS is included, which illustrates the applicability of this approach, the detection of a bug, and the reduction in model checking complexity.},
booktitle = {Proceedings of the 9th European Software Engineering Conference Held Jointly with 11th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {277–286},
numpages = {10},
keywords = {verification, composition, component, model checking, abstraction},
location = {Helsinki, Finland},
series = {ESEC/FSE-11}
}

@article{10.1145/949952.940109,
author = {Xie, Fei and Browne, James C.},
title = {Verified Systems by Composition from Verified Components},
year = {2003},
issue_date = {September 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/949952.940109},
doi = {10.1145/949952.940109},
abstract = {This paper presents an approach to integration of model checking into component-based development of software systems. This approach assists in development of highly reliable component-based software systems and reduces the complexity of verifying these systems by utilizing their compositional structures. Temporal properties of a software component are specified, verified, and packaged with the component. Selection of a component for reuse considers not only its functionality but also its temporal properties. When a component is composed from other components, a property of the component is verified on an abstraction of the component. The abstraction is constructed from environment assumptions of the component and verified properties of its sub-components. A general component model that enables component verification is defined. Component verification is discussed in the context of the instantiation of the general component model on an Asynchronous Interleaving Message-passing computation model. This approach has been applied to improve reliability of instances of TinyOS, a component-based run-time system for networked sensors. A case study on TinyOS is included, which illustrates the applicability of this approach, the detection of a bug, and the reduction in model checking complexity.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {277–286},
numpages = {10},
keywords = {composition, component, abstraction, verification, model checking}
}

@inproceedings{10.1145/2577080.2577097,
author = {Zanetti, Marcelo Serrano and Tessone, Claudio Juan and Scholtes, Ingo and Schweitzer, Frank},
title = {Automated Software Remodularization Based on Move Refactoring: A Complex Systems Approach},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577097},
doi = {10.1145/2577080.2577097},
abstract = {Modular design is a desirable characteristic of complex software systems that can significantly improve their comprehensibility, maintainability and thus quality. While many software systems are initially created in a modular way, over time modularity typically degrades as components are reused outside the context where they were created. In this paper, we propose an automated strategy to remodularize software based on move refactoring, i.e. moving classes between packages without changing any other aspect of the source code. Taking a complex systems perspective, our approach is based on complex networks theory applied to the dynamics of software modular structures and its relation to an n-state spin model known as the Potts Model.In our approach, nodes are probabilistically moved between modules with a probability that nonlinearly depends on the number and module membership of their adjacent neighbors, which are defined by the underlying network of software dependencies. To validate our method, we apply it to a dataset of 39 Java open source projects in order to optimize their modularity. Comparing the source code generated by the developers with the optimized code resulting from our approach, we find that modularity (i.e. quantified in terms of a standard measure from the study of complex networks) improves on average by 166+-77 percent. In order to facilitate the application of our method in practical studies, we provide a freely available Eclipse plug-in.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {73–84},
numpages = {12},
keywords = {remodularization, refactoring, complex networks},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@article{10.1145/2567652,
author = {Kim, Min H. and Rushmeier, Holly and Ffrench, John and Passeri, Irma and Tidmarsh, David},
title = {Hyper3D: 3D Graphics Software for Examining Cultural Artifacts},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/2567652},
doi = {10.1145/2567652},
abstract = {Art conservators now have access to a wide variety of digital imaging techniques to assist in examining and documenting physical works of art. Commonly used techniques include hyperspectral imaging, 3D scanning, and medical computed tomography imaging. However, viewing most of this digital image data frequently requires both specialized software, which is often associated with a particular type of acquisition device, and professional knowledge of and experience with each type of data. In addition, many of these software packages are focused on particular applications (such as medicine or remote sensing) and do not permit users to access and fully exploit all the information contained in the data. In this paper, we address two practical barriers to using high-tech digital data in art conservation. First, users must deal with a wide variety of interfaces specialized for applications besides conservation. We provide an open-source software tool with a single intuitive interface consistent with conservators' needs that handles various types of 2D and 3D image data and preserves user-generated metadata and annotations. Second, previous software has largely allowed visualizing a single type or only a few types of data. The software we present is designed and structured to accommodate multiple types of digital imaging data, including as yet unspecified or unimplemented formats, in an integrated environment. This allows conservators to access different forms of information and to view a variety of image types simultaneously.},
journal = {J. Comput. Cult. Herit.},
month = {feb},
articleno = {14},
numpages = {19},
keywords = {visualization, Open-source software, cultural heritage}
}

@inproceedings{10.1109/E2SC.2014.11,
author = {Nandamuri, Anilkumar and Malik, Abid M. and Qawasmeh, Ahmad and Chapman, Barbara M.},
title = {Power and Energy Footprint of OpenMP Programs Using OpenMP Runtime API},
year = {2014},
isbn = {9781479970360},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/E2SC.2014.11},
doi = {10.1109/E2SC.2014.11},
abstract = {Power and energy have become dominant aspects of hardware and software design in the High Performance Computing (HPC). Recently, the Department of Defense (DOD) has put a constraint that applications and architectures need to attain 75 GFLOPS/Watt in order to support the future missions. This requires a significant research effort towards power and energy optimization. OpenMP programming model is an integral part of HPC. Comprehensive analysis of OpenMP programs for power and execution performance is an active research area. Work has been done to characterize OpenMP programs with respect to power performance at kernel level. However, no work has been done at the OpenMP event level. OpenMP Runtime API (ORA), proposed by the OpenMP standard committee, allow a performance tool to collect information at the OpenMP event level. In this paper, we present a comprehensive analysis of the OpenMP programs using ORA for power and execution performance. Using hardware counters in the Intel SandyBridge x86-64 and Running Average Power Limit (RAPL) energy sensors, we measure power and energy characteristics of OpenMP benchmarks. Our results show that the best execution performance does not always give the best energy usage. We also find out that the waiting time at the barriers and in queue are the main factors for high power consumption for a given OpenMP program. Our results also show that there are unique patterns at the fine level that can be used by the dynamic power management system to enhance the power performance. Our results show substantial variation in energy usage depending upon the runtime environment.},
booktitle = {Proceedings of the 2nd International Workshop on Energy Efficient Supercomputing},
pages = {79–88},
numpages = {10},
keywords = {power, energy, runtime API, performance analysis, OpenMP},
location = {New Orleans, Louisiana},
series = {E2SC '14}
}

@inproceedings{10.1145/1292331.1292345,
author = {Wellens, Matthias and Riihij\"{a}rvi, Janne and Rerkrai, Krisakorn and Bandholz, Marten and M\"{a}h\"{o}nen, Petri},
title = {Enabling Seamless Vertical Handovers Using Unified Link-Layer API},
year = {2006},
isbn = {1595935193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1292331.1292345},
doi = {10.1145/1292331.1292345},
abstract = {Wireless channels vary drastically over time and mobility adds further difficulties to the engineering problem of providing stable and robust communication performance. Handovers occur regularly in mobile environments and the increasing heterogeneity of present networks leads to the requirement for seamless vertical handovers. An important practical problem is the lack of a generic interface that enables applications, such as a vertical handover decision engine, to retrieve comparable link-layer information independently of the deployed technology or the used software and hardware platform. In this paper we propose the Unified Link-Layer API (ULLA) as a solution to this problem. We describe its architecture, how it can be utilized for vertical handover scenarios and introduce our Linux prototype. The prototype performance proves that ULLA is appropriate for mobility management.},
booktitle = {Proceedings of the 3rd International Conference on Mobile Technology, Applications &amp; Systems},
pages = {12–es},
location = {Bangkok, Thailand},
series = {Mobility '06}
}

@inproceedings{10.1145/1094855.1094862,
author = {Booch, Grady},
title = {On Creating a Handbook of Software Architecture},
year = {2005},
isbn = {1595931937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1094855.1094862},
doi = {10.1145/1094855.1094862},
abstract = {It is a sign of maturity for any given engineering discipline when we can name, study, and apply the patterns relevant to that domain. In civil engineering, chemical engineering, mechanical engineering, electrical engineering, and now even genomic engineering, there exist libraries of common patterns that have proven themselves useful in practice. Unfortunately, no such architectural reference yet exists for software-intensive systems. Although the patterns community has pioneered the vocabulary of design patterns through the work of the Hillside Group and the Gang of Four, our industry has no parallel to the architecture handbooks found in more mature design disciplines.Following the work of Bruce Anderson, who over a decade ago conducted a series of workshops at OOPSLA, I've begun an effort to fill this void in software engineering by codifying a the architecture of a large collection of interesting software-intensive systems, presenting them in a manner that exposes their essential patterns and that permits comparison across domains and architectural styles.In this presentation, we'll examine the nature of architectural patterns and the process of conducting architectural digs to harvest them, and then examine a few of the systems studied thus far.},
booktitle = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {8},
numpages = {1},
location = {San Diego, CA, USA},
series = {OOPSLA '05}
}

@inproceedings{10.1145/3550356.3563130,
author = {Chiang, Thomas and Mendoza, Rodrigo Gomez and Mahmood, Johan and Paige, Richard F.},
title = {Towards the Adoption of Model Based System Safety Engineering in the Automotive Industry},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3563130},
doi = {10.1145/3550356.3563130},
abstract = {Model-Driven Engineering techniques are becoming increasingly common for use in automotive software engineering, particularly to enable architectural modeling as well as safety analysis, especially fault tree analysis (FTA). One common MDE tool is Medini Analyze from Ansys, which has many tool suites and interfaces that allow for building complex models and performing safety analysis on them. However it can be tedious, error-prone and repetitive to have to construct models by hand. In this paper we introduce strategies for adopting Medini in industry, streamlining the model generation process, and enabling engineers to focus more on safety analysis. The strategies introduced leverage the already existing technologies in Medini, specifically the Javascript API that it enables, as well as interfaces with Microsoft Excel.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {579–587},
numpages = {9},
keywords = {model driven engineering, software safety, FTA, safety engineering, assurance, automotive, model generation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/1940761.1940874,
author = {Knox, Emily},
title = {A Confirmatory Factor Analysis of Library Use},
year = {2011},
isbn = {9781450301213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1940761.1940874},
doi = {10.1145/1940761.1940874},
abstract = {For the past 25 years, public library administrators have pondered their institution's relationship to the computer. Since the 1980s, the integrated library system, which is based on a server-client model, has been a ubiquitous part of the library. These systems, however, are solely for use by the library staff. There have long been computers available for patrons but for many years these only provided access to the online catalog and databases on CD-ROM. Over time more applications were added to the patron computers including integrated office suites and web browsers. Public computers for patrons with access to the Internet and loaded with various productivity applications are now available in almost all public libraries across the country.At the same time that personal computers were proliferating, in both the private and public sphere more knowledge content became available digitally. Materials that were once available only in print, including books, periodicals, and databases, can now be accessed through the Internet. It is not surprising that it has been difficult for libraries to adapt to this change. An institution that was originally established to collect print materials now has to offer access to knowledge in many different formats.This study investigates one aspect of this change. The concept of "library use" has changed with the proliferation of digital media. Libraries of all types and the various administrative boards that control their funding allocated revenue budgets solely on circulation statistics. That is, the amount of money given to the library was based on how many books were checked out in a given year. This statistic does not adequately account for the actual services that libraries provide. In 2009, the editors of Library Journal [1] sponsored research for the development of a new scale measurement for rating library services based on four indicators.This study is a confirmatory factor analysis of this scale in order to validate this new measurement model.The initial data consisted of three hundred three (N =303) from the 2008 New Jersey Public Library Statistics [2]. 23 libraries did not respond to the survey, leaving a total of two hundred eighty libraries in the data set (n=280). These data are collected every year by the New Jersey State Library and are freely available on the institution's website (www.njstatelib.org). The New Jersey State Library collects these data via an electronic survey. An additional 38 libraries were removed because their scores fell more than three standard deviations beyond the mean on any one of the variables that were tested. Eight libraries were removed because they were multivariate outliers. This resulted in a sample size of two hundred thirty-four public libraries in New Jersey (n = 234).All questions on the survey (except for identification information) required whole number (ratio level) responses from the libraries. Items regarding revenue and expenses required whole dollar amounts. Computer Readable Materials Budget refers to the total amount spent on software, electronic books, and other items that must be used on a computer. Databases Owned indicates the total number of licensed databases for which the library pays. Libraries also indicated the number of computers available for public use. The survey also included two categorical questions. One asked whether or not the library made password free Wi-Fi available to the general public. The other asked whether or not the library made JerseyClicks, a full-text search portal funded by the state, available on their website.According to the new Library Journal Index mentioned above, four indicators are used to construct the library use score: library visits, circulation, program attendance, and public Internet computer use. Library visits refers to the total number of people who enter the library for any purpose. Circulation indicates the total number of "check-outs" for all materials including any renewals. It does not include virtual circulation or interlibrary loan. The total number of people at all programs either sponsored or hosted by the library is indicated by program attendance. Finally, public Internet computer use refers to the number of individuals who used public accessible computer terminals in a given year. These indicators were transformed into per-capita data by dividing each reported amount by the population of the library's municipality as indicated in the 2000 Census. Each number was then transformed into a z-score.A confirmatory factor analysis was conducted to assess the fit between the NJPL data and proposed factor structure for the Library Use Index proposed in Library Journal. The chi-square value for the overall model fit was not significant (χ2 = .720, df = 2, p &gt;.001) indicating that there is an adequate fit between the factor structure of the Library Use Index and the data.A "library use" score was then computed using the sum of the four standardized indicators. In accordance with the procedure given in Library Journal [3], since this preliminary score included scores with negative values (with a minimum value of -4.99), a correction factor of 5 was added to each score so that the variable Library Use Score would not include negative values. The Library Use Score for the libraries was M = 4.97, SD = 3.26 with a range of .007 to 15.981. The Library Use Score provides a single measure that can be used to compare individual public libraries with peer institutions.},
booktitle = {Proceedings of the 2011 IConference},
pages = {697–698},
numpages = {2},
location = {Seattle, Washington, USA},
series = {iConference '11}
}

@inproceedings{10.5555/962367.962397,
author = {Arjomandi, Eshrat and Kalas, Ivan and O'Farrell, William},
title = {Concurrency Abstractions in a C++ Class Library},
year = {1993},
publisher = {IBM Press},
abstract = {C++ is increasingly becoming the language of choice among software developers. It is therefore not surprising to see the many attempts that have been made to add concurrency to C++. Concurrency can be added to C++ either by extending the language or through the use of a class library.There is much debate among language designers about the approach to take in the integration of two paradigms of concurrency and object-oriented programming (OOP). The library approach keeps the language small, allows the programmer to work with familiar compilers and tools, provides the option of sup - porting many concurrent models through a variety of libraries, and eases porting of code to other architectures (usually, a small amount of assembler code needs to be changed). Software developers typically have large investments in existing code and are reluctant to adopt a new language. A class library with sufficient flexibility that can provide most of the functionality of a new or extended language is often more palatable. On the other hand, new or extended languages can use the compiler to provide higher-level constructs, compile-time type checking, and enhanced performance.The Parallel Computing Project at the Centre for Advanced Studies has been researching the possibility of adding concurrency to C++ with the assistance of a class library. The library consists of two layers. The bottom layer provides a set of low level concurrency primitives. The top layer, ABC++, uses the bottom layer to provide implicit parallelism. To our knowledge, ABC++ is the only concurrent class library that provides almost all of the functionality of extended languages without requiring the users to deal with explicit concurrency control and without imposing severe limitations. This paper outlines the progress of the project, the low level concurrency primitives and describes how ABC++ differs from other existing concurrent class libraries for C++.},
booktitle = {Proceedings of the 1993 Conference of the Centre for Advanced Studies on Collaborative Research: Distributed Computing - Volume 2},
pages = {919–932},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '93}
}

@article{10.1145/3577934.3577940,
author = {Gasser, Ralph and Rossetto, Luca and Heller, Silvan and Schuldt, Heiko},
title = {Multimedia Retrieval and Analysis with Cottontail DB},
year = {2022},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
url = {https://doi.org/10.1145/3577934.3577940},
doi = {10.1145/3577934.3577940},
abstract = {Analysis and retrieval of media collections get more and more challenging the larger the collections become. Keeping everything in the main memory becomes less feasible, and more and more time and effort have to be spent to deal with the data management. However, traditional relational databases do not support primitives often used in multimedia workloads, such as the nearest-neighbour search on vectors. In this column, we introduce Cottontail DB, an open-source database management system for multimedia features. Cottontail DB supports traditional relational database operations and text retrieval based on Lucene and, most importantly, efficient vector-space retrieval operations for large datasets. Cottontail DB is the new data storage system powering the vitrivr multimedia retrieval stack, which was also previously featured in the SIGMM Records [7]. Just like the other components of vitrivr, Cottontail DB is released under the permissive MIT license. It is written in Kotlin, runs on all major operating systems, and comes with a flexible and easy-to-use gRPC API, which makes it usable in many applications, independent of the programming languages used. Cottontail DB's clean and modular architecture enables the easy extension of its functionalities and also makes it useful in an educational context. In the following, we will give a brief introduction on how Cottontail DB works, what we are using it for, and, most importantly, how it can help you manage your data. To learn more about Cottontail DB, including performance evaluations, we kindly refer readers to our Open Source Software Track Contribution at ACM MM 2020 [1], where Cottontail DB was honored with that year's Best Open Source Award.},
journal = {SIGMultimedia Rec.},
month = {dec},
articleno = {6},
numpages = {1}
}

@inproceedings{10.1145/1289971.1289976,
author = {Gil, Joseph (Yossi) and Lenz, Keren},
title = {Simple and Safe SQL Queries with C++ Templates},
year = {2007},
isbn = {9781595938558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1289971.1289976},
doi = {10.1145/1289971.1289976},
abstract = {Most software applications use a relational database for data management and storage. Interaction with such a database is often done by letting the program construct strings with valid SQL statements, which are then sent for execution to the database engine. The fact that these statements are only checked for correctness at runtime is a source for many potential problems such as type and syntax errors and vulnerability to injection attacks.The ARARAT system presented here offers a method for dealing with these predicaments, by coercing the host C++ compiler to do the necessary checks of the generated strings. A library of templates and preprocessor directives effectively extends C++ with a little language representing an augmented relational algebra formalism. Type checking of this language extension, carried out by our template library, assures, at compile-time, the correctness and safety of the generated SQL strings. That is to say that all SQL statements constructed by the system are syntactically correct, legal with respect to the database schema, and immune to injection attacks.Standard techniques (e.g., "expression templates") for compile time representation of symbolic structures, are enhanced in our system to support a type system (featuring structural equivalence) and a symbol table lookup of the symbolic structure. Our work may also open the way for embedding other domain specific languages in C++.The system provides also initial support for the task of defining C++ data structures required for storing the results returned by database queries. An optional pre-processor can be used to define the database scheme to the C++ program.},
booktitle = {Proceedings of the 6th International Conference on Generative Programming and Component Engineering},
pages = {13–24},
numpages = {12},
keywords = {domain specific languages, C++, template programming, embedded languages, relational algebra, structural type equivalence, databases},
location = {Salzburg, Austria},
series = {GPCE '07}
}

@inproceedings{10.1145/1159842.1159851,
author = {Diatchki, Iavor S. and Jones, Mark P.},
title = {Strongly Typed Memory Areas Programming Systems-Level Data Structures in a Functional Language},
year = {2006},
isbn = {1595934898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159842.1159851},
doi = {10.1145/1159842.1159851},
abstract = {Modern functional languages offer several attractive features to support development of reliable and secure software. However, in our efforts to use Haskell for systems programming tasks-including device driver and operating system construction-we have also encountered some significant gaps in functionality. As a result, we have been forced, either to code some non-trivial components in more traditional but unsafe languages like C or assembler, or else to adopt aspects of the foreign function interface that compromise on strong typing and type safety.In this paper, we describe how we have filled one of these gaps by extending a Haskell-like language with facilities for working directly with low-level, memory-based data structures. Using this extension, we are able to program a wide range of examples, including hardware interfaces, kernel data structures, and operating system APIs. Our design allows us to address concerns about representation, alignment, and placement (in virtual or physical address spaces) that are critical in some systems applications, but clearly beyond the scope of most existing functional languages.Our approach leverages type system features that are wellknown and widely supported in existing Haskell implementations, including kinds, multiple parameter type classes, functional dependencies, and improvement. One interesting feature is the use of a syntactic abbreviation that makes it easy to define and work with functions at the type level.},
booktitle = {Proceedings of the 2006 ACM SIGPLAN Workshop on Haskell},
pages = {72–83},
numpages = {12},
keywords = {improvement, data representation, qualified types, systems programming, memory areas, memory manipulation},
location = {Portland, Oregon, USA},
series = {Haskell '06}
}

@inproceedings{10.1145/3149457.3149475,
author = {Endo, Wataru and Taura, Kenjiro},
title = {Parallelized Software Offloading of Low-Level Communication with User-Level Threads},
year = {2018},
isbn = {9781450353724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149457.3149475},
doi = {10.1145/3149457.3149475},
abstract = {Although recent HPC interconnects are assumed to achieve low latency and high bandwidth communication, in practical terms, their performance is often bounded by the network software stacks rather than the underlying hardware because message processing requires a certain amount of computation in CPUs. To exploit the hardware capacity, some existing communication libraries provide an interface for parallelizing accesses to network endpoints with manual hints. However, with growing core counts per node in modern clusters, it is increasingly difficult for users to efficiently handle communication resources in multi-threading environments.We implemented a low-level communication library that can automatically schedule communication requests by offloading them to multiple dedicated threads via lockless circular buffers. To enhance the efficiency of offloading, we developed a novel technique to dynamically change the number of offloading threads using a user-level thread library. We demonstrate that our offloading architecture exhibits better performance characteristics in microbenchmark results than the existing approaches.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {289–298},
numpages = {10},
location = {Chiyoda, Tokyo, Japan},
series = {HPC Asia 2018}
}

@inproceedings{10.1145/3468737.3494089,
author = {Shi, Yaying and Wang, Anjia and Yan, Yonghong and Liao, Chunhua},
title = {RDS: A Cloud-Based Metaservice for Detecting Data Races in Parallel Programs},
year = {2021},
isbn = {9781450385640},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468737.3494089},
doi = {10.1145/3468737.3494089},
abstract = {Data races are notorious concurrency bugs which can cause severe problems, including random crashes and corrupted execution results. However, existing data race detection tools are still challenging for users to use. It takes a significant amount of effort for users to install, configure and properly use a tool. A single tool often cannot find all the bugs in a program. Requiring users to use multiple tools is often impracticable and not productive because of the differences in tool interfaces and report formats.In this paper, we present a cloud-based, service-oriented design and implementation of a race detection service (RDS)1 to detect data races in parallel programs. RDS integrates multiple data race detection tools into a single cloud-based service via a REST API. It defines a standard JSON format to represent data race detection results, facilitating producing user-friendly reports, aggregating output of multiple tools, as well as being easily processed by other tools. RDS also defines a set of policies for aggregating outputs from multiple tools. RDS significantly simplifies the workflow of using data race detection tools and improves the report quality and productivity of performing race detection for parallel programs. Our evaluation shows that RDS can deliver more accurate results with much less effort from users, when compared with the traditional way of using any individual tools. Using four selected tools and DataRaceBench, RDS improves the Adjusted F-1 scores by 8.8% and 12.6% over the best and the average scores, respectively. For the NAS Parallel Benchmark, RDS improves 35% of the adjusted accuracy compared to the average of the tools.Our work studies a new approach of composing software tools for parallel computing via a service-oriented architecture. The same approach and framework can be used to create metaservice for compilers, performance tools, auto-tuning tools, and so on.},
booktitle = {Proceedings of the 14th IEEE/ACM International Conference on Utility and Cloud Computing},
articleno = {18},
numpages = {10},
keywords = {microservices, metaservices, parallel computing, data race detection, cloud service API},
location = {Leicester, United Kingdom},
series = {UCC '21}
}

@inproceedings{10.1109/ASE.2003.1240302,
author = {Morel, Brandon and Alexander, Perry},
title = {Automating Component Adaptation for Reuse},
year = {2003},
isbn = {0769520359},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2003.1240302},
doi = {10.1109/ASE.2003.1240302},
abstract = {Reuse is a sound and practical design technique in many engineering disciplines. Although successful instances of software reuse are becoming more common, the cost of reuse tends to outweigh the potential benefits. The costs of software reuse include establishing and maintaining a library of reusable components, searching for applicable components to be reused, as well as adapting components toward a solution to a design problem. In this paper we present a framework, called SPARTACAS, for automating specification-based component retrieval and adaptation. Components that partially satisfy the constraints of a design problem are adapted using adaptation architectures. Adaptation architectures modify the behavior of a software component by imposing interactions with other components. Based on the functionality specified in the problem and the partially-matched component, a sub-problem that specifies the missing functionality is synthesized. The subproblem is used to query the library for components for adaptation. The framework was implemented and evaluated empirically, the results suggest that automated adaptation using architectures successfully promotes software reuse, and hierarchically organizes a solution to a design problem.},
booktitle = {Proceedings of the 18th IEEE International Conference on Automated Software Engineering},
pages = {142–151},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {ASE'03}
}

@inproceedings{10.1145/67386.67410,
author = {Tomlinson, C. and Kim, W. and Scheevel, M. and Singh, V. and Will, B. and Agha, G.},
title = {Rosette: An Object-Oriented Concurrent Systems Architecture},
year = {1988},
isbn = {0897913043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/67386.67410},
doi = {10.1145/67386.67410},
abstract = {A number of concurrent computers have been designed, such as [Ametek 1988] and [NCUBE 1986]. Taking advantage of concurrency on these computers will require new languages, operating systems, and environments to support the programming, monitoring and control of applications. The Rosette project at MCC is developing a system architecture for general-purpose concurrent computation based on the Actor model [Agha 1986]. The aim of the Rosette architecture is to support the use of concurrent computers in solving computationally intensive problems whose structure is not statically determined. The architecture will support variable grain size, dynamic resource management, reflection, and heterogeneity.A number of concurrent language proposals have employed computational models that are limited in their applicability because of assumptions such as granularity of concurrency, static process topology, or inherent communications cost. Others, for example, functional programming, cannot be used to model shared, modifiable objects. The Actor model addresses these issues directly.However, the difficulties of using concurrent computers cannot be addressed solely through programming languages. Traditional operating systems treat each process as an independent entity for the purposes of resource management (i.e., for the run-time allocation of processor, storage, and communication resources.) In the context of concurrent applications, effective operation on a concurrent computer requires that communities of processes be managed together, taking into account the ways in which they interact, and providing for the application itself to guide resource management.For example, consider that programming systems often provide only limited support for the control of resource consumption in search problems in the face of combinatorial explosion; further, it is generally recognized that to optimize search performance, it is useful to vary the resources provided depending on how promising intermediate results of a given path are judged to be. The Rosette architecture will allow the programmer to integrate high-level algorithms within the application for dividing resources dynamically among the sub-computations that are spawned. These algorithms will provide an assessment of the progress of the sub-computations, and will be used to make the large number of low-level resource management decisions that are necessary in carrying out the computation. Thus, operating system functionality will be folded into the application.The Rosette architecture has two major components: an interface layer and a system environment. The interface layer provides mechanisms for the monitoring and control of concurrent applications. It supports a uniform view of the system resources and applications through the use of actors and message-passing semantics. The interface layer includes a set of actors that represent the processing, storage and communication resources across the hardware base. The system environment contains actor communities which implement resource management policies, and support for programming concurrent applications. The environment consists of an extensible set of actors that are organized via a class-based object model. These actors provide monitoring, debugging, resource management system simulation, and compilation/transformation facilities.To demonstrate the effectiveness of the Rosette architecture, as well as to drive its development, two classes of applications are being used. The first class is the Rosette system itself: all of the system components necessary for resource management, monitoring, debugging, and program development are being constructed in the Rosette language and will take full advantage of both concurrency and object-oriented programming support. The second category of applications is chosen to be somewhat independent of the Rosette system so as to ensure adequate feed-back concerning applicability to domains other than software engineering. These applications are in the general area of knowledge-based signal interpretation.The Rosette architecture is implemented via a concurrent object-oriented programming language that provides the following features: Inherent concurrency: A declarative approach is needed which allows fine-grained concurrency to arise naturally from the program. If the programmer is forced to express all of the details of the concurrent execution of an application, it becomes difficult to re-map a given program to different concurrent computers which have varying concurrency characteristics.Dynamic creation/modification of objects: This allows the system to be extensible and reconfigurable.Sharing: The language should support mechanisms such as inheritance so that code, structure, and values need not be replicated.Classes: Objects can be organized into classes to permit more effective management of resources, as for example in a very large database. Classes should be first-class objects and hence it should be possible to create classes dynamically.The Rosette language is a high-level language that is translated into a base language that directly expresses concurrent computations in terms of the semantics of the Actor model. The Rosette language extends the base language with data abstraction facilities and a library of classes, and is used to implement the non-built-in actors of the interface layer, components of the system environment, and applications.Using data abstraction extensions to the base language, the Rosette language organizes actors into classes according to common sets of behaviors and local state. Inheritance is used as a means of sharing code and structure that is common among classes. There is considerable evidence, for example [LaLonde 1986] and [Snyder 1987], that a robust class system should support separate hierarchies that represent what is modeled by a class and how some class of objects are actually implemented. Our approach is to use three different kinds of classes to organize the various kinds of actors in a system and information about these actors: abstract classes specify observable requests, responses, and actions;representation classes specify resource management characteristics of implementations; andbehavior classes contain the actual implementations.Associated with an abstract class is the set of representations that are available to implement the particular abstraction. Each representation is associated with the behavior class that contains the corresponding implementation. The power of this separation is that each of the three kinds of class may be related to others in a class hierarchy. This means that the relationships between abstract classes are not forced to have the same structure as the relationships among implementations. Sharing via inheritance among behavior classes is related to engineering concerns; relations among abstract classes are dictated by external/observable behavior. As an example, an abstract class might specify the operations available over real-matrices. There are several ways of representing a matrix as a community of actors, and each of these would be specified with appropriate representation and behavior classes. There is a well-defined abstract hierarchy of matrices such as invertible, positive, etc. These abstract kinds of matrices are independent of the various implementations, which may be chosen according to application and hardware resource constraints.The Rosette language embodies a reflective model of the interface layer that is similar to that used in ABCL/R, [Watanabe and Yonezawa 1988]. This model is termed the execution model and provides an abstract implementation of an actor in terms of other resource actors. There are three classes of resource actors in the model: container, processor, and mailbox.The container class of resource actor models the storage local to an actor, and is very similar to a frame or unit in knowledge-based systems. A container is a set of slots, where each slot is an association of a key and a value. Both keys and values are just other actors. Slots can be added or deleted from a container. These actions model allocation and deallocation of storage.The basic operation performed by processor resource actors is the execution of a message, which defines how to determine the method for responding to a message. An evaluation context is then created and the method is applied to the context and the message.The third class of resource actors implements the communications interface to an actor. The main function of a mailbox is the buffering of incoming messages until they are requested by a processor. Mailboxes respond to requests that signify message arrivals at the actor and to requests which are used by processors to accept messages.Object-oriented interfaces for each of the three kinds of resource actors are defined and organized in a class hierarchy. For any actor, any combination of these three resource actors may be built-in or not. The possibility of defining new resource actors within the context of the execution model makes the Rosette architecture extensible - allowing new forms of monitoring and control to be defined. Resource management policies are defined in terms of interactions among the resource actors of an application's actor community.},
booktitle = {Proceedings of the 1988 ACM SIGPLAN Workshop on Object-Based Concurrent Programming},
pages = {91–93},
numpages = {3},
location = {San Diego, California, USA},
series = {OOPSLA/ECOOP '88}
}

@article{10.1145/67387.67410,
author = {Tomlinson, C. and Kim, W. and Scheevel, M. and Singh, V. and Will, B. and Agha, G.},
title = {Rosette: An Object-Oriented Concurrent Systems Architecture},
year = {1988},
issue_date = {April 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/67387.67410},
doi = {10.1145/67387.67410},
abstract = {A number of concurrent computers have been designed, such as [Ametek 1988] and [NCUBE 1986]. Taking advantage of concurrency on these computers will require new languages, operating systems, and environments to support the programming, monitoring and control of applications. The Rosette project at MCC is developing a system architecture for general-purpose concurrent computation based on the Actor model [Agha 1986]. The aim of the Rosette architecture is to support the use of concurrent computers in solving computationally intensive problems whose structure is not statically determined. The architecture will support variable grain size, dynamic resource management, reflection, and heterogeneity.A number of concurrent language proposals have employed computational models that are limited in their applicability because of assumptions such as granularity of concurrency, static process topology, or inherent communications cost. Others, for example, functional programming, cannot be used to model shared, modifiable objects. The Actor model addresses these issues directly.However, the difficulties of using concurrent computers cannot be addressed solely through programming languages. Traditional operating systems treat each process as an independent entity for the purposes of resource management (i.e., for the run-time allocation of processor, storage, and communication resources.) In the context of concurrent applications, effective operation on a concurrent computer requires that communities of processes be managed together, taking into account the ways in which they interact, and providing for the application itself to guide resource management.For example, consider that programming systems often provide only limited support for the control of resource consumption in search problems in the face of combinatorial explosion; further, it is generally recognized that to optimize search performance, it is useful to vary the resources provided depending on how promising intermediate results of a given path are judged to be. The Rosette architecture will allow the programmer to integrate high-level algorithms within the application for dividing resources dynamically among the sub-computations that are spawned. These algorithms will provide an assessment of the progress of the sub-computations, and will be used to make the large number of low-level resource management decisions that are necessary in carrying out the computation. Thus, operating system functionality will be folded into the application.The Rosette architecture has two major components: an interface layer and a system environment. The interface layer provides mechanisms for the monitoring and control of concurrent applications. It supports a uniform view of the system resources and applications through the use of actors and message-passing semantics. The interface layer includes a set of actors that represent the processing, storage and communication resources across the hardware base. The system environment contains actor communities which implement resource management policies, and support for programming concurrent applications. The environment consists of an extensible set of actors that are organized via a class-based object model. These actors provide monitoring, debugging, resource management system simulation, and compilation/transformation facilities.To demonstrate the effectiveness of the Rosette architecture, as well as to drive its development, two classes of applications are being used. The first class is the Rosette system itself: all of the system components necessary for resource management, monitoring, debugging, and program development are being constructed in the Rosette language and will take full advantage of both concurrency and object-oriented programming support. The second category of applications is chosen to be somewhat independent of the Rosette system so as to ensure adequate feed-back concerning applicability to domains other than software engineering. These applications are in the general area of knowledge-based signal interpretation.The Rosette architecture is implemented via a concurrent object-oriented programming language that provides the following features: Inherent concurrency: A declarative approach is needed which allows fine-grained concurrency to arise naturally from the program. If the programmer is forced to express all of the details of the concurrent execution of an application, it becomes difficult to re-map a given program to different concurrent computers which have varying concurrency characteristics.Dynamic creation/modification of objects: This allows the system to be extensible and reconfigurable.Sharing: The language should support mechanisms such as inheritance so that code, structure, and values need not be replicated.Classes: Objects can be organized into classes to permit more effective management of resources, as for example in a very large database. Classes should be first-class objects and hence it should be possible to create classes dynamically.The Rosette language is a high-level language that is translated into a base language that directly expresses concurrent computations in terms of the semantics of the Actor model. The Rosette language extends the base language with data abstraction facilities and a library of classes, and is used to implement the non-built-in actors of the interface layer, components of the system environment, and applications.Using data abstraction extensions to the base language, the Rosette language organizes actors into classes according to common sets of behaviors and local state. Inheritance is used as a means of sharing code and structure that is common among classes. There is considerable evidence, for example [LaLonde 1986] and [Snyder 1987], that a robust class system should support separate hierarchies that represent what is modeled by a class and how some class of objects are actually implemented. Our approach is to use three different kinds of classes to organize the various kinds of actors in a system and information about these actors: abstract classes specify observable requests, responses, and actions;representation classes specify resource management characteristics of implementations; andbehavior classes contain the actual implementations.Associated with an abstract class is the set of representations that are available to implement the particular abstraction. Each representation is associated with the behavior class that contains the corresponding implementation. The power of this separation is that each of the three kinds of class may be related to others in a class hierarchy. This means that the relationships between abstract classes are not forced to have the same structure as the relationships among implementations. Sharing via inheritance among behavior classes is related to engineering concerns; relations among abstract classes are dictated by external/observable behavior. As an example, an abstract class might specify the operations available over real-matrices. There are several ways of representing a matrix as a community of actors, and each of these would be specified with appropriate representation and behavior classes. There is a well-defined abstract hierarchy of matrices such as invertible, positive, etc. These abstract kinds of matrices are independent of the various implementations, which may be chosen according to application and hardware resource constraints.The Rosette language embodies a reflective model of the interface layer that is similar to that used in ABCL/R, [Watanabe and Yonezawa 1988]. This model is termed the execution model and provides an abstract implementation of an actor in terms of other resource actors. There are three classes of resource actors in the model: container, processor, and mailbox.The container class of resource actor models the storage local to an actor, and is very similar to a frame or unit in knowledge-based systems. A container is a set of slots, where each slot is an association of a key and a value. Both keys and values are just other actors. Slots can be added or deleted from a container. These actions model allocation and deallocation of storage.The basic operation performed by processor resource actors is the execution of a message, which defines how to determine the method for responding to a message. An evaluation context is then created and the method is applied to the context and the message.The third class of resource actors implements the communications interface to an actor. The main function of a mailbox is the buffering of incoming messages until they are requested by a processor. Mailboxes respond to requests that signify message arrivals at the actor and to requests which are used by processors to accept messages.Object-oriented interfaces for each of the three kinds of resource actors are defined and organized in a class hierarchy. For any actor, any combination of these three resource actors may be built-in or not. The possibility of defining new resource actors within the context of the execution model makes the Rosette architecture extensible - allowing new forms of monitoring and control to be defined. Resource management policies are defined in terms of interactions among the resource actors of an application's actor community.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {91–93},
numpages = {3}
}

@inproceedings{10.5555/3433701.3433779,
author = {De Matteis, Tiziano and de Fine Licht, Johannes and Hoefler, Torsten},
title = {FBLAS: Streaming Linear Algebra on FPGA},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {Spatial computing architectures pose an attractive alternative to mitigate control and data movement overheads typical of load-store architectures. In practice, these devices are rarely considered in the HPC community due to the steep learning curve, low productivity, and the lack of available libraries for fundamental operations. High-level synthesis (HLS) tools are facilitating hardware programming, but optimizing for these architectures requires factoring in new transformations and resources/performance trade-offs. We present fBLAS, an open-source HLS implementation of BLAS for FPGAs, that enables reusability, portability and easy integration with existing software and hardware codes. fBLAS' implementation allows scaling hardware modules to exploit on-chip resources, and module interfaces are designed to natively support streaming on-chip communications, allowing them to be composed to reduce offchip communication. With fBLAS, we set a precedent for FPGA library design, and contribute to the toolbox of customizable hardware components necessary for HPC codes to start productively targeting reconfigurable platforms.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {59},
numpages = {13},
keywords = {hardware library, spatial architectures, high level synthesis},
location = {Atlanta, Georgia},
series = {SC '20}
}

@inproceedings{10.1145/996566.996599,
author = {Muttreja, Anish and Raghunathan, Anand and Ravi, Srivaths and Jha, Niraj K.},
title = {Automated Energy/Performance Macromodeling of Embedded Software},
year = {2004},
isbn = {1581138288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/996566.996599},
doi = {10.1145/996566.996599},
abstract = {Efficient energy and performance estimation of embedded software is a critical part of any system-level design flow. Macromodeling based estimation is an attempt to speed up estimation by exploiting reuse that is inherent in the design process. Macromodeling involves pre-characterizing reusable software components to construct high-level models, which express the execution time or energy consumption of a sub-program as a function of suitable parameters. During simulation, macromodels can be used instead of detailed hardware models, resulting in orders of magnitude simulation speedup. However, in order to realize this potential, significant challenges need to be overcome in both the generation and use of macromodels--- including how to identify the parameters to be used in the macromodel, how to define the template function to which the macromodel is fitted, em etc. This paper presents an automatic methodology to perform characterization-based high-level software macromodeling, which addresses the aforementioned issues. Given a sub-program to be macromodeled for execution time and/or energy consumption, the proposed methodology automates the steps of parameter identification, data collection through detailed simulation, macromodel template selection, and fitting. We propose a novel technique to identify potential macromodel parameters and perform data collection, which draws from the concept of bf data structure serialization used in distributed programming. We utilize bf symbolic regression techniques to concurrently filter out irrelevant macromodel parameters, construct a macromodel function, and derive the optimal coefficient values to minimize fitting error. Experiments with several realistic benchmarks suggest that the proposed methodology improves estimation accuracy and enables wide applicability of macromodeling to complex embedded software, while realizing its potential for estimation speedup. We describe a case study of how macromodeling can be used to rapidly explore algorithm-level energy tradeoffs, for the tt zlib data compression library.},
booktitle = {Proceedings of the 41st Annual Design Automation Conference},
pages = {99–102},
numpages = {4},
keywords = {regression, data serialization, macromodeling, symbolic, genetic programming, embedded software},
location = {San Diego, CA, USA},
series = {DAC '04}
}

@inproceedings{10.1145/1858996.1859040,
author = {Morin, Brice and Mouelhi, Tejeddine and Fleurey, Franck and Le Traon, Yves and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Security-Driven Model-Based Dynamic Adaptation},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859040},
doi = {10.1145/1858996.1859040},
abstract = {Security is a key-challenge for software engineering, especially when considering access control and software evolutions. No satisfying solution exists for maintaining the alignment of access control policies with the business logic. Current implementations of access control rely on the separation between the policy and the application code. In practice, this separation is not so strict and some rules are hard-coded within the application, making the evolution of the policy difficult. We propose a new methodology for implementing security-driven applications. From a policy defined by a security expert, we generate an architectural model, reflecting the access control policy. We leverage the advances in the models@runtime domain to keep this model synchronized with the running system. When the policy is updated, the architectural model is updated, which in turn reconfigures the running system. As a proof of concept, we apply the approach to the development of a library management system.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {205–214},
numpages = {10},
keywords = {models@runtime, model-driven engineering, adaptive system, access-control},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/21850.253088,
author = {Alberts, Henry C.},
title = {Gaming and Simulation: The next Twenty-Five Years},
year = {1985},
isbn = {0911801073},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/21850.253088},
doi = {10.1145/21850.253088},
abstract = {This paper was written for presentation to the March 1982 meeting of the Military Operations Research Society. It was the theme paper for the session on Gaming and Simulation. Since that meeting, the ideas have been presented at a number of meetings of Professional societies; and informally to members of both the Simulation and the Test and Evaluation Communities.The message is that both Communities need to combine and integrate their efforts so that we may not only build a consistent, consistently expanding data base which can be used to design increasingly more focussed Test and Evaluation programs; but also use that data base to run simulations simultaneously with our tests. The result of that combination of capability will not only be beneficial to both communities, but will also provide means to validate in the field the many critical assumptions necessarily built into any model.In short, this paper suggests that Real Time Analysis is of benefit to everyone with a part to play in the complex business of System Design, Acquisition, Operation, and Support, bases began to emerge. These concepts led to establishment of the Combat Development Experimentation Center (CDEC) at Ford Ord, California; the facility which provided the basis for a continuing field data collection effort to try to understand the dynamics of ground warfare. A parallel development was establishment of the Modern Army Selected System Test, Evaluation and Research (MASSTER) activity at Fort Hood, Texas.By 1962 there were ground warfare models, and some data coming from the CDEC activity. But in 1962 CDEC's data collection devices consisted of stop watches, clipboards, and a rather gross position location system based on signs placed over the terrain which the players identified and called out. The real data collection device was the controller, with All his human failings.Why this quick review of history? Because if look back on our current state of the art and methodology from the vantage point of 2008, we might say that little progress had been made in integrating and using data bases for modeling and simulation during 1958 to 1985. I will illustrate what I mean.The past 25 years have seen staggering advances in computational capability. Along with hardware improvements there has been some advance in software technology. My computer friends confidently inform me that we can expect ever greater advances quickly. They predict that 1 megabyte memories soon will be found within a soldier's back pack - thus providing im with picosecond computational capability.The number of models used for computer simulation has increased enormously. Figure 1 lists the 36 models for Air-Ground Force conventional conflict listed in the latest SAGA manual. Each was developed to address different aspects of a Ground-Air battle But there is one common requirement: All models and computer assisted games use data base information as the basis of model or game operation. The quantity and quality of the data base may be the cause of considerable difference in results obtained from one model/game to the next. This is true even when models have been structured to provide comparable results. Even setting aside data base limitations, there is an additional problem - Models and Games are difficult Model rules generally are the product of cerebration with due regard to a data base. Once programmed, any changes to model decision rules can become very difficult. Most importantly, model validation, the acid test, continues to raise practical problems. The Army Model Improvement Program (AMIP) is looking at these problems and is tasked not only to provide improved models for future use, but also to look at the data base used to exercise those models. There is considerable hope that the end result of all this effort will be a believable data base useful across the entire spectrum of models in the Army's active library.},
booktitle = {Proceedings of the 17th Conference on Winter Simulation},
pages = {102–110},
numpages = {9},
location = {San Francisco, California, USA},
series = {WSC '85}
}

@inproceedings{10.1145/512035.512061,
author = {Grechanik, Mark and Perry, Dewayne and Batory, Don},
title = {An Approach to Evolving Database Dependent Systems},
year = {2002},
isbn = {1581135459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512035.512061},
doi = {10.1145/512035.512061},
abstract = {It is common in client/server architectures for components to have SQL statements embedded in their source code. Components submit queries to relational databases using such standards as Universal Data Access (UDA) and Open Database Connectivity (ODBC). The API that implements these standards is complex and requires the embedding of SQL statements in the language that is used to write the components. Such programming practices are widespread and result in increased complexity in maintaining systems.We propose an approach that eliminates the embedding of SQL in programming languages, thereby enabling the automation of important software maintenance tasks.},
booktitle = {Proceedings of the International Workshop on Principles of Software Evolution},
pages = {113–116},
numpages = {4},
location = {Orlando, Florida},
series = {IWPSE '02}
}

@article{10.1145/778559.778562,
author = {Kistler, Thomas and Franz, Michael},
title = {Continuous Program Optimization: A Case Study},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/778559.778562},
doi = {10.1145/778559.778562},
abstract = {Much of the software in everyday operation is not making optimal use of the hardware on which it actually runs. Among the reasons for this discrepancy are hardware/software mismatches, modularization overheads introduced by software engineering considerations, and the inability of systems to adapt to users' behaviors.A solution to these problems is to delay code generation until load time. This is the earliest point at which a piece of software can be fine-tuned to the actual capabilities of the hardware on which it is about to be executed, and also the earliest point at wich modularization overheads can be overcome by global optimization.A still better match between software and hardware can be achieved by replacing the already executing software at regular intervals by new versions constructed on-the-fly using a background code re-optimizer. This not only enables the use of live profiling data to guide optimization decisions, but also facilitates adaptation to changing usage patterns and the late addition of dynamic link libraries.This paper presents a system that provides code generation at load-time and continuous program optimization at run-time. First, the architecture of the system is presented. Then, two optimization techniques are discussed that were developed specifically in the context of continuous optimization. The first of these optimizations continually adjusts the storage layouts of dynamic data structures to maximize data cache locality, while the second performs profile-driven instruction re-scheduling to increase instruction-level parallelism. These two optimizations have very different cost/benefit ratios, presented in a series of benchmarks. The paper concludes with an outlook to future research directions and an enumeration of some remaining research problems.The empirical results presented in this paper make a case in favor of continuous optimization, but indicate that it needs to be applied judiciously. In many situations, the costs of dynamic optimizations outweigh their benefit, so that no break-even point is ever reached. In favorable circumstances, on the other hand, speed-ups of over 120% have been observed. It appears as if the main beneficiaries of continuous optimization are shared libraries, which at different times can be optimized in the context of the currently dominant client application.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
pages = {500–548},
numpages = {49},
keywords = {dynamic reoptimization, continuous program optimization, Dynamic code generation}
}

@inproceedings{10.1145/3545008.3545067,
author = {Shah, Milan and Neff, Reece and Wu, Hancheng and Minutoli, Marco and Tumeo, Antonino and Becchi, Michela},
title = {Accelerating Random Forest Classification on GPU and FPGA},
year = {2023},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545008.3545067},
doi = {10.1145/3545008.3545067},
abstract = {Random Forests (RFs) are a commonly used machine learning method for classification and regression tasks spanning a variety of application domains, including bioinformatics, business analytics, and software optimization. While prior work has focused primarily on improving performance of the training of RFs, many applications, such as malware identification, cancer prediction, and banking fraud detection, require fast RF classification. In this work, we accelerate RF classification on GPU and FPGA. In order to provide efficient support for large datasets, we propose a hierarchical memory layout suitable to the GPU/FPGA memory hierarchy. We design three RF classification code variants based on that layout, and we investigate GPU- and FPGA-specific considerations for these kernels. Our experimental evaluation, performed on an Nvidia Xp GPU and on a Xilinx Alveo U250 FPGA accelerator card using publicly available datasets on the scale of millions of samples and tens of features, covers various aspects. First, we evaluate the performance benefits of our hierarchical data structure over the standard compressed sparse row (CSR) format. Second, we compare our GPU implementation with cuML, a machine learning library targeting Nvidia GPUs. Third, we explore the performance/accuracy tradeoff resulting from the use of different tree depths in the RF. Finally, we perform a comparative performance analysis of our GPU and FPGA implementations. Our evaluation shows that, while reporting the best performance on GPU, our code variants outperform the CSR baseline both on GPU and FPGA. For high accuracy targets, our GPU implementation yields a 5-9 \texttimes{} speedup over CSR, and up to a 2 \texttimes{} speedup over Nvidia’s cuML library.},
booktitle = {Proceedings of the 51st International Conference on Parallel Processing},
articleno = {4},
numpages = {11},
keywords = {random forest classification, FPGA, GPU},
location = {Bordeaux, France},
series = {ICPP '22}
}

@inproceedings{10.1145/2484313.2484351,
author = {Davi, Lucas Vincenzo and Dmitrienko, Alexandra and N\"{u}rnberger, Stefan and Sadeghi, Ahmad-Reza},
title = {Gadge Me If You Can: Secure and Efficient Ad-Hoc Instruction-Level Randomization for X86 and ARM},
year = {2013},
isbn = {9781450317672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484313.2484351},
doi = {10.1145/2484313.2484351},
abstract = {Code reuse attacks such as return-oriented programming are one of the most powerful threats to contemporary software. ASLR was introduced to impede these attacks by dispersing shared libraries and the executable in memory. However, in practice its entropy is rather low and, more importantly, the leakage of a single address reveals the position of a whole library in memory. The recent mitigation literature followed the route of randomization, applied it at different stages such as source code or the executable binary. However, the code segments still stay in one block. In contrast to previous work, our randomization solution, called Xifer, (1) disperses all code (executable and libraries) across the whole address space, (2) re-randomizes the address space for each run, (3) is compatible to code signing, and (4) does neither require offline static analysis nor source-code. Our prototype implementation supports the Linux ELF file format and covers both mainstream processor architectures x86 and ARM. Our evaluation demonstrates that Xifer performs efficiently at load- and during run-time (1.2% overhead).},
booktitle = {Proceedings of the 8th ACM SIGSAC Symposium on Information, Computer and Communications Security},
pages = {299–310},
numpages = {12},
keywords = {software diversity, ASLR, randomization, return-oriented programming, return-into-libc},
location = {Hangzhou, China},
series = {ASIA CCS '13}
}

@inproceedings{10.1145/360276.360346,
author = {Sundararajan, Prasanna and Guccione, Steven A.},
title = {Run-Time Defect Tolerance Using JBits},
year = {2001},
isbn = {1581133413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/360276.360346},
doi = {10.1145/360276.360346},
abstract = {The ability to tolerate defects in semiconductor devices has the potential for both increasing yields of devices being manufactured and making it economically feasible to manufacture even larger devices. While FPGA devices appear to be well suited to providing defect tolerance, practical application of existing research and techniques has been somewhat elusive. One barrier to acceptance is that existing defect tolerance techniques for FPGAs have tended to rely on either modifications to device architectures or modifications to design tools. We describe a software-based technique for providing defect tolerance which requires neither changes to device hardware or software tools. This approach uses the Xilinx JBits$^{(tm)}$ toolkit and operates at the core library level. Addressing defect tolerance locally using core library elements rather than taking a global approach helps provide direct support for run-time reconfiguration. Circuits may be configured and reconfigured rapidly in the presence of these defects. This rapid configuration also provides a path for practical use in more traditional manufacturing environments.},
booktitle = {Proceedings of the 2001 ACM/SIGDA Ninth International Symposium on Field Programmable Gate Arrays},
pages = {193–198},
numpages = {6},
keywords = {cores, defect tolerance, Java, FPGA, run-time reconfiguration},
location = {Monterey, California, USA},
series = {FPGA '01}
}

@inproceedings{10.1145/1508128.1508196,
author = {Zhang, Bowei and Gu, Guochang and Sun, Lin and Wu, Yanxia},
title = {32-Bit Floating-Point FPGA Gaussian Elimination},
year = {2009},
isbn = {9781605584102},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1508128.1508196},
doi = {10.1145/1508128.1508196},
abstract = {The well-known Gaussian elimination (with partial pivoting) is a widely-used algorithm, one of traditional methods for solving dense linear systems of equations (LSEs). This paper presents a hardware-optimized variant of Gaussian elimination and its 32-bit ANSI/IEEE Std 754-1985 floating-point implementation on a Xilinx Virtex-5 FPGA with highly efficient design. The logic of the traditional algorithm is changed in order to make use of parallelism in hardware. According to this change the proposed hardware architecture can accomplish the solution very fast. Its average running time for n\texttimes{}n 32-bit floating-point matrices with uniformly distributed entries equals around n2(clock cycles) as opposed to n3 in software. Meanwhile, an open source library FPLibrary, which provides parameterizable pipelined floating-point operators, is used in the design. In realization, the design is finally integrated in an developed prototype system to accelerate the general purpose processor's work with the data exchanging through PCI-express between host and FPGA with DMA access method. Furthermore, by means of Strasson's algorithm, large LSEs also can be solved based on multiple FPGAs' co-work. The whole implementation placed and routed in the xc5vlx110t-3 FPGA with the applicability for solving LSE at most dimension 22, can be clocked with a frequency of up to 200MHz and computes the solution in 5.39 ¼s on average, providing a speed-up of up to almost 15 times over an equivalent software implementation on a Pentium IV 2.6GHz CPU. To the best of authors' knowledge, there has been no previous work on floating-point LSEs solving hardware and its implementation used as an application function unit in reconfigurable computing system.},
booktitle = {Proceedings of the ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {283–284},
numpages = {2},
keywords = {gaussian elimination, floating-point, fpga.},
location = {Monterey, California, USA},
series = {FPGA '09}
}

@inproceedings{10.1145/75110.75113,
author = {Balzer, R.},
title = {Process Programming: Passing into a New Phase},
year = {1988},
isbn = {0897913140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/75110.75113},
doi = {10.1145/75110.75113},
abstract = {Our whole experience, as a community, has been with a single type of program and its associated lifecycles, hereafter called Product Programs and Product Programming. Because of its importance to us as a multi-billion dollar industry, as our chosen profession, and as an intellectual challenge, we draw many distinctions within this broad category. For the same reasons, the Eskimo has 27 different words for snow to distinguish what for them are important differences.We naturally use this “vast” experience base to organize our view of the world. When new situations arise, we categorize them according to this existing structure, occasionally extending the structure by drawing finer distinctions. This is a very successful strategy and is the process by which community knowledge is built.However, this strategy is unsuccessful and quite debilitating on those rare occasions when truly new phenomena are encountered, because by operating at the micro level it tends to hide macro distinctions through a series of micro adaptations. Consider the dilemma of some hypothetical Eskimos who first encountered water and attempted to fit it into their “snow structure.” For them it was just a soft version of this rather obscure and troublesome form of snow called “slush.”This accommodation of the existing theory hid the fact that although there were some similarities and all the same “stuff” was involved, that because that material was in a different phase (as used technically in physics) it had fundamentally different properties which necessitated fundamentally different ways of handling it and different uses to which it was put.I believe that we find ourselves in just such a situation with process programming. It's constituted out of the same primitive components as product programs, but its organizational composition places it into a different “phase” of the material which gives it fundamentally different properties and forces us to handle and use it quite differently.We do ourselves a great disservice by adopting the following two assumptions without careful critical assessment: Process Programs are similar to Product Programs;Process Programming is similar to Product Programming.I believe these widely held assumptions are false and misleading and hope to demonstrate so in the rest of this position paper.Our limited process experience lies at the two ends of the spectrum: Very low level — routine procedures which are easily embedded (procedurally) in an automated tool.Very high level — broad organizational lifecycle models which have been so structured and regularized that they can be codified into formal models.At both of these levels, Process Programs have been expressed either procedurally (in some standard programming language) or via some state transition formalism. The conceptual basis for both is to identify all the options potentially available at some point and the preconditions necessary for employing each.Many corporations, partly driven by government pressure, are making major investments in codifying their lifecycle models and in automating their routine low level processes.Yet, I know of no interesting Process Program (with one exception described below). If you look at any of them, all you see is a program much like any other. How could it be otherwise, since we use the same formalisms as we do for Product Programs and conceptualize them similarly?Furthermore, even if you broaden the scope to include process programs from other fields, such as CAD, the results are remarkably similar — existing Process Programs mirror Product Programs. In fact, CAD has focused much more on prototyping support than on process support. They facilitate getting immediate feedback from models, and the construction and modification of those models. They thereby allow people to explore the implications of various designs and do tradeoff analysis, but the process is occurring outside the system. It is not captured, represented, or directly supported. Rather, the support arises from better tools for processing the artifacts resulting from the process.The single exception to this lack of interesting Process Program examples is Make (and all its descendents) which constructs the necessary process to build a configuration. What makes this class of Process Programs interesting is that it devised a special purpose model for describing its domain (configurations), structural properties (dependencies), and methods (build processes). Users instantiate this configuration model, and like other fourth generation application generators, the processor is able to “compile” the instantiated model, in this case into a Process Program for constructing the configuration.The key to this success was the invention of an appropriate model which allowed uses to occur by instantiation and in which any such instantiation could be processed by a fixed (and relatively simple) program.To me, this is the heart of Process Programming. Instantiation is the key. You don't write Process Programs, you instantiate and particularize existing ones. Furthermore, although some Process Programs (like Make) can be fully automated, many are specifically intended to include people and be interactive. These need not, and in fact, should not be fully instantiated before they begin. Rather, as more information is gathered or difficulties encountered, additional instantiation can occur to take advantage of the new information or overcome the difficulty.Such dynamic instantiation and monitoring is just what Planning Languages were designed for, and they seem a natural place to start looking for foundations for Process Programming. However, I expect that most of the leverage will come from a series of special purpose processors for particular domains (like Make) with their own languages and forms of instantiation.It should also be noted that instantiation necessarily implies reuse. Instantiation provides the basis for reusing preexisting models. The real bugaboo for reuse has always been adaptation. It is ludicrous to believe that you can find exactly what you want in any library of components; the space is simply too large. Instantiation is a way of adapting a general model through particularization, and is more general than simple parameterization. Even so, I expect that we will severely strain existing notions of instantiation in pursuing Process Programming and that, therefore, much of the insight and leverage will come from special purpose mechanisms.If this picture of Process Programs and Process Programming is accurate, then we need to treat them as separate entities rather than trying to force them into product formalisms and lifecycles. Nothing could be further from our current product preoccupation with building artifacts that satisfy a predefined static specification than a process world which incrementally defines and constructs its artifact (process) through instantiation based reuse while it is being used (executed).Clearly Process Programs and Process Programming are a phase of software as distinct from Product Programs and Product Programming as water is from snow. We must, therefore, not restrict our formalisms, techniques, and approaches to the former to those that have worked on the latter.},
booktitle = {Proceedings of the 4th International Software Process Workshop on Representing and Enacting the Software Process},
pages = {43–45},
numpages = {3},
location = {Devon, United Kingdom},
series = {ISPW '88}
}

@article{10.1145/75111.75113,
author = {Balzer, R.},
title = {Process Programming: Passing into a New Phase},
year = {1988},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/75111.75113},
doi = {10.1145/75111.75113},
abstract = {Our whole experience, as a community, has been with a single type of program and its associated lifecycles, hereafter called Product Programs and Product Programming. Because of its importance to us as a multi-billion dollar industry, as our chosen profession, and as an intellectual challenge, we draw many distinctions within this broad category. For the same reasons, the Eskimo has 27 different words for snow to distinguish what for them are important differences.We naturally use this “vast” experience base to organize our view of the world. When new situations arise, we categorize them according to this existing structure, occasionally extending the structure by drawing finer distinctions. This is a very successful strategy and is the process by which community knowledge is built.However, this strategy is unsuccessful and quite debilitating on those rare occasions when truly new phenomena are encountered, because by operating at the micro level it tends to hide macro distinctions through a series of micro adaptations. Consider the dilemma of some hypothetical Eskimos who first encountered water and attempted to fit it into their “snow structure.” For them it was just a soft version of this rather obscure and troublesome form of snow called “slush.”This accommodation of the existing theory hid the fact that although there were some similarities and all the same “stuff” was involved, that because that material was in a different phase (as used technically in physics) it had fundamentally different properties which necessitated fundamentally different ways of handling it and different uses to which it was put.I believe that we find ourselves in just such a situation with process programming. It's constituted out of the same primitive components as product programs, but its organizational composition places it into a different “phase” of the material which gives it fundamentally different properties and forces us to handle and use it quite differently.We do ourselves a great disservice by adopting the following two assumptions without careful critical assessment: Process Programs are similar to Product Programs;Process Programming is similar to Product Programming.I believe these widely held assumptions are false and misleading and hope to demonstrate so in the rest of this position paper.Our limited process experience lies at the two ends of the spectrum: Very low level — routine procedures which are easily embedded (procedurally) in an automated tool.Very high level — broad organizational lifecycle models which have been so structured and regularized that they can be codified into formal models.At both of these levels, Process Programs have been expressed either procedurally (in some standard programming language) or via some state transition formalism. The conceptual basis for both is to identify all the options potentially available at some point and the preconditions necessary for employing each.Many corporations, partly driven by government pressure, are making major investments in codifying their lifecycle models and in automating their routine low level processes.Yet, I know of no interesting Process Program (with one exception described below). If you look at any of them, all you see is a program much like any other. How could it be otherwise, since we use the same formalisms as we do for Product Programs and conceptualize them similarly?Furthermore, even if you broaden the scope to include process programs from other fields, such as CAD, the results are remarkably similar — existing Process Programs mirror Product Programs. In fact, CAD has focused much more on prototyping support than on process support. They facilitate getting immediate feedback from models, and the construction and modification of those models. They thereby allow people to explore the implications of various designs and do tradeoff analysis, but the process is occurring outside the system. It is not captured, represented, or directly supported. Rather, the support arises from better tools for processing the artifacts resulting from the process.The single exception to this lack of interesting Process Program examples is Make (and all its descendents) which constructs the necessary process to build a configuration. What makes this class of Process Programs interesting is that it devised a special purpose model for describing its domain (configurations), structural properties (dependencies), and methods (build processes). Users instantiate this configuration model, and like other fourth generation application generators, the processor is able to “compile” the instantiated model, in this case into a Process Program for constructing the configuration.The key to this success was the invention of an appropriate model which allowed uses to occur by instantiation and in which any such instantiation could be processed by a fixed (and relatively simple) program.To me, this is the heart of Process Programming. Instantiation is the key. You don't write Process Programs, you instantiate and particularize existing ones. Furthermore, although some Process Programs (like Make) can be fully automated, many are specifically intended to include people and be interactive. These need not, and in fact, should not be fully instantiated before they begin. Rather, as more information is gathered or difficulties encountered, additional instantiation can occur to take advantage of the new information or overcome the difficulty.Such dynamic instantiation and monitoring is just what Planning Languages were designed for, and they seem a natural place to start looking for foundations for Process Programming. However, I expect that most of the leverage will come from a series of special purpose processors for particular domains (like Make) with their own languages and forms of instantiation.It should also be noted that instantiation necessarily implies reuse. Instantiation provides the basis for reusing preexisting models. The real bugaboo for reuse has always been adaptation. It is ludicrous to believe that you can find exactly what you want in any library of components; the space is simply too large. Instantiation is a way of adapting a general model through particularization, and is more general than simple parameterization. Even so, I expect that we will severely strain existing notions of instantiation in pursuing Process Programming and that, therefore, much of the insight and leverage will come from special purpose mechanisms.If this picture of Process Programs and Process Programming is accurate, then we need to treat them as separate entities rather than trying to force them into product formalisms and lifecycles. Nothing could be further from our current product preoccupation with building artifacts that satisfy a predefined static specification than a process world which incrementally defines and constructs its artifact (process) through instantiation based reuse while it is being used (executed).Clearly Process Programs and Process Programming are a phase of software as distinct from Product Programs and Product Programming as water is from snow. We must, therefore, not restrict our formalisms, techniques, and approaches to the former to those that have worked on the latter.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {apr},
pages = {43–45},
numpages = {3}
}

@inproceedings{10.1145/52964.52986,
author = {Cannon, Robert and Gorgone, John and Ho, Tom and McGregor, John D.},
title = {Proposed Criteria for Accreditation of Computer Information Systems Programs},
year = {1988},
isbn = {089791256X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/52964.52986},
doi = {10.1145/52964.52986},
abstract = {A working group, representing ACM, DPMA, and IEEE-CS, was formed to draft a set of guidelines, including criteria, for the accreditation of computer information systems' undergraduate programs. The guidelines and criteria are summarized below.Faculty: Typically a minimum of 4 faculty, with 3 full time, are needed. Normally, 25% of a faculty member's time should be available for scholarly activity and development. Teaching loads should not exceed 12 hours and should not exceed 4 courses with 2 preparations.Curriculum: Curricular assume a 120 semester hour, four year, baccalaureate program. The program should consist of approximately 30% computer information systems, 20% business, at least 40% in general education and up to 10% other.Forty to 60 percent of the CIS portion should cover a broad core that includes a) computer concepts and software systems, b) program, data, and file structures, c) data management,d) data and computer communications, and e) systems analysis and design. Students should be exposed to a variety of programming languages and be proficient in one structured language. The remaining courses should cover breadth and depth.Resources: Appropriate computing facilities must exist for students and faculty. Adequate software and documentation must be available.Students: Established standards and procedures must insure that graduates have the requisite qualifications to function as CIS professionals.Institutional Support: Adequate support must be provided to support the faculty, department office administration, and library. Faculty support includes leave programs, reasonable teaching loads, competitive salaries, and travel support.},
booktitle = {Proceedings of the Nineteenth SIGCSE Technical Symposium on Computer Science Education},
pages = {88},
numpages = {1},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '88}
}

@article{10.1145/52965.52986,
author = {Cannon, Robert and Gorgone, John and Ho, Tom and McGregor, John D.},
title = {Proposed Criteria for Accreditation of Computer Information Systems Programs},
year = {1988},
issue_date = {Feb. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/52965.52986},
doi = {10.1145/52965.52986},
abstract = {A working group, representing ACM, DPMA, and IEEE-CS, was formed to draft a set of guidelines, including criteria, for the accreditation of computer information systems' undergraduate programs. The guidelines and criteria are summarized below.Faculty: Typically a minimum of 4 faculty, with 3 full time, are needed. Normally, 25% of a faculty member's time should be available for scholarly activity and development. Teaching loads should not exceed 12 hours and should not exceed 4 courses with 2 preparations.Curriculum: Curricular assume a 120 semester hour, four year, baccalaureate program. The program should consist of approximately 30% computer information systems, 20% business, at least 40% in general education and up to 10% other.Forty to 60 percent of the CIS portion should cover a broad core that includes a) computer concepts and software systems, b) program, data, and file structures, c) data management,d) data and computer communications, and e) systems analysis and design. Students should be exposed to a variety of programming languages and be proficient in one structured language. The remaining courses should cover breadth and depth.Resources: Appropriate computing facilities must exist for students and faculty. Adequate software and documentation must be available.Students: Established standards and procedures must insure that graduates have the requisite qualifications to function as CIS professionals.Institutional Support: Adequate support must be provided to support the faculty, department office administration, and library. Faculty support includes leave programs, reasonable teaching loads, competitive salaries, and travel support.},
journal = {SIGCSE Bull.},
month = {feb},
pages = {88},
numpages = {1}
}

@inproceedings{10.1145/41579.41587,
author = {Froberg, C.-E.},
title = {BIT—a Child of the Computer},
year = {1987},
isbn = {0897912292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/41579.41587},
doi = {10.1145/41579.41587},
abstract = {The back-ground of the Scandinavian computer journal B I T will be outlined, in particular with respect to computational demands in science, technology, industry and defense. The history of B I T will be described and related to the evolution of computers, numerical mathematics and computer science. Some contributed papers which have had an impact on the general development will be discussed briefly.The 19th century could perhaps be characterized as a period of preparation for the advent of the computer. It so happened that quite a few Swedish inventors played a role in this development. Scheutz, father and son, as well as Wiberg constructed mechanical devices for a somewhat automatized calculation for solving simple arithmetic problems by series of pre-determined operations. In fact, Wiberg was able to compute a logarithm table which even appeared in print. Later, Odhner built a robust mechanical, hand-driven calculator which around 1930 was followed by electromechanical calculators. All lengthy calculations had to be performed manually by this time. Let me mention a few examples from Sweden.One such problem was to find periods of so-called internal waves in the sea. These waves are huge, up to 20-30 meters in size, but nevertheless invisible. They are generated by the moon and observed as sharp changes in the salinity. The method used was numerical autoanalysis, that is a kind of Fourier analysis of the function by itself.During the war there was a great need for ballistie tables, and I belonged to a group involved in computing bomb tables for the Swedish Air Force. We used the classical Runga-Kutta method with air resistance represented graphically, and we had only electro-mechanical calculators at our disposal. After having computed a basic set of orbits we could produce the wanted tables by a suitable interpolation procedure. It is a sad fact that all our tables could probably have been computed in a couple of minutes on a fast modern computer. After the war I was involved in some rather lengthy computations on the deuteron concerning energy levels and quadrupole moment and also in problems on scattering.However, in 1946 some people in the Swedish Navy and in the Academy for Engineering Sciences got interested in the progress in the United States and after having visited the key projects they reported back with great enthusiasm. It was quickly decided to offer scholarships to four young students; they were selected in the spring of 1947. They arrived already in August or September; two of them went to Harvard and MIT while two, including myself went to Princeton. As far as I am concerned I enjoyed a phantastic hospitality, particularly from Herman Goldstine with whom I established a lifelong friendship. Back home in 1948 some of us got involved in the construction of a relay computer (BARK), completed in 1950. However, it was soon understood that there was a need for more computer facilities, and the construction of BESK under Erik Stemme was initiated. It was completed in 1953, and during a short period of time it was considered as one of the most powerful computers in the world. Clearly its structure was very much the same as that of the Princeton computer.In 1956 a simplified copy of BESK called SMIL was completed at Lund University, built with a minimum budget of some 20,000 $. This computer was used for a large variety of problems in nuclear physics (particularly eigenvalue problems), spectroscopy, mathematics (number theory, construction of tables), and also social sciences (geographical simulations). Several problems coming from industry and different research organisations were also treated.The interest in and use of computers created a very natural demand for conferences since the literature on the subject for obvious reasons was very scarce by this time. The first Scandinavian conference on computers was held May 1959 in Karlskrona, later known as the place where a Soviet submarine ran aground in 1981. One reason for this choice of site was the fact that the Swedish Navy played an important role in initiating the computer development, another that, especially in spring, it was a lovely place, situated on the Baltic. Preliminary discussions were held informally on the need for a Nordic journal on computer related problems, and at the next conference in Copenhagen in 1960 a more formal meeting was arranged. Niels Ivar Bech acted as chairman, and further Peter Naur of Denmark, Jan Garwick, Norway, Olli Lokki, Finland, and myself from Sweden were present. It was unanimously decided to start a Nordic journal within the computer area, to appear quarterly. The journal was intended to be international with papers published in English, German, or the Scandinavian languages. As it turned out, only about 4-5 papers have been published in German, and very soon papers in the Scandinavian languages gradually disappeared. Nowadays it is required that all papers be written in English.The name of the journal was a long one: Nordisk Tidskrift for Informationsbehandling, but playing around with the initials in a clever way we were able to form the name B I T. In fact, this name is most convenient because of its shortness which makes it very easy to quote papers printed in the journal. As is well known it is somewhat dangerous to suggest an activity including work since there is a great risk that the proposer is elected to carry through the project. This is exactly what happened in this case, and from the very beginning up to this time I have served as Editor of B I T. Naturally, we have also an Editorial Board with members from the Nordic countries. Peter Naur of Copenhagen has been a member right from the beginning in 1961 and Germund Dahlquist from 1962. We got financial support from the Danish company Regnecentralen under Niels Ivar Bech and from several official sources including the Nordic Research Organisations for Natural Sciences. Finally, just a few years ago we managed to become self-supporting, perhaps mostly through favorable exchange rates.During the first decade B I T tried to @@@@ the public to get acquainted with new developments within the computer area. It is natural that the growing crowds of people working with computer applications of different kinds felt an increasing difficulty in keeping up with the fast progress, both in hardware and in software. That left a gap which B I T tried to fill. Simultaneously we also tried to accommodate scientific papers, particularly in numerical analysis and in computer languages. Very early we opened a special column for algorithms written in ALGOL 60. As a consequence of this policy our subscribers to a large extent were private Scandinavians during the first decade. Then the situation changed slowly. The need for general information decreased because this was treated in special new publications of type Datamation and also in ordinary and weekly newspapers. Simultaneously the number of scientific contributions to B I T increased strongly, first in numerical mathematics, later also in computer science. As a result of this development the number of Scandinavian subscribers decreased while the number of non-Scandinavian subscribers, mostly libraries of research organisations and universities increased, the net result being slightly positive. From 1980 it was clearly indicated that B I T was divided in two sections, one for Computer Science, and one for Numerical Mathematics. In spite of obvious difficulties we have been able to strike a reasonable balance between these two.The first volume (1961) had 290 pages and was type-written and photographed. Already volume 2 was printed in an ordinary way. B I T had obviously been observed also abroad since two contributions, one from the US (Louis Fein) and one from the Netherlands (Peter Wynn) appeared already in the first volume, while several “foreign” papers (among them one by Gene Golub) were presented in volume 2. From the beginning there was a certain ambivalence with respect to papers on hardware: during the first 10 years we published a few of that kind, but finally they disappeared.Turning to computer science there is an important subject which attracted considerable attention during the first 10-15 years, namely computer languages and compiler construction. The main interest was centered on ALGOL 60 since by that time FORTRAN was only available for users while the corresponding compilers were held secret. However, different aspects on other programming languages, e.g. COBOL, ALGOL 68, PASCAL and SIMULA, have also been treated.It is of course hard to tell which papers have had an impact on the general development, but I think that papers by Dahlquist and others on stability problems, Enright-Hull-Lindberg on numerical methods for stiff systems and a whole bunch on Runge-Kutta methods have had a considerable influence. Finally I think it is fair to mention that we offered a special issue dedicated to Germund Dahlquist on his 60th birthday, followed by one dedicated to B I T on its 25th birthday, both with about 300 pages.Concerning the geographical distribution of authors and subscribers we can say roughly that the Nordic countries, the rest of Europe, and USA plus Canada account for about 1/3 each in both respects. The most striking feature is the steep increase in offered contributions from Taiwan, and we have also had quite a few from mainland China. In both cases the quality has been rather good. Also some exotic countries are represented by authors: Nigeria, Singapore, Ecuador, Sudan and the Fiji Islands, just to mention a few. Even if some papers must be rejected we try to encourage the authors, and in many cases the papers can be published after a more or less thorough revision. As a mean value the time between reception of a paper and publication is nine months.},
booktitle = {Proceedings of the ACM Conference on History of Scientific and Numeric Computation},
pages = {91–93},
numpages = {3},
location = {Princeton, New Jersey, USA},
series = {HSNC '87}
}

@article{10.1145/3182160,
author = {Georgiev, Iliyan and Ize, Thiago and Farnsworth, Mike and Montoya-Vozmediano, Ram\'{o}n and King, Alan and Lommel, Brecht Van and Jimenez, Angel and Anson, Oscar and Ogaki, Shinji and Johnston, Eric and Herubel, Adrien and Russell, Declan and Servant, Fr\'{e}d\'{e}ric and Fajardo, Marcos},
title = {Arnold: A Brute-Force Production Path Tracer},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/3182160},
doi = {10.1145/3182160},
abstract = {Arnold is a physically based renderer for feature-length animation and visual effects. Conceived in an era of complex multi-pass rasterization-based workflows struggling to keep up with growing demands for complexity and realism, Arnold was created to take on the challenge of making the simple and elegant approach of brute-force Monte Carlo path tracing practical for production rendering. Achieving this required building a robust piece of ray-tracing software that can ingest large amounts of geometry with detailed shading and lighting and produce images with high fidelity, while scaling well with the available memory and processing power.Arnold’s guiding principles are to expose as few controls as possible, provide rapid feedback to artists, and adapt to various production workflows. In this article, we describe its architecture with a focus on the design and implementation choices made during its evolutionary development to meet the aforementioned requirements and goals. Arnold’s workhorse is a unidirectional path tracer that avoids the use of hard-to-manage and artifact-prone caching and sits on top of a ray-tracing engine optimized to shoot and shade billions of spatially incoherent rays throughout a scene. A comprehensive API provides the means to configure and extend the system’s functionality, to describe a scene, render it, and save the results.},
journal = {ACM Trans. Graph.},
month = {aug},
articleno = {32},
numpages = {12},
keywords = {path tracing, global illumination, production rendering, ray tracing, Rendering systems, Monte Carlo}
}

@inproceedings{10.1145/2517208.2517220,
author = {Sujeeth, Arvind K. and Gibbons, Austin and Brown, Kevin J. and Lee, HyoukJoong and Rompf, Tiark and Odersky, Martin and Olukotun, Kunle},
title = {Forge: Generating a High Performance DSL Implementation from a Declarative Specification},
year = {2013},
isbn = {9781450323734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2517208.2517220},
doi = {10.1145/2517208.2517220},
abstract = {Domain-specific languages provide a promising path to automatically compile high-level code to parallel, heterogeneous, and distributed hardware. However, in practice high performance DSLs still require considerable software expertise to develop and force users into tool-chains that hinder prototyping and debugging. To address these problems, we present Forge, a new meta DSL for declaratively specifying high performance embedded DSLs. Forge provides DSL authors with high-level abstractions (e.g., data structures, parallel patterns, effects) for specifying their DSL in a way that permits high performance. From this high-level specification, Forge automatically generates both a na\"{\i}ve Scala library implementation of the DSL and a high performance version using the Delite DSL framework. Users of a Forge-generated DSL can prototype their application using the library version, and then switch to the Delite version to run on multicore CPUs, GPUs, and clusters without changing the application code. Forge-generated Delite DSLs perform within 2x of hand-optimized C++ and up to 40x better than Spark, an alternative high-level distributed programming environment. Compared to a manually implemented Delite DSL, Forge provides a factor of 3-6x reduction in lines of code and does not sacrifice any performance. Furthermore, Forge specifications can be generated from existing Scala libraries, are easy to maintain, shield DSL developers from changes in the Delite framework, and enable DSLs to be retargeted to other frameworks transparently.},
booktitle = {Proceedings of the 12th International Conference on Generative Programming: Concepts &amp; Experiences},
pages = {145–154},
numpages = {10},
keywords = {code generation, domain-specific languages, multi-stage programming, parallel programming},
location = {Indianapolis, Indiana, USA},
series = {GPCE '13}
}

@article{10.1145/2637365.2517220,
author = {Sujeeth, Arvind K. and Gibbons, Austin and Brown, Kevin J. and Lee, HyoukJoong and Rompf, Tiark and Odersky, Martin and Olukotun, Kunle},
title = {Forge: Generating a High Performance DSL Implementation from a Declarative Specification},
year = {2013},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/2637365.2517220},
doi = {10.1145/2637365.2517220},
abstract = {Domain-specific languages provide a promising path to automatically compile high-level code to parallel, heterogeneous, and distributed hardware. However, in practice high performance DSLs still require considerable software expertise to develop and force users into tool-chains that hinder prototyping and debugging. To address these problems, we present Forge, a new meta DSL for declaratively specifying high performance embedded DSLs. Forge provides DSL authors with high-level abstractions (e.g., data structures, parallel patterns, effects) for specifying their DSL in a way that permits high performance. From this high-level specification, Forge automatically generates both a na\"{\i}ve Scala library implementation of the DSL and a high performance version using the Delite DSL framework. Users of a Forge-generated DSL can prototype their application using the library version, and then switch to the Delite version to run on multicore CPUs, GPUs, and clusters without changing the application code. Forge-generated Delite DSLs perform within 2x of hand-optimized C++ and up to 40x better than Spark, an alternative high-level distributed programming environment. Compared to a manually implemented Delite DSL, Forge provides a factor of 3-6x reduction in lines of code and does not sacrifice any performance. Furthermore, Forge specifications can be generated from existing Scala libraries, are easy to maintain, shield DSL developers from changes in the Delite framework, and enable DSLs to be retargeted to other frameworks transparently.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {145–154},
numpages = {10},
keywords = {code generation, parallel programming, multi-stage programming, domain-specific languages}
}

@inproceedings{10.1145/3535044.3535053,
author = {Mori, Tatsuma and Furukawa, Daiki and Motoyoshi, Keigo and Ikehara, Haruto and Ohira, Kaito and Manabe, Taito and Shibata, Yuichiro and Ueno, Tomohiro and Sano, Kentaro},
title = {Stream Computation of 3D Approximate Convex Hulls with an FPGA},
year = {2022},
isbn = {9781450396608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535044.3535053},
doi = {10.1145/3535044.3535053},
abstract = {The convex hull is the minimum convex set which encloses a given point set. A problem to find convex hulls is not only one of the most fundamental algorithms in computer geometry, but also has a wide variety of practical applications such as robotics and geographic informatics. This paper proposes and evaluates an efficient pipelined FPGA implementation of approximate convex hull computing for 3D points. The proposed architecture does not require the input points to be sorted in advance, and can execute the algorithm in a pipelined manner without storing all the points in memory. We implemented the architecture on an Intel Stratix 10 FPGA to reveal the tradeoff relationship among its performance, resource usage, and approximation accuracy. As a result, we demonstrated 9 to 115 times faster performance compared to the convex hull software library Qhull, which was run on the Intel Core i9-9900K. The accuracy assessment revealed that the approximation error normalized to the diameters of point sets was only 0.037% to 3.173%, which was acceptably small for practical use cases.},
booktitle = {International Symposium on Highly-Efficient Accelerators and Reconfigurable Technologies},
pages = {69–75},
numpages = {7},
keywords = {approximation algorithm, FPGA, hardware algorithm, convex hull},
location = {Tsukuba, Japan},
series = {HEART2022}
}

@proceedings{10.1145/1570506,
title = {STOP '09: Proceedings for the 1st Workshop on Script to Program Evolution},
year = {2009},
isbn = {9781605585437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Recent years have seen increased interest in scripting languages, notably script to program migration/evolution and the interplay between typed and untyped code.Scripting languages are lightweight, dynamic programming languages designed to maximize productivity by offering high-level abstractions and reducing the syntactic overhead found in most system's languages. The rising popularity of scripting languages such as Perl, Python, PHP, Tcl, JavaScript, Ruby, and Groovy have many underlying causes: they allow partial execution of programs, permitting easy unit testing, interactive experimentation, and even demoing of software at all times; their support for powerful and flexible high-level datatypes and dynamic typing admits quick interim solutions that can later be revised; etc. In short, scripting languages optimize development time rather than machine time, a good approach early in the software development life cycle.However, once the understanding of the system has reached a critical point and requirements have stabilized, scripting languages become less appealing. The compromises made to optimize development time make it harder to reason about program correctness, harder to do semantic-preserving refactorings, and harder to optimize execution speed. The lack of type information makes the code harder to navigate.This situation often leads to a rewrite of a program in a less dynamic language, which may be costly and may introduce many bugs due to human error or semantic differences between the scripting language and the new target language (e.g., lack of garbage collection in C++, stricter type rules in Java, differences in available libraries, etc.). Sometimes only parts of the program are reimplemented, e.g., in C or assembler, as an optimization technique for a particularly computation-intensive method. Bridging a high-level scripting language to lower-level C introduces new opportunities for errors, possibly introduces platform-specific ties, and increases the number of languages a programmer must know to maintain the system. Both these approaches, especially the first one, have the downside of slowing down future development as they effectively preclude further use of the scripting language for prototyping new features.Recently, the concept of pluggable types has been proposed, which encourages the use of different optional type systems which do not have any effect on the run-time semantics of the programming language. It is believed that untyped scripts can be retrofitted to work with pluggable type systems (2), but with very few exceptions (i.e., Strongtalk), practical reports are yet to be found. Various ways of integrating or interfacing typed with untyped code have been proposed, like gradual typing, which allows for evolving script code to a more "safe state" more suitable for program analysis and compile-time optimisations.The STOP workshop is interested in evolution of scripts in the sense of largely untyped pieces of code into safer programs, with more rigid structure and constrained behaviour through the use of gradual/hybrid/pluggable typing, optional contract checking, extensible languages, refactoring tools, and the like. The goal is to further the understanding of such systems in practise, and connect practise and theory.STOP's subject areas have recently seen increased interest by several research groups, but there is no common understanding of how gradual/hybrid/pluggable typing, script to program evolution, typed to untyped evolution, etc. interplay, and there are few experience reports from extant languages that support pluggable types or optional type annotations (Cecil, CLOS, Dylan, PLT Scheme, Strongtalk, and others). This workshop aims to bring researchers together for passionate discussion about these topics, and to promote not only the theory, but practical evalution of these ideas, and experience reports.},
location = {Genova, Italy}
}

@inproceedings{10.1145/1944862.1944868,
author = {Daloukas, Konstantis and Antonopoulos, Christos D. and Bellas, Nikolaos},
title = {GLOpenCL: OpenCL Support on Hardware- and Software-Managed Cache Multicores},
year = {2011},
isbn = {9781450302418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944862.1944868},
doi = {10.1145/1944862.1944868},
abstract = {OpenCL is an industry supported standard for writing programs that execute on multicore platforms as well as on accelerators, such as GPUs or the SPEs of the Cell B.E. In this paper we introduce GLOpenCL, a unified development framework which supports OpenCL on both homogeneous, shared memory, as well as on heterogeneous, distributed memory multicores. The framework consists of a compiler, based on the LLVM compiler infrastructure, and a run-time library, sharing the same basic architecture across all target platforms. The compiler recognizes OpenCL constructs, performs source-to-source code transformations targeting both efficiency and semantic correctness, and adds calls to the run-time library. The latter offers functionality for work creation, management and execution, as well as for data transfers. We evaluate our framework using benchmarks from the distributions of OpenCL implementations by hardware vendors. We find that our generic system performs comparably or better than customized, platform-specific vendor distributions. OpenCL is designed and marketed as a write-once run-anywhere software development framework. However, the standard leaves enough room for target platform specific optimizations. Our experimentation with different, customized implementations of kernels reveals that optimized, hardware mapped implementations are both possible and necessary in the context of OpenCL -- especially on non-conventional multicores -- if performance is considered a higher priority than programmability.},
booktitle = {Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers},
pages = {15–24},
numpages = {10},
keywords = {OpenCL, software-managed cache multicores, compilers, hardware-managed cache multicores, runtime},
location = {Heraklion, Greece},
series = {HiPEAC '11}
}

@inproceedings{10.1145/2530544.2530551,
author = {Nelson, Andrew and Molnos, Anca and Nejad, Ashkan Beyranvand and Mirzoyan, Davit and Cotofana, Sorin and Goossens, Kees},
title = {Embedded Computer Architecture Laboratory: A Hands-on Experience Programming Embedded Systems with Resource and Energy Constraints},
year = {2012},
isbn = {9781450317658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2530544.2530551},
doi = {10.1145/2530544.2530551},
abstract = {Embedded systems are complex, requiring multi-disciplinary skills for their design. Developing appropriate educational curricula is a non trivial problem. Embedded system design requires both theoretical and practical understanding. It is common in embedded system education to provide practical laboratory sessions to put into practice what is learnt from lectures and textbooks.In this paper, we present our embedded systems laboratory that is given as part of the Embedded Computer Architecture (ECA) module at Delft University of Technology. Our laboratory provides practical, hands-on experience of programming a multiprocessor embedded system, that is prototyped on an FPGA. We provide details of the hardware platform and software APIs that are provided to the students, along with the laboratory assignment that was given to the ECA students in the 2011-2012 academic year. We present example results that were submitted by groups taking part in the laboratory, and describe the lessons we learned from our own practical experience of giving the laboratory.},
booktitle = {Proceedings of the Workshop on Embedded and Cyber-Physical Systems Education},
articleno = {7},
numpages = {8},
keywords = {education, multiprocessor system on chip, embedded system design},
location = {Tampere, Finland},
series = {WESE '12}
}

@inproceedings{10.1145/544220.544313,
author = {Ray, Joyce and Dale, Robin and Moore, Reagan and Reich, Vicky and Underwood, William and McCray, Alexa T.},
title = {Panel on Digital Preservation},
year = {2002},
isbn = {1581135130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/544220.544313},
doi = {10.1145/544220.544313},
abstract = {Digital information in any form is at risk. Software and hardware become obsolete, and versions and file formats change, making data inaccessible. Data stored in even the simplest form are in danger due to computer media degradation and obsolescence. On-line information such as e-journals and databases are susceptible. They may become partially or entirely unreadable, and may not be recoverable by the time the problem is detected. Preservation strategies such as emulation (keeping alive the software and hardware needed to access a digital object), migration (converting the digital object to new versions and formats), and other long-term archival methods have been proposed [1-7]. Models such as the Open Archival Information System (OAIS) provide an architecture for conducting digital preservation research and experimentation [8-10]. The importance of preservation metadata has been recognized by a number of groups and efforts to develop and deploy metadata standards are underway [11-14].As more and more digital information is created, attention must be paid to what information should be preserved and how it can be preserved most economically and effectively. It is clear that for preservation to be successful, we need to pay attention not only to the format of digital objects, but also to the commitment we make to providing long-term access to the information. Thus, decisions about digital preservation will involve technical issues as well as economic, legal, social, and organizational ones. Is it possible or feasible to preserve all digital data automatically and in a cost effective way? How much functionality can or must be preserved? What type of metadata will be needed to ensure both access and preservation? What metrics do we use to evaluate whether our methods will be successful.Panelists will make short presentations about work in which they have been involved and which reflect a variety of aspects of digital preservation. Reagan Moore will discuss the levels of abstraction that are needed to create infrastructure independent representations for data, information, and knowledge, and he will discuss a prototype persistent digital archive. The persistent archive infrastructure has been developed for use by the National Archives and Records Administration and other Federal agencies. William Underwood will report on lessons learned in preserving digital records created on personal computers. The records being examined are the digital records created on personal computers during the administration of President George Bush (1988-1992). Vicky Reich will present work on the LOCKSS (Lots of Copies Keep Stuff Safe) project, which is a permanent web publishing and access system. LOCKSS software allows libraries to retain local collection control of materials delivered through the web while preserving the functionality of the original web based content. Robin Dale will report on activities of the preservation program of the Research Libraries Group (RLG). She will focus on the joint work of RLG and OCLC (Online Computer Library Center) on preservation metadata. Following the presentations by the four panelists, Alexa McCray will provide brief comments and then open the discussion for audience participation.},
booktitle = {Proceedings of the 2nd ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {365–367},
numpages = {3},
keywords = {digital preservation, digital libraries, metadata, archival systems},
location = {Portland, Oregon, USA},
series = {JCDL '02}
}

@inproceedings{10.1145/3551902.3551962,
author = {Duarte Maia, Jo\~{a}o Tiago and Figueiredo Correia, Filipe},
title = {Service Mesh Patterns},
year = {2023},
isbn = {9781450395946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551902.3551962},
doi = {10.1145/3551902.3551962},
abstract = {As the benefits and applicability of microservice architectures become better understood by the software industry, and this architecture becomes increasingly more adopted for building stable, independent and scalable cloud applications, a new set of concerns have alerted developers regarding communication between the different microservices. A service mesh tries to address this issue by creating a clear separation of concerns between application logic and the infrastructure needed for the communication between the different services. This is accomplished by abstracting the cross-cutting concerns related with communication out of the internal services making it possible to be reused by the different services. Existing literature describes a service mesh pattern and a sidecar pattern. This paper leans on these patterns and proposes six patterns found by observing the, what is commonly called, good practices. The six patterns are service mesh, shared communication library, node agent, sidecar, service mesh team and control plane per cluster.},
booktitle = {Proceedings of the 27th European Conference on Pattern Languages of Programs},
articleno = {2},
numpages = {12},
keywords = {Design Patterns, Control Plane, Data Plane, Architectural Patterns, Cloud-native, Microservices, Service Mesh},
location = {Irsee, Germany},
series = {EuroPLop '22}
}

@inproceedings{10.5555/317498.317696,
author = {Cheatham, Thomas E.},
title = {Process Programming and Process Models},
year = {1990},
isbn = {0818621044},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {At the Fourth Software Process Workshop the final session was devoted to identifying “emerging issues”. The issues that were identified fell into two categories. One had to do with language issues — how to provide the facilities that support the writing of the process programs that implement process models. The other had to do with reporting on experience with writing, evaluating, and sharing process models.Since the last workshop we have made significant progress on several of the issues concerned with specifying and implementing process programs. In the paragraphs below we will comment briefly on these.The context in which we have explored these issues is the E-L System, a new programming language and supporting environment that became operational in prototype form in the Summer of 1989. While we cannot provide much detail about E-L here, we believe that it provides many of the facilities — including both language and system facilities — required to support process modeling and programming.A central concern of E-L is extensibility, both in the language as well as the environment. The environment has a shared software development database, accommodates multiple simultaneous users, and facilitates communication and cooperation among a variety of users.From the user's point of view, the material that would constitute a “program” or “module” in a conventional environment is, in E-L, organized into documents. A document has parts, recursively, and, ultimately, is composed of various objects like program fragments, text, and pictures.We call document as well as their constituents artifacts. An artifact is typed — examples being documents, text in LATEX format, pictures in postscript, and E-L program fragments in the surface syntax or some extension of it.1 In general, an artifact includes other artifacts and has a predecessor2. Once an artifact has been created (and committed) it is not subject to change, although an edit that starts with a copy of it may be the basis for developing its successor. A given artifact is, typically, shared in the sense of being included in several different documents. Examples of sharing include references to library entities, a reference to some artifact in the “current” version of some document as well as references to that same artifact in “previous” versions, and a reference to an artifact in the “current release” of a product as well as in one or more “current developments” of that product.There are three major reasons for using documents and artifacts in the organizing framework for E-L. The first is to provide for literate programming in the sense articulated by Knuth. That is, program constructs like functions and types are presented in the context of a document3 that motivates, explains, and defends the design decisions that underlie these constructs. An “implementation module” is simply the collection of leaves of some document that are program entities. The second reason is to provide for sharing of artifacts so that, given two versions of some document, we can identify what is common and what is different about them. The third reason is that the structure provided facilitates the addition of new types of artifacts.The user interface to E-L provides each user with a “window” into the collection of artifacts that are of interest to that user and supports three main activities. The first is browsing — following various “links” to inspect some collection of artifacts and attributes. In addition to the includes and included-by links that provide a natural way to navigate within a document, there are other links that may be followed like calls, called-by, and recently-modified. The second activity that is supported is editing — evolving some document or set of documents. The editor insures that the integrity of artifacts is maintained.4The third activity is program execution, usually aided by a collection of sophisticated debugging tools.The user is not concerned with conventional tool invocation — calling compilers, analyzers, text processing tools, and the like. In general, such tools, whose purpose is to derive new artifacts or attributes, are scheduled automatically and run opportunistically.A number of the programming language and environment issues that were identified at the last workshop are dealt with directly by E-L, including: The basic system provides facilities for dealing with concurrency and communication, including mechanisms that provide “notification” to interested users upon the occurrence of certain events.The artifact machinery provides the basis for sharing and containment as well as for hierarchies and decomposition.The type system of the E-L language is quite rich and permits user-defined type constructors, expandable unions, and type templates — “types”, useable in function headers, that contain parameters that are bound for use in function bodies.Several other issues are being investigated: Long Term Execution A model for writing programs that execute indefinitely has been developed, a mechanism that transforms such models into program fragments that contain the necessary constructs to insure the persistence of such programs between “active” periods has been designed, and a prototype implementation of the facility is scheduled to be operational in the Fall of 1989.Rules An extension to E-L that provides for rules that react to changes in the database and permit arbitrary programmed responses to such changes has been designed. A preliminary overview of this extension is provided in [Che89].Sketches A “sketch” is a representation of a process program that depicts the process as a state transition diagram. We are currently designing a sketch facility for which we expect to have an operational prototype early in 1990.Specific Process Models We are currently developing specifications for several process models, probably including models for handling software trouble reports, cooperative program development and modification, and developing and maintaining families of programs targeted for a variety of parallel architectures. We are also investigating a process model that has nothing to do with the software process but is concerned with the USAF process for issuing RFPs.Support for Process Modeling and Analysis We are presently investigating the use of Colored Hierarchical Petri Nets (see [Jen86]) for high level specification of process models as well as the use of the Design/CPM tool5 construct graphical representations of these models. Our experience to this point is very positive.},
booktitle = {Proceedings of the 5th International Software Process Workshop on Experience with Software Process Models},
pages = {49–51},
numpages = {3},
location = {Kennebunkport, Maine, USA},
series = {ISPW '90}
}

@inproceedings{10.1109/ISCA.2006.9,
author = {McDonald, Austen and Chung, JaeWoong and Carlstrom, Brian D. and Minh, Chi Cao and Chafi, Hassan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Architectural Semantics for Practical Transactional Memory},
year = {2006},
isbn = {076952608X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCA.2006.9},
doi = {10.1109/ISCA.2006.9},
abstract = {Transactional Memory (TM) simplifies parallel programming by allowing for parallel execution of atomic tasks. Thus far, TM systems have focused on implementing transactional state buffering and conflict resolution. Missing is a robust hardware/software interface, not limited to simplistic instructions defining transaction boundaries. Without rich semantics, current TM systems cannot support basic features of modern programming languages and operating systems such as transparent library calls, conditional synchronization, system calls, I/O, and runtime exceptions. This paper presents a comprehensive instruction set architecture (ISA) for TM systems. Our proposal introduces three key mechanisms: two-phase commit; support for software handlers on commit, violation, and abort; and full support for open- and closed-nested transactions with independent rollback. These mechanisms provide a flexible interface to implement programming language and operating system functionality. We also show that these mechanisms are practical to implement at the ISA and microarchitecture level for various TM systems. Using an execution-driven simulation, we demonstrate both the functionality (e.g., I/O and conditional scheduling within transactions) and performance potential (2.2\texttimes{} improvement for SPECjbb2000) of the proposed mechanisms. Overall, this paper establishes a rich and efficient interface to foster both hardware and software research on transactional memory.},
booktitle = {Proceedings of the 33rd Annual International Symposium on Computer Architecture},
pages = {53–65},
numpages = {13},
series = {ISCA '06}
}

@article{10.1145/1150019.1136491,
author = {McDonald, Austen and Chung, JaeWoong and Carlstrom, Brian D. and Minh, Chi Cao and Chafi, Hassan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Architectural Semantics for Practical Transactional Memory},
year = {2006},
issue_date = {May 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {0163-5964},
url = {https://doi.org/10.1145/1150019.1136491},
doi = {10.1145/1150019.1136491},
abstract = {Transactional Memory (TM) simplifies parallel programming by allowing for parallel execution of atomic tasks. Thus far, TM systems have focused on implementing transactional state buffering and conflict resolution. Missing is a robust hardware/software interface, not limited to simplistic instructions defining transaction boundaries. Without rich semantics, current TM systems cannot support basic features of modern programming languages and operating systems such as transparent library calls, conditional synchronization, system calls, I/O, and runtime exceptions. This paper presents a comprehensive instruction set architecture (ISA) for TM systems. Our proposal introduces three key mechanisms: two-phase commit; support for software handlers on commit, violation, and abort; and full support for open- and closed-nested transactions with independent rollback. These mechanisms provide a flexible interface to implement programming language and operating system functionality. We also show that these mechanisms are practical to implement at the ISA and microarchitecture level for various TM systems. Using an execution-driven simulation, we demonstrate both the functionality (e.g., I/O and conditional scheduling within transactions) and performance potential (2.2\texttimes{} improvement for SPECjbb2000) of the proposed mechanisms. Overall, this paper establishes a rich and efficient interface to foster both hardware and software research on transactional memory.},
journal = {SIGARCH Comput. Archit. News},
month = {may},
pages = {53–65},
numpages = {13}
}

@inproceedings{10.1145/1529282.1529430,
author = {Garcia-Ojeda, Juan C. and DeLoach, Scott A. and Robby},
title = {AgentTool Process Editor: Supporting the Design of Tailored Agent-Based Processes},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529430},
doi = {10.1145/1529282.1529430},
abstract = {This paper describes the agentTool Process Editor (APE), an Eclipse plug-in based on the Eclipse Process Framework. The aim of APE is to facilitate the design, verification, and management of custom agent-oriented software development processes. APE provides five basic structures. The Library is a repository of agent-oriented method fragments. A Process Editor allows the management of tailored processes. Task Constraints help process engineers specify guidelines to constrain how tasks can be assembled, while a Process Consistency mechanism verifies the consistency of tailored processes against those constraints. Finally, the Process Management integrates APE with the agentTool III development environment and provides a way to measure project progress using Earned Value Analysis.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {707–714},
numpages = {8},
keywords = {CASE tool, agentTool process editor, method engineering, O-MaSE, agentTool III},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/3477132.3483589,
author = {Kim, Wook-Hee and Krishnan, R. Madhava and Fu, Xinwei and Kashyap, Sanidhya and Min, Changwoo},
title = {PACTree: A High Performance Persistent Range Index Using PAC Guidelines},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483589},
doi = {10.1145/3477132.3483589},
abstract = {Non-Volatile Memory (NVM), which provides relatively fast and byte-addressable persistence, is now commercially available. However, we cannot equate a real NVM with a slow DRAM, as it is much more complicated than we expect. In this work, we revisit and analyze both NVM and NVM-specific persistent memory indexes. We find that there is still a lot of room for improvement if we consider NVM hardware, its software stack, persistent index design, and concurrency control. Based on our analysis, we propose Packed Asynchronous Concurrency (PAC) guidelines for designing high-performance persistent index structures. The key idea behind the guidelines is to 1) access NVM hardware in a packed manner to minimize its bandwidth utilization and 2) exploit asynchronous concurrency control to decouple the long NVM latency from the critical path of the index.We develop PACTree, a high-performance persistent range index following the PAC guidelines. PACTree is a hybrid index that employs a trie index for its internal nodes and B+-tree-like leaf nodes. The trie index structure packs partial keys in internal nodes. Moreover, we decouple the trie index and B+-tree-like leaf nodes. The decoupling allows us to prevent blocking concurrent accesses by updating internal nodes asynchronously. Our evaluation shows that PACTree outperforms state-of-the-art persistent range indexes by 7x in performance and 20x in 99.99 percentile tail latency.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {424–439},
numpages = {16},
keywords = {Non-volatile Memory, Index structures},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{10.5555/2872965.2872979,
author = {Denil, Joachim and Meyers, Bart and De Meulenaere, Paul and Vangheluwe, Hans},
title = {Explicit Semantic Adaptation of Hybrid Formalisms for FMI Co-Simulation},
year = {2015},
isbn = {9781510801059},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {With the advent of Software-Intensive and Cyber-Physical Systems, hybrid formalisms can be used to intuitively model the interactions of different models in different formalisms. Hybrid formalisms combine discrete (time/event) model constructs with continuous-time model constructs. These hybrid formalisms usually require a dedicated simulator. In this work we explicitly model the interfaces involved in the semantic adaptation of different formalisms and implement the execution using the Functional Mock-up Interface standard for co-simulation. The interfaces and co-simulation units are automatically generated using transformations. On the one hand, this allows tool builders to reuse the existing simulation tools without the need to create a new simulation kernel for the hybrid formalism. On the other hand, our approach supports the generation of different bus architectures to address different constraints, such as the use of hardware in the loop, the API of the legacy simulator, bus or processor load performance, and real-time constraints. We apply our approach to the modelling and (co-)simulation of an automotive power window.},
booktitle = {Proceedings of the Symposium on Theory of Modeling &amp; Simulation: DEVS Integrative M&amp;S Symposium},
pages = {99–106},
numpages = {8},
keywords = {co-simulation, functional mock-up interface, heterogeneous modelling, semantic adaptation, MDE},
location = {Alexandria, Virginia},
series = {DEVS '15}
}

@article{10.1145/3001910,
author = {Creech, Timothy and Barua, Rajeev},
title = {Transparently Space Sharing a Multicore Among Multiple Processes},
year = {2016},
issue_date = {December 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2329-4949},
url = {https://doi.org/10.1145/3001910},
doi = {10.1145/3001910},
abstract = {As hardware becomes increasingly parallel and the availability of scalable parallel software improves, the problem of managing multiple multithreaded applications (processes) becomes important. Malleable processes, which can vary the number of threads used as they run, enable sophisticated and flexible resource management. Although many existing applications parallelized for SMPs with parallel runtimes are in fact already malleable, deployed runtime environments provide no interface nor any strategy for intelligently allocating hardware threads or even preventing oversubscription. Prior research methods either depend on profiling applications ahead of time to make good decisions about allocations or do not account for process efficiency at all, leading to poor performance. None of these prior methods have been adapted widely in practice. This article presents the Scheduling and Allocation with Feedback (SCAF) system: a drop-in runtime solution that supports existing malleable applications in making intelligent allocation decisions based on observed efficiency without any changes to semantics, program modification, offline profiling, or even recompilation. Our existing implementation can control most unmodified OpenMP applications. Other malleable threading libraries can also easily be supported with small modifications without requiring application modification or recompilation.In this work, we present the SCAF daemon and a SCAF-aware port of the GNU OpenMP runtime. We present a new technique for estimating process efficiency purely at runtime using available hardware counters and demonstrate its effectiveness in aiding allocation decisions.We evaluated SCAF using NAS NPB parallel benchmarks on five commodity parallel platforms, enumerating architectural features and their effects on our scheme. We measured the benefit of SCAF in terms of sum of speedups improvement (a common metric for multiprogrammed environments) when running all benchmark pairs concurrently compared to equipartitioning—the best existing competing scheme in the literature. We found that SCAF improves on equipartitioning on four out of five machines, showing a mean improvement factor in sum of speedups of 1.04 to 1.11x for benchmark pairs, depending on the machine, and 1.09x on average.Since we are not aware of any widely available tool for equipartitioning, we also compare SCAF against multiprogramming using unmodified OpenMP, which is the only environment available to end users today. SCAF improves on the unmodified OpenMP runtimes for all five machines, with a mean improvement of 1.08 to 2.07x, depending on the machine, and 1.59x on average.},
journal = {ACM Trans. Parallel Comput.},
month = {nov},
articleno = {17},
numpages = {35},
keywords = {oversubscription, resource management, Multithreaded programming, parallelism, user-level scheduling}
}

@inproceedings{10.1145/3470496.3533042,
author = {Lichtenau, Cedric and Buyuktosunoglu, Alper and Bertran, Ramon and Figuli, Peter and Jacobi, Christian and Papandreou, Nikolaos and Pozidis, Haris and Saporito, Anthony and Sica, Andrew and Tzortzatos, Elpida},
title = {AI Accelerator on IBM Telum Processor: Industrial Product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533042},
doi = {10.1145/3470496.3533042},
abstract = {IBM Telum is the next generation processor chip for IBM Z and LinuxONE systems. The Telum design is focused on enterprise class workloads and it achieves over 40% per socket performance growth compared to IBM z15. The IBM Telum is the first server-class chip with a dedicated on-chip AI accelerator that enables clients to gain real time insights from their data as it is getting processed.Seamlessly infusing AI in all enterprise workloads is highly desirable to get real business insight on every transaction as well as to improve IT operation, security, and data privacy. While it would undeniably provide significant additional value, its application in practice is often accompanied by hurdles from low throughput if run on-platform to security concerns and inconsistent latency if run off-platform. The IBM Telum chip introduces an on-chip AI accelerator that provides consistent low latency and high throughput (over 200 TFLOPS in 32 chip system) inference capacity usable by all threads. The accelerator is memory coherent and directly connected to the fabric like any other general-purpose core to support low latency inference while meeting the system's transaction rate. A scalable architecture providing transparent access to AI accelerator functions via a non-privileged general-purpose core instruction further reduces software orchestration and library complexity as well as provides extensibility to the AI functions. On a global bank customer credit card fraud detection model, the AI accelerator achieves 22\texttimes{} speed up in latency compared to a general purpose core utilizing vector execution units. For the same model, the AI accelerator achieves 116k inferences every second with a latency of only 1.1 msec. As the system is scaled up from one chip to 32 chips, it performs more than 3.5 Million inferences/sec and the latency still stays very low at only 1.2 msec.This paper briefly introduces the IBM Telum chip and later describes the integrated AI accelerator. IBM Telum's AI accelerator architecture, microarchitecture, integration into the system stack, performance, and power are covered in detail.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {1012–1028},
numpages = {17},
keywords = {AI on server-class processor, Telum, enterprise workload AI, low-latency in-transaction inference, on-chip AI accelerator, z16},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3146347.3146356,
author = {Awan, Ammar Ahmad and Subramoni, Hari and Panda, Dhabaleswar K.},
title = {An In-Depth Performance Characterization of CPU- and GPU-Based DNN Training on Modern Architectures},
year = {2017},
isbn = {9781450351379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3146347.3146356},
doi = {10.1145/3146347.3146356},
abstract = {Traditionally, Deep Learning (DL) frameworks like Caffe, TensorFlow, and Cognitive Toolkit exploited GPUs to accelerate the training process. This has been primarily achieved by aggressive improvements in parallel hardware as well as through sophisticated software frameworks like cuDNN and cuBLAS. However, recent enhancements to CPU-based hardware and software has the potential to significantly enhance the performance of CPU-based DL training. In this paper, we provide a complete performance landscape of CPU- and GPU-based DNN training. We characterize performance of DNN training for AlexNet and ResNet-50 for a wide-range of CPU and GPU architectures including the latest Intel Xeon Phi (Knights Landing) processors and NVIDIA Pascal GPUs. We also present multi-node DNN training performance results for AlexNet and ResNet-50 using Intel Machine Learning Scaling (MLSL) Library and Intel-Caffe. In addition, we provide a CPU vs. GPU comparison for multi-node training using OSU-Caffe and Intel-Caffe. To the best of our knowledge, this is the first study that dives deeper into the performance of DNN training in a holistic manner yet provides an in-depth look at layer-wise performance for different DNNs. We provide multiple key insights: 1) Convolutions account for the majority of time (up to 83% time) consumed in DNN training, 2) GPU-based training continues to deliver excellent performance (up to 18% better than KNL) across generations of GPU hardware and software, and 3) Recent CPU-based optimizations like MKL-DNN and OpenMP-based thread parallelism leads to excellent speed-ups over under-optimized designs (up to 3.2X improvement for AlexNet training).},
booktitle = {Proceedings of the Machine Learning on HPC Environments},
articleno = {8},
numpages = {8},
keywords = {Unified Memory, High-Performance Computing, Caffe, Pascal Architecture, Deep Learning},
location = {Denver, CO, USA},
series = {MLHPC'17}
}

@inproceedings{10.1145/2500828.2500840,
author = {Hayashi, Akihiro and Grossman, Max and Zhao, Jisheng and Shirako, Jun and Sarkar, Vivek},
title = {Accelerating Habanero-Java Programs with OpenCL Generation},
year = {2013},
isbn = {9781450321112},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2500828.2500840},
doi = {10.1145/2500828.2500840},
abstract = {The initial wave of programming models for general-purpose computing on GPUs, led by CUDA and OpenCL, has provided experts with low-level constructs to obtain significant performance and energy improvements on GPUs. However, these programming models are characterized by a challenging learning curve for non-experts due to their complex and low-level APIs. Looking to the future, improving the accessibility of GPUs and accelerators for mainstream software developers is crucial to bringing the benefits of these heterogeneous architectures to a broader set of application domains. A key challenge in doing so is that mainstream developers are accustomed to working with high-level managed languages, such as Java, rather than lower-level native languages such as C, CUDA, and OpenCL.The OpenCL standard enables portable execution of SIMD kernels across a wide range of platforms, including multi-core CPUs, many-core GPUs, and FPGAs. However, using OpenCL from Java to program multi-architecture systems is difficult and error-prone. Programmers are required to explicitly perform a number of low-level operations, such as (1) managing data transfers between the host system and the GPU, (2) writing kernels in the OpenCL kernel language, (3) compiling kernels &amp; performing other OpenCL initialization, and (4) using the Java Native Interface (JNI) to access the C/C++ APIs for OpenCL.In this paper, we present compile-time and run-time techniques for accelerating programs written in Java using automatic generation of OpenCL as a foundation. Our approach includes (1) automatic generation of OpenCL kernels and JNI glue code from a parallel-for construct (forall) available in the Habanero-Java (HJ) language, (2) leveraging HJ's array view language construct to efficiently support rectangular, multi-dimensional arrays on OpenCL devices, and (3) implementing HJ's phaser (next) construct for all-to-all barrier synchronization in automatically generated OpenCL kernels. Our approach is named HJ-OpenCL. Contrasting with past approaches to generating CUDA or OpenCL from high-level languages, the HJ-OpenCL approach helps the programmer preserve Java exception semantics by generating both exception-safe and unsafe code regions. The execution of one or the other is selected at runtime based on the safe language construct introduced in this paper.We use a set of ten Java benchmarks to evaluate our approach, and observe performance improvements due to both native OpenCL execution and parallelism. On an AMD APU, our results show speedups of up to 36.7\texttimes{} relative to sequential Java when executing on the host 4-core CPU, and of up to 55.0x on the integrated GPU. For a system with an Intel Xeon CPU and a discrete NVIDIA Fermi GPU, the speedups relative to sequential Java are 35.7\texttimes{} for the 12-core CPU and 324.0\texttimes{} for the GPU. Further, we find that different applications perform optimally in JVM execution, in OpenCL CPU execution, and in OpenCL GPU execution. The language features, compiler extensions, and runtime extensions included in this work enable portability, rapid prototyping, and transparent execution of JVM applications across all OpenCL platforms.},
booktitle = {Proceedings of the 2013 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
pages = {124–134},
numpages = {11},
keywords = {GPGPU, OpenCL, Java, Habanero-Java},
location = {Stuttgart, Germany},
series = {PPPJ '13}
}

@inproceedings{10.1145/2818950.2818984,
author = {del Mundo, Carlo C. and Lee, Vincent T. and Ceze, Luis and Oskin, Mark},
title = {NCAM: Near-Data Processing for Nearest Neighbor Search},
year = {2015},
isbn = {9781450336048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818950.2818984},
doi = {10.1145/2818950.2818984},
abstract = {Emerging classes of computer vision applications demand unprecedented computational resources and operate on large amounts of data. In particular, k-nearest neighbors (kNN), a cornerstone algorithm in these applications, incurs significant data movement. To address this challenge, the underlying architecture and memory subsystems must vertically evolve to address memory bandwidth and compute demands. To enable large-scale computer vision, we propose a new class of associative memories called NCAMs which encapsulate logic with memory to accelerate k-nearest neighbors. We estimate that NCAMs can improve the performance of kNN by orders of magnitude over the best off-the-shelf software libraries (e.g., FLANN) and commodity platforms (e.g., GPUs).},
booktitle = {Proceedings of the 2015 International Symposium on Memory Systems},
pages = {274–275},
numpages = {2},
location = {Washington DC, DC, USA},
series = {MEMSYS '15}
}

@inproceedings{10.1145/322917.322923,
author = {Micco, Mary},
title = {An Undergraduate Curriculum in Expert Systems Design or Knowledge Engineering},
year = {1987},
isbn = {0897912187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322917.322923},
doi = {10.1145/322917.322923},
abstract = {By way of introduction, Chatham College is a small liberal arts college that has obtained a grant to implement a curriculum in artificial intelligence even with the minimal equipment and resources that we do possess.We had been involved with preparing and teaching courses to familiarize undergraduate students with the field of artificial intelligence for several years. Last year we decided to develop a cluster of advanced courses that would turn out 'knowledge engineers'. There is a serious shortage of such skilled people in the Pittsburgh area. We planned for them to go immediately to Industry and begin working on expert systems, with a wide range of applications. These included but were not limited to diagnostic systems, financial modelling applications, and shop floor management. Under the terms of our funding we also encouraged students to become entrepreneurs by first developing and then marketing their own expert systems targeted for specialized markets.Students selecting this upper level cluster have already completed our core computer science requirements and have a good math background. One of the key factors in this project has been that we worked closely with the business community seeking their input into the curriculum planning process. They also provided us with the internship opportunities we requested and gave the students exposure to a much wider range of equipment than we could afford to provide in house.The program has been designed to run on IBM PC's with 512K. We purchased copies of Golden Common Lisp and Turbo Prolog as well as using an Altos with Xenix and C to get us started. We are currently negotiating for a grant for 2 Hewlett Packard artificial intelligence workstations with four terminals each. These cost about $50,000 each and are far beyond our price range. Two things are encouraging. Prices are falling very rapidly and the other is that NSF and other foundations are beginning to make instrumentation grants available to smaller institutions for the purchase of sophisticated equipment of this type. Our most difficult task has proven to be finding faculty to help teach these courses. We have had to draw on local industry to help with part time instructorsThe whole cluster consists of 6 courses and a 6 credit internship. The first introductory course. Artificial Intelligence, involves an overview of the field. The history of the movement from its roots to its current popular status are explored. We based the course on Patrick Winston's text Artificial Intelligence. Each of the major research areas is explored. The approach taken was one of studying the classic toy problems. This made it possible to show the difficulties that have been encountered at a practical level and then to review progress in the field, while permitting students to gain actual programming experience. For example, in Natural Language we introduce the Eliza program as an early example of an attempt to use the keywords approach. Students are required to produce a modified conversation. We also look at the Turing test. Drawing from Rodger Schank's book The Cognitive Computer we go on to talk about scripts, using the Restaurant Script as an example. Students are asked to put together a travel script in teams of four. This section is wrapped up with a discussion of natural language programs today.In the area of vision research, so critical in Robotics, we used the classic toy blocks problem, encouraging the students to key it in and modify it. We found that actually working with the code helped the students to gain a much more vivid appreciation of the difficulties involved. Winston has an excellent presentation on the subject. It was difficult to improve on it.Having introduced two specialized problems in the field, we move the students into the challenge of finding a general problem solving technique. At this stage they have no difficulty in accepting the fact that no such algorithm can be developed and that it is rather a matter of determining the rules of thumb or heuristics governing the specific problem domain with a sufficiently large knowledge base to cover all likely contingencies and options.To help to clarify this point, we delve into game playing strategies. Students are asked to write a tic tac toe game wherein the computer learns from its own mistakes. The first version they are asked to produce simply stores patterns of play in an array. The program searches for a pattern match. If one is found it will play the next move in the pattern string. If none is found a move is picked at random from those available. As it plays it keeps adding to the number of pattern strings it recognizes. The key element in this program is that the last byte in the pattern string contains a score for that particular pattern. The score for each winning pattern is incremented by one each time it is selected. Losing patterns have one subtracted from their score. Eventually when the score reaches zero that particular pattern will not be played again. The principle involved is that of reward and punishment. The patterns that win are reinforced by means of higher scores, while losing patterns are punished by having points taken away.The next refinement students are asked to add in, is an algorithm that will assist the computer to pick a better move not at random but with a set of clearly specified rules. This has proven to be an interesting and challenging exercise for our students. It has provided them with a thorough backdrop for an in-depth discussion of game-playing strategies…brute force versus attempting to minimize the search path.To help students to gain a better appreciation of the work being done, we also made use of many examples of industrial applications captured on videotape from the Texas Instruments satellite broadcasts done last year. The lectures by various prominent figures were used to stimulate discussion of the relative benefits of artificial intelligence applications and the new technologies available.This leads very naturally to the final topic, the Japanese Challenge and 'our response to it, particularly in the development of new hardware. We look at the progress in VLSI chips and parallel processing that is making possible much larger and more sophisticated applications.Obviously it is also important to give the students training in the languages of the field. We have developed a course that concentrates on Lisp and then follow it with a course in Prolog. I would like to have added several other of the artificial intelligence languages including Ops5, and Flavors but decided to wait until we obtained the artificial intelligence workstations in order to keep our software costs down to manageable levels.The Lisp course, based on Golden Common Lisp, available for micros with 512k minimum, focuses on the structure of list processing languages in general and the use of primitive functions to build ever more complex functions to perform all sorts of information processing tasks on lists. It took a considerable amount of time to select the version of Lisp that would provide all the functions that we required for the course. While Golden Common Lisp was not as comprehensive as the version of Franz Lisp we had been running on a VAX mainframe, it did have enough features to support a first course.During the course students have a number of small assignments and are required to write a variety of functions. Here the emphasis is on the mechanics of list processing languages. We usually finish up with two larger assignments. The first is an inheritance problem where students explore the use of properties and a search problem where they are asked to get the computer to determine the best route from one city to any other when several options are available. A variety of search strategies are proposed and then compared…breadth first, depth first, hill climbing. The text we used for this course was Robert Wilensky's Lispcraft.Meanwhile, running concurrently, students have also been taking a course in Learning, Memory and Cognition, where they are studying in some depth human information processing and problem solving behavior. In our case, because we do not have the expertise in the Computer Science Department to handle this course it is being taught by the Psychology Department. The course is important because it provides a strong theoretical base upon which to build. If, in fact, we can develop computers that can think like humans and exercise judgment, then we must have a good working knowledge of how we do in fact perform these tasks ourselves.A constant theme running through these courses is the concept that there is no one problem solving technique but rather that in each type of problem specialised knowledge is needed and in addition we need to have rules of thumb or heuristics to guide us when more than one option is available.In the Prolog course students begin working with developing several small expert systems. We build a diagnostic system using the bicycle repair one as a model. Students are free to choose their own topic. They have ranged from diagnosing physical symptoms of illness to lawnmover repair. Here they are working with a very limited domain. As part of this course we discuss numerous examples of expert systems currently being used. Another assignment that students find quite challenging is the Monkey and Bananas toy problem, taken from Lee Brownston's book “Programming Expert Systems in Ops5: an introduction to rule based programming”. Here the problem domain is limited to a room with a number of objects in it. The monkey is presented with the problem of reaching the bananas no matter where they may be positioned in the room, even if they appear on the ceiling. Several other objects are in the room, including a ladder, a sofa. Students explore in some detail how to handle all the various sets of rules that are needed depending on the room layout and the actual location of the bananas.At the same time students are being given a course in C running in Unix to give them exposure to a portable language that could be called from the Lisp programs and that would help to make their applications run more efficiently. It was clear from the industry experts that we talked to that in many cases the expert systems are rewritten in C after first being developed in either Lisp or Prolog.Having built a reasonably firm foundation, the students are then moved into the Expert systems design course where they undertake more ambitious projects of their own under directed supervision. The first time this course was offered we did not yet have any industry contacts and were limited to what was available on campus. We selected as our project the design of an expert system that would provide the students some first hand experience at knowledge representation. They were to develop a system that would substitute for the reference librarian in the library. The great advantage of this project was that the expert whose knowledge base was to be tapped was readily available to the students.In the first few weeks of the course the students discussed the selection criteria for good projects. Then the class as a whole interviewed one of our reference librarians, Irma Smith who had agreed to participate as our resident expert. The class explained what they were trying to accomplish and decided with her input to use the case study approach. Each group of students then met with her individually to evaluate how she deals with 3 particular reference requests. They were asked to develop sets of rules for the three case studies assigned to them, and develop a prototype system. This took a couple of weeks. In the meantime they were getting lectures on managing costs and risks in project development.The next stage in the process involved meeting as a whole class and combining the best features of each of the different sets of rules. The variety was surprising, and promoted a great deal of heated discussion as the different teams defended their approach to the problem. By the time the students had merged all the rules they had come up with, it was clear that the rules could not simply be added at random. We discussed breaking the process down into clearly defined subsets of rules that would be accessed only when needed for specific applications. Modular design was the best available solution to the problem that presented itself and helped to ensure that the correct sets of rules would be triggered.The need for testing was obvious and each team was asked to test the merged system with the specific cases that they had started with. This necessitated tracing through the firing of the rules and tinkering with the firing order. This proved to be much more time consuming than we had anticipated.At this point we introduce the problem of integrating their little expert system into the existing environment of the library. Here we used the existing and proposed library automation projects and explored how to integrate our expert system as a front end to direct the patron to the appropriate on line databases. Students quickly realized that there was a great deal more involved than had appeared at first. They explored the interface with the on-line card catalog, discovered that they would also have to connect with the circulation system and the interlibrary loan system.The problem of technology transfer was discussed at length...why it occurs, how to recognize problems before they get out of hand, how to ease the introduction of new technology. Several of the library staff felt very threatened by the project and were very critical of the students' efforts.Following this step, students were asked to explore cost estimating techniques for sizeable systems. The next step involved a critical analysis of the user interface. The importance of considering the humans who must use the systems and different techniques for building friendlier user interfaces were explored. Particular emphasis was placed on the use of graphics in the front end design. Students were shown a number of interfaces, both good and bad then they were asked to develop a proposal for their own system. We did not require them to develop any code because of the time constraints.Although originally we intended to get the students to work on an industrial application in the Expert Systems Design course, the library project proved to be very successful particularly because the problems were reasonably familiar to the students and they have ready access to a real expert who can be interviewed at some length. Another great advantage from our point of view is that the project could be repeated each time the course is offered.Because of the need to integrate their systems with databases of various types, we strongly encourage students to also take a course in Data Base Management Systems. We also hope to add in a required course in Graphics or Human Factors in Systems with a strong emphasis on designing better front-ends.By the time students have completed this sequence of six courses in a minimum of two semesters, they are ready for an internship in industry helping to develop real, working expert systems. Our limited experience in finding internships suggests that many manufacturers are interested in diagnostic systems of one sort or another, and financial institutions are also beginning to be interested in possible applications to the financial management area. However it is still difficult to find people who are willing to help the students to have a good experience and there are not many good field work supervisors available. The “toy problem” approach to teaching has worked well for us. We start the students out with examples from Ph.D. dissertations then gradually build them up to designing substantial expert systems of their own, thereby providing them with practical experience with many different approaches and techniques.In closing I should mention that this 'expert systems design' track as we have chosen to call it is still very new and experimental. We are still learning from our mistakes.},
booktitle = {Proceedings of the 15th Annual Conference on Computer Science},
pages = {36–39},
numpages = {4},
location = {St. Louis, Missouri, USA},
series = {CSC '87}
}

@inproceedings{10.5555/800091.802963,
author = {Eanes, R. Sterling and Hitchon, Carl K. and Thall, Richard M. and Brackett, John W.},
title = {An Environment for Producing Well-Engineered Microcomputer Software},
year = {1979},
publisher = {IEEE Press},
abstract = {The Microprocessor Software Engineering Facility (MSEF) is an integrated set of software tools to support the development and maintenance of microcomputer software. The MSEF is hosted on a PDP-11 computer under the UNIX operating system but could support the production of software for many different microcomputers. The MSEF Change Control Library promotes defining, updating, and integrating parts of a software configuration, isolation of user work environments, and version control. The MSEF supports the organized testing of software components by associating test scenarios and test results with the components to be tested. The MSEF also provides automatic change logging with a configuration audit trail. A macro assembler supporting structured langauge constructs is included in the tool complement.},
booktitle = {Proceedings of the 4th International Conference on Software Engineering},
pages = {386–398},
numpages = {13},
location = {Munich, Germany},
series = {ICSE '79}
}

@article{10.1145/3446000,
author = {Osborn, Sarah},
title = {Replicated Computational Results (RCR) Report for “Adaptive Precision Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra Software”},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/3446000},
doi = {10.1145/3446000},
abstract = {The article by Flegar et&nbsp;al. titled “Adaptive Precision Block-Jacobi for High Performance Preconditioning in the Ginkgo Linear Algebra Software” presents a novel, practical implementation of an adaptive precision block-Jacobi preconditioner. Performance results using state-of-the-art GPU architectures for the block-Jacobi preconditioner generation and application demonstrate the practical usability of the method, compared to a traditional full-precision block-Jacobi preconditioner. A production-ready implementation is provided in the Ginkgo numerical linear algebra library.In this report, the Ginkgo library is reinstalled and performance results are generated to perform a comparison to the original results when using Ginkgo’s Conjugate Gradient solver with either the full or the adaptive precision block-Jacobi preconditioner for a suite of test problems on an NVIDIA GPU accelerator. After completing this process, the published results are deemed reproducible.},
journal = {ACM Trans. Math. Softw.},
month = {apr},
articleno = {15},
numpages = {4},
keywords = {replicated computational results, Krylov subspace methods, GPU, block-Jacobi preconditioning, adaptive precision}
}

@inproceedings{10.1145/2445196.2445407,
author = {Wallace, Charles and Kumar, Shreya},
title = {Communication Patterns: A Tool for Analyzing Communication in Emerging Computer Science Educational Practices (Abstract Only)},
year = {2013},
isbn = {9781450318686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2445196.2445407},
doi = {10.1145/2445196.2445407},
abstract = {We introduce Communication Patterns (CPs) as a tool for rigorous qualitative analysis of project communication. Using our library of student capstone project case studies as a test bed, we describe our approach to communication analysis. We identify, analyze and compare the communication, at various granularities, that takes place in student projects. The patterns allow us to objectively describe the structure (what, how, who, where, when, why) of the different forms of communication that occur in a software project. Patterns range from strategic (choice of genre) to tactical (wording or tone). We use CPs to sensitize students to their communication choices as developers and users. The CP framework is familiar to students (akin to design patterns) and generative (readily extendible to new scenarios). In addition, we as researchers and instructors plan to use CPs to assess how new pedagogical practices, moving from traditional classroom communication towards more blended learning, are changing the student experience and assess the effectiveness of current software project communication practices. We use CPs to analyze the communication that occurs in teaching computer use to novice senior citizens. The examples illustrate how CPs can be used to study communication practices in many different contexts of software development or use.},
booktitle = {Proceeding of the 44th ACM Technical Symposium on Computer Science Education},
pages = {729},
numpages = {1},
keywords = {analytical tool, communication, qualitative, education, case studies},
location = {Denver, Colorado, USA},
series = {SIGCSE '13}
}

@inproceedings{10.1145/3534678.3539059,
author = {Markov, Igor L. and Wang, Hanson and Kasturi, Nitya S. and Singh, Shaun and Garrard, Mia R. and Huang, Yin and Yuen, Sze Wai Celeste and Tran, Sarah and Wang, Zehui and Glotov, Igor and Gupta, Tanvi and Chen, Peng and Huang, Boshuang and Xie, Xiaowen and Belkin, Michael and Uryasev, Sal and Howie, Sam and Bakshy, Eytan and Zhou, Norm},
title = {Looper: An End-to-End ML Platform for Product Decisions},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539059},
doi = {10.1145/3534678.3539059},
abstract = {Modern software systems and products increasingly rely on machine learning models to make data-driven decisions based on interactions with users, infrastructure and other systems. For broader adoption, this practice must (i) accommodate product engineers without ML backgrounds, (ii) support finegrain product-metric evaluation and (iii) optimize for product goals. To address shortcomings of prior platforms, we introduce general principles for and the architecture of an ML platform, Looper, with simple APIs for decision-making and feedback collection. Looper covers the end-to-end ML lifecycle from collecting training data and model training to deployment and inference, and extends support to personalization, causal evaluation with heterogenous treatment effects, and Bayesian tuning for product goals. During the 2021 production deployment, Looper simultaneously hosted 440-1,000 ML models that made 4-6 million real-time decisions per second. We sum up experiences of platform adopters and describe their learning curve.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3513–3523},
numpages = {11},
keywords = {platform, machine learning, MLOps},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/1576702.1576705,
author = {P\"{u}schel, Markus},
title = {Automatic Synthesis of High Performance Mathematical Programs},
year = {2009},
isbn = {9781605586090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1576702.1576705},
doi = {10.1145/1576702.1576705},
abstract = {The evolution of computing platforms is at a historic inflection point. CPU frequency has stalled (in 2004 at about 3GHz), which means future performance gains will only be achievable due to increasing parallelism in the form of multiple cores and vector instructions sets. The impact on the developers of high performance libraries implementing important mathematical functionality such as matrix-multiplication, linear transforms, and many others is profound. Traditionally, an algorithm developer ensures correctness and minimizes the operations count. A software engineer then performs the actual implementation (in a compilable language like C) and performance optimization. However, on modern platforms, two implementations with the exact same operations count may differ by 10, 100, or even 1000x in runtime: instead, the structure of an algorithm becomes a major factor and determines how well it can be parallelized, vectorized, and matched to the memory hierarchy. Ideally, a compiler would perform all these tasks, but the current state of knowledge suggest that this may be inherently impossible for many types of code. The reason may be two-fold. First, many transformations, in particular for parallelism, require domain-knowledge that the compiler simply does possess. Second, often there are simply too many choices of transformations that the compiler cannot or does not know how to explore.As a consequence, the development of high performance libraries for mathematical functions becomes extraordinarily difficult, since the developer needs to have a good understanding of available algorithms, the target microarchitecture, and implementation techniques such as threading and vector instruction set such as SSE on Intel. To make things worse, optimal code is often platform specific, that is, code that runs very fast on one platform can be suboptimal on another. This means that if highest performance is desired, library developers are constantly forced to reimplement and reoptimize the same functionality. A commercial example following this model are Intel's IPP and MKL libraries, which provide a very broad set of mathematical functions needed in scientific computing, signal and image processing, communication, and security applications.An attractive solution would be to automate the library development, which means let the computer write the code and rewrite it for every new platforms. There are several challenges involved with this proposal. First, for a given desired function (such as multiplying matrices or computing a discrete Fourier transform), the existing algorithm knowledge has to be encoded into a form or language that is suitable for computer representation. Second, structural algorithm transformations for parallelism or locality that are typically performed by the programmer also have to be encoded into this form. Third, available choices have to be explored systematically and efficiently. As we will show for a specific domain, techniques from symbolic computation provide the answers.In this talk we present Spiral [6, 1], a domain-specific program generation system for important mathematical functionality such as linear transforms, filters, Viterbi decoders, and basic linear algebra routines. Spiral completely replaces the human programmer. For a desired function, Spiral generates alternative algorithms, optimizes them, compiles them into programs, and "intelligently"' searches for the best match to the computing platform. The main idea behind Spiral is a mathematical, symbolic, declarative, domain-specific language to represent algorithms and the use of rewriting systems to generate and structurally optimize algorithms at a high level of abstraction. Optimization includes parallelization, vectorization, and locality improvement for the memory hierarchy [3, 4, 5, 7, 2]. Experimental results show that the code generated by Spiral competes with, and sometimes outperforms, the best available human-written code.},
booktitle = {Proceedings of the 2009 International Symposium on Symbolic and Algebraic Computation},
pages = {5–6},
numpages = {2},
keywords = {program generation, vectorization, automation, domain-specific language, high performance, parallelization, rewriting, fourier transform, matrix algebra},
location = {Seoul, Republic of Korea},
series = {ISSAC '09}
}

@inproceedings{10.1145/2342441.2342461,
author = {Raghavendra, Ramya and Lobo, Jorge and Lee, Kang-Won},
title = {Dynamic Graph Query Primitives for SDN-Based Cloudnetwork Management},
year = {2012},
isbn = {9781450314770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2342441.2342461},
doi = {10.1145/2342441.2342461},
abstract = {The need to provide customers with the ability to configure the network in current cloud computing environments has motivated the Networking-as-a-Service (NaaS) systems designed for the cloud. Such systems can provide cloud customers access to virtual network functions, such as network-aware VM placement, real time network monitoring, diagnostics and management, all while supporting multiple device management protocols. These network management functionalities depend on a set of underlying graph primitives. In this paper, we present the design and implementation of the software architecture including a shared graph library that can support network management operations. Using the illustrative case of all pair shortest path algorithm, we demonstrate how scalable lightweight dynamic graph query mechanisms can be implemented to enable practical computation times, in presence of network dynamism.},
booktitle = {Proceedings of the First Workshop on Hot Topics in Software Defined Networks},
pages = {97–102},
numpages = {6},
keywords = {naas, graph},
location = {Helsinki, Finland},
series = {HotSDN '12}
}

@inproceedings{10.5555/1791889.1791947,
author = {Pabalkar, Amit and Shrivastava, Aviral and Kannan, Arun and Lee, Jongeun},
title = {SDRM: Simultaneous Determination of Regions and Function-to-Region Mapping for Scratchpad Memories},
year = {2008},
isbn = {9783540898931},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many programmable embedded systems feature low power processorscoupled with fast compiler controlled on-chip scratchpad memories (SPMs) toreduce their energy consumption. SPMs are more efficient than caches in termsof energy consumption, performance, area and timing predictability. However,unlike caches SPMs need explicit management by software, the quality ofwhich can impact the performance of SPM based systems. In this paper, wepresent a fully-automated, dynamic code overlaying technique for SPMs basedon pure static analysis. Static analysis is less restrictive than profiling and canbe easily extended to general compiler framework where the time consumingand expensive task of profiling may not be feasible. The SPM code mappingproblem is harder than bin packing problem, which is NP-complete. Therefore weformulate the SPMcode mapping as a binary integer linear programming problemand also propose a heuristic, determining simultaneously the region (bin) sizesas well as the function-to-region mapping. To the best of our knowledge, thisis the first heuristic which simultaneously solves the interdependent problemsof region size determination and the function-to-region mapping. We evaluateour approach for a set of MiBench applications on a horizontally split I-cache and SPM architecture (HSA). Compared to a cache-only architecture (COA),the HSA gives an average energy reduction of 35%, with minimal performancedegradation. For the HSA, we also compare the energy results from our proposedSDRM heuristic against a previous static analysis based mapping heuristic andobserve an average 27% energy reduction.},
booktitle = {Proceedings of the 15th International Conference on High Performance Computing},
pages = {569–582},
numpages = {14},
keywords = {compilers, static code analysis, scratchpad memory, code overlay},
location = {Bangalore, India},
series = {HiPC'08}
}

@inproceedings{10.5555/2484920.2485078,
author = {Gerard, Scott N. and Singh, Munindar P.},
title = {Evolving Protocols and Agents in Multiagent Systems},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We consider multiagent systems that involve two or more business partners interacting via autonomous software agents. A (business) protocol describes the messages exchanged by the agents in high-level terms. Such systems pose a major challenge with requirements evolution. Current approaches couple agent and protocol designs, requiring coordinated changes. In contrast, we propose an approach that decouples agent and protocol designs, while maintaining interoperability. We build on the well-known architectural construct of an interceptor. We introduce interaction refactorings to transform interactions in response to evolving requirements, with each refactoring incrementally changing agents, interceptors, and the protocol. We identify three main forms of requirements evolution and propose an extensible library of refactorings that help address each form. We demonstrate the approach through examples and a JADE prototype.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {997–1004},
numpages = {8},
keywords = {refactor, interaction, interceptor, agent communications, communication protocols, commitments},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3194793.3194797,
author = {Alsaeed, Ziyad and Young, Michal},
title = {Extending Existing Inference Tools to Mine Dynamic APIs},
year = {2018},
isbn = {9781450357548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194793.3194797},
doi = {10.1145/3194793.3194797},
abstract = {APIs often feature dynamic relations between client and service provider, such as registering for notifications or establishing a connection to a service. Dynamic specification mining techniques attempt to fill gaps in missing or decaying documentation, but current miners are blind to relations established dynamically. Because they cannot recover properties involving these dynamic structures, they may produce incomplete or misleading specifications. We have devised an extension to current dynamic specification mining techniques that ameliorates this shortcoming. The key insight is to monitor not only values dynamically, but also properties to track dynamic data structures that establish new relations between client and service provider. We have implemented this approach as an extension to the instrumentation component of Daikon, the leading example of dynamic invariant mining in the research literature. We evaluated our tool by applying it to selected modules of widely used software systems published on GitHub.},
booktitle = {Proceedings of the 2nd International Workshop on API Usage and Evolution},
pages = {23–26},
numpages = {4},
keywords = {design patterns, specification mining, dynamic analysis},
location = {Gothenburg, Sweden},
series = {WAPI '18}
}

@inproceedings{10.1145/2666310.2666488,
author = {Myers, Aaron and Movva, Sunil and Karthik, Rajasekar and Bhaduri, Budhendra and White, Devin and Thomas, Neil and Chase, Adrian},
title = {BioenergyKDF: Enabling Spatiotemporal Data Synthesis and Research Collaboration},
year = {2014},
isbn = {9781450331319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666310.2666488},
doi = {10.1145/2666310.2666488},
abstract = {The Bioenergy Knowledge Discovery Framework (BioenergyKDF) is a scalable, web-based collaborative environment for scientists working on bioenergy related research in which the connections between data, literature, and models can be explored and more clearly understood. The fully-operational and deployed system, built on multiple open source libraries and architectures, stores contributions from the community of practice and makes them easy to find, but that is just its base functionality. The BioenergyKDF provides a national spatiotemporal decision support capability that enables data sharing, analysis, modeling, and visualization as well as fosters the development and management of the U.S. bioenergy infrastructure, which is an essential component of the national energy infrastructure. The BioenergyKDF is built on a flexible, customizable platform that can be extended to support the requirements of any user community---especially those that work with spatiotemporal data. While there are several community data-sharing software platforms available, some developed and distributed by national governments, none of them have the full suite of capabilities available in BioenergyKDF. For example, this component-based platform and database independent architecture allows it to be quickly deployed to existing infrastructure and to connect to existing data repositories (spatial or otherwise). As new data, analysis, and features are added; the BioenergyKDF will help lead research and support decisions concerning bioenergy into the future, but will also enable the development and growth of additional communities of practice both inside and outside of the Department of Energy. These communities will be able to leverage the substantial investment the agency has made in the KDF platform to quickly stand up systems that are customized to their data and research needs.},
booktitle = {Proceedings of the 22nd ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {497–500},
numpages = {4},
keywords = {bioenergy, communities of practice, knowledge discovery, critical infrastructure, open-source, cyberinfrastructure, spatial data infrastructure, web-based, spatial visualization},
location = {Dallas, Texas},
series = {SIGSPATIAL '14}
}

@inproceedings{10.1145/1216919.1216936,
author = {Wee, Sewook and Casper, Jared and Njoroge, Njuguna and Tesylar, Yuriy and Ge, Daxia and Kozyrakis, Christos and Olukotun, Kunle},
title = {A Practical FPGA-Based Framework for Novel CMP Research},
year = {2007},
isbn = {9781595936004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1216919.1216936},
doi = {10.1145/1216919.1216936},
abstract = {Chip-multiprocessors are quickly gaining momentum in all segments of computing. However, the practical success of CMPs strongly depends on addressing the difficulty of multithreaded application development. To address this challenge, it is necessary to co-develop new CMP architecture with novel programming models. Currently, architecture research relies on software simulators which are too slow to facilitate interesting experiments with CMP software without using small datasets or significantly reducing the level of detail in the simulated models. An alternative to simulation is to exploit the rich capabilities of modern FPGAs to create FPGA-based platforms for novel CMP research. This paper presents ATLAS, the first prototype for CMPs with hardware support for Transactional Memory (TM), a technology aiming to simplify parallel programming. ATLAS uses the BEE2 multi-FPGA board to provide a system with 8 PowerPC cores that run at 100MHz and runs Linux. ATLAS provides significant benefits for CMP research such as 100x performance improvement over a software simulator and good visibility that helps with software tuning and architectural improvements. In addition to presenting and evaluating ATLAS, we share our observations about building a FPGA-based framework for CMP research. Specifically, we address issues such as overall performance, challenges of mapping ASIC-style CMP RTL on to FPGAs, software support, the selection criteria for the base processor, and the challenges of using pre-designed IP libraries.},
booktitle = {Proceedings of the 2007 ACM/SIGDA 15th International Symposium on Field Programmable Gate Arrays},
pages = {116–125},
numpages = {10},
keywords = {transactional memory, FPGA-based emulation, chip multi-processor},
location = {Monterey, California, USA},
series = {FPGA '07}
}

@article{10.1145/2736284,
author = {Massolino, Pedro Maat C. and Barreto, Paulo S. L. M. and Ruggiero, Wilson V.},
title = {Optimized and Scalable Co-Processor for McEliece with Binary Goppa Codes},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2736284},
doi = {10.1145/2736284},
abstract = {Asymmetric cryptographic primitives are essential to enable secure communications in public networks or public mediums. Such primitives can be deployed as software libraries or hardware co-processors, the latter being more commonly employed in systems on chip (SoC) scenarios, embedded devices, or application-specific servers. Unfortunately, the most commonly available solutions, based on RSA or elliptic curve cryptography (ECC), are highly processing intensive due to the underlying extended-precision modular arithmetic. Consequently, they are not available on highly constrained platforms. Aiming to tackle this issue, we here investigate an alternative asymmetric encryption scheme that relies on lightweight arithmetic: McEliece. This scheme is especially appealing because, being based on error correction codes, it displays a simpler arithmetic and leads to better performance when compared to RSA or ECC. To evaluate the implementation of this scheme in hardware, we propose and analyze a flexible architecture whose security level and time versus area usage characteristics can be reconfigured as desired. The proposed architecture is suitable to all usual security levels, ranging from 80 to 256 bits. It is also very efficient, being able to perform data decryption with binary Goppa codes in 56µs with 3,402 slices on a Xilinx Spartan-3AN FPGA, whereas the best-known result in the literature for the same FPGA is 115µs with 7,331 slices. Alternatively, the architecture can operate with quasi-dyadic Goppa (QD-Goppa) codes, which involves smaller keys than traditional binary Goppa codes. In the latter case, for an 80-bit security level, the decryption operation can take from 1.1ms with 1,129 slices to 68µs with 8,268 sices. By choosing a more hardware-friendly decoding algorithm, focusing hardware resources on most bottleneck operations and sharing hardware resource for two different algorithms, better results than the those in the literature were obtained.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {apr},
articleno = {45},
numpages = {32},
keywords = {Goppa codes, McEliece, Code-based encryption, FPGA, hardware}
}

@inproceedings{10.1145/1596627.1596637,
author = {Canou, Benjamin and Darrasse, Alexis},
title = {Fast and Sound Random Generation for Automated Testing and Benchmarking in Objective Caml},
year = {2009},
isbn = {9781605585093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596627.1596637},
doi = {10.1145/1596627.1596637},
abstract = {Numerous software testing methods involve random generation of data structures. However, random sampling methods currently in use by testing frameworks are not satisfactory: often manually written by the programmer or at best extracted in an ad-hoc way relying on no theoretical background. On the other end, random sampling methods with good theoretical properties exist but have a too high cost to be used in testing, in particular when large inputs are needed.In this paper we describe how we applied the recently developed Boltzmann model of random generation to algebraic data types. We obtain a fully automatic way to derive random generators from Objective Caml type definitions. These generators have linear complexity and, the generation method being uniform, can also be used as a sound sampling back-end for benchmarking tools.As a result, we provide testing and benchmarking frameworks with a sound and fast generation basis. We also provide a testing and benchmarking library, available for download, showing the viability of this experiment.},
booktitle = {Proceedings of the 2009 ACM SIGPLAN Workshop on ML},
pages = {61–70},
numpages = {10},
keywords = {algebraic data types, random generation, specification-based testing, Boltzmann model},
location = {Edinburgh, Scotland},
series = {ML '09}
}

@inproceedings{10.1145/3144763.3144765,
author = {G\"{o}tz, Markus and Book, Matthias and Bodenstein, Christian and Riedel, Morris},
title = {Supporting Software Engineering Practices in the Development of Data-Intensive HPC Applications with the JuML Framework},
year = {2017},
isbn = {9781450351355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144763.3144765},
doi = {10.1145/3144763.3144765},
abstract = {The development of high performance computing applications is considerably different from traditional software development. This distinction is due to the complex hardware systems, inherent parallelism, different software lifecycle and workflow, as well as (especially for scientific computing applications) partially unknown requirements at design time. This makes the use of software engineering practices challenging, so only a small subset of them are actually applied. In this paper, we discuss the potential for applying software engineering techniques to an emerging field in high performance computing, namely large-scale data analysis and machine learning. We argue for the employment of software engineering techniques in the development of such applications from the start, and the design of generic, reusable components. Using the example of the Juelich Machine Learning Library (JuML), we demonstrate how such a framework can not only simplify the design of new parallel algorithms, but also increase the productivity of the actual data analysis workflow. We place particular focus on the abstraction from heterogeneous hardware, the architectural design as well as aspects of parallel and distributed unit testing.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for High Performance Computing in Computational and Data-Enabled Science &amp; Engineering},
pages = {1–8},
numpages = {8},
keywords = {Architecture Design, Software Engineering, Data Analysis, Testing, High Performance Computing},
location = {Denver, CO, USA},
series = {SE-CoDeSE'17}
}

@inproceedings{10.1145/3342195.3387541,
author = {Orenbach, Meni and Baumann, Andrew and Silberstein, Mark},
title = {Autarky: Closing Controlled Channels with Self-Paging Enclaves},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387541},
doi = {10.1145/3342195.3387541},
abstract = {As the first widely-deployed secure enclave hardware, Intel SGX shows promise as a practical basis for confidential cloud computing. However, side channels remain SGX's greatest security weakness. Inparticular, the "controlled-channel attack" on enclave page faults exploits a longstanding architectural side channel and still lacks effective mitigation.We propose Autarky: a set of minor, backward-compatible modifications to the SGX ISA that hide an enclave's page access trace from the host, and give the enclave full control over its page faults. A trusted library OS implements an enclave self-paging policy.We prototype Autarky on current SGX hardware and the Graphene library OS, implementing three paging schemes: a fast software oblivious RAM system made practical by leveraging the proposed ISA, a novel page cluster abstraction for application-aware secure self-paging, and a rate-limiting paging mechanism for unmodified binaries. Overall, Autarky provides a comprehensive defense for controlled-channel attacks which supports efficient secure demand paging, and adds no overheads in page-fault free execution.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {7},
numpages = {16},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{10.1145/1882362.1882420,
author = {Payton, Jamie and Julien, Christine},
title = {Integrating Participatory Sensing in Application Development Practices},
year = {2010},
isbn = {9781450304276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882362.1882420},
doi = {10.1145/1882362.1882420},
abstract = {With the widespread capabilities of commodity mobile devices, applications will increasingly incorporate participatory sensing functionality. Participatory sensing directly involves end-users in collecting (and ultimately sharing) information about the environment. Applications that rely on participatory sensing range from those that simply enable information sharing, to environmental monitoring and response, and route and behavior planning. As more and more applications demand the incorporation of participatory sensing, it becomes imperative to create software architectures, design patterns, and programming libraries that enable the integration of participatory sensing with software engineering theory and practice. In this position paper, we explore the new challenges that participatory sensing applications present, specifically focusing on challenges that demand a reevaluation of software engineering design principles, tools, and techniques. For these challenges, we also posit possible ways forward.},
booktitle = {Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research},
pages = {277–282},
numpages = {6},
keywords = {design patterns, software architecture, participatory sensing, techniques, programming libraries, tools},
location = {Santa Fe, New Mexico, USA},
series = {FoSER '10}
}

@inproceedings{10.1145/501516.501568,
author = {Price, Jonathan},
title = {Taking an Object-Oriented Approach to Restructuring Legacy Documents for the Web},
year = {2001},
isbn = {1581132956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/501516.501568},
doi = {10.1145/501516.501568},
abstract = {We've entered the age of information recycling, but many of us have inherited whole libraries of legacy documents that are so poorly chunked and so incoherently written that we cannot easily move them into a Web site.In this intensive one-day workshop, you will learn how to adopt an object-oriented approach to editing current documentation to make it more effective on the Web. You will see how to rethink the purpose of each sentence, paragraph, and section, in reference material, process descriptions, and procedures, to prepare for extensive re-use in a variety of contexts, formats, and media. Done badly, the transformation of legacy materials ends in customer rebellion. Done well, it improves the quality, efficiency, and impact of your documentation.Working with hands-on exercises, discussion, and extensive readings, you will learn to reorganize, revise, and, if necessary, completely rewrite legacy materials, revising them for object-oriented disassembly and multiple re-use, modularizing them for random access, and developing guidelines for consistency across the site.The guidelines in this workshop come from usability research and practitioner lore, as well as my own experience consulting on large conversion projects. An extensive bibliography helps you look up relevant articles and books. This workshop does not promote or teach any particular software; the focus is on writing and editing skills you can apply in any situation. With this consistent, object-oriented approach to structure, you can convert the resulting documents into XML or SGML more easily than you could have transformed the tangled originals.Participants get a high-level overview of XML, because it is a formal way of describing rhetorical objects. Every structural analysis includes sample Document Type Descriptions (DTDs). But this workshop is not a hands-on course in XML. We look at XML because it provides a context in which we may soon be working, and its tags offer us a way of describing rhetorical objects in terms that external software and other organizations can recognize, allowing for additional re-use and manipulation of the material.},
booktitle = {Proceedings of the 19th Annual International Conference on Computer Documentation},
pages = {233},
numpages = {1},
location = {Sante Fe, New Mexico, USA},
series = {SIGDOC '01}
}

@article{10.1145/3110261,
author = {Protzenko, Jonathan and Zinzindohou\'{e}, Jean-Karim and Rastogi, Aseem and Ramananandro, Tahina and Wang, Peng and Zanella-B\'{e}guelin, Santiago and Delignat-Lavaud, Antoine and Hri\c{t}cu, C\u{a}t\u{a}lin and Bhargavan, Karthikeyan and Fournet, C\'{e}dric and Swamy, Nikhil},
title = {Verified Low-Level Programming Embedded in F*},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {ICFP},
url = {https://doi.org/10.1145/3110261},
doi = {10.1145/3110261},
abstract = {We present Low*, a language for low-level programming and verification, and its application to high-assurance optimized cryptographic libraries. Low* is a shallow embedding of a small, sequential, well-behaved subset of C in F*, a dependently- typed variant of ML aimed at program verification. Departing from ML, Low* does not involve any garbage collection or implicit heap allocation; instead, it has a structured memory model \`{a} la CompCert, and it provides the control required for writing efficient low-level security-critical code. By virtue of typing, any Low* program is memory safe. In addition, the programmer can make full use of the verification power of F* to write high-level specifications and verify the functional correctness of Low* code using a combination of SMT automation and sophisticated manual proofs. At extraction time, specifications and proofs are erased, and the remaining code enjoys a predictable translation to C. We prove that this translation preserves semantics and side-channel resistance. We provide a new compiler back-end from Low* to C and, to evaluate our approach, we implement and verify various cryptographic algorithms, constructions, and tools for a total of about 28,000 lines of code. We show that our Low* code delivers performance competitive with existing (unverified) C cryptographic libraries, suggesting our approach may be applicable to larger-scale low-level software.},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {17},
numpages = {29},
keywords = {\'{z}Theory of computation \'{z} Hoare logic, Type theory, \'{z}Software and its engineering \'{z} Correctness, Compilers, Functional languages, Semantics, Software verifcation, Source code generation}
}

@inproceedings{10.5555/2337159.2337171,
author = {Kayaalp, Mehmet and Ozsoy, Meltem and Abu-Ghazaleh, Nael and Ponomarev, Dmitry},
title = {Branch Regulation: Low-Overhead Protection from Code Reuse Attacks},
year = {2012},
isbn = {9781450316422},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Code reuse attacks (CRAs) are recent security exploits that allow attackers to execute arbitrary code on a compromised machine. CRAs, exemplified by return-oriented and jump-oriented programming approaches, reuse fragments of the library code, thus avoiding the need for explicit injection of attack code on the stack. Since the executed code is reused existing code, CRAs bypass current hardware and software security measures that prevent execution from data or stack regions of memory. While software-based full control flow integrity (CFI) checking can protect against CRAs, it includes significant overhead, involves non-trivial effort of constructing a control flow graph, relies on proprietary tools and has potential vulnerabilities due to the presence of unintended branch instructions in architectures such as x86---those branches are not checked by the software CFI. We propose branch regulation (BR), a lightweight hardware-supported protection mechanism against the CRAs that addresses all limitations of software CFI. BR enforces simple control flow rules in hardware at the function granularity to disallow arbitrary control flow transfers from one function into the middle of another function. This prevents common classes of CRAs without the complexity and run-time overhead of full CFI enforcement. BR incurs a slowdown of about 2% and increases the code footprint by less than 1% on the average for the SPEC 2006 benchmarks.},
booktitle = {Proceedings of the 39th Annual International Symposium on Computer Architecture},
pages = {94–105},
numpages = {12},
location = {Portland, Oregon},
series = {ISCA '12}
}

@article{10.1145/2366231.2337171,
author = {Kayaalp, Mehmet and Ozsoy, Meltem and Abu-Ghazaleh, Nael and Ponomarev, Dmitry},
title = {Branch Regulation: Low-Overhead Protection from Code Reuse Attacks},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0163-5964},
url = {https://doi.org/10.1145/2366231.2337171},
doi = {10.1145/2366231.2337171},
abstract = {Code reuse attacks (CRAs) are recent security exploits that allow attackers to execute arbitrary code on a compromised machine. CRAs, exemplified by return-oriented and jump-oriented programming approaches, reuse fragments of the library code, thus avoiding the need for explicit injection of attack code on the stack. Since the executed code is reused existing code, CRAs bypass current hardware and software security measures that prevent execution from data or stack regions of memory. While software-based full control flow integrity (CFI) checking can protect against CRAs, it includes significant overhead, involves non-trivial effort of constructing a control flow graph, relies on proprietary tools and has potential vulnerabilities due to the presence of unintended branch instructions in architectures such as x86---those branches are not checked by the software CFI. We propose branch regulation (BR), a lightweight hardware-supported protection mechanism against the CRAs that addresses all limitations of software CFI. BR enforces simple control flow rules in hardware at the function granularity to disallow arbitrary control flow transfers from one function into the middle of another function. This prevents common classes of CRAs without the complexity and run-time overhead of full CFI enforcement. BR incurs a slowdown of about 2% and increases the code footprint by less than 1% on the average for the SPEC 2006 benchmarks.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {94–105},
numpages = {12}
}

@inproceedings{10.5555/792768.793520,
author = {Vissers, K. A.},
title = {Trade-Offs in the Design of Mixed Hardware-Software Systems-a Perspective from Industry},
year = {1997},
isbn = {081867895X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many systems in the field of consumer electronics devices and computers consist of a hardware platform and of software running on that platform. In the design of these systems many trade-offs have to be made. In the design of the hardware platform trade-offs have to be made between programmable components and dedicated components. The programming of the hardware platform also contains many trade-offs. Here a "software architecture" needs to be developed that spans several layers, using well defined interfaces, e.g. application programming interfaces (APIs). The software contains often device drivers, an operating system, and end-user applications. In embedded systems the end-user can often not program the system directly, e.g. one cannot program the look and feel or contents of the on-screen display of your TV. In practical situations system design is based on many constraints, and seldom starts from scratch. The hardware interface to the system can be given, the models of processors that can be used can be limited, and software interfaces can be required. The trade-offs are in the hardware platform design and in the software design.},
booktitle = {Proceedings of the 5th International Workshop on Hardware/Software Co-Design},
pages = {65},
keywords = {operating system, well defined interfaces, end-user applications, hardware platform, programmable components, hardware interface, hardware-software systems design, application programming interfaces, software architecture, consumer electronics devices, high level synthesis, software interfaces, dedicated components, industry perspective, television, device drivers, on-screen display, embedded systems},
series = {CODES '97}
}

