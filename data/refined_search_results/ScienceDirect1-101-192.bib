@article{HAESAERT201775,
title = {Certified policy synthesis for general Markov decision processes: An application in building automation systems},
journal = {Performance Evaluation},
volume = {117},
pages = {75-103},
year = {2017},
issn = {0166-5316},
doi = {https://doi.org/10.1016/j.peva.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166531617301049},
author = {Sofie Haesaert and Nathalie Cauchi and Alessandro Abate},
keywords = {Verification, Synthesis, General Markov decision processes, Safety, Building automation systems, Temperature control},
abstract = {In this paper, we present an industrial application of new approximate similarity relations for Markov models, and show that they are key for the synthesis of control strategies. Typically, modern engineering systems are modelled using complex and high-order models which make the correct-by-design controller construction computationally hard. Using the new approximate similarity relations, this complexity is reduced and we provide certificates on the performance of the synthesised policies. The application deals with stochastic models for the thermal dynamics in a “smart building” setup: such building automation system set-up can be described by discrete-time Markov decision processes evolving over an uncountable state space and endowed with an output quantifying the room temperature. The new similarity relations draw a quantitative connection between different levels of model abstraction, and allow to quantitatively refine over complex models control strategies synthesised on simpler ones. The new relations, underpinned by the use of metrics, allow in particular for a useful trade-off between deviations over probability distributions on states and distances between model outputs. We develop a software toolbox supporting the application and the computational implementation of these new relations.}
}
@article{HORMANSEDER1985215,
title = {MUESLI, an operating system dedicated to teaching microprocessor software development},
journal = {Journal of Microcomputer Applications},
volume = {8},
number = {3},
pages = {215-229},
year = {1985},
issn = {0745-7138},
doi = {https://doi.org/10.1016/0745-7138(85)90002-8},
url = {https://www.sciencedirect.com/science/article/pii/0745713885900028},
author = {R. Hörmanseder and J.R. Mühlbacher and K. Stadler},
abstract = {This article describes a program development system for the 808085 processor family with strong emphasis on the support of inexperienced users. MUESLI is a dedicated system for teaching purposes that provides sufficient support for typical beginners' problems. It was designed for use in the ‘Mikroprozessor-Software-Labor’ at Johannes Kepler University, where each student has individual access to a computer. Therefore it is an important virtue of MUESLI that the system is operable even with rather limited hardware. The MUESLI system resides in the processor's memory and occupies a block of 16 kbyte ROM (or EPROM) plus 512 bytes RAM to hold alterable system parameters. Therefore all components (editor, assembler and debugger) are resident and accessable all the time. The programmer's data (source code etc.) is also held in RAM, enabling rapid switching between the development steps. This concept allows full use of the functions of the system even if the peripheral mass storage is of low quality. The assembler notation (FGD-2) and the command language of the system are very similar and their strictly logical construction contributes much to success in didactic applications. In connection with the intelligent screen handler and an extensive subroutine library MUESLI provides a high-quality program development environment.}
}
@article{MOON1990697,
title = {3D database searching and de novo construction methods in molecular design},
journal = {Tetrahedron Computer Methodology},
volume = {3},
number = {6, Part C},
pages = {697-711},
year = {1990},
note = {Three-dimensional chemical structure handling},
issn = {0898-5529},
doi = {https://doi.org/10.1016/0898-5529(90)90168-8},
url = {https://www.sciencedirect.com/science/article/pii/0898552990901688},
author = {Joseph B. Moon and W.Jeffrey Howe},
keywords = {3D database search, Molecular design,  molecular construction, Peptide design, Drug design},
abstract = {Computer-based lead finding algorithms which attempt to design, on a de novo basis, ligands that will complement a known receptor site cavity face some major problems in terms of a combinatorial design space and the synthesizability of the designed molecules. On the other hand, typical 3D database search methods provide a different set of challenges. Both of these approaches are ultimately pointed toward the same goal and can be used together productively. In this article we describe advances in both areas: we first describe extensions to our de novo ligand design software which combines (a) a tree-based conformational search over a library of fragments, and (b) a form of simulated annealing which allows designed ligands to crawl around the binding site even as their structures are changing. In the second part, we then discuss an implementation of the database approach which allows users to formulate 3D substructure, superstructure, or similarity queries based upon demonstrated or hypothetical requirements for activity. Finally, we draw the two approaches together with an example of current research interest, showing how one method can feed the other.}
}
@article{KAPUR1994307,
title = {An overview of the Tecton proof system},
journal = {Theoretical Computer Science},
volume = {133},
number = {2},
pages = {307-339},
year = {1994},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(94)90192-9},
url = {https://www.sciencedirect.com/science/article/pii/0304397594901929},
author = {D. Kapur and X. Nie and D.R. Musser},
abstract = {The Tecton proof system is an experimental tool for constructing proofs of first-order logic formulas and of program specifications expressed using formulas in Hoare's axiomatic proof formalism. It is designed to make interactive proof construction easier than with previous proof tools, by maintaining multiple proof attempts internally in a structured form called a proof forest; displaying them in an easy to comprehend form, using a combination of tabular formats, graphical representations, and hypertext links; and automating substantial parts of proofs through rewriting, induction, case analysis, and generalization inference mechanisms, along with a linear arithmetic decision procedure. Further development of the system is planned as part of an overall framework aimed at supporting the kind of abstractions and specializations necessary for building libraries of generic software and hardware components.}
}
@article{CASIMIR2000423,
title = {Raideur dynamique de plaques relative aux effets de membrane},
journal = {Mécanique & Industries},
volume = {1},
number = {4},
pages = {423-429},
year = {2000},
issn = {1296-2139},
url = {https://www.sciencedirect.com/science/article/pii/S1296213900010526},
author = {Jean-Baptiste Casimir and Claude Duforêt and Imad Tawfiq and Yvon Chevalier},
keywords = {plaques, eléments continus, raideur dynamique, effets de membrane, réponse harmonique, plates, continuum elements, dynamic stiffness, in-plane effects, harmonic response},
abstract = {Résumé
La méthode de la raideur dynamique également dénommée “méthode des éléments continus” est un outil efficace pour l'étude de la réponse des structures à une sollicitation harmonique. Quelques codes de calcul basés sur cette formulation ont permis de développer l'utilisation “industrielle” de cette méthode mais son champ d'application reste limité, pour l'essentiel, aux assemblages de poutres. Le développement d'éléments continus de type plaque est engagé depuis quelques années mais l'utilisation effective de ce type d'éléments pour la modélisation de structures tridimensionnelles passe par une formulation complète de ces éléments, c'est à dire relative à l'ensemble de leurs degrés de libertés. L'assemblage de ces éléments selon les trois dimensions de l'espace sera alors possible. L'objet de cet article est de compléter les travaux concernant la flexion en présentant la démarche permettant d'établir la matrice de raideur dynamique relative aux mouvements dans le plan d'une plaque rectangulaire. Un cas particulier de conditions aux limites est présenté de manière à illustrer la démarche, d'autres types de conditions aux limites sont traités de la même façon. Le cas de la plaque libre pourra ensuite être envisagé comme superposition de ces cas élémentaires.
In-plane dynamic stiffness matrix of plates. The “dynamic stiffness method”, also known as “continuum element method” is an efficient procedure to study very accurately the harmonic response of structures. Several software packages based on this formulation are used in an industrial context but the field of their application is limited to beam assemblies. The development of plate continuum elements has been in progress for several years but the effective use of these elements to model three-dimensional structures will not be possible until a complete formulation of these elements is achieved including in-plane and out-of-plane movements. Then, the assembly of these elements in the three dimensions of the space will be possible. This paper deals with the construction procedure of the dynamic stiffness matrix relative to the in-plane effects for a rectangular plate. The boundary conditions used are chosen in order to obtain simple expressions of the displacement solutions. Particular boundary conditions are used to illustrate the procedure but many other configurations are treated in such a way. The free plate could then be seen as a superposition of these elementary cases.}
}
@article{SHIRINZADEH199641,
title = {A CAD-Based hierarchical approach to interference detection among fixture modules in a reconfigurable fixturing system},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {12},
number = {1},
pages = {41-53},
year = {1996},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(95)00024-0},
url = {https://www.sciencedirect.com/science/article/pii/0736584595000240},
author = {Bijan Shirinzadeh},
abstract = {A reconfigurable fixturing system has been developed for a computer-integrated assembly environment. The fixturing system employs a number of fixture modules which are set-up, adjusted and changed automatically by the assembly robot. A dedicated software program has been developed for the design, analysis, and verification of the fixture layout. The software program has been integrated with a commercially available computer-aided design (CAD) package to provide a user-friendly platform for modeling and display purposes. The robot program for setting up, adjusting, and dismantling the designed fixture is generated automatically. Interference between fixture modules during the fixture construction may arise due to incorrect selection of the fixture contact points at the design stage. The objective of the work described here is to develop a hierarchical approach for calculation of interference between fixture modules in a reconfigurable fixturing system. The formulation for the interference detection employs geometrical constraints as the basis. The approach does not require detailed simulation of the fixture construction for interference detection.}
}
@incollection{WITTEN2003283,
title = {6 - Construction: Building collections with Greenstone},
editor = {Lan H. Witten and David Bainbridge},
booktitle = {How to Build a Digital Library},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {283-353},
year = {2003},
series = {The Morgan Kaufmann Series in Multimedia Information and Systems},
isbn = {978-1-55860-790-3},
doi = {https://doi.org/10.1016/B978-155860790-3/50009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607903500098},
author = {Lan H. Witten and David Bainbridge},
abstract = {Publisher Summary
This chapter focuses on Greenstone software and methodology of its usage and operations. In any digital library there is an important distinction between the processes involved in building collections and those involved in delivering the information they contain to users. It's easy to build simple collections with Greenstone. Greenstone is a comprehensive system for constructing and presenting collections of thousands or millions of documents, including text, images, audio, and video. A typical digital library contains many collections, individually organized— although they bear a strong family resemblance. Greenstone can be installed on any Windows or Linux computer; a standard installation program is used that includes precompiled binaries. Collections can be used locally on the computer where the software is installed; also, if this computer is connected to a network, remote users can access them using an ordinary Web browser. One of Greenstone's design features is that each collection can be organized in a different manner. Requirements vary from collection to collection in several ways. One is the format (or formats) in which the source documents are supplied—whether plain text, HTML, PostScript, PDF, Word, e-mail, or some other file type. A second dimension of variation is what metadata is available and how it is supplied—whether embedded in the document itself, perhaps using metadata expressed as ‘‘fields” in Microsoft Word or <meta> tags in HTML, or information coded into the file name and its enclosing directories, or available separately as a spreadsheet or other data file, or in an explicitly designed metadata format such as MARC.}
}
@article{FORDE2020114383,
title = {Temporal optimization for affordable and resilient Passivhaus dwellings in the social housing sector},
journal = {Applied Energy},
volume = {261},
pages = {114383},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.114383},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919320707},
author = {Joe Forde and Christina J. Hopfe and Robert S. McLeod and Ralph Evins},
keywords = {Multi-criteria optimization, Decision support, Social housing, Affordable housing, Genetic algorithm, Overheating},
abstract = {Scarcity of affordable energy efficient dwellings is a defining characteristic of the global housing crisis. In many countries this problem has been exacerbated by single objective cost-models which favour the homogeneous development of market tenures at the expense of delivering high-quality affordable homes. Despite the obvious environmental and fuel-poverty alleviation benefits of advanced energy performance standards, such as Passivhaus, they are often dismissed as an affordable housing solution due to elevated build-cost premiums. The present work attempts to reconcile this housing affordability – energy performance nexus by establishing a novel decision support framework for Passivhaus design using genetic multi-objective optimization. The use of constrained genetic algorithms coupled to the Passive House Planning Package software is shown to produce cost optimal designs which are fully compliant with the Passivhaus standard. The findings also reveal that the precise choice of Passivhaus certification criteria has significant impacts on overheating risks using future probabilistic climate data. This means that the design implications of using either the peak heating load or annual heating demand certification criteria must be temporally evaluated to ensure resilient whole-life design outcomes. In a typical UK context, the findings show that affordable Passivhaus dwelling construction costs can be reduced by up to £366/m2 (or 22% of build cost). Use of this evidence-based decision support tool could thereby enable local authorities and developers to make better-informed decisions in relation to cost optimal trade-offs between achieving advanced energy performance standards and the viability of large affordable housing developments.}
}
@incollection{KETELAAR198631,
title = {ANSYS®: ENGINEERING SOFTWARE WITH THE DESIGN AND ANALYSIS ANSWERS},
editor = {A. NIKU-LARI},
booktitle = {Structural Analysis Systems},
publisher = {Pergamon},
pages = {31-39},
year = {1986},
isbn = {978-0-08-032577-4},
doi = {https://doi.org/10.1016/B978-0-08-032577-4.50010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325774500109},
author = {C. Ketelaar},
abstract = {ABSTRACT
ANSYS is a general purpose, finite element computer program for engineering analysis which is developed, marketed, and supported by Swanson Analysis Systems, Inc. in Houston, PA. ANSYS has the ability to solve a wide range of structural, electromagnetic and heat transfer problems and is used by the design engineer to determine displacements, forces, stresses, strains, temperatures and magnetic fields. Graphics, preprocessing, solution and postprocessing are all integrated in this complete package. These extensive analytic capabilities, in addition to quality customer support and unmatched ease of use, have attracted ANSYS users from many industries including nuclear, aerospace, transportation, medical, petrochemical, steel, electronics, farm equipment and civil construction. ANSYS is an integral part of the overall CAD environment. It can provide information about physical structures – information which is essential to proper design decisions. Engineers may select optimum materials and construction designs which are indicated by the analysis results and incorporate these modifications early in the design cycle. ANSYS users can simulate two- and three-dimensional models including surfaces, shells, springs, beams and others. These models can be subjected to proposed loading and the resulting stress effects are then available for detailed study.}
}
@article{CONTRERAS2002961,
title = {FBF: a software package for the construction of balanced cross-sections},
journal = {Computers & Geosciences},
volume = {28},
number = {8},
pages = {961-969},
year = {2002},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(02)00019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0098300402000195},
author = {Juan Contreras},
keywords = {Fault-related folding, Brittle deformation simulation, Cellular automaton},
abstract = {FBF is a series of modular programs coded in ANSI C++ to simulate thrust and normal faulting in cross-section. The employed deformation function preserves area and therefore balanced cross-sections can be obtained with these programs. The programs run in text mode and the source code can be ported to and compiled on most computer platforms. The package consists of a preprocessor, a processor, and a postprocessor. The preprocessor generates meshes that represent stratigraphic units in their undeformed state. Units in the initial state can have tabular geometries, thin laterally, or a composite geometry. The processor carries out a forward simulation to obtain the dislocated state of the units induced by a fault of known shape and displacement. Finally, the postprocessor converts the processor output to a format suitable for plotting. The structural models obtained with the package are in good agreement with structures observed in fold-and-thrust belts and in extensional areas. Moreover, FBF can be used to test activity sequences of faults, to calculate potential fields associated with subsurface structures, and to simulate more complex geological systems and processes such as the stratigraphic response to synsedimentary brittle deformation.}
}
@article{GAUTHIER19961,
title = {SAGE: An object-oriented framework for the construction of farm decision support systems},
journal = {Computers and Electronics in Agriculture},
volume = {16},
number = {1},
pages = {1-20},
year = {1996},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(96)00018-X},
url = {https://www.sciencedirect.com/science/article/pii/S016816999600018X},
author = {Laurent Gauthier and Thierry Néel},
keywords = {Farm management, Decision support systems, Smalltalk, Object-oriented software, Object-oriented databases},
abstract = {In agriculture as in other domains, there exists a need for multifaceted and comprehensive decision support frameworks enabling the integration and use of different types of knowledge and information processing tools. The object-oriented paradigm provides a foundation for the construction of such general decision support frameworks. The objective of the described project was to build an object-oriented framework (called SAGE) for knowledge management and decision support in the area of agro-ecosystem management. The Smalltalk object-oriented programming system was the basic technology used to build the SAGE system. A Smalltalk-based object-oriented database management system was also used to provide persistence for Smalltalk objects. The result of the design and implementation effort is a library of Smalltalk classes that constitutes a framework onto which developers can build systems to represent agroecosystems and support the management of these systems. These classes are described and their design and implementation issues are discussed.}
}
@article{YILMAZ2008993,
title = {A case study for mapping of spatial distribution of free surface heave in alluvial soils (Yalova, Turkey) by using GIS software},
journal = {Computers & Geosciences},
volume = {34},
number = {8},
pages = {993-1004},
year = {2008},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2007.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0098300407002270},
author = {Işık Yilmaz},
keywords = {Clay, GIS, Spatial distribution, Swell percent, Swell pressure, Surface heave, Turkey},
abstract = {A procedure for producing a surface heave map using GIS package in clayey alluvial soils is proposed. An active zone was first defined, and the layers in the active zone were subdivided according to their swelling characteristics. The free surface heave values for each cell of the digitized map of the study area were calculated by using the available equation in the literature, and a spatial distribution map was then constructed interpolating the data belonging to each borehole location. Soils having a high swelling capacity are widely distributed in the study area, and will cause serious heave problems on light structures. Clayey soils in the study area have generally moderate–very high swelling potentials, and swell pressures in many locations are much higher (up to 98kPa) for low-rise structures. Moreover, differential movements sourced from surface heave are also expected in many locations. It was calculated that the minimum expected heave was 0.00cm while the maximum was 12.24cm, indicating “very severe” differential movement. The results obtained in this paper can be used as basic data to assist surface heave hazard management and land use planning. The information derived from this study also has a special importance for assessing the probable deformations on intended light construction applications in Yalova city. The methods used in this study will be valid for generalized planning and assessment purposes; although they may be less useful on the site-specific scale, where local geology and geographic heterogeneities may prevail.}
}
@article{AGARWALA20003,
title = {Inverse inbreeding coefficient problems with an application to linkage analysis of recessive diseases in inbred populations},
journal = {Discrete Applied Mathematics},
volume = {104},
number = {1},
pages = {3-44},
year = {2000},
issn = {0166-218X},
doi = {https://doi.org/10.1016/S0166-218X(00)00193-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X00001931},
author = {Richa Agarwala and Leslie G. Biesecker and Alejandro A. Schäffer},
keywords = {Pedigree, Linkage analysis, Inbreeding coefficient, Recessive diseases, Cycles, Graphs},
abstract = {Medical geneticists connect relatives having the same disease into a family structure called a pedigree. Genetic linkage analysis uses pedigrees to find the approximate chromosomal locations of disease-causing genes. The problem of choosing a pedigree is particularly interesting for diseases inherited in an autosomal recessive pattern in inbred populations because there are many possible paths of inheritance to choose from. A variety of shortcuts are taken to produce plausible pedigrees from inbred populations. We lay the mathematical foundations for a shortcut that was recently used in a pedigree-disease study of an inbred Mennonite population. Recessive disease genes can be localized using the shortcut of homozygosity mapping by finding regions of the genome where affected persons are homozygous. An important quantity in homozygosity mapping is the inbreeding coefficient of a person, which is the prior probability that the person inherited the same piece of DNA on both copies of the chromosome from a single ancestor. Software packages are ill-suited to handle large pedigrees with many inbreeding loops. Therefore, we consider the problem of generating small pedigrees that match the inbreeding coefficient of one or more affected persons in the larger pedigree. We call such a problem an inverse inbreeding coefficient problem. We focus on the case where there is one sibship with one or more affected persons, and consider the problem of constructing a pedigree so that it is “simpler” and gives the sibship a specified inbreeding coefficient. First, we give a construction that yields small pedigrees for any inbreeding coefficient. Second, we add the constraint that ancestor-descendant matings are not allowed, and we give another more complicated construction to match any inbreeding coefficient. Third, we show some examples of how to use the one-sibship construction to do pedigree replacement on real pedigrees with multiple affected sibships. Fourth, we give a different construction to match the inbreeding coefficient of one sibship, while attempting to minimize a measure of the inbreeding loop complexity.}
}
@article{RIVEIRO2013140,
title = {A novel approach to evaluate masonry arch stability on the basis of limit analysis theory and non-destructive geometric characterization},
journal = {Automation in Construction},
volume = {31},
pages = {140-148},
year = {2013},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2012.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S0926580512002300},
author = {B. Riveiro and M. Solla and I. {de Arteaga} and P. Arias and P. Morer},
keywords = {Photogrammetry, Ground Penetrating Radar, Masonry vaults, Rigid blocks method},
abstract = {Knowledge of the functional and conservation state of a structure is a fundamental aspect in order to achieve its maintenance and preservation. Having adequate techniques for reaching this purpose is one of the most important aspects for professionals working about built-up structures. Geometry usually plays an important role in the diagnosis of these structures, and for masonry arches particularly. The most common software packages focused on the stability analysis of masonry arches use rigid blocks assuming masonry as plastic material into the context of Limit Analysis Theory. This paper presents the first results of a novel methodology for the analysis of arch bridge stability based on the construction of integral 3D models of entire vaults. This geometric reconstruction is achieved thanks to the employment of non-destructive techniques such as photogrammetry and Ground Penetrating Radar. Then, stability of vaults is evaluated through a tool specifically developed using Matlab software.}
}
@article{YIN2013561,
title = {Streaming Breakpoint Graph Analytics for Accelerating and Parallelizing the Computation of DCJ Median of Three Genomes},
journal = {Procedia Computer Science},
volume = {18},
pages = {561-570},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.220},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913003633},
author = {Zhaoming Yin and Jijun Tang and Stephen W. Schaeffer and David A. Bader},
keywords = {Genome Rearrangement, Double-cut-and-joining Median, Parallel Programming},
abstract = {The problem of finding the median of three genomes is the key process in building the most parsimonious phylogenetic trees from genome rearrangement data. The median problem using Double-Cut-and-Join (DCJ) distance is NP-hard and the best exact algorithm is based on a branch-and-bound best-first search strategy to explore sub-graph patterns in Multiple BreakPoint Graph (MBG). In this paper, by taking advantage of the “streaming” property of MBG, we introduce the “footprint-based” data structure to reduce the space requirement of a single search nodes from O(v2) to O(v); minimize the redundant computation in counting cycles/paths to update bounds, which leads to dramatically decrease of workload of a single search node. Additional heuristic of branching strategy is introduced to help reducing the searching space. Last but not least, the introduction of a multi-thread shared memory parallel algorithm with two load balancing strategies bring in additional benefit by distributing search work efficiently among different processors. We conduct extensive experiments on simulated datasets and our results show significant improvement on all datasets. And we test our DCJ median algorithm with GASTS, a state of the art software phylogenetic tree construction package. On the real high resolution Drosophila data set, our exact algorithm run as fast as the heuristic algorithm and help construct a better phylogenetic tree.}
}
@article{CARSWELL2006491,
title = {3D solid fin model construction from 2D shapes using non-uniform rational B-spline surfaces},
journal = {Advances in Engineering Software},
volume = {37},
number = {8},
pages = {491-501},
year = {2006},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2006.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S096599780600010X},
author = {Dave Carswell and Nick Lavery},
keywords = {Computer aided design (CAD), NACA airfoils, NURBS surfaces, Computational fluid dynamics (CFD)},
abstract = {A computer aided design (CAD) tool has been specifically developed for rapid and easy design of solid models for surfboard and sailboard fins. This tool simplifies the lofting of advanced fin cross-sectional foils, in this instance based upon the family of standard airfoil series set by the National Advisory Committee for Aeronautics (NACA), whilst retaining a basic parametric description at each cross-section. This paper describes the way in which non-uniform rational B-spline (NURBS) surfaces are created from 2D profile splines, and are then used to generate 3D geometrical surfaces of the fins, which can be imported directly into commercial software packages for finite element stress analysis (FEA) and computational fluid dynamics (CFD). Pressure distributions, lift and drag forces are determined from a CFD flow analysis for various fins designed with this tool, and the results suggest that the incorporation of advanced foils into surfboard fins could indeed lead to increased performance over fins foiled using current standard techniques.}
}
@incollection{BONHAMCARTER1994177,
title = {CHAPTER 7 - Tools for Map Analysis: Single Maps},
editor = {Graeme F. Bonham-Carter},
booktitle = {Geographic Information Systems for Geoscientists},
publisher = {Pergamon},
pages = {177-220},
year = {1994},
isbn = {978-0-08-041867-4},
doi = {https://doi.org/10.1016/B978-0-08-041867-4.50012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080418674500126},
author = {Graeme F. Bonham-Carter},
abstract = {Publisher Summary
This chapter focuses tools for single map analysis. It discusses some examples of geological applications of operations on single maps and their associated attributes. The analysis of spatial patterns on maps is the ultimate objective of many geological applications of GIS. At present, there is no real consensus in the GIS community about what computing tools should be included for spatial analysis or how they should be organized. Because the demands of the many fields that use GIS are diverse, it is unlikely that any single system will provide a complete range of all the functions that would ever be needed. Future GIS will probably meet the needs of spatial research by improving the linkages with external software, and accommodate the particular requirements of specialized applications by creating customized solutions. Thus in the former case, a new method of, say, multifactorial kriging, might be used with GIS by an efficient interchange of information with a specialized external program. In the latter case, a standard procedure for, say, site evaluation for dam construction, could be packaged with a specialized procedure and user-friendly front-end designed solely for this one application. Reclassification of a single map on the basis of several attributes in an attribute table can be carried out with statements in a modeling language. It can be regarded as a type of query operation, searching for objects that satisfy a single criterion or a set of multi-attribute criteria, but extending the process to produce a new map.}
}
@incollection{PARISHER2002237,
title = {Chapter 15 - Three— Dimensional Modeling of Piping Systems},
editor = {Roy A. Parisher and Robert A. Rhea},
booktitle = {Pipe Drafting and Design (Second Edition)},
publisher = {Gulf Professional Publishing},
edition = {Second Edition},
address = {Woburn},
pages = {237-254},
year = {2002},
isbn = {978-0-7506-7439-3},
doi = {https://doi.org/10.1016/B978-075067439-3/50043-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780750674393500435},
author = {Roy A. Parisher and Robert A. Rhea},
abstract = {Publisher Summary
This chapter discusses the three-dimensional (3D) modeling of piping systems. An advancement in piping software that has produced a wealth of design and construction possibilities is 3D computer modeling. One of the advantages of a 3D model over conventional drawings is an improved ability to show clearly what occurs during the design phase of a piping facility. Once a 3D model is developed, a virtual tour or “walkthrough” can be created. Walkthroughs hasten client approval and are used as training videos by plant personnel responsible for safely maintaining and operating the facility. Another valuable benefit of 3D computer modeling is the ability to perform interference detection. Isometric drawings of pipe configurations can be generated using 3D modeling software. The isometrics are created from the 3D model, and with a few manual touchups and additions, are ready for construction issue. Some software packages even have the ability to create isometrics complete with dimensions. Another computer task that can be accomplished using 3D models is that of stress-analysis calculations on pipe and equipment.}
}
@article{ARDITI198691,
title = {Software needs for construction planning and scheduling},
journal = {International Journal of Project Management},
volume = {4},
number = {2},
pages = {91-96},
year = {1986},
issn = {0263-7863},
doi = {https://doi.org/10.1016/0263-7863(86)90035-9},
url = {https://www.sciencedirect.com/science/article/pii/0263786386900359},
author = {David Arditi and Ann Rackas},
keywords = {planning, construction, software, network analysis},
abstract = {In the past few years, the market has been flooded with sophisticated and versatile computer packages which deal with various aspects of the building process: structural analysis and design, cost control, estimating, scheduling, financial management, etc. Potential users are faced with the difficult task of identifying their needs and locating and selecting the software that best suits their requirements. The findings are reported of an investigation that attempts to systematically identify general contractors' needs in one such area: network scheduling.}
}
@article{KEVICZKY1976143,
title = {On Simultaneous Optimal Control of Raw Material Blending and a Ball Grinding Mill},
journal = {IFAC Proceedings Volumes},
volume = {9},
number = {5},
pages = {143-158},
year = {1976},
note = {2nd IFAC Symposium on Automation in Mining, Mineral and Metal Processing, Johannesburg, S Africa, 13-17 September},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)67200-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017672008},
author = {L. Keviczky and R. Haber and J. Kolostori and M. Hilger},
abstract = {In this paper an off-line computer control system developed for use in a cement raw material blending plant is described. A desk-top calculator is used to implement a new self-tuning (ST) minimum variance (MV) regulator algorithm developed for multiple-input, multiple-output (MIMO) systems. After a brief survey of technology the algorithm of the MIMO-ST-MV regulator is discussed. A description of the whole off-line computer control system follows. A program written in the PROCAL language is provided to illustrate computeraided design devices which facilitate the development of the software for the above-mentioned control tasks. The main steps in the preparation of a real-time software package for the raw material blending procedure as applicable to the present phase of the research are also considered. Starting from the material balance equations and the basic dynamic behaviour a new type of mill model has been constructed for control purposes. This new model describes the static and dynamic behaviour of the mill more exactly than previously published models. It contains conventional elements (integrator, summator, dead-time, nonlinearity) which can easily be simulated by computer techniques. The construction of the model follows the geometrical arrangement of the mill and thus its structure is clear and the effect of any modifications can easily be determined from the model. The model also produces both static and dynamic unstable phenomena. It is useful for designing extremum control equipment and in adjusting its parameters via simulation. Generally this model facilitates the design of conventional control loops of ball mills and the preparation of DDC circuits.}
}
@article{STAMATI20111814,
title = {A parametric feature-based approach to reconstructing traditional filigree jewelry},
journal = {Computer-Aided Design},
volume = {43},
number = {12},
pages = {1814-1828},
year = {2011},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2011.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010448511001692},
author = {V. Stamati and G. Antonopoulos and Ph. Azariadis and I. Fudos},
keywords = {Filigree jewelry, Parametric design, Jewelry patterns, Feature-based reconstruction, Re-engineering, Braids},
abstract = {This paper presents a novel approach to reconstructing traditional filigree jewelry. Our method aims at producing an editable CAD representation that can accurately capture the original design and be capable of re-parameterization and modification prior to manufacturing (for example to insert custom designs and abide to free-form artistic alterations). To achieve this, we have developed robust and accurate representations of patterns, used in the design of such jewelry, based on spirals, circular and elliptic arcs, curve segments and braids of various types; all optimized by fairness criteria for aesthetic purposes. We have also built a library of parametric, constraint-based, manufacturable solid patterns that occur frequently in filigree jewelry. For the purposes of this work, a suite of software tools called ReJCAD has been developed, that is able to process a highly accurate point cloud of jewelry pieces and to detect features which are fitted by the primitives of the pattern library through user interaction. The point cloud, in the current framework, guides the assembling of all patterns into one robust manufacturable solid piece. We demonstrate the unique capabilities of ReJCAD by reconstructing a filigree brooch part commonly used in late 19th century in northwestern Greece.}
}
@article{BOHM200077,
title = {A micromachined silicon valve driven by a miniature bi-stable electro-magnetic actuator},
journal = {Sensors and Actuators A: Physical},
volume = {80},
number = {1},
pages = {77-83},
year = {2000},
issn = {0924-4247},
doi = {https://doi.org/10.1016/S0924-4247(99)00298-8},
url = {https://www.sciencedirect.com/science/article/pii/S0924424799002988},
author = {S Böhm and G.J Burger and M.T Korthorst and F Roseboom},
keywords = {Microvalve, Bi-stable, Electromagnetic actuator},
abstract = {In this paper a novel combination of a micromachined silicon valve with low dead volume and a bi-stable electromagnetic actuator produced by conventional machining is presented. The silicon valve part, 7×7×1 mm3 in dimensions, is a sandwich construction of two KOH etched silicon wafers with a layer of chemical resistant silicone rubber bonded in between. This middle layer provides the flexibility needed to move the valve boss positioned in the top wafer during valve operation, but also results in improved sealing if the valve is closed. In order to drive the valve, a dedicated bi-stable electromagnetic actuator has been designed by applying a finite element software package. The resulting actuator consists of a spring-biased armature that can move 0.2 mm up and down in a magnetically soft iron housing, incorporating a permanent magnet and a coil. This large stroke makes the valve particle tolerant. A major advantage of the bi-stable design is that only electrical energy is needed to switch the valve between the open and closed state. The actuator has been manufactured by conventional machining and was attached to the individual silicon valve parts resulting in a valve with a footprint of 7×7 mm2 and a height of 21 mm. The valve showed an open/closed ratio of more than 100 at 0.1 bar.}
}
@incollection{HANTUCH1980285,
title = {AUTOMATIC SYNTHESIS OF A PACKAGED PROGRAM FOR REAL TIME TECHNOLOGICAL PROCESS CONTROL},
editor = {M.A. CUENOD},
booktitle = {Computer Aided Design of Control Systems},
publisher = {Pergamon},
pages = {285-298},
year = {1980},
isbn = {978-0-08-024488-4},
doi = {https://doi.org/10.1016/B978-0-08-024488-4.50049-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244884500498},
author = {I. Hantuch},
abstract = {This contribution refers to certain problems bearing upon the formation of a system generator programing medium of an automatic synthesis for the modular construction of a packaged program for real time technological process control. The package is made up of standard subroutines-functional modules, carrying out the basic control functions. The user defines the task in the problem-oriented “graf language” designed by an analysis of project documentation for automated systems of technological process control. From the viewpoint of software, this is an establishment of a communication and synchronization system, implemented by standard connectors and starting subroutines, operating over the central database and by a system of tables for recording required items of information on the structure of the control package.}
}
@article{FOLGADODELAROSA2020105570,
title = {A method to validate scoring systems based on logistic regression models to predict binary outcomes via a mobile application for Android with an example of a real case},
journal = {Computer Methods and Programs in Biomedicine},
volume = {196},
pages = {105570},
year = {2020},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105570},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720301681},
author = {David Manuel {Folgado-de la Rosa} and Antonio Palazón-Bru and Vicente Francisco Gil-Guillén},
keywords = {Mobile applications, Models, Statistical software, Validation, Validation studies as topic},
abstract = {Background and objectives
To use a points system based on a logistic regression model to predict a binary event in a given population, the validation of this system is necessary. The most correct way to do this is to calculate discrimination and calibration using bootstrapping. Discrimination can be addressed through the area under the receiver operating characteristic curve (AUC) and calibration through the representation of the smoothed calibration plot (most recommended method). As this is not a simple task, we developed a methodology to construct a mobile application in Android to perform this task.
Methods
The construction of the application is based on source code written in language supported by Android. It is designed to use a database of subjects to be analyzed and to be able to apply statistical methods widely used in the scientific literature to validate a points system (bootstrap, AUC, logistic regression models and smooth curves). As an example our methodology was applied on simulated points system data (doi: 10.1111/ijcp.12851) to predict mortality on admission to intensive care units (Google Play: ICU mortality). The results were compared with those obtained applying the same methods in the R statistical package.
Results
No differences were found between the results obtained in the mobile application and those from the R statistical package, an expected result when applying the same mathematical techniques.
Conclusions
Our methodology may be applied to other point systems for predicting binary events, as well as to other types of predictive models.}
}
@article{LAGIN2008530,
title = {Status of the National Ignition Facility Integrated Computer Control System (ICCS) on the path to ignition},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {530-534},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0920379607004942},
author = {L.J. Lagin and R.C. Bettenhausen and G.A. Bowers and R.W. Carey and O.D. Edwards and C.M. Estes and R.D. Demaret and S.W. Ferguson and J.M. Fisher and J.C. Ho and A.P. Ludwigsen and D.G. Mathisen and C.D. Marshall and J.T. Matone and D.L. McGuigan and R.J. Sanchez and E.A. Stout and E.A. Tekle and S.L. Townsend and P.J. {Van Arsdall} and E.F. Wilson},
keywords = {National Ignition Facility, Integrated Computer Control, Automation, CORBA, Software frameworks},
abstract = {The National Ignition Facility (NIF) at the Lawrence Livermore National Laboratory is a stadium-sized facility under construction that will contain a 192-beam, 1.8-MJ, 500-TW, ultraviolet laser system together with a 10-m diameter target chamber with room for multiple experimental diagnostics. NIF is the world's largest and most energetic laser experimental system, providing a scientific center to study inertial confinement fusion (ICF) and matter at extreme energy densities and pressures. NIF's laser beams are designed to compress fusion targets to conditions required for thermonuclear burn, liberating more energy than required to initiate the fusion reactions. NIF is comprised of 24 independent bundles of eight beams each using laser hardware that is modularized into more than 6000 line replaceable units such as optical assemblies, laser amplifiers, and multi-function sensor packages containing 60,000 control and diagnostic points. NIF is operated by the large-scale Integrated Computer Control System (ICCS) in an architecture partitioned by bundle and distributed among over 800 front-end processors and 50 supervisory servers. NIF's automated control subsystems are built from a common object-oriented software framework based on CORBA distribution that deploys the software across the computer network and achieves interoperation between different languages and target architectures. A shot automation framework has been deployed during the past year to orchestrate and automate shots performed at the NIF using the ICCS. In December 2006, a full cluster of 48 beams of NIF was fired simultaneously, demonstrating that the independent bundle control system will scale to full scale of 192 beams. At present, 72 beams have been commissioned and have demonstrated 1.4-MJ capability of infrared light. During the next 2 years, the control system will be expanded in preparation for project completion in 2009 to include automation of target area systems including final optics, target positioners and diagnostics. Additional capabilities to support fusion ignition shots in a National Ignition Campaign (NIC) beginning in 2010 will include a cryogenic target system, target diagnostics, and integrated experimental shot data analysis with tools for data visualization and archiving. This talk discusses the current status of the control system implementation and discusses the plan to complete the control system on the path to ignition.}
}
@article{SORENSEN1991489,
title = {Towards a development environment for fifth generation systems},
journal = {Microprocessing and Microprogramming},
volume = {32},
number = {1},
pages = {489-496},
year = {1991},
note = {Euromicro symposium on microprocessing and microprogramming},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(91)90391-6},
url = {https://www.sciencedirect.com/science/article/pii/0165607491903916},
author = {H. Sorensen and T.A. Delaney and W.P. Kenneally and S.J.M. Murphy and F.B. O'Flaherty and A.B. O'Mahony and D.M.J. Power},
abstract = {In this paper, we describe DESKS (Developmental & Experimental Support for Knowledge-Based Systems), an integrated software package designed to support the development of advanced knowledge-based systems, including expert systems and other AI applications. DESKS follows the Fifth Generation approach in adopting a logic programming paradigm (Prolog) as the mechanism for application development. It provides system designers with an interacting set of sophisticated tools and database facilities to assist in the development process. This paper discusses the aims and achievements of DESKS, and chronicles the design and construction of the package.}
}
@article{RAKKIYAPPAN2015132,
title = {Leader-following consensus of multi-agent systems via sampled-data control with randomly missing data},
journal = {Neurocomputing},
volume = {161},
pages = {132-147},
year = {2015},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.02.056},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215002283},
author = {R. Rakkiyappan and B. Kaviarasan and Jinde Cao},
keywords = {Leader-following consensus, Multi-agent system (MAS), Sampled-data control, Missing data},
abstract = {The objective of this paper is to inspect the leader-following consensus of distributed multi-agent system (MAS) under sampled-data control. The communication flow among neighbor agents is described by an undirected graph. The control protocols are designed by using sampling period technique and zero-order hold circuit along with missing data. A stochastic variable satisfying Bernoulli distributed white noise sequences is introduced to model the missing data. Moreover, by employing the input-delay approach, we transform the sampling data into time-varying delayed data and on the basis of receiving data, two sampled-data control models are proposed. Through the construction of a suitable Lyapunov–Krasovskii functional and by the utilization of integral inequalities, new delay-dependent consensus conditions for the concerned system are derived in the form of linear matrix inequalities (LMIs) which can be readily solved by utilizing any of the valid software packages. The effectiveness of the proposed algorithms is illustrated by two numerical simulations.}
}
@article{TRAVUSH2016766,
title = {Experimental Study of Possible Ways to Increase Cohesion Strength in the “Steel-Concrete” Contact Zone under Displacement Conditions},
journal = {Procedia Engineering},
volume = {153},
pages = {766-772},
year = {2016},
note = {XXV Polish – Russian – Slovak Seminar “Theoretical Foundation of Civil Engineering"},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.240},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816323888},
author = {Vladimir I. Travush and Galina G. Kashevarova and Anna S. Martirosyan and Irina A. Avhacheva},
keywords = {experimental test, steel-concrete composite construction, rigid reinforcement, adhesion bonds destruction},
abstract = {Traditionally, to increase bond strength between steel reinforcement and concrete, flexible fluted reinforcement are used. In modern tower buildings, to increase load-carrying power high-loaded columns of lower floors, rigid reinforcement (I-shaped, U-beam or another) without roughening is used along with flexible reinforcement. The problem which arises is that of reliable cohesion between rigid reinforcement and concrete to ensure their collaboration in the structure. The state of stress in the contact “steel-concrete” zones is highly diverse. It depends on a number of factors and varies a lot in the course of loading. Analytical solutions may be obtained just for a very limited class of contact problems. To apply numerical methods and state-of-the-art software packages, it is necessary to know the parameters of the initial bonds’ default (separation) – loading and displacement, which depend on both mechanical adhesion and chemical interaction of materials in the contact zone. For this purpose, three classes of concrete and two kinds of fibrous concrete have been subjected to experimental testing. Under consideration were the ways of increasing cohesion strength in the contact “steel-concrete” zone in case of displacement by introducing additional coupling elements onto rigid reinforcement and applying epoxy resin coating onto metal.}
}
@article{KUMAR1989164,
title = {Developing strategies and philosophies early for successful project implementation},
journal = {International Journal of Project Management},
volume = {7},
number = {3},
pages = {164-171},
year = {1989},
issn = {0263-7863},
doi = {https://doi.org/10.1016/0263-7863(89)90035-5},
url = {https://www.sciencedirect.com/science/article/pii/0263786389900355},
author = {D Kumar},
keywords = {project management, turnkey projects, engineering contractor, database software, computeraided design, project strategy, project philosophy},
abstract = {In today's competitive environment, project managers are faced with ever increasing demands for compressed schedules, minimum costs, quality construction and safe plant. There is also an increased tendency to award work on a turnkey basis in view of the individual responsibility entailed. The engineering contractor is required to face very large risks in terms of finance and reputation, yet cannot expect handsome profits because of cut-throat competition and market conditions. The revolution in microcomputer technology and the development of advanced database software packages and computeraided design have greatly enhanced management capabilities, but at the same time these techniques require large investments. Under these circumstances, the single most important factor for achieving success is the early development of major project strategies and philosophies. In this paper, an attempt is made to cover various issues related to project strategies and philosophies. An unorthodox approach is adopted by taking a sample project and gradually developing project strategies and philosophies with side discussions on major issues of importance.}
}
@incollection{OLSSON1996725,
title = { - Software tools for parallel CFD on composite grids*},
editor = {A. Ecer and J. Periaux and N. Satdfuka and S. Taylor},
booktitle = {Parallel Computational Fluid Dynamics 1995},
publisher = {North-Holland},
address = {Amsterdam},
pages = {725-732},
year = {1996},
isbn = {978-0-444-82322-9},
doi = {https://doi.org/10.1016/B978-044482322-9/50143-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444823229501432},
author = {Peter Olsson and Jarmo Rantakokko and Michael Thuné},
abstract = {Publisher Summary
This chapter presents a library of software tools—that is, COGITO for construction of portable, parallel partial differential equations (PDE) solvers on composite structured grids. Two of the goals for the COGITO project are to abstract away from computer dependencies for portability, and to abstract away from low-level representations of the data structures. The tools have an object-oriented design and they decouple different program components and give support for complex data structures. The chapter describes an implementation showing that the tools can be used for realistic problems and that the object-oriented design raises the level of abstraction, which increases the readability of the code. This solver has been compared to a code written in plain Fortran 77. Execution timings show that the tools are comparable in performance with hard-coded solvers and that they scale very well on parallel computers of multiple instruction multiple data (MIMD) type with distributed memory.}
}
@article{WU20111184,
title = {ETFE foil spring cushion structure and its analytical method},
journal = {Thin-Walled Structures},
volume = {49},
number = {9},
pages = {1184-1190},
year = {2011},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2011.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0263823111001170},
author = {Minger Wu and Yuanyuan Wu and Jae-Yeol Kim},
keywords = {ETFE foil, Cushion structure, Membrane structure, Spring, Numerical method, Model experiment},
abstract = {The traditional Ethylene Tetra Fluoro Ethylene (ETFE) foil cushion is in the form of air cushion, whose structural stiffness is offered by the inner air pressure. Because the air supply and control systems are needed, air cushion structures cost extra energy and need much maintenance. In order to overcome the shortcomings of air cushion, the ETFE foil spring cushion that uses a spring to take the place of air pressure is developed in this paper. At first, the analytical method for shape finding analysis and stress analysis of the spring cushion is described in the paper. A numerical example is shown by means of the software package ANSYS and the suggested method so as to verify the validity of the method. Then, a model experiment on the ETFE foil spring cushion is carried out. In the loading test, the compression of spring is recorded and the experimental results are compared with the analytical results. At the end of the paper, an experimental hall using both ETFE foil air cushion units and spring cushion units as its roof structure is introduced. Through the construction and daily use of the experimental hall, the spring cushion system shows advantages such as easier construction, needing no air supply and control equipments, no running energy and little maintenance compared with the air cushion system.}
}
@article{ZHU2015173,
title = {Establishment and optimization of an evaluation index system for brownfield redevelopment projects: An empirical study},
journal = {Environmental Modelling & Software},
volume = {74},
pages = {173-182},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215300554},
author = {Yuming Zhu and Keith W. Hipel and Ginger Y. Ke and Ye Chen},
keywords = {Brownfield redevelopment project, Evaluation index system, Establishment and optimization, Factor analysis, Structural equation modeling},
abstract = {Brownfield redevelopment has recently become the focus of attention of governments, communities, environmental advocates, scientists, and researchers around the world. The purpose of this study is to provide a framework for establishing and optimizing an evaluation index for brownfield redevelopment projects (BRPs). This framework involves three steps: the initial design, testing and optimization, and verification. With the help of two standard statistical software packages, the reliability and validity of the initialized index system are established, and then the optimization of the initial index system is carried out by means of Factor Analysis. The effectiveness of the optimization of the index system is verified through Structural Equation Modeling. Furthermore, an illustration example is used to show how to apply the established index system in the real world.}
}
@article{ZHANG2018172,
title = {Quantitative synergy assessment of regional wind-solar energy resources based on MERRA reanalysis data},
journal = {Applied Energy},
volume = {216},
pages = {172-182},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.02.094},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918302198},
author = {Hengxu Zhang and Yongji Cao and Yi Zhang and Vladimir Terzija},
keywords = {Wind energy, Solar energy, MERRA, Spatiotemporal characteristics, Synergy effects, Resources assessment},
abstract = {Considering the volatility and synergies of renewable energy sources, sufficient resource assessment is of great significance for investors and planners to reduce power fluctuations, increase integration capacity, and improve economic and social benefits. This paper proposes a tri-level framework to evaluate and visualize the spatiotemporal characteristics of regional wind and solar energy resources from the perspective of data mining. Furthermore, a free, open-source software package named Quantitative Synergy Assessment Toolbox for renewable energy sources (QSAT V1.0) has been developed with Python and hosted on GitHub, which is a useful tool for the resources assessment and preliminary regional synergetic planning. In the first level, the long-term reanalysis meteorological data of wind speed, solar irradiation and ambient temperature are acquired from MERRA, processed via virtual generation systems, and corrected by in-situ measured data. For the progressive two levels of single-site and wide-area data assessments, the data mining methods incorporating attribute construction, principal components analysis and k-means clustering are used to reduce the dimensionality and capture the temporal and spatial synergy patterns. According to the extracted patterns, the rational combinations of sub-regions can be selected as candidates to make full use of the synergies. Shandong province in China is taken as a demonstration to quantify and analyze the complementarities of solar and wind resources. The proposed method and tools can help enhance the planning of renewable energy sources.}
}
@article{AHN2014671,
title = {BIM interface for full vs. semi-automated building energy simulation},
journal = {Energy and Buildings},
volume = {68},
pages = {671-678},
year = {2014},
note = {The 2nd International Conference on Building Energy and Environment (COBEE), 2012, University of Colorado at Boulder, USA},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2013.08.063},
url = {https://www.sciencedirect.com/science/article/pii/S0378778813005604},
author = {Ki-Uhn Ahn and Young-Jin Kim and Cheol-Soo Park and Inhan Kim and Keonho Lee},
keywords = {Building information modeling, Industry foundation classes, EnergyPlus, Interface, Sensitivity analysis},
abstract = {BIM (building information model) enables information sharing and reuse for interoperability between prevalent software tools in the AEC (Architecture, Engineering, and Construction) industry. Although a BIM based energy simulation tool can reduce costs and time required for building energy simulation work, no practical interface between CAD tools and dynamic energy analysis tools has been developed so far. With this in mind, this study suggests two approaches (Full automated interface (FAI) and semi automated interface (SAI)) enabling information transition from CAD tools (e.g., IFC) to EnergyPlus input file, IDF. FAI, if ideally developed, can convert IFC to IDF based on the use of pre-defined defaults without requiring human intervention. In contrast, SAI converts geometry information drawn from IFC to IDF and then require human data entry for uncertain simulation inputs. For this study, a library building was chosen and space boundary generated from ArchiCAD 13 was employed for geometry mapping. The Morris method, one of sensitivity analysis methods, was used for identifying significant inputs. In FAI and SAI, dominant inputs, out of the Morris method, were identified for Monte Carlo simulation to quantify probabilistic simulation outputs. In the paper, FAI and SAI simulation results are cross-compared, and pros and cons of FAI and SAI are discussed.}
}
@article{TEIXEIRA2018225,
title = {Data mart construction based on semantic annotation of scientific articles: A case study for the prioritization of drug targets},
journal = {Computer Methods and Programs in Biomedicine},
volume = {157},
pages = {225-235},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717309252},
author = {Marlon Amaro Coelho Teixeira and Kele Teixeira Belloze and Maria Cláudia Cavalcanti and Floriano P. Silva-Junior},
keywords = {Semantic annotation, Decision support systems, Drug target prioritization},
abstract = {Background and objectives
Semantic text annotation enables the association of semantic information (ontology concepts) to text expressions (terms), which are readable by software agents. In the scientific scenario, this is particularly useful because it reveals a lot of scientific discoveries that are hidden within academic articles. The Biomedical area has more than 300 ontologies, most of them composed of over 500 concepts. These ontologies can be used to annotate scientific papers and thus, facilitate data extraction. However, in the context of a scientific research, a simple keyword-based query using the interface of a digital scientific texts library can return more than a thousand hits. The analysis of such a large set of texts, annotated with such numerous and large ontologies, is not an easy task. Therefore, the main objective of this work is to provide a method that could facilitate this task.
Methods
This work describes a method called Text and Ontology ETL (TOETL), to build an analytical view over such texts. First, a corpus of selected papers is semantically annotated using distinct ontologies. Then, the annotation data is extracted, organized and aggregated into the dimensional schema of a data mart.
Results
Besides the TOETL method, this work illustrates its application through the development of the TaP DM (Target Prioritization data mart). This data mart has focus on the research of gene essentiality, a key concept to be considered when searching for genes showing potential as anti-infective drug targets.
Conclusions
This work reveals that the proposed approach is a relevant tool to support decision making in the prioritization of new drug targets, being more efficient than the keyword-based traditional tools.}
}
@article{PIMENTEL2007117,
title = {A Method for Patching Interleaving-Replay Attacks in Faulty Security Protocols},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {174},
number = {4},
pages = {117-130},
year = {2007},
note = {Proceedings of the Workshop on Verification and Debugging (V&D 2006)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107002009},
author = {Juan Carlos Lopez Pimentel and Raul Monroy and Dieter Hutter},
keywords = {Fault localization, patching, replay attacks, security protocols, verification},
abstract = {The verification of security protocols has attracted a lot of interest in the formal methods community, yielding two main verification approaches: i) state exploration, e.g. FDR [Gavin Lowe. Breaking and fixing the needham-schroeder public-key protocol using FDR. In TACAs'96: Proceedings of the Second International Workshop on Tools and Algorithms for Construction and Analysis of Systems, pages 147–166, London, UK, 1996. Springer-Verlag] and OFMC [A.D. Basin, S. Mödersheim, and L. Viganò. An on-the-fly model-checker for security protocol analysis. In D. Gollmann and E. Snekkenes, editors, ESORICS'03: 8th European Symposium on Research in Computer Security, number 2808 in Lecture Notes in Computer Science, pages 253–270, Gjøvik, Norway, 2003. Springer-Verlag]; and ii) theorem proving, e.g. the Isabelle inductive method [Lawrence C. Paulson. The inductive approach to verifying cryptographic protocols. Journal in Computer Security, 6(1-2):85–128, 1998] and Coral [G. Steel, A. Bundy, and M. Maidl. Attacking the asokan-ginzboorg protocol for key distribution in an ad-hoc bluetooth network using coral. In H. König, M. Heiner, and A. Wolisz, editors, IFIP TC6 /WG 6.1: Proceedings of 23rd IFIP International Conference on Formal Techniques for Networked and Distributed Systems, volume 2767, pages 1–10, Berlin, Germany, 2003. FORTE 2003 (work in progress papers)]. Complementing formal methods, Abadi and Needham's principles aim to guide the design of security protocols in order to make them simple and, hopefully, correct [M. Abadi and R. Needham. Prudent engineering practice for cryptographic protocols. IEEE Transactions on Software Engineering, 22(1):6–15, 1996]. We are interested in a problem related to verification but far less explored: the correction of faulty security protocols. Experience has shown that the analysis of counterexamples or failed proof attempts often holds the key to the completion of proofs and for the correction of a faulty model. In this paper, we introduce a method for patching faulty security protocols that are susceptible to an interleaving-replay attack. Our method makes use of Abadi and Needham's principles for the prudent engineering practice for cryptographic protocols in order to guide the location of the fault in a protocol as well as the proposition of candidate patches. We have run a test on our method with encouraging results. The test set includes 21 faulty security protocols borrowed from the Clark-Jacob library [J. Clark and J. Jacob. A survey of authentication protocol literature: Version 1.0. Technical report, Department of Computer Science, University of York, November 1997. A complete specification of the Clark-Jacob library in CAPSL is available at http://www.cs.sri.com/millen/capsl/].}
}
@article{CHI2001144,
title = {Automatic proxy-based watermarking for WWW},
journal = {Computer Communications},
volume = {24},
number = {2},
pages = {144-154},
year = {2001},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(00)00309-1},
url = {https://www.sciencedirect.com/science/article/pii/S0140366400003091},
author = {C.-H Chi and Y Lin and J Deng and X Li and T.-S Chua},
keywords = {Internet, Digital libraries, Copyright, Watermarking, Proxy server, Content transformation},
abstract = {With the appearance of digital libraries and information archive centers on the Internet, copyright issues are becoming very concerning. Unlike hardcopies, digital data can be modified and distributed very easily. Watermarking is the solution to this copyright problem. By embedding a secure, identifiable mark on the digital data, ownership and content integrity can be ensured. Currently, the watermarking process is usually done manually and off-line. This makes updating of watermarking techniques and watermark logo content difficult. Enforcement of watermarking policies within an organization for providing content on the Internet is also not easy. Furthermore, installing watermarking software on each workstation of an organization is not very cost effective. In this paper, we propose a novel approach to address the copyright issues of digital data on the Internet. The basic idea is to move the watermarking process from the content developers to a reverse proxy server that is usually close to the organization's gateway. Data will be watermarked only when it is retrieved through this proxy. The main advantages of this approach are that: (i) it is cost effective as being independent of the amount of data on the web server and also independent of the number of content developers in the organization, (ii) it makes updates and maintenance tasks easy, being an one-for-all solution, and (iii) the reverse proxy effectively enforces the watermarking guideline. Both design considerations and implementation details of watermarking in a fully functional, Squid-based reverse proxy server are presented and its performance is analyzed. All results show that automatic watermarking in a reverse proxy server is a practical, cost efficient and enforcement effective solution for handling copyright issues in Internet digital libraries.}
}
@article{PILLMANN1980189,
title = {A3.1 : The Process Computer Control System of the Hydroelectric Power Stations Along the River Danube in Austria},
journal = {IFAC Proceedings Volumes},
volume = {13},
number = {9},
pages = {189-196},
year = {1980},
note = {6th IFAC/IFIP Conference on Digital Computer Applications to Process Control, Dusseldorf, Germany, 14-17 October},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)64568-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017645683},
author = {W. Pillmann and R. Stefanich},
keywords = {Hydro-electric power plant, power station control, digital computer applications, flow control, water level control, direct digital control, identification, digital simulation},
abstract = {In Austria the control centers of the six hydroelectric power stations along the river Danube are equipped with process computers. The control systems are designed for advanced control of the plants, including multilevel process control, interactive man-machine interfaces, advanced monitoring, security related functions and for supervising the hydroelectric plant cascade in the future. During implementation particular attentions was paid to the water level and flow rate control system in the power stations. The principles of design, the realisation of this direct digital control and the up to date experiences with the controller operation are presented in this paper. Furthermore hardware structure of the control center, the construction principles of the software package and the man machine interface are discussed briefly}
}
@article{FOTHI2003815,
title = {The structured complexity of object-oriented programs},
journal = {Mathematical and Computer Modelling},
volume = {38},
number = {7},
pages = {815-827},
year = {2003},
note = {Hungarian Applied Mathematics},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(03)90066-5},
url = {https://www.sciencedirect.com/science/article/pii/S0895717703900665},
author = {Á. Fóthi and J. Nyéky-Gaizler and Z. Porkoláb},
keywords = {Software metrics, Object-oriented programming, Complexity},
abstract = {There are several methods measuring the complexity of object-oriented programs. Most of them are based on some special object-oriented feature: number of methods/classes, cohesion of classes, inheritance, etc. In practice, however, object-oriented programs are constructed with the help of the same control structures as traditional ones. Moreover, recent ideas of multiparadigm programming (i.e., emerging use of generic programming and aspect-oriented programming) has the effect that in modern programs—and even in class libraries—object-orientation is only one (however major) construction tool among others. An adequate measure therefore should not be based on special features of one paradigm, but on basic language elements and construction rules which could be applied to many different paradigms. In our model discussed here, the complexity of a program is the sum of three components: the complexity of its control structure, the complexity of data types used, and the complexity of the data handling (i.e., the complexity of the connection between the control structure and the data types). We suggest a new complexity measure. First, we show that this measure works well on procedural programs, and then we extend it to object-oriented programs. There is a software tool under development based on gnu g++ compiler which computes our new measure. We can apply this tool to C and C++ sources to gain a number of quantitative results with our measure.}
}
@article{POZAS2016300,
title = {Getting Results in an Historical Dwelling Stock in a Thermal Simulation with EnergyPlus},
journal = {Procedia Engineering},
volume = {161},
pages = {300-306},
year = {2016},
note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning Symposium 2016, WMCAUS 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.560},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816327680},
author = {Beatriz Montalbán Pozas and Montaña Jiménez Espada},
keywords = {building simulation, historic building type, thermal model, model simulation inputs},
abstract = {The invaluable historic dwelling stock of many cities today needs an accurate analysis in order to implement a relevant program of research for its conservation, maintenance, or refurbishment. Additionally, energy efficiency studies are essential in every case to guarantee sustainability. In these situations, a simulation process will be a suitable approach to obtaining results. However, historic dwellings are complicated to simulate. They require a comprehensive study of traditional crafts and techniques as well as of ancient constructions which rely heavily on local builders’ design skills and traditions as well as on the regional materials and architectural knowledge. Furthermore, the intricate geometries and the large number of different dwelling cases demand a amplification and a typological definition of representative buildings. In addition, in an energy simulation process, it is indispensable to know what the lifestyle was. In a traditional dwelling, the ancient customs have to be taken into account, studying the occupation, metabolism, activities, and traditional clothing in order to define a reliable computer model. Moreover, an operation usage program has to be defined in order to set other building simulation parameters such as the ventilation, shading, and internal gains. The present work proposes a method using DesignBuilder, an EnergyPlus software package, to generate a model for the energy simulation of historic dwellings through a case study of an historic dwelling stock in the Sistema Central of Spain. How the type was generated from a real case with an architectonic, constructive, and activity definition is described. Guidelines are provided for modeling the building and for setting the simulation parameters necessary to obtain correct results.}
}
@article{SCHLITZER20021211,
title = {Interactive analysis and visualization of geoscience data with Ocean Data View},
journal = {Computers & Geosciences},
volume = {28},
number = {10},
pages = {1211-1218},
year = {2002},
note = {Shareware and freeware in the Geosciences II. A special issue in honour of John Butler},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(02)00040-7},
url = {https://www.sciencedirect.com/science/article/pii/S0098300402000407},
author = {Reiner Schlitzer},
keywords = {Geoscience, Visualization, Interactive, Oceanography},
abstract = {Ocean Data View (ODV) is a freeware package for the interactive exploration and graphical display of multi-parameter profile or sequence data. Although originally developed for oceanographic observations only, the underlying concept is more general, and data or model output from other areas of geosciences, like for instance geology, geophysics, geography and atmospheric research can be maintained and explored with ODV as well. The data format of ODV is designed for dense storage and direct data access, and allows the construction of very large datasets, even on affordable and portable hardware. ODV supports display of original data by colored dots or actual data values at the measurement locations. In addition, two fast and reliable variable-resolution gridding algorithms allow color shading and contouring of gridded fields along sections and on general 3D surfaces. A large number of derived quantities can be selected and calculated online. These variables are displayed and analyzed in the same way as the basic variables stored in disk files. ODV runs on PCs under Windows and on UNIX workstations under SUN Solaris. The software and extensive sets of coastline, topography, river-, lake- and border outlines as well as various gazetteers of topographic features are available at no cost over the Internet. In addition, the electronic atlas eWOCE that consists of oceanographic data from the World Ocean Circulation Experiment (WOCE) is also available free of charge over the Internet. A gallery of prepared plots of property distributions along WOCE sections provides a quick overview over hydrographic, nutrient, oxygen and transient tracer fields in the ocean and, apart from the scientific use for oceanographic research, can serve as tutorial material for introductory or advanced courses on oceanography.}
}
@article{FORMISANO2018179,
title = {Seismic vulnerability of Italian masonry churches: The case of the Nativity of Blessed Virgin Mary in Stellata of Bondeno},
journal = {Journal of Building Engineering},
volume = {20},
pages = {179-200},
year = {2018},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2018.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S2352710218306326},
author = {Antonio Formisano and Generoso Vaiano and Francesco Fabbrocino and Gabriele Milani},
keywords = {Seismic vulnerability, Masonry church, Seismic risk coefficients, Damage index, Facade overturning mechanisms, Non-linear dynamic analyses},
abstract = {In this paper the problem of seismic vulnerability of masonry churches is analysed with reference to the Nativity of Blessed Virgin Mary ecclesiastic complex in Stellata of Bondeno (Italy). This religious construction is composed of a church, two bell towers, a cloister and the Saint Domenico´s oratory. The church, made of masonry brick stones, is characterized by a single hall with chapels, which is divided into three parts by arches, and has an apse elevated by a few steps with reference to the hall. The study herein presented is carried out according to the Italian Standards and Guidelines on Cultural Heritage. In a first step, the seismic risk coefficients both at Damage Limit State (αDLS) and Ultimate Limit State (αULS) are evaluated by using the 3Muri calculation software for masonry structures. These coefficients indicate the ratio between the ground acceleration leading towards attainment of the two mentioned limit states and the PGAs of the site referred to a given reference return period. Afterwards, the variability of such coefficients is examined by changing both the masonry type and the seismic zone in order to detect the worst situations on the Italian land. In a second analysis step a comparison among the seismic risk coefficients αULS and the damage index calculated through a fast method provided in a suitable form by the Italian Civil Protection Department is proposed. Moreover, overturning mechanisms of facades are checked by using the 3Muri software with the ultimate goal to compare the predictive theoretical results with the real damages detected after the 2012 Emilia Romagna earthquake. Finally, in order to obtain a more precise assessment of the seismic behaviour of the church under study, linear and non-linear dynamic analyses are performed on a 3D FEM model setup through the ABAQUS software package.}
}
@article{LOVE2013448,
title = {Documentation errors in instrumentation and electrical systems: Toward productivity improvement using System Information Modeling},
journal = {Automation in Construction},
volume = {35},
pages = {448-459},
year = {2013},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2013.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S0926580513000952},
author = {Peter E.D. Love and Jingyang Zhou and Chun-pong Sing and Jeong Tai Kim},
keywords = {CAD, Documentation errors, Costs, Productivity, System Information Modeling},
abstract = {Documentation errors have been identified as a significant problem within the construction and engineering industry. Errors contained with contract documents can contribute to loss of profit, reduced productivity, and cost and time overruns as well as contractual disputes. Research has identified that as much as 60% of variations in construction and engineering projects are a result of errors and omissions contained within poor quality documentation. Considering this alarming statistic and the impact that poor quality documentation can have on productivity, the research presented in this paper classifies and quantifies errors in 106 drawings and the cable schedule used to document for the electrical package for an Iron Ore Stacker Conveyor. The research reveals that Computer Aided Design (CAD) used to produce the electrical drawings was ineffective, inefficient and costly to produce as they contained an array of errors. In addressing the need to eliminate documentation errors and improve productivity, the cable schedule is used to create a Systems Information Management to develop a 1:1 object orientated model using the software Dynamic Asset Documentation. As a result, of using this approach it is estimated that a 94% cost saving and a substantial improvement in productivity could have been attained in this particular case.}
}
@article{KOSICHENKO20161503,
title = {Design of Impervious Coatings with Enhanced Reliability Made from Innovative Materials},
journal = {Procedia Engineering},
volume = {150},
pages = {1503-1509},
year = {2016},
note = {2nd International Conference on Industrial Engineering (ICIE-2016)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.096},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816314059},
author = {Y.M. Kosichenko and O.A. Baev},
keywords = {construction coatings, geocomposite materials, theoretical model, water permeability, hydro mechanical method},
abstract = {This article considers development and research issues of creating impervious screens with enhanced reliability for irrigation canals, ponds and reservoirs using geo-composite materials (geomembranes, bentonite mats and other). In this connection, the authors conducted the present research and impervious screens that provide a high impervious effect and a long service life were developed. Also, the process of water permeability of probable damage inlets in the geocomposite screen was conducted in the form of small holes using Comsol Multiphysics software package. A hydro-mechanical method based on the conformal mappings method was used to find an analytical solution for the problems. Dependences were obtained to estimate the permeability of the geocomposite screens design for plausible accidental damages: seepage losses per unit damage and the average filter coefficients, given the size of the damage and the frequency of distribution. Based on the calculations, using the derived formulas, a graph was drafted, summarizing the degree of permeability of the geocomposite screens, based on which the rational field of their application was determined and the requirements for impermeability indices were set.}
}
@article{MOUSSA1994217,
title = {TraPhyC-BV: A hydrologic information system},
journal = {Environmental Software},
volume = {9},
number = {4},
pages = {217-226},
year = {1994},
issn = {0266-9838},
doi = {https://doi.org/10.1016/0266-9838(94)90020-5},
url = {https://www.sciencedirect.com/science/article/pii/0266983894900205},
author = {R. Moussa and C. Bocquillon},
keywords = {Digital Elevation Models, Hydrology, Geographic Information System, Channel network},
abstract = {Digital Elevation Models are used to automatically map the drainage network, the stream channel and divide networks of a watershed. This has been done with the TraPhyC-BV software package. Construction of a code describing the network may form the basis for an efficient hydrologic information system. This form of discretization of a watershed produces natural units for problems involving surface or subsurface water flow. This methodology is designed to aid in the parametrization of a distributed components approach to watershed simulation in the fields of hydrology, sedimentology and geomorphology.}
}
@article{KITCHENHAM20132049,
title = {A systematic review of systematic review process research in software engineering},
journal = {Information and Software Technology},
volume = {55},
number = {12},
pages = {2049-2075},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001560},
author = {Barbara Kitchenham and Pearl Brereton},
keywords = {Systematic review, Systematic literature review, Systematic review methodology, Mapping study},
abstract = {Context
Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.
Objective
To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.
Method
We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools.
Results
We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult.
Conclusion
We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem.}
}
@article{LIU2010238,
title = {Preventing progressive collapse through strengthening beam-to-column connection, Part 2: Finite element analysis},
journal = {Journal of Constructional Steel Research},
volume = {66},
number = {2},
pages = {238-247},
year = {2010},
issn = {0143-974X},
doi = {https://doi.org/10.1016/j.jcsr.2009.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0143974X09002260},
author = {J.L. Liu},
keywords = {Beam-to-column connection, Collapse, Ductility, Catenary action, Finite element analysis},
abstract = {This two-part paper presents part of the results of a study devoted to the investigation of retrofitting effect of simple steel construction. Through strengthening simple beam-to-column connection, progressive collapse can be prevented by catenary action. The evaluation of catenary action represents a complex analytical problem with a large tension affecting its structural behavior. With the advent of high speed computers and powerful calculation software package, the finite element method offers an ideal tool for tackling such a complex problem. This paper develops sophisticated one-, two- and three-dimensional models of catenary action, and simulates the post-attack behavior of the original and the strengthened structures by means of the ABAQUS finite element package. The global behavior of the one-dimensional beam element model is close to that corresponding to the two-dimensional solid or the three-dimensional shell models, particularly for the structures with strengthened joints. Comparison of results between this study and literature has been carried out for the purpose of validating the present finite element prediction model. Through the comparing computational results before and after strengthening, the advantages of proposed retrofitting scheme are demonstrated.}
}
@article{AHLANDER20021007,
title = {Einstein summation for multidimensional arrays},
journal = {Computers & Mathematics with Applications},
volume = {44},
number = {8},
pages = {1007-1017},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)00210-9},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102002109},
author = {K. Åhlander},
keywords = {Index notation, Mathematical software, Domain specific language, Tensor calculus},
abstract = {One of the most common data structures, at least in scientific computing, is the multidimensional array. Some numerical algorithms may conveniently be expressed as a generalized matrix multiplication, which computes a multidimensional array from two other multidimensional arrays. By adopting index notation with the Einstein summation convention, an elegant tool for expressing generalized matrix multiplications is obtained. Index notation is the succinct and compact notation primarily used in tensor calculus. In this paper, we develop computer support for index notation as a domain specific language. Grammar and semantics are proposed, yielding an unambiguous interpretation algorithm. An object-oriented implementation of a C++ library that supports index notation is described. A key advantage with computer support of index notation is that the notational gap between a mathematical index notation algorithm and its implementation in a computer language is avoided. This facilitates program construction as well as program understanding. Program examples that demonstrate the close resemblance between code and the original mathematical formulation are presented.}
}
@article{PRATHAP1994295,
title = {The displacement-type finite element approach—From art to science},
journal = {Progress in Aerospace Sciences},
volume = {30},
number = {4},
pages = {295-405},
year = {1994},
issn = {0376-0421},
doi = {https://doi.org/10.1016/0376-0421(94)90007-8},
url = {https://www.sciencedirect.com/science/article/pii/0376042194900078},
author = {G. Prathap},
abstract = {The finite element method originated in the aerospace industry in the mid- 1950s to solve practical stress analysis problems associated with the struc- tural design of aerospace vehicles. It is today the most overwhelmingly popular analysis and design tool in structural and solid mechanics and is being extensively applied to a very wide spectrum of engineering science, e.g. fluid mechanics, heat transfer and electromagnetics. In its formative years, the development of the method was guided mostly by engineering intuition, heuristic judgment and trial and error experimentation and vali- dation. Its achievements have been remarkable and there are now very power- ful general-purpose software codes that make a variety of analyses and design tasks routinely simple, that were once considered to be intractable. This is not to say that the progress of the method has been free of hurdles, especi- ally in finding a complete scientific basis for it. Based on the recent stud -ies of this author and his colleagues, this review attempts to provide a more complete paradigmatic understanding of the issues involved. Concepts such as consistency and (variational) correctness are introduced. These together with the more familiar completeness and continuity requirements are then employed to guide the construction of error-free robust finite elements and also provide procedures to perform a priori error estimates for the quality of approximation. These C-concepts, as we shall call them, are elucidated and their relevance to the design of several key elements commonly found in general-purpose packages used by the aerospace, automobile and mechanical engineering industries is briefly covered. The article reviews what has been achieved in areas where the C-concepts can be applied fruitfully in the study of the displacement type finite element method.}
}
@incollection{DEDIANA1987411,
title = {Some Aspects of Pre-testing of Tutorial Courseware Prior to the Prototype Phase},
editor = {JEF MOONEN and TJEERD PLOMP},
booktitle = {Eurit 86: Developments in Educational Software and Courseware},
publisher = {Pergamon},
pages = {411-416},
year = {1987},
isbn = {978-0-08-032693-1},
doi = {https://doi.org/10.1016/B978-0-08-032693-1.50068-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008032693150068X},
author = {I.P.F. {De Diana}},
abstract = {ABSTRACT
In the CAD-CAI research project, a methodology and related software packages are under construction for the pre-testing of some aspects of tutorial courseware. Pre-testing pertains to the evaluation of a product prior to its prototype phase. Pre-testing is based upon modelling of the product. Model analyzing facilities have been developed and integrated in a software environmen for the design and production of tutorial courseware. This system, EDUC, contains two mathematical packages, one for modelling and analyzing the instructional structure and one for modelling and analyzing the effects of an instructional strategy (mastery learning) upon the expected behavior of learners in an instructional network.}
}
@article{KISS2014273,
title = {Adaptive CAD model (re-)construction with THB-splines},
journal = {Graphical Models},
volume = {76},
number = {5},
pages = {273-288},
year = {2014},
note = {Geometric Modeling and Processing 2014},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2014.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1524070314000241},
author = {Gábor Kiss and Carlotta Giannelli and Urška Zore and Bert Jüttler and David Großmann and Johannes Barner},
keywords = {Truncated hierarchical B-splines, Adaptive refinement, CAD interfaces, Turbine blades},
abstract = {Computer Aided Design (CAD) software libraries rely on the tensor-product NURBS model as standard spline technology. However, in applications of industrial complexity, this mathematical model does not provide sufficient flexibility as an effective geometric modeling option. In particular, the multivariate tensor-product construction precludes the design of adaptive spline representations that support local refinements. Consequently, many patches and trimming operations are needed in challenging applications. The investigation of generalizations of tensor-product splines that support adaptive refinement has recently gained significant momentum due to the advent of Isogeometric Analysis (IgA) [2], where adaptivity is needed for performing local refinement in numerical simulations. Moreover, traditional CAD models containing many small (and possibly trimmed) patches are not directly usable for IgA. Truncated hierarchical B-splines (THB-splines) provide the possibility of introducing different levels of resolution in an adaptive framework, while simultaneously preserving the main properties of standard B-splines. We demonstrate that surface fitting schemes based on THB-spline representations may lead to significant improvements for the geometric (re-)construction of critical turbine blade parts. Furthermore, the local THB-spline evaluation in terms of B-spline patches can be properly combined with commercial geometric modeling kernels in order to convert the multilevel spline representation into an equivalent – namely, exact – CAD geometry. This software interface fully integrates the adaptive modeling tool into CAD systems that comply with the current NURBS standard. It also paves the way for the introduction of isogeometric simulations into complex real world applications.}
}
@article{SAETER1988455,
title = {Software techniques for integrating text and graphics in VLSI CAD tools},
journal = {Microprocessing and Microprogramming},
volume = {24},
number = {1},
pages = {455-460},
year = {1988},
note = {Supercomputers: Technology and Applications},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(88)90096-8},
url = {https://www.sciencedirect.com/science/article/pii/0165607488900968},
author = {Tore Sæter},
abstract = {This paper presents ideas based on attribute grammars and syntax directed editors that allow textual layout representations to be manipulated and displayed graphically. By using these ideas a CAD system can be made that allow a designer to work with both textual and graphical representation of a symbolic layout at the same time based on one common representation. These techniques for integration of text and graphics can lead to the construction of better tools especially for the design of generic hardware descriptions. This can again lead to the reduced cost in maintaining component libraries in todays most advanced CAD tools like Silicon Compilers and Module Generators.}
}
@article{AHSON1985179,
title = {The use of FORTH language in process control},
journal = {Computer Languages},
volume = {10},
number = {3},
pages = {179-187},
year = {1985},
issn = {0096-0551},
doi = {https://doi.org/10.1016/0096-0551(85)90015-3},
url = {https://www.sciencedirect.com/science/article/pii/0096055185900153},
author = {S.I. Ahson and S.S. Lamba},
keywords = {Process control, Programming languages, Multiloop control systems, Microcomputers, Digital control},
abstract = {A tutorial discussion is given on the high-level programming language FORTH with particular reference to process control application. The advantages of FORTH are illustrated through a practical example. FORTH, originally developed for telescope control, contains many features designed to facilitate the construction of software for process control systems. In this paper, we describe some of these features and provide a design of a software package for a multiloop control system to illustrate its use. The software package, developed for Rockwell's AIM 65 microcomputer, contains programs written in FORTH for implementing feedback, feed-forward, ratio and cascade control functions. Experimental results pertaining to a furnace simulated on an EAL-580 hybrid computer are presented. A brief discussion is given on experience gained in the use of FORTH language for process control.}
}
@article{HANTUCH1979285,
title = {Automatic Synthesis of a Packaged Program for Real Time Technological Process Control},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {7},
pages = {285-298},
year = {1979},
note = {IFAC Symposium on computer Aided Design of Control Systems, Zurich, Switzerland, 29-31 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65611-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017656118},
author = {I. Hantuch},
keywords = {Control system synthesis, control engineering computer applications, programing languages, real time applications},
abstract = {This contribution refers to certain problems bearing upon the formation of a system generator programing medium of an automatic synthesis for the modular construction of a packaged program for real time technological process control. The package is made up of standard subroutines-functional modules, carrying out the basic control functions. The user defines the task in the problem-oriented "graf language" designed by an analysis of project documentation for automated systems of technological process control. From the viewpoint of software, this is an establishment of a communication and synchronization system, implemented by standard connectors and starting subroutines, operating over the central database and by a system of tables for recording required items of information on the structure of the control package.}
}
@article{OHBA1989125,
title = {New Distributed Control System Based on Mini-Map},
journal = {IFAC Proceedings Volumes},
volume = {22},
number = {15},
pages = {125-130},
year = {1989},
note = {9th IFAC Workshop on Distributed Computer Control Systems 1989, Tokyo, Japan, 26-28 September},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-037870-1.50025-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080378701500253},
author = {A. Ohba and K. Iizuka},
keywords = {Automation, Control systems, Distributed control, Instrumentation, Man-machine system, Microprocessors, Process control, Local area networks, Programmable controllers},
abstract = {This paper describes a new integrated control system based on mini-MAP called the Toshiba Integrated Control System. In this system, a computer, an instrumentation system, and an electrical control system have been integrated into a single architecture. ADMAP (Advanced MAP) based on mini-MAP conforming to MAP V3.0 with an added original scan transmission function is used as a control-level LAN. This permits construction of a multivendor system for equipment based on mini-MAP. A new operator interface, OIS, which is a human interface common to stations on the ADMAP, is provided. The OIS promotes operator-friendliness by adopting of a touch panel and multiwindow displays and enables mass data handling and high-speed display. PCS, a new control station, is an amalgamation of the conventional instrumentation controller and the programmable controller into a single architecture. Use of multiple configuration languages and multitasking helps support extensive applications, including PID loop, batch sequence, and sequential motor control. An engineering work terminal (EWT) using a laptop computer is provided as a system engineering tool. With EWT and various engineering tool packages, application software can be built concurrently by two or more engineers in an interactive mode. Moreover, the system can be used with AI stations and industrial computers. It is a new generation of control system supporting PA, FA, and BA fields and offering greater friendliness with EA and OA.}
}
@article{FORREST2001387,
title = {safepaq-II, a new tool for the production of activation data libraries},
journal = {Fusion Engineering and Design},
volume = {54},
number = {3},
pages = {387-395},
year = {2001},
issn = {0920-3796},
doi = {https://doi.org/10.1016/S0920-3796(00)00557-3},
url = {https://www.sciencedirect.com/science/article/pii/S0920379600005573},
author = {R.A. Forrest},
keywords = {European Activation File (-99), , },
abstract = {Activation data and inventory codes are a major input to much of the safety related work carried out on fusion devices. The inventory code recommended for European activation calculations is fispact-99; this requires a large amount of nuclear data, which is available in the European Activation File (EAF-99). The production of an EAF library uses new sources of data, both evaluated and calculated, differential measurements and integral data. In order to store, evaluate, and use all the various data sources an efficient software tool is required. Earlier versions of EAF have been produced using the tools sympal and safepaq, which enabled a large degree of automation as compared with the original construction ‘by hand’. However, these relied on the direct manipulation of the ENDF formatted text files using fortran-77. This is not an efficient approach, as editing of the text files is inconvenient and liable to errors. It was decided to use relational databases to store the data, with data extraction carried out by standard queries written in sql. Other objectives were the provision of a user-friendly graphical interface to allow data to be viewed and manipulated and a high level of QA by logging all data changes. These objectives have been realised by the safepaq-II application; this uses the ideas of the previous tools, but has been designed from scratch using new methods. Visual Basic is used to build the application running under Windows NT 4, which is linked to a series of access databases.}
}
@article{YANG2015129,
title = {Numerical simulation of a jointed rock block mechanical behavior adjacent to an underground excavation and comparison with physical model test results},
journal = {Tunnelling and Underground Space Technology},
volume = {50},
pages = {129-142},
year = {2015},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2015.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0886779815001315},
author = {Xuxu Yang and P.H.S.W. Kulatilake and Hongwen Jing and Shengqi Yang},
keywords = {Mechanical behavior, Non-persistent joints, Jointed rock, PFC, Underground excavations},
abstract = {Mechanical behavior of a jointed rock mass with non-persistent joints located adjacent to a free surface on the wall of an excavation was simulated under without and with support stress on the free surface using approximately 0.5m cubical synthetic jointed rock blocks having 9 non-persistent joints of length 0.5m, width 0.1m and a certain orientation arranged in an en echelon and a symmetrical pattern using PFC3D software package. The joint orientation was changed from one block to another to study the effect of joint orientation on strength, deformability and failure modes of the jointed blocks. First the micro-mechanical parameters of the PFC3D model were calibrated using the macro mechanical properties of the synthetic intact standard cylindrical specimens and macro mechanical properties of a limited number of physical experiments performed on synthetic jointed rock blocks of approximately 0.5m cubes. Under no support stress, the synthetic jointed rock blocks exhibited the same three failure modes: (a) intact rock failure, (b) step-path failure and (c) planar failure under both physical experiments and numerical simulations for different orientations. The jointed blocks which failed under intact rock failure mode and planar or step-path failure mode produced high and low jointed block strengths, respectively. Three phases of convergence of free surface were discovered. The joint orientation and support stress played important roles on convergence magnitude. The average increment of jointed block strength turned out to be about 10, 7.9 and 6.6 times the support stress when support stresses of 0.06MPa, 0.20MPa and 0.40MPa were applied, respectively. The modeling results offer some guideline in support design for underground excavations.}
}
@article{BAZILIAN200257,
title = {Modelling of a photovoltaic heat recovery system and its role in a design decision support tool for building professionals},
journal = {Renewable Energy},
volume = {27},
number = {1},
pages = {57-68},
year = {2002},
issn = {0960-1481},
doi = {https://doi.org/10.1016/S0960-1481(01)00165-3},
url = {https://www.sciencedirect.com/science/article/pii/S0960148101001653},
author = {Morgan D. Bazilian and Deo Prasad},
abstract = {A numerical model has been created to simulate the performance of a residential-scale building integrated photovoltaic (BiPV) cogeneration system. The investigation examines the combined heat and power system in the context of heat transfer. The PV cogeneration system will be based on existing BiPV roofing technology with the addition of a modular heat recovery unit that can be used in new or renovation construction schemes. The convection of the air behind the panels will serve to cool the PV panels while providing a heat source for the residence. This model was created in the Engineering Equation Solver software package (EES), from a series of highly coupled non-linear partial differential equations that are solved iteratively. The model's ability to utilize climatic data to simulate annual performance of the system will be presented along with a comparison to experimental data. A graphical front-end has been added to the model in order to facilitate its use as a predictive tool for building professionals. It will thus become a decision support tool used in identifying areas for implementation of a PV cogen system.}
}
@article{ABUDEIF2017124,
title = {Dynamic geotechnical properties evaluation of a candidate nuclear power plant site (NPP): P- and S-waves seismic refraction technique, North Western Coast, Egypt},
journal = {Soil Dynamics and Earthquake Engineering},
volume = {99},
pages = {124-136},
year = {2017},
issn = {0267-7261},
doi = {https://doi.org/10.1016/j.soildyn.2017.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267726117304463},
author = {A.M. Abudeif and A.E. Raef and A.A. {Abdel Moneim} and M.A. Mohammed and A.F. Farrag},
keywords = {Nuclear Power Plant, Seismic Refraction Technique, Geotechnical, Egypt},
abstract = {Determination of the dynamic geotechnical properties and Vs30 of soil and rocks from seismic wave velocities serves as essential inputs for a foundation design cognizant of seismic site response and rock strength. This study evaluates a site which was suggested for a Nuclear Power Plant (NPP) in El-Dabaa area, north western coast of Egypt. On the near subsurface geology is made up of a thick succession of limestone overlain by a thin layer of soft soil. Assessment of geotechnical materials and Vs30 of the near sub-surface lithological layers are required for design of the foundation of critical structures like turbo-generator and reactor buildings. Interpretation of ninety one shallow P-waves and S-waves seismic refraction profiles distributed within the study area in conjunction with data of 76 boreholes were undertaken to delineate the dynamic properties of shallow soil for construction NPP. The velocity of the P- and S-waves were acquired and interpreted using SeisImager Software Package, then the results were used to build a velocity-depth model to estimate the depth to the bedrock and the thicknesses of overburden layers. This model was verified using boreholes data dissected the seismic profiles to improve the final velocity depth model. The depth to bedrock was determined from both shallow seismic refraction profiles and boreholes. Vs30, elastic moduli and dynamic geotechnical parameters were calculated and the site was classified as a National Earthquake Hazard Reduction Program (NEHRP) class “B”. The values of seismic velocities, the engineering consolidations, and the strength parameters showed that the bedrock in the study area is characterized by more competent rock quality.}
}
@article{MARI2009844,
title = {A computational system for uncertainty propagation of measurement results},
journal = {Measurement},
volume = {42},
number = {6},
pages = {844-855},
year = {2009},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2009.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0263224109000153},
author = {Luca Mari},
keywords = {Uncertainty propagation, Automatic differentiation, Computational methods for measurement},
abstract = {This paper discusses some design issues in the implementation of the law of uncertainty propagation according to an automatic differentiation strategy in the context of a simulation engine supporting the construction and the interactive testing of models of dynamic systems. The proposed solution propagates not only the partial derivatives, as usual in automatic differentiation, but also the input uncertainties, so to make their various modifications visible to the user of the evaluation system, and give him the opportunity to analyze the partial contributions to the standard uncertainty of the output measurands. A tool for uncertainty propagation in a general, user-oriented, computational system, instead of a software library or a dedicated system, makes uncertainty propagation transparently computable also for vector/matrix measurands, even in the case of dynamic systems, and makes uncertainty evaluation an inherent component of computational processes instead of an optional, ad hoc, addendum to them.}
}
@article{CASTANONJANO2018212,
title = {Use of explicit FEM models for the structural and parametrical analysis of rockfall protection barriers},
journal = {Engineering Structures},
volume = {166},
pages = {212-226},
year = {2018},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2018.03.064},
url = {https://www.sciencedirect.com/science/article/pii/S0141029617318461},
author = {Laura Castanon-Jano and Elena Blanco-Fernandez and Daniel Castro-Fresno and Diego Ferreño},
keywords = {Rockfall barrier, Interception nets, Numerical modelling, Parametric analysis, Geometric variables},
abstract = {This paper illustrates the experimental test procedure and results of two flexible barriers of low and medium energy, the so-called IBT-150 and IBT-500. For this purpose, ETAG 027 European Guideline is used. All the requirements for the tests performance are followed and the two energy-level tests performance requirements have been fulfilled in both rockfall barriers. Numerical modelling helps to understand and predict the behavior of these barriers with different configurations drastically reducing the costs of performing real tests. The results of the real test on IBT-150 and IBT-500 have been taken as references to validate two numerical models using Abaqus Explicit software. Afterwards, a presentation of some alternatives of the barrier IBT-150 are stated, which allow a more economical design removing some components that do not affect the energy level of 150 kJ set by the manufacturer. Also, a parametrical analysis of the IBT-500 numerical model has been performed varying the geometrical characteristics, such as the net grid dimension, the diameter of the perimeter cable, the length of the functional modules and its height. The aim of this analysis is the enhancement of maximum energy capacity of the barrier related with the amount of material used to build it. Following the ETAG recommendation, the maximum energy level (MEL) test is achieved if the barrier is able to retain the block. Thus, the MEL level for each numerical model was determined by increasing the initial speed of the block until it trespasses the barrier.}
}
@article{NAKHIMOVSKY199255,
title = {Building an engineering application for the PC-386},
journal = {Advances in Engineering Software},
volume = {15},
number = {1},
pages = {55-66},
year = {1992},
issn = {0965-9978},
doi = {https://doi.org/10.1016/0965-9978(92)90044-G},
url = {https://www.sciencedirect.com/science/article/pii/096599789290044G},
author = {Gregory Nakhimovsky and Charles E. Doherty},
keywords = {DOS Extenders, structural analysis, software integration, FORTRAN},
abstract = {With the advent of the Intel 80386 chip, the road has been cleared for application programmers to build large PC software products. This article describes our experience developing a complex engineering package designed specifically for the PC-386. The topics discussed include planning of the development approach, choice of the operating system and the programming language, programming methods and tools, and software integration techniques. The integration involves connecting analytical modules with each other and with menu-driven interfaces and graphics. The construction of a simple built-in text editor linked to specialized graphics modules is discussed. The paper also contains our comments and suggestions regarding the development tools and standards for programming languages. We hope our experience will be useful for application programmers, system programmers and developers of programming tools.}
}
@article{BOSCHE201290,
title = {Plane-based registration of construction laser scans with 3D/4D building models},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {1},
pages = {90-102},
year = {2012},
note = {Network and Supply Chain System Integration for Mass Customization and Sustainable Behavior},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000784},
author = {Frédéric Bosché},
keywords = {Construction, Coarse Registration, Laser Scan, Point Cloud, 3D model, BIM},
abstract = {With the development of building information modelling (BIM) and terrestrial laser scanning (TLS) in the architecture, engineering, construction and facility management (AEC/FM) industry, the registration of site laser scans and project 3D (BIM) models in a common coordinate system is becoming critical to effective project control. The co-registration of 3D datasets is normally performed in two steps: coarse registration followed by fine registration. Focusing on the coarse registration, model-scan registration has been well investigated in the past, but it is shown in this article that the context of the AEC/FM industry presents specific (1) constraints that make fully-automated registration very complex and often ill-posed, and (2) advantages that can be leveraged to develop simpler yet effective registration methods. This paper thus presents a novel semi-automated plane-based registration system for coarse registration of laser scanned 3D point clouds with project 3D models in the context of the AEC/FM industry. The system is based on the extraction of planes from the laser scanned point cloud and project 3D/4D model. Planes are automatically extracted from the 3D/4D model. For the point cloud data, two methods are investigated. The first one is fully automated, and the second is a semi-automated but effective one-click RANSAC-supported extraction method. In both cases, planes are then manually but intuitively matched by the user. Experiments, which compare the proposed system to software packages commonly used in the AEC/FM industry, demonstrate that at least as good registration quality can be achieved by the proposed system, in a simpler and faster way. It is concluded that, in the AEC/FM context, the proposed plane-based registration system is a compelling alternative to standard point-based registration techniques.}
}
@article{NATH2000177,
title = {SEISRES — a Visual C++ program for the sequential inversion of seismic refraction and geoelectric data},
journal = {Computers & Geosciences},
volume = {26},
number = {2},
pages = {177-200},
year = {2000},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(99)00086-2},
url = {https://www.sciencedirect.com/science/article/pii/S0098300499000862},
author = {Sankar Kumar Nath and Shamsuddin Shahid and Pawan Dewangan},
keywords = {RINSE, Evolutionary programming, Pseudo-section, Quasi-2D section, Aquifer},
abstract = {Refraction seismic and geoelectric methods are usually applied for the delineation of near surface structures in environmental, engineering and hydrogeological investigations. When applied independently, these techniques yield sufficiently accurate subsurface models. But the inversion of these data may also lead to incorrect parameter estimation specially in complicated geological situations, namely, blind zone problems in seismics, suppression and equivalence problems in geoelectrics. Stability and non-uniqueness can be reduced to a great extent by integrating physically different sets of data into a joint or sequential inversion scheme. In the present paper, we aim at introducing one such algorithm, wherein the seismic refraction and DC resistivity inversion routines are amalgamated. Even though the seismic and geoelectric methods may independently see different interfaces due to completely different physical responses, the joint or sequential inversion needs a common parameter, the layer thickness being the one in this situation. The proposed scheme is coded in Visual C++ on Microsoft Windows '95 environment using the concept of object-oriented programming. The program SEISRES is exclusively menu driven and customised for running on personal computers. It has several options, namely, seismic ray inversion for near-surface estimation, curve dissemination for generating a starting model using seismic depth section, 1D resistivity inversion for Schlumberger and Wenner single electrode arrays using evolutionary programming for global optimisation, 1D forward calculations, creation of resistivity data sets from Wenner multi-electrode pseudo-section and construction of quasi-2D geoelectric section of the subsurface. The software is tested on a variety of synthetic examples with complex litho-stratigraphic relationships. The present paper deals with three such synthetic examples for the delineation of an aquifer in a three-layer setting in the first two examples and the detection of a thin conductive clay lens embedded in an aquifer in a four layered earth model emulating an equivalence problem. The seismic-guided 1D Schlumberger and Wenner inversions and the quasi-2D sections from pseudo-section interpretation led to subsurface information with better precision compared to the direct 2D inversion. A detailed field investigation was undertaken in Midnapur District for ground water prospecting. The results of two test sites at Satkui and Tangasol are presented here for judging the performance of the software package. Available borehole lithologs and geophysical logs validated the findings from SEISRES. The strength of the present scheme lies in its ability to model the subsurface seismically and geoelectrically, even in 2D environment by 1D approximation on a laptop/personal computer at-site cost-effectively.}
}
@article{ADELI1991773,
title = {A hierarchical expert system for design of floors in highrise buildings},
journal = {Computers & Structures},
volume = {41},
number = {4},
pages = {773-788},
year = {1991},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(91)90187-Q},
url = {https://www.sciencedirect.com/science/article/pii/004579499190187Q},
author = {H. Adeli and D.W. Hawkins},
abstract = {A prototype knowledge-based expert system, called COFDEX (for COmposite Floor Design EXpert), has been developed for integrated design of composite floors in multistory buildings in accordance with the 1986 American Institute of Steel Construction (AISC) Load and Resistance Factor Design (LRFD) Specification. Design options include design for practical minimum cost or for minimum depth. Developed in the expert system programming environment GURU, COFDEX is a coupled system with a hierarchical structure in which procedural modules interact with rule sets and data bases. The expert system COFDEX demonstrates how AI technology complements the traditional numerical processing in automating the complicated process of engineering design and developing highly interactive software packages.}
}
@incollection{PILLMANN1980189,
title = {THE PROCESS COMPUTER CONTROL SYSTEM OF THE HYDROELECTRIC POWER STATIONS ALONG THE RIVER DANUBE IN AUSTRIA},
editor = {R. Isermann and H. Kaltenecker},
booktitle = {Digital Computer Applications to Process Control},
publisher = {Pergamon},
pages = {189-196},
year = {1980},
isbn = {978-0-08-026749-4},
doi = {https://doi.org/10.1016/B978-0-08-026749-4.50025-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080267494500254},
author = {W. Pillmann and R. Stefanich},
abstract = {Abstract
In Austria the control centers of the six hydroelectric power stations along the river Danube are equipped with process computers. The control systems are designed for advanced control of the plants, including multilevel process control, interactive man-machine interfaces, advanced monitoring, security related functions and for supervising the hydroelectric plant cascade in the future. During implementation particular attentions was paid to the water level and flow rate control system in the power stations. The principles of design, the realisation of this direct digital control and the up to date experiences with the controller operation are presented in this paper. Furthermore hardware structure of the control center, the construction principles of the software package and the man machine interface are discussed briefly.}
}
@article{ABDALRAHMAN20171353,
title = {Pitch angle control for a small-scale Darrieus vertical axis wind turbine with straight blades (H-Type VAWT)},
journal = {Renewable Energy},
volume = {114},
pages = {1353-1362},
year = {2017},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2017.07.068},
url = {https://www.sciencedirect.com/science/article/pii/S0960148117306961},
author = {Gebreel Abdalrahman and William Melek and Fue-Sang Lien},
keywords = {Darrieus vertical axis wind turbine (H-type VAWT), Computational fluid dynamics (CFD), Variable pitch angle control, Multilayer perceptron artificial neural network (MLP-ANN)},
abstract = {Unlike horizontal axis wind turbines (HAWTs), the Darrieus vertical axis wind turbine (H-type VAWT) has been the subject of only a few recent studies directed at improving its self-starting capability and/or aerodynamic performance. The technique currently used for improving the performance of this type of turbine is pitch angle control. This paper presents intelligent blade pitch control for enhancing the performance of H-type VAWTs with respect to power output. To determine the optimum pitch angles, ANSYS Fluent Computational Fluid Dynamics (CFD) software was used for a study of the aerodynamic performance of a 2D variable pitch angle H-type VAWT at a variety of tip speed ratios (TSRs). For each case examined, the power coefficient (Cp) was calculated and compared to published experimental and CFD findings. The results obtained from the CFD model were then applied for the construction of an aerodynamic model of an H-type VAWT rotor, which constituted a prerequisite for designing an intelligent pitch angle controller using a multilayer perceptron artificial neural network (MLP-ANN) method. The performance of the MLP-ANN blade pitch controller was compared to that of a conventional controller (PID). The findings demonstrate that for an H-type VAWT, compared to a conventional PID controller, an MLP-ANN results in superior power output.}
}
@article{ABDELHAMEED202249,
title = {Experimental and numerical investigation on the performance of adhesive steel-dowels used in precast reinforced concrete elements},
journal = {Structures},
volume = {40},
pages = {49-63},
year = {2022},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2022.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352012422002673},
author = {Samya {Abd El-Hameed} and Mohamed Eladawy and Mohamed H. Agamy and Hesham Haggag},
keywords = {Pre-cast reinforced concrete, Push-off test, Beam–column dowel connection, Shear capacity, Adhesive steel dowel, Finite element, Shear friction, ANSYS},
abstract = {Precast reinforced concrete (PCRC) applications provide several structural, economic, and environmental benefits. Therefore, recently the PCRC elements have been widely used in construction implementation. However, the connection's behavior and its possible failure under different types of loads are considered essential challenges during the design of the PCRC elements. This study aimed to investigate the performance of the adhesive dowels connections and their shear capacity. The experimental phases focused on examining eighteen push-off concrete specimens and four reinforced concrete (RC) beams-column connections. While the numerical phase was conducted using the finite-element software package ANSYS to simulate concrete connections behavior. Push-off specimens consisted of two concrete blocks measured 250 mm in width. Blocks connected by steel dowel that crossed the shear plane. While beams measured 120 mm in width, 300 mm in depth, and 1400 mm in length. Beams connected to existing 300 mm square column with adhesive dowels. All specimens were loaded monotonically until failure through a constant loading rate. The experimental study aimed to evaluate the effects of the shear plane depth, dowels’ embedded length, dowels’ diameter, the edge distance, and dowels ratio The test results revealed that the shear plane conditions and area, as well as the adhesive dowels configurations, had a significant effect on the shear capacity of dowel specimen. Moreover, the finite element model provided a better understanding of the different modes of failure observed during experimental testing and the shear capacity of adhesive dowels.}
}
@article{RUB1976171,
title = {Operating System Modules for Process Control Application},
journal = {IFAC Proceedings Volumes},
volume = {9},
number = {2},
pages = {171-174},
year = {1976},
note = {1st IFAC/IFIP Symposium on Software For Computer Control, Tallinn, USSR, 25-28 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)67420-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017674202},
author = {W. Rüb and G. Schrott},
abstract = {Any effort towards portable software for special process control problems has to provide for efficient, economic and adaptable operating system design. In this paper, an approach is shown developing a basic layer suitable for constructing a dedicated realtime operating system with little ef-fort. These functions are divided into mod-ules according to their scope: interrupt handling, processor dispatching, synchro-nization, communication, short-time sched-uling and primitive input/output. The re-sulting program package yields a tool for the construction of a realtime operating system. Hardware-dependent parts are func-tionally described and strategies are sep-arated into modules which allows easy mod-ifications to adapt to special applica-tions. The concept is illustrated with a set of functions implemented for the PDP11 family. Construction, implementation de-tails and experiences are given.}
}
@article{MENS200211,
title = {Barcelona, Spain, October 7-8, 2002: Graph-Based Tools (GraBaTs 2002)},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {72},
number = {2},
pages = {11-13},
year = {2002},
note = {GraBaTs 2002, Graph-Based Tools (First International Conference on Graph Transformation)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/S1571-0661(05)80523-7},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105805237},
author = {Tom Mens and Andy Schürr and Gabriele Taentzer},
abstract = {Graphs are well-known, well-understood, and frequently used means to depict networks of related items. They are successfully used as the underlying mathematical concept in various application domains. In all these domains tools are developed that store, retrieve, manipulate and display graphs as underlying data structures, despite of the fact that in most cases these graphs have a different name such as object diagrams, (meta) class diagrams, hyper documents, semantic webs etc. It is the purpose of this workshop to summarize the state of the art of graph-based tool development, bring together developers of graph-based tools in different application fields and to encourage new tool development cooperations. Motivation Graphs are an obvious means to describe structural aspects in various fields of computer science. They have been successfully used in application areas such as compiler compiler toolkits, constraint solving problems, generation of CASE tools, pattern recognition techniques, program analysis, software engineering, software evolution, software visualization and animation, and visual languages. In all these areas tools have been developed that use graphs as an important underlying data structure. Since graphs are a very general structure mechanism, it is a challenge to handle graphs in an effective way. Using graphs inside tools the following topics play an important role: efficient graph algorithms, empirical and experimental results on the scalability of graphs, reusable graph-manipulating software components, software architectures and frameworks for graph-based tools, standard data exchange formats for graphs, more general graph-based tool integration techniques, and meta CASE tools or generators for graph-based tools. The aim of the workshop on graph-based tools (GraBaTs) is to bring together developers of all kinds of graph-based tools in order to exchange their experiences, problems, and solutions concerning the efficient handling of graphs. The GraBaTs workshop is, therefore, of special relevance for the http://link.springer.de/link/service/series/0558/tocs/t2505.htm 1st Intl. Conference on Graph Transformation (ICGT) which hosts GraBaTs as a satellite event: In many cases the application of graph transformation technology requires the existence of reliable, user-friendly and efficiently working graph transformation tools. These tools in turn have to be built on top of basic services or frameworks for graphs, which are the main topic of our workshop. Today, several graph transformation tool implementations have emerged which do not share any basic graph services (e.g. for graph pattern matching or graph layout purposes) and which implement rather different graph concepts and graph transformation approaches. Some of these tools - as a kind of survey of the state of the art - were presented in a special session, which is part of the main conference as well as of this satellite workshop. The presented tools are AGG, DiaGen, Fujaba, GenGED, and UPGRADE. The GraBaTs workshop was held for 1 12 days. Its schedule contained in addition to the afore-mentioned session on graph transformation tools, an invited talk by Tiziana Margaria (University of Dortmund, Germany) on ETI, an electronic tool integration platform where graph-based tools will play an important role. Apart from four sessions with presentations of 15 accepted papers (out of 19 submissions) on various graph-based tools and tool-relevant topics, a successful discussion ''Towards Standard Exchange Formats for Graph and Graph Transformation'' took place. Workshop Issues The workshop aims at bringing together tool developers from different fields, dealing with graphs from different perspectives. In the following, we give an overview on the most important perspectives. Meta-modeling by Graphs For a long time the syntax and static semantics of most visual modeling or programming languages was only defined by means of characteristic examples and informal descriptions. To improve this situation the visual language community invented grammar-based formalisms for the definition of the syntax of their languages, such as constraint grammars, graph grammars, relational grammars, etc. Unfortunately it turned out that the grammar-based definition of visual languages is rather complicated compared with the meta-modeling approach developed in parallel. The Meta-modeling approach for the definition of visual languages uses a combination of class diagrams (ER-diagrams, etc.) and predicate logic expressions (Z, OCL, etc.) to define the syntax and static semantics of visual languages. It became popular with the standardization of the OO-modeling language UML and is used by various meta-modeling (meta-CASE) tools which are able to generate domain-specific CASE tools. The so-called MOF approach (Meta-Object Facility) is one attempt to come up with a meta-modeling standard. Despite of its limited expressiveness (compared with ER diagrams or UML class diagrams) MOF builds the basis for the formal definition of UML and other visual languages. All meta-modeling approaches used nowadays have one common property: they offer graph-like diagrams for the definition of the structure (syntax) of graph-like diagram languages. Therefore, meta-modeling is in fact the formal definition of graph languages by means of graphs which are instances of “meta” graph languages. As a consequence, meta-CASE tools are a special class of graph-based tools, which need at least basic services for storing, visualizing, and analyzing graphs. Graph Visualization Facilities for visualizing graphs are needed by all kinds of graph-based tools, independent of the fact whether they are e.g. used for meta-modeling or rule-based programming purposes. Furthermore, graph visualization techniques are the most important means for visualizing various aspects of softwarearchitectures, the dynamic behavior of running systems, their evolution history, and so forth. Software components developed for these purposes usually have to deal with huge graphs and need services for making these graphs persistent, for introducing abstractions based on hierarchical graph models, for computing reasonable graph layouts (efficiently), and for displaying graphs effectively using “fish-eye-techniques” and the like. And last but not least, graph visualization techniques are often employed for teaching purposes in computer science courses on “data structures and (graph) algorithms”. To summarize, almost all kinds of graph-based tools urgently need efficiently and effectively working graph visualization services, whereas graph visualization tools may profit from research activities on graph query and graph transformation engines for the computation of graph abstractions or views. We, therefore, hope that this workshop encourages researchers to start new cooperations, such as adapting graph visualization tools to the needs of graph manipulation tools or exploiting graph manipulation and transformation techniques to compute sensible abstractions of huge graphs. Graph Queries and Graph Algorithms Most, if not all, graph-based tools use to a certain degree software components (libraries, subsystems, etc.) for executing graph queries and/or various kinds of standard graph algorithms. For example, graph transformation tools rely on rather sophisticated means for computing graph matches (rule occurrences) and graph-based reverse engineering tools need rather powerful query engines for determining critical substructures of software architectures. On the other hand, quite a number of database management systems have already been developed using graphs (networks of related objects) as the underlying data model and offering query languages based on graph path expressions or even graph transformations. Vice versa, graph transformation languages like PROGRES are not only used for specifying and visualizing graph algorithms, but incorporate many elements of database query languages such as means for the construction of indexes, the materialization and incremental update of views, etc. Therefore, we like to encourage tool developers again to start cooperating across the boundaries of research areas. Graph Transformation Graph transformation means the rule-based manipulation of graphs. Several graph transformation approaches have emerged which differ w.r.t. to the underlying kind of graphs as well as in the way how rules are applied to graphs, i.e. graph transformation takes place. The kind of graphs used by these tools include labeled, directed graphs, hypergraphs, and graph structures. Their rules, the basic means to manipulate graphs, differ w.r.t. to the formal definition of their semantics, the way how occurrences (matches) are searched for, and how matching rules are applied eventually. In tools, graph transformation is applied to visual languages, specification, code generation, verification, restructuring, evolution and programming of software systems, etc. Developers of graph transformation tools may profit from other workshop participants concerning more efficient realizations of basic functionality, while developers of other graph-based tools might find the graph transformation paradigm attractive to implement certain graph manipulations. The workshop may also provide insights to apply these tools to other application domains. Common Exchange Formats for Graphs and Graph Transformation To support interoperability between various graph-based tools, several initiatives on the development of common exchange formats for graphs have been founded. These formats are all based on the extensible markup language XML developed to interchange documents of arbitrary types. Preceding events like three subgroup meetings of the EU Working Group APPLIGRAPH, a Workshop on Standard Exchange Formats, and a satellite workshop of the 8th Intl. Symposium on Graph Drawing (GD 2000)discussed various ideas which are currently converging to one format being GXL. During the GraBaTs workshop a further discussion round on this topic was organized focusing especially on graph layout and graph attributes. Another topic of interest for this discussion is an exchange format for graph transformation systems called GTXL, which is under development and which will be built on top of GXL. Workshop Organizers The Program Committee of the workshop consists of: Luciano Baresi (Italy)Giuseppe Di Battista (Italy)Ulrik Brandes (Germany)Scott Marshall (The Netherlands)Tom Mens (Belgium) (Co-chair)Andy Schürr (Germany) (Co-chair)Gabriele Taentzer (Germany) (Co-chair)Andreas Winter (Germany)Albert Zündorf (Germany) We are very grateful to Hartmut Ehrig for his help with the organization of the Workshop as satellite event of the 1st Int. Conference on Graph Transformation (ICGT) and to Mike Mislove, one of the Managing Editors of the ENTCS series. Thanks are also due to Fernando Orejas and his local organizers at UPC in Barcelona who supplied preprints of this volume for all workshop participants.}
}
@article{SCHNEIDER2008355,
title = {Simulating the cornering behaviour of multiple trailed implements},
journal = {Biosystems Engineering},
volume = {100},
number = {3},
pages = {355-361},
year = {2008},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2008.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1537511008000871},
author = {Till Schneider and John M. Fielke},
abstract = {The trend towards using larger sowing widths and capacities has led to the construction of heavy, multi-axle machinery that can be used in a configuration of one implement behind the other. An Australian example of this is a 300kW four-wheel drive tractor towing an 18m wide cultivator behind which is hitched to a 15000l 3 bin pneumatic seeder. Additional soil-levelling implements may also be hitched behind this combination. The use of several implements hitched in series influences the steerability, and may limit the minimum turning radius. A knowledge of the performance of linkages and machine hitching options will assist both designers and users of machinery. This paper presents a method of simulating implement tracking to aid the design of steering and hitching components for use in optimising a machine combination. A simulation was developed using the software packages SolidWorks and CosmosMotion to control solid modelled machines using a range of geometric and tyre performance parameters. The simulation was verified against standard truck turning path templates and under conditions when Ackerman geometry was achieved. The methods used and typical outputs from the simulations are described.}
}
@article{JACOME201663,
title = {BIOMedical Search Engine Framework: Lightweight and customized implementation of domain-specific biomedical search engines},
journal = {Computer Methods and Programs in Biomedicine},
volume = {131},
pages = {63-77},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715300560},
author = {Alberto G. Jácome and Florentino Fdez-Riverola and Anália Lourenço},
keywords = {Search engine framework, Biomedical literature, Vertical engine, Text mining, Web application},
abstract = {Background and objectives
Text mining and semantic analysis approaches can be applied to the construction of biomedical domain-specific search engines and provide an attractive alternative to create personalized and enhanced search experiences. Therefore, this work introduces the new open-source BIOMedical Search Engine Framework for the fast and lightweight development of domain-specific search engines. The rationale behind this framework is to incorporate core features typically available in search engine frameworks with flexible and extensible technologies to retrieve biomedical documents, annotate meaningful domain concepts, and develop highly customized Web search interfaces.
Methods
The BIOMedical Search Engine Framework integrates taggers for major biomedical concepts, such as diseases, drugs, genes, proteins, compounds and organisms, and enables the use of domain-specific controlled vocabulary. Technologies from the Typesafe Reactive Platform, the AngularJS JavaScript framework and the Bootstrap HTML/CSS framework support the customization of the domain-oriented search application. Moreover, the RESTful API of the BIOMedical Search Engine Framework allows the integration of the search engine into existing systems or a complete web interface personalization.
Results
The construction of the Smart Drug Search is described as proof-of-concept of the BIOMedical Search Engine Framework. This public search engine catalogs scientific literature about antimicrobial resistance, microbial virulence and topics alike. The keyword-based queries of the users are transformed into concepts and search results are presented and ranked accordingly. The semantic graph view portraits all the concepts found in the results, and the researcher may look into the relevance of different concepts, the strength of direct relations, and non-trivial, indirect relations. The number of occurrences of the concept shows its importance to the query, and the frequency of concept co-occurrence is indicative of biological relations meaningful to that particular scope of research. Conversely, indirect concept associations, i.e. concepts related by other intermediary concepts, can be useful to integrate information from different studies and look into non-trivial relations.
Conclusions
The BIOMedical Search Engine Framework supports the development of domain-specific search engines. The key strengths of the framework are modularity and extensibilityin terms of software design, the use of open-source consolidated Web technologies, and the ability to integrate any number of biomedical text mining tools and information resources. Currently, the Smart Drug Search keeps over 1,186,000 documents, containing more than 11,854,000 annotations for 77,200 different concepts. The Smart Drug Search is publicly accessible at http://sing.ei.uvigo.es/sds/. The BIOMedical Search Engine Framework is freely available for non-commercial use at https://github.com/agjacome/biomsef.}
}
@article{THOMAS198545,
title = {An approach to data encapsulation in the teaching of computing},
journal = {Education and Computing},
volume = {1},
number = {1},
pages = {45-49},
year = {1985},
issn = {0167-9287},
doi = {https://doi.org/10.1016/S0167-9287(85)93659-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167928785936593},
author = {P.G. Thomas},
keywords = {Data encapsulation, Data hiding, programming languages, OUSBASIC, Package, Modularity, Educational software},
abstract = {Data Encapsulation, also known as data hiding, has been recognised as an important concept in the construction of maintainable programs [5]. It is a concept that should form part of a modern programming course. Such a course ought to provide practical experience of encapsulating data. Unfortunately, many programming languages used in the teaching of programming do not support this facility. This paper argues the case for teaching about data encapsulation early in a student's career and illustrates how the technique can usefully be employed in a first course on programming. The paper shows the approach taken in OUSBASIC — a programming language designed for teaching computing.}
}
@article{GLUECKSTERN1985469,
title = {Use of microprocessors for control, data aquisition and on line performance evaluation in a reverse osmosis plant},
journal = {Desalination},
volume = {55},
pages = {469-480},
year = {1985},
issn = {0011-9164},
doi = {https://doi.org/10.1016/0011-9164(85)80091-6},
url = {https://www.sciencedirect.com/science/article/pii/0011916485800916},
author = {P. Glueckstern and M. Wilf and J. Etgar and J. Ricklis},
abstract = {Design and implementation of microprocessor's control and data aquisition in a large (over 3 MGD) reverse osmosis desalting plant will be reported. The RO plant desalt brackish water of about 6,000 ppm TDS salinity and supplies potable water to the town of Eilat. The first desalting unit of the plant commenced operation in 1978. The control system of the plant was initially based on conventional electromechanical relays, timers and counters. The measuring instruments gave local indication only. An evaluation performed a year ago indicated that in order to optimize the plant operation and to improve data aquisition, a modernization of the instrumentation and control system is required. The new control system is based on programable controlers (PLC) receiving digital and analoge signals of the field measured parameters through sensors and transmiters. The PLC transfers the information into an industrial computer which controls the operation of the desalting plant according to the software package especially prepared for this plant. The control network includes also an on line personal computer for storage of historical data and reports generation. The new system is at the final stages of construction.}
}
@article{SHEMSHADIAN201989,
title = {Numerical study of the behavior of intermeshed steel connections under mixed-mode loading},
journal = {Journal of Constructional Steel Research},
volume = {160},
pages = {89-100},
year = {2019},
issn = {0143-974X},
doi = {https://doi.org/10.1016/j.jcsr.2019.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0143974X18306825},
author = {Mohammad E. Shemshadian and Jia-Liang Le and Arturo E. Schultz and Patrick McGetrick and Salam Al-Sabah and Debra F. Laefer and Anthony Martin and Linh Truong Hong and Minh Phuoc Huynh},
keywords = {Steel connections, Intermeshed, Mixed loading, Finite element analysis, Interaction},
abstract = {In recent years, advanced manufacturing techniques, such as high-definition plasma, water jet, and laser cutting, have opened up an opportunity to create a new class of steel connections that rely on intermeshed (i.e. interlocked) components. The main advantage of this type of connection is that they do not require either welding or bolting, which allows faster construction. Although the interest in intermeshed connections has increased in recent years, the mechanical behavior of these connections has not been fully understood. This paper presents a numerical study on the ultimate load capacity failure modes of intermeshed connections under mixed-mode loading. The experimental behavior of the connection components is also investigated through a series of tests. The study considers a recently developed intermeshed connection for beams and columns. The numerical simulations were performed by using a commercially available 3D finite element software package. By considering different types of mixed mode loading, interaction diagrams of axial, shear, and moment capacities of the intermeshed connection were obtained. The results indicated that there exists an intricate interaction among axial, shear, and moment capacities, which arises from the intermeshed configuration of the flanges and web. For each interaction diagram, the corresponding failure mechanism was analyzed. The simulated interaction between axial, shear, and moment capacities were further compared with the provision of the current design codes. While the intermeshed connection studied here showed promise for gravity loading, further study is needed to ensure alignment of the flanges so as to avoid axial and/or flexural failures.}
}
@incollection{DATTA1989293,
title = {PARALLEL AND LARGE SCALE MATRIX COMPUTATIONS IN CONTROL: SOME IDEAS****Permanent Address: Department of Mathematical Sciences, Northern Illinois University, DeKalb, IL 60115},
editor = {CHEN ZHEN-YU},
booktitle = {Computer Aided Design in Control Systems 1988},
publisher = {Pergamon},
address = {Oxford},
pages = {293-299},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035738-6},
doi = {https://doi.org/10.1016/B978-0-08-035738-6.50051-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357386500513},
author = {B.N. Datta and K. Datta},
abstract = {The design and analysis of time-invariant linear control systems give rise to a variety of interesting linear algebra problems. Numerically effective methods now exist for several of these problems. However, algorithms for large scale computations and efficient parallel algorithms for these problems are virtually nonexistent. In this paper, we propose several efficient generalpurpose parallel algorithms for single-input and multi-input eigenvalue assignment problems. A desirable feature of these algorithms is that they are composed of simple linear algebraic operations such as matrix-vector multiplication, solution of a linear system, and computations of eigensystem and singular values of a symmetric matrix, for which efficient parallel algorithms have already been developed and parallel software libraries are being built based on these algorithms. The proposed algorithms thus have potential for implementations on some existing and future parallel processors. We also propose a numerical method for Sylvester matrix equation arising in the construction of Luenberger observer. The method does not need reduction to “condensed” forms and is thus suitable for large and sparse matrices. The method also exhibits certain parallelism.}
}
@incollection{SCHEJBAL1990147,
title = {Methods and Techniques of the Prediction of Metallic and Nonmetallic Raw Materials Using Microcomputers in Czechoslovakia},
editor = {GABOR GAÁL and DANIEL F. MERRIAM},
booktitle = {Computer Applications in Resource Estimation},
publisher = {Pergamon},
address = {Amsterdam},
pages = {147-154},
year = {1990},
series = {Computers and Geology},
isbn = {978-0-08-037245-7},
doi = {https://doi.org/10.1016/B978-0-08-037245-7.50016-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080372457500160},
author = {C. Schejbal and J. Hruska},
abstract = {ABSTRACT
During the last ten years, various methods and computer techniques have been designed or derived from existing models of mineral-potential assessment in Czechoslovakia. Three centers have reached practical and scientific results of national and international importance regarding recent microcmputers:(1)Geoindustria in Jihlava - statistical models, particularly those based on pattern recognition, also including comprehensive heuristic principles for regional assessment (system “Prognos”). The construction of a metallogenic-geochemical and geophysical scheme for 1:100,000 to 1:500,000 geological-deposit synthetic maps has been implemented on a H-P 9845 B including graphics for the HP 7580.(2)Mining geology center for computer applications and geostatistics at the Faculty of Mining and Geology in Ostrava has developed and currently exploits a Geostratistical Software Package “Micro GAD” to solve ore-reserve estimation and sampling problems. A supplemental spatial model on ore-district prediction of volumetric values of metal included applies multivariate statistics. Advanced Geostatistical Methods for Geology and Mining now are being tested for PC-TNS, a compatible Czechoslovak variety of IBM-PC.(3)Mathematical geology unit at Geological exploration Enterprise in Spisska Nova Ves, Slovakia, using Olivetti PC facilities, has elaborated a comprehensive system of geochemical and structure-metallogenic data evaluation for mineral-potential assessment focused on mineral deposits mapping on a 1:25,000 scale. Statistical methods used are mostly multiple regression with factor or characteristic analysis.}
}
@article{LIU2001563,
title = {An XML-enabled data extraction toolkit for web sources},
journal = {Information Systems},
volume = {26},
number = {8},
pages = {563-583},
year = {2001},
note = {Data Extraction,Cleaning and Reconciliation},
issn = {0306-4379},
doi = {https://doi.org/10.1016/S0306-4379(01)00040-0},
url = {https://www.sciencedirect.com/science/article/pii/S0306437901000400},
author = {Ling Liu and Calton Pu and Wei Han},
keywords = {Information extraction, Internet data management, Program generation, XML, Wrappers, World Wide Web},
abstract = {The amount of useful semi-structured data on the web continues to grow at a stunning pace. Often interesting web data are not in database systems but in HTML pages, XML pages, or text files. Data in these formats are not directly usable by standard SQL-like query processing engines that support sophisticated querying and reporting beyond keyword-based retrieval. Hence, the web users or applications need a smart way of extracting data from these web sources. One of the popular approaches is to write wrappers around the sources, either manually or with software assistance, to bring the web data within the reach of more sophisticated query tools and general mediator-based information integration systems. In this paper, we describe the methodology and the software development of an XML-enabled wrapper construction system—XWRAP for semi-automatic generation of wrapper programs. By XML-enabled we mean that the metadata about information content that are implicit in the original web pages will be extracted and encoded explicitly as XML tags in the wrapped documents. In addition, the query-based content filtering process is performed against the XML documents. The XWRAP wrapper generation framework has three distinct features. First, it explicitly separates tasks of building wrappers that are specific to a web source from the tasks that are repetitive for any source, and uses a component library to provide basic building blocks for wrapper programs. Second, it provides inductive learning algorithms that derive or discover wrapper patterns by reasoning about sample pages or sample specifications. Third and most importantly, we introduce and develop a two-phase code generation framework. The first phase utilizes an interactive interface facility to encode the source-specific metadata knowledge identified by individual wrapper developers as declarative information extraction rules. The second phase combines the information extraction rules generated at the first phase with the XWRAP component library to construct an executable wrapper program for the given web source.}
}
@article{DATTA1988293,
title = {Parallel and Large Scale Matrix Computations in Control: Some Ideas**},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {8},
pages = {293-299},
year = {1988},
note = {4th IFAC Symposium on computer aided Design in Control Systems 1988, Beijing, PRC, 23-25 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54968-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701754968X},
author = {B.N. Datta and K. Datta},
keywords = {Large scale computations, parallel computations, pole-placement algorithms, observer matrix equation},
abstract = {The design and analysis of time-invariant linear control systems give rise to a variety of interesting linear algebra problems. Numerically effective methods now exist for several of these problems. However, algorithms for large scale computations and efficient parallel algorithms for these problems are virtually nonexistent. In this paper, we propose several efficient generalpurpose parallel algorithms for single-input and multi-input eigenvalue assignment problems. A desirable feature of these algorithms is that they are composed of simple linear algebraic operations such as matrix-vector multiplication, solution of a linear system, and computations of eigensystem and singular values of a symmetric matrix, for which efficient parallel algorithms have already been developed and parallel software libraries are being built based on these algorithms. The proposed algorithms thus have potential for implementations on some existing and future parallel processors. We also propose a numerical method for Sylvester matrix equation arising in the construction of Luenberger observer. The method does not need reduction to”condensed” forms and is thus suitable for large and sparse matrices. The method also exhibits certain parallelism.}
}
@article{VASQUEZ20031585,
title = {Nonlinear interaction design provisions using spectral superposition},
journal = {Engineering Structures},
volume = {25},
number = {13},
pages = {1585-1595},
year = {2003},
issn = {0141-0296},
doi = {https://doi.org/10.1016/S0141-0296(03)00124-X},
url = {https://www.sciencedirect.com/science/article/pii/S014102960300124X},
author = {Jorge Vásquez},
keywords = {Modal combination, Cross-estimators, Nonlinear interaction capacity requirements, Single direction excitation, Most unfavorable direction},
abstract = {The problem of enforcing two-variable nonlinear interaction code provisions within a spectral superposition approach to design, is addressed. An analytical procedure based on the linearization of the interaction curve through a set of tangents or secants is developed. Safety with respect to the interaction curve is approximated by requiring safety with respect to that set of straight lines. For calculating the required estimators, the cross-estimator based formula for estimators of the linear combination of variables, derived in a companion paper, is used. The analytical method developed was shown to be equivalent to a graphical method proposed by Gupta, based on inscribing an estimator ellipse within the interaction curve. The analytical method is more straightforward and handles the directional maximum in single-component excitation, which the graphical method does not. The analytical method is much easier to apply. However, if a study calls for actually drawing the estimator ellipse, straightforward construction methods are presented, making unnecessary a cumbersome equivalent modal response approximation that had been suggested. The inherent antisymmetry of the spectral superposition formula, and its implications, are discussed. The effect of static loading and of symmetry of the interaction curve is also analyzed. Within the analysis of an example, the work required for the application of the procedure for nonlinear interaction for design purposes is discussed. It is found that its implementation can be achieved through a very simple function written for any standard numerical computation software package. The example also makes quite apparent the advantages of the analytical over the graphical method. The application example, which considers the design of a concrete column in a simple 10-storey building structure, shows the overconservativeness of a design based only in the standard estimators. The design criterion used in the example is that of the most unfavorable direction of a single-component earthquake.}
}
@article{MORANDEAU201314,
title = {Optimisation of marine energy installation operations},
journal = {International Journal of Marine Energy},
volume = {3-4},
pages = {14-26},
year = {2013},
note = {Special Issue – Selected Papers - EWTEC2013},
issn = {2214-1669},
doi = {https://doi.org/10.1016/j.ijome.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214166913000283},
author = {Maxime Morandeau and Rich T. Walker and Richard Argall and Rachel F. Nicholls-Lee},
keywords = {Dynamic Positioning (DP) vessel, Marine renewable energy, Offshore operations planning tool, Operational downtime, Project planning, Tidal array installation, Weather window},
abstract = {In order to meet the increasing demand for energy from renewable sources, the UK is being encouraged towards offshore array deployments and marine energy parks. The pressure to perform efficient, timely and cost effective marine operations consequently increases. Central to success is the accessibility of the site by suitable installation vessels and associated with this is the availability of appropriate weather windows, subsequently, via careful planning, the reduction of downtime and its associated cost. There are a number of methods in existence which analyse the likelihood of accessible conditions occurring; however few methods sufficiently address complete, complex installation operations and vessel selection. Furthermore there exists a shortage of cost effective, appropriate installation vessels, particularly when seeking those capable of working in severe tidal currents. Efficient utilisation of available resource is therefore essential. This work discusses two, complementary, solutions to these problems. Firstly a software package that has been specifically engineered for the analysis and optimisation of marine energy installations, Mermaid (Marine Economic Risk Management Aid). The second is a fit-for-purpose vessel designed specifically to perform optimally during installation, maintenance and decommissioning of marine energy devices – the HiFlo-4 Installation Vessel (HF4). Herein Mermaid is applied to a case study tidal energy array installation, demonstrating the methods which may be used to optimise the process, ultimately achieving a reduction in capital expenditure. Throughout this analysis traditional offshore construction vessels are considered alongside the HF4. This process demonstrates the importance of performing a thorough weather risk analysis, and of selecting the most suitable tools and methods for the task at hand.}
}
@article{WIERZBICKI1999374,
title = {Multi-objective modeling for engineering applications:DIDASN++ system 11The research reported in this paper was partly supported by the grant No. 0958/P4/94/06 of the Committee for Scientific Research of Poland.},
journal = {European Journal of Operational Research},
volume = {113},
number = {2},
pages = {374-389},
year = {1999},
issn = {0377-2217},
doi = {https://doi.org/10.1016/S0377-2217(98)00222-7},
url = {https://www.sciencedirect.com/science/article/pii/S0377221798002227},
author = {Andrzej P. Wierzbicki and Janusz Granat},
keywords = {Multi-objective optimization, Modeling, Decision support, Computer-aided design},
abstract = {Abstract
Modeling and simulation of various physical, technical, environmental or socioeconomic processes is often a preliminary step for using the resulting models in computer-aided design or decision support. In engineering computer-aided design, the decisions of the designer might be supported by multicriteria optimization - which in this case should not be considered as a tool for supporting the final choice of the design, but much more as a tool for helping in a flexible analysis of various design options or even various modeling and simulation options. The paper shows how multicriteria optimization techniques can be used for multi-objective analysis of a model from the beginning stages of model construction. With the advancement of computing technology and the methodology of decision support, it is now possible to revise this way basic approaches to modeling and simulation. Various formats of defining nonlinear and time-discrete models are discussed together with related problems of inverse and softly constrained multi-objective simulation. Algebraic differentiation and sensitivity analysis, fuzzy set representation of modeler preferences are also useful techniques of multi-objective modeling. Such techniques are illustrated by engineering applications of a software package DIDASN++ in mechanics and automatic control.}
}
@article{DZWIERZYNSKA20161608,
title = {Direct Construction of an Inverse Panorama from a Moving View Point},
journal = {Procedia Engineering},
volume = {161},
pages = {1608-1614},
year = {2016},
note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning Symposium 2016, WMCAUS 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.634},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816328636},
author = {Jolanta Dzwierzynska},
keywords = {inverse panorama, moving view, panoramic projection, objecft location},
abstract = {The aim of the herby study was a direct and practical mapping of an inverse cylindrical panorama with computer aid. An inverse panoramic projection it is the projection onto a cylindrical, rotary surface or on a fragment of this surface, in which the center of projection is not located like in a typical cylindrical panorama “inside” the cylindrical surface but outside of it. Spreading this idea in the paper, it was taken into consideration the kind of the inverse cylindrical projection from a view point being not stationary but moving. This representation was defined as a multicentre projection from the centres dispersed on a line path which could be straight or curved. Such an approach gave maximum approximation of the received results of the considered projection to real perception one experienced observing the image. The graphical mapping the effects of the representation could be realized directly on the unfolded surface - flat background of the projection. That is due to the projective and graphical connection between points displayed on the cylindrically curved background and their counterparts received on the unrolled flat surface. It allowed to develop a descriptive method for creating edge images of given objects. However, for a significant improvement of the construction of lines, the analytical algorithms were formulated in Mathcad software. Still, they can be implemented in majority of the computer graphical packages, which makes drawing panoramas more efficient and easier. The presented inverse panoramic representation, and the way of its mapping directly on the unrolled flat background can find application in different representations of architectural space in advertisement and art when drawings are displayed on the cylindrically curved surfaces.}
}
@article{TSENG1989207,
title = {A CNC machining system for education},
journal = {Journal of Manufacturing Systems},
volume = {8},
number = {3},
pages = {207-214},
year = {1989},
issn = {0278-6125},
doi = {https://doi.org/10.1016/0278-6125(89)90042-3},
url = {https://www.sciencedirect.com/science/article/pii/0278612589900423},
author = {A.A. Tseng and S.P. Kolluri and P. Radhakrishnan},
keywords = {Computer Numerical Control, Microprocessor, Machine Tool, APT Programming},
abstract = {A computer numerical control (CNC) machining system has been developed. The system includes a three-axis milling/drilling machine, a microprocessor, system software, and an automatically programmed tools (APT) processor. The objective of developing this machining system is to enable new features to be introduced, especially for the educational environment. A unique feature of the system is the use of a general purpose microprocessor as a controller. The CNC system is designed to include fundamental features with simple software format and a small scale construction. These make the system a versatile educational tool. An APT processor is developed, which can be used to link a geometric file devised by a commercial computer aided design software package to the CNC machine. The cost trade-offs of using the newly designed system and a comparison with commercial systems are also included.}
}
@article{SANDERS199589,
title = {The UltraSAN modeling environment},
journal = {Performance Evaluation},
volume = {24},
number = {1},
pages = {89-115},
year = {1995},
note = {Performance Modeling Tools},
issn = {0166-5316},
doi = {https://doi.org/10.1016/0166-5316(95)00012-M},
url = {https://www.sciencedirect.com/science/article/pii/016653169500012M},
author = {W.H. Sanders and W.D. Obal and M.A. Qureshi and F.K. Widjanarko},
keywords = {Stochastic Petri nets, Stochastic activity networks, Model-based evaluation, Performance, Dependability, Performability},
abstract = {Model-based evaluation of computer systems and networks is an increasingly important activity. For modeling to be used effectively, software environments are needed that ease model specification, construction, and solution. Easy to use, graphical methods for model specification that support solution of families of models with differing parameter values are also needed. Since no model solution technique is ideal for all situations, multiple analysis and simulation-based solution techniques should be supported. This paper describes UltraSAN, one such software environment. The design of UltraSAN reflects its two main purposes: to facilitate the evaluation of realistic computer systems and networks, and to provide a test-bed for investigating new modeling techniques. In UltraSAN, models are specified using stochastic activity networks, a stochastic variant of Petri nets, using a graphical X-Window based interface that supports large-scale model specification, construction, and solution. Models may be parameterized to reduce the effort required to solve families of models, and a variety of analysis and simulation-based solution techniques are supported. The package has a modular organization that makes it easy to add new construction and solution techniques as they become available. In addition to describing the features, capabilities, and organization of UltraSAN, the paper illustrates the use of the package in the solution for the unreliability of a fault-tolerant multiprocessor using two solution techniques.}
}
@incollection{JANJIC2002831,
title = { - The unit load method-some recent applications},
editor = {S.L. Chan and J.G. Teng and K.F. Chung},
booktitle = {Advances in Steel Structures (ICASS '02)},
publisher = {Elsevier},
address = {Oxford},
pages = {831-837},
year = {2002},
isbn = {978-0-08-044017-0},
doi = {https://doi.org/10.1016/B978-008044017-0/50097-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080440170500974},
author = {D. Janjic and M. Pircher and H. Pircher},
abstract = {Publisher Summary
The Unit Load Method is originally proposed as a procedure to optimize the tensioning process for the stay-cables in cable-stayed bridges and is implemented in a well-established bridge-design software package for this purpose. The implementation of this method takes into account all relevant effects for the design of cable stayed bridges including construction sequence, second order theory, large displacements, cable sag, and time-dependent effects, such as creep and shrinkage or cable relaxation. The underlying ideas of this method can also be applied to other optimization problems in structural engineering. This chapter gives an overview about the wide range of possible applications for which this method can be used, and it finishes with examples from practical experiences with the Unit Load Method. The Unit Load Method has been developed to achieve a pre-defined target configuration of section forces in cable-stayed bridges by optimizing the tensioning of the stay-cables. Recently, the method is developed further into a versatile design tool that allows the definition of a target distribution of section forces or deflections in any structure. Using the Unit Load Method, the necessary adjustments in a pre-determined set of constrains is computed to achieve exactly this distribution. The chapter briefly describes the method and gives three application examples where this method is used: a cable-stayed bridge, a concrete arch, and the application of this method to the automated simulation of the incremental launching process of bridges.}
}
@article{MARTINEZGARCIA2021102635,
title = {Assessment of mussel shells building solutions: A real-scale application},
journal = {Journal of Building Engineering},
volume = {44},
pages = {102635},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.102635},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221004939},
author = {Carolina Martínez-García and Belén González-Fonteboa and Diego Carro-López and Juan Luis Pérez-Ordóñez},
keywords = {Recycled mussel shells, Loose-fill insulation, Coating mortars, Wood structure, Energy efficiency, Thermal transmittance},
abstract = {The construction sector is a key generator of greenhouse emissions, so the use of alternative low-emission building materials is a growing tendency. This work describes and analyses an innovative sustainable building that includes mussel shells in all its constructive elements. This material is a by-product of the canning industry that is nowadays landfilled. Mussel shells were used as aggregate in the concrete strip footing (foundation) and in the exterior and interior coating mortars (walls), and as loose-fill material for the whole envelope insulation (floor, walls, and roof). The results from both the laboratory and the constructive process were useful to improve the solutions and to develop a building with low energy consumption. Finally, the energy demand of the building was assessed using the Passive House Planning Package (PHPP) software and the blower door test was carried out to measured air tightness. It can be concluded that mussel shell materials meet the requirements of Passive House standard for energy efficient buildings: simulation results showed a primary energy consumption of 86 kWh/(m2yr), that is a 28.3% lower than the value fixed by the standard.}
}
@article{KRASUSKI2018123,
title = {Improvement of AMF distribution in the inter-contact gap of VCB},
journal = {Electric Power Systems Research},
volume = {164},
pages = {123-138},
year = {2018},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2018.07.025},
url = {https://www.sciencedirect.com/science/article/pii/S0378779618302189},
author = {K. Krasuski and W. Krajewski and H. Sibilski},
keywords = {Axial magnetic field (AMF), Finite element method (FEM), Vacuum arc stabilisation, Vacuum circuit breaker (VCB)},
abstract = {Nowadays, many manufacturers of vacuum circuit breakers (VCB) apply the axial magnetic field (AMF) to stabilise the electric arc diffusion during current breaking process. In many papers, various designs of contacts are proposed to improve the AMF distribution between circuit breaker’s contacts. In the present paper, some methodological aspects of AMF investigations have been presented and discussed. In the beginning, a short explanation of AMF influence on arc behaviour has been presented. A simplified physical model of phenomena considered has been applied for this purpose. The above analysis indicates that special attention should be paid to reach possibly high value of the axial magnetic field density especially in the peripheral electrode region. An own laboratory stand for AMF measurements has been described. Measurement and computational results of AMF distribution have been compared. The computations have been done with a commercial software package Maxwell (Ansoft) based on the finite element method (FEM). Good convergence of the results compared indicates that AMF can be investigated in experimental as well as in theoretical way. The numerical analysis presented enabled to select the contact configuration which ensures more stable arc behaviour during the circuit breaking process. Images of arc behaviour registered with a high speed camera for a new contact construction as well as contacts known from industrial practice have been presented.}
}
@incollection{199155,
title = {9 - Major Vendors Plan Thrusts},
booktitle = {Introduction to Neural Networks (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Amsterdam},
pages = {55},
year = {1991},
isbn = {978-1-85617-120-5},
doi = {https://doi.org/10.1016/B978-1-85617-120-5.50014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9781856171205500140},
abstract = {Publisher Summary
This chapter provides an overview of the major corporations such as Apple, IBM, and Motorola, planning to enter the neural computer market. Apple, IBM, and Motorola among others plan to enter the neural computer market. Apple Computer Inc. is spinning off a neural network company, Natural Mind Systems, to exploit a unique architecture that automatically reconfigures its topology to suit the solution of a problem. The architecture utilizes any standard microprocessor to avoid the need of a coprocessor for speed. Natural Mind Systems hopes to automate the construction of a working neural-based application and plans to outperform other software simulators by combining RISC technology with its self-configuring topology. IBM makes its entry into the neural network software market with the Application System/400 Neural Network Utility, a software package that mimics human learning ability. The release makes IBM the first major US Corporation to offer neural network software, a move that is contrary to the firm's usual strategy of entering a market only after it has been well-established by smaller companies.}
}
@article{BERNOLD1990337,
title = {A prototype for intelligent computer integrated wood truss fabrication},
journal = {Robotics and Autonomous Systems},
volume = {6},
number = {4},
pages = {337-349},
year = {1990},
issn = {0921-8890},
doi = {https://doi.org/10.1016/S0921-8890(05)80015-6},
url = {https://www.sciencedirect.com/science/article/pii/S0921889005800156},
author = {Leonard E. Bernold and Eric E. Livingston},
keywords = {Construction, Automated process planning, Feature-based, Expert system, Knowledge base, Truss},
abstract = {This paper describes research efforts in which an expert process planning system, based on the Semi-intelligent Process Selector (SIPS) expert system shell, is linked with an industrial robot arm to perform fixed plant cutting and assembly operations (wood truss fabrication). The operational protocol of this system involves providing truss design input to the process planner via a file of truss feature parameter frames. This planner, which is resident on a TI Explorer workstation, generates the fabrication sequence and transfers this sequence to a robot control program which is resident on a separate microcomputer. This control program creates the required robot commands to produce robot movement. This paper provides a general discussion of knowledge-based expert systems and computer-aided process planning followed by the specifics of the development and operation of a SIPS based automated truss manufacturing system. This latter section includes an overview of the process planning shell utilized in this effort, and the structure of the knowledge bases developed. The makeup and function of the interpreter software package and the specifics of the major system hardware components are also discussed.}
}
@article{ALI2022114431,
title = {Shear capacity and behavior of high-strength concrete beams with openings},
journal = {Engineering Structures},
volume = {264},
pages = {114431},
year = {2022},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2022.114431},
url = {https://www.sciencedirect.com/science/article/pii/S0141029622005429},
author = {Sardar R. Mohammad Ali and Jalal A. Saeed},
keywords = {High strength concrete, Beams with openings, Shear strength, Finite element, Concrete damaged plasticity, ABAQUS, Strut and tie model, Stress concentration, Disturbed region, Strain distribution},
abstract = {Transverse openings alter the structural behavior and undermine the integrity of concrete beams, resulting in stiffness degradation, shear strength reduction, stress concentration, premature cracking, and force-flow disturbances. Consequently, they require special attention during the design and construction of concrete beams with openings. Owing to the abrupt change in the beam's cross-sectional area, the strain distribution in the region around an opening is disturbed and nonlinear, making conventional bending and shear theories irrelevant. Few studies have investigated the impact of openings on concrete beams, especially that of openings on high-strength concrete (HSC) beams. Therefore, in this study, we perform tests on 12 reinforced concrete beam specimens to investigate the influence of depth, length, and location of openings on the ultimate shear strength of HSC beams with openings. The results revealed that the opening depth and length of HSC beams are negatively and almost linearly correlated to the ultimate shear strength; furthermore, based on their size and location, the shear capacity reduces by 2–53%. Similarly, the opening location along the shear span had a substantial impact on the capacity and behavior of the beam. Subsequently, finite element models of the tested beams were developed in ABAQUS software package and validated in compliance with the experimental results. The simulations exhibited good performance in predicting the ultimate strength, load–deflection relationship, and crack propagation in the beams. Accordingly, the developed models can be utilized to conduct further parametric studies and develop strut-and-tie models.}
}
@article{KANTHASAMY2022113366,
title = {Shear behaviour of doubly symmetric rectangular hollow flange beam with circular edge-stiffened openings},
journal = {Engineering Structures},
volume = {250},
pages = {113366},
year = {2022},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2021.113366},
url = {https://www.sciencedirect.com/science/article/pii/S0141029621014784},
author = {Elilarasi Kanthasamy and Kajaharan Thirunavukkarasu and Keerthan Poologanathan and Perampalam Gatheeshgar and Shaun Todhunter and Thadshajini Suntharalingam and Muhammadh {Fareedh Muhammadh Ishqy}},
keywords = {Cold-formed steel, Doubly symmetric rectangular hollow flange beam, Un-stiffened openings, Edge-stiffeners, Numerical modelling, Direct strength method},
abstract = {Cold-Formed Steel (CFS) sections are evolved through the last two decades compromising various aspects of construction needs. Doubly symmetric Rectangular Hollow Flange Beam (RHFB) is one of the innovative CFS sections, which was introduced to eliminate the drawbacks of conventional open CFS sections such as prone to complex buckling and torsional effects. Edge-stiffened web holes are recently recommended and available for extensive usage in the floor beams. Even though previous studies conducted researches on various kinds of stiffeners, there is no definite design equation or findings that were presented on shear behaviour. Therefore, this study focuses on the effect of edge-stiffened circular web openings on the shear capacity of doubly symmetric RHFB. Non-linear numerical models were developed using the ABAQUS software package for validation purposes and then comprehensive parametric studies were carried out for doubly symmetric RHFB with edge-stiffened openings. A total of 558 models consisting of edge-stiffeners, un-stiffened openings and plain webs were analysed in this study. Parametric results of with edge stiffeners exhibited shear capacity increment (1–90%) compared to unstiffened web openings. Hence, new design equations were proposed in the form of reduction factor and based on Direct Strength Method (DSM). Finally, optimum edge-stiffener length of 15 mm was recommended regardless of web opening size.}
}