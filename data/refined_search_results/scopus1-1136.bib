
@ARTICLE{Sidorin2023,
author={Sidorin, A.E. and Tikhonov, S.A. and Samoilov, I.S. and Osmushko, I.S. and Svistunova, I.V. and Tretyakova, G.O. and Puzyr’kov, Z.N. and Vovna, V.I.},
title={Electronic structure, cationic, and excited states of nitrogen-containing spiroborates},
journal={Journal of Molecular Modeling},
year={2023},
volume={29},
number={3},
doi={10.1007/s00894-023-05465-z},
art_number={69},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148259376&doi=10.1007%2fs00894-023-05465-z&partnerID=40&md5=aafd4182f56f06245741d451b6c5ab0c},
abstract={Context: This paper presents the results of the study of the electronic structure and cationic and excited states of three spiroborate complexes (2-acetylacetonato-1,3,2-benzodioxaborol, its NH- and NMe-derivatives) and three corresponding ligands (acetylacetone, 4-aminopent-3-en-2-one, and 4-methylaminopent-3-en-2-one). Materials based on spiroborates are used in medicine, for example, as a drug carrier. In industry, spiroborate anions are used in ionic liquids and as alternative high performance lubricants. Analysis of experimental and calculated data allowed determining the influence of functional groups on the parameters of the electronic structure and energy of electronic transitions. Compared to acetylacetone and its NH- and NMe-derivatives, the upper filled molecular orbitals of the corresponding spiroborates are stabilized at 0.4–1.7 eV, which is due to the positive charge of the ligand due to the acceptor properties of the dioxyphenylene fragment. Among the studied compounds, when replacing the oxygen atom in the α-position with the NH- or NMe-group, a bathochromic shift of intense bands in the absorption spectra is observed, since the energy intervals between the orbitals of the π3 and π4 ligand are reduced. In addition, in a number of spiroborates, the violation of C2v symmetry when replacing an oxygen atom leads to the appearance of a low-intensity maximum in the long-wave part of the absorption spectrum, due to the π2X → π4 transition. Method: Complexes were studied by photoelectron spectroscopy, absorption spectroscopy, and high-level ab initio quantum chemical computations, including the algebraic diagrammatic construction method for the polarization propagator of the second order (ADC(2)), the outer-valence Green’s function (OVGF), the density functional theory (DFT), the time-dependent density functional theory (TDDFT) and the domain-based local pair natural orbital (EOM-DLPNO) methods. X-ray photoelectronic spectra of two spiroborates in the condensed state were measured using a two-chamber high-vacuum system MXPS XP (Omicron, Germany). UV–visible absorption spectra were recorded using a spectrophotometer 2550 (Shimadzu-UV, Japan). The geometry of all studied compounds was optimized by the DFT/B3LYP/Def2-SVP method. The energy of electron levels in the S0 state and the distribution of electron density at each MO were obtained by the DFT/CAMB3LYP/cc-pVDZ method. The energies of excited states were obtained by the TDDFT/CAMB3LYP/cc-pVDZ, ADC(2)/cc-pVDZ and EOM-DLPNO/cc-pVDZ methods. All DFT and TDDFT calculations were carried out in the GAMESS (US) software computing package. ADC(2) calculations of excited states were performed using the Orca 4.0.1 software package. EOM-DLPNO and OVGF calculations were carried out in the Gaussian 16 software package. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={ADC(2);  Density functional theory;  Electronic structure;  Outer-valence Green’s function (OVGF) method;  Photoelectron spectroscopy;  Spiroborate complexes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Martínez-Estévez2023,
author={Martínez-Estévez, I. and Domínguez, J.M. and Tagliafierro, B. and Canelas, R.B. and García-Feal, O. and Crespo, A.J.C. and Gómez-Gesteira, M.},
title={Coupling of an SPH-based solver with a multiphysics library},
journal={Computer Physics Communications},
year={2023},
volume={283},
doi={10.1016/j.cpc.2022.108581},
art_number={108581},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141464387&doi=10.1016%2fj.cpc.2022.108581&partnerID=40&md5=2cb408dd4edfa96c492a41fe7314253c},
abstract={A two-way coupling between the Smoothed Particle Hydrodynamics-based (SPH) code with a multiphysics library to solve complex fluid-solid interaction problems is proposed. This work provides full access to the package for the use of this coupling by releasing the source code, completed with guidelines for its compilation and utilization, and self-contained template setups for practical uses of the novel implemented features, is provided here. The presented coupling expands the applicability of two different solvers allowing to simulate fluids, multibody systems, collisions with frictional contacts using either non-smooth contact (NSC) or smooth contact (SMC) methods, all integrated under the same framework. The fluid solver is the open-source code DualSPHysics, highly optimised for simulating free-surface phenomena and structure interactions, uniquely positioned as a general-purpose Computational Fluid Dynamics (CFD) software with a GPU-accelerated solver. Mechanical systems that comprise collision detection and/or multibody dynamics are solved by the multiphysics library Project Chrono, which uses a Discrete Element Method (DEM). Therefore, this SPH-DEM coupling approach can manage interactions between fluid and complex multibody systems with relative constraints, springs, or mechanical joints. Program summary: Program title: DualSPHysics-Chrono CPC Library link to program files: https://doi.org/10.17632/g2cc37dw4f.1 Licensing provisions: DualSPHysics and DSPHChronoLib under GNU Lesser General Public License (LGPL); Project Chrono under BSD-3-Clause License. Programming language: C++ and CUDA Nature of problem: The simulation of turbulent free-surface flows in interaction with complex fixed or floating structures is essential to address typical marine and coastal engineering problems. The Smoothed Particle Hydrodynamics (SPH) method is particularly suitable for solving this type of nonlinear problems. However, this type of application usually requires mechanical restrictions between the different structural elements (spherical joints, hinges, or springs), as well as the correct simulation of collisions between solid objects. In these cases, it is necessary to combine the SPH method with other numerical methods that allow performing these multiphysics simulations. Solution method: DualSPHysics-Chrono is a two-way coupling between an SPH solver and a multiphysics library that combines fluid simulation using a Lagrangian approximation and interactions between solids using Discrete Element Method (DEM). DualSPHysics solver is a GPU-optimized implementation of the SPH method that allows efficient simulation of fluid-structure interaction problems. Whereas Project Chrono is a DEM implementation for simulating multibody dynamics that includes collision detection and numerous mechanical constraints to solve complex mechanisms. The coupling of DualSPHysics and Project Chrono combines the capabilities of both models under a free software framework. Additional comments including restrictions and unusual features: The SPH solver includes a version implemented with CUDA (Compute Unified Device Architecture) to exploit the parallelism of NVIDIA graphics processing units (GPUs). © 2022 The Authors},
author_keywords={DEM;  DualSPHysics;  Multiphysics coupling;  Project Chrono;  SPH},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lapina2023213,
author={Lapina, E. and Oumaziz, P. and Bouclier, R. and Passieux, J.-C.},
title={A fully non-invasive hybrid IGA/FEM scheme for the analysis of localized non-linear phenomena},
journal={Computational Mechanics},
year={2023},
volume={71},
number={2},
pages={213-235},
doi={10.1007/s00466-022-02234-2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140296436&doi=10.1007%2fs00466-022-02234-2&partnerID=40&md5=dddc179fb3d52663d460057eb94c7b6d},
abstract={This work undertakes to combine the interests of IsoGeometric Analysis (IGA) and standard Finite Element Methods (FEM) for the global/local simulation of structures. The idea is to adopt a hybrid global-IGA/local-FEM modeling, thereby benefiting from: (i) the superior geometric description and per-Degree-Of-Freedom accuracy of IGA for capturing global, regular responses, and (ii) the ability of FEM to compute local, strongly non-linear or even singular behaviors. For the sake of minimizing the implementation effort, we develop a coupling scheme that is fully non-invasive in the sense that the initial global spline model to be enriched is never modified and the construction of the coupling operators can be performed using conventional FE packages. The key ingredient is to express the FEM-to-IGA bridge, based on Bézier extraction, to transform the initial global spline interface into a FE one on which the local FE mesh can be constructed. This allows to resort to classic FE trace operators to implement the coupling. It results in a strategy that offers the opportunity to simply couple an isogeometric code with any robust FE code suitable for the modelling of complex local behaviors. The method also easily extends in case the users only have at their disposal FE codes. This is the situation that is considered for the numerical illustrations. More precisely, we only make use of the FE industrial software Code_Aster to perform efficiently and accurately the hybrid global-IGA/local-FEM simulation of structures subjected locally to cracks, contact, friction and delamination. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Bézier extraction;  Industrial FE code;  Isogeometric Analysis;  Multiscale;  Non-invasive global/local coupling},
document_type={Article},
source={Scopus},
}

@ARTICLE{Amato2023318,
author={Amato, D. and Lo Bosco, G. and Giancarlo, R.},
title={Standard versus uniform binary search and their variants in learned static indexing: The case of the searching on sorted data benchmarking software platform},
journal={Software - Practice and Experience},
year={2023},
volume={53},
number={2},
pages={318-346},
doi={10.1002/spe.3150},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137798738&doi=10.1002%2fspe.3150&partnerID=40&md5=928489a7c1289d617aad052b4f22d7b2},
abstract={Learned Indexes use a model to restrict the search of a sorted table to a smaller interval. Typically, a final binary search is done using the lower_bound routine of the Standard C++ library. Recent studies have shown that on current processors other search approaches (such as k-ary search) can be more efficient in some applications. Using the SOSD learned indexing benchmarking software, we extend these results to show that k-ary search is indeed a better choice when using learned indexes. We highlight how such a choice may be dependent on the computer architecture used, for example, Intel I7 or Apple M1, and provide guidelines for the selection of the Search routine within the learned indexing framework. © 2022 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.},
author_keywords={algorithms with prediction;  binary search variants;  learned index structures;  search on sorted data platform},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hu2023,
author={Hu, M. and Zhang, Y.},
title={An empirical study of the Python/C API on evolution and bug patterns},
journal={Journal of Software: Evolution and Process},
year={2023},
volume={35},
number={2},
doi={10.1002/smr.2507},
art_number={e2507},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137771541&doi=10.1002%2fsmr.2507&partnerID=40&md5=f1a4fc0d31ae04f3eb0c61f6091f3c22},
abstract={Python is a popular programming language, and a large part of its appeal comes from diverse libraries and extension modules. In the bloom of data science and machine learning, Python frontend with C/C++ native implementation achieves both productivity and performance and has almost become the standard structure for many mainstream software systems. However, feature discrepancies between two languages such as exception handling, memory management, and type system can pose many safety hazards in the interface layer using the Python/C API. In this paper, we carry out an empirical study of the Python/C API on evolution and bug patterns. The evolution analysis includes Python/C API design in CPython compilers and its usage in mainstream software. By designing and applying a static analysis toolset, we reveal the evolution and usage statistics of the Python/C API and provide a summary and classification of 9 common bug patterns. In Pillow, a widely used Python imaging library, we find 48 bugs, 19 of which are undiscovered before. Our toolset can be easily extended to access different types of syntactic bug-finding checkers, and our systematical taxonomy to classify bugs can guide the construction of more highly automated and high-precision bug-finding tools. © 2022 John Wiley & Sons Ltd.},
author_keywords={bug patterns;  evolution analysis;  fact extraction;  Python/C API;  static analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Akhukov2023,
author={Akhukov, M.A. and Chorkov, V.A. and Gavrilov, A.A. and Guseva, D.V. and Khalatur, P.G. and Khokhlov, A.R. and Kniznik, A.A. and Komarov, P.V. and Okun, M.V. and Potapkin, B.V. and Yu. Rudyak, V. and Shirabaykin, D.B. and Skomorokhov, A.S. and Trepalin, S.V.},
title={MULTICOMP package for multilevel simulation of polymer nanocomposites},
journal={Computational Materials Science},
year={2023},
volume={216},
doi={10.1016/j.commatsci.2022.111832},
art_number={111832},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139298308&doi=10.1016%2fj.commatsci.2022.111832&partnerID=40&md5=89501d7f9bbcb49eb60b57e3e5b8a9c9},
abstract={We present a MULTICOMP package developed for multiscale modeling of polymer-based nanomaterials. The package implements GUI-based tools for the automatic construction of various types of polymer systems with the transition from the atomistic level of modeling to the mesolevel and from the mesolevel to the macrolevel. Using automated scripts, researchers can construct different kinds of nanomaterials, create flexible simulation schemas and define how data are transferred between engaged software modules and tools. The package makes it possible to analyze structural, thermophysical, mechanical properties, the cohesive energy density, the viscosity of polymer melts, and the diffusion of small molecules. Due to the client–server organization, the package can perform calculations on local and remote computing facilities and transfer the most time-consuming calculations to supercomputers. Examples of preparation and characterization of a cross-linked polymer matrix, a clay-based nanocomposite, and analyzing their properties illustrate MULTICOMP package operation. © 2022 Elsevier B.V.},
author_keywords={Computer design;  Multiscale simulations;  Nanotechnology;  Polymer nanocomposites;  Software package},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2023622,
author={Liu, X. and Zhang, C.},
title={Controlling and simulation system for hydraulic valve testing based on Qt},
journal={International Conference on Information Networking},
year={2023},
volume={2023-January},
pages={622-625},
doi={10.1109/ICOIN56518.2023.10049037},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186699&doi=10.1109%2fICOIN56518.2023.10049037&partnerID=40&md5=cc0b3250ade48f85bbe15baa0c7851d7},
abstract={Hydraulic valve is not only an important part in construction machinery but also widely used in modern industry area. In Hydraulic valve's research and design process, it needs large amount of testing experiments to verify the accuracy of design. The design in this paper is a real-time controlling and simulation system for hydraulic valve testing experiments based on Qt platform. In this design, the controlling parameters would be preset. Then the stimulation waves and testing data are plotted and outputted based on the feedback information from lower position machine. This system creates the sub-tasks unites and different classes based on the requirement of testing experiment, followed by plotting the software interface with OpenGL. © 2023 IEEE.},
author_keywords={controlling system;  hydraulic valve testing;  Qt;  simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhu2023,
author={Zhu, S. and Zhou, X. and Xu, S. and Chen, E. and Ye, Y. and Yan, Q. and Guo, T.},
title={Design of Metasurface Skin Cloak Based on A Low-complexity Monitoring Model and Deep Learning},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2023},
volume={12315},
doi={10.1117/12.2642101},
art_number={1231515},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147500079&doi=10.1117%2f12.2642101&partnerID=40&md5=69093a32556e5aac9d83f40a873d520c},
abstract={This work proposes a research scheme to speed up the design of metasurface skin cloak through low-complexity phase monitoring model and deep learning.This skin cloak conceals a three-dimensional arbitrarily shaped object by complete restoration of the phase of the reflected light at specific wavelength. And the possibility of realizing spectral prediction by deep learning is analyzed.During the study, a phase monitoring system was designed in which the detector, the light source and the monitored nano-antenna were sequentially distributed at equal distances from the emitted wavelength of the light source, so that the monitored phase amount was exactly equal to the phase change before the reflected wave, thus eliminating the need for multiple monitors to measure and calculate the phase change before and after the reflection.The traditional metasurface design is usually constructed by manual library construction based on the phase distribution and the relationship between phase variation and dimensional variation of the cell structure, so this work combines the aforementioned monitoring model with deep learning to generate the database required for modeling.The two variable parameters of device length and width were first defined, and the reflected wavefront phase change used as the optical response, and we reprocessed the original data and finally build and trained an artificial neural network model for forward prediction of optical response.This network can obtain its MSE below 0.001 for the test set after the training is completed. Thus the scheme can replace the role of simulation software to some extent, and its prediction process can be completed in a few milliseconds, improving the efficiency of the design metasurface process. © 2022 SPIE.},
author_keywords={ANN;  Deep learning;  Metasurface design;  Monitoring model;  Phase detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kumar2023531,
author={Kumar, N.S. and Noorani, A.A. and Anjali, K. and Supriya, R.},
title={Compile Time Analyzer to Detect Violations of Programming Guidelines},
journal={Lecture Notes in Networks and Systems},
year={2023},
volume={516},
pages={531-542},
doi={10.1007/978-981-19-5221-0_52},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144487437&doi=10.1007%2f978-981-19-5221-0_52&partnerID=40&md5=0ce3cf7db2e5dba335ec5238559e875f},
abstract={The maintenance of software is the biggest challenge in the software industry today. The quality of code affects maintainability of software which can be improved by following well-defined programming guidelines. In academia as well, we are required to check the quality of students’ code and if the students have used the expected approach. We propose a method to find violations of guidelines like length of identifiers, use of global variables and depth of control structures. We also intend to check if a particular programming approach has been used such as dynamic programming or recursion. This paper discusses a method through which the given code can be analyzed statically to detect for the violations of selected guidelines. We have a library of guidelines to choose from and provide a novel mechanism for the user to specify a new guideline in a structured form developed by us. This in turn results in the generation of code which performs the checks. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Code quality;  Programming education;  Programming guidelines;  Static analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2023211,
author={Zhang, S. and Guo, X.-W. and Li, C. and Liu, Y. and Fan, S. and Zhao, R. and Yang, C.},
title={A large scale parallel fluid-structure interaction computing platform for simulating structural responses to a detonation shock},
journal={Software - Practice and Experience},
year={2023},
volume={53},
number={1},
pages={211-240},
doi={10.1002/spe.3051},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118621523&doi=10.1002%2fspe.3051&partnerID=40&md5=d5d5626cb44e04e61c3033cf591d4f6c},
abstract={Due to the intrinsic nature of multi-physics, it is prohibitively complex to design and implement a simulation software platform for study of structural responses to a detonation shock. In this article, a partitioned fluid-structure interaction computing platform is designed for parallel simulating structural responses to a detonation shock. The detonation and wave propagation are modeled in an open-source multi-component solver based on OpenFOAM and blastFoam, and the structural responses are simulated through the finite element library deal.II. To capture the interaction dynamics between the fluid and the structure, both solvers are adapted to preCICE. For improving the parallel performance of the computing platform, the inter-solver data is exchanged by peer-to-peer communications and the intermediate server in conventional multi-physics software is eliminated. Furthermore, the coupled solver with detonation support has been deployed on a computing cluster after considering the distributed data storage and load-balancing between solvers. The 3D numerical result of structural responses to a detonation shock is presented and analyzed. On 256 processor cores, the speedup ratio of the simulations for a detonation shock reach 178.0 with 5.1 million of mesh cells and the parallel efficiency achieve 69.5%. The results demonstrate good potential of massively parallel simulations. Overall, a general-purpose fluid-structure interaction software platform with detonation support is proposed by integrating open source codes. And this work has important practical significance for engineering application in fields of construction blasting, mining, and so forth. © 2021 John Wiley & Sons, Ltd.},
author_keywords={FSI;  high-explosive detonation;  HPC;  numerical simulation;  OpenFOAM;  preCICE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mehmood202277,
author={Mehmood, H.U. and Ujang, U. and Azri, S. and Choon, T.L.},
title={CONCEPTUAL DOMAIN MODEL FOR MAINTENANCE MANAGEMENT OF HIGH-RISE RESIDENTIAL STRATA},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2022},
volume={48},
number={4/W3-2022},
pages={77-82},
doi={10.5194/isprs-archives-XLVIII-4-W3-2022-77-2022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144352823&doi=10.5194%2fisprs-archives-XLVIII-4-W3-2022-77-2022&partnerID=40&md5=71d0434341d58448cbc83cf54af4992b},
abstract={Due to urban regions' lack of land for housing and infrastructure development, multi-level (vertical) constructions have developed rapidly. Massive developments and integration of high-rise buildings and structures, such as multi-story houses and apartments, are now prevalent in Malaysia. When it comes to the housing industry, preventive maintenance is critical because it helps individuals maintain their daily routines while also enhancing the productivity of activities in and around their residences. Therefore, the primary focus of this research is to review the temporal strata maintenance issues, build a conceptual domain model for maintenance procedures of residential strata in Malaysia, and link it with existing Land Administration Domain Model (LADM) packages. Furthermore, a Unified Modeling Language (UML) model for this LADM extension was created and is presented here on a conceptual level. This conceptual domain model for maintaining high-rise residential strata is designed by identifying the actors, classes, and their relationships derived from literature review, then created in the Enterprise Architect software. This conceptual domain model can further be used to develop maintenance management databases and software, which are nowadays essential for effective strata maintenance management. © 2022 International Society for Photogrammetry and Remote Sensing. All rights reserved.},
author_keywords={Domain Model;  Enterprise Architect;  LADM;  Maintenance;  Strata Management;  Temporal Management},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{deHond2022,
author={de Hond, A.A.H. and Leeuwenberg, A.M. and Hooft, L. and Kant, I.M.J. and Nijman, S.W.J. and van Os, H.J.A. and Aardoom, J.J. and Debray, T.P.A. and Schuit, E. and van Smeden, M. and Reitsma, J.B. and Steyerberg, E.W. and Chavannes, N.H. and Moons, K.G.M.},
title={Guidelines and quality criteria for artificial intelligence-based prediction models in healthcare: a scoping review},
journal={npj Digital Medicine},
year={2022},
volume={5},
number={1},
doi={10.1038/s41746-021-00549-7},
art_number={2},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122738619&doi=10.1038%2fs41746-021-00549-7&partnerID=40&md5=0d7d18844b4bdc7089ecaf87c2457fb6},
abstract={While the opportunities of ML and AI in healthcare are promising, the growth of complex data-driven prediction models requires careful quality and applicability assessment before they are applied and disseminated in daily practice. This scoping review aimed to identify actionable guidance for those closely involved in AI-based prediction model (AIPM) development, evaluation and implementation including software engineers, data scientists, and healthcare professionals and to identify potential gaps in this guidance. We performed a scoping review of the relevant literature providing guidance or quality criteria regarding the development, evaluation, and implementation of AIPMs using a comprehensive multi-stage screening strategy. PubMed, Web of Science, and the ACM Digital Library were searched, and AI experts were consulted. Topics were extracted from the identified literature and summarized across the six phases at the core of this review: (1) data preparation, (2) AIPM development, (3) AIPM validation, (4) software development, (5) AIPM impact assessment, and (6) AIPM implementation into daily healthcare practice. From 2683 unique hits, 72 relevant guidance documents were identified. Substantial guidance was found for data preparation, AIPM development and AIPM validation (phases 1–3), while later phases clearly have received less attention (software development, impact assessment and implementation) in the scientific literature. The six phases of the AIPM development, evaluation and implementation cycle provide a framework for responsible introduction of AI-based prediction models in healthcare. Additional domain and technology specific research may be necessary and more practical experience with implementing AIPMs is needed to support further guidance. © 2022, The Author(s).},
document_type={Review},
source={Scopus},
}

@ARTICLE{Ali20228825,
author={Ali, M. and Berrendorf, M. and Hoyt, C.T. and Vermue, L. and Galkin, M. and Sharifzadeh, S. and Fischer, A. and Tresp, V. and Lehmann, J.},
title={Bringing Light into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models under a Unified Framework},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2022},
volume={44},
number={12},
pages={8825-8845},
doi={10.1109/TPAMI.2021.3124805},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118619402&doi=10.1109%2fTPAMI.2021.3124805&partnerID=40&md5=229ddfe44a90c5df8681805513a5259b},
abstract={The heterogeneity in recently published knowledge graph embedding models' implementations, training, and evaluation has made fair and thorough comparisons difficult. To assess the reproducibility of previously published results, we re-implemented and evaluated 21 models in the PyKEEN software package. In this paper, we outline which results could be reproduced with their reported hyper-parameters, which could only be reproduced with alternate hyper-parameters, and which could not be reproduced at all, as well as provide insight as to why this might be the case. We then performed a large-scale benchmarking on four datasets with several thousands of experiments and 24,804 GPU hours of computation time. We present insights gained as to best practices, best configurations for each model, and where improvements could be made over previously published best configurations. Our results highlight that the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations is crucial for a model's performance and is not only determined by its architecture. We provide evidence that several architectures can obtain results competitive to the state of the art when configured carefully. We have made all code, experimental configurations, results, and analyses available at https://github.com/pykeen/pykeen and https://github.com/pykeen/benchmarking. © 1979-2012 IEEE.},
author_keywords={benchmarking;  Knowledge graph embeddings;  link prediction;  reproducibility},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khan2022,
author={Khan, A.W. and Khan, S.U. and Alwageed, H.S. and Khan, F. and Khan, J. and Lee, Y.},
title={AHP-Based Systematic Approach to Analyzing and Evaluating Critical Success Factors and Practices for Component-Based Outsourcing Software Development},
journal={Mathematics},
year={2022},
volume={10},
number={21},
doi={10.3390/math10213982},
art_number={3982},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141878034&doi=10.3390%2fmath10213982&partnerID=40&md5=a88d8d196b03dc1b263ca135a72ec505},
abstract={Component-based software development (CBSD) is a difficult method for creating complicated products or systems. In CBSD, multiple components are used to construct software or a product. A complex system or program can be created with CBSD quickly and with money while maintaining excellent quality and security. On the other hand, this research will persuade outsourced vendor companies to embrace CBSD approaches for component software development. We conducted a systemic literature review (SLR) to investigate the success factors that have a favorable impact on software outsourcing vendors’ organizations, and we selected 91 relevant research publications by creating a search string based on the study questions. This useful information was compiled using Google Scholar, IEEE Explore, MDPI, WILLEY Digital Library, and Elsevier. Furthermore, we completed all of the procedures in SLR for the full literature review, including the formulation of the SLR protocol, initial and final data collection, retrieval, assessment processes, and data synthesis. Among the ten (10) critical success factors we identified are a well-trained and skilled team, proper component selection, use of design standards, well-defined architecture, well-defined analysis and testing, well-defined integration, quality assurance, good organization of documentation, and well-organized security, and proper certification. Furthermore, the proposed SLR includes 46 best practices for these critical success factors, which could assist vendor organizations in enhancing critical success factors for CBOSD. According to our findings, the discovered success factors are similar and distinct across different periods, continents, databases, and approaches. The recommended SLR will also assist software vendor organizations in implementing the CBSD idea. We used the analytical hierarchy process (AHP) method to prioritize and analyze the success factors of component-based outsourcing software development and the result of different equations of the AHP approach to construct the pairwise comparison matrix. The largest eigenvalue was 3.096 and the CR value was 0.082, which is less than 0.1, and thus sufficient and acceptable. © 2022 by the authors.},
author_keywords={CBSD;  component based outsourcing;  practices;  SLR;  software development;  SPSS;  success factors;  vendor},
document_type={Article},
source={Scopus},
}

@ARTICLE{ErazoRamirez2022,
author={Erazo Ramirez, C. and Sermet, Y. and Molkenthin, F. and Demir, I.},
title={HydroLang: An open-source web-based programming framework for hydrological sciences},
journal={Environmental Modelling and Software},
year={2022},
volume={157},
doi={10.1016/j.envsoft.2022.105525},
art_number={105525},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138115144&doi=10.1016%2fj.envsoft.2022.105525&partnerID=40&md5=a2516724d1ef29deda6f9e14f74618c8},
abstract={This paper introduces HydroLang, an open-source and integrated community-driven computational web framework for hydrology and water resources research and education. HydroLang employs client-side web technologies and standards to carry out various routines aimed at acquiring, managing, transforming, analyzing, and visualizing hydrological datasets. HydroLang consists of four major high-cohesion low-coupling modules: (1) retrieving, manipulating, and transforming raw hydrological data, (2) statistical operations, hydrological analysis, and model creation, (3) generating graphical and tabular data representations, and (4) mapping and geospatial data visualization. To demonstrate the framework's capabilities, portability, and interoperability, two detailed case studies (assessment of lumped models and construction of a rainfall disaggregation model) have been presented. HydroLang's unique modular architecture and open-source nature allow it to be easily tailored into any use case and web framework, and it encourages iterative enhancements with community involvement to establish the comprehensive next-generation hydrological software toolkit. © 2022 Elsevier Ltd},
author_keywords={Hydrological analysis;  Neural networks;  Scientific visualization;  Software libraries;  Web frameworks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cloosters202230,
author={Cloosters, T. and Paaßen, D. and Wang, J. and Draissi, O. and Jauernig, P. and Stapf, E. and Davi, L. and Sadeghi, A.-R.},
title={RiscyROP: Automated Return-Oriented Programming Attacks on RISC-V and ARM64},
journal={ACM International Conference Proceeding Series},
year={2022},
pages={30-42},
doi={10.1145/3545948.3545997},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142506394&doi=10.1145%2f3545948.3545997&partnerID=40&md5=dd727fe069ff9b7dd6956a8e9b1621ac},
abstract={Return-oriented programming (ROP) is a powerful run-time exploitation technique to attack vulnerable software. Modern RISC architectures like RISC-V and ARM64 pose new challenges for ROP execution due to the lack of a stack-based return instruction and strict instruction alignment. Further, the large number of caller-saved argument registers significantly reduces the gadget space available to the attacker. Consequently, existing ROP gadget tools for other processor architectures cannot be applied to these RISC architectures. Previous work on RISC-V provides only manual construction of ROP attacks against specially crafted programs, and no analysis of ROP attacks has been conducted for ARM64 yet. In this paper, we address these challenges and present RiscyROP, the first automated ROP gadget finding and chaining toolkit for RISC-V and ARM64. RiscyROP analyzes available gadgets utilizing symbolic execution, and automatically generates complex multi-stage chains to conduct arbitrary function calls. Our approach enables the first investigation of the gadget space on RISC-V and ARM64 real-world binaries. RiscyROP successfully builds ROP chains that enable an attacker to execute arbitrary function calls for the nginx web server as well as any binary that contains the libc library. © 2022 ACM.},
author_keywords={ARM64;  Exploitation;  Return-Oriented Programming;  RISC-V;  Symbolic Execution},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jablonski2022,
author={Jablonski, A.},
title={Multiple elastic scattering of electrons in condensed matter (new version announcement)},
journal={Computer Physics Communications},
year={2022},
volume={278},
doi={10.1016/j.cpc.2022.108402},
art_number={108402},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129990202&doi=10.1016%2fj.cpc.2022.108402&partnerID=40&md5=301c10ec82d1e84f9429fc97543fadbc},
abstract={The software MULTI_SCATT for calculations of parameters describing an electron transport in condensed matter in Jablonski (2017) [1] is replaced with a new program MULTI_SCATT_v2 which has the following improvements: (i) extension of electron energies to the range from 50 eV to 20 keV; (ii) detailed analysis of accuracy of elastic scattering cross sections from the implemented formalism; (iii) a new option for calculating the inelastic mean free path for compounds from elastic backscattering probabilities. The energy extension to 20 keV was stimulated by the current interest in applications of hard X-ray photoelectron spectroscopy (HAXPES). New version program summary: Program title: MULTI_SCATT_v2 CPC Library link to program files: https://doi.org/10.17632/cvt9yz9gj8.2 Licensing provisions: GNU General Public License 3 Programming language: Fortran 90 Journal reference of previous version: Comput. Phys. Commun. 210 (2017) 92–102. Does the new version supersede the previous version?: Yes Reasons for the new version: (1) We currently observe a growing interest in analytical applications of photoelectron spectroscopies that involve photoelectron signal intensities with energy exceeding 1500 eV. These applications are designated by the acronym HAXPES (Hard X-ray Photoelectron Spectroscopy). In the previously published software MULTI_SCATT, the upper energy limit was set at 5000 eV [1]. However, applications of HAXPES may require parameterization of electron transport for higher energies, frequently exceeding 10 keV. It was decided to extend the upper energy limit of the present version of the program MULTI_SCATT up to 20 keV. (2) The inelastic mean free path of electrons (IMFP) is an important parameter describing the electron transport in condensed matter. The software MULTI_SCATT made possible to calculate the so-called calibration curves (dependence of the elastic backscattering probabilities on assumed value of the IMFP) for elemental solids to facilitate determination of the IMFPs [2]. Presently, another option is added that can be used for determination of IMFPs for samples of more practical interest: alloys and compounds. In addition to calibration curves, the IMFPs are calculated from experimental data obtained from elastic peak electron spectroscopy (EPES). (3) Recently, new sources of parameters were published that are needed for calculations implemented in the original program MULTI_SCATT, e.g., the differential elastic scattering cross sections (DCS) [3] and the IMFPs for elemental solids [4]. Consequently, the databases accompanying the new version of the program MULTI_SCATT are correspondingly updated. Summary of revisions: The elastic interactions between an electron and atoms are well known to affect the electron trajectories in a solid. Consequently, the relevant corrections must be taken into account in quantitative applications of electron spectroscopies [5]. Important parameters needed in the formalism of quantitative analysis are the elastic scattering cross sections (DCS), dσel/dΩ. These parameters can be obtained from the program MULTI_SCATT in which they are calculated from the expansion of the DCS in a series of Legendre polynomials [1,2]. In the algorithm implemented there, advantage was taken of the analytical relation between the expansion coefficients and the relativistic phase shifts corresponding to a given element and electron energy. The upper energy limit was set then at 5000 eV. However, in recent years, new constructions of the laboratory X-ray sources used for ejection of photoelectrons are emitting the monochromatic radiation with energy reaching 10 keV. Higher X-ray energies are available in synchrotron facilities. Thus, it was decided to raise the upper energy limit in current version of MULTI_SCATT_v2 to 20 keV. Another parameter of importance in quantitative electron spectroscopies is the electron inelastic mean free path (IMFP) of signal electrons. These parameters can be determined from elastic peak electron spectroscopy (EPES) that is based on measured probabilities of elastic electron backscattering from solid surfaces. The relevant computational procedure was implemented in the program MULTI_SCATT [1] which provided the theoretically determined relation between IMFPs and the experimental elastic-backscattering intensities measured for elemental solids. In experimental practice, majority of samples of analytical interest are compounds or alloys. Consequently, an additional option is added in the program MULTI_SCATT_v2 that calculates the calibration curve for these solids. Furthermore, if the experimentally determined signal intensities of elastically backscattered electrons are available, also the IMFP values are calculated and compared with the IMFPs from the so-called predictive formula. The theoretical background and the outline of relevant case studies are available in the Supplementary Information included in the distribution package. Nature of problem: Main numerical problems were encountered during calculations of the database of Al coefficients for a dense grid of energies. The increase of the upper energy to 20 keV required a large number of these coefficients in the range of high energies. For example, the series of coefficients Al up to l=1800 was determined for cesium at 20 keV. Consequently, calculations of a large number of phase shifts was needed. The volume of the directory ALs with all the coefficients Al increased from 96.2 MB in the first version of MULTI_SCATT [1] to 195 MB. It was a major computational effort; however, these calculations need to be made only once. Solution method: The Al coefficients were calculated from the series of the relativistic phase shifts determined for a given element and electron energy. This relation is based on a set of values of the Wigner 3-j symbols. Calculations of these values may be very slow when a large number of the Al coefficients is needed, particularly at high energies. Consequently, efforts were made to derive and implement an effective algorithm. Additional comments: (i) Restrictions: The phase shifts and consequently the Al coefficients were calculated for the model of isolated atom, and Dirac-Fock interaction potential taking into account the exchange contribution. Also, one should stress that the option of calculating the calibration curves and the IMFPs is limited to certain experimental configurations in which electrons of the primary beam are incident along the surface normal, while the emission angle is contained in the range 0∘&lt;α≤85∘ measured with respect to surface normal. (ii) Unusual features: One should stress that the file DATAFILE must accompany the executable files in the case of calculations of IMFPs for a compound. This file contains information defining a given compound; its detailed structure is described in the Supplementary Information. References: [1] A. Jablonski, Comput. Phys. Commun. 210 (2017) 92–102. [2] A. Jablonski, J. Electron Spectrosc. Relat. Phenom. 206 (2016) 24–45. [3] F. Salvat, A. Jablonski, C.J. Powell, Comput. Phys. Commun. 261 (2021) 107704; the software is available at the web address: https://doi.org/10.17632/w4hm5vymym.1. [4] H. Shinotsuka, S. Tanuma, C.J. Powell, D.R. Penn, Surf. Interface Anal. 47 (2015) 871–888; Erratum: Surf. Interface Anal. 47 (2015) 1132. [5] A. Jablonski, C.J. Powell, J. Phys. Chem. Ref. Data 49 (2020) 033102. © 2022 Elsevier B.V.},
author_keywords={Elastic electron backscattering from surfaces;  Elastic electron scattering on atoms;  Electron inelastic mean free path;  Electron transport in condensed matter},
document_type={Article},
source={Scopus},
}

@ARTICLE{Giudetti20225056,
author={Giudetti, G. and Polyakov, I. and Grigorenko, B.L. and Faraji, S. and Nemukhin, A.V. and Krylov, A.I.},
title={How Reproducible Are QM/MM Simulations? Lessons from Computational Studies of the Covalent Inhibition of the SARS-CoV-2 Main Protease by Carmofur},
journal={Journal of Chemical Theory and Computation},
year={2022},
volume={18},
number={8},
pages={5056-5067},
doi={10.1021/acs.jctc.2c00286},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135599517&doi=10.1021%2facs.jctc.2c00286&partnerID=40&md5=d83b561509fe62e0f22606df4cf45cab},
abstract={This work explores the level of transparency in reporting the details of computational protocols that is required for practical reproducibility of quantum mechanics/molecular mechanics (QM/MM) simulations. Using the reaction of an essential SARS-CoV-2 enzyme (the main protease) with a covalent inhibitor (carmofur) as a test case of chemical reactions in biomolecules, we carried out QM/MM calculations to determine the structures and energies of the reactants, the product, and the transition state/intermediate using analogous QM/MM models implemented in two software packages, NWChem and Q-Chem. Our main benchmarking goal was to reproduce the key energetics computed with the two packages. Our results indicate that quantitative agreement (within the numerical thresholds used in calculations) is difficult to achieve. We show that rather minor details of QM/MM simulations must be reported in order to ensure the reproducibility of the results and offer suggestions toward developing practical guidelines for reporting the results of biosimulations. © 2022 American Chemical Society. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Walker2022755,
author={Walker, L.A. and Williams, J.S. and Li, Y. and Roossien, D.H. and Lee, W.J. and Michki, N.S. and Cai, D.},
title={nGauge: Integrated and Extensible Neuron Morphology Analysis in Python},
journal={Neuroinformatics},
year={2022},
volume={20},
number={3},
pages={755-764},
doi={10.1007/s12021-022-09573-8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126036087&doi=10.1007%2fs12021-022-09573-8&partnerID=40&md5=605be3e73e928b2c0eee0b63b2c0cb5a},
abstract={The study of neuron morphology requires robust and comprehensive methods to quantify the differences between neurons of different subtypes and animal species. Several software packages have been developed for the analysis of neuron tracing results stored in the standard SWC format. The packages, however, provide relatively simple quantifications and their non-extendable architecture prohibit their use for advanced data analysis and visualization. We developed nGauge, a Python toolkit to support the parsing and analysis of neuron morphology data. As an application programming interface (API), nGauge can be referenced by other popular open-source software to create custom informatics analysis pipelines and advanced visualizations. nGauge defines an extendable data structure that handles volumetric constructions (e.g. soma), in addition to the SWC linear reconstructions, while remaining lightweight. This greatly extends nGauge’s data compatibility. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Neuron morphometrics;  Neuron reconstruction;  Neuron visualization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kulkarni2022,
author={Kulkarni, R.U. and Wang, C.L. and Bertozzi, C.R.},
title={Analyzing nested experimental designs—A user-friendly resampling method to determine experimental significance},
journal={PLoS Computational Biology},
year={2022},
volume={18},
number={5},
doi={10.1371/journal.pcbi.1010061},
art_number={e1010061},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130633001&doi=10.1371%2fjournal.pcbi.1010061&partnerID=40&md5=6467785013fce17508624608a7d19dbb},
abstract={While hierarchical experimental designs are near-ubiquitous in neuroscience and biomedical research, researchers often do not take the structure of their datasets into account while performing statistical hypothesis tests. Resampling-based methods are a flexible strategy for performing these analyses but are difficult due to the lack of open-source software to automate test construction and execution. To address this, we present Hierarch, a Python package to perform hypothesis tests and compute confidence intervals on hierarchical experimental designs. Using a combination of permutation resampling and bootstrap aggregation, Hierarch can be used to perform hypothesis tests that maintain nominal Type I error rates and generate confidence intervals that maintain the nominal coverage probability without making distributional assumptions about the dataset of interest. Hierarch makes use of the Numba JIT compiler to reduce p-value computation times to under one second for typical datasets in biomedical research. Hierarch also enables researchers to construct user-defined resampling plans that take advantage of Hierarch’s Numba-accelerated functions. Copyright: © 2022 Kulkarni et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Poole20222144,
author={Poole, D. and Galvez Vallejo, J.L. and Gordon, M.S.},
title={A Task-Based Approach to Parallel Restricted Hartree-Fock Calculations},
journal={Journal of Chemical Theory and Computation},
year={2022},
volume={18},
number={4},
pages={2144-2161},
doi={10.1021/acs.jctc.1c00820},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128168571&doi=10.1021%2facs.jctc.1c00820&partnerID=40&md5=ae85e1743138d5d5bd584c55ebf4971d},
abstract={In recent years, parallelism via multithreading has become extremely important to the optimization of high-performance electronic structure theory codes. Such multithreading is generally achieved via OpenMP constructs, using a fork-join threading model to enable thread-level data parallelism within the code. An alternative approach to multithreading is task-based parallelism, which displays multiple benefits relative to fork-join thread parallelism. A novel Restricted Hartree-Fock (RHF) algorithm, utilizing task-based parallelism to achieve optimal performance, was developed and implemented into the JuliaChem electronic structure theory software package. The new RHF algorithm utilizes a unique method of shell quartet batch creation, enabling construction and distribution of fine-grained shell quartet batches in a load-balanced manner using the Julia task construct. These shell quartet batches are then distributed statically across message-passing interface (MPI) ranks and dynamically across threads within an MPI rank, requiring no explicit inter-rank or interthread synchronization to do so. Compared to the hybrid MPI/OpenMP RHF algorithm present in the GAMESS software package, the task-based algorithm demonstrates speedups of up to ∼40% for systems in the S22(3) test set of molecules, with system sizes up to ∼1000 basis functions. The JuliaChem algorithm demonstrates the viability of both the task-based parallelism model and the Julia programming language for construction of performant electronic structure theory codes targeting systems of a size of chemical interest. © 2022 American Chemical Society. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zanovello2022,
author={Zanovello, U. and Seifert, F. and Bottauscio, O. and Winter, L. and Zilberti, L. and Ittermann, B.},
title={CoSimPy: An open-source python library for MRI radiofrequency Coil EM/Circuit Cosimulation},
journal={Computer Methods and Programs in Biomedicine},
year={2022},
volume={216},
doi={10.1016/j.cmpb.2022.106684},
art_number={106684},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124249302&doi=10.1016%2fj.cmpb.2022.106684&partnerID=40&md5=84cb0eff79eeada9c5e73427624a175f},
abstract={Background and objectives: The Electromagnetic/Circuit cosimulation method represents a valuable and effective strategy to address those problems where a radiative structure has to interact with external supporting circuitries. This is of particular concern for Magnetic Resonance Imaging (MRI), radiofrequency (RF) coils design, where the supporting circuitry optimisation represents, generally, a crucial aspect. This article presents CoSimPy, an open-source Python circuit simulation library for Electromagnetic/Circuit cosimulations and specifically optimised for MRI, RF coils design. Methods: CoSimPy is designed following an Object-orientated programming. In addition to the essential methods aimed to performed the Electromagnetic/Circuit cosimulations, many others are implemented both to simplify the standard workflow and to evaluate the RF coils performance. In this article, the theory which underlies the fundamental methods of CoSimPy is shown together with the basic framework of the library. Results: In the paper, the reliability of CoSimPy is successfully tested against a full-wave electromagnetic simulations involving a reference setup. The library is made available under https://github.com/umbertozanovello/CoSimPy together with a detailed documentation providing guidelines and examples. CoSimPy is distributed under the Massachusetts Institute of Technology (MIT) license. Conclusions: CoSimPy demonstrated to be an agile tool employable for Electromagnetic/Circuit cosimulations. Its distribution is meant to fulfil the needs of several researchers also avoiding duplication of effort in writing custom implementations. CoSimPy is under constant development and aims to represent a coworking environment where scientists can implement additional methods whose sharing can represent an advantage for the community. Finally, even if CoSimPy is designed with special focus on MRI, it represents an efficient and practical tool potentially employable wherever electronic devices made of radiative and circuitry components are involved. © 2022},
author_keywords={Electromagnetic (EM) simulations;  EM/Circuit cosimulation;  Magnetic Resonance Imaging (MRI);  Open source Python library;  Radio-frequency (RF) coils},
document_type={Article},
source={Scopus},
}

@ARTICLE{Goenka2022,
author={Goenka, N. and Tiwari, S.},
title={AlzVNet: A volumetric convolutional neural network for multiclass classification of Alzheimer's disease through multiple neuroimaging computational approaches},
journal={Biomedical Signal Processing and Control},
year={2022},
volume={74},
doi={10.1016/j.bspc.2022.103500},
art_number={103500},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122827052&doi=10.1016%2fj.bspc.2022.103500&partnerID=40&md5=4840872f9ad773af7e9d9cf72b328045},
abstract={Alzheimer's disease is a degenerative neurological disease that causes a loss of cognitive skills and has no known treatment. Alzheimer's disease (AD) must be detected early, before symptoms appear, in order to be treated effectively. In this study, we used a deep learning approach called a convolutional neural network to classify Alzheimer's disease into three categories using a neuroimaging biomarker called T1w-MRI. Our research is the first to look at the results of three neuroimaging computational approaches in a systematic way (3D subject-level, 3D patch-based and slice-based). To show Alzheimer detection using deep convolutional neural networks, three distinct Slice Based methods are used (Subset selection method, uniform selection method, Interpolation zoom method). For 3D patch-based approaches, we investigated the classification accuracy of distinct non-overlapping patches ranging in size from small to medium to large (from 32, 40, 48, 56, 64, 72, 80, till 88). Our findings revealed that 1) our 3-class classification model performed best, with 98.3 percent accuracy percent (highest accuracy obtained until now as per our best knowledge); 2) The 3D Subject-level approach was the most efficient, followed by 3D-patch-based and then Slice-based approaches, with classification accuracy of 98.26 percent, 97.48 percent, and 95.40 percent, respectively; and 3) The same network had the most accuracy for bigger patches (size 72, 80, 88), followed by medium-sized (size 56, 64) to tiny patches (size 32, 40, 48). Large patches had a classification accuracy of 97.48 percent, while medium patches had a classification accuracy of 96.62 percent, and small patches had an accuracy of 86.49 percent. 4)) Even slice selection and interpolation selection exceeded subset slice selection with three-class classification accuracy of 95.37 percent and 94.57 percent, respectively, compared to 92.57 percent for subset slice selection. © 2022 Elsevier Ltd},
author_keywords={Alzheimer;  Brain imaging;  Convolutional neural network;  Neuroimaging;  T1w MRI},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stair20221555,
author={Stair, N.H. and Evangelista, F.A.},
title={QForte: An Efficient State-Vector Emulator and Quantum Algorithms Library for Molecular Electronic Structure},
journal={Journal of Chemical Theory and Computation},
year={2022},
volume={18},
number={3},
pages={1555-1568},
doi={10.1021/acs.jctc.1c01155},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125887418&doi=10.1021%2facs.jctc.1c01155&partnerID=40&md5=5ce14bd0e3c31ec94b23a7622a578ba1},
abstract={We introduce a novel open-source software package QForte, a comprehensive development tool for new quantum simulation algorithms. QForte incorporates functionality for handling molecular Hamiltonians, Fermionic encoding, ansatz construction, time evolution, and state-vector emulation, requiring only a classical electronic structure package as a dependency. QForte also contains black-box implementations of a wide variety of quantum algorithms, including variational and projective quantum eigensolvers, adaptive eigensolvers, quantum imaginary time evolution, and quantum Krylov methods. We highlight two features of QForte: (i) how the Python class structure of QForte enables the facile implementation of new algorithms, and (ii) how existing algorithms can be executed in just a few lines of code. © 2022 American Chemical Society. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mathema2022,
author={Mathema, V.B. and Duangkumpha, K. and Wanichthanarak, K. and Jariyasopit, N. and Dhakal, E. and Sathirapongsasuti, N. and Kitiyakara, C. and Sirivatanauksorn, Y. and Khoomrung, S.},
title={CRISP: A deep learning architecture for GC × GC-TOFMS contour ROI identification, simulation and analysis in imaging metabolomics},
journal={Briefings in Bioinformatics},
year={2022},
volume={23},
number={2},
doi={10.1093/bib/bbab550},
art_number={bbab550},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127523691&doi=10.1093%2fbib%2fbbab550&partnerID=40&md5=3f762388322e13eff9912604b6ed64ed},
abstract={Two-dimensional gas chromatography-time-of-flight mass spectrometry (GC × GC-TOFMS) provides a large amount of molecular information from biological samples. However, the lack of a comprehensive compound library or customizable bioinformatics tool is currently a challenge in GC × GC-TOFMS data analysis. We present an open-source deep learning (DL) software called contour regions of interest (ROI) identification, simulation and untargeted metabolomics profiler (CRISP). CRISP integrates multiple customizable deep neural network architectures for assisting the semi-automated identification of ROIs, contour synthesis, resolution enhancement and classification of GC × GC-TOFMS-based contour images. The approach includes the novel aggregate feature representative contour (AFRC) construction and stacked ROIs. This generates an unbiased contour image dataset that enhances the contrasting characteristics between different test groups and can be suitable for small sample sizes. The utility of the generative models and the accuracy and efficacy of the platform were demonstrated using a dataset of GC × GC-TOFMS contour images from patients with late-stage diabetic nephropathy and healthy control groups. CRISP successfully constructed AFRC images and identified over five ROIs to create a deepstacked dataset. The high fidelity, 512 × 512-pixels generative model was trained as a generator with a FrCrossed D sign ©chet inception distance of <47.00. The trained classifier achieved an AUROC of >0.96 and a classification accuracy of >95.00% for datasets with and without column bleed. Overall, CRISP demonstrates good potential as a DL-based approach for the rapid analysis of 4-D GC × GC-TOFMS untargeted metabolite profiles by directly implementing contour images. CRISP is available at https://github.com/vivekmathema/GCxGC-CRISP.},
author_keywords={bioinformatics;  Chronic kidney disease;  deep learning;  GC × GC-TOF;  imaging metabolomics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pinto2022732,
author={Pinto, P. and Bispo, J. and Cardoso, J.M.P. and Barbosa, J.G. and Gadioli, D. and Palermo, G. and Martinovic, J. and Golasowski, M. and Slaninova, K. and Cmar, R. and Silvano, C.},
title={Pegasus: Performance Engineering for Software Applications Targeting HPC Systems},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={3},
pages={732-754},
doi={10.1109/TSE.2020.3001257},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086722258&doi=10.1109%2fTSE.2020.3001257&partnerID=40&md5=818fe15a97dae01d56be6b298d516e84},
abstract={Developing and optimizing software applications for high performance and energy efficiency is a very challenging task, even when considering a single target machine. For instance, optimizing for multicore-based computing systems requires in-depth knowledge about programming languages, application programming interfaces (APIs), compilers, performance tuning tools, and computer architecture and organization. Many of the tasks of performance engineering methodologies require manual efforts and the use of different tools not always part of an integrated toolchain. This paper presents Pegasus, a performance engineering approach supported by a framework that consists of a source-to-source compiler, controlled and guided by strategies programmed in a Domain-Specific Language, and an autotuner. Pegasus is a holistic and versatile approach spanning various decision layers composing the software stack, and exploiting the system capabilities and workloads effectively through the use of runtime autotuning. The Pegasus approach helps developers by automating tasks regarding the efficient implementation of software applications in multicore computing systems. These tasks focus on application analysis, profiling, code transformations, and the integration of runtime autotuning. Pegasus allows developers to program their strategies or to automatically apply existing strategies to software applications in order to ensure the compliance of non-functional requirements, such as performance and energy efficiency. We show how to apply Pegasus and demonstrate its applicability and effectiveness in a complex case study, which includes tasks from a smart navigation system. © 1976-2012 IEEE.},
author_keywords={domain-specific languages;  high-performance computing;  methodology;  Performance engineering;  source-to-source compilers},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou2022101,
author={Zhou, B. and Guo, Z.-Y. and Han, C.-C. and Du, H. and Yan, Y.-M. and Luo, Y.-T.},
title={Graph convolution network based BREP→CSG conversion method and its application},
journal={Journal of Graphics},
year={2022},
volume={43},
number={1},
pages={101-109},
doi={10.11996/JG.j.2095-302X.2022010101},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141679566&doi=10.11996%2fJG.j.2095-302X.2022010101&partnerID=40&md5=994839306c6bc3d8ecc6363077e1cef4},
abstract={Boundary representation (BREP) and construction solid geometry (CSG) serve as the two most widely employed entity representations. There remains an urgent need for the BREP→CSG automatic conversion algorithm in such fields as particle transport calculation auxiliary modeling. However, the most commonly adopted segmentation-based BREP→CSG conversion algorithm is disadvantageous in “large amount of calculation and too complicated CSG expression”. Through the observation that “the CSG expression structure of the topologically similar BREP model is similar”, it was proposed to establish a model library containing the two tuples BREP and CSG. For the BREP model to be converted, the similar model was retrieved from the model library, and then the conversion result was generated based on the CSG expression of the similar model. On the one hand, this method can improve the conversion speed, and on the other hand, by optimizing the CSG expression, it can overcome the shortcomings of the space-based segmentation method. The extended attribute adjacency graph was applied to the description of the topological characteristics of the BREP model, the model similarity problem was regarded as the attribute adjacency graph classification problem, and then the graph convolutional network (GCN) was utilized to achieve fast model retrieval. The extended attributes of the attribute adjacency graph were also carefully designed to boost the accuracy of model retrieval. The algorithm has been integrated into the self-developed particle transport visual modeling software cosVMPT (COSINE visual modelling of particle transport), and tests were performed using the typical complex component divertor model in China Fusion Engineering Test Reactor (CFETR). The test results show the time validity of the algorithm and the superiority of the CSG results. © 2022, Editorial of Board of Journal of Graphics. All rights reserved.},
author_keywords={attribute adjacency graph;  BREP→CSG conversion;  China Fusion Engineering Test Reactor;  graph convolutional network;  similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Eremina2022,
author={Eremina, T. and Korolchenko, D. and Minaylov, D.},
title={Experimental Evaluation of Fire Resistance Limits for Steel Constructions with Fire-Retardant Coatings at Various Fire Conditions},
journal={Sustainability (Switzerland)},
year={2022},
volume={14},
number={4},
doi={10.3390/su14041962},
art_number={1962},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124531097&doi=10.3390%2fsu14041962&partnerID=40&md5=ebe1fd94cb6efd860aafcaf6e314d011},
abstract={The experimental evaluation of fire resistance limits for steel constructions with fire-retardant coatings consists of a lot of experiments on the heating of steel structures of buildings by solving a heat engineering problem at various fire conditions. Building design implies the assessment of compliance of actual fire resistance limits for steel constructions with the required limits. Fire resistance limits for steel constructions are determined for “standard” temperature mode, and this can lead to overestimated fire resistance and underestimated heat influence for a real fire. Estimation of the convergence for “standard” temperature mode and possible “real” fire mode, as well as of the compliance of actual fire resistance limits with real fire conditions, was realized in the following stages: mathematical modeling of real fire development by the field model in software package Fire Dynamics Simulation (FDS) with various fire loads and mathematical modeling of steel construction heating for the standard temperature mode obtained by modeling “real” fire modes (the finite difference method of solving the Fourier heat conduction equation at external and internal nonlinearities was used for modeling the process of steel structure heating with the implementation in the ANSYS mechanical software package). Experiments of the assessment of fire-protective paint’s effectiveness were carried out for standard temperature mode and obtained by modeling “real” fire modes. The equivalent fire duration dependence on fire load type was determined. This dependence can be taken into account in determination of fire resistance limits for steel constructions in warehouse building roofing. Fire-protective paint effectiveness was estimated for “standard” temperature mode and various other temperature modes. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Fire Dynamics Simulator software package;  Fire resistance estimation;  Fire-retardant material;  Standard temperature mode;  Steel structure},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen2022252,
author={Chen, L. and Song, Y. and Zhang, Y. and Pei, G.},
title={Construction and application of BIM Structural Design course project library based on GSRevit software},
journal={Proceedings - 2022 3rd International Conference on Education, Knowledge and Information Management, ICEKIM 2022},
year={2022},
pages={252-255},
doi={10.1109/ICEKIM55072.2022.00063},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147944258&doi=10.1109%2fICEKIM55072.2022.00063&partnerID=40&md5=8819a3c64e685e94f84a522d6b88be4c},
abstract={With the development of building informatization and industrialization technology, the prefabricated concrete structures based on information technology has become a hot spot in the construction industry. 'BIM Structural Design' combines BIM Technology and Prefabricated building technology with the course content, and uses GSRevit software to carry out classroom teaching in order to cultivate students' structural design ability. The project teaching method is adopted in the course teaching method, and the project library composed of three different practical projects is reformed and created. In combination with students' learning ability and professional knowledge level, different engineering projects are selected for teaching for students at different levels, which can stimulate students' learning interest and initiative, effectively improve the teaching quality of the course, and enhance students' structural design ability and engineering practice ability. © 2022 IEEE.},
author_keywords={BIM;  Project Teaching;  Structural design;  Structural modeling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hoff2022119,
author={Hoff, A. and Gerling, L. and Seidl, C.},
title={Utilizing Software Architecture Recovery to Explore Large-Scale Software Systems in Virtual Reality},
journal={Proceedings - 2022 Working Conference on Software Visualization, VISSOFT 2022},
year={2022},
pages={119-130},
doi={10.1109/VISSOFT55257.2022.00020},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146234736&doi=10.1109%2fVISSOFT55257.2022.00020&partnerID=40&md5=4e7e9a53a7ddfa8ec1aaa66fc58a81b9},
abstract={Exploring an unfamiliar large-scale software system is challenging, especially when based solely on source code. While software visualizations help in gaining an overview of a system, they generally neglect architecture knowledge in their representations, e.g., by arranging elements along package structures rather than functional components or locking users in a specific abstraction only slightly above the source code. In this paper, we introduce an automated approach for software architecture recovery and use its results in an immersive 3D virtual reality software visualization to aid accessing and relating architecture knowledge. We further provide a semantic zoom that allows a user to access and relate information both horizontally on the same abstraction level, e.g., by following method calls, and vertically across different abstraction levels, e.g., from a class to its containing component. We evaluate our contribution in a controlled experiment contrasting the usefulness regarding software exploration and comprehension of our concepts with those of the established CityVR visualization and the Eclipse IDE. © 2022 IEEE.},
author_keywords={Empirical Software Engineering;  Software Architecture Recovery;  Software Visualization;  Virtual Reality},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pandya2022,
author={Pandya, B. and Pourabdollah, A. and Lotfi, A.},
title={A Comparative Study of Stand-alone and Cloud-Based Fuzzy Logic Systems for Human Fall Detection},
journal={International Journal of Fuzzy Systems},
year={2022},
doi={10.1007/s40815-022-01437-2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144340158&doi=10.1007%2fs40815-022-01437-2&partnerID=40&md5=6c486a5393c46054117595823e5b8a50},
abstract={Traditionally, fuzzy logic systems are linked to specific hardware or software systems. Observations reveal that dispersed and distributed designs of intelligent systems are gaining attraction. Due to the possible complexities of fuzzy logic computations, distributed architectures have the potential to add value to the development of fuzzy systems. However, the absence of best practices and standard methodologies may prevent widespread adoption. By broadening the IEEE-1855 (2016) standard in terms of system definition and data exchange, this research offers a standard solution for building a Service-Oriented Architecture (SOA) as a novel method of implementing fuzzy logic systems by means of a cloud-based collecting, processing, and examining data over the web. A comparison between the performances of a stand-alone hardware-dependent solution and a cloud-based solution (known as fuzzy-as-a-service) is performed. The analysis is also carried out on two different cloud service providers and software libraries (Amazon Web Services using JFML as a java-based library and Azure Web Services using Simpful as a python-based library). The analysis and evaluation are performed on a human fall detection scenario involving wearable sensors. The proposed algorithm can identify between fall and non-fall events. However, the results show that the processing time taken per 10,000 samples using smartwatch and mobile was 2220 s and 101 s for a cloud-based non-fuzzy machine learning system, 1111 s and 45 s for a cloud-based fuzzy system with AWS and JFML, and 1250 s and 97 s for a cloud-based fuzzy system with Microsoft Azure and Simpful libraries. It has been observed that a smartwatch with a fuzzy stand-alone crashed after processing 5000 samples and a mobile phone requires 179.42 s to process 10,000 samples. © 2022, Crown.},
author_keywords={Fall detection;  Fuzzy logic;  JFML;  Service-oriented architecture;  Simpful;  Web services},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mario2022,
author={Mario, V. and Madineni, M.C. and Garrett, B. and Yang, X. and Xu, H.},
title={Case Studies of Configurable Binary Design Library on FPGA},
journal={Proceedings - International Symposium on Measurement and Control in Robotics: Robotics and Virtual Tools for a New Era, ISMCR 2022},
year={2022},
doi={10.1109/ISMCR56534.2022.9950580},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143760399&doi=10.1109%2fISMCR56534.2022.9950580&partnerID=40&md5=78a9da49ef0b9cec30ef8adb5762661f},
abstract={This paper presents a configurable binary design library including fundamental arithmetic circuits like full-adder, full-subtractor, binary multiplier, shifter, and more. The Chisel Hardware Construction Language (HCL) is employed to build the parameterizable designs with different precision including half-word, word, double-word, and quad-word. Chisel HCL is an open-source embedded domain-specific language that inherits the object-oriented and functional programming aspects of Scala for constructing hardware. Experimental results show the same accuracy achieved by our proposed work compared with the Verilog HDL implementations. The hardware cost in terms of slice count, power consumption, and the maximum clock frequency is further estimated. Compared with traditional design intellectual properties (IPs) provided by IP vendors, our proposed work is configurable and expandable to the other arithmetic implementations and projects. © 2022 IEEE.},
author_keywords={arithmetic circuits;  Chisel Hardware Construction Language;  FPGA;  Verilog Hardware Description Language},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao202225,
author={Zhao, K. and Liu, W. and Mu, Q. and Wang, H. and Li, R. and He, Z.},
title={PCIe Cryptographic Accelerator Based on Domestic FPGA},
journal={Proceedings - 2022 International Conference on Computing, Communication, Perception and Quantum Technology, CCPQT 2022},
year={2022},
pages={25-31},
doi={10.1109/CCPQT56151.2022.00011},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143051813&doi=10.1109%2fCCPQT56151.2022.00011&partnerID=40&md5=6fbe9978ea562961381cc02efb059587},
abstract={FPGA offers great advantages over ASICs (Application Specific Integrated Circuits) and software when being used for cryptographic applications. However, PCIe cryptographic cards are based on foreign FPGA chips such as Xilinx and Intel. In this paper, a system design and implementation method of high-speed PCIe cryptographic accelerator based on domestic FPGA is proposed, which implements SM4 algorithm. The system consists of hardware module, logic cryptographic algorithm module, host driver and API (Application Programming Interface) module. An optimized scheme for PCIe DMA is implemented in this paper. The PCIe Gen2*4 simultaneous read/write bandwidth is up to 14.08 Gbps with 88% utilization. Based-on pipeline mode, SM4 cryptographic algorithm achieves encryption/decryption speeds of 13.53/13.51 Gbps, respectively. The FPGA accelerated implementation is twice faster than the software implementation in both encryption and decryption modes. The cryptographic system is able to be used as an integral part of CPU-FPGA heterogeneous secure data cryptographic applications. © 2022 IEEE.},
author_keywords={Cryptography;  FPGA;  PCIe;  SM4},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Song202271,
author={Song, Y. and Wang, Y. and Fang, J. and Wang, H.},
title={SaaS Service Architecture of Online System Platform for Engineering Project Management Teaching based on Computer Data Monitoring Algorithm},
journal={International Conference on Edge Computing and Applications, ICECAA 2022 - Proceedings},
year={2022},
pages={71-74},
doi={10.1109/ICECAA55415.2022.9936291},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142695486&doi=10.1109%2fICECAA55415.2022.9936291&partnerID=40&md5=12d3df5204b5bb1c73793f96a3e77b40},
abstract={This paper proposes a data layer architecture based on mass customization for SaaS software services, and analyzes and compares three data architecture design schemes. A dynamic data expansion model is proposed for the engineering project management teaching course and shared Schema structure. The composition structure and functional modules of the computer basic course teaching resource library are expounded, and the construction and use of the cloud computing-based computer basic course teaching resource library are analyzed with effective ways. Manage the data access of the open SaaS service platform and conduct simulation analysis. The combination of open SaaS service architecture and LRU-RSIZE algorithm can effectively solve the problem of on-demand storage and management of user data. © 2022 IEEE.},
author_keywords={Computer Data Monitoring Algorithm;  Engineering Project Management;  Online System Platform;  SaaS Service Architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jacoby2022,
author={Jacoby, M. and Volz, F. and Weisenbacher, C. and Muller, J.},
title={FA3ST Service - An Open Source Implementation of the Reactive Asset Administration Shell},
journal={IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
year={2022},
volume={2022-September},
doi={10.1109/ETFA52439.2022.9921584},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141393777&doi=10.1109%2fETFA52439.2022.9921584&partnerID=40&md5=fe6c3c6cccdeee50faf02b7e82e64d12},
abstract={As one of the cornerstones of Industry 4.0, the concept of digital twins (DTs) is recently gaining more and more importance. The Asset Administration Shell (AAS) specification by Plattform Industrie 4.0 is a concrete adaption of the DT concept with focus on the domain of industrial production. This paper presents the FA3ST Service, an open source software implementation of the reactive AAS as defined by the specification. FA3ST Service embraces an open architecture making it easy to customize and extend. It also enables synchronization of the DT with the underlying asset or other data sources. Internally, FA3ST Service operates in a protocol-agnostic manner which enables supporting multiple different communication protocols such as HTTP and OPC UA for its AAS-conform API as well as OPC UA and MQTT for connecting assets. By offering a command-line interface, a ready-to-use docker container, and being embeddable into custom applications it supports most application scenarios. FA3ST Service is part of the FA3ST ecosystem which aims to provide tools targeting the whole life cycle of a DT. © 2022 IEEE.},
author_keywords={AAS;  Asset Administration Shell;  Digital Twin;  FA3ST},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2022238,
author={Zhang, Y.},
title={Application of BIM in Underground Continuous Wall},
journal={Proceedings - 2022 International Conference on Artificial Intelligence in Everything, AIE 2022},
year={2022},
pages={238-242},
doi={10.1109/AIE57029.2022.00051},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140874249&doi=10.1109%2fAIE57029.2022.00051&partnerID=40&md5=240e4ca9c6f649e0fcf148b955503936},
abstract={Building Information Modeling (BIM) is the direct application of information technology in the construction industry. It has many advantages such as shared API data interfaces, high data integration, data consistency, data relevance and construction demonstration functions. It has been widely used and achieved remarkable results. In the 21st century, the development trend of Beijing city is to develop towards the sky and underground. Especially in the past ten years, with the increase of high-rise and super high-rise buildings in Beijing, and the booming development of subway construction. The underground continuous wall is a construction technology with very high use value whether it is used as the main body (or part of the main body) of the underground structure or as a waterproof and soil retaining facility during construction. However, in the Beijing area, the underground diaphragm wall (depth greater than 40 meters, width greater than or equal to 1.2 meters) due to the difficulty of the project, complex technology, and many construction problems, the relevant design and construction experience is less, and the relevant technical talents are scarce. The purpose of this article is to study the application of BIM in underground continuous walls. This article is based on the Microsoft Visual Studio 2017 development platform, based on the Visual C# programming language, using the API data interface provided by the Revit 2018 software, based on the built BIM model, for engineering quantity statistics, collision checking, drawing output, collaborative work and construction models. Wait for operations to show the application of BIM technology in the underground continuous wall. Experiments show that the use of the collision detection function of BIM technology can not only reasonably arrange pipelines, but also adjust the positions of brackets, buttresses and other components to facilitate the on-site installation of pipelines. The use of BIM technology can simulate the hoisting of the pipe gallery, find out the collision points in the hoisting process, and improve the hoisting plan. © 2022 IEEE.},
author_keywords={building information model;  collision check;  construction simulation;  diaphragm wall},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santos2022515,
author={Santos, M.S. and Silva, N.E.M. and Bessa, A.R.R. and Santos, A.M. and Santos, A.V. and Amoedo, D.A. and Mattos, E.V.C.U. and Pereira, A.M.C. and Valney, M.N. and Torres, G.M. and Silva, A.P. and Cruz, C.F.S. and Romulo, S.F. and Belem, R.J.S. and Bezerra, T.B. and Waldir, S.S. and Carvalho, C.B.},
title={Design and Implementation of a Software Library for Industrial IoT Sensor/Actuator Nodes},
journal={Proceedings - 2022 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2022},
year={2022},
pages={515-516},
doi={10.1109/ICCE-Taiwan55306.2022.9869233},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138755176&doi=10.1109%2fICCE-Taiwan55306.2022.9869233&partnerID=40&md5=99174a62633af0573bc900354aac51d3},
abstract={In this paper, conducted by two partners, called UFAM/CETELI and, Envision (TPV Group), we developed a software library that integrates the STM32L4+/ESP8266 micro-controllers, creating an IoT node that performs its functions concurrently with other processes, namely the MQTT/WiFi persistent communication functions, the publication and subscription of MQTT topics from the broker and, actuator functions, which is not found in the literature. © 2022 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tsou2022,
author={Tsou, Y.-T. and Lin, H.-H. and Chen, W.-C. and Chen, C.-C. and Fang, R.-W. and Hsu, C.-H.},
title={Exploring the Impacts of Students in Hospitality Programs of Vocational High Schools Watching YouTubers' Travel Programs on Travel Intention from the Perspective of Digital Business Opportunities in the Postpandemic Era},
journal={Mobile Information Systems},
year={2022},
volume={2022},
doi={10.1155/2022/6193685},
art_number={6193685},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138020744&doi=10.1155%2f2022%2f6193685&partnerID=40&md5=10ce145f398f9b197d3e184e2a2fd943},
abstract={The purpose of this study is to explore the impacts of students in the hospitality programs of vocational high schools watching YouTubers' travel programs on their travel intentions, as viewed from the perspective of digital business opportunities in the postpandemic era. Students in the hospitality programs of vocational high schools were taken as the research subjects, and a questionnaire survey was conducted with convenience sampling. A total of 350 questionnaires were distributed, and 320 questionnaires were recovered for a recovery rate of 91%. After 35 invalid questionnaires were excluded, there were 285 valid questionnaires for an effective recovery rate of 89%. The software, Statistical Package for the Social Sciences and Analysis of Moment Structures, was used for statistical analysis. The research results show that (1) there was no significant difference between different background variables in travel intention; (2) the degree of involvement had a significant impact on travel intention; (3) trust had a significant impact on travel intention; (4) perceived value had a significant impact on travel intention; and (5) destination image had a significant impact on travel intention. Finally, based on the results of this study, relevant suggestions were provided for practical application. © 2022 Yi-Ta Tsou et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2022,
title={SEKE 2022 - Proceedings of the 34th International Conference on Software Engineering and Knowledge Engineering},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2022},
page_count={661},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137286960&partnerID=40&md5=c0bb671391e41a5ccd3954393a7d1b72},
abstract={The proceedings contain 117 papers. The topics discussed include: a systematic mapping study of information retrieval approaches applied to requirements trace recovery; a framework for requirements specification of machine-learning systems; requirements debt: causes, consequences, and mitigating practices; on the implications of human-paper interaction for software requirements engineering education; identifying risks for collaborative systems during requirements engineering: an ontology-based approach; multi-label code smell detection with hybrid model based on deep learning; an enhanced data augmentation approach to support multi-class code readability classification; contrastive learning for multi-modal automatic code review; utilizing edge attention in graph-based code search; refactoring of object-oriented package structure based on complex network; adaptive prior-knowledge-assisted function naming based on multi-level information explorer; and a distributed graph inference computation framework based on graph neural network model.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Bo2022934,
author={Bo, S. and Mao, X. and Yang, S. and Chen, L.},
title={Towards An Efficient Searching Approach of ROS Message by Knowledge Graph},
journal={Proceedings - 2022 IEEE 46th Annual Computers, Software, and Applications Conference, COMPSAC 2022},
year={2022},
pages={934-943},
doi={10.1109/COMPSAC54236.2022.00145},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137000833&doi=10.1109%2fCOMPSAC54236.2022.00145&partnerID=40&md5=08a07bb985d91d38c37b724d8cbf02e0},
abstract={The Robot Operating System (ROS) has become the most popular robot development framework in the last few years, which has loosely coupled structure and provides remote communications between different component nodes. The ROS messages are critical to bridge the communication channels and clearly define the data structures. The developers can use the standardized or user-customized ROS message types to construct a communication channel between two component nodes uniquely. However, it becomes increasingly difficult for developers to find the required ROS message type from thousands of diverse ROS message types in ROS-based robotic software development. Finding the proper ROS message type is a non-trivial task because developers may hardly know the exact names of required ROS messages but only has a rough knowledge of the task domain features. To tackle this challenge, we construct a novel ROS Message Knowledge Graph (RMKG) with 4543 entities and 14320 relationships, including all ROS message types and message packages. We take the shortest path algorithm to search ROS message in RMKG by searching with ROS message feature or ROS message package and visualize the subgraph structure of the search results. Moreover, we develop a ROS message package library that supports fuzzy queries to find the required message package. A comprehensive evaluation of RMKG shows the high accuracy of our knowledge construction approach. A user study indicates that RMKG is promising in helping developers find suitable ROS message types for robotics software development tasks. An effect evaluation of message package fuzzy query shows the good effects of our fuzzy query method under different situations. © 2022 IEEE.},
author_keywords={feature;  fuzzy query;  Knowledge Graph;  message;  ROS;  search},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Paradis2022107,
author={Paradis, C. and Kazman, R.},
title={Building the MSR Tool Kaiaulu: Design Principles and Experiences},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13365 LNCS},
pages={107-129},
doi={10.1007/978-3-031-15116-3_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136942294&doi=10.1007%2f978-3-031-15116-3_6&partnerID=40&md5=4cd46cd9f4c92988497d8d6fdcda8345},
abstract={Background: Since Alitheia Core was proposed and subsequently retired, tools that support empirical studies of software projects continue to be proposed, such as Codeface, Codeface4Smells, GrimoireLab and SmartSHARK, but they all make different design choices and provide overlapping functionality. Aims: We seek to understand the design decisions adopted by these tools–the good and the bad–along with their consequences, to understand why their authors reinvented functionality already present in other tools, and to help inform the design of future tools. Method: We used action research to evaluate the tools, and to determine a set of principles and anti-patterns to motivate a new tool design. Results: We identified 7 major design choices among the tools: 1) Abstraction Debt, 2) the use of Project Configuration Files, 3) the choice of Batch or Interactive Mode, 4) Minimal Paths to Data, 5) Familiar Software Abstractions, 6) Licensing and 7) the Perils of Code Reuse. Building on the observed good and bad design decisions, we created our own tool architecture and implemented it as an R package. Conclusions: Tools should not require onerous setup for users to obtain data. Authors should consider the conventions and abstractions used by their chosen language and build upon these instead of redefining them. Tools should encourage best practices in experiment reproducibility by leveraging self-contained and readable schemas that are used for tool automation, and reuse must be done with care to avoid depending on dead code. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Action research;  Design choices;  Mining software repositories},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bandara2022,
author={Bandara, M. and Rabhi, F.A. and Bano, M.},
title={A knowledge-driven approach for designing data analytics platforms},
journal={Requirements Engineering},
year={2022},
doi={10.1007/s00766-022-00385-5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136515962&doi=10.1007%2fs00766-022-00385-5&partnerID=40&md5=0f37b786a6059be379f420cf0d54e1f2},
abstract={Big data analytics technologies are rapidly expanding across all industry sectors as organisations try to make analytics an integral part of their everyday decision-making. Although there are many software tools and libraries to assist analysts and software engineers in developing solutions, organisations are looking for flexible analytics platforms that can address their specific objectives and requirements. To minimise costs, such platforms also need to co-exist with existing IT infrastructures and reuse knowledge and resources already accumulated within the organisation. To address such needs, this paper proposes the Data Analytics Solution Engineering (DASE) framework—a knowledge-driven approach supported by semantic web technologies for requirements engineering, design and development of new data analytics platforms. It includes a meta-model that captures data analytics platform requirements via a Knowledge Base, a set of guidelines that organisations can follow in engineering data analytics platforms and a reference architecture that demonstrates how to use these guidelines. We evaluate the DASE framework through two case studies and demonstrate how it can facilitate knowledge-based and requirements-driven data analytics platform engineering. The resulting data analytics platforms are observed to be user friendly, easy to maintain and flexible in handling changes to requirements. This work contributes to the body of knowledge in knowledge-driven requirements engineering, and data analytics platform engineering by providing a meta-model and a reference architecture that can be tailored to different analytics application domains. © 2022, The Author(s).},
author_keywords={Data analytics platform;  Knowledge driven;  Ontology;  Requirements engineering;  Semantic web;  Software architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2022,
author={Zhang, T.},
title={Design and Implementation of Software Engineering Network Teaching System Based on CS Mode},
journal={Security and Communication Networks},
year={2022},
volume={2022},
doi={10.1155/2022/7436988},
art_number={7436988},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136480808&doi=10.1155%2f2022%2f7436988&partnerID=40&md5=6f1482edb42a599fb616e6a246c65b31},
abstract={With the rapid development of computer network technology, all aspects of human life have begun to show a trend of networking and intelligence, and the education industry is no exception. This study wants to improve the quality of classroom teaching and student performance. Through demand analysis, this paper understands the teaching characteristics of software engineering and the shortcomings of the original teaching methods and develops a comprehensive network teaching system (TS). This article introduces the framework design of the system around screenshots, electronic whiteboards, and image compression schemes; starting from three aspects of teaching courseware, exercise library, and teaching practice, it illustrates the advantages of intelligent and personalized teaching. From the experimental data, it can be seen that the excellent performance rate (score>80) of students who study software engineering courses through the network TS after one semester is 75.56%, which is compared with the 46% excellent rate in the class using traditional methods. It can be seen that an intelligent and personalized network TS can play an excellent role in stimulating students' enthusiasm for learning and improving students' academic performance in practice. © 2022 Tianyi Zhang.},
document_type={Retracted},
source={Scopus},
}

@ARTICLE{Brown2022155,
author={Brown, K. and Patterson, E. and Hanks, T. and Fairbanks, J.},
title={Computational Category-Theoretic Rewriting},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13349 LNCS},
pages={155-172},
doi={10.1007/978-3-031-09843-7_9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135075560&doi=10.1007%2f978-3-031-09843-7_9&partnerID=40&md5=94d1198576c7929fe5351595907db881},
abstract={We demonstrate how category theory provides specifications that can efficiently be implemented via imperative algorithms and apply this to the field of graph transformation. By examples, we show how this paradigm of software development makes it easy to quickly write correct and performant code. We provide a modern implementation of graph rewriting techniques at the level of abstraction of finitely-presented C -sets and clarify the connections between C -sets and the typed graphs supported in existing rewriting software. We emphasize that our open-source library is extensible: by taking new categorical constructions (such as slice categories, structured cospans, and distributed graphs) and relating their limits and colimits to those of their underlying categories, users inherit efficient algorithms for pushout complements and (final) pullback complements. This allows one to perform double-, single-, and sesqui-pushout rewriting over a broad class of data structures. Graph transformation researchers, scientists, and engineers can then use this library to computationally manipulate rewriting systems and apply them to their domains of interest. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={category theory;  Double pushout rewriting;  graph rewriting},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kissi2022381,
author={Kissi, S.Y. and Ameur-Boulifa, R. and Seladji, Y.},
title={Security Vulnerabilities Detection Through Assertion-Based Approach},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13299 LNCS},
pages={381-387},
doi={10.1007/978-3-031-10363-6_25},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135044417&doi=10.1007%2f978-3-031-10363-6_25&partnerID=40&md5=241a45bd69f2cfd0e0002acdd32641fd},
abstract={Organizations and companies develop very complex software today. Errors and flaws can be introduced at different phases of the software development life cycle and can lead to exploitable vulnerabilities. Furthermore, considering that most systems are exposed to multiple users and environments, such flaws can lead to attacks (or actions) with unpredictable consequences in terms of damage and costs. Most research that deals with security-related issues of software focuses their efforts on coding errors and flaws, regardless of the infrastructure and platforms that run the software applications. Often, such analyses of software applications vulnerabilities may lack sufficient specification details, thus possibly miss larger systematic flaws, and consequently obscure the existence of serious vulnerabilities. Our research aims at developing a technique capable of discovering the security weaknesses, specifically buffer overflow vulnerabilities in C/C++ programs, through the analysis of source code combined with architecture specifications. The proposed approach relies on the notion of platform assertions that is, a collection of logical relationships used to characterize a platform (execution environment). In this paper, we focus on such assertions and show how vulnerabilities analysis of software applications can be performed with our assertion-based approach. Furthermore, the generation of assertion specifications as well as the construction of an assertion library including various platforms are explored. © 2022, Springer Nature Switzerland AG.},
author_keywords={Assertions;  Execution environment;  Formal analysis;  Vulnerabilities detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Smith2022545,
author={Smith, S. and Michalski, P.},
title={Digging Deeper into the State of the Practice for Domain Specific Research Software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13353 LNCS},
pages={545-559},
doi={10.1007/978-3-031-08760-8_45},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134332858&doi=10.1007%2f978-3-031-08760-8_45&partnerID=40&md5=9aef324a5a7a68c35c7211520a78d468},
abstract={To improve the productivity of research software developers we need to first understand their development practices. Previous studies on this topic have collected data by surveying as many developers as possible, across a broad range of application domains. We propose to dig deeper into the state of the practice by instead looking at what developers in specific domains create, as evidenced by the contents of their repositories. Our methodology prescribes the following steps: i) Identify the domain; ii) Identify a list of candidate software; iii) Filter the list to a length of about 30 packages; iv) Collect repository related data on each package, like number of stars, number of open issues, number of lines of code; v) Fill in the measurement template (the template consists of 108 questions to assess 9 qualities (including the qualities of installability, usability and visibility)); vi) Rank the software using the Analytic Hierarchy Process (AHP); vii) Interview developers (the interview consists of 20 questions and takes about an hour); and, viii) Conduct a domain analysis. The collected data is analyzed by: i) comparing the ranking by best practices against the ranking by popularity; ii) comparing artifacts, tools and processes to current research software development guidelines; and, iii) exploring pain points. We estimate the time to complete an assessment at 173 person hours. The method is illustrated via the example of Lattice Boltzmann Solvers, where we find that the top packages engaged in most of recommended best practices, but still show room for improvement with respect to providing API documentation, a roadmap, a code of conduct, programming style guide and continuous integration. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Developer pain points;  Empirical measures;  Lattice Boltzmann Method;  Research software;  Software artifacts;  Software quality},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Green20221070,
author={Green, H. and Avgerinos, T.},
title={GraphFuzz: Library API Fuzzing with Lifetime-aware Dataflow Graphs},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={1070-1081},
doi={10.1145/3510003.3510228},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133521889&doi=10.1145%2f3510003.3510228&partnerID=40&md5=aca4aacd8b97580fec0a3f6e3e22e428},
abstract={We present the design and implementation of GraphFuzz, a new structure-, coverage- and object lifetime-aware fuzzer capable of automatically testing low-level Library APIs. Unlike other fuzzers, GraphFuzz models sequences of executed functions as a dataflow graph, thus enabling it to perform graph-based mutations both at the data and at the execution trace level. GraphFuzz comes with an automated specification generator to minimize the developer integration effort. We use GraphFuzz to analyze Skia-the rigorously tested Google Chrome graphics library-and benchmark GraphFuzz-generated fuzzing harnesses against hand-optimized, painstakingly written libFuzzer harnesses. We find that GraphFuzz generates test cases that achieve 2-3x more code coverage on average with minimal development effort, and also uncovered previous unknown defects in the process. We demonstrate GraphFuzz's applicability on low-level APIs by analyzing four additional open-source libraries and finding dozens of previously unknown defects. All security relevant findings have already been reported and fixed by the developers. Last, we open-source GraphFuzz under a permissive license and provide code to reproduce all results in this paper. © 2022 ACM.},
author_keywords={fuzzing;  graph;  security;  structure aware},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Baibichenkov2022,
author={Baibichenkov, A.R. and Titova, G.R.},
title={Applying Software Packages for Designing the Power Supply System of Temporary Buildings and Constructions},
journal={2022 6th International Conference on Information Technologies in Engineering Education, Inforino 2022 - Proceedings},
year={2022},
doi={10.1109/Inforino53888.2022.9782975},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132435427&doi=10.1109%2fInforino53888.2022.9782975&partnerID=40&md5=122fd9ddb671b94f0427bc0d665cfd65},
abstract={This article considers various programs for designing power supply systems of temporary buildings and constructions. Examples of calculations of lighting of various objects in the DIALux software package are presented. Conclusions are made about the introduction of various programs to improve the design. © 2022 IEEE.},
author_keywords={AutoCAD;  DIALux;  ETAP;  NanoCAD;  power supply;  Revit;  temporary buildings and structures},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Timperley202256,
author={Timperley, C.S. and Durschmid, T. and Schmerl, B. and Garlan, D. and Goues, C.L.},
title={ROSDiscover: Statically Detecting Run-Time Architecture Misconfigurations in Robotics Systems : per},
journal={2022 IEEE 19th International Conference on Software Architecture Companion, ICSA-C 2022},
year={2022},
pages={56},
doi={10.1109/ICSA-C54293.2022.00055},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132149751&doi=10.1109%2fICSA-C54293.2022.00055&partnerID=40&md5=e33ea214c8e603c535664e0656016879},
abstract={This is the replication package for the paper, ROSDiscover: Statically Detecting Run-Time Architecture Mis-configurations in Robotics Systems, which is published at the International Conference on Software Architecture (ICSA), 2022.The artifact contains (a) the tool ROSDiscover, which is a component and connector architecture recovery tool that recovers and checks robotics systems built in the Robot Operating System (ROS) 1; (b) data set of architecture misconfiguration bugs of real-world open-source ROS 1 systems on GitHub; and (c) scripts and instructions for replicating the results produced in the paper that show that it is both possible to accurately recover run-time architectures of ROS 1 systems, and that these architectures can be used to detect misconfiguration bugs that were found in real systems. © 2022 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Konersmann2022157,
author={Konersmann, M. and Kaplan, A. and Kuhn, T. and Heinrich, R. and Koziolek, A. and Reussner, R. and Jurjens, J. and Al-Doori, M. and Boltz, N. and Ehl, M. and Fuchs, D. and Groser, K. and Hahner, S. and Keim, J. and Lohr, M. and Saglam, T. and Schulz, S. and Toberg, J.-P.},
title={Evaluation Methods and Replicability of Software Architecture Research Objects},
journal={Proceedings - IEEE 19th International Conference on Software Architecture, ICSA 2022},
year={2022},
pages={157-168},
doi={10.1109/ICSA53651.2022.00023},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132036201&doi=10.1109%2fICSA53651.2022.00023&partnerID=40&md5=833a6f54f3b7c2803f4de2ffc8bec1a0},
abstract={Context: Software architecture (SA) as research area experienced an increase in empirical research, as identified by Galster and Weyns in 2016 [1]. Empirical research builds a sound foundation for the validity and comparability of the research. A current overview on the evaluation and replicability of SA research objects could help to discuss our empirical standards as a community. However, no such current overview exists.Objective: We aim at assessing the current state of practice of evaluating SA research objects and replication artifact provision in full technical conference papers from 2017 to 2021.Method: We first create a categorization of papers regarding their evaluation and provision of replication artifacts. In a systematic literature review (SLR) with 153 papers we then investigate how SA research objects are evaluated and how artifacts are made available.Results: We found that technical experiments (28%) and case studies (29%) are the most frequently used evaluation methods over all research objects. Functional suitability (46% of evaluated properties) and performance (29%) are the most evaluated properties. 17 papers (11%) provide replication packages and 97 papers (63%) explicitly state threats to validity. 17% of papers reference guidelines for evaluations and 14% of papers reference guidelines for threats to validity.Conclusions: Our results indicate that the generalizability and repeatability of evaluations could be improved to enhance the maturity of the field; although, there are valid reasons for contributions to not publish their data. We derive from our findings a set of four proposals for improving the state of practice in evaluating software architecture research objects. Researchers can use our results to find recommendations on relevant properties to evaluate and evaluation methods to use and to identify reusable evaluation artifacts to compare their novel ideas with other research. Reviewers can use our results to compare the evaluation and replicability of submissions with the state of the practice. © 2022 IEEE.},
author_keywords={evaluation;  meta-research;  software architecture research;  systematic literature review},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20221731,
author={Wang, L.},
title={HTML Data Capture and User Portrait Data Mining of Landscape Co-Design Platform Based on Distributed Mobile Workstations},
journal={2022 6th International Conference on Trends in Electronics and Informatics, ICOEI 2022 - Proceedings},
year={2022},
pages={1731-1734},
doi={10.1109/ICOEI53556.2022.9776793},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131960265&doi=10.1109%2fICOEI53556.2022.9776793&partnerID=40&md5=7441f821aa77d2638e37f0d4c9768f9b},
abstract={The application of CoDesign platform in landscape construction is discussed. Through software such as TerraBuilder and TerraExplorer Pro, lightweight Web data mining and transformation can be realized by using HTML Tidy. The transformation process mainly solves the separation of schema information to be expressed by HTML documents and their collections. The conversion step is to use the standard class library provided by the distributed mobile workstation, and use the super computing performance of the cloud platform to structure the unstructured HTML data to improve the data processing efficiency of the mobile intelligent terminal. Comprehensive application. © 2022 IEEE.},
author_keywords={Distributed Mobile Workstations;  HTML Data Capture;  Landscape Co-Design Platform;  User Portrait},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Singhal202221,
author={Singhal, S. and Crasta, C.R. and Abdulla, M. and Barmawer, F. and Emberson, D. and Ahobala, R. and Bhat, G. and Rajak, R.K. and Soumya, P.N.},
title={OpenFAM: A Library for Programming Disaggregated Memory},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13159 LNCS},
pages={21-38},
doi={10.1007/978-3-031-04888-3_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130897971&doi=10.1007%2f978-3-031-04888-3_2&partnerID=40&md5=5a8683ba33e9fc9ed734b1ee57d7b6c5},
abstract={HPC architectures are increasingly handling workloads such as AI/ML or high performance data analytics where the working data set cannot be easily partitioned, or does not fit into node local memory. This poses challenges for programming models such as OpenSHMEM, which require data in the working set to fit in the symmetric heap. Emerging fabric-attached memory (FAM) architectures enable data to be held in external memory accessible to all compute nodes, thus providing a new approach to handling large data sets. Unfortunately, most HPC libraries do not currently support FAM, and programmers use file system or key-value store abstractions to access data that is resident off-node, resulting in lower application performance because of the deep software stack necessary in the data path. The OpenFAM API treats data in FAM as memory-resident, and provides memory management and data operation APIs patterned after OpenSHMEM. In this paper, we discuss the design of an open-source reference implementation of the API, and demonstrate its efficiency using micro-benchmarks on a 32-node EDR InfiniBand cluster. We conclude with a discussion of future work and relation to OpenSHMEM. © 2022, Springer Nature Switzerland AG.},
author_keywords={Disaggregated memory;  Fabric attached memory;  OpenFAM implementation;  Programming API},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lyakhov202241,
author={Lyakhov, P.A. and Lyakhova, U.A. and Baboshina, V.A.},
title={Neural Network Classification of Dermatoscopic Images of Pigmented Skin Lesions},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={424},
pages={41-49},
doi={10.1007/978-3-030-97020-8_5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129882797&doi=10.1007%2f978-3-030-97020-8_5&partnerID=40&md5=bc62a8aee54fd187f79bff317dfe646e},
abstract={Today, skin cancer can be regarded as one of the leading causes of death in humans. Skin cancer is the most common type of malignant neoplasm in the body. Rapid and highly accurate diagnosis of malignant skin lesions can reduce the risk of mortality in patients. The paper proposes a neural network classification system of pigmented skin lesions according to 10 diagnostically significant categories. Modeling was carried out using the MATLAB R2020b software package on clinical dermatoscopic images from the international open archive ISIC Melanoma Project. The main convolutional neural network architectures used were SqueezeNet, AlexNet, GoogLeNet, and ResNet101, pre-trained on the ImageNet set of natural images. The highest accuracy rate was achieved using the AlexNet convolutional neural network architecture and amounted to 80.15%. The use of the proposed neural network system for the recognition and classification of dermatoscopic images of pigmented lesions by specialists will improve the accuracy and efficiency of the analysis compared to the methods of visual diagnostics. Timely diagnosis will allow starting treatment at an earlier stage of the disease, which directly affects the percentage of survival and recovery of patients. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Convolutional neural networks;  Deep learning;  Image classification;  Machine learning;  Melanoma;  Pigmented skin neoplasms;  Skin cancer},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reddy2022,
author={Reddy, Y.M. and Nadampalli, M. and Panigrahy, A.K. and Divyasree, K.S. and Jahnavi, A. and Arun Vignesh, N.},
title={Automated Facemask Detection and Monitoring of Body Temperature using IoT Enabled Smart Door},
journal={2022 2nd International Conference on Artificial Intelligence and Signal Processing, AISP 2022},
year={2022},
doi={10.1109/AISP53593.2022.9760551},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129599271&doi=10.1109%2fAISP53593.2022.9760551&partnerID=40&md5=65a126462045c342009b37e313aaac8a},
abstract={Living with the novel Coronavirus is becoming the new normal as nations around the globe resume. However, in order to stop the virus from spreading, we must isolate Covid-infected persons from the rest of the population.Fever is the most common symptom of coronavirus infection, according to the CDC [1], with up to 83 percent of symptomatic patients presenting indications of fever. Early symptom detection and good hygiene standards are therefore critical, particularly in situations where people come into random contact with one another. As a result, temperature checks and masks are now required in schools, colleges, offices, and other public spaces. However, manually monitoring each individual and measuring their respective body temperatures is a cumbersome task. Currently, most of the temperature checkups are done manually which can be inefficient, impractical, and riskybecause sometimes people checking manually may be reluctant to check every person’s temperature or sometimes allow people even if they violate the guidelines. Moreover, the person assigned to manually check will be at high risk as he is exposed to a lot of people. To solve these issues, we propose a project that reduces the growth of COVID-19 by monitoring the presence of a facial mask and measuring their temperature. The Face Mask Detection can be done using the TensorFlow software library, Mobilenet V2 architecture and OpenCV.A non-contact IR temperature sensor is used to monitor the individual's body temperature. To avoid false positives, the system will be strengthened by training it with a variety of cases. Once the system detects a mask, it measures the body temperature of the person. If the temperature is within the normal range, sanitization is done,and the person is permitted entry through an IOT enabled smart door. However, if the system fails to detect a mask or the person's temperature falls out of the predefined range, a buzzer rings and the door remains closed. Our model is intended to be effective in preventing the spread of this infectious disease. © 2022 IEEE.},
author_keywords={Covid 19;  Facial Mask Detection;  IOT;  Mobilenet V2;  OpenCV;  Raspberry pi;  Temperature Detection;  TensorFlow},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Abgaryan202288,
author={Abgaryan, K.K. and Gavrilov, E.S.},
title={SOFTWARE PACKAGE FOR MULTISCALE MODELING OF STRUCTURAL PROPERTIES OF COMPOSITE MATERIALS [ПРОГРАММНЫЙ КОМПЛЕКС ДЛЯМНОГОМАСШТАБНОГО МОДЕЛИРОВАНИЯ СТРУКТУРНЫХ СВОЙСТВ КОМПОЗИЦИОННЫХМАТЕРИАЛОВ]},
journal={Informatika i ee Primeneniya},
year={2022},
volume={16},
number={1},
pages={88-97},
doi={10.14357/19922264220113},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127970262&doi=10.14357%2f19922264220113&partnerID=40&md5=c1958a5cea770007b761f5b72f876fa5},
abstract={Today, creation of new composite materials and methods of their construction with predictable properties is one of the urgent and most important tasks connected with modernization of industrial production in our country. For their solution, technologies of multiscale computer modeling are actively developed. They have become a link between fundamental physics (chemistry) and engineering materials science. The paper presents a software package for modeling structural properties of composite materials which allows solving a number of problems of this class. It is focused on high-performance computations. The complex is based on an original multiscale technology which allows one to promptly conduct multivariate analysis of different classes of composite materials and conduct research on designing the new ones with predictable properties. The developed approaches in combination with experimental data can be used for a better understanding of the physical foundations of the change of properties depending on the structure and, as a consequence, for cheaper and faster search of new composite materials with predetermined properties. © 2022 Federal Research Center "Computer Science and Control" of Russian Academy of Sciences. All rights reserved.},
author_keywords={composite materials;  distributed system;  integration platform;  Multiscale modeling;  software package},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Feder2022858,
author={Feder, M. and Giusti, A. and Vidoni, R.},
title={An approach for automatic generation of the URDF file of modular robots from modules designed using SolidWorks},
journal={Procedia Computer Science},
year={2022},
volume={200},
pages={858-864},
doi={10.1016/j.procs.2022.01.283},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127764805&doi=10.1016%2fj.procs.2022.01.283&partnerID=40&md5=707462309e7be4516b935a1adb35b4df},
abstract={Modular robot manipulators promise to enhance current automation practice by providing higher flexibility and quick recovery in case of failures with respect to fixed-structure robots. The configuration of the modular robot control for each new assembly, to account for the new system kinematics and dynamics, can be time consuming and requires robot modelling expertise. We propose an approach for automatically generating the Unified Robot Description Format (URDF) file of modular robot manipulators, starting from the kinematic and dynamic descriptions expressed following the URDF of the single modules they can be composed of. The approach has been implemented and numerically verified by exploiting off-the-shelf software tools from Robot Operating System (ROS) libraries. © 2022 The Authors. Published by Elsevier B.V.},
author_keywords={Industry 4.0;  Kinematics;  Modular reconfigurable robots;  Robot Operating System;  URDF},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Al-Saeedi2022,
author={Al-Saeedi, A.A.K. and Yurievna, S.O. and Khairi, T.W.},
title={Improving the efficiency of sparse matrix class processing by using the SPM-CSR parallel algorithm and OpenMP technology},
journal={Proceedings of the 2022 4th International Youth Conference on Radio Electronics, Electrical and Power Engineering, REEPE 2022},
year={2022},
doi={10.1109/REEPE53907.2022.9731504},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127717225&doi=10.1109%2fREEPE53907.2022.9731504&partnerID=40&md5=8ff65e8e06ff8bb1857eb518f87b9720},
abstract={The development of computationally efficient algorithms and the improvement of their software implementation are urgent issues that require continuous attention due to the ongoing development of computer system architecture. In multithreading systems bad coordination cause a big problems and give unwilling results. The purpose of the work is to improve performance of the parallel algorithm for sparse matrix multiplication in CSR format by using vector (SpMV-CSR) using OpenMP technology. Using OpenMP technology and reduction directive to solve sparse matrix time execution and bad coordination problem leads to execution time reduction and speed-up increase. Results obtained an acceleration increase of 14% when using reduction. © 2022 IEEE.},
author_keywords={CSR;  OpenMP;  reduction;  sparse matrix;  SpMV},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Korobkov2022349,
author={Korobkov, S. and Gnyrya, A. and Kuznetsov, S.},
title={Computer Simulation of Electric Heating of Concrete Column},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={403 LNNS},
pages={349-357},
doi={10.1007/978-3-030-96383-5_39},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127274093&doi=10.1007%2f978-3-030-96383-5_39&partnerID=40&md5=d5f21494df95df7d2c140041aeacb8b2},
abstract={The paper discusses the results of computer modeling of electrical heating of monolithic reinforced concrete columns in the ELCUT Pro 6.3 software-computing complex. The purpose of this work was to solve the thermodynamic problem of electric heating of concrete of a monolithic rectangular column at negative temperatures. The block section of the monolithic residential building under construction is taken as a basis. The relevance of these studies in the construction due to the need to ensure the temperature conditions required for hardening and a set of concrete strength in the winter. Thermal field modeling is based on the finite element method. In the Elcut Pro software package with the WinConcret add-in, the initial parameters were introduced for the structures being erected. As a result of the calculation, the step of placing single rod electrodes was determined, which leads to a uniform distribution of temperature fields over the section of the structure. The time of isothermal heating of concrete depending on the temperature of the outside air and the strength of concrete at the end of heating are determined. The substantiation of the insulation of the monolithic column formwork, depending on the outside air temperature, has been carried out. The obtained results of modeling in the Elcut Pro software package will be the basis for the development of a technology for concreting columns with electric heating of concrete at negative temperatures, which can be implemented in a construction site conditions. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Computer modelling;  Concrete columns;  Electric heating;  Rod electrodes;  Winter time},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Romanovich20221459,
author={Romanovich, A. and Lisyatnikov, M. and Vlasov, A. and Aleksiievets, V.},
title={Geodesic Domes with Installing Floor Using a Cable Stay System},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={403 LNNS},
pages={1459-1466},
doi={10.1007/978-3-030-96383-5_163},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127266098&doi=10.1007%2f978-3-030-96383-5_163&partnerID=40&md5=d40d054c2c716827d6e7ac7b5b56805e},
abstract={Geodesic domes refer to spatial structures of cover design. Their application provides architectural expressiveness and rational use of building materials. The use of such shells is inextricably linked with a decrease in the consumption of materials for coating structures, a decrease in the cost and construction time. The question of the possibility of embedding the floor in a geodesic dome is being considered, namely, hanging the floor using a cable system. Such a constructive solution will make it possible to use the internal space of geodesic domes more efficiently, to use not only horizontal, but also vertical functional zoning and to increase the usable area of the building several times. The need for research is due to the lack of bases of calculation methods and the necessary experience. The question of the possibility of installing a ceiling suspended with the help of a cable stay system in geodesic domes and a study of load options were studied: the load is applied to the nodes, the load is applied to the rods. Numerical studies and analysis of the stress-strain state of the models have been carried out. Modeling was carried out using the LIRA software package. The operability of the computational models under study has been confirmed. The nature of the distribution of loads over the rods of the system is revealed. The main problems of the proposed circuits are the appearance of stress concentrators in the areas of load application. Solutions for eliminating stress and strain concentrators in nodes are proposed. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Cable stay system;  Geodesic dome;  Installing floor;  Spatial structures},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Benin2022286,
author={Benin, A. and Labutin, N.},
title={Assessment of the Aerodynamic Impact on Pedestrian Overpasses in High-Speed Traffic},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={402 LNNS},
pages={286-294},
doi={10.1007/978-3-030-96380-4_32},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127169583&doi=10.1007%2f978-3-030-96380-4_32&partnerID=40&md5=5ca65d47a8287915c961bec5dde905ec},
abstract={Increasing speeds and the development of high-speed passenger services (including the construction of new high-speed lines) on railways are placing greater demands not only on the aerodynamics of high-speed rolling stock but also on the surrounding infrastructure. Increasing speeds progressively raises the level of aerodynamic impact of high-speed trains on infrastructure expressed as aerodynamic impact. The article presents the magnitude of the air dynamic force components (lifting force and drag force) acting on pedestrian overpasses both by existing methods and by modelling in a software package. An analysis of the calculations carried out leads to the conclusion that the existing methodologies are imperfect. For example, the latter do not take into account factors such as the cross-sectional shape of the overpass. Also, the existing methodologies do not take into account the drag force acting on the spanning structure when a high-speed train passes underneath it and reversed in the direction of its travel. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Aerodynamic impact;  Aerodynamics;  High-speed motorway;  Pedestrian overpass},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kuznetsov2022799,
author={Kuznetsov, A. and Demin, A.},
title={Energy Efficient Design Solution for the Interface Node Between the Floor Slab and the Wall},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={402 LNNS},
pages={799-807},
doi={10.1007/978-3-030-96380-4_87},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127136318&doi=10.1007%2f978-3-030-96380-4_87&partnerID=40&md5=e34d4f8b267f6036e91df969bfc4eeb6},
abstract={The most of the housing stock in the Russian Federation is located in difficult climatic conditions. In such a situation, an important place is occupied by issues related to the energy efficiency of the enclosing structures of residential buildings. In Russia, in the field of civil engineering, a significant proportion of buildings erected using monolithic technology do not meet modern thermal protection requirements. Improving the energy efficiency of such buildings at the design and modernization stages is an important scientific and practical task. The article proposes innovative design solutions aimed at increasing the thermal protection of the building envelopes and the parameters of the microclimate of the premises, which are various options for the joints of the floor disc with the wall. The proposed solutions can be used in the renovation of existing buildings, as well as in the design and construction of new monolithic residential buildings. The issues of heat transfer processes in the junction of the floor disc with the wall are considered. For the studied fragments of the enclosing structures, mathematical models are built based on the heat conduction equation. The results of computational experiments in the ANSYS software package are presented. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Building heat insulation;  Cast-in-place construction;  Energy performance of buildings;  Microclimate parameters;  “Thermal bridge”},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2022,
title={2022 2nd International Conference on Consumer Electronics and Computer Engineering, ICCECE 2022},
journal={2022 2nd International Conference on Consumer Electronics and Computer Engineering, ICCECE 2022},
year={2022},
page_count={959},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126960615&partnerID=40&md5=2799bbaeb86f44dd979362b4953c47c6},
abstract={The proceedings contain 186 papers. The topics discussed include: potential purchase intention capturing method based on consumer's interests mining in social networks; multi-perspective prediction method of users' consumption tendency based on positive and negative attitude; data distribution scheme for information service based on blockchain; an anonymous auction scheme based on blockchain and cryptographic techniques; the construction path of university smart library based on digital twin; research on drug centralized quantity purchase model based on patient brand preference; a risk assessment software system for urban communities; human resource recommendation based on recurrent convolutional neural network; using attention mechanism to solve job shop scheduling problem; using handle system to provide persistent identifiers for diploma; and querying scheme of tobacco traceability information based on double-blockchain structure.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Sun2022821,
author={Sun, X. and Yu, H. and Solvang, W.D.},
title={System Integration for Smart Reverse Logistics Management},
journal={2022 IEEE/SICE International Symposium on System Integration, SII 2022},
year={2022},
pages={821-826},
doi={10.1109/SII52469.2022.9708743},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126230107&doi=10.1109%2fSII52469.2022.9708743&partnerID=40&md5=f966f64f9182cad4b83c36037abe47a9},
abstract={To maximize the value and material recovery from waste products, smart reverse logistics aims at managing the complex flows of physical items, cash, data, and information. The effective management of these flows requires optimal decision making at strategic, tactical, and operational levels. To support the decision making, predictive, prescriptive, and descriptive analytics have been proved to be valuable at all three levels. However, because these analytical tools require different software packages, different coding languages, and different structures of data, the decision support for complex problems combining various analytical methods is usually an ad-hoc process and requires thus significant efforts. There is a lack of standardized solutions that comprise all the necessary modules for smart reverse logistics management. Thus, this paper proposes a conceptual framework with the purpose of guiding the next-generation system integration for smart reverse logistics management. It goes further with the design of six criteria for evaluating the integration maturity of a system. The initial concept is shown with existing software solutions through a case study in Norway, and several challenges are identified for future improvements. © 2022 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Brykov2022193,
author={Brykov, N.A. and Volkov, K.N. and Emelyanov, V.N.},
title={Vectorized numerical algorithms for the solution of continuum mechanics problems},
journal={Scientific and Technical Journal of Information Technologies, Mechanics and Optics},
year={2022},
volume={22},
number={1},
pages={193-205},
doi={10.17586/2226-1494-2022-22-1-193-205},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125807058&doi=10.17586%2f2226-1494-2022-22-1-193-205&partnerID=40&md5=8dd18dd1285988ea38ca2ae05bc13480},
abstract={The aim of the work is to study the possibilities provided by new information technologies, object-oriented programming tools and modern operating systems for solving boundary value problems of continuum mechanics described by partial differential equations. To discretize the basic equations, we applied the method of finite differences and finite volumes, which are widely used to solve problems in the mechanics of liquids and gases. The paper considers the implementation of the finite difference methods and the finite volume method with vectorized grid structures, including access to the inner and boundary cells of the grid, as well as the features of the implementation of algorithms at singular points of the computational domain. To solve boundary value problems described by partial differential equations, we developed an approach to the construction of vectorized algorithms and considered the features of their software implementation in the MATLAB package. Vectorization in such tasks, excluding nested loops, is ensured by appropriate data organization and the use of vectorized operations. On the one hand, the developed algorithms widely use MATLAB functions designed for processing vectors and sparse matrices, and on the other hand, they are distinguished by high efficiency and computation speed, comparable to those of programs written in C/C++. The main results imply the numerical solution of a number of problems in continuum mechanics associated with the calculation of stresses in a separate body and the calculation of the field of velocity and temperature in the flow of a viscous incompressible fluid. The features of discretization of the basic equations and the implementation of the corresponding finite-difference and finite-volume algorithms are shown. The use of the MATLAB system opens up new possibilities for the formalization and implementation of finite-difference and finite-volume methods for the numerical solution of boundary value problems in continuum mechanics. Despite the fact that the capabilities of the developed algorithms are illustrated by the example of solving fairly simple problems, they admit a relatively simple generalization to more complex problems, for example, solving the Euler equations and Navier–Stokes equations. As part of the work, computational modules were prepared with user programming tools that expand the capabilities of the package and are focused on solving problems in continuum mechanics. © 2022, ITMO University. All rights reserved.},
author_keywords={Mathematical modeling;  Mechanics of continuum medium;  Numerical algorithm;  Vectorization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Naik2022129,
author={Naik, P. and Chatterjee, U.},
title={Network Data Remanence Side Channel Attack on SPREAD, H-SPREAD and Reverse AODV},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13162 LNCS},
pages={129-147},
doi={10.1007/978-3-030-95085-9_7},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125579959&doi=10.1007%2f978-3-030-95085-9_7&partnerID=40&md5=57605ade163d6bebe12e88a7e1336a6e},
abstract={Side Channel Attacks (SCAs) was first introduced by Paul Kocher in 1996 to break the secret key of cryptographic algorithms using the inherent property of the implementation along with the mathematical structure of the cipher. These categories of attacks become more robust as they do not require any mathematical cryptanalysis to retrieve the key. Instead, they exploit the timing measurements, power consumption, leaked electromagnetic radiation of the software/hardware platforms to execute key-dependent operations for the cipher. This, in turn, aids the adversary to gather some additional information about the computation. The overall concept of leaking secrets through side channel has been extended for Wireless Sensor Networks (WSNs) that implements Secret Sharing (SS) Scheme to exchange secrets between two nodes across multiple paths. Now in the idealized network model, it is assumed that for such SS schemes, all the paths between the two communicating nodes are atomic and have the same propagation delay. However, in the real implementation of TCP/IP networks, the shares propagate through every link and switch sequentially. Hence the attacker can probe any number of paths or switches to get the residual shares from previous messages that still exist in the network even when a new message is being sent. This kind of side channel vulnerability is known as Network Data Remanence (NDR) attacks. In this paper, we specifically target two SS schemes named Secure Protocol for Reliable Data Delivery (SPREAD) and Hybrid-Secure Protocol for Reliable Data Delivery (H-SPREAD), and an on-demand routing protocol named Path Hopping Based on Reverse AODV (PHR-AODV) to launch NDR based side channel attacks on the WSNs. We then show two specific categories of NDR attacks; a) NDR Blind and b) NDR Planned on the schemes mentioned above. We use an in-house C++ library to simulate our proposed attacks, and the experimental results reveal that the impact of NDR Blind attacks is negligible for these schemes, whereas the probability of data recovery for NDR Planned attacker proportionally increases with the path length. © 2022, Springer Nature Switzerland AG.},
author_keywords={Network data remanence;  Secret sharing scheme;  Side channel attack;  Wireless sensor networks},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Loehr2022,
author={Loehr, D. and Walker, D.},
title={Safe, modular packet pipeline programming},
journal={Proceedings of the ACM on Programming Languages},
year={2022},
volume={6},
number={POPL},
doi={10.1145/3498699},
art_number={3498696},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123215930&doi=10.1145%2f3498699&partnerID=40&md5=968c934a7fcc9d232727be9b83405e4a},
abstract={The P4 language and programmable switch hardware, like the Intel Tofino, have made it possible for network engineers to write new programs that customize operation of computer networks, thereby improving performance, fault-tolerance, energy use, and security. Unfortunately, possible does not mean easy - there are many implicit constraints that programmers must obey if they wish their programs to compile to specialized networking hardware. In particular, all computations on the same switch must access data structures in a consistent order, or it will not be possible to lay that data out along the switch's packet-processing pipeline. In this paper, we define Lucid 2.0, a new language and type system that guarantees programs access data in a consistent order and hence are pipeline-safe. Lucid 2.0 builds on top of the original Lucid language, which is also pipeline-safe, but lacks the features needed for modular construction of data structure libraries. Hence, Lucid 2.0 adds (1) polymorphism and ordering constraints for code reuse; (2) abstract, hierarchical pipeline locations and data types to support information hiding; (3) compile-time constructors, vectors and loops to allow for construction of flexible data structures; and (4) type inference to lessen the burden of program annotations. We develop the meta-theory of Lucid 2.0, prove soundness, and show how to encode constraint checking as an SMT problem. We demonstrate the utility of Lucid 2.0 by developing a suite of useful networking libraries and applications that exploit our new language features, including Bloom filters, sketches, cuckoo hash tables, distributed firewalls, DNS reflection defenses, network address translators (NATs) and a probabilistic traffic monitoring service. © 2022 Owner/Author.},
author_keywords={Network programming languages;  P4;  PISA;  type and effect systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gurevich202215,
author={Gurevich, L. and Danenko, V. and Bogdanov, A. and Kulevich, V.},
title={Analysis of the stress-strain state of steel closed ropes under tension and torsion},
journal={International Journal of Advanced Manufacturing Technology},
year={2022},
volume={118},
number={1-2},
pages={15-22},
doi={10.1007/s00170-021-07128-w},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122224194&doi=10.1007%2fs00170-021-07128-w&partnerID=40&md5=5e8dbb6159b38206b90b13150fe04a8d},
abstract={The analysis of the stress-strain state of the closed rope elements under axial tension and torsion was carried out by the finite element method using the licensed software package SIMULIA/Abaqus. The closed rope consists of an outer layer of Z-profile wires, a subsurface layer of alternating round and H-profile wires, an intermediate layer, and a core of the structure 1 + 7 + 7/7 + 14 of round wires. Modeling made it possible to determine the values of the axial force P, the torque M, the relative elongation ε, and the relative angle of torsion θ of the rope sample at the given values of the axial displacement and the rotation angle the cross-section. The results of the analytical calculation of the stress-strain state of a spiral rope using traditional approaches were compared with those obtained in the course of computer finite element modeling. The results of modeling the rope deformation under tension were verified by experimental data obtained by stretching the rope sample on a universal horizontal hydraulic test machine LabTest 6.2000N.7. The results of computer modeling of the rope pure tension correlate well with the results of the calculation by the method of M.F. Glushko, which more accurately takes into account the real construction of the ropes. Computer simulation of the stress-strain state of the closed rope elements made it possible to determine the contact stresses between shaped wires in layers and between layers at different values of the gap between shaped wires; therefore, it can be used to optimize the gaps. Computer simulation of the stress-strain state of the rope elements of a closed structure makes it possible to assess the consistency of the layers narrowing during axial tension by analyzing the contact stresses between adjacent wires in the cross-section of the rope. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Closed rope;  Deformation;  Equivalent stress;  Finite element modeling;  Stress-strain state;  Stretching;  Torsion;  Wire},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mahmoud20222159,
author={Mahmoud, S. and Salman, A.},
title={Cost estimate and input energy of floor systems in low seismic regions},
journal={Computers, Materials and Continua},
year={2022},
volume={71},
number={2},
pages={2159-2173},
doi={10.32604/cmc.2022.022357},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120896925&doi=10.32604%2fcmc.2022.022357&partnerID=40&md5=0a5574e2b7f523f4ca6f162d17369fb6},
abstract={Reinforced concrete (RC) as a material is most commonly used for buildings construction. Several floor systems are available following the structural and architectural requirements. The current research study provides cost and input energy comparisons of RC office buildings of different floor systems. Conventional solid, ribbed, flat plate and flat slab systems are considered in the study. Building models in three-dimensional using extended three-dimensional analysis of building systems (ETABS) and in two-dimensional using slab analysis by the finite element (SAFE) are developed for analysis purposes. Analysis and design using both software packages and manual calculations are employed to obtain the optimum sections and reinforcements to fit cities of low seismic intensities for all the considered building systems. Two ground motion records of low peak ground acceleration (PGA) levels are used to excite the models to measure the input energies. Uniformat cost estimating system is adopted to categorize building components according to 12 divisions. Also, Microsoft (MS) Project is utilized to identify the construction cost and duration of each building system. The study shows that floor system significantly causes changes in the input energy to structures. In addition, the slight increase in the PGA increases the amount of input energy particularly flat plate system. Estimated cost of the flat plate system that the flat slab system is of higher value as compared to ribbed and conventional slab systems. The use of drop panels increases this value as well. Moreover, the estimated cost of the ribbed slab system exceeds that of conventional system. © 2022 Tech Science Press. All rights reserved.},
author_keywords={Cost estimating;  Energy response;  Floor systems;  Office buildings;  Uniformat system},
document_type={Article},
source={Scopus},
}

@ARTICLE{Arzo2022,
author={Arzo, S.T. and Scotece, D. and Bassoli, R. and Barattini, D. and Granelli, F. and Foschini, L. and Fitzek, F.H.P.},
title={MSN: A Playground Framework for Design and Evaluation of MicroServices-Based sdN Controller},
journal={Journal of Network and Systems Management},
year={2022},
volume={30},
number={1},
doi={10.1007/s10922-021-09631-7},
art_number={19},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117582249&doi=10.1007%2fs10922-021-09631-7&partnerID=40&md5=2908057b246b96ca5ad52281fbec0fd9},
abstract={Software-defined networking decouples control and data plane in softwarized networks. This allows for centralized management of the network, but complete centralization of the controller functions raises potential issues related to failure, latency, and scalability. Distributed controller deployment is adopted to optimize scalability and latency problems. However, existing controllers are monolithic, resulting in code inefficiency for distributed deployment. Some seminal ongoing efforts have been proposed with the idea of disaggregating the SDN controller architecture into an assembly of various subsystems, each of which can be responsible for a certain controller task. These subsystems are typically implemented as microservices and deployed as virtual network functions, in particular as Docker Containers. This enables flexible deployment of controller functions. However, these proposals (e.g., μONOS) are still in their early stage of design and development, so that a full decomposition of the SDN controller is not been available yet. To fill that gap, this article derives some important design guidelines to decompose an SDN controller into a set of microservices. Next, it also proposes a microservices-based decomposed controller architecture, foreseeing communications issues between the controller sub-functions. These design and performance considerations are also proven via the implementation of the proposed architecture as a solution, called Micro-Services based SDN controller (MSN), based on the Ryu SDN controller. Moreover, MSN includes different network communication protocols, such as gRPC, WebSocket, and REST-API. Finally, we show experimental results that highlight the robustness and latency of the system on a networking testbed. Collected results prove the main pros and cons of each network communication protocol and an evaluation of our proposal in terms of system resilience, scalability and latency. © 2021, The Author(s).},
author_keywords={5G;  Docker container;  ETSI management and orchestration;  Microservice-based decomposition architecture;  Network function virtualization;  Software defined networking},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wilhelm2022425,
author={Wilhelm, M.E. and Stuber, M.D.},
title={EAGO.jl: easy advanced global optimization in Julia},
journal={Optimization Methods and Software},
year={2022},
volume={37},
number={2},
pages={425-450},
doi={10.1080/10556788.2020.1786566},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089251067&doi=10.1080%2f10556788.2020.1786566&partnerID=40&md5=05c29398a950cc6b7e95f3fddc158a5f},
abstract={An extensible open-source deterministic global optimizer (EAGO) programmed entirely in the Julia language is presented. EAGO was developed to serve the need for supporting higher-complexity user-defined functions (e.g. functions defined implicitly via algorithms) within optimization models. EAGO embeds a first-of-its-kind implementation of McCormick arithmetic in an Evaluator structure allowing for the construction of convex/concave relaxations using a combination of source code transformation, multiple dispatch, and context-specific approaches. Utilities are included to parse user-defined functions into a directed acyclic graph representation and perform symbolic transformations enabling dramatically improved solution speed. EAGO is compatible with a wide variety of local optimizers, the most exhaustive library of transcendental functions, and allows for easy accessibility through the JuMP modelling language. Together with Julia's minimalist syntax and competitive speed, these powerful features make EAGO a versatile research platform enabling easy construction of novel meta-solvers, incorporation and utilization of new relaxations, and extension to advanced problem formulations encountered in engineering and operations research (e.g. multilevel problems, user-defined functions). The applicability and flexibility of this novel software is demonstrated on a diverse set of examples. Lastly, EAGO is demonstrated to perform comparably to state-of-the-art commercial optimizers on a benchmarking test set. © 2020 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={branch-and-bound;  Deterministic global optimization;  Julia;  McCormick relaxations;  nonconvex programming;  optimization software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cerasoli2021,
author={Cerasoli, F.T. and Supka, A.R. and Jayaraj, A. and Costa, M. and Siloi, I. and Sławińska, J. and Curtarolo, S. and Fornari, M. and Ceresoli, D. and Buongiorno Nardelli, M.},
title={Advanced modeling of materials with PAOFLOW 2.0: New features and software design},
journal={Computational Materials Science},
year={2021},
volume={200},
doi={10.1016/j.commatsci.2021.110828},
art_number={110828},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114128819&doi=10.1016%2fj.commatsci.2021.110828&partnerID=40&md5=d62de2926911a63133834429ffa316e0},
abstract={Recent research in materials science opens exciting perspectives to design novel quantum materials and devices, but it calls for quantitative predictions of properties which are not accessible in standard first principles packages. PAOFLOW, is a software tool that constructs tight-binding Hamiltonians from self-consistent electronic wavefunctions by projecting onto a set of atomic orbitals. The electronic structure provides numerous materials properties that otherwise would have to be calculated via phenomenological models. In this paper, we describe recent re-design of the code as well as the new features and improvements in performance. In particular, we have implemented symmetry operations for unfolding equivalent k-points, which drastically reduces the runtime requirements of first principles calculations, and we have provided internal routines of projections onto atomic orbitals enabling generation of real space atomic orbitals. Moreover, we have included models for non-constant relaxation time in electronic transport calculations, doubling the real space dimensions of the Hamiltonian as well as the construction of Hamiltonians directly from analytical models. Importantly, PAOFLOW has been now converted into a Python package, and is streamlined for use directly within other Python codes. The new object oriented design treats PAOFLOW's computational routines as class methods, providing an API for explicit control of each calculation. © 2021 Elsevier B.V.},
author_keywords={Ab initio tight-binding;  DFT;  Electronic structure;  High-throughput calculations},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gerlero2021,
author={Gerlero, G.S. and Márquez Damián, S. and Kler, P.A.},
title={electroMicroTransport v2107: Open-source toolbox for paper-based electromigrative separations},
journal={Computer Physics Communications},
year={2021},
volume={269},
doi={10.1016/j.cpc.2021.108143},
art_number={108143},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113358618&doi=10.1016%2fj.cpc.2021.108143&partnerID=40&md5=1792047bd4d795111556819db4172b2b},
abstract={Paper-based electromigrative separations have recently gained relevance due to the rise of paper-based microfluidic devices and their combination with electrophoretic methods, used for many different analytical applications [1]. A new version of the open source toolbox electroMicroTransport for the numerical solution of electromigrative separations is presented, now featuring support for porous substrates, like paper, nitrocellulose and other materials used in paper-based microfluidics. This new version is based on a novel mathematical model for these phenomena recently published by our group. Similar to its previous versions, the toolbox was implemented using OpenFOAM®, meaning that it features native 3D problem handling, support for parallel computation, and a GNU GPL license. This new version of electroMicroTransport includes full support for electroosmotic flow and the novels mechanical and electrical dispersion effects. It is now integrated with a well-recognized electrolyte database with its own management utility, and also includes a renewed algorithm for computing and controlling the electric current drainage in arbitrary surfaces. Moreover, for the first time electroMicroTransport is available for installation as a Docker image, which means that it is able to correctly run on any operating system. Finally, new tutorial examples and a user manual are provided. This new version of electroMicroTransport will enable efficient and reliable numerical prototypes of paper-based electromigrative separations to boost the continuous growth of paper-based microfluidics. New version program summary: Program Title: electroMicroTransport CPC Library link to program files:: https://doi.org/10.17632/9wpvypzj9y.2 Developer's repository link: https://gitlab.com/santiagomarquezd/electromicrotransport Licensing provisions: GPLv3 Programming language: C++, Python Journal reference of previous version: Computer Physics Communications 237 (2019) 244-252 Does the new version supersede the previous version?: Yes Reasons for the new version: This new version adds several new features, including support for porous substrates, new boundary conditions, a comprehensive electrolyte database, new tutorial cases, a streamlined installation process, and a Docker installation option. Summary of revisions: • Electrolyte database – electroMicroTransport now includes an electrolyte database based on the one bundled with Simul 6, itself built from the work of Prof. Hirokawa. [2,3] – The dabatabase includes the properties of 518 components by default. – Simulation cases can directly name database components for easy initialization of electrolyte properties. – A new electrolytes command-line utility provides access the default electrolyte database to search and query for component properties—as well as to add, update and remove user-defined components. • New algorithm for electric constant current – The new boundary condition called uniformCurrentDensity replaces the earlier uniform1DCurrentDensity boundary condition [4] which was valid only for 1D domains. With this new condition, it is possible to prescribe a value for constant electric current drained from an arbitrary 3D shaped electrode. – Drained current I is calculated as the sum of all of the individual currents flowing through every cell adjacent to the electrode surface. – When using a uniformCurrentDensity boundary condition, on each iteration, electroMicroTransport will provide the user with information on the applied electric current and current density values, the target current, and the effective applied electric potential at the electrode. • Support for paper substrates – This new version of electroMicroTransport offers full support for paper-based electrophoretic devices with modified transport equations based on a recent model [5]. – Users can provide different substrate properties such as porosity, tortuosity, Darcy permeability, bias conductivity, and electrokinetic potential. – Mechanical and electrophoretically driven dispersion terms require setting values for the coefficients sf and se. – When the user cannot provide reliable values for these parameters, default values are used which match the physical characteristics of Whatman #1 paper. – A new body force term in the momentum equation for fluid flow was introduced in order to obtain a proper representation of EOF in porous substrates [6]. • New tutorial cases showcasing the new functionality – Paper-based zone electrophoresis experiments: paperBasedZE, paperBasedZE1, paperBasedZE2 – Paper-based moving-boundary electrophoresis cases: paperBasedMBE1 (constant voltage), paperBasedMBE2 (constant current) – Paper-based free-flow isoelectric focusing case: paperBasedFFIEF • Automated test suite – The electroMicroTransport project now includes an automated test suite used by developers to validate the implementations. Test cases are written using the pytest framework. – Following best practices, the test suite is run automatically as part of the continuous integration/deployment (CI/CD) process. • User manual – A user manual of electroMicroTransport is now available, which guides users through the installation and use of the software. – The manual also covers many details of the software that might be of interest to users. • Streamlined installation process – This new version of electroMicroTransport drops the requirement of a user-writable OpenFOAM installation. This means that electroMicroTransport can now be set up to use a system-wide installation of OpenFOAM without any privileges. – To allow this, code that previously required modifications to base OpenFOAM has now been packaged into a standalone shared library. – This change will benefit users who get OpenFOAM from their system's package manager, as well as any users of shared computers where they might not be able to modify existing packages (e.g. computer clusters). – This improvement in the architecture of the program implies a significant reduction in compilation time and resources. • Docker installation option – electroMicroTransport is now also available as a Docker image for use with the Docker software [7]. This image is based on an official Docker image of OpenFOAM and contains pre-built installation of electroMicroTransport. It does not require an existing OpenFOAM installation. – Official images of the toolbox can be obtained from the microfluidica/electromicrotransport repository on Docker Hub. – All Docker images are built and tested automatically during the CI/CD process. This ensures that Docker users always have access to an up-to-date, validated version of the toolbox. – Docker allows electroMicroTransport to be used on non-Linux systems that do not natively support OpenFOAM. Nature of problem: The electroMicroTransport toolbox is intended to offer a complete suite for performing numerical simulations of electromigrative transport, including electroosmotic flow in both open channels and porous substrates. In order to perform these simulations, three partial differential equations and two non-linear conservation equations are solved. The coupled solution of this set of equations provides the different concentration fields for all electrolyte components of the system, the pH and conductivity fields, the fluid flow velocity and pressure fields (associated to the solvent, usually water), and the electric potential field over the entire calculation domain at any simulation time. Numerical simulations provided by electroMicroTransport can improve design process of microfluidic electrophoretic devices and its operational parameters as well as to contribute in basic research on developing new separation strategies for emerging analytical applications. Solution method: As was previously mentioned, electroMicroTransport numerical solutions are based on the coupled solving of three partial differential equations and two non-linear conservation equations. The solution method for the partial (in both space and time) differential equations is the Finite Volume Method whose basic methods and functions are provided by the OpenFOAM® platform. The non-linear equations are solved by using ad hoc implemented Newton–Raphson methods in native C++ language. The different equations are solved sequentially employing weak coupling and temporal sub-looping strategies. These strategies are justified by the fact that the various phenomena (chemical, electrochemical, mechanical and electrical) have different characteristic times for their evolutions. This scheme has been validated and has demonstrated both robustness and computational efficiency. Additional comments including restrictions and unusual features: Except when using the Docker image, installing and running electroMicroTransport requires an active installation of OpenFOAM. Only official OpenFOAM® releases distributed by OpenCFD Ltd. are supported by electroMicroTransport. This version of electroMicroTransport has been tested with OpenFOAM versions 1912, 2006 and 2012. References: [1] G. I. Salentijn, M. Grajewski, E. Verpoorte, Anal. Chem. 90 (23) (2018) 13815–13825. [2] T. Hirokawa, M. Nishino, N. Aoki, Y. Kiso, Y. Sawamoto, T. Yagi, J.-I. Akiyama, J. Chromatogr. A 271 (2) (1983) D1–D106. [3] B. Gaš, P. Bravenec, Electrophoresis (2021). [4] S. Márquez Damián, F. Schaumburg, P.A. Kler, Comput. Phys. Commun. 237 (2019) 244–252. [5] F. Schaumburg, P.A. Kler, C.L.A. Berli, Electrophoresis 41 (7–8) (2020) 598–606. [6] N. Franck, F. Schaumburg, R. Urteaga, P.A. Kler, Electrophoresis 42 (7–8) (2021) 975–982. [7] G.S. Gerlero, S.M. Damián, F. Schaumburg, N. Franck, P.A. Kler, Electrophoresis 42 (16) (2021) 1543–1551. © 2021 Elsevier B.V.},
author_keywords={Electroosmotic flow;  Electrophoresis;  Finite volume method;  High performance computing;  Paper-based microfluidics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Přívratský2021,
author={Přívratský, J. and Novák, J.},
title={MassSpecBlocks: a web-based tool to create building blocks and sequences of nonribosomal peptides and polyketides for tandem mass spectra analysis},
journal={Journal of Cheminformatics},
year={2021},
volume={13},
number={1},
doi={10.1186/s13321-021-00530-2},
art_number={51},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109602209&doi=10.1186%2fs13321-021-00530-2&partnerID=40&md5=08ef54b21d155cea18ec6b5c41e90351},
abstract={Nonribosomal peptides and polyketides are natural products commonly synthesized by microorganisms. They are widely used in medicine, agriculture, environmental protection, and other fields. The structures of natural products are often analyzed by high-resolution tandem mass spectrometry, which becomes more popular with its increasing availability. However, the characterization of nonribosomal peptides and polyketides from tandem mass spectra is a nontrivial task because they are composed of many uncommon building blocks in addition to proteinogenic amino acids. Moreover, many of them have cyclic and branch-cyclic structures. Here, we introduce MassSpecBlocks – an open-source and web-based tool that converts the input chemical structures in SMILES format into sequences of building blocks. The structures can be searched in public databases PubChem, ChemSpider, ChEBI, NP Atlas, COCONUT, and Norine and edited in a user-friendly graphical interface. Although MassSpecBlocks can serve as a stand-alone database, our primary goal was to enable easy construction of custom sequence and building block databases, which can be used to annotate mass spectra in CycloBranch software. CycloBranch is an open-source, cross-platform, and stand-alone tool that we recently released for annotating spectra of linear, cyclic, branched, and branch-cyclic nonribosomal peptides and polyketide siderophores. The sequences and building blocks created in MassSpecBlocks can be easily exported into a plain text format used by CycloBranch. MassSpecBlocks is available online or can be installed entirely offline. It offers a REST API to cooperate with other tools. [Figure not available: see fulltext.] © 2021, The Author(s).},
author_keywords={Building blocks;  CycloBranch;  Mass spectrometry;  MassSpecBlocks;  Nonribosomal petides;  Polyketides;  Siderophores;  SmilesDrawer;  Tanimoto similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sorgalla2021,
author={Sorgalla, J. and Wizenty, P. and Rademacher, F. and Sachweh, S. and Zündorf, A.},
title={Applying Model-Driven Engineering to Stimulate the Adoption of DevOps Processes in Small and Medium-Sized Development Organizations: The Case for Microservice Architecture},
journal={SN Computer Science},
year={2021},
volume={2},
number={6},
doi={10.1007/s42979-021-00825-z},
art_number={459},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126199760&doi=10.1007%2fs42979-021-00825-z&partnerID=40&md5=4ca896cea820fe9f9c158b7126af1938},
abstract={Microservice architecture (MSA) denotes an increasingly popular architectural style in which business capabilities are wrapped into autonomously developable and deployable software components called microservices. Microservice applications are developed by multiple DevOps teams each owning one or more services. In this article, we explore the state of how DevOps teams in small and medium-sized organizations (SMOs) cope with MSA and how they can be supported. We show through a secondary analysis of an exploratory interview study comprising six cases, that the organizational and technological complexity resulting from MSA poses particular challenges for small and medium-sized organizations (SMOs). We apply model-driven engineering to address these challenges. As results of the second analysis, we identify the challenge areas of building and maintaining a common architectural understanding, and dealing with deployment technologies. To support DevOps teams of SMOs in coping with these challenges, we present a model-driven workflow based on LEMMA—the Language Ecosystem for Modeling Microservice Architecture. To implement the workflow, we extend LEMMA with the functionality to (i) generate models from API documentation; (ii) reference remote models owned by other teams; (iii) generate deployment specifications; and (iv) generate a visual representation of the overall architecture. We validate the model-driven workflow and our extensions to LEMMA through a case study showing that the added functionality to LEMMA can bring efficiency gains for DevOps teams. To develop best practices for applying our workflow to maximize efficiency in SMOs, we plan to conduct more empirical research in the field in the future. © 2021, The Author(s).},
author_keywords={Development process;  DevOps;  Microservice architecture;  Model-driven engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ali20211023,
author={Ali, B. and Zahoor, H. and Aibinu, A. and Nasir, A.R. and Tariq, A. and Imran, U. and Khan, R.M.},
title={BIM AIDED INFORMATION and VISUALIZATION REPOSITORY for MANAGING CONSTRUCTION DELAY CLAIMS},
journal={Journal of Information Technology in Construction},
year={2021},
volume={26},
pages={1023-1040},
doi={10.36680/J.ITCON.2021.054},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121919505&doi=10.36680%2fJ.ITCON.2021.054&partnerID=40&md5=4ffcf47ae5a368141ada3f79ff09348a},
abstract={Delays in construction result in a multitude of negative effects on project performance, and severe dismays among participating parties. This study aims to digitize the traditional process of recording and managing the construction delays using Building Information Modeling (BIM). Extensive literature review followed by semi-structure interviews of 21 industry experts were carried out to identify the issues faced by construction stakeholders in managing construction delays. To resolve these issues, a plugin named BIM-based Construction Delays Recorder (BIM-CDR) is developed using Application Programming Interface (API) of the most commonly used BIM software i.e. Autodesk Revit. BIM-CDR provides a centralized repository, encompassing detailed information related to delays, which can be retrieved and visualized to analyze their impact on delay claims. To assess the effectiveness of BIM-CDR, a feasibility study is conducted with the experts’ review panel. The results revealed that BIM-CDR can record wide-ranging information related to all the significant issues causing delays on construction sites, and can help in effectively managing their corresponding claims. The advantages of the developed prototype include visualization of delays’ location, facilitation of delay analysis and effective delays management. Moreover, it also promotes transparency and speedy settlement of delay related claims without any unwanted disputes. COPYRIGHT: © 2021 The author(s). This is an open access article distributed under the terms of the Creative Commons Attribution 4.0 International (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
author_keywords={Application Programming Interface;  Building Information Modeling;  Claims;  Construction delays;  Delay analysis;  Plugin},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tian202166,
author={Tian, Z. and Wang, A.-B.},
title={A Comprehensive Review of Traffic Signal Timing Practice and Techniques in the United States [美国交通信号配时实践与技术综述]},
journal={Jiaotong Yunshu Xitong Gongcheng Yu Xinxi/Journal of Transportation Systems Engineering and Information Technology},
year={2021},
volume={21},
number={5},
pages={66-76},
doi={10.16097/j.cnki.1009-6744.2021.05.007},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117920177&doi=10.16097%2fj.cnki.1009-6744.2021.05.007&partnerID=40&md5=e240884fa9f6c48bc20c225eeafb7381},
abstract={Traffic signal control is a critical part of urban transportation management. As a cost-effective approach to traffic operational improvements, facilitating the development of signal timing has been of a great interest to traffic engineers and researchers in recent years. The United States (US) is one of the most advanced countries where traffic signals were first implemented, and its best practices and lessons learned could be beneficial to many other countries. This paper provides a comprehensive review of traffic signal timing practice and techniques in the US, focusing on signal control characteristics, timing development process and software tools, as well as emerging technologies and techniques. The signal control practice in the US (also Canada) possesses some unique characteristics comparing to other countries. First, some signal timing terminologies may only exist in the US or have different meanings, such as a "signal phase". Those who are interested in understanding the US practice must go through the details of the term definitions documented in the US manuals and standards. Secondly, a special ring-barrier structure is widely adopted to describe signal phasing and timing. Through a combination of rings and barriers that imply the duration, sequence and compatibility of signal phases, this structure allows for safe and efficient implementation of signal timing, especially at typical intersections with actuated traffic signals. Thirdly, the US signal control facilities and signal timing techniques are mostly on traffic actuated control. In the context of an actuated control mode, timing development for isolated signals can be simplified, as signal operations are largely in response to detector actuations; hence, the major effort goes to developing signal coordination. An outcome-based procedure has been highly encouraged in the US signal timing development process. The procedure is comprised of several steps, such as data collection, data analysis, timing development, and timing maintenance, in which the effectiveness and efficiency of signal timing development must rely on advanced software tools. In this paper, five signal timing software tools are analyzed regarding features of data management, timing optimization, timing diagnosis, and performance evaluation. While all of these software tools have basic functions of signal timing optimization and data management, some major differences exist in terms of timing diagnosis and performance evaluation. Timing diagnosis is important for identifying erroneous and abnormal signal operations, which ensures consistency between the designed timing plans and actual field operations. The mobile version of TranSync features active time- space diagrams and visualization of ring- barrier- strucature-based signal operations, allowing for convenient timing diagnosis in the field. Performance measurement is needed for monitoring the quality of signal timing and validating the effectiveness of signal re- timing. Earlier software packages, such as TRANSYT- 7F, PASSER and Synchro, only provide performance evaluation functions based on deterministic algrithoms and simulation, while emerging tools such as Tru-Traffic and TranSync allow users to collect actual travel- run trajectories and produce performance evaluations accordingly. The signal timing practice and techniques in the US are transforming along with the emerging technologies. Numerous studies have been conducted in recent years, and two topics are highlighted in this review, i.e., traffic signal performance meansures based on new data sources and new signal control methodologies considering a connected-and autonomous- vehicles (CAV) environment. In recent years, a variety of data sources have been used to assess traffic control performance. Automated Traffic Signal Performance Measures (ATSPMs) is an influential research effort that incorporates high-resolution controller and detector event data to gauge traffic control efficieny such as the quality of platoon progression. Travel-run trajectory data are also used in some studies to evaluate signal coordination according to travel time and the number of stops. A vast number of studies are conducted on the second topic, investigating the impacts of future CAV applications and formulating many possibilities of next- generation traffic signal control. In addition to the two topics, a few studies are devoted to improving the classic signal timing optimization algorithms. The improvements are aimed at signal coordination for multimodal traffic and special cases. Lastly, this paper provides an outlook for future signal timing practice and techniques. Signal timing tools will continue to play an important role in satisfying complex traffic management strategies and goals. The innovative use of multi-source data will enhance the traffic timing process in terms of performance monitoring. The application of CAV technologies may lead to a revolution of signal control; however, a dramatic change of the current signal control infrastructure is unlikely in the near future due to the immaturity of the current research and the mixed driving environment during a transition stage. As a result, it advocates studies for improving traffic signal timing during the transition stage towards a complete CAV circumstance. Research efforts are also required for developing necessary updates for the classic signal timing theories that were established decades ago. Copyright © 2021 by Science Press.},
author_keywords={Coordinated actuated control;  Research review;  Signal timing;  Signal timing software;  Traffic engineering},
document_type={Review},
source={Scopus},
}

@ARTICLE{Zhao2021,
author={Zhao, Y. and Li, L. and Sun, X. and Liu, P. and Grundy, J.},
title={Icon2Code: Recommending code implementations for Android GUI components},
journal={Information and Software Technology},
year={2021},
volume={138},
doi={10.1016/j.infsof.2021.106619},
art_number={106619},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107688670&doi=10.1016%2fj.infsof.2021.106619&partnerID=40&md5=a5ace19bd94adb898f10d9ef181f78cd},
abstract={Context: Event-driven programming plays a crucial role in implementing GUI-based software systems such as Android apps. However, such event-driven code is inherently challenging to design and implement correctly. Despite a significant amount of research to help developers efficiently implement such software, improved approaches are still needed to assist developers in better handling events and associated callback methods. Objective: This work aims at inventing an intelligent recommendation system for helping app developers efficiently and effectively implement Android GUI components. Methods: To achieve the aforementioned objective, we introduce in this work a novel approach called Icon2Code. Given an icon or UI widget provided by designers as input, Icon2Code first searches from a large-scale app database to locate similar icons used in existing popular apps. It then learns from the implementation of these similar apps and leverages a collaborative filtering model to select and recommend the most relevant APIs. Results: Our approach can achieve an 81% success rate when only five recommended APIs are considered, and a 94% success rate if twenty results are considered, based on ten-fold cross-validation with a large-scale dataset containing over 45,000 icons and their code implementations. Conclusion: It is feasible to automatically recommend code implementations for Android GUI components and Icon2Code is useful and effective in helping achieve such an objective. © 2021 Elsevier B.V.},
author_keywords={Android;  API recommendation;  App development;  Collaborative filtering;  Icon implementation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pukas2021568,
author={Pukas, A. and Melnyk, A. and Voytyuk, I. and Smal, V. and Fenovka, V. and Pylypiuk, T.},
title={Implementation of Service-Oriented Architecture for Static and Dynamic Objects Interval Modeling Software},
journal={2021 11th International Conference on Advanced Computer Information Technologies, ACIT 2021 - Proceedings},
year={2021},
pages={568-571},
doi={10.1109/ACIT52158.2021.9548485},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116626125&doi=10.1109%2fACIT52158.2021.9548485&partnerID=40&md5=3c53386e0563a50c876d7b9a66a4ef0a},
abstract={Construction of interval models of static and dynamic systems based on methods of analysis of interval data leads to the formation of common structural elements, based on the specifics of their use for specific subject areas. The use of common structural elements in their software implementation inevitably leads to duplication and redundancy of program code.This problem can be solved by creating a single software package for computer simulation of static and dynamic systems implemented on the algorithms of the bee colony with guaranteed accuracy.This article focuses on the implementation of software architects, which includes requirements analysis, implementation of use cases, as well as information software. The article also describes the specifics of the implementation of service-oriented software architecture, as one of the options for using the developed database of model experiments for use in various subject areas. © 2021 IEEE.},
author_keywords={interval modeling;  Service-Oriented Architecture;  service-oriented interaction;  the software package for interval modelling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kanemaru202145,
author={Kanemaru, S. and Sasaki, Y. and Takahashi, K. and Toyoshima, T.},
title={Design and Implementation of Automatic Generation Method for API Adapter Test Code},
journal={2021 22nd Asia-Pacific Network Operations and Management Symposium, APNOMS 2021},
year={2021},
pages={45-48},
doi={10.23919/APNOMS52696.2021.9562611},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118166232&doi=10.23919%2fAPNOMS52696.2021.9562611&partnerID=40&md5=7a281ab27b726739b62d4b9939e3604f},
abstract={In the Business-to-Business-to-X (B2B2X) business model, the importance of an API orchestrator for constructing and operating services by combining multiple firstB services is increasing. In the development of RESTful API-based services, service providers are required to keep up with changes in specifications of new firstB services and existing services in a short period of time and at low cost. To do so, they need to quickly and inexpensively develop API adapters that can absorb API differences with various services. In this paper, we proposed a method for automating the test code generation while considering the API execution order, input parameter combination, test type (Normal/Semi-Normal), and state transition pattern. We implemented our method using open source software (OSS) and evaluated it from qualitative and quantitative viewpoints. The results show its effectiveness in tests with an actual software development project. © 2021 IEICE.},
author_keywords={API Adapter;  Orchestrator;  Test Automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fink2021,
author={Fink, S.D. and Pfretzschner, M. and Rutter, I.},
title={Experimental comparison of PC-trees and PQ-trees},
journal={Leibniz International Proceedings in Informatics, LIPIcs},
year={2021},
volume={204},
doi={10.4230/LIPIcs.ESA.2021.43},
art_number={43},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115047170&doi=10.4230%2fLIPIcs.ESA.2021.43&partnerID=40&md5=e627500d2c0c6083c93884065fd49ed1},
abstract={PQ-trees and PC-trees are data structures that represent sets of linear and circular orders, respectively, subject to constraints that specific subsets of elements have to be consecutive. While equivalent to each other, PC-trees are conceptually much simpler than PQ-trees; updating a PC-tree so that a set of elements becomes consecutive requires only a single operation, whereas PQ-trees use an update procedure that is described in terms of nine transformation templates that have to be recursively matched and applied. Despite these theoretical advantages, to date no practical PC-tree implementation is available. This might be due to the original description by Hsu and McConnell [14] in some places only sketching the details of the implementation. In this paper, we describe two alternative implementations of PC-trees. For the first one, we follow the approach by Hsu and McConnell, filling in the necessary details and also proposing improvements on the original algorithm. For the second one, we use a different technique for efficiently representing the tree using a Union-Find data structure. In an extensive experimental evaluation we compare our implementations to a variety of other implementations of PQ-trees that are available on the web as part of academic and other software libraries. Our results show that both PC-tree implementations beat their closest fully correct competitor, the PQ-tree implementation from the OGDF library [6, 15], by a factor of 2 to 4, showing that PC-trees are not only conceptually simpler but also fast in practice. Moreover, we find the Union-Find-based implementation, while having a slightly worse asymptotic runtime, to be twice as fast as the one based on the description by Hsu and McConnell. © Simon D. Fink, Matthias Pfretzschner, and Ignaz Rutter; licensed under Creative Commons License CC-BY 4.0},
author_keywords={Circular consecutive ones;  Experimental evaluation;  Implementation;  PC-tree;  PQ-tree},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Garcés2021,
author={Garcés, L. and Martínez-Fernández, S. and Oliveira, L. and Valle, P. and Ayala, C. and Franch, X. and Nakagawa, E.Y.},
title={Three decades of software reference architectures: A systematic mapping study},
journal={Journal of Systems and Software},
year={2021},
volume={179},
doi={10.1016/j.jss.2021.111004},
art_number={111004},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107088983&doi=10.1016%2fj.jss.2021.111004&partnerID=40&md5=9ef67ce307ea298ad2120ca80990b65e},
abstract={Software reference architectures have played an essential role in software systems development due to the possibility of knowledge reuse. Although increasingly adopted by industry, these architectures are not yet completely understood. This work presents a panorama on existing software reference architectures, characterizing them according to their context, goals, perspectives, application domains, design approaches, and maturity, as well as the industry involvement for their construction. For this, we planned and conducted a systematic mapping study. During last decade, the number of reference architectures in very diverse application domains has increased, resulting from efforts of industry, academia, and through their collaborations. Academic reference architectures are oriented to facilitate the reuse of architectural and domain knowledge. The industry has focused on architectures for standardization with certain maturity level. However, the great amount of architectures studied in this work have been designed without following a systematic process, and they lack the maturity to be used in real software projects. Further investigations can be oriented to gathering empirical evidences, from different sources than academic data libraries, that allow to understand how references architectures have been constructed, utilized, and maintained during the whole software life-cycle. © 2021},
author_keywords={Reference architecture;  Secondary study;  Software architecture;  Systematic mapping},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hussain20219433,
author={Hussain, T. and Amin, S. and Zabit, U. and Ayguadé, E.},
title={Implementation of a high-accuracy phase unwrapping algorithm using parallel-hybrid programming approach for displacement sensing using self-mixing interferometry},
journal={Journal of Supercomputing},
year={2021},
volume={77},
number={9},
pages={9433-9453},
doi={10.1007/s11227-021-03634-6},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100752247&doi=10.1007%2fs11227-021-03634-6&partnerID=40&md5=dad9beddd07e543d29a905f8b46f7522},
abstract={Phase unwrapping is an integral part of multiple algorithms with diverse applications. Detailed phase unwrapping is also necessary for achieving high-accuracy metric sensing using laser feedback-based self-mixing interferometry (SMI). Among SMI specific phase unwrapping approaches, a technique called Improved Phase Unwrapping Method (IPUM) provides the highest accuracy. However, due to its complex, sequential, and compute-intensive nature, this method requires a high-performance computing architecture, capable of scalable parallel processing so that such a high-accuracy algorithm can be used for high-bandwidth sensing applications. In this work, the existing sequential IPUM C program is parallelized by using hybrid OpenMP/MPI (Open Multi-Processing/Message Passing Interface) parallel programming models and tested on Barcelona Supercomputing Center Nord-III Supercomputer. The computational performance of the proposed parallel-hybrid IPUM algorithm is compared with existing IPUM sequential code by executing multi-core and uni-core processor architecture, respectively. While comparing the performance of sequential IPUM with the parallel-hybrid IPUM algorithm on 16 nodes of Nord-III supercomputer, the results show that the parallel-hybrid algorithm gets 345.9x times performance improvement as compared to IPUM’s standard, sequential implementation on a single node system. The results show that the parallel-hybrid version of IPUM gives a scalable performance for different target velocities and a different number of processing cores. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
author_keywords={HPC;  Interferometry;  Parallel processing;  Phase unwrapping;  Supercomputing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Son2021279,
author={Son, D.T. and Anh, M.T. and Tu, D.D. and Van Chuong, L. and Cuong, T.H. and Phuong, H.S.},
title={The practice of mapping-based navigation system for indoor robot with RPLIDAR and raspberry Pi},
journal={Proceedings of 2021 International Conference on System Science and Engineering, ICSSE 2021},
year={2021},
pages={279-282},
doi={10.1109/ICSSE52999.2021.9538474},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116232105&doi=10.1109%2fICSSE52999.2021.9538474&partnerID=40&md5=6ea850441c61f258a5fae9e43afb6fa1},
abstract={This paper presents the prototyped design implementation of a mapping mobile robot for an indoor environment. The system design in this paper includes a Raspberry Pi connected to RPLIDAR A1 and other devices like Arduino, encoder. The low-cost mapping mobile robot emerges with features like SLAM which has the capability to form the Map of the environment using Lidar scans and robotic operating system software package to communicate with ROS in the Raspberry Pi using ROS network configurations. Moreover, the indoor map was built by using an open-source algorithm with hector SLAM software package for indoor SLAM, which can get the indoor grid maps in ROS graphical tool RVIZ. The experimental results of the open-source algorithms were instructed to explore the corridor inside a building and do mapping in real-time show that the mobile robot for SLAM is feasible and high-precision grid maps can be constructed. © 2021 IEEE.},
author_keywords={A mapping mobile robot;  Raspberry Pi;  ROS;  SLAM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zeeshan202153,
author={Zeeshan, M.A. and Waris, S.S.},
title={Reverse engineering application instruments and code reliability: A comparative study of tools},
journal={Advances in Transdisciplinary Engineering},
year={2021},
volume={15},
pages={53-63},
doi={10.3233/ATDE210012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116372551&doi=10.3233%2fATDE210012&partnerID=40&md5=4800f0e41c1055a46649bf378f50711c},
abstract={Based on different scenarios (professional guidelines) practice of Software Reverse Engineering (SRE) is used to analyse the combined instructions system to extract information regarding design and implementation of either part of, or the whole software application. These business rules are implemented in the form of a line code whereas actual source code is hidden and only gets the binary form of the code. Technologies that used for reverse engineering are CVF, V7, CFC, 14D, RTR, B#. These instruments are used for a better understanding of the program algorithm, logic, and program specifics in windows API functions, programming assembler language, network interaction principle. The tools that are discussed will not disturb the code consistency and basic structure of software. Present research shows a comparative analysis of various tools to establish which reverse engineering tool is better based on what characteristics. © 2021 The authors and IOS Press.},
author_keywords={Code consistency;  Comparative analysis of tools;  Programming instrument assessment;  Reverse engineering (RE)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ren20211515,
author={Ren, X. and Ye, X. and Xing, Z. and Xia, X. and Xu, X. and Zhu, L. and Sun, J.},
title={KGAMD: An API-misuse detector driven by fine-grained API-constraint knowledge graph},
journal={ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2021},
pages={1515-1519},
doi={10.1145/3468264.3473112},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116288313&doi=10.1145%2f3468264.3473112&partnerID=40&md5=cc0511f5dc19037c094def018b74cfac},
abstract={Application Programming Interfaces (APIs) typically come with usage constraints. The violations of these constraints (i.e. API misuses) can cause significant problems in software development. Existing methods mine frequent API usage patterns from codebase to detect API misuses. They make a naive assumption that API usage that deviates from the most-frequent API usage is a misuse. However, there is a big knowledge gap between API usage patterns and API usage constraints in terms of comprehensiveness, explainability and best practices. Inspired by this, we propose a novel approach named KGAMD (API-Misuse Detector Driven by Fine-Grained API-Constraint Knowledge Graph) that detects API misuses directly against the API constraint knowledge, rather than API usage pat-terns. We first construct a novel API-constraint knowledge graph from API reference documentation with open information extraction methods. This knowledge graph explicitly models two types of API-constraint relations (call-order and condition-checking) and enriches return and throw relations with return conditions and exception triggers. Then, we develop the KGAMD tool that utilizes the knowledge graph to detect API misuses. There are three types of frequent API misuses we can detect-missing calls, missing condition checking and missing exception handling, while existing detectors mostly focus on only missing calls. Our quantitative evaluation and user study demonstrate that our KGAMD is promising in helping developers avoid and debug API misuses Demo Video: https://www.youtube.com/watch?v=TN4LtHJ-494 IntelliJ plug-in: https://github.com/goodchar/KGAMD © 2021 ACM.},
author_keywords={API Misuse;  Java Documentation;  Knowledge Graph},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mao20211661,
author={Mao, X.-J.},
title={A Systematic Review on Software Engineering for Autonomous Robot [自主机器人软件工程的研究综述]},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2021},
volume={44},
number={8},
pages={1661-1678},
doi={10.11897/SP.J.1016.2021.01661},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112101049&doi=10.11897%2fSP.J.1016.2021.01661&partnerID=40&md5=acbc6f3682a544c2e0f096940b4d17e6},
abstract={Autonomous robot is a complex cyber physical system that operates in open environments and has autonomous behaviors. Software that plays an important and core role in such a system provides various critical functions of computations, controls and decisions, in order to drive the robotic systems to operate in a safe, flexible and efficient way. To develop software for autonomous robot is essentially challenged by a number of development complexities resulting from the system itself, operating environments and circumstance constraints. Software engineering for autonomous robot, an emerging interdisciplinary field, aims to offer method, technique and platform supports for the development, running and maintenance of autonomous robot software. It has gained increasing attentions from both academic and industry, and has made great progresses in recent years. The rapid development and increasing demands of autonomous robot software require us to have an in-depth analysis and understanding of the complexities of autonomous robot software and the resulting challenges to develop such complex systems, and a global view of current researches and progresses of software engineering for autonomous robot. Currently there are some surveys on specific topics of software engineering for autonomous robot or the integration of AI and robotics. However, there still lacks of survey literatures to present global and comprehensive views on the software engineering for autonomous robot. The aim of this paper is to present a systematic survey on the researches and practices of software engineering for autonomous robot. We design three research questions to guide the whole survey research, collect more than 300 publications from several academic literature resources such as Google Scholar, ACM Digital library, IEEE Explore, EI Compendex and ISI Web of Science. Based on the defined criteria, we carefully select and ultimately filter more than 80 research papers as references that cover multiple disciplines such as robotics, artificial intelligence, software engineering, etc. Our contributions are threefold. The first is the in-depth analysis of the characteristics and the development complexities of autonomous robot software from external viewpoint of environment, internal viewpoint of system, and real-world viewpoint of circumstance. The second is the systematic reviews on the research progresses of software engineering for autonomous robot from five research branches, including software architecture, construction technology, model-driven development, quality assurance method, and supporting software platform. The research problems, main progresses and typical works of each branch are examined. The third is the detailed discussions about the potential limitations of existing researches and the opportunities of future researches, including foundational theory, domain and requirement engineering, development methodology, technology standardization, and software quality verification and validation. © 2021, Science Press. All right reserved.},
author_keywords={Autonomous robot;  Cyber-physical system;  Open environment;  Software engineering},
document_type={Review},
source={Scopus},
}

@ARTICLE{Huang20211687,
author={Huang, T.-W. and Lin, Y. and Lin, C.-X. and Guo, G. and Wong, M.D.F.},
title={Cpp-Taskflow: A General-Purpose Parallel Task Programming System at Scale},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
year={2021},
volume={40},
number={8},
pages={1687-1700},
doi={10.1109/TCAD.2020.3025075},
art_number={9200667},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091691543&doi=10.1109%2fTCAD.2020.3025075&partnerID=40&md5=eee178fb46e27302a13020c180e74536},
abstract={This article introduces Cpp-Taskflow, a high-performance parallel task programming system, to streamline the building of large and complex parallel applications. Cpp-Taskflow leverages the power of modern C++ and task-based approaches to enable efficient implementations of parallel decomposition strategies. Our programming model can quickly handle not only traditional loop-level parallelism but also irregular patterns, such as graph algorithms and dynamic control flows. Compared with existing libraries, Cpp-Taskflow is more cost efficient in performance scaling and software integration. We have evaluated Cpp-Taskflow on both micro-benchmarks and large-scale design automation problems of million-scale tasking. In a particular timing analysis workload, Cpp-Taskflow outperformed OpenMP by 2 times faster using 2 times fewer lines of code. We have also shown Cpp-Taskflow achieved up to 47.81% speed-up with 28.5% less code over the industrial-strength library, Intel Threading Building Blocks, on a detailed placement problem. © 1982-2012 IEEE.},
author_keywords={Computer-aided design (CAD);  parallel programming systems;  task parallelism},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nielsen202129,
author={Nielsen, B.B. and Torp, M.T. and Møller, A.},
title={Modular call graph construction for security scanning of Node.js applications},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={29-41},
doi={10.1145/3460319.3464836},
art_number={3464836},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111422140&doi=10.1145%2f3460319.3464836&partnerID=40&md5=0e651184a1cb91e49577be2f50c20484},
abstract={Most of the code in typical Node.js applications comes from third-party libraries that consist of a large number of interdependent modules. Because of the dynamic features of JavaScript, it is difficult to obtain detailed information about the module dependencies, which is vital for reasoning about the potential consequences of security vulnerabilities in libraries, and for many other software development tasks. The underlying challenge is how to construct precise call graphs that capture the connectivity between functions in the modules. In this work we present a novel approach to call graph construction for Node.js applications that is modular, taking into account the modular structure of Node.js applications, and sufficiently accurate and efficient to be practically useful. We demonstrate experimentally that the constructed call graphs are useful for security scanning, reducing the number of false positives by 81% compared to npm audit and with zero false negatives. Compared to js-callgraph, the call graph construction is significantly more accurate and efficient. The experiments also show that the analysis time is reduced substantially when reusing modular call graphs. © 2021 ACM.},
author_keywords={JavaScript;  Modularity;  Static analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zahedi2021108,
author={Zahedi, M. and Van Duijnen, R. and Wong, S. and Hamdioui, S.},
title={Tile Architecture and Hardware Implementation for Computation-in-Memory},
journal={Proceedings of IEEE Computer Society Annual Symposium on VLSI, ISVLSI},
year={2021},
volume={2021-July},
pages={108-113},
doi={10.1109/ISVLSI51109.2021.00030},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114963480&doi=10.1109%2fISVLSI51109.2021.00030&partnerID=40&md5=579df3b6567b5805046df3c697137cd0},
abstract={Computation-in-memory (CIM) shows great promise for specific applications by employing emerging (non-volatile) memory technologies such as memristors for both storage and compute, greatly reducing energy consumption, and improving performance. Based on our own observations, we can clearly perceive the contours of a generic approach encompassing the use of a memristor array - using technologies such as PCM and ReRAM. In this paper, we present a new instruction-set architecture (ISA) to control a single CIM-tile that comprises the analog memory array itself and all necessary analog and digital periphery. The newly introduced ISA provides the following advantages: (1) flexibility in programming new CIM functionalities by simply rescheduling the instructions from the ISA, (2) definition of a simulation framework, (3) a hardware implementation of the digital periphery, and (4) a design-space exploration of specific CIM-tile operations targeting the aforementioned technologies. For (1), we defined our own compiler that can translate CIM-tile operations to a sequence of instructions from our ISA. The implementation of the digital periphery is synthesized with the 15 nm Nangate library and results regarding power/energy and area are presented. Finally, the design-space exploration is made possible by using the technology-specific parameters with values that have been verified by accurate technology models. All codes of the compiler and simulator as well as the HDL code of the digital periphery are publicly available. © 2021 IEEE.},
author_keywords={Architecture;  computation in memory;  instruction set;  memristor},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Huang20211335,
author={Huang, W.-P. and Cheung, R.C.C. and Yan, H.},
title={An Efficient Parallel Processor for Dense Tensor Computation},
journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
year={2021},
volume={29},
number={7},
pages={1335-1347},
doi={10.1109/TVLSI.2021.3080318},
art_number={9442854},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107180935&doi=10.1109%2fTVLSI.2021.3080318&partnerID=40&md5=47bd8fafaa9031269abe8df04f1b876f},
abstract={Nowadays, many data are multidimensional, which are called tensors. Tensor computations have been applied in different fields and various software libraries have been developed. However, not much attention has been received for developing a hardware architecture to accelerate the tensor computations. In this article, an efficient and unified processing element (PE) array for the 3-D tensor computation is demonstrated. Our PE array is optimized for thin and tall tensor-matrix multiplication and two types of tensor times matrices chain (TTMc) operations. Our design is evaluated in three study cases and compared with the state-of-the-art design. By using computation partition and rearrangement, data movement between the field-programmable gate array (FPGA) and off-chip DDR memory can be reduced by O(I2), where I is the maximum range among all the dimensions of the data tensor. For TTMc implementation, clock frequency has been increased by 18% compared with the state-of-the-art implementation on the same FPGA chip. An experiment on 3-D volumetric data set rendering by tensor approximation method is conducted for demonstration. For the bricks reconstruction process, the runtime decreased by 50%, i.e., two times faster, on our FPGA implementation compared with that running on GPU. In CANDECOMP/PARAFAC decomposition, for one iteration, the runtime has been decreased by up to 93% compared with the programs implemented by Tensorly, which is a python library. © 1993-2012 IEEE.},
author_keywords={Field-programmable gate array (FPGA);  hardware architecture;  parallel processor;  tensor computation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Boguslawski2021,
author={Boguslawski, K. and Leszczyk, A. and Nowak, A. and Brzęk, F. and Żuchowski, P.S. and Kędziera, D. and Tecmer, P.},
title={Pythonic Black-box Electronic Structure Tool (PyBEST). An open-source Python platform for electronic structure calculations at the interface between chemistry and physics},
journal={Computer Physics Communications},
year={2021},
volume={264},
doi={10.1016/j.cpc.2021.107933},
art_number={107933},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102612061&doi=10.1016%2fj.cpc.2021.107933&partnerID=40&md5=442d34ff82a75f8b11c03462b44d278c},
abstract={Pythonic Black-box Electronic Structure Tool (PYBEST) represents a fully-fledged modern electronic structure software package developed at Nicolaus Copernicus University in Toruń. The package provides an efficient and reliable platform for electronic structure calculations at the interface between chemistry and physics using unique electronic structure methods, analysis tools, and visualization. Examples are the (orbital-optimized) pCCD-based models for ground- and excited-states electronic structure calculations as well as the quantum entanglement analysis framework based on the single-orbital entropy and orbital-pair mutual information. PYBEST is written primarily in the Python programming language with additional parts written in C＋＋, which are interfaced using Pybind11, a lightweight header-only library. By construction, PYBEST is easy to use, to code, and to interface with other software packages. Moreover, its modularity allows us to conveniently host additional Python packages and software libraries in future releases to enhance its performance. The electronic structure methods available in PYBEST are tested for the half-filled 1-D model Hamiltonian. The capability of PYBEST to perform large-scale electronic structure calculations is demonstrated for the model vitamin B12 compound. The investigated molecule is composed of 190 electrons and 777 orbitals for which an orbital optimization within pCCD and an orbital entanglement and correlation analysis are performed for the first time. Program summary: Program title: PyBEST CPC Library link to program files: https://doi.org/10.17632/xf9kb7yfwr.1 Developer's repository link: https://zenodo.org/record/3925278#.X5KAZS8Rq6s Licensing provisions: GNU General Public License 3 Programming language: Python, C＋＋ Nature of problem: Efficient and reliable modeling of electronic structures featuring both weakly- and strongly-correlated electrons. Small- and large-scale quantum-mechanical problems at the interface between chemistry and physics comprising both quantum chemical and model Hamiltonians. Specifically, modeling potential energy surfaces of complex electronic structures including bond breaking/formation, elucidating complex electronic structures through the picture of interacting orbitals, describing noncovalent interactions, ultra-cold trapped quantum gases, and a variety of applications in interdisciplinary quantum mechanical-based problems. Solution method: Modular implementation of a series of unconventional (and conventional) electronic structure models based on the pCCD ansatz to solve the electronic Schrödinger equation. These include the description of both ground- and excited-states, the determination of interaction energies, and the analysis and interpretation of electronic wavefunctions. All modules are implemented in the modern Python programming language, where bottleneck operations are handled by C＋＋ code interfaced by the Pybind11 header-only library. The implemented (wavefunction) modules and modular code structure make PYBEST a very efficient alternative to existing electronic structure packages. Additional comments including restrictions and unusual features: PYBEST features unconventional electronic structure methods (pCCD and post-pCCD methods) that are not available in any other quantum chemistry/physics software package. It also includes a general orbital entanglement and correlation module that supports both pCCD and selected post-pCCD methods. PyBEST is designed to be easy to use and code in. Due to its modularity (for instance of the tensor contraction engine), new Python modules and features can be straightforwardly imported and exploited without changing any wavefunction modules directly. References: http://pybest.fizyka.umk.pl © 2021 Elsevier B.V.},
author_keywords={Coupled cluster theory;  C＋＋;  Many-body perturbation theory;  Python;  Quantum many-body software package;  Quantum physics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Singh2021441,
author={Singh, M. and Singh, G. and Singh, J. and Kumar, Y.},
title={Design and Validation of Wearable Smartphone Based Wireless Cardiac Activity Monitoring Sensor},
journal={Wireless Personal Communications},
year={2021},
volume={119},
number={1},
pages={441-457},
doi={10.1007/s11277-021-08219-3},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102207520&doi=10.1007%2fs11277-021-08219-3&partnerID=40&md5=fc6f742776358c7b494798a504b62ca8},
abstract={Due to technological advancements in electronics industry, wireless sensors in conjunction with mobile phones can be used anytime anywhere for ubiquitous healthcare applications. This paper presents the design and implementation of convenient and efficient method to display the visualization of real time ECG signal transmitted wirelessly using developed ECG sensor on smartphone. The proposed light weight, wearable and affordable system can be used by patients having persistent heart diseases for preliminary self-recognition. The sensor output is analyzed by calculating percentage error for R–R interval of acquired ECG waveform and heart rate using DALE technologies ECG simulator by setting it at 30, 60, 120 and 240 beats per minute (bpm). Sensor shows 100 % accuracy in heart rate validation at 30 and 60 bpm alongwith 99.8 % and 98.8 % accuracy at 120 and 240 bpm respectively. For R–R interval evaluation, it shows 100 % accuracy at 30 and 60 bpm whereas at 120 and 240 bpm accuracy remains at 98.00 % and 96.048 %. Clinical validation has been performed by comparing traces of developed prototype ECG sensor with Recorders and Medicare Systems commercial multilead ECG machine. It shows that the acquired QRS peak of developed ECG sensor is clear and of high quality with no visible noise superimposed on the ECG signal when compared with commercial multilead ECG machine. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
author_keywords={Application program interface (API);  Bluetooth (BT);  Dalvik virtual machine (DVM);  Electrocardiography (ECG);  Software development kit (SDK)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Neumann2021957,
author={Neumann, A. and Laranjeiro, N. and Bernardino, J.},
title={An Analysis of Public REST Web Service APIs},
journal={IEEE Transactions on Services Computing},
year={2021},
volume={14},
number={4},
pages={957-970},
doi={10.1109/TSC.2018.2847344},
art_number={8385157},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048566194&doi=10.1109%2fTSC.2018.2847344&partnerID=40&md5=1890d66563648220faaaffc1ce3b813f},
abstract={Businesses are increasingly deploying their services on the web, in the form of web applications, SOAP services, message-based services, and, more recently, REST services. Although the movement towards REST is widely recognized, there is not much concrete information regarding the technical features being used in the field, such as typical data formats, how HTTP verbs are being used, or typical URI structures, just to name a few. In this paper, we go through the Alexa.com top 4000 most popular sites to identify precisely 500 websites claiming to provide a REST web service API. We analyze these 500 APIs for key technical features, degree of compliance with REST architectural principles (e.g., resource addressability), and for adherence to best practices (e.g., API versioning). We observed several trends (e.g., widespread JSON support, software-generated documentation), but, at the same time, high diversity in services, including differences in adherence to best practices, with only 0.8 percent of services strictly complying with all REST principles. Our results can help practitioners evolve guidelines and standards for designing higher quality services and also understand deficiencies in currently deployed services. Researchers may also benefit from the identification of key research areas, contributing to the deployment of more reliable services. © 2008-2012 IEEE.},
author_keywords={API;  HTTP;  REST;  RESTful;  web;  web services;  web services analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chesnokov2021165,
author={Chesnokov, A. and Mikhailov, V. and Dolmatov, I.},
title={Learning activities for mastering flexible roof constructions},
journal={Proceedings - 2021 1st International Conference on Technology Enhanced Learning in Higher Education, TELE 2021},
year={2021},
pages={165-170},
doi={10.1109/TELE52840.2021.9482484},
art_number={9482484},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112109202&doi=10.1109%2fTELE52840.2021.9482484&partnerID=40&md5=ed7270a1ec334ddd9c109cb60ba9c53c},
abstract={Flexible roofs become an integral part of modern building constructions. Being widely used for public buildings, they have great potential in the field of industrial construction as well. Flexible roofs, however, possess complex structural behavior. They require deep understanding of the key concepts by designers and project managers, who implement them into a real life. Civil engineering and architectural specializations need to incorporate flexible constructions into the academic activity. It will ensure competitiveness of prospective specialists in the job market. Active learning approach is considered for mastering flexible roof structures. Prototyping is an essential tool for long-term memorization of the key concepts. Modern computer technologies and specialized software packages, however, gradually displace laboratory modeling. Structural deformability, geometrically nonlinear behavior, preliminary stressing and equilibrium structural shape can be easily grasped by means of computer simulation and numerical investigation of exemplary constructions. © 2021 IEEE.},
author_keywords={Active learning;  Cable and membrane constructions;  Computer simulation;  Professional competence},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Coti2021,
author={Coti, C. and Malony, A.D.},
title={DiPOSH: A portable OpenSHMEM implementation for short API-to-network path},
journal={Concurrency and Computation: Practice and Experience},
year={2021},
volume={33},
number={11},
doi={10.1002/cpe.6179},
art_number={e6179},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100357023&doi=10.1002%2fcpe.6179&partnerID=40&md5=d7c6162b154e9a0ad2e53e8e69f9fe03},
abstract={In this article, we introduce DiPOSH, a multi-network, distributed implementation of the OpenSHMEM standard. The core idea behind DiPOSH is to have an API-to-network software stack as slim as possible, in order to minimize the software overhead. Following the heritage of its non-distributed parent POSH, DiPOSH's communication engine is organized around the processes' shared heaps, and remote communications are moving data from and to these shared heaps directly. This article presents its architecture and several communication drivers, including one that takes advantage of a helper process, called the Hub, for inter-process communications. This architecture allows use to explore different options for implementing the communication drivers, from using high-level, portable, optimized libraries to low-level, close to the hardware communication routines. We present the perspectives opened by this additional component in terms of communication scheduling between and on the nodes. DiPOSH is available at https://github.com/coti/DiPOSH. © 2021 John Wiley & Sons, Ltd.},
author_keywords={distributed run-time environment;  high-performance communication library;  OpenSHMEM},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kotyra2021,
author={Kotyra, B. and Chabudziński and Stpiczyński, P.},
title={High-performance parallel implementations of flow accumulation algorithms for multicore architectures},
journal={Computers and Geosciences},
year={2021},
volume={151},
doi={10.1016/j.cageo.2021.104741},
art_number={104741},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102651060&doi=10.1016%2fj.cageo.2021.104741&partnerID=40&md5=530ad52b65b6dc6db2e3128ba9afe042},
abstract={The calculation of flow accumulation is one of the tasks in digital terrain analysis that is not easy to parallelize. The aim of this work was to develop new, faster ways to calculate flow accumulation and achieve shorter execution times than popular software tools for this purpose. We prepared six implementations of algorithms based on both top-down and bottom-up approaches and compared their performance using 118 different data sets (including 59 subcatchments and 59 full frames) of various sizes but the same area and resolution. Our results clearly show that the parallel top-down algorithm (without the use of OpenMP tasks) is the most suitable implementation for flow accumulation calculations of all we have tested. The mean and median execution times of this algorithm are the shortest in all cases studied. The implementation is characterized by high speedups. The execution times of the parallel top-down implementation are two orders of magnitude shorter compared to the Flow Accumulation tool from ArcGIS Desktop. This is important, considering the performance of popular GIS platforms, where it takes hours to perform the same kind of operations with the use of similar equipment. © 2021 Elsevier Ltd},
author_keywords={Flow accumulation;  GIS;  Manycore architectures;  Multicore processors;  OpenMP;  Parallel algorithms},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sadi2021185,
author={Sadi, M.H. and Yu, E.},
title={RAPID: a knowledge-based assistant for designing web APIs},
journal={Requirements Engineering},
year={2021},
volume={26},
number={2},
pages={185-236},
doi={10.1007/s00766-020-00342-0},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100507276&doi=10.1007%2fs00766-020-00342-0&partnerID=40&md5=12c35a96011787355ebf27e6ee286167},
abstract={With the rise in initiatives such as software ecosystems and Internet of Things (IoT), developing web Application Programming Interfaces (web APIs) has become an increasingly common practice. One main concern in developing web APIs is that they expose back-end systems and data toward clients. This exposure threatens critical non-functional requirements, such as the security of back-end systems, the performance of provided services, and the privacy of communications with clients. Although dealing with non-functional requirements during software design has been long studied, there is still no framework to specifically assist software developers in addressing these requirements in web APIs. In this paper, we introduce Rational API Designer (RAPID), an open-source assistant that advises on designing non-functional requirements in the architecture of web APIs. We have equipped RAPID with a broad range of expert knowledge about API design, systematically collected and extracted from the literature. The API design knowledge has been encoded as a set of 156 rules using the Non-Functional Requirements (NFR) multi-valued logic, a formal framework commonly used to describe non-functional and functional requirements of software systems. RAPID uses the encoded knowledge in a stepwise inference procedure to arrive from a given requirement, to a set of design alternatives to a final recommendation for a given API design specification. Seven well-experienced software engineers have blindly evaluated the accuracy of RAPID’s consultations over seven different cases of web API design and on providing design guidelines for thirty design questions. The results of the evaluation show that RAPID’s recommendations meet acceptable standards of the majority of the evaluators 73.3% of the time. Moreover, analysis of the evaluators’ comments suggests that more than one-third of the unacceptable ratings (33.8%) given to RAPID’s answers are due to valid but incomplete design guidelines. We thus expect that the accuracy of the consultations will increase as RAPID’s knowledge of API design is extended and refined. © 2021, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Automated design generation;  Chatbots;  Knowledge-based question answering;  Non-functional requirements;  Recommendation systems;  Search-based software engineering;  Software design and architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saddik2021,
author={Saddik, A. and Latif, R. and Elhoseny, M. and El Ouardi, A.},
title={Real-time evaluation of different indexes in precision agriculture using a heterogeneous embedded system},
journal={Sustainable Computing: Informatics and Systems},
year={2021},
volume={30},
doi={10.1016/j.suscom.2020.100506},
art_number={100506},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099830112&doi=10.1016%2fj.suscom.2020.100506&partnerID=40&md5=0f8a3cd8877d563d2860d47832a8cdb8},
abstract={In this work, we present a real-time embedded implementation of an algorithm dedicated to monitoring agricultural fields. This algorithm is based on normalized indices, such as the Normalized Difference Vegetation Index (NDVI) and the Normalized Difference Water Index (NDWI). The problem of most algorithms in this context is real-time processing, especially when we talk about applications that require time precision. The proposed implementation is based on the application of a Hardware/Software Co-design approach. For the embedded platform, we used the heterogeneous system contains CPU and GPU type XU4 and TX1. In this context, we used the parallel programming language OpenMP to have an optimal embedded implementation. The results showed that we could process 66 images/s using a desktop, 20 images/s in the XU4, and 17 images/s for TX1. © 2020 Elsevier Inc.},
author_keywords={Hardware/software co-design;  Heterogeneous system;  NDVI;  NDWI;  OpenMP;  Precision agriculture;  Real-time},
document_type={Article},
source={Scopus},
}

@ARTICLE{Obukhov20217249,
author={Obukhov, A.D. and Krasnyanskiy, M.N.},
title={Automated organization of interaction between modules of information systems based on neural network data channels},
journal={Neural Computing and Applications},
year={2021},
volume={33},
number={12},
pages={7249-7269},
doi={10.1007/s00521-020-05491-5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096019619&doi=10.1007%2fs00521-020-05491-5&partnerID=40&md5=03bef5cedc3c10e612650f3b3c8598f4},
abstract={The automation of the process of information systems construction is an important and urgent problem, as it allows reducing the negative influence of a person during decision making and developing software, releasing additional time and material resources to solve more complex and creative problems. Most modern information systems are developed on a modular base; therefore, a significant design stage is the implementation of links between system components. The purpose of the study is to automate the organization of intermodular interaction in information systems, which will reduce the complexity, time and cost of this implementing process. In order to achieve the result, a method is proposed for the organization of interaction between modules of information systems based on neural network data channels, realized within the general concept of a neural network architecture. The structure of neural network channels, the principles of their functioning, theoretical substantiation, mathematical and algorithmic support and area of application are considered in detail. A classification of neural network channels is presented, based on two of their characteristics: categories and degrees. As a result of the conducted research, the practical implementation of two neural network data channels is realized (transmission and adaptation), the structure of the program code, the used tools and libraries are analyzed. Based on a set of metrics for the complexity of the program code (Halstead, Jilb), the estimation of the computational complexity of algorithms, time and material costs for implementation, a comparative analysis of neural network data transmission channels and adaptation with classical approaches in the form of a set of network data transmission protocol and the required algorithmic support for data processing is carried out. The obtained experimental results confirm the lower complexity of neural network channels (reduction by at least 20% according to Halstead metrics and cyclometric complexity), reduction in time (by 12–32%) and cost (by 36–63%) of implementation and increase in the accuracy of the problem solving (by 11.8–15.5%). This demonstrates the effectiveness of using neural network data channels to automate the organization of intermodular interaction in information systems. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Adaptation and transmission of information;  Automation of intermodular interaction;  Machine learning;  Neural network architecture},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jia20212,
author={Jia, J. and Wang, B. and Gong, N.Z.},
title={Robust and Verifiable Information Embedding Attacks to Deep Neural Networks via Error-Correcting Codes},
journal={ASIA CCS 2021 - Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
year={2021},
pages={2-13},
doi={10.1145/3433210.3437519},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108074033&doi=10.1145%2f3433210.3437519&partnerID=40&md5=ce2721c6446885a73653eab9f9108fe9},
abstract={In the era of deep learning, a user often leverages a third-party machine learning tool to train a deep neural network (DNN) classifier and then deploys the classifier as an end-user software product (e.g., a mobile app) or a cloud service. In an information embedding attack, an attacker is the provider of a malicious third-party machine learning tool. The attacker embeds a message into the DNN classifier during training and recovers the message via querying the API of the black-box classifier after the user deploys it. Information embedding attacks have attracted growing attention because of various applications such as watermarking DNN classifiers and compromising user privacy. State-of-the-art information embedding attacks have two key limitations: 1) they cannot verify the correctness of the recovered message, and 2) they are not robust against post-processing (e.g., compression) of the classifier. In this work, we aim to design information embedding attacks that are verifiable and robust against popular post-processing methods. Specifically, we leverage Cyclic Redundancy Check to verify the correctness of the recovered message. Moreover, to be robust against post-processing, we leverage Turbo codes, a type of error-correcting codes, to encode the message before embedding it to the DNN classifier. In order to save queries to the deployed classifier, we propose to recover the message via adaptively querying the classifier. Our adaptive recovery strategy leverages the property of Turbo codes that supports error correcting with a partial code. We evaluate our information embedding attacks using simulated messages and apply them to three applications (i.e., training data inference, property inference, DNN architecture inference), where messages have semantic interpretations. We consider 8 popular methods to post-process the classifier. Our results show that our attacks can accurately and verifiably recover the messages in all considered scenarios, while state-of-the-art attacks cannot accurately recover the messages in many scenarios. © 2021 ACM.},
author_keywords={error-correcting code;  information embedding attacks;  machine learning security},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Razavi2021,
author={Razavi, F. and Raminfard, S. and Kalantar Hormozi, H. and Sisakhti, M. and Batouli, S.A.H.},
title={A Probabilistic Atlas of the Pineal Gland in the Standard Space},
journal={Frontiers in Neuroinformatics},
year={2021},
volume={15},
doi={10.3389/fninf.2021.554229},
art_number={554229},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107065273&doi=10.3389%2ffninf.2021.554229&partnerID=40&md5=a7d372e76f2cb7aee6a3e9d163fed28a},
abstract={Pineal gland (PG) is a structure located in the midline of the brain, and is considered as a main part of the epithalamus. There are numerous reports on the facilitatory role of this area for brain function; hormone secretion and its role in sleep cycle are the major reports. However, reports are rarely available on the direct role of this structure in brain cognition and in information processing. A suggestion for the limited number of such studies is the lack of a standard atlas for the PG; none of the available MRI templates and atlases has provided parcellations for this structure. In this study, we used the three-dimensional (3D) T1-weighted MRI data of 152 healthy young volunteers, and provided a probabilistic map of the PG in the standard Montreal Neurologic Institute (MNI) space. The methods included collecting the data using a 64-channel head coil on a 3-Tesla Prisma MRI Scanner, manual delineation of the PG by two experts, and robust template and atlas construction algorithms. This atlas is freely accessible, and we hope importing this atlas in the well-known neuroimaging software packages would help to identify other probable roles of the PG in brain function. It could also be used to study pineal cysts, for volumetric analyses, and to test any associations between the cognitive abilities of the human and the structure of the PG. © Copyright © 2021 Razavi, Raminfard, Kalantar Hormozi, Sisakhti and Batouli.},
author_keywords={MNI space;  MRI;  normal template;  pineal gland;  probabilistic atlas},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hua202160,
author={Hua, S.},
title={Applications of cloud desktop in library information system construction},
journal={Proceedings - 2021 International Symposium on Artificial Intelligence and its Application on Media, ISAIAM 2021},
year={2021},
pages={60-65},
doi={10.1109/ISAIAM53259.2021.00019},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115098900&doi=10.1109%2fISAIAM53259.2021.00019&partnerID=40&md5=47cc9ba8176b4638d70ceb5e6f46423d},
abstract={As a new technology in the internet era, cloud desktop has been widely used in library information system construction. The applications of cloud desktop can not only enhance the reader experience, improve the service quality, ensure the information security of readers, but also effectively improve the efficiency and level of library management. This paper analyses the hardware and software requirements of the cloud desktop information system of university library, gives the overall architecture of the cloud desktop library information system, and selects the IDV scheme after comparison. IDV scheme has the advantages of smooth operation, good experience, energy saving and environmental protection and provide more convenient and high-quality information service for teachers and students. © 2021 IEEE.},
author_keywords={Cloud desktop;  Information system;  Library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ramesh202135,
author={Ramesh, S. and Malony, A.D. and Carns, P. and Ross, R.B. and Dorier, M. and Soumagne, J. and Snyder, S.},
title={SYMBIOSYS: A methodology for performance analysis of composable HPC data services},
journal={Proceedings - 2021 IEEE 35th International Parallel and Distributed Processing Symposium, IPDPS 2021},
year={2021},
pages={35-45},
doi={10.1109/IPDPS49936.2021.00013},
art_number={9460467},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113483511&doi=10.1109%2fIPDPS49936.2021.00013&partnerID=40&md5=d7c116aa8cef80bcb7c3fc65aa4f2659},
abstract={Microservices are a powerful new way of building, customizing, and deploying distributed services owing to their flexibility and maintainability. Several large-scale distributed platforms have emerged to serve the growing needs of data-centric workloads and services in commercial computing. Concurrently, high-performance computing (HPC) systems and software are rapidly evolving to meet the demands of diversified applications and heterogeneity. The interplay of hardware factors, software configuration parameters, and the flexibility offered with a microservice architecture makes it nontrivial to estimate the optimal service instantiation for a given application workload. Further, this problem is exacerbated when considering that these services operate in a dynamic and heterogeneous HPC environment. An optimally integrated service can be vastly more performant than a haphazardly integrated one. Existing performance tools for HPC either fail to understand the request-response model of communication inherent to microservices or they operate within a narrow scope, limiting the insight that can be gleaned from employing them in isolation.We propose a methodology for integrated performance analysis of HPC microservices frameworks and applications called SYMBIOSYS. We describe its design and implementation within the context of the Mochi framework. This integration is achieved by combining distributed callpath profiling and tracing with a performance data exchange strategy that collects fine-grained, low-level metrics from the RPC communication library and network layers. The result is a portable, low-overhead performance analysis setup that provides a holistic profile of the dependencies among microservices and how they interact with the Mochi RPC software stack. Using HEPnOS, a production-quality Mochi data service, we demonstrate the low-overhead operation of SYMBIOSYS at scale and use it to identify the root causes of poorly performing service configurations. © 2021 IEEE.},
author_keywords={Microservices;  Performance;  Storage;  Tools},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Morado20212026,
author={Morado, J. and Mortenson, P.N. and Verdonk, M.L. and Ward, R.A. and Essex, J.W. and Skylaris, C.-K.},
title={ParaMol: A Package for Automatic Parameterization of Molecular Mechanics Force Fields},
journal={Journal of Chemical Information and Modeling},
year={2021},
volume={61},
number={4},
pages={2026-2047},
doi={10.1021/acs.jcim.0c01444},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104968420&doi=10.1021%2facs.jcim.0c01444&partnerID=40&md5=c002a56e9b5bbad7f838f347cde8d692},
abstract={The ensemble of structures generated by molecular mechanics (MM) simulations is determined by the functional form of the force field employed and its parameterization. For a given functional form, the quality of the parameterization is crucial and will determine how accurately we can compute observable properties from simulations. While accurate force field parameterizations are available for biomolecules, such as proteins or DNA, the parameterization of new molecules, such as drug candidates, is particularly challenging as these may involve functional groups and interactions for which accurate parameters may not be available. Here, in an effort to address this problem, we present ParaMol, a Python package that has a special focus on the parameterization of bonded and nonbonded terms of druglike molecules by fitting to ab initio data. We demonstrate the software by deriving bonded terms' parameters of three widely known drug molecules, viz. aspirin, caffeine, and a norfloxacin analogue, for which we show that, within the constraints of the functional form, the methodologies implemented in ParaMol are able to derive near-ideal parameters. Additionally, we illustrate the best practices to follow when employing specific parameterization routes. We also determine the sensitivity of different fitting data sets, such as relaxed dihedral scans and configurational ensembles, to the parameterization procedure, and discuss the features of the various weighting methods available to weight configurations. Owing to ParaMol's capabilities, we propose that this software can be introduced as a routine step in the protocol normally employed to parameterize druglike molecules for MM simulations. © 2021 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Belova-Ploniene2021,
author={Belova-Ploniene, D. and Katkevicius, A.},
title={Analysis of Frequency Characteristics of Meander Structures with Different Connecting Electrodes},
journal={2020 IEEE 8th Workshop on Advances in Information, Electronic and Electrical Engineering, AIEEE 2020 - Proceedings},
year={2021},
doi={10.1109/AIEEE51419.2021.9435773},
art_number={9435773},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107602412&doi=10.1109%2fAIEEE51419.2021.9435773&partnerID=40&md5=9c5270b64dd1414eb2a2b276066f4d38},
abstract={Meander structures are often used in microwave devices because of small dimensions and positive features. The features of the meander structures like passband, characteristic impedance, delay time and others could be adjusted by using different materials or structures of the meander conductor. The construction of the connecting electrodes of the meander conductor has a significant impact to the operation of the meander structure. The frequency characteristics of the planar and hybrid meander structures are calculated using the method of moment (MoM) in the Sonnet® software package. The created meander structure with different connecting electrodes are lossless. The results of passband, input impedance, delay time and electric field distribution are discussed. Different sets of design parameters are presented in order to obtain the same electrical characteristics. © 2021 IEEE.},
author_keywords={frequency characteristics;  hybrid structures;  meander structures;  method of moments},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tazaki202115,
author={Tazaki, H. and Moroo, A. and Kuga, Y. and Nakamura, R.},
title={How to design a library OS for practical containers?},
journal={VEE 2021 - Proceedings of the 17th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
year={2021},
pages={15-28},
doi={10.1145/3453933.3454011},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104659873&doi=10.1145%2f3453933.3454011&partnerID=40&md5=7c32973e939d9a6d262efa530d795192},
abstract={Container engines with operating-system virtualization have been widely used and now offer extensions to replace core functionalities that are derived from the host kernel. Because such extensions with an alternate kernel, which is often implemented in a library operating system (libOS), can be designed to have free choice, developers are tempted to take a clean-slate approach, i.e., implement the kernels from scratch. However, this design decision makes it difficult to cover broad features of the original Linux kernel, and some application programs may not work on such kernels. Precise emulation of the huge codebase and rich feature set of the Linux kernel is not easily possible. In this paper, we have tried to improve the level of compatibility in a libOS by using the source code of the Linux kernel as the container kernel. We present μKontainer, an alternate container kernel based on a libOS by extending the existing open-source software, Linux Kernel Library, while preserving the lightweight property of conventional containers. We have studied the level of compatibility with the conformance tests of network protocol implementation of nine different libOSs, and μKontainer performs identically like the Linux kernel. The network-related benchmark shows mostly comparable results with a conventional container and a native Linux host; in the best case, the goodput of the short-sized packet is up to 84% faster than that of a native Linux host. This paper sheds light on the design space of the libOS when we introduced the extended container kernel. © 2021 ACM.},
author_keywords={anykernel;  library OS;  unikernels},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hughes2021,
author={Hughes, D. and Birkinshaw, S. and Parkin, G.},
title={A method to include reservoir operations in catchment hydrological models using SHETRAN},
journal={Environmental Modelling and Software},
year={2021},
volume={138},
doi={10.1016/j.envsoft.2021.104980},
art_number={104980},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100417135&doi=10.1016%2fj.envsoft.2021.104980&partnerID=40&md5=3194df6fc1f6418c5ffd1b96e1965af4},
abstract={Reservoir construction and operation have significant impacts on catchment hydrology, flood risk and fluvial processes. However, few available hydrological modelling packages can simulate complex, dynamic, manually-operated reservoir control structures. We present SHETRAN-Reservoir, a physically-based spatially distributed modelling tool to simulate catchment hydrology, including reservoir operations. We also propose a method for deriving parsimonious reservoir operation rules from real-world observations. Application of SHETRAN-Reservoir to the Upper Cocker catchment in the Lake District National Park, UK, is shown to improve modelling of hydrological response. Modelling combined climate change and water resource management scenarios demonstrates the influence of operational control rules on hydrological impacts, especially during droughts. We discuss how SHETRAN-Reservoir can be applied to other reservoir-containing catchments to guide decisions concerning water resources, ecology and flood risk. We also discuss potential future software developments. © 2021 The Authors},
author_keywords={Climate change;  Hydrology;  Reservoir management;  Reservoir operations;  SHETRAN;  Water resources},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2021653,
author={Lee, S. and Wu, R. and Cheung, S.-C. and Kang, S.},
title={Automatic Detection and Update Suggestion for Outdated API Names in Documentation},
journal={IEEE Transactions on Software Engineering},
year={2021},
volume={47},
number={4},
pages={653-675},
doi={10.1109/TSE.2019.2901459},
art_number={8651318},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062153013&doi=10.1109%2fTSE.2019.2901459&partnerID=40&md5=b4f37d930e990037fe5affaefb896271},
abstract={Application programming interfaces (APIs) continually evolve to meet ever-changing user needs, and documentation provides an authoritative reference for their usage. However, API documentation is commonly outdated because nearly all of the associated updates are performed manually. Such outdated documentation, especially with regard to API names, causes major software development issues. In this paper, we propose a method for automatically updating outdated API names in API documentation. Our insight is that API updates in documentation can be derived from API implementation changes between code revisions. To evaluate the proposed method, we applied it to four open source projects. Our evaluation results show that our method, FreshDoc, detects outdated API names in API documentation with 48 percent higher accuracy than the existing state-of-the-art methods do. Moreover, when we checked the updates suggested by FreshDoc against the developers' manual updates in the revised documentation, FreshDoc detected 82 percent of the outdated names. When we reported 40 outdated API names found by FreshDoc via issue tracking systems, developers accepted 75 percent of the suggestions. These evaluation results indicate that FreshDoc can be used as a practical method for the detection and updating of API names in the associated documentation. © 1976-2012 IEEE.},
author_keywords={Application programming interfaces;  documentation;  history;  software maintenance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rodrigues2021244,
author={Rodrigues, F. and Antunes, F. and Matos, R.},
title={Safety plugins for risks prevention through design resourcing BIM},
journal={Construction Innovation},
year={2021},
volume={21},
number={2},
pages={244-258},
doi={10.1108/CI-12-2019-0147},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094145611&doi=10.1108%2fCI-12-2019-0147&partnerID=40&md5=cd82f82574f5f570f33e33e013f0323f},
abstract={Purpose: The use of building information modelling (BIM) methodology has been increasing in the architecture, engineering, construction and operation sector, driven to a new paradigm of work with the use of three-dimensional (3D) parametric models. However, building information modelling (BIM) has been mostly used for as-built models of a building, not yet been widely used by designers during project and construction phases for occupational risks prevention and safety planning. This paper aims to show the capacity of developing tools that allow adding functionalities to Revit software to improve safety procedures and reduce the time spent on modelling them during the design phase. Design/methodology/approach: To reach this objective, a structural 3D model of a building is used to validate the developed tools. A plugin prototype based on legal regulations was developed, allowing qualitative safety assessment through the application of job hazard analysis (JHA), SafeObject and checklists. These tools allow the automated detection of falls from height situations and the automated placement of the correspondent safety systems. Findings: Revit application programming interface allowed the conception and addition of several functionalities that can be used in BIM methodology, and more specifically in the prevention of occupational risks in construction, contributing this paper to the application of a new approach to the prevention through design. Originality/value: This paper is innovative and important because the developed plugins allowed: automated detection of potential falls from heights in the design stage; automated introduction of safety objects from a BIM Safety Objects Library; and the intercommunication between a BIM model and a safety database, bringing JHA integration directly on the project. The prototype of this work was validated for fall from height hazards but can be extended to other potentials hazards since the initial design stage. © 2020, Emerald Publishing Limited.},
author_keywords={Accidents;  Automated hazards detection;  BIM;  Building;  Building information modelling;  Civil engineering and surveying;  Construction 4.0;  Construction technology;  Fall hazard;  Health and safety;  Prevention;  Risk assessment/risk management;  Safety},
document_type={Article},
source={Scopus},
}

@ARTICLE{Balaban2021,
author={Balaban, G. and Grytten, I. and Rand, K.D. and Scheffer, L. and Sandve, G.K.},
title={Ten simple rules for quick and dirty scientific programming},
journal={PLoS Computational Biology},
year={2021},
volume={17},
number={3},
doi={10.1371/journal.pcbi.1008549},
art_number={e1008549},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102622744&doi=10.1371%2fjournal.pcbi.1008549&partnerID=40&md5=94e2278e3239de9fe63346b27612f02c},
document_type={Review},
source={Scopus},
}

@ARTICLE{Raza20211,
author={Raza, S.M. and Jeong, J. and Kim, M. and Kang, B. and Choo, H.},
title={Empirical performance and energy consumption evaluation of container solutions on resource constrained iot gateways},
journal={Sensors (Switzerland)},
year={2021},
volume={21},
number={4},
pages={1-18},
doi={10.3390/s21041378},
art_number={1378},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101053814&doi=10.3390%2fs21041378&partnerID=40&md5=4e44544d1c97076d4b5be9520f0c6b6c},
abstract={Containers virtually package a piece of software and share the host Operating System (OS) upon deployment. This makes them notably light weight and suitable for dynamic service deployment at the network edge and Internet of Things (IoT) devices for reduced latency and energy consumption. Data collection, computation, and now intelligence is included in variety of IoT devices which have very tight latency and energy consumption conditions. Recent studies satisfy latency condition through containerized services deployment on IoT devices and gateways. They fail to account for the limited energy and computing resources of these devices which limit the scalability and concurrent services deployment. This paper aims to establish guidelines and identify critical factors for containerized services deployment on resource constrained IoT devices. For this purpose, two container orchestration tools (i.e., Docker Swarm and Kubernetes) are tested and compared on a baseline IoT gateways testbed. Experiments use Deep Learning driven data analytics and Intrusion Detection System services, and evaluate the time it takes to prepare and deploy a container (creation time), Central Processing Unit (CPU) utilization for concurrent containers deployment, memory usage under different traffic loads, and energy consumption. The results indicate that container creation time and memory usage are decisive factors for containerized micro service architecture. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Containers;  Docker swarm;  Empirical analysis;  Energy consumption;  Internet of things;  Kubernetes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dang2021,
author={Dang, H.-L. and Do, D.-P.},
title={Finite element implementation of coupled hydro-mechanical modeling of transversely isotropic porous media in DEAL.II},
journal={International Journal of Modeling, Simulation, and Scientific Computing},
year={2021},
volume={12},
number={1},
doi={10.1142/S1793962321500033},
art_number={2150003},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098536689&doi=10.1142%2fS1793962321500033&partnerID=40&md5=e6d7b826b07dbee122bde19ec44cda39},
abstract={In geotechnical engineering, modeling geo-structures is challenging, particularly in cases where the interaction between the structures and soil or rock is complex. Most well-known commercial modeling software is based on homogenous and isotropic materials. However, soil and rock are often modeled in heterogeneous and anisotropic media because of the inherent anisotropy of sedimentary rock masses and their stratified structure. In recent decades, coupled hydro-mechanical (HM) interactions in isotropic porous media have been studied; however, the behavior of transversely isotropic porous media is rarely considered. In addition, it is difficult for commercial software such as Plaxis and Flac3D to express complex rock formation where the anisotropy of the material and the associated cracks and fractures could be assembled into a single model. In this study, a finite element implementation using Differential Equation Analysis Library (DEAL.II), an open-source library of finite element codes, was developed to model the fully coupled HM behavior of transversely isotropic porous media. The proposed implementation can be applied to both isotropic and transversely isotropic porous media based on Biot's theory. The developed code can be used to model poroelastic media with (1) equations of linear elasticity for the solid matrix and (2) diffusion equations for fluid flow based on mass and linear-momentum conservation laws. We verified the performance and accuracy of the code through two examples, i.e., Mandel's problem with a compared analytical solution and a tunnel excavation process with the Flac3D software. On the basis of these numerical applications, we present the code to model the behavior of various geo-structures such as tunnels and pile-soil interactions with anisotropic materials. © 2021 World Scientific Publishing Company.},
author_keywords={Biot's theory;  Coupled hydro-mechanical modeling;  DEAL.II;  finite element method;  transversely isotropic porous media},
document_type={Article},
source={Scopus},
}

@ARTICLE{Klymenko2021,
author={Klymenko, M.V. and Vaitkus, J.A. and Smith, J.S. and Cole, J.H.},
title={NanoNET: An extendable Python framework for semi-empirical tight-binding models},
journal={Computer Physics Communications},
year={2021},
volume={259},
doi={10.1016/j.cpc.2020.107676},
art_number={107676},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093703421&doi=10.1016%2fj.cpc.2020.107676&partnerID=40&md5=64cae03ab9761ddb74469ce7662352ea},
abstract={We present a novel open-source Python framework called NanoNET (Nanoscale Non-equilibrium Electron Transport) for modeling electronic structure and transport. Our method is based on the tight-binding method and non-equilibrium Green's function theory. The core functionality of the framework is providing facilities for efficient construction of tight-binding Hamiltonian matrices from a list of atomic coordinates and a lookup table of the two-center integrals in dense, sparse, or block-tridiagonal forms. The framework implements a method based on kd-tree nearest-neighbor search and is applicable to isolated atomic clusters and periodic structures. A set of subroutines for detecting the block-tridiagonal structure of a Hamiltonian matrix and splitting it into series of diagonal and off-diagonal blocks is based on a new greedy algorithm with recursion. Additionally the developed software is equipped with a set of programs for computing complex band structure, self-energies of elastic scattering processes, and Green's functions. Examples of usage and capabilities of the computational framework are illustrated by computing the band structure and transport properties of a silicon nanowire as well as the band structure of bulk bismuth. Program summary: Program Title: NanoNET CPC Library link to program files: http://dx.doi.org/10.17632/b9p7kyzdj9.1 Developer's repository link: http://github.com/freude/NanoNet Licensing provisions: MIT Programming language: Python Nature of problem: The framework NanoNET solves a problem which is, having a set of atomic coordinates and tight-binding parameters, to construct Hamiltonian matrices in one of several desired forms. In particular, some applications require those matrices to have a reduced bandwidth and/or to possess a block-tridiagonal structure. Solution method: The problem is solved using a combination of kd-tree-based fast nearest-neighbor search and atomic coordinate sorting. Furthermore, a new greedy recursive algorithm is proposed for detecting block-tridiagonal structure of a matrix in a non-optimal way. Additionally, we propose an algorithm of a polynomial time for optimizing block sizes. Additional features: Although the resulting matrices can be processed by many existing software packages, the framework also has built-in standard tools for diagonalizing Hamiltonian matrices and computing Green's functions that make it an independent tool for solving electronic structure and transport problems. © 2020 The Author(s)},
author_keywords={Band matrix;  Block-tridiagonal matrix;  Hamiltonian matrix;  kd-tree;  Non-equilibrium Green's functions;  Tight-binding method},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Thiele2021260,
author={Thiele, C.-D. and Huyeng, T.-J. and Mosler, P. and Rüppel, U.},
title={Concept to support the estimation of static load capacity on construction sites using in-situ AR-based methods},
journal={EG-ICE 2021 Workshop on Intelligent Computing in Engineering, Proceedings},
year={2021},
pages={260-269},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134216637&partnerID=40&md5=087bcce7494eca403ea26300d304c4ad},
abstract={Due to the advancing digitization in the AEC industry (Architecture, Engineering and Construction), existing workflows are undergoing a transformation towards modern work patterns. In particular, there is potential for optimization in the area of building in existing structures. For example, the on-site capture of a comprehensive model in the context of Historic Building Information Modeling (HBIM) and especially of building elements can be supported by modern technologies. This paper uses an exemplary use case to demonstrate how such transformation from a traditional to a modern, digitally enhanced workflow is made possible to support an engineer on-site. The use case is the evaluation of a construction state with a temporary local load increase. The traditional workflow consists of multiple manual and partially iterative steps. The proposed workflow uses advanced augmented reality (AR) technology on mobile devices as well as a self-developed Web API for an existing structural analysis software. This enables an on-site estimation of the static load capacity of a structural subsystem. © 2021 Universitätsverlag der Technischen Universität Berlin. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Florea-Burduja2021129,
author={Florea-Burduja, E. and Raru, A. and Farîma, D. and Irovan, M. and Lupu, I.},
title={SOFTWARE APPLICATION FOR CONSTRUCTION-FUNCTIONAL DESIGN OF PANTS FOR PEOPLE WITH LOWER LIMB AMPUTATIONS},
journal={eLearning and Software for Education Conference},
year={2021},
pages={129-134},
doi={10.12753/2066-026X-21-157},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127236376&doi=10.12753%2f2066-026X-21-157&partnerID=40&md5=0e03bfa7ed24347cd2dfb81c95abbc36},
abstract={The course "CAD systems in garments" offered within the master's program at the Faculty of Textile and Polygraphy, deals with the study of various CAD systems used in the textile field. The paper proposes a software application designed to design the product pants for people with lower limb amputations, for women and men, worn during rehabilitation or social integration. The application has a structure with open architecture that allows completing, modifying it according to needs but also automating engineering activity. For the realization of the basic pattern, the data related to the carrier and the product are considered as initial data. The model construction is elaborated by taking into account the product sketch created by the designer which provides information about the types of changes, the location of the product, the contour of the constructive-decorative elements, etc. For the construction of the model, a library was created with functional-constructive elements, depending on the level of amputation of people and their needs. By selecting the necessary constructive-functional elements and combining with the basic construction, we obtain the model constructions of the trousers products for women and men, which have lower limb amputations. The application is designed to allow the subsequent completion of the database with various products for people with locomotor disabilities and the completion of the graphic library with functional-constructive and decorative elements, depending on the type of disability, doctors' recommendations and wishes. All patterns can be easily changed as needed. © 2021, National Defence University - Carol I Printing House. All rights reserved.},
author_keywords={basic pattern;  e-learning;  pants;  people with amputations;  software application},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Habte20211009,
author={Habte, B. and Guyo, E.},
title={APPLICATION of BIM for STRUCTURAL ENGINEERING: A CASE STUDY USING REVIT and CUSTOMARY STRUCTURAL ANALYSIS and DESIGN SOFTWARE},
journal={Journal of Information Technology in Construction},
year={2021},
volume={26},
pages={1009-1022},
doi={10.36680/j.itcon.2021.053},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126070364&doi=10.36680%2fj.itcon.2021.053&partnerID=40&md5=69a33dc1ba4fa9e28abf3a264e7e015e},
abstract={Building information modelling (BIM) represents a workflow whose application on a construction project will enable all involved players to compile as well as work with information on every aspect of a building in a common model/database. Through BIM, the entire building can be virtually designed and built on a computer. BIM touches every part of a building s life cycle starting from the design phase well into the construction phase and beyond that into asset management. This research examines the experiences of early adopters of BIM and use that insight to introduce BIM, specially focusing on the structural analysis and design stage of a building. The study demonstrates how all structural design activities can be integrated with each other and how cross-discipline collaboration with the architect can be achieved through the adoption of BIM without leaving ones customary structural design platform. As a demonstration, a sample building is modelled using Revit along with conventional structural software packages ETABS and SAFE. Plugins and applications were developed for these software packages to facilitate interoperability amongst them so that they all act together as a single platform. Modelling, analysis, design and clash detections were facilitated by applying BIM. Major benefits of employing BIM in a structural design project are illustrated through this research. © 2021 International Council for Research and Innovation in Building and Construction. All rights reserved.},
author_keywords={BIM;  Integration;  Interoperability;  Structural Analysis and Design},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bikku2021757,
author={Bikku, T. and Karthik, J. and Rao, G.R.K. and Satya Sree, K.P.N.V. and Srinivas, P.V.V.S. and Prasad, C.},
title={Brain Tissue Segmentation via Deep Convolutional Neural Networks},
journal={Proceedings of the 5th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud), I-SMAC 2021},
year={2021},
pages={757-763},
doi={10.1109/I-SMAC52330.2021.9640635},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124220127&doi=10.1109%2fI-SMAC52330.2021.9640635&partnerID=40&md5=c1c055d204c1357f3ff7ef83dff888ea},
abstract={Deep convolutional neural networks were used to successfully segment several important neural tissue classes in MRI brain images, and approaches for integrating prior information into the networks to increase their performance on this task were investigated. Regrettably, only the first of them is addressed in this paper. To make the implementation of nonstandard architectures, which was expected to be required for the second goal, it was determined to provide a framework for defining and training networks by only using fundamental components. While this was an educational experience, the amount of progress accomplished was far less than if a conventional network package had been utilized instead. The requirement to deal with all of the lowest level aspects of network construction, from initialization schemes to adaptive learning rates and all the other components of the optimizer pipeline has left no time for utilizing this infrastructure to do something which has not been accomplished by the existing frameworks, and of course in all other respects it is far more limited than they are. It would in-stead be focused on a clear and detailed analysis of the full pipeline, which is required to build a network for solving the first problem. Despite several difficulties tracking down bugs in the optimizer, GPU memory allocation, and the last-minute accidental deletion of a large portion of the experimental results, the software implementation made available at: achieves DICE results of 0:8, which, while not class leading, would still place well in many benchmarks. © 2021 IEEE.},
author_keywords={Autodesk 3D Max software;  network construction;  Nonstandard architectures;  optimizer Random Decision Forest;  Prosthetic;  Regression},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chesnokov2021523,
author={Chesnokov, A. and Mikhailov, V. and Dolmatov, I.},
title={Numerical Algorithm for Finding Optimal Parameters of Pre-stressed Roof Structure},
journal={Proceedings - 2021 3rd International Conference on Control Systems, Mathematical Modeling, Automation and Energy Efficiency, SUMMA 2021},
year={2021},
pages={523-527},
doi={10.1109/SUMMA53307.2021.9632034},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123725966&doi=10.1109%2fSUMMA53307.2021.9632034&partnerID=40&md5=0dcbe282b83196ab4ff5aea4b87bc3c9},
abstract={Numerical technique for finding optimal parameters of the roof structure is proposed. The structure consists of pre-stressed bearer and backstay chords made of high-strength steel cables. The roof has reduced overall height. It is the appropriate solution for long-span buildings. The coordinate descent method is used to perform structural optimization. This approach allows gaining precise results which are to be used for further nonlinear structural analysis by specialized software packages. The present study contributes to automated structural simulation of cable and membrane building constructions. © 2021 IEEE.},
author_keywords={automated structural simulation;  cable roof;  coordinate descent method;  numerical technique},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{TorresLima2021,
author={Torres Lima, S.A. and Montenegro Flor, C.D. and Legarda, D.A. and Munoz Chaves, J.A.},
title={Automatic determination of the surface hydrophobicity of titanium alloys using the sessile drop method based on image processing [Determinación automática de la hidrofobicidad superficial de aleaciones de titanio usando el método de gota sésil basado en procesamiento de imágenes]},
journal={2021 22nd Symposium on Image, Signal Processing and Artificial Vision, STSIVA 2021 - Conference Proceedings},
year={2021},
doi={10.1109/STSIVA53688.2021.9591666},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123563751&doi=10.1109%2fSTSIVA53688.2021.9591666&partnerID=40&md5=5f3b1767c46598e36b6d0e996dd22d8a},
abstract={This paper presents the design and implementation, completely homemade, of an automated system for hydrophobicity studies of materials based on the sessile drop method. This parameter was evaluated from the measurement of the contact angle between the deposited liquid and the surface of the material. For this purpose, software for digital image processing was implemented using Python programming language in conjunction with the OpenCV library. Through various filters and functions, an adequate segmentation of the acquired images was achieved to identify their contours and make it possible to measure the contact angle. The system was validated using titanium alloys with their modified surface, finding a good agreement between the results obtained and those reported in the literature to evaluate their potential application as an implantable metallic biomaterial. © 2021 IEEE.},
author_keywords={Contact angle;  hydrophobicity;  image processing;  titanium alloys},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2021,
title={7th Russian Supercomputing Days Conference, RuSCDays 2021},
journal={Communications in Computer and Information Science},
year={2021},
volume={1510 CCIS},
page_count={537},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123304629&partnerID=40&md5=97d3753aaffdccff155c9ad4f0f3793f},
abstract={The proceedings contain 40 papers. The special focus in this conference is on Russian Supercomputing Day. The topics include: Implementation of Elliptic Solvers Within ParCS Parallel Framework; JAD-Based SpMV Kernels Using Multiple-Precision Libraries for GPUs; Modeling the Process of Mixing Water in the Estuarine Area in the Presence of a Density Gradient of the Aquatic Environment Using Hybrid MPI/OpenMP Technology; multicomp: Software Package for Multiscale Simulations; parallel Algorithm for Calculating the Radius of Stability in Multicriteria Optimization Conditions for Catalytic Reforming of Gasoline; parallel Global Search Algorithm for Optimization of the Kinetic Parameters of Chemical Reactions; parameters Optimization of Linear and Nonlinear Solvers in GeRa Code; quantum-Chemical Calculations of the Structure and Thermochemical Properties of 3,6-bis(2,2,2-trinitroethylnitramino)-1,2,4,5-tetrazine; Solution of Large-Scale Black Oil Recovery Problem in Parallel Using INMOST Platform; Application of Docking and Quantum Chemistry to the Search for Inhibitors of SARS-CoV-2 Main Protease; Supercomputer Simulation of Turbulent Flow Around Isolated UAV Rotor and Associated Acoustic Fields; using 16-th Order Multioperators-Based Scheme for Supercomputer Simulation of the Initial Stage of Laminar-Turbulent Transitions; wave Spectrum of Flowing Drops; advanced Genetic Algorithm in the Problem of Linear Solver Parameters Optimization; Analysis of Software Package Usage Based on Methods for Identifying Similar HPC Applications; Comparative Efficiency Analysis of MPI Blocking and Non-blocking Communications with Coarray Fortran; Investigating Performance of the XAMG Library for Solving Linear Systems with Multiple Right-Hand Sides; “Mini-Benchmarking” Approach to Optimize Evolutionary Methods of Neural Architecture Search; on the Prospects of Quantum Computing in Models of Social Behavior; Qualitative and Quantitative Study of Modern GPU Synchronization Approaches; calculation of Integral Coefficients for Correlation Magnetodynamics and Verification of the Theory; rust Language for Supercomputing Applications; study of the Algorithms Information Structure as the Basis of a Training Workshop; multi-cloud Privacy-Preserving Logistic Regression.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Wu2021,
author={Wu, S.},
title={Intelligent Communication Management Terminal in the Construction of Human Resource Management Mode},
journal={Wireless Communications and Mobile Computing},
year={2021},
volume={2021},
doi={10.1155/2021/7106104},
art_number={7106104},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534041&doi=10.1155%2f2021%2f7106104&partnerID=40&md5=0d62c1da5b5cd1f85c9ddf02f9377037},
abstract={With the rapid development of the economy, the integration of corporate strategic management and human resource management has become an issue of concern. This research mainly discusses the role of intelligent communication management terminal in the construction of human resource management mode. In this research, the system development process of this research mainly uses the class library in the software architecture layer to support the software development process. The main development language of Android, JAVA, is to install the Android Develop Tools plug-in on eclipse and install the Android SDK in the computer operating system to build the Android development environment. The development and application of the system not only make the enterprise managers more convenient and efficient in the process of managing the enterprise but also smooth the operation of the enterprise while reducing the human resource investment and also gives the employees more right to know and the right to participate in the enterprise construction. By creating more value while reducing human resource input, enterprises will enable it to obtain more benefits, and thus enter a cycle of good development and contribute to society. The system has the functions of personnel management, recruitment management, attendance management, training management, work management, and salary management. The recruitment management function of the system is mainly composed of recruitment plan management, recruitment information management, and talent pool. In the system's recruitment plan management function, important information such as the recruitment part, the number of recruits, personnel requirements, and the specific arrival time of the personnel must be clarified. The personnel in charge of the enterprise personnel department shall conduct corresponding regulations according to the specific needs of the enterprise and shall be experienced by the personnel department. The review is carried out, and all parts of the enterprise are coordinated and completed at the same time. In the platform performance test, when the number of concurrent users reaches 50000, the request time is about 6 seconds, which meets the requirement that the response time of 10000 people per second is less than 10 seconds. This research puts forward suggestions and countermeasures for the optimization of human resource management, which can not only improve the efficiency of Y company's human resource management but also provide useful reference and reference for other enterprises facing the same problem. © 2021 Shumei Wu.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yamashkin2021583,
author={Yamashkin, S.A. and Yamashkin, A.A. and Yamashkina, E.O. and Radovanovic, M.M.},
title={Development of Architecture and Software Implementation of Deep Neural Network Models Repository for Spatial Data Analysis},
journal={International Journal of Advanced Computer Science and Applications},
year={2021},
volume={12},
number={8},
pages={583-588},
doi={10.14569/IJACSA.2021.0120868},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118974375&doi=10.14569%2fIJACSA.2021.0120868&partnerID=40&md5=26dd49b7af01151f3aa85eb84b951a2f},
abstract={The article presents the key aspects of designing and developing a repository of deep neural network models for analyzing and predicting the development of spatial processes based on spatial data. The framework of the system operates on the basis of the MVC pattern, in which the framework is decomposed into modules for working with the system's business logic, its data and graphical interfaces. The characteristics of the developed web interfaces, a module for visual programming for editing of models, an application programming interface for unified interaction with the repository are given. The stated aim of the study determined the structure of the scientific article and the results obtained. The paper describes the functional requirements for the repository as a signific part of software design, presents the developed formalized storage scheme for neural network models, describes the aspects of development of a repository of neural network models and an API of the repository. The developed system allows us to approach the solution of the scientific problem of integrating neural networks with the possibility of their subsequent use to solve design problems of the digital economy. © 2021. International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
author_keywords={artificial neural network;  deep learning;  Repository;  software design;  spatial data;  visual programming},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Paradis2021,
author={Paradis, C. and Kazman, R.},
title={Design choices in building an MSR tool: The case of Kaiaulu},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2978},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117760287&partnerID=40&md5=81955a0a280b9878ff39973f036d6744},
abstract={Background: Since Alitheia Core was proposed and subsequently retired, tools that support empirical studies of software continue to be proposed, such as Codeface, Codeface4Smells, GrimoireLab and SmartSHARK, but they all make different design choices with overlapping functionality. Aims: We seek to understand the design decisions adopted on these tools–good and bad–and their consequences to understand why their authors reinvented functionality already present in other tools, and to help inform the design of future tools. Method: We used action research to evaluate the tools, and determine principles and anti-patterns to motivate a new tool design. Results: We identified 7 major design choices among the tools: 1) Abstraction Debt, 2) the use of Project Configuration Files, 3) the choice of Batch or Interactive Mode, 4) Minimal Paths to Data, 5) Familiar Software Abstractions, 6) Licensing and 7) the Perils of Code Reuse. Building on the observed good and bad design decisions, we created our own architecture and implemented it as an R package. Conclusions: Tools should not require onerous setup for users to obtain data. Authors should consider the conventions and abstractions used by their chosen language and build upon these instead of redefining them. Tools should encourage best practices in experiment reproducibility by leveraging self-contained and readable schemas that are used for tool automation, and reuse must be done with care to avoid depending on dead code. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
author_keywords={Action research;  Design choices;  Mining software repositories},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kotstein2021154,
author={Kotstein, S. and Bogner, J.},
title={Which RESTful API Design Rules Are Important and How Do They Improve Software Quality? A Delphi Study with Industry Experts},
journal={Communications in Computer and Information Science},
year={2021},
volume={1429 CCIS},
pages={154-173},
doi={10.1007/978-3-030-87568-8_10},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116433444&doi=10.1007%2f978-3-030-87568-8_10&partnerID=40&md5=7dc0d53aa21ccff688c456a8547d914b},
abstract={Several studies analyzed existing Web APIs against the constraints of REST to estimate the degree of REST compliance among state-of-the-art APIs. These studies revealed that only a small number of Web APIs are truly RESTful. Moreover, identified mismatches between theoretical REST concepts and practical implementations lead us to believe that practitioners perceive many rules and best practices aligned with these REST concepts differently in terms of their importance and impact on software quality. We therefore conducted a Delphi study in which we confronted eight Web API experts from industry with a catalog of 82 REST API design rules. For each rule, we let them rate its importance and software quality impact. As consensus, our experts rated 28 rules with high, 17 with medium, and 37 with low importance. Moreover, they perceived usability, maintainability, and compatibility as the most impacted quality attributes. The detailed analysis revealed that the experts saw rules for reaching Richardson maturity level 2 as critical, while reaching level 3 was less important. As the acquired consensus data may serve as valuable input for designing a tool-supported approach for the automatic quality evaluation of RESTful APIs, we briefly discuss requirements for such an approach and comment on the applicability of the most important rules. © 2021, Springer Nature Switzerland AG.},
author_keywords={Delphi study;  Design rules;  REST APIs;  Software quality},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Taieb202177,
author={Taieb, S.H. and Loukil, T.M. and Mhamedi, A.E. and Hani, Y.},
title={Two Variants of Bi-objective Vehicle Routing Problem in Home (Health)-Care Fields},
journal={IFIP Advances in Information and Communication Technology},
year={2021},
volume={634 IFIP},
pages={77-86},
doi={10.1007/978-3-030-85914-5_9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115327406&doi=10.1007%2f978-3-030-85914-5_9&partnerID=40&md5=374c15b8a2c1e733b1481e7f9d0568ea},
abstract={In this research paper, we consider two bi-objectives models, the Vehicle Routing Problem Preferences with Time windows, temporal dependencies, multiple structures and multiple specialties “VRPPTW-TD-2MS” and the Vehicle Routing Problem Balance with Time windows, temporal dependencies, multiple structures and multiple specialties “VRPBTW-TD-2MS”, in Home (Health)-Care “HHC” service field. In this context, The HHC services is mostly assigned to certain persons such as people who have a chronic disease or who have difficulty leaving their houses by offering them medical, social, and/or medico-social services like nursing, dressing, bathing, toileting, feeding, shopping, hygiene assistance, etc. These several services are provided by different public and private home care structures. These structures work together in coordination to provide high HHC quality for patients. Therefore, patients are not in a single structure anymore, but rather at home and need to be served by caregivers. As a result, the problem is no longer limited to the care organization, but also to the construction of optimal tours for the caregivers, the delivery of medicines, and the equipment needed for care. According to our knowledge, our work is different from those made in the literature. Our problem consists to determine the caregivers’ tours while optimizing bi-objective function, which aims to minimize the total traveling time and the total negative preference or the maximal difference workload of caregivers under set of constraints such as time window, synchronization, precedence and disjunction constraints with multiple structures and multiple caregivers’ skills. A benchmark of instances is considered from literature. Cplex (Optimization Software package) is used to solve our problems to obtain efficient solutions. © 2021, IFIP International Federation for Information Processing.},
author_keywords={Bi-objective;  Home (Health)-Care;  VRPBTW-TD-2MS;  VRPPTW-TD-2MS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang20214187,
author={Zhang, H. and Anilkumar, A. and Fredrikson, M. and Agarwal, Y.},
title={Capture: Centralized library management for heterogeneous IoT devices},
journal={Proceedings of the 30th USENIX Security Symposium},
year={2021},
pages={4187-4204},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114449473&partnerID=40&md5=14fa819f9cc7b8e37087cf67993f2405},
abstract={With their growing popularity, Internet-of-Things (IoT) devices have become attractive targets for attack. Like most modern software systems, IoT device firmware depends on external third-party libraries extensively, increasing the attack surface of IoT devices. Furthermore, we find that the risk is compounded by inconsistent library management practices and delays in applying security updates-sometimes hundreds of days behind the public availability of critical patches-by device vendors. Worse yet, because these dependencies are “baked into” the vendor-controlled firmware, even security-conscious users are unable to take matters into their own hands when it comes to good security hygiene. We present Capture, a novel architecture for deploying IoT device firmware that addresses this problem by allowing devices on a local network to leverage a centralized hub with third-party libraries that are managed and kept up-to-date by a single trusted entity. An IoT device supporting Capture comprises of two components: Capture-enabled firmware on the device and a remote driver that uses third-party libraries on the Capture hub in the local network. To ensure isolation, we introduce a novel Virtual Device Entity (VDE) interface that facilitates access control between mutually-distrustful devices that reside on the same hub. Our evaluation on a prototype implementation of Capture, along with 9 devices and 3 automation applets ported to our framework, shows that our approach incurs low overhead in most cases (<15% increased latency, <10% additional resources). We show that a single Capture Hub with modest hardware can support hundreds of devices, keeping their shared libraries up-to-date. © 2021 by The USENIX Association. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2021411,
author={Chen, L. and Mao, X. and Zhang, Y. and Yang, S. and Wang, S.},
title={An efficient ROS package searching approach powered by knowledge graph},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2021},
volume={2021-July},
pages={411-416},
doi={10.18293/SEKE2021-063},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114283258&doi=10.18293%2fSEKE2021-063&partnerID=40&md5=33e213145f09134302ef21743f6e61f0},
abstract={Over the past several years, the Robot Operating System (ROS), has grown from a small research project into the most popular framework for robotics development. It offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to program robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. However, finding the proper ROS package is a nontrivial task because ROS packages involve different functions and even with the same function, there are different ROS packages for different tasks. So it is time-consuming for developers to find suitable ROS packages for given task, especially for newcomers. To tackle this challenge, we build a ROS package knowledge graph, ROSKG, including the basic information of ROS packages and ROS package characteristics extracted from text descriptions, to comprehensively and precisely characterize ROS packages. Based on ROSKG, we support ROS packages search with specific task description or attributes as input. A comprehensive evaluation of ROSKG shows the high accuracy of our knowledge construction approach. A user study shows that ROSKG is promising in helping developers find suitable ROS packages for robotics software development tasks. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.},
author_keywords={Knowledge graph;  NLP;  ROS package searching},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Du2021395,
author={Du, Z. and An, Z. and Xing, J.},
title={TPL: A Novel Analysis and Optimization Model for RDMA P2P Communication},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12639 LNCS},
pages={395-406},
doi={10.1007/978-3-030-79478-1_34},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112612899&doi=10.1007%2f978-3-030-79478-1_34&partnerID=40&md5=17666586020643e9d404efc72be52e03},
abstract={With increasing demand for networks with high throughput and low latency, RDMA is widely used because of its high performance. Because optimization for RDMA can fully exploit the performance potential of RDMA, methods for RDMA optimization is very important. Existing mainstream researches design optimization methods by constructing a more complete hardware view and exploring relation between software implementation and specific hardware behavior. However, the hardware architecture of NIC (like InfiniBand) is a “black box”, which limits development of this type of optimization. So existing methods leave unsolvable problems. Besides, with development of RDMA technology, new features are proposed constantly. So, analysis and optimization methods of RDMA communication performance should be advancing with the times. The contributions of this paper are as follows: 1) We propose a new RDMA point-to-point communication performance analysis and optimization model: TPL. This model provides a more comprehensive perspective on RDMA optimization. 2) Guided by TPL, we design corresponding optimization algorithms for an existing problem, like WQE cache miss and a new scenario, like DCT. 3) We implement a new RDMA communication library, named ORCL, to put our optimizations together. ORCL eliminates WQE cache miss in real-time. And we simulate the workload of the in-memory KV system. Compared with existing RDMA communication implement, ORCL increases throughput by 95% and reduces latency by 10%. © 2021, IFIP International Federation for Information Processing.},
author_keywords={Optimization model;  Performance tuning;  RDMA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Allaban20213,
author={Allaban, A.A. and Bonnie, D. and Knapp, E. and Gokhale, P. and Moulard, T.},
title={Developing Production-Grade Applications with ROS 2},
journal={Studies in Computational Intelligence},
year={2021},
volume={962},
pages={3-54},
doi={10.1007/978-3-030-75472-3_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111439855&doi=10.1007%2f978-3-030-75472-3_1&partnerID=40&md5=0c2ae3ee2c83a7996610aff1e65ec425},
abstract={Developing production-grade robotic applications is a critical component of building a robotic product. However, those techniques, best practices, and patterns are often tribal knowledge, learned on the job, but infrequently documented. This chapter covers some of these topics, such as: how to compile ROS software for a non-native architecture (such as ARM), how to tune a software stack using Quality of Service (QOS) settings, how to build a robust CI system for your packages, how to maintain and release ROS 2 software packages, how to monitor ROS 2 nodes running on a fleet of robots in production? This chapter presents step-by-step tutorials, and workflows adopted by the ROS Tooling Working Group. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Best practices;  Continuous integration;  Cross-compilation;  DevOps;  Monitoring;  QoS;  Release management;  ROS 2},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Boehme2021431,
author={Boehme, D. and Aschwanden, P. and Pearce, O. and Weiss, K. and LeGendre, M.},
title={Ubiquitous Performance Analysis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12728 LNCS},
pages={431-449},
doi={10.1007/978-3-030-78713-4_23},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111432745&doi=10.1007%2f978-3-030-78713-4_23&partnerID=40&md5=c9d1095174dd43326e7c0cd8054735db},
abstract={In an effort to guide optimizations and detect performance regressions, developers of large HPC codes must regularly collect and analyze application performance profiles across different hardware platforms and in a variety of program configurations. However, traditional performance profiling tools mostly focus on ad-hoc analysis of individual program runs. Ubiquitous performance analysis is a new approach to automate and simplify the collection, management, and analysis of large numbers of application performance profiles. In this regime, performance profiling of large HPC codes transitions from a sporadic process that often requires the help of experts into a routine activity in which the entire development team can participate. We discuss the design and implementation of an open source ubiquitous performance analysis software stack with three major components: the Caliper instrumentation library with a new API to control performance profiling programmatically; Adiak, a library for automatic program metadata capture; and SPOT, a web-based visualization interface for comparing large sets of runs. A case study shows how ubiquitous performance analysis has helped the developers of the Marbl simulation code for over a year with analyzing performance and understanding regressions. © 2021, Springer Nature Switzerland AG.},
author_keywords={Caliper;  Instrumentation;  Measurement;  Performance},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Incardona2021272,
author={Incardona, P. and Bianucci, T. and Sbalzarini, I.F.},
title={Distributed Sparse Block Grids on GPUs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12728 LNCS},
pages={272-290},
doi={10.1007/978-3-030-78713-4_15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111424777&doi=10.1007%2f978-3-030-78713-4_15&partnerID=40&md5=254b94f6dbe5828c6e4ccde4dfa2b9b5},
abstract={We present a design and implementation of distributed sparse block grids that transparently scale from a single CPU to multi-GPU clusters. We support dynamic sparse grids as, e.g., occur in computer graphics with complex deforming geometries and in multi-resolution numerical simulations. We present the data structures and algorithms of our approach, focusing on the optimizations required to render them computationally efficient on CPUs and GPUs alike. We provide a scalable implementation in the OpenFPM software library for HPC. We benchmark our implementation on up to 16 Nvidia GTX 1080 GPUs and up to 64 Nvidia A100 GPUs showing state-of-the-art scalability (68% to 96% parallel efficiency) on three benchmark problems. On a single GPU, our implementation is 14 to 140-fold faster than on a multi-core CPU. © 2021, Springer Nature Switzerland AG.},
author_keywords={Block grid;  CUDA;  Distributed data;  GPU;  Sparse grid},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2021749,
author={Wang, S. and Dai, J. and Yang, M. and Zhao, B.},
title={Research and Design Based on Public Safety Big Data Evaluation Platform},
journal={Lecture Notes on Data Engineering and Communications Technologies},
year={2021},
volume={81},
pages={749-755},
doi={10.1007/978-3-030-79197-1_108},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111414334&doi=10.1007%2f978-3-030-79197-1_108&partnerID=40&md5=978bbbd6ef79cb55efb2792c6fbc4f17},
abstract={With the continuous advancement of the technological wave, the informatization construction of the public safety industry is also undergoing continuous iterative upgrades in technology and business. All provinces, cities and localities are carrying out informatization reforms and building big data platforms. The platform has accumulated a large amount of police data, such as bayonet, video, criminals, social collection, basic data, case incidents and other data are growing rapidly, and the public security industry has new requirements for big data platforms. How to build a stable, reliable, and safe system has become a core requirement. With the introduction of the concept of new requirements for the construction of big data systems, the previous computing operating platforms and computing architectures are no longer competent in the face of such a huge data scale. A new big data evaluation system is needed to evaluate the big data system from all aspects. The construction situation. At present, some manufacturers have proposed industry benchmark tests and provide corresponding test software packages. However, starting from the versatility of big data and the true flexibility of testing, most of the benchmark test suites can only meet part of the requirements of big data evaluation. Some cannot cover the typical areas involved in big data. The big data evaluation software system takes these requirements as the basis for development, and has developed representative big data benchmark evaluation software that meets the characteristics of big data. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Big data;  Evaluation;  Police data;  Public security},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bartman202116,
author={Bartman, P. and Arabas, S.},
title={On the Design of Monte-Carlo Particle Coagulation Solver Interface: A CPU/GPU Super-Droplet Method Case Study with PySDM},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12743 LNCS},
pages={16-30},
doi={10.1007/978-3-030-77964-1_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111364322&doi=10.1007%2f978-3-030-77964-1_2&partnerID=40&md5=69da189655ef07efbc099502f77c19a5},
abstract={Super-Droplet Method (SDM) is a probabilistic Monte-Carlo-type model of particle coagulation process, an alternative to the mean-field formulation of Smoluchowski. SDM as an algorithm has linear computational complexity with respect to the state vector length, the state vector length is constant throughout simulation, and most of the algorithm steps are readily parallelizable. This paper discusses the design and implementation of two number-crunching backends for SDM implemented in PySDM, a new free and open-source Python package for simulating the dynamics of atmospheric aerosol, cloud and rain particles. The two backends share their application programming interface (API) but leverage distinct parallelism paradigms, target different hardware, and are built on top of different lower-level routine sets. First offers multi-threaded CPU computations and is based on Numba (using Numpy arrays). Second offers GPU computations and is built on top of ThrustRTC and CURandRTC (and does not use Numpy arrays). In the paper, the API is discussed focusing on: data dependencies across steps, parallelisation opportunities, CPU and GPU implementation nuances, and algorithm workflow. Example simulations suitable for validating implementations of the API are presented. © 2021, Springer Nature Switzerland AG.},
author_keywords={Coagulation;  GPU;  Monte-Carlo;  Super-Droplet Method},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sahu20211518,
author={Sahu, S. and Salihoglu, S.},
title={Graphsurge: Graph Analytics on View Collections Using Differential Computation},
journal={Proceedings of the ACM SIGMOD International Conference on Management of Data},
year={2021},
pages={1518-1530},
doi={10.1145/3448016.3452837},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108972956&doi=10.1145%2f3448016.3452837&partnerID=40&md5=3ad4c07e2cd768b2942168fb3dcf77d3},
abstract={This paper presents the design and implementation of a new open-source view-based graph analytics system called Graphsurge. Graphsurge is designed to support applications that analyze multiple snapshots or views of a large-scale graph. Users program Graphsurge through a declarative graph view definition language (GVDL) to create views over input graphs and a Differential Dataflow-based programming API to write analytics computations. A key feature of GVDL is the ability to organize views into view collections, which allows Graphsurge to automatically share computation across views, without users writing any incrementalization code, by performing computations differentially. We then introduce two optimization problems that naturally arise in our setting. First is the collection ordering problem to determine the order of views that leads to minimum differences across consecutive views. We prove this problem is NP-hard and show a constant-factor approximation algorithm drawn from literature. Second is the collection splitting problem to decide on which views to run computations differentially vs from scratch, for which we present an adaptive solution that makes decisions at runtime. We present extensive experiments to demonstrate the benefits of running computations differentially for view collections and our collection ordering and splitting optimizations. © 2021 ACM.},
author_keywords={adaptive execution;  collection ordering;  dataflow computation;  differential computation;  graph views;  view collection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Grodowitz2021318,
author={Grodowitz, M. and Shamis, P. and Poole, S.},
title={Openshmem i/o extensions for fine-grained access to persistent memory storage},
journal={Communications in Computer and Information Science},
year={2021},
volume={1315 CCIS},
pages={318-333},
doi={10.1007/978-3-030-63393-6_21},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107313459&doi=10.1007%2f978-3-030-63393-6_21&partnerID=40&md5=7e5148a31ae243c6bc65da905f8dc012},
abstract={Application workflows use files to communicate between stages of data processing and analysis kernel executions. In large-scale high performance distributed systems, file based communication significantly penalizes performance by introducing overheads such as meta-data access, contention for file locks, and slow speed of spinning disks. Using files as system wide persistent storage also hinders fine-grained access to data when files are stored on block devices handled through the I/O software stack. To address speed and granularity, we employ persistent memory (PMEM) devices, which provide DRAM-like speeds and byte granular access combined with persistent storage capabilities. To address file and I/O software stack overheads, we deploy an Arm-based Mellanox Bluefield SmartNIC with attached NVDIMM-N modules. Both Smart-NIC and PMEM introduce API design and system software integration challenges. We address this with the design and implementation for an innovative client-server software architecture with a client API extension to the OpenSHMEM library. We benchmark the implementation using a workflow of invocations of OpenSHMEM kernels on a persistent data set. Compared to the same workflow using a network file I/O client, our solution shows no degradation of performance as the number of clients increases. We accelerate startup and shutdown phases of each kernel by reducing the time to move file data in and out of OpenSHMEM process memory to the speed of one-sided memory access. We also support the creation of many small files with minimal overhead. OpenSHMEM workflows can leverage these changes to create more, shorter lived kernels with lower penalty. This API can replace file I/O with code that appears and behaves similar to other OpenSHMEM remote memory accesses. © Springer Nature Switzerland AG 2020.},
author_keywords={Fault tolerance;  Non-volatile memory;  OpenSHMEM;  Persistent memory;  PGAS;  PMEM;  RDMA;  SmartNIC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Basyuk2021528,
author={Basyuk, T. and Vasyliuk, A.},
title={Approach to a subject area ontology visualization system creating},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2870},
pages={528-540},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107204751&partnerID=40&md5=9d7c3b05ca3d2911449f286d2002d0b0},
abstract={This article analyzes the existing technologies and systems for displaying ontologies, presents the features and shows the need to create a new web-based visualization system of this data structure. The display structures analysis is performed and the ontology representation peculiarities in the form of frame models and application of Simple Knowledge Organization System (SKOS) for the Network router concept are shown. The ontology construction stages of the subject area are outlined and described, which include: formation of the list of concepts included in the ontological structure of the upper level; ordering the list of concepts; creation of a glossary; mapping arcs between created concepts. An example of building an ontology for the subject area "information security" in describing the concept features of "network computer attack". The main functions of the system are highlighted and the stage of its design is performed using the object-oriented approach and UML language tools. The diagram of precedents which visualizes the basic variants of the developed system usage is shown. The construction architecture was chosen in favor of three-tiered client-server architecture and the peculiarities of interaction between levels are described. The development language choice peculiarities are given and the choice of libraries and frameworks is proved, which ensure its effective functioning. The result of the work is a prototype of a software system for visualization of ontologies, which can be used to visualize the specified data structure. The study creates the prerequisites for building a full-featured system of visualization of ontologies, to be part of the decision support system for a particular subject area. © 2021 CEUR-WS. All rights reserved.},
author_keywords={Ontograph;  Ontology;  Subject area;  Visualization;  Web system},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xin2021,
author={Xin, Y. and Zhang, D. and Qiu, G.},
title={Real-Time Animation Complexity of Interactive Clothing Design Based on Computer Simulation},
journal={Complexity},
year={2021},
volume={2021},
doi={10.1155/2021/9988623},
art_number={9988623},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107125787&doi=10.1155%2f2021%2f9988623&partnerID=40&md5=6042bba6cae15edd762286f8aa0e3497},
abstract={With the innovation of computer, virtual clothing has also emerged. This research mainly discusses the real-time animation complex of interactive clothing design based on computer simulation. In the process of realizing virtual clothing, the sample interpolation synthesis method is used, and the human body sample library is constructed using the above two methods (primitive construction method and model reconstruction method) first, and then, the human body model is obtained by interpolation calculation according to the personalized parameters. Building a clothing model is particularly important for the effect of trying on. The clothing that needs to be displayed can be scanned and then input into the computer to build the model. The model can be directly built in 3DMAX and other software and then its surface texture can be mapped, or the clothing model can be directly built. The 3D model in the 3ds file is loaded by the loop body nested switch branch selection structure. Correspondingly, the write-back operation of 3ds files is similar. Just follow the general structure of the 3ds file and write the root block, version information block, edit information block, key frame information block, etc. to a brand new file in sequence. The main reason for this article to perform the 3ds file write-back operation is that, after the clothing model is dynamically simulated through the dynamic principle, the deformed key animation frame needs to be saved as a 3ds file so that it can be further imported into the 3DSMAX software and generated by the renderer, form high-quality picture information, and finally get high-definition animation video. In the CPU-GPU hybrid method, modules such as force calculation, collision processing, and position update use the GPU method, while overstretching is processed by the CPU method, making the overall performance 10 times higher than the pure CPU method. This research helps to promote the development of 3D virtual clothing design. © 2021 Yufeng Xin et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Myers202111,
author={Myers, K. and Secco, E.L.},
title={A Low-Cost Embedded Computer Vision System for the Classification of Recyclable Objects},
journal={Lecture Notes on Data Engineering and Communications Technologies},
year={2021},
volume={61},
pages={11-30},
doi={10.1007/978-981-33-4582-9_2},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106413732&doi=10.1007%2f978-981-33-4582-9_2&partnerID=40&md5=e302f2d29be9b2e8117a0f0710d58e5c},
abstract={Due to rapid urbanization, increasing population and industrialization, there has been a sharp rise in solid waste pollution across the globe. Here, we present a novel solution to this inefficiency, by the use of embedded computer vision (CV) in the material recovery facilities (MRF). The proposed architecture employs software (i.e., TensorFlow and OpenCV) and hardware (i.e., Raspberry Pi) as an embedded platform in order to classify daily life objects according to their visual aspect. The CV system is trained using modules contained within the TensorFlow API with two datasets, namely the TrashNet and a combination of the TrashNet and a set of Web images. This solution allows greater accuracy, with a baseline performance of 90% which drops to 70% when deployed on the embedded platform, due to the quality of the images taken by an integrated camera for the real-time classification. The speed results are also promising with a baseline speed of 10 FPS at simulation level, which drops to 1. 4fps when running on the platform. Such a system is cheap at less than £100, it is perfectly adequate to be used to identify recyclables in the MRF for sorting. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Computer vision;  TensorFlow low-cost prototype},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Op’tLand2021228,
author={Op ’t Land, M. and Krouwel, M.R. and Gort, S.},
title={Testing the Concept of the RUN-Time Adaptive Enterprise: Combining Organization and IT Agnostic Enterprise Models with Organization Implementation Variables and Low Code Technology},
journal={Lecture Notes in Business Information Processing},
year={2021},
volume={411 LNBIP},
pages={228-242},
doi={10.1007/978-3-030-74196-9_13},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105961478&doi=10.1007%2f978-3-030-74196-9_13&partnerID=40&md5=b5b3103ee9c77f7f36d65ec19a86243e},
abstract={Our research program aims at finding and testing methods that allow organizations to become more RUN-time adaptive enterprises, with a near-zero time-to-market for new or changed products and services. Indeed, to thrive in increasingly more disruptive environments, an enterprise’s IT time-to-market should sustainably no longer be on the critical path of business time-to-market. Earlier research suggests that key elements for such a method are (a) an organization and IT agnostic way of modeling products and services and its collaboration network, using DEMO, (b) a universal transaction pattern of coordination and production acts as atoms for building business processes, and (c) designing the aspects in which organizational adaptivity is desired, in terms of Organization Implementation Variables (OIVs). To test the combined application of these elements in practice, ICTU and Capgemini conducted a Proof of Concept on Social Housing during 5 weeks. The resulting application, built with the Mendix low code platform, showed the ability to reorganize – in this case: reconfiguring accountabilities inside and across organizations – at run-time, without the need to change software. Future research should clarify to what extent such intrinsic application adaptivity can be increased, e.g., by (a) embedding the use of APIs, (b) automated support of all coordination acts in the universal transaction pattern, and (c) parametrizing more OIVs. © 2021, Springer Nature Switzerland AG.},
author_keywords={Agile;  DEMO;  Enterprise engineering;  Intrinsic application adaptivity;  Low code;  Organization implementation;  Time-to-market},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ali202182,
author={Ali, N. and Lai, R.},
title={Global Software Development: A Review of its Practices},
journal={Malaysian Journal of Computer Science},
year={2021},
volume={34},
number={1},
pages={82-129},
doi={10.22452/mjcs.vol34no1.5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101533538&doi=10.22452%2fmjcs.vol34no1.5&partnerID=40&md5=85744ea74df2f77193cd36516e375306},
abstract={Global Software Development (GSD) is multi-site software development with software teams scattered across different places around the world. To gain the benefits of the lower cost of software development and access to international talents, many organizations are using GSD. However, earlier studies reported that achieving these benefits can be difficult and GSD involves risks. This paper presents a systematic review of GSD and aim to provide the findings about the current practices, benefits, associated risks, and difficulties associated with it.The review was conducted in accordance with the systematic review procedures and processes defined by Kitchenham. By following these guidelines, six digital libraries to gather information on the prevalent trends and practices in GSD were searched. The outcomes presented in this study are based on 204 studies, published in peer-reviewed conferences and journals. Our findings will enable readers to understand the current practices, benefits, risks and difficulties associated with GSD. As a result, they can form realistic expectations before making a decision to engage in GSD or not, and formulate better pre-contract and post-contract planning in order to increase the chances of project success. A merit of this paper as compared to previous survey papers on GSD is that it reports the issues related to cultural diversity, requirements engineering and software architecture. © 2021. All rights reserved.},
author_keywords={distributed teams;  Global software development;  systematic review},
document_type={Article},
source={Scopus},
}

@ARTICLE{Timokhin202167,
author={Timokhin, P.Y. and Mikhaylyuk, M.V. and Vozhegov, E.M. and Panteley, K.D.},
title={Technology and Methods for Deferred Synthesis of 4K Stereo Clips for Complex Dynamic Virtual Scenes},
journal={Programming and Computer Software},
year={2021},
volume={47},
number={1},
pages={67-75},
doi={10.1134/S0361768820070063},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101347162&doi=10.1134%2fS0361768820070063&partnerID=40&md5=02ea023178c450d374d82faa30d95d7b},
abstract={Abstract: We consider the task of capturing the result of a researcher-driven stereo visualization of a complex dynamic virtual scene into a video sequence of stereo pairs (stereo clip) of ultrahigh resolution. An efficient technology of deferred synthesis of stereo clips is proposed. It allows one to create stereo clips without interfering with real-time visualization. The technology includes real-time construction of visualization scenario and offline transformation of the scenario into a stereo clip. Methods for implementing these stages for the task of stereo visualization of the saturation isosurface of displacing fluid are considered. For this purpose, a special file format «scr» for the visualization scenario is developed on the basis of chunk data structures. This format provides a compact representation of neighboring repeated frames. The scenario file is transformed into a sequence of 4K stereo pairs by means of the offscreen rendering technology of the virtual scene, and stereo pairs are added to the stereo clip using a number of open-source FFmpeg libraries designed for processing digital video content. The generated stereo clip is placed within an MP4 container, and the video compressing standard H.264 is used. The proposed technologies and methods of 4K stereo clip deferred synthesis are implemented in a software designed for visualizing simulation results of the unstable displacement of oil from porous media. Using this software, a 4K stereo clip is created, which illustrates the evolution of the saturation isosurface during the process of unstable oil displacement. Testing confirmed the validity of the solution. The software can be used in virtual laboratories, for designing virtual environment systems and scientific visualization systems, in educational applications etc. © 2020, Pleiades Publishing, Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kildeeva202157,
author={Kildeeva, S.S. and Katasev, A.S. and Talipov, N.G.},
title={Models and Methods of Forecasting and Tasks Distribution by Performers in Electronic Document Management Systems},
journal={Studies in Systems, Decision and Control},
year={2021},
volume={333},
pages={57-71},
doi={10.1007/978-3-030-63563-3_6},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101161097&doi=10.1007%2f978-3-030-63563-3_6&partnerID=40&md5=a118578d8efbe280427fb2ae468e22c8},
abstract={This paper describes the problem of task distribution received through the electronic document management system. The fuzzy-production model underlying the solution to this problem is described. Based on the proposed model, a software package was developed for decision-making support of task performers selection, its structure is presented. A model of task distribution is considered taking into account its forecast values. The basic steps in forecasting model construction are described. The effectiveness of this approach for task distribution, based on workload indicators of specialists with different levels of working capacity and qualifications, is shown. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Data mining;  Electronic document management system;  Forecasting;  Fuzzy production model;  Task distribution},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Kamidoi202124390,
author={Kamidoi, Y. and Yamauchi, R. and Wakabayashi, S.},
title={A Protocol for Preventing Transaction Commitment without Recipient's Authorization on Blockchain and It's Implementation},
journal={IEEE Access},
year={2021},
volume={9},
pages={24390-24405},
doi={10.1109/ACCESS.2021.3056623},
art_number={9344667},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100751563&doi=10.1109%2fACCESS.2021.3056623&partnerID=40&md5=352ef65353b7afd3ce91ae26f892713a},
abstract={In recent years, blockchain is utilized practically as a distributed secure digital ledger of some sorts of transactions. Blockchain is regarded as one of the most important next generation infrastructure technologies of the financial industry, as well as artificial intelligence and big data. In 2020, cryptocurrencies based on blockchain, such as Bitcoin, Ethereum, or XRP, have a value of more than $450 billion in the market capitalization. Furthermore, on blockchains such as Ethereum, transactions can also represent automatic executions of programs, which are called smart contracts. Thus, many institutes in various categories show their positive attitude toward processing financial transactions or non-financial contracts on blockchain. Although many researchers have studied for various types of issues on blockchain, there always exist security and privacy concerns for blockchain. In this paper, we point out a new concern for abusing the publicity of blockchain and also show the possibility of suspicions aroused by the concern. Then we propose a selective mechanism for self-protecting against the approach from crimes or computer viruses on blockchain, whether the disclosure of user's privacy occurs or not. Next, we also propose a concrete implementation of our proposed selective mechanism with two new address types. We aim to incorporate the mechanism in Bitcoin Core, which is the official Bitcoin client software, and using libbitcoin library functions for Bitcoin software development. We show experimental results to estimate overhead costs for processing our proposed address types toward processing the current standard address type in nodes on the peer-to-peer network. © 2013 IEEE.},
author_keywords={abuse of publicity;  Blockchain;  implementation;  recipient's authorization;  security},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li202118085,
author={Li, B. and Feng, F. and Chen, X. and Cao, Y.},
title={Reconfigurable and High-Efficiency Password Recovery Algorithms Based on HRCA},
journal={IEEE Access},
year={2021},
volume={9},
pages={18085-18111},
doi={10.1109/ACCESS.2021.3053068},
art_number={9328759},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099733774&doi=10.1109%2fACCESS.2021.3053068&partnerID=40&md5=b71630247b63d0de9a8ef10f99e5ea7b},
abstract={Cryptographic algorithms are widely used in information security fields such as network protocol authentication and commercial encryption software. Password recovery based on the hash algorithm is an important means of electronic forensics, encrypted information restoration, illegal information filtering, and network security maintenance. The traditional password recovery system is based mainly on the CPU and GPU and has a low energy efficiency ratio and cracking efficiency and cannot meet high-performance computing requirements. To further improve the computational efficiency and application flexibility of password recovery algorithms, this paper proposes a reconfigurable computing kernel design method based on a hybrid reconfigurable computing array (HRCA). Through in-depth analysis of the hash algorithm, the basic computing kernel set is extracted, and the combination design is carried out from the unit kernel, interconnection and storage structure to reconstruct the hash algorithm to match the application with the appropriate structure. Second, combined with the pipeline technology, the full pipeline hash and high-speed password attack algorithms are optimized and implemented to meet the needs of high-performance computing. Finally, an advanced computing kernel library is established, and the combination of a computing kernel map from the control and communication levels to achieve multidimensional reconfigurable computing and an overall placement strategy is used to make full use of the chip resources to improve computational efficiency. The experimental results and analysis show that compared with traditional CPU and GPU methods, the password recovery algorithm designed in this paper has the highest cracking speeds at 78.22 times and 2.65 times that of the CPU and GPU, respectively, and the highest energy efficiency ratio is 25.88 times and 3.16 times that of the CPU and GPU, respectively. Furthermore, the recovery efficiency has been significantly improved and meets the requirements of high-performance password recovery computing. © 2013 IEEE.},
author_keywords={computing kernel;  hash algorithm;  HRCA;  password recovery;  reconfigurable},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen202114576,
author={Chen, L. and Xia, C. and Lei, S. and Wang, T.},
title={Detection, Traceability, and Propagation of Mobile Malware Threats},
journal={IEEE Access},
year={2021},
volume={9},
pages={14576-14598},
doi={10.1109/ACCESS.2021.3049819},
art_number={9316662},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099538360&doi=10.1109%2fACCESS.2021.3049819&partnerID=40&md5=f0d914af1fc88540f716971fe976f5b7},
abstract={In recent years, the application of smartphones, Android operating systems and mobile applications have become more prevalent worldwide. To study the traceability, propagation, and detection of the threats, we perform research on all aspects of the end-to-end environment. With machine learning based on the mobile malware detection algorithms that integrate the dynamic and static research of the identification algorithm, application software samples are collected to study sentences. Through knowledge labeling and knowledge construction, the association relationship of knowledge is extracted to realize the research of knowledge map construction. Flooding is closely correlated with the complexity of the Android mobile version of the kernel and malicious programs. A static dynamic analysis of the mobile malicious program is carried out, and the social network social diagram is constructed to model the propagation of the mobile malicious program. We extended the approach of deriving common malware behavior through graph clustering. On this basis, Android behavior analysis is performed through our virtual machine execution engine. We extend the family characteristics to the concept of DNA race genes. By studying SMS/MMS, Bluetooth, 5G base station networks, metropolitan area networks, social networks, homogeneous communities, telecommunication networks, and application market ecosystem propagation scenarios, we discovered the law of propagation. In addition, we studied the construction of the mobile Internet big data knowledge graph. Quantitative data for the main family chronology of mobile malware are obtained. We conducted detailed research and comprehensive analysis of Android application package (APK) details and behavior, relationship, resource-centric, and syntactic aspects. Furthermore, we summarized the architecture of mobile malware security analysis. We also discuss encryption of malware traffic discrimination. These precise modeling and quantified research results constitute the architecture of mobile malware analysis. © 2013 IEEE.},
author_keywords={Android mobile malware;  architecture of mobile malware security analysis;  detection analysis;  family chronology;  infected system environment;  knowledge map construction;  propagation models;  threat traceability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Uribe-Hurtado202131,
author={Uribe-Hurtado, A.-L. and Villegas-Jaramillo, E.-J. and Orozco-Alzate, M.},
title={Parallel implementation of nearest feature line and rectified nearest feature line segment classifiers using openmp},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1237 AISC},
pages={31-40},
doi={10.1007/978-3-030-53036-5_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089723255&doi=10.1007%2f978-3-030-53036-5_4&partnerID=40&md5=8f3e71ffd8c2f13081bb01bf46129dec},
abstract={Parallelizing computationally expensive classification algorithms, such as the Rectified Nearest Feature Line Segment (RNFLS), remains a task for expert programmers due to the complexity involved in rewriting the application and the required knowledge of the available hardware and tools. A simple parallel implementation of the Nearest Feature Line (NFL) and the RNFLS algorithms using OpenMP over multicore architectures is presented. Both non-parametric classifiers are used in the classification of datasets that contain few samples. The training and testing evaluation technique is used, with a 70–30 ratio respectively and seven datasets from the UCI repository, to verify the speedup and the accuracy of the classifier. Results and experiments derived from the parallel execution of both algorithms, calculating the average of 20 repetitions on a small architecture of 6 physical cores and 12 ones with multi-threading, show that accelerations of up to 21 times can be achieved with NFL and up to 13 times with RNFLS. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2021.},
author_keywords={Nearest Feature Line;  OpenMP;  Parallel computing;  Rectified Nearest Feature Line},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kamensky2021634,
author={Kamensky, D.},
title={Open-source immersogeometric analysis of fluid–structure interaction using FEniCS and tIGAr},
journal={Computers and Mathematics with Applications},
year={2021},
volume={81},
pages={634-648},
doi={10.1016/j.camwa.2020.01.023},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079852745&doi=10.1016%2fj.camwa.2020.01.023&partnerID=40&md5=5e6a08065667e82e0dea0478aac28c48},
abstract={We recently developed the open-source library tIGAr, which extends the FEniCS finite element automation framework to isogeometric analysis. The present contribution demonstrates the utility of tIGAr in complex problems by applying it to immersogeometric fluid–structure interaction (FSI) analysis. This application is implemented as the new open-source library CouDALFISh (Coupling, via Dynamic Augmented Lagrangian, of Fluids with Immersed Shells, pronounced “cuttlefish”), which uses the dynamic augmented Lagrangian (DAL) method to couple fluid and shell structure subproblems. The DAL method was introduced previously, over a series of papers largely focused on heart valve FSI, but an open-source implementation making extensive use of automation to compile numerical routines from high-level mathematical descriptions brings newfound transparency and reproducibility to these earlier developments on immersogeometric FSI analysis. The portions of CouDALFISh that do not use code generation also illustrate how a framework like FEniCS remains useful even when some functionality is outside the scope of its standard workflow. This paper summarizes the workings of CouDALFISh and documents a variety of benchmarks demonstrating its accuracy. Although the implementation emphasizes transparency and extensibility over performance, it is nonetheless demonstrated to be sufficient to simulate 3D FSI of an idealized aortic heart valve. Source code will be maintained at https://github.com/david-kamensky/CouDALFISh. © 2020 Elsevier Ltd},
author_keywords={FEniCS;  Fluid–structure interaction;  Heart valve;  Immersed boundary;  Isogeometric analysis;  Open-source software},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Thakre2020,
author={Thakre, M.P. and Sayali, S.S. and Jain, M.A.},
title={Stability and Total Harmonic Distortion Analysis with Performance of Grid-Tied PV Systems},
journal={ICPECTS 2020 - IEEE 2nd International Conference on Power, Energy, Control and Transmission Systems, Proceedings},
year={2020},
doi={10.1109/ICPECTS49113.2020.9337036},
art_number={9337036},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101091637&doi=10.1109%2fICPECTS49113.2020.9337036&partnerID=40&md5=0b78da30a962cfb786c76b2d810bb6df},
abstract={The stability and Total Harmonic Distortion (THD) of the grid integration photovoltaic (PV) array was discussed and analyzed in this article using two separate maximum power point (MPPT) tracking algorithms. A PV array access to the grid by means of a VSI and a boost converter were applied in this work. The structures in the PSCAD environment were simulated, including MPPT methodologies for Incremental Conductivity (InC) and Perturbation Observation (PO). Software package is used in both normal and temporary environments to evaluate grid disorders. The results indicate that the InC MPPT procedure provides excellent response in normal condition than those of the PO MPPT control system. That being said, in both temporary conditions the PO MPPT algorithm offers more recovery time than the InC MPPT algorithm. The entire model postulates the architecture of a hybrid MPPT algorithm in such a way that it operates efficiently under both temporary and normal conditions. Power converter and inverter use both PWM and SPWM switching methodologies during this whole approach. Finally, an analysis of the overall THD of the PCC output current of the inverter is being used, compared with the limits of the THD values obtained by regulatory standards such as IEEE Std 519-1992. © 2020 IEEE.},
author_keywords={distributed generation;  MPPT;  PV system;  Smart Grid (SG);  Stability;  total harmonic distortion},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu20202282,
author={Liu, X. and Bu, F. and Sun, T. and Qin, H.},
title={The analysis on the role of social network in the field of anti-terrorism-take the "East Turkistan" organization as an example},
journal={Proceedings - 2020 5th International Conference on Mechanical, Control and Computer Engineering, ICMCCE 2020},
year={2020},
pages={2282-2285},
doi={10.1109/ICMCCE51767.2020.00493},
art_number={9421847},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106954245&doi=10.1109%2fICMCCE51767.2020.00493&partnerID=40&md5=8d8b58a7abb511e81a79a9f4572337e1},
abstract={In order to analyze the network structure of the "East Turkistan" organization and identify members with higher individual threats, so as to conduct a better attack on the "East Turkistan" organization. We use Gephi, a network analysis software, and collect open source information on three batches of 25 terrorists announced by the Ministry of Public Security and use networkX, a Python package to study the structure of complex networks, to conduct social network analysis(SNA) of the "East Turkistan" organization personnel. The "East Turkistan" organization presents a combination of scale-free network and hierarchical network in the topological structure. To manifest the analysis of social network methods' importance in combating terrorism, we simulate the effect of removing the core members of the "East Turkistan" organization. The construction of SNA requires a large deal of open source information. How to obtain intelligence is an important factor that restricts precision strikes. In addition, how to combine social network analysis with computer deep learning knowledge is also a new hot spot and direction in the field of anti-terrorism. © 2020 IEEE.},
author_keywords={Anti-terrorism;  East Turkistan;  Gephi;  Python;  Social network analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suri2020386,
author={Suri, G. and Fu, J. and Zheng, R. and Liu, X.},
title={Family identification of AGE-generated android malware using tree-based feature},
journal={Proceedings - 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2020},
year={2020},
pages={386-393},
doi={10.1109/TrustCom50675.2020.00060},
art_number={9343226},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101250514&doi=10.1109%2fTrustCom50675.2020.00060&partnerID=40&md5=48d835006689208e7ae69925d9fb8f84},
abstract={Application Generation Engine(AGE) is a development tool that can automatically generate simple Android applications by utilizing some boilerplate codes. People with little software programming background could also develop Android applications by using this tool based on their requirements. The emergence of AGE dramatically improves the ease of developing essential software and lowers the level of programming skills required for app developers. However, it also provides easy access for attackers to quickly develop a large number of malicious applications, which will seriously affect the device and data security of regular users. Since AGE mainly generates applications based on some boilerplate codes, the code structures of malicious apps created by AGE have a high degree of similarity when these apps belong to the same family. Based on the assumption that the package directory structures of the software from the same family are also similar, we designed a novel feature construction method to describe the application. Using this method, we extracted features from the leaf nodes of the smali tree, while each smali tree corresponds to the smali directory of the application. Unlike traditional static feature extraction of applications, the tree-based feature proposed in this paper can effectively counteract problems such as code obfuscation or reflection cause it can adequately reflect the semantic features of the smali files. To prove the effectiveness of tree-based features, we also conducted some experiments based on a dataset provided by the enterprise. This dataset contains 1792 AGE-generated applications, and these applications belong to 17 malicious families. We demonstrated that the feature construction method proposed in this paper is usable and can be applied to machine learning classification algorithms for the identification of malicious applications. © 2020 IEEE.},
author_keywords={AAndroid Malware;  AGE;  Family identification;  Tree-based feature},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tello-Rodríguez2020584,
author={Tello-Rodríguez, M. and Ocharán-Hernández, J.O. and Pérez-Arriaga, J.C. and Limón, X. and Sánchez-García, Á.J.},
title={A Design Guide for Usable Web APIs},
journal={Programming and Computer Software},
year={2020},
volume={46},
number={8},
pages={584-593},
doi={10.1134/S0361768820080241},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098464584&doi=10.1134%2fS0361768820080241&partnerID=40&md5=70169b23f11fbf94e7729fba559a4168},
abstract={Abstract: Cloud computing trends such as Software as a Service (SaaS) enable providers to host complex applications over the Internet, making them available to external consumers through an Application Programming Interface (API). The success of a SaaS, and in some sense any distributed system, is greatly influenced by its API. Highly usable APIs improve the efficiency of the development process and its quality, ensuring that programmers continue to appreciate other aspects of the API while increasing their productivity. Different studies state that the design phase within the development process of an API is the most appropriate to address usability issues. Therefore, usability should be considered as an explicit criterion in the design of an API. In this paper, we propose a design guide for web APIs with an emphasis on usability, using the best practices of usable web APIs design. Our design guide is based on an adaptation of the design science research methodology (DSRM), and it is complemented with a systematic literature review and gray literature analysis concerning methods, techniques, and tools used to develop usable APIs. © 2020, Pleiades Publishing, Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Diara20201,
author={Diara, F. and Rinaudo, F.},
title={Ifc classification for foss hbim: Open issues and a schema proposal for cultural heritage assets},
journal={Applied Sciences (Switzerland)},
year={2020},
volume={10},
number={23},
pages={1-23},
doi={10.3390/app10238320},
art_number={8320},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096574040&doi=10.3390%2fapp10238320&partnerID=40&md5=bafcc4406287bd29667da4b540001f89},
abstract={The IFC (Industry Foundation Classes) open format has been developed by BuildingSMART and regularized through ISO standards. It has been implemented into a BIM (Building Information Modeling) informative system for the AEC industry (Architecture Engineering and Construction). The IFC format has changed interoperability processes concerning architectural and technical entities in a semantic way. However, because this standard open format was specifically designed for the modern AEC industry, it may not cater to the demands of cultural heritage assets. Since IFC classification is fundamental for informative systems, it should become a standard also concerning heritage assets, even if nowadays there is no regularized IFC classification for historical existing buildings. Specific cultural heritage peculiarities therefore need semantic classification based on historical asset families. For this reason, this work is based on a proposal and experimental IFC classification implemented inside an HBIM open source software (FreeCAD), whereby limitations of IFC standards can be overcome thanks to the freedom of access to libraries and codes. Moreover, this work is based on IFC objects management outside the platform for interoperability purposes. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={BIM;  Cultural heritage;  DBMS;  FOSS;  HBIM;  IFC;  Interoperability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Taylor2020,
author={Taylor, N.T. and Davies, F.H. and Rudkin, I.E.M. and Price, C.J. and Chan, T.H. and Hepplestone, S.P.},
title={ARTEMIS: Ab initio restructuring tool enabling the modelling of interface structures},
journal={Computer Physics Communications},
year={2020},
volume={257},
doi={10.1016/j.cpc.2020.107515},
art_number={107515},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089105509&doi=10.1016%2fj.cpc.2020.107515&partnerID=40&md5=c2142ebf0522e404ee8343d1ec772c40},
abstract={A program, ARTEMIS, has been developed for the study of interface structures. This software allows for the generation of interfaces by identifying lattice matches between two parent crystal structures. To allow for further exploration of the energetic space of the interface, multiple surface terminations parallel to the Miller plane, interface alignments and intermixings are used to generate sets of potential interfaces for each lattice match. These interface structures can then be used in atomic simulations to determine the most energetically favourable interface. The software reported here can help to both drastically reduce the work of generating and exploring interfaces, and aid in understanding of how the interface structure influences the subsequent properties. Using several test cases, we demonstrate how ARTEMIS can both identify the location of an interface in existing structures, and also predict an optimum interface separation based upon the parents’ atomic structures, which aims to accelerate and inform the study of interface science. Program summary: Program Title: ARTEMIS CPC Library link to program files: https://dx.doi.org/10.17632/5bcrh67xty.1 Developer's repository link: http://www.artemis-materials.co.uk/ Licensing provisions: CC BY NC 3.0 Programming language: Fortran 2003 Nature of problem: Construction and identification of the interface between any two crystals. Complicating factors include the choice of Miller planes, alignment of the two crystals and potential intermixing of them. Solution method: This problem is tackled by generating sets of interface structures that allow the user to explore the energy space using atomic simulations in order to identify the most favourable interface to form between two such crystals. Additional comments: The source code and working examples can be found in the compressed file obtainable from http://www.artemis-materials.co.uk/. The code has been tested and developed using the GNU 7.2.0 and the Intel 17.0.4 Fortran compilers on Unix/Linux operating systems. © 2020 The Authors},
author_keywords={Atomic modelling;  Crystals;  Interface;  Lattice matching;  Miller plane;  Surfaces},
document_type={Article},
source={Scopus},
}

@ARTICLE{Salha2020,
author={Salha, L. and Bleyer, J. and Sab, K. and Bodgi, J.},
title={Mesh-adapted stress analysis of multilayered plates using a layerwise model},
journal={Advanced Modeling and Simulation in Engineering Sciences},
year={2020},
volume={7},
number={1},
doi={10.1186/s40323-020-0142-y},
art_number={2},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079119699&doi=10.1186%2fs40323-020-0142-y&partnerID=40&md5=39d36074703bd38837da36da854ebf2f},
abstract={This paper proposes a new finite-element modelling of a recent layerwise model for multilayered plates. This layerwise model is built from a specific 3D stress-field expansion along the thickness direction and involves, in particular, interlaminar transverse shear and out-of-plane stresses as generalized stresses. Its main feature is that 3D equilibrium equations and free-edge boundary conditions are directly taken into account into the stress-based construction of the model. A dual displacement-based finite-element discretization is implemented using the FEniCS software package and a remeshing strategy is proposed based on a novel error indicator. The error indicator is built based on the 3D stress field directly deduced from the layerwise generalized stresses and compared to a reconstructed stress field based on the model generalized displacements. The proposed error indicator is shown to identify the most critical parts of a laminate structure associated with complex 3D stress fields such as boundaries or stress concentration/singularity regions (near free-edges or delamination fronts). Through the combination of thickness discretization and in-plane mesh refinement in regions of interest, the proposed framework therefore offers an attractive alternative to 3D solid finite elements for an accurate prediction of stress states in composite laminates. © 2020, The Author(s).},
author_keywords={Finite element;  Free-edge;  Interlaminar stresses;  Layerwise model;  Multilayer},
document_type={Article},
source={Scopus},
}

@ARTICLE{Beskrovny2020502,
author={Beskrovny, A.S. and Bessonov, L.V. and Ivanov, D.V. and Kirillova, I.V. and Kossovich, L.Y.},
title={Using the Mask-RCNN convolutional neural network to automate the construction of two-dimensional solid vertebral models},
journal={Izvestiya of Saratov University. New Series. Series: Mathematics. Mechanics. Informatics},
year={2020},
volume={20},
number={4},
pages={502-516},
doi={10.18500/1816-9791-2020-20-4-502-516},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098330442&doi=10.18500%2f1816-9791-2020-20-4-502-516&partnerID=40&md5=b231a8a1b18dff18964c7d00d1ee5b13},
abstract={Biomechanical modeling requires the construction of an accurate solid model of the object under study based on the data of a particular patient. This problem can be solved manually using modern software packages for medical data processing or using computer-aided design systems. This approach is used by many researchers and allows you to create accurate solid models, but is time consuming. In this regard, the automation of the construction of solid models suitable for performing biomechanical calculations is an urgent task and can be carried out using neural network technologies. This study presents the implementation of one of the methods for processing computed tomography data in order to create two-dimensional accurate solid models of vertebral bodies in a sagittal projection. An artificial neural network Mask-RCNN was used for automatic recognition of vertebrae. The assessment of the quality of the automatic recognition performed by the neural network was carried out on the basis of comparison with the Sorensen measure with manual segmentation performed by practitioners. Application of the method makes it possible to significantly speed up the process of modeling bone structures of the spine in 2D mode. The implemented technique was used in the development of a solid-state model module, which is included in the SmartPlan Ortho 2D medical decision support system developed at Saratov State University within the framework of the Advanced Research Foundation project. © 2020 Saratov State University. All rights reserved.},
author_keywords={2D segmentation;  Biomechanical modeling;  Convolutional neural network;  DiCOM;  SmartPlan Ortho 2D;  Solid model;  Sorensen measure},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ciesko202035,
author={Ciesko, J. and Poliakoff, D. and Hollman, D.S. and Trott, C.C. and Lebrun-Grandie, D.},
title={Towards generic parallel programming in computer science education with kokkos},
journal={Proceedings of EduHPC 2020: Workshop on Education for High Performance Computing, Held in conjunction with SC 2020: The International Conference for High Performance Computing, Networking, Storage and Analysis},
year={2020},
pages={35-42},
doi={10.1109/EduHPC51895.2020.00010},
art_number={9308747},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101155509&doi=10.1109%2fEduHPC51895.2020.00010&partnerID=40&md5=e1197b3c5cb53f32ab20108021322d5a},
abstract={Parallel patterns, views, and spaces are promising abstractions to capture the programmer's intent as well as the contextual information that can be used by an underlying runtime to efficiently map software to parallel hardware. These abstractions can be valuable in cases where an algorithm must accommodate requirements of code and performance portability across hardware architectures and vendor programming models. Kokkos is a parallel programming model for host- and accelerator architectures that relies on these abstractions and targets these requirements. It consists of a pure C++ interface, a specification, and a programming library. The programming library exposes patterns and types and maps them to an underlying abstract machine model. The abstract machine model offers a generic view of parallel hardware. While Kokkos is gaining popularity in large-scale HPC applications at some DOE laboratories, we believe that the implemented concepts are of interest to a broader audience including academia as they may contribute to a generic, vendor, and architecture-independent education of parallel programming. In this work, we give an insight into the design considerations of this programming model and list important abstractions. Further, we document best practices obtained from giving virtual classes on Kokkos and give pointers to resources that the reader may consider valuable for a lecture on generic parallel programming for students with preexisting knowledge on this matter. © 2020 IEEE.},
author_keywords={C++;  Kokkos;  Parallel programming},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Herbst2020,
author={Herbst, M.F. and Scheurer, M. and Fransson, T. and Rehn, D.R. and Dreuw, A.},
title={adcc: A versatile toolkit for rapid development of algebraic-diagrammatic construction methods},
journal={Wiley Interdisciplinary Reviews: Computational Molecular Science},
year={2020},
volume={10},
number={6},
doi={10.1002/wcms.1462},
art_number={e1462},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078592695&doi=10.1002%2fwcms.1462&partnerID=40&md5=d401673248c2c44f28631c300b450539},
abstract={ADC-connect (adcc) is a hybrid python/C++ module for performing excited state calculations based on the algebraic-diagrammatic construction scheme for the polarization propagator (ADC). Key design goal is to restrict adcc to this single purpose and facilitate connection to external packages, for example, for obtaining the Hartree–Fock references, plotting spectra, or modeling solvents. Interfaces to four self-consistent field codes have already been implemented, namely pyscf, psi4, molsturm, and veloxchem. The computational workflow, including the numerical solvers, is implemented in python, whereas the working equations and other expensive expressions are done in C++. This equips adcc with adequate speed, making it a flexible toolkit for both rapid development of ADC-based computational spectroscopy methods as well as unusual computational workflows. This is demonstrated by three examples. Presently, ADC methods up to third order in perturbation theory are available in adcc, including the respective core-valence separation and spin-flip variants. Both restricted or unrestricted Hartree–Fock references can be employed. This article is categorized under: Software > Simulation Methods Electronic Structure Theory > Ab Initio Electronic Structure Methods Theoretical and Physical Chemistry > Spectroscopy Software > Quantum Chemistry. © 2020 Wiley Periodicals, Inc.},
author_keywords={algebraic diagrammatic construction methods;  computational spectroscopy;  python},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hassan20201887,
author={Hassan, S.U. and Gridin, I. and Delgado-Lozano, I.M. and García, C.P. and Chi-Domínguez, J.-J. and Aldaya, A.C. and Brumley, B.B.},
title={Déjà Vu: Side-Channel Analysis of Mozilla's NSS},
journal={Proceedings of the ACM Conference on Computer and Communications Security},
year={2020},
pages={1887-1902},
doi={10.1145/3372297.3421761},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096157246&doi=10.1145%2f3372297.3421761&partnerID=40&md5=ea1931297b5a0d83dd78f23c9819233e},
abstract={Recent work on Side Channel Analysis (SCA) targets old, well-known vulnerabilities, even previously exploited, reported, and patched in high-profile cryptography libraries. Nevertheless, researchers continue to find and exploit the same vulnerabilities in old and new products, highlighting a big issue among vendors: effectively tracking and fixing security vulnerabilities when disclosure is not done directly to them. In this work, we present another instance of this issue by performing the first library-wide SCA security evaluation of Mozilla's NSS security library. We use a combination of two independently-developed SCA security frameworks to identify and test security vulnerabilities. Our evaluation uncovers several new vulnerabilities in NSS affecting DSA, ECDSA, and RSA cryptosystems. We exploit said vulnerabilities and implement key recovery attacks using signals - -extracted through different techniques such as timing, microarchitecture, and EM - -and improved lattice methods. © 2020 Owner/Author.},
author_keywords={applied cryptography;  CVE-2020-12399;  CVE-2020-12401;  CVE-2020-12402;  CVE-2020-6829;  DSA;  ECDSA;  lattice-based cryptanalysis;  NSS;  public key cryptography;  RSA;  side-channel analysis;  software security},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nikolov2020113,
author={Nikolov, N. and Nakov, O.},
title={Design and Implementation of a System for Remote IoT Device Management},
journal={28th National Conference with International Participation, TELECOM 2020 - Proceedings},
year={2020},
pages={113-116},
doi={10.1109/TELECOM50385.2020.9299568},
art_number={9299568},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099483335&doi=10.1109%2fTELECOM50385.2020.9299568&partnerID=40&md5=54c0dacd8937906fc7dce46b1e07e445},
abstract={In this article are described the design and implementation of a system for remote IoT device management. The idea is to control the remote devices using 3G connection, where the internet or WiFi is not available. In the article are shown the general architecture, hardware and software realization. © 2020 IEEE.},
author_keywords={Device;  Embedded System;  IoT;  MQTT;  RESTfull API},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Silva2020332,
author={Silva, D. and Gomes, A. and Macieira, R. and Silva, E. and Soares, S.},
title={Pah Pum: A project management tool based on TAKT PM},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={332-337},
doi={10.1145/3422392.3422504},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099347074&doi=10.1145%2f3422392.3422504&partnerID=40&md5=6f81a2d7db1658e79b4392cecc7d2401},
abstract={This article presents Pah Pum, a project management tool based on Takt PM, a hybrid project management methodology. The tool was developed at SENAI Institute of Innovation for Information and Communication Technologies to facilitate adherence to Takt PM, offering processes systematization and using best practices suggested by the methodology. The tool supports (1) project planning using a service package diagram called Handoff Network, (2) project monitoring through Kanban and Kanban Quantum boards, (3) automatic Gantt generation and update, according to the project Handoff Network and service packages deliveries, (4) monitoring teams through the observation of a unified board that demonstrates the allocation of the team with the activities that are running or waiting to be performed, (5) automatic allocation of service packages to the teams considering the limit of in progress activities of the team configured to the organization, implementing the pull system, (6) status report of service package delivery and project progress, in a non-declarative way with evidences, and (7) visualization of the project timeline showing the transaction history that includes the project start, service packages deliveries, and service packages rejections that were performed during the project. We used an MVC software architecture to implement the solution, composed by front-end in Vue.js, an API following the REST standard in .Net core, and a SQL Server database. Also, the tool was integrated with the SENAI Technology Management System (a particular ERP system). All modules developed were dockerized and deployed on AWS machines. To access the tool, sign up on https://pahpum.isitics.com, and the administrators will approve the access. We are currently evaluating the tool through a qualitative study to understand the positive and negative impacts through the tool's adoption during the execution of real projects. © 2020 ACM.},
author_keywords={Agile methodologies;  Project management;  Project planning;  Software development;  Takt PM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yeoh202094,
author={Yeoh, C.E. and Kim, D.B. and Won, Y.B. and Lee, S.R. and Yi, H.},
title={Constructing ROS package for legged robot in gazebo simulation from scratch},
journal={International Conference on Control, Automation and Systems},
year={2020},
volume={2020-October},
pages={94-99},
doi={10.23919/ICCAS50221.2020.9268358},
art_number={9268358},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098096259&doi=10.23919%2fICCAS50221.2020.9268358&partnerID=40&md5=3af393739c5d50e01367e6916010b73d},
abstract={Robot Operating System, (ROS) is one of the open-source, meta-operating system, which is now widely used as the robotic software platform and can be applicable for anyone who wanted to build their robot from scratch. For the credit of the beneficial of ROS, the work of this paper describes all the process and structure of the package construction for the legged robot simulation in Gazebo. There are five mains folders consisted in the package, which are configuration file (config), launch file (launch), meshes folder (meshes), script folder (script), Universal robot definition format folder (urdf), and worlds folder (worlds). In this research, Pseudo-inverse Jacobian was implemented to obtain the optimal angular joint for every step walking during the simulation. Result of the walking robot simulation are shown to have the least error range around 0.0365 m to 0.0867 m differ from the actual target position. © 2020 Institute of Control, Robotics, and Systems - ICROS.},
author_keywords={Gazebo simulation;  Legged robot;  Pseudo-inverse Jacobian;  Robot operating system (ROS)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pan2020,
author={Pan, H. and Li, Z. and Zhang, P. and Salamatian, K. and Xie, G.},
title={Misconfiguration Checking for SDN: Data Structure, Theory and Algorithms},
journal={Proceedings - International Conference on Network Protocols, ICNP},
year={2020},
volume={2020-October},
doi={10.1109/ICNP49622.2020.9259353},
art_number={9259353},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097816653&doi=10.1109%2fICNP49622.2020.9259353&partnerID=40&md5=4d93f7e4d3bd53551facf5572259e2d6},
abstract={Software-Defined Networking (SDN) facilitates net-work innovations with programmability. However, programming the network is error-prone no matter using low-level APIs or high-level programming languages. That said, SDN policies deployed in networks may contain misconfigurations. Prior studies focus on either traditional access control policies or network-wide states, and thus are unable to effectively detect potential misconfigurations in SDN policies with bitmask patterns and complex action behaviorsTo address this gap, this paper first presents a new data structure, minimal interval set, to represent the match patterns of rulesets. This representation serves the basis for composition algebra construction and fast misconfiguration checking. We then propose the principles and algorithms for fast and accurate con-figuration verification. We finally implement a misconfiguration checking tool in Covisor with optimisations to further reduce the overhead. Experiments with synthetic and random rulesets show its fitness for purpose. © 2020 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nielebock202073,
author={Nielebock, S. and Heumuller, R. and Kruger, J. and Ortmeier, F.},
title={Cooperative API Misuse Detection Using Correction Rules},
journal={Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2020},
year={2020},
pages={73-76},
doi={10.1145/3377816.3381735},
art_number={9397526},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104597101&doi=10.1145%2f3377816.3381735&partnerID=40&md5=bbaabcb0db93a7582f573fe88506eb8d},
abstract={Application Programming Interfaces (APIs) grant developers access to the functionalities of code libraries. Due to missing knowledge of how an API is correctly used, developers can unintentionally misuse APIs, and thus introduce bugs. To tackle this issue, recent techniques aim to automatically infer specifications for correct API usage and detect misuses. Unfortunately, these techniques suffer from high false-positive rates, leading to many false alarms. While we believe that existing techniques will improve in the future, in this paper, we propose to investigate a different route: We assume that a developer manually detected and fixed an API misuse relating to a third-party library. Based on the change, we can infer a correction rule for the API misuse. Then, we can use this correction rule to detect the same and similar API misuses in the same or other projects. This represents a cooperative technique to transfer the knowledge of API-misuse fixes to other developers. We report promising insights on an implementation and empirical evidence on the applicability of our technique based on 43 real-world API misuses. CCS CONCEPTS • Software and its engineering ? Error handling and recovery; Software defect analysis; Collaboration in software development. © 2020 ACM.},
author_keywords={API Misuse;  Bug Fix;  Misuse Detection;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sune2020210,
author={Sune, A.E.M.},
title={Formalization and analysis of quantitative attributes of distributed systems},
journal={Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
year={2020},
pages={210-213},
doi={10.1145/3377812.3381387},
art_number={9270314},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098595574&doi=10.1145%2f3377812.3381387&partnerID=40&md5=914448f58e1967a18751d1d493663071},
abstract={While there is not much discussion on the importance of formally describing and analyzing quantitative requirements in the process of software construction; in the paradigm of API-based software systems, it could be vital. Quantitative attributes can be thought of as attributes determining the Quality of Service-QoS provided by a software component published as a service. In this sense, they play a determinant role in classifying software artifacts according to specific needs stated as requirements.In this work, we present a research program consisting of the development of formal languages and tools to characterize and analyze the Quality of Service attributes of software components in the context of distributed systems. More specifically, our main motivational scenario lays on the execution of a service-oriented architecture. © 2020 ACM.},
author_keywords={distributed systems;  formal verification;  non functional requirements;  qos ranking;  quality of service;  quantitative attributes;  service level agreement;  service oriented computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hu202052,
author={Hu, J. and Hu, J. and Wang, W. and Kang, J. and Wang, H. and Gao, Z.},
title={Constructing formal specification models from domain specific natural language requirements},
journal={Proceedings - 2020 6th International Symposium on System and Software Reliability, ISSSR 2020},
year={2020},
pages={52-60},
doi={10.1109/ISSSR51244.2020.00017},
art_number={9265877},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098324503&doi=10.1109%2fISSSR51244.2020.00017&partnerID=40&md5=a89cd94768651813a00341752759776d},
abstract={One important way to improve the quality of safety-critical software is to produce a good software requirement satisfying several key properties, such as: integrity, consistency, and well organized, etc. Our work is based on airborne software domain, and propose a framework to translate the software requirements, which are itemized with domain natural language in avionics, effectively into a formal specification model VRM (Variable Relation Model), which has table-style structures with formal semantics. Firstly, considering avionics domain characteristics, a domain concept library is established including different types of variables and concepts. Then, a set of domain-oriented requirements templates are defined, such as: general event/condition, display event/condition, etc. According to VRM model element semantics, three types model construction algorithms are designed to complete the translation automatically. And in the case study, the Engine Indication and Crew Warning System (EICAS) was selected to show how to construct formal models from natural language requirements. © 2020 IEEE.},
author_keywords={domain template library;  EICAS;  model transition;  safety-critical software;  VRM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Williams2020144,
author={Williams, B. and Leidel, J. and Wang, X. and Donofrio, D. and Chen, Y.},
title={CircusTent: A Benchmark Suite for Atomic Memory Operations},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={144-157},
doi={10.1145/3422575.3422789},
art_number={3422789},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103594155&doi=10.1145%2f3422575.3422789&partnerID=40&md5=c6a7a6a9253f198ef2c75cf10b6fc1b6},
abstract={A paradigm shift is currently taking place in the field of computer architecture. Consistent silicon-level processor improvements, relied upon in the past to drive the augmentation of system scalability, have stalled. As such, it is widely believed that future systems, wherein the design of hardware and software are more closely coupled, will need to leverage an increased degree of heterogeneity in order to realize further improvements. Parallel processing and corresponding programming models, already ubiquitous to high performance computing, will play a crucial role in these systems. Consequently, it is critically important to understand the interaction between these components. However, the behavior of atomic operations and associated synchronization primitives, which already represent a bottleneck in current systems, is difficult to quantify. Therefore, in this work, we introduce CircusTent, an open source, modular, and natively extensible benchmark suite for shared and distributed memory systems that is designed to measure the performance of a target architecture's memory subsystem with respect to atomic operations. Herein, we first detail the design of CircusTent, which includes eight different kernels designed to replicate common atomic memory access patterns using two atomic primitives. We then demonstrate the capabilities of CircusTent through an evaluation of fourteen different platforms using our OpenMP benchmark implementation. In short, we believe CircusTent will prove to be an invaluable tool for the design and prototyping of emerging architectures. © 2020 ACM.},
author_keywords={Atomic Memory Operations;  Benchmark;  MPI;  OpenMP;  OpenSHMEM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bolme2020,
author={Bolme, D.S. and Srinivas, N. and Brogan, J. and Cornett, D.},
title={Face Recognition Oak Ridge (FaRO): A Framework for Distributed and Scalable Biometrics Applications},
journal={IJCB 2020 - IEEE/IAPR International Joint Conference on Biometrics},
year={2020},
doi={10.1109/IJCB48548.2020.9304933},
art_number={9304933},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099695890&doi=10.1109%2fIJCB48548.2020.9304933&partnerID=40&md5=cefc0ec1b8a7d7c59fa65cec8cdb5f5f},
abstract={The facial biometrics community has seen a recent abundance of high-accuracy facial analytic models become freely available. Although these models' capabilities in facial detection, landmark detection, attribute analysis, and recognition are ever-increasing, they aren't always straightforward to deploy in a real-world environment. In reality, the use of the field's ever growing collection of models is becoming exceedingly difficult as library dependencies update and deprecate. Researchers often encounter headaches when attempting to utilize multiple models requiring different or conflicting software packages. Face Recognition Oak Ridge (FaRO) is an open-source project designed to provide a highly modular, flexible framework for unifying facial analytic models through a compartmentalized plug-and-play paradigm built on top of the gRPC (Google Remote Procedure Call) protocol. FaRO's server-client architecture and flexible portability allows easy construction of modularized and heterogeneous face analysis pipelines, distributed over many machines with differing hardware and software resources. This paper outlines FaRO's architecture and current capabilities, along with some experiments in model testing and distributed scaling through FaRO. © 2020 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kanemaru2020275,
author={Kanemaru, S. and Ikegaya, T. and Takahashi, K. and Toyoshima, T.},
title={Design and implementation of comprehensive test automation method for API adapter in C-plane and U-plane},
journal={APNOMS 2020 - 2020 21st Asia-Pacific Network Operations and Management Symposium: Towards Service and Networking Intelligence for Humanity},
year={2020},
pages={275-278},
doi={10.23919/APNOMS50412.2020.9237052},
art_number={9237052},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096987064&doi=10.23919%2fAPNOMS50412.2020.9237052&partnerID=40&md5=da744f2339db64f3a9d84506282dd3d8},
abstract={The Business-to-Business-to-X (B2B2X) model has increased the importance of orchestrators that build and operate services that consists of various wholesale services. To develop RESTful API-based services, service providers need to catch up on the specifications of new wholesale services and specification changes of existing services quickly and inexpensively. Therefore, it is important to develop software called an API adapter that absorbs API differences from various services quickly and inexpensively. In this paper, we propose a method for comprehensively automating testing of not only C-Plane signals such as service provisioning/change/abolishment but also U-Plane signals such as communication between user devices and servers. We implemented our proposal using open source software and found that it improved test efficiency in an actual software development project. © 2020 KICS.},
author_keywords={API Adapter;  Orchestrator;  Test automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suciu2020381,
author={Suciu, A. and Hangan, A. and Marginean, A. and Joldos, M. and Voitcu, G. and Echim, M.},
title={Parallel implementation of a PIC simulation algorithm using OpenMP},
journal={Proceedings of the 2020 Federated Conference on Computer Science and Information Systems, FedCSIS 2020},
year={2020},
pages={381-385},
doi={10.15439/2020F130},
art_number={9222948},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095777715&doi=10.15439%2f2020F130&partnerID=40&md5=7d05d23fed9fb948e4f9eb845d89536b},
abstract={Particle-in-cell (PIC) simulations are focusing on the individual trajectories of a very large number of particles in self-consistent and external electric and magnetic fields; they are widely used in the study of plasma jets, for example. The main disadvantage of PIC simulations is the large simulation runtime, which often requires a parallel implementation of the algorithm. The current paper focuses on a PIC1d3v simulation algorithm [1] [2] and describes the successful implementation of a parallel version of it on a multicore architecture, using OpenMP, with very promising experimental and theoretical results. © 2020 Polish Information Processing Society - as it is since 2011.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ramasubramani2020,
author={Ramasubramani, V. and Dice, B.D. and Harper, E.S. and Spellings, M.P. and Anderson, J.A. and Glotzer, S.C.},
title={freud: A software suite for high throughput analysis of particle simulation data},
journal={Computer Physics Communications},
year={2020},
volume={254},
doi={10.1016/j.cpc.2020.107275},
art_number={107275},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082555873&doi=10.1016%2fj.cpc.2020.107275&partnerID=40&md5=963975b8264195293e011affc1704e20},
abstract={The freud Python package is a library for analyzing simulation data. Written with modern simulation and data analysis workflows in mind, freud provides a Python interface to fast, parallelized C++ routines that run efficiently on laptops, workstations, and supercomputing clusters. The package provides the core tools for finding particle neighbors in periodic systems, and offers a uniform API to a wide variety of methods implemented using these tools. As such, freud users can access standard methods such as the radial distribution function as well as newer, more specialized methods such as the potential of mean force and torque and local crystal environment analysis with equal ease. Rather than providing its own trajectory data structure, freud operates either directly on NumPy arrays or on trajectory data structures provided by other Python packages. This design allows freud to transparently interface with many trajectory file formats by leveraging the file parsing abilities of other trajectory management tools. By remaining agnostic to its data source, freud is suitable for analyzing any particle simulation, regardless of the original data representation or simulation method. When used for on-the-fly analysis in conjunction with scriptable simulation software such as HOOMD-blue, freud enables smart simulations that adapt to the current state of the system, allowing users to study phenomena such as nucleation and growth. Program summary: Program Title: freud Program Files doi: http://dx.doi.org/10.17632/v7wmv9xcct.1 Licensing provisions: BSD 3-Clause Programming language: Python, C++ Nature of problem: Simulations of coarse-grained, nano-scale, and colloidal particle systems typically require analyses specialized to a particular system. Certain more standardized techniques – including correlation functions, order parameters, and clustering – are computationally intensive tasks that must be carefully implemented to scale to the larger systems common in modern simulations. Solution method: freud performs a wide variety of particle system analyses, offering a Python API that interfaces with many other tools in computational molecular sciences via NumPy array inputs and outputs. The algorithms in freud leverage parallelized C++ to scale to large systems and enable real-time analysis. The library's broad set of features encode few assumptions compared to other analysis packages, enabling analysis of a broader class of data ranging from biomolecular simulations to colloidal experiments. Additional comments including restrictions and unusual features: 1. freud provides very fast parallel implementations of standard analysis methods like RDFs and correlation functions. 2. freud includes the reference implementation for the potential of mean force and torque (PMFT). 3. freud provides various novel methods for characterizing particle environments, including the calculation of descriptors useful for machine learning. The source code is hosted on GitHub (https://github.com/glotzerlab/freud), and documentation is available online (https://freud.readthedocs.io/). The package may be installed via pip install freud-analysis or conda install -c conda-forge freud. © 2020 Elsevier B.V.},
author_keywords={Computational materials science;  Molecular dynamics;  Monte Carlo;  Simulation analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2020208,
author={Li, A. and Chang, T.-J. and Wentzlaff, D.},
title={Automated Design of FPGAs Facilitated by Cycle-Free Routing},
journal={Proceedings - 30th International Conference on Field-Programmable Logic and Applications, FPL 2020},
year={2020},
pages={208-213},
doi={10.1109/FPL50879.2020.00042},
art_number={9221519},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095607530&doi=10.1109%2fFPL50879.2020.00042&partnerID=40&md5=d586c7feef7356cee517ada0274c4ceb},
abstract={As device technology advances into the sub-10nm era, the design costs of Field Programmable Gate Arrays (FPGAs) built out of fully-custom, hand-layout blocks have increased dramatically. At the same time, Embedded FPGAs (eFPGAs) are picking up steam in heterogeneous system-on-chip designs, in which case supporting customizable FPGA architectures and reducing design costs are more crucial than squeezing the last bit of performance out of the transistors. To reduce the cost and complexity of FPGA designs, prior works have proposed to build FPGAs using Electronic Design Automation (EDA) tools and standard-cell libraries. Though functionally viable, this approach faces two challenges: 1) An accurate timing model is crucial for FPGA implementation tools to produce correct and optimal results. However, post-layout Static Timing Analysis (STA) with EDA tools is error-prone on FPGAs, because the typical FPGA routing graphs contain many cycles at design time. 2) Conventional FPGA design relies heavily on iterative/empirical improvements to achieve optimal floorplanning and time-budgeting. Without such insights, blocks may be shaped and constrained sub-optimally. This work addresses the first challenge by proposing an algorithm to derive cycle-free sub-graphs. A cycle-free subgraph is achieved by logically ranking the routing tracks and selectively removing some switch block connections. Each subgraph enables accurate, per-switch, post-layout STA, and the union of multiple sub-graphs covers all the timing arcs of the FPGA. Furthermore, our proposed approach addresses the second challenge by enabling the creation of intrinsically cycle-free FPGAs that facilitate a flat multi-block or full-chip design flow. By blending the blocks, the EDA tools can exploit more optimization opportunities and automatically adapt to heterogeneous blocks. Our experiments show that the routability of cycle-free routing graphs is comparable to conventional FPGA routing graphs, and the Quality of Results (QoR) of the FPGA layout is superior to the result of previous approaches. © 2020 IEEE.},
author_keywords={EDA Flow;  FPGA;  FPGA Routing Architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mitropoulos2020479,
author={Mitropoulos, D. and Sotiropoulos, T. and Koutsovasilis, N. and Spinellis, D.},
title={PDGuard: an architecture for the control and secure processing of personal data},
journal={International Journal of Information Security},
year={2020},
volume={19},
number={4},
pages={479-498},
doi={10.1007/s10207-019-00468-5},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073973286&doi=10.1007%2fs10207-019-00468-5&partnerID=40&md5=1ff0cb9721ce6faa6e6e5dcedd2b5025},
abstract={Online personal data are rarely, if ever, effectively controlled by the users they concern. Worse, as demonstrated by the numerous leaks reported each week, the organizations that store and process them fail to adequately safeguard the required confidentiality. In this paper, we propose pdguard, a framework that defines prototypes and demonstrates an architecture and an implementation that address both problems. In the context of pdguard, personal data are always stored encrypted as opaque objects. Processing them can only be performed through the pdguard application programming interface (api), under data and action-specific authorizations supplied online by third party agents. Through these agents, end-users can easily and reliably authorize and audit how organizations use their personal data. A static verifier can be employed to identify accidental api misuses. Following a security by design approach, pdguard changes the problem of personal data management from the, apparently, intractable problem of supervising processes, operations, personnel, and a large software stack to that of auditing the applications that use the framework for compliance. We demonstrate the framework’s applicability through a reference implementation, by building a pdguard-based e-shop, and by integrating pdguard into the The Guardian newspaper’s website identity application. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Auditing;  Encrypted data;  Personal data;  Software architecture},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Soni2020329,
author={Soni, H. and Rifai, M. and Kumar, P. and Doenges, R. and Foster, N.},
title={Composing Dataplane Programs with μp4},
journal={SIGCOMM 2020 - Proceedings of the 2020 Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
year={2020},
pages={329-343},
doi={10.1145/3387514.3405872},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094867791&doi=10.1145%2f3387514.3405872&partnerID=40&md5=a730d40110bd582d047e40a8d651fd6b},
abstract={Dataplane languages like P4 enable flexible and efficient packet-processing using domain-specific primitives such as programmable parsers and match-action tables. Unfortunately, P4 programs tend to be monolithic and tightly coupled to the hardware architecture, which makes it hard to write programs in a portable and modular way-e.g., by composing reusable libraries of standard protocols. To address this challenge, we present the design and implementation of a novel framework (μP4) comprising a lightweight logical architecture that abstracts away from the structure of the underlying hardware pipelines and naturally supports powerful forms of program composition. Using examples, we show how enables modular programming. We present a prototype of the compiler that generates code for multiple lower-level architectures, including Barefoot's Tofino Native Architecture. We evaluate the overheads induced by our compiler on realistic examples. © 2020 ACM.},
author_keywords={Composition;  Modularity;  P4;  Programmable dataplanes},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cleveland2020181,
author={Cleveland, S.B. and Jamthe, A. and Padhy, S. and Stubbs, J. and Packard, M. and Looney, J. and Terry, S. and Cardone, R. and Dahan, M. and Jacobs, G.A.},
title={Tapis API Development with Python: Best Practices in Scientific REST API Implementation: Experience implementing a distributed Stream API},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={181-187},
doi={10.1145/3311790.3396647},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089265784&doi=10.1145%2f3311790.3396647&partnerID=40&md5=7053e703b75e6c7dd0ca6acc9d5242da},
abstract={In the last decade, the rise of hosted Software-as-a-Service (SaaS) application programming interfaces (APIs) across both academia and industry has exploded, and simultaneously, microservice architectures have replaced monolithic application platforms for the flexibility and maintainability they offer. These SaaS APIs rely on small, independent and reusable microservices that can be assembled relatively easily into more complex applications. As a result, developers can focus on their own unique functionality and surround it with fully functional, distributed processes developed by other specialists, which they access through APIs. The Tapis framework, a NSF funded project, provides SaaS APIs to allow researchers to achieve faster scientific results, by eliminating the need to set up a complex infrastructure stack. In this paper, we describe the best practices followed to create Tapis APIs using Python and the Stream API as an example implementation illustrating authorization and authentication with the Tapis Security Kernel, Tenants and Tokens APIs, leveraging OpenAPI v3 specification for the API definitions and docker containerization. Finally, we discuss our deployment strategy with Kubernetes, which is an emerging orchestration technology and the early adopter use cases of the Streams API service. © 2020 Owner/Author.},
author_keywords={Abaco;  API;  CHORDS;  real-time streaming data;  Tapis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2020985,
author={Lee, J. and Kim, T. and Kang, S.},
title={Recovering Software Product Line Architecture of Product Variants Developed with the Clone-and-Own Approach},
journal={Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020},
year={2020},
pages={985-990},
doi={10.1109/COMPSAC48688.2020.0-143},
art_number={9202695},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094125137&doi=10.1109%2fCOMPSAC48688.2020.0-143&partnerID=40&md5=d9811cd1d8cea795f47b05734fe10aa4},
abstract={Software products developed with the clone-and-own approach pose difficulties in maintenance. Migrating to software product line can solve this problem. This paper proposes an approach to recover software product line architecture (PLA) from a family of products developed with the clone-and-own approach. The proposed approach decomposes all source code of the family of products and analyses cloned copy classes, cloned modification classes and product-specific classes. From the results, it recovers a PLA. For determining common and variable classes, the proposed approach uses Harmonized Total Constant Commonality Indices of packages or classes of a product line (HTCCIPL). We apply our approach to recover the PLA of the Apo-Games developed with the clone-and-own approach. The results show that our approach recovers the Apo-Games PLA with a set of guidelines that can assist product line engineers in making decisions on commonality and variability of architectural elements of a PLA. © 2020 IEEE.},
author_keywords={clone-and-own approach;  commonality index;  product line architecture recovery;  software product line},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ayub2020,
author={Ayub, M. and Knowles, P. and Phan, B. and Coffman, J.},
title={The Cost of a Macaroon},
journal={Systems Security Symposium, SSS 2020 - Conference Proceedings},
year={2020},
doi={10.1109/SSS47320.2020.9174369},
art_number={9174369},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092735043&doi=10.1109%2fSSS47320.2020.9174369&partnerID=40&md5=d684d7ce422781382957927c3889377e},
abstract={Macaroons are authorization credentials which have high efficiency and expressiveness that rivals public-key-based mechanisms like Simple Public Key Infrastructure (SPKI) / Simple Distributed Security Infrastructure (SDSI). Macaroons are simple and easy to deploy similar to web cookies. Additionally, they offer secure delegation via their chained hashed message authentication code (HMAC) construction which provides a construct for decentralized delegation. These characteristics are motivating factors for the adoption of macaroons; however, the modern web environment has changed since the initial concept was demonstrated on a local server and since that time there has been a broad adoption of cloud services and an expansion of mobile and embedded devices with the trend of Internet of Things (IoT).The approach utilized by macaroons has a much lower processing overhead and lower latency compared to other authorization methods used in the cloud. Our work investigates factors relating to the overhead costs of macaroon primitives. We implement a macaroons library in Python and replicate the measurements taken by Birgisson et al. in 2014. Then we evaluate the overhead of macaroon primitives and validate the prior findings regarding the cost of authorization primitives on a comparable computing platform. We also run tests with different payload sizes to assess if the prior results continue to hold true using a variety of common payload sizes. Our results provided insight into the trade offs of how efficient and expressive macaroons can be. We additionally evaluate the differences for the cost of authorization primitives across a variety of platforms to assess macaroons relative to the use of modern computer processors and modern software libraries, including ARM architecture processors to evaluate their use with IoT devices. Our results indicate that hardware and software advances have decreased the cost of using macaroons and they are efficient enough to use in low-power embedded platforms. © 2020 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Spengler2020415,
author={Spengler, H. and Gatz, I. and Kohlmayer, F. and Kuhn, K.A. and Prasser, F.},
title={Improving data quality in medical research: A monitoring architecture for clinical and translational data warehouses},
journal={Proceedings - IEEE Symposium on Computer-Based Medical Systems},
year={2020},
volume={2020-July},
pages={415-420},
doi={10.1109/CBMS49503.2020.00085},
art_number={9183153},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091188815&doi=10.1109%2fCBMS49503.2020.00085&partnerID=40&md5=785e94755c01f21f43226bc7f6e2ac76},
abstract={Clinical and translational data warehouses are important infrastructure building blocks for modern data-driven approaches in medical research. These analytics-oriented databases have been designed to integrate heterogeneous biomedical datasets from different sources and to support use cases such as cohort selection and ad-hoc data analyses. However, the lack of clear definitions of source data and controlled data collection procedures often raises concerns about the quality of data provided in such environments and, consequently, about the evidence level of related findings. To address these problems, we present an architecture that helps to monitor data quality issues when importing data into warehousing solutions using ETL (Extraction, Transformation, Load) processes. Our approach provides software developers with an API (Application Programming Interface) for logging detailed and structured information about data quality issues encountered. This information can then be displayed in dynamic dashboards, the evolution of data quality can be monitored over time, and quality issues can be traced back to their source. Our architecture supports several well-known data quality dimensions, addressing conformance, completeness, and plausibility. We present an open-source implementation, which is compatible with common clinical and translational data warehousing platforms, such as i2b2 and tranSMART, and which can be used in conjunction with many ETL environments. © 2020 IEEE.},
author_keywords={Architecture;  Data quality;  Data warehouses;  I2b2b;  Monitoring;  TranSMART},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barbosa2020374,
author={Barbosa, P. and Figueiredo, A. and Souto, S. and Gaeta, E. and Araujo, E. and Teixeira, T.},
title={An open source software architecture and ready-to-use components for health IoT},
journal={Proceedings - IEEE Symposium on Computer-Based Medical Systems},
year={2020},
volume={2020-July},
pages={374-379},
doi={10.1109/CBMS49503.2020.00077},
art_number={9183119},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091156332&doi=10.1109%2fCBMS49503.2020.00077&partnerID=40&md5=c95995da61e22bf644a2fb837bb50613},
abstract={This paper presents the definition of a software architecture and implementation as open-source components specially designed for the state-of-art of Health IoT technologies. These activities were conducted through a H2020 project that developed a smart IoT solution for childhood obesity and two contracted Health IoT projects for the leading brazilian clinics in wound treatment. The software components and the guidelines presented in this paper are fully available as github repositories and contributors can find immediate application in Health IoT projects. A survey with contributors in four different countries was conducted about the way the realized software architecture addresses quality requirements for Health IoT. The results shows that the integration of components was the main concern of this team due to the distributed nature of the projects and the implementation of patterns such as API Gateway using open source technologies are very likely to be reused in the future. The initiative continues collecting user's experiences for continuous improvements and research goals. © 2020 IEEE.},
author_keywords={Health IoT;  Open source;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Naylor202017,
author={Naylor, M. and Moore, S.W. and Mokhov, A. and Thomas, D. and Beaumont, J.R. and Fleming, S. and Markettos, A.T. and Bytheway, T. and Brown, A.},
title={Termination detection for fine-grained message-passing architectures},
journal={Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors},
year={2020},
volume={2020-July},
pages={17-24},
doi={10.1109/ASAP49362.2020.00012},
art_number={9153260},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090846140&doi=10.1109%2fASAP49362.2020.00012&partnerID=40&md5=e4fd8bd3e31e8a2d157a089712180b66},
abstract={Barrier primitives provided by standard parallel programming APIs are the primary means by which applications implement global synchronisation. Typically these primitives are fully-committed to synchronisation in the sense that, once a barrier is entered, synchronisation is the only way out. For message-passing applications, this raises the question of what happens when a message arrives at a thread that already resides in a barrier. Without a satisfactory answer, barriers do not interact with message-passing in any useful way.In this paper, we propose a new refutable barrier primitive that combines with message-passing to form a simple, expressive, efficient, well-defined API. It has a clear semantics based on termination detection, and supports the development of both globally-synchronous and asynchronous parallel applications.To evaluate the new primitive, we implement it in a prototype large-scale message-passing machine with 49, 152 RISC-V threads distributed over 48 FPGAs. We show that hardware support for the primitive leads to a highly-efficient implementation, capable of synchronisation rates that are an order-of-magnitude higher than what is achievable in software. Using the primitive, we implement synchronous and asynchronous versions of a range of applications, observing that each version can have significant advantages over the other, depending on the application. Therefore, a barrier primitive supporting both styles can greatly assist the development of parallel programs. © 2020 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DeGaetani2020,
author={De Gaetani, C.I. and Mert, M. and Migliaccio, F.},
title={Interoperability analyses of BIM platforms for construction management},
journal={Applied Sciences (Switzerland)},
year={2020},
volume={10},
number={13},
doi={10.3390/app10134437},
art_number={4437},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087909007&doi=10.3390%2fapp10134437&partnerID=40&md5=597a0a33deb94b4658b27d9c3dc550e5},
abstract={It is incontrovertible that an exchange of files is essentially required at several stages of the workflow in the architecture, engineering, and construction (AEC) industry. Therefore, investigating and detecting the capabilities/inabilities of building information modeling (BIM) software packages with respect to interoperability can be informative to stakeholders who exchange data between various BIM packages. The work presented in this paper includes a discussion on the interoperability of different software platforms commonly used in the AEC industry. Although, in theory, flawless interoperability of some types of files between different BIM platforms is ensured, in practical applications, this is not always the case. Hence, this research aims to identify faults in data exchange by assessing different possible scenarios where a sample Industry Foundation Classes (IFC) four-dimensions (4D) BIM model and related Gantt charts are exchanged. Throughout the interoperability analysis of both IFC file and Gantt charts, the following checks were carried out: geometrical and nongeometrical information exchange through IFC files, 4D information correct readability, and presence of missing schedule information in Gantt charts after their import/export procedure. The results show that interoperability between the analyzed platforms is not always ensured, providing useful insight into realistic scenarios. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={BIM;  Gantt chart;  IFC;  Interoperability;  Project management},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abbasi2020643,
author={Abbasi, M. and Shokrollahi, A. and Khosravi, M.R. and Menon, V.G.},
title={High-performance flow classification using hybrid clusters in software defined mobile edge computing},
journal={Computer Communications},
year={2020},
volume={160},
pages={643-660},
doi={10.1016/j.comcom.2020.07.002},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087821434&doi=10.1016%2fj.comcom.2020.07.002&partnerID=40&md5=002e406a152309a3eb6617a0ae86589a},
abstract={Mobile Edge Computing (MEC) provides different storage and computing capabilities within the access range of mobile devices. This moderates the burden of offloading compute/storage-intensive processes of the mobile devices to the centralized cloud data centers. As a result, the network latency is reduced and the quality of service provided for the mobile end users is improved. Different applications benefit from the large-scale deployments of MEC servers. However, the considerable complexity of managing large scale deployments of the sheer number of applications for the millions of mobile devices is a challenge. Recently, Software Defined Networking (SDN) is leveraged to resolve the problem by providing unified and programmable interfaces for managing network devices. Most of the current SDN packet processing services are tightly dependent on the packet classification service. This primary service classifies any incoming packet based on matching a set of specific fields of its header against a flow table. Acceleration of this basic process considerably increases the performance of the SDN-based MEC. In this paper, the hierarchical tree algorithm, which is a packet classification method, is parallelized using popular platforms on a cluster of Graphics Processing Units (GPUs), a cluster of Central Processing Units (CPUs), and a hybrid cluster. The best scenario for the parallel implementation of this algorithm on the CPU cluster is that which combines OpenMP and MPI. In this case, the throughput of the classifier is 4.2 million packets per second (MPPS). On the GPU cluster, two different scenarios have been used. In the first scenario, the global memory is used to store the rules and the Hierarchical-trie of the classifier while in the second scenario we break the filter set in a way that the resulting Hierarchical-trie of each subset could be stored in the shared memory of GPU. According to the results, although the first GPU cluster scenario achieves a throughput of 29.19 MPPS and a speedup 58 times as great as the serial mode, the second scenario is 12 times faster due to using the shared memory. The best performance, however, belongs to the hybrid cluster mode. The hybrid cluster achieves a throughput of 30.59 which is 1.4 MPPS more than the GPU cluster. © 2020 Elsevier B.V.},
author_keywords={Flow classification;  GPU cluster;  Graphics Processing Unit (GPU);  Mobile edge computing;  Software Defined Mobile Edge Computing (SDMEC);  Software Defined Networking},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tian2020,
author={Tian, J. and Xing, W. and Li, Z.},
title={BVDetector: A program slice-based binary code vulnerability intelligent detection system},
journal={Information and Software Technology},
year={2020},
volume={123},
doi={10.1016/j.infsof.2020.106289},
art_number={106289},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082000530&doi=10.1016%2fj.infsof.2020.106289&partnerID=40&md5=e58b5bc759432ab32bf4264115084962},
abstract={Context: Software vulnerability detection is essential to ensure cybersecurity. Currently, most software is published in binary form, thus researchers can only detect vulnerabilities in these software by analysing binary programs. Although existing research approaches have made a substantial contribution to binary vulnerability detection, there are still many deficiencies, such as high false positive rate, detection with coarse granularity, and dependence on expert experience. Objective: The goal of this study is to perform fine-grained intelligent detection on the vulnerabilities in binary programs. This leads us to propose a fine-grained representation of binary programs and introduce deep learning techniques to intelligently detect the vulnerabilities. Method: We use program slices of library/API function calls to represent binary programs. Additionally, we design and construct a Binary Gated Recurrent Unit (BGRU) network model to intelligently learn vulnerability patterns and automatically detect vulnerabilities in binary programs. Results: This approach yields the design and implementation of a program slice-based binary code vulnerability intelligent detection system called BVDetector. We show that BVDetector can effectively detect vulnerabilities related to library/API function calls in binary programs, which reduces the false positive rate and false negative rate of vulnerability detection. Conclusion: This paper proposes a program slice-based binary code vulnerability intelligent detection system called BVDetector. The experimental results show that BVDetector can effectively reduce the false negative rate and false positive rate of binary vulnerability detection. © 2020},
author_keywords={Binary program;  Deep learning;  Library/API function call;  Program slice;  Vulnerability detection},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen202032,
author={Chen, Y. and Santosa, A.E. and Yi, A.M. and Sharma, A. and Sharma, A. and Lo, D.},
title={A Machine Learning Approach for Vulnerability Curation},
journal={Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020},
year={2020},
pages={32-42},
doi={10.1145/3379597.3387461},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093688307&doi=10.1145%2f3379597.3387461&partnerID=40&md5=69ea6acf785954d248b95ec09c1add64},
abstract={Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources. © 2020 ACM.},
author_keywords={application security;  classifiers ensemble;  machine learning;  open-source software;  self-training},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen202090,
author={Chen, Y. and Santosa, A.E. and Sharma, A. and Lo, D.},
title={Automated identification of libraries from vulnerability data},
journal={Proceedings - International Conference on Software Engineering},
year={2020},
pages={90-99},
doi={10.1145/3377813.3381360},
art_number={3381360},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092544486&doi=10.1145%2f3377813.3381360&partnerID=40&md5=5b30ca9f94c9eecb3f1b0845132b3929},
abstract={Software Composition Analysis (SCA) has gained traction in recent years with a number of commercial offerings from various companies. SCA involves vulnerability curation process where a group of security researchers, using various data sources, populate a database of open-source library vulnerabilities, which is used by a scanner to inform the end users of vulnerable libraries used by their applications. One of the data sources used is the National Vulnerability Database (NVD). The key challenge faced by the security researchers here is in figuring out which libraries are related to each of the reported vulnerability in NVD. In this article, we report our design and implementation of a machine learning system to help identify the libraries related to each vulnerability in NVD. The problem is that of extreme multi-label learning (XML), andwe developed our system using the state-of-the-art FastXML algorithm. Our system is iteratively executed, improving the performance of the model over time. At the time of writing, it achieves F1@1 score of 0.53 with average F1@k score for k = 1, 2, 3 of 0.51 (F1@k is the harmonic mean of precision@k and recall@k). It has been deployed in Veracode as part of a machine learning system that helps the security researchers identify the likelihood of web data items to be vulnerability-related. In addition, we present evaluation results of our feature engineering and the FastXML tree number used. Our work formulates and solves for the first time library name identification from NVD data as XML, and deploys the solution in a complete production system. © 2020 IEEE Computer Society. All rights reserved.},
author_keywords={Application security;  Machine learning;  Open source software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{vanBinsbergen2020,
author={van Binsbergen, L.T. and Scott, E. and Johnstone, A.},
title={Purely functional GLL parsing},
journal={Journal of Computer Languages},
year={2020},
volume={58},
doi={10.1016/j.cola.2020.100945},
art_number={100945},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084918755&doi=10.1016%2fj.cola.2020.100945&partnerID=40&md5=c38079609dd8c16a0328a2a7145df4b0},
abstract={Generalised parsing has become increasingly important in the context of software language design and several compiler generators and language workbenches have adopted generalised parsing algorithms such as GLR and GLL. The original GLL parsing algorithms are described in low-level pseudo-code as the output of a parser generator. This paper explains GLL parsing differently, defining the FUN-GLL algorithm as a collection of pure, mathematical functions and focussing on the logic of the algorithm by omitting implementation details. In particular, the data structures are modelled by abstract sets and relations rather than specialised implementations. The description is further simplified by omitting lookahead and adopting the binary subtree representation of derivations to avoid the clerical overhead of graph construction. Conventional parser combinators inherit the drawbacks from the recursive descent algorithms they implement. Based on FUN-GLL, this paper defines generalised parser combinators that overcome these problems. The algorithm is described in the same notation and style as FUN-GLL and uses the same data structures. Both algorithms are explained as a generalisation of basic recursive descent algorithms. The generalised parser combinators of this paper have several advantages over combinator libraries that generate internal grammars. For example, with the generalised parser combinators it is possible to parse larger permutation phrases and to write parsers for languages that are not context-free. The ‘BNF combinator library’ is built around the generalised parser combinators. With the library, embedded and executable syntax specifications are written. The specifications contain semantic actions for interpreting programs and constructing syntax trees. The library takes advantage of Haskell's type-system to type-check semantic actions and Haskell's abstraction mechanism enables ‘reuse through abstraction’. The practicality of the library is demonstrated by running parsers obtained from the syntax descriptions of several software languages. © 2020 Elsevier Ltd},
author_keywords={Functional programming;  Generalised parsing;  Parser combinators;  Syntax descriptions;  Top-down parsing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chiacchio2020,
author={Chiacchio, F. and Aizpurua, J.I. and Compagno, L. and D'Urso, D.},
title={SHyFTOO, an object-oriented Monte Carlo simulation library for the modeling of Stochastic Hybrid Fault Tree Automaton},
journal={Expert Systems with Applications},
year={2020},
volume={146},
doi={10.1016/j.eswa.2019.113139},
art_number={113139},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077361292&doi=10.1016%2fj.eswa.2019.113139&partnerID=40&md5=1509f175b22de9aecd6e6ead877b84b3},
abstract={Dependability assessment is a crucial activity to ensure the correct operation of complex systems. The output of dependability assessment activities include the quantification of reliability, availability, maintenance and safety related metrics. These metrics can assist in the identification of the system weak points or in the conception of mitigation strategies to increase the system dependability level. The development of advanced computer-aided methodologies to support dependability assessment activities is essential to automate and reduce the efforts implied by this process and similarly, the development of accurate dependability assessment methods is very important to increase the quality of the results. In this context, it is possible to identify different contributions that improve the dependability assessment through general-purpose modeling methodologies. However, existing solutions are ad-hoc applications specified with low-level stochastic formalisms and this complicates their adoption in the industry. Accordingly, this paper presents Stochastic Hybrid Fault Tree Automaton (SHyFTA) based simulation algorithm that allows the accurate dependability analysis of repairable multi-state systems. SHyFTA integrates the stochastic and deterministic operation of the system under study as well as their interactions. The algorithm is formalized through an object-oriented software architecture, which is developed as a software library for the modeling and simulation of repairable SHyFTA models. Following the proposed architecture, a Matlab® implementation of this library, SHyFTOO, has been developed and validated with a thorough test campaign. In order to provide a guideline to the end-users and show the potential of the SHyFTOO library, the case study of a feed-water pumping system is implemented in detail and it is used to evaluate different preventive maintenance policies. The SHyFTOO library can open the way to further investigations that address the interactions between the failure behavior and the functional operation of a system and their combined effect on system dependability. © 2019},
author_keywords={Discrete event simulation;  Dynamic fault tree;  Dynamic reliability;  Matlab®;  Multi-state systems;  Repairable systems},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Reddi2020446,
author={Reddi, V.J. and Cheng, C. and Kanter, D. and Mattson, P. and Schmuelling, G. and Wu, C.-J. and Anderson, B. and Breughe, M. and Charlebois, M. and Chou, W. and Chukka, R. and Coleman, C. and Davis, S. and Deng, P. and Diamos, G. and Duke, J. and Fick, D. and Gardner, J.S. and Hubara, I. and Idgunji, S. and Jablin, T.B. and Jiao, J. and St. John, T. and Kanwar, P. and Lee, D. and Liao, J. and Lokhmotov, A. and Massa, F. and Meng, P. and Micikevicius, P. and Osborne, C. and Pekhimenko, G. and Rajan, A.T.R. and Sequeira, D. and Sirasao, A. and Sun, F. and Tang, H. and Thomson, M. and Wei, F. and Wu, E. and Xu, L. and Yamada, K. and Yu, B. and Yuan, G. and Zhong, A. and Zhang, P. and Zhou, Y.},
title={MLPerf Inference Benchmark},
journal={Proceedings - International Symposium on Computer Architecture},
year={2020},
volume={2020-May},
pages={446-459},
doi={10.1109/ISCA45697.2020.00045},
art_number={9138989},
note={cited By 121},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092007781&doi=10.1109%2fISCA45697.2020.00045&partnerID=40&md5=c7c908eee4883359ce62752fe3e4a9ea},
abstract={Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability. © 2020 IEEE.},
author_keywords={Benchmarking;  Inference;  Machine Learning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ham2020254,
author={Ham, T.J. and Bruns-Smith, D. and Sweeney, B. and Lee, Y. and Seo, S.H. and Song, U.G. and Oh, Y.H. and Asanovic, K. and Lee, J.W. and Wills, L.W.},
title={Genesis: A Hardware Acceleration Framework for Genomic Data Analysis},
journal={Proceedings - International Symposium on Computer Architecture},
year={2020},
volume={2020-May},
pages={254-267},
doi={10.1109/ISCA45697.2020.00031},
art_number={9138974},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091992604&doi=10.1109%2fISCA45697.2020.00031&partnerID=40&md5=d93709daae6a52db70717d8e59df22ed},
abstract={In this paper, we describe our vision to accelerate algorithms in the domain of genomic data analysis by proposing a framework called Genesis (genome analysis) that contains an interface and an implementation of a system that processes genomic data efficiently. This framework can be deployed in the cloud and exploit the FPGAs-as-a-service paradigm to provide cost-efficient secondary DNA analysis. We propose conceptualizing genomic reads and associated read attributes as a very large relational database and using extended SQL as a domain-specific language to construct queries that form various data manipulation operations. To accelerate such queries, we design a Genesis hardware library which consists of primitive hardware modules that can be composed to construct a dataflow architecture specialized for those queries. As a proof of concept for the Genesis framework, we present the architecture and the hardware implementation of several genomic analysis stages in the secondary analysis pipeline corresponding to the best known software analysis toolkit, GATK4 workflow proposed by the Broad Institute. We walk through the construction of genomic data analysis operations using a sequence of SQL-style queries and show how Genesis hardware library modules can be utilized to construct the hardware pipelines designed to accelerate such queries. We exploit parallelism and data reuse by utilizing a dataflow architecture along with the use of on-chip scratchpads as well as non-blocking APIs to manage the accelerators, allowing concurrent execution of the accelerator and the host. Our accelerated system deployed on the cloud FPGA performs up to $ 19.3× better than GATK4 running on a commodity multi-core Xeon server and obtains up to $ 15× better cost savings. We believe that if a software algorithm can be mapped onto a hardware library to utilize the underlying accelerator(s) using an already-standardized software interface such as SQL, while allowing the efficient mapping of such interface to primitive hardware modules as we have demonstrated here, it will expedite the acceleration of domainspecific algorithms and allow the easy adaptation of algorithm changes. © 2020 IEEE.},
author_keywords={FPGA;  genome sequencing;  genomic data analysis;  hardware accelerator;  SQL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Covaciu2020,
author={Covaciu, F. and Pisla, A. and Vaida, C. and Gherman, B. and Pisla, D.},
title={Development of a Virtual Reality Simulator for a Lower Limb Rehabilitation Robot},
journal={2020 22nd IEEE International Conference on Automation, Quality and Testing, Robotics - THETA, AQTR 2020 - Proceedings},
year={2020},
doi={10.1109/AQTR49680.2020.9129981},
art_number={9129981},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089605680&doi=10.1109%2fAQTR49680.2020.9129981&partnerID=40&md5=e3d045f717b3928a5a132c219d137eb8},
abstract={The paper describes the design of a virtual reality simulator destined to a robotic system designed for patient's lower limb medical recovery. The VR simulator is created in Unity 5 environment. In this environment was imported the mechanical design of the robotic structure, together with a virtual human model (an avatar) that latter was coupled with the robotic structure. The robotic mechanical structure is designed in the Siemens NX environment, a professional mechatronics design software and the human virtual model was created using the MakeHuman program, an open source 3D computer graphics middleware. In the first step for creating the virtual reality simulator, an enclosure was defined where the robotic structure used to recover the lower limb has been introduced. In the second stage, the human virtual model was loaded into Unity 5 environment and 'attached' to the robotic mechanical system. In the last stage was defined a Robotic system User Interface (RUI) for the robotic system control. The RUI was created using C Sharp (C#) programming language that exists within the Visual Studio software package. © 2020 IEEE.},
author_keywords={lower limb recovery simulation;  rehabilitation robot design;  robotic system user interface;  virtual reality},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jiao2020,
author={Jiao, X. and Liu, W. and Mehari, M. and Aslam, M. and Moerman, I.},
title={Openwifi: A free and open-source IEEE802.11 SDR implementation on SoC},
journal={IEEE Vehicular Technology Conference},
year={2020},
volume={2020-May},
doi={10.1109/VTC2020-Spring48590.2020.9128614},
art_number={9128614},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088296447&doi=10.1109%2fVTC2020-Spring48590.2020.9128614&partnerID=40&md5=4161da7d22208f76134ba7354caca164},
abstract={Open source Software Defined Radio (SDR) project, such as srsLTE and Open Air Interface (OAI), has been widely used for 4G/5G research. However the SDR implementation of the IEEE802.11 (Wi-Fi) is still difficult. The Wi-Fi Short InterFrame Space (SIFS) requires acknowledgement (ACK) packet being sent out in 10 \mu \mathrm {s}/ 16 \mu \mathrm {s}(2.4 GHz/5GHz) after receiving a packet successfully, thus the Personal Computer (PC) based SDR architecture hardly can be used due to the latency (\ge 100 \mu \mathrm {s}) between PC and Radio Frequency (RF) front-end. Researchers have to do simulation, hack a commercial chip or buy an expensive reference design to test their ideas. To change this situation, we have developed an open-source full-stack IEEE802.11a/g/n SDR implementation - openwifi. It is based on Xilinx Zynq Systemon-Chip (SoC) that includes Field Programmable Gate Array (FPGA) and ARM processor. With the low latency connection between FPGA and RF front-end, the most critical SIFS timing is achieved by implementing Physical layer (PHY) and low level Media Access Control (low MAC) in FPGA. The corresponding driver is implemented in the embedded Linux running on the ARM processor. The driver instantiates Application Programming Interfaces (APIs) defined by Linux mac80211 subsystem, which is widely used for most SoftMAC Wi-Fi chips. Researchers could study and modify openwifi easily thanks to the modular design. Compared to PC based SDR, the SoC is also a better choice for portable and embedded scenarios. © 2020 IEEE.},
author_keywords={AD9361;  FPGA;  IEEE80211;  Linux driver;  mac80211;  open source;  SDR;  SoC;  SoftMAC;  Wi-Fi;  Zynq},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rafique20204543,
author={Rafique, W. and Zhao, X. and Yu, S. and Yaqoob, I. and Imran, M. and Dou, W.},
title={An Application Development Framework for Internet-of-Things Service Orchestration},
journal={IEEE Internet of Things Journal},
year={2020},
volume={7},
number={5},
pages={4543-4556},
doi={10.1109/JIOT.2020.2971013},
art_number={8978572},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084924820&doi=10.1109%2fJIOT.2020.2971013&partnerID=40&md5=c2503ae314c8c019150b9b573c3c106e},
abstract={Application development for the Internet of Things (IoT) poses immense challenges due to the lack of standard development frameworks, tools, and techniques to assist end users in dealing with the complexity of IoT systems during application development. These challenges invoke the use of model-driven development (MDD) along with the representational state transfer (REST) architecture to develop IoT applications, supporting model generation at different abstraction levels while generating software implementation artifacts for heterogeneous platforms and ensuring loose coupling in complex IoT systems. This article proposes an IoT application development framework, named IADev, which uses attribute-driven design and MDD to address the above-mentioned challenges. This framework is composed of two major steps, including iterative architecture development using attribute-driven design and generating models to guide the transformation using MDD. IADev uses attribute-driven design to transform the requirements into a solution architecture by considering the concerns of all involved stakeholders, and then, MDD metamodels are generated to hierarchically transform the design components into the software artifacts. We evaluate IADev for a smart vehicle scenario in an intelligent transportation system to generate an executable implementation code for a real-world system. The case study experiments proclaim that IADev achieves higher satisfaction of the participants for the IoT application development and service orchestration, as compared to conventional approaches. Finally, we propose an architecture that uses IADev with the Siemens IoT cloud platform for service orchestration in industrial IoT. © 2014 IEEE.},
author_keywords={Attribute-driven design;  Internet-of-Things (IoT) application development;  IoT service orchestration;  model-driven design;  representational state transfer (REST) application programming interface (API);  software architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nishimoto202089,
author={Nishimoto, K. and Tochino, T. and Hatano, T. and Asaka, K. and Kani, J.-I. and Terada, J.},
title={Mini-PON: Disaggregated module-type PON architecture for realizing various PON deployments},
journal={Journal of Optical Communications and Networking},
year={2020},
volume={12},
number={5},
pages={89-98},
doi={10.1364/JOCN.383717},
art_number={9036750},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082079771&doi=10.1364%2fJOCN.383717&partnerID=40&md5=e7ba4b9238dc4857529dfce2149c3ed5},
abstract={We propose a new optical access network concept named Mini-PON. This concept features the separation of passive optical network (PON)-optical line terminal (OLT) functions into micro module-type hardware that offers switch port pluggability and minimal modular software running on a server to literally minimize the space needed for PON deployment, giving more flexibility to operators who want to deploy PONs in various fields quickly and cost effectively. We explore the optimal function split between hardware and software to balance performance and flexibility. A prototype demonstrates that the proposed implementation architecture can perform as well as a conventional PON in terms of throughput and upstream latency, while also realizing (i) dynamic bandwidth allocation softwarization and modularization via standards-based application programming interfaces and (ii) remote placement of the hardware module from the central office on which the OLT software runs: both are key advances. © 2009-2012 OSA.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Correia2020,
author={Correia, A. and Felber, P. and Ramalhete, P.},
title={Persistent memory and the rise of universal constructions},
journal={Proceedings of the 15th European Conference on Computer Systems, EuroSys 2020},
year={2020},
doi={10.1145/3342195.3387515},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087110141&doi=10.1145%2f3342195.3387515&partnerID=40&md5=27502d07181ed89a80ca5aa651b9ed93},
abstract={Non-Volatile Main Memory (NVMM) has brought forth the need for data structures that are not only concurrent but also resilient to non-corrupting failures. Until now, persistent transactional memory libraries (PTMs) have focused on providing correct recovery from non-corrupting failures without memory leaks. Most PTMs that provide concurrent access do so with blocking progress. The main focus of this paper is to design practical PTMs with wait-free progress based on universal constructions. We first present CX-PUC, the first bounded wait-free persistent universal construction requiring no annotation of the underlying sequential data structure. CX-PUC is an adaptation to persistence of CX, a recently proposed universal construction. We next introduce CX-PTM, a PTM that achieves better throughput and supports transactions over multiple data structure instances, at the price of requiring annotation of the loads and stores in the data structure-As is commonplace in software transactional memory. Finally, we propose a new generic construction, Redo-PTM, based on a finite number of replicas and Herlihy's wait-free consensus, which uses physical instead of logical logging. By exploiting its capability of providing wait-free ACID transactions, we have used Redo-PTM to implement the world's first persistent key-value store with bounded wait-free progress. © 2020 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jacobs2020,
author={Jacobs, R. and Mayeshiba, T. and Afflerbach, B. and Miles, L. and Williams, M. and Turner, M. and Finkel, R. and Morgan, D.},
title={The Materials Simulation Toolkit for Machine learning (MAST-ML): An automated open source toolkit to accelerate data-driven materials research},
journal={Computational Materials Science},
year={2020},
volume={176},
doi={10.1016/j.commatsci.2020.109544},
art_number={109544},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078221343&doi=10.1016%2fj.commatsci.2020.109544&partnerID=40&md5=2017f2224a4c8b5049fb112d56311ef7},
abstract={As data science and machine learning methods are taking on an increasingly important role in the materials research community, there is a need for the development of machine learning software tools that are easy to use (even for nonexperts with no programming ability), provide flexible access to the most important algorithms, and codify best practices of machine learning model development and evaluation. Here, we introduce the Materials Simulation Toolkit for Machine Learning (MAST-ML), an open source Python-based software package designed to broaden and accelerate the use of machine learning in materials science research. MAST-ML provides predefined routines for many input setup, model fitting, and post-analysis tasks, as well as a simple structure for executing a multi-step machine learning model workflow. In this paper, we describe how MAST-ML is used to streamline and accelerate the execution of machine learning problems. We walk through how to acquire and run MAST-ML, demonstrate how to execute different components of a supervised machine learning workflow via a customized input file, and showcase a number of features and analyses conducted automatically during a MAST-ML run. Further, we demonstrate the utility of MAST-ML by showcasing examples of recent materials informatics studies which used MAST-ML to formulate and evaluate various machine learning models for an array of materials applications. Finally, we lay out a vision of how MAST-ML, together with complementary software packages and emerging cyberinfrastructure, can advance the rapidly growing field of materials informatics, with a focus on producing machine learning models easily, reproducibly, and in a manner that facilitates model evolution and improvement in the future. © 2020 Elsevier B.V.},
author_keywords={Data science;  Machine learning;  Materials informatics;  Open source software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Philip202098,
author={Philip, M.M. and Natarajan, K. and Ramanathan, A. and Balakrishnan, V.},
title={Composite pattern to handle variation points in software architectural design of evolving application systems},
journal={IET Software},
year={2020},
volume={14},
number={2},
pages={98-105},
doi={10.1049/iet-sen.2019.0006},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083333673&doi=10.1049%2fiet-sen.2019.0006&partnerID=40&md5=8f2b1149e24bd48927fa601e75e11784},
abstract={The variation points in software architecture arise as a result of the availability of large number of filters and component libraries. An integration of different architectural styles is crucial and necessary in the development of large-scale software application systems to handle the variation points. This article proposes a composite software architectural style for building application systems involving data streams, user interactivity, and dynamic mode. It uses a pattern within a pattern approach for combining the architectural styles. This approach provides flexibility to add or delete any filter or component at run time. In addition, the changes in the order of processing of the different filters or components can also be incorporated. The software architectural specification for any combination of input components and their order of processing is generated automatically. This specification acts as a baseline for the subsequent design and implementation phases of the application system. This model is generic and has been successfully validated for a prototype application system involving all the three modes of operation. © The Institution of Engineering and Technology 2019.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jadhav2020,
author={Jadhav, S.S. and Gloster, C. and Naher, J. and Doss, C. and Kim, Y.},
title={An FPGA-based Application-Specific Processor for Implementing the Exponential Function},
journal={Conference Proceedings - IEEE SOUTHEASTCON},
year={2020},
volume={2020-March},
doi={10.1109/SoutheastCon44009.2020.9249746},
art_number={9249746},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097830706&doi=10.1109%2fSoutheastCon44009.2020.9249746&partnerID=40&md5=0d91d1b94edf9971c44b25f45cab61a7},
abstract={In this paper, we present a pipelined, high throughput single precision floating point implementation of the exponential function with a latency of 19 cycles. We also present the Application Specific Processor (ASP), a multiple memory architecture that increases memory bandwidth and overall performance of a computationally intensive application. The exponential function hardware unit is used as a function core or arithmetic unit of the ASP. Our experimental results show that executing a hardware implementation of the exponential function on a Field Programmable Gate Array (FPGA) is significantly faster than executing a software implementation on a multi-core processor. While the maximum clock rate of our FPGA board (200 MHz) is an order-of-magnitude slower than our multi-core processor (3.4 GHz), the FPGA-based hardware implementation of the exponential function is 29X and 8X faster than the multi-core processor-based software implementation and OpenMP implementation respectively. © 2020 IEEE.},
author_keywords={exponential function;  field programmable gate array;  multi-memory architecture;  reconfigurable computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xingguo2020835,
author={Xingguo, Q. and Kun, W.},
title={Design and Implementation of Toolbox Based on Component Technology},
journal={Proceedings - 2020 International Conference on Computer Engineering and Application, ICCEA 2020},
year={2020},
pages={835-838},
doi={10.1109/ICCEA50009.2020.00183},
art_number={9103839},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086402566&doi=10.1109%2fICCEA50009.2020.00183&partnerID=40&md5=db08827b3cd176ef921902189a1f448d},
abstract={With the country's implementation of smart mines, Internet technology has been well integrated into the mine disaster prevention system. In order to improve the accuracy and timeliness of mine monitoring data, each mine has established a mine data monitoring system suitable for its situation. As the most important part of the monitoring software, the data monitoring module needs to be able to dynamically meet the various needs of users. This article mainly introduces the tools that use the component technology to package the data monitoring module into a toolbox. The user calls the dynamic link library through reflection technology to implement various functions in the toolbox, and implements the toolbox controls and data monitoring software developed by third-party platforms Organic combination. Solved the poor scalability of monitoring software, Problems requiring frequent software replacement for customers after system update. This method has proved its good flexibility and timeliness in a hydrological monitoring system. © 2020 IEEE.},
author_keywords={Component technology;  Data monitoring system;  Dynamic link library;  reflection;  Toolbox},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{JoannaSantos202063,
author={Joanna Santos, C.S. and Moshtari, S. and Mirakhorli, M.},
title={An Automated Approach to Recover the Use-case View of an Architecture},
journal={Proceedings - 2020 IEEE International Conference on Software Architecture Companion, ICSA-C 2020},
year={2020},
pages={63-66},
doi={10.1109/ICSA-C50368.2020.00020},
art_number={9095618},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085767195&doi=10.1109%2fICSA-C50368.2020.00020&partnerID=40&md5=ee5f706e779f3a9c484aa9f6cafc6d9c},
abstract={Many tools and techniques were described in the literature for automated recovery of software architecture from software artifacts, such as code. These approaches generate models of software architecture at different levels of granularity and notations. Whereas there is a vast literature in recovering components, packages, and interactions between them, we lack automated approaches for recovering the use-case view of an architecture. In the 4+1 view of the architecture, use-case view or scenarios represents a view of the architecture in terms of the systems' core functionalities provided to end-users. This view is essential in understanding the system and its underlying architectural decisions. Manually recovering and documenting the application's scenarios from source code is time-consuming, as large-scale enterprise systems can have a large number of scenarios. In this NEMI paper, we present a novel automated approach for recovering the scenarios from Web applications source code. These scenarios are shown in use case diagrams alongside with sequence diagrams that further describe how each use case is implemented in the system. Our approach works under the assumption that the URLs (endpoints) of a Web application can give us clues to the system's use cases. Therefore, our technique combines a set of heuristics and static analysis in order to detect the endpoints in a Java Web application as well as the backend classes and methods that will process the request. Subsequently, it uses Natural Language Processing (NLP) techniques to extract use cases from these identified endpoints and uses the computed program slices to generate sequence diagrams for each identified use case. We conducted an initial evaluation of our approach by detecting endpoints in Sagan, an existing open-source Web application. We then demonstrate the use cases generated and how their implementation looks like through sequence diagrams. © 2020 IEEE.},
author_keywords={Architecture recovery;  Use case view;  Web applications endpoints},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ladines2020,
author={Ladines, A.N. and Hammerschmidt, T. and Drautz, R.},
title={BOPcat software package for the construction and testing of tight-binding models and bond-order potentials},
journal={Computational Materials Science},
year={2020},
volume={173},
doi={10.1016/j.commatsci.2019.109455},
art_number={109455},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076245208&doi=10.1016%2fj.commatsci.2019.109455&partnerID=40&md5=fee3dd54d81f76be08781ac2e7ca293b},
abstract={Atomistic models like tight-binding (TB), bond-order potentials (BOP) and classical potentials describe the interatomic interaction in terms of mathematical functions with parameters that need to be adjusted for a particular material. The procedures for constructing TB/BOP models differ from the ones for classical potentials. We developed the BOPcat software package as a modular python code for the construction and testing of TB/BOP parameterizations. It makes use of atomic energies, forces and stresses obtained by TB/BOP calculations with the BOPfox software package. It provides a graphical user interface and flexible control of raw reference data, of derived reference data like defect energies, of automated construction and testing protocols, and of parallel execution in queuing systems. We demonstrate the concepts and usage of the BOPcat software and illustrate its key capabilities by exemplary constructing and testing a parameterization of a magnetic BOP for Fe. We provide a parameterization protocol with a successively increasing set of reference data that leads to good transferability to a variety of properties of the ferromagnetic bcc groundstate and to crystal structures which were not part of the training set. © 2019 Elsevier B.V.},
author_keywords={Atomistic simulations;  Bond-order potentials;  Tight-binding},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hu2020532,
author={Hu, M. and Zhang, Y.},
title={The Python/C API: Evolution, Usage Statistics, and Bug Patterns},
journal={SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering},
year={2020},
pages={532-536},
doi={10.1109/SANER48275.2020.9054835},
art_number={9054835},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083582294&doi=10.1109%2fSANER48275.2020.9054835&partnerID=40&md5=fa73da4917836e87608f266defec9925},
abstract={Python has become one of the most popular programming languages in the era of data science and machine learning, especially for its diverse libraries and extension modules. Python front-end with C/C++ native implementation achieves both productivity and performance, almost becoming the standard structure for many mainstream software systems. However, feature discrepancies between two languages can pose many security hazards in the interface layer using the Python/C API. In this paper, we applied static analysis to reveal the evolution and usage statistics of the Python/C API, and provided a summary and classification of its 10 bug patterns with empirical bug instances from Pillow, a widely used Python imaging library. Our toolchain can be easily extended to access different types of syntactic bug-finding checkers. And our systematical taxonomy to classify bugs can guide the construction of more highly automated and high-precision bug-finding tools. © 2020 IEEE.},
author_keywords={Bug pattern;  Evolution analysis;  Fact extraction;  Python/C API;  Static analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lai2020,
author={Lai, H.-H.},
title={Applicability of a design assessment and management for the current ammunition depots in Taiwan},
journal={Applied Sciences (Switzerland)},
year={2020},
volume={10},
number={3},
doi={10.3390/app10031041},
art_number={1041},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081556854&doi=10.3390%2fapp10031041&partnerID=40&md5=814b2edb4f5c693edacc3b9ed381f8df},
abstract={In Taiwan, many ammunition depots have become outdated after having been in service for a long period of time, and if they are not properly managed, then accidental explosions might erupt inside. Leakage pressure after an explosion is closely related to the opening of the structure and the thickness of the wall. In order to reduce the risk of implosion, it is necessary to design a new structure or strengthen the existing ammunition libraries for the storage of ammunition required for combat. In order to evaluate the applicability of an existing ammunition depot design, making management simpler and safer, this study integrates the scale model experiment of an ammunition depot with computer simulation, the arbitrary Lagrangian-Eulerian (ALE) algorithm in ANSYS/LS-DYNA software, and it compares the results with the UFC3-340-02 specification in order to verify its applicability. The results show that computer simulation can verify that the data related to an implosion of an ammunition depot is similar to the specification. Therefore, the design results of the ammunition depot optimized by computer simulation can be used as a reference for the construction or strengthening of ammunition depots. © 2020 by the authors.},
author_keywords={Accidental explosion;  Arbitrary Lagrangian-Eulerian (ALE);  Internal blast;  Leakage pressure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mert2020353,
author={Mert, A.C. and Ozturk, E. and Savas, E.},
title={Design and Implementation of Encryption/Decryption Architectures for BFV Homomorphic Encryption Scheme},
journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
year={2020},
volume={28},
number={2},
pages={353-362},
doi={10.1109/TVLSI.2019.2943127},
art_number={8866755},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078835901&doi=10.1109%2fTVLSI.2019.2943127&partnerID=40&md5=7800b9fb96fce577b83312c296723755},
abstract={Fully homomorphic encryption (FHE) is a technique that allows computations on encrypted data without the need for decryption and it provides privacy in various applications such as privacy-preserving cloud computing. In this article, we present two hardware architectures optimized for accelerating the encryption and decryption operations of the Brakerski/Fan-Vercauteren (BFV) homomorphic encryption scheme with high-performance polynomial multipliers. For proof of concept, we utilize our architectures in a hardware/software codesign accelerator framework, in which encryption and decryption operations are offloaded to an FPGA device, while the rest of operations in the BFV scheme are executed in software running on an off-the-shelf desktop computer. Specifically, our accelerator framework is optimized to accelerate Simple Encrypted Arithmetic Library (SEAL), developed by the Cryptography Research Group at Microsoft Research. The hardware part of the proposed framework targets the XILINX VIRTEX-7 FPGA device, which communicates with its software part via a peripheral component interconnect express (PCIe) connection. For proof of concept, we implemented our designs targeting 1024-degree polynomials with 8-bit and 32-bit coefficients for plaintext and ciphertext, respectively. The proposed framework achieves almost 12× and 7× latency speedups, including I/O operations for the offloaded encryption and decryption operations, respectively, compared to their pure software implementations. © 1993-2012 IEEE.},
author_keywords={Fan-Vercauteren (FV);  FPGA;  hardware;  number theoretic transform;  Simple Encrypted Arithmetic Library (SEAL)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mamedov2020157,
author={Mamedov, T. and Doroshenko, A. and Shevchenko, R.},
title={Static analysis of .NET programs using rewriting rules},
journal={CEUR Workshop Proceedings},
year={2020},
volume={2866},
pages={157-163},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106740172&partnerID=40&md5=8961000d153d6771f3b43f172fa522f2},
abstract={A software tool that finds problems of resource consumption in case of work with files using rewriting rules was implemented. To reach the goal, the TermWare system, which could be embedded into those systems running on JVM, was used. TermWare system is a term processing system that provides a language for describing rewriting rules that operate on a data structure called term and a rules interpreter for transforming terms. In order to work with C#-programs, the special plugin for TermWare, which helps to generate appropriate terms from source code, was developed. Roslyn compiler was embedded in the plugin to make an analysis of the program. Roslyn provides definitive syntax and sematic code analysis API for C# and Visual Basic programming languages. Those, Roslyn allows users to find syntax errors in programs and focus on the primary task of generating terms from source code during the development of the plugin. Other methods and basic approaches to solving a problem of resource consumption were found, such as runtime inspection, and the advantages and disadvantages of those ways were described. The description of the problem includes the reasons for those issues being appeared and the states of the program that needs to be generated and analyzed were illustrated with the examples with syntax constructions of the C# language. Also, a practical application based on the TermWare system - a static analyzer that finds problems of open-close files, was implemented. The example of rules to find the problem of open-close files in the C#-program was illustrated in the article. Copyright © 2020 for this paper by its authors.},
author_keywords={Analysis of resource consumption;  Rewriting rules;  Terms generator;  TermWare},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Huang2020191726,
author={Huang, Y.-P. and Tsai, T.-Y. and Chen, T.-H. and Chueh, C.-B. and Tsai, Y.-N. and Chang, Y.-W. and Wu, Y.-C. and Chen, H.-W. and Tsai, M.-T. and Hung, Y.-P. and Lee, H.-C.},
title={A generic framework for Fourier-domain optical coherence tomography imaging: Software architecture and hardware implementations},
journal={IEEE Access},
year={2020},
volume={8},
pages={191726-191736},
doi={10.1109/ACCESS.2020.3032105},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102797364&doi=10.1109%2fACCESS.2020.3032105&partnerID=40&md5=bd1315ea6aad1927191109f3a9898562},
abstract={Fourier-domain optical coherence tomography (FD-OCT), including spectral-domain OCT (SD-OCT) and swept-source OCT (SS-OCT), allows the volumetric imaging of the tissue architecture with a faster speed and higher detection sensitivity than does time-domain OCT. Although the hardware implementations of SD-OCT and SS-OCT are different, these technologies share very similar signal processing steps for image reconstruction. In this study, we developed hardware implementations and software architectures to design a generic framework for FD-OCT. For SD-OCT systems, an external synchronization approach was used to realize a data acquisition schematic similar to that used in SS-OCT by carefully managing the timing clocks in the detection unit and for the waveform generation. In addition, by utilizing modules and factory concepts, a software engine can be developed that supports various acquisition devices and software operations or image processing functions with high operation flexibility while maintaining its robustness. Data processing and data saving were optimized using the parallel computing method with the OpenMP library and by leveraging the parallelism within the acquired data, respectively. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
author_keywords={Biomedical optical imaging;  Biophotonics;  Data acquisition;  Medical diagnostic imaging;  Parallel processing;  Software architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kurmoiartseva202080,
author={Kurmoiartseva, K. and Kotelnikova, N. and Trusov, P.},
title={Modeling of Polycrystalline Materials Deformation with Dislocation Structure Evolution and Transition to Fracture},
journal={Communications in Computer and Information Science},
year={2020},
volume={1304},
pages={80-94},
doi={10.1007/978-3-030-66895-2_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101584693&doi=10.1007%2f978-3-030-66895-2_6&partnerID=40&md5=4db6cfbe6aa0d01ce2caf1f99dc49fb0},
abstract={Computing experiments on simulation of deformation and destruction processes of polycrystalline materials is a relevant and efficient research method. This approach suggests building a mathematical model of the material and its exploitation for numerical experiments. The development of the physically based mathematical model for the description of the material behavior during deformation will allow optimizing the properties of constructions without conducting numerous natural experiments. It has been established that when solving similar tasks, it is necessary to take into account the evolution of the meso- and microstructure, including nucleation and development of defects at various structural and scale levels. Consideration of the dislocation structure evolution enables establishing the regions of dislocation accumulation and, as a consequence, simulating the nucleation and subsequent development of microcracks. The purpose of the present study is to develop and realize the dislocation-oriented direct elasto-visco-plastic model, taking into account the processes of microcrack nucleation. The structure and the algorithm of the numerical implementation of the direct model have been presented; the structural levels of the description of elasto-plastic deformation have been identified; a system of evolution equations for the description of dislocation motions and dislocation substructures formation on slip systems with subsequent transition to destruction has been presented. Using the parameters, characterising the dislocation structure, a criterion of the transition into the destructed state has been suggested, in which the material element of the appropriate scale level loses an ability to resist external impacts. The description of the application software package intended for the implementation of multilevel models of the representative volume of polycrystalline solids has been given. The submodel of the description of the dislocation density evolution of the crystallite has been realised; the influence of the mechanisms of nucleation and annihilation of dislocations on their total density has been analysed. © 2020, Springer Nature Switzerland AG.},
author_keywords={Algorithms;  Crystal plasticity;  Dislocation densities;  Dislocation-Oriented model;  Microcrack nucleation;  Numerical simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ives2020,
author={Ives, T. and Knudstrup, J. and Pellegrin, F.},
title={Moving to the open source open62541 library for PLC communication at the European Southern Observatory's Very Large Telescope},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2020},
volume={11452},
doi={10.1117/12.2561532},
art_number={114521P},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099368150&doi=10.1117%2f12.2561532&partnerID=40&md5=72da3c73dc6c4ffcccbd41e6ee4f0f64},
abstract={In the last ten years the European Southern Observatory's (ESO) Very Large Telescope (VLT) Instrumentation Framework has begun moving its low level interface to instrument functions away from VME-based Local Control Units (LCU's) to Commercial Off The Shelf (COTS) components connected with industry standard fieldbuses. This move has resulted in the adoption of PC-based Programmable Logical Controllers (PLC's) that directly control the instrument devices, connected via Ethernet to Linux workstations that provide high level coordination and user-interfaces. To enable this shift to COTS components, a new "fieldbus-aware"Instrument Control System Base (IC0FB) was developed which utilizes, among others, the Open Platform Communications Unified Architecture (OPC UA) standard for communication between the workstations and the PLC's. The initial implementation of IC0FB used closed source libraries for this OPC-UA-based communication, however, licensing restrictions made compiling and distributing difficult throughout the VLT project. This has prompted the recent re-implementation of the OPC UA IC0FB Communication Interface using the open source library open625411 and the adoption of open62541 for ESO's new Extreme Large Telescope. In this paper, we discuss the lessons learned in moving to open source implementation of an industry standard. We compare the performance of the open62541 implementation and the implementation based on the commercially licensed Softing Automation SDK2 and show that the performance of the open source solution is comparable to the closed source implementation. © 2020 SPIE.},
author_keywords={Fieldbus;  OPC-UA;  open62541;  VLT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Oya2020,
author={Oya, I. and CTA Observatoryb},
title={Supporting the coordination of a software work-package of the Cherenkov Telescope Array via model-driven methodologies},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2020},
volume={11450},
doi={10.1117/12.2560929},
art_number={1145011},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099296426&doi=10.1117%2f12.2560929&partnerID=40&md5=be5c966b23eac25ebe60533531d0f4e7},
abstract={The Cherenkov Telescope Array (CTA) is the next-generation atmospheric Cherenkov gamma-ray observatory. CTA will be deployed as two installations, one in the Northern and the other in the Southern Hemisphere, containing telescopes of three different sizes, for covering different energy domains, with varying designs. The CTA Observatory (CTAO) is building a complex and distributed software system for the efficient operation of the arrays and the management and scientific exploitation of the CTA data. The largest fraction of the construction budget of CTA will come as in-kind contributions (IKCs) from CTA Shareholders (participating countries and research centres), with the prominent IKC example of the telescopes themselves. Most of the effort to build the CTA software will be provided by IKCs as well, with CTAO personnel managing the coordination between partners, ensuring common development practices, and facilitating that the requirements, integration, quality level, and deadlines are properly met. This contribution presents the management plan of one of the main CTA software work packages, the Array Control and Data Acquisition (ACADA). The ACADA software will be responsible for the coordination of the control and data acquisition of telescopes, as well as many auxiliary instruments. The ACADA team will be composed of about 30 developers distributed in eight IKC teams, CTAO personnel, and company contracts. The contribution shows how model-driven software architecture practices are essential for the management of such a distributed team, which is predominantly composed of IKCs © 2020 SPIE.},
author_keywords={Cherenkov Telescope Array;  In-kind contributions;  Model-driven development;  Software;  Software project management},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alrubaye2020245,
author={Alrubaye, H. and Alshoaibi, D. and Alomar, E. and Mkaouer, M.W. and Ouni, A.},
title={How Does Library Migration Impact Software Quality and Comprehension? An Empirical Study},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12541 LNCS},
pages={245-260},
doi={10.1007/978-3-030-64694-3_15},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097834210&doi=10.1007%2f978-3-030-64694-3_15&partnerID=40&md5=5ef51ece62a21c920bb56c7c5a3629cf},
abstract={The process of migration between different third-party software libraries, while being an typical library reuse practice, is complex, time consuming and error-prone. Typically, during a library migration process, developers opt to replace methods from a retired library with other methods from a new library without altering the software behavior. However, the extent to which the process of migrating to new libraries will be rewarded with improved software quality is still unknown. In this paper, our goal is to study the impact of library API migration on software quality. We conducted a large-scale empirical study on 9 popular API migrations, collected from a corpus of 57,447 open-source Java projects. We computed the values of commonly-used software quality metrics before and after a migration occurs. The statistical analysis of the obtained results provides evidence that library migrations are likely to improve different software quality attributes including significantly reduced coupling, increased cohesion, and improved code readability. Furthermore, we released an online portal that helps software developers to understand the impact of a library migration on software quality and recommend migration examples that adopt best design and implementation practices to improve software quality. Finally, we provide the software engineering community with a large scale dataset to foster research in software library migration. © 2020, Springer Nature Switzerland AG.},
author_keywords={API migration;  Code comprehension;  Software quality},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sukhinov2020341,
author={Sukhinov, A. and Chistyakov, A. and Litvinov, V. and Atayan, A. and Nikitina, A. and Filina, A.},
title={Supercomputer Modeling of the Hydrodynamics of Shallow Water with Salt and Heat Transport},
journal={Communications in Computer and Information Science},
year={2020},
volume={1331},
pages={341-352},
doi={10.1007/978-3-030-64616-5_30},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097829414&doi=10.1007%2f978-3-030-64616-5_30&partnerID=40&md5=6f4369b0b644473b6d96cd9673f43e44},
abstract={The paper covers the investigation of hydrodynamics processes in shallow water taking into account the salt and temperature transport on supercomputer. Proposed model is based on the equations of motion (Navier-Stokes), the continuity equation for the case of variable density, as well as the equations of transport of salts and heat. The complex geometry of the coastline and bottom surface of natural reservoirs involves solving a number of problems related to the selection of parameters and the construction of the computational grid used for the numerical implementation of a discrete analogue of the mathematical model of hydrodynamic processes. Improving the accuracy of calculations, fulfilling the conditions of convergence and stability of the numerical solution of the problem involves crushing the step of the computational grid, which leads to the need to use supercomputer computing systems to obtain calculation results for a given period of time. The developed numerical methods and algorithms formed the basis of a software package tested on a supercomputer cluster using OpenMP and MPI technologies. Algorithms are being developed that adapt individual elements of the software package for the NVIDIA CUDA architecture and cloud computing. © 2020, Springer Nature Switzerland AG.},
author_keywords={Hydrodynamics;  Parallel algorithm;  Salt and heat transport;  Shallow water;  Software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chumack202030,
author={Chumack, V. and Tsyvinskyi, S. and Kovalenko, M. and Ponomarev, O. and Tkachuk, I.},
title={MATHEMATHICAL MODELING OF A SYNCHRONOUS GENERATOR WITH COMBINED EXCITATION},
journal={Eastern-European Journal of Enterprise Technologies},
year={2020},
volume={1},
number={5},
pages={30-36},
doi={10.15587/1729-4061.2020.193495},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097248792&doi=10.15587%2f1729-4061.2020.193495&partnerID=40&md5=2373d80944cf6ea24d5596155bb940eb},
abstract={The generators of classical design ‒ with a cylindrical stator and rotor ‒ are of interest. This is predetermined by that a given structure is the most common, simple, and technological. The result of the development of such electric machines is a possibility to build a combined series of induction motors and magnetoelectric synchronous machines. In these machines, replacing a short-circuited rotor by a rotor with permanent magnets and controlled working magnetic flux turns the induction machine into a magnetoelectric synchronous one. All existing generators with permanent magnets have a major drawback: there is almost no possibility to control output voltage and, in some cases, power. This is especially true for autonomous power systems. Known methods of output voltage control lead to higher cost, compromised reliability, deterioration of mass-size indicators. This paper reports the construction of a three-dimensional field mathematical model of a magnetoelectric synchronous generator with permanent magnets. The model has been implemented using a finite element method in the software package COMSOL Multiphysics. We show the distribution of the electromagnetic field in the active volume of the generator under control and without it. The impact of a control current in the magnetized winding on the external characteristics of the generator at a different coefficient of load power has been calculated. Applying the devised model has enabled the synthesis of a current control law in the magnetizing winding at a change in the load over a wide range. The results obtained demonstrate that it is possible to control output voltage of the generator with permanent magnets by using an additional magnetizing winding. The winding acts as an electromagnetic bridge for the main magnetic flux, which is created by permanent magnets. Our analysis of results has shown that it is possible to regulate the output voltage of the generator with constant magnets within –35 %, +15 %. © 2020 Vadim Chumack, Serhii Tsyvinskyi, Mykhailo Kovalenko, Alexej Ponomarev, Ihor Tkachuk.},
author_keywords={generator voltage control;  magnetizing winding;  magnetoelectric excitation;  permanent magnets},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2020212467,
author={Lee, K.-S. and Lee, C.-G.},
title={Identifying Semantic Outliers of Source Code Artifacts and Their Application to Software Architecture Recovery},
journal={IEEE Access},
year={2020},
volume={8},
pages={212467-212477},
doi={10.1109/ACCESS.2020.3040024},
art_number={9269345},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097137703&doi=10.1109%2fACCESS.2020.3040024&partnerID=40&md5=359703a9aef085e231383c8674dd464a},
abstract={Understanding software architecture is essential to software maintenance. There has been much effort to derive software architecture views from source code artifacts. Typically, along with structural information, the semantic information derived from an identifier name and comments are helpful. However, because code vocabulary choice depends on a developer's subjective decision, some source code may have semantically low text quality, leading to an inaccurate architecture recovery. This paper aims to improve the architecture recovery of a software system by identifying and removing the semantic outliers of source code artifacts. Accordingly, we propose a novel measure Conceptual Conformity (CC), which computes the similarity between two latent topic distributions obtained from both the source code and its package. We use CC to identify source code that is not relevant to the package's semantic context and define it as a semantic outlier. Because the semantic outliers may cause inaccurate architecture recovery, we remove them during the recovery process. We apply our approach to three open-source projects. The results demonstrate that, for projects with low recovery performance, removing outliers leads to higher recovery accuracy. © 2020 IEEE.},
author_keywords={semantic outlier;  Software architecture recovery;  software quality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2020,
author={Zhang, L.-L. and Zhao, Q. and Wang, L. and Zhang, L.-Y.},
title={Research on Urban Traffic Signal Control Systems Based on Cyber Physical Systems},
journal={Journal of Advanced Transportation},
year={2020},
volume={2020},
doi={10.1155/2020/8894812},
art_number={8894812},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095844795&doi=10.1155%2f2020%2f8894812&partnerID=40&md5=ea2e986b581427e976fbe3820ce31e63},
abstract={In this paper, we present a traffic cyber physical system for urban road traffic signal control, which is referred to as UTSC-CPS. With this proposed system, managers and researchers can realize the construction and simulation of various types of traffic scenarios, the rapid development, and optimization of new control strategies and can apply effective control strategies to actual traffic management. The advantages of this new system include the following. Firstly, the fusion architecture of private cloud computing and edge computing is proposed for the first time, which effectively improves the performance of software and hardware of the urban road traffic signal control system and realizes information security perception and protection in cloud and equipment, respectively, within the fusion framework; secondly, using the concept of parallel system, the depth of real-time traffic control subsystem and real-time simulation subsystem is realized. Thirdly, the idea of virtual scene basic engine and strategy agent engine is put forward in the system design, which separates data from control strategy by designing a general control strategy API and helps researchers focus on control algorithm itself without paying attention to detection data and basic data. Finally, considering China, the system designs a general control strategy API to separate data from control strategy. Most of the popular communication protocols between signal controllers and detectors are private protocols. The standard protocol conversion middleware is skillfully designed, which decouples the field equipment from the system software and achieves the universality and reliability of the control strategy. To further demonstrate the advantages of the new system, we have carried out a one-year practical test in Weifang City, Shandong Province, China. The system has been proved in terms of stability, security, scalability, practicability and rapid practice, and verification of the new control strategy. At the same time, it proves the superiority of the simulation subsystem in the performance and simulation scale by comparing the different-scale road networks of Shunyi District in Beijing and Weifang City in Shandong Province. Further tests were conducted using real intersections, and the results were equally valid. © 2020 Li-li Zhang et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Serban2020908,
author={Serban, C. and Shaikh, M.},
title={Software reliability prediction using package level modularization metrics},
journal={Procedia Computer Science},
year={2020},
volume={176},
pages={908-917},
doi={10.1016/j.procs.2020.09.086},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093363108&doi=10.1016%2fj.procs.2020.09.086&partnerID=40&md5=60561b43919118a493ab2231ad1a1df3},
abstract={Reliability ensures architectural strength and error free operations of software systems. Software design and architectural measures have been studied as key indicators of faults in software systems. However, association between quantification of reliability prediction using design-level metrics has not been explored. This paper presents a novel approach of developing reliability metrics and it's prediction using package level metrics. In particular, relevant fault severity information is empirically experimented with package level metrics in an effort-aware classification and ranking scenario. Results obtained hint significant view to predict the reliability of software systems using architectural level metrics. Therefore, the empirical analysis can guide development process to be design-focused and avoid accumulation of faults in implementation phase. © 2020 The Authors. Published by Elsevier B.V.},
author_keywords={Machine Learning;  Reliability;  Software Assessment;  Software Package Metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fomin202033,
author={Fomin, O. and Lovska, A. and Kudelya, V. and Smyrnova, I.},
title={Determining the dynamic loading and strength of the bearing structure of a covered wagon when firing from it},
journal={Eastern-European Journal of Enterprise Technologies},
year={2020},
volume={4},
number={7-106},
pages={33-41},
doi={10.15587/1729-4061.2020.208407},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092340604&doi=10.15587%2f1729-4061.2020.208407&partnerID=40&md5=bda623fdd755d61a2a8daa2cc799311b},
abstract={The bearing structure of a covered rail wagon has been improved to enable firing from it at motion. The covered wagon of model 11-217 was chosen as a prototype. To enable firing in the vertical plane, it has been proposed to use a sliding roof, which consists of shutters that move by means of a pneumatic or hydraulic drive. To accommodate military equipment inside the covered wagon, its frame is equipped with supporting sectors. Mathematical modeling was performed in order to determine the dynamic load on a covered rail wagon when firing from it. The mathematical model was solved in the Mathcad software package. We have established the dependence of the accelerations of the bearing structure of a covered rail wagon on the recoil force induced by the combat equipment that it hosts. It has been found that in order to maintain the dynamics indicators within acceptable limits, combat equipment should have a maximum recoil at a shot of about 3.2 kN. The maximum accelerations that act on the bearing structure of a covered wagon in a vertical plane are about 6 m/s2. In the zones of interaction between the body and bogies, the maximum accelerations are about 9.5 m/s2 and the accelerations of bogies are 10 m/s2. To reduce the dynamic load on the bearing structure of a covered rail wagon, it has been proposed to use a viscous connection between the supporting sectors and frame. We have determined the dependence of accelerations on the coefficient of viscous resistance between the supporting sectors and the bearing structure of a wagon. It has been established that taking into consideration the use of a viscous connection between the supporting sectors and frame makes it possible to reduce the dynamic load on a wagon at least by 15 %. The basic indicators of strength for the bearing structure of a covered rail wagon when firing from it have been determined. We have derived the dependence of the maximum equivalent stresses in the bearing structure of a covered wagon on the recoil force of combat equipment. The maximum equivalent stresses at a recoil force of 3.2 kN arise in the console part of the girder of a covered wagon and are about 300 MPa. The maximum displacements were registered in the area where the front stops of the auto-coupling are arranged; they are equal to 2.9 mm. The maximum deformations amounted to 6.98·10-3. Modal analysis of the bearing structures of a covered rail wagon has been carried out. It has been determined that the values of the oscillation natural frequencies are within the permissible limits. Our study will contribute to the construction of innovative rolling stock for the transportation of military equipment and for firing at motion. © 2020, O. Fomin, A. Lovska, V. Kudelya, I. Smyrnova.},
author_keywords={Bearing structure;  Covered wagon;  Dynamic load;  Modal analysis;  Structural strength;  Transport mechanics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Katsupeev202044,
author={Katsupeev, A. and Aleksanyan, G. and Kombarova, E. and Polyakov, R.},
title={Study of graphics libraries related to the problem of visualization of electrical impedance tomography images},
journal={Eastern-European Journal of Enterprise Technologies},
year={2020},
volume={4},
number={2-106},
pages={44-54},
doi={10.15587/1729-4061.2020.210523},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090897662&doi=10.15587%2f1729-4061.2020.210523&partnerID=40&md5=b3e847e947158f0255bbe69534880817},
abstract={The structure of the software for electric impedance tomography has been presented. This structure of application construction makes it possible to carry out the real-time EIT research and can be implemented on medical and technical devices, in particular, integrated into AVL devices. The algorithm for visualization of the results of conductivity field reconstruction was presented. Within this algorithm, there are two approaches to presenting color models and selecting colors for each particular finite element. The choice of one of these approaches depends on the needs of the study and leads either to faster performance, or to the higher quality of an image. The algorithm of neighboring finite elements, allowing reducing the time consumed to visualize the model by uniting neighboring elements with a similar color in one polygon, has been proposed. Reducing the number of finite elements leads to a higher speed of their output on the screen. A list of graphics libraries that can be used for the problems of visualization of the results of electric impedance tomography was presented. As a result of the research, it was found that among the analyzed libraries, the best time is demonstrated by the OpenGL library, which ensures the visualization of 0.02 s faster than in the case of the analogs. This is due to the high operation speed, which is provided by the implementation of the GPU visualization. It was shown that the use of the proposed algorithm of neighboring finite elements actually allows reducing the time spent on displaying the model on the screen from 0.05 s to 0.03 s for the OpenGL library. At the same time, the total time spent on visualization depends on the used graphics library. The obtained data can be used in the development of medical visualization systems, which should meet increased requirements in terms of the amount of displayed information.},
author_keywords={Electric impedance tomography;  Graphics libraries;  Image reconstruction;  Medical visualization;  Software},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Morandi2020117,
author={Morandi, F. and Caniato, M. and Robin, O. and Barbaresi, L. and Gasparella, A. and Masson, P. and Atalla, N.},
title={Modelling the sound insulation of mass timber floors using the finite transfer matrix method},
journal={Building Simulation Applications},
year={2020},
volume={2020-June},
pages={117-122},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090838038&partnerID=40&md5=e7df49c7891490022c60f741d77b89f1},
abstract={Cross Laminated Timber (CLT) technology has revolutionized the use of timber in construction in just 20 years. However, the development of mid- and high-rise CLT buildings has raised concerns about the sound insulation provided by these structural elements and about the reliability of simulation tools and models which are currently used. The mechanical characteristics of mass timber elements do not allow simplifications such as infinite out-of-plane stiffness, diffuseness of the vibrational field and perfectly elastic behavior upon impact, to mention a few, which are commonly assumed for traditional structures. The availability of modelling/simulation tools (and the relative input data) that provide accurate predictions of airborne and structure-borne sound insulation is therefore a current and relevant topic. This work presents an investigation into the input parameters to use for modelling CLT elements using the Finite Transfer Matrix Method (FTMM). The results of laboratory measurements on two CLT floors are compared to results obtained using two FTMM-based software packages in a three-step procedure. First, measurements were performed on two timber floor solutions. Following this, two operators working with different FTMM-based software packages performed blind simulations, based upon the information shared on the materials' characteristics. Finally, the input data were modified in order to return the best fit to the experimental data. The aim of the work is twofold: (1) to verify the degree of accuracy of the software and (2) following a reverse-engineering process, to retrieve the properties of the materials that need to be modelled through equivalent physical dimensions. The results for the bare CLT floor show that using dynamic E value for the plate modelling returns slightly more accurate results. Conversely, the question of modelling of a complete floor, including a floating floor, deserves greater attention, as modelling the resilient underlay using static values of dynamic stiffness can alter the results to a great extent. © 2020 Free University of Bozen Bolzano. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sotiropoulos2020441,
author={Sotiropoulos, S. and Kazakis, G. and Lagaros, N.D.},
title={High performance topology optimization computing platform},
journal={Procedia Manufacturing},
year={2020},
volume={44},
pages={441-448},
doi={10.1016/j.promfg.2020.02.272},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088024954&doi=10.1016%2fj.promfg.2020.02.272&partnerID=40&md5=0dd52c8da66d4739a2a767c5c1a9a54e},
abstract={One of the most challenging tasks in the construction industry nowadays, is to reduce the material demands and distribute, in the same time, the material among the structural system in the best possible way. Topology optimization is a design procedure that is increasingly used, to generate optimized forms of structures in several engineering fields. The current paper presents the Topology Optimization (TO) module of the High-Performance Optimization Computing Platform (HP-OCP) which focuses on civil engineering problems. More specifically the SIMP method [1] is implemented and the topology optimization problem is solved by using the OC algorithm. The HP-OCP is a platform which evaluates several objective functions, such as the volume of the structure, the compliance etc. and can solve constrained or unconstrained structural optimization problems. The above libraries are developed in C#. The core of the platform is created in such way that it can be integrated with any CAE program that has OAPI, XML or any other type of data exchange format. In the proposed work the structural analysis and design software SAP2000 is used. Theoretical aspects are discussed in order to implement the mathematical formulation in a commercial software. Basic and specific features are applied and representative examples are performed. One of the highlights of the proposed work is that the above module can be used for all kind of finite elements. Benchmark tests are presented with structures that are simulated by 2D plane-stress elements, 3D-solid elements and shell elements. Furthermore, it is independent of the type of the mesh, structured or unstructured, so both examples are presented. In the proposed work a powerful tool for both architects and civil engineers is introduced. The analysis and design of the structures are performed in SAP2000 software, in order to achieve a realistic result that could be a solution for a real-world structure. © 2020 The Authors.},
author_keywords={C#;  HP-OCP;  SAP2000;  Topology optimization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rabbi202066,
author={Rabbi, F. and Daley, C.S. and Aktulga, H.M. and Wright, N.J.},
title={Evaluation of Directive-Based GPU Programming Models on a Block Eigensolver with Consideration of Large Sparse Matrices},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12017 LNCS},
pages={66-88},
doi={10.1007/978-3-030-49943-3_4},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087789213&doi=10.1007%2f978-3-030-49943-3_4&partnerID=40&md5=914b282d7dbe2b65317144005e646a6b},
abstract={Achieving high performance and performance portability for large-scale scientific applications is a major challenge on heterogeneous computing systems such as many-core CPUs and accelerators like GPUs. In this work, we implement a widely used block eigensolver, Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG), using two popular directive based programming models (OpenMP and OpenACC) for GPU-accelerated systems. Our work differs from existing work in that it adopts a holistic approach that optimizes the full solver performance rather than narrowing the problem into small kernels (e.g., SpMM, SpMV). Our LOPBCG GPU implementation achieves a 2.8$${\times }$$–4.3$${\times }$$ speedup over an optimized CPU implementation when tested with four different input matrices. The evaluated configuration compared one Skylake CPU to one Skylake CPU and one NVIDIA V100 GPU. Our OpenMP and OpenACC LOBPCG GPU implementations gave nearly identical performance. We also consider how to create an efficient LOBPCG solver that can solve problems larger than GPU memory capacity. To this end, we create microbenchmarks representing the two dominant kernels (inner product and SpMM kernel) in LOBPCG and then evaluate performance when using two different programming approaches: tiling the kernels, and using Unified Memory with the original kernels. Our tiled SpMM implementation achieves a 2.9$${\times }$$ and 48.2$${\times }$$ speedup over the Unified Memory implementation on supercomputers with PCIe Gen3 and NVLink 2.0 CPU to GPU interconnects, respectively. © 2020, Springer Nature Switzerland AG.},
author_keywords={Directive based programming models;  OpenACC;  OpenMP 4.5;  Performance optimization;  Performance portability;  Sparse solvers},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang2020169,
author={Yang, J. and Kim, J. and Hoseinzadeh, M. and Izraelevitz, J. and Swanson, S.},
title={An empirical guide to the behavior and use of scalable persistent memory},
journal={Proceedings of the 18th USENIX Conference on File and Storage Technologies, FAST 2020},
year={2020},
pages={169-182},
note={cited By 198},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086282913&partnerID=40&md5=d819a98a1a1843ef5028e26244880896},
abstract={After nearly a decade of anticipation, scalable nonvolatile memory DIMMs are finally commercially available with the release of Intel's Optane DIMM. This new nonvolatile DIMM supports byte-granularity accesses with access times on the order of DRAM, while also providing data storage that survives power outages. Researchers have not idly waited for real nonvolatile DIMMs (NVDIMMs) to arrive. Over the past decade, they have written a slew of papers proposing new programming models, file systems, libraries, and applications built to exploit the performance and flexibility that NVDIMMs promised to deliver. Those papers drew conclusions and made design decisions without detailed knowledge of how real NVDIMMs would behave or how industry would integrate them into computer architectures. Now that Optane NVDIMMs are actually here, we can provide detailed performance numbers, concrete guidance for programmers on these systems, reevaluate prior art for performance, and reoptimize persistent memory software for the real Optane DIMM. In this paper, we explore the performance properties and characteristics of Intel's new Optane DIMM at the micro and macro level. First, we investigate the basic characteristics of the device, taking special note of the particular ways in which its performance is peculiar relative to traditional DRAM or other past methods used to emulate NVM. From these observations, we recommend a set of best practices to maximize the performance of the device. With our improved understanding, we then explore and reoptimize the performance of prior art in application-level software for persistent memory. Copyright © Proc. of the 18th USENIX Conference on File and Storage Tech., FAST 2020. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Votipka2020109,
author={Votipka, D. and Fulton, K.R. and Parker, J. and Hou, M. and Mazurek, M.L. and Hicks, M.},
title={Understanding security mistakes developers make: Qualitative analysis from build it, break it, fix it},
journal={Proceedings of the 29th USENIX Security Symposium},
year={2020},
pages={109-126},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085605852&partnerID=40&md5=75c64a2231ef23d34dea610413af91fb},
abstract={Secure software development is a challenging task requiring consideration of many possible threats and mitigations. This paper investigates how and why programmers, despite a baseline of security experience, make security-relevant errors. To do this, we conducted an in-depth analysis of 94 submissions to a secure-programming contest designed to mimic real-world constraints: correctness, performance, and security. In addition to writing secure code, participants were asked to search for vulnerabilities in other teams' programs; in total, teams submitted 866 exploits against the submissions we considered. Over an intensive six-month period, we used iterative open coding to manually, but systematically, characterize each submitted project and vulnerability (including vulnerabilities we identified ourselves). We labeled vulnerabilities by type, attacker control allowed, and ease of exploitation, and projects according to security implementation strategy. Several patterns emerged. For example, simple mistakes were least common: only 21% of projects introduced such an error. Conversely, vulnerabilities arising from a misunderstanding of security concepts were significantly more common, appearing in 78% of projects. Our results have implications for improving secure-programming APIs, API documentation, vulnerability-finding tools, and security education. © 2020 by The USENIX Association. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xia2020,
author={Xia, L.-Y. and Zakowski, Y. and He, P. and Hur, C.-K. and Malecha, G. and Pierce, B.C. and Zdancewic, S.},
title={Interaction trees: Representing recursive and impure programs in Coq},
journal={Proceedings of the ACM on Programming Languages},
year={2020},
volume={4},
number={POPL},
doi={10.1145/3371119},
art_number={51},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083995421&doi=10.1145%2f3371119&partnerID=40&md5=bb0242c64c72b9290cef0e6c7fe52b40},
abstract={Interaction trees (ITrees) are a general-purpose data structure for representing the behaviors of recursive programs that interact with their environments. A coinductive variant of "free monads," ITrees are built out of uninterpreted events and their continuations. They support compositional construction of interpreters from event handlers, which give meaning to events by defining their semantics as monadic actions. ITrees are expressive enough to represent impure and potentially nonterminating, mutually recursive computations, while admitting a rich equational theory of equivalence up to weak bisimulation. In contrast to other approaches such as relationally specified operational semantics, ITrees are executable via code extraction, making them suitable for debugging, testing, and implementing software artifacts that are amenable to formal verification. We have implemented ITrees and their associated theory as a Coq library, mechanizing classic domain- and category-theoretic results about program semantics, iteration, monadic structures, and equational reasoning. Although the internals of the library rely heavily on coinductive proofs, the interface hides these details so that clients can use and reason about ITrees without explicit use of Coq's coinduction tactics. To showcase the utility of our theory, we prove the termination-sensitive correctness of a compiler from a simple imperative source language to an assembly-like target whose meanings are given in an ITree-based denotational semantics. Unlike previous results using operational techniques, our bisimulation proof follows straightforwardly by structural induction and elementary rewriting via an equational theory of combinators for control-flow graphs. © 2020 Copyright held by the owner/author(s).},
author_keywords={Coinduction;  Compiler correctness;  Coq;  Monads},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sheidani2020,
author={Sheidani, S. and Fazlali, M. and Eslami, Z.},
title={Accelerating Robust Watermarking through Parallelization},
journal={2020 25th International Computer Conference, Computer Society of Iran, CSICC 2020},
year={2020},
doi={10.1109/CSICC49403.2020.9050124},
art_number={9050124},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083396977&doi=10.1109%2fCSICC49403.2020.9050124&partnerID=40&md5=12620c7a87ab39e12d5ddd52a6dec4f9},
abstract={The rapid online usage of digital images has pushed us to accelerate the watermarking method to protect it. Lately, in order to accelerate the applied science and engineering methods multicore architectures are used more and more. Therefore, at this research we have tried to increase the speed of watermarking method on GPU by CUDA. This acceleration by CUDA allows rapid embedding and extraction of the watermarks. For the sake of the fact that better methods have more computational cost, parallelization leads up to the use of a better watermarking method which has a better performance in terms of imperceptibility, robustness, and security. It assists to the rapid secure use of digital images, as well. The results show that the speeds of CUDA implementations are superior to multi cores which have been implemented by OpenMP, although, when the images are big enough OpenMP implementations show a slightly better performance than sequential implementations. The proposed method is evaluated on an NVIDIA 940MX GPU which can reduce the execution time by 50% in comparison with sequential implementations. © 2020 IEEE.},
author_keywords={Manycore system;  Robust watermarking;  Watermarking acceleration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hernandez2020211,
author={Hernandez, H.G.M. and Mahmood, S. and Brandalero, M. and Hübner, M.},
title={A Modular Software Library for Effective High Level Synthesis of Convolutional Neural Networks},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12083 LNCS},
pages={211-220},
doi={10.1007/978-3-030-44534-8_16},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083029212&doi=10.1007%2f978-3-030-44534-8_16&partnerID=40&md5=f054bb22876fcb04cbb5f8f9cf0f0fce},
abstract={Convolutional Neural Networks (CNNs) have applications in many valuable domains such as object detection for autonomous cars and security using facial recognition. This vast field of application usually places strict non-functional requirements such as resource-efficient implementations on the hardware devices, while at the same time requiring flexibility. In response, this work presents a C++-based software library of reusable modules to build arbitrary CNNs that support High-Level-Synthesis to be implemented as FPGA hardware accelerators for the inference process. Our work demonstrates how parametrization and modularization of basic building blocks of a CNN enable easier customization of the hardware to match the software model. This project also works with low-precision parameters throughout the CNN to provide a more resource-efficient implementation. © 2020, Springer Nature Switzerland AG.},
author_keywords={C library;  Convolutional Neural Networks;  FPGA;  High Level Synthesis;  HW acceleration;  Inference acceleration;  Library of components;  Machine learning;  Modular approach},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ilhan2020125,
author={Ilhan, B. and Kog, F.},
title={BIM and sustainability integration: Multi-agent system approach},
journal={Communications in Computer and Information Science},
year={2020},
volume={1188 CCIS},
pages={125-136},
doi={10.1007/978-3-030-42852-5_10},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082474972&doi=10.1007%2f978-3-030-42852-5_10&partnerID=40&md5=99a2475ac9687d1f99cea6fbe3a202ea},
abstract={The Architecture, Engineering, Construction (AEC) and Facility Management (FM) industry is under pressure to move towards technological innovations as well as sustainability. New technologies are of great importance to sustainable design and construction processes by providing the means to act efficiently. Building Information Modeling (BIM) as the state-of-the-art technology in the industry provides a digital representation of physical and functional characteristics and information resources of a facility by forming a reliable basis to manage the processes and decisions during its entire life cycle. Despite the opportunities and potential advantages of BIM in sustainability for certification and decision-making, there are still challenges of integrated and collaborated systems. This paper addresses a more flexible and dynamic object-oriented approach for successful sustainable project deliveries. The aim is to provide a guideline through the development of an automated solution based on industry foundation classes (IFC) and a multi-agent system (MAS). The proposed approach comprises of three main phases, which are (1) development of sustainability assessment database and library, and extension of the criteria concerning the sustainability assessment methods into BIM software via property sets, (2) generation of the BIM model and its conversion to ifcXML and XML file formats and (3) evaluation of the data and presentation of the potential alternatives by extracting the related data from the converted file. It will support effective decision-making throughout the complete building life cycle by offering the possible criteria selection according to the user-provided project properties and certification level target. © Springer Nature Switzerland AG 2020.},
author_keywords={BIM;  IFC;  Integration;  MAS;  Sustainability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhu202042674,
author={Zhu, Y. and Liu, Y. and Zhang, G.},
title={FT-PBLAS: PBLAS-Based Fault-Tolerant Linear Algebra Computation on High-performance Computing Systems},
journal={IEEE Access},
year={2020},
volume={8},
pages={42674-42688},
doi={10.1109/ACCESS.2020.2975832},
art_number={9007454},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081550338&doi=10.1109%2fACCESS.2020.2975832&partnerID=40&md5=69d2b80f1c34bfdd0b88fec9ba1cb1b1},
abstract={As high-performance computing (HPC) systems have scaled up, resilience has become a great challenge. To guarantee resilience, various kinds of hardware and software techniques have been proposed. However, among popular software fault-tolerant techniques, both the checkpoint-restart approach and the replication technique face challenges of scalability in the era of peta-and exa-scale systems due to their numerous processes. In this situation, algorithm-based approaches, or algorithm-based fault tolerance (ABFT) mechanisms, have become attractive because they are efficient and lightweight. Although the ABFT technique is algorithm-dependent, it is possible to implement it at a low level (e.g., in libraries for basic numerical algorithms) and make it application-independent. However, previous ABFT approaches have mainly aimed at achieving fault tolerance in integrated circuits (ICs) or at the architecture level and are therefore not suitable for HPC systems; e.g., they use checksums of rows and columns of matrices rather than checksums of blocks to detect errors. Furthermore, they cannot deal with errors caused by node failure, which are common in current HPC systems. To solve these problems, this paper proposes FT-PBLAS, a PBLAS-based library for fault-tolerant parallel linear algebra computations that can be regarded as a fault-tolerant version of the parallel basic linear algebra subprograms (PBLAS), because it provides a series of fault-tolerant versions of interfaces in PBLAS. To support the underlying error detection and recovery mechanisms in the library, we propose a block-checksum approach for non-fatal errors and a scheme for addressing node failure, respectively. We evaluate two fault-tolerant mechanisms and FT-PBLAS on HPC systems, and the experimental results demonstrate the performance of our library. © 2013 IEEE.},
author_keywords={Algorithm-based fault tolerance;  HPC systems;  linear algebra computations;  matrix multiplication;  node failure},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lukashin2020573,
author={Lukashin, P.S. and Melnikova, V.G. and Shcheglov, G.A. and Strijhak, S.V.},
title={Using open source software for solving aeroelasticity case for wind turbine blade},
journal={Proceedings of the 6th European Conference on Computational Mechanics: Solids, Structures and Coupled Problems, ECCM 2018 and 7th European Conference on Computational Fluid Dynamics, ECFD 2018},
year={2020},
pages={573-584},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081066418&partnerID=40&md5=28da40c89338ad795d31ae5216f58fa7},
abstract={Due to the development of Wind Energy and construction of new wind farms in the Russian Federation and Europe there is a need for solution of application-oriented problems and development of effective methods for numerical modelling of wind turbine's elements. One of the directions for computational continuous mechanics is connected with problems in aeroelasticity (fluid-structure interaction). The possibility of solving one of the problem in aeroelasticity using a complex program approach on the basis of open source software OpenFOAM and Code_Aster is shown in this article. On the example of the blade for wind turbine, which has 61.5 meters long, the techniques of solving problem for a static and dynamic aeroelasticity in which flow simulation of the blade with a subsonic air flow is done in OpenFOAM library (solvers simpleFoam and pimpleFoam) are considered. The calculation of the stress-strain state of the blade is done in Code_Aster. The flowcharts for four different approaches for solving problems of aeroelasticity are provided in article. The finite-volume mesh consisting hexahedral elements, the total number is about 400000 elements, for simulation of the flow around the blade is created in OpenFOAM library, the finite-element mesh consisting of triangular shell elements of first order, the total number is 7714, for calculation of the stress-strain state is created in Salome-Meca. The results of analysis are provided in the form of pressure and velocities fields; projections of aerodynamic force from time; displacement and stress diagrams; the values of pressure for two points on the surface of the blade and displacement of the tip of the blade from time. The calculations are run using resources of UniHUB web-laboratory ISPRAS. Copyright © Crown copyright (2018).All right reserved.},
author_keywords={Aeroelasticity;  Code_Aster;  DES turbulence model;  Fluid-structure interaction;  OpenFOAM;  Wind turbine blade},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xu2020755,
author={Xu, B. and An, L. and Thung, F. and Khomh, F. and Lo, D.},
title={Why reinventing the wheels? An empirical study on library reuse and re-implementation},
journal={Empirical Software Engineering},
year={2020},
volume={25},
number={1},
pages={755-789},
doi={10.1007/s10664-019-09771-0},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073805876&doi=10.1007%2fs10664-019-09771-0&partnerID=40&md5=46b56370fa5f74150b855f28211d59e4},
abstract={Nowadays, with the rapid growth of open source software (OSS), library reuse becomes more and more popular since a large amount of third- party libraries are available to download and reuse. A deeper understanding on why developers reuse a library (i.e., replacing self-implemented code with an external library) or re-implement a library (i.e., replacing an imported external library with self-implemented code) could help researchers better understand the factors that developers are concerned with when reusing code. This understanding can then be used to improve existing libraries and API recommendation tools for researchers and practitioners by using the developers concerns identified in this study as design criteria. In this work, we investigated the reasons behind library reuse and re-implementation. To achieve this goal, we first crawled data from two popular sources, F-Droid and GitHub. Then, potential instances of library reuse and re-implementation were found automatically based on certain heuristics. Next, for each instance, we further manually identified whether it is valid or not. For library re-implementation, we obtained 82 instances which are distributed in 75 repositories. We then conducted two types of surveys (i.e., individual survey to corresponding developers of the validated instances and another open survey) for library reuse and re-implementation. For library reuse individual survey, we received 36 responses out of 139 contacted developers. For re-implementation individual survey, we received 13 responses out of 71 contacted developers. In addition, we received 56 responses from the open survey. Finally, we perform qualitative and quantitative analysis on the survey responses and commit logs of the validated instances. The results suggest that library reuse occurs mainly because developers were initially unaware of the library or the library had not been introduced. Re-implementation occurs mainly because the used library method is only a small part of the library, the library dependencies are too complicated, or the library method is deprecated. Finally, based on all findings obtained from analyzing the surveys and commit messages, we provided a few suggestions to improve the current library recommendation systems: tailored recommendation according to users’ preferences, detection of external code that is similar to a part of the users’ code (to avoid duplication or re-implementation), grouping similar recommendations for developers to compare and select the one they prefer, and disrecommendation of poor-quality libraries. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Code re-implementation;  Code reuse;  Library recommendation systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Procopio2020245,
author={Procopio, G.},
title={Safety and Security in GNU/Linux Real Time Operating System Domain},
journal={Advances in Intelligent Systems and Computing},
year={2020},
volume={925},
pages={245-254},
doi={10.1007/978-3-030-14687-0_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064184196&doi=10.1007%2f978-3-030-14687-0_22&partnerID=40&md5=62257308b4c75ea7b3d0a0fa72c8a2a2},
abstract={Historically, because of regulatory constraints systems focused only on hardware and software safety, and considered security independently as an add-on, if anything required. But, it is widely recognized safety-critical systems today are quite certainly security-critical too, as well as safety and security functions may influence each other. It also happens that the system’s usage context is not completely known nor understood at development time which means system maintenance will not just limited to bug-fixes and will involve continuous hazard analysis. Especially in Avionic and Automotive sectors, the growing awareness of conjoint safety and security pushed the research new paradigms for design, development, verification and validation, and the promotion of holistic methods and techniques for conducting safety and security co-engineering, co-assessment and certification/qualification. Finally, there is not a standard that provides conjoint guidelines for the safety and security domains so that compliance to multiple standards is currently the used approach. This paper focuses briefly on the safety and security real-time operating systems, the architecture they are required to provide for addressing safety and security, and the applicable standards. It then highlights how a real-time GNU/Linux OS can be included in a formal certification package as demanded by SIL2 applications that meet the IEC 61508 requirements, and how such operating system should be improved for use into interconnected systems. © 2020, Springer Nature Switzerland AG.},
author_keywords={Co-engineering;  GNU/Linux;  Safety;  Security},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hao201943,
author={Hao, G. and Yongsheng, R. and Zhangtao, X.},
title={Design and implementation of web-based dynamic geometry drawing technology},
journal={Journal of China Universities of Posts and Telecommunications},
year={2019},
volume={26},
number={6},
pages={43-53},
doi={10.19682/j.cnki.1005.8885.2019.1025},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086479255&doi=10.19682%2fj.cnki.1005.8885.2019.1025&partnerID=40&md5=7088329279cb66acedc7180b8c765921},
abstract={Dynamic geometry software, as a piece of computer-assisted instruction (CAI) software, is closely and deeply associated with mathematics, and is widely applied to mathematics teaching activities in primary and secondary schools. Meanwhile, web technology also has become an important technology for assisting education and teaching. This paper expounds a web-based dynamic geometry software development process, and analyses specific requirements regarding graphical application programming interface (API) required by dynamic geometry software. With experiments and comparison on the two different hypertext markup language (HTML) 5 graphical API technologies, i. e., scalable vector graphics (SVG) and Canvas, on different apparatuses and browsers, we draw the conclusion that it is more suitable to adopt Canvas as the graphical API technology for the web-based dynamic geometry software, thus further proposed the principles and methods for an object-oriented Canvas design. The dynamic geometry software based on the newly-designed Canvas has technical advantages and educational value, well incorporating aesthetic education into mathematics education. © 2019, Beijing University of Posts and Telecommunications. All rights reserved.},
author_keywords={Canvas;  Dynamic geometry;  Subject tools;  Web},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lima2019,
author={Lima, C. and Assunção, W.K. and Martinez, J. and Mendonça, W. and Machado, I.C. and Chavez, C.F.},
title={Product line architecture recovery with outlier filtering in software families: the Apo-Games case study},
journal={Journal of the Brazilian Computer Society},
year={2019},
volume={25},
number={1},
doi={10.1186/s13173-019-0088-4},
art_number={7},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067872359&doi=10.1186%2fs13173-019-0088-4&partnerID=40&md5=4497f8c29c854213aabc104f3b10b6b6},
abstract={Software product line (SPL) approach has been widely adopted to achieve systematic reuse in families of software products. Despite its benefits, developing an SPL from scratch requires high up-front investment. Because of that, organizations commonly create product variants with opportunistic reuse approaches (e.g., copy-and-paste or clone-and-own). However, maintenance and evolution of a large number of product variants is a challenging task. In this context, a family of products developed opportunistically is a good starting point to adopt SPLs, known as extractive approach for SPL adoption. One of the initial phases of the extractive approach is the recovery and definition of a product line architecture (PLA) based on existing software variants, to support variant derivation and also to allow the customization according to customers’ needs. The problem of defining a PLA from existing system variants is that some variants can become highly unrelated to their predecessors, known as outlier variants. The inclusion of outlier variants in the PLA recovery leads to additional effort and noise in the common structure and complicates architectural decisions. In this work, we present an automatic approach to identify and filter outlier variants during the recovery and definition of PLAs. Our approach identifies the minimum subset of cross-product architectural information for an effective PLA recovery. To evaluate our approach, we focus on real-world variants of the Apo-Games family. We recover a PLA taking as input 34 Apo-Game variants developed by using opportunistic reuse. The results provided evidence that our automatic approach is able to identify and filter outlier variants, allowing to eliminate exclusive packages and classes without removing the whole variant. We consider that the recovered PLA can help domain experts to take informed decisions to support SPL adoption. © 2019, The Author(s).},
author_keywords={Product line architecture;  Product line architecture recovery;  Software product lines;  Variability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pritchard20194814,
author={Pritchard, B.P. and Altarawy, D. and Didier, B. and Gibson, T.D. and Windus, T.L.},
title={New Basis Set Exchange: An Open, Up-to-Date Resource for the Molecular Sciences Community},
journal={Journal of Chemical Information and Modeling},
year={2019},
volume={59},
number={11},
pages={4814-4820},
doi={10.1021/acs.jcim.9b00725},
note={cited By 819},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074260190&doi=10.1021%2facs.jcim.9b00725&partnerID=40&md5=870db70034f5dd302d17892b8324441d},
abstract={The Basis Set Exchange (BSE) has been a prominent fixture in the quantum chemistry community. First publicly available in 2007, it is recognized by both users and basis set creators as the de facto source for information related to basis sets. This popular resource has been rewritten, utilizing modern software design and best practices. The basis set data has been separated into a stand-alone library with an accessible API, and the Web site has been updated to use the current generation of web development libraries. The general layout and workflow of the Web site is preserved, while helpful features requested by the user community have been added. Overall, this design should increase adaptability and lend itself well into the future as a dependable resource for the computational chemistry community. This article will discuss the decision to rewrite the BSE, the new architecture and design, and the new features that have been added. Copyright © 2019 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Neuman20195235,
author={Neuman, S.M. and Koolen, T. and Drean, J. and Miller, J.E. and Devadas, S.},
title={Benchmarking and Workload Analysis of Robot Dynamics Algorithms},
journal={IEEE International Conference on Intelligent Robots and Systems},
year={2019},
pages={5235-5242},
doi={10.1109/IROS40897.2019.8967694},
art_number={8967694},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081158994&doi=10.1109%2fIROS40897.2019.8967694&partnerID=40&md5=f964a750acbd90cc5d63ad970d960ac9},
abstract={Rigid body dynamics calculations are needed for many tasks in robotics, including online control. While there currently exist several competing software implementations that are sufficient for use in traditional control approaches, emerging sophisticated motion control techniques such as nonlinear model predictive control demand orders of magnitude more frequent dynamics calculations. Current software solutions are not fast enough to meet that demand for complex robots. The goal of this work is to examine the performance of current dynamics software libraries in detail. In this paper, we (i) survey current state-of-the-art software implementations of the key rigid body dynamics algorithms (RBDL, Pinocchio, RigidBodyDynamics.jl, and RobCoGen), (ii) establish a methodology for benchmarking these algorithms, and (iii) characterize their performance through real measurements taken on a modern hardware platform. With this analysis, we aim to provide direction for future improvements that will need to be made to enable emerging techniques for real-time robot motion control. To this end, we are also releasing our suite of benchmarks to enable others to help contribute to this important task. © 2019 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Soto-Hidalgo2019204,
author={Soto-Hidalgo, J.M. and Vitiello, A. and Alonso, J.M. and Acampora, G. and Alcala-Fdez, J.},
title={Design of fuzzy controllers for embedded systems with JFML},
journal={International Journal of Computational Intelligence Systems},
year={2019},
volume={12},
number={1},
pages={204-214},
doi={10.2991/ijcis.2018.125905646},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065069962&doi=10.2991%2fijcis.2018.125905646&partnerID=40&md5=2dd99843a243b5487dfdcfca4660ee45},
abstract={Fuzzy rule-based systems (FRBSs) have been successfully applied to a wide range of real-world problems. However, they suffer from some design issues related to the difficulty to implement them on different hardware platforms without additional efforts. To bridge this gap, recently, the IEEE Computational Intelligence Society has sponsored the publication of the standard IEEE Std 1855-2016 which is aimed at providing the fuzzy community with a well-defined approach to model FRBSs in a hardware-independent way. In order to provide a runnable version of an FRBS that is designed in accordance with the IEEE Std 1855-2016, the open source library Java Fuzzy Markup Language (JFML) has been developed. However, due to hardware and/or software limitations of embedded systems, it is not always possible to run an IEEE Std 1855-2016 FRBS on this kind of systems. The aim of this paper is to overcome this drawback by developing a new JFML module that assists developers in the design and implementation of FRBSs for open hardware–embedded systems. In detail, the module supports several connection types (WiFi, Bluetooth, and USB) in order to make feasible running FRBSs in a remote computer when, due to hardware limitations, it is not possible that they run locally in the embedded systems. The new JFML module is ready for Arduino™ and Raspberry Pi, but it can be easily extended to other hardware architectures. Moreover, the new JFML module allows to automatically generate runnable files on Arduino™ or Raspberry Pi in order to support nonexpert users, that is, users without specific knowledge about embedded systems or without strong programming skills. The use of the new JFML module is illustrated in two case studies. © 2019 The Authors. Published by Atlantis Press SARL.},
author_keywords={Embedded systems;  Fuzzy Rule-Based Systems;  IEEE Std 1855-2016;  JFML;  Open hardware;  Open source software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lin201915023,
author={Lin, W.},
title={Analysis on the effect of assignment system of track and field web course based on software programming method},
journal={Cluster Computing},
year={2019},
volume={22},
pages={15023-15036},
doi={10.1007/s10586-018-2494-3},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044087491&doi=10.1007%2fs10586-018-2494-3&partnerID=40&md5=bc7f196acf8a885c4a9d54e63ae4d431},
abstract={The paper was to improve the assignment module’s construction and application effect of the track and field (T&F) web course in physical education schools. Literature study, expert interview, system design method, software programming method, questionnaire survey. First, the editing and feedback of the assignment module are strongly practical and the effects of which are very obvious; Second, the content and form of the assignments are various, which initiate the extending thinking of the students; Third, the structure of assignment module helps with the assigning, submitting, prompting and correcting and the comprehensive statistics of the assignments and it is easy to operate with its multi-function and it also reflects the teaching progress subjectively. This research is applicable to the web-based assignment pattern of track and field teaching for physical education majors (selective course, required course, minor course), the regular maintenance and complimentary administration of assignment library resources shall be further optimized; the editing and update of assignments shall be adjusted and arranged timely according to the teaching practice problems. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Application analysis;  Assignment system;  Physical education school;  Track and field;  Web course},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ghosal201978,
author={Ghosal, R. and Rana, B. and Kapur, I. and Parnami, A.},
title={Rapid prototyping of pneumatically actuated inflatable structures},
journal={UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
year={2019},
pages={78-80},
doi={10.1145/3332167.3357121},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074820240&doi=10.1145%2f3332167.3357121&partnerID=40&md5=5c572ebe0a8f3fffe7441598296960ed},
abstract={Fabricating and actuating inflatables for shape-changing interfaces and soft robotics is challenging and time-consuming, requiring knowledge in diverse domains such as pneumatics, manufacturing processes for elastomers, and embedded systems. We propose in this poster a scheme for rapid prototyping and pneumatically actuating piecewise multi-chambered inflatables, using balloons as our building blocks. We provide a construction kit containing pneumatic control boards, pneumatic components, and balloons for constructing simple actuated balloon models. We also provide various primitives of actuation and locomotion to help the user put together their desired actuator, along with an Android app and software API for controlling it via Bluetooth. Finally, we demonstrate the construction and actuation of these inflatable structures using three sample applications. © 2019 Copyright held by the owner/author(s).},
author_keywords={Inflatables;  Pneumatic system;  Shape changing interfaces;  Soft actuator;  Soft robotics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gui2019708,
author={Gui, Y. and Liu, H. and Yin, H. and Li, Z. and Guo, N.},
title={Design and implementation of hex computer game platform},
journal={Proceedings - 2019 IEEE International Conferences on Ubiquitous Computing and Communications and Data Science and Computational Intelligence and Smart Computing, Networking and Services, IUCC/DSCI/SmartCNS 2019},
year={2019},
pages={708-711},
doi={10.1109/IUCC/DSCI/SmartCNS.2019.00146},
art_number={8982629},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081102792&doi=10.1109%2fIUCC%2fDSCI%2fSmartCNS.2019.00146&partnerID=40&md5=f66f2e41d9448ac4b5f68a888a0e0245},
abstract={In order to improve the development efficiency of Hex game computer game program, the program which can load different game engines and carry out human-machine competitiveas a test platform in the development process is designed and realized. Through pipeline communication to realize the interaction between the platform and the game program, design the Hex game referee terminal logic. This paper introduces the function of the program, designs the goal, and lists the treatment of the key problems in the program. The practical application proves that the game program runs fast and stable, and the design and realization of the man-machine game program has certain reference value to complete other game computer game programs. © 2019 IEEE.},
author_keywords={Computer games;  Game platform;  Hex Game;  Tinker Graphics Library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suleri2019,
author={Suleri, S. and Jarke, M. and Kipi, N. and Tran, L.C.},
title={UI design pattern-driven rapid prototyping for agile development of mobile applications},
journal={Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2019},
year={2019},
doi={10.1145/3338286.3344399},
art_number={a52},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073532827&doi=10.1145%2f3338286.3344399&partnerID=40&md5=37af69fc35577ea8fbc3967ba8ae4fc0},
abstract={In agile development, lean UX designers perform rapid prototyping and quick evaluation of prototypes to ensure fast releases. To understand designers' workflow during rapid prototyping, we interviewed 15 lean UX designers. We identified the following pain points in the workflow: 1) Compromise on quality of UI design due to time constraint 2) UI design knowledge being scattered among numerous sources such as websites and books 3) Inability of developers to reproduce the same quality of UI design due to lack of UI design knowledge. To address these issues, we propose a UI design pattern-driven approach for rapid prototyping. To realize this approach, we introduce Kiwi, a library for UI design patterns and guidelines that aims to consolidate UI design knowledge for mobile applications. Each UI design pattern consists of a problem statement, context, rationale, and a proposed solution. Additionally, Kiwi provides downloadable and customizable GUI examples, layout blueprints and front-end code for each pattern. Usability evaluation (SUS) of Kiwi with 21 lean UX designers depict good usability and high learnability. © 2019 Association for Computing Machinery.},
author_keywords={Agile development;  Lean UX;  Pattern library;  Rapid prototyping;  UI design guidelines;  UI design pattern;  User interface design},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Maher20192319,
author={Maher, G. and Wilson, N. and Marsden, A.},
title={Accelerating cardiovascular model building with convolutional neural networks},
journal={Medical and Biological Engineering and Computing},
year={2019},
volume={57},
number={10},
pages={2319-2335},
doi={10.1007/s11517-019-02029-3},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071414029&doi=10.1007%2fs11517-019-02029-3&partnerID=40&md5=96a5a58295fe89601fe5292d3b68e01f},
abstract={The objective of this work is to reduce the user effort required for 2D segmentation when building patient-specific cardiovascular models using the SimVascular cardiovascular modeling software package. The proposed method uses a fully convolutional neural network (FCNN) to generate 2D cardiovascular segmentations. Given vessel pathlines, the neural network generates 2D vessel enhancement images along the pathlines. Thereafter, vessel segmentations are extracted using the marching-squares algorithm, which are then used to construct 3D cardiovascular models. The neural network is trained using a novel loss function, tailored for partially labeled segmentation data. An automated quality control method is also developed, allowing promising segmentations to be selected. Compared with a threshold and level set algorithm, the FCNN method improved 2D segmentation accuracy across several metrics. The proposed quality control approach further improved the average DICE score by 25.8%. In tests with users of SimVascular, when using quality control, users accepted 80% of segmentations produced by the best performing FCNN. The FCNN cardiovascular model building method reduces the amount of manual segmentation effort required for patient-specific model construction, by as much as 73%. This leads to reduced turnaround time for cardiovascular simulations. While the method was used for cardiovascular model building, it is applicable to general tubular structures. © 2019, International Federation for Medical and Biological Engineering.},
author_keywords={Cardiovascular modeling;  Cardiovascular simulation;  Convolutional neural networks;  Patient-specific modeling;  SimVascular},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vincke201977,
author={Vincke, S. and Hernandez, R.D.L. and Bassier, M. and Vergauwen, M.},
title={Immersive visualisation of construction site point cloud data, meshes and bim models in a vr environment using a gaming engine},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2019},
volume={42},
number={5/W2},
pages={77-83},
doi={10.5194/isprs-archives-XLII-5-W2-77-2019},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074661782&doi=10.5194%2fisprs-archives-XLII-5-W2-77-2019&partnerID=40&md5=00cb8903ac1ee80dd034e292e212fc32},
abstract={By adopting Building Information Modelling (BIM) software, the architecture, engineering and construction (AEC) industry shifted from a two-dimensional approach to a three-dimensional one in the design phase of a building. However, a similar three-dimensional approach for the visualisation of the current state of the construction works is lacking. Currently, progress reports typically include numerous pictures of the construction site or elements, alongside the appropriate parts of the 3D as-design BIM model. If a proper transition to a 3D design versus 3D current state were achieved, the evolved type of reports would become more comprehensible, resulting in more well-informed decision-making. This requires a single, unique software platform that is able to import, process, analyse and visualise both the as-design BIM model as well as the recorded data of the current construction state. At present however, the visualisation and interpretation of the different datasets alone requires already multiple software packages. As a partial solution this work presents a platform to easily visualise and interpret various data sources such as point clouds, meshes and BIM models and analysis results. Recent advances of gaming engines focus on and allow for an excellent visualisation of mesh data. Therefore all of the aforementioned data sources are converted into mesh objects upon importing. Moreover, gaming engines provide the necessary tools to traverse the scene intuitively allowing construction site managers and other stakeholders to gain a more complete and better oversight of the construction project. Furthermore, these engines also provide the possibility to take the immersion to the next level: Incorporating the 3D entities into a Virtual Reality (VR) environment makes the visualised data and the executed analyses even more comprehensible. By means of a case study, the potential of the presented approach is showcased. The real-world construction site recordings, models and analyses are visualised and implemented in VR using the Unity gaming engine. © Authors 2019.},
author_keywords={Construction;  Gaming Engine;  Remote Sensing;  Virtual Reality;  Visualisation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Baik201915,
author={Baik, A.},
title={From point cloud to existing bim for modelling and simulation purposes},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2019},
volume={42},
number={5/W2},
pages={15-19},
doi={10.5194/isprs-archives-XLII-5-W2-15-2019},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074632559&doi=10.5194%2fisprs-archives-XLII-5-W2-15-2019&partnerID=40&md5=124667397011c7015de31ee63adc13f9},
abstract={Many BIM experts agree that employing BIM for new construction is an easy task. However, applying BIM to existing construction will be difficult but is more suitable for heritage buildings cases. These heritage buildings have unique façades and architectural vocabulary, which are of special interest. Furthermore, studying these architecture heritages require some advanced tools in order to understand and analyse their structure, components, and design. Relying only on traditional methods is not adequate, especially for architectural engineers and experts who need digital representations of architectural heritage in order to draw a complete image of any aspect of the project. Moreover, lots of these heritage architectural elements are not documented or provided in the digital architectural libraries, which in turn requires advanced and easy access methods and tools that can extract basic information professionally and explain the essence of heritage. BIM has emerged as an efficient solution that could possibly help in analysing architectural heritage through effective learning processes. Existing BIM is characterised by their ability to create and operate within a digital database of any existing by 3-D laser through scanning the building and transforming it into point-cloud as digital data, so that engineers and experts can work on existing and buildings via the BIM software. As with many heritage buildings in the world, many of the heritage buildings in the Historic district of Jeddah city, Saudi Arabia, face serious issues in terms of conservation, restoration, documentation, managing, recording, and monitoring of these valuable heritage buildings. Therefore, this paper will examine and evaluate the use of BIM in modelling and for simulation purposes, (e.g. structure and energy simulation) with regard to one of the existing heritage buildings in the Historic district of Jeddah. © Authors 2019.},
author_keywords={BIM;  Historic Jeddah;  modelling and simulation;  Point Cloud;  Scan to BIM;  TLS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gavrilov201954,
author={Gavrilov, D. and Lazarenko, L. and Zakirov, E.},
title={AI Recognition in Skin Pathologies Detection},
journal={Proceedings - 2019 International Conference on Artificial Intelligence: Applications and Innovations, IC-AIAI 2019},
year={2019},
pages={54-56},
doi={10.1109/IC-AIAI48757.2019.00017},
art_number={9007333},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081593633&doi=10.1109%2fIC-AIAI48757.2019.00017&partnerID=40&md5=23f1f3bb153f9762bd6217c3d06021a9},
abstract={Skin cancer is the most common type of cancer [1]. Between different malignant skin pathology melanoma is the most fleeting and mortality. Despite the superficial location of pathologies, only half of patients seek medical assistance on the early stages[2]. Treatment on the early (epidermal) stage provides a significantly higher chance of recovery. To assist a wide range of people in the early skin cancer detection, a software package was developed. The software based on deep convolutional neural networks technology. This complex allows to classify normal and malignant pathology on the uploaded photos. In clinical practice doctors use the ABCDE symptom's complex. This complex characterizes the observation of pigment spot asymmetry, border irregularities, color unevenness, diameter, and evolution [3]. The machine learning approach involves the computer evaluating similar factors when processing multiple images of different skin formations. The paper presents an algorithm for classification of skin lesions into pathology and norm using convolutional neural network architecture Xception with prior images segmentation. The upper classifying layers were frozen and new ones were added to classify skin diseases in the pre-trained neural network Xception. As a result, the classification of benign and malignant skin tumors provided at least 89% accuracy. At the moment, the result of research work is designed in form of application software that allows to download the image of pigmented skin spots from the camera. It is available on http://skincheckup.online. © 2019 IEEE.},
author_keywords={artificial intelligence;  CNN;  medical information systems;  neural networks;  Preventive care;  recommendation systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu2019223,
author={Xu, Q. and Xiong, X. and Feng, G.-L. and Guo, M.-J. and Wan, L.},
title={Design of intelligent campus multimedia interactive system based on internet of things technology},
journal={Proceedings - 2019 International Conference on Virtual Reality and Intelligent Systems, ICVRIS 2019},
year={2019},
pages={223-226},
doi={10.1109/ICVRIS.2019.00062},
art_number={8921364},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077131113&doi=10.1109%2fICVRIS.2019.00062&partnerID=40&md5=f24d83b678903c2afa8585f3cb6123c2},
abstract={In order to perfect the construction of intelligent campus multimedia interactive information network and improve the ability of campus multimedia interaction and intelligent education information management, a design scheme of intelligent campus multimedia interactive system based on Internet of things technology is proposed. The system development environment is based on Multigen Creator3.2 and embedded PCI bus development environment, and the intelligent campus multimedia interactive infrastructure is constructed by using Internet of things and radio frequency identification (RFID) construction scheme. The virtual scene simulation and multimedia information interaction design of multimedia information platform, intelligent classroom, library interactive platform and student information management system are realized on the intelligent multimedia information processing terminal to meet the needs of multimedia interaction in intelligent campus. The hardware structure and software development of the intelligent campus multimedia interactive system are carried out. the test results show that the system has high information integration and strong multimedia information processing ability. The artificial intelligence is good, the system is stable and reliable. © 2019 IEEE.},
author_keywords={Bus;  Intelligent campus;  Internet of things;  Multimedia interactive system;  Network protocol},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{ZeynepOnur20192723,
author={Zeynep Onur, A. and Nouban, F.},
title={Software in the architectural presentation and design of buildings: State-of-the-art},
journal={International Journal of Innovative Technology and Exploring Engineering},
year={2019},
volume={8},
number={10},
pages={2723-2729},
doi={10.35940/ijitee.J9486.0881019},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071329761&doi=10.35940%2fijitee.J9486.0881019&partnerID=40&md5=c54fa24c710fa6a32bf61dd3d60c2fee},
abstract={Since in the recent years the architecture software have completely changed the work of architects, today, the architects begin to use software to achieve every kind of design that they are working for from the very beginning until the end. As presented in this paper, the choice of architectural software depends on the specific needs of the designs ranging from two-dimensional design to three-dimensional or BIM technologies. Since some architects prefer to use different software specific for the design that they are working on and they combine the results to produce the intended ultimate result, while some prefer only one software for the entire work, in this paper, it has been attempted to cover both cases.The main aim of this paper is to introduce the most used architectural and building design software from two-dimensional to BIM technologies to cover the needs of architects in the presentation and design of buildings. Twelve architectural and building design software were selected and classified depending on the purpose of their application. The selected software have been analyzed according to the BIM levels, and most commonly used software in every BIM level and their advantages and disadvantages are presented. Two software from BIM level 0, three software from BIM level 1, and four software from BIM level 2, together with 3 software for the design of buildings, a total of twelve software have been selected and technically described by giving the technical specifications, characteristics, application domains, limitation, technical popularity and capabilities that can be a complete guideline for architects and building designers to choose the suitable software to be applied in architectural and building design professions. Finally for the architectural presentation the software of BIM level 0 (AutoCAD LT and DraftSight), BIM level 1 (AutoCAD Architecture, Chief Architect and TurboCAD), BIM level 2 (ArchiCAD, Revit, Vectorworks Architect and Allplan Architecture), and for the buildings design the CSI software packages (SAP 2000, ETABS and SAFE) and STAAD.PRO software are recommended to be employed. © BEIESP.},
author_keywords={Architecture;  Bim;  Cad;  Computer-Aided Design;  Csi;  Design;  Presentation;  Software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sholanke2019251,
author={Sholanke, A.B. and Opoko, A.P. and Onakoya, A.O. and Adigun, T.F.},
title={Awareness level and adoption of modular construction for affordable housing in nigeria: Architects’ perspective},
journal={International Journal of Innovative Technology and Exploring Engineering},
year={2019},
volume={8},
number={9},
pages={251-257},
doi={10.35940/ijitee.i8113.078919},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070230786&doi=10.35940%2fijitee.i8113.078919&partnerID=40&md5=be6a805fb44c00810c1030b2e12212c8},
abstract={Modular construction is a construction method where pre-designed building units are constructed off-site, conveyed to site as components and assembled to form a building or structure. The construction method is adjudged to significantly reduce building time and offers good economic value which are some of its key benefits over traditional construction methods. This study evaluated architects’ awareness level and adoption of modular construction method in Nigeria, in order to ascertain its application level towards providing prompt affordable housing projects delivery in the country. The study is a survey research that engaged a pragmatic research approach. Both qualitative and quantitative inquiry methods were employed to carry out the research. Qualitative inquiry method was employed to gather relevant data used to develop a structured questionnaire that was used to collect quantitative data. The questionnaire was distributed to a hundred architects who were randomly selected from a hundred architectural firms registered to practice in Lagos State, the most populous state in Nigeria. A total of seventy-one participants provided data for the study. The data were analysed with statistical package for social sciences (SPSS) software, version 21. The result depicts that majority (87%) of the respondents are aware of modular construction as an approach used in building construction, but just few (32%) of the respondents have adopted modular construction in the delivery of affordable housing scheme in the study area. In addition, it was found that modular construction for affordable housing projects is mostly adopted for its waste reduction, construction time reduction and cost saving benefits. The study concludes that though architects’ awareness level of modular construction method and its benefits is high in Nigeria, its adoption by the architects for prompt delivery of affordable housing projects in the study area is low. Hence, appropriate measures were recommended to enhance the adoption of modular construction method by architects to combat housing challenges experienced in Nigeria. © BEIESP.},
author_keywords={Affordable housing;  Architects’ perspective;  Modular construction;  Nigeria},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Monev2019137,
author={Monev, V. and Hristova, M.},
title={OAtools - Software package for investigation of orthogonal arrays},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={137-140},
doi={10.1145/3345252.3345293},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073078539&doi=10.1145%2f3345252.3345293&partnerID=40&md5=da8066daf7bdb1d75acf44e6aca655cd},
abstract={Orthogonal arrays are important combinatorial structures, both from a theoretical and a practical point of view. This fact is based on their close relation with other combinatorial structures and their practical application for testing and planning of experiments. For this reason, the orthogonal arrays are of a great research interest for many authors who explore different methods for construction, classification of orthogonal arrays, etc. In this paper we present a software package for investigation of orthogonal arrays. One of its functionalities is to check whether a matrix (or a set of matrices) satisfies the conditions for orthogonal array with fixed parameters. Another functionality of this package is the identification of isomorphic orthogonal arrays. Furthermore, OAtools provides the ability to calculate distance distribution. © 2019 Copyright Association for Computing Machinery.},
author_keywords={Distance distribution;  Hash tables;  Non-isomorphic orthogonal arrays;  Orthogonal arrays;  Q-Extension},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Riesch2019,
author={Riesch, M. and Tchipev, N. and Bungartz, H.-J. and Jirauschek, C.},
title={Numerical simulation of the quantum cascade laser dynamics on parallel architectures},
journal={Proceedings of the Platform for Advanced Scientific Computing Conference, PASC 2019},
year={2019},
doi={10.1145/3324989.3325715},
art_number={5},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068741013&doi=10.1145%2f3324989.3325715&partnerID=40&md5=5a1a0a556d8a6e7bd7ff8d2e091b0565},
abstract={Over the last decades, quantum cascade lasers (QCLs) have become established sources of mid-infrared and terahertz light. For their anticipated applications, e.g., in spectroscopy, their dynamical behavior is particularly interesting. Numerical simulations constitute an essential tool for investigating the QCL dynamics but exhibit considerable computational workload. In order to accelerate the simulations and thereby aid the design process of QCLs, we present efficient parallel implementations of an established numerical method using OpenMP. Performance measurements on a 28-core CPU confirm their efficiency. © 2019 Copyright held by the owner/author(s).},
author_keywords={Mathematical software performance;  Multicore architectures;  Physics;  Quantum mechanic simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Radescu2019,
author={Radescu, R. and Dragu, M.},
title={Automatic Analysis of Potential Hazard Events Using Unmanned Aerial Vehicles},
journal={Proceedings of the 11th International Conference on Electronics, Computers and Artificial Intelligence, ECAI 2019},
year={2019},
doi={10.1109/ECAI46879.2019.9042120},
art_number={9042120},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084547083&doi=10.1109%2fECAI46879.2019.9042120&partnerID=40&md5=7df0481d31cb23e8455107b013021e30},
abstract={This paper is motivated by the possibility of developing a wide variety of applications and domains in which Unmanned Aerial Vehicles (UAVs) can be used globally for various purposes. UAVs are currently used by public administrations and security forces such as police, fire brigades, civil protection, research institutions, construction, and agriculture entities. The purpose of this paper is to facilitate the handling of UAVs to retrieve various data from the environment. The drone (UAV) visits some points to collect data (image and/or video input) from sensors like GPS, camera, gyroscope, and accelerometer. GPS sensor coordinates are used to compare the data taken with subsequent results through processing with specialized software. The drone is used as an access gate with built-in sensors. Certain hazard events (fires, floods, avalanches, landslides) are not limited to narrow geographical areas, but can impact the environment by triggering negative chain events. 3D modeling offers a wide range of possibilities to prevent potential hazard events, or, if such an event has occurred, makes it possible to monitor the affected area and assess the damage by comparing the area in the pre-event configuration with the after-event one. After image processing and data acquisition, a report is generated that includes the map and the 3D model of the analyzed object. A hazard is an agent that has the potential to cause damage to a particular target. Terms such as risk or danger can be used in similar contexts. TensorFlow is an open source software library in high-performance computing. Flexible architecture allows easy deployment of computing on a variety of platforms (CPU, GPU, TPU), from desktop to server or mobile devices. We used the learning transfer: at first we used a model that was already prepared for another problem, and then we re-qualified it on a similar problem. Deep learning from scratch can take several days, but learning transfer can be done shortly. We applied Python along with TensorFlow to train an image classifier and classify images with it. We formed a consistent set of training pictures, using three labels: fire, flood (detectable hazards) and nature (non-hazard images). We then re-qualified an efficient, small-sized neural network by (re)training the image set in order to get the best results in the hazards prediction selection process with a progressive higher accuracy as (re) training evolves at optimal rating. With Python and OpenCV technologies, we used four decision algorithms to generate prediction of hazard: Support Vector Machine, Naive Bayes, Logistic Regression, and Decision Tree Classifier. Each generated report includes precision, recall, f1-score, and support indices, depending on the class and intervals used. We also used the confusion matrix as an alternative method to evaluate the classification accuracy. Analyzing the 4 algorithms we noticed that they behave differently. Training using TensorFlow generated better results than the other methods. For the main classes tested hazard is recognized up to 99%. © 2019 IEEE.},
author_keywords={Classifiers;  Decision;  Hazard;  Prediction;  UAV},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qasaimeh2019,
author={Qasaimeh, M. and Denolf, K. and Lo, J. and Vissers, K. and Zambreno, J. and Jones, P.H.},
title={Comparing energy Efficiency of CPU, GPU and FPGA implementations for vision kernels},
journal={2019 IEEE International Conference on Embedded Software and Systems, ICESS 2019},
year={2019},
doi={10.1109/ICESS.2019.8782524},
art_number={8782524},
note={cited By 72},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070913064&doi=10.1109%2fICESS.2019.8782524&partnerID=40&md5=bb8b6cdd54b6a7996d29b826cafd2f16},
abstract={Developing high performance embedded vision applications requires balancing run-time performance with energy constraints. Given the mix of hardware accelerators that exist for embedded computer vision (e.g. multi-core CPUs, GPUs, and FPGAs), and their associated vendor optimized vision libraries, it becomes a challenge for developers to navigate this fragmented solution space. To aid with determining which embedded platform is most suitable for their application, we conduct a comprehensive benchmark of the run-time performance and energy efficiency of a wide range of vision kernels. We discuss rationales for why a given underlying hardware architecture innately performs well or poorly based on the characteristics of a range of vision kernel categories. Specifically, our study is performed for three commonly used HW accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2× compared to the others for simple kernels. While for more complicated kernels and complete vision pipelines, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3×. It is also observed that the FPGA performs increasingly better as a vision application's pipeline complexity grows. © 2019 IEEE.},
author_keywords={CPUs;  Embedded Vision;  Energy Efficiency;  FPGAs;  GPUs},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sholanke20192008,
author={Sholanke, A.B. and Opoko, A.P. and Yakubu, P.O. and Ukwedeh, J.O.},
title={Effects and resolution guidelines of land-use conflict in construction management in Lagos state, Nigeria},
journal={International Journal of Innovative Technology and Exploring Engineering},
year={2019},
volume={8},
number={8},
pages={2008-2013},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067901401&partnerID=40&md5=3d18b8ec710d403a94d0e61a1606d6c2},
abstract={Construction management involves the organisation of human and material resources to achieve prompt and cost-effective construction project delivery. The structure of construction management is complex in nature. It involves several individuals, including professionals who handle different aspects of projects. Like every other field that involves the management of human and material resources, construction management is prone to several challenges, one of which is land-use conflict. Due to the unprecedented urbanization process fuelled by rural-urban migration in Lagos State, Nigeria, there has been a rapid growth of informal settlements whose existence usually result in land-use conflicts between the government and the communities. As informal settlements, the communities are often neglected in the provision of basic services and opinions of residents are usually not considered in respect to land-use pattern developments. This study examined the effects of land-use conflict in construction management in Lagos State, Nigeria, with a view to developing managerial guidelines for its effective resolution. The study used quantitative survey instrument in retrieving data from professionals in the State Ministries. The questionnaires were analysed using statistical package for social sciences (SPSS) software and results presented using descriptive method with the aid of tables. The result shows that land-use conflict affects the progress, sequence and completion phase of construction with the planning, problem solving and communication skills of the manager found to be the most important soft skills in resolving land-use conflicts. The study also found that change in socio-economic benefits in informal settlement is the most likely reason for land-use conflict. Among the recommendation of the study is that education curriculum of construction managers should integrate courses on soft skills to complement the professional’s technical skill acquired for construction management. © BEIESP.},
author_keywords={Construction Management;  Lagos State and Nigeria;  Land-use Conflict;  Managerial Skills},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Schottle201971,
author={Schottle, M. and Kienzle, J.},
title={On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: The case for signature extension},
journal={Proceedings - 2019 IEEE/ACM 11th International Workshop on Modelling in Software Engineering, MiSE 2019},
year={2019},
pages={71-77},
doi={10.1109/MiSE.2019.00018},
art_number={8876961},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074878358&doi=10.1109%2fMiSE.2019.00018&partnerID=40&md5=d39601879c67b41ebc1816148f85620f},
abstract={Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level. © 2019 IEEE.},
author_keywords={API;  Modularity;  Reuse;  Separation of concerns;  Usage interface},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Huang2019974,
author={Huang, T.-W. and Lin, C.-X. and Guo, G. and Wong, M.},
title={CPP-taskflow: Fast task-based parallel programming using modern C++},
journal={Proceedings - 2019 IEEE 33rd International Parallel and Distributed Processing Symposium, IPDPS 2019},
year={2019},
pages={974-983},
doi={10.1109/IPDPS.2019.00105},
art_number={8821011},
note={cited By 45},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067794112&doi=10.1109%2fIPDPS.2019.00105&partnerID=40&md5=17bdca625ba57921952dc8fbef21542f},
abstract={In this paper we introduce Cpp-Taskflow, a new C++ tasking library to help developers quickly write parallel programs using task dependency graphs. Cpp-Taskflow leverages the power of modern C++ and task-based approaches to enable efficient implementations of parallel decomposition strategies. Our programming model can quickly handle not only traditional loop-level parallelism, but also irregular patterns such as graph algorithms, incremental flows, and dynamic data structures. Compared with existing libraries, Cpp-Taskflow is more cost efficient in performance scaling and software integration. We have evaluated Cpp-Taskflow on both micro-benchmarks and real-world applications with million-scale tasking. In a machine learning example, Cpp-Taskflow achieved 1.5-2.7× less coding complexity and 14-38% speed-up over two industrial-strength libraries OpenMP Tasking and Intel Threading Building Blocks (TBB). © 2019 IEEE},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2019280,
author={Wang, Z. and Rezazadeh Azar, E.},
title={BIM-based draft schedule generation in reinforced concrete-framed buildings},
journal={Construction Innovation},
year={2019},
volume={19},
number={2},
pages={280-294},
doi={10.1108/CI-11-2018-0094},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063995374&doi=10.1108%2fCI-11-2018-0094&partnerID=40&md5=5613ef6170e0ea631f8cfb7c7366339a},
abstract={Purpose: Project schedules have a vital role in the effective management of time, cost, scope and resources in construction projects, and creating schedules requires schedulers with construction knowledge and experience. The increase in the complexity of building projects and the emergence of building information modeling (BIM) in the architecture, engineering and construction industry have encouraged researchers to explore BIM capabilities for automated schedule generation. The scope and capabilities of the developed systems, however, are limited and the link between design and scheduling is still underdeveloped. This paper aims to investigate methods to develop a BIM-based framework to automatically generate schedules for concrete-framed buildings. Design/methodology/approach: This system first extracts the required data from the building information model, including elements’ dimensions, quantities, spatial information, materials and other related attributes. It then applies construction rules, prior knowledge and production rate data to create project work-packages, calculate their durations and determine their relationships. Finally, it organizes these results into a schedule using project management software. Findings: This system provides an automated and easy-to-use approach to generate schedules for concrete-framed buildings that are modeled in a BIM platform. It provides two schedules for each project, both a sequential and an overlapped solution, which the schedulers can modify into a practical schedule based on conditions and available resources. Originality/value: This research project presents an innovative approach to use BIM-based attributes of structural elements to develop list of work-packages and estimate their durations, and then it uses a combination of rule-based and case-based reasoning to generate the schedules. © 2019, Emerald Publishing Limited.},
author_keywords={Automation;  Building information modelling;  Case-based reasoning;  Concrete-framed building;  Construction scheduling;  Project management},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wiegreffe20191342,
author={Wiegreffe, D. and Alexander, D. and Stadler, P.F. and Zeckzer, D.},
title={RNapuzzler: Efficient outerplanar drawing of RNA-secondary structures},
journal={Bioinformatics},
year={2019},
volume={35},
number={8},
pages={1342-1349},
doi={10.1093/bioinformatics/bty817},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068566075&doi=10.1093%2fbioinformatics%2fbty817&partnerID=40&md5=ec897d6656acfef7b8e30c595284f1ff},
abstract={Motivation: RNA secondary structure is a useful representation for studying the function of RNA, which captures most of the free energy of RNA folding. Using empirically determined energy parameters, secondary structures of nucleic acids can be efficiently computed by recursive algorithms. Several software packages supporting this task are readily available. As RNA secondary structures are outerplanar graphs, they can be drawn without intersection in the plane. Interpretation by the practitioner is eased when these drawings conform to a series of additional constraints beyond outerplanarity. These constraints are the reason why RNA drawing is difficult. Many RNA drawing algorithms therefore do not always produce intersection-free (outerplanar) drawings. Results: To remedy this shortcoming we propose here the RNApuzzler algorithm which is guaranteed to produce intersection-free drawings. It is based on a drawing algorithm respecting constraints based on nucleotide distances (RNAturtle). We investigate relaxations of these constraints allowing for intersection-free drawings. Based on these relaxations, we implemented a fully automated, simple, and robust algorithm that produces aesthetic drawings adhering to previously established guidelines. We tested our algorithm using the RFAM database and found that we can compute intersection-free drawings of all RNAs therein efficiently. Availability and implementation: The software can be accessed freely at: https://github.com/dwiegreffe/RNApuzzler. © The Author(s) 2018. Published by Oxford University Press. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Du2019768,
author={Du, Y. and Lim, Y. and Tan, Y.},
title={Activity recognition using RFID phase profiling in smart library},
journal={IEICE Transactions on Information and Systems},
year={2019},
volume={E102D},
number={4},
pages={768-776},
doi={10.1587/transinf.2018DAP0010},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063980158&doi=10.1587%2ftransinf.2018DAP0010&partnerID=40&md5=9e1ffcc56958763e6891f7009d6c0cac},
abstract={In the library, recognizing the activity of the reader can better uncover the reading habit of the reader and make book management more convenient. In this study, we present the design and implementation of a reading activity recognition approach based on passive RFID tags. By collecting and analyzing the phase profiling distribution feature, our approach can trace the reader’s trajectory, recognize which book is picked up, and detect the book misplacement. We give a detailed analysis of the factors that can affect phase profiling in theory and combine these factors with relevant activities. The proposed approach recognizes the activities based on the amplitude of the variation of phase profiling, so that the activities can be inferred in real time through the phase monitoring of tags. We then implement our approach with off-the-shelf RFID equipment, and the experiments show that our approach can achieve high accuracy and efficiency in activity recognition in a real-world situation. We conclude our work and further discuss the necessity of a personalized book recommendation system in future libraries. Copyright © 2019 The Institute of Electronics, Information and Communication Engineers.},
author_keywords={Activity recognition;  Internet of things;  RFID;  Smart library;  Wireless sensing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pérez2019183,
author={Pérez, G. and Yovine, S.},
title={Formal specification and implementation of an automated pattern-based parallel-code generation framework},
journal={International Journal on Software Tools for Technology Transfer},
year={2019},
volume={21},
number={2},
pages={183-202},
doi={10.1007/s10009-017-0465-2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026498954&doi=10.1007%2fs10009-017-0465-2&partnerID=40&md5=4dbfb5b172d0329caa05253d3b58b09b},
abstract={Programming correct parallel software in a cost-effective way is a challenging task requiring a high degree of expertise. As an attempt to overcoming the pitfalls undermining parallel programming, this paper proposes a pattern-based, formally grounded tool that eases writing parallel code by automatically generating platform-dependent programs from high-level, platform-independent specifications. The tool builds on three pillars: (1) a platform-agnostic parallel programming pattern, called PCR, (2) a formal translation of PCRs into a parallel execution model, namely Concurrent Collections (CnC), and (3) a program rewriting engine that generates code for a concrete runtime implementing CnC. The experimental evaluation carried out gives evidence that code produced from PCRs can deliver performance metrics which are comparable with handwritten code but with assured correctness. The technical contribution of this paper is threefold. First, it discusses a parallel programming pattern, called PCR, consisting of producers, consumers, and reducers which operate concurrently on data sets. To favor correctness, the semantics of PCRs is mathematically defined in terms of the formalism FXML. PCRs are shown to be composable and to seamlessly subsume other well-known parallel programming patterns, thus providing a framework for heterogeneous designs. Second, it formally shows how the PCR pattern can be correctly implemented in terms of a more concrete parallel execution model. Third, it proposes a platform-agnostic C++ template library to express PCRs. It presents a prototype source-to-source compilation tool, based on C++ template rewriting, which automatically generates parallel implementations relying on the Intel CnCC++ library. © 2017, Springer-Verlag GmbH Germany.},
author_keywords={Automated code generation;  Formal methods;  Parallel programming;  Software design patterns},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lenhard2019241,
author={Lenhard, J. and Blom, M. and Herold, S.},
title={Exploring the suitability of source code metrics for indicating architectural inconsistencies},
journal={Software Quality Journal},
year={2019},
volume={27},
number={1},
pages={241-274},
doi={10.1007/s11219-018-9404-z},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042912586&doi=10.1007%2fs11219-018-9404-z&partnerID=40&md5=ae18d59ef67d7c999f753fb1f017d66e},
abstract={Software architecture degradation is a phenomenon that frequently occurs during software evolution. Source code anomalies are one of the several aspects that potentially contribute to software architecture degradation. Many techniques for automating the detection of such anomalies are based on source code metrics. It is, however, unclear how accurate these techniques are in identifying the architecturally relevant anomalies in a system. The objective of this paper is to shed light on the extent to which source code metrics on their own can be used to characterize classes contributing to software architecture degradation. We performed a multi-case study on three open-source systems for each of which we gathered the intended architecture and data for 49 different source code metrics taken from seven different code quality tools. This data was analyzed to explore the links between architectural inconsistencies, as detected by applying reflexion modeling, and metric values indicating potential design problems at the implementation level. The results show that there does not seem to be a direct correlation between metrics and architectural inconsistencies. For many metrics, however, classes more problematic as indicated by their metric value seem significantly more likely to contribute to inconsistencies than less problematic classes. In particular, the fan-in, a classes’ public API, and method counts seem to be suitable indicators. The fan-in metric seems to be a particularly interesting indicator, as class size does not seem to have a confounding effect on this metric. This finding may be useful for focusing code restructuring efforts on architecturally relevant metrics in case the intended architecture is not explicitly specified and to further improve architecture recovery and consistency checking tool support. © 2018, The Author(s).},
author_keywords={Architectural inconsistencies;  Code anomalies;  Multi-case study;  Reflexion modeling;  Software architecture degradation;  Source code metrics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liang2019305,
author={Liang, Y. and Wan, S.},
title={The Design and Implementation of Books Recommendation System},
journal={Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
year={2019},
volume={2018-November},
pages={305-308},
doi={10.1109/ICSESS.2018.8663914},
art_number={8663914},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063631834&doi=10.1109%2fICSESS.2018.8663914&partnerID=40&md5=fac9453cef0cedb9b6ae703e99cdcfd7},
abstract={Personalized recommendation technology is a new technology which can mine products by using user's information, and that meet user's preferences through a series of algorithms, so as to achieve better recommendation effect. The number of books in university library is increasing rapidly. How to find interesting books from a large number of books is a problem that every reader is concerned about. In order to help these users find the books that they are interested in, this author designs a books recommendation system based on collaborative filtering algorithm. The system can basically meet the needs of users to recommend functions, and achieved good results. © 2018 IEEE.},
author_keywords={books recommendation system;  data mining;  personalized service},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2019924,
author={Wang, Z. and Liu, X. and Chang, Q. and Zhang, K.},
title={Design and Implementation of a Cross-Cloud Management System},
journal={Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
year={2019},
volume={2018-November},
pages={924-927},
doi={10.1109/ICSESS.2018.8663756},
art_number={8663756},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063624376&doi=10.1109%2fICSESS.2018.8663756&partnerID=40&md5=4a9aa455c39a942ddfefcb05fdbe2550},
abstract={Many organizations face the problem to manage their public cloud services from multiple vendors. This paper introduces a Web-based cross-cloud management system and the details of the methods implemented in the system are discussed. Based on the Grails framework, the system builds an API adapter model to integrate the API of many cloud server vendors, and unifies the return data from different vendors. Real-time scheduling is implemented through Quartz and multi-granular data visualization is implemented with ECharts. After testing, the system achieves the designed objectives and provides a convenient platform for cloud-crossing management. © 2018 IEEE.},
author_keywords={API adapter;  cross-cloud managemnent;  Data Visualization;  Grails;  Quartz},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Celio201952,
author={Celio, C. and Chiu, P.-F. and Asanović, K. and Nikolić, B. and Patterson, D.},
title={BROOM: An Open-Source Out-of-Order Processor with Resilient Low-Voltage Operation in 28-nm CMOS},
journal={IEEE Micro},
year={2019},
volume={39},
number={2},
pages={52-60},
doi={10.1109/MM.2019.2897782},
art_number={8634812},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061295335&doi=10.1109%2fMM.2019.2897782&partnerID=40&md5=ac7983903b9e205fcbce799890685444},
abstract={The Berkeley resilient out-of-order machine (BROOM) is a resilient, wide-voltage-range implementation of an open-source out-of-order (OoO) RISC-V processor implemented in an ASIC flow. A 28-nm test-chip contains a BOOM OoO core and a 1-MiB level-2 (L2) cache, enhanced with architectural error tolerance for low-voltage operation. It was implemented by using an agile design methodology, where the initial OoO architecture was transformed to perform well in a high-performance, low-leakage CMOS process, informed by synthesis, place, and route data by using foundry-provided standard-cell library and memory compiler. The two-person-team productivity was improved in part thanks to a number of open-source artifacts: The Chisel hardware construction language, the RISC-V instruction set architecture, the rocket-chip SoC generator, and the open-source BOOM core. The resulting chip, taped out using TSMC's 28-nm HPM process, runs at 1.0 GHz at 0.9 V, and is able to operate down to 0.47 V. © 2019 IEEE.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Demeshko2019332,
author={Demeshko, I. and Watkins, J. and Tezaur, I.K. and Guba, O. and Spotz, W.F. and Salinger, A.G. and Pawlowski, R.P. and Heroux, M.A.},
title={Toward performance portability of the Albany finite element analysis code using the Kokkos library},
journal={International Journal of High Performance Computing Applications},
year={2019},
volume={33},
number={2},
pages={332-352},
doi={10.1177/1094342017749957},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048513132&doi=10.1177%2f1094342017749957&partnerID=40&md5=14ff168ad354f33360589ebf2ffe4cb5},
abstract={Performance portability on heterogeneous high-performance computing (HPC) systems is a major challenge faced today by code developers: parallel code needs to be executed correctly as well as with high performance on machines with different architectures, operating systems, and software libraries. The finite element method (FEM) is a popular and flexible method for discretizing partial differential equations arising in a wide variety of scientific, engineering, and industrial applications that require HPC. This article presents some preliminary results pertaining to our development of a performance portable implementation of the FEM-based Albany code. Performance portability is achieved using the Kokkos library. We present performance results for the Aeras global atmosphere dynamical core module in Albany. Numerical experiments show that our single code implementation gives reasonable performance across three multicore/many-core architectures: NVIDIA General Processing Units (GPU’s), Intel Xeon Phis, and multicore CPUs. © The Author(s) 2018.},
author_keywords={climate simulations;  finite element code;  Kokkos library;  many-core programming;  Performance portability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pibiri2019,
author={Pibiri, G.E. and Venturini, R.},
title={Handling massive n-gram datasets efficiently},
journal={ACM Transactions on Information Systems},
year={2019},
volume={37},
number={2},
doi={10.1145/3302913},
art_number={a25},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062334164&doi=10.1145%2f3302913&partnerID=40&md5=3cd3796e423500242ab5f72a4d9d506c},
abstract={Two fundamental problems concern the handling of large n-gram language models: indexing, that is, compressing the n-grams and associated satellite values without compromising their retrieval speed, and estimation, that is, computing the probability distribution of the n-grams extracted from a large textual source. Performing these two tasks efficiently is vital for several applications in the fields of Information Retrieval, Natural Language Processing, and Machine Learning, such as auto-completion in search engines and machine translation. Regarding the problem of indexing, we describe compressed, exact, and lossless data structures that simultaneously achieve high space reductions and no time degradation with respect to the state-of-the-art solutions and related software packages. In particular, we present a compressed trie data structure in which each word of an n-gram following a context of fixed length k, that is, its preceding k words, is encoded as an integer whose value is proportional to the number of words that follow such context. Since the number of words following a given context is typically very small in natural languages, we lower the space of representation to compression levels that were never achieved before, allowing the indexing of billions of strings. Despite the significant savings in space, our technique introduces a negligible penalty at query time. Specifically, the most space-efficient competitors in the literature, which are both quantized and lossy, do not take less than our trie data structure and are up to 5 times slower. Conversely, our trie is as fast as the fastest competitor but also retains an advantage of up to 65% in absolute space. Regarding the problem of estimation, we present a novel algorithm for estimating modified Kneser-Ney language models that have emerged as the de-facto choice for language modeling in both academia and industry thanks to their relatively low perplexity performance. Estimating such models from large textual sources poses the challenge of devising algorithms that make a parsimonious use of the disk. The state-of-the-art algorithm uses three sorting steps in external memory: we show an improved construction that requires only one sorting step by exploiting the properties of the extracted n-gram strings. With an extensive experimental analysis performed on billions of n-grams, we show an average improvement of 4.5 times on the total runtime of the previous approach. © 2019 Copyright held by the owner/author(s).},
author_keywords={Algorithm engineering;  Efficiency;  Scalability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li20191,
author={Li, L. and Sau, C. and Fanni, T. and Li, J. and Viitanen, T. and Christophe, F. and Palumbo, F. and Raffo, L. and Huttunen, H. and Takala, J. and Bhattacharyya, S.S.},
title={An integrated hardware/software design methodology for signal processing systems},
journal={Journal of Systems Architecture},
year={2019},
volume={93},
pages={1-19},
doi={10.1016/j.sysarc.2018.12.010},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059818575&doi=10.1016%2fj.sysarc.2018.12.010&partnerID=40&md5=2f3902bc568cc09c8481791002ee85a9},
abstract={This paper presents a new methodology for design and implementation of signal processing systems on system-on-chip (SoC) platforms. The methodology is centered on the use of lightweight application programming interfaces for applying principles of dataflow design at different layers of abstraction. The development processes integrated in our approach are software implementation, hardware implementation, hardware-software co-design, and optimized application mapping. The proposed methodology facilitates development and integration of signal processing hardware and software modules that involve heterogeneous programming languages and platforms. As a demonstration of the proposed design framework, we present a dataflow-based deep neural network (DNN) implementation for vehicle classification that is streamlined for real-time operation on embedded SoC devices. Using the proposed methodology, we apply and integrate a variety of dataflow graph optimizations that are important for efficient mapping of the DNN system into a resource constrained implementation that involves cooperating multicore CPUs and field-programmable gate array subsystems. Through experiments, we demonstrate the flexibility and effectiveness with which different design transformations can be applied and integrated across multiple scales of the targeted computing system. © 2019 The Authors},
author_keywords={Dataflow;  Deep learning;  Hardware/software co-design;  Low power techniques;  Model-based design;  Signal processing systems},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sena2019640,
author={Sena, A.C. and Marzulo, L.A.J. and Nascimento, A.P. and Vasconcelos, C.N.},
title={A Malleable Vectorized Auction Algorithm for Modern Multicore Architectures},
journal={Proceedings - 20th International Conference on High Performance Computing and Communications, 16th International Conference on Smart City and 4th International Conference on Data Science and Systems, HPCC/SmartCity/DSS 2018},
year={2019},
pages={640-648},
doi={10.1109/HPCC/SmartCity/DSS.2018.00115},
art_number={8622853},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062484717&doi=10.1109%2fHPCC%2fSmartCity%2fDSS.2018.00115&partnerID=40&md5=ac11293aecac980f617a009f8b597b20},
abstract={The auction algorithm has been widely used to solvev the bipartite graph matching problem and its parallel implementation is employed to find solutions in a reasonable computational time. Moreover, the new multicore architectures, besides its various cores, have a SIMD instruction set that can increase application performance when exactly the same operations are to be performed on multiple data objects. The aim of this paper is to efficiently execute the auction algorithm on these architectures. To achieve that, a vectorized version was implemented and evaluated. These versions were then run in parallel using the OpenMP library. Finally, to optimize the number of threads used during the execution, a malleable strategy is proposed and evaluated. Results show that the vectorized version outperforms the sequential one by a factor of 10, while the malleable vectorized version was able to adapt its execution to exploit the full potential of multicore architectures. © 2018 IEEE.},
author_keywords={Auction Algorithm;  Malleable applications;  Multicore architectures},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2019293,
author={Wang, Y. and Liu, X. and Wu, X. and Yuan, X. and Meng, J. and Tang, Y.},
title={AMPrimer: An R package for anchored multiplex PCR primer design},
journal={3rd International Conference on Biological Information and Biomedical Engineering, BIBE 2019},
year={2019},
pages={293-296},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096565225&partnerID=40&md5=3cb0653672d0ceaf35e5d2aac6bdcb73},
abstract={Next-generation sequencing (NGS) has revolutionized life science for its ability to massively parallel sequencing. The quality of sequencing highly depends on the quality of the library constructed using PCR. A novel PCR has been developed to tackle some problems in the analysis of NGS termed anchored multiplex PCR (AMP), such as PCR bias. AMP has been shown as a robust and powerful method to enrich targets. Primer design plays an important role in library construction. We implemented an R package, AMPrimer, a panel for designing primers used in AMP. AMPrimer functions as an interface communicating R and Primer3, an opensource primer design software. The input file of AMPrimer contains the location of targets needed to be amplified. Target sequences are classified into two categories according to their length. Longer sequences are split into shorter fragments due to the sequencer's ability. Boulder-IO file fed into Primer3 is also generated by R. Primer3 would be used twice. The first run of Primer3 is to generate outer primers while the second run of Primer3 is to generate inner primers. Primer pairs are evaluated in terms of directionality, nested primer location, and secondary structure. © VDE VERLAG GMBH · Berlin · Offenbach.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shaikh2019592,
author={Shaikh, M. and Ibarhimov, D. and Zardari, B.},
title={Assessing architectural sustainability during software evolution using package-modularization metrics},
journal={International Journal of Advanced Computer Science and Applications},
year={2019},
volume={10},
number={12},
pages={592-608},
doi={10.14569/ijacsa.2019.0101277},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078428146&doi=10.14569%2fijacsa.2019.0101277&partnerID=40&md5=ea073d1ab499298cfce5367c617a5597},
abstract={Sustainability of software architectures is largely dependent on cost-effective evolution and modular architecture. Careful modularization, characterizing proper design of complex system is cognitive and challenging task for insuring improved sustainability. Moreover, failure to modularize the software systems during its evolution phases often results in requiring extra effort towards managing design deterioration and solving unforeseen inter-dependencies. In this paper, we present an empirical perspective of package-level modularization metrics proposed by Sarkar, Kak and Rama to characterize modularization quality through packages. In particular, we explore impact of these design based modularization metrics on other well known modularity metrics and software quality metrics. Our experimental examination over open source java software systems illustrates that package-level modularization metrics significantly correlate with architectural sustainability measures and quality metrics of software systems. © Science and Information Organization.},
author_keywords={Packages;  Software architecture;  Software modularity;  Software quality},
document_type={Article},
source={Scopus},
}

@ARTICLE{NoAuthor2019,
title={10th International Conference on Supercomputing, ISUM 2019},
journal={Communications in Computer and Information Science},
year={2019},
volume={1151 CCIS},
page_count={345},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077518799&partnerID=40&md5=b283a39b5e3a706b2eecbc91c125a990},
abstract={The proceedings contain 26 papers. The special focus in this conference is on International Conference on Supercomputing. The topics include: Multiphase Flows Simulation with the Smoothed Particle Hydrodynamics Method; a Parallel Implementation for Solving the Fluid and Rigid Body Interaction; Parallel High-Performance Computing Algorithm to Generate FEM-Compliant Volumetric Mesh Representations of Biomolecules at Atomic Scale; A Simple Model of the Flow in the Steam Chamber in SAGD Oil Recovery; software Defined Data Center for High Performance Computing Applications; ETL Processing in Business Intelligence Projects for Public Transportation Systems; Molecular Rotors in Viscous Fluids: A Numerical Analysis Aid by GPU Computing; SPH Numerical Simulations of the Deformation of a Liquid Surface in Two Dimensions; siO2 Electronic Structure in Gas Giants’ Planetary Cores: A Density Functional Theory Approach; Three Body Problem Applied to the Helium Atom System: The Independent Electron Approximation (IEA); solution of Schrödinger Equation Not Implementing Conventional Separation of Variables: Using the Trial and Error Brute Force Permutation Method; preface; generation and Classification of Energy Load Curves Using a Distributed MapReduce Approach; Lagrangian Approach for the Study of Heat Transfer in a Nuclear Reactor Core Using the SPH Methodology; public Transportation System Real-Time Re-organization Due to Civil Protection Events; multi-objective Evolutive Multi-thread Path-Planning Algorithm Considering Real-Time Extreme Weather Events; multi-objective Optimization of Vehicle Routing with Environmental Penalty; algorithm for Removing Secondary Lines Blended with Balmer Lines in Synthetic Spectra of Massive Stars; Multi GPU Implementation to Accelerate the CFD Simulation of a 3D Turbo-Machinery Benchmark Using the RapidCFD Library.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Molinero2019173,
author={Molinero, D. and Galván, S. and Pacheco, J. and Herrera, N.},
title={Multi GPU Implementation to Accelerate the CFD Simulation of a 3D Turbo-Machinery Benchmark Using the RapidCFD Library},
journal={Communications in Computer and Information Science},
year={2019},
volume={1151 CCIS},
pages={173-187},
doi={10.1007/978-3-030-38043-4_15},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077500721&doi=10.1007%2f978-3-030-38043-4_15&partnerID=40&md5=1a015d98d64d9d19870ab10d8580b26a},
abstract={Recently, several research groups have demonstrated significant speedups of scientific computations using General Purpose Graphics Processor Units (GPGPU) as massively-parallel “co-processors” to the Central Processing Unit (CPU). However, the tremendous computational power of GPGPUs has come with a high price since their implementation to Computational Fluids Dynamics (CFD) solvers is still a challenge. To achieve this implementation, the RapidCFD library was developed from the Open Field Operation and Manipulation (OpenFOAM) CFD software to let that the multi-GPGPU were able of running almost the entire simulation in parallel. The parallel performance, as fixed-size speed-up, efficiency and parallel fraction, according to the Amdahl’s law, were compared in two massively parallel multi-GPGPU architectures using Nvidia Tesla C1060 and M2090 units. The simulations were executed on a 3D turbo-machinery benchmark which consist of a structured grid domain of 1 million cells. The results obtained from the implementation of the new library on different software and hardware layouts show that by transferring directly all the computations executed by the linear system solvers to the GPGPU, is possible to make a typical CFD simulation until 9 times faster. Additionally a grid convergence analysis and pressure recovery measurements were executed over scaled computational domains. Thus, it is expected to obtain an affordable low computational cost when the domain be scaled in order to achieve a high flow resolution. © Springer Nature Switzerland AG 2019.},
author_keywords={CFD;  Draft tube;  GPGPU},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2019897,
author={Zhang, L. and Swanson, S.},
title={Pangolin: A fault-tolerant persistent memory programming library},
journal={Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019},
year={2019},
pages={897-911},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077022507&partnerID=40&md5=80a76bd654ee7e6fa773733278952d59},
abstract={Non-volatile main memory (NVMM) allows programmers to build complex, persistent, pointer-based data structures that can offer substantial performance gains over conventional approaches to managing persistent state. This programming model removes the file system from the critical path which improves performance, but it also places these data structures out of reach of file system-based fault tolerance mechanisms (e.g., block-based checksums or erasure coding). Without fault-tolerance, using NVMM to hold critical data will be much less attractive. This paper presents Pangolin, a fault-tolerant persistent object library designed for NVMM. Pangolin uses a combination of checksums, parity, and micro-buffering to protect an application’s objects from both media errors and corruption due to software bugs. It provides these protections for objects of any size and supports automatic, online detection of data corruption and recovery. The required storage overhead is small (1% for gigabyte-sized pools of NVMM). Pangolin provides stronger protection, requires orders of magnitude less storage overhead, and achieves comparable performance relative to the current state-of-the-art fault-tolerant persistent object library. © Proceedings of the 2019 USENIX Annual Technical Conference, USENIX ATC 2019. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yerimbetov20191582,
author={Yerimbetov, B.T. and Chalabayev, B.M. and Kunanbayeva, Y.B. and Ussenkulov, Z.A. and Orazbayev, Z.I. and Aldiyarov, Z.A.},
title={Seismic resistance of multi-storey reinforced concrete wall-frame structures at destructive earthquakes},
journal={Periodicals of Engineering and Natural Sciences},
year={2019},
volume={7},
number={4},
pages={1582-1598},
doi={10.21533/pen.v7i4.841},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075853499&doi=10.21533%2fpen.v7i4.841&partnerID=40&md5=c7bd05be306f1b94efc66bfd73a065ae},
abstract={This article shows results of a numerical study of the behavior of multi-storey reinforced concrete wall-frame structure under loads of special combination, considering seismic impact that corresponds to destructive earthquake. The purpose of the study is to identify conditions that increase energy absorption capacity of wall-frame structure under the effect of destructive earthquakes as well as methods for assessing the energy absorption capacity of wall-frame structure at the design stage. Numerical studies were carried on 9-storey frame building, designed for construction in the area with an estimated seismicity of 7 points. Loads of special combination were applied until the complete exhaustion of the bearing capacity of the structure. The calculations were made using the LIRA software package. Performed studies made it possible to identify and assess bearing capacity margin of buildings designed to meet the requirements of valid antiseismic construction regulations if earthquake intensity exceeds the design calculated value. As a result of a numerical study of the work of a 9-storey frame reinforced concrete building of a frame scheme corresponding to the third version of the system, the building withstood the load exceeding the estimated norm by 30%. The value of the coefficient showing the deformation properties during operation of the system at loads exceeding the calculated values before failure, amounted to K = 3.1. The results obtained give the designer the opportunity to create conditions for the appearance of plastic joints in as many cross sections of frame elements as possible. This in turn leads to an increase in the energy intensity of the skeleton, capable of absorbing the excess energy of a destructive earthquake. We have developed the recommendations for determining bearing capacity margin of buildings at design stage if earthquake intensity exceeds calculated value. © 2019 International University of Sarajevo.},
author_keywords={Bearing capacity margin;  Energy absorption capacity of the system;  Loads of a special combination;  Multi-storey framed buildings;  Seismic impacts},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bingmann2019285,
author={Bingmann, T. and Bradley, P. and Gauger, F. and Iqbal, Z.},
title={COBS: A Compact Bit-Sliced Signature Index},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11811 LNCS},
pages={285-303},
doi={10.1007/978-3-030-32686-9_21},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075649462&doi=10.1007%2f978-3-030-32686-9_21&partnerID=40&md5=24f224c961997f3176b200385c33acf4},
abstract={We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over between an inverted index and Bloom filters. Our target application is to index k-mers of DNA samples or q-grams from text documents and process approximate pattern matching queries on the corpus with a user-chosen coverage threshold. Query results may contain a number of false positives which decreases exponentially with the query length. We compare COBS to seven other index software packages on 100 000 microbial DNA samples. COBS’ compact but simple data structure outperforms the other indexes in construction time and query performance with Mantis by Pandey et al. in second place. However, unlike Mantis and other previous work, COBS does not need the complete index in RAM and is thus designed to scale to larger document sets. © 2019, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{GalloNieves2019274,
author={Gallo Nieves, M. and Diaz Rodriguez, J.L. and Gonzalez Castellanos, J.A.},
title={Speed Estimation of an Induction Motor with Sensorless Vector Control at Low Speed Range Using Neural Networks},
journal={Communications in Computer and Information Science},
year={2019},
volume={1052},
pages={274-284},
doi={10.1007/978-3-030-31019-6_24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075646135&doi=10.1007%2f978-3-030-31019-6_24&partnerID=40&md5=aa36fecb03dc898b6300294e2b2066ff},
abstract={This paper deals with the study, design and implementation of a speed estimation technique, using an Artificial Neural Network (ANN), for a three-phase induction motor type Squirrel Cage. The main objective of this technique is to estimate the speed of the engine from its own conditions at low speed, with the purpose of applying vector control without the use of a traditional speed sensor that supplies the control loop with the speed of rotation the motor. When replacing the traditional sensor with RNA, a sensorless control loop is obtained. The training of the RNA was done collecting data in different ranges of engine speed, both empty and loaded. The simulation of the system is carried out in the Simulink platform, from the Matlab software package and the implementation of the control strategy is done using the Raspberry Pi 3B card. © 2019, Springer Nature Switzerland AG.},
author_keywords={Artificial neural networks;  Induction motor;  Sensorless control;  Speed estimation;  Vector control},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schimanski201964,
author={Schimanski, C.P. and Marcher, C. and Toller, G. and Pasetti Monizza, G. and Matt, D.T.},
title={Enhancing Automation in the Construction Equipment Industry Through Implementation of BIM},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11792 LNCS},
pages={64-73},
doi={10.1007/978-3-030-30949-7_8},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075582895&doi=10.1007%2f978-3-030-30949-7_8&partnerID=40&md5=a0d5407f6ee7eda749a26cfac9932750},
abstract={Building Information Modeling (BIM) is becoming increasingly important in the construction industry and affects almost all stakeholders from building owners, planners and contractors to building operators. However, little importance is given in the academic debate to the relationship between BIM and construction equipment providers. This raises the question how the BIM method can be used to generate benefits in this sector. The presenting researchers, in cooperation with a local construction equipment provider, have selected its container construction division as a case study and analyzed how current processes can be streamlined and automated with the help of BIM. In this context, new automation processes, BIM-object libraries and add-ins for one of the most common BIM authoring software have been developed. Through this modular design approach, a flexible and lean BIM-based Configure-to-Order production system is created. This study presents the conceptual development of this BIM-based production system for construction equipment providers considering both methodological examinations and functioning software prototypes. The BIM-based production system is currently tested by the industry partner in pilot projects and preliminary findings are presented in this study. © 2019, Springer Nature Switzerland AG.},
author_keywords={Algorithms;  BIM;  Configure-to-Order;  Industry 4.0;  Lean production system design},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2019140,
author={Wang, A. and Shi, Y. and Yi, X. and Yan, Y. and Liao, C. and de Supinski, B.R.},
title={Ompparser: A standalone and unified openmp parser},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11718 LNCS},
pages={140-152},
doi={10.1007/978-3-030-28596-8_10},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072855794&doi=10.1007%2f978-3-030-28596-8_10&partnerID=40&md5=326987f6455e85a12df3746eb05d6800},
abstract={OpenMP has been quickly evolving to meet the insatiable demand for productive parallel programming on high performance computing systems. Creating a robust and optimizing OpenMP compiler has become increasingly challenging due to the expanding capabilities and complexity of OpenMP, especially for its latest 5.0 release. Although OpenMP’s syntax and semantics are very similar between C/C++ and Fortran, the corresponding compiler support, such as parsing and lowering are often separately implemented, which is a significant obstacle to support the fast changing OpenMP specification. In this paper, we present the design and implementation of a standalone and unified OpenMP parser, named ompparser, for both C/C++ and Fortran. ompparser is designed to be useful both as an independent tool and an integral component of an OpenMP compiler. It can be used for syntax and semantics checking of OpenMP constructs, validating and verifying the usage of existing constructs, and helping to prototype new constructs. The formal grammar included in ompparser also helps interpretation of the OpenMP standard. The ompparser implementation supports the latest OpenMP 5.0, including complex directives such as metadirective. It is released as open-source from https://github.com/passlab/ompparser with a BSD-license. We also demonstrate how it is integrated with the ROSE’s open-source OpenMP compiler. © Springer Nature Switzerland AG 2019.},
author_keywords={Compiler;  Intermediate representation;  OpenMP;  Parser},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shen2019,
author={Shen, H.T. and Liu, J. and Huang, Z. and Ngo, C. and Wang, W.},
title={Near-duplicate Video Retrieval: Current Research and Future Trends - Withdrawn},
journal={IEEE Multimedia},
year={2019},
doi={10.1109/MMUL.2011.39},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072201698&doi=10.1109%2fMMUL.2011.39&partnerID=40&md5=7c5de11a48c5225ddc5657913d4220b7},
abstract={Withdrawn. IEEE},
author_keywords={Copyright protection;  Databases;  Feature extraction;  H.2.0.b Database design;  H.2.0.c Query design and implementation language;  H.2.2.c Indexing methods;  H.2.4.d Distributed databases;  H.2.4.e Multimedia databases;  H.2.4.e Multimedia databases;  H.2.4.h Query processing;  H.2.8.e Feature extraction or construction;  H.2.8.g Image databases;  H.2.8.g Image databases;  H.3.1 Content Analysis and Indexing;  H.3.1 Content Analysis and Indexing H.3 Information Storage and Retrieval HInformation Technology and Systems;  H.3.3 Information Search and Retrieval;  H.3.3 Information Search and Retrieval;  H.3.3 Information Search and Retrieval H.3 Information Storage and Retrieval HInformation Technology and Systems;  H.3.7 Digital Libraries;  H.3.7 Digital Libraries;  H.5.1.f Image/video retrieval;  H.5.1.g Video;  I.2.1.b Computer vision;  I.2.10 Vision and Scene Understanding;  I.5.4.b Computer vision;  Image color analysis;  K.2.c Software;  modeling and management;  Semantics;  Streaming media;  Visualization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yang20191217,
author={Yang, C.-H. and Wu, T.-H. and Xiao, B. and Kang, S.-C.},
title={Design of a robotic software package for modular home builder},
journal={Proceedings of the 36th International Symposium on Automation and Robotics in Construction, ISARC 2019},
year={2019},
pages={1217-1222},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071495084&partnerID=40&md5=76af4ebcf9c84af70936a354bb2420c6},
abstract={This paper is sharing an on-going project about the design of a software package, tentatively coined as RS4B, for the builders of modularized construction. The shortage of labour resources and safety awareness issues have become the emerging problems in the construction industry. As the price of robots gradually decreases within these years, it is a promising direction to integrate robotic technology with the construction process to bring better productivity. However, shifting the long-term labour-based process to the robot-based process is not a simple task. Therefore, this research designed an assisted software package for filling the gap between the conventional process and robotic process. Four kinds of software were proposed in this research with their required functions and user interfaces design. With such an assisting tool, builders of modular home manufacturing will be able to extract information from the existing BIM model and transfer the information for the robot control. © 2019 International Association for Automation and Robotics in Construction I.A.A.R.C. All rights reserved.},
author_keywords={Industrial robots;  Modular homes;  Robotic construction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Poslavsky20191,
author={Poslavsky, S.},
title={Rings: An Efficient JVM Library for Commutative Algebra (Invited Talk)},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11661 LNCS},
pages={1-11},
doi={10.1007/978-3-030-26831-2_1},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071464296&doi=10.1007%2f978-3-030-26831-2_1&partnerID=40&md5=d3bc9bec6b95245e13bcbdd1669d5109},
abstract={Rings is an open-source library, written in Java and Scala programming languages, which implements basic concepts and algorithms from computational commutative algebra. The goal of the Rings library is to provide a high-performance implementation packed into a lightweight library (not a full-featured CAS) with a clean application programming interface (API), which meets modern standards of software development. Polynomial arithmetic, GCDs, factorization, and Gröbner bases are implemented with the use of modern fast algorithms. Rings provides a simple API with a fully typed hierarchy of algebraic structures and algorithms for commutative algebra. The use of the Scala language brings a quite novel powerful, strongly typed functional programming model allowing to write short, expressive, and fast code for applications. © 2019, Springer Nature Switzerland AG.},
author_keywords={Commutative algebra;  Computer algebra software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Katasev2019477,
author={Katasev, A.S.},
title={Neuro-fuzzy model of fuzzy rules formation for objects state evaluation in conditions of uncertainty [Нейронечеткая модель формирования нечетких правил для оценки состояния объектов в условиях неопределенности]},
journal={Computer Research and Modeling},
year={2019},
volume={11},
number={3},
pages={477-492},
doi={10.20537/2076-7633-2019-11-3-477-492},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071340799&doi=10.20537%2f2076-7633-2019-11-3-477-492&partnerID=40&md5=9c5ee1d1c250e9f35bf6c794fc98d009},
abstract={This article solves the problem of constructing a neuro-fuzzy model of fuzzy rules formation and using them for objects state evaluation in conditions of uncertainty. Traditional mathematical statistics or simulation modeling methods do not allow building adequate models of objects in the specified conditions. Therefore, at present, the solution of many problems is based on the use of intelligent modeling technologies applying fuzzy logic methods. The traditional approach of fuzzy systems construction is associated with an expert attraction need to formulate fuzzy rules and specify the membership functions used in them. To eliminate this drawback, the automation of fuzzy rules formation, based on the machine learning methods and algorithms, is relevant. One of the approaches to solve this problem is to build a fuzzy neural network and train it on the data characterizing the object under study. This approach implementation required fuzzy rules type choice, taking into account the processed data specificity. In addition, it required logical inference algorithm development on the rules of the selected type. The algorithm steps determine the number and functionality of layers in the fuzzy neural network structure. The fuzzy neural network training algorithm developed. After network training the formation fuzzy-production rules system is carried out. Based on developed mathematical tool, a software package has been implemented. On its basis, studies to assess the classifying ability of the fuzzy rules being formed have been conducted using the data analysis example from the UCI Machine Learning Repository. The research results showed that the formed fuzzy rules classifying ability is not inferior in accuracy to other classification methods. In addition, the logic inference algorithm on fuzzy rules allows successful classification in the absence of a part of the initial data. In order to test, to solve the problem of assessing oil industry water lines state fuzzy rules were generated. Based on the 303 water lines initial data, the base of 342 fuzzy rules was formed. Their practical approbation has shown high efficiency in solving the problem. © 2019 Alexey S. Katasev.},
author_keywords={Fuzzy neural network;  Fuzzy production rule;  Knowledge base formation;  Neuro-fuzzy model;  Object state evaluation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Vu2019381,
author={Vu, H. and Fertig, T. and Braun, P.},
title={Model-driven integration testing of hypermedia systems},
journal={Journal of Web Engineering},
year={2019},
volume={18},
number={4-6},
pages={381-408},
doi={10.13052/jwe1540-9589.18465},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071274363&doi=10.13052%2fjwe1540-9589.18465&partnerID=40&md5=bc25d08c6060792f08a87eff3f8fbed0},
abstract={The proper design of Representational State Transfer (REST) APIs is not trivial because developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature. Especially hypermedia testing is not mentioned at all. Manual hypermedia testing is time-consuming and hard to maintain. Testing a hypermedia API requires many test cases that have similar structure, especially when different user roles and error cases are considered. In order to tackle this problem, we proposed a Model-Driven Testing (MDT) approach for hypermedia systems using the metamodel within our existing Model Driven Software Development (MDSD) approach. This work discusses challenges and results of hypermedia testing for RESTful APIs using MDT techniques that were discovered within our research. MDT allows white-box testing, hence covering complete program structure and behavior of the generated application. By doing this, we are able to achieve a high automated test coverage. Moreover, any runtime behavior deviated from the metamodel reveals bugs within the generators. © 2019 River Publishers},
author_keywords={Hypermedia Testing;  Integration Testing;  MDE;  MDSD;  MDT;  Model-Driven Testing;  REST;  RESTful API},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Demidova201926,
author={Demidova, A.V. and Druzhinina, O.V. and Masina, O.N. and Tarova, E.D.},
title={Computer research of nonlinear stochastic models with migration flows},
journal={CEUR Workshop Proceedings},
year={2019},
volume={2407},
pages={26-37},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069695154&partnerID=40&md5=4cad2d3ad93fb6bb115e9696d4eb2bd3},
abstract={The problems of the design methods extension and computer research of nondeterministic finite-dimensional population models describing migration flows are studied. The significant difficulties arise in the construction of high dimension dynamic models in the course of analytical research. Computer research allows not only to obtain the results of numerical experiments to search for trajectories and estimate the parameters of deterministic models, but also to reveal the effects caused by stochasticization. The model parameters are estimated and local phase portraits are constructed for the initial four-dimensional migration-population model. The transition from the vector ordinary differential equation to the corresponding stochastic differential equation is performed. The structure of the stochastic model is described on the basis of applying the method of constructing self-consistent stochastic models. As a tool for the study of population-migration models, a software package is used to solve numerically the differential equations systems using modified Runge–Kutta methods. The software package allows numerical experiments based on the implementation of algorithms for generating trajectories of multidimensional Wiener processes and multipoint distributions and algorithms for solving stochastic differential equations. The comparative analysis of the computer research results obtained for stochastic models is carried out. The properties of migration-population systems in deterministic and stochastic cases are characterized. The comparison of the results obtained for the three-dimensional and four-dimensional cases is carried out. The effects inherent in models with migration flows are revealed.The obtained results can be applied to the problems of modeling and forecasting the behavior of multidimensional systems describing the migration flows. Copyright © 2019 for the individual papers by the papers’ authors.},
author_keywords={Computer modeling;  Nonlinear models of migration flows;  Stochasticization of one-step processes;  Symbol computing libraries;  Symbolic representation algorithms},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Faria2019225,
author={Faria, H. and Solís, P. and Bordim, J. and Hagstrom, R.},
title={A backup-as-a-service (BaAS) software solution},
journal={CLOSER 2019 - Proceedings of the 9th International Conference on Cloud Computing and Services Science},
year={2019},
pages={225-232},
doi={10.5220/0007250902250232},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067429250&doi=10.5220%2f0007250902250232&partnerID=40&md5=2eb060fae1dbbd98a445bcaa94b8f5ed},
abstract={Backup is a replica of any data that can be used to restore its original form. However, the total amount of digital data created worldwide more than doubles every two years and is expected to reach 44 trillions of gigabytes in 2020, bringing constant new challenges to backup processes. Enterprise backup is one of the oldest and most performed tasks by infrastructure and operations professionals. Still, most backup systems have been designed and optimized for outdated environments and use cases. That fact, generates frustration over currently backup challenges and leads to a greater willingness to modernize and to consider new technologies. Traditional backup and archive solutions are no longer able to meet users current needs. The ideal modern backup and recovery software should not only provide features to attend a traditional data center, but also allow the integration and exploration of the growing Cloud, including “backup client as a service” and “backup storage as a service”. The present study proposed and deploys a Backup as a Service software solution. For that, the cloud/backup parameters, cloud backup challenges, researched architectures and Backup-as-a-Service (BaaS) system requirements are specified. Then, a selected set of BaaS desired features are developed, resulting in the first truly cloud REST API based Backup-as-a-Service interface, namely “bcloud”. Finally, this work conducts an on-line usability poll with a significant number of users. The analysis of results in an overall average objective zero to ten questions evaluation was 8.29%, indicating a very satisfactory user perception of the bcloud BaaS interface prototype. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
author_keywords={Backup-as-a-Service;  Backups;  Cloud;  Disaster Recovery;  Software-as-a-Service},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tarasenko2019679,
author={Tarasenko, A. and Chepur, P. and Gruchenkova, A.},
title={Substantiation of Technological Solutions for the Repair of the Anodic Tank Grounding},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={983},
pages={679-685},
doi={10.1007/978-3-030-19868-8_66},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067082398&doi=10.1007%2f978-3-030-19868-8_66&partnerID=40&md5=d998d9e5b179a169a3ca918c13353dd3},
abstract={The article reviews the features of applying the technology of repairing the bottom and anodic earthing of a vertical steel cylindrical tank without dismantling the floating roof during its reconstruction. The main non-trivial task of this project was to carry out work inside the tank in the cramped conditions of the sub-pontoon space. To replace the bottom of the tank, along with the extended anodic earthing devices, located at a depth of 0.9 m from the surface, a technological corridor free from floating roof racks was proposed, as well as a technological corridor for moving dismantled materials, soil, new bottom sheets towards the installation opening in the wall. As part of the proposed installation diagram, the floating roof was in the installation position provided for by the tank construction project with the provision of a 2.2 m high sub-pontoon space. To solve the problem of ensuring the strength of the structure, a finite element model of a floating roof was built in the ANSYS software package, which allows it to be analyzed at different types of mounting loads. The results obtained using the model formed the basis of the adopted design decisions. Using FEM [4&#x2013;6, 8&#x2013;10, 12&#x2013;15], the design of a temporary reinforcement frame was theoretically justified and proposed, which ensured a fivefold safety margin sufficient for construction and installation works. © 2019, Springer Nature Switzerland AG.},
author_keywords={Floating roof;  Repair;  Tank},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ibe2019758,
author={Ibe, E. and Shibaeva, G. and Portnyagin, D. and Afanasyeva, E.},
title={Thermal Protection of Multi-layer Exterior Walls with an Expanded Polystyrene Core},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={983},
pages={758-767},
doi={10.1007/978-3-030-19868-8_74},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067062852&doi=10.1007%2f978-3-030-19868-8_74&partnerID=40&md5=e18bf690468d8ed7b5dcc40fd32d55e4},
abstract={In the article, the results of the study of temperature and humidity conditions of a multilayered structure in climatic conditions of Southern Siberia are stated. The complex analysis of a heat-shielding of a building is made. The humidity condition of multilayered external walls is studied. The basic construction units with the purpose of revealing of cold bridges are researched. Fields of temperature deformations of external walls are received. Unsuccessful experience of application of 3D-panels is shown, and ways of their elimination from the point of view of increase of a heat-shielding are offered. Methods of research represent modeling by means of ELCUT and SCAD Office software packages. Recommendations for the design of construction units of a building using 3D-panel technology are proposed in accordance with the requirements of not only normative documents, but also from the perspective of a comfortable thermal environment of the rooms. © 2019, Springer Nature Switzerland AG.},
author_keywords={Energy efficiency;  External wall;  Multilayered structure;  Temperature field;  Thermal bridge},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ye2019331,
author={Ye, P.-J. and Huang, C.-G. and Hu, Y.-W.},
title={An Adaptive Algorithm of Showing Intuitively the Structure of a Binary Tree},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={834},
pages={331-338},
doi={10.1007/978-981-13-5841-8_35},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066023601&doi=10.1007%2f978-981-13-5841-8_35&partnerID=40&md5=71445436f7476c7bb01409d4bfa25759},
abstract={The adaptive software development(ASD) method is an agile development method based on complex adaptive system theory. It develops adaptive abilities through methods such as candidate program library, dynamic display selection, semantic self-description and self-test, and establishes expert system through the study of business rules, achieves intelligent adaptable software by decision theory, diagnosis, recovery and other methods. This paper presents an adaptive algorithm of showing intuitively the structure of a binary tree, the algorithm module is plug and play, and can be inserted directly into any program source code related to the binary tree without any modification. This technology can improve the efficiency of system design, reduce the workload of program maintenance and has some innovation and practicability. © 2019, Springer Nature Singapore Pte Ltd.},
author_keywords={Adaptive;  Binary tree;  Plug and play introduction;  Self-description;  Semantic},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2019,
title={25th International Conference on Tools and Algorithms for the Construction and Analysis of Systems conference series, TACAS 2019 held as part of the 22nd European Joint Conferences on Theory and Practice of Software, ETAPS 2019},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11427 LNCS},
page_count={410},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064917088&partnerID=40&md5=ec0ebca0f65dc7ffaeda8280faf9722e},
abstract={The proceedings contain 22 papers. The special focus in this conference is on Tools and Algorithms for the Construction and Analysis of Systems conference series. The topics include: Incremental analysis of evolving alloy models; extending a brainiac prover to lambda-free higher-order logic; LCV: A verification tool for linear controller software; semantic fault localization and suspiciousness ranking; computing coupled similarity; reachability analysis for termination and confluence of rewriting; voxlogica: A spatial model checker for declarative image analysis; on reachability in parameterized phaser programs; abstract dependency graphs and their application to model checking; nonreach – a tool for nonreachability analysis; Parallel SAT simplification on GPU architectures; the quantitative verification benchmark set; ilang: a modeling and verification platform for SoCs using instruction-level abstractions; metacsl: specification and verification of high-level properties; roll 1.0: ω -Regular language learning library; symbolic regex matcher; compass 3.0; debugging of behavioural models with clear; omega-regular objectives in model-free reinforcement learning; verifiably safe off-model reinforcement learning; encoding redundancy for satisfaction-driven clause learning; WAPS: Weighted and Projected Sampling; building better bit-blasting for floating-point problems; The axiom profiler: Understanding and debugging SMT quantifier instantiations; on the empirical time complexity of scale-free 3-sat at the phase transition; Modular and efficient divide-and-conquer SAT solver on top of the painless framework; quantitative verification of masked arithmetic programs against side-channel attacks.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Ivanchikj2019470,
author={Ivanchikj, A. and Gjorgjiev, I. and Pautasso, C.},
title={REStalk miner: Mining REStful conversations, pattern discovery and matching},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11434 LNCS},
pages={470-475},
doi={10.1007/978-3-030-17642-6_46},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064883475&doi=10.1007%2f978-3-030-17642-6_46&partnerID=40&md5=79bc168058f0dce5935ceb0bef19e809},
abstract={REST has become the architectural style of choice for APIs, where clients need to instantiate a potentially lengthy sequence of requests to the server in order to achieve their goal, effectively leading to a RESTful conversation between clients and servers. Mining the logs of such RESTful conversations can facilitate knowledge sharing among API designers regarding design best practices as well as API usage and optimization. In this demo paper, we present the RESTalk Miner, which takes logs from RESTful services as an input and uses RESTalk, a domain specific language, to visualize them. It provides interactive coloring to facilitate graph reading, as well as statistics to compare the relative frequency of conversations performed by different clients. Furthermore, it supports searching for predefined patterns as well as pattern discovery. © Springer Nature Switzerland AG 2019.},
author_keywords={Mining;  Pattern search;  REST APIs;  RESTful conversations;  Visualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Örnhag201924,
author={Örnhag, M.V. and Wadenbäck, M.},
title={Planar motion bundle adjustment},
journal={ICPRAM 2019 - Proceedings of the 8th International Conference on Pattern Recognition Applications and Methods},
year={2019},
pages={24-31},
doi={10.5220/0007247700240031},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064591845&doi=10.5220%2f0007247700240031&partnerID=40&md5=5cec2b9ad479a1078e81e3f5b3fc53d3},
abstract={In this paper we consider trajectory recovery for two cameras directed towards the floor, and which are mounted rigidly on a mobile platform. Previous work for this specific problem geometry has focused on locally minimising an algebraic error between inter-image homographies to estimate the relative pose. In order to accurately track the platform globally it is necessary to refine the estimation of the camera poses and 3D locations of the feature points, which is commonly done by utilising bundle adjustment; however, existing software packages providing such methods do not take the specific problem geometry into account, and the result is a physically inconsistent solution. We develop a bundle adjustment algorithm which incorporates the planar motion constraint, and devise a scheme that utilises the sparse structure of the problem. Experiments are carried out on real data and the proposed algorithm shows an improvement compared to established generic methods. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved},
author_keywords={Bundle Adjustment;  Planar Motion},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhu2019471,
author={Zhu, H. and Miao, C. and Zhuang, M.},
title={Analysis of office-teaching comprehensive buildings using a modified seismic performance evaluation method},
journal={CMES - Computer Modeling in Engineering and Sciences},
year={2019},
volume={118},
number={3},
pages={471-491},
doi={10.31614/cmes.2019.04382},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063930325&doi=10.31614%2fcmes.2019.04382&partnerID=40&md5=e0d6fe8b8f57e3151889e33f7c2f3851},
abstract={Current building design codes allow the appearance of structural and nonstructural damage under design basis earthquakes. The research regarding probabilistic seismic loss estimation in domestic building structure is urgent. The evaluation in this paper is based on a 11-story reinforced concrete office building, incremental dynamic analysis (IDA) is conducted in Perform 3D program using models capable to simulate all possible limit states up to collapse. Next, the probability distribution of post-earthquake casualties, rebuild costs repair costs and business downtime loss are calculated in PACT software for the studied building considering the modified component vulnerability groups and population flow models. The evaluation procedure can also shed light on other types of buildings in China. For non-typical functional building structures, this article proposes to build a finite element model of structural components and to classify the vulnerability groups based on the construction drawings, and to supply and improve the vulnerability library of appendages in FEMA P-58 according to the actual situation. In this way, the application scope of building seismic performance evaluation can be expanded. Copyright © 2019 Tech Science Press.},
author_keywords={A modified seismic capacity assessment method;  Frame-shear wall structure;  Incremental dynamic analysis (IDA);  Seismic performance assessment analysis software (PACT)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Asaka2019A33,
author={Asaka, K. and Ujikawa, H. and Uzawa, H. and Nakamura, H. and Kani, J.-I. and Otaka, A. and Terada, J.},
title={Disaggregation of time-critical applications in flexible access system architecture [invited]},
journal={Journal of Optical Communications and Networking},
year={2019},
volume={11},
number={1},
pages={A33-A39},
doi={10.1364/JOCN.11.000A33},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058454085&doi=10.1364%2fJOCN.11.000A33&partnerID=40&md5=febb97e3f98b0b696d5b60bfd628f3cd},
abstract={To quickly meet the diverse requirements imposed by emerging new services on optical access networks, we recently proposed the new concept of "flexible access system architecture" (FASA), which features the disaggregation of time-critical applications. This paper introduces our activities on FASA along with related technical trends. As our key ongoing challenge, the modularization of the dynamic bandwidth assignment function in a passive optical network is reviewed, and information on its use cases, approach, and sequences is given. Furthermore, a reference hardware implementation of FASA for mobile service is described. © 2019 Optical Society of America.},
author_keywords={Application programming interfaces;  Broadband communication;  Network function virtualization;  Software defined networking},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kumar2019709,
author={Kumar, U. and Taterh, S. and Murugan Kaliyamurthy, N.},
title={Software-defined networking—imposed security measures over vulnerable threats and attacks},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={742},
pages={709-715},
doi={10.1007/978-981-13-0589-4_67},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053925587&doi=10.1007%2f978-981-13-0589-4_67&partnerID=40&md5=b342b1293ab07dd071eb10ff6cb47501},
abstract={Software-defined networking (SDN) is a new attempt in addressing the existing challenges in the legacy network architecture, SDN is renowned due to its basic approach in managing the networks and its capability of programmability. In software-defined networks’ implementation, priority remains on high security. The advantage of SDN itself opens a wide ground in posing new security threats and challenges. Focusing on the security of the software-defined networks is a prime factor as it reflects on the growth of SDN technology implementation. This paper focused and designed with an introduction on software-defined networking, its architecture, the available security solutions for the network on the various existing security solutions available for software-defined networking, and the real challenge in securing the SDN networks providing the researchers a paved platform to work on further securing the networks, and finally it concludes with the requirements of security factors and schemes in SDN. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={Application programming interface (API);  Open flow (OF);  Open network foundation;  Software-defined networking;  Spoofing;  Transmission control protocol/Internet protocol (TCP/IP)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Krasić20191611,
author={Krasić, S. and Ando, N. and Pejić, P. and Tošić, Z.},
title={Comparative analysis of teaching in geometric surfaces at architectural faculties in Niš and Tokyo},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={809},
pages={1611-1623},
doi={10.1007/978-3-319-95588-9_144},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050585310&doi=10.1007%2f978-3-319-95588-9_144&partnerID=40&md5=ca857df54745113f301e1c3e031a010d},
abstract={For architect engineers it is very important to know geometric surfaces in order to use them in the design of buildings. Architectural objects, other than functional, should also satisfy the aesthetic criteria which is achieved through design. The existence of different software packages for modeling like Rhinoceros, with additional programs (Grasshopper), allows doing the rational design and construction of these architectural objects. All these facts were noticed by the authors of this paper, which was further followed by giving the students of Faculty of Civil Engineering and Architecture in Niš, an elective subject Geometric surfaces in architecture in the school year of 2009/10. The content of this subject includes study of all geometric surfaces in architecture practice. The importance of studying Geometric surfaces for students of architecture was noticed by the Faculty of Engineering and Design, Hosei University in Tokyo, Japan, which resulted with offering this elective subject to the students in the school year of 2017/18. The students of both faculties have learned to recognize geometric surfaces and how to apply them in complex geometric forms with attractive appearance. In this paper, the work on both faculties will be shown as they are different, the results will be compared, similarities and differences of working on two different continents. With proper analysis it is possible to make a model of study for this subject in different educational systems. © 2019, Springer International Publishing AG, part of Springer Nature.},
author_keywords={Faculties of architecture;  Geometric surfaces;  Nis;  Rhinoceros program;  Tokyo},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2018432,
author={Wang, Y.-L. and Wang, J.},
title={Construction of Interlibrary Loan Information System Based on SaaS Mode},
journal={Proceedings - 9th International Conference on Information Technology in Medicine and Education, ITME 2018},
year={2018},
pages={432-436},
doi={10.1109/ITME.2018.00103},
art_number={8589335},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061342925&doi=10.1109%2fITME.2018.00103&partnerID=40&md5=81d1cbb9a3d06cf3907c772b3a926d79},
abstract={In order to increase sharing level and effect of books and information between libraries, construction of books-checkout information system in the interlibrary using the system architecture of SaaS (Software as a Service), books- checkout information system consists of four modules, including administrator management, user management, function management and literature management. The system structure and module design of information system software are also introduced in detail, and function analysis of each module are made, task content of each module is designed, the relation between entity class and attribute involved in information system are demonstrated, and system class diagram is listed, finally main module function of information system is implemented by Java Web technology and mysql database. © 2018 IEEE.},
author_keywords={Books checkout library;  Information system},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suciu201861,
author={Suciu, G. and Dragu, M. and Hussain, I. and Iliescu, A.-M. and Orza, O. and Mocanu, C.},
title={3D Modeling Using Parrot Bebop 2 FPV},
journal={Proceedings - 16th International Conference on Embedded and Ubiquitous Computing, EUC 2018},
year={2018},
pages={61-65},
doi={10.1109/EUC.2018.00016},
art_number={8588849},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061428219&doi=10.1109%2fEUC.2018.00016&partnerID=40&md5=eb41510914fca949fa4003f6e29eb4cd},
abstract={It is expected that drones will play a major role in connecting the world in future. They will be delivering packages and merchandise, serving as mobile hotspots for broadband wireless access, will deal with surveillance purposes and security of smart cities. Drones, while can be used for the betterment of the community, can also be used by malevolent entities to gather physical and cyber-attacks, and threaten the society. However, we took a different approach using Parrot Bebop 2 by doing 3D modeling of our building. In this paper the main problems and the available solutions are addressed for the generation of 3D models from drone images. Close range photogrammetry has dealt for many years with manual or automatic image measurements for precise 3D modelling. Nowadays drones are also becoming a major source for scanning and snapping, but image-based modelling remains the most complete, portable, flexible and widely used approach. In this article we used 3D modeling as a process of establishing a mathematical representation of a 3-dimensional building by using the software Pix4D or Pix4D cloud. 3D models are now widely used on industrial scale and real estate dealers, architecture, construction, dealing with hazardous situations and product development using 3D models for visualizing, simulating and depiction graphic designs. © 2018 IEEE.},
author_keywords={3D modeling;  3D reconstruction;  Parrot Bebop 2 FPV;  Photogrammetry;  Pix4D cloud software;  UAV},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sychev2018,
author={Sychev, I. and Zhdanenko, O. and Bonetto, R. and Fitzek, F.H.P.},
title={ARIES: Low Voltage smArt gRid dIscrete Event Simulator to Enable Large Scale Learning in the Power Distribution Networks},
journal={2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids, SmartGridComm 2018},
year={2018},
doi={10.1109/SmartGridComm.2018.8587538},
art_number={8587538},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061027355&doi=10.1109%2fSmartGridComm.2018.8587538&partnerID=40&md5=5ad9dd3b07eff4a6cddbcb2ffe2d4c5d},
abstract={Accurate software based simulation of (complex) dynamic and, possibly, stochastic systems is a key component of the design and test of control strategies. Simulation tools developed according to software design best practices provide engineers and researchers with efficient and easy to use representations of the world. Hence, allowing for faster control algorithms design and test. Here we present ARIES, a (low voltage) smArt gRid dIscrete Event Simulator meant to enable large scale learning and easy smart grid applications design and testing. ARIES is designed according to object oriented best practices, and it is implemented in Python 3. ARIES is equipped with a REST API to actively interact with the simulations, it features a simulation results storage system based on a MongoDB database, and a event management system based on a redis in-memory data structure store used as message broker. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Oses20182477,
author={Oses, C. and Gossett, E. and Hicks, D. and Rose, F. and Mehl, M.J. and Perim, E. and Takeuchi, I. and Sanvito, S. and Scheffler, M. and Lederer, Y. and Levy, O. and Toher, C. and Curtarolo, S.},
title={AFLOW-CHULL: Cloud-Oriented Platform for Autonomous Phase Stability Analysis},
journal={Journal of Chemical Information and Modeling},
year={2018},
volume={58},
number={12},
pages={2477-2490},
doi={10.1021/acs.jcim.8b00393},
note={cited By 54},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055091081&doi=10.1021%2facs.jcim.8b00393&partnerID=40&md5=b484a05006ab48f39041e18bd7c6dc78},
abstract={A priori prediction of phase stability of materials is a challenging practice, requiring knowledge of all energetically competing structures at formation conditions. Large materials repositories - housing properties of both experimental and hypothetical compounds - offer a path to prediction through the construction of informatics-based, ab initio phase diagrams. However, limited access to relevant data and software infrastructure has rendered thermodynamic characterizations largely peripheral, despite their continued success in dictating synthesizability. Herein, a new module is presented for autonomous thermodynamic stability analysis, implemented within the open-source, ab initio framework AFLOW. Powered by the AFLUX Search-API, AFLOW-CHULL leverages data of more than 1.8 million compounds characterized in the AFLOW.org repository, and can be employed locally from any UNIX-like computer. The module integrates a range of functionality: the identification of stable phases and equivalent structures, phase coexistence, measures for robust stability, and determination of decomposition reactions. As a proof of concept, thermodynamic characterizations have been performed for more than 1300 binary and ternary systems, enabling the identification of several candidate phases for synthesis based on their relative stability criterion - including 17 promising C15b-type structures and 2 half-Heuslers. In addition to a full report included herein, an interactive, online web application has been developed showcasing the results of the analysis and is located at aflow.org/aflow-chull. © 2018 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bezzubtsev2018,
author={Bezzubtsev, S. and Miroshnik, V. and Skobtsova, Y. and Smelyanskiy, R. and Vasin, V. and Volkanov, D. and Zhailauova, S.},
title={An Approach to the Construction of a Network Processing Unit},
journal={2018 International Scientific and Technical Conference Modern Computer Network Technologies, MoNeTeC 2018 - Proceedings},
year={2018},
doi={10.1109/MoNeTeC.2018.8572051},
art_number={8572051},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060469165&doi=10.1109%2fMoNeTeC.2018.8572051&partnerID=40&md5=5ea47adba2b8cbadb728deb567a04921},
abstract={In this paper we consider an approach to designing of a network processing unit (NPU). The paper contains description of proposed NPU architecture with functionally specific processing pipeline stages. It is designed for the set of common use case scenarios, but can be easily extended to other ones. We implemented a simulation model of the proposed NPU architecture using SystemC library and evaluated its performance. Model implementation uses SystemC library. NPU performance evaluation was carried out with simulation model of the proposed NPU architecture. © 2018 IEEE.},
author_keywords={computer architecture;  computer networks;  network processing unit;  simulation modeling;  software defined networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Farahmand2018193,
author={Farahmand, F. and Sharif, M.U. and Briggs, K. and Gaj, K.},
title={A High-Speed Constant-Time Hardware Implementation of NTRUEncrypt SVES},
journal={Proceedings - 2018 International Conference on Field-Programmable Technology, FPT 2018},
year={2018},
pages={193-200},
doi={10.1109/FPT.2018.00036},
art_number={8742269},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068323678&doi=10.1109%2fFPT.2018.00036&partnerID=40&md5=563291c88dd4cccab44fb491ca473f4b},
abstract={In this paper, we present a high-speed constant-Time hardware implementation of NTRUEncrypt Short Vector Encryption Scheme (SVES), fully compliant with the IEEE 1363.1 Standard Specification for Public Key Cryptographic Techniques Based on Hard Problems over Lattices. Our implementation follows an earlier proposed Post-Quantum Cryptography (PQC) Hardware Application Programming Interface (API), which facilitates its fair comparison with implementations of other PQC schemes. The paper contains the detailed flow and block diagrams, timing analysis, as well as results in terms of latency (in clock cycles), maximum clock frequency, and resource utilization in modern high-performance Field Programmable Gate Arrays (FPGAs). Our design takes full advantage of the ability to parallelize the major operation of NTRU, polynomial multiplication, in hardware. As a result, the execution time bottleneck shifts to the hash function, SHA-256, which is sequential in nature and as a result cannot be easily sped up in hardware. The obtained FPGA results for NTRU Encrypt SVES are compared with the equivalent results for Classic McEliece, a competing, well-established Post-Quantum Cryptography encryption scheme, with a long history of unsuccessful attempts at breaking. Our code for NTRUEncrypt SVES is being made open-source to speed-up further design-space exploration and benchmarking on multiple hardware platforms. © 2018 IEEE.},
author_keywords={API;  hardware;  lattice-based;  NTRU;  P1363.1},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao201839,
author={Zhao, T. and Huang, X.},
title={Design and implementation of DeepDSL: A DSL for deep learning},
journal={Computer Languages, Systems and Structures},
year={2018},
volume={54},
pages={39-70},
doi={10.1016/j.cl.2018.04.004},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046700910&doi=10.1016%2fj.cl.2018.04.004&partnerID=40&md5=234039f83774d2866958b2325574e72c},
abstract={Deep Learning (DL) has found great success in well-diversified areas such as machine vision, speech recognition, and multimedia understanding. However, the state-of-the-art tools (e.g. Caffe, TensorFlow, and CNTK), are programming libraries with many dependencies and implemented in languages such as C++ that need to be compiled to a specific runtime environment and require users to install the entire tool libraries for training or inference, which limits the portability of DL applications. In this work, we introduce DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles DL networks encoded with DeepDSL to efficient, compact, and portable Java source programs for DL training and inference. DeepDSL represents DL networks as abstract tensor functions, performs symbolic gradient derivations to generate Intermediate Representation (IR), optimizes the IR expressions, and translates the optimized IR expressions to Java code that runs on GPU without additional dependencies other than the necessary GPU libraries and the related invocation interfaces: a small set of JNI (Java Native Interface) wrappers. Our experiments show DeepDSL outperforms existing tools in several benchmark programs adopted from the current mainstream Deep Neural Networks (DNNs). © 2018 Elsevier Ltd},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Dorofeev201863,
author={Dorofeev, D. and Shestakov, S.},
title={2-tier vs. 3-tier architectures for data processing software},
journal={ACM International Conference Proceeding Series},
year={2018},
pages={63-68},
doi={10.1145/3274856.3274869},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058650397&doi=10.1145%2f3274856.3274869&partnerID=40&md5=5db3a864ee0f91b876ec1326d1045329},
abstract={The rise of data-centric computing, NoSQL and newSQL databases with powerful scripting capabilities, popularity of REST API raise a question: is it feasible to serve clients directly from the DB, with REST API server residing inside the database? What would be the balance between data processing, application, and presentation logic for such a scenario on a server side and on a client side? In this paper we will compare 2 real-world implementations of the commercial Luxms BI analytical platform based on 2-tier and on 3-tier architecture. Our research shows that despite popularity of 3-tier architectures, in-database application server approach delivers better performance in both throughput and latency in analytical client-server application development. © 2018 Copyright held by the owner/author(s).},
author_keywords={2-tier;  3-tier;  Benchmarks;  Data centric;  Data processing;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Akhoundan2018575,
author={Akhoundan, M.R. and Khademi, K. and Bahmanoo, S. and Wakil, K. and Mohamad, E.T. and Khorami, M.},
title={Practical use of computational building information modeling in repairing and maintenance of hospital building- case study},
journal={Smart Structures and Systems},
year={2018},
volume={22},
number={5},
pages={575-586},
doi={10.12989/sss.2018.22.5.575},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058214937&doi=10.12989%2fsss.2018.22.5.575&partnerID=40&md5=f8e3fb2d13f4335ad6e0aa4c0f65a768},
abstract={Computational Building Information Modeling (BIM) is an intelligent 3D model-based process that provides architecture, engineering, and construction professionals the insight to plan, design, construct, and manage buildings and infrastructure more efficiently. This paper aims at using BIM in Hospitals configurations protection. Infrastructure projects are classified as huge structural projects taking advantage of many resources such as finance, materials, human labor, facilities and time. Immense expenses in infrastructure programs should be allocated to estimating the expected results of these arrangements in domestic economy. Hence, the significance of feasibility studies is inevitable in project construction, in this way the necessity in promoting the strategies and using global contemporary technologies in the process of construction maintenance cannot be neglected. This paper aims at using the building information modeling in covering Imam Khomeini Hospital's equipment. First, the relationship between hospital constructions maintenance and repairing, using the building information modeling, is demonstrated. Then, using library studies, the effective factors of constructions' repairing and maintenance were collected. Finally, the possibilities of adding these factors in Revit software, as one of the most applicable software within BIM is investigated and have been identified in some items, where either this software can enter or the software for supporting the repairing and maintenance phase lacks them. The results clearly indicated that the required graphical factors in construction information modeling can be identified and applied successfully. Copyright © 2018 Techno-Press, Ltd.},
author_keywords={Computational Building Information Modeling (BIM);  Hospital construction;  Maintenance;  Repairing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Becker2018237,
author={Becker, P.H.E. and Sartor, A.L. and Brandalero, M. and Schneider Beck, A.C.},
title={BRAM-based function reuse for multi-core architectures in FPGAs},
journal={Microprocessors and Microsystems},
year={2018},
volume={63},
pages={237-248},
doi={10.1016/j.micpro.2018.09.007},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054339175&doi=10.1016%2fj.micpro.2018.09.007&partnerID=40&md5=5a0cd0101ed7407dec941fb51411b7a8},
abstract={Modern processors contain several specific hardware modules and multiple cores to ensure performance for a wide range of applications. In this context, FPGAs are frequently used as the implementation platform, since they offer architecture customization and fast time-to-market. However, many of them may not have the needed resources to implement all the necessary features, because of costs or complexity of the system to be implemented. When some needed functionalities do not fit in the target, they must be mapped into the much slower software domain. In this work, we exploit the fact these designs usually underuse their available BRAMs and propose a low-cost hardware-based function reuse mechanism for FPGAs, recovering some of the performance lost from the software part of applications that could not be implemented in hardware logic, with minimal impact on LUT usage. This is achieved by saving the inputs and outputs of the most frequently executed functions in a BRAM-based reuse table, so the next function executions with the same arguments can be skipped. This mechanism supports both precise and approximate modes and is evaluated with a 4-issue VLIW processor implemented in HDL, also considering a multi-core environment. Precise reuse, in single and multi-core scenarios, is assessed by running applications that use a software library to emulate floating point operations. Approximate reuse is evaluated over a single-core image-processing application that tolerates a certain level of error. Our scheme achieves 1.39 × geomean speedup in the precise single-core, while the multi-core case demonstrates application improvements from 1.25 × to 1.9 × when we start sharing the reuse table. In the approximate scenario, we achieve 1.52 × speedup with less than 10% error. © 2018 Elsevier B.V.},
author_keywords={FPGA;  Function-reuse;  Multi-core architectures;  Soft-processors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stachtiari201852,
author={Stachtiari, E. and Mavridou, A. and Katsaros, P. and Bliudze, S. and Sifakis, J.},
title={Early validation of system requirements and design through correctness-by-construction},
journal={Journal of Systems and Software},
year={2018},
volume={145},
pages={52-78},
doi={10.1016/j.jss.2018.07.053},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051117215&doi=10.1016%2fj.jss.2018.07.053&partnerID=40&md5=0aad1023612f203e6a19662c15eceeb1},
abstract={Early validation of requirements aims to reduce the need for the high-cost validation testing and corrective measures at late development stages. This work introduces a systematic process for the unambiguous specification of system requirements and the guided derivation of formal properties, which should be implied by the system ’s structure and behavior in conjunction with its external stimuli. This rigorous design takes place through the incremental construction of a model using the BIP (Behavior-Interaction-Priorities) component framework. It allows building complex designs by composing simpler reusable designs enforcing given properties. If some properties are neither enforced nor verified, the model is refined or certain requirements are revised. A validated model provides evidence of requirements’ consistency and design correctness. The process is semi-automated through a new tool and existing verification tools. Its effectiveness was evaluated on a set of requirements for the control software of the CubETH nanosatellite and an extract of software requirements for a Low Earth Orbit observation satellite. Our experience and obtained results helped in identifying open challenges for applying the method in industrial context. These challenges concern with the domain knowledge representation, the expressiveness of used specification languages, the library of reusable designs and scalability. © 2018},
author_keywords={Correctness-by-construction;  Model-based design;  Requirements formalization;  Rigorous system design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zapalowski2018125,
author={Zapalowski, V. and Nunes, I. and Nunes, D.J.},
title={The WGB method to recover implemented architectural rules},
journal={Information and Software Technology},
year={2018},
volume={103},
pages={125-137},
doi={10.1016/j.infsof.2018.06.012},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049334136&doi=10.1016%2fj.infsof.2018.06.012&partnerID=40&md5=8b101f7322e4be736961e208281daf3d},
abstract={Context: The identification of architectural rules, which specify allowed dependencies among architectural modules, is a key challenge in software architecture recovery. Existing approaches either retrieve a large set of rules, compromising their practical use, or are limited to supporting the understanding of such rules, which are manually recovered. Objective: To propose and evaluate a method to recover architectural rules, focusing on those implemented in the source code, which may differ from planned or conceptual rules. Method: We propose the WGB method, which analyzes dependencies among architectural modules as a graph, adding weights that correspond to the proposed module dependency strength (MDS) metric and identifies the set of implemented architectural rules by solving a mathematical optimization problem. We evaluated our method with a case study and an empirical study that compared rules extracted by the method with the conceptual architecture and source code dependencies of six systems. These comparisons considered efficiency and effectiveness of our method. Results: Regarding efficiency, our method took 45.55 s to analyze the largest system evaluated. Considering effectiveness, our method captured package dependencies as extracted rules with a reduction of 87.6%, on average, to represent this information. Using allowed architectural dependencies as a reference point (but not a gold standard), provided rules achieved 37.1% of precision and 37.8% of recall. Conclusion: Our empirical evaluation shows that the implemented architectural rules recovered by our method consist of abstract representations of (a large number of) module dependencies, providing a concise view of dependencies that can be inspected by developers to identify occurrences of architectural violations and undocumented rules. © 2018 Elsevier B.V.},
author_keywords={Architectural rule;  Architecture recovery;  Software architecture;  Source code dependency},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sani2018299,
author={Sani, M.J. and Rahman, A.A.},
title={GIS and BIM integration at data level: A review},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2018},
volume={42},
number={4/W9},
pages={299-306},
doi={10.5194/isprs-archives-XLII-4-W9-299-2018},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057621759&doi=10.5194%2fisprs-archives-XLII-4-W9-299-2018&partnerID=40&md5=9b5eda3a2081cc103fe1b9013910d22e},
abstract={City Geography Markup Language (CityGML) and Industry Foundation Class (IFC) are the two most popular data exchange format for the integration of Geographic Information System (GIS) and Building Information Modelling (BIM) respectively and has been identified by many researchers as an auspicious means of data interoperability between the two domains but with challenges on the compatibility between them. The main issue is the data loss in the process of information transformation. The success of integrating these two domains (GIS and BIM objects) is a great achievement toward solving problems in Architecture, Engineering and Construction (AEC), Facility Management (FM), Disaster Management (DM) sectors. Nevertheless, as we all know GIS and BIM are different fields used by different professionals using different software packages, used for different purposes, it is definitely face with many challenges including data interoperability, mismatch and loss of semantic information are bound to occur during the process of integration. In order to comprehend the two domains and their data models of CityGML and IFC. This paper review existing models on GIS and BIM developed by different researchers, the complementarity and compatibility of GIS and BIM on the previous integration techniques were also reviewed and finally, the paper review the integration of GIS and BIM at the data level aimed at solving different problems surrounding it by considering the transformation of coordinates at geometric level from CityGML to IFC, in order to achieve flow of information between GIS and BIM. © Authors 2018. CC BY 4.0 License.},
author_keywords={BIM;  CityGML;  Data Integration models;  IFC;  KEYWORD: GIS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhu20188,
author={Zhu, J. and Song, W. and Zhu, Z. and Ying, J. and Li, B. and Tu, B. and Shi, G. and Hou, R. and Meng, D.},
title={CPU security benchmark},
journal={Proceedings of the ACM Conference on Computer and Communications Security},
year={2018},
pages={8-14},
doi={10.1145/3267494.3267499},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056906587&doi=10.1145%2f3267494.3267499&partnerID=40&md5=3cc0602c5560e05c91f125f1c37452f2},
abstract={The current electronic-economy is booming, electronic-wallets, encrypted virtual-money, mobile payments, and other new generations of economic instruments are springing up. As the most important cornerstone, CPU is facing serious security challenges. And with the blowout of actual application requirements, the importance of CPU security testing is increasing. However, the actual security threats to computer systems are also becoming increasingly rampant (now attackers often use multiple different types of vulnerabilities to construct complex attack systems, not just a single attack chain). The traditional vulnerability detection model is not capable of comprehensive security assessment. We first proposed a comprehensive CPU Security Benchmark solution with high coverage for existing known vulnerabilities, including Undocumented Instructions detection, Control Flow Integrity test, Memory Errors detection, and Cache Side Channels detection, Out of Order and Speculative execution vulnerabilities (Meltdown and Spectre series) tests, and more. Our benchmark provides meaningful and constructive feedbacks for evading architecture/microarchitecture design flaws, system security (OS and libraries) software patches design, and user programming vulnerabilities tips. We hope that the work of this paper will promote the computer system security testing from the past scatter point and line mode (single specific vulnerability and attack chain testing) to coordinated and whole surface mode (multi-type vulnerabilities and attack network testing), thus creating a new research direction of the comprehensive and balanced CPU Security Benchmark. Our test suite will play an inspiring role in the comprehensive assessment of security in personal computer devices (PC/Mobile Phone) and large server clusters (Servers/Cloud), as well as the construction of more secure Block-Chain nodes (IOT), and many other practical applications. © 2018 Association for Computing Machinery.},
author_keywords={Comprehensive benchmark;  CPU security;  Vulnerability detection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Joshi2018855,
author={Joshi, K. and Bohara, V.A. and Aggarwal, P.},
title={Demo: Design and implementation of LTE advanced underlay device to device communication framework},
journal={Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM},
year={2018},
pages={855-857},
doi={10.1145/3241539.3267728},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056846993&doi=10.1145%2f3241539.3267728&partnerID=40&md5=56b302a96a2b676fd243f65c07e1caf6},
abstract={Device to Device (D2D) communication is one of the key enhancement of Long-Term Evolution (LTE) Advanced standard which is a precursor to fifth generation (5G) cellular standard. D2D has gained considerable attention due to several drawbacks associated with the existing mobile networks such as multi-hop transmission, higher latency, high power consumption and significant path loss. In an underlay D2D communication, both cellular and D2D users coexist by simultaneously sharing the licensed cellular spectrum. However, the performance of an underlay D2D communication is limited by the amount of interference from the cellular user(CU) to the D2D user and vice versa. In this paper, we design and develop a test-bed to evaluate the real-world performance of the underlay D2D communication framework. We demonstrate the impact of interference caused by CU on D2D receiver's throughput at different cellular angles and position with respect to the base station (BS). The demonstration has been performed on National Instruments Universal Software Radio Peripheral (USRP) RIO platform with LTE application framework module of LabVIEW communication system design suite as a supportive Application programming interface (API). © 2018 Association for Computing Machinery.},
author_keywords={CU;  D2D;  LTE;  RIO;  underlay;  USRP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khokhlov2018,
author={Khokhlov, I. and Jain, C. and Miller-Jacobson, B. and Heyman, A. and Reznik, L. and St.Jacques, R.},
title={MeetCI: A computational intelligence software design automation framework},
journal={IEEE International Conference on Fuzzy Systems},
year={2018},
volume={2018-July},
doi={10.1109/FUZZ-IEEE.2018.8491664},
art_number={8491664},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060497902&doi=10.1109%2fFUZZ-IEEE.2018.8491664&partnerID=40&md5=71a592b2495c7bc8ab276e1940780dea},
abstract={Computational Intelligence (CI) algorithms/techniques are packaged in a variety of disparate frameworks/applications that all vary with respect to specific supported functionality and implementation decisions that drastically change performance. Developers looking to employ different CI techniques are faced with a series of trade-offs in selecting the appropriate library/framework. These include resource consumption, features, portability, interface complexity, ease of parallelization, etc. Considerations such as language compatibility and familiarity with a particular library make the choice of libraries even more difficult. The paper introduces MeetCI, an open source software framework for computational intelligence software design automation that facilitates the application design decisions and their software implementation process. MeetCI abstracts away specific framework details of CI techniques designed within a variety of libraries. This allows CI users to benefit from a variety of current frameworks without investigating the nuances of each library/framework. Using an XML file, developed in accordance with the specifications, the user can design a CI application generically, and utilize various CI software without having to redesign their entire technology stack. Switching between libraries in MeetCI is trivial and accessing the right library to satisfy a user's goals can be done easily and effectively. The paper discusses the framework's use in design of various applications. The design process is illustrated with four different examples from expert systems and machine learning domains, including the development of an expert system for security evaluation, two classification problems and a prediction problem with recurrent neural networks. © 2018 IEEE.},
author_keywords={Computational intelligence libraries;  Computational intelligence techniques;  Software design automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2018,
title={ACM International Conference Proceeding Series},
journal={ACM International Conference Proceeding Series},
year={2018},
page_count={123},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055727028&partnerID=40&md5=8c7a5bacacb759669c114d1287b90782},
abstract={The proceedings contain 11 papers. The topics discussed include: large-scale empirical software engineering research using GitHub data; investigating the relationship between code smell agglomerations and architectural concerns: similarities and dissimilarities from distributed, service-oriented, and mobile systems; on identifying architectural smells in search-based product line designs; are you still smelling it? a comparative study between Java and Kotlin language; towards an automated product line architecture recovery: the Apo-games case study; cloning in customization classes: a case of a worldwide software product line; towards a taxonomy of software mediators for systems-of-systems; characteristics and performance assessment of approaches pre-rendering and isomorphic Javascript as a complement to SPA architecture; analysing the evolution of exception handling anti-patterns in large-scale projects: a case study; mining software repositories to identify library experts; an approach for creating KDM2PSM transformation engines in ADM context: the RUTE-K2J case; and on the use of metaprogramming and domain specific languages: an experience report in the logistics domain.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Moon2018855,
author={Moon, C.B. and Kim, B.M. and Kim, D.-S.},
title={Design Implementation of Real-Time Parallel Image Processing Scheme on Fire-Control System},
journal={International Conference on Ubiquitous and Future Networks, ICUFN},
year={2018},
volume={2018-July},
pages={855-858},
doi={10.1109/ICUFN.2018.8437022},
art_number={8437022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052536722&doi=10.1109%2fICUFN.2018.8437022&partnerID=40&md5=56e85659431d770d6db247958665d0d3},
abstract={In this paper, a parallel image-processing method based on OpenMP is proposed for a software application in a fire-control system(FCS). The proposed method for image-processing is composed of automatic and semi-manual detection methods respectively. In the automatic method, a target is detected automatically by the image-processing dynamically for moving target in CCD with IR for bright target. For the performance evaluation, the proposed method is compared with a CUDA and normal image processing. The simulation results show that, the proposed method has good performance in term of total image processing time in the FCS. © 2018 IEEE.},
author_keywords={CUDA;  Fire-Control System;  Military Application;  OpenMP;  Real-time Parallel image processing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Junger2018441,
author={Junger, D. and Hundt, C. and Schmidt, B.},
title={WarpDrive: Massively parallel hashing on multi-GPU nodes},
journal={Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium, IPDPS 2018},
year={2018},
pages={441-450},
doi={10.1109/IPDPS.2018.00054},
art_number={8425198},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052217108&doi=10.1109%2fIPDPS.2018.00054&partnerID=40&md5=59250c7e0b99925e0e56e173e7dac1bc},
abstract={Hash maps are among the most versatile data structures in computer science because of their compact data layout and expected constant time complexity for insertion and querying. However, associated memory access patterns during the probing phase are highly irregular resulting in strongly memory-bound implementations. Massively parallel accelerators such as CUDA-enabled GPUs may overcome this limitation by virtue of their fast video memory featuring almost one TB/s bandwidth in comparison to main memory modules of state-of-The-Art CPUs with less than 100 GB/s. Unfortunately, the size of hash maps supported by existing single-GPU hashing implementations is restricted by the limited amount of available video RAM. Hence, hash map construction and querying that scales across multiple GPUs is urgently needed in order to support structured storage of bigger datasets at high speeds. In this paper, we introduce WarpDrive-a scalable, distributed single-node multi-GPU implementation for the construction and querying of billions of key-value pairs. We propose a novel subwarp-based probing scheme featuring coalesced memory access over consecutive memory regions in order to mitigate the high latency of irregular access patterns. Our implementation achieves 1.4 billion insertions per second in single-GPU mode for a load factor of 0.95 thereby outperforming the GPU-cuckoo implementation of the CUDPP library by a factor of 2.8 on a P100. Furthermore, we present transparent scaling to multiple GPUs within the same node with up to 4.3 billion operations per second for high load factors on four P100 GPUs connected by NVLink technology. WarpDrive is free software and can be downloaded at https://github.com/sleeepyjack/warpdrive. © 2018 IEEE.},
author_keywords={CUDA;  Distributed;  GPU;  Hash table;  NVLINK},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Okunbor201859,
author={Okunbor, D.},
title={Software Implementation of LSFR-Based Stream Ciphers for GSM Cryptosystems},
journal={2018 7th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2018},
year={2018},
pages={59-65},
doi={10.1109/ICRITO.2018.8748397},
art_number={8748397},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069181885&doi=10.1109%2fICRITO.2018.8748397&partnerID=40&md5=63f27bf5564150397de14af33b4539d1},
abstract={Encrypted data transmissions through mobile communication channels are potentially vulnerable to cryptographic attacks since malicious agents may steal confidential information by running algorithms that can break the security of the encrypted data. Secure transfer of data through the combination of encryption and decryption, known as cipher, is the general approach to solving this problem in mobile communications. Specifically, stream ciphers do not suffer from propagation errors compared to other ciphers, and they tend to have better software efficiency. This research focuses on the applications of mobile communications for which stream ciphers are the natural choice. The research addresses the security of both Global System Mobile Communications (GSM) and Code Division Multiple Access (CDMA), the two most prominent standards. By strengthening such communication channels, conversations can be secured, data can be saved from interception, and cellular fraud can be prevented. We investigate many aspects of stream ciphers, particularly Linear Feedback Shift Registers (LFSR)-based stream ciphers. We develop comprehensive software libraries written in a prominent computer-programming language, with effective design of software interfaces. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ibem2018902,
author={Ibem, E.O. and Uwakonye, U.O. and Akpoiroro, G.O. and Somtochukwu, M. and Oke, C.A.},
title={Building information modeling (BIM) adoption in architectural firms in Lagos, Nigeria},
journal={International Journal of Civil Engineering and Technology},
year={2018},
volume={9},
number={9},
pages={902-915},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054414571&partnerID=40&md5=eade4b649658e68bfbfa4d040411c97f},
abstract={As Nigeria develops technologically, the use of building information modeling (BIM) in delivery of building and infrastructure projects cannot be overemphasized. In spite of the benefits of BIM in the construction industry, very little is known on the impact of BIM in architectural practices in Nigeria. Therefore, the aim of this study was to investigate the impact of BIM on architectural firms in Lagos, Southwest Nigeria. Among other things, the study investigated the level of awareness of BIM in architectural firms, the most commonly used BIM software packages, the aspects of architectural work supported by BIM and its benefits in architectural practice. The data were collected via a questionnaire survey of 110 architects in Lagos and analysed using descriptive statistics. The result shows that there is a high level of awareness of BIM among architects in Lagos and the most common BIM software packages used are Autodesk Revit Architecture, AUTOCAD, and Google Sketchup. The respondents used these software packages more in the preparation of 2D drawings, 3D visualization, architectural detailing, and modeling and less of analyses. The result also revealed that the use of BIM enhanced the overall productivity of architectural firms in the study area. To maximise the benefits of BIM in project delivery, the study recommends that specific programmes and policies be put in place by firms, professional associations and government aimed at improving the knowledge base of architects on BIM and promoting its industry wide adoption in Nigeria. IAEME Publication. © IAEME Publication.},
author_keywords={Architects;  BIM;  Construction industry;  Lagos;  Questionnaire survey},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen20181378,
author={Chen, C.-C. and Hung, M.-H. and Li, P.-Y. and Lin, Y.-C. and Liu, Y.-Y. and Cheng, F.-T.},
title={A Novel Automated Construction Scheme for Efficiently Developing Cloud Manufacturing Services},
journal={IEEE Robotics and Automation Letters},
year={2018},
volume={3},
number={3},
pages={1378-1385},
doi={10.1109/LRA.2018.2799420},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063309012&doi=10.1109%2fLRA.2018.2799420&partnerID=40&md5=8a329606caf3a5cd0425e17982013c22},
abstract={Cloud manufacturing (CMfg) has emerged as a next-generation manufacturing paradigm that has potential to revolutionize the manufacturing industry. In further promotion of CMfg, how to build CMfg services in an automatic and efficient manner is an essential and challenging subject. Currently, there is no literature addressing such issue. Aimed at facilitating rapid construction of CMfg services, this letter proposes a novel automated construction scheme for developing CMfg services, called Manufacturing Service Automated Construction Scheme (MSACS). First, we develop a three-phase workflow of MSACS to address the issues of how to construct CMfg services automatically using standalone software library package (SSLP). Next, we design a system architecture of MSACS to delineate how to implement MSACS. Then, we depict the designs of MSACS's core components. Finally, we apply MSACS to conduct industrial case studies to build the automatic virtual metrology cloud service and intelligent yield management cloud service for an intelligent manufacturing platform. Testing results demonstrate that MSACS can automatically construct the target CMfg services in a very efficient manner after uploading the required SSLPs. Thus, MSACS can significantly alleviate the burden of engineers in building CMfg services, and in turn can facilitate the promotion of CMfg. © 2016 IEEE.},
author_keywords={cloud manufacturing service;  fast automated construct scheme;  Intelligent and flexible manufacturing;  middleware and programming environments},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khomh2018151,
author={Khomh, F. and Abtahizadeh, S.A.},
title={Understanding the impact of cloud patterns on performance and energy consumption},
journal={Journal of Systems and Software},
year={2018},
volume={141},
pages={151-170},
doi={10.1016/j.jss.2018.03.063},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045462134&doi=10.1016%2fj.jss.2018.03.063&partnerID=40&md5=0d07058d3d463220ec67779d9373bb32},
abstract={Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application. © 2018 Elsevier Inc.},
author_keywords={Cloud patterns;  Energy consumption;  Energy efficiency;  Performance optimization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Manevich2018226,
author={Manevich, Y. and Barger, A. and Tock, Y.},
title={Poster: Service discovery for hyperledger fabric},
journal={DEBS 2018 - Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
year={2018},
pages={226-229},
doi={10.1145/3210284.3219766},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050563174&doi=10.1145%2f3210284.3219766&partnerID=40&md5=73b1525b58060aedc12717f1b9d31f7d},
abstract={Hyperledger Fabric (HLF) is a modular and extensible permissioned blockchain platform released to open-source and hosted by the Linux Foundation. The platform's design exhibits principles required by enterprise grade business applications like supply-chains, nancial transactions, asset management, food safety, and many more. For that end HLF introduces several innovations, two of which are smart contracts in general purpose languages (chaincode in HLF), and exible endorsement policies, which govern whether a transaction is considered valid. Typical blockchain applications are comprised of two tiers: the rst tier focuses on the modelling of the data schema and embedding of business rules into the blockchain by means of smart contracts (chaincode) and endorsment policies; and the second tier uses the SDK (Software Development Kit) provided by HLF to implement client side application logic. However there is a gap between the two tiers that hinders the rapid adoption of changes in the chaincode and endorsement policies within the client SDK. Currently, the chaincode location and endorsement policies are statically congured into the client SDK. This limits the reliability and availability of the client in the event of changes in the platform, and makes the platform more dicult to use. In this work we address and bridge the gap by describing the design and implementation of Service Discovery. Service Discovery provides APIs which allow dynamic discovery of the conguration required for the client SDK to interact with the platform, alleviating the client from the burden of maintaining it. This enables the client to rapidly adapt to changes in the platform, thus signicantly improving the reliability of the application layer. It also makes the HLF platform more consumable, simplifying the job of creating blockchain applications. © 2018 Copyright held by the owner/author(s).},
author_keywords={Blockchain;  Distributed Ledger;  Service Discovery},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Esteban2018912,
author={Esteban, F.J. and Díaz, D. and Hernández, P. and Caballero, J.A. and Dorado, G. and Gálvez, S.},
title={MC64-Cluster: Many-Core CPU Cluster Architecture and Performance Analysis in B-Tree Searches},
journal={Computer Journal},
year={2018},
volume={61},
number={6},
pages={912-925},
doi={10.1093/comjnl/bxx114},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048729820&doi=10.1093%2fcomjnl%2fbxx114&partnerID=40&md5=0d3350f501786534be19488e33b0cf20},
abstract={The MC64-Cluster computer platform was designed, based on many-core CPU microprocessors: Tile64. MC64-Cluster architecture was outlined in terms of both hardware and software, including commands available to manage jobs and provided application programming interfaces to communicate and synchronize tiles, making this system easy to use. Massively concurrent-searches of keys in B-trees, which are used in many applications, including bioinformatics, were used. Remarkable performance improvements were obtained when the cluster resources were combined with those available in host machine (hybrid or heterogeneous environments). These results were even more outstanding when analyzed in terms of performance-per-watt, highlighting their green-computing advantages. Together with the cluster architecture, they represent the main contributions of this work. To our knowledge, this is the first cluster implementation of this kind being developed. © The British Computer Society 2017. All rights reserved.},
author_keywords={B-tree;  bioinformatics;  green computing;  hardware architecture;  high-performance computing;  hybrid computing;  parallel programming},
document_type={Article},
source={Scopus},
}

@ARTICLE{Faik2018117,
author={Faik, S. and Tauschwitz, A. and Iosilevskiy, I.},
title={The equation of state package FEOS for high energy density matter},
journal={Computer Physics Communications},
year={2018},
volume={227},
pages={117-125},
doi={10.1016/j.cpc.2018.01.008},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042656585&doi=10.1016%2fj.cpc.2018.01.008&partnerID=40&md5=78083b6993b7719e53ead74fba6cd36c},
abstract={Adequate equation of state (EOS) data is of high interest in the growing field of high energy density physics and especially essential for hydrodynamic simulation codes. The semi-analytical method used in the newly developed Frankfurt equation of state (FEOS) package provides an easy and fast access to the EOS of – in principle – arbitrary materials. The code is based on the well known QEOS model (More et al., 1988; Young and Corey, 1995) and is a further development of the MPQeos code (Kemp and Meyer-ter Vehn, 1988; Kemp and Meyer-ter Vehn, 1998) from Max-Planck-Institut für Quantenoptik (MPQ) in Garching Germany. The list of features contains the calculation of homogeneous mixtures of chemical elements and the description of the liquid–vapor two-phase region with or without a Maxwell construction. Full flexibility of the package is assured by its structure: A program library provides the EOS with an interface designed for Fortran or C/C++ codes. Two additional software tools allow for the generation of EOS tables in different file output formats and for the calculation and visualization of isolines and Hugoniot shock adiabats. As an example the EOS of fused silica (SiO2) is calculated and compared to experimental data and other EOS codes. Program summary: Program Title: FEOS — Frankfurt equation of state Program Files doi: http://dx.doi.org/10.17632/6vjsv6v48p.1 Licensing provisions: GNU General Public License version 3 Programming language: C++ Supplementary material: Documentation/manual, exemplary input files for aluminum (Al) and fused silica (SiO2) Nature of problem: The description of a thermodynamic system — e.g. by the solution of the hydrodynamic conservation equations — presumes in the most cases reliable equation of state (EOS) data for different materials and for a wide range in temperature–density space. The FEOS code provides the required thermodynamic quantities like the pressure or the specific internal energy per unit mass as functions of density and temperature for, in principle, arbitrary materials, for single elements as well as for homogeneous mixtures of elements. FEOS is based on the well known QEOS model [2] and is a further development of the MPQeos code [1] from Max-Planck-Institut für Quantenoptik in Garching. Since the model was designed for the high energy density matter regimes, it can be applied e.g. to high-power laser or ion beam and inertial fusion science applications. The most important advantage of the FEOS package is an easy and fast access to materials which may not be available by more complex EOS codes. Solution method: In the QEOS model the thermodynamic quantities are derived from the specific Helmholtz free energy f=ϵ−Ts, which is composed of three contributions f=fe+fi+fb: (1) the uncorrected electronic part fe, calculated by a numerical scheme based on the simple Thomas–Fermi statistical model, (2) the ionic part fi, using the Cowan model [2], which employs analytical formulas to smoothly interpolate between the Debye solid, the normal solid and the liquid states, and (3) the semi-empirical bonding correction fb, which is included to compensate for the negligence of bonding forces in the simple Thomas–Fermi model. Although the total EOS is calculated for a single temperature T=Te=Ti, the user is free to calculate the ionic and the corrected electronic contributions independently with different temperatures. For homogeneous mixtures of elements the partial volumes of all element species k are iteratively adjusted in order to equilibrate the Thomas–Fermi pressures pe,k and to fulfill an additive volume rule for the electronic contribution. Furthermore, in the liquid–vapor two-phase region the model provides the (metastable) EOS with its characteristic features like van-der-Waals loops. The fully equilibrium EOS inside the two-phase region can be calculated by an iterative Maxwell construction scheme. Finally, despite the existence of the bonding correction, pressures near the critical point are often overestimated. Therefore, an improved cold curve can be applied to fit the location of the critical point to theoretical or experimental data. Additional comments: Besides the material's composition, the user must specify a reference density ρo and the bulk modulus Ko=ρ∂p∕∂ρs at a reference point (p,T)=(0,To) for a new material. The reference temperature To is usually chosen such that at p=0 the studied material is in the solid state. All fixed material parameters are stored in a material parameter database file which can be easily exchanged between users. The code is designed to calculate the equation of state within the following density and temperature limits: 10−7≤ρ∕ρo≤106, 10−4eV≤T≤106eV. Homogeneous mixtures with more than three elements may implicate numerical difficulties and/or uneconomical computing times. For temperatures close to and below the critical point one must be careful to check the accuracy of the model. If available, a more complex EOS in this regime is preferable. The FEOS package was designed to provide the best possible flexibility and therefore consists of three parts: (1) the FEOS library which contains all the routines for the calculation of the EOS and which provides a C/C++ as well as a Fortran interface for this purpose, (2) the FEOS table generation tool which accesses the FEOS library in order to generate EOS table files (e.g. in the SESAME database [3] format), and (3) the SHOWEOS table visualization tool which was developed to provide isotherms, isochores, isentropes, and Hugoniot curves from the FEOS or SESAME tables. Application of the code was first demonstrated in a publication on liquid–vapor metastable states in volumetrically heated matter [4]. [1] A. J. Kemp, J. Meyer-ter Vehn, An equation of state code for hot dense matter, based on the QEOS description, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 415 (3) (1998) 674–676. doi:10.1016/S0168-9002(98)00446-X. [2] R. M. More, K. H. Warren, D. A. Young, G. B. Zimmerman, A new quotidian equation of state (QEOS) for hot dense matter, Physics of Fluids 31 (1988) 3059. doi:10.1063/1.866963. [3] S. P. Lyon, J. D. Johnson, SESAME: The Los Alamos National Laboratory equation of state database, Tech. Rep. LA-UR-92-3407, Los Alamos National Laboratory (1992). [4] S. Faik, M. M. Basko, A. Tauschwitz, I. Iosilevskiy, J. A. Maruhn, Dynamics of volumetrically heated matter passing through the liquid–vapor metastable states, High Energy Density Physics 8 (4) (2012) 349–359. doi:10.1016/j.hedp.2012.08.003. © 2018 Elsevier B.V.},
author_keywords={Equation of state;  Homogeneous mixtures of elements;  Liquid–vapor two-phase region;  MPQeos code;  QEOS model;  Thomas–Fermi eos},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kaur2018543,
author={Kaur, P.J. and Kaushal, S. and Sangaiah, A.K. and Piccialli, F.},
title={A Framework for Assessing Reusability Using Package Cohesion Measure in Aspect Oriented Systems},
journal={International Journal of Parallel Programming},
year={2018},
volume={46},
number={3},
pages={543-564},
doi={10.1007/s10766-017-0501-6},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017137016&doi=10.1007%2fs10766-017-0501-6&partnerID=40&md5=f05bfc42d363a50165502ced20185976},
abstract={Due to better modularization of crosscutting concerns, the Aspect oriented programming approach enhances the quality of the system as it results in less complex and more readable implementation of the system. As the software applications grow in size and complexity, they require some kind of high level organization. For high level organization of software system, packages are required. A lot of work has been carried out for measuring cohesion in Aspect Oriented Systems (AOS) at class level but very less research has been done for designing package level cohesion metric. Package cohesion metrics plays an important role in analyzing quality of software at package level. According to object oriented design principle, a good software design must have high cohesion with high reusability. Thus a relationship must therefore exist between cohesion and reusability. Number of attempts has been made to evaluate effect of cohesion on external attributes but at class level only. Impact of package level cohesion metrics on reusability for AOS is not yet explored. (a) To implement the proposed package cohesion measure, PCohA, on AspectJ sample packages, (b) to theoretically validate the proposed measure and (c) to find the impact of package cohesion on measuring reusability for AOS. Theoretical validation has been done by proving its validity on four theorems given by Briand et al. For finding the impact of proposed measure on external attributes, correlation has been found between package cohesion, PCohA, and external attribute—reusability. After theoretical validation, it has been proved that the proposed measure is suitable for measuring cohesion at package level. Correlation between package cohesion metric (PCohA) and reusability is calculated by using Karl Pearson Product Moment correlation. The computed values show a strong positive relation between PCohA and Reusability. The proposed package cohesion measure is found to be a useful indicator of external quality factors such as reusability. The proposed metric is also established as a better predictor of code reusability than the existing cohesion measures. The work discussed in this paper can be used for designing high quality software by developing new package level metrics for other quality attributes such as maintainability, changeability etc. as a future work. © 2017, Springer Science+Business Media New York.},
author_keywords={Aspect Oriented System;  AspectJ;  Package cohesion;  Quality metrics;  Reusability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ufuktepe2018455,
author={Ufuktepe, E. and Tuglular, T.},
title={Estimating software robustness in relation to input validation vulnerabilities using Bayesian networks},
journal={Software Quality Journal},
year={2018},
volume={26},
number={2},
pages={455-489},
doi={10.1007/s11219-017-9359-5},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016124190&doi=10.1007%2fs11219-017-9359-5&partnerID=40&md5=3735e491f0f241a57cfc852e118c3035},
abstract={Estimating the robustness of software in the presence of invalid inputs has long been a challenging task owing to the fact that developers usually fail to take the necessary action to validate inputs during the design and implementation of software. We propose a method for estimating the robustness of software in relation to input validation vulnerabilities using Bayesian networks. The proposed method runs on all program functions and/or methods. It calculates a robustness value using information on the existence of input validation code in the functions and utilizing common weakness scores of known input validation vulnerabilities. In the case study, ten well-known software libraries implemented in the JavaScript language, which are chosen because of their increasing popularity among software developers, are evaluated. Using our method, software development teams can track changes made to software to deal with invalid inputs. © 2017, Springer Science+Business Media New York.},
author_keywords={Bayesian networks;  Input validation vulnerabilities;  Robustness},
document_type={Article},
source={Scopus},
}

@ARTICLE{Madden201891,
author={Madden, D. and Schneider, H. and Kohler, K.},
title={Multiscreen patterns - Interactions across the borders of devices},
journal={i-com},
year={2018},
volume={17},
number={1},
pages={91-98},
doi={10.1515/icom-2018-0008},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061234552&doi=10.1515%2ficom-2018-0008&partnerID=40&md5=1b16331405d3271ff8f20c3f6feebd86},
abstract={Design patterns are solutions for common design problems used in a number of felds including architecture, software development and user experience design. We compiled a pattern library for the usage of gesture-enabled interactions between different devices with screens, the so called multiscreen context. This library provides simple and intuitive gestures for connecting and disconnecting devices wirelessly as well as gestures for the exchange of data between these devices, like swiping a document from one's tablet to the tablets of the surrounding colleagues in the same room. The library is the result of a 2.5 years running project in cooperation with three small and medium companies. We compiled the library by conducting an intense literature survey in parallel with several iterations of practical software projects in the multiscreen context. The literature survey inspired the practice-oriented project work. Being involved in software projects enabled us to identify challenges in the design and implementation of the gesture set. Therefore, we generated valuable insight for a comprehensive description of gestural interactions. With our set of patterns, we aim to support interaction designers to choose the appropriate gesture for their given context. The patterns serve as inspiration by showing the different possibilities but also provide guidance how to design and implement a selected gesture for a given context. They help designing the details of an interaction by breaking it down into its smallest parts. To support the developers of these interactions our pattern descriptions are enriched with an Android library containing lifecycle events and the necessary gesture recognition logic. This paper provides an overview of the pattern library. In addition, the structure and the usage of the library is described in more detail with the means of one sample pattern. The pattern library is openly accessible. © 2011-2019 by Walter de Gruyter GmbH.},
author_keywords={Gestural Interactions;  Interaction Pattern;  Multiscreen;  Pattern Library},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Toumazi2018163,
author={Toumazi, A. and Comets, E. and Alberti, C. and Friede, T. and Lentz, F. and Stallard, N. and Zohar, S. and Ursino, M.},
title={dfpk: An R-package for Bayesian dose-finding designs using pharmacokinetics (PK) for phase I clinical trials},
journal={Computer Methods and Programs in Biomedicine},
year={2018},
volume={157},
pages={163-177},
doi={10.1016/j.cmpb.2018.01.023},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041428029&doi=10.1016%2fj.cmpb.2018.01.023&partnerID=40&md5=529ccd2c6ce849d6d41318736e94f4d5},
abstract={Background and objective: Dose-finding, aiming at finding the maximum tolerated dose, and pharmacokinetics studies are the first in human studies in the development process of a new pharmacological treatment. In the literature, to date only few attempts have been made to combine pharmacokinetics and dose-finding and to our knowledge no software implementation is generally available. In previous papers, we proposed several Bayesian adaptive pharmacokinetics-based dose-finding designs in small populations. The objective of this work is to implement these dose-finding methods in an R package, called dfpk. Methods: All methods were developed in a sequential Bayesian setting and Bayesian parameter estimation is carried out using the rstan package. All available pharmacokinetics and toxicity data are used to suggest the dose of the next cohort with a constraint regarding the probability of toxicity. Stopping rules are also considered for each method. The ggplot2 package is used to create summary plots of toxicities or concentration curves. Results: For all implemented methods, dfpk provides a function (nextDose) to estimate the probability of efficacy and to suggest the dose to give to the next cohort, and a function to run trial simulations to design a trial (nsim). The sim.data function generates at each dose the toxicity value related to a pharmacokinetic measure of exposure, the AUC, with an underlying pharmacokinetic one compartmental model with linear absorption. It is included as an example since similar data-frames can be generated directly by the user and passed to nsim. Conclusion: The developed user-friendly R package dfpk, available on the CRAN repository, supports the design of innovative dose-finding studies using PK information. © 2018 The Authors},
author_keywords={Dose-finding;  Maximum tolerated dose;  Pharmacokinetics;  Phase I clinical trials;  R package},
document_type={Article},
source={Scopus},
}

@ARTICLE{DAGAND2018,
author={DAGAND, P.-É. and TABAREAU, N. and TANTER, É.},
title={Foundations of dependent interoperability},
journal={Journal of Functional Programming},
year={2018},
doi={10.1017/S0956796818000011},
note={cited By 12; Article in Press},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043714054&doi=10.1017%2fS0956796818000011&partnerID=40&md5=755e8e84580b376333c6d34c881eb3a9},
abstract={Full-spectrum dependent types promise to enable the development of correct-by-construction software. However, even certified software needs to interact with simply-typed or untyped programs, be it to perform system calls, or to use legacy libraries. Trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simply-typed values can safely be coerced to dependent types and, conversely, dependently-typed programs can defensively be exported to a simply-typed application. In this article, we give a semantic account of dependent interoperability. Our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. Specifically, we develop the notions of type-theoretic partial Galois connections as a key foundation for dependent interoperability, which accounts for the partiality of the coercions between types. We explore the applicability of both type-theoretic Galois connections and anticonnections in the setting of dependent interoperability. A partial Galois connection enforces a translation of dependent types to runtime checks that are both sound and complete with respect to the invariants encoded by dependent types. Conversely, picking an anticonnection instead lets us induce weaker, sound conditions that can amount to more efficient runtime checks. Our framework is developed in Coq; it is thus constructive and verified in the strictest sense of the terms. Using our library, users can specify domain-specific partial connections between data structures. Our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. It thus becomes possible, as we shall illustrate, to internalize and hand-tune the extraction of dependently-typed programs to interoperable OCaml programs within Coq itself. Copyright © Cambridge University Press 2018},
document_type={Article in Press},
source={Scopus},
}

@ARTICLE{Adarme2018302,
author={Adarme, M. and Jabba Molinares, D.},
title={SEED: A software tool and an active-learning strategy for data structures courses},
journal={Computer Applications in Engineering Education},
year={2018},
volume={26},
number={2},
pages={302-313},
doi={10.1002/cae.21885},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032981870&doi=10.1002%2fcae.21885&partnerID=40&md5=280c367329670558afd4493807fa3e9f},
abstract={SEED is a software tool designed for Java consisting of a class library and a set of simulators to facilitate learning of the main data structures. The educational component is based on an active case-solving methodology accompanied by a pedagogical strategy. This strategy allows for the development of a multilayer-model-programming work for the construction of basic and advanced applications in real domains and for integration of its use, development and operation assessment with data structures. The evaluation results of SEED as a pedagogical mediator in the issue of binary trees is presented. The evaluation indicates that students prefer to use SEED due to its simplicity and attractive GUIs which facilitate data structures learning. © 2017 Wiley Periodicals, Inc.},
author_keywords={animation;  class library;  data structures},
document_type={Article},
source={Scopus},
}

@ARTICLE{Urban2018207,
author={Urban, G. and Subrahmanya, N. and Baldi, P.},
title={Inner and Outer Recursive Neural Networks for Chemoinformatics Applications},
journal={Journal of Chemical Information and Modeling},
year={2018},
volume={58},
number={2},
pages={207-211},
doi={10.1021/acs.jcim.7b00384},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042669209&doi=10.1021%2facs.jcim.7b00384&partnerID=40&md5=01a09c9e76c0e0e84e31077dcdb7fc62},
abstract={Deep learning methods applied to problems in chemoinformatics often require the use of recursive neural networks to handle data with graphical structure and variable size. We present a useful classification of recursive neural network approaches into two classes, the inner and outer approach. The inner approach uses recursion inside the underlying graph, to essentially "crawl" the edges of the graph, while the outer approach uses recursion outside the underlying graph, to aggregate information over progressively longer distances in an orthogonal direction. We illustrate the inner and outer approaches on several examples. More importantly, we provide open-source implementations [available at www.github.com/Chemoinformatics/InnerOuterRNN and cdb.ics.uci.edu] for both approaches in Tensorflow which can be used in combination with training data to produce efficient models for predicting the physical, chemical, and biological properties of small molecules. © 2018 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bende20181394,
author={Bende, R. and Kulkarni, A. and Khandare, A. and Bakade, K.},
title={Transceiver module of GPR for soil parameter measurement},
journal={Proceedings of the 2017 International Conference on Wireless Communications, Signal Processing and Networking, WiSPNET 2017},
year={2018},
volume={2018-January},
pages={1394-1397},
doi={10.1109/WiSPNET.2017.8299992},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046362622&doi=10.1109%2fWiSPNET.2017.8299992&partnerID=40&md5=46fda4a9607023a53c9d4708b004f132},
abstract={The proposed design and construction of a ground penetrating radar transceiver module is operating under frequency of 200 to 1200 MHz. The system describes the specifications of the radar, the design procedure used, the implementation of the radar transceiver, and measurements made to determine the performance of the transceiver. The objective is to design, simulate, test and validate the transceiver module by using COTS components to measure the properties of soil. After building the complete module the following parameters will be verified: penetration depth, unambiguous range, resolution, range resolution, scan rate. The purpose of proposed system is to provide information which can be used to determine the subsurface structure and location and ultimately lead to the identification of the subsurface feature. © 2017 IEEE.},
author_keywords={Frequency stepsize;  Penetration depth;  Range resolution;  Resolution;  Scan rate;  SFCW-GPR;  Unambiguous range},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nedovodeev2018126,
author={Nedovodeev, K.},
title={Adaptive libraries for multicore architectures with explicitly-managed memory hierarchies},
journal={Conference of Open Innovation Association, FRUCT},
year={2018},
volume={2012-April},
pages={126-135},
doi={10.23919/FRUCT.2012.8253116},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044742800&doi=10.23919%2fFRUCT.2012.8253116&partnerID=40&md5=aed92ed6cbf2b1925e03f61d12f1d80d},
abstract={Programming of commodity multicore processors is a challenging task and it becomes even harder when the processor has an explicitly-managed memory hierarchy (EMMA). Software libraries in the field of matrix algebra try to keep pace with this challenge by using the dataflow model of computation and constructing tiled algorithms. A new approach to high-performance software library construction is proposed, which moves scheduling decisions to compile-time and is portable between different EMMA platforms. Performance and scalability analyses both demonstrate promising results. Experiments demonstrate near linear speedup on a synthetic multicore architecture, incorporating up to 16 working computational cores. Performance of a generated code is competitive with vendor BLAS implementations for the Cell processor. © 2012 FRUCT.},
author_keywords={adaptive library;  BLAS;  explicitly-managed memory hierarchy},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Matskanis2018351,
author={Matskanis, N. and Deprez, J.-C. and Estievenart, F. and Ponsard, C.},
title={Systems for Electronic Evidence Handling and Exchange: Technical Issues and Findings Using a Proof of Concept Implementation},
journal={Law, Governance and Technology Series},
year={2018},
volume={39},
pages={351-374},
doi={10.1007/978-3-319-74872-6_17},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126059212&doi=10.1007%2f978-3-319-74872-6_17&partnerID=40&md5=2f06ae98570f866ce5c04ae22d519b75},
abstract={The EVIDENCE project aims at providing a roadmap (guidelines, recommendations and technical standards) for realising the missing Common European Framework for the systematic and uniform application of new technologies in the collection, use and exchange of Electronic Evidence. This chapter provides insights and expert’s recommendations for the roadmap regarding the software architecture design, the technologies, protocols and standards of software systems for digital evidence collection, handling and exchange. To this goal a software application and library prototypes have been developed that implement several of these protocols and have used the recommended representation language for applying a structure on the electronic evidence data, as well as metadata and for keeping the provenance of all steps of the digital forensic investigation lifecycle. © 2018, Springer International Publishing AG, part of Springer Nature.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Wei2018,
author={Wei, R. and Schwartz, L. and Adve, V.},
title={DLVM: A modern compiler infrastructure for deep learning systems},
journal={6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
year={2018},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083951068&partnerID=40&md5=5b0df1e48591ed2808ad6d62119bb4ba},
abstract={Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain-specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning. © 6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kazakov2018207,
author={Kazakov, K. and Morozov, S. and Semenov, V. and Zolotov, V.},
title={Software library for path planning in complex construction environments},
journal={eWork and eBusiness in Architecture, Engineering and Construction - Proceedings of the 12th European Conference on Product and Process Modelling, ECPPM 2018},
year={2018},
pages={207-214},
doi={10.1201/9780429506215-26},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079329540&doi=10.1201%2f9780429506215-26&partnerID=40&md5=addac75a6b34c6fc049dbc6d2d3b27d5},
abstract={Recently spatial-temporal (4D) modeling is becoming increasingly important in the construction industry due to better planning of project works. One of the advantages not reachable through the usage of traditional methods is the detection of spatial-temporal conflicts in construction schedules. In the paper mathematical methods and software tools for the detection of a special type of conflicts—path conflicts are discussed. A software library for path planning in complex construction environments with non-trivial topology and dynamic behavior is presented. The main attention is paid to its general organization, functionality and object-oriented design. Computational experiments confirm the efficiency of the library and its advanced capabilities for spatial-temporal validation of construction schedules against path conflicts. © 2018 Taylor & Francis Group, London.},
author_keywords={4D modeling;  Collision detection;  Motion planning;  Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Severyn2018339,
author={Severyn, V. and Pashchenko, A. and Mytrofanov, P.},
title={Probabilistic analysis of structures under snow load},
journal={International Journal of Engineering and Technology(UAE)},
year={2018},
volume={7},
number={3},
pages={339-342},
doi={10.14419/ijet.v7i3.2.14431},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070990456&doi=10.14419%2fijet.v7i3.2.14431&partnerID=40&md5=7b9f8666271b0f775d48a019d5789a14},
abstract={The authors of the article present a model of snow load on the construction of buildings and structures for the territory of Ukraine. This model can be used for others countries of the world. Numeric probabilistic models of snow load have been worked out for PC. The digital snow load model was developed based on observations of many Ukrainian weather stations. This model is implemented as a software package in the programming language C ++. The results of snow load simulation can be used to assess the reliability of building structures. Finally, the implementation of this algorithm will allow to calculate the reliability of a number of existing designs. The received reliability forecasts will allow to reveal potentially dangerous types of building structures and to justify on their basis a proposal to improve Code calculations. © 2018 Authors.},
author_keywords={Numeric probabilistic models;  Probabilistic analysis;  Snow load},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vu2018404,
author={Vu, H. and Fertig, T. and Braun, P.},
title={Automation of integration testing of restful hypermedia systems: A model-driven approach},
journal={WEBIST 2018 - Proceedings of the 14th International Conference on Web Information Systems and Technologies},
year={2018},
pages={404-411},
doi={10.5220/0006932004040411},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059035673&doi=10.5220%2f0006932004040411&partnerID=40&md5=4f2746ca55616752943de090288a724d},
abstract={The proper design of Representational State Transfer (REST) APIs is not trivial because developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature. Especially hypermedia testing is not mentioned at all. Manual hypermedia testing is time-consuming and hard to maintain. Testing a hypermedia API requires many test cases that have similar structure, especially when different user roles and error cases are considered. In order to tackle this problem, we proposed a Model-driven Testing (MDT) approach for hypermedia systems using the metamodel within our existing Model Driven Software Development (MDSD) approach. This work discusses challenges and results of hypermedia testing for RESTful APIs using MDT techniques that were discovered within our research. MDT allows white-box testing, hence covering complete program structure and behavior of the generated application. By doing this, we are able to achieve a high automated test coverage. Moreover, any runtime behavior deviated from the metamodel reveals bugs within the generators. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved},
author_keywords={Hypermedia testing;  Integration testing;  MDE;  MDSD;  MDT;  Model-driven testing;  REST;  RESTful API},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dar201863859,
author={Dar, H. and Lali, M.I. and Ashraf, H. and Ramzan, M. and Amjad, T. and Shahzad, B.},
title={A systematic study on software requirements elicitation techniques and its challenges in mobile application development},
journal={IEEE Access},
year={2018},
volume={6},
pages={63859-63867},
doi={10.1109/ACCESS.2018.2874981},
art_number={8513829},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055714786&doi=10.1109%2fACCESS.2018.2874981&partnerID=40&md5=cb93c2c0b1073912e1b3c734847c0bc8},
abstract={Software Requirements Engineering has paved its roots in both industry and academia, as today's complex systems are programmed to provide efficient user-centric functionalities. This also refers to the emergence of challenges in Requirements Elicitation techniques, approaches, and tools while performing them. Particularly, in the area of Requirements Engineering for software development, a number of techniques and approaches have been observed in literature but for mobile application development, which is different from the traditional software development, has not been discussed much in past studies. Short development cycle, device limitations, and less development time for mobile application development are some of the issues to which there is no 'silver bullet' available. Therefore, the Requirement Analysts are in dire need of defined guidelines for Requirement Elicitation in mobile application development. With this study, we aim to provide a detailed overview of Requirements Elicitation techniques and its challenges. We have conducted a systematic literature review by surveying 4507 initial and 36 primary studies. A comprehensive set of 22 elicitation techniques were measured based on quality assessment criteria, including time and cost factors, resource effectiveness, and domain understanding. Furthermore, the challenges in Requirements Elicitation were also grouped into eight different categories based on their applicability. Our study effectively contributes in highlighting Software Requirements Elicitation Techniques and its challenges in mobile application development. © 2013 IEEE.},
author_keywords={Android application;  Requirement elicitation;  Requirement gathering;  Requirements engineering;  Software development life cycle;  Software engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Manotas2018278,
author={Manotas, I. and Clause, J. and Pollock, L.},
title={Exploring evolutionary search strategies to improve applications’ energy efficiency},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11036 LNCS},
pages={278-292},
doi={10.1007/978-3-319-99241-9_15},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053153361&doi=10.1007%2f978-3-319-99241-9_15&partnerID=40&md5=737c1ba2544dd5bceacac14f804739b6},
abstract={Energy consumption have become an important non-functional requirement for applications running on battery powered devices through data centers. Despite the increased interest on detecting and understanding what causes an application to be energy inefficient, few works focus on helping developers to automatically make their applications more energy efficient based on developers’ design and implementation decisions. This paper explores how search strategies based on genetic algorithms can help developers automatically find an energy efficient version of an application based on transformations corresponding to developers’ high level decisions (e.g., selecting API implementations). Our results show how different search strategies can help to improve the energy efficiency for nine Java applications. © Springer Nature Switzerland AG 2018.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{McBride20181783,
author={McBride, K. and Linke, S. and Xu, S. and Sundmacher, K.},
title={Computer Aided Design of Green Thermomorphic Solvent Systems for Homogeneous Catalyst Recovery},
journal={Computer Aided Chemical Engineering},
year={2018},
volume={44},
pages={1783-1788},
doi={10.1016/B978-0-444-64241-7.50292-5},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050611490&doi=10.1016%2fB978-0-444-64241-7.50292-5&partnerID=40&md5=b2ebca2714daa97ca59cb9469bea20f6},
abstract={This work presents a new computer-aided thermomorphic solvent system (TMS) design methodology for the recovery of homogeneous catalysts that incorporates quantitative structure-activity relationships (QSAR) to predict various environmental, health, and safety (EHS) criteria using modern software packages. The quantum chemical method Conductor-like Screening Model for Real Solvents (COSMO-RS) (Klamt, 1995) is used for predicting catalyst solubility and the liquid-liquid equilibrium behavior of potential TMS designs. This methodology is then exemplified on the hydroformylation of 1- decene using the rhodium-Biphephos (Rh-BPP) transition metal catalyst and several green TMS designs were identified. Two of these TMS were then selected for experimental validation of both their LLE behavior and reaction performance. © 2018 Elsevier B.V.},
author_keywords={computer-aided molecular design (CAMD);  COSMO-RS;  green solvents;  homogeneous catalysis},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Arrington2018139,
author={Arrington, K. and Leventhal, A. and Murray, K.},
title={Architecture, design & engineering - archiving digital assets: Past, present and future},
journal={IS and T Archiving Conference, ARCHIVING 2018},
year={2018},
volume={2018-April},
pages={139-142},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049245606&partnerID=40&md5=f8e492ee6e4b2ea9d701410c7d701f69},
abstract={For decades, design in the worlds of architecture, design and engineering have been digital and the software tools to support the work operate under a business model of rapid change and proprietary output. This paper reports on the outcome of a two day Summit held at the Library of Congress in November 2017 (Designing the Future Landscape: Digital Architecture, Design & Engineering Assets) bringing together creators, archivists, researchers, project managers, and standards and guidelines developers to illuminate the issues and challenges for preserving and accessing this work product, to explore new research possibilities created by design as data, and to identify initiatives contributing to addressing issues of preservation and access. Like the event itself, this paper hopes to increase awareness of the challenges and issues, and to share and encourage actions and collaborations for preserving this material. An in-depth consolidation of the themes and issues from the Summit can be found in the report written by Aliza Leventhal for the Library of Congress released in March 2018 entitled: Designing the Future Landscape: Digital Architecture, Design & Engineering Assets. © 2018 Society for Imaging Science and Technology. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Saramud201891,
author={Saramud, M.V. and Kovalev, I.V. and Losev, V.V. and Karaseva, M.V. and Kovalev, D.I.},
title={On the application of a modified ant algorithm to optimize the structure of a multiversion software package},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10941 LNCS},
pages={91-100},
doi={10.1007/978-3-319-93815-8_10},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049090250&doi=10.1007%2f978-3-319-93815-8_10&partnerID=40&md5=941bb010698882538dc7539f6096f855},
abstract={The article considers the possibility of applying an optimization algorithm based on the behavior of an ant colony to the problem of forming a multiversion fault-tolerant software package. The necessary modifications of the basic algorithm and a model of graph construction for the implementation of the ant algorithm for the chosen problem are proposed. The optimization takes into account such features as cost, reliability and evaluation of the successful implementation of each version with the specified characteristics. A certain combination of versions in each module affects the characteristics of the module, and each characteristic of the module affects the characteristics of the system, so it is important to choose the optimal structure for each module to ensure the required characteristics of the system as a whole. The program system that implements the proposed algorithm is considered. The simulation results obtained with the help of the proposed software tool are considered. The results confirm the applicability of the ant algorithms to the problem of forming a multiversion software package, and they show their effectiveness. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={Ant algorithm;  Multiversion programming;  Optimization;  Reliability;  Software redundancy},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ferreira2018165,
author={Ferreira, R. and Braga, M. and Alves, V.},
title={Forecast in the pharmaceutical area – Statistic models vs deep learning},
journal={Advances in Intelligent Systems and Computing},
year={2018},
volume={747},
pages={165-175},
doi={10.1007/978-3-319-77700-9_17},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045330495&doi=10.1007%2f978-3-319-77700-9_17&partnerID=40&md5=f67ff91b63899ca49c15735c1776869e},
abstract={The main goal of this work was to evaluate the application of statistical and connectionist models for the problem of pharmacy sales forecasting. Since R is one of the most used software environment for statistical computation, we used the functions presented in its forecast package. These functions allowed for the construction of models that were then compared with the models developed using Deep Learning algorithms. The Deep Learning architecture was constructed using Long Short-Term Memory layers. It is very common to use statistical models in time series forecasting, namely the ARIMA model, however, with the arising of Deep Learning models our challenge was to compare the performance of these two approaches applied to pharmacy sales. The experiments studied, showed that for the used dataset, even a quickly developed LSTM model, outperformed the long used R forecasting package ARIMA model. This model will allow the optimization of stock levels, consequently the reduction of stock costs, possibly increase the sales and the optimization of human resources in a pharmacy. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={ARIMA;  Deep learning;  Forecast;  LSTM;  Pharmacy sales},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ochieng2018527,
author={Ochieng, E.G. and Ovbagbedia, O.O. and Zuofa, T. and Abdulai, R. and Matipa, W. and Ruan, X. and Oledinma, A.},
title={Utilising a systematic knowledge management based system to optimise project management operations in oil and gas organisations},
journal={Information Technology and People},
year={2018},
volume={31},
number={2},
pages={527-556},
doi={10.1108/ITP-08-2016-0198},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044615786&doi=10.1108%2fITP-08-2016-0198&partnerID=40&md5=725ee464f5afacfc78881f11d157fc61},
abstract={Purpose: The purpose of this paper is to examine the efficacy of knowledge management (KM) based systems and best practices that could be used to address operational issues in the oil and gas sector. Design/methodology/approach: Given little was known empirically about the strategies and practices which contribute to improved performance, innovation and continuous improvement in the oil and gas sector qualitative method was used. Semi-structured interviews were used to derive senior managers’ constructs of project delivery efficiency and KM based systems. The interviews were analysed through the use of a qualitative analysis software package NUDIST NVivoTM. Participants were selected using purposive sampling. Validity and reliability were achieved by first assessing the plausibility in terms of already existing knowledge on some of the operational issues raised by participants. Findings: These were synthesised into a framework capturing seven well-defined stages. All these steps emerged as being related; they are comprised of independent variables. These steps were found to comprise of knowledge management technology approaches, knowledge management people approaches, KM strategies and value enhancing practices. Research limitations/implications: Although the findings are pertinent to oil and gas organisations, it will be important to conduct follow-up research validating the potential for using the results of this study to establish frameworks for knowledge and information management in different organisations and contexts. This will provide not only data about the validity of the framework in generic terms but will also generate additional data on the application of KM strategy. Practical implications: As shown in this study, successful KM based systems requires the aligning of business strategy, technology for KM, project management operations with an enterprise knowledge-sharing culture. Such sharing requires managing the behaviour of project personnel such that knowledge transfer becomes part of the organisation’s norm. Social implications: The implementation of KM based systems requires deliberate planning and action to create the conditions for success and put in place the strategy, leadership, goals, process, skills, systems, issue resolution, and structure to direct and exploit the dynamic nature of project work. The strategies proposed in this research cannot be expected to resolve all KM issues in the oil and gas sector. However, their use defines an approach that is superior to the traditional approaches typically adopted and consequently merits far wider application. Originality/value: The proposed framework presents a better way of optimising the performance of project-based operations thus enabling oil and gas organisations to reform their poor performance on projects and empower them to better manage emerging cultural challenges in their future projects. Reflecting on their experiences, the participants confirmed that the proposed KM framework and its seven well-defined stages were central to the effectiveness of KM in oil and gas operations. Although the scope of this research was restricted to projects in Nigeria and the UK, the geographical focus of this research does not invalidate these results with respect to other countries. The fact is that the oil and gas sector globally shares some common fundamental characteristics. © 2018, Emerald Publishing Limited.},
author_keywords={Cross-cultural issues;  Decision making;  Developing countries;  IT project management;  Knowledge adoption;  Knowledge integration;  Knowledge management systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pleshkova2018105,
author={Pleshkova, S. and Bekiarski, A.},
title={Development of fast parallel algorithms based on visual and audio information in motion control systems of mobile robots},
journal={Intelligent Systems Reference Library},
year={2018},
volume={136},
pages={105-138},
doi={10.1007/978-3-319-67994-5_5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032591050&doi=10.1007%2f978-3-319-67994-5_5&partnerID=40&md5=ab0f120ef9cb562046a4209f790a7108},
abstract={Decision making for movement is one of the essential activities in motion control systems of mobile robots. It is based on methods and algorithms of data processing obtained from the mobile robot sensors, usually video and audio sensors, like video cameras and microphone arrays. After image processing, information about the objects and persons including their current positions in area of mobile robot observation can be obtained. The aim of methods and algorithms is to achieve the appropriate precision and effectiveness of mobile robot’s visual perception, as well as the detection and tracking of objects and persons applying the mobile robot motion path planning. The precision in special cases of visual speaking person’s detection and tracking can be augmented adding the information of sound arrival in order to receive and execute the voice commands. There exist algorithms using only visual perception and attention or also the joined audio perception and attention. These algorithms are usually tested in the most cases as simulations and cannot provide a real time tracking objects and people. Therefore, the goal in this chapter is to develop and test the fast parallel algorithms for decision making in the motion control systems of mobile robots. The depth analysis of the existing methods and algorithms was conducted, which provided the main ways to increase the speed of an algorithm, such as the optimization, simplification of calculations, applying high level programming languages, special libraries for image and audio signal processing based on the hybrid hardware and software implementations, using processors like Digital Signal Processor (DSP) and Field-Programmable Gate Array (FPGA). The high speed proposed algorithms were implemented in the parallel computing multiprocessor hardware structure and software platform using the well known NVIDIA GPU processor and GUDA platform, respectively. The experimental results with different parallel structures confirm the real time execution of algorithms for the objects and speaking person’s detection and tracking using the given mobile robot construction. © 2018, Springer International Publishing AG.},
author_keywords={CUDA;  GPU;  Mobile robot;  Motion control system;  Parallel algorithm;  Visual and audio decision making;  Visual and audio perception and attention},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Izraelevitz2017209,
author={Izraelevitz, A. and Koenig, J. and Li, P. and Lin, R. and Wang, A. and Magyar, A. and Kim, D. and Schmidt, C. and Markley, C. and Lawson, J. and Bachrach, J.},
title={Reusability is FIRRTL ground: Hardware construction languages, compiler frameworks, and transformations},
journal={IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
year={2017},
volume={2017-November},
pages={209-216},
doi={10.1109/ICCAD.2017.8203780},
note={cited By 86},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043501935&doi=10.1109%2fICCAD.2017.8203780&partnerID=40&md5=1b67bc2183aaa5362bea4d9f6d2fa469},
abstract={Enabled by modern languages and retargetable compilers, software development is in a virtual 'Cambrian explosion' driven by a critical mass of powerfully parameterized libraries; but hardware development practices lag far behind. We hypothesize that existing hardware construction languages (HCLs) and novel hardware compiler frameworks (HCFs) can put hardware development on a similar evolutionary path by enabling new hardware libraries to be independent of underlying process technologies including FPGA mappings. We support this claim by (1) evaluating the degree with which Chisel, an existing HCL, can support powerfully parameterized libraries, and (2) introducing the concept and implementation of an HCF that uses an open-source hardware intermediate representation, FIRRTL (Flexible Intermediate Representation for RTL), to transform target-independent RTL into technology-specific RTL. Finally, we evaluate many hardware compiler transformations, including simplifying transformations, analyses, optimizations, instrumentations, and specializations, which demonstrate the power of a combined HCL and HCF approach. © 2017 IEEE.},
author_keywords={ASIC;  Chisel;  Compiler;  Design;  FIRRTL;  FPGA;  Hardware;  Hardware Construction Language;  Hardware Design Language;  Intermediate Representation;  Modeling;  Reusability;  RTL;  Transformations},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2017947,
author={Li, Q. and Liang, D.},
title={Design of high voltage power supply control system of stationary digital breast tomosynthesis based on NI compactrio},
journal={Proceedings - 2017 4th International Conference on Information Science and Control Engineering, ICISCE 2017},
year={2017},
pages={947-951},
doi={10.1109/ICISCE.2017.200},
art_number={8110427},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040613505&doi=10.1109%2fICISCE.2017.200&partnerID=40&md5=47257e9b9ae1b586c85997f31b581375},
abstract={In the Stationary Digital Breast Tomosynthesis(s-DBT) system, the role of the focusing electrode is to obtain clearer scanned images by focusing the electrons emitted by the carbon nanotubes. As one part of the control system of the s-DBT, the real-time control system of the focusing electrode high voltage power supply is designed. The control system using the NI CompactRIO device and LabVIEW as platform. The hardware unit mainly includes NI CompactRIO 9068, analog output module NI 9263 and analog acquisition module NI 9201. The software design includes the allocation of I/O ports, the choice of CompactRIO development architecture, the design of the power supply control's flow chart and the generation of dynamic link library. Through the construction of the test platform, the control system's feasibility and reliability were tested, analysis of the experimental data shows that the control system's relative error of the high voltage power supply's voltage control is less than 0.5%. © 2017 IEEE.},
author_keywords={Control system;  High voltage power supply;  NI compactrio;  Realtime},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rusnak2017,
author={Rusnak, P. and Rabcan, J.},
title={The software library used in teaching of multiple-valued logic and logic function},
journal={ICETA 2017 - 15th IEEE International Conference on Emerging eLearning Technologies and Applications, Proceedings},
year={2017},
doi={10.1109/ICETA.2017.8102524},
art_number={8102524},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040791990&doi=10.1109%2fICETA.2017.8102524&partnerID=40&md5=57fd41f1688fd12704b8919cc8e299da},
abstract={Multiple-valued logic (MVL) is used in various fields of knowledge such as logic, philosophy, logic circuits design, reliability engineering or artificial intelligence. MVL is generalization of well-known binary logic and unlike binary logic MVL allows operating with a larger number of truth degrees and provides numerous tools for problem formalizing and solving. In order to describe relationships between inputs and the result of certain phenomena in the MVL a mapping of n multi-valued inputs into m-valued output called MVL-function is needed. The MVL-function can be used for example in electrical engineering, computer engineering, logic or reliability engineering. MVL-function is a fundamental element of MVL. Therefore a software library for working with MVL and MVL-functions is needed for implementation of numerous applied problems. The software implementation of demanding process such as symbolic calculations with MVL-functions is hard. New software library for working with MVL and logic functions is introduced in this paper. This software library is programmed in C++ for fast and efficient performance. It can be used for helping student with symbolic calculations of MVL-function or as a support library for complex programs, which need to work with logic function. © 2017 IEEE.},
author_keywords={direct partial logic derivative;  logic function;  multiple-valued logic;  software library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boonchuay2017668,
author={Boonchuay, K. and Intasorn, Y. and Rattanaopas, K.},
title={Design and implementation a REST API for association rule mining},
journal={ECTI-CON 2017 - 2017 14th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology},
year={2017},
pages={668-671},
doi={10.1109/ECTICon.2017.8096326},
art_number={8096326},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039920456&doi=10.1109%2fECTICon.2017.8096326&partnerID=40&md5=1228f7075f6a8b01eb42c5d36bddf3c0},
abstract={Association rule mining is one of the popular topics in data mining. It can be applied with various types of applications. In these days, an organization applies multiple software applications to manage its jobs. These applications are also based on several types of platforms. Hence, an interoperability software development using web service becomes one of popular topics in nowadays. In this paper, we emphasized the idea of design and implementation association rule mining web service for a small or medium organization having various software applications on multiple platforms. This service is based on a web service architecture called REST API. The design is intended to be simple. All development tools are also non-commercial software. Therefore, our design in this paper can be easily applied with organization's applications. Our service is tested by using three datasets. According to the result, the processing time depends on the number of transaction in a dataset. From our testing, the service can support a large dataset having 100,000 transactions. © 2017 IEEE.},
author_keywords={association rule mining;  REST API;  web services},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chien2017564,
author={Chien, A. and Balaji, P. and Dun, N. and Fang, A. and Fujita, H. and Iskra, K. and Rubenstein, Z. and Zheng, Z. and Hammond, J. and Laguna, I. and Richards, D. and Dubey, A. and van Straalen, B. and Hoemmen, M. and Heroux, M. and Teranishi, K. and Siegel, A.},
title={Exploring versioned distributed arrays for resilience in scientific applications: global view resilience},
journal={International Journal of High Performance Computing Applications},
year={2017},
volume={31},
number={6},
pages={564-590},
doi={10.1177/1094342016664796},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035022545&doi=10.1177%2f1094342016664796&partnerID=40&md5=604e0ce46db4c0249b6aacca162bdcd4},
abstract={Exascale studies project reliability challenges for future HPC systems. We present the Global View Resilience (GVR) system, a library for portable resilience. GVR begins with a subset of the Global Arrays interface, and adds new capabilities to create versions, name versions, and compute on version data. Applications can focus versioning where and when it is most productive, and customize for each application structure independently. This control is portable, and its embedding in application source makes it natural to express and easy to maintain. The ability to name multiple versions and “partially materialize” them efficiently makes ambitious forward-recovery based on “data slices” across versions or data structures both easy to express and efficient. Using several large applications (OpenMC, preconditioned conjugate gradient (PCG) solver, ddcMD, and Chombo), we evaluate the programming effort to add resilience. The required changes are small (< 2% lines of code (LOC)), localized and machine-independent, and perhaps most important, require no software architecture changes. We also measure the overhead of adding GVR versioning and show that overheads < 2% are generally achieved. This overhead suggests that GVR can be implemented in large-scale codes and support portable error recovery with modest investment and runtime impact. Our results are drawn from both IBM BG/Q and Cray XC30 experiments, demonstrating portability. We also present two case studies of flexible error recovery, illustrating how GVR can be used for multi-version rollback recovery, and several different forward-recovery schemes. GVR’s multi-version enables applications to survive latent errors (silent data corruption) with significant detection latency, and forward recovery can make that recovery extremely efficient. Our results suggest that GVR is scalable, portable, and efficient. GVR interfaces are flexible, supporting a variety of recovery schemes, and altogether GVR embodies a gentle-slope path to tolerate growing error rates in future extreme-scale systems. © 2016, © The Author(s) 2016.},
author_keywords={application-based fault tolerance;  exascale;  fault-tolerance;  Resilience;  scalable computing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Briola20171773,
author={Briola, D. and Deufemia, V. and Mascardi, V. and Paolino, L.},
title={Agent-oriented and ontology-driven digital libraries: the IndianaMAS experience},
journal={Software - Practice and Experience},
year={2017},
volume={47},
number={11},
pages={1773-1799},
doi={10.1002/spe.2494},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014921914&doi=10.1002%2fspe.2494&partnerID=40&md5=2a51cf87e2a3f18f556487a3f952079a},
abstract={This paper describes IndianaMAS, a multiagent system able to automatically classify and manage images, sketches, and multilingual documents in a cultural heritage domain. The latter has been formalized by means of an ontology, which enables the semantic integration of heterogeneous data from different sources, drives the agent communication with the internal and external environment, and provides an abstract and human-readable interface between the system and the user. IndianaMAS is able to expose to the world the classified data via a digital library. Modularity and reusability are the key engineering principles followed in the system design and implementation. We present the details of the IndianaMAS system and discuss how its architecture can be generalized to create – with the minimal effort – systems addressing similar classification, storage, and management problems, but operating in different domains and driven by different ontologies. The concrete problems we faced and their solutions are described to share our lesson learned and, at the same time, to show the applicability and reusability of our modular approach based on ontologies, agents, and digital libraries. Copyright © 2017 John Wiley & Sons, Ltd. Copyright © 2017 John Wiley & Sons, Ltd.},
author_keywords={digital libraries;  intelligent software agents;  modularity;  ontologies;  restful programming;  reusability;  rock art},
document_type={Article},
source={Scopus},
}

@ARTICLE{Azzawi2017,
author={Azzawi, W.A. and Epaarachchi, J.A. and Islam, M. and Leng, J.},
title={Implementation of a finite element analysis procedure for structural analysis of shape memory behaviour of fibre reinforced shape memory polymer composites},
journal={Smart Materials and Structures},
year={2017},
volume={26},
number={12},
doi={10.1088/1361-665X/aa928e},
art_number={125002},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034752972&doi=10.1088%2f1361-665X%2faa928e&partnerID=40&md5=fc0b879973f71531e9903ca0233ad1eb},
abstract={Shape memory polymers (SMPs) offer a unique ability to undergo a substantial shape deformation and subsequently recover the original shape when exposed to a particular external stimulus. Comparatively low mechanical properties being the major drawback for extended use of SMPs in engineering applications. However the inclusion of reinforcing fibres in to SMPs improves mechanical properties significantly while retaining intrinsic shape memory effects. The implementation of shape memory polymer composites (SMPCs) in any engineering application is a unique task which requires profound materials and design optimization. However currently available analytical tools have critical limitations to undertake accurate analysis/simulations of SMPC structures and slower derestrict transformation of breakthrough research outcomes to real-life applications. Many finite element (FE) models have been presented. But majority of them require a complicated user-subroutines to integrate with standard FE software packages. Furthermore, those subroutines are problem specific and difficult to use for a wider range of SMPC materials and related structures. This paper presents a FE simulation technique to model the thermomechanical behaviour of the SMPCs using commercial FE software ABAQUS. Proposed technique incorporates material time-dependent viscoelastic behaviour. The ability of the proposed technique to predict the shape fixity and shape recovery was evaluated by experimental data acquired by a bending of a SMPC cantilever beam. The excellent correlation between the experimental and FE simulation results has confirmed the robustness of the proposed technique. © 2017 IOP Publishing Ltd.},
author_keywords={finite element analysis;  fixity;  shape memory polymer composite;  shape recovery;  viscoelastic effect},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Smirnov2017282,
author={Smirnov, V.U. and Kos, O.I.},
title={Program module for calculating the optimal interval of preventive substitutions},
journal={Proceedings of the 2017 International Conference "Quality Management, Transport and Information Security, Information Technologies", IT and QM and IS 2017},
year={2017},
pages={282-283},
doi={10.1109/ITMQIS.2017.8085811},
art_number={8085811},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040110729&doi=10.1109%2fITMQIS.2017.8085811&partnerID=40&md5=e992b3c8af0181ef23907acd53434737},
abstract={Railway transport is one of the main transport modes of the Russian Federation, which is strategically important for the country. Artificial constructions on the railways are complex and responsible elements of the track economy. The probability of their failure-free operation must be high enough. For the adoption of optimal solutions for managing the technical state of man-made structures, it is necessary to use probabilistic operating models, build algorithms for calculating reliability and development indices using these algorithms of the software package, which makes it possible to automate the process of controlling the operation of artificial constructions on railways. © 2017 IEEE.},
author_keywords={Optimal interval;  Program module;  Reliability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ohshima20177,
author={Ohshima, Y. and Freudenberg, B. and Amelang, D.},
title={Kanto: A multi-participant screen-sharing system for Etoys, Snap!, and GP},
journal={PX/17.2 - Proceedings of the 3rd ACM SIGPLAN International Workshop on Programming Experience, Co-located with SPLASH 2017},
year={2017},
pages={7-10},
doi={10.1145/3167106},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041218427&doi=10.1145%2f3167106&partnerID=40&md5=e048e3f8fcab9d3ecfbf8433268b2998},
abstract={This paper demonstrates an implementation strategy for a general real-time remote collaboration framework called Kanto. Kanto is a web-based library that provides screen sharing, voice chat and bi-directional user interaction among participants over the Internet. The generality of Kanto's design makes it straightforward to add its facilities to the programming systems Squeak Etoys, Snap! and GP with little modification to those systems. Because Kanto is webbased, no additional software installation is required on the computers that use it. Kanto takes advantage of the WebRTC framework, which supports peer-to-peer video, voice and other data transmission. One insight is that if an application uses a single HTML canvas to render all graphics, we can simply stream the contents of the canvas to other hosts to do screen sharing. Luckily, the above-mentioned blocks-based programming languages follow this single-canvas implementation strategy, which is influenced by Smalltalk. Kanto embodies a particular set of choices within the vast design space of collaboration systems. For example, Kanto maintains its application state by designating one node as the state holder, and streaming just that node's display contents to the other nodes. This simplifies the implementation, but for a remote user introduces a delay between an action and its corresponding display update. In our experience the speed of response is acceptable even at intercontinental distances, but below we discuss alternative designs that would avoid this issue. © 2017 Copyright held by the owner/author(s). Publication rights licensed to the Association for Computing Machinery.},
author_keywords={Blocks-based languages;  Collaboration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Weber201750,
author={Weber, S. and Coblenz, M. and Myers, B. and Aldrich, J. and Sunshine, J.},
title={Empirical Studies on the Security and Usability Impact of Immutability},
journal={Proceedings - 2017 IEEE Cybersecurity Development Conference, SecDev 2017},
year={2017},
pages={50-53},
doi={10.1109/SecDev.2017.21},
art_number={8077806},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035762813&doi=10.1109%2fSecDev.2017.21&partnerID=40&md5=531f79fc41a53a5cb701eae5f417ce76},
abstract={Although it is well-known that API design has a large and long-term impact on security, the literature contains few substantial guidelines for practitioners on how to design APIs that improve security. Even fewer of those guidelines have been evaluated empirically. Security professionals have proposed that software engineers choose immutable APIs and architectures to enhance security. Unfortunately, prior empirical research argued that immutablity decreases API usability. This paper brings together the results from a number of previous papers that together aim to show that immutability, when carefully designed using usability as a first-class requirement, can have positive effects on both usability and security. We also make observations on study design in this field. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tarantino201775,
author={Tarantino, P. and Lee, N. and Close, S.},
title={A CubeSat platform for characterizing the reliability of electronic components},
journal={2017 USNC-URSI Radio Science Meeting (Joint with AP-S Symposium), USNC-URSI 2017},
year={2017},
pages={75-76},
doi={10.1109/USNC-URSI.2017.8074904},
art_number={8074904},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039974998&doi=10.1109%2fUSNC-URSI.2017.8074904&partnerID=40&md5=b7bcb8240e350d3fee57e048c83bffd7},
abstract={Because the space environment's effect on the operation of affordable, commercially available components is not well characterized, these devices are often ignored as viable candidate parts. This paper presents the design of the Cube-Sat Reliability Experiment (CRX) platform, a 1U customizable CubeSat-compatible system built to test and validate a variety of commercial off-The-shelf electronic components through direct exposure to the space environment. The system reports measurable device under test (DUT) health and environmental characteristics to the spacecraft bus. Data comparisons between shielded and exposed DUTs indicate DUT reliability. This work discusses the CRX system architecture as well as the demonstrated prototype implementation of hardware and software designs for a MOSFET DUT. This mockup establishes the system as an effective, low-cost utility to expand the list of qualified space-ready parts by determining accurate reliabilities of COTS components. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alrababah20175237,
author={Alrababah, A.A.},
title={Implementation of software systems packages in visual internal structures},
journal={Journal of Theoretical and Applied Information Technology},
year={2017},
volume={95},
number={19},
pages={5237-5244},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031743936&partnerID=40&md5=c51a674986474b259bb5b876ae2ba3e7},
abstract={This manuscript discusses the visualization methods of software systems architecture with composition of reverse engineering tools and restoration of software systems architecture. The visualization methods and analysis of dependencies in software packages are written in Java. To use this performance graph it needs to describe the relationships between classes inside the analyzed packages and between classes of different packages. This article discusses system visualization with using matrices of incoming and outgoing packet dependencies, allowing analyzing existing dependencies between classes within a package, and between classes of different packages. Obtaining such Information allows us to understand the reason for the emergence of dependencies between packages that determine architecture of the system, and also if necessary refactoring systems. In the manuscript also described the possibility of tools to provide the infrastructure for subsequent detection and error correction design in software systems and its refactoring. © 2005 – ongoing JATIT & LLS.},
author_keywords={Dependency;  Package;  Reverse engineering;  Software architecture;  Software visualization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yamauchi20172295,
author={Yamauchi, T. and Ikegami, Y. and Ban, Y.},
title={Mitigating use-after-free attacks using memory-reuse-prohibited library},
journal={IEICE Transactions on Information and Systems},
year={2017},
volume={E100D},
number={10},
pages={2295-2306},
doi={10.1587/transinf.2016INP0020},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030232120&doi=10.1587%2ftransinf.2016INP0020&partnerID=40&md5=d38103de49711da519dcb2f5f2f36189},
abstract={Recently, there has been an increase in use-after-free (UAF) vulnerabilities, which are exploited using a dangling pointer that refers to a freed memory. In particular, large-scale programs such as browsers often include many dangling pointers, and UAF vulnerabilities are frequently exploited by drive-by download attacks. Various methods to prevent UAF attacks have been proposed. However, only a few methods can effectively prevent UAF attacks during runtime with low overhead. In this paper, we propose HeapRevolver, which is a novel UAF attackpreventionmethod that delays and randomizes the timing of release of freed memory area by using a memory-reuse-prohibited library, which prohibits a freed memory area from being reused for a certain period. The first condition for reuse is that the total size of the freed memory area is beyond the designated size. The threshold for the conditions of reuse of the freed memory area can be randomized by HeapRevolver. Furthermore, we add a second condition for reuse in which the freed memory area is merged with an adjacent freed memory area before release. Furthermore, HeapRevolver can be applied without modifying the target programs. In this paper, we describe the design and implementation of HeapRevolver in Linux and Windows, and report its evaluation results. The results show that HeapRevolver can prevent attacks that exploit existing UAF vulnerabilities. In addition, the overhead is small. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.},
author_keywords={Memory-reuse-prohibited library;  System security;  UAF attack-prevention;  Use-after-free (UAF) vulnerabilities},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tan2017878,
author={Tan, X. and Bosch, J. and Vidal, M. and Alvarez, C. and Jimenez-Gonzalez, D. and Ayguade, E. and Valero, M.},
title={Picos, a hardware task-dependence manager for task-based dataflow programming models},
journal={Proceedings - 2017 International Conference on High Performance Computing and Simulation, HPCS 2017},
year={2017},
pages={878-880},
doi={10.1109/HPCS.2017.134},
art_number={8035173},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032371786&doi=10.1109%2fHPCS.2017.134&partnerID=40&md5=1d7d51a92c9f4f20c75514c88de655ba},
abstract={Task-based programming Task-based programming models such as OpenMP, Intel TBB and OmpSs are widely used to extract high level of parallelism of applications executed on multi-core and manycore platforms. These programming models allow applications to be expressed as a set of tasks with dependences to drive their execution at runtime. While managing these dependences for task with coarse granularity proves to be highly beneficial, it introduces noticeable overheads when targeting fine-grained tasks, diminishing the potential speedups or even introducing performance losses. To overcome this drawback, we propose a hardware/software co-design Picos that manages inter-task dependences efficiently. In this paper we describe the main ideas of our proposal and a prototype implementation. This prototype is integrated with a parallel task- based programming model and evaluated with real executions in Linux embedded system with two ARM Cortex-A9 and a FPGA. When compared with a software runtime, our solution results in more than 1.8x speedup and 40% of energy savings with only 2 threads. © 2017 IEEE.},
author_keywords={Data Flow Machines;  Fine-Grain Parallelism and Architectures;  Reconfigurable Computing & FPGA Based Architectures},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Baumann20171519,
author={Baumann, F.W. and Kopp, O. and Roller, D.},
title={Abstract API for 3D printing hardware and software resources},
journal={International Journal of Advanced Manufacturing Technology},
year={2017},
volume={92},
number={1-4},
pages={1519-1535},
doi={10.1007/s00170-017-0260-y},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015249046&doi=10.1007%2fs00170-017-0260-y&partnerID=40&md5=2684c8d85575c5e2b28c3715b77f9243},
abstract={With this research, an implementation of an overlay and abstracting RESTful API (application programming interface) for 3D printers is proposed to expose these resources to the Internet for utilization within and for cloud services. This is to abstract the underlying communication structure and means for accessing and controlling a 3D printer resource in one of three ways. The first way is a proprietary protocol or a 3D printer driver in Microsoft Windows. The second way is the control via a USB-serial connection between a controlling computer and the printer resource. This protocol can either be in a proprietary format or based on open standards like G-Code (ISO 6983-1:2009). The third way of control is based on physical storage devices attached to the printer with machining instructions stored on them. This research excludes the communication and control means involving proprietary protocols or drivers due complexity restrictions within the implementation. The approach is designed with extensibility in mind so that future access to proprietary protocols can be added to the control API. 3D printer resources with only the third control method available are also excluded from this research due to their lack of remote controllability. This work describes the design and implementation of an abstraction API layer between varying software and hardware components with an extensible architecture for future hardware and software components for within the domain of additive manufacturing (AM). With this research, the connection to further cloud services as 3D printing resources as well as a cloud printing service for usage and control of this API is demonstrated. This enables the use of AM machinery within cloud or business process-oriented architectures as the AM machinery and the associated software are exposed in an abstract and unified way and usable as services. © 2017, Springer-Verlag London.},
author_keywords={3D printing;  Abstraction layer;  Additive manufacturing;  API;  CBM;  Cloud-based manufacturing;  Cloud-based service;  CPS},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gizaw2017619,
author={Gizaw, A.A. and Bygstad, B. and Nielsen, P.},
title={Open generification},
journal={Information Systems Journal},
year={2017},
volume={27},
number={5},
pages={619-642},
doi={10.1111/isj.12112},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989246439&doi=10.1111%2fisj.12112&partnerID=40&md5=1bd721d4a1adf38355add53c8025df34},
abstract={To what extent can software ‘travel’ to organizations and countries for which it was not designed for, and how important are local contexts for a successful design and implementation of generic software? Information systems researchers have differing views on this, some emphasizing the strengths of the generic and others the importance of contextual aspects. Contributing to this debate, Pollock and Williams have coined the term generification in order to describe how large vendors succeed in globalizing software packages through management by community, content and social authority. In this paper, we explore an approach that we call open generification, which extends Pollock and Williams' work in the sense that we acknowledge the need for and the feasibility of generic software, but propose an alternative model for the governance of it. Open generification is not about managing the community of users attached to a software package by homogenization or segmentation but aims at addressing the diverse needs of the community the software is expected to serve. Our empirical basis is a longitudinal study of the development of an open-source health information system software (District Health Information software version 2), which is being used in more than 47 countries. Its success is attributed to a continuous interplay between generic and specific software and continuous cycles of embedding (implementing the global in the local context) and disembedding (taking local innovations into the global). We identify and discuss the contingent mechanisms of this interplay. © 2016 The Authors. Information Systems Journal Published by John Wiley & Sons Ltd.},
author_keywords={free and open-source software;  generification;  global software package;  health information system},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shah20178,
author={Shah, R. and Vutukuru, M. and Kulkarni, P.},
title={Devolve-Redeem: Hierarchical SDN controllers with adaptive offloading},
journal={ACM International Conference Proceeding Series},
year={2017},
pages={8-14},
doi={10.1145/3106989.3107001},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054163516&doi=10.1145%2f3106989.3107001&partnerID=40&md5=86910af1f965a3a1e4f0f9c75ad5e9cb},
abstract={Towards improving SDN control plane scalability, past work has proposed SDN controller frameworks that offload computation which depends on local state to controllers residing on the switches. Our work identifies another type of computation that can be offloaded to local controllers: that which depends on state that is generated globally but can be used within local controllers with loose synchronization. Because using such state locally incurs a synchronization cost, such offload makes sense only when the benefits of the offload out-weigh the synchronization cost. We present the design and implementation of Devolve-Redeem, an SDN controller framework that can offload computation to local controllers depending on the mix of various control messages in the incoming traffic. The offload decision in our framework is made by computing a cost metric that captures the relative costs of processing every control message at the central and local controllers, taking into account synchronization costs. The SDN application developer using our framework writes a single application that runs at both the central and local controllers, using our state management API to access of-floadable state. Our framework migrates between various offload modes using the computed cost metric, by manipulating the rules in the SDN switches that forward control messages to the controllers. Our framework also transparently handles state synchronization between central and local controllers in a manner that is consistent with the offload mode. We have implemented the SDN-based LTE EPC application in our framework, and experiments with our prototype demonstrate the effectiveness of our adaptive offload framework. © Copyright 2017 Association for Computing Machinery.},
author_keywords={Scalability;  Software-defined networking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chang201722,
author={Chang, S. and Pan, A.},
title={Customized HPC Cluster Software Stack on QCT Developer Cloud},
journal={Proceedings - 4th IEEE International Conference on Cyber Security and Cloud Computing, CSCloud 2017 and 3rd IEEE International Conference of Scalable and Smart Cloud, SSC 2017},
year={2017},
pages={22-26},
doi={10.1109/CSCloud.2017.56},
art_number={7987170},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028632276&doi=10.1109%2fCSCloud.2017.56&partnerID=40&md5=ff2065b6c6e476fbf8cc7a7232d49763},
abstract={OpenHPC is a collaborative project conducted by Linux Foundation to lower barriers to deployment, management, and use of modern HPC system with reference collection of open-source HPC software components and best practices. Quanta Cloud Technology (QCT) customized HPC cluster software stack including system provisioning, core HPC services, development tools, and optimized applications and libraries, which are distributed as pre-built and validated binaries and are meant to seamlessly layer on top of popular Linux distributions with the integration conventions defined by OpenHPC project. The architecture of QCT HPC Cluster Software Stack is intentionally modular to allow end users to pick and choose from the provided components, as well as to foster a community of open contribution. This paper presents an overview of the underlying customized vision, system architecture, software components and run tests on QCT Developer Cloud. © 2017 IEEE.},
author_keywords={HPC;  OpenHPC;  QCT Developer Cloud;  Software Stack},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guaman2017,
author={Guaman, D. and Quezada-Sarmiento, P.A. and Barba-Guaman, L. and Enciso, L.},
title={Use of SQALE and tools for analysis and identification of code technical debt through static analysis [Uso de SQALE y herramientas para análisis e identificación de deuda técnica de código a través de análisis estático]},
journal={Iberian Conference on Information Systems and Technologies, CISTI},
year={2017},
doi={10.23919/CISTI.2017.7975677},
art_number={7975677},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027061720&doi=10.23919%2fCISTI.2017.7975677&partnerID=40&md5=eef8d93e827e3f21df85796fa2c60e9b},
abstract={Technical Debt (TD), also known as technical debt design or technical debt code, analyze the consequence that could have a system once it has been designed architecturally, coding or implemented. TD refers to work to be performed rather than software design or coding is considered complete or correct. Static analysis is a technique to identify and analyze software characteristics from source code; through static analysis we can identify elements such as packages, classes, relationships, lines of code (LOC's), bugs, complexity, coding violations and others. In addition subsystems, components and their relationships supported by tools, algorithms, frameworks to analyze the code were identified. SQALE[1] is a quality and analysis model contains the internal properties expected from the code in the context of the evaluation, it has been used to perform many assessments of software source code, of various sizes in different application domains and programming language. SonarQube[2], Kiuwan[3] and PMD[4] are an open source platform to manage the source code quality, this cover seven axes of code quality among which stand: architecture and design, duplications, unit test, complexity, potential bugs, codifications rules, comments, among others; this platform work with over 20 programming languages. This paper, use as input the source code of the software applications written in different programming language for through static analysis identify metrics, characteristics, and technical debt with the aim to improve the quality when writing code, also supported in static analysis identify aspects such as correct apply of quality attributes, standards and best practices of programming that based in ISO 9126 and SQALE ensure the correct software development in terms of design and coding. © 2017 AISTI.},
author_keywords={Quality attributes;  SonarQube;  Source code;  SQALE;  Static analysis;  Technical debt},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Parrish20173185,
author={Parrish, R.M. and Burns, L.A. and Smith, D.G.A. and Simmonett, A.C. and DePrince, A.E. and Hohenstein, E.G. and Bozkaya, U. and Sokolov, A.Y. and Di Remigio, R. and Richard, R.M. and Gonthier, J.F. and James, A.M. and McAlexander, H.R. and Kumar, A. and Saitow, M. and Wang, X. and Pritchard, B.P. and Verma, P. and Schaefer, H.F. and Patkowski, K. and King, R.A. and Valeev, E.F. and Evangelista, F.A. and Turney, J.M. and Crawford, T.D. and Sherrill, C.D.},
title={Psi4 1.1: An Open-Source Electronic Structure Program Emphasizing Automation, Advanced Libraries, and Interoperability},
journal={Journal of Chemical Theory and Computation},
year={2017},
volume={13},
number={7},
pages={3185-3197},
doi={10.1021/acs.jctc.7b00174},
note={cited By 825},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021886502&doi=10.1021%2facs.jctc.7b00174&partnerID=40&md5=68ce6f080cada421c1c25b3e26c409ac},
abstract={Psi4 is an ab initio electronic structure program providing methods such as Hartree-Fock, density functional theory, configuration interaction, and coupled-cluster theory. The 1.1 release represents a major update meant to automate complex tasks, such as geometry optimization using complete-basis-set extrapolation or focal-point methods. Conversion of the top-level code to a Python module means that Psi4 can now be used in complex workflows alongside other Python tools. Several new features have been added with the aid of libraries providing easy access to techniques such as density fitting, Cholesky decomposition, and Laplace denominators. The build system has been completely rewritten to simplify interoperability with independent, reusable software components for quantum chemistry. Finally, a wide range of new theoretical methods and analyses have been added to the code base, including functional-group and open-shell symmetry adapted perturbation theory, density-fitted coupled cluster with frozen natural orbitals, orbital-optimized perturbation and coupled-cluster methods (e.g., OO-MP2 and OO-LCCD), density-fitted multiconfigurational self-consistent field, density cumulant functional theory, algebraic-diagrammatic construction excited states, improvements to the geometry optimizer, and the "X2C" approach to relativistic corrections, among many other improvements. © 2017 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Larsen2017,
author={Larsen, A.H. and Kuisma, M. and Löfgren, J. and Pouillon, Y. and Erhart, P. and Hyldgaard, P.},
title={Libvdwxc: A library for exchange-correlation functionals in the vdW-DF family},
journal={Modelling and Simulation in Materials Science and Engineering},
year={2017},
volume={25},
number={6},
doi={10.1088/1361-651X/aa7320},
art_number={065004},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021093739&doi=10.1088%2f1361-651X%2faa7320&partnerID=40&md5=49b9883380c7ff3881c5c96ab673f3db},
abstract={We present libvdwxc, a general library for evaluating the energy and potential for the family of vdW-DF exchange-correlation functionals. libvdwxc is written in C and provides an efficient implementation of the vdW-DF method and can be interfaced with various general-purpose DFT codes. Currently, the Gpaw and Octopus codes implement interfaces to libvdwxc. The present implementation emphasizes scalability and parallel performance, and thereby enables ab initio calculations of nanometer-scale complexes. The numerical accuracy is benchmarked on the S22 test set whereas parallel performance is benchmarked on ligand-protected gold nanoparticles () up to 9696 atoms. © 2017 IOP Publishing Ltd.},
author_keywords={density functional theory;  nonlocal correlation;  parallel computation;  software library;  van der Waals forces;  vdW-DF},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jarema201749,
author={Jarema, D. and Bungartz, H.J. and Görler, T. and Jenko, F. and Neckel, T. and Told, D.},
title={Block-structured grids in full velocity space for Eulerian gyrokinetic simulations},
journal={Computer Physics Communications},
year={2017},
volume={215},
pages={49-62},
doi={10.1016/j.cpc.2017.02.005},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014045147&doi=10.1016%2fj.cpc.2017.02.005&partnerID=40&md5=e0ca2dfa087749d7edb32e6e5a6cda17},
abstract={Global, i.e., full-torus, gyrokinetic simulations play an important role in exploring plasma microturbulence in magnetic fusion devices with strong radial variations. In the presence of steep temperature profiles, grid-based Eulerian approaches can become quite challenging as the correspondingly varying velocity space structures need to be accommodated and sufficiently resolved. A rigid velocity space grid then requires a very high number of discretization nodes resulting in enormous computational costs. To tackle this issue and reduce the computational demands, we introduce block-structured grids in the all velocity space dimensions. The construction of these grids is based on a general approach, making them suitable for various Eulerian implementations. In the current study, we explain the rationale behind the presented approach, detail the implementation, and provide simulation results obtained with the block-structured grids. The achieved reduction in the number of computational nodes depends on the temperature profile and simulation scenario provided. In the test cases at hand, about ten times fewer grid points are required for nonlinear simulations performed with block-structured grids in the plasma turbulence code GENE (http://genecode.org). With the speed-up found to scale almost exactly reciprocal to the number of grid points, the new implementation greatly reduces the computational costs and therefore opens new possibilities for applications of this software package. © 2017 Elsevier B.V.},
author_keywords={Glock-structured grids;  Gyrokinetic simulation;  Plasma turbulence;  Thermal speed disparity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vacca2017551,
author={Vacca, G. and Pili, D. and Fiorino, D.R. and Pintus, V.},
title={A webgis for the knowledge and conservation of the historical wall structures of the 13th - 18th centuries},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2017},
volume={42},
number={5W1},
pages={551-556},
doi={10.5194/isprs-Archives-XLII-5-W1-551-2017},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020731515&doi=10.5194%2fisprs-Archives-XLII-5-W1-551-2017&partnerID=40&md5=8a8ee515d3a249510cec350eb8866186},
abstract={The presented work is part of the research project, titled "Tecniche murarie tradizionali: conoscenza per la conservazione ed il miglioramento prestazionale" (Traditional building techniques: from knowledge to conservation and performance improvement), with the purpose of studying the building techniques of the 13th - 18th centuries in the Sardinia Region (Italy) for their knowledge, conservation, and promotion. The end purpose of the entire study is to improve the performance of the examined structures. In particular, the task of the authors within the research project was to build a WebGIS to manage the data collected during the examination and study phases. This infrastructure was entirely built using Open Source software. The work consisted of designing a database built in PostgreSQL and its spatial extension PostGIS, which allows to store and manage feature geometries and spatial data. The data input is performed via a form built in HTML and PHP. The HTML part is based on Bootstrap, an open tools library for websites and web applications. The implementation of this template used both PHP and Javascript code. The PHP code manages the reading and writing of data to the database, using embedded SQL queries. As of today, we surveyed and archived more than 300 buildings, belonging to three main macro categories: fortification architectures, religious architectures, residential architectures. The masonry samples investigated in relation to the construction techniques are more than 150. The database is published on the Internet as a WebGIS built using the Leaflet Javascript open libraries, which allows creating map sites with background maps and navigation, input and query tools. This too uses an interaction of HTML, Javascript, PHP and SQL code.},
author_keywords={Conservation;  Cultural heritage;  GIS;  Historical wall;  WEBGIS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Infantolino2017,
author={Infantolino, J. and Ross, J. and Richie, D.},
title={Portable high-performance software design using templated meta-programming for em calculations},
journal={2017 International Applied Computational Electromagnetics Society Symposium - Italy, ACES 2017},
year={2017},
doi={10.23919/ROPACES.2017.7916375},
art_number={7916375},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020021972&doi=10.23919%2fROPACES.2017.7916375&partnerID=40&md5=363ddb88a23d5f8c7afbdb2aab6e2e5b},
abstract={The Finite Difference Time Domain (FDTD) Method is used for full-wave electromagnetic (EM) simulations. FDTD is computationally intensive with performance depending critically on architecture-specific optimizations that have become more challenging given the rapidly changing architectures in modern high-performance computing platforms. We examine a templated meta-programming technique to implement the computational kernels in canonical form, without any architecture-specific optimizations, such that data layout and loop order optimizations can be applied through code transformations. These transformations are abstracted behind a simple yet flexible API for the application developer and require no special tools, relying only on a modern optimizing C++ compiler. Optimizations for data layout and loop order are selected at compile-time using C++ typedefs without the need to modify source code implementations of the algorithm. © 2017 ACES.},
author_keywords={FDTD;  Optimizations},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chisnall2017569,
author={Chisnall, D. and Davis, B. and Gudka, K. and Brazdil, D. and Joannou, A. and Woodruff, J. and Theodore Markettos, A. and Edward Maste, J. and Norton, R. and Son, S. and Roe, M. and Moore, S.W. and Neumann, P.G. and Laurie, B. and Watson, R.N.M.},
title={CHERI JNI: Sinking the Java security model into the C},
journal={ACM SIGPLAN Notices},
year={2017},
volume={52},
number={4},
pages={569-583},
doi={10.1145/3037697.3037725},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084480191&doi=10.1145%2f3037697.3037725&partnerID=40&md5=c5296acccd625e5ffbb760721ec00d4a},
abstract={Java provides security and robustness by building a high-level security model atop the foundation of memory protection. Unfortunately, any native code linked into a Java program-including the million lines used to implement the standard library-is able to bypass both the memory protection and the higher-level policies. We present a hardware-assisted implementation of the Java native code interface, which extends the guarantees required for Java's security model to native code. Our design supports safe direct access to buffers owned by the JVM, including hardware-enforced read-only access where appropriate. We also present Java language syntax to declaratively describe isolated compartments for native code. We show that it is possible to preserve the memory safety and isolation requirements of the Java security model in C code, allowing native code to run in the same process as Java code with the same impact on security as running equivalent Java code. Our approach has a negligible impact on performance, compared with the existing unsafe native code interface. We demonstrate a prototype implementation running on the CHERI microprocessor synthesized in FPGA. © 2017 ACM.},
author_keywords={Architecture;  Capability systems;  Cheri;  Compartmentalization;  Compilers;  Hardware security;  Java;  Jni;  Language security;  Memory protection;  Sandboxing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Churchill2017313,
author={Churchill, B. and Sharma, R. and Bastien, J.F. and Aiken, A.},
title={Sound loop superoptimization for google native client},
journal={ACM SIGPLAN Notices},
year={2017},
volume={52},
number={4},
pages={313-326},
doi={10.1145/3037697.3037754},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084475093&doi=10.1145%2f3037697.3037754&partnerID=40&md5=ca04395d90accac4bf04b676474f374f},
abstract={Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google. © 2017 ACM.},
author_keywords={Assembly;  Bounded verification;  Data-driven verification;  Equivalence checking;  Native client;  Superoptimization;  Verification;  X86-64},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chisnall2017569,
author={Chisnall, D. and Davis, B. and Gudka, K. and Brazdil, D. and Joannou, A. and Woodruff, J. and Markettos, A.T. and Maste, J.E. and Norton, R. and Son, S. and Roe, M. and Moore, S.W. and Neumann, P.G. and Laurie, B. and Watson, R.N.M.},
title={CHERI JNI: Sinking the Java security model into the C},
journal={International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
year={2017},
volume={Part F127193},
pages={569-583},
doi={10.1145/3037697.3037725},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021954876&doi=10.1145%2f3037697.3037725&partnerID=40&md5=586d9073c2ea14ec03052fab0b8c5b42},
abstract={Java provides security and robustness by building a highlevel security model atop the foundation of memory protection. Unfortunately, any native code linked into a Java program - including the million lines used to implement the standard library - is able to bypass both the memory protection and the higher-level policies.We present a hardwareassisted implementation of the Java native code interface, which extends the guarantees required for Java's security model to native code. Our design supports safe direct access to buffers owned by the JVM, including hardware-enforced read-only access where appropriate. We also present Java language syntax to declaratively describe isolated compartments for native code. We show that it is possible to preserve the memory safety and isolation requirements of the Java security model in C code, allowing native code to run in the same process as Java code with the same impact on security as running equivalent Java code. Our approach has a negligible impact on performance, compared with the existing unsafe native code interface. We demonstrate a prototype implementation running on the CHERI microprocessor synthesized in FPGA. © 2017 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Churchill2017313,
author={Churchill, B. and Sharma, R. and Bastien, J.F. and Aiken, A.},
title={Sound loop superoptimization for Google Native Client},
journal={International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
year={2017},
volume={Part F127193},
pages={313-326},
doi={10.1145/3037697.3037754},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021878924&doi=10.1145%2f3037697.3037754&partnerID=40&md5=12a9e5382824c407c2305bc3a36034b9},
abstract={Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google. © 2017 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Riansyah2017507,
author={Riansyah, M.I. and Nugraha, Y.P. and Ridlwan, H.M. and Trilaksono, B.R.},
title={3D mapping hexacopter simulation using gazebo and robot operating sytem (ROS)},
journal={ACM International Conference Proceeding Series},
year={2017},
volume={Part F128357},
pages={507-510},
doi={10.1145/3055635.3056659},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025160630&doi=10.1145%2f3055635.3056659&partnerID=40&md5=bbc030bd626f8b78ca2cf3c3da8b6bac},
abstract={This paper present a simulation of Unmanned Aerial Vehicle which is type of hexacopter for building 3D maps of exploration environment. This simulation using Gazebo Simulator environment with Software In the Loop (SITL) ardupilot that is integrated with Robot Operating System as a open source flexible framework for writing robot software. To proceed 3D maps construction, we have installed Intel Realsense R200 RGB-D camera on hexacopter for getting RGB image data and Depth data that will be computed by open source octomap ROS package to result 3D Maps. Octomap using octree data structure to form 3D Map of voxel with odometry of hexacopter. © 2017 ACM.},
author_keywords={3D map;  Gazebo;  Octomap;  Odometry;  R200;  Ros;  Simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nainggolan2017,
author={Nainggolan, J. and Christian, G. and Adari, K. and Bandung, Y. and Mutijarsa, K. and Subekti, L.B.},
title={Design and implementation of virtual class box 5.0 for distance learning in rural areas},
journal={Proceedings of 2016 8th International Conference on Information Technology and Electrical Engineering: Empowering Technology for Better Future, ICITEE 2016},
year={2017},
doi={10.1109/ICITEED.2016.7863258},
art_number={7863258},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015992531&doi=10.1109%2fICITEED.2016.7863258&partnerID=40&md5=7739647663c388b657f5907e862b5af1},
abstract={Educational resources and transportation infrastructure scarcity has become the major obstructions for Indonesian education development. To solve these problems, Virtual Class Box (VCB) 5.0 device is designed as an effort to support digital distance learning. It provides videoconference to connect teachers and students from different school. This paper explains the design and implementation of a videoconference system, along with its supporting operating system, applications and hardware. The hardware includes CPU, filter, converter and peripheral user interface modules while the software is built based on Linux Ubuntu 14.04.3 LTS operating system, Linphone Voice-over-IP application, and other open-source packages. This videoconference system facilitates learning-multimedia-sharing with a simple user interface and can operate in internet bandwidth of under 2Mbps while generates delay propagation time less than 100ms. VCB 5.0 operating system provides a software that can automatically configure the designed application, as well as easing the usage of the application. The operating system also consumes minimum resource and can be recreated for mass production purpose easily. VCB 5.0 hardware made the whole system portable and compatible with various types of supporting hardware. VCB 5.0 device is also accompanied with manual book and usage video so it can be introduced widely to consumers. © 2016 IEEE.},
author_keywords={distance learning;  information and communication techonology;  rural areas;  videoconference},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Taylor2017295,
author={Taylor, J.R. and Drumwright, E.M. and Hsu, J.},
title={Analysis of grasping failures in multi-rigid body simulations},
journal={2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots, SIMPAR 2016},
year={2017},
pages={295-301},
doi={10.1109/SIMPAR.2016.7862410},
art_number={7862410},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015976016&doi=10.1109%2fSIMPAR.2016.7862410&partnerID=40&md5=c4b1e10b3c589ea4dfaf418725f3c0b4},
abstract={Rigid body simulation libraries are sophisticated software systems that include multiple, tricky to implement numerical algorithms: solving initial value problems, root finding, geometric intersection (collision detection) and contact determination, and solving mathematical programming and optimization problems. How and why such systems fail to produce expected behavior is not readily known and not easy to discern; few of the components described above use reference implementations or are modular, for example, which makes attempting to identify points of failure challenging. Additionally, most libraries present an extraordinary number of parameters to be tuned, which complicates assessment. For the field of robotics, such failures are frustrating. One of the most seminal tasks in the domain, grasping of rigid objects, can require considerable parameter tuning. This paper uses a recent development, the support of four open-source physics engines in the GAZEBO simulator, to assess the ability of simulated robots to maintain grasps of rigid objects. We develop a metric that captures grasping performance and run a multitude of experiments to ascertain causes of failure. We have made all of our experimental code and data freely available, which allows others to reproduce our results and the authors of the corresponding physics engines to compete toward maximizing performance on the grasping task. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barn2017212,
author={Barn, B. and Barat, S. and Clark, T.},
title={Conducting systematic literature reviews and systematic mapping studies},
journal={ACM International Conference Proceeding Series},
year={2017},
pages={212-213},
doi={10.1145/3021460.3021489},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054160759&doi=10.1145%2f3021460.3021489&partnerID=40&md5=ba878a25d13e3067bc05559f79afd043},
abstract={Context: An essential part of conducting software engineering (SE) research is the ability to identify extant research on tools, technologies, concepts and methods in order to evaluate and make rational and scientific decisions. The domain from which such knowledge is extracted is typically existing research literature found in journals, conference proceedings, books and gray literature. Empirical approaches that include various systematic review (SR) methodologies such as systematic literature review (SLR) and systematic mapping study (SMS) are found to be effective in this context. They adopt rigorous planning, follow repeatable and well-defined processes, and produce unbiased and evidence-based outcomes. Despite these significant benefits, the general trend on using these systematic review (SR) methodologies is not encouraging in SE research. The primary reasons emerging are twofold - a) SR methodologies are largely cited as time-consuming activities and b) lack of guidance to conduct systematic reviews. This tutorial discusses these concerns and describes an effective way of using SR methodologies for SE research. Objectives: Attendees will be introduced to the key concepts, methods and processes for conducting systematic literature reviews (SLR) and systematic mapping studies (SMS). The benefits, limitations, guidelines for using SR methodologies in an effective manner will discussed in the session. Attendees will be guided on the appropriate formulation of a research question and sub questions; the development of a review protocol such as inclusion criteria, exclusion criteria, quality criteria and classification structures; and execution of review protocol using digital libraries and syntheses of review data. A web based software tool1, for supporting the systematic literature review process will be demonstrated and attendees will get the opportunity to use the tool to conduct the review to help in identification of relevant research and extraction and synthesis of data. Method: We will use a blend of information presentation, interactive hands-on session and knowledge sharing session. The presentation will introduce the key concepts, benefits, limitations and how to overcome the limitations; hands on session will illustrate a review process with a case study, and finally the knowledge sharing session will discuss the experiences, best practices and the lesson learnt. © 2017 ACM.},
author_keywords={Literature review;  Meta modeling;  Model based literature review;  Systematic literature review;  Systematic mapping study},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu201796,
author={Yu, C.Y. and Ma, J. and Ren, Q. and Zhao, Y.Y.},
title={Design and Implementation of Software Case Library Supporting Software Capability Training},
journal={Proceedings - 5th International Conference on Educational Innovation through Technology, EITT 2016},
year={2017},
pages={96-101},
doi={10.1109/EITT.2016.26},
art_number={7839501},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015762467&doi=10.1109%2fEITT.2016.26&partnerID=40&md5=3fe3a0ce5a006e6819202a5c7d522c39},
abstract={Software talents mainly come from university graduates, but the graduates could not meet the need of employers. Practical ability of software talents cultivating has aroused wide concern of the researches and teachers. In this paper, we consider that the software case is the most important factor and proposed SCL (Software Case Library) could provide real and attractive software case in the teaching and students' practical projects. The SCL consists of source and different documents and could adapt different kinds of practical training. We developed a SCL and applied it in Web Programming with 102 students. We have collected the data of the course achievement by questionnaire, face-to-face interview, big project, students' grading for the course. The result indicated that SCL promoted the satisfaction and practical abilities of students. © 2016 IEEE.},
author_keywords={case teaching;  project-driven;  software case library;  software talents cultivating},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Cherif2017151,
author={Cherif, E.},
title={Online real estate demand chain integration},
journal={Strategic Information Systems and Technologies in Modern Organizations},
year={2017},
pages={151-187},
doi={10.4018/978-1-5225-1680-4.ch007},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028438581&doi=10.4018%2f978-1-5225-1680-4.ch007&partnerID=40&md5=600e64caccae476ae1baa3aee9111ec3},
abstract={In this paper, we propose a solution for demand chain management using APIs (application programming interfaces) integration in the online real estate. We propose online real estate management system that includes advanced modules that can be bundled together, creating differentiation and enhancing the value chain. We propose a simplified implementation architecture for an integrated demand and supply chain management system for online real estate services. We use a formal specification language for specifying the functional components of the demand chain management system and interaction with real estate entities and actors. We choose an open source Customer Relationship Management system as a platform to manage some of the online real estate modules. Other value-added modules are integrated from third-party providers using their open interfaces. © 2017 by IGI Global. All rights reserved.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{He2017197,
author={He, Y. and Zheng, Y. and Deng, J. and Pan, H.},
title={Design and implementation of a POI collection and management system based on public map service},
journal={4th International Conference on Ubiquitous Positioning, Indoor Navigation and Location-Based Services - Proceedings of IEEE UPINLBS 2016},
year={2017},
pages={197-200},
doi={10.1109/UPINLBS.2016.7809971},
art_number={7809971},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015161884&doi=10.1109%2fUPINLBS.2016.7809971&partnerID=40&md5=e7e9a39ec5e5a24cffdef6e0ce8332fd},
abstract={In this study, the Application Interface (API) of Baidu map service was took as an example to establish a POI (Point of Interest) collection and management system (POI-CMS) based on GeoServer, PostgreSQL and PostGIS via visual studio 2010 and C# language. The newly developed POI-CMS offers one-stop POI service, including POI collection, storage, management and dissemination. The successful design and implementation of the POI-CMS indicates that the concept of construction of POI-CMS with open source software and public map service is feasible and can significantly shorten the development cycle and reduce monetary cost. © 2016 IEEE.},
author_keywords={Baidu Map API;  GeoServer;  POI;  PostGIS;  PostgreSQL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rysanek20172620,
author={Rysanek, A. and Miller, C. and Schlueter, A.},
title={A workflow for managing building information and performance data using virtual reality: an alternative to BIM for existing buildings?},
journal={Building Simulation Conference Proceedings},
year={2017},
volume={5},
pages={2620-2627},
doi={10.26868/25222708.2017.817},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107609038&doi=10.26868%2f25222708.2017.817&partnerID=40&md5=fd2899496d54aa5f26ed43e68a8ffe57},
abstract={Building information modelling (BIM) has carved a growing niche in the construction industry for the support of new building projects. The same can- not be said for existing buildings, where the preva- lence of uncertain data, and unclear information, has been di_cult to reconcile with the unambiguous na- ture of BIM parameterization. An opportunity to alleviate these challenges may have arrived from the recent boon of virtual reality platforms for navigating physical environments. This paper demonstrates how labeling of equirectangular images with data or text widgets is possible using publicly-available software libraries. A prototype is presented and tested on a building in Singapore. © 2017 Building Simulation Conference Proceedings. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jamshed2017113,
author={Jamshed, M. and Moon, Y. and Kim, D. and Han, D. and Park, K.},
title={MOS: A reusable networking stack for flow monitoring middleboxes},
journal={Proceedings of the 14th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2017},
year={2017},
pages={113-129},
note={cited By 54},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076890916&partnerID=40&md5=c238ca5564efbfe9949ab1695abb1a80},
abstract={Stateful middleboxes, such as intrusion detection systems and application-level firewalls, have provided key functionalities in operating modern IP networks. However, designing an efficient middlebox is challenging due to the lack of networking stack abstraction for TCP flow processing. Thus, middlebox developers often write the complex flow management logic from scratch, which is not only prone to errors, but also wastes efforts for similar functionalities across applications. This paper presents the design and implementation of mOS, a reusable networking stack for stateful flow processing in middlebox applications. Our API allows developers to focus on the core application logic instead of dealing with low-level packet/flow processing themselves. Under the hood, it implements an efficient event system that scales to monitoring millions of concurrent flow events. Our evaluation demonstrates that mOS enables modular development of stateful middleboxes, often significantly reducing development efforts represented by the source lines of code, while introducing little performance overhead in multi-10Gbps network environments.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wright2017211,
author={Wright, S.},
title={Mythology of cyber-crime—insecurity & governance in cyberspace: Some critical perspectives},
journal={Advanced Sciences and Technologies for Security Applications},
year={2017},
pages={211-227},
doi={10.1007/978-3-319-54975-0_13},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075823676&doi=10.1007%2f978-3-319-54975-0_13&partnerID=40&md5=ea688f582f5435d14204eb6c55df51c4},
abstract={There was a time within even this author’s memory, when there was no cyberspace, no cybercrime of note, no viruses and no anti-virus software, no hacking and no hackers. Cyber-delinquency was unknown, criminals had to do their criminality in the physical world and academic research was done in libraries not ‘on-line’. The speed of banking in that far off time was pedestrian. During the Fifties, letters took weeks to arrive overseas, with anything more urgent being sent by costly telegram, over phone wires. In the intelligence world, the success of decoding Enigma and the entire field of de-encryption remained a secret, Alan Turing continued to be an unsung hero, and machine intelligence had very little acknowledged role, it was mainly human centred. In the Sixties, protest was on the streets and no-one, apart from traffic engineers, knew what networking meant. In just one lifetime, all that has changed and the pace of that change has rapidly accelerated too. The evolution of cyberspace has brought many advantages to societies once separated by distances but now able to communicate, bank, educate and socialize online and in real time. It has also brought many unanticipated dangers. Some, including radicalization, grooming, phishing, banking fraud, stalking, identity theft and denial of service attacks, are the stuff of daily news. Others, including the security and defence revolutions in military affairs, are much less discussed, despite the fact that the cyber-world originated and is firmly rooted in a military architecture of space based satellites and associated communications infrastructure. This chapter critically assesses some of the mythology of just who are the cyber bad guys, the extent to which these constructions are open to wider processes of perceptions management and the need to identify the rather more hidden agendas facilitated by emerging new capability sets in cyberspace and the so called ‘internet of things.’ That world is still tremendously Anglo-centric, notions of just whose security is being protected remain contested, and we are only at the beginning of a more global debate on big data and the challenge of meaningful governance. © Springer International Publishing AG 2017.},
author_keywords={Crackers;  Cyber-attacks;  Cybercriminals;  Governance;  Hackers;  Hactivists;  ICRAC;  Mythology;  NSA;  OSCE;  Perception Management;  Prism;  Snowden;  State-Related Actors;  STOA;  Targeting},
document_type={Book Chapter},
source={Scopus},
}

@BOOK{Kaylor2017613,
author={Kaylor, J. and Läufer, K. and Thiruvathukal, G.K.},
title={RestFS: The filesystem as a connector abstraction for flexible resource and service composition},
journal={Cloud Computing: Methodology, Systems, and Applications},
year={2017},
pages={613-644},
doi={10.1201/b11149},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052563573&doi=10.1201%2fb11149&partnerID=40&md5=302defca68a0297156757789a3ec5925},
abstract={The broader context for this chapter comprises business scenarios requiring resource and/or service composition, such as (intra-company) enterprise application integration (EAI) and (inter-company) web service orchestration. The resources and services involved vary widely in terms of the protocols they support, which typically fall into remote procedure call (RPC) [129], resourceoriented (HTTP [270] and WEBDAV [758]) and message-oriented protocols. By recognizing the similarity between web-based resources and the kind of resources exposed in the form of filesystems in operating systems, we have found it feasible to map the former to the latter using a uniform, configurable connector layer. Once a remote resource has been exposed in the form of a local filesystem, one can access the resource programmatically using the operating system’s standard filesystem application programming interface (API). Taking this idea one step further, one can then aggregate or otherwise orchestrate two or more remote resources using the same standard API. Filesystem APIs are available in all major operating systems. Some of those, most notably, all flavors of UNIX including GNU/Linux, have a rich collection of small, flexible command-line utilities, as well as various inter-process communication (IPC) mechanisms. These tools can be used in scripts and programs that compose the various underlying resources in powerful ways. Further explorations of the role of a filesystem-based connector layer in the enterprise application architecture have lead us to the question whether one can achieve a fully compositional, arbitrarily deep hierarchical architecture by re-exposing the aggregated resources as a single, composite resource that, in turn, can be accessed in the same form as the original resources. This is indeed possible in two flavors: 1) the composite resource can be exposed internally as a filesystem for further local composition; 2) the composite resource is exposed externally as a restful resource for further external composition. We expect the ability hierarchically to compose resources to facilitate the construction of complex, robust resource- and service-oriented software systems, and we hope that concrete case studies will further substantiate our position. Leveraging our prior work on the Naked Objects Filesystem (NOFS) [419], which exposes object-oriented domain model functionality as a Linux filesystem in user space (FUSE) [685], we have implemented RestFS [418], a (dynamically re)configurable mechanism for exposing remote restful resources and as local filesystems. Several sample adapters specific to well-known services such as Yahoo! Placefinder and Twitter are already available. Authentication poses a challenge in that it cannot always be automated; in practice, when systems such as OAuth are used, it is often only the initial granting of authentication that must be manual, and the resulting authentication token can then be included in the connector configuration. As future work, we plan to develop plugins to support resources across a broader range of protocols, such as FTP, SFTP, or SMTP. © 2012 by Taylor & Francis Group, LLC.},
document_type={Book Chapter},
source={Scopus},
}

@BOOK{BrentNeal20171,
author={Brent Neal, F. and Russ, J.C.},
title={Measuring shape},
journal={Measuring Shape},
year={2017},
pages={1-461},
doi={10.1201/b12092},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052164888&doi=10.1201%2fb12092&partnerID=40&md5=f3105099cb0663ae1c48089e644b4782},
abstract={“John Russ is the master of explaining how image processing gets applied to real-world situations. With Brent Neal, he’s done it again in Measuring Shape, this time explaining an expanded toolbox of techniques that includes useful, state-of-the-art methods that can be applied to the broad problem of understanding, characterizing, and measuring shape. He has a gift for finding the kernel of a particular algorithm, explaining it in simple terms, then giving concrete examples that are easily understood. His perspective comes from solving real-world problems and separating out what works in practice from what is just an abstract curiosity.” -Tom Malzbender, Hewlett-Packard Laboratories, Palo Alto, California, USA. Useful for those working in fields including industrial quality control, research, and security applications, Measuring Shape is a handbook for the practical application of shape measurement. Covering a wide range of shape measurements likely to be encountered in the literature and in software packages, this book presents an intentionally diverse set of examples that illustrate and enable readers to compare methods used for measurement and quantitative description of 2D and 3D shapes. It stands apart through its focus on examples and applications, which help readers quickly grasp the usefulness of presented techniques without having to approach them through the underlying mathematics. An elusive concept, shape is a principal governing factor in determining the behavior of objects and structures. Essential to recognizing and classifying objects, it is the central link in manmade and natural processes. Shape dictates everything from the stiffness of a construction beam, to the ability of a leaf to catch water, to the marketing and packaging of consumer products. This book emphasizes techniques that are quantitative and produce a meaningful yet compact set of numerical values that can be used for statistical analysis, comparison, correlation, classification, and identification. Written by two renowned authors from both industry and academia, this resource explains why users should select a particular method, rather than simply discussing how to use it. Showcasing each process in a clear, accessible, and well-organized way, they explore why a particular one might be appropriate in a given situation, yet a poor choice in another. Providing extensive examples, plus full mathematical descriptions of the various measurements involved, they detail the advantages and limitations of each method and explain the ways they can be implemented to discover important correlations between shape and object history or behavior. This uncommon assembly of information also includes sets of data on real-world objects that are used to compare the performance and utility of the various presented approaches. © 2012 by Taylor & Francis Group, LLC.},
document_type={Book},
source={Scopus},
}

@ARTICLE{Gorshenin201738,
author={Gorshenin, A.K.},
title={Pattern-based analysis of probabilistic and statistical characteristics of extreme precipitation},
journal={Informatika i ee Primeneniya},
year={2017},
volume={11},
number={4},
pages={38-46},
doi={10.14357/19922264170405},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042674672&doi=10.14357%2f19922264170405&partnerID=40&md5=1d0a1e89d94f2117f5346416352285d0},
abstract={Precipitations are the key parameters of hydrological models; so, research related to precipitation processes is necessary for solving various applied problems. The paper demonstrates a violation of the Markov property for precipitation observed in essentially different climatic regions-in the cities of Potsdam and Elista. Such information about the data, along with previously studied properties, represents the basic information which is necessary for the further correct construction of probabilistic models, in particular, for probability distribution of the volumes of extreme precipitation. For the analysis of the probabilistic behavior of the precipitation process and the construction of forecasts, it is suggested to use chains of events (patterns) extracted from the data. At the same time, statistical procedures are automated using the software tools of the MATLAB package. Neural networks were used as an alternative forecasting tool based on patterns, and the best results were demonstrated via the architecture that takes into account a seasonality, has two hidden layers of neurons and a sigmoid activation function. The ideas for further research in this field are suggested.},
author_keywords={Forecast;  Markov property;  Neural networks;  Patterns;  Precipitations;  Probabilistic forecasting},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Oliveira-Ciabati2017748,
author={Oliveira-Ciabati, L. and Alves, D. and Barbosa-Junior, F. and Vieira, E.M. and Souza, J.P.},
title={SISPRENACEL - MHealth tool to empower PRENACEL strategy},
journal={Procedia Computer Science},
year={2017},
volume={121},
pages={748-755},
doi={10.1016/j.procs.2017.11.142},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040235555&doi=10.1016%2fj.procs.2017.11.142&partnerID=40&md5=2eb58c290a37b6899fc2fddf38e9c1ae},
abstract={Objective: this study aims to describe the development process of a web-based tool, capable of managing and delivering content through short message service (SMS) to pregnant women and their partners during antenatal and postnatal care (PRENACEL strategy). Methods: We used software engineer best practices to implement an adequate tool for PRENACEL researchers. We used sociotechnical approach to gather the requirements and built prototypes to quickly assess the developed functionalities. SISPRENACEL was created using CakePHP and MySQL and it is based on a client-server architecture, in order to make it accessible over the internet. To develop the graphic interface a free template was used, named AdminLTE version 1.0, created using the Bootstrap library. Results: SISPRENACEL delivered 22,296 scheduled SMS, received 1,249 messages and answered questions through 1,823 SMS. Besides that, we found out that using techniques that included the stakeholders in the development process resulted in a well evaluated, easy to use, pleasant looking and useful system. Conclusion: SISPRENACEL was an essential tool for bringing PRENACEL strategy to reality, managing and delivering content through SMS to women and their partners, without imposing a burden to health professionals. © 2017 The Authors. Published by Elsevier B.V.},
author_keywords={maternal health;  mHealth;  sociotechnical approach;  usability evaluation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chan2017,
author={Chan, C.-Y. and Hwang, P.-W. and Huang, T.-M.},
title={Development of an athermalized optomechanical system of large aperture remote sensing instruments},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2017},
volume={10371},
doi={10.1117/12.2273779},
art_number={103710U},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040092414&doi=10.1117%2f12.2273779&partnerID=40&md5=d245484108d902561d404f8ddd915e28},
abstract={An integrated optimum athermalization design and analysis system will be developed in the study. The distance between the primary and secondary mirrors for a remote sensing instrument (RSI) will be taken as the objective function to improve the influence of the environment temperature variation on the optical images. Under a developing RSI model, the athermalization design to the secondary mirror based on the established system integrating a computer-aided design software, materials library construction, finite element analysis and optimization program will be executed. The design variables of the barrel, supporting structure and shims will be carried under low temperature change requirement. The displacements of the secondary mirror with respect to primary mirror with the optimum athermalization design can be reduced to almost zero from-95.6 μm for low temperature thermal boundary conditions. © 2017 SPIE.},
author_keywords={Athermalization;  Finite element analysis;  Optimization;  Reflective mirror},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bekesiene201791,
author={Bekesiene, S. and Hoskova-Mayerova, S. and Diliunas, P.},
title={Structural equation modeling using the amos and regression of effective organizational commitment indicators in lithuanian military forces},
journal={16th Conference on Applied Mathematics, APLIMAT 2017 - Proceedings},
year={2017},
pages={91-102},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035343993&partnerID=40&md5=8cab3bad719ffe489419fcc6229d0ffc},
abstract={The recognition of their importance to the scientific research suggests the need to evaluate and compare different types of techniques so that the scientific research designs can be selected adequately. The article presents a consecutively example, which analyzes the same dataset via two very different statistical techniques. After that it compares two modeling possibilities: modeling using Analysis of Moment Structures (AMOS) and statistical software package SPSS. Finally, the article discusses linear regression models and offers guidelines as to when SEM techniques and when regression techniques should be used.},
author_keywords={AMOS;  Construct validity;  Formative constructs;  Regression;  Research techniques;  Structural equation modeling (SEM);  The leading behavioral traits},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Byckling2017123,
author={Byckling, M. and Kataja, J. and Klemm, M. and Zwinger, T.},
title={OpenMP* SIMD vectorization and threading of the elmer finite element software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10468 LNCS},
pages={123-137},
doi={10.1007/978-3-319-65578-9_9},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029387850&doi=10.1007%2f978-3-319-65578-9_9&partnerID=40&md5=d65ecda76de8d14836cc886dfc3de5b0},
abstract={We describe the design and implementation of hierarchical high-order basis functions with OpenMP* SIMD constructs in the Elmer Finite Element software. We give rationale of our design decisions and present some of the key challenges encountered during the implementation. Our numerical results on a platform supporting Intel® AVX2 show that the new basis function implementation is 3x to 4x faster when compared to the same code without OpenMP SIMD in use, or 5x to 10x faster when compared to the original Elmer implementation. In addition, our numerical results show similar speedups for the entire finite element assembly process. © Springer International Publishing AG 2017.},
author_keywords={Basis functions;  Finite elements;  Implementation;  OpenMP;  SIMD},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Acosta-Ortiz2017644,
author={Acosta-Ortiz, D. and Ramos-Pollán, R. and Pedraza, G.},
title={A general purpose architecture for IoT data acquisition},
journal={Communications in Computer and Information Science},
year={2017},
volume={735},
pages={644-658},
doi={10.1007/978-3-319-66562-7_46},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028851127&doi=10.1007%2f978-3-319-66562-7_46&partnerID=40&md5=e0f3f8838f213195c8f3fbfa43484e96},
abstract={This paper presents a description of a data acquisition IoT platform architecture which has been designed to provide two additional extensible mechanisms at server and device levels. From the device perspective we support implementations for two data acquisition modes: near real time and batch. From the server side we allow data analytics processes to be seamlessly configured and launched over the data acquired. Our approach pursues to reduce the efforts and cost of integration of new hardware devices. Along with the architecture description we provide an API definition, a platform reference implementation and its performance evaluation. Our main purpose is to guarantee integration, modularity, flexibility and extensibility to cover a wide range of applications and use cases. © Springer International Publishing AG 2017.},
author_keywords={API;  Data acquisition;  Embedded systems;  Internet of Things (IoT);  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bychkov2017278,
author={Bychkov, I.V. and Oparin, G.A. and Bogdanova, V.G. and Pashinin, A.A. and Gorsky, S.A.},
title={Automation development framework of scalable scientific web applications based on subject domain knowledge},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10421 LNCS},
pages={278-288},
doi={10.1007/978-3-319-62932-2_27},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028723113&doi=10.1007%2f978-3-319-62932-2_27&partnerID=40&md5=7b6f65b288025a201464e4759644c6ba},
abstract={Currently high-performance computing technologies using computational capabilities for solving scientific, are actively improving. The purpose of our research is the development of toolkit for construction and execution of scientific service-oriented application in heterogeneous distributed computing environment (HDCE). These tools provide the access for subject domain experts to the high-capacity computing resource, using these resources without extensive knowledge of computing architecture and low-level software, and the parallel execution of the user application on the base of the service-oriented technology and multi-agent control. We describe an architecture and functional capabilities of automated toolkit for the service-oriented application creation based on applied programs package, and multi-agent control of this application parallel running in HDCE. We demonstrate an example of the creation of the web-application for parametric feedback synthesis of linear dynamic object by these tools. The offered technology allows simplifying service creation and provides new qualitative opportunities of controlling parallel high-performance computations. © Springer International Publishing AG 2017.},
author_keywords={Parametric synthesis of control law;  Scalable application;  Service},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Menshov2017136,
author={Menshov, I. and Sheverdin, V.},
title={A parallel locally-adaptive 3D model on cartesian nested-type grids},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10421 LNCS},
pages={136-142},
doi={10.1007/978-3-319-62932-2_12},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028714963&doi=10.1007%2f978-3-319-62932-2_12&partnerID=40&md5=655678fac010cc49d5f34bf07ca58537},
abstract={The paper addresses the 3D extension of the Cartesian multilevel nested-type grid methodology and its software implementation in an application library written in C++ object-oriented language with the application program interface OpenMP for parallelizing calculations on shared memory. The library accounts for the specifics of multithread calculations of 3D problems on Cartesian grids, which makes it possible to substantially minimize the loaded memory via non-storing the grid information. The loop order over cells is represented by a special list that remarkably simplifies parallel realization with the OpenMP directives. Test results show high effectiveness of dynamical local adaptation of Cartesian grids, and increasing of this effectiveness while the number of adaptation levels becomes larger. © Springer International Publishing AG 2017.},
author_keywords={Gas dynamics equations;  Locally-adaptive nested-type cartesian grid;  Shared memory parallel calculation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guaman2017171,
author={Guaman, D. and Sarmiento, P.A.-Q. and Barba-Guamán, L. and Cabrera, P. and Enciso, L.},
title={SonarQube as a tool to identify software metrics and technical debt in the source code through static analysis},
journal={2017 7th International Workshop on Computer Science and Engineering, WCSE 2017},
year={2017},
pages={171-175},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027839441&partnerID=40&md5=10c2c2f05475c04186d6fbeb3f45a1f9},
abstract={Technical Debt (TD), also known as technical debt design or technical debt code, analyze the consequence that could have a system once it has been designed architecturally, coding or implemented. TD refers to work to be performed rather than software design or coding is considered complete or correct. Static analysis is a technique to identify and analyze software characteristics from source code; through static analysis we can identify elements such as packages, classes, relationships, lines of code (LOC's), bugs, complexity, coding violations and others. In addition, subsystems, components and their relationships supported by tools, algorithms, frameworks to analyze the code were identified. SQALE is a quality and analysis model contains the internal properties expected from the code in the context of the evaluation, it has been used to perform many assessments of software source code, of various sizes in different application domains and programming language. SonarQube is an open source platform to manage the source code quality, this cover seven axes of code quality among which stand: architecture and design, duplications, unit test, complexity, potential bugs, codifications rules, comments, among others; this platform work with over 20 programming languages. This paper, use as input the source code of the software applications written in different programming language for through static analysis identify metrics, characteristics, and technical debt with the aim to improve the quality when writing code, also supported in static analysis identify aspects such as correct apply of quality attributes, standards and best practices of programming that based in ISO 9126 and SQALE ensure the correct software development in terms of design and coding.},
author_keywords={Quality attributes;  SonarQube;  Source code;  SQALE;  Static analysis;  Technical debt},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tsoeunyane2017,
author={Tsoeunyane, L. and Winberg, S. and Inggs, M.},
title={Software-Defined Radio FPGA Cores: Building towards a Domain-Specific Language},
journal={International Journal of Reconfigurable Computing},
year={2017},
volume={2017},
doi={10.1155/2017/3925961},
art_number={3925961},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027137916&doi=10.1155%2f2017%2f3925961&partnerID=40&md5=1454db15adf40582089a8c728598d6d1},
abstract={This paper reports on the design and implementation of an open-source library of parameterizable and reusable Hardware Description Language (HDL) Intellectual Property (IP) cores designed for the development of Software-Defined Radio (SDR) applications that are deployed on FPGA-based reconfigurable computing platforms. The library comprises a set of cores that were chosen, together with their parameters and interfacing schemas, based on recommendations from industry and academic SDR experts. The operation of the SDR cores is first validated and then benchmarked against two other cores libraries of a similar type to show that our cores do not take much more logic elements than existing cores and that they support a comparable maximum clock speed. Finally, we propose our design for a Domain-Specific Language (DSL) and supporting tool-flow, which we are in the process of building using our SDR library and the Delite DSL framework. We intend to take this DSL and supporting framework further to provide a rapid prototyping system for SDR application development to programmers not experienced in HDL coding. We conclude with a summary of the main characteristics of our SDR library and reflect on how our DSL tool-flow could assist other developers working in SDR field. © 2017 Lekhobola Tsoeunyane et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Akbudak201722,
author={Akbudak, K. and Ltaief, H. and Mikhalev, A. and David, K.},
title={Tile low rank cholesky factorization for climate/weather modeling applications on manycore architectures},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10266 LNCS},
pages={22-40},
doi={10.1007/978-3-319-58667-0_2},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026206907&doi=10.1007%2f978-3-319-58667-0_2&partnerID=40&md5=dfadc05086ddf98e425fe772ffd24837},
abstract={Covariance matrices are ubiquitous in computational science and engineering. In particular, large covariance matrices arise from multivariate spatial data sets, for instance, in climate/weather modeling applications to improve prediction using statistical methods and spatial data. One of the most time-consuming computational steps consists in calculating the Cholesky factorization of the symmetric, positive-definite covariance matrix problem. The structure of such covariance matrices is also often data-sparse, in other words, effectively of low rank, though formally dense. While not typically globally of low rank, covariance matrices in which correlation decays with distance are nearly always hierarchically of low rank. While symmetry and positive definiteness should be, and nearly always are, exploited for performance purposes, exploiting low rank character in this context is very recent, and will be a key to solving these challenging problems at large-scale dimensions. The authors design a new and flexible tile row rank Cholesky factorization and propose a high performance implementation using OpenMP task-based programming model on various leading-edge manycore architectures. Performance comparisons and memory footprint saving on up to 200K × 200K covariance matrix size show a gain of more than an order of magnitude for both metrics, against state-of-the-art open-source and vendor optimized numerical libraries, while preserving the numerical accuracy fidelity of the original model. This research represents an important milestone in enabling large-scale simulations for covariance-based scientific applications. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DaSilva2017382,
author={Da Silva, C.P. and Messai, N. and Sam, Y. and Devogele, T.},
title={Diamond: A cube model proposal based on a centric architecture approach to enhance liquid software model approaches},
journal={WEBIST 2017 - Proceedings of the 13th International Conference on Web Information Systems and Technologies},
year={2017},
pages={382-387},
doi={10.5220/0006372803820387},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024496030&doi=10.5220%2f0006372803820387&partnerID=40&md5=92807bc8857a1f53b395d48814e2d6d9},
abstract={The adoption of multiple connected devices in our lives is a reality which the available technology is not able to deal with. The concept of Liquid Software emerged in the end of the 90s, however, its full potential of a unified interface which can drift between different connected devices and bring with its behavior and complexities is still not fully applied. Thus, enhancements of current Web application architecture, in other words, a new approach able to deal with our technology requirements is required. In this context, we propose a centric-basic architecture to deal with Liquid Software principles and constraints. The CUBE, once built, should be able to deal with all these requirements, making use of best practices from different technologies. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={API;  CUBE model;  HTTP;  Liquid software;  RESTful;  Web services},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2017,
title={Proceedings of SPIE - The International Society for Optical Engineering},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2017},
volume={10196},
page_count={207},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023206893&partnerID=40&md5=99e1b6321b85b7a42c6aa8dd2e3a0399},
abstract={The proceedings contain 20 papers. The topics discussed include: an update on the OpenOrbiter I mission and its paradigm's benefits for the defense, homeland security and intelligence communities; an adaptive software defined radio design based on a standard space telecommunication radio system API; machine learning based intelligent cognitive network using fog computing; joint sparsity based heterogeneous data-level fusion for target detection and estimation; an orbital emulator for pursuit-evasion game theoretic sensor management; war-gaming application for future space systems acquisition: part 2 - acquisition and bidding war-gaming modeling and simulation approaches for FFP and FPIF; cooperative angle-only orbit initialization via fusion of admissible areas; war-gaming application for future space systems acquisition: part 1 - program and technical baseline war-gaming modeling and simulation approaches; consideration of materials for creating 3D printed space sensors and systems; detecting faint nearby companions to geostationary satellites with optical interferometry; CubeSat mechanical design: creating low mass and durable structures; irradiation effect on back-gate graphene field-effect transistor; low-cost satellite mechanical design and construction; and electrical design for origami solar panels and a small spacecraft test mission.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Álvarez2017256,
author={Álvarez, D. and Prieto, D. and Asensio, M.I. and Cascón, J.M. and Ferragut, L.},
title={Parallel implementation of a simplified semi-physical wildland fire spread model using openMP},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10334 LNCS},
pages={256-267},
doi={10.1007/978-3-319-59650-1_22},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021727151&doi=10.1007%2f978-3-319-59650-1_22&partnerID=40&md5=e56fab634bb9d2711cd575a43af307c8},
abstract={We present a parallel 2D version of a simplified semi-physical wildland fire spread model based on conservation equations, with convection and radiation as the main heat transfer mechanisms. This version includes some 3D effects. The OpenMP framework allows distributing the prediction operations among the available threads in a multicore architecture, thereby reducing the computational time and obtaining the prediction results much more quickly. The results from the experiments using data from a real fire in Galicia (Spain) confirm the benefits of using the parallel version. © Springer International Publishing AG 2017.},
author_keywords={OpenMP;  Parallel computing;  Performance;  Wildland fire model},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor20171,
title={24th International Conference on Computer Networks, CN 2017},
journal={Communications in Computer and Information Science},
year={2017},
volume={718},
pages={1-457},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020466057&partnerID=40&md5=252403e54be6f9ae7ae072766c12debe},
abstract={The proceedings contain 35 papers. The special focus in this conference is on Computer Networks. The topics include: Traffic flows ateb-prediction method with fluctuation modeling using Dirac functions; improving accuracy of a network model basing on the case study of a distributed system with a mobile application and an API; method for determining effective diagnostic structures within the military IoT networks; QOS-based power control and resource allocation in cognitive LTE-femtocell networks; secure and reliable localization in wireless sensor network based on RSSI mapping; application of fault-tolerant GQP algorithm in multihop AMI networks; evaluation of connectivity gaps impact on TCP transmissions in maritime communications; path loss model for a wireless sensor network in different weather conditions; behavioral analysis of bot activity in infected systems using honeypots; impact of histogram construction techniques on information - theoretic anomaly detection; information technology for botnets detection based on their behaviour in the corporate area network; utilization of redundant communication network throughput for non-critical data exchange in networked control systems; software defined home network for distribution of the svc video based on the dash principles; minimum transmission range estimation for vehicular ad hoc networks in signalised arterials; the possibilities and limitations of the application of the convolution algorithm for modeling network systems; an efficient method for calculation of the radiation from copper installations with wideband transmission systems and analytical modelling of multi-tier cellular networks with traffic overflow.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Shaji2017734,
author={Shaji, A. and Lokeshwaran, N.},
title={Comparative analytical investigation of 2D steel frames subjected to lateral load using steel cable bracing},
journal={International Journal of Civil Engineering and Technology},
year={2017},
volume={8},
number={4},
pages={734-743},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019157423&partnerID=40&md5=0af290fffab114858184a85b8d62696e},
abstract={Steel structures can be strengthened against lateral loads in different ways. The most common and efficient way of strengthening steel structures is by providing bracing systems to them. From the past studies, it's clear that braced frames or bracing systems helps in maintaining more structural stability or gives better resistance when lateral loads are applied. The efficiency of the braced frame when compared with unbraced frames depends upon the selection of bracing system. The rapid development in construction industry globally demands for new bracing systems. So, this paper deals with the analytical study of a new and innovative steel cable bracing connection for strengthening and for reducing the effect of lateral load acting to the structure. For this purpose, G+11 multi storied 2D steel frames were considered. The 2D steel frames considered for this purpose are same in dimension but differentiable by their bracing systems provided, which includes Unbraced, Single diagonal braced, X braced, V braced, Inverted V braced, K braced, Knee braced and steel cable braced frames. ANSYS Mechanical APDL 15.0 software package have been used for getting analytical values like displacement, bending moment and shear force. Design and load calculations had been done as per Indian Standard codes. © IAEME Publication.},
author_keywords={2D steel cable bracing;  Bracing system;  Displacement;  Lateral load;  Steel structures},
document_type={Article},
source={Scopus},
}

@ARTICLE{Savino201771,
author={Savino, S. and Orio, N.},
title={Searching and exploring data in a software architecture for film-induced tourism},
journal={Communications in Computer and Information Science},
year={2017},
volume={701},
pages={71-81},
doi={10.1007/978-3-319-56300-8_7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018700161&doi=10.1007%2f978-3-319-56300-8_7&partnerID=40&md5=20f2e25939dc45c3cd0f0c6412622d83},
abstract={Film induced tourism is a recent phenomenon, rising increasing interest in tourism management and promotion. A research project on this topic has been recently investigated at the Department of Cultural Heritage of the University of Padua, resulting in the development of a software architecture for the promotion of film-induced tourism, capable of storing and providing rich information about the movies produced in a selected geographic area. This paper presents the design and implementation of the solutions developed to search and explore the data stored in the software architecture. © Springer International Publishing AG 2017.},
author_keywords={Film annotation;  Film-induced tourism;  Personalization;  User interfaces},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Macheta2017323,
author={Macheta, J. and Dabrowska-Boruch, A. and Russek, P. and Wiatr, K.},
title={ArPALib: A big number arithmetic library for hardware and software implementations. A case study for the miller–rabin primality test},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10216 LNCS},
pages={323-330},
doi={10.1007/978-3-319-56258-2_28},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017557029&doi=10.1007%2f978-3-319-56258-2_28&partnerID=40&md5=0db3cfdec1c99a42a0f2ee837dc04fff},
abstract={In this paper, we present the Arbitrary Precision Arithmetic Library - ArPALib, suitable for algorithms that require integer data representation with an arbitrary bit-width (up to 4096-bit in this study). The unique feature of the library is suitability to be synthesized by HLS (High Level Synthesis) tools, while maintaining full compatibility with C99 standard. To validate the applicability of ArPALib for the FPGAenhanced SoCs, the Miller-Rabin primality test algorithm is considered as a case study. Also, we provide the performance analysis of our library in the software and hardware applications. The presented results show the speedup of 1.5 of the hardware co-processor over its software counterpart when ApPALib is used. © Springer International Publishing AG 2017.},
author_keywords={Big numbers;  FPGA;  High-Level Synthesis;  Primality tests},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sangtiani20171014,
author={Sangtiani, S. and Simon, J. and Satyanarayana, J. and Sangtiani, D.},
title={Performance of tall buildings under lateral loads with different type of structural systems},
journal={International Journal of Civil Engineering and Technology},
year={2017},
volume={8},
number={3},
pages={1014-1022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016822828&partnerID=40&md5=80490e88de3520ec6097a2f655686daf},
abstract={It is acceptable that construction industry is reaching to new bench marks by constructing certain Tall Buildings, includes of significant engineering. Construction of Tall buildings may be a complicated affair with the lateral loads playing a dominant role in design of the certain structures, where buildings need certain lateral load resisting structural system. With the evolution of technology numerous structural systems came out such as Shear Wall System, Tube in Tube, Core Out-trigger System etc. Considering the Building G+53 story's concrete Structure which is analyzed in ETABS V16.0.0 software package with different Earthquake Zones and using different Structural Systems such as Conventional System, Shear wall System, Core Out-trigger System and Wind forces exerted from IS-875-2015 (Gust Analysis), an attempt is made to compare the Performance of the three Structural Systems in all four earthquake zones Base shear, time period, top story displacement, story Drift, seismic weight of structure, and results are compared to arrive the foremost economical structure in a specific Earthquake Zone for a particular plan. © IAEME Publication.},
author_keywords={Reinforced cement concrete;  Response spectrum analysis;  Structural systems;  Wind gust analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lakshmi2017806,
author={Lakshmi, P. and Asadi, S.S. and Raju, P.P. and Chandra, D.S.},
title={An integrated approach for site selection by using remote sensing and design of group houses in and around Vijayawada: A model study},
journal={International Journal of Civil Engineering and Technology},
year={2017},
volume={8},
number={3},
pages={806-815},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016715082&partnerID=40&md5=0529075f5320030ba8d93a9bfbc25237},
abstract={In order to competent in the ever growing competent market, it is very important for a structural engineer to save time. As a sequel to this, an attempt is made to analyse and design of Multi-storeyed Building by using a software package Staad Pro. In addition, the cost of the whole building has been estimated. Shelter is the basic necessity for everyone. The Rental market in Vijayawada for housing has been increased, as it became the new capital city of Andhra Pradesh, low and middle class people cannot afford this cost. So it is better to own a house with low cost. For analysing these low, medium and high income group houses one has to consider all the possible loadings and observe that the structure is safe against all possible loading conditions or not. The Staad Pro software is a very effective tool which can save much time and is very accurate in analysis and design. In connection to above mentioned, for low-cost housing, the site selection has been done by using GIS. The group housing is a better approach which decreases the overall cost of construction, which turns benefit to the people who owns this house. This construction based on special and individual needs by performing surveys on population needs and rational use of materials and resources. © IAEME Publication.},
author_keywords={Estimation;  Low;  Medium and high income houses;  Pro;  RIS & GIS;  STAAD},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2017119,
author={Li, Y. and He, J. and Jiang, Y. and An, L.},
title={Semi-physical simulation of AUV homing and docking processes},
journal={Jiqiren/Robot},
year={2017},
volume={39},
number={1},
pages={119-128},
doi={10.13973/j.cnki.robot.2017.0119},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016172494&doi=10.13973%2fj.cnki.robot.2017.0119&partnerID=40&md5=b37bc70ca7b60810962613cf5105e62c},
abstract={Existing simulation systems ignore the ocean current interference, so that the real-time motion state of the model can't be directly reflected. To solve the problem, a semi-physical simulation system based on 3D visual scene is designed to directly display the motion states of the AUV (autonomous underwater vehicle) during the homing process and the recovery process through docking like a helicopter landing. The AUV and the underwater terrain are modeled using Multigen Creator software. The visual simulation is realized by calling the simulation interface library functions of Vega software in Visual C++. For motion control, a global path planning method for homing is designed based on B-spline theory, and a global path satisfying underactuation constraints is searched by genetic algorithm. A path tracking controller is designed by the layered structure of guidance control and executive control, in which the PID (proportional-integral-differential) guidance law can adjust the reference attitude angle according to the ocean current information, and the S-plane control law can make stable, fast and accurate response to the status information and the reference attitude angle. All the processes are simulated on the test platform, including the AUV starting from the release position, autonomously homing by tracking the calculated global path, till docking. The results show that the influence of ocean current on the path tracking deviation of AUV is small in homing process, but its influence in docking process is very large when the relative angle between the ocean current and the heading direction is big, which maybe cause a recovery failure. A reasonable homing path can be planned for AUV to complete the recovery process under the influence of ocean current, and all the processes can be directly reflected in real-time in the designed semi-physical simulation system. © 2017, Science Press. All right reserved.},
author_keywords={Autonomous underwater vehicle;  Path planning;  Semi-physical simulation;  Underwater recovery},
document_type={Article},
source={Scopus},
}

@ARTICLE{Koyuncu2017,
author={Koyuncu, I. and Şahin, I. and Gloster, C. and Sarltekin, N.K.},
title={A neuron library for rapid realization of artificial neural networks on FPGA: A case study of rössler chaotic system},
journal={Journal of Circuits, Systems and Computers},
year={2017},
volume={26},
number={1},
doi={10.1142/S0218126617500153},
art_number={1750015},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984819095&doi=10.1142%2fS0218126617500153&partnerID=40&md5=8ed5756767f4637eece3aac917ba590a},
abstract={Artificial neural networks (ANNs) are implemented in hardware when software implementations are inadequate in terms of performance. Implementing an ANN as hardware without using design automation tools is a time consuming process. On the other hand, this process can be automated using pre-designed neurons. Thus, in this work, several artificial neural cells were designed and implemented to form a library of neurons for rapid realization of ANNs on FPGA-based embedded systems. The library contains a total of 60 different neurons, two-, four- and six-input biased and non-biased, with each having 10 different activation functions. The neurons are highly pipelined and were designed to be connected to each other like Lego pieces. Chip statistics of the neurons showed that depending on the type of the neuron, about 25 selected neurons can be fit in to the smallest Virtex-6 chip and an ANN formed using the neurons can be clocked up to 576.89MHz. ANN based Rössler system was constructed to show the effectiveness of using neurons in rapid realization of ANNs on embedded systems. Our experiments with the neurons showed that using these neurons, ANNs can rapidly be implemented as hardware and design time can significantly be reduced. © World Scientific Publishing Company.},
author_keywords={artificial neural networks;  FPGA;  neuron library;  Rössler system;  VHDL},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Edenhofer2016160,
author={Edenhofer, S. and Rädler, S. and Hoß, M. and Von Mammen, S.},
title={Self-organised construction with Revit},
journal={Proceedings - IEEE 1st International Workshops on Foundations and Applications of Self-Systems, FAS-W 2016},
year={2016},
pages={160-161},
doi={10.1109/FAS-W.2016.44},
art_number={7789461},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011115941&doi=10.1109%2fFAS-W.2016.44&partnerID=40&md5=a7858986717cc1096a4173d56c215ad4},
abstract={Due to innovations in software, robotics and 3D printing, self-organised construction is within reach. It bears great potential for the automatic generation of a wide variety of designs, their integration into the built environment, their structural and automatised optimisation, as well as their dynamic adaptation over long periods of time. In this paper, motivated by the latest empirical findings on the construction methods of social insects, we present a software pipeline for the generation of architectural designs based on self-organisation. A probabilistic, grid-based multi-Agent system that implements a flexible stigmergy-based behavioural construction model generates threedimensional structures. Next, the generated structures could be evaluated in terms of their energy efficiency and these results be fed into an optimisation engine to improve the local behaviours of the construction agents. For visualisation and evaluation of the generated designs we utilise the API of the framework Revit 20161, a software for Building Information Modelling (BIM) provided by Autodesk. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murvay2016117,
author={Murvay, P.-S. and Matei, A. and Solomon, C. and Groza, B.},
title={Development of an AUTOSAR compliant cryptographic library on state-of-the-art automotive grade controllers},
journal={Proceedings - 2016 11th International Conference on Availability, Reliability and Security, ARES 2016},
year={2016},
pages={117-126},
doi={10.1109/ARES.2016.60},
art_number={7784562},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015307838&doi=10.1109%2fARES.2016.60&partnerID=40&md5=efac903463ed2d5af017349cde326df0},
abstract={In the light of the recently reported attacks on intra-vehicle networks, it has become clear that cryptography is vital for assuring the security of in-vehicle communications. The current preoccupation of industry professionals in this direction is proved by the inclusion of a comprehensive cryptographic extension in the recent-most version of the AUTOSAR (AUTomotive Open System ARchitecture) standard. In this work we try to give an answer on how prepared are current state-of-the-art automotive controllers for implementing cryptographic primitives and what is the exact cost of software implementations. We take into account automotive grade controllers that range from some of the most constrained platforms, e.g., from 8051 based tire sensors with 8-bit cores, up to 32-bit Infineon TriCore architectures, as well as devices that lay in between these two. We provide experimental results on several symmetric cryptographic primitives, i.e., block ciphers and hash functions, mainly focusing on the lightest constructions proposed in the literature, e.g., Speck, Katan, Blake, as well as on past or current standards, e.g., AES, SHA2 or SHA3. As expected, the results are sparse, some of the platforms being well prepared, capable to easily handle software implementation or carrying dedicated hardware, while for others no dedicated hardware exists while software implementation of current cryptographic standards cannot be handled, especially with the overhead incurred by the cohesion to the AUTOSAR standard. © 2016 IEEE.},
author_keywords={Automotive;  AUTOSAR;  Cryptographic primitives library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Balogh2016137,
author={Balogh, G. and Gergely, T. and Beszedes, A. and Gyimothy, T.},
title={Are my unit tests in the right package?},
journal={Proceedings - 2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation, SCAM 2016},
year={2016},
pages={137-146},
doi={10.1109/SCAM.2016.10},
art_number={7781807},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010809669&doi=10.1109%2fSCAM.2016.10&partnerID=40&md5=2a59273f9ecc326f6dfe47285fad71ed},
abstract={The software development industry has adopted written and de facto standards for creating effective and maintainable unit tests. Unfortunately, like any other source code artifact, they are often written without conforming to these guidelines, or they may evolve into such a state. In this work, we address a specific type of issues related to unit tests. We seek to automatically uncover violations of two fundamental rules: 1) unit tests should exercise only the unit they were designed for, and 2) they should follow a clear packaging convention. Our approach is to use code coverage to investigate the dynamic behaviour of the tests with respect to the code elements of the program, and use this information to identify highly correlated groups of tests and code elements (using community detection algorithm). This grouping is then compared to the trivial grouping determined by package structure, and any discrepancies found are treated as 'bad smells.' We report on our related measurements on a set of large open source systems with notable unit test suites, and provide guidelines through examples for refactoring the problematic tests. © 2016 IEEE.},
author_keywords={Clusterization;  Code coverage;  Community detection;  Package hierarchy;  Test smells and refactoring;  Unit testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sarkar2016274,
author={Sarkar, P. and Mukhopadhyay, P.},
title={Full-text etd retrieval in library discovery system: Designing a framework},
journal={Annals of Library and Information Studies},
year={2016},
volume={63},
number={4},
pages={274-288},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010410429&partnerID=40&md5=1cf2bd54c2793c0279bfe00132736bb1},
abstract={This paper discusses designing an open source software based library discovery system for full-text ETD retrieval on the basis of a cataloguing framework developed by using available global standards and best practices in the domain of theses cataloguing. The purpose of this prototype framework is to provide a single-window search and retrieval system for end users for discovering ETD at metadata level and at full-text level. The prototype framework is based on three-layer architecture with Koha ILS as backend metadata provider, Apache-Tika as full-text extractor and VuFind as discovery system. A MARC-21 bibliographic format, especially designed to handle TDs, is working as data handler mechanism in Koha ILS and the harvester of VuFind is tuned to fetch bibliographic data related to ETD in marcxml format. The user interface of VuFind is also configured to support accessing ETDs from global-scale services like NDLTD, OATD, IndCat, ShodhGanga etc. apart from the local level ETD collection in order to provide an all-in-one search interface for users. © 2016 Institute of China Studies. All rights reserved.},
author_keywords={ETD cataloguing;  ETD retrieval system;  Full-text retrieval system;  Library discovery;  VuFind},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2016,
author={Li, S. and Wang, Y. and Wen, W. and Wang, Y. and Chen, Y. and Li, H.},
title={A data locality-aware design framework for reconfigurable sparse matrix-vector multiplication kernel},
journal={IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD},
year={2016},
volume={07-10-November-2016},
doi={10.1145/2966986.2966987},
art_number={2966987},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001094028&doi=10.1145%2f2966986.2966987&partnerID=40&md5=27f5e967e6cd22511301f771bdb55e5c},
abstract={Sparse matrix-vector multiplication (SpMV) is an important computational kernel in many applications. For performance improvement, software libraries designated for SpMV computation have been introduced, e.g., MKL library for CPUs and cuSPARSE library for GPUs. However, the computational throughput of these libraries is far below the peak floating-point performance offered by hardware platforms, because the efficiency of SpMV kernel is greatly constrained by the limited memory bandwidth and irregular data access patterns. In this work, we propose a data locality-aware design framework for FPGA-based SpMV acceleration. We first include the hardware constraints in sparse matrix compression at software level to regularize the memory allocation and accesses. Moreover, a distributed architecture composed of processing elements is developed to improve the computation parallelism. We implement the reconfigurable SpMV kernel on Convey HC-2ex and conduct the evaluation by using the University of Florida sparse matrix collection. The experiments demonstrate an average computational efficiency of 48.2%, which is a lot better than those of CPU and GPU implementations. Our FPGA-based kernel has a comparable runtime as GPU, and achieves 2.1x reduction than CPU. Moreover, our design obtains substantial saving in energy consumption, say, 9.3x and 5.6x better than the implementations on CPU and GPU, respectively. © 2016 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Facchinetti2016,
author={Facchinetti, T. and Benetti, G. and Koledoye, M.A. and Roveda, G.},
title={Design and implementation of a web-centric remote data acquisition system},
journal={IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
year={2016},
volume={2016-November},
doi={10.1109/ETFA.2016.7733698},
art_number={7733698},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996598392&doi=10.1109%2fETFA.2016.7733698&partnerID=40&md5=629f2eaa047b1caa602832376138622b},
abstract={Data acquisition systems are fundamental components of modern distributed monitoring and control systems. The wide-spreading use of standard networking technologies in industrial scenarios, such as Ethernet, and the consolidation of web-based communication protocols, architectures and tools suggest the possibility to integrate out-of-the-box components to build a robust and reliable data acquisition system. This paper describes the design and implementation of a data acquisition system suitable to collect data from distributed embedded devices equipped with sensors based on a client-server architecture. The main feature of the proposed design is the integration of a set of technologies and tools widely adopted in the development of modern web services. The server component is based on Django, a popular Python web framework. While the backend runs a PostgreSQL database, the frontend includes data visualization tools leveraging the D3 JavaScript library. Metering points can be any embedded device able to interface with the desired sensors. The client-server communication architecture supports a RESTful API, as well as the ZeroMQ communication library. Messages are encapsulated in a human-readable JSON format. An important common characteristic of aforementioned technologies and tools is to be Free/Libre and Open Source Software (FLOSS). The proposed system thus represents a successful example of integration of FLOSS components to build a web-centric data acquisition system. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kalra2016290,
author={Kalra, S. and Goel, A. and Khanna, D. and Dhawan, M. and Sharma, S. and Purandare, R.},
title={POLLUX: Safely upgrading dependent application libraries},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={290-300},
doi={10.1145/2950290.2950345},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997112138&doi=10.1145%2f2950290.2950345&partnerID=40&md5=01a7996eed9ede209bb755a8abe7bab6},
abstract={Software evolution in third-party libraries across version upgrades can result in addition of new functionalities or change in existing APIs. As a result, there is a real danger of impairment of backward compatibility. Application developers, therefore, must keep constant vigil over library enhancements to ensure application consistency, i.e., application retains its semantic behavior across library upgrades. In this paper, we present the design and implementation of POLLUX, a framework to detect applicationaffecting changes across two versions of the same dependent nonadversarial library binary, and provide feedback on whether the application developer should link to the newer version or not. POLLUX leverages relevant application test cases to drive execution through both versions of the concerned library binary, records all concrete effects on the environment, and compares them to determine semantic similarity across the same API invocation for the two library versions. Our evaluation with 16 popular, opensource library binaries shows that POLLUX is accurate with no false positives and works across compiler optimizations. © 2016 ACM.},
author_keywords={Dynamic binary analysis.;  Library upgrade;  Software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{He20161231,
author={He, W. and Wang, F.-K.},
title={Integrating a case-based reasoning shell and Web 2.0: design recommendations and insights},
journal={World Wide Web},
year={2016},
volume={19},
number={6},
pages={1231-1249},
doi={10.1007/s11280-015-0380-y},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951800976&doi=10.1007%2fs11280-015-0380-y&partnerID=40&md5=bc0e009c63b9cfa1fee5821421a19c2c},
abstract={The design and implementation of case-based reasoning (CBR) applications is time-consuming. To facilitate the development of CBR applications in various problem domains, the CBR community has created a number of CBR shells and software frameworks in the past twenty years. This paper provides a review of the state-of-the-art of CBR shells and software frameworks, highlights why the integration of Web 2.0 and CBR development tools is useful, and gives an example as to how we implement such integration. We use this example to illustrate how Web 2.0 features such as blogging functions can be integrated in a CBR system. Design recommendations and insights for implementing a Web 2.0-based CBR shell are also provided. © 2015, Springer Science+Business Media New York.},
author_keywords={Case base;  Case library;  Case-based reasoning;  CBR shell;  Web 2.0},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Danelutto201610,
author={Danelutto, M. and De Matteis, T. and Mencagli, G. and Torquati, M.},
title={A divide-and-conquer parallel pattern implementation for multicores},
journal={SEPS 2016 - Proceedings of the 3rd International Workshop on Software Engineering for Parallel Systems, co-located with SPLASH 2016},
year={2016},
pages={10-19},
doi={10.1145/3002125.3002128},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997787441&doi=10.1145%2f3002125.3002128&partnerID=40&md5=2256168cb3d05efd6788f6fce24dd761},
abstract={Divide-and-Conquer (DaC) is a sequential programming paradigm which models a large class of algorithms used in real-life applications. Although suitable to extract parallelism in a straightforward way, the parallel implementation of DaC algorithms still requires some expertise in parallel programming tools by the programmer. In this paper we aim at providing to non-expert programmers a high-level solution for fast prototyping parallel DaC programs on multicores with minimal programming effort. Following the rationale of parallel design pattern methodology, we design a C++11-compliant template interface for developing parallel DaC programs. The interface is implemented using different back-end frameworks (i.e. OpenMP, Intel TBB and FastFlow) supporting source code reuse and a certain amount of performance portability. Experiments on a 24-core Intel server show the effectiveness of our approach: with a reduced programming effort the programmer easily prototypes parallel versions with performance comparable with hand-made parallelizations. © 2016 ACM.},
author_keywords={Divide and conquer;  High-level parallel patterns},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Steindorfer2016168,
author={Steindorfer, M.J. and Vinju, J.J.},
title={Towards a software product line of trie-based collections},
journal={ACM SIGPLAN Notices},
year={2016},
volume={52},
number={3},
pages={168-172},
doi={10.1145/2993236.2993251},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084191705&doi=10.1145%2f2993236.2993251&partnerID=40&md5=0a8c4afb07bc0283072b8a310850a40b},
abstract={Collection data structures in standard libraries of programming languages are designed to excel for the average case by carefully balancing memory footprint and runtime performance. These implicit design decisions and hard-coded trade-offs do constrain users from using an optimal variant for a given problem. Although a wide range of specialized collections is available for the Java Virtual Machine (JVM), they introduce yet another dependency and complicate user adoption by requiring specific Application Program Interfaces (APIs) incompatible with the standard library. A product line for collection data structures would relieve library designers from optimizing for the general case. Furthermore, a product line allows evolving the potentially large code base of a collection family efficiently. The challenge is to find a small core framework for collection data structures which covers all variations without exhaustively listing them, while supporting good performance at the same time. We claim that the concept of Array Mapped Tries (AMTs) embodies a high degree of commonality in the sub-domain of immutable collection data structures. AMTs are flexible enough to cover most of the variability, while minimizing code bloat in the generator and the generated code. We implemented a Data Structure Code Generator (DSCG) that emits immutable collections based on an AMT skeleton foundation. The generated data structures outperform competitive hand-optimized implementations, and the generator still allows for customization towards specific workloads. © 2016 ACM.},
author_keywords={Code generation;  Hash trie;  Immutability;  Performance;  Persistent data structure;  Software product line},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kuramitsu201629,
author={Kuramitsu, K.},
title={Nez: Practical open grammar language},
journal={Onward! 2016 - Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year={2016},
pages={29-42},
doi={10.1145/2986012.2986019},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997078953&doi=10.1145%2f2986012.2986019&partnerID=40&md5=66455a1ef7af599bf38e5302cb60bb96},
abstract={Nez is a PEG(Parsing Expressing Grammar)-based open grammar language that allows us to describe complex syntax constructs without action code. Since open grammars are declarative and free from a host programming language of parsers, software engineering tools and other parser applications can reuse once-defined grammars across programming languages. A key challenge to achieve practical open grammars is the expressiveness of syntax constructs and the resulting parser performance, as the traditional action code approach has provided very pragmatic solutions to these two issues. In Nez, we extend the symbol-based state management to recognize context-sensitive language syntax, which often appears in major programming languages. In addition, the Abstract Syntax Tree constructor allows us to make flexible tree structures, including the left-Associative pair of trees. Due to these extensions, we have demonstrated that Nez can parse many grammars of practical programming languages. Nez can generate various types of parsers since all Nez operations are independent of a specific parser language. To highlight this feature, we have implemented Nez with dynamic parsing, which allows users to integrate a Nez parser as a parser library that loads a grammar at runtime. To achieve its practical performance, Nez operators are assembled into low-level virtual machine instructions, including automated state modifications when backtracking, transactional controls of AST construction, and efficient memoization in packrat parsing. We demonstrate that Nez dynamic parsers achieve very competitive performance compared to existing efficient parser generators. © 2016 ACM.},
author_keywords={And grammar specification language;  Context-sensitive grammars;  Parser generators;  Parsing expression grammars},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jia20161882,
author={Jia, C. and Li, Q. and Li, N.},
title={Design and implementation of bus real-time human traffic statistics system},
journal={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2016},
year={2016},
pages={1882-1886},
doi={10.1109/FSKD.2016.7603466},
art_number={7603466},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997755013&doi=10.1109%2fFSKD.2016.7603466&partnerID=40&md5=4c3c6ad0d0a6b394935a35bf2b09c984},
abstract={The human traffic in each bus station is estimated in real-time by means of an embedded ARM(Acorn RISC Machine) platform and the API(Application Programming Interface) in OpenCV(Open Source Computer Vision). For high-density human traffic situations, this paper proposed a method in which the Hough Circle Transform, centroid coordinates and color are used as the features for detecting, tracking and counting the moving targets. Experimental results show that the proposed method can solve the problem of counting the moving targets from the video sequences in the case of high-density human traffic. © 2016 IEEE.},
author_keywords={embedded system;  moving objects tracking;  real-time human traffic statistics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ercan2016112,
author={Ercan, M.B. and Goodall, J.L.},
title={Design and implementation of a general software library for using NSGA-II with SWAT for multi-objective model calibration},
journal={Environmental Modelling and Software},
year={2016},
volume={84},
pages={112-120},
doi={10.1016/j.envsoft.2016.06.017},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977142839&doi=10.1016%2fj.envsoft.2016.06.017&partnerID=40&md5=29a8b91d1497ce8b6ffe74646da6a26f},
abstract={Calibrating watershed-scale hydrologic models remains a critical but challenging step in the modeling process. The Soil and Water Assessment Tool (SWAT) is one example of a widely used watershed-scale hydrologic model that requires calibration. The calibration algorithms currently available to SWAT modelers through freely available and open source software, however, are limited and do not include many multi-objective genetic algorithms (MOGAs). The Non-Dominated Sorting Genetic Algorithm II (NSGA-II) has been shown to be an effective and efficient MOGA calibration algorithm for a wide variety of applications including for SWAT model calibration. Therefore, the objective of this study was to create an open source software library for multi-objective calibration of SWAT models using NSGA-II. The design and implementation of the library are presented, followed by a demonstration of the library through a test case for the Upper Neuse Watershed in North Carolina, USA using six objective functions in the model calibration. © 2016 Elsevier Ltd},
author_keywords={Genetic algorithms;  Multi-objective calibration;  NSGA-II;  SWAT;  Watershed modeling},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dagand2016298,
author={Dagand, P.-E. and Tabareau, N. and Tanter, É.},
title={Partial type equivalences for verified dependent interoperability},
journal={ACM SIGPLAN Notices},
year={2016},
volume={51},
number={9},
pages={298-310},
doi={10.1145/2951913.2951933},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084497989&doi=10.1145%2f2951913.2951933&partnerID=40&md5=0da582820eeacc5ed8bc54ebe6da7c49},
abstract={Full-spectrum dependent types promise to enable the development of correct-by-construction software. However, even certified software needs to interact with simply-Typed or untyped programs, be it to perform system calls, or to use legacy libraries. Trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simply-Typed values can safely be coerced to dependent types and, conversely, dependently-Typed programs can defensively be exported to a simply-Typed application. In this paper, we give a semantic account of dependent interoperability. Our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. Specifically, we develop the notion of partial type equivalences as a key foundation for dependent interoperability. Our framework is developed in Coq; it is thus constructive and verified in the strictest sense of the terms. Using our library, users can specify domain-specific partial equivalences between data structures. Our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. It thus becomes possible, as we shall illustrate, to internalize and hand-Tune the extraction of dependently-Typed programs to interoperable OCaml programs within Coq itself. © 2016 ACM.},
author_keywords={dependent types;  interoperability;  type equivalences},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sheikhizadeh2016i487,
author={Sheikhizadeh, S. and Schranz, M.E. and Akdel, M. and De Ridder, D. and Smit, S.},
title={PanTools: Representation, storage and exploration of pan-genomic data},
journal={Bioinformatics},
year={2016},
volume={32},
number={17},
pages={i487-i493},
doi={10.1093/bioinformatics/btw455},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990911225&doi=10.1093%2fbioinformatics%2fbtw455&partnerID=40&md5=b19a8101b4e3e3a33b2dc4eaca243848},
abstract={Motivation: Next-generation sequencing technology is generating a wealth of highly similar genome sequences for many species, paving the way for a transition from single-genome to pan-genome analyses. Accordingly, genomics research is going to switch from reference-centric to pan-genomic approaches. We define the pan-genome as a comprehensive representation of multiple annotated genomes, facilitating analyses on the similarity and divergence of the constituent genomes at the nucleotide, gene and genome structure level. Current pan-genomic approaches do not thoroughly address scalability, functionality and usability. Results: We introduce a generalized De Bruijn graph as a pan-genome representation, as well as an online algorithm to construct it. This representation is stored in a Neo4j graph database, which makes our approach scalable to large eukaryotic genomes. Besides the construction algorithm, our software package, called PanTools, currently provides functionality for annotating pan-genomes, adding sequences, grouping genes, retrieving gene sequences or genomic regions, reconstructing genomes and comparing and querying pan-genomes. We demonstrate the performance of the tool using datasets of 62 E. coli genomes, 93 yeast genomes and 19 Arabidopsis thaliana genomes. Availability and Implementation: The Java implementation of PanTools is publicly available at http://www.bif.wur.nl. © 2016 The Author 2016. Published by Oxford University Press. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Aranki2016282,
author={Aranki, D. and Kurillo, G. and Mani, A. and Azar, P. and Gaalen, J.V. and Peng, Q. and Nigam, P. and Reddy, M.P. and Sankavaram, S. and Wu, Q. and Bajcsy, R.},
title={A Telemonitoring Framework for Android Devices},
journal={Proceedings - 2016 IEEE 1st International Conference on Connected Health: Applications, Systems and Engineering Technologies, CHASE 2016},
year={2016},
pages={282-291},
doi={10.1109/CHASE.2016.28},
art_number={7545843},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987657193&doi=10.1109%2fCHASE.2016.28&partnerID=40&md5=0ff06c90099244cf32d5c96add8f6b7f},
abstract={Real-time telemonitoring of patient's well-being through various wearable sensors and other electronic accessories holds promise to provide better quality healthcare at lower costs. The design and implementation of telemonitoring applications is however still a cumbersome process as it requires implementation of user interfaces, data acquisition, data storage, and proper security and privacy mechanisms using various APIs. This process requires a high level of experience in software development and design, as well as a certain level of knowledge in the healthcare domain. The multi-disciplinary nature of such applications limits the growth of telemonitoring. In addition, a large number of applications aim to use smartphone-based monitoring, which adds an extra level of complexity due to the fault-prone nature of such systems. In this paper, we describe a general-purpose framework that can be used to easily implement telemonitoring applications on Android-enabled devices. © 2016 IEEE.},
author_keywords={chronic health conditions;  mHealth;  telemonitoring},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{UrRahman2016462,
author={Ur Rahman, M.F. and Naveen, D.},
title={Verification of a digital video broadcasting - Satellite to handheld (DVB-SH) IP using UVM},
journal={Proceedings - 2016 2nd International Conference on Computational Intelligence and Communication Technology, CICT 2016},
year={2016},
pages={462-467},
doi={10.1109/CICT.2016.98},
art_number={7546652},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987653170&doi=10.1109%2fCICT.2016.98&partnerID=40&md5=afa88c48a5c375ae9b2e8d53bc0f3a92},
abstract={Today's SoCs design and verification Environment is increasingly complex in nature which throws challenges to a verificationEngineer. This complexity demands various skills like understanding of different architectures, analysis, debugging and learning various software and HDL & HVL languages like C, C ++, Verilog, SystemVerilog (SV), Universal Verification Methodology (UVM), that codifies the best practices for an efficient and exhaustive verification by creation of robust, reusable and interoperable verification IP and test bench components. This Paper proposes the development of UVM test bench environment to verify ordering Rules for DVBSH. Digital Video Broadcasting - Satellite to Hand held (DVB-SH) IP is the name of a mobile broadcast standard designed to deliver video, audio and dataservices to small handheld devices such as mobile telephone and to vehicle mounted devices. The DVB-SH IP along the serial datapath by employing the stimulus Directed testing approach in SV and UVM environment. DVBSH IP is verified by writing the test cases to test various features to ensure the functionality ofDVB-SH as per the protocol specification. The verification is done using the SV classes and UVM environment components like Driver, Monitor, Environment, Agent, Test, Top, etc., forchecking the functionality and fixing the bugs associated with it using VCS debugger to ensure the correctness of output. The desired output which is strategically placed in scoreboard iscompared with the output obtained from the DUT. Verification has been done by using a vcsmx-vJ-2014.12-1 tool from Synopsys and with the UVM 1.1 library. © 2016 IEEE.},
author_keywords={Environment;  I2C;  SoC;  SystemVerilog;  Testbench;  UVM;  Verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dagand2016298,
author={Dagand, P.-E. and Tabareau, N. and Tanter, E.},
title={Partial type equivalences for verified dependent interoperability},
journal={ICFP 2016 - Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
year={2016},
pages={298-310},
doi={10.1145/2951913.2951933},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054972887&doi=10.1145%2f2951913.2951933&partnerID=40&md5=11711fa2df04bfedd59e0809ff882d3d},
abstract={Full-spectrum dependent types promise to enable the development of correct-by-construction software. However, even certified software needs to interact with simply-typed or untyped programs, be it to perform system calls, or to use legacy libraries. Trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simplytyped values can safely be coerced to dependent types and, conversely, dependently-typed programs can defensively be exported to a simply-typed application. In this paper, we give a semantic account of dependent interoperability. Our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. Specifically, we develop the notion of partial type equivalences as a key foundation for dependent interoperability. Our framework is developed in Coq; it is thus constructive and verified in the strictest sense of the terms. Using our library, users can specify domain-specific partial equivalences between data structures. Our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. It thus becomes possible, as we shall illustrate, to internalize and hand-tune the extraction of dependently-typed programs to interoperable OCaml programs within Coq itself. © 2016 ACM.},
author_keywords={Dependent types;  Interoperability;  Type equivalences},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kallel2016163,
author={Kallel, E. and Aoudni, Y. and Abid, M.},
title={Java-based approach for high level OpenMP loops synthesis},
journal={2016 IEEE/ACIS 14th International Conference on Software Engineering Research, Management and Applications, SERA 2016},
year={2016},
pages={163-170},
doi={10.1109/SERA.2016.7516142},
art_number={7516142},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983385448&doi=10.1109%2fSERA.2016.7516142&partnerID=40&md5=71ccfa27e0d318b52397295b6656bdd5},
abstract={This paper presents an automatic VHDL code generation method based on the OpenMP parallel programming specification. In order to synthesize C code for loops into hardware, we applied the directives of OpenMP, which specifies portable implementations of shared memory parallel programs. The proposed design flow using this method is described and its implementation details are provided. Experimental results show that the generated vhdl code from OpenMP is competitive with optimized code. © 2016 IEEE.},
author_keywords={C for loops;  Java packages;  OpenMP directives;  VHDL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Minato2016143,
author={Minato, S.-I.},
title={Power of enumeration - BDD/ZDD-based techniques for discrete structure manipulation},
journal={Proceedings of The International Symposium on Multiple-Valued Logic},
year={2016},
volume={2016-July},
pages={143},
doi={10.1109/ISMVL.2016.49},
art_number={7515537},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981321204&doi=10.1109%2fISMVL.2016.49&partnerID=40&md5=2d110c1f165402e5b8d0f4af1cb387a1},
abstract={Discrete structures are foundational materials for computer science and mathematics, which are related to set theory, symbolic logic, inductive proof, graph theory, combinatorics, probability theory, etc. Many problems are decomposed into discrete structures using simple primitive algebraic operations. A Binary Decision Diagram (BDD) is a representation of a Boolean function, one of the most basic models of discrete structures. After the epoch-making paper [1] by Bryant in 1986, BDD-based methods have attracted a great deal of attention. The BDD was originally developed for the efficient Boolean function manipulation required in VLSI logic design, however, later they are also used for sets of combinations which represent many kinds of combinatorial patterns. A Zero-suppressed BDD (ZDD) [2] is a variant of the BDD, customized for representing a set of combinations. ZDDs have been successfully applied not only to VLSI design, but also for solving various combinatorial problems, such as constraint satisfaction, frequent pattern mining, and graph enumeration. Recently, ZDDs have become more widely known, since D. E. Knuth intensively discussed ZDD-based algorithms in the latest volume of his famous series of books [3]. Although a quarter of a century has passed since Bryant first put forth his idea, there are still many interesting and exciting research topics related to BDDs and ZDDs [4]. One of the most important topics would be that, Knuth presented an extremely fast algorithm "Simpath" [3] to construct a ZDD which enumerates all the paths connecting two points in a given graph structure. This work is important because many kinds of practical problems are efficiently solved by some variations of this algorithm. We generically call such ZDD construction methods "frontier-based methods." The above techniques of data structures and algorithms have been implemented and published as an open software library, named "Graphillion"[5], [6]. Graphillion is a library for manipulating very large sets of graphs, based on ZDDs and frontier-based method. Graphillion is implemented as a Python extension in C++, to encourage easy development of its applications without introducing significant performance overhead. In order to organize an integrated method of algebraic operations for manipulating various types of discrete structures, and to construct standard techniques for efficiently solving large-scale and practical problems in various fields, A governmental agency in Japan started a nation-wide project: ERATO MINATO Discrete Structure Manipulation System Project in 2009. The project was successfully finished in this year, and a successor project, JSPS KAKENHI(S), is now running until 2020. Many interesting research results were produced in the last ERATO project, and some of topics are still attractive to be explored more. In this talk, we first show an overview of our research project, and then explain the basic techniques of BDDs and ZDDs used for various discrete structure manipulation. We also present a brief history of the research activity related to BDDs and ZDDs. We then show an overview of the frontier-based method for efficiently enumerating and indexing the solutions of combinatorial problems. We also present several topics on various applications of those stateof- the-art techniques. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2016663,
author={Guo, F. and Yu, L.},
title={Design and implementation of TV two-dimensional code publishing system},
journal={Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
year={2016},
volume={0},
pages={663-666},
doi={10.1109/ICSESS.2016.7883155},
art_number={7883155},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016992336&doi=10.1109%2fICSESS.2016.7883155&partnerID=40&md5=99f665c97afa9e591fa265243b8e0a6a},
abstract={The emergence of TV two-dimensional code not only makes the traditional TV media and new media to connect more closely, but also achieves the 'mobile-television-network' direct interaction, greatly promoting the trend of media convergence. This paper, basing on the two-dimensional code generation techniques-PHP QR code, uses the MVC framework and Open API technology to design and complete the TV two-dimensional code publishing system, and finally realizes dual screen interaction between mobile phone and TV. © 2016 IEEE.},
author_keywords={Open API;  publishing system;  QR code;  two-dimensional code},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Aichhorn2016200,
author={Aichhorn, M. and Pourovskii, L. and Seth, P. and Vildosola, V. and Zingl, M. and Peil, O.E. and Deng, X. and Mravlje, J. and Kraberger, G.J. and Martins, C. and Ferrero, M. and Parcollet, O.},
title={TRIQS/DFTTools: A TRIQS application for ab initio calculations of correlated materials},
journal={Computer Physics Communications},
year={2016},
volume={204},
pages={200-208},
doi={10.1016/j.cpc.2016.03.014},
note={cited By 94},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992295854&doi=10.1016%2fj.cpc.2016.03.014&partnerID=40&md5=20adaa27ac1610695ce05ccd1e56bdc0},
abstract={We present the TRIQS/DFTTools package, an application based on the TRIQS library that connects this toolbox to realistic materials calculations based on density functional theory (DFT). In particular, TRIQS/DFTTools together with TRIQS allows an efficient implementation of DFT plus dynamical mean-field theory (DMFT) calculations. It supplies tools and methods to construct Wannier functions and to perform the DMFT self-consistency cycle in this basis set. Post-processing tools, such as band-structure plotting or the calculation of transport properties are also implemented. The package comes with a fully charge self-consistent interface to the Wien2k band structure code, as well as a generic interface that allows to use TRIQS/DFTTools together with a large variety of DFT codes. It is distributed under the GNU General Public License (GPLv3). © 2016 Elsevier B.V. All rights reserved.},
author_keywords={ab-initio calculations;  Dynamical mean-field theory;  Many-body physics;  Strongly-correlated electrons},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ra201672,
author={Ra, H.-K. and Yoon, H.J. and Salekin, A. and Lee, J.-H. and Stankovic, J.A. and Son, S.H.},
title={Poster: Software architecture for efficiently designing cloud applications using node.js},
journal={MobiSys 2016 Companion - Companion Publication of the 14th Annual International Conference on Mobile Systems, Applications, and Services},
year={2016},
pages={72},
doi={10.1145/2938559.2948790},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984979160&doi=10.1145%2f2938559.2948790&partnerID=40&md5=1dbbe74e19c9e100be23d5bfdc86e305},
abstract={We propose a practical solution for cloud application de- velopment using Node.js and Express library by presenting: (1) a software architecture which utilizes two standard in- heritance pattern techniques, the top-down and divide and conquer approaches, to effectively organize the structure of the application for improved maintainability and extensi- bility in the long-run, and (2) an easy-to-follow guideline that instructs the implementation procedures for develop- ing Node.js cloud applications. © 2016 Copyright held by the owner/author(s).},
author_keywords={Cloud applications;  Node.js;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Afanasiev2016,
author={Afanasiev, M. and Boehm, C. and Gokhberg, A. and Fichtner, A.},
title={Automatic Global multiscale seismic inversion: Insights into model, data, and workflow management},
journal={PASC 2016 - Proceedings of the Platform for Advanced Scientific Computing Conference},
year={2016},
doi={10.1145/2929908.2929910},
art_number={2929910},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978701715&doi=10.1145%2f2929908.2929910&partnerID=40&md5=03063ef9137b28f778e273ad9b07d201},
abstract={Modern global seismic waveform tomography is formulated as a PDE-constrained nonlinear optimization problem, where the optimization variables are Earth's visco-elastic parameters. This particular problem has several defining characteristics. First, the solution to the forward problem, which involves the numerical solution of the elastic wave equation over continental to global scales, is computationally expensive. Second, the determinedness of the inverse problem varies dramatically as a function of data coverage. This is chiey due to the uneven distribution of earthquake sources and seismometers, which in turn results in an uneven sampling of the parameter space. Third, the seismic wavefield depends nonlinearly on the Earth's structure. Sections of a seismogram which are close in time may be sensitive to structure greatly separated in space. In addition to these theoretical difficulties, the seismic imaging community faces additional issues which are common across HPC applications. These include the storage of massive checkpoint files, the recovery from generic system failures, and the management of complex workows, among others. While the community has access to solvers which can harness modern heterogeneous computing architectures, the computational bottleneck has fallen to these memory- And manpower-bounded issues. We present a two-tiered solution to the above problems. To deal with the problems relating to computational expense, data coverage, and the increasing nonlinearity of waveform tomography with scale, we present the Collaborative Seismic Earth Model (CSEM). This model, and its associated framework, takes an open-source approach to globalscale seismic inversion. Instead of attempting to monolithically invert all available seismic data, the CSEM approach focuses on the inversion of specific geographic subregions, and then consistently integrates these subregions via a common computational framework. To deal with the workow and storage issues, we present a suite of workow management software, along with a custom designed optimization and data compression library. It is the goal of this paper to synthesize these above concepts, originally developed in isolation, into components of an automatic global-scale seismic inversion. © 2016 ACM.},
author_keywords={Compression;  Full-waveform inversion;  Seismic tomography;  Workow management},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Parton2016,
author={Parton, D.L. and Grinaway, P.B. and Hanson, S.M. and Beauchamp, K.A. and Chodera, J.D.},
title={Ensembler: Enabling High-Throughput Molecular Simulations at the Superfamily Scale},
journal={PLoS Computational Biology},
year={2016},
volume={12},
number={6},
page_count={25},
doi={10.1371/journal.pcbi.1004728},
art_number={e1004728},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978900622&doi=10.1371%2fjournal.pcbi.1004728&partnerID=40&md5=93e79e9e65e22bb474884f2a8f6610e8},
abstract={The rapidly expanding body of available genomic and protein structural data provides a rich resource for understanding protein dynamics with biomolecular simulation. While computational infrastructure has grown rapidly, simulations on an omics scale are not yet widespread, primarily because software infrastructure to enable simulations at this scale has not kept pace. It should now be possible to study protein dynamics across entire (super)families, exploiting both available structural biology data and conformational similarities across homologous proteins. Here, we present a new tool for enabling high-throughput simulation in the genomics era. Ensembler takes any set of sequences—from a single sequence to an entire superfamily—and shepherds them through various stages of modeling and refinement to produce simulation-ready structures. This includes comparative modeling to all relevant PDB structures (which may span multiple conformational states of interest), reconstruction of missing loops, addition of missing atoms, culling of nearly identical structures, assignment of appropriate protonation states, solvation in explicit solvent, and refinement and filtering with molecular simulation to ensure stable simulation. The output of this pipeline is an ensemble of structures ready for subsequent molecular simulations using computer clusters, supercomputers, or distributed computing projects like Folding@home. Ensembler thus automates much of the time-consuming process of preparing protein models suitable for simulation, while allowing scalability up to entire superfamilies. A particular advantage of this approach can be found in the construction of kinetic models of conformational dynamics—such as Markov state models (MSMs)—which benefit from a diverse array of initial configurations that span the accessible conformational states to aid sampling. We demonstrate the power of this approach by constructing models for all catalytic domains in the human tyrosine kinase family, using all available kinase catalytic domain structures from any organism as structural templates. Ensembler is free and open source software licensed under the GNU General Public License (GPL) v2. It is compatible with Linux and OS X. The latest release can be installed via the conda package manager, and the latest source can be downloaded from https://github.com/choderalab/ensembler. © 2016 Parton et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{VanZee2016,
author={Van Zee, F.G. and Smith, T.M. and Marker, B. and Low, T.M. and Van De Geijn, R.A. and Igual, F.D. and Smelyanskiy, M. and Zhang, X. and Kistler, M. and Austel, V. and Gunnels, J.A. and Killough, L.},
title={The BLIS framework: Experiments in portability},
journal={ACM Transactions on Mathematical Software},
year={2016},
volume={42},
number={2},
doi={10.1145/2755561},
art_number={12},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975482275&doi=10.1145%2f2755561&partnerID=40&md5=46c1ecbd6efc23a35424cd538b2a49d3},
abstract={BLIS is a new software framework for instantiating high-performance BLAS-like dense linear algebra libraries. We demonstrate how BLIS acts as a productivity multiplier by using it to implement the level-3 BLAS on a variety of current architectures. The systems for which we demonstrate the framework include state-of-the-art general-purpose, low-power, and many-core architectures. We show, with very little effort, how the BLIS framework yields sequential and parallel implementations that are competitive with the performance of ATLAS, OpenBLAS (an effort to maintain and extend the GotoBLAS), and commercial vendor implementations such as AMD's ACML, IBM's ESSL, and Intel's MKL libraries. Although most of this article focuses on single-core implementation, we also provide compelling results that suggest the framework's leverage extends to the multithreaded domain. © 2016 ACM.},
author_keywords={BLAS;  High performance;  Libraries;  Linear algebra;  Matrix;  Multiplication},
document_type={Article},
source={Scopus},
}

@ARTICLE{Estebanez2016407,
author={Estebanez, A. and Llanos, D.R. and Gonzalez-Escribano, A.},
title={New Data Structures to Handle Speculative Parallelization at Runtime},
journal={International Journal of Parallel Programming},
year={2016},
volume={44},
number={3},
pages={407-426},
doi={10.1007/s10766-014-0347-0},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921341163&doi=10.1007%2fs10766-014-0347-0&partnerID=40&md5=4447d3cb6ee1d04188190d1999ea4140},
abstract={Software-based, thread-level speculation (TLS) is a software technique that optimistically executes in parallel loops whose fully-parallel semantics can not be guaranteed at compile time. Modern TLS libraries allow to handle arbitrary data structures speculatively. This desired feature comes at the high cost of local store and/or remote recovery times: The easier the local store, the harder the remote recovery. Unfortunately, both times are on the critical path of any TLS system. In this paper we propose a solution that performs local store in constant time, while recover values in a time that is in the order of (Formula presented.) , being (Formula presented.) the number of threads. As we will see, this solution, together with some additional improvements, makes the difference between slowdowns and noticeable speedups in the speculative parallelization of non-synthetic, pointer-based applications on a real system. Our experimental results show a gain of 3.58 (Formula presented.) to 28 (Formula presented.) with respect to the baseline system, and a relative efficiency of up to, on average, 65 % with respect to a TLS implementation specifically tailored to the benchmarks used. © 2015, Springer Science+Business Media New York.},
author_keywords={Memory improvements;  Speculative parallelism;  Thread-level speculation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bonardi201632,
author={Bonardi, M. and Brioschi, M. and Fuggetta, A. and Verga, E.S. and Zuccalà, M.},
title={Fostering collaboration through API Economy: The E015 Digital Ecosystem},
journal={Proceedings - 3rd International Workshop on Software Engineering Research and Industrial Practice, SER and IP 2016},
year={2016},
pages={32-38},
doi={10.1145/2897022.2897026},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974533832&doi=10.1145%2f2897022.2897026&partnerID=40&md5=f9bf62d961c29dffc5412e6f58ce99c8},
abstract={The API Economy trend is nowadays a concrete opportunity to go beyond the traditional development of vertical ICT solutions and to unlock additional business value by enabling innovative collaboration patterns between different players, e.g., companies, public authorities and researchers. Thus, an effective API Economy initiative has to be comprehensive, focusing not only on technical issues but also on other complementary dimensions. This paper illustrates a successful API Economy initiative, the E015 Digital Ecosystem developed for Expo Milano 2015, showing how a comprehensive approach to information systems interoperability and Service-Oriented Architectures can foster synergetic collaboration between industry and academia in particular, hence enabling the development of value-added solutions for the end-users. © 2016 ACM.},
author_keywords={API Economy;  APIs;  Application mashup;  Collaboration mechanisms;  Digital assets;  Digital Ecosystems;  E015;  Expo Milano 2015;  ICT governance;  Interoperability;  Service-oriented architectures;  Standards and guidelines;  Web services},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DaRosa20161006,
author={Da Rosa, T.R. and Mesquida, T. and Lemaire, R. and Clermidy, F.},
title={MCAPI-compliant Hardware Buffer Manager mechanism to support communication in multi-core architectures},
journal={Proceedings of the 2016 Design, Automation and Test in Europe Conference and Exhibition, DATE 2016},
year={2016},
pages={1006-1011},
art_number={7459453},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973616535&partnerID=40&md5=3011273ab851c5290bb11ad8b213c044},
abstract={High performance and high power efficiency are two mandatory constraints for multi-core systems in order to successfully handle the most recent applications in several fields, e.g. image processing and communication standards. Nowadays, hardware accelerators are often used along with several processing cores to achieve the desired performance while keeping high power efficiency. However, such systems impose an increased programming complexity due to the lack of software standards that supports heterogeneity, frequently leading to custom solutions. On the other hand, implementing a standard software solution for embedded systems might induce significant overheads. This work presents a hardware mechanism in co-design with a standard programming interface (API) for embedded systems focusing to decrease overheads imposed by software implementation while increasing programmability and communication performance. The results show gains of up to 97% in latency and an increase of 40 times in throughput for synthetic traffics and an average decrease of 95% in communication time for an image processing application. © 2016 EDAA.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cassel2016233,
author={Cassel, S. and Howar, F. and Jonsson, B. and Steffen, B.},
title={Active learning for extended finite state machines},
journal={Formal Aspects of Computing},
year={2016},
volume={28},
number={2},
pages={233-263},
doi={10.1007/s00165-016-0355-5},
note={cited By 70},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957934145&doi=10.1007%2fs00165-016-0355-5&partnerID=40&md5=c3a44872af4fbfd0a965315ad84870a8},
abstract={We present a black-box active learning algorithm for inferring extended finite state machines (EFSM)s by dynamic black-box analysis. EFSMs can be used to model both data flow and control behavior of software and hardware components. Different dialects of EFSMs are widely used in tools for model-based software development, verification, and testing. Our algorithm infers a class of EFSMs called register automata. Register automata have a finite control structure, extended with variables (registers), assignments, and guards. Our algorithm is parameterized on a particular theory, i.e., a set of operations and tests on the data domain that can be used in guards. Key to our learning technique is a novel learning model based on so-called tree queries. The learning algorithm uses tree queries to infer symbolic data constraints on parameters, e.g., sequence numbers, time stamps, identifiers, or even simple arithmetic. We describe sufficient conditions for the properties that the symbolic constraints provided by a tree query in general must have to be usable in our learning model. We also show that, under these conditions, our framework induces a generalization of the classical Nerode equivalence and canonical automata construction to the symbolic setting. We have evaluated our algorithm in a black-box scenario, where tree queries are realized through (black-box) testing. Our case studies include connection establishment in TCP and a priority queue from the Java Class Library. © 2016, British Computer Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Avallone201698,
author={Avallone, S. and Di Stasi, G.},
title={Design and implementation of WiMesh: A tool for the performance evaluation of multi-radio wireless mesh networks},
journal={Journal of Network and Computer Applications},
year={2016},
volume={63},
pages={98-109},
doi={10.1016/j.jnca.2015.12.011},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960156511&doi=10.1016%2fj.jnca.2015.12.011&partnerID=40&md5=7a3dd02637acaf31243994240038cb8a},
abstract={In this paper we present WiMesh, a software tool we developed during the last few years of research conducted in the field of multi-radio wireless mesh networks. WiMesh serves two main purposes: (i) to evaluate and compare different algorithms for the static configuration of a wireless mesh network (assignment of channels, transmission rate and power to the available network radios, explicit routing); (ii) to automatically setup and run packet level simulations (by using the ns-3 network simulator) based on the network configuration returned by such algorithms. WiMesh consists of a core library, three libraries dedicated to distinct functionalities of WiMesh and three corresponding utilities that allow us to easily conduct experiments. To ensure ease of use and flexibility, all such utilities accept as input an XML configuration file where various options and parameters can be specified. WiMesh is freely available to the research community as open source software, with the purpose of easing the development of new algorithms and the verification of their performances. In this paper, we first present the architecture of WiMesh and its features and capabilities by illustrating the design and the usage of each of the provided utilities. Then, to the benefit of those willing to implement their own solution within WiMesh or extend its functionalities, we illustrate the design of the WiMesh libraries. Finally, we report some of the results, which we were able to show in previous research work thanks to the use of WiMesh. © 2016 Elsevier Ltd. All rights reserved.},
author_keywords={Channel assignment and routing algorithms;  Multi-radio wireless mesh networks;  Network simulation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Seth2016274,
author={Seth, P. and Krivenko, I. and Ferrero, M. and Parcollet, O.},
title={TRIQS/CTHYB: A continuous-time quantum Monte Carlo hybridisation expansion solver for quantum impurity problems},
journal={Computer Physics Communications},
year={2016},
volume={200},
pages={274-284},
doi={10.1016/j.cpc.2015.10.023},
note={cited By 147},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955289157&doi=10.1016%2fj.cpc.2015.10.023&partnerID=40&md5=83b360d277ee6f4bc26738280db06d2e},
abstract={We present TRIQS/CTHYB, a state-of-the art open-source implementation of the continuous-time hybridisation expansion quantum impurity solver of the TRIQS package. This code is mainly designed to be used with the TRIQS library in order to solve the self-consistent quantum impurity problem in a multi-orbital dynamical mean field theory approach to strongly-correlated electrons, in particular in the context of realistic electronic structure calculations. It is implemented in C++ for efficiency and is provided with a high-level Python interface. The code ships with a new partitioning algorithm that divides the local Hilbert space without any user knowledge of the symmetries and quantum numbers of the Hamiltonian. Furthermore, we implement higher-order configuration moves and show that such moves are necessary to ensure ergodicity of the Monte Carlo in common Hamiltonians even without symmetry-breaking. Program summary Program title: TRIQS/CTHYB Catalogue identifier: AEYU-v1-0 Program summary URL:http://cpc.cs.qub.ac.UK/summaries/AEYU-v1-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland. Licensing provisions: GNU General Public Licence (GPLv3) No. of lines in distributed program, including test data, etc.: 159,017 No. of bytes in distributed program, including test data, etc.: 10,215,893 Distribution format: tar.gz Programming language: C++/Python. Computer: Any architecture with suitable compilers including PCs and clusters. Operating system: Unix, Linux, OSX. RAM: Highly problem-dependent Classification: 7.3, 4.4. External routines: TRIQS, cmake. Nature of problem: Accurate solvers for quantum impurity problems are needed in condensed matter theory. Solution method: We present an efficient C++/Python open-source implementation of a continuous-time hybridisation expansion solver. Running time: Tests take less than a minute. Otherwise it is highly problem dependent (from minutes to several days). © 2015 Elsevier B.V.},
author_keywords={C++;  DMFT;  Impurity solvers;  Many-body physics;  Monte Carlo;  Python;  Strongly-correlated systems},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nama2016270,
author={Nama, G.F. and Ulvan, M. and Ulvan, A. and Hanafi, A.M.},
title={Design and implementation web based geographic information system for public services in Bandar Lampung City - Indonesia},
journal={Proceedings - 2015 International Conference on Science in Information Technology: Big Data Spectrum for Future Information Economy, ICSITech 2015},
year={2016},
pages={270-275},
doi={10.1109/ICSITech.2015.7407816},
art_number={7407816},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966559909&doi=10.1109%2fICSITech.2015.7407816&partnerID=40&md5=1c422142d05a1c444cd93d5dd2d6794f},
abstract={The work on this paper has point of view on development of public service geographic information system at Bandar Lampung City. The idea of this application deployment was driven to provide a better public services for the citizens. Currently, information of public service locations category which are needed by the citizens those are; lodging/housing information, health facilities, government office, service station, worship place, traditional market, modern market, banking, culinary, sports facilities, tourism, industry, police station, school/university place, tour and travel. The developed application enables the possibility to locate the public service information through web page easily. In this research, deployment of public services application based on Unified Modeling Language (UML) approach, including the use case model, activity diagram, and object diagram. The authors also explored usage of several non commercial applications under General Public License that is; apache2 web server, PHP5 programming language, MySQL5 for data base server, Google Maps API as base map of city visualization. A lot of public service information on Bandar Lampung city have been gathered, which are emerged the location on application including its picture, short video, and short description about the places. This application can be easily integrated with existing of city government web portal by using XML data transport technology, to improved a better public service for the citizens. © 2015 IEEE.},
author_keywords={GIS;  public services on bandar lampung city;  UML;  web based geographical information system},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{L'Ecuyer201631,
author={L'Ecuyer, P.},
title={Random number generation with multiple streams for sequential and parallel computing},
journal={Proceedings - Winter Simulation Conference},
year={2016},
volume={2016-February},
pages={31-44},
doi={10.1109/WSC.2015.7408151},
art_number={7408151},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962881203&doi=10.1109%2fWSC.2015.7408151&partnerID=40&md5=19b2647d462c27af59028323d0a1fc25},
abstract={We provide a review of the state of the art on the design and implementation of random number generators (RNGs) for simulation, on both sequential and parallel computing environments. We focus on the need for multiple independent streams and substreams of random numbers, explain how they can be constructed and managed, review software libraries that offer them, and illustrate their usefulness via examples. We also review the basic quality criteria for good random number generators and their theoretical and empirical testing. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hsieh2016441,
author={Hsieh, F.-S.},
title={Location-aware workflow scheduling in supply chains based on multi-agent systems},
journal={TAAI 2015 - 2015 Conference on Technologies and Applications of Artificial Intelligence},
year={2016},
pages={441-448},
doi={10.1109/TAAI.2015.7407087},
art_number={7407087},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964211454&doi=10.1109%2fTAAI.2015.7407087&partnerID=40&md5=756165b201d7edfad84b495827f6ca57},
abstract={In construction industry, companies form a supply chain to respond to business opportunities. The complex workflows, dependency between partners and their location pose a big challenge in construction project management. How to schedule activities to meet the construction project requirements under resource constraints is an important issue. To create a feasible schedule for a construction project, companies in a typical construction supply chain need to negotiate with each other. Development of an effective software system to support negotiation and collaboration between the partners in a construction supply chain is urgent. Execution of workflows in a construction project usually depends on location. Although workflow management problems have been extensively studied for decades, location information of workflows is rarely taken into account in existing literature. In this paper, we will study the development of a location-aware workflow scheduling system for construction supply chains. We will propose a flexible scheduling system to optimize the construction project schedule based on collaboration of entities/partners in a construction supply chain. We propose a methodology that includes modeling of location-aware workflows in construction projects based on formal workflow models and develop a technique to transform workflow models to formulate and solve a project scheduling problem. We propose architecture to implement a location-aware multi-agent scheduling system based on JADE and Google API. The proposed methodology is verified by an example. © 2015 IEEE.},
author_keywords={location aware;  multi-agent;  scheduling;  supply chain;  workflow},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2016,
title={2015 International Conference on ReConFigurable Computing and FPGAs, ReConFig 2015},
journal={2015 International Conference on ReConFigurable Computing and FPGAs, ReConFig 2015},
year={2016},
page_count={507},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964389027&partnerID=40&md5=6fd2430226668f1a04b91ba13502c2bc},
abstract={The proceedings contain 75 papers. The topics discussed include: adaptive controller using runtime partial hardware reconfiguration for unmanned aerial vehicles (UAVs); accelerating the construction of BRIEF descriptors using an FPGA-based architecture; a resource-efficient multi-camera GigE vision IP core for embedded vision processing platforms; aging effects on ring-oscillator-based physical unclonable functions on FPGAs; achieving energy-efficiency on MPSoCs: performance and power optimizations; accurate in-situ runtime measurement of energy per operation of system-on-chip on FPGA; a universal hardware API for authenticated ciphers; a real-time reconfigurable architecture for face detection; a 297MOPS/0.4mW ultra low power coarse-grained reconfigurable accelerator CMA-SOTB-2; analysis of FPGA and software approaches to simulate unconventional computer architectures; and an optimized radix-tree for hardware-accelerated dictionary generation for semantic web databases.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Homsirikamol2016,
author={Homsirikamol, E. and Diehl, W. and Ferozpuri, A. and Farahmand, F. and Sharif, M.U. and Gaj, K.},
title={A universal hardware API for authenticated ciphers},
journal={2015 International Conference on ReConFigurable Computing and FPGAs, ReConFig 2015},
year={2016},
doi={10.1109/ReConFig.2015.7393283},
art_number={7393283},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964319409&doi=10.1109%2fReConFig.2015.7393283&partnerID=40&md5=2bb67073cad824a153532af76e6b68df},
abstract={In this paper, we propose a universal hardware Application Programming Interface (API) for authenticated ciphers. In particular, our API is intended to meet the requirements of all algorithms submitted to the CAESAR competition. Two major parts of the API, the interface and the communication protocol, were developed with the goal of reducing any potential biases in benchmarking of authenticated ciphers in hardware. Our highspeed implementation of the proposed hardware API includes universal, open-source pre-processing and post-processing units, common for all CAESAR candidates and the current standards, such as AES-GCM and AES-CCM. Apart from the full documentation, examples, and the source code of the pre-processing and post-processing units, we have made available in public domain a) a universal testbench to verify the functionality of any CAESAR candidate implemented using our hardware API, b) a Python script used to automatically generate test vectors for this testbench, c) VHDL wrappers used to determine the maximum clock frequency and the resource utilization of all implementations, and d) RTL VHDL source codes of highspeed implementations of AES and the Keccak Permutation F, which may be used as building blocks in implementations of related ciphers. We hope that the existence of these resources will substantially reduce the time necessary to develop hardware implementations of all CAESAR candidates for the purpose of evaluation, comparison, and future deployment in real products. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ignatius2016211,
author={Ignatius, A.R. and Wolfe, K. and Parmar, R. and Flaishans, J. and Harwell, L. and Yee, S. and Purucker, T. and Galvin, M.},
title={Design and Implementation of a REST API for the Human Well Being Index (HWBI)},
journal={Environmental Modelling and Software for Supporting a Sustainable Future, Proceedings - 8th International Congress on Environmental Modelling and Software, iEMSs 2016},
year={2016},
volume={1},
pages={211-217},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088025341&partnerID=40&md5=d87046c45905983a0b1c3dd0b1d3518b},
abstract={Interoperable software development uses principles of component reuse, systems integration, flexible data transfer, and standardized ontological documentation to promote access, reuse, and integration of code. While interoperability principles are increasingly considered technology standards in software engineering, adoption by the environmental modeling community has been slow. We created an Application Programming Interface (API) for the U.S. Environmental Protection Agency's Human Well-being Index (HWBI) model based on interoperability principles. The HWBI characterizes economic, social, and environmental services during years 2000-2010. For each county in the U.S., specific metrics (e.g., life expectancy, housing affordability, voter turnout) are used to calculate scores for eight domains of well-being: Connection to Nature, Cultural Fulfillment, Education, Health, Leisure Time, Living Standards, Safety and Security, and Social Cohesion. These eight domain values are then used to determine an overall HWBI classification for each U.S. county, state, and region. Interoperability best-practices are demonstrated through the reuse of the code and database for both a web application and a desktop application. The web application accesses the HWBI API services with a browser-based interface to encourage data exploration and obtain feedback from the public. Desktop access is enabled via a plug-in for an agent-based modeling system that uses the HWBI API for output calculations. Our software incorporates best-practices for frontand back-end design that includes automated code testing, data transfer standardization for efficiency and responsiveness, emerging documentation standards, and terminology services and controlled vocabularies to promote reusability. © Environmental Modelling and Software for Supporting a Sustainable Future, Proceedings - 8th International Congress on Environmental Modelling and Software, iEMSs 2016. All rights reserved.},
author_keywords={Interoperability;  REST API;  USEPA;  Web services},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Navaee201611,
author={Navaee, S. and Kang, J.},
title={Enhancement of a finite element analysis course for structural engineering},
journal={Computers in Education Journal},
year={2016},
volume={16},
number={4},
pages={11-24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052791199&partnerID=40&md5=c3bc56e19d4425815a5b643debccf2e4},
abstract={In this paper the enhancement of an introductory Finite Element course in the newly established Civil Engineering and Construction Management Department at Georgia Southern University is discussed. Typically in an Introductory Finite Element course offered in many engineering schools, a simple less elaborate FE package is used to deliver the course concepts. In the newly developed course discussed in the paper, a state-of-the-art commercial software package is planned to be utilized to further enhance the marketability of the students upon graduation. Along with this choice comes the challenge of developing suitable tutorials and examples to familiarize the students with various important tools and special features of this sophisticate package in the minimum amount of time possible. The submitted paper explores one possible strategy to accomplish this task. This course is designed for exploring the civil engineering applications focusing specifically on analysis of structural components, rather than solving problems related to other fields such as fluid mechanics or heat transfer. The planned projects in the course explore strategies in analyzing a variety of structural members such as trusses, beams, frames, as well as other solid continuums. The course is expected to complement other structural engineering related courses delivered in the curriculum such as Structural Analysis and Advanced Structural Analysis. Solutions of problems obtained in this course using the FE package can be compared and contrasted against the results from other classical methods discussed in these courses. Some of these methods include: Slope-Deflection Method, Moment Distribution Method, and Matrix Stiffness Method. These comparisons further enhance the knowledge of the students in area of structures and provide them with new perspectives. Several prepared example problems for the course are included in the paper to further illustrate the value of the project. Each of these examples is selected for a unique purpose, covering specific topics, and illustrating important principles. Collection of the prepared course examples such as the ones included in this paper will provide the students with the strong foundation in the areas of analysis of structures, preparing them well for their future studies and for pursuing rewarding professional careers. © 2016 American Society for Engineering Education. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gawron2016,
author={Gawron, P. and Ostaszewski, M. and Satagopam, V. and Gebel, S. and Mazein, A. and Kuzma, M. and Zorzan, S. and McGee, F. and Otjacques, B. and Balling, R. and Schneider, R.},
title={MINERVA—A platform for visualization and curation of molecular interaction networks},
journal={npj Systems Biology and Applications},
year={2016},
volume={2},
doi={10.1038/npjsba.2016.20},
art_number={16020},
note={cited By 43},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028318822&doi=10.1038%2fnpjsba.2016.20&partnerID=40&md5=5d2f5b0470206484351b4264635983dd},
abstract={Our growing knowledge about various molecular mechanisms is becoming increasingly more structured and accessible. Different repositories of molecular interactions and available literature enable construction of focused and high-quality molecular interaction networks. Novel tools for curation and exploration of such networks are needed, in order to foster the development of a systems biology environment. In particular, solutions for visualization, annotation and data cross-linking will facilitate usage of network-encoded knowledge in biomedical research. To this end we developed the MINERVA (Molecular Interaction NEtwoRks VisuAlization) platform, a standalone webservice supporting curation, annotation and visualization of molecular interaction networks in Systems Biology Graphical Notation (SBGN)-compliant format. MINERVA provides automated content annotation and verification for improved quality control. The end users can explore and interact with hosted networks, and provide direct feedback to content curators. MINERVA enables mapping drug targets or overlaying experimental data on the visualized networks. Extensive export functions enable downloading areas of the visualized networks as SBGN-compliant models for efficient reuse of hosted networks. The software is available under Affero GPL 3.0 as a Virtual Machine snapshot, Debian package and Docker instance at http://r3lab.uni.lu/web/minerva-website/. We believe that MINERVA is an important contribution to systems biology community, as its architecture enables set-up of locally or globally accessible SBGN-oriented repositories of molecular interaction networks. Its functionalities allow overlay of multiple information layers, facilitating exploration of content and interpretation of data. Moreover, annotation and verification workflows of MINERVA improve the efficiency of curation of networks, allowing life-science researchers to better engage in development and use of biomedical knowledge repositories. © The Author(s) 2016.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fatkina2016213,
author={Fatkina, A. and Iakushkin, O. and Gasanova, O. and Tazieva, L.},
title={Optimization of selected components in MPD root: Capabilities of distributed programming techniques},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1787},
pages={213-217},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016222634&partnerID=40&md5=8cdf72742827b4ef1544d6045adfe7ea},
abstract={The article analyses the prospects of optimizing the architecture and the execution logic of selected scripts available in MPD Root project. We considered the option of porting the scripts to allow execution on massive parallel architectures. We collected and structured large data illustrating the project's work: • the project's dependency tables were drawn up with regard to the support of parallel and concurrent computing; • the source code database was indexed to identify cross dependencies among architectural entities; • the code profiling measurements were made at launch time of the scripts in question, the call sequences were analyzed, the execution time of the scripts was evaluated. The study evaluated the prospects of using various libraries and platforms: CUD A, OpenMP, OpenCL, TBB, and MPI. The obtained measurements and the analysis of the best practices of the software under consideration allows to make recommendations for modifying MPD Root in order to optimize: • vectorization of loops; • transfer of continuous computing segments to co-processing architecture; • source code segments whose operation can be represented as call graphs; • source code segments that can be subject to load allocation between computing nodes. © 2016 Fatkina A., Iakushkin O, Gasanova O, Tazieva L.},
author_keywords={CUD A;  MPD Root;  Parallel and distributed computing;  STL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Iakushkin2016530,
author={Iakushkin, O. and Kondratiuk, A. and Sedova, O. and Grishkin, V.},
title={Jupyter extension for Creating CAD designs and their subsequent analysis by the finite element method},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1787},
pages={530-534},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016174672&partnerID=40&md5=cc4d8f3178d9cf0652843e4ddd056bc7},
abstract={Creating designs in CAD and performing their stress-strain analysis are complex computational tasks. Their successful solution depends on a number of prerequisites: availability of large computational power; comprehensive knowledge of physical and mathematical computing; and solid skills of programming and working in a variety of separate software products that are not integrated to each other directly. The paper presents a system aimed at CAD models development and verification from the ground up. The system integrates geometry construction, mesh model creation and deformation analysis into a uniform computing environment operating as a SaaS solution. It is based exclusively on open source software and allows to use the Python programming language and SALOME, GMSH, FEniCS and ParaView libraries. The system's architecture and certain issues of working with libraries are discussed. The paper also presents a browser-based tool for CAD design creation and analysis, which tool is the front end of the software product we created. © 2016 Iakushkin O. O., Kondratiuk A. N., Sedova O. S, Grishkin V. M.},
author_keywords={CAD;  Cloud;  Ipython;  Services},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Choudhary2016920,
author={Choudhary, S.K. and Jadoun, R.S. and Gupta, N.},
title={Identification of critical issues and solutions during ERP software development life cycle},
journal={Lecture Notes in Engineering and Computer Science},
year={2016},
volume={2226},
pages={920-925},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013393711&partnerID=40&md5=97a8109a63636819ec403580110c8862},
abstract={Information technology is revolutionizing the way the business intelligence is being done. For any organization to succeed, all business unit or departments should work in harmony towards a common goal. ERP is a very powerful tool, which provides perfect information system to maintain the various functional business modules of an enterprise. Most organization is turning to available ERP software package for solution to their information management problem. ERP package if chosen correctly, implemented sensibly and used capably will enhance the output and profit of the any association considerably. ERP software package is at the cutting frame of information system technology. ERP software package help to manage institute and firm extensive business processes, using a common database and shared management reporting tool. ERP software supports the well-organized operation of business actions, including sales, marketing, manufacturing, accounting and staffing. The aim of this study is to provide a contribution to the research field of the ERP software development life cycle (SDLC), critical success factors, issues for ERP package development and implementation with various kinds of organization and discuss about the Essential factor for ERP Selection and ERP Benefits etc.},
author_keywords={Enterprise resource planning (ERP);  ERP critical success factor;  ERP implementation;  Software development life cycle;  Supply chain management (SCM)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Buur2016,
author={Buur, H. and Subramaniam, A. and Gillies, K. and Dumas, C. and Bhatia, R.},
title={TMT approach to observatory software development process},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9913},
doi={10.1117/12.2234102},
art_number={991319},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006446430&doi=10.1117%2f12.2234102&partnerID=40&md5=c4b4effb767cc25e86dfa7dfc3026f43},
abstract={The purpose of the Observatory Software System (OSW) is to integrate all software and hardware components of the Thirty Meter Telescope (TMT) to enable observations and data capture; thus it is a complex software system that is defined by four principal software subsystems: Common Software (CSW), Executive Software (ESW), Data Management System (DMS) and Science Operations Support System (SOSS), all of which have interdependencies with the observatory control systems and data acquisition systems. Therefore, the software development process and plan must consider dependencies to other subsystems, manage architecture, interfaces and design, manage software scope and complexity, and standardize and optimize use of resources and tools. Additionally, the TMT Observatory Software will largely be developed in India through TMT's workshare relationship with the India TMT Coordination Centre (ITCC) and use of Indian software industry vendors, which adds complexity and challenges to the software development process, communication and coordination of activities and priorities as well as measuring performance and managing quality and risk. The software project management challenge for the TMT OSW is thus a multi-faceted technical, managerial, communications and interpersonal relations challenge. The approach TMT is using to manage this multifaceted challenge is a combination of establishing an effective geographically distributed software team (Integrated Product Team) with strong project management and technical leadership provided by the TMT Project Office (PO) and the ITCC partner to manage plans, process, performance, risk and quality, and to facilitate effective communications; establishing an effective cross-functional software management team composed of stakeholders, OSW leadership and ITCC leadership to manage dependencies and software release plans, technical complexities and change to approved interfaces, architecture, design and tool set, and to facilitate effective communications; adopting an agile-based software development process across the observatory to enable frequent software releases to help mitigate subsystem interdependencies; defining concise scope and work packages for each of the OSW subsystems to facilitate effective outsourcing of software deliverables to the ITCC partner, and to enable performance monitoring and risk management. At this stage, the architecture and high-level design of the software system has been established and reviewed. During construction each subsystem will have a final design phase with reviews, followed by implementation and testing. The results of the TMT approach to the Observatory Software development process will only be preliminary at the time of the submittal of this paper, but it is anticipated that the early results will be a favorable indication of progress. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author_keywords={Agile process;  Decomposition;  Engagement model;  Performance tracking;  Traceability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Galvin2016,
author={Galvin, M. and Kim, Y. and Kasdin, N.J. and Sirbu, D. and Vanderbei, R. and Echeverri, D. and Sagolla, G. and Rousing, A. and Balasubramanian, K. and Ryan, D. and Shaklan, S. and Lisman, D.},
title={Design and construction of a 76m long-travel laser enclosure for a space occulter testbed},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9912},
doi={10.1117/12.2231093},
art_number={99126N},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996931604&doi=10.1117%2f12.2231093&partnerID=40&md5=dc97bb7ac0921e31a913f28d1062e665},
abstract={Princeton University is upgrading our space occulter testbed. In particular, we are lengthening it to ∼76m to achieve flightlike Fresnel numbers. This much longer testbed required an all-new enclosure design. In this design, we prioritized modularity and the use of commercial off-the-shelf (COTS) and semi-COTS components. Several of the technical challenges encountered included an unexpected slow beam drift and black paint selection. Herein we describe the design and construction of this long-travel laser enclosure. © 2016 SPIE.},
author_keywords={Beam drift;  Black paint;  Enclosure;  Laser;  Long-travel;  Occulter;  Starshade;  Testbed},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kienzle201631,
author={Kienzle, J. and Koziolek, A. and Busch, A. and Reussner, R.},
title={Towards concern-oriented design of component-based systems},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1723},
pages={31-36},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996721566&partnerID=40&md5=bb5002747b593cec218cedffa43a28db},
abstract={Component-based software engineering (CBSE) is based on defining, implementing and composing loosely coupled, independent components, thus increasing modularity, analyzability, separation of concerns and reuse. However, complete separation of concerns is difficult to achieve in CBSE when concerns crosscut several components. Furthermore, in some cases, reuse of components is limited because component developers make certain implementation choices that are incompatible with the non-functional requirements of the application that is being built. In this paper we outline how to integrate CBSE and concern-oriented reuse (CORE), a novel reuse paradigm that extends Model-Driven Engineering (MDE) with best practices from aspect-oriented software composition and Software Product Lines (SPL). Concretely, we outline how to combine the Palladio Component Model (PCM) capable of expressing complex software architectures with CORE class and sequence diagrams for low-level design. As a result, multiple solutions for addressing concerns that might even crosscut component boundaries can be modularized in a reusable way, and integrated with applications that reuse them using aspect-oriented techniques. Additionally, thanks to CORE, component developers can avoid premature decision making when reusing existing libraries during implementation.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Iurlova20161920,
author={Iurlova, N.A. and Matveenko, V.P. and Oshmarin, D.A. and Sevodina, N.V. and Yurlov, M.A.},
title={Layout optimization of piezoelectric elements with external electric circuits in smart constructions based on solution of the natural vibrations problem},
journal={ECCOMAS Congress 2016 - Proceedings of the 7th European Congress on Computational Methods in Applied Sciences and Engineering},
year={2016},
volume={1},
pages={1920-1929},
doi={10.7712/100016.1930.7575},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995451843&doi=10.7712%2f100016.1930.7575&partnerID=40&md5=ec28a999701046ef7a7459284c8d4161},
abstract={The presence of the elements, which work as sensors or actuators, is one of the main features of smart-constructions. These elements are commonly made of piezoelectric materials, the main feature of which is a direct and inverse piezoelectric effect, which allows its functioning as a sensor and actuator. In addition, one and the same element can combine both functions. Moreover electroding of surfaces of the piezoelectric elements provides an additional means of controlling the dynamic processes due to application of different electric circuits connected to piezoelectric elements. The location of the piezoelectric element, its size and geometry play a significant role in damping the vibration of structures containing elements made of piezoelectric materials. In this paper, a comparative analysis of different possible options of smart-function realization is made by comparing the patterns of strains and electric potentials generated on the piezoelectric element surface. The numerical results of determining the optimal piezoelectric element location, which will provide the generation of maximal electric potential at certain resonant frequencies, are presented. The numerical implementation is carried out by the finite element method using the commercial ANSYS software package.},
author_keywords={Electric potential;  Electroviscoelesticity;  Natural vibrations;  Optimal placement;  Piezoelectric element},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reuter2016,
author={Reuter, M.A. and Cook, K.H. and Delgado, F. and Petry, C.E. and Ridgway, S.T.},
title={Simulating the LSST OCS for conducting survey simulations using the LSST scheduler},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9911},
doi={10.1117/12.2232680},
art_number={991125},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992597845&doi=10.1117%2f12.2232680&partnerID=40&md5=b6b4b6b231b05b3eeba903319b665b92},
abstract={The Operations Simulator was used to prototype the Large Synoptic Survey Telescope (LSST) Scheduler. Currently, the Scheduler is being developed separately to interface with the LSST Observatory Control System (OCS). A new Simulator is under concurrent development to adjust to this new architecture. This requires a package simulating enough of the OCS to allow execution of realistic schedules. This new package is called the Simulated OCS (SOCS). In this paper we detail the SOCS construction plan, package structure, LSST communication middleware platform use, provide some interesting use cases that the separated architecture allows and the software engineering practices used in development. © 2016 SPIE.},
author_keywords={LSST;  observing strategy;  simulations},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Marrero2016,
author={Marrero, J.},
title={Optomechanical design software for segmented mirrors},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9911},
doi={10.1117/12.2232950},
art_number={99112A},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992549239&doi=10.1117%2f12.2232950&partnerID=40&md5=5862f91897db5ef8a687b5066b3cdd6a},
abstract={The software package presented in this paper, still under development, was born to help analyzing the influence of the many parameters involved in the design of a large segmented mirror telescope. In summary, it is a set of tools which were added to a common framework as they were needed. Great emphasis has been made on the graphical presentation, as scientific visualization nowadays cannot be conceived without the use of a helpful 3d environment, showing the analyzed system as close to reality as possible. Use of third party software packages is limited to ANSYS, which should be available in the system only if the FEM results are needed. Among the various functionalities of the software, the next ones are worth mentioning here: Automatic 3d model construction of a segmented mirror from a set of parameters, geometric ray tracing, automatic 3d model construction of a telescope structure around the defined mirrors from a set of parameters, segmented mirror human access assessment, analysis of integration tolerances, assessment of segments collision, structural deformation under gravity and thermal variation, mirror support system analysis including warping harness mechanisms, etc. © 2016 SPIE.},
author_keywords={3d;  mirror support system;  segmented mirror;  simulation;  software;  telescope},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu2016472,
author={Wu, Q. and Yuan, H.},
title={Application development of monitor and diagnosis system based on simulation platform},
journal={Communications in Computer and Information Science},
year={2016},
volume={646},
pages={472-481},
doi={10.1007/978-981-10-2672-0_48},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988862083&doi=10.1007%2f978-981-10-2672-0_48&partnerID=40&md5=ed75056ae41f0f92c8c22b5c20883422},
abstract={In industry automation, fault monitor and diagnosis play vital role in ensuring on demand operation of complex system with safety and reliability. Simulation platform is useful way for researchers to design and develop application-based system monitor and diagnosis, especially in the field of engineering based vehicle. A method using modular design is presented in this paper to achieve information synthesis, distributed control and supervisory management based on the field bus simulation platform. With the simulation platform is proposed, method of design and development for engineering vehicle supervisory management system is proposed and implemented. The system is divided into separate functional modules and each module is designed in the form of dynamic link library encapsulation in order to achieve the goal of high cohesion and low coupling. Furthermore, the module is loaded on demand so that operation resource requirement is optimized in the system. At last, detail analysis and research work are carried out regarding design and implementation plan according to a case study from equipment monitor and diagnosis system. The proposed design method can improve compatibility and adaptability of the monitor and diagnosis system, and operation efficiency is improved as well. © Springer Science+Business Media Singapore 2016.},
author_keywords={Diagnosis;  Fault;  Hardware-in-the-loop;  Simulation platform;  Software design;  Vehicle},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Moreno-Delgado2016134,
author={Moreno-Delgado, A. and Durán, F. and Meseguer, J.},
title={Towards generic monitors for object-oriented real-time maude specifications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9942 LNCS},
pages={134-151},
doi={10.1007/978-3-319-44802-2_7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986198907&doi=10.1007%2f978-3-319-44802-2_7&partnerID=40&md5=b6cc316a6fef7ff4489d1ea054922bfc},
abstract={Non-Functional Properties (NFPs) are crucial in the design of software. Specification of systems is used in the very first phases of the software development process for the stakeholders to make decisions on which architecture or platform to use. These specifications may be analyzed using different formalisms and techniques, simulation being one of them. During a simulation, the relevant data involved in the analysis of the NFPs of interest can be measured using monitors. In this work, we show how monitors can be parametrically specified so that the instrumentation of specifications to be monitored can be automatically performed. We prove that the original specification and the automatically obtained specification with monitors are bisimilar by construction. This means that the changes made on the original system by adding monitors do not affect its behavior. This approach allows us to have a library of possible monitors that can be safely added to analyze different properties, possibly on different objects of our systems, at will. © Springer International Publishing Switzerland 2016.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Savitsky2016,
author={Savitsky, T.D.},
title={Bayesian nonparametric mixture estimation for time-indexed functional data in R},
journal={Journal of Statistical Software},
year={2016},
volume={72},
page_count={34},
doi={10.18637/jss.v072.i02},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983070883&doi=10.18637%2fjss.v072.i02&partnerID=40&md5=b1568c533e72f73137564a56ca7f74f6},
abstract={We present growfunctions for R that offers Bayesian nonparametric estimation models for analysis of dependent, noisy time series data indexed by a collection of domains. This data structure arises from combining periodically published government survey statistics, such as are reported in the Current Population Study (CPS). The CPS publishes monthly, by-state estimates of employment levels, where each state expresses a noisy time series. Published state-level estimates from the CPS are composed from household survey responses in a model-free manner and express high levels of volatility due to insufficient sample sizes. Existing software solutions borrow information over a modeled time-based dependence to extract a de-noised time series for each domain. These solutions, however, ignore the dependence among the domains that may be additionally leveraged to improve estimation efficiency. The growfunctions package offers two fully nonparametric mixture models that simultaneously estimate both a time and domain-indexed dependence structure for a collection of time series: (1) A Gaussian process (GP) construction, which is parameterized through the covariance matrix, estimates a latent function for each domain. The covariance parameters of the latent functions are indexed by domain under a Dirichlet process prior that permits estimation of the dependence among functions across the domains: (2) An intrinsic Gaussian Markov random field prior construction provides an alternative to the GP that expresses different computation and estimation properties. In addition to performing denoised estimation of latent functions from published domain estimates, growfunctions allows estimation of collections of functions for observation units (e.g., households), rather than aggregated domains, by accounting for an informative sampling design under which the probabilities for inclusion of observation units are related to the response variable. growfunctions includes plot functions that allow visual assessments of the fit performance and dependence structure of the estimated functions. Computational efficiency is achieved by performing the sampling for estimation functions using compiled C++. © 2016, American Statistical Association. All rights reserved.},
author_keywords={Bayesian hierarchical models;  C++;  Dirichlet process;  Functional data;  Gaussian Markov random field;  Gaussian process;  R;  Time series},
document_type={Article},
source={Scopus},
}

@ARTICLE{Buchberger2016443,
author={Buchberger, B.},
title={The GDML and EuKIM projects: Short report on the initiative},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9725},
pages={443-446},
doi={10.1007/978-3-319-42432-3_56},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978864731&doi=10.1007%2f978-3-319-42432-3_56&partnerID=40&md5=28c5c42bca55250520e77a0b30740737},
abstract={We give a report on the EuKIM project, which was recently submitted to the EU Horizon 2020 program, INFRAIA-02-2017 (Integrating Activities for Starting Communities) topic, by a consortium of twelve European research groups. The project aims at building up a "Global Digital Math Library" (knowledge base) integrating and extending current efforts worldwide. A central part of the project is the design and implementation of a software system that organizes open and onestop access to mathematical knowledge and to various tools for processing mathematical knowledge. Recent progress in automated reasoning is an important issue for achieving more sophisticated levels in this endeavor. © Springer International Publishing Switzerland 2016.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lavarone2016116,
author={Lavarone, G. and Orio, N. and Polato, F. and Savino, S.},
title={Modeling the concept of movie in a software architecture for film-induced tourism},
journal={Communications in Computer and Information Science},
year={2016},
volume={612},
pages={116-125},
doi={10.1007/978-3-319-41938-1_13},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978252649&doi=10.1007%2f978-3-319-41938-1_13&partnerID=40&md5=ab1e6c98ea2ef58a635cca404cae3685},
abstract={Film induced tourism is a recent phenomenon, which is rising increasing interest in tourism management and promotion. A research project on this topic is currently investigated at the Department of Cultural Heritage of the University of Padova, with the aim of developing a software architecture for promoting film-induced tourism. One of the challenges in the development of such system was the design of a suitable model to capture the concept of movie and all the related information. This paper presents the design and implementation of this model: how the entity of movie and its related information have been represented, how the design reflects the special needs and purposes of the system, how a database was implemented and populated and the outcomes of the developed software. © Springer International Publishing Switzerland 2016.},
author_keywords={Film annotation;  Film-induced tourism;  User requirements},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Staszewski2016746,
author={Staszewski, P. and Woldan, P. and Korytkowski, M. and Scherer, R. and Wang, L.},
title={Query-by-example image retrieval in Microsoft SQL server},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9693},
pages={746-754},
doi={10.1007/978-3-319-39384-1_66},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977556129&doi=10.1007%2f978-3-319-39384-1_66&partnerID=40&md5=1baf03c4324960936ef4383497a229ca},
abstract={In this paper we present a system intended for content-based image retrieval tightly integrated with a relational database management system. Users can send query images over the appropriate web service channel or construct database queries locally. The presented framework analyses the query image based on descriptors which are generated by the bag-of-features algorithm and local interest points. The system returns the sequence of similar images with a similarity level to the query image. The software was implemented in.NET technology and Microsoft SQL Server 2012. The modular construction allows to customize the system functionality to client needs but it is especially dedicated to business applications. Important advantage of the presented approach is the support by SOA (Service-Oriented Architecture), which allows to use the system in a remote way. It is possible to build software which uses functions of the presented system by communicating over the web service API with the WCF technology. © Springer International Publishing Switzerland 2016.},
author_keywords={Bag-of-features;  Content-based image retrieval;  Microsoft SQL Server;  Query by image;  Relational databases;  WCF},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sobuś2016431,
author={Sobuś, J. and Woda, M.},
title={CPU utilization analysis of selected genetic algorithms in multi-core systems for a certain class of problems},
journal={Advances in Intelligent Systems and Computing},
year={2016},
volume={470},
pages={431-444},
doi={10.1007/978-3-319-39639-2_38},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976367286&doi=10.1007%2f978-3-319-39639-2_38&partnerID=40&md5=fe98b966db42385b8b40da1446c78fea},
abstract={This work was carried out in order to examine and compare selected models of genetic algorithms (through the implementation), using the latest tools and libraries that allow for multithreaded programming in a.NET environment. Implemented algorithms were then tested for the use of available resources, such as CPU cycles/cores consumption and the time at which they are able to provide the quality results at acceptable pace. With a choice of multi-core processors—allowing for parallel calculations on their cores, as well as genetic algorithms, one should think about how to implement the chosen algorithm so as to avoid the deadlocks and bottlenecks to make optimal use of the computing power of cores. There are many approaches to deal with such issues—a lot of tools and software libraries facilitate the implementation of such algorithms. This paper tries to address two essential questions what algorithms fit the best into multicore architecture, and which one benefits the best from available logical/physical cores producing the best possible results. © Springer International Publishing Switzerland 2016.},
author_keywords={Genetic algorithms;  Implementation;  Multi-threaded computation;  Resources consumption},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Canepa2016130,
author={Canepa, A. and Infante, G. and Hitschfeld, N. and Lobos, C.},
title={Camarón: An open-source visualization tool for the quality inspection of polygonal and polyhedral meshes},
journal={VISIGRAPP 2016 - Proceedings of the 11th Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
year={2016},
pages={130-137},
doi={10.5220/0005830501280135},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968752805&doi=10.5220%2f0005830501280135&partnerID=40&md5=423f2d4383601d40079e38da6188e987},
abstract={The numerical simulation of phenomena requires a good quality discretization (mesh) of the domain. Depending on the problem to be simulated, the mesh has to fulfil different quality criteria. Because of geometry restrictions or point density requirements, several mesh elements might not satisfy the required quality criteria and sometimes it is also not required that all elements fulfil them. Then, it would be helpful to know where unwanted elements are located in order to see if they need to be repaired or not. That is why a visualization tool that allows the user to inspect a mesh before a simulation is performed can be useful to prevent simulation problems. Moreover, if data from simulations is available, the visualization of geometrical properties together with simulation data could be also helpful to understand not expected results. These challenges have motivated us to develop Camarón, a visualization tool for large surface and volume meshes described in this paper. The surface meshes can be composed any polygonal cell and the 3D meshes can include any convex polyhedral cell. This tool was implemented in C++ and the OpenGL Shading Language (GLSL). We discuss the design and implementation issues that make our software portable, extensible and different from other visualization tools. We also compare the performance between Camarón and GeomView, TetView and MeshLab. © Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Data Visualization;  GLSL;  Polygonal Meshes;  Polyhedral Meshes;  Quality Criteria},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hassan2016478,
author={Hassan, A. and Oussalah, M.},
title={Meta-evolution style for software architecture evolution},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9587},
pages={478-489},
doi={10.1007/978-3-662-49192-8_39},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956651937&doi=10.1007%2f978-3-662-49192-8_39&partnerID=40&md5=6dfeacc45466eb106c405a565bff22c8},
abstract={Changes over time are commonplace and inevitable for any software system if it is to remain effective. Since the system changes fairly frequently, it is essential that its architecture is restructured to keep abreast of these changes. Recently the term ’evolution style’ has emerged in some studies as a technique for modeling potential architecture evolution scenarios in a particular domain that can provide reusable knowledge that encapsulates the best practices in this domain. Analysis and comparison of these alternatives assists architects in planning and thinking about architecture evolution. Our approach endeavors to unify the solutions and standardize the modeling concepts in order to develop evolution styles library that exploits the best methods and elements in the existing approaches. To this end, the main contribution of this paper is a Meta-Evolution Style (MES) for software architecture evolution, which promotes mapping and comparing of evolution styles, as well as it will help in approaching issues like reuse and interchange elements among evolution styles. © Springer-Verlag Berlin Heidelberg 2016.},
author_keywords={Architecture evolution;  Evolution styles;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Barrón-González201681,
author={Barrón-González, H.G. and Martínez-Espronceda, M. and Trigo, J.D. and Led, S. and Serrano, L.},
title={Lessons learned from the implementation of remote control for the interoperability standard ISO/IEEE11073-20601 in a standard weighing scale},
journal={Computer Methods and Programs in Biomedicine},
year={2016},
volume={123},
pages={81-93},
doi={10.1016/j.cmpb.2015.09.015},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951275570&doi=10.1016%2fj.cmpb.2015.09.015&partnerID=40&md5=760e3f9d828d0f9ea9200cec46de3b66},
abstract={The Point of Care (PoC) version of the interoperability standard ISO/IEEE11073 (X73) provided a mechanism to control remotely agents through documents X73-10201 and X73-20301. The newer version of X73 oriented to Personal Health Devices (PHD) has no mechanisms to do such a thing. The authors are working toward a common proposal with the PHD Working Group (PHD-WG) in order to adapt the remote control capabilities from X73PoC to X73PHD. However, this theoretical adaptation has to be implemented and tested to evaluate whether or not its inclusion entails an acceptable overhead and extra cost. Such proof-of-concept assessment is the main objective of this paper. For the sake of simplicity, a weighing scale with a configurable operation was chosen as use case. First, in a previous stage of the research - the model was defined. Second, the implementation methodology - both in terms of hardware and software - was defined and executed. Third, an evaluation methodology to test the remote control features was defined. Then, a thorough comparison between a weighing scale with and without remote control was performed. The results obtained indicate that, when implementing remote control in a weighing scale, the relative weight of such feature represents an overhead of as much as 53%, whereas the number of Implementation Conformance Statements (ICSs) to be satisfied by the manufacturer represent as much as 34% regarding the implementation without remote control. The new feature facilitates remote control of PHDs but, at the same time, increases overhead and costs, and, therefore, manufacturers need to weigh this trade-off. As a conclusion, this proof-of-concept helps in fostering the evolution of the remote control proposal to extend X73PHD and promotes its inclusion as part of the standard, as well as it illustrates the methodological steps for its extrapolation to other specializations. © 2015 Elsevier Ireland Ltd.},
author_keywords={Extension package;  Interoperability;  ISO/IEEE11073;  Personal health device (PHD);  Remote control;  Standard},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ureel2015,
author={Ureel, L.C., II and Wallace, C.},
title={WebTA: Automated iterative critique of student programming assignments},
journal={Proceedings - Frontiers in Education Conference, FIE},
year={2015},
volume={2015},
doi={10.1109/FIE.2015.7344225},
art_number={7344225},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960384083&doi=10.1109%2fFIE.2015.7344225&partnerID=40&md5=b426f7924eabe3e1c0e3b686cd03805a},
abstract={We introduce an interactive tool called WebTA that facilitates learning through automatic critique of student source code. Our tool provides immediate feedback to students and gives them experience with test-driven development. Students receive the benefits of cognitive apprenticeship through the feedback they receive in the tool. This facilitates tight, productive cycles of inquiry, critique and learning. WebTA compiles each student submission and executes it over a series of shakedown tests. Immediate feedback is given concerning errors and warnings, coupled with suggestions for debugging. The tool performs a textual analysis of the students source code and critiques programming style based on standard programming guidelines. To encourage inquiry through test-driven development, edge-case coverage, and API compliance, students develop and submit their own tests to be evaluated by the software. We report on use of WebTA in one first-year programming course and one second-year data structures course. Lab and assignment scores have improved with WebTA, and student comments attest to the effectiveness of the tool. Preliminary results indicate students receive higher grades with WebTA. One area with mixed results is WebTAs analysis of student developed JUnit tests; this feature improved API compliance but reduced edge-case testing. With these successful initial results, we offer suggestions for future development. © 2015 IEEE.},
author_keywords={Education;  Java;  Programming profession;  Software;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deng2015124,
author={Deng, J. and Li, T. and Jiang, L. and Han, J. and Shen, X.},
title={Design and implementation of the graphics accelerator oriented to OpenGL},
journal={Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University},
year={2015},
volume={42},
number={6},
pages={124-130},
doi={10.3969/j.issn.1001-2400.2015.06.022},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952063060&doi=10.3969%2fj.issn.1001-2400.2015.06.022&partnerID=40&md5=1ca5023ccf91c4ad3940e8dbb3643e10},
abstract={In order to explore the self-development of the graphics processing unit, this paper presents the design of a graphics accelerator which utilizes the classical pipelined structure and supports OpenGL primary commands. The matrix computational unit, which is the critical operation of geometric transformation, projection transformation and viewport transformation, primitive assembly unit and rasterization unit have been optimized to improve the efficiency. The accelerator realizes the rendering effects of Gouraud shading, lighting, full-screen anti-aliasing and texture mapping. In order to verify the circuit, the software and hardware simulation workbench based on C/C++ and SystemVerilog respectively have been established. The prototype system is implemented on EP2C70F896C6 of Altera which takes up to 83% of the FPGA resource, with the speed being up to 100MHz. The test results of plenty of rendering applications show that the accelerator possesses the capability of graphics rendering. © 2015, Science Press. All right reserved.},
author_keywords={Full-screen anti-aliasing;  Graphics accelerator;  Matrix computation;  Primitive assembly;  Prototype system},
document_type={Article},
source={Scopus},
}

@ARTICLE{Castro-León201598,
author={Castro-León, M. and Meyer, H. and Rexachs, D. and Luque, E.},
title={Fault tolerance at system level based on RADIC architecture},
journal={Journal of Parallel and Distributed Computing},
year={2015},
volume={86},
pages={98-111},
doi={10.1016/j.jpdc.2015.08.005},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942092276&doi=10.1016%2fj.jpdc.2015.08.005&partnerID=40&md5=5ae684d1608a489c0bf8e5c316c40f50},
abstract={The increasing failure rate in High Performance Computing encourages the investigation of fault tolerance mechanisms to guarantee the execution of an application in spite of node faults. This paper presents an automatic and scalable fault tolerant model designed to be transparent for applications and for message passing libraries. The model consists of detecting failures in the communication socket caused by a faulty node. In those cases, the affected processes are recovered in a healthy node and the connections are reestablished without losing data. The Redundant Array of Distributed Independent Controllers architecture proposes a decentralized model for all the tasks required in a fault tolerance system: protection, detection, recovery and masking. Decentralized algorithms allow the application to scale, which is a key property for current HPC system. Three different rollback recovery protocols are defined and discussed with the aim of offering alternatives to reduce overhead when multicore systems are used. A prototype has been implemented to carry out an exhaustive experimental evaluation through Master/Worker and Single Program Multiple Data execution models. Multiple workloads and an increasing number of processes have been taken into account to compare the above mentioned protocols. The executions take place in two multicore Linux clusters with different socket communications libraries. © 2015 Published by Elsevier Inc.},
author_keywords={Message passing;  RADIC;  Resilience;  Semi-coordinated checkpoint;  Socket;  Software fault tolerance;  Uncoordinated checkpoint},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sozykin201535,
author={Sozykin, A. and Epanchintsev, T.},
title={MIPr-a framework for distributed image processing using Hadoop},
journal={9th International Conference on Application of Information and Communication Technologies, AICT 2015 - Proceedings},
year={2015},
pages={35-39},
doi={10.1109/ICAICT.2015.7338511},
art_number={7338511},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960938704&doi=10.1109%2fICAICT.2015.7338511&partnerID=40&md5=e65152e0c4b1793f485532b8f2d50fe8},
abstract={Nowadays, the sizes of image collections are increasing dramatically and reaching petabytes of data. Such large volumes cannot be analyzed on personal computer within a reasonable time. Therefore, processing of modern image collections requires distributed computing. This paper presents a MapReduce Image Processing framework (MIPr), which provides the ability to use distributed computing for image processing. MIPr is based on MapReduce and its open source implementation Apache Hadoop. MIPr provides various forms of image representations in Hadoop internal format and the input/output tools for integration of image processing into Hadoop data workflow. The image formats in the MIPr framework are based on the popular image processing libraries. Furthermore, the MIPr includes the high-level Image processing API for developers who are not familiar with Hadoop. This API allows to create sequential functions that process one image or a group of related images. The MIPr framework applies such functions to the large amount of images in parallel. In addition, MIPr includes MapReduce implementations of popular image processing algorithms, which can be used for distributed image processing without any software development. The MIPr framework significantly simplifies image processing in Hadoop distributed environment. © 2015 IEEE.},
author_keywords={distributed computing;  hadoop;  image processing;  mapreduce},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2015803,
author={Zhang, J. and Cao, S.},
title={Design and implementation of a third-party developers auditing system based on open API},
journal={Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS},
year={2015},
volume={2015-November},
pages={803-806},
doi={10.1109/ICSESS.2015.7339178},
art_number={7339178},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958254646&doi=10.1109%2fICSESS.2015.7339178&partnerID=40&md5=f92cc1706e97d1560ef2d83bacfb1674},
abstract={With the commercial value of open services gradually reflected, major Internet companies have all launched their own open platform strategies to attract more third-party developers. In this paper, we design a third-party developers auditing system, which helps to achieve better management of all the developers. This system is build based on Open API (Application Programming Interface) in order to better adapt to the open architecture of next gene ration networks. © 2015 IEEE.},
author_keywords={developers auditing system;  MVC framework;  Open API;  REST},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rushaidat20151859,
author={Rushaidat, K. and Schwiebert, L. and Mick, J. and Jackman, B. and Potoff, J.},
title={Evaluation of hybrid parallel cell list algorithms for Monte Carlo simulation},
journal={Proceedings - 2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security and 2015 IEEE 12th International Conference on Embedded Software and Systems, HPCC-CSS-ICESS 2015},
year={2015},
pages={1859-1864},
doi={10.1109/HPCC-CSS-ICESS.2015.260},
art_number={7336443},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961683932&doi=10.1109%2fHPCC-CSS-ICESS.2015.260&partnerID=40&md5=90f3f4d35e76703aaee9fe3b41ca51ee},
abstract={This paper describes efficient, scalable parallel implementations of the conventional cell list method and a modified cell list method to calculate the total system intermolecular Lennard-Jones force interactions in the Monte Carlo Gibbs ensemble. We targeted this part of the Gibbs ensemble for optimization because it is the most computationally demanding part of the force interactions in the simulation, as it involves all the molecules in the system. The modified cell list implementation reduces the number of particles that are outside the interaction range by making the cells smaller, thus reducing the number of unnecessary distance evaluations. Evaluation of the two cell list methods is done using a hybrid MPI+OpenMP approach and a hybrid MPI+CUDA approach. The cell list methods are evaluated on a small cluster of multicore CPUs, Intel Phi coprocessors, and GPUs. The performance results are evaluated using different combinations of MPI processes, threads, and problem sizes. © 2015 IEEE.},
author_keywords={Cell List;  Gibbs Ensemble;  Hybrid Parallel Architectures;  Monte Carlo Simulations},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Molnar2015185,
author={Molnar, A.-J.},
title={Live visualization of GUI application code coverage with GUITracer},
journal={2015 IEEE 3rd Working Conference on Software Visualization, VISSOFT 2015 - Proceedings},
year={2015},
pages={185-189},
doi={10.1109/VISSOFT.2015.7332434},
art_number={7332434},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960900917&doi=10.1109%2fVISSOFT.2015.7332434&partnerID=40&md5=98270351d3c325cea120b100da7d06a7},
abstract={The present paper introduces the initial implementation of a software exploration tool targeting graphical user interface (GUI) driven applications. GUITracer facilitates the comprehension of GUI-driven applications by starting from their most conspicuous artefact-the user interface itself. The current implementation of the tool can be used with any Java-based target application that employs one of the AWT, Swing or SWT toolkits. The tool transparently instruments the target application and provides real time information about the GUI events fired. For each event, call relations within the application are displayed at method, class or package level, together with detailed coverage information. The tool facilitates feature location, program comprehension as well as GUI test creation by revealing the link between the application's GUI and its underlying code. As such, GUITracer is intended for software practitioners developing or maintaining GUI-driven applications. We believe our tool to be especially useful for entry-level practitioners as well as students seeking to understand complex GUI-driven software systems. The present paper details the rationale as well as the technical implementation of the tool. As a proof-of-concept implementation, we also discuss further development that can lead to our tool's integration into a software development workflow. © 2015 IEEE.},
author_keywords={Computer architecture;  Graphical user interfaces;  Industries;  Java;  Libraries;  Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Anzt2015,
author={Anzt, H. and Ponce, E. and Peterson, G.D. and Dongarra, J.},
title={GPU-accelerated co-design of induced dimension reduction: Algorithmic fusion and kernel overlap},
journal={Proceedings of Co-HPC 2015: 2nd International Workshop on Hardware-Software Co-Design for High Performance Computing - Held in conjunction with SC 2015: The International Conference for High Performance Computing, Networking, Storage and Analysis},
year={2015},
doi={10.1145/2834899.2834907},
art_number={5},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009135191&doi=10.1145%2f2834899.2834907&partnerID=40&md5=7c8c3c288d9722f68bc3f65de29e5bb6},
abstract={In this paper we present an optimized GPU co-design of the Induced Dimension Reduction (IDR) algorithm for solving linear systems. Starting from a baseline implementation based on the generic BLAS routines from the MAGMA software library, we apply optimizations that are based on kernel fusion and kernel overlap. Runtime experiments are used to investigate the benefit of the distinct optimization techniques for different variants of the IDR algorithm. A comparison to the reference implementation reveals that the interplay between them can succeed in cutting the overall runtime by up to about one third. Copyright © 2015 ACM.},
author_keywords={Co-design;  GPU;  Induced Dimension Reduction (IDR);  Kernel fusion;  Kernel overlap},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Munkhdorj2015,
author={Munkhdorj, B. and Takahashi, K. and Khureltulga, D. and Watashiba, Y. and Kido, Y. and Date, S. and Shimojo, S.},
title={Design and implementation of control sequence generator for SDN-enhanced MPI},
journal={Proceedings of NDM 2015: 5th International Workshop on Network-Aware Data Management - Held in conjunction with SC 2015: The International Conference for High Performance Computing, Networking, Storage and Analysis},
year={2015},
doi={10.1145/2832099.2832103},
art_number={a4},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959344215&doi=10.1145%2f2832099.2832103&partnerID=40&md5=a24aa86942be9c592172002d4542e7c2},
abstract={MPI (Message Passing Interface) offers a suite of APIs for inter-process communication among parallel processes. We have approached to the acceleration of MPI collective communication such as MPI Bcast and MPI Allreduce, taking advantage of network programmability brought by Software Definned Networking (SDN). The basic idea is to allow a SDN controller to dynamically control the packet ows generated by MPI collective communication based on the communication pattern and the underlying network conditions. Al- Though our research have succeeded to accelerate an MPI collective communication in terms of execution time, the switching of network control functionality for MPI collec- Tive communication along MPI program execution have not been considered yet. This paper presents a mechanism that provides the control sequence for SDN controller to control packet ows based on the communication plan for the entire MPI application. The control sequence encloses a chronologically ordered list of the MPI collectives operated in the MPI application and the process-related information of each in the list. To verify if the SDN-enhanced MPI collectives can be used in combination with the proposed mechanism, the envisioned environment was prototyped. As a result, SDN-enhanced MPI collectives were able to be used in com-bination. © 2015 ACM.},
author_keywords={Collective communication;  Control sequence;  High performance computing;  Message passing interface;  Software- defined networking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wunderlich2015,
author={Wunderlich, S. and Cabrera, J. and Fitzek, F.H.P. and Pedersen, M.V.},
title={Network Coding Parallelization Based on Matrix Operations for Multicore Architectures},
journal={2015 IEEE International Conference on Ubiquitous Wireless Broadband, ICUWB 2015},
year={2015},
doi={10.1109/ICUWB.2015.7324482},
art_number={7324482},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962325283&doi=10.1109%2fICUWB.2015.7324482&partnerID=40&md5=ce5f8d52af1a9248e5f6b123785b2529},
abstract={Network coding has the potential to improve the performance of current and future communication systems (including transportation and storage) and is currently even considered for communication architectures between the individual processors on same board or different boards in close proximity. Despite the fact that single core implementations show already comparable coding speeds with standard coding approaches, this paper pushes network coding to the next level by exploiting multicore architectures. The disruptive idea presented in the paper is to break with current software implementations and coding approaches and to adopt highly optimized dense matrix operations from the high performance computation field for network coding in order to increase the coding speed. The paper presents the novel coding approach for multicore architectures and shows coding speed gains on a commercial platform such as the Raspberry Pi2 with four cores in the order of up to one full magnitude. The speed increase gain is even higher than the number of cores of the Raspberry Pi2 since the newly introduced approach exploits the cache architecture way better than by-the-book matrix operations. © 2015 IEEE.},
author_keywords={Decoding;  Encoding;  Libraries;  Multicore processing;  Network coding;  Throughput},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Scherer20155525,
author={Scherer, M.K. and Trendelkamp-Schroer, B. and Paul, F. and Pérez-Hernández, G. and Hoffmann, M. and Plattner, N. and Wehmeyer, C. and Prinz, J.-H. and Noé, F.},
title={PyEMMA 2: A Software Package for Estimation, Validation, and Analysis of Markov Models},
journal={Journal of Chemical Theory and Computation},
year={2015},
volume={11},
number={11},
pages={5525-5542},
doi={10.1021/acs.jctc.5b00743},
note={cited By 542},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946887423&doi=10.1021%2facs.jctc.5b00743&partnerID=40&md5=b9fe6d01e91b4c181bf0deeec9bd523c},
abstract={Markov (state) models (MSMs) and related models of molecular kinetics have recently received a surge of interest as they can systematically reconcile simulation data from either a few long or many short simulations and allow us to analyze the essential metastable structures, thermodynamics, and kinetics of the molecular system under investigation. However, the estimation, validation, and analysis of such models is far from trivial and involves sophisticated and often numerically sensitive methods. In this work we present the open-source Python package PyEMMA (http://pyemma.org) that provides accurate and efficient algorithms for kinetic model construction. PyEMMA can read all common molecular dynamics data formats, helps in the selection of input features, provides easy access to dimension reduction algorithms such as principal component analysis (PCA) and time-lagged independent component analysis (TICA) and clustering algorithms such as k-means, and contains estimators for MSMs, hidden Markov models, and several other models. Systematic model validation and error calculation methods are provided. PyEMMA offers a wealth of analysis functions such that the user can conveniently compute molecular observables of interest. We have derived a systematic and accurate way to coarse-grain MSMs to few states and to illustrate the structures of the metastable states of the system. Plotting functions to produce a manuscript-ready presentation of the results are available. In this work, we demonstrate the features of the software and show new methodological concepts and results produced by PyEMMA. © 2015 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schneider20152111,
author={Schneider, N. and Sayle, R.A. and Landrum, G.A.},
title={Get Your Atoms in Order-An Open-Source Implementation of a Novel and Robust Molecular Canonicalization Algorithm},
journal={Journal of Chemical Information and Modeling},
year={2015},
volume={55},
number={10},
pages={2111-2120},
doi={10.1021/acs.jcim.5b00543},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945560421&doi=10.1021%2facs.jcim.5b00543&partnerID=40&md5=610802fd17b2401f4109e8b810875527},
abstract={Finding a canonical ordering of the atoms in a molecule is a prerequisite for generating a unique representation of the molecule. The canonicalization of a molecule is usually accomplished by applying some sort of graph relaxation algorithm, the most common of which is the Morgan algorithm. There are known issues with that algorithm that lead to noncanonical atom orderings as well as problems when it is applied to large molecules like proteins. Furthermore, each cheminformatics toolkit or software provides its own version of a canonical ordering, most based on unpublished algorithms, which also complicates the generation of a universal unique identifier for molecules. We present an alternative canonicalization approach that uses a standard stable-sorting algorithm instead of a Morgan-like index. Two new invariants that allow canonical ordering of molecules with dependent chirality as well as those with highly symmetrical cyclic graphs have been developed. The new approach proved to be robust and fast when tested on the 1.45 million compounds of the ChEMBL 20 data set in different scenarios like random renumbering of input atoms or SMILES round tripping. Our new algorithm is able to generate a canonical order of the atoms of protein molecules within a few milliseconds. The novel algorithm is implemented in the open-source cheminformatics toolkit RDKit. With this paper, we provide a reference Python implementation of the algorithm that could easily be integrated in any cheminformatics toolkit. This provides a first step toward a common standard for canonical atom ordering to generate a universal unique identifier for molecules other than InChI. © 2015 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hu20151299,
author={Hu, Z. and Liu, P. and Gong, J. and Wang, Q.},
title={Design and implementation of multidimensional and animated visualization system for typhoon on virtual globes},
journal={Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University},
year={2015},
volume={40},
number={10},
pages={1299-1305},
doi={10.13203/j.whugis20130669},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945976723&doi=10.13203%2fj.whugis20130669&partnerID=40&md5=1cf4ce51047cac5d54ebff3cb49ee5dd},
abstract={Typhoons, a type of tropical cyclone, are the main severe weather event effecting areas on the Southeastern coast of China. Typhoon visualization is a signifiant component in weather research and forecasting applications, and plays an important role in disaster prevention and harm reduction. However, challenges arise when the volume of data is huge, virtual globes can be regarded as a logical platform to visualize such geospatial data over the Internet, but they provide few advanced visualization tools for rendering volumetric data. This paper proposes a virtual globe-based multidimensional and animated visualization system for typhoons. We describe the key technologies, including coordinate transformation, data organization and GPU-based volume rendering. Then, we present the proposed design and implementation in World Wind, introducing the main functions. To demonstrate the capabilities of this system, the data for a simulated typhoon event are rendered on the globe. ©, 2015, Wuhan University All right reserved.},
author_keywords={GPU;  Typhoon;  Virtual globes;  Volume rendering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ruivo2015105,
author={Ruivo, P. and Oliveira, T. and Neto, M.},
title={Using resource-based view theory to assess the value of ERP commercial-packages in SMEs},
journal={Computers in Industry},
year={2015},
volume={73},
pages={105-116},
doi={10.1016/j.compind.2015.06.001},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941945769&doi=10.1016%2fj.compind.2015.06.001&partnerID=40&md5=282c88b51f53f75c6c940bc7defd69bb},
abstract={This study explores the enterprise resource planning (ERP) variations in value on small and medium enterprises (SMEs) across four commercial-packages (Microsoft NAV, SAP All-in-one, ORACLE JDE, and SAGE X3). Grounded on the resource-based view (RBV) theory of the firm, we assess a research model linking three determinants; ERP use, collaboration, and analytics to explain the ERP value in three effects (individual productivity, management control, and customer satisfaction). Using a survey data set of 883 firms across European SMEs we test the theoretical model through structural equation modelling. This study provides empirical evidence on how European SMEs find value from the top four commercial-packaged ERPs. Whereas for Dynamics and ORACLE the most important factor is analytics system capability, for SAP and SAGE it is greater collaboration system capability. Furthermore, for SAP and ORACLE greater ERP use is perceived as an important factor, but not for Dynamics and SAGE. In addition, the study finds that both collaboration and analytics capabilities are the greatest differentiators to ERP value, which is consistent with the RBV. The finding provide guidance to business implementation strategies and to software development. The limitations and future work of the study are noted. © 2015 Published by Elsevier B.V.},
author_keywords={Commercial-package;  ERP;  ERP value;  Resource-based view;  SME},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kiefer2015405,
author={Kiefer, M.A. and Molitorisz, K. and Bieler, J. and Tichy, W.F.},
title={Parallelizing a Real-Time Audio Application - A Case Study in Multithreaded Software Engineering},
journal={Proceedings - 2015 IEEE 29th International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2015},
year={2015},
pages={405-414},
doi={10.1109/IPDPSW.2015.32},
art_number={7284339},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962229274&doi=10.1109%2fIPDPSW.2015.32&partnerID=40&md5=2b8e352f20051cc0d82d815a915fcb76},
abstract={Multicore hardware is ubiquitous, but billions of lines of code in performance-critical commodity software are still sequential. Although parallel libraries, design patterns, and best practice guidelines are available, thinking parallel is still a big challenge for many software engineers. In this paper we present a case study on parallelizing commodity software using a commercial real-time audio application with over 700,000 lines of code. In contrast to best practice guidelines, our goal is to investigate what parallelization strategy can effectively be used in data stream-intensive applications. Performing an in-depth analysis of the software architecture and its run-time performance, we locate parallelization potential and propose three different parallelization strategies. We evaluate them with respect to their parallel performance impact. Regarding the application's intrinsic real-time requirement and a very short audio cycle turnaround time, a busy-waiting strategy offers the best audio performance of 327 μs per cycle on an eight-core machine. With an efficiency of 99% this is close to the optimal schedule. © 2015 IEEE.},
author_keywords={Multicore;  Parallelization;  Software Engineering;  Study},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kolek2015536,
author={Kolek, L. and Ibrahim, M.Y. and Gunawan, I. and Laribi, M.A. and Zegloul, S.},
title={Evaluation of control system reliability using combined dynamic fault trees and Markov models},
journal={Proceeding - 2015 IEEE International Conference on Industrial Informatics, INDIN 2015},
year={2015},
pages={536-543},
doi={10.1109/INDIN.2015.7281791},
art_number={7281791},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949493624&doi=10.1109%2fINDIN.2015.7281791&partnerID=40&md5=c24631e7f026b5b0f16b8272c88df687},
abstract={In this paper, dynamic simulation methods for reliability evaluation of common industry-based control system architectures are investigated. Control system design often employs complex reliability structures in the forms of several levels of software and hardware redundancies, hot and cold standby systems. This is required in order to achieve certain plant availability and safety functions. Control system maintenance requires expert knowledge due to the complexity of troubleshooting steps involved with a hardware or software failures of a large system. Hence, it is crucial to understand the effect of recovery time on reliability and on overall availability in a critical control system. Dynamic Fault Tree Analysis (DFTA), Markov Chains and Reliability Block Diagrams (RBD) are presented and a block library is introduced for addressing the aforementioned modelling problems. In order to be able to evaluate dynamic fault trees and Markov Chains, Monte Carlo simulation has been used. An industry-based case study is presented, where critical failures of a redundant Programmable Logic Controller (PLC) system are identified by a Failure Mode and Effect Analysis (FMEA). The bottom up process of modelling control system reliability is discussed. © 2015 IEEE.},
author_keywords={control system reliability;  DFTA;  failure recovery;  FMEA;  Markov model;  Monte Carlo simulation;  PLC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lindgren20151438,
author={Lindgren, P. and Lindner, M. and Lindner, A. and Pereira, D. and Pinho, L.M.},
title={Well-formed control flow for critical sections in RTFM-core},
journal={Proceeding - 2015 IEEE International Conference on Industrial Informatics, INDIN 2015},
year={2015},
pages={1438-1445},
doi={10.1109/INDIN.2015.7281944},
art_number={7281944},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949489320&doi=10.1109%2fINDIN.2015.7281944&partnerID=40&md5=895f1b12a9ec4a7f913bce305d7cff3a},
abstract={The mainstream of embedded software development as of today is dominated by C programming. To aid the development, hardware abstractions, libraries, kernels and lightweight operating systems are commonplace. Such kernels and operating systems typically impose a thread based abstraction to concurrency. However, in general thread based programming is hard, plagued by race conditions and dead-locks. For this paper we take an alternative outset in terms of a language abstraction, RTFM-core, where the system is modelled directly in terms of tasks and resources. In compliance to the Stack Resource Policy (SRP) model, the language enforces (well-formed) LIFO nesting of claimed resources, thus SRP based analysis and scheduling can be readily applied. For the execution onto bare-metal single core architectures, the rtfm-core compiler performs SRP analysis on the model and render an executable that is deadlock free and (through RTFM-kernel primitives) exploits the underlying interrupt hardware for efficient scheduling. The RTFM-core language embeds C-code and links to C-object files and libraries, and is thus applicable to the mainstream of embedded development. However, while the language enforces well-formed resource management, control flow in the embedded C-code may violate the LIFO nesting requirement. In this paper we address this issue by lifting a subset of C into the RTFM-core language allowing arbitrary control flow at the model level. In this way well-formed LIFO nesting can be enforced, and models ensured to be correct by construction. We demonstrate the feasibility by means of a prototype implementation in the rtfm-core compiler. Additionally, we develop a set of running examples and show in detail how control flow is handled at compile time and during run-time execution. © 2015 IEEE.},
author_keywords={Concurrent computing;  Hardware;  Kernel;  Libraries;  Programming;  Switches;  Synchronization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hong2015353,
author={Hong, J.-H. and Chung, K.-S.},
title={Parallel LDPC decoding on a GPU using OpenCL and global memory for accelerators},
journal={Proceedings of the 2015 IEEE International Conference on Networking, Architecture and Storage, NAS 2015},
year={2015},
pages={353-354},
doi={10.1109/NAS.2015.7255228},
art_number={7255228},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960849972&doi=10.1109%2fNAS.2015.7255228&partnerID=40&md5=82c03a07aec16e30632a134e02e1c4be},
abstract={This paper introduces a parallel software decoder of Low Density Parity Check (LDPC) codes with an Open Computing Language (OpenCL) framework including Global Memory for ACcelerators (GMAC). The LDPC code is one of the most popular and strongest error correcting codes for mobile communication systems. OpenCL is an open standard programming framework that supports programming languages and application programming interfaces (APIs) for heterogeneous platforms. GMAC is a software implementation of Asymmetric Distributed Shared Memory (ADSM) that maintains a shared logical memory space for the host to access memory objects in the physical memory of an OpenCL device. In this paper, we parallelize the iterative LDPC decoding steps on a graphics processing unit (GPU) using OpenCL. To improve the performance of the proposed decoder, data transfer optimization techniques between the host and the GPU including pre-pinned OpenCL memory objects for GMAC are applied. In terms of the entire decoding time, the speedup of the proposed LDPC decoder over a conventional OpenCL implementation is 1.28. © 2015 IEEE.},
author_keywords={GMAC;  LDPC decoder;  OpenCL},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Searle2015440,
author={Searle, S. and Wolski, M. and Simons, N. and Richardson, J.},
title={Librarians as partners in research data service development at Griffith University},
journal={Program},
year={2015},
volume={49},
number={4},
pages={440-460},
doi={10.1108/PROG-02-2015-0013},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941335865&doi=10.1108%2fPROG-02-2015-0013&partnerID=40&md5=f7d4c3690dd2d15f5f11610b31e5b915},
abstract={Purpose – The purpose of this paper is to describe the evolution to date and future directions in research data policy, infrastructure, skills development and advisory services in an Australian university, with a focus on the role of librarians. Design/methodology/approach – The authors have been involved in the development of research data services at Griffith, and the case study presents observations and reflections arising from their first-hand experiences. Findings – Griffith University’s organisational structure and “whole-of-enterprise” approach has facilitated service development to support research data. Fostering strong national partnerships has also accelerated development of institutional capability. Policies and strategies are supported by pragmatic best practice guidelines aimed directly at researchers. Iterative software development and a commitment to well-supported enterprise infrastructure enable the provision of a range of data management solutions. Training programs, repository support and data planning services are still relatively immature. Griffith recognises that information services staff (including librarians) will need more opportunities to develop knowledge and skills to support these services as they evolve. Originality/value – This case study provides examples of library-led and library-supported activities that could be used for comparative purposes by other libraries. At the same time, it provides a critical perspective by contrasting areas of good practice within the University with those of less satisfactory progress. While other institutions may have different constraints or opportunities, some of the major concepts within this paper may prove useful to advance the development of research data capability and capacity across the library profession. © 2015, Emerald Group Publishing Limited.},
author_keywords={Data;  Infrastructure;  Libraries;  Policy;  Services;  Skills},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ebad2015453,
author={Ebad, S.A. and Ahmed, M.A.},
title={Functionality-based software packaging using sequence diagrams},
journal={Software Quality Journal},
year={2015},
volume={23},
number={3},
pages={453-481},
doi={10.1007/s11219-014-9245-3},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928708405&doi=10.1007%2fs11219-014-9245-3&partnerID=40&md5=89388faf4c9ba9980f5e8b6d24b11b43},
abstract={Modular software design is characterized by partitioning the system into discrete scalable, reusable modules consisting of isolated, self-contained functional elements. Software architects use modularity to allow independent offerings and reuse. Moreover, modularity allows dealing with changing customer demands, as it offers software architectures that are stable and more adaptive to changes. There have been some attempts at automatic partitioning of object-oriented classes into modules (i.e., packages). However, all these attempts are based on source code, i.e., they occur late in the development process at the implementation stage. In this paper, we present a metric and a search-based mechanism to allow automatic functionality-based system partitioning during the architecture design phase using requirements conceptual sequence diagrams. The metric is validated against applicable theoretical properties and also experimentally against hypothetical and real-case studies using different search techniques. Results suggest that the metric together with the partitioning mechanism is promising and can be used in effectively partitioning system conceptual classes into packages. © 2014, Springer Science+Business Media New York.},
author_keywords={Automatic software packaging;  Package functionality;  Packaging metric;  Search-based software engineering;  Software architecture;  Software modularization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Filho201581,
author={Filho, R.S.S. and Tewari, A.},
title={Distributed Architecture for Mobile Contextual Integrated Field Work Applications},
journal={Proceedings - 2015 IEEE 3rd International Conference on Mobile Services, MS 2015},
year={2015},
pages={81-88},
doi={10.1109/MobServ.2015.21},
art_number={7226675},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954122783&doi=10.1109%2fMobServ.2015.21&partnerID=40&md5=f61d435e4c23f7233499744a33453b2f},
abstract={The current generation of corporate software tools &amp; applications were not designed to support the unique needs of industrial field service work. Business software applications such as project management and time keeping, for example, are typically designed for traditional desktop computing office environments. As such, they assume low user mobility, high network availability and WIMP (Windows, Icons, Menus, and Pointer) user interfaces. These are also agnostic to physical environment context and are loosely integrated with one another, often requiring users to maintain duplicated information records. As a result, Field Services Personnel as Engineers, Superintendents and Craftsmen end up spending significant amount of their work time dealing with the consequences of these inefficiencies. In this paper, we describe a distributed architecture for mobile, contextual and integrated fieldwork software applications (or MCI) designed for mobile and wearable computing platforms. This software architecture defines a contextual and mobility-aware client side API, a flexible integration middleware, and instrumented backend services. We show how MCI can enable the construction of portable, mobile, context-aware and integrated software applications discussing its use in the implementation of Smart Outage, a mobile app used for automating common tasks in Field Engineering work. © 2015 IEEE.},
author_keywords={Application Integration;  Distributed Software Architecture;  Field Engineering Automation;  Mobile &amp; Contextual Computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Beyer2015859,
author={Beyer, S.},
title={DIETs: Recommender Systems for Mobile API Developers},
journal={Proceedings - International Conference on Software Engineering},
year={2015},
volume={2},
pages={859-862},
doi={10.1109/ICSE.2015.278},
art_number={7203099},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951770011&doi=10.1109%2fICSE.2015.278&partnerID=40&md5=6bfd3771d3ef5aaf08adda263ceb955a},
abstract={The increasing number of posts related to mobile app development indicates unaddressed problems in the usage of mobile APIs. Arguing that these problems result from in- adequate documentation and shortcomings in the design and implementation of the APIs, the goal of this research is to develop and evaluate two developers' issues elimination tools (DIETs) for mobile API developers to diminish the problems of mobile applications (apps) development.After categorizing the problems, we investigate their causes, by exploring the relationships between the topics and trends of posts on Stack Overflow, the app developers' experience, the API and test code, and its changes. The results of these studies will be used to develop two DIETs that support API developers to improve the documentation, design, and implementation of their APIs. © 2015 IEEE.},
author_keywords={Api;  Mobile api developers;  Recommender systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Palomba2015669,
author={Palomba, F. and Tufano, M. and Bavota, G. and Oliveto, R. and Marcus, A. and Poshyvanyk, D. and De Lucia, A.},
title={Extract Package Refactoring in ARIES},
journal={Proceedings - International Conference on Software Engineering},
year={2015},
volume={2},
pages={669-672},
doi={10.1109/ICSE.2015.219},
art_number={7203039},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951766305&doi=10.1109%2fICSE.2015.219&partnerID=40&md5=04b79f829eb35ba2e55c02ed5fd68290},
abstract={Software evolution often leads to the degradation of software design quality. In Object-Oriented (OO) systems, this often results in packages that are hard to understand and maintain, as they group together heterogeneous classes with unrelated responsibilities. In such cases, state-of-the-art re-modularization tools solve the problem by proposing a new organization of the existing classes into packages. However, as indicated by recent empirical studies, such approaches require changing thousands of lines of code to implement the new recommended modularization. In this demo, we present the implementation of an Extract Package refactoring approach in ARIES (Automated Refactoring In EclipSe), a tool supporting refactoring operations in Eclipse. Unlike state-of-the-art approaches, ARIES automatically iden- tifies and removes single low-cohesive packages from software systems, which represent localized design flaws in the package organization, with the aim to incrementally improve the overall quality of the software modularisation. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Munch2015136,
author={Munch, D. and Paulitsch, M. and Herkersdorf, A.},
title={Monitoring of I/O for safety-critical systems using PCI express advanced error reporting},
journal={2015 10th IEEE International Symposium on Industrial Embedded Systems, SIES 2015 - Proceedings},
year={2015},
pages={136-139},
doi={10.1109/SIES.2015.7185049},
art_number={7185049},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959470991&doi=10.1109%2fSIES.2015.7185049&partnerID=40&md5=a4c5a3424d3660dbd2217cf940d445ed},
abstract={Certification is the process in which a manufacturer has to prove to authorities that an aircraft and systems like safety-critical avionic systems work safely as intended without unacceptable hazards. Means to achieve this are redundancy, separation/protection, monitoring, and recovery mechanisms. The objective of this paper is to provide a cost-efficient solution for monitoring of Commercial Off-The-Shelf (COTS) Input/Output (I/O) devices using COTS components and standards. The first contribution of this paper is an evaluation-proved single-event monitoring concept to permit PCI Express (PCIe) debugging without an expensive PCIe protocol analyzer and to facilitate build-in self-tests for COTS PCIe I/O devices. This is proved with an evaluation. The second contribution is a run-time monitoring concept to monitor, detect, prevent errors and to recover from errors for spatial separation of COTS PCIe I/O devices during operation. Both monitoring types use only available PCIe-compatible COTS components. This makes them portable to various computing architectures (Intel, ARM, PowerPC) and applicable for existing I/O virtualization and I/O sharing solutions. A practical evaluation reveals that the performance overhead (transfer time, transfer rate) inserted by the run-time monitoring is with 0.88% small and insignificant. © 2015 IEEE.},
author_keywords={Aerospace electronics;  Hardware;  Monitoring;  Performance evaluation;  Protocols;  Standards;  Virtualization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peng2015,
author={Peng, C. and Deng, J. and Zhang, B.},
title={Research of multi-views based weapon and equipment simulation model architecture},
journal={Xitong Fangzhen Xuebao / Journal of System Simulation},
year={2015},
volume={27},
number={8},
pages={1861-1867 and 1874},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945537101&partnerID=40&md5=ae6b23c96b3dc4dbbd0dc5a3ab06de5a},
abstract={It's an important approach to build reasonable and complete simulation model architecture for promoting the simulation interoperability, establishing simulation model standards and promoting reusability of models and codes. Some problems existed in the research of simulation model architecture were analyzed. The weapon and equipment simulation model architecture describing method based on multi-views was brought out by using the system engineering theory, which emphasized on the complicated relations between modes and its environment. The method could describe simulation model architecture from different views of different users. Therefore, based on DoDAF and the popular software architecture, five-view models of weapon equipment simulation model architecture framework were introduced, named process view, development view, process view, physical view and scenario view. The unified simulation model library construction plan based on the framework was analyzed, and the trend of research in the future was described briefly. ©, 2015, Chinese Association for System Simulation. All right reserved.},
author_keywords={Interoperability;  Simulation model architecture;  View;  Weapon and equipment},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Juhasz2015299,
author={Juhasz, Z. and Kozmann, Gy.},
title={A GPU-based simultaneous real-time EEG processing and visualization system for brain imaging applications},
journal={2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2015 - Proceedings},
year={2015},
pages={299-304},
doi={10.1109/MIPRO.2015.7160283},
art_number={7160283},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946134480&doi=10.1109%2fMIPRO.2015.7160283&partnerID=40&md5=3e137df9057352ec74094097e48462bb},
abstract={A data-driven prototype software is presented for EEG processing and visualization. The system relies on the GPU architecture for providing simultaneous processing and visualization of the EEG data. Two example brain imaging algorithms, the surface Laplacian and the spherical forward solution are used for illustrating the effective use of the massively parallel GPU hardware in speeding up computations. The paper describes the architecture of our system, the key design decisions, and the performance optimization of the parallel implementation. Using the CUDA-OpenGL interoperability, the computing subsystem can directly modify potential data in the OpenGL vertex memory, avoiding unnecessary GPU-Host data transfers. The system and our parallel implementations demonstrate that real-time processing and visualization is possible for a range of algorithms during EEG processing. We are confident that these results can pave the way for supercomputing-class implementations and open up new opportunities in the clinical practice and neuroscience research. © 2015 MIPRO.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rossiter2015,
author={Rossiter, S.},
title={Simulation design: Trans-paradigm best-practice from software engineering},
journal={JASSS},
year={2015},
volume={18},
number={3},
page_count={14},
doi={10.18564/jasss.2842},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934779890&doi=10.18564%2fjasss.2842&partnerID=40&md5=9aada9cdb00ab39727804b5614c87f6b},
abstract={There are growing initiatives to apply software engineering (SE) best-practice to computational science, which includes simulation. One area where the simulation literature appears to be particularly light is in the overall structural design of simulations, and what architectures and features are valuable for what reasons. (Part of the problem is that parts of this knowledge are abstracted away in simulation toolkits which are often not easily comparable, and have different conceptual aims.) To address this, I outline three key software properties which embody SE best-practices, and then define an 'idealised' software architecture for simulation-what SE would call a reference architecture-which strongly exhibits them. I show that this is universal to all simulations (largely because modelling-paradigm-specific detail is encapsulated into a 'single black box' layer of functionality) but that simulation toolkits tend to differ in how they map to them; this relates to the aims of the toolkits, which I provide a useful categorisation of. I show that, interestingly, there are several core features of this architecture that are not fully represented in any simulation toolkit that I am aware of. I present a library-JSIT-which provides some proof-of-concept implementations of them for Java-based toolkits. This library, and other ideas in the reference architecture, are put into practice on a published, multi-paradigm model of health and social care which uses the AnyLogic toolkit. I conclude with some thoughts on why this area receives so little focus, how to take it forwards, and some of the related cultural issues. © 2015 JASSS.},
author_keywords={Best-practice;  Reference-architecture;  Simulation-toolkits;  Software-engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ristov2015783,
author={Ristov, S. and Korenčic, D.},
title={Fast construction of space-optimized recursive automaton},
journal={Software - Practice and Experience},
year={2015},
volume={45},
number={6},
pages={783-799},
doi={10.1002/spe.2261},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927698862&doi=10.1002%2fspe.2261&partnerID=40&md5=7408d29f58c89feec9acfd3af70dd889},
abstract={Finite-state automata are important components in information retrieval and natural language processing software. A recursive automaton is the most compact representation of the acyclic deterministic finite-state automata. It is based on merging not only the equivalent states but also the identical substructures in an automaton. The LZ trie variant is the state-of-the-art in automata compression regarding space, but the time needed for its construction was, until now, quadratic, which has made it impractical for large inputs. In this paper, we present the first algorithm for LZ trie construction that runs in effectively linear time thereby making it an attractive choice for finite-state automata implementation. We achieve this goal by adding a new functionality to the enhanced suffix array data structure. We present two variants of the construction procedure - an optimal method regarding the final size and a method that sacrifices some compression for low intermediate memory usage. We have made the implementation of our algorithms available in an open source software package LzLex. Copyright © 2014 John Wiley & Sons, Ltd.},
author_keywords={compressed data structures;  finite-state automata;  lexicon implementation;  recursive automaton},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ekaputra20151055,
author={Ekaputra, F.J. and Sabou, M. and Serral, E. and Biffl, S.},
title={Collaborative exchange of systematic literature review results: The case of empirical software engineering},
journal={WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web},
year={2015},
pages={1055-1056},
doi={10.1145/2740908.2742027},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968648343&doi=10.1145%2f2740908.2742027&partnerID=40&md5=52789adc8092c75b8777980fde88c4c3},
abstract={Complementary to managing bibliographic information as done by digital libraries, the management of concrete research objects (e.g., experimental workflows, design patterns) is a pre-requisite to foster collaboration and re-use of research results. In this paper we describe the case of the Empirical Software Engineering domain, where researchers use systematic literature reviews (SLRs) to conduct and report on literature studies. Given their structured nature, the outputs of such SLR processes are a special and complex type of research object. Since performing SLRs is a time consuming process, it is highly desirable to enable sharing and reuse of the complex knowledge structures produced through SLRs. This would enable, for example, conducting new studies that build on the findings of previous studies. To support collaborative features necessary for multiple research groups to share and re-use each other's work, we hereby propose a solution approach that is inspired by software engineering best-practices and is implemented using Semantic Web technologies.},
author_keywords={Collaboration;  EMSE;  Research Publication;  SLR},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Du20151360,
author={Du, L. and Lan, Z.},
title={An on-the-fly surface-hopping program jade for nonadiabatic molecular dynamics of polyatomic systems: Implementation and applications},
journal={Journal of Chemical Theory and Computation},
year={2015},
volume={11},
number={4},
pages={1360-1374},
doi={10.1021/ct501106d},
note={cited By 92},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927670435&doi=10.1021%2fct501106d&partnerID=40&md5=285906a446d03c88844ea6e284936ef6},
abstract={Nonadiabatic dynamics simulations have rapidly become an indispensable tool for understanding ultrafast photochemical processes in complex systems. Here, we present our recently developed on-the-fly nonadiabatic dynamics package, JADE, which allows researchers to perform nonadiabatic excited-state dynamics simulations of polyatomic systems at an all-atomic level. The nonadiabatic dynamics is based on Tullys surface-hopping approach. Currently, several electronic structure methods (CIS, TDHF, TDDFT(RPA/TDA), and ADC(2)) are supported, especially TDDFT, aiming at performing nonadiabatic dynamics on medium- to large-sized molecules. The JADE package has been interfaced with several quantum chemistry codes, including Turbomole, Gaussian, and Gamess (US). To consider environmental effects, the Langevin dynamics was introduced as an easy-to-use scheme into the standard surface-hopping dynamics. The JADE package is mainly written in Fortran for greater numerical performance and Python for flexible interface construction, with the intent of providing open-source, easy-to-use, well-modularized, and intuitive software in the field of simulations of photochemical and photophysical processes. To illustrate the possible applications of the JADE package, we present a few applications of excited-state dynamics for various polyatomic systems, such as the methaniminium cation, fullerene (C20), p-dimethylaminobenzonitrile (DMABN) and its primary amino derivative aminobenzonitrile (ABN), and 10-hydroxybenzo[h]quinoline (10-HBQ). © 2015 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xie201547,
author={Xie, Y. and Zhang, L. and Ren, Y. and Wang, F.},
title={Design and implementation of energy consumption simulation software for energy-using systems},
journal={Proceedings - 2014 7th International Symposium on Computational Intelligence and Design, ISCID 2014},
year={2015},
volume={1},
pages={47-50},
doi={10.1109/ISCID.2014.28},
art_number={7064136},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931078349&doi=10.1109%2fISCID.2014.28&partnerID=40&md5=56d43712e6e846938ee01ab84e852093},
abstract={As lacking clear understanding of energy-using characteristics for devices, it is difficult to reasonably assess their consumption levels of energy. In order to solve this problem, chart-module integration theory is applied to energy consumption simulation field to design a software system that supports dynamic energy consumption simulation in this article. Firstly, the overall architecture of the software mainly composed by drawing core engine is described. Then, the realization processes and methods of the graph component library and drawing core engine are illustrated in detail. Finally, the process of modeling and simulation of energy consumption based on the designed software is given. The software users can use graphs to represent energy consumption modules, and give them energy consumption related properties. Applications show that the software provides a unified graphical, integrated, visual environment for energy-using systems, which effectively enhances flexibility and maneuverability of energy simulation analysis. © 2014 IEEE.},
author_keywords={energy consumption simulation;  energy-using system;  graphic primitive;  modeling and simulation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hadjidoukas20151,
author={Hadjidoukas, P.E. and Angelikopoulos, P. and Papadimitriou, C. and Koumoutsakos, P.},
title={Π4U: A high performance computing framework for Bayesian uncertainty quantification of complex models},
journal={Journal of Computational Physics},
year={2015},
volume={284},
pages={1-21},
doi={10.1016/j.jcp.2014.12.006},
note={cited By 81},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920129689&doi=10.1016%2fj.jcp.2014.12.006&partnerID=40&md5=1432dd4adbc8889106ca8711012166de},
abstract={We present Π4U,. 1 an extensible framework, for non-intrusive Bayesian Uncertainty Quantification and Propagation (UQ+P) of complex and computationally demanding physical models, that can exploit massively parallel computer architectures. The framework incorporates Laplace asymptotic approximations as well as stochastic algorithms, along with distributed numerical differentiation and task-based parallelism for heterogeneous clusters. Sampling is based on the Transitional Markov Chain Monte Carlo (TMCMC) algorithm and its variants. The optimization tasks associated with the asymptotic approximations are treated via the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). A modified subset simulation method is used for posterior reliability measurements of rare events. The framework accommodates scheduling of multiple physical model evaluations based on an adaptive load balancing library and shows excellent scalability. In addition to the software framework, we also provide guidelines as to the applicability and efficiency of Bayesian tools when applied to computationally demanding physical models. Theoretical and computational developments are demonstrated with applications drawn from molecular dynamics, structural dynamics and granular flow. © 2014 Elsevier Inc.},
author_keywords={Bayesian inference;  Distributed computing;  Parallel computing;  Reliability;  Uncertainty quantification},
document_type={Article},
source={Scopus},
}

@CONFERENCE{DaRosa201517,
author={Da Rosa, T.R. and Lemaire, R. and Clermidy, F.},
title={A co-design approach for hardware optimizations in multicore architectures using MCAPI},
journal={Proceedings - 2015 9th International Workshop on Interconnection Network Architectures: On-Chip, Multi-Chip, INA-OCMC 2015},
year={2015},
pages={17-20},
doi={10.1109/INA-OCMC.2015.11},
art_number={7051998},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934344047&doi=10.1109%2fINA-OCMC.2015.11&partnerID=40&md5=993e5ac82b2390c8f5ea1f816503a233},
abstract={Current SoC platforms targeting high-performance with high power efficiency rely on replicating several processing cores while adding dedicated hardware units for specific tasks. However, programming such architectures demand a high effort when compared to homogeneous multiprocessors since there is no widely used standard for heterogeneous embedded systems. The use of standard application programming interfaces (APIs) increases the programmability but also costs performance/memory usage overheads. Providing mechanisms at the software level leveraging on dedicated hardware resources can help reducing that impact. To address this point, this work presents a co-design approach for improving programming based on a standard API deployed through a mix of hardware and software support for tasks synchronization. Results present a reduction of up to 88% in network traffic and processor active times during synchronization phases when compared to a pure software implementation. © 2015 IEEE.},
author_keywords={HW-SW Co-Design;  MCAPI;  multicore communication;  multicore progammability.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Meyer201559,
author={Meyer, H. and Bruder, I. and Finger, A. and Heuer, A.},
title={Building digital archives: Design decisions: A best practice example},
journal={2015 4th International Symposium on Emerging Trends and Technologies in Libraries and Information Services, ETTLIS 2015 - Proceedings},
year={2015},
pages={59-64},
doi={10.1109/ETTLIS.2015.7048172},
art_number={7048172},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936129273&doi=10.1109%2fETTLIS.2015.7048172&partnerID=40&md5=f46c856d1934c46db5ebced31a26b6bb},
abstract={Building digital library applications is often a search for an applicable and adequate data or document model as well as for software tools which meets the requirements. Especially in digital archives, there are several data and document models to be considered. Unfortunately, there is no one-size-fits-all document model or system. For each application, the requirements and properties of a project have to be analyzed. In many cases, on one hand, a developer mostly together with a domain expert has to carry out time-consuming tests with possible solutions. On the other hand, one of the most important questions is sustainability and long-term archiving. Questions regarding these aspects are raised during the technical implementation, at least. In many academic projects, the maintenance and sustainability aspects leave open questions. Therefore, challenges, problems and solutions regarding document models, software systems, and sustainability are exhaustively analyzed in this paper. This is done in the context of the digital archive project DARL (Digitales Archiv Rostocker Liederbuch, engl. Digital Archive of the Rostock Songbook) conducted at University of Rostock. © 2015 IEEE.},
author_keywords={Digital archive;  Medival song texts;  Metadata;  Sustainability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Borchard201535,
author={Borchard, L. and Biondo, M. and Kutay, S. and Morck, D. and Weiss, A.P.},
title={Making journals accessible front & back: Examining open journal systems at CSU northridge},
journal={OCLC Systems and Services},
year={2015},
volume={31},
number={1},
pages={35-50},
doi={10.1108/OCLC-02-2014-0013},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921799089&doi=10.1108%2fOCLC-02-2014-0013&partnerID=40&md5=a7e555c99cd4cfdbe7f2f7fa99afd8bf},
abstract={Purpose – This study aims to examine Public Knowledge Project (PKP) Open Journal Systems (OJS) for its overall web accessibility and compliance with the Federal Electronic and Information Technology Accessibility and Compliance Act, also known as Section 508.Design/methodology/approach – Twenty-one individual web pages in the CSUN test instance of PKP’s OJS version 2.4.0 used in three back-end journal development user roles were examined using three web-accessibility tools (WAVE, Fangs, Functional Accessibility Evaluator). Errors in accessibility were then logged and mapped to specific Web Content Accessibility Guidelines (WCAG) criteria.Findings – In all, 202 accessibility errors were reported across the 21 OJS pages selected for testing. Because of this, the OJS cannot be efficiently utilized by assistive technologies and therefore does not pass the minimal level of acceptability as described in the WCAG 2.0. However, the authors found that the types of errors reported in this study could be simply and effectively remedied.Research limitations/implications – Further studies will need to corroborate, on a larger scale, the problems of accessibility found in the specific pages. Only three user roles were examined; other roles will need to be analyzed for their own problems with accessibility. Finally, although specific errors were noted, most can be easily fixed.Practical implications – There is an important need for accessible software design. In the case of CSUN, one of the campus partners will be better served by improving the web accessibility of the authors’ online open access journals.Originality/value – Although many studies and analyses of Section 508 compliance of front-facing web resources have been conducted, very few appear to address the back-end of such tools. This is the first to examine what problems in accessibility journal users with disabilities might encounter as OJS system administrators, journal managers or journal editors. © Emerald Group Publishing Limited.},
author_keywords={Accessible software design;  Library publishing;  Open access;  Open journal systems;  Section 508;  Web accessibility},
document_type={Article},
source={Scopus},
}

@ARTICLE{Parida2015,
author={Parida, B.K. and Panda, P.K. and Misra, N. and Mishra, B.K.},
title={MaxMod: a hidden Markov model based novel interface to MODELLER for improved prediction of protein 3D models},
journal={Journal of Molecular Modeling},
year={2015},
volume={21},
number={2},
page_count={10},
doi={10.1007/s00894-014-2563-3},
art_number={30},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963944276&doi=10.1007%2fs00894-014-2563-3&partnerID=40&md5=241c0fe1f528fe1e09079e10ee2a2666},
abstract={Modeling the three-dimensional (3D) structures of proteins assumes great significance because of its manifold applications in biomolecular research. Toward this goal, we present MaxMod, a graphical user interface (GUI) of the MODELLER program that combines profile hidden Markov model (profile HMM) method with Clustal Omega program to significantly improve the selection of homologous templates and target-template alignment for construction of accurate 3D protein models. MaxMod distinguishes itself from other existing GUIs of MODELLER software by implementing effortless modeling of proteins using templates that bear modified residues. Additionally, it provides various features such as loop optimization, express modeling (a feature where protein model can be generated directly from its sequence, without any further user intervention) and automatic update of PDB database, thus enhancing the user-friendly control of computational tasks. We find that HMM-based MaxMod performs better than other modeling packages in terms of execution time and model quality. MaxMod is freely available as a downloadable standalone tool for academic and non-commercial purpose at http://www.immt.res.in/maxmod/. © 2015, Springer-Verlag Berlin Heidelberg.},
author_keywords={Clustal omega;  Graphical user interface;  Hidden markov model;  Homology modeling;  Modified residues},
document_type={Article},
source={Scopus},
}

@ARTICLE{Marek2015100,
author={Marek, L. and Zheng, Y. and Ansaloni, D. and Bulej, L. and Sarimbekov, A. and Binder, W. and Tůma, P.},
title={Introduction to dynamic program analysis with DiSL},
journal={Science of Computer Programming},
year={2015},
volume={98},
number={P1},
pages={100-115},
doi={10.1016/j.scico.2014.01.003},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916942804&doi=10.1016%2fj.scico.2014.01.003&partnerID=40&md5=be10583723b08468a9475643559c6262},
abstract={Dynamic program analysis (DPA) tools assist in many software engineering and development tasks, such as profiling, program comprehension, and performance model construction and calibration. On the Java platform, many DPA tools are implemented either using aspect-oriented programming (AOP), or rely on bytecode instrumentation to modify the base program code. The pointcut/advice model found in AOP enables rapid tool development, but does not allow expressing certain instrumentations due to limitations of mainstream AOP languages - developers thus use bytecode manipulation to gain more expressiveness and performance. However, while the existing bytecode manipulation libraries handle some low-level details, they still make tool development tedious and error-prone. Targeting this issue, we provide the first complete presentation of DiSL, an open-source instrumentation framework that reconciles the conciseness of the AOP pointcut/advice model and the expressiveness and performance achievable with bytecode manipulation libraries. Specifically, we extend our previous work to provide an overview of the DiSL architecture, advanced features, and the programming model. We also include case studies illustrating successful deployment of DiSL-based DPA tools. © 2014 Elsevier B.V.},
author_keywords={Aspect-oriented programming;  Bytecode instrumentation;  Domain-specific languages;  Dynamic program analysis;  Java Virtual Machine},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ashok20151179,
author={Ashok, S. and Arjun, A. and Subashri, T.},
title={Dynamic ECDH mechanism for enhancing privacy of voice calls on mobile phones over VoIP server},
journal={Proceedings of 2014 IEEE International Conference on Advanced Communication, Control and Computing Technologies, ICACCCT 2014},
year={2015},
pages={1179-1184},
doi={10.1109/ICACCCT.2014.7019284},
art_number={7019284},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923288046&doi=10.1109%2fICACCCT.2014.7019284&partnerID=40&md5=3f2de6d9c519e92c1105e3434e8c5690},
abstract={VoIP networks are prone to confidentiality threats due to weak keys used by the AES algorithm for encryption of VoIP packets. So, in order to strengthen the key for encryption and decryption, Elliptic Curve Diffie-Hellman (ECDH) Algorithm key agreement scheme is employed with smaller key sizes resulting in faster computations. This Paper presents the Design and Implementation of an Asterisk VoIP server which serves as an exchange for placing voice calls in private Wi-Fi cloud and legacy networks. Voice over internet protocol refers to the transmission of speech across networks. The elliptic curve used in this paper is a modified NIST P-256 curve. The key generated by ECDH is highly secure because the discrete logarithmic problem is very difficult to crack in this scheme. This Method is successfully carried out for voice calls on supported handhelds connected with the wireless network. Further, every user is provided with his/her own extension number that can be used to connect to and from the outside world. This ECDH key exchanging mechanism for voice calls in real time is implemented using Asterisk PBX (Private Branch eXchange), which is an open source software. © 2014 IEEE.},
author_keywords={Asterisk PBX;  Dynamic ECDH;  IAX;  Key exchange;  SIP;  Softphone;  VoIP;  wireless communications},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DelGrosso2015285,
author={Del Grosso, A.M. and Nahli, O.},
title={Towards a flexible open-source software library for multi-layered scholarly textual studies: An Arabic case study dealing with semi-automatic language processing},
journal={Colloquium in Information Science and Technology, CIST},
year={2015},
volume={2015-January},
number={January},
pages={285-290},
doi={10.1109/CIST.2014.7016633},
art_number={7016633},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938086244&doi=10.1109%2fCIST.2014.7016633&partnerID=40&md5=ce2c69e6a6d1e549ffd662a217259d9b},
abstract={This paper presents both the general model and a case study of the Computational and Collaborative Philology Library (CoPhiLib), an ongoing initiative underway at the Institute for Computational Linguistics (ILC) of the National Research Council (CNR), Pisa, Italy. The library, designed and organized as a reusable, abstract and open-source software component, aims at solving the needs of multi-lingual and cross-lingual analysis by exposing common Application Programming Interfaces (APIs). The core modules, coded by the Java programming language, constitute the groundwork of a Web platform designed to deal with textual scholarly needs. The Web application, implemented according to the Java Enterprise specifications, focuses on multi-layered analysis for the study of literary documents and related multimedia sources. This ambitious challenge seeks to obtain the management of textual resources, on the one hand by abstracting from current language, on the other hand by decoupling from the specific requirements of single projects. This goal is achieved thanks to methodologies declared by the 'agile process', and by putting into effect suitable use case modeling, design patterns, and component-based architectures. The reusability and flexibility of the system have been tested on an Arabic case study: the system allows users to choose the morphological engine (such as AraMorph or Al-Khalil), along with linguistic granularity (i.e. with or without declension). Finally, the application enables the construction of annotated resources for further statistical engines (training set). © 2014 IEEE.},
author_keywords={API Design;  Arabic Natural Language Processing;  Design Patterns;  Information Engineering;  Text Processing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Garcia2015331,
author={Garcia, M.J. and Retsin, G.},
title={Design Methods for Large Scale Printing},
journal={Proceedings of the International Conference on Education and Research in Computer Aided Architectural Design in Europe},
year={2015},
volume={2},
pages={331-339},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102401089&partnerID=40&md5=51ce5f68e20618caf9330a204538e6f5},
abstract={With an exponential increase in the possibilities of computation and computer-controlled fabrication, high density information is becoming a reality in digital design and architecture. However, construction methods and industrial fabrication processes have not yet been reshaped to accommodate the recent changes in those disciplines. Although it is possible to build up complex simulations with millions of particles, the simulation is often disconnected from the actual fabrication process. Our research proposes a bridge between both stages, where one drives the other, producing a smooth transition from design to production. The research showcased in this paper investigates tectonic systems associated with large scale 3D printing and additive manufacturing methods, inheriting both material properties and fabrication constraints at all stages from design to production. Computational models and custom design software packages are designed and developed as strategies to organise material in space in response to specific structural and logistical input. Filamentrics, the first of two projects described, intends to develop free-form space frames with robotic plastic extrusion. Through the use of custom made extruders a vast range of prototypes were developed, evolving the design process towards the fabrication of precise structures that can be materialised using additive manufacturing without the use of a layered printing method. Instead, material limitations were studied and embedded in custom algorithms that allow depositing material in the air for internal connectivity. While Filamentrics is reshaping the way we could design and build light-weight structures, the second project Microstrata aims to establish new construction methods for compression based materials. A layering 3D printing method combines both the deposition of the binder and the distribution of an interconnected network of capillaries. These capillaries are organised following structural principles, configuring a series of channels which are left empty within the mass. In a second stage aluminium is cast in this hollow space to build a continuous tension reinforcement. © 2015, Education and research in Computer Aided Architectural Design in Europe. All rights reserved.},
author_keywords={3D printing;  Algorithm;  Digital prototyping;  Fabrication;  Robotics},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Wang2015169,
author={Wang, C. and Li, X. and Zhou, X. and Nedjah, N. and Wang, A.},
title={Codem: Software/Hardware codesign for embedded multicore systems supporting hardware services},
journal={Reconfigurable and Adaptive Computing: Theory and Applications},
year={2015},
pages={169-190},
doi={10.1201/b19157},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053976543&doi=10.1201%2fb19157&partnerID=40&md5=3d93ffc2f07663bb9871ef6626667e92},
abstract={However, although enjoying the benefits brought by reconfigurable computing technologies, current FPGA research community is still suffering from both high design complexity and limited programmability. Due to the large gap between diverse embedded applications, a high-level abstraction for programming model as well as IP accelerator integration is becoming a key problem to be attacked. In order to minimize the redesign complexity and also to maintain the flexibility among diverse IP accelerators, service-oriented architecture (SOA) concepts can be applied as a high-level modeling approach for reconfigurable coprocessor 8.5 Experiments for Software Profiling 182 8.5.1 Execution Time 183 8.5.1.1 Default Execution Time 183 8.5.1.2 Execution Time with Profiling 184 8.5.2 Hot Spot Location 184 8.5.3 Profiling Overheads 185 8.5.4 Running Mode, Optimization, and Code Size 186 8.5.4.1 ARM Mode versus Thumb Mode 186 8.5.5 ARM and Thumb Modes on Different Task Scales 187 8.5.6 ARM and Thumb Modes on Optimization Levels 187 8.6 Conclusions 188 Funding 188 References 188 architectures in FPGA platform. SOA concepts are originally raised and widely used in software engineering and web service research areas. It is common knowledge that the most significant advantage of SOA is the highly adoptable modules across different ranges of computing resources. Therefore from the exploration of SOA concepts’ benefits, we can conclude that there are two significant advantages introducing SOA to reconfigurable MPSoC platform: First, coprocessor integration interfaces are well defined, which could help researchers to construct high-level models by adding/removing modularized processors or IP cores expeditiously. Second, given uniform high-level application programming interfaces (API), programmers are no longer able to obtain full knowledge of the hardware implementation or the scheduling scheme, which will be automatically handled by the middleware. Both features will significantly ease the burden of programmers, and reduce design complexity to build an application-specific MPSoC. © 2016 by Taylor & Francis Group, LLC.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Lee2015,
author={Lee, M.H. and Ahn, H.S. and Macdonald, B.A.},
title={A case study: Robot manager for multi-robot systems with heterogeneous component-based frameworks},
journal={Australasian Conference on Robotics and Automation, ACRA},
year={2015},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023764528&partnerID=40&md5=e4c30a15b618005de1facafc4b961c2d},
abstract={Robotic software developers may need to use different component-based software frameworks, such as ROS, OPROS and OpenRTM, for developing intelligent robots, but then face challenges with interoperability, code maintainability and reusability over multiple frameworks. In this paper, we present a design and implementation of the robot manager software for our robotic software framework, which helps to integrate different programming frameworks easily, and to minimize the impact of different frameworks and new versions. We have designed the robot manager to fulfil the following four specifications: interoperability, compatibility, heterogeneous systems, and Application Program Interface (API) abstraction. The proposed framework allows us to use different applications with various components from different frameworks. It also supports communication between heterogeneous robot systems. For an evaluation case study, we have developed two different robot systems, which cooperate with each other using our robotic software framework, which in turn integrates components from different frameworks. We report on our evaluation of the effectiveness of the four design specifications of the robot manager.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Abalenkovs201567,
author={Abalenkovs, M. and Abdelfattah, A. and Dongarra, J. and Gates, M. and Haidar, A. and Kurzak, J. and Luszczek, P. and Tomov, S. and Yamazaki, I. and YarKhan, A.},
title={Parallel programming models for dense linear algebra on heterogeneous systems},
journal={Supercomputing Frontiers and Innovations},
year={2015},
volume={2},
number={4},
pages={67-86},
doi={10.14529/jsfi150405},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016800976&doi=10.14529%2fjsfi150405&partnerID=40&md5=bf443d167bb1d33bbdf74b64fa4c431c},
abstract={We present a review of the current best practices in parallel programming models for dense linear algebra (DLA) on heterogeneous architectures. We consider multicore CPUs, stand alone manycore coprocessors, GPUs, and combinations of these. Of interest is the evolution of the programming models for DLA libraries - in particular, the evolution from the popular LAPACK and ScaLAPACK libraries to their modernized counterparts PLASMA (for multicore CPUs) and MAGMA (for heterogeneous architectures), as well as other programming models and libraries. Besides providing insights into the programming techniques of the libraries considered, we outline our view of the current strengths and weaknesses of their programming models - especially in regards to hardware trends and ease of programming high-performance numerical software that current applications need - in order to motivate work and future directions for the next generation of parallel programming models for high-performance linear algebra libraries on heterogeneous systems. © The Authors 2015.},
author_keywords={Dense linear algebra;  GPU;  HPC;  Multicore;  Programming models;  Runtime},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2015,
author={Liu, S. and Huang, S. and Rao, J. and Ye, W. and Krogh, A. and Wang, J. and The Genome Denmark Consortium},
title={Discovery, genotyping and characterization of structural variation and novel sequence at single nucleotide resolution from de novo genome assemblies on a population scale},
journal={GigaScience},
year={2015},
volume={4},
number={1},
doi={10.1186/s13742-015-0103-4},
art_number={64},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991529236&doi=10.1186%2fs13742-015-0103-4&partnerID=40&md5=a74cf60dd2be292ebb00edfabe80d10f},
abstract={Background: Comprehensive recognition of genomic variation in one individual is important for understanding disease and developing personalized medication and treatment. Many tools based on DNA re-sequencing exist for identification of single nucleotide polymorphisms, small insertions and deletions (indels) as well as large deletions. However, these approaches consistently display a substantial bias against the recovery of complex structural variants and novel sequence in individual genomes and do not provide interpretation information such as the annotation of ancestral state and formation mechanism. Findings: We present a novel approach implemented in a single software package, AsmVar, to discover, genotype and characterize different forms of structural variation and novel sequence from population-scale de novo genome assemblies up to nucleotide resolution. Application of AsmVar to several human de novo genome assemblies captures a wide spectrum of structural variants and novel sequences present in the human population in high sensitivity and specificity. Conclusions: Our method provides a direct solution for investigating structural variants and novel sequences from de novo genome assemblies, facilitating the construction of population-scale pan-genomes. Our study also highlights the usefulness of the de novo assembly strategy for definition of genome structure. © 2016 Liu et al.},
author_keywords={De novo assembly;  Novel sequence;  Structural variation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Petrenko201514,
author={Petrenko, E. and Gharakhanlou, M.},
title={Research of enhanced base strain on landslide-prone slopes under anthropogenic impact},
journal={Eastern-European Journal of Enterprise Technologies},
year={2015},
volume={3},
number={7},
pages={14-22},
doi={10.15587/1729-4061.2015.43727},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979888082&doi=10.15587%2f1729-4061.2015.43727&partnerID=40&md5=457361acd2a56779cc892f93c3d6f7b4},
abstract={Design and construction in landslide-prone areas are associated with both ensuring the soil mass stability and evaluating possible appearance and enhancement of the natural and anthropogenic factors. Finite element method is the most rational solution for this class of problems. Further development of computational methods is associated with the expansion of using the mathematical analogs of ground models based on numerical calculation methods. For such problems, a model that allows to consider natural conditions and variable anthropogenic factors in landslide-prone areas, taking into account the plastic deformation of soils within the "slope-retaining structure-building" system was proposed. The paper deals with the stress-strain state of the landslide-prone slope and influence of anthropogenic factors on the process. The simulation was performed using the SATER.SOIL software package. The results have allowed to determine the areas of plastic deformation of the soil, which allowed to estimate the degree of approximation to the ultimate stress state along the slope at all stages of loading, taking into account natural and anthropogenic factors. The change in the stress-strain state of the soil mass using various engineering protection structures and their effectiveness within the "slope-retaining structure-building" system was considered.},
author_keywords={Base;  Finite element method;  landslide-prone area;  Retaining wall;  Slope},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bouraou201545,
author={Bouraou, N. and Tsybulnik, S. and Shevchuk, D.},
title={Investigation of the model of the vibration measuring channel of the complex monitoring system of steel tanks},
journal={Eastern-European Journal of Enterprise Technologies},
year={2015},
volume={5},
number={9},
pages={45-52},
doi={10.15587/1729-4061.2015.50980},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969157289&doi=10.15587%2f1729-4061.2015.50980&partnerID=40&md5=d4b2f9b7034c4225293403bcbe2e50bf},
abstract={The presence of defects and damage incurred during the manufacture, installation and operation raises the problem of controlling the technical condition of critical structures of engineering and construction facilities on one of the first places in the diagnosis of objects. In the modern world practice, this problem is solved by using complex intelligent monitoring systems. Due to the wide range of opportunities, these tools for functional diagnostics are widely used in various industries. The paper considers the problems of improving methodical, software and hardware support of the vibration measuring channel. In the software environment LabVIEW for signal recording, the virtual device with support for up to seven measuring channels (two accelerometers, one inclinometer, and four strain gauges) was developed. In the mathematical package MATLAB, the processing program of diagnostic information was implemented. In the processing program, the possibility of excluding the constant component from the measured signals was added. Assessment of the impact of ADC noise on the useful signal was performed. In the graphic package CATIA, a simplified three-dimensional model of the tank with the volume of 0.04 m3 was developed. Simplification of the model lies in excluding certain structural elements for optimizing the ratio "calculation time/ accuracy of the results" in the simulation. The vibration signals were produced by impulse excitation of structural oscillations. The length of each signal is 8192 points for optimizing processing algorithms. Using the software package ANSYS, a modal analysis of the structure was carried out. It is shown that the efficiency of the model of the vibration measuring channel is over 90 %.},
author_keywords={ANSYS;  Diagnostic system;  LabVIEW;  Vertical steel tank;  Vibration diagnostics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yue2015201,
author={Yue, S. and Gray, J.},
title={Openc: Extending C programs with computational reflection},
journal={24th International Conference on Software Engineering and Data Engineering, SEDE 2015},
year={2015},
pages={201-206},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964033535&partnerID=40&md5=cc2f63006b63382908551d6805f1465f},
abstract={Computational reflection has shown much promise for improving the quality of software by providing programming language techniques to address issues of modularity, reusability, maintainability, and extensibility. In this paper, we describe how to bring the power of computational reflection to C through a meta-object protocol (MOP), named OpenC, which offers a framework to build arbitrary source-to-source program transformation libraries for large software systems written in C. The design focus of OpenC is to automate program transformations in a straightforward and transparent way through techniques of code generation, so that client users only need to add a simple annotation to their code to be manipulated, while removing the need to know the details on how the transformations are performed. The paper provides a general motivation for using reflection and explains briefly the design and implementation of the OpenC framework. In addition, this paper will show, as an example, how OpenC can be used to build a library that can automate the use of OpenMP to parallelize a sequential program written in C.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2015,
author={Lee, K. and Kim, K. and Kang, S.},
title={Mobile service for open data visualization on geo-based images},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9808},
doi={10.1117/12.2206080},
art_number={98080L},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958231217&doi=10.1117%2f12.2206080&partnerID=40&md5=1ee5bc040f15a0f1e776eeca73440072},
abstract={Since the early 2010s, governments in most countries have adopted and promoted open data policy and open data platform. Korea are in the same situation, and government and public organizations have operated the public-accessible open data portal systems since 2011. The number of open data and data type have been increasing every year. These trends are more expandable or extensible on mobile environments. The purpose of this study is to design and implement a mobile application service to visualize various typed or formatted public open data with geo-based images on the mobile web. Open data cover downloadable data sets or open-accessible data application programming interface API. Geo-based images mean multi-sensor satellite imageries which are referred in geo-coordinates and matched with digital map sets. System components for mobile service are fully based on open sources and open development environments without any commercialized tools: PostgreSQL for database management system, OTB for remote sensing image processing, GDAL for data conversion, GeoServer for application server, OpenLayers for mobile web mapping, R for data analysis and D3.js for web-based data graphic processing. Mobile application in client side was implemented by using HTML5 for cross browser and cross platform. The result shows many advantageous points such as linking open data and geo-based data, integrating open data and open source, and demonstrating mobile applications with open data. It is expected that this approach is cost effective and process efficient implementation strategy for intelligent earth observing data. © 2015 SPIE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ci2015,
author={Ci, W. and Xie, K. and Li, T.},
title={Design and implementation of automatic color information collection system},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9815},
doi={10.1117/12.2205453},
art_number={98151N},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957669688&doi=10.1117%2f12.2205453&partnerID=40&md5=08f9a590fd2e3819605bc9c90e693b31},
abstract={In liquid crystal display (LCD) colorimetric characterization, it needs to convert RGB the device-dependent color space to CIEXYZ or CIELab the device-independent color space. Namely establishing the relationship between RGB and CIE using the data of device color and the corresponding data of CIE. Thus a color automatic message acquisition software is designed. We use openGL to fulfill the full screen display function, write c++ program and call the Eyeone equipment library functions to accomplish the equipment calibration, set the sample types, and realize functions such as sampling and preservation. The software can drive monitors or projectors display the set of sample colors automatically and collect the corresponding CIE values. The sample color of RGB values and the acquisition of CIE values can be stored in a text document, which is convenient for future extraction and analysis. Taking the cubic polynomial as an example, each channel is sampled of 17 sets using this system. And 100 sets of test data are also sampled. Using the least square method we can get the model. The average of color differences are around 2.4874, which is much lower than the CIE2000 commonly required level of 6.00.The successful implementation of the system saves the time of sample color data acquisition, and improves the efficiency of LCD colorimetric characterization. © 2015 SPIE.},
author_keywords={automatic collection;  cubic polynomial;  LCD colorimetric characterization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhu2015,
author={Zhu, R. and Li, M. and Long, Y. and Zeng, Y. and An, W.},
title={Parallel algorithm of real-time infrared image restoration based on total variation theory},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2015},
volume={9646},
doi={10.1117/12.2194264},
art_number={96460Y},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957065704&doi=10.1117%2f12.2194264&partnerID=40&md5=ca161ed57bb5396500cbd5706e5a7ac3},
abstract={Image restoration is a necessary preprocessing step for infrared remote sensing applications. Traditional methods allow us to remove the noise but penalize too much the gradients corresponding to edges. Image restoration techniques based on variational approaches can solve this over-smoothing problem for the merits of their well-defined mathematical modeling of the restore procedure. The total variation (TV) of infrared image is introduced as a L1 regularization term added to the objective energy functional. It converts the restoration process to an optimization problem of functional involving a fidelity term to the image data plus a regularization term. Infrared image restoration technology with TV-L1 model exploits the remote sensing data obtained sufficiently and preserves information at edges caused by clouds. Numerical implementation algorithm is presented in detail. Analysis indicates that the structure of this algorithm can be easily implemented in parallelization. Therefore a parallel implementation of the TV-L1 filter based on multicore architecture with shared memory is proposed for infrared real-time remote sensing systems. Massive computation of image data is performed in parallel by cooperating threads running simultaneously on multiple cores. Several groups of synthetic infrared image data are used to validate the feasibility and effectiveness of the proposed parallel algorithm. Quantitative analysis of measuring the restored image quality compared to input image is presented. Experiment results show that the TV-L1 filter can restore the varying background image reasonably, and that its performance can achieve the requirement of real-time image processing. © 2015 SPIE.},
author_keywords={Image restoration;  multicore architecture;  OpenMP;  parallel algorithm;  total variation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dimitrienko2015399,
author={Dimitrienko, Y. and Koryakov, M. and Zakharov, A.},
title={Development of SIGMA software for the supercomputer simulation of coupled aerodynamic and thermomechanical processes in composite structures of high-speed aircraft},
journal={CEUR Workshop Proceedings},
year={2015},
volume={1482},
pages={399-410},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954505297&partnerID=40&md5=5994f566f393ff9eb31ef69eb60b6c06},
abstract={An algorithm for the numerical simulation of coupled aerodynamic and thermomechanical processes in composite structures of high-speed aircraft is proposed, which allows to calculate all parameters of the three-dimensional aerogasdynamic flow near the surface of the aircraft, the heat exchange on the surface, the internal heat-and-mass transfer processes in the construction of thermodecomposition polymer composite material, and dynamic processes of heat displacement in the composite construction, including the effects of changes in the elastic characteristics of the composite, variable thermal deformation, shrinkage caused by thermodecomposition, the formation of the internal pore pressure in the composite. The computer-aided software package SIGMA that implements obtained algorithms and is capable to perform calculations on high-performance computers is developed. An example of the numerical solution of the coupled problem of a model composite construction of high-speed aircraft, showing the possibility of the proposed algorithm is given.},
author_keywords={Aerogasdynamics;  Coupled simulation;  Heat displacement;  Heatand-mass transfer;  Hypersonic flows;  Layering;  Parallel processing;  Polymer composites;  Thermodecomposition;  Thermomechanics;  Thermotension},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dragomir201539,
author={Dragomir, C. and Mogosanu, L. and Carabas, M. and Deaconescu, R. and Tapus, N.},
title={Towards the property-based testing of an L4 microkernel API},
journal={CEUR Workshop Proceedings},
year={2015},
volume={1431},
pages={39-49},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954101580&partnerID=40&md5=05d9e3f4c6d29f37e88e677ee0f1f27d},
abstract={Software testing has been a significant part of the software development process for the last 30 years and is gaining even more importance with the increasing complexity of software products. As each application has its own requirements, multiple software testing methodologies exist. It is the decision of the developers to choose the best suited types of testing methodologies for their product. This paper presents the design and implementation of a property-based testing framework. Unlike traditional testing methods this methodology uses the formal specification of the API to automatically generate the input and validate the output. The framework will be used to test the API of an L4 microkernel (called VMXL4); VMXL4 possesses the constraints of an embedded environment and of an ongoing development of a stateful system.},
author_keywords={API;  L4 microkernel;  Property-based testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Coşulschi201581,
author={Coşulschi, M. and Gabroveanu, M. and Slabu, F. and Sbîrcea, A.},
title={Scaling up a distributed computing of similarity coefficient with mapreduce},
journal={International Journal of Computer Science and Applications},
year={2015},
volume={12},
number={2},
pages={81-98},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945282957&partnerID=40&md5=b811aae3fbbb802c7b9b30432bf10958},
abstract={The work presented in this paper addresses the design and implementation of a Hadoop application and the experiments performed with this application in order to compute the Jaccard similarity metrics for two very large graphs. The algorithm involved uses the MapReduce programming model, whose aim is to distribute the computing process over several machines in order to reduce the overall running time. As a distributed programming model, MapReduce is one of the most important techniques behind Cloud computing metaphor, focused on data intensive computing in clustered environments. Hadoop open source framework provides to developers a Java API for implementing applications based on MapReduce programming paradigm. In this philosophy, the main task is divided into several smaller subtasks that can be executed or re-executed on any node in the cluster. The experimental results presented in this paper were obtained after performing various tests over two large data sets (WEBSPAM-UK 2007 and Slashdot) on a distributed cluster. © Technomathematics Research Foundatio.},
author_keywords={Big data;  Hadoop;  Jaccard similarity;  MapReduce},
document_type={Article},
source={Scopus},
}

@BOOK{Farhat2015331,
author={Farhat, E.O. and Casha, O. and John, A.},
title={Optimization of RF on-chip inductors using genetic algorithms},
journal={Computational Intelligence in Analog and Mixed-Signal (AMS) and Radio-Frequency (RF) Circuit Design},
year={2015},
pages={331-361},
doi={10.1007/978-3-319-19872-9_12},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944570307&doi=10.1007%2f978-3-319-19872-9_12&partnerID=40&md5=38a9b9128991aa204adfc53aea5d0944},
abstract={This chapter discusses the optimization of the geometry of RF on-chip inductors by means of a genetic algorithm in order to achieve adequate performance. Necessary background theory together with the modeling of these inductors is included in order to aid the discussion. A set of guidelines for the design of such inductors with a good quality factor in a standard CMOS process is also provided. The optimization process is initialized by using a set of empirical formulae in order to estimate the physical parameters of the required structure as constrained by the technology. Then, automated design optimization is executed to further improve its performance by means of dedicated software packages. The authors explain how to use state-of-the-art computer-aided design tools in the optimization process and how to efficiently simulate the inductor performance using electromagnetic simulators © Springer International Publishing Switzerland 2015.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Wu2015389,
author={Wu, X. and Shi, J. and Xie, X.},
title={The design and implementation of APK eBooks online generation system based on FBReader},
journal={Communications in Computer and Information Science},
year={2015},
volume={520},
pages={389-400},
doi={10.1007/978-3-662-47401-3_50},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944329557&doi=10.1007%2f978-3-662-47401-3_50&partnerID=40&md5=73af80afbda9042e9f93cb6e1d58acf7},
abstract={Users can easily upload custom books and complete e-book production online through automatically generating APK eBooks. Rich the e-books service of library, providing a new way for the library to built eBooks warehouse, but also to provide readers with a new service of making a custom APK eBook. Designed and implementated of APK eBooks Online Generation System based on FBReader with Android compilation mechanism and Ant, FBReader and other open source software. Solved the key technical issues, such as the production of eBook template, automated APK generation and signature conflicts during packaging process. After testing, the system is stable, simple to operate, and has a good interactive interface. The original uploaded file format types needs further expanded. In digital libraries and mobile reading and other fields have good application prospects. © Springer-Verlag Berlin Heidelberg 2015.},
author_keywords={APK ebooks;  Mobile reading;  Online generation;  Open source software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jayathilaka2015275,
author={Jayathilaka, H. and Krintz, C. and Wolski, R.},
title={EAGER: Deployment-time API governance for modern PaaS clouds},
journal={Proceedings - 2015 IEEE International Conference on Cloud Engineering, IC2E 2015},
year={2015},
pages={275-278},
doi={10.1109/IC2E.2015.69},
art_number={7092929},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944324859&doi=10.1109%2fIC2E.2015.69&partnerID=40&md5=704bc0c013e48c2bb3fe2b79229fabd5},
abstract={To track, control, and compel reuse of web APIs, we investigate a new approach to API governance - combined policy, implementation, and deployment control of web APIs. Our approach, called EAGER, provides a software architecture that integrates into PaaS platforms to support systemwide, deployment-time enforcement of governance policies. Specifically, EAGER checks for and prevents backward incompatible API changes from being deployed into production PaaS clouds, enforces service reuse, and facilitates enforcement of other best practices in software maintenance via policies. Our experiments with an EAGER prototype show that enforcing API governance at deployment-time in PaaS clouds is efficient and scalable to thousands of APIs and policies. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chien201529,
author={Chien, A. and Balaji, P. and Beckman, P. and Dun, N. and Fang, A. and Fujita, H. and Iskra, K. and Rubenstein, Z. and Zheng, Z. and Schreiber, R. and Hammond, J. and Dinan, J. and Laguna, I. and Richards, D. and Dubey, A. and Van Straalen, B. and Hoemmen, M. and Heroux, M. and Teranishi, K. and Siegel, A.},
title={Versioned distributed arrays for resilience in scientific applications: Global View Resilience},
journal={Procedia Computer Science},
year={2015},
volume={51},
number={1},
pages={29-38},
doi={10.1016/j.procs.2015.05.187},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939214452&doi=10.1016%2fj.procs.2015.05.187&partnerID=40&md5=bd01086211eae7387ceb2969e3e1ec21},
abstract={Exascale studies project reliability challenges for future high-performance computing (HPC) systems. We propose the Global View Resilience (GVR) system, a library that enables applications to add resilience in a portable, application-controlled fashion using versioned distributed arrays. We describe GVR's interfaces to distributed arrays, versioning, and cross-layer error recovery. Using several large applications (OpenMC, the preconditioned conjugate gradient solver PCG, ddcMD, and Chombo), we evaluate the programmer effort to add resilience. The required changes are small (<2% LOC), localized, and machine-independent, requiring no software architecture changes. We also measure the overhead of adding GVR versioning and show that generally overheads <2% are achieved. We conclude that GVR's interfaces and implementation are flexible and portable and create a gentle-slope path to tolerate growing error rates in future systems. © The Authors. Published by Elsevier B.V.},
author_keywords={Application-based fault tolerance;  Exascale;  Fault tolerance;  Resilience;  Scalable computing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dongarra2015,
author={Dongarra, J. and Gates, M. and Haidar, A. and Jia, Y. and Kabir, K. and Luszczek, P. and Tomov, S.},
title={HPC Programming on Intel Many-Integrated-Core Hardware with MAGMA Port to Xeon Phi},
journal={Scientific Programming},
year={2015},
volume={2015},
doi={10.1155/2015/502593},
art_number={502593},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929340698&doi=10.1155%2f2015%2f502593&partnerID=40&md5=49f408faa5233d4ec1729be90545d083},
abstract={This paper presents the design and implementation of several fundamental dense linear algebra (DLA) algorithms for multicore with Intel Xeon Phi coprocessors. In particular, we consider algorithms for solving linear systems. Further, we give an overview of the MAGMA MIC library, an open source, high performance library, that incorporates the developments presented here and, more broadly, provides the DLA functionality equivalent to that of the popular LAPACK library while targeting heterogeneous architectures that feature a mix of multicore CPUs and coprocessors. The LAPACK-compliance simplifies the use of the MAGMA MIC library in applications, while providing them with portably performant DLA. High performance is obtained through the use of the high-performance BLAS, hardware-specific tuning, and a hybridization methodology whereby we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware by minimizing data movements and mapping algorithmic requirements to the architectural strengths of the various heterogeneous hardware components. Our methodology and programming techniques are incorporated into the MAGMA MIC API which abstracts the application developer from the specifics of the Xeon Phi architecture and is therefore applicable to algorithms beyond the scope of DLA. Copyright © 2015 Jack Dongarra et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Clements2015,
author={Clements, A.T. and Kaashoek, M.F. and Zeldovich, N. and Morris, R.T. and Kohler, E.},
title={The scalable commutativity rule: Designing scalable software for multicore processors},
journal={ACM Transactions on Computer Systems},
year={2015},
volume={32},
number={4},
doi={10.1145/2699681},
art_number={a10},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921449778&doi=10.1145%2f2699681&partnerID=40&md5=f46bfa8da8157a874171bd215c6158c5},
abstract={What opportunities for multicore scalability are latent in software interfaces, such as system call APIs? Can scalability challenges and opportunities be identified even before any implementation exists, simply by considering interface specifications? To answer these questions, we introduce the scalable commutativity rule: whenever interface operations commute, they can be implemented in a way that scales. This rule is useful throughout the development process for scalablemulticore software, from the interface design through implementation, testing, and evaluation. This article formalizes the scalable commutativity rule. This requires defining a novel form of commutativity, SIM commutativity, that lets the rule apply even to complex and highly stateful software interfaces. We also introduce a suite of software development tools based on the rule. Our COMMUTER tool accepts high-level interface models, generates tests of interface operations that commute and hence could scale, and uses these tests to systematically evaluate the scalability of implementations. We apply COMMUTER to a model of 18 POSIX file and virtual memory system operations. Using the resulting 26,238 scalability tests, COMMUTER highlights Linux kernel problems previously observed to limit application scalability and identifies previously unknown bottlenecks that may be triggered by future workloads or hardware. Finally, we apply the scalable commutativity rule and COMMUTER to the design and implementation sv6, a new POSIX-like operating system. sv6's novel file and virtual memory system designs enable it to scale for 99% of the tests generated by COMMUTER. These results translate to linear scalability on an 80-core x86 machine for applications built on sv6's commutative operations.},
author_keywords={Design;  Performance;  Theory},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Segura2014335,
author={Segura, A.M. and Cuadrado, J.S. and De Lara, J.},
title={ODaaS: Towards the model-driven engineering of open data applications as data services},
journal={Proceedings - IEEE International Enterprise Distributed Object Computing Workshop, EDOCW},
year={2014},
pages={335-339},
doi={10.1109/EDOCW.2014.55},
art_number={6975379},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919754075&doi=10.1109%2fEDOCW.2014.55&partnerID=40&md5=469650c698fab90984e654d3b5152ebd},
abstract={The Data-as-a-Service (DaaS, or Data Services) paradigm enables an on-demand, service-based access to data, relying on similar principles to Software-as-a-Service (SaaS). DaaS permits centralized data quality management, a uniform view and access to heterogeneous data, and enables exposing a richer, domain-specific data model to users. Within this context, we are witnessing a trend in institutions to make information public as open data. However, such information is normally released 'as-is', in heterogeneous formats, requiring costly, ad-hoc pre-processing steps for cleansing and analysis of its underlying structure. This paper proposes an adaptation of the DaaS paradigm for the construction of open data applications. For this purpose, we introduce an architecture based on Model-Driven Engineering (MDE), consisting of (i) multi-level modelling for the description of domains, based on generic meta-models, (ii) a library of injectors to bring data on demand from heterogeneous sources into the MDE technical space, and (iii) a REST-based infrastructure to access the data services. This work presents the architecture of such framework and the first steps in its realization. © 2014 IEEE.},
author_keywords={Data services;  Model-driven engineering;  Multi-level modelling;  Open data;  Open Data as a Service (ODaaS)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Samak2014735,
author={Samak, M. and Ramanathan, M.K.},
title={Omen+: A precise dynamic deadlock detector for multithreaded Java libraries},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2014},
volume={16-21-November-2014},
pages={735-738},
doi={10.1145/2635868.2661670},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986887474&doi=10.1145%2f2635868.2661670&partnerID=40&md5=4da875ccda157832676dde808e02587e},
abstract={Designing thread-safe libraries without concurrency defects can be a challenging task. Detecting deadlocks while invoking methods in these libraries concurrently is hard due to the possible number of method invocation combinations, the object assignments to the parameters and the associated thread interleavings. In this paper, we describe the design and implementation of OMEN+ that takes a multithreaded library as the input and detects true deadlocks in a scalable manner. We achieve this by automatically synthesizing relevant multithreaded tests and analyze the associated execution traces using a precise deadlock detector. We validate the usefulness of OMEN+ by applying it on many multithreaded Java libraries and detect a number of deadlocks even in documented thread-safe libraries. The tool is available for free download at http://www. csa.iisc.ernet.in/~sss/tools/omenplus.html. Copyright 2014 ACM.},
author_keywords={Concurrency;  Deadlock detection;  Dynamic analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ruan2014388,
author={Ruan, W. and Zhai, Z.},
title={Kernel-level design to support partitioning and hierarchical real-time scheduling of ARINC 653 for VxWorks},
journal={Proceedings - 2014 World Ubiquitous Science Congress: 2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing, DASC 2014},
year={2014},
pages={388-393},
doi={10.1109/DASC.2014.76},
art_number={6945721},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916221712&doi=10.1109%2fDASC.2014.76&partnerID=40&md5=4e5e98e360ba0345a70f6c97de9467b8},
abstract={The Integrated Modular Avionic (IMA) architecture has been proposed for the next-generation avionics systems. ARINC 653 is the standards for Application Programming Interfaces (APIs) of avionics software for IMA architecture [1]. There are a great many researches on design and implementation of ARINC 653. Though some of them including VxWorks recently show high potential of providing software platform for avionics systems, efficient partition management have not been considered much for a base operating system of ARINC 653. In this paper, we propose a kernel-level design to support partitioning and hierarchical real-time scheduling of ARINC 653 for VxWorks. We have assurance that our suggestion can provide a very valuable reference for raising the level of partition scheduling for ARINC 653 especially because of the integrity of the VxWorks kernel. We show that the overhead and jitter of the proposed design is significantly lower compared with legacy partition management algorithm-a user-level design. © 2014 IEEE.},
author_keywords={arinc653;  overhead and jitter;  partition management;  VxWorks kernel},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Langr2014,
author={Langr, D. and Tvrdík, P. and Dytrych, T. and Draayer, J.P.},
title={Algorithm 947: Paraperm-parallel generation of random permutations with MPI},
journal={ACM Transactions on Mathematical Software},
year={2014},
volume={41},
number={1},
doi={10.1145/2669372},
art_number={5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908510837&doi=10.1145%2f2669372&partnerID=40&md5=d8f725f1fcbbd1557a0bdaee5a4c0ff8},
abstract={An algorithm for parallel generation of a random permutation of a large set of distinct integers is presented. This algorithm is designed for massively parallel systems with distributed memory architectures and the MPI-based runtime environments. Scalability of the algorithm is analyzed according to the memory and communication requirements. An implementation of the algorithm in a form of a software library based on the C++ programming language and the MPI application programming interface is further provided. Finally, performed experiments are described and their results discussed. The biggest of these experiments resulted in a generation of a random permutation of 241 integers in slightly more than four minutes using 131072 CPU cores. © 2014 ACM.},
author_keywords={C++;  Distributed memory;  Implementation;  MPI;  Parallel computing;  Random permutation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fang2014,
author={Fang, H.},
title={dcGOR: An R Package for Analysing Ontologies and Protein Domain Annotations},
journal={PLoS Computational Biology},
year={2014},
volume={10},
number={10},
page_count={7},
doi={10.1371/journal.pcbi.1003929},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908319821&doi=10.1371%2fjournal.pcbi.1003929&partnerID=40&md5=f0ba8b9bf1b356a9acd72064833d9807},
abstract={I introduce an open-source R package ‘dcGOR’ to provide the bioinformatics community with the ease to analyse ontologies and protein domain annotations, particularly those in the dcGO database. The dcGO is a comprehensive resource for protein domain annotations using a panel of ontologies including Gene Ontology. Although increasing in popularity, this database needs statistical and graphical support to meet its full potential. Moreover, there are no bioinformatics tools specifically designed for domain ontology analysis. As an add-on package built in the R software environment, dcGOR offers a basic infrastructure with great flexibility and functionality. It implements new data structure to represent domains, ontologies, annotations, and all analytical outputs as well. For each ontology, it provides various mining facilities, including: (i) domain-based enrichment analysis and visualisation; (ii) construction of a domain (semantic similarity) network according to ontology annotations; and (iii) significance analysis for estimating a contact (statistical significance) network. To reduce runtime, most analyses support high-performance parallel computing. Taking as inputs a list of protein domains of interest, the package is able to easily carry out in-depth analyses in terms of functional, phenotypic and diseased relevance, and network-level understanding. More importantly, dcGOR is designed to allow users to import and analyse their own ontologies and annotations on domains (taken from SCOP, Pfam and InterPro) and RNAs (from Rfam) as well. The package is freely available at CRAN for easy installation, and also at GitHub for version control. The dedicated website with reproducible demos can be found at http://supfam.org/dcGOR. © 2014 Hai Fang.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chavez-Burbano2014133,
author={Chavez-Burbano, P. and Marin-Garcia, I. and Muñoz-Arcentales, A.},
title={Ad-hoc network implementation and experimental testing using low cost and COTS components},
journal={2014 International Work Conference on Bio-Inspired Intelligence: Intelligent Systems for Biodiversity Conservation, IWOBI 2014 - Proceedings},
year={2014},
pages={133-137},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923960804&partnerID=40&md5=aaed3bee10cb39e6efc4cbf44ec45b79},
abstract={In developing countries like Ecuador, the farmers don't have access to the technology needed to increase field production or reduce the water and energy waste typical of old farming methods. This paper presents the implementation of a low cost smart irrigation system that uses a modified wireless sensor network to provide the technology tools to those farmers in developing countries willing to change their method of production. Most of the system parts were designed, manufactured and integrated with low and local components minimizing the use of expensive imported parts in order to reduce the implementation cost and create new work opportunities. This project was focused in the substitution of components, hardware integration, protocol selection and modification, design for easy and non-specialized worker assembly. © 2014 IEEE.},
author_keywords={Agricultural Applications;  Network Architecture;  Smart Irrigation Systems;  Wireless Sensor Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vincke2014712,
author={Vincke, B. and Elouardi, A. and Lambert, A. and Dine, A.},
title={SIMD and OpenMP optimization of EKF-SLAM},
journal={International Conference on Multimedia Computing and Systems -Proceedings},
year={2014},
pages={712-716},
doi={10.1109/ICMCS.2014.6911157},
art_number={6911157},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928745576&doi=10.1109%2fICMCS.2014.6911157&partnerID=40&md5=114db6063ea2eb6f854a819907907aa1},
abstract={SLAM algorithms are widely used by autonomous robots operating in unknown environments. Several works have presented optimizations mainly focused on the algorithm complexity. New computing technologies (SIMD coprocessors, multicore architecture) can greatly accelerate the processing time but require rethinking the algorithm implementation. This paper presents an efficient implementation of the EKF-SLAM algorithm on an embedded system implementing OMAP multi-core architecture and SIMD optimizations. The aim is to optimize the algorithm implementation to improve the localization quality. Results demonstrate that an optimized implementation is always needed to achieve efficient performances and can help to design embedded systems implementing a low-cost multi-core architecture operating under real time constraints. © 2014 IEEE.},
author_keywords={EKF-SLAM;  multi-core implementation;  OMAP architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cappos201453,
author={Cappos, J. and Zhuang, Y. and Oliveira, D. and Rosenthal, M. and Yeh, K.-C.},
title={Vulnerabilities as blind spots in developer's heuristic-based decision-making processes},
journal={ACM International Conference Proceeding Series},
year={2014},
volume={15-18-September-2014},
pages={53-61},
doi={10.1145/2683467.2683472},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984985662&doi=10.1145%2f2683467.2683472&partnerID=40&md5=850c88b237824f04588b99fabab24b19},
abstract={The security community spares no effort in emphasizing security awareness and the importance of building secure software. However, the number of new vulnerabilities found in today's systems is still increasing. Furthermore, old and well-studied vulnerability types such as buffer overflows and SQL injections, are still repeatedly reported in vulnerability databases. Historically, the common response has been to blame the developers for their lack of security education. This paper discusses a new hypothesis to explain this problem by introducing a new security paradigm where software vulnerabilities are viewed as developers' blind spots in their decision making. We argue that such a flawed mental process is heuristic-based, where humans solve problems without considering all the information available, just like taking shortcuts. This paper's thesis is that security thinking tends to be left out by developers during their programming, as vulnerabilities usually exist in corner cases with unusual information flows. Leveraging this paradigm, this paper introduces a novel methodology for capturing and understanding security-related blind spots in Application Programming Interfaces (APIs). Finally, it discusses how this methodology can be applied to the design and implementation of the next generation of automated diagnosis tools. Copyright 2014 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jin20142723,
author={Jin, J. and Zhang, C.-H. and Zhang, Q.},
title={Optimality of Graphlet Screening in high dimensional variable selection},
journal={Journal of Machine Learning Research},
year={2014},
volume={15},
pages={2723-2772},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908042652&partnerID=40&md5=a7a310b191cec28d5e25e6f3752a2050},
abstract={Consider a linear model Y = Xβ+σz, where X has n rows and p columns and z ∼ N(0, In). We assume both p and n are large, including the case of p &gt; n. The unknown signal vector β is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection). We are primarily interested in the regime where signals are both rare and weak so that successful variable selection is challenging but is still possible. We assume the Gram matrix G = X'X is sparse in the sense that each row has relatively few large entries (diagonals of G are normalized to 1). The sparsity of G naturally induces the sparsity of the socalled Graph of Strong Dependence (GOSD). The key insight is that there is an interesting interplay between the signal sparsity and graph sparsity: in a broad context, the signals decompose into many small-size components of GOSD that are disconnected to each other. We propose Graphlet Screening for variable selection. This is a two-step Screen and Clean procedure, where in the first step, we screen subgraphs of GOSD with sequential χ2-tests, and in the second step, we clean with penalized MLE. The main methodological innovation is to use GOSD to guide both the screening and cleaning processes. For any variable selection procedure βC, we measure its performance by the Hamming distance between the sign vectors of βC and β, and assess the optimality by the minimax Hamming distance. Compared with more stringent criteria such as exact support recovery or oracle property, which demand strong signals, the Hamming distance criterion is more appropriate for weak signals since it naturally allows a small fraction of errors. We show that in a broad class of situations, Graphlet Screening achieves the optimal rate of convergence in terms of the Hamming distance. Unlike Graphlet Screening, wellknown procedures such as the L0/L1-penalization methods do not utilize local graphic structure for variable selection, so they generally do not achieve the optimal rate of convergence, even in very simple settings and even if the tuning parameters are ideally set. The the presented algorithm is implemented as R-CRAN package ScreenClean and in matlab (available at http://www.stat.cmu.edu/~jiashun/Research/software/GS-matlab/). © 2014 Jiashun Jin, Cun-Hui Zhang, and Qi Zhang.},
author_keywords={Asymptotic minimaxity;  Graph of least favorables (GOLF);  Graph of Strong Dependence (GOSD);  Graphlet screening (GS);  Hamming distance;  Phase diagram;  Rare and weak signal model;  Screen and Clean;  Sparsity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sattinger20146550,
author={Sattinger, V. and Supanz, D. and Turcin, I.},
title={Development of a new calculation software for large deformation problems},
journal={11th World Congress on Computational Mechanics, WCCM 2014, 5th European Conference on Computational Mechanics, ECCM 2014 and 6th European Conference on Computational Fluid Dynamics, ECFD 2014},
year={2014},
pages={6550-6559},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923972970&partnerID=40&md5=318b60d37c5fbf894fa524880aa924f8},
abstract={The developed calculation program serves as an effective tool to predict the stress-distributions inside the material under large deformations. Furthermore, this calculation program provides the basis for further improvements and enhancements and thus serves as an effective tool for research and teaching.
This paper covers the development of a new stand-alone 3D finite element software package called "MSIM", which allows large deformation calculations as it occurs in the field of manufacturing technology. Since existing commercial software packages do need a lot of time to solve the highly nonlinear system of equations and do not often incorporate suitable elements as it is common in several forming processes, a new calculation program was implemented to remedy this deficiency. The first part of this work describes the development and the object-oriented implementation of the finite element program MSIM in C++, which is highly superior to commercial software packages with respect to calculation time and storage capacities. The parallelization of the assembly algorithm and the implementation of the solution algorithm for the highly nonlinear system of equations is one of the core issues. The second part focuses on the development of a novel solid-shell element and treats the locking phenomenon resulting from low order interpolation functions. The customized solid-shell element formulation is embedded in a static implicit total Lagrange formulation, which is able to deal with large deformations in accordance with the finite strain theory. For the constitutive relation, a hyperelastic material law was used in preparation to a further extension to a hyperelastic-plastic material behavior. Therefore, the determination of the elastic predictor gained from the right Cauchy Green strains, turned out to be a crucial step in hyperelasto-plasticity. Several benchmark tests were performed to evaluate the computing speed of the new calculation program MSIM and the accuracy of these solid-shell elements in comparison to the results attained by the FEM-package ANSYS. ©. The results obtained so far agrees satisfactorily by gaining a significantly reduced calculation time.},
author_keywords={FEM-software development;  Finite element technology;  Hyperelasto-plastic deformation;  Locking phenomena;  Parallel algorithms;  Solid-shell elements},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gamache2014931,
author={Gamache, P.},
title={Pragmatic hypermedia: Creating a generic, self-inflating API client for production use},
journal={WWW 2014 Companion - Proceedings of the 23rd International Conference on World Wide Web},
year={2014},
pages={931-936},
doi={10.1145/2567948.2579220},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990987666&doi=10.1145%2f2567948.2579220&partnerID=40&md5=6646457005efa0d79381e990ff454ba5},
abstract={Hypermedia API design is a method of creating APIs using hyperlinks to represent and publish an API's functionality. Hypermediabased APIs bring theoretical advantages over many other designs, including the possibility of self-updating, generic API client software. Such hypermedia API clients only lately have come to exist, and the existing hypermedia client space did not compare favorably to custom API client libraries, requiring somewhat tedious manual access to HTTP resources. Nonetheless, the limitations in creating a compelling hypermedia client were few. This paper describes the design and implementation of Hyper- Resource [19], a fully generic, production-ready Ruby client library for hypermedia APIs. The project leverages the inherent practicality of hypermedia design, demonstrates its immediate usefulness in creating self-generating API clients, enumerates several abstractions and strategies that help in creating hypermedia APIs and clients, and promotes hypermedia API design as the easiest option available to an API programmer. © Copyright 2014 by the International World Wide Web Conferences Steering Committee.},
author_keywords={Generic API client;  Hypermedia API;  Service-oriented architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Burégio2014867,
author={Burégio, V. and Nascimento, L. and Rosa, N.S. and Meira, S.},
title={Personal APIs as an enabler for designing and implementing people as social machines},
journal={WWW 2014 Companion - Proceedings of the 23rd International Conference on World Wide Web},
year={2014},
pages={867-872},
doi={10.1145/2567948.2578829},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990985758&doi=10.1145%2f2567948.2578829&partnerID=40&md5=d6fc5695203618d14ed36b7f4560ae19},
abstract={In this paper, we extend the initial classification scheme for Social Machines (SM) by including Personal APIs as a new SM-related topic of research inquiry. Personal APIs basi- cally refer to the use of Open Application Programming In- Terfaces (Open APIs) to programmatically access informa- Tion about a person (e.g., personal basic info, health-related statistics, busy data) and/or trigger his/her human capabil- ities in a standardized way. Here, we provide an overview of some existing Personal APIs and show how this approach can be used to enable the design and implementation of people as individual SMs on the Web. A proof-of-concept system that demonstrates these ideas is also outlined in this paper. © Copyright 2014 by the International World Wide Web Conferences Steering Committee.},
author_keywords={Personal APIs;  Social machines;  Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2014168,
author={Wang, F. and Wang, H. and Lei, B. and Ma, W.},
title={A research on high-performance SDN controller},
journal={Proceedings - 2014 International Conference on Cloud Computing and Big Data, CCBD 2014},
year={2014},
pages={168-174},
doi={10.1109/CCBD.2014.41},
art_number={7062891},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946690206&doi=10.1109%2fCCBD.2014.41&partnerID=40&md5=d131fe508162e59883ff56d783c82c53},
abstract={Software Defined Networking (SDN) is a new programmable network construction technology that enables centrally management and control, which is considered to be the future evolution trend of networks. A modularized carrier-grade SDN controller according to the characteristics of carrier-grade networks is designed and proposed, resolving the problem of controlling large-scale networks of carrier. The modularized architecture offers the system flexibility, scalability and stability. Functional logic of modules and core modules, such as link discovery module and topology module, are designed to meet the carrier's need. Static memory allocation, multi-threads technique and stick-package processing are used to improve the performance of controller, which is C programming language based. Processing logic of the communication mechanism of the controller is introduced, proving that the controller conforms to the OpenFlow specification and has a good interaction with OpenFlow-based switches. A controller cluster management system is used to interact with controllers through the east-west interface in order to manage large-scale networks. Furthermore, the effectiveness and high performance of the work in this paper has been verified by the testing using Cbench testing program. Moreover, the SDN controller we proposed has been running in China Telecom's Cloud Computing Key Laboratory, which showed the good results is achieved. © 2014 IEEE.},
author_keywords={carrier-grade networks;  controller;  Software Defined Networking},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Karus2014335,
author={Karus, S.},
title={XML development with plug-ins as a service},
journal={Software - Practice and Experience},
year={2014},
volume={44},
number={3},
pages={335-352},
doi={10.1002/spe.2246},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893762023&doi=10.1002%2fspe.2246&partnerID=40&md5=003f901d6395cf883a8a1c0caee5cf84},
abstract={Extensible Markup Language (XML) has quickly become a mainstream language in software development. Not only is it used for message and document interchange, it is also used to define application logic and interfaces. However, modern general purpose integrated development environments have rather limited support for XML development. The wide variety of XML-based languages makes it a challenge to build tools for comprehensive support of XML development. In this paper, we present extensions to a library exposed as an add-in for Microsoft Visual Studio and a command line tool to improve the development experience by providing access to subscribable service-based pluggable helper tools. The architecture of the solution allows easier publishing and deployment of the tools, minimising the exposure of development data to third parties and providing service providers with much needed tool usage and development process data. The tools offer developers new means to check their files against good and bad practices, automatically fix errors in XML or improve the files' conformance with development guidelines, current state of project with other projects, or run service queries to estimate their projects future progress. Copyright © 2013 John Wiley & Sons, Ltd. Copyright © 2013 John Wiley & Sons, Ltd.},
author_keywords={automation;  IDE;  quality assurance;  rules;  services;  visual studio;  XML},
document_type={Article},
source={Scopus},
}

@ARTICLE{Doröz2014766,
author={Doröz, Y. and Öztürk, E. and Sunar, B.},
title={A million-bit multiplier architecture for fully homomorphic encryption},
journal={Microprocessors and Microsystems},
year={2014},
volume={38},
number={8},
pages={766-775},
doi={10.1016/j.micpro.2014.06.003},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027955426&doi=10.1016%2fj.micpro.2014.06.003&partnerID=40&md5=7bd7175e0e284acd512f87047f88c6a1},
abstract={In this work we present a full and complete evaluation of a very large multiplication scheme in custom hardware. We designed a novel architecture to realize a million-bit multiplication scheme based on the Schönhage-Strassen Algorithm. We constructed our scheme using Number Theoretical Transform (NTT). The construction makes use of an innovative cache architecture along with processing elements customized to match the computation and access patterns of the NTT-based recursive multiplication algorithm. We realized our architecture with Verilog and using a 90 nm TSMC library, we could get a maximum clock frequency of 666 MHz. With this frequency, our architecture is able to compute the product of two million-bit integers in 7.74 ms. Our data shows that the performance of our design matches that of previously reported software implementations on a high-end 3 GHz Intel Xeon processor, while requiring only a tiny fraction of the area. 1 © 2014 Elsevier B.V. All rights reserved.},
author_keywords={Fully homomorphic encryption;  Number theoretic transform;  Very-large number multiplication},
document_type={Article},
source={Scopus},
}

@ARTICLE{Martins2014244,
author={Martins, P. and Fernandes, J.P. and Saraiva, J.},
title={A web portal for the certification of open source software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={7991 LNCS},
pages={244-260},
doi={10.1007/978-3-642-54338-8_20},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958552092&doi=10.1007%2f978-3-642-54338-8_20&partnerID=40&md5=45949c08a65628ccecb092b00437d028},
abstract={This paper presents a web portal for the certification of open source software. The portal aims at helping programmers in the internet age, when there are (too) many open source reusable libraries and tools available. Our portal offers programmers a web-based and easy setting to analyze and certify open source software, which is a crucial step to help programmers choosing among many available alternatives, and to get some guarantees before using one piece of software. The paper presents our first prototype of such web portal. It also describes in detail a domain specific language that allows programmers to describe with a high degree of abstraction specific open source software certifications. The design and implementation of this language is the core of the web portal. © Springer-Verlag Berlin Heidelberg 2014.},
author_keywords={Open source software;  Programming languages;  Software analysis;  Software certification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Baruah20141,
author={Baruah, S.},
title={The modeling and analysis of mixed-criticality systems extended abstract},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8711 LNCS},
pages={1-6},
doi={10.1007/978-3-319-10512-3_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958543784&doi=10.1007%2f978-3-319-10512-3_1&partnerID=40&md5=01fa364a50a1fa1b9f21dade90a2dee7},
abstract={Methodologies that are currently widely used in the design and implementation of safety-critical real-time application systems are primarily focused on ensuring correctness. This, in conjunction with the trend towards implementing such systems using COTS components, may lead to very poor utilization of the implementation platform resources during run-time. Mixed-criticality implementations have been proposed as one means of achieving more efficient resource utilization upon such platforms. The real-time scheduling community has been developing a theory of mixed-criticality scheduling that seeks to solve resource allocation problems for mixed-criticality systems. There is a need for the formal methods and analysis community to work on developing methodologies for the design and analysis of mixed-criticality systems; such methodologies, in conjunction with the work on mixed-criticality scheduling currently being done in the real-time scheduling community, has the potential to significantly enhance our ability to design and implement large, complex, real-time systems in a manner that is both provably correct and resource-efficient. © 2014 Springer International Publishing Switzerland.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2014,
title={3rd International Conference on Green Communications and Networks, GCN 2013},
journal={WIT Transactions on Information and Communication Technologies},
year={2014},
volume={54 VOLUME 1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940270291&partnerID=40&md5=9c4ec01d617eef65fc544c1c5c1d7f17},
abstract={The proceedings contain 148 papers. The special focus in this conference is on Green Communications and Networks. The topics include: Research of routing algorithm in wireless communications; logistics distribution system based on wireless network; a new mechanism to configure IPv6 node roaming in IPv4 network; education management system based on web 2.0; BBS-based campus culture constructing scheme; study on B/S-based enterprise item bank management system; trust evaluation model of P2P system based on the probability statistics method; security evaluation based on fault tree; scientific research and statistics system based on data mining; study on the characteristic books logistics system; study on apartment security system in colleges; cloud resource protection based on security auditing; study on maintenance scheme of digital language laboratories; a region economy control model for china exhibition industry; study on internal service recovery strategy using CIT; design of complex user-defined report on WINCC in innovative engineering environment; research on vertical industry B2B E-commerce; research on tourism E-commerce platform; QSIM-based simulation of the credit behavior in C2C e-commerce; electronic commerce network marketing based on data mining; study on a mixed E-commerce profit-making model; study on efficient cultivation scheme of e-commerce engineering; multimedia technology in the physical education; singularity point detection based on improved WTMM; LED virtual simulation based on Web3D; study of computer graphics and image processing technology; visual communication in multimedia engineering design; digital music copyright protection engineering based on encryption and digital watermarking; multimedia teaching management system based on SIP and P2P; sensitive information filtering system; study on the function characteristics of computer dynamic simulation software; research of English listening system based on web technology; design of education web site based on content management; intelligent course taking system based on evolutionary game theory; software reuses technology based on component engineering; the method of time series analysis and forecasting based on EMD; improvement on the fixed point theorems of mixed monotone operators; distributed database system network sites clustering technique; research of B-SLIM based on multiple intelligences theory; research on microblog-based higher education scheme; design of component retrieval based on semantic web technology; construction scheme of digital campus network; multipath real-time transport based on application-level relay; study on digital campus based on cloud services; enterprise strategy based on cloud computing; research of network convergence technique based on internet of things; WEB database query based on ASP technology; study of LAN security defense system in business district; study of network system security and data security in digital libraries; data integration in security information system based on web model; research on control performance of computer equipment procurement; research of surveying engineering quality management; study on centralized management of distributed computer network; research on core value system based on decision regression coefficient; tourism supply chain management based on integrated model; study on agricultural economy forecasting using grey relationship model; computer electronic service based on application software; twitter audience analysis on business; study on logistics park based on fuzzy analytic hierarchy process; encoding oriented depth video spatial processing algorithm; study of flash animation application based on action script; study on text design based on multimedia; PCA face recognition algorithm based on face edge features; study on electronic engineering design software tools; website evaluation based on factor analysis; a scenario-based lightweight software architecture analysis method; research of online registration system on autonomous enrollment; individual behavior correction system based on agent; research on intelligent management strategy based on power community; design and implementation of computerized accounting system; study on economic analysis based on matrix; study on RFID lamb back management system; study of random variable in mathematical problem solution; analysis on modeling of electronic information system based on UML; study of interactive behaviors in IPTV; study of web information extraction rules based on DOM; semantic web mining based on ontology learning and study of on-line PE theory testing system based on web database.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Humernbrum2014150,
author={Humernbrum, T. and Glinka, F. and Gorlatch, S.},
title={Using Software-Defined Networking for Real-Time Internet applications},
journal={Lecture Notes in Engineering and Computer Science},
year={2014},
volume={2209},
number={January},
pages={150-155},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938253520&partnerID=40&md5=9fc5ae7db3308b88563f3b74f5770b89},
abstract={We consider an emerging class of challenging Internet applications called Real-Time Online Interactive Applications (ROIA). Examples of ROIA are multiplayer online games, computation- and interaction-intensive training, simulation-based e-learning, etc. These applications make high QoS demands on the underlying network which involve the number of users and the actual application state and, therefore, vary at runtime. In traditional networks, the reconfiguration possibilities of the network to meet the dynamic QoS demands of ROIA are limited due to the lack of control of the network. The emerging architecture of Software-Defined Networking (SDN) decouples the control and forwarding logic from the network infrastructure, making it programmable for applications. This paper describes the specification, design and implementation of a novel Northbound API for developing ROIA that can use the advantages of SDN. We describe the basic architectural design of the SDN Module which implements the API functionality required by ROIA, and we report experimental testing and evaluation results for its prototype implementation.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Klinc20141344,
author={Klinc, R. and Peruš, I. and Dolenc, M.},
title={Re-using engineering tools: Engineering SaaS web application framework},
journal={Computing in Civil and Building Engineering - Proceedings of the 2014 International Conference on Computing in Civil and Building Engineering},
year={2014},
pages={1344-1351},
doi={10.1061/9780784413616.167},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934294557&doi=10.1061%2f9780784413616.167&partnerID=40&md5=ee874c86c4a0e53a856e603010397379},
abstract={While early days of structural analysis (and other engineering) software have been dominated by self-developed and self-maintained programs and applications, most of these tools have lately been abandoned in favor of out-of-the-box commercial software packages, usually due to the inability to cope with the advancements in the computer industry from both software and hardware perspectives. Even though the computational engines of these software tools and applications are still sound, the pre- and post-processors are outdated and not suitable for modern engineering use. On the other hand, the last decade offered simple, intuitive, effective and widely accessible services of the modern web resulting in the fusion of services blurring the boundaries between desktop computers, laptops and other consumer devices. Consequently, more and more applications are developed as web services move from the desktop environment to the browser. This paper presents the architecture, development and deployment of the web application framework that can transform console stand-alone tools into fully functional web applications. The framework was developed following the principles of the service-oriented architecture using cloud computing guidelines and the software-as-a-service model of deployment. The use of modern web programming principles and techniques provides the ability to use the framework on a wide range of consumer devices while the underlying high-performance/throughput computing system ensures scalability, flexibility and performance. Last but not least, two case applications utilizing the proposed framework are presented. © ASCE 2014.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vasilyev2014,
author={Vasilyev, G.P. and Peskov, N.V. and Gornov, V.F. and Yurchenko, I.A. and Zakharov, P.E. and Kolesova, M.V.},
title={The influence of subway's underground facilities operation on the natural thermal conditions of adjacent soil},
journal={2014 the 4th International Workshop on Computer Science and Engineering - Summer, WCSE 2014},
year={2014},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925004029&partnerID=40&md5=fb6d2cbee4b79548cc8c76183010bcdb},
abstract={This paper contains the results of research dedicated to improving the energy efficiency of Moscow subway facilities and creation of stations with "zero" external sources heat consumption. The paper presents a method of simulation of the thermal conditions for subway tunnels, which allows overcoming the difficulties associated with the informative uncertainty of baseline data, as well as difficulties associated with the approximation of the external parameters, affecting the soil's thermal conditions. Advantage of this method in comparison with the traditional approach to the modeling of thermal processes in such systems is the fact that the usage of so-called "base" experimentally obtained information about the natural soil thermal conditions in this model, can partially take into account the whole range of factors (such as presence of the groundwater, it's speed and thermal conditions, the structure and location of the soil layers, the "thermal" background from the Earth, precipitation, phase transformation of moisture in the pore space, and more), which have significant influence on the formation of the thermal regime of the tunnel but are almost impossible to be taken into account within the strict formulation of the problem today. The method has been implemented in the software package INSOLAR.GSHP.12, created by JSC "INSOLAR- INVEST". Thermal conditions; Heat exchange; Mathematical model; Heat source/sink; Underground construction; Subway.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Podobas201445,
author={Podobas, A. and Brorsson, M. and Vlassov, V.},
title={TurboBŁYSK: Scheduling for improved data-driven task performance with fast dependency resolution},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8766},
pages={45-57},
doi={10.1007/978-3-319-11454-5_4},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921513746&doi=10.1007%2f978-3-319-11454-5_4&partnerID=40&md5=23365ef3008b63bcd509f3c00a5060b6},
abstract={Data-driven task-parallelism is attracting growing interest and has now been added to OpenMP (4.0). This paradigm simplifies the writing of parallel applications, extracting parallelism, and facilitates the use of distributed memory architectures. While the programming model itself is becoming mature, a problem with current run-time scheduler implementations is that they require a very large task granularity in order to scale. This limitation goes at odds with the idea of task-parallel programing where programmers should be able to concentrate on exposing parallelism with little regard to the task granularity. To mitigate this limitation, we have designed and implemented TurboBŁYSK, a highly efficient run-time scheduler of tasks with explicit data-dependence annotations. We propose a novel mechanism based on pattern-saving that allows the scheduler to re-use previously resolved dependency patterns, based on programmer annotations, enabling programs to use even the smallest of tasks and scale well. We experimentally show that our techniques in TurboBŁYSK enable achieving nearly twice the peak performance compared with other run-time schedulers. Our techniques are not OpenMP specific and can be implemented in other task-parallel frameworks. © Springer International Publishing Switzerland 2014.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Toschev2014224,
author={Toschev, A.S.},
title={Thinking model and machine understanding in automated user request processing},
journal={CEUR Workshop Proceedings},
year={2014},
volume={1297},
pages={224-226},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915754622&partnerID=40&md5=309b88bc8fd352718b1d459f0755237d},
abstract={A mechanism of machine understanding in processing and resolving of problems generated and formulated by users in natural language is considered. The theory described is based on the Minsky thinking model. An architecture and software implementation of the computer system based on the described algorithm are presented.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schacher2014861,
author={Schacher, J.C. and Miyama, C. and Lossius, T.},
title={The SpatDIF library - Concepts and practical applications in audio software},
journal={Proceedings - 40th International Computer Music Conference, ICMC 2014 and 11th Sound and Music Computing Conference, SMC 2014 - Music Technology Meets Philosophy: From Digital Echos to Virtual Ethos},
year={2014},
pages={861-868},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908874832&partnerID=40&md5=015435e8cbd4ae96bf7b81f9a74b408d},
abstract={The development of SpatDIF, the Spatial Sound Description Interchange Format, continues with the implementation of concrete software tools. In order to make SpatDIF usable in audio workflows, two types of code implementations are developed. The first is the C/C++ software library 'libspatdif', whose purpose is to provide a reference implementation of SpatDIF. The class structure of this library and its main components embodies the principles derived from the concepts and specification of SpatDIF. The second type of tool are specific implementations in audio programming environments, which demonstrate the methods and best-use practices for working with SpatDIF. Two practical scenarios demonstrates the use of an external in MaxMSP and Pure Data as well as the implementation of the same example in a C++ environment. A short-term goal is the complete implementation of the existing specification within the library. A long-term perspective is to develop additional extensions that will further increase the utility of the SpatDIF format. Copyright: © 2014 Jan C. Schacher et al.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{PinedaOsorio2014,
author={Pineda Osorio, M.},
title={Using quantum filters to process images of diffuse axonal injury},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2014},
volume={9112},
doi={10.1117/12.2049789},
art_number={91121Q},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907053760&doi=10.1117%2f12.2049789&partnerID=40&md5=396c1f33ac13f01797f9b0b828a4a823},
abstract={Some images corresponding to a diffuse axonal injury (DAI) are processed using several quantum filters such as Hermite Weibull and Morse. Diffuse axonal injury is a particular, common and severe case of traumatic brain injury (TBI). DAI involves global damage on microscopic scale of brain tissue and causes serious neurologic abnormalities. New imaging techniques provide excellent images showing cellular damages related to DAI. Said images can be processed with quantum filters, which accomplish high resolutions of dendritic and axonal structures both in normal and pathological state. Using the Laplacian operators from the new quantum filters, excellent edge detectors for neurofiber resolution are obtained. Image quantum processing of DAI images is made using computer algebra, specifically Maple. Quantum filter plugins construction is proposed as a future research line, which can incorporated to the ImageJ software package, making its use simpler for medical personnel. © 2014 Copyright SPIE.},
author_keywords={Computer Algebra;  Diffuse Axonal Injury;  Edge detectors;  Hermite filters;  image processing;  ImageJ;  Maple;  Morse filters;  Quantum filters;  Weibull filters},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Avni2014156,
author={Avni, G. and Kupferman, O.},
title={Synthesis from component libraries with costs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8704 LNCS},
pages={156-172},
doi={10.1007/978-3-662-44584-6_12},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906761682&doi=10.1007%2f978-3-662-44584-6_12&partnerID=40&md5=5db4ea7c7ab7d10f9eb93b2192357b29},
abstract={Synthesis is the automated construction of a system from its specification. In real life, hardware and software systems are rarely constructed from scratch. Rather, a system is typically constructed from a library of components. Lustig and Vardi formalized this intuition and studied LTL synthesis from component libraries. In real life, designers seek optimal systems. In this paper we add optimality considerations to the setting. We distinguish between quality considerations (for example, size - the smaller a system is, the better it is), and pricing (for example, the payment to the company who manufactured the component). We study the problem of designing systems with minimal quality-cost and price. A key point is that while the quality cost is individual - the choices of a designer are independent of choices made by other designers that use the same library, pricing gives rise to a resource-allocation game - designers that use the same component share its price, with the share being proportional to the number of uses (a component can be used several times in a design). We study both closed and open settings, and in both we solve the problem of finding an optimal design. In a setting with multiple designers, we also study the game-theoretic problems of the induced resource-allocation game. © 2014 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{TeBrinke20141206,
author={Te Brinke, S. and Malakuti, S. and Bockisch, C. and Bergmans, L. and Akşit, M. and Katz, S.},
title={A tool-supported approach for modular design of energy-aware software},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2014},
pages={1206-1212},
doi={10.1145/2554850.2554964},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905668389&doi=10.1145%2f2554850.2554964&partnerID=40&md5=f88654f8e42ce1fdf0b53ea071f01ca9},
abstract={The reduction of energy usage by software-controlled systems has many advantages, including prolonged battery life and reduction of greenhouse gas emissions. Thus, being able to implement energy optimization in software is essential. This requires a model of the energy utilization-or more general resource utilization-for each component in the system. Optimizer components, then, analyze resource utilization of other components in terms of such a model and adapt their behavior accordingly. We have devised a notation for Resource-Utilization Models (RUMs) that can be part of a component's application programming interface (API) to facilitate the modular implementation of optimizers. In this paper, we present tools for extracting such RUMs from components with an existing implementation. Copyright 2014 ACM.},
author_keywords={CEGAR;  Energy-aware software;  Minimal abstraction;  Model checking;  Modularity;  Resource-utilization model},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bevilacqua201439,
author={Bevilacqua, V. and Pannarale, P.},
title={A semantic expert system for the evolutionary design of synthetic gene networks},
journal={GECCO 2014 - Companion Publication of the 2014 Genetic and Evolutionary Computation Conference},
year={2014},
pages={39-40},
doi={10.1145/2598394.2598504},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905652901&doi=10.1145%2f2598394.2598504&partnerID=40&md5=bd5ddb55405e985d29b133d1ffbae21c},
abstract={This work tries to cope with the standardization issue by the adoption of model exchange standards like CellML, BioBrick standard biological parts and standard signal carriers for modeling purpose. The BioBricks are easily assemblable [1] standard DNA sequences coding for well-defined structures and functions and represent an effort to introduce the engineering principles of abstraction and standardization in synthetic biology. Web applications as GenoCAD [2] are available and implements an algorithm of syntax check of the circuits designed [3], while some other tools for automatic design and optimization of genetic circuits have appeared [4] and are also specific for BioBrick systems [5]. Our generated models are made of Standard Virtual Parts modular components. Model complexity includes more interaction dynamics than previous works. The inherent software complexity has been handled by a rational use of ontologies and rule engine. The database of parts and interactions is automatically created from publicly available whole system models. We implemented a genetic algorithm searching the space of possible genetic circuits for an optimal circuit meeting user defined input-output dynamics. The tools performing structural optimization usually use stochastic strategies, while those optimizing the parameters or matching the components for a given structure can take advantage of both stochastic and deterministic strategies. In most cases it is however necessary human intervention, for example to set the value of certain kinetic parameters. To our best knowledge no tool exists which does not show a couple of these limitations, then our tool is the only capable of using a library of parts, dynamically generated from other system models available from public databases [6]. The tool automatically infers the chemical and genetic interactions occurring between entities of the repository models and applies them in the target model if opportune. The repository models have to be modeled by a specific CellML standard, the Standard Virtual Parts (SVP) [7] formalism and the components have to be annotated with OWL for unique identifiers. The output is a sequence of readily composable biological components, deposited in the registry of parts, and a complete CellML kinetic model of the system. Accordingly, a model can be generated and simulated from a sequence of BioBrick, without any human intervention. Actual tools present a moderated degree of accuracy in the prediction of the behavior, principally due to the lack of consideration of many cellular factors. Despite the advances in molecular construction, modeling and fine-tuning the behavior of synthetic circuits remains extremely challenging [8]. We tried to cope with this issue of scalability by means of ontologies coupled with a rule engine [9]. Model complexity includes more interaction dynamics than previous works, including gene regulation, interaction between small molecules and proteins but also protein-protein and post-transcriptional regulation. The domain was described by using Ontology Web Language (OWL) ontologies in conjunction with CellML [10], while complex logic was added by Jess rules [11]. The system has been successfully tested on a single test case and looks towards the creation of a web platform [12].},
author_keywords={Biological system modeling;  Design automation;  DNA;  Expert systems;  Genetic algorithms},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Artini201477,
author={Artini, M. and Atzori, C. and Bardi, A. and La Bruzzo, S. and Manghi, P. and Mikulicic, M. and Zoppi, F.},
title={The Heritage of the People's Europe Project: An Aggregative Data Infrastructure for Cultural Heritage},
journal={Communications in Computer and Information Science},
year={2014},
volume={385 CCIS},
pages={77-80},
doi={10.1007/978-3-642-54347-0_9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904704187&doi=10.1007%2f978-3-642-54347-0_9&partnerID=40&md5=457d7e8d2b39adb530f43b9ba38820c2},
abstract={HOPE (Heritage of the People's Europe) is a "Best Practice Network" for archives, libraries, museums and institutions operating in the fields of social and union history. The project provides unified access to materials about the European social and labour history from the 18th to 21st centuries. HOPE proposes guidelines and tools for the management, aggregation, harmonisation, curation and provision of digital Cultural Heritage (CH) metadata and digital objects. Moreover, it offers to institutions joining the HOPE network an operational Aggregative Data Infrastructure (ADI) for the collection, aggregation and access of metadata records from CH content providers. The HOPE ADI is realized using and extending the D-NET Software Toolkit, an enabling framework for data infrastructures. © Springer-Verlag Berlin Heidelberg 2014.},
author_keywords={aggregation;  cultural heritage;  D-NET;  data infrastructures;  mapping;  metadata records;  serviceoriented architectures},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rughetti201481,
author={Rughetti, D. and Sanzo, P.D. and Ciciani, B. and Quaglia, F.},
title={Analytical/ML mixed approach for concurrency regulation in software transactional memory},
journal={Proceedings - 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2014},
year={2014},
pages={81-91},
doi={10.1109/CCGrid.2014.118},
art_number={6846443},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904560213&doi=10.1109%2fCCGrid.2014.118&partnerID=40&md5=c4cc8b5f05578191f2f4d45c376812e6},
abstract={In this article we exploit a combination of analytical and Machine Learning (ML) techniques in order to build a performance model allowing to dynamically tune the level of concurrency of applications based on Software Transactional Memory (STM). Our mixed approach has the advantage of reducing the training time of pure machine learning methods, and avoiding approximation errors typically affecting pure analytical approaches. Hence it allows very fast construction of highly reliable performance models, which can be promptly and effectively exploited for optimizing actual application runs. We also present a real implementation of a concurrency regulation architecture, based on the mixed modeling approach, which has been integrated with the open source Tiny STM package, together with experimental data related to runs of applications taken from the STAMP benchmark suite demonstrating the effectiveness of our proposal. © 2014 IEEE.},
author_keywords={Concurrency;  Energy Optimization;  Performance Models;  Performance Optimization;  Software Transactional Memory},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bradatsch20142034,
author={Bradatsch, C. and Kluge, F. and Ungerer, T.},
title={A Cross-domain system architecture for embedded hard real-time many-core systems},
journal={Proceedings - 2013 IEEE International Conference on High Performance Computing and Communications, HPCC 2013 and 2013 IEEE International Conference on Embedded and Ubiquitous Computing, EUC 2013},
year={2014},
pages={2034-2041},
doi={10.1109/HPCC.and.EUC.2013.293},
art_number={6832176},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904012240&doi=10.1109%2fHPCC.and.EUC.2013.293&partnerID=40&md5=423c3ade64e2a131a0b048a4488b5555},
abstract={The EC project parMERASA investigates techniques for the parallelization of industrial real-time applications from automotive, avionic, and construction machinery domains. The aim is to execute such applications on many-core processors with up to 64 cores. The system software plays a key role in the deployment of applications. However, requirements of application domains differ widely, and thus no general solution can be implemented. In this paper we present the cross-domain system architecture utilized in the parMERASA project to provide Runtime Environments (RTEs) for the three different domains. The approach eases the implementation of domain-specific RTEs through a generic kernel library that provides basic hardware abstractions and timing-analyzable synchronization mechanisms. © 2013 IEEE.},
author_keywords={automotive;  avionics;  construction machinery;  cross-domain;  hard real-time;  many-core;  system architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chu2014430,
author={Chu, C.-H. and Wan, M. and Yang, Y. and Gao, J. and Deng, L.},
title={Building on-demand marketing SaaS for crowdsourcing},
journal={Proceedings - IEEE 8th International Symposium on Service Oriented System Engineering, SOSE 2014},
year={2014},
pages={430-438},
doi={10.1109/SOSE.2014.63},
art_number={6830942},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903639197&doi=10.1109%2fSOSE.2014.63&partnerID=40&md5=059cc478c31ff70e972c9ca6386e1284},
abstract={The concept of crowdsourcing has been introduced for years and greatly accepted by companies and enterprises. As opposed to current marketing systems, this paper designs a new marketing system by merging the idea of crowdsourcing into marketing information system (MkIS). Our purpose is to develop a cloud-based MkIS which serves as a platform and provides info publishing/tracking via various media and crowdsourcing features. To ensure accessibility and cost-efficiency, the product will be deployed as Software as a Service (SaaS), which will be capable of supporting large number of customers. The implementation of this idea is to construct a prototype system which targets enterprise and crowdsourcing service providers. The outcome has two deliverables. One is application server, which hosts the service API for web clients. Another is database server, which stores and replicates application data. The work serves as one of the first attempts for an information system \flexible enough to adapt marketing, crowdsourcing, and other business workflows. Moreover, it shows the potential of the form of MkIS. This paper also presents the design and implementation of the system. © 2014 IEEE.},
author_keywords={crowdsourcing; marketing information system; MkIS; software as a service; SaaS; workflow},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dongarra2014571,
author={Dongarra, J. and Gates, M. and Haidar, A. and Jia, Y. and Kabir, K. and Luszczek, P. and Tomov, S.},
title={Portable HPC programming on intel many-integrated-core hardware with MAGMA port to Xeon Phi},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8384 LNCS},
number={PART 1},
pages={571-581},
doi={10.1007/978-3-642-55224-3_53},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901274098&doi=10.1007%2f978-3-642-55224-3_53&partnerID=40&md5=052eb83cf9c345ee3f388399546ae9ae},
abstract={This paper presents the design and implementation of several fundamental dense linear algebra (DLA) algorithms for multicore with Intel Xeon Phi Coprocessors. In particular, we consider algorithms for solving linear systems. Further, we give an overview of the MAGMA MIC library, an open source, high performance library that incorporates the developments presented, and in general provides to heterogeneous architectures of multicore with coprocessors the DLA functionality of the popular LAPACK library. The LAPACK-compliance simplifies the use of the MAGMA MIC library in applications, while providing them with portably performant DLA. High performance is obtained through use of the high-performance BLAS, hardware-specific tuning, and a hybridization methodology where we split the algorithm into computational tasks of various granularities. Execution of those tasks is properly scheduled over the heterogeneous hardware components by minimizing data movements and mapping algorithmic requirements to the architectural strengths of the various heterogeneous hardware components. Our methodology and programming techniques are incorporated into the MAGMA MIC API, which abstracts the application developer from the specifics of the Xeon Phi architecture and is therefore applicable to algorithms beyond the scope of DLA. © 2014 Springer-Verlag.},
author_keywords={Communication and computation overlap;  Dynamic runtime scheduling using dataflow dependences;  Hardware accelerators and coprocessors;  Intel Xeon Phi processor;  Many Integrated Cores;  Numerical linear algebra},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alnusair2014315,
author={Alnusair, A. and Zhao, T. and Yan, G.},
title={Rule-based detection of design patterns in program code},
journal={International Journal on Software Tools for Technology Transfer},
year={2014},
volume={16},
number={3},
pages={315-334},
doi={10.1007/s10009-013-0292-z},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901191663&doi=10.1007%2fs10009-013-0292-z&partnerID=40&md5=cb8dc493bff6396391f8f14e54d06e72},
abstract={The process of understanding and reusing software is often time-consuming, especially in legacy code and open-source libraries. While some core code of open-source libraries may be well-documented, it is frequently the case that open-source libraries lack informative API documentation and reliable design information. As a result, the source code itself is often the sole reliable source of information for program understanding activities. In this article, we propose a reverse-engineering approach that can provide assistance during the process of understanding software through the automatic recovery of hidden design patterns in software libraries. Specifically, we use ontology formalism to represent the conceptual knowledge of the source code and semantic rules to capture the structures and behaviors of the design patterns in the libraries. Several software libraries were examined with this approach and the evaluation results show that effective and flexible detection of design patterns can be achieved without using hard-coded heuristics. © 2013 Springer-Verlag Berlin Heidelberg.},
author_keywords={Design patterns;  Design recovery;  Knowledge representation;  Ontology formalisms;  Semantic inference;  Software maintenance},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Buono2014262,
author={Buono, D. and De Matteis, T. and Mencagli, G. and Vanneschi, M.},
title={Optimizing message-passing on multicore architectures using hardware multi-threading},
journal={Proceedings - 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, PDP 2014},
year={2014},
pages={262-270},
doi={10.1109/PDP.2014.63},
art_number={6787285},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899410018&doi=10.1109%2fPDP.2014.63&partnerID=40&md5=742e783bbf7b8ca7eb76ea7600e82697},
abstract={Shared-memory and message-passing are two opposite models to develop parallel computations. The shared-memory model, adopted by existing frameworks such as OpenMP, represents a de-facto standard on multi-/many-core architectures. However, message-passing deserves to be studied for its inherent properties in terms of portability and flexibility as well as for its better ease of debugging. Achieving good performance from the use of messages in shared-memory architectures requires an efficient implementation of the run-time support. This paper investigates the definition of a delegation mechanism on multi-threaded architectures able to: (i) overlap communications with calculation phases, (ii) parallelize distribution and collective operations. Our ideas have been exemplified using two parallel benchmarks on the Intel Phi, showing that in these applications our message-passing support outperforms MPI and reaches similar performance compared to standard OpenMP implementations. © 2014 IEEE.},
author_keywords={communications-calculation threading. overlapping;  hardware multi-threading;  message passing;  multi-core;  shared memory},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ganesan2014,
author={Ganesan, D. and Lindvall, M.},
title={ADAM: External dependency-driven architecture discovery and analysis of quality attributes},
journal={ACM Transactions on Software Engineering and Methodology},
year={2014},
volume={23},
number={2},
doi={10.1145/2529998},
art_number={17},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897988189&doi=10.1145%2f2529998&partnerID=40&md5=beb014b796c4eaf226fab4958cd6d66e},
abstract={This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAMsupports the discovery ofmodule and runtime views as well as the analysis of quality attributes, such as testability, performance, andmaintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed. © 2014 ACM.},
author_keywords={Concerns;  External entities;  Knowledge base;  Maintainability;  Module and runtime views;  Quality;  Reverse engineering;  Software architecture;  Testability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kameyama20143,
author={Kameyama, Y. and Kiselyov, O. and Shan, C.-C.},
title={Combinators for impure yet hygienic code generation},
journal={PEPM 2014 - Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation, Co-located with POPL 2014},
year={2014},
pages={3-14},
doi={10.1145/2543728.2543740},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897688579&doi=10.1145%2f2543728.2543740&partnerID=40&md5=c718b3a984516bbaf2bbbcf2bfc6e444},
abstract={Code generation is the leading approach to making high-performance software reusable. Effects are indispensable in code generators, whether to report failures or to insert let-statements and if- guards. Extensive painful experience shows that unrestricted effects interact with generated binders in undesirable ways to produce unexpectedly unbound variables, or worse, unexpectedly bound ones. These subtleties hinder domain experts in using and extending the generator. A pressing problem is thus to express the desired effects while regulating them so that the generated code is correct, or at least correctly scoped, by construction. We present a code-combinator framework that lets us express arbitrary monadic effects, including mutable references and delimited control, that move open code across generated binders. The static types of our generator expressions not only ensure that a well-typed generator produces well-typed and well-scoped code. They also express the lexical scopes of generated binders and prevent mixing up variables with different scopes. For the first time ever we demonstrate statically safe and well-scoped loop exchange and constant factoring from arbitrarily nested loops. Our framework is implemented as a Haskell library that embeds an extensible typed higher-order domain-specific language. It may be regarded as 'staged Haskell.' To become practical, the library relies on higher-order abstract syntax and polymorphism over generated type environments, and is written in the mature language. Categories and Subject Descriptors D.3.1 [Programming Languages]: Formal Definitions and Theory; D.3.3 [Programming Languages]: Language Constructs and Features-Control structures; polymorphism; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs-Type structure.},
author_keywords={Binders;  CPS;  Higher-order abstract syntax;  Lexical scope;  Multi-stage programming;  Mutable state and control effects},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2014,
title={2013 International Conference of Information Science and Management Engineering, ISME 2013, Volume 2},
journal={WIT Transactions on Information and Communication Technologies},
year={2014},
volume={46 VOLUME 2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888123352&partnerID=40&md5=0a2d92c5bf816567516c019e500d9bdf},
abstract={The proceedings contain 398 papers. The special focus in this conference is on Production and Operation Management, Logistics and Supply Chain Management, Advanced Manufacturing System, Management Information System, Computer Engineering and Technology, Computer Networks, Financial Engineering and Risk Management, Safety Management and Human Factors Engineering, Technological Management, Innovation and Evaluation, Optimisation Technology and Algorithm, Knowledge Management and Data Mining and Digital Manufacturing. The topics include: Research of power demand side management under smart grid environment; research of cost management mode of power transmission project and its evaluation; influence of investment fund on stock dividend policy; IPO accounting fraud, audit supervision, and media monitoring; heavy commercial vehicles sound quality database system design; a method by using Gaussian estimation for the semi-parametrical panel data models; agricultural monitoring system based on wireless sensor network; design of smart grid system based on cloud computing; detection of water quantity in leaves based on image processing technology; the research and applications of agricultural automation based on internet of things; application discussion of salary management during the human resource management; algorithm optimization of sentence similarity based on sematic disambiguation; research on onshore oil and gas pipeline management system based on the dynamic risk assessment; the risk assessment of supply chain financing based on core enterprise; the highway bridge head jumps off cause and prevention research; the new utility current power generation device design; medical risks and management countermeasures; teaching reform of welding robot based on integration of teaching, learning, and practice; an empirical study of consumer acceptance of e-commerce web site; the securities market month K line forecast based on SVC; evaluation and empirical analysis of innovation capability of electronic information industry; the performance evaluation of fund family based on the super-efficiency DEA model; exploring the impacts of customer orientation on college students' satisfaction; the application of research of storage method for civil aviation emergency domain ontology; study on the fitness model for regional technological innovation ecosystem based on niche theory; social network analysis on Chinese urban community; self-maintenance for service cooperation driven by contract-performing circumstance; the optimization of mining areas and ore dressing plants match in Yunnan tin regional mine; determine the degree of annihilators by using run-length property of pseudorandom sequences; distributed lag structure of knowledge production; an investigation of digital management for paper periodicals in university library; the integration of follow-up audit resources in the post-disaster reconstruction projects; a new method of XML-based information exchanging between databases; the adaptation of governance for venture capital firm in China; the model of advertising on social networking platform; economic value estimation-based pricing for software as a service; design of the intelligent internet of things for examination service; fast PSO algorithm for community detection in graph; a knowledge discovery method based on web information retrieval; a point symmetry clustering algorithm based on PSO; a robust image hiding approach for secure multimodal biometrics; wireless sensor networks data recovery based on zero-norm minimization; design of state feedback controller for discrete linear systems with time delay; computer forensics based on particle swarm optimization in cloud computing; a hierarchical federated integration framework for trust e-commerce cloud; research of outliers in time series of stock prices based on improved K-MEANS clustering algorithm; group finance company and expropriation of controlling shareholder; Mongolian word segmentation system based on unsupervised statistical mode; the role agent design based on the rules engine; computer application of finance analysis on the accounting management; design and application of iris recognition system based on neural network; study on developing data warehouse for guidance ammunition support; an empirical study on the structural of corporate social responsibility performance; feature points matching method of UAV images based on local RANSAC; research on the paths and trends of Chinese social management system; formalization and semantics computing of Chinese sentence type; a high embedding efficiency speech information hiding approach; response RS interpretation for disaster situation and secondary geologic hazards of Wenchuan earthquake; a new algorithm for weakening phase distortion in GPS receiver; research on the knowledge-based e-learning teaching application platform; Tate pairing algorithm resistant to fault attack over binary field; a study on the impact of supply chain management practice on enterprise performance; uniform access control platform of web service based on semantic message; the models of quality assurance in assembly process of complex mechanical products; study on the risk assessment of iron ore shipping channel in China; a new lifetime distribution with decreasing failure rate; decision support system for inventory policies of complex distribution systems; a new approach to building ontologies from heterogeneous databases; emotion monitoring and abnormal warning based on online comments; an improved multiple sparse representation classification approach; design and implementation of commodity information query system based on android and mobile internet; the method of network-based weighted voronoi diagram; research on acquisition algorithm based on prior information for weak signal; discussion of agile development used in large enterprise application system; research on influence maximization problem in dynamic networks; segmentation for brain MRI image based on the fuzzy c-means clustering algorithm; dynamic fault-tolerant routing based on link state in fat tree; analysis and realization on the image filter; research on network public expenditure; green economic development and management and the research on financing model of traffic safety facilities.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{NoAuthor2014,
title={2013 International Conference of Information Science and Management Engineering, ISME 2013, Volume 3},
journal={WIT Transactions on Information and Communication Technologies},
year={2014},
volume={46 VOLUME 3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888106429&partnerID=40&md5=0e198142036790a01e390972a5726ea1},
abstract={The proceedings contain 398 papers. The special focus in this conference is on Production and Operation Management, Logistics and Supply Chain Management, Advanced Manufacturing System, Management Information System, Computer Engineering and Technology, Computer Networks, Financial Engineering and Risk Management, Safety Management and Human Factors Engineering, Technological Management, Innovation and Evaluation, Optimisation Technology and Algorithm, Knowledge Management and Data Mining and Digital Manufacturing. The topics include: Research of power demand side management under smart grid environment; research of cost management mode of power transmission project and its evaluation; influence of investment fund on stock dividend policy; IPO accounting fraud, audit supervision, and media monitoring; heavy commercial vehicles sound quality database system design; a method by using Gaussian estimation for the semi-parametrical panel data models; agricultural monitoring system based on wireless sensor network; design of smart grid system based on cloud computing; detection of water quantity in leaves based on image processing technology; the research and applications of agricultural automation based on internet of things; application discussion of salary management during the human resource management; algorithm optimization of sentence similarity based on sematic disambiguation; research on onshore oil and gas pipeline management system based on the dynamic risk assessment; the risk assessment of supply chain financing based on core enterprise; the highway bridge head jumps off cause and prevention research; the new utility current power generation device design; medical risks and management countermeasures; teaching reform of welding robot based on integration of teaching, learning, and practice; an empirical study of consumer acceptance of e-commerce web site; the securities market month K line forecast based on SVC; evaluation and empirical analysis of innovation capability of electronic information industry; the performance evaluation of fund family based on the super-efficiency DEA model; exploring the impacts of customer orientation on college students' satisfaction; the application of research of storage method for civil aviation emergency domain ontology; study on the fitness model for regional technological innovation ecosystem based on niche theory; social network analysis on Chinese urban community; self-maintenance for service cooperation driven by contract-performing circumstance; the optimization of mining areas and ore dressing plants match in Yunnan tin regional mine; determine the degree of annihilators by using run-length property of pseudorandom sequences; distributed lag structure of knowledge production; an investigation of digital management for paper periodicals in university library; the integration of follow-up audit resources in the post-disaster reconstruction projects; a new method of XML-based information exchanging between databases; the adaptation of governance for venture capital firm in China; the model of advertising on social networking platform; economic value estimation-based pricing for software as a service; design of the intelligent internet of things for examination service; fast PSO algorithm for community detection in graph; a knowledge discovery method based on web information retrieval; a point symmetry clustering algorithm based on PSO; a robust image hiding approach for secure multimodal biometrics; wireless sensor networks data recovery based on zero-norm minimization; design of state feedback controller for discrete linear systems with time delay; computer forensics based on particle swarm optimization in cloud computing; a hierarchical federated integration framework for trust e-commerce cloud; research of outliers in time series of stock prices based on improved K-MEANS clustering algorithm; group finance company and expropriation of controlling shareholder; Mongolian word segmentation system based on unsupervised statistical mode; the role agent design based on the rules engine; computer application of finance analysis on the accounting management; design and application of iris recognition system based on neural network; study on developing data warehouse for guidance ammunition support; an empirical study on the structural of corporate social responsibility performance; feature points matching method of UAV images based on local RANSAC; research on the paths and trends of Chinese social management system; formalization and semantics computing of Chinese sentence type; a high embedding efficiency speech information hiding approach; response RS interpretation for disaster situation and secondary geologic hazards of Wenchuan earthquake; a new algorithm for weakening phase distortion in GPS receiver; research on the knowledge-based e-learning teaching application platform; Tate pairing algorithm resistant to fault attack over binary field; a study on the impact of supply chain management practice on enterprise performance; uniform access control platform of web service based on semantic message; the models of quality assurance in assembly process of complex mechanical products; study on the risk assessment of iron ore shipping channel in China; a new lifetime distribution with decreasing failure rate; decision support system for inventory policies of complex distribution systems; a new approach to building ontologies from heterogeneous databases; emotion monitoring and abnormal warning based on online comments; an improved multiple sparse representation classification approach; design and implementation of commodity information query system based on android and mobile internet; the method of network-based weighted voronoi diagram; research on acquisition algorithm based on prior information for weak signal; discussion of agile development used in large enterprise application system; research on influence maximization problem in dynamic networks; segmentation for brain MRI image based on the fuzzy c-means clustering algorithm; dynamic fault-tolerant routing based on link state in fat tree; analysis and realization on the image filter; research on network robot real-time communication strategy; method analysis and design on self-organizing network; identity-based proxy signcryption schemes; study on innovation integration based on technological growth; an algorithm for energy efficient placement of base station in wireless sensor networks; stabilized voltage design and magnetic circuit analysis on hybrid excitation generator for vehicle application; research on the colleges and universities accounting and enterprise accounting system; the research of data exchange platform based on web services; value-driven and bilateral resource integration oriented service information systems; design of wireless sensor networks nodes for hydraulic system pressure monitoring; method for test results analysis of e-learning based on rough set theory; a modified hexagon diamond search algorithm for fast motion estimation; RFID security authentication protocol based on key update mechanism; MFCC extraction in AAC domain for audio content analysis; a research on the training mode of applied financial talents based on the market demand; design and implementation of a component based on multi-bidirectional queue; discovery of direct and indirect temporal association patterns in large transaction databases; a lightweight timestamp-based method for data and China's development of low carbon economy; foreign trade development in Shandong province of Bohai economic circle; Color study and design of biological garments; sustainable development of holiday economy in China; research on the relationship between urbanization and urban public expenditure; green economic development and management and the research on financing model of traffic safety facilities.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Doröz2013955,
author={Doröz, Y. and Öztürk, E. and Sunar, B.},
title={Evaluating the hardware performance of a million-bit multiplier},
journal={Proceedings - 16th Euromicro Conference on Digital System Design, DSD 2013},
year={2013},
pages={955-962},
doi={10.1109/DSD.2013.108},
art_number={6628381},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890074079&doi=10.1109%2fDSD.2013.108&partnerID=40&md5=31c9a032497de9de06621afd75c8ecf4},
abstract={In this work we present the first full and complete evaluation of a very large multiplication scheme in custom hardware. We designed a novel architecture to realize a million-bit multiplication architecture based on the Schönhage-Strassen Algorithm and the Number Theoretical Transform (NTT). The construction makes use of an innovative cache architecture along with processing elements customized to match the computation and access patterns of the FFT-based recursive multiplication algorithm. When synthesized using a 90nm TSMC library operating at a frequency of 666 MHz, our architecture is able to compute the product of integers in excess of a million bits in 7.74 milliseconds. Estimates show that the performance of our design matches that of previously reported software implementations on a high-end 3 Ghz Intel Xeon processor, while requiring only a tiny fraction of the area. © 2013 IEEE.},
author_keywords={FFT;  Homomorphic encryption;  Large multiplier;  Number theoretical transform},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Raemaekers2013257,
author={Raemaekers, S. and Nane, G.F. and Van Deursen, A. and Visser, J.},
title={Testing principles, current practices, and effects of change localization},
journal={IEEE International Working Conference on Mining Software Repositories},
year={2013},
pages={257-266},
doi={10.1109/MSR.2013.6624037},
art_number={6624037},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889013469&doi=10.1109%2fMSR.2013.6624037&partnerID=40&md5=02e5676b3538bb281384680047f3037a},
abstract={Best practices in software development state that code that is likely to change should be encapsulated to localize possible modifications. In this paper, we investigate the application and effects of this design principle. We investigate the relationship between the stability, encapsulation and popularity of libraries on a dataset of 148,253 Java libraries. We find that bigger systems with more rework in existing methods have less stable interfaces and that bigger systems tend to encapsulate dependencies better. Additionally, there are a number of factors that are associated with change in library interfaces, such as rework in existing methods, system size, encapsulation of dependencies and the number of dependencies. We find that current encapsulation practices are not targeted at libraries that change the most. We also investigate the strength of ripple effects caused by instability of dependencies and we find that libraries cause ripple effects in systems using them and that these effects can be mitigated by encapsulation. © 2013 IEEE.},
author_keywords={Encapsulation;  Ripple effects;  Software libraries},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Novikova201323672,
author={Novikova, I.V. and Hennelly, S.P. and Sanbonmatsu, K.Y.},
title={Tackling structures of long noncoding RNAs},
journal={International Journal of Molecular Sciences},
year={2013},
volume={14},
number={12},
pages={23672-23684},
doi={10.3390/ijms141223672},
note={cited By 60},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889608134&doi=10.3390%2fijms141223672&partnerID=40&md5=a2c2dfb9e8b8b11af6db0dfdd8882d8f},
abstract={RNAs are important catalytic machines and regulators at every level of gene expression. A new class of RNAs has emerged called long non-coding RNAs, providing new insights into evolution, development and disease. Long non-coding RNAs (lncRNAs) predominantly found in higher eukaryotes, have been implicated in the regulation of transcription factors, chromatin-remodeling, hormone receptors and many other processes. The structural versatility of RNA allows it to perform various functions, ranging from precise protein recognition to catalysis and metabolite sensing. While major housekeeping RNA molecules have long been the focus of structural studies, lncRNAs remain the least characterized class, both structurally and functionally. Here, we review common methodologies used to tackle RNA structure, emphasizing their potential application to lncRNAs. When considering the complexity of lncRNAs and lack of knowledge of their structure, chemical probing appears to be an indispensable tool, with few restrictions in terms of size, quantity and heterogeneity of the RNA molecule. Probing is not constrained to in vitro analysis and can be adapted to high-throughput sequencing platforms. Significant efforts have been applied to develop new in vivo chemical probing reagents, new library construction protocols for sequencing platforms and improved RNA prediction software based on the experimental evidence. © 2013 by the authors; licensee MDPI, Basel, Switzerland.},
author_keywords={Chemical probing;  Epigenetics;  lncRNAs;  Long noncoding RNAs;  Secondary structure;  Shape},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Xie20134210,
author={Xie, K. and Wang, H. and Xiao, J.},
title={The value of IT-institution alignment: A managerial perspective of IT-business alignment},
journal={International Conference on Information Systems (ICIS 2013): Reshaping Society Through Information Systems Design},
year={2013},
volume={5},
pages={4210-4229},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897788166&partnerID=40&md5=f62dfa1aedfba7f526f0729faff3df48},
abstract={Present commercial software packages (such as ERP) have incorporated management insights, industry solutions, and best-practices to facilitate IT-business alignment. However, poor alignment of IT-business still exists on a large scale in practice. In this study, we investigate the notion of IT-business alignment from a managerial perspective and argue that, in addition to the four well-studied dimensions of ITbusiness alignment (i.e., knowledge, structure, society and culture), IT should also be aligned with institution to control opportunistic behaviors in administrative practices. Our study shows that: 1) IT exhibits a function of sensing to detect problems promptly, whereas institution exhibits a function of responding to solve problems effectively. From the perspective of governance, IT-business alignment refers to technology efficiency achieved by the coordination between IT and institution; 2) The alignment process undergoes three major stages: at the first stage, alignment is achieved between institution and IT but not between business process and IT; at the second stage, with the embedded management insights and best-practice solutions, IT integration succeeds in aligning with the business process but not with the original institution; at the third stage, the organization further develops integrated IT into customized IT and adapts the existing institution to achieve both IT-institution alignment and IT-business process alignment. A decline in the managerial performance of IT systems can be expected during this process; 3) It is not always wise to improve IT systems and revise original institution to pursue a higher level in IT-institution alignment. Practitioners should take the three-stage framework into account and fully examine the required effort before they start the process of IT-institution alignment. © (2013) by the AIS/ICIS Administrative Office All rights reserved.},
author_keywords={Governance;  Incomplete contract;  Institution;  IT-business alignment;  Private information},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dore2013473,
author={Dore, C. and Murphy, M.},
title={Semi-automatic techniques for as-built BIM façade modeling of historic buildings},
journal={Proceedings of the DigitalHeritage 2013 - Federating the 19th Int'l VSMM, 10th Eurographics GCH, and 2nd UNESCO Memory of the World Conferences, Plus Special Sessions fromCAA, Arqueologica 2.0 et al.},
year={2013},
volume={1},
pages={473-480},
doi={10.1109/DigitalHeritage.2013.6743786},
art_number={6743786},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896795452&doi=10.1109%2fDigitalHeritage.2013.6743786&partnerID=40&md5=cfda825ca0d1aba51bb23c7745857af5},
abstract={This paper presents two new developments for as-built BIM modeling of historical buildings. The first is a new library of interactive parametric objects designed for modeling classical architectural elements from survey data. These library objects are dynamic and have parameters that can alter the shape and size of objects for multiple uses and not just once off static modeling. The parametric architectural objects have been designed from historic manuscripts and architectural pattern books. These parametric objects were built using an embedded programming language within the ArchiCAD BIM software called Geometric Description Language (GDL). The second development which is described in more detail in this paper is a parametric building façade which has been developed as a template for fast and efficient modeling of endless configurations of building façades. The design of this parametric façade incorporates concepts from procedural modeling which is an automated approach to generating 3D geometries based on rules and algorithms. With this developed parametric façade, the structure of a façade can be automatically generated by altering parameters for the number of stories, number or horizontal tiles and door position. When automatically generating a façade, the initial position and size of elements are estimated using classical architectural proportions. After the façade is automatically generated users can then interactively edit the position, size and other parameters of façade elements to accurately map objects to survey data. Parametric library objects such as windows, ashlar block wall detail and other architectural elements are incorporated into the parametric façade. The parametric façade template has also been implemented with the Geometric Description Language for ArchiCAD BIM software. This enables the tools developed to utilize the full benefits of BIM software which includes automated construction or conservation documents, semantic object orientated objects based on IFC semantic classes, automatic lists of objects and material and the ability to add and link additional information to the model. Initial tests have shown that the parametric façade is more efficient than existing manual BIM methods for creating façade models from survey data. The façade template also provides an easier solution for generating façade models when compared to existing methods. Non-specialist users with little experience in 3D modeling can easily generate and modify the façade template by altering parameters graphically or from a dialogue box. © 2013 IEEE.},
author_keywords={Architectural Modeling;  As-Built BIM;  Cultural Heritage;  Historic Building Information Modeling;  Laser Scanning;  Parametric Modeling;  Photogrammetry;  Procedural Modeling},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Erwig201355,
author={Erwig, M. and Walkingshaw, E.},
title={Variation programming with the choice calculus},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7680 LNCS},
pages={55-100},
doi={10.1007/978-3-642-35992-7_2},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894117789&doi=10.1007%2f978-3-642-35992-7_2&partnerID=40&md5=567eceed83c02f21cbc253d0541216c6},
abstract={The choice calculus provides a language for representing and transforming variation in software and other structured documents. Variability is captured in localized choices between alternatives. The space of all variations is organized by dimensions, which provide scoping and structure to choices. The variation space can be reduced through a process of selection, which eliminates a dimension and resolves all of its associated choices by replacing each with one of their alternatives. The choice calculus also allows the definition of arbitrary functions for the flexible construction and transformation of all kinds of variation structures. In this tutorial we will first present the motivation, general ideas, and principles that underlie the choice calculus. This is followed by a closer look at the semantics. We will then present practical applications based on several small example scenarios and consider the concepts of "variation programming" and "variation querying". The practical applications involve work with a Haskell library that supports variation programming and experimentation with the choice calculus. © Springer-Verlag Berlin Heidelberg 2013.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chang20131219,
author={Chang, S.E. and Lee, P.-F.},
title={Leveraging social network APIs for enhancing smartphone apps: An example of VoIP app},
journal={Proceedings - 2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing, GreenCom-iThings-CPSCom 2013},
year={2013},
pages={1219-1224},
doi={10.1109/GreenCom-iThings-CPSCom.2013.212},
art_number={6682225},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893515053&doi=10.1109%2fGreenCom-iThings-CPSCom.2013.212&partnerID=40&md5=5a0dee7c22cfae66ddd6565459e8ee53},
abstract={With the rapid advancement of communication technology and mobile devices, many software companies and individual developers have started to develop a variety of mobile applications. However, since the resources individual developers can get are far less than a software company, it is important for individual developers to utilize the free API resources available on the Internet to harvest the benefit of the cloud computing. This research explores the benefits that individual developers can acquire from free APIs and service-oriented computing (SOC) in building desired smartphone applications (apps). Using a VoIP application (app) as an example, the design and implementation of a smartphone app using the proposed approach is described in this article. In addition, by comparing this VoIP app with some other smartphone apps which also provide VoIP services, the advantages and potential value of our approach to integrating social network APIs and SOC concept will be discussed. © 2013 IEEE.},
author_keywords={Mobile device;  Smartphone application;  Social network API;  Social-oriented computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sung2013,
author={Sung, Y.C. and Sung, J.-F.},
title={Short message service for internet-mobile platform},
journal={15th Asia-Pacific Network Operations and Management Symposium: "Integrated Management of Network Virtualization", APNOMS 2013},
year={2013},
art_number={6665255},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893487900&partnerID=40&md5=d37f15e4954876a5cd5a385684f89dcd},
abstract={In 2003, the Parlay group proposed a concept and technique of the web service and proposed the other suite of Open Services Architecture (OSA) specifications, Parlay X. The Parlay X Application Programming Interface (API) defines a set of easy-to-use API, which utilizes the web service for application developer to access telecommunications functions more easily. In this paper, we use Parlay X-based Internet-Mobile platform called IBM WebSphere software for Telecom (WsT). To accommodate the telecom services (i.e., Short Message Service (SMS), Multimedia Messaging Service (MMS)) on IBM WsT, a set of API named OpenAPI is implemented. The paper studies the SMS behaviors and investigates the OpenAPI SMS performance. To improve the OpenAPI SMS delivery delay, a timeout timer is implemented. Our research also predicts SMS delivery failure and provides guidelines to select appropriate timeout timer values for OpenAPI SMS. © 2013 IEICE.},
author_keywords={IBM WebSphere software for Telecom (WsT);  Open Services Architecture (OSA);  Parlay X;  Short Message Service (SMS)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Abdeen2013212,
author={Abdeen, H. and Sahraoui, H. and Shata, O. and Anquetil, N. and Ducasse, S.},
title={Towards automatically improving package structure while respecting original design decisions},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2013},
pages={212-221},
doi={10.1109/WCRE.2013.6671296},
art_number={06671296},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893418696&doi=10.1109%2fWCRE.2013.6671296&partnerID=40&md5=9b189a31617a8031e9d557fa6c89c582},
abstract={Recently, there has been an important progress in applying search-based optimization techniques to the problem of software re-modularization. Yet, a major part of the existing body of work addresses the problem of modularizing software systems from scratch, regardless of the existing packages structure. This paper presents a novel multi-objective optimization approach for improving existing packages structure. The optimization approach aims at increasing the cohesion and reducing the coupling and cyclic connectivity of packages, by modifying as less as possible the existing packages organization. Moreover, maintainers can specify several constraints to guide the optimization process with regard to extra design factors. To this contribution, we use the Non-Dominated Sorting Genetic Algorithm (NSGA-II). We evaluate the optimization approach through an experiment covering four real-world software systems. The results promise the effectiveness of our optimization approach for improving existing packages structure by doing very small modifications. © 2013 IEEE.},
author_keywords={Cohesion and Coupling Principles;  Multi-Objective Optimization;  Software Modularization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2013,
title={Proceedings - 20th Working Conference on Reverse Engineering, WCRE 2013},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2013},
page_count={504},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893356182&partnerID=40&md5=fb654d54b5318c585ce1f87eea9c2139},
abstract={The proceedings contain 61 papers. The topics discussed include: who allocated my memory? detecting custom memory allocators in C binaries; MemPick: high-level data structure detection in C/C++ binaries; reconstructing program memory state from multi-gigabyte instruction traces to support interactive analysis; static binary rewriting without supplemental information: overcoming the tradeoff between coverage and correctness; an incremental update framework for efficient retrieval from software libraries for bug localization; accurate developer recommendation for bug resolution; automatic recovery of root causes from bug-fixing changes; distilling useful clones by contextual differencing; effects of cloned code on software maintainability: a replicated developer study; the influence of non-technical factors on code review; towards understanding how developers spend their effort during maintenance activities; and leveraging specifications of subcomponents to mine precise specifications of composite components.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Jin20133107,
author={Jin, C.-C. and Zuo, H.-F.},
title={Design and implementation of airline's maintenance quality management system},
journal={Jisuanji Jicheng Zhizao Xitong/Computer Integrated Manufacturing Systems, CIMS},
year={2013},
volume={19},
number={12},
pages={3107-3113},
doi={10.13196/j.cims.2013.12.jincancan.3107.7.20131222},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892866263&doi=10.13196%2fj.cims.2013.12.jincancan.3107.7.20131222&partnerID=40&md5=09b192a26a7de44b09d1a3bcd6fafa4b},
abstract={To monitor the airline's safety status and maintenance quality in real-time, a complete maintenance quality management system which included security situation assessment, trend analysis early-warning and correction feedback measures was developed. A comprehensive and effective evaluation index system was built, and the quantitative method for qualitative index was introduced. Java class library of Netica-J Application Programming Interface (API) software was used to complete comprehensive assessment based on Bayesian networks. The responsible department was searched through trend analysis module, and the preventive and corrective actions were obtained by using reporting module obtain. The system architecture and function modules were introduced. Application examples showed that the safety and quality management of the airlines maintenance system was more specific, standardization and indexing, and the airline maintenance efficiency to improve safety management level was also promoted.},
author_keywords={Bayesian networks;  Engineering maintenance;  Netica software;  Safety evaluation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gorringe2013176,
author={Gorringe, C. and Brown, M.},
title={Recommendations and best practices for creating reusable test signal framework definitions},
journal={AUTOTESTCON (Proceedings)},
year={2013},
pages={176-184},
doi={10.1109/AUTEST.2013.6645063},
art_number={6645063},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891362067&doi=10.1109%2fAUTEST.2013.6645063&partnerID=40&md5=1dcfe72f9ca062d9b2bef06042cbe310},
abstract={Critical to the successful implementation of Open System Architecture (OSA) test software, in accordance with the requirements of IEEE 1641[1][2], is the implementation Signal Libraries using the Test Signal Frameworks (TSFs) that provide the test program set (TPS) Developer with the suite of defined signals to be used for the test software. This paper aims to provide insight for the TPS developers and guidance on the best practices to be followed when creating and reusing Signal Libraries. It also considers issues to be kept in mind when bringing together a collection of existing Signal Libraries to be used for testing, and references examples that are available for future developers. The paper considers research into the effects of different methods that could be used for matching the TSFs used in describing UUT test requirements and instrument capabilities. This includes evaluation of their relative merits using a variety of "use cases" and example TSFs. Typical User questions are also considered along with their answers. The example TSFs referenced within the paper represent a collection of Signal Libraries available used in previous demonstrations and brought up to the IEEE Std. 1641-2010 standard. These include Signal Libraries provided as part of 1641 demonstrations and previous reports and form the basis of an OSA Signal Library. The technical analysis considers both symbolic signal expression and signal simulation, to address issues such as signal equivalence, quantization, phase jitter and overall performance. The purpose of the paper being to produce material for inclusion into recommendations and guidelines that will ensure best practice can be followed when generating Signal libraries for a wide range of signal types. This will ensure that Signal libraries are fit for purpose, and achieve the desired level of quality necessary for future matching UUT test requirements against the instrument capabilities. In addition, this will ensure that the UUT requirements are NOT over specified against the instrument capability used to initially implement the test solution. To ensure best practice across OSA RTS users, formal guidelines are required for the process of creating and defining IEEE 1641 TSFs in order to minimize differences between signal libraries and any effects that become apparent when implemented on different ATEs, using different instrumentation. The paper considers guidelines which could form the basis of a recommended practice to ensure standardization across the 1641 user base for use on Military or Commercial Automatic Test Systems (ATS). © 2013 IEEE.},
author_keywords={ATML;  IEEE Std 1641;  IEEE Std 1671;  modelling;  Signal Library;  simulation;  Test Signal Framework;  TSF},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Falessi201388,
author={Falessi, D. and Shull, F.},
title={Towards flexible automated support to improve the quality of computational science and engineering software},
journal={2013 5th International Workshop on Software Engineering for Computational Science and Engineering, SE-CSE 2013 - Proceedings},
year={2013},
pages={88-91},
doi={10.1109/SECSE.2013.6615104},
art_number={6615104},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886699207&doi=10.1109%2fSECSE.2013.6615104&partnerID=40&md5=a9f934e3d369c2e67f2966ca53cddc01},
abstract={Continual evolution of the available hardware (e.g. in terms of increasing size, architecture, and computing power) and software (e.g. reusable libraries) is the norm rather than exception. Our goal is to enable CSE developers to spend more of their time finding scientific results by capitalizing on these evolutions instead of being stuck in fixing software engineering (SE) problems such as porting the application to new hardware, debugging, reusing (unreliable) code, and integrating open source libraries. In this paper we sketch a flexible automated solution supporting scientists and engineers in developing accurate and reliable CSE applications. This solution, by collecting and analyzing product and process metrics, enables the application of well-established software engineering best practices (e.g., separation of concerns, regression testing and inspections) and it is based upon the principles of automation, flexibility and iteration. © 2013 IEEE.},
author_keywords={Computational science and engineering software;  empirical software engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tulvan20131239,
author={Tulvan, C. and Preda, M.},
title={3D graphics coding in a reconfigurable environment},
journal={Signal Processing: Image Communication},
year={2013},
volume={28},
number={10},
pages={1239-1254},
doi={10.1016/j.image.2013.08.010},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888204443&doi=10.1016%2fj.image.2013.08.010&partnerID=40&md5=f812e516d7a708ee226d049e3ec1962f},
abstract={The main objective of this paper is to introduce the concept of Reconfigurable Graphic Coding and its validation under the form of a Functional Units (FU) library. The heterogeneity of data for 3D graphics objects representation requires the adaptability of the compression schemas to various types of content. While such adaptation can be relatively easy to support in software implementations, the same is much more difficult to implement in hardware. Although compression schemas inherently share the same data processing chain, the components forming it may vary with respect to the number and type of components to encode, data range and correlation type. Based on the analysis of the state of the art on 3D graphics compression approaches, we propose a set of processing units. We show how this set can be configured/connected into a network, including hardware networks, to obtain reference decoders. Moreover, the network can be reconfigured at runtime, based on information that is provided with the encoded object. This modular concept of functional units, allows optimized management of computation (such as identification of parallelizable functions or functions that are suitable for acceleration) relative to the hardware architecture (CPU, GPU, FPGA, etc.). In the four decoders presented, at least half of the FUs are being reused at least once. The results were performed by generating and compiling C code from RVC-CAL code and comparing the results with the MPEG reference software implementation. The FUs described in this paper were standardized by MPEG as part of the ISO/IEC 23001-4. © 2013 Elsevier B.V.},
author_keywords={3D graphics;  Compression;  Dataflow process network;  Media tool library;  MPEG reconfigurable media coding;  Reconfigurable decoders},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chong20131994,
author={Chong, C.Y. and Lee, S.P. and Ling, T.C.},
title={Efficient software clustering technique using an adaptive and preventive dendrogram cutting approach},
journal={Information and Software Technology},
year={2013},
volume={55},
number={11},
pages={1994-2012},
doi={10.1016/j.infsof.2013.07.002},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884146788&doi=10.1016%2fj.infsof.2013.07.002&partnerID=40&md5=0111d4a2e3f9bc7bf38051219657a7fd},
abstract={Context Software clustering is a key technique that is used in reverse engineering to recover a high-level abstraction of the software in the case of limited resources. Very limited research has explicitly discussed the problem of finding the optimum set of clusters in the design and how to penalize for the formation of singleton clusters during clustering. Objective This paper attempts to enhance the existing agglomerative clustering algorithms by introducing a complementary mechanism. To solve the architecture recovery problem, the proposed approach focuses on minimizing redundant effort and penalizing for the formation of singleton clusters during clustering while maintaining the integrity of the results. Method An automated solution for cutting a dendrogram that is based on least-squares regression is presented in order to find the best cut level. A dendrogram is a tree diagram that shows the taxonomic relationships of clusters of software entities. Moreover, a factor to penalize clusters that will form singletons is introduced in this paper. Simulations were performed on two open-source projects. The proposed approach was compared against the exhaustive and highest gap dendrogram cutting methods, as well as two well-known cluster validity indices, namely, Dunn's index and the Davies-Bouldin index. Results When comparing our clustering results against the original package diagram, our approach achieved an average accuracy rate of 90.07% from two simulations after the utility classes were removed. The utility classes in the source code affect the accuracy of the software clustering, owing to its omnipresent behavior. The proposed approach also successfully penalized the formation of singleton clusters during clustering. Conclusion The evaluation indicates that the proposed approach can enhance the quality of the clustering results by guiding software maintainers through the cutting point selection process. The proposed approach can be used as a complementary mechanism to improve the effectiveness of existing clustering algorithms. © 2013 Elsevier B.V. All rights reserved.},
author_keywords={Design recovery;  Remodularization;  Software clustering;  Software maintenance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Teixeira20132511,
author={Teixeira, A.L. and Falcao, A.O.},
title={Noncontiguous atom matching structural similarity function},
journal={Journal of Chemical Information and Modeling},
year={2013},
volume={53},
number={10},
pages={2511-2524},
doi={10.1021/ci400324u},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887067689&doi=10.1021%2fci400324u&partnerID=40&md5=3a4c9da5bdbe6923344c856a37b67dac},
abstract={Measuring similarity between molecules is a fundamental problem in cheminformatics. Given that similar molecules tend to have similar physical, chemical, and biological properties, the notion of molecular similarity plays an important role in the exploration of molecular data sets, query-retrieval in molecular databases, and in structure-property/activity modeling. Various methods to define structural similarity between molecules are available in the literature, but so far none has been used with consistent and reliable results for all situations. We propose a new similarity method based on atom alignment for the analysis of structural similarity between molecules. This method is based on the comparison of the bonding profiles of atoms on comparable molecules, including features that are seldom found in other structural or graph matching approaches like chirality or double bond stereoisomerism. The similarity measure is then defined on the annotated molecular graph, based on an iterative directed graph similarity procedure and optimal atom alignment between atoms using a pairwise matching algorithm. With the proposed approach the similarities detected are more intuitively understood because similar atoms in the molecules are explicitly shown. This noncontiguous atom matching structural similarity method (NAMS) was tested and compared with one of the most widely used similarity methods (fingerprint-based similarity) using three difficult data sets with different characteristics. Despite having a higher computational cost, the method performed well being able to distinguish either different or very similar hydrocarbons that were indistinguishable using a fingerprint-based approach. NAMS also verified the similarity principle using a data set of structurally similar steroids with differences in the binding affinity to the corticosteroid binding globulin receptor by showing that pairs of steroids with a high degree of similarity (>80%) tend to have smaller differences in the absolute value of binding activity. Using a highly diverse set of compounds with information about the monoamine oxidase inhibition level, the method was also able to recover a significantly higher average fraction of active compounds when the seed is active for different cutoff threshold values of similarity. Particularly, for the cutoff threshold values of 86%, 93%, and 96.5%, NAMS was able to recover a fraction of actives of 0.57, 0.63, and 0.83, respectively, while the fingerprint-based approach was able to recover a fraction of actives of 0.41, 0.40, and 0.39, respectively. NAMS is made available freely for the whole community in a simple Web based tool as well as the Python source code at http://nams.lasige.di.fc.ul.pt/. © 2013 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2013296,
author={Li, X. and Lu, C. and Sjogren, J.A.},
title={Parallel implementation of exact matrix computation using multiple P-adic arithmetic},
journal={SNPD 2013 - 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing},
year={2013},
pages={296-302},
doi={10.1109/SNPD.2013.78},
art_number={6598480},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886011127&doi=10.1109%2fSNPD.2013.78&partnerID=40&md5=ed6491ea11355e70ccc7a07959062fd5},
abstract={A P-adic Exact Scientific Computational Library (ESCL) for rational matrix operations has been developed over the past few years. The effort has been focusing on converting all rational number operations to integer calculation, and fully taking advantage of the fast integer multiplication of modern computer architectures. In this paper, we report our progress on parallel implementation of P-adic arithmetic by means of a multiple modulus rational system related to the Chinese remainder theorem. Experimental results are given to illustrate computational efficiency. © 2013 IEEE.},
author_keywords={Chinese remainder theorem;  Computational efficiency;  Error-free;  Multiple modulus rational system;  P-adic;  Parallel computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Huang2013123,
author={Huang, W.-C. and Knottenbelt, W.J.},
title={Self-adaptive containers: Building resource-efficient applications with low programmer overhead},
journal={ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems},
year={2013},
pages={123-132},
doi={10.1109/SEAMS.2013.6595499},
art_number={6595499},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884994906&doi=10.1109%2fSEAMS.2013.6595499&partnerID=40&md5=753774a84e6c2a04e4d6a6ef02d9dd12},
abstract={Despite advances in operating system resource management and the availability of standardised container libraries, developing scalable high-capacity applications remains a nontrivial endeavour. Naïve implementations of fundamental algorithms often rapidly exhaust system resources under heavy load. Resolving this via manual refactoring is usually possible but requires significant programmer effort, an effort which often has to be repeated in order to meet the resource constraints encountered in each different execution environment. This paper proposes a library of self-adaptive containers which provide a ready route to developing scalable applications with low programmer overhead. Given an execution environment, the library flexibly adapts its use of data structures in an effort to meet programmer-specified service level objectives. The library features a mechanism for tighter functionality specification than that provided by standard container libraries. This enables greater scope for efficiency optimisations, including the exploitation of probabilistic data structures and out-of-core storage. We have demonstrated the capabilities of the proposed library through a prototype implementation in C++. We show that when a Breadth First Search explicit state space exploration algorithm is executed, using the proposed library reduces insertion time by 68.5%, search time by 86.1%, and primary memory usage by 90.1% compared with the Standard Template Library. © 2013 IEEE.},
author_keywords={Containers;  Probabilistic Data Structures;  Self-Adaptive Systems;  Standard Template Library},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Amollo2013608,
author={Amollo, B.A.},
title={Feasibility of adaptation of open source ILS for libraries in Kenya: A practical evaluation},
journal={Electronic Library},
year={2013},
volume={31},
number={5},
pages={608-634},
doi={10.1108/EL-12-2011-0171},
art_number={17097653},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884895552&doi=10.1108%2fEL-12-2011-0171&partnerID=40&md5=c1e2789ac23328a3279880ade17448b3},
abstract={Purpose - Despite its fast growth and penetration in all sectors, it has been noted that open source software (OSS) is yet to find its optimal place in libraries, particularly libraries in the developing countries. Lack of documented information on the experiences and use of open source integrated library system (ILS) is a major drawback, and so the need for this study. The proposed study aims to help to investigate and test usability and cost effectiveness of a typical OSS for ILS. It will involve deploying the software from installation, configuration to creating customized user interfaces and structures that are specific to the requirements of the library's parent organization. The cost and performance of the OSS will then be compared with that of a typical commercial based software with the same functionalities. Design/methodology/approach - A preliminary study has been conducted to collect data from libraries in the country through distribution of questionnaires to provide data for accurate analysis that will form the basis for recommendations. The target group includes library and IT personnel in the various institutions and the end-users within sample group. A case study is proposed to help establish OSS effectiveness in libraries. To test a typical OSS, parameters are to be drawn from two models - open source maturity model and business readiness rating. Findings - A casual observation of the Kenyan situation reveals that the majority of academic, public and research libraries depend on commercial, free or locally developed systems. This scenario may be attributed to lack of knowledge (or interest) in OSS alternatives and lack of sufficient technical expertise to support them. While there are quite a number of libraries and librarians worldwide that have shown a great interest in OSS, few library administrators have actually implemented OSS. Could this be due to fear of taking on the risks that may come with reliance on open source library automation systems? Is the low uptake due to lack of sufficient technical expertise in the libraries? The research outcomes will help formulate a model and guidelines to be used by systems librarians considering the use of OSS for library processes. Factors to be considered when deciding on OSS will be outlined. Research limitations/implications - This paper is of importance to library personnel in Kenya as it establishes the effectiveness of OSS, with the aim of empowering the library staff who have for a long time relied on their IT departments and vendors for systems installation and implementation. Originality/value - The study will result in a comprehensive evaluation of the economic and functional advantages of OSS as an alternative for the library in Kenya. Librarians involved in selection of software for their libraries will find this helpful when deciding on the type of software to select for their libraries. It will help to enlighten library professional about the value of OSS and how they can participate in the development of their own systems, instead of always relying on vendors. Copyright © 2013 Emerald Group Publishing Limited.},
author_keywords={Information systems;  Integrated software;  Kenya;  Libraries;  Library automation;  Library systems;  Open systems;  Research},
document_type={Article},
source={Scopus},
}

@ARTICLE{Daneshgar20131741,
author={Daneshgar, F. and Low, G.C. and Worasinchai, L.},
title={An investigation of 'build vs. buy' decision for software acquisition by small to medium enterprises},
journal={Information and Software Technology},
year={2013},
volume={55},
number={10},
pages={1741-1750},
doi={10.1016/j.infsof.2013.03.009},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880830526&doi=10.1016%2fj.infsof.2013.03.009&partnerID=40&md5=2d97a2587ee8f10c517deffa6fe3b0c8},
abstract={Context The prevalence of computing and communication technologies, combined with the availability of sophisticated and highly specialised software packages from software vendors has made package acquisition a viable option for many organisations. While some research has addressed the factors that influence the selection of the software acquisition method in large organisations, little is known about the factors affecting SMEs. Objective To provide an understanding of factors that affect the decision process of software acquisition for SMEs. It is expected that results from this study: (i) will assist the SME decision process for software acquisition, and (ii) will assist policy makers in terms of developing appropriate guidelines for SME software acquisition. Method A positivist research perspective has been adopted involving semi-structured interviews in eight SMEs in Thailand with the interviewees assigning to each of the potential factors. Results The study found that the following factors affect both SMEs and large organisations: requirements fit, cost, scale and complexity, commoditization/flexibility, time, in-house experts, support structure, and operational factors. Factors mainly applying to large organisations were strategic role of the software, intellectual property concerns, and risk, Factors particularly relevant to SMEs (ubiquitous systems, availability of free download, and customizable to specific government/tax regulations). Conclusion The results suggest that: (i) when deciding on their software acquisition method, SMEs are generally less likely to pursue a long-term vision compared with larger organisations, possibly because SMEs mainly serve their local markets; and (ii) contrary to the large organisations, the role that the IT plays in SMEs may not be as vital to the SMEs' core business processes, to their supply chains, and/or to the management of their customer relationship. Furthermore, neither the level of technological intensity nor size of the SME appears to affect the ranks given by the interviewees for the various factors. © 2013 Elsevier B.V. All rights reserved.},
author_keywords={Buy vs. build;  In-house development;  SME;  Software acquisition;  Software package;  Thailand},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hoque2013988,
author={Hoque, A.S.M. and Halder, P.K. and Parvez, M.S. and Szecsi, T.},
title={Integrated manufacturing features and Design-for-manufacture guidelines for reducing product cost under CAD/CAM environment},
journal={Computers and Industrial Engineering},
year={2013},
volume={66},
number={4},
pages={988-1003},
doi={10.1016/j.cie.2013.08.016},
note={cited By 60},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888301970&doi=10.1016%2fj.cie.2013.08.016&partnerID=40&md5=d8b35c83afcb2b86323fa4bb03cdd44b},
abstract={The main contribution of the work is to develop an intelligent system for manufacturing features in the area of CAD/CAM. It brings the design and manufacturing phase together in design stage and provides an intelligent interface between design and manufacturing data by developing a library of features. The library is called manufacturing feature library which is linked with commercial CAD/CAM software package named Creo Elements/Pro by toolkit. Inside the library, manufacturing features are organised hierarchically. A systematic database system also have been developed and analysed for each feature consists of parameterised geometry, manufacturing information (including machine tool, cutting tools, cutting conditions, cutting fluids and recommended tolerances and surface finishing values, etc.), design limitations, functionality guidelines, and Design-for-manufacture guidelines. The approach has been applied in two case studies in which a rotational part (shaft) and a non-rotational part are designed through manufacturing features. Therefore, from manufacturing feature library a design can compose entirely in a bottom-up manner using manufacturable entities in the same way as they would be produced during the manufacturing phase. Upon insertion of a feature, the system ensures that no functionality or manufacturing guidelines are violated. The designers are warned if they attempt to include features that violate Design-for- manufacture and Design functionality guidelines. If a feature is modified, the system validates the feature by making sure that it remains consistent with its original functionality and Design-for-manufacture guidelines are re-applied. The system will be helped the process planner/manufacturing engineer by automatically creating work-piece data structure. © 2013 Elsevier Ltd. All rights reserved.},
author_keywords={Design functionality;  Design-for-manufacture;  Manufacturing feature library},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cook2013,
author={Cook, R.P.},
title={An OpenMP library for Java},
journal={Conference Proceedings - IEEE SOUTHEASTCON},
year={2013},
doi={10.1109/SECON.2013.6567466},
art_number={6567466},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883244066&doi=10.1109%2fSECON.2013.6567466&partnerID=40&md5=7ca1267f66925e69e2daf5a9529e4e08},
abstract={The design and implementation of an OpenMP library for Java is described. The library not only encodes OpenMP semantics as open source but also provides a simple platform to enable further experimentation. The Java OpenMP library design was tested against OpenMP C programs using Visual Studio 2010 and gcc. © 2013 IEEE.},
author_keywords={Java;  library;  OpenMP;  software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sánchez-Bojorge20133537,
author={Sánchez-Bojorge, N.A. and Rodríguez-Valdez, L.M. and Flores-Holguín, N.},
title={DFT calculation of the electronic properties of fluorene-1,3,4-thiadiazole oligomers},
journal={Journal of Molecular Modeling},
year={2013},
volume={19},
number={9},
pages={3537-3542},
doi={10.1007/s00894-013-1878-9},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882997213&doi=10.1007%2fs00894-013-1878-9&partnerID=40&md5=c021c68dc8004e4f6b0aeb77d8adf570},
abstract={Thiadiazole derivatives have been widely employed in the areas of pharmaceutical, agricultural, industrial, and polymer chemistry. The electronic and molecular structures of thiadiazoles are of interest because they have an equal number of valence electrons and similar molecular structures to thiophenes, which are currently used in the construction of organic solar cells due to their relatively high hole mobilities and good light-harvesting properties. For this reason, the electronic properties of fluorene-1,3,4- thiadiazole oligomers warrant investigation. In the present work, the structure of fluorene-1,3,4-thiadiazole with one thiadiazole unit in the structure was analyzed. This molecule was then expanded until there were 10 thiadiazole units in the structure. The band gap, HOMO and LUMO distributions, and absorption spectrum were analyzed for each molecule. All calculations were performed by applying the B3LYP/6-31G(d) chemical model in the Gaussian 03W and GaussView software packages. The electronic properties were observed to significantly enhance as the number of monomeric units increased, which also caused the gap energy to decrease from 3.51 eV in the oligomer with just one thiadiazole ring to 2.33 eV in the oligomer with 10 units. The HOMO and LUMO regions were well defined and separated for oligomers with at least 5 monomer units of thiadiazole. [Figure not available: see fulltext.] © 2013 Springer-Verlag Berlin Heidelberg.},
author_keywords={DFT;  Photovoltaic;  Thiadiazole},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Palma20138,
author={Palma, R. and Hołubowicz, P. and Page, K. and Corcho, O. and Pérez, S. and Mazurek, C.},
title={Digital libraries for the preservation of research methods and associated artifacts},
journal={ACM International Conference Proceeding Series},
year={2013},
pages={8-15},
doi={10.1145/2499583.2499589},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882382288&doi=10.1145%2f2499583.2499589&partnerID=40&md5=1725660aa409a30fb43f82212df0ea06},
abstract={New digital artifacts are emerging in data-intensive science. For example, scientific workflows are executable descriptions of scientific procedures that define the sequence of computational steps in an automated data analysis, supporting reproducible research and the sharing and replication of best-practice and know-how through reuse. Workflows are specified at design time and interpreted through their execution in a variety of situations, environments, and domains. Hence it is essential to preserve both their static and dynamic aspects, along with the research context in which they are used. To achieve this, we propose the use of multidimensional digital objects (Research Objects) that aggregate the resources used and/or produced in scientific investigations, including workflow models, provenance of their executions, and links to the relevant associated resources, along with the provision of technological support for their preservation and efficient retrieval and reuse. In this direction, we specified a software architecture for the design and implementation of a Research Object preservation system, and realized this architecture with a set of services and clients, drawing together practices in digital libraries, preservation systems, workflow management, social networking and Semantic Web technologies. In this paper, we describe the backbone system of this realization, a digital library system built on top of dLibra.},
author_keywords={Evolution;  Libraries;  Preservation;  Semantic},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sellers2013,
author={Sellers, G. and Van Waveren, J.M.P. and Cozzi, P. and Ring, K. and Persson, E. and De Vahl, J.},
title={Rendering massive virtual worlds},
journal={ACM SIGGRAPH 2013 Courses, SIGGRAPH 2013},
year={2013},
doi={10.1145/2504435.2504458},
art_number={23},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882331091&doi=10.1145%2f2504435.2504458&partnerID=40&md5=ff3322e179a821fe6ea3f7d1a92a7aca},
abstract={In recent years, connectivity of systems has meant that online worlds with huge streaming data sets have become common and widely available. From applications such as map rendering and virtual globes to online gaming, users expect online content to be presented in a timely and seamless manner, and expect the volume and variety of offline content to match that which is available online. Generating, retrieving and displaying this content to users presents a number of considerable challenges. This course addresses some real-world solutions to the problems presented in the rendering of massive virtual worlds. Massive worlds present many daunting challenges. In our first talk, we introduce the two most prominent challenges: data management and rendering artifacts. We look at handling massive datasets like real-world terrain with out-of-core rendering, including parallelism, cache hierarchies on the client and in the cloud, and level-of-detail. Then we explore handling jittering artifacts due to performing vertex transform on large world coordinates with 32-bit precision on commodity GPUs, and handling z-fighting artifacts due to lack of depth buffer precision with large near-to-far distances. This serves as an introduction to the following talk, World-Scale Terrain Rendering. Rendering small and medium scale terrain is fairly straightforward these days, but rendering terrain for a detailed world the size of Earth is much more challenging. In our second talk, we discuss the design and implementation of a terrain engine that can render a zoomedout view of the entire globe all the way down to a zoomed-in view where sub-meter details are visible. We discuss processing "off the shelf" terrain data for efficient streaming and rendering, an asynchronous load pipeline for bringing chunks of terrain through a cache hierarchy, efficiently culling chunks of terrain that are below the horizon, driving terrain level-of-detail selection based on an estimate of pixel error, and more. With these techniques, we are able to achieve excellent performance even in the constrained environment of WebGL and JavaScript running inside a web browser. Once we have dealt with the topic of storing and streaming huge amounts of content, we must next contend with production and generation of that content. The next talk will go through the production pipeline at Avalanche Studios and the issues encountered when filling Just Cause 2 with interesting content. It is mix of horror stories, good practices, and lessons learned and applied to titles currently in production. Issues discussed will include the reliability problems for the content tool-chain, the long turn-around times we had, undocumented and poorly understood data dependencies, and all the problems that followed with these. Then we will cover how we have solved these problems for our current content pipeline. We will also talk about our approach to authoring the landscape, vegetation and locations for our large game worlds, and how we maintain productivity without sacrificing variation. Many of the concepts discussed to this point address efficient generation, storage, retrieval and transmission of content. Recent advances in graphics hardware allow GPUs to assist in functions such as streaming texture data, managing sparse data sets and providing reasonable visual results in cases where not all of the data is available to render a scene. In the next talk, we take a deep-dive into AMD's partially resident texture hardware, briefly cover sparse texture extensions for OpenGL and then explore some use cases for the hardware and software features, including some live demos. In our final presentation, we discuss the practical challenges of integrating support for hardware virtual texturing into a real-world game engine, idTech5, which powers RAGE and a number of other titles. We will describe cases where hardware virtual texturing 'just worked', and cases where more effort was required to integrate the technology into an existing engine, whilst maintaining support for software virtual texturing without loss of performance or features. We assume that course participants are familiar with modern graphics rendering techniques, data compression, cache hierarchies and graphics hardware acceleration. We will discuss in some detail virtual memory systems, culling techniques, level-of-detail selection and other related techniques.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fang201388,
author={Fang, Z. and Xiang, B. and Xiao, H. and He, K. and Liu, J.},
title={Dynamic and control strategy of launch and recovery device in harsh sea conditions},
journal={Jixie Gongcheng Xuebao/Journal of Mechanical Engineering},
year={2013},
volume={49},
number={15},
pages={88-95},
doi={10.3901/JME.2013.15.088},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884165919&doi=10.3901%2fJME.2013.15.088&partnerID=40&md5=379e33339ed9039596a55def20cd3bf0},
abstract={In order to reduce the swing of underwater device and the tension fluctuation of wire rope, taken the recovery drum and anti-sway drum as actuating mechanism, the control strategy of unilateral anti-sway and rope tension is presented for underwater device. The dynamic model of steel cable is established by discrete beam finite elements and the relative nodal displacements method. The winding contact model between wire rope and drum is also introduced. Taken self-developed launch and recovery device as research object, combined technology of virtual prototype and control, the study of kinematics, kinetic and control strategy is carried out in the process of recovering underwater device. The dynamics simulation model is established in software RecurDyn, and the collaborative simulation model of mechanical system and control is built for launch and recovery device, which the algorithm is designed to control unilateral anti-sway and tension in software RecurDyn library Colink. The kinetic and collaborative simulation is studied, which provides the theoretical basis and data references about improving the stability and safety, reducing dynamic load, optimizing overall structure, guiding conceptual design for launch and recovery and underwater device. ©2013 Journal of Mechanical Engineering.},
author_keywords={Anti-sway control;  Collaboration simulation;  Dynamic simulation;  Tension control;  Wire rope model},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2013,
title={QoSA 2013 - Proceedings of the 9th International ACM Sigsoft Conference on the Quality of Software Architectures},
journal={QoSA 2013 - Proceedings of the 9th International ACM Sigsoft Conference on the Quality of Software Architectures},
year={2013},
page_count={176},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880555981&partnerID=40&md5=cafee5a4d805697cfc1aecf164eff986},
abstract={The proceedings contain 18 papers. The topics discussed include: ORQA: modeling energy and quality of service within AUTOSAR models; a systematic review of system-of-systems architecture research; cloud API issues: an empirical study and impact; architecture-based self-protecting software systems; performance analysis of self-adaptive systems for requirements validation at design-time; towards cost-aware service recovery; a multi-dimensional measure for intrusion - the intrusiveness quality attribute; model-based performance analysis of software architectures under uncertainty; combining fUML and profiles for non-functional analysis based on model execution traces; evaluation framework for software architecture viewpoint languages; on the appropriate rationale for using design patterns and pattern documentation; and improving product copy consolidation by architecture-aware difference analysis.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Usamentiaga2013,
author={Usamentiaga, R. and Molleda, J. and García, D.F. and Bulnes, F.G.},
title={Software architecture for time-constrained machine vision applications},
journal={Journal of Electronic Imaging},
year={2013},
volume={22},
number={1},
doi={10.1117/1.JEI.22.1.013001},
art_number={13001},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879987656&doi=10.1117%2f1.JEI.22.1.013001&partnerID=40&md5=09bf2904cd27b6520fa04e4e002015c7},
abstract={Real-time image and video processing applications require skilled architects, and recent trends in the hardware platform make the design and implementation of these applications increasingly complex. Many frameworks and libraries have been proposed or commercialized to simplify the design and tuning of real-time image processing applications. However, they tend to lack flexibility, because they are normally oriented toward particular types of applications, or they impose specific data processing models such as the pipeline. Other issues include large memory footprints, difficulty for reuse, and inefficient execution on multicore processors. We present a novel software architecture for time-constrained machine vision applications that addresses these issues. The architecture is divided into three layers. The platform abstraction layer provides a high-level application programming interface for the rest of the architecture. The messaging layer provides a message-passing interface based on a dynamic publish/subscribe pattern. A topic-based filtering in which messages are published to topics is used to route the messages from the publishers to the subscribers interested in a particular type of message. The application layer provides a repository for reusable application modules designed for machine vision applications. These modules, which include acquisition, visualization, communication, user interface, and data processing, take advantage of the power of well-known libraries such as OpenCV, Intel IPP, or CUDA. Finally, the proposed architecture is applied to a real machine vision application: a jam detector for steel pickling lines. © 2013 The Authors.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu201369,
author={Liu, X. and Mellor-Crummey, J. and Fagan, M.},
title={A new approach for performance analysis of openMP programs},
journal={Proceedings of the International Conference on Supercomputing},
year={2013},
pages={69-80},
doi={10.1145/2464996.2465433},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879808467&doi=10.1145%2f2464996.2465433&partnerID=40&md5=3f0118ff599a34b6b17b9825a32d0974},
abstract={The number of hardware threads is growing with each new generation of multicore chips; thus, one must effectively use threads to fully exploit emerging processors. OpenMP is a popular directive-based programming model that helps programmers exploit thread-level parallelism. In this paper, we describe the design and implementation of a novel performance tool for OpenMP. Our tool distinguishes itself from existing OpenMP performance tools in two principal ways. First, we develop a measurement methodology that attributes blame for work and inefficiency back to program contexts. We show how to integrate prior work on measurement methodologies that employ directed and undirected blame shifting and extend the approach to support dynamic thread-level parallelism in both time-shared and dedicated environments. Second, we develop a novel deferred context resolution method that supports online attribution of performance metrics to full calling contexts within an OpenMP program execution. This approach enables us to collect compact call path profiles for OpenMP program executions without the need for traces. Support for our approach is an integral part of an emerging standard performance tool application programming interface for OpenMP. We demonstrate the effectiveness of our approach by applying our tool to analyze four well-known application benchmarks that cover the spectrum of OpenMP features. In case studies with these benchmarks, insights from our tool helped us significantly improve the performance of these codes. © 2013 ACM.},
author_keywords={openmp;  performance analysis;  performance measurement;  software tools},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xu20135281,
author={Xu, F. and Zhu, M.},
title={Design and implementation of library monitoring system based on ZigBee},
journal={Journal of Computational Information Systems},
year={2013},
volume={9},
number={13},
pages={5281-5289},
doi={10.12733/jcis6357},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880351396&doi=10.12733%2fjcis6357&partnerID=40&md5=6a8bc9298830b601e14004f4181b86ff},
abstract={Most of libraries have to get the real-time temperature, humidity, illumination and other parameters to protect the books and other devices. Using the low-cost, miniature, lightweight and intelligent physiological sensor nodes with wireless communication modules can easily get the real-time parameters. By analyzing the information gathered by wireless sensor network (WSN) can easily get the useful information and status of library. This paper discusses the design and implementation issues and describes the whole system which consists of wireless sensor network based on ZigBee, data concentrator and system controller. The WSN utilizes the 802.15.4 compliant network nodes integrated with some sensors to gather the information of the library. The data concentrator receives data from WSN and control commands from system controller. The system controller processes information received from all sensing networks and can also send control command to the whole system or a particular group. The paper presents system architecture and hardware and software implementation, as well as the some solutions such as many-to-one routing and trust center to promote the efficiency, robustness and security of the system. © 2013 by Binary Information Press.},
author_keywords={802.15.4;  CC2530;  Library monitoring system;  Wireless sensor network;  ZigBee},
document_type={Article},
source={Scopus},
}

@ARTICLE{Konwar2013,
author={Konwar, K.M. and Hanson, N.W. and Pagé, A.P. and Hallam, S.J.},
title={MetaPathways: A modular pipeline for constructing pathway/genome databases from environmental sequence information},
journal={BMC Bioinformatics},
year={2013},
volume={14},
number={1},
doi={10.1186/1471-2105-14-202},
art_number={202},
note={cited By 74},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879121025&doi=10.1186%2f1471-2105-14-202&partnerID=40&md5=2ac00d1c21904b21725060745a9cbf07},
abstract={Background: A central challenge to understanding the ecological and biogeochemical roles of microorganisms in natural and human engineered ecosystems is the reconstruction of metabolic interaction networks from environmental sequence information. The dominant paradigm in metabolic reconstruction is to assign functional annotations using BLAST. Functional annotations are then projected onto symbolic representations of metabolism in the form of KEGG pathways or SEED subsystems.Results: Here we present MetaPathways, an open source pipeline for pathway inference that uses the PathoLogic algorithm to map functional annotations onto the MetaCyc collection of reactions and pathways, and construct environmental Pathway/Genome Databases (ePGDBs) compatible with the editing and navigation features of Pathway Tools. The pipeline accepts assembled or unassembled nucleotide sequences, performs quality assessment and control, predicts and annotates noncoding genes and open reading frames, and produces inputs to PathoLogic. In addition to constructing ePGDBs, MetaPathways uses MLTreeMap to build phylogenetic trees for selected taxonomic anchor and functional gene markers, converts General Feature Format (GFF) files into concatenated GenBank files for ePGDB construction based on third-party annotations, and generates useful file formats including Sequin files for direct GenBank submission and gene feature tables summarizing annotations, MLTreeMap trees, and ePGDB pathway coverage summaries for statistical comparisons.Conclusions: MetaPathways provides users with a modular annotation and analysis pipeline for predicting metabolic interaction networks from environmental sequence information using an alternative to KEGG pathways and SEED subsystems mapping. It is extensible to genomic and transcriptomic datasets from a wide range of sequencing platforms, and generates useful data products for microbial community structure and function analysis. The MetaPathways software package, installation instructions, and example data can be obtained from http://hallam.microbiology.ubc.ca/MetaPathways. © 2013 Konwar et al.; licensee BioMed Central Ltd.},
author_keywords={Environmental pathway/Genome Database (ePGDB);  Metabolic interaction networks;  Metabolism;  MetaCyc;  Metagenome;  Microbial community;  PathoLogic;  Pathway tools},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saucedo-Tejada20136173,
author={Saucedo-Tejada, G. and Mendoza, S. and Decouchant, D.},
title={F2FMI: A toolkit for facilitating face-to-face mobile interaction},
journal={Expert Systems with Applications},
year={2013},
volume={40},
number={15},
pages={6173-6184},
doi={10.1016/j.eswa.2013.05.043},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879060617&doi=10.1016%2fj.eswa.2013.05.043&partnerID=40&md5=9af8a0c6b0c378974d8c6dca27d60387},
abstract={The importance of mobile groupware systems resides in the specific tasks that they can perform and other systems cannot. On the one hand, groupware systems allow groups of users to work together providing facilities that single-user systems are unable to offer. On the other hand, unlike stationary systems, mobile systems allow users to work on the move. The intersection of these two technologies offers a new support for activities, such as spontaneous collaboration, that could be facilitated neither by stationary groupware systems nor by mobile single-user systems. However, implementations of this new support are uncommon, probably because of the high development effort required and the seemingly little benefit obtained. In this paper, we aim to reduce this effort by facilitating the development of mobile groupware applications that support such activities. Our proposal to achieve this objective involves the design and implementation of the Face-to-Face Mobile Interaction (F2FMI) toolkit, whose goal is to provide generic and reusable software components required in most cases. We expect this strategy to yield a higher variety of successfully deployed applications, which in turn will demonstrate the benefits of supporting this kind of interactions through mobile devices. © 2013 Elsevier Ltd. All rights reserved.},
author_keywords={Architectures;  Collaborative computing;  Mobile environments;  Reusable libraries},
document_type={Article},
source={Scopus},
}

@ARTICLE{Affeldt201359,
author={Affeldt, R.},
title={On construction of a library of formally verified low-level arithmetic functions},
journal={Innovations in Systems and Software Engineering},
year={2013},
volume={9},
number={2},
pages={59-77},
doi={10.1007/s11334-013-0195-x},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878389462&doi=10.1007%2fs11334-013-0195-x&partnerID=40&md5=fd995baf5facd3f1f3e83c55c044c532},
abstract={Arithmetic functions are used in many important computer programs such as computer algebra systems and cryptographic software. The latter are critical applications whose correct implementation deserves to be formally guaranteed. They are also computation-intensive applications, so that programmers often resort to low-level assembly code to implement arithmetic functions. We propose an approach for the construction of a library of formally verified low-level arithmetic functions. To build our library, we first introduce a formalization of data structures for signed multi-precision arithmetic in low-level programs. We use this formalization to verify the implementation of several primitive arithmetic functions using Separation logic, an extension of Hoare logic to deal with pointers. Since this direct style of formal verification leads to technically involved specifications, we also propose for larger functions to show a formal simulation relation between pseudo-code and assembly. This style of verification is illustrated with a concrete implementation of the binary extended gcd algorithm. © 2013 Springer-Verlag London.},
author_keywords={Assembly programming language;  Coq;  Hoare logic;  Multi-precision arithmetic;  Proof-assistant;  Separation logic;  Simulation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Alnusair20131062,
author={Alnusair, A. and Zhao, T. and Yan, G.},
title={Automatic recognition of design motifs using semantic conditions},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2013},
pages={1062-1067},
doi={10.1145/2480362.2480564},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877987924&doi=10.1145%2f2480362.2480564&partnerID=40&md5=bfadea27ca3fbc4e1f3fa7e022abf6a4},
abstract={Program comprehension is vital for building, enhancing and maintaining existing software systems. In this paper, we propose an automatic reverse engineering approach that leverages understanding and reusing software libraries through the automatic recovery of the motifs described by design patterns. Initially we formalise the description of common patterns. We then exploit Semantic Web knowledge representation mechanisms for capturing these descriptions in source-code. Empirical evaluations of this approach show evidence that when conceptual knowledge of source-code is represented using ontology formalisms, and when semantic rules are used to capture pattern structure and behavior, we can achieve an effective and flexible detection of patterns without relying on hard-coded heuristics. Copyright 2013 ACM.},
author_keywords={Design patterns;  Ontology formalisms;  Program comprehension;  Semantic reasoning;  Software maintenance},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Stewart2013563,
author={Stewart, D. and Domeika, M. and Hissam, S.A. and Hovsmith, S. and Ivers, J. and Dickson, R. and Lintault, I. and Olsen, S. and Baik, H. and Bodin, F. and Oshana, R.},
title={Multicore software development for embedded systems: This chapter draws on material from the multicore programming practices guide (MPP) from the multicore association},
journal={Software Engineering for Embedded Systems: Methods, Practical Techniques, and Applications},
year={2013},
pages={563-612},
doi={10.1016/B978-0-12-415917-4.00017-7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135551117&doi=10.1016%2fB978-0-12-415917-4.00017-7&partnerID=40&md5=860096af20491776b112623a3acac542},
abstract={Multicore software development is growing in importance and applicability in many areas of embedded systems from automotive to networking, to wireless base stations. This chapter is a summary of key sections of the recently released Multicore Programming Practices (MPP) from the Multicore Association (MCA). The MPP standardized "best practices" guide is written specifically for engineers and engineering managers of companies considering or implementing a development project involving multicore processors and favoring use of existing multicore technology. There is an important need to better understand how today's C/C++ code may be written to be "multicore ready", and this was accomplished under the influence of the MPP working group. The guide will enable you to (a) produce higher-performing software; (b) reduce the bug rate due to multicore software issues; (c) develop portable multicore code which can be targeted at multiple platforms; (d) reduce the multicore programming learning curve and speed up development time; and (e) tie into the current structure and roadmap of the Multicore Association's API infrastructure. © 2013 Copyright © 2013 Elsevier Inc.},
author_keywords={Granularity;  MCAPI;  MRAPI;  Multicore Association (MCA);  Multicore Programming Practices (MPP);  Mutexes;  Parallel decomposition;  Parallelism;  PPthreads;  Serial performance;  Synchronization;  Threads},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Sievers2013989,
author={Sievers, F. and Dineen, D. and Wilm, A. and Higgins, D.G.},
title={Making automated multiple alignments of very large numbers of protein sequences},
journal={Bioinformatics},
year={2013},
volume={29},
number={8},
pages={989-995},
doi={10.1093/bioinformatics/btt093},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876220535&doi=10.1093%2fbioinformatics%2fbtt093&partnerID=40&md5=5f5da5e58ecf4ee9e6b9e24e1716d07b},
abstract={Motivation: Recent developments in sequence alignment software have made possible multiple sequence alignments (MSAs) of >100 000 sequences in reasonable times. At present, there are no systematic analyses concerning the scalability of the alignment quality as the number of aligned sequences is increased.Results: We benchmarked a wide range of widely used MSA packages using a selection of protein families with some known structures and found that the accuracy of such alignments decreases markedly as the number of sequences grows. This is more or less true of all packages and protein families. The phenomenon is mostly due to the accumulation of alignment errors, rather than problems in guide-tree construction. This is partly alleviated by using iterative refinement or selectively adding sequences. The average accuracy of progressive methods by comparison with structure-based benchmarks can be improved by incorporating information derived from high-quality structural alignments of sequences with solved structures. This suggests that the availability of high quality curated alignments will have to complement algorithmic and/or software developments in the long-term. © 2013 The Author.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abdelfattah2013207,
author={Abdelfattah, A. and Keyes, D. and Ltaief, H.},
title={Systematic approach in optimizing numerical memory-bound kernels on GPU},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7640 LNCS},
pages={207-216},
doi={10.1007/978-3-642-36949-0_23},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874448461&doi=10.1007%2f978-3-642-36949-0_23&partnerID=40&md5=926901814055975fcbe8d5d8d852675a},
abstract={The use of GPUs has been very beneficial in accelerating dense linear algebra computational kernels (DLA). Many high performance numerical libraries like CUBLAS, MAGMA, and CULA provide BLAS and LAPACK implementations on GPUs as well as hybrid computations involving both, CPUs and GPUs. GPUs usually score better performance than CPUs for compute-bound operations, especially those characterized by a regular data access pattern. This paper highlights a systematic approach for efficiently implementing memory-bound DLA kernels on GPUs, by taking advantage of the underlying device's architecture (e.g., high throughput). This methodology proved to outperform existing state-of-the-art GPU implementations for the symmetric matrix-vector multiplication (SYMV), characterized by an irregular data access pattern, in a recent work (Abdelfattah et. al, VECPAR 2012). We propose to extend this methodology to the general matrix-vector multiplication (GEMV) kernel. The performance results show that our GEMV implementation achieves better performance for relatively small to medium matrix sizes, making it very influential in calculating the Hessenberg and bidiagonal reductions of general matrices (radar applications), which are the first step toward computing eigenvalues and singular values, respectively. Considering small and medium size matrices (≤4500), our GEMV kernel achieves an average 60% improvement in single precision (SP) and an average 25% in double precision (DP) over existing open-source and commercial software solutions. These results improve reduction algorithms for both small and large matrices. The improved GEMV performances engender an averge 30% (SP) and 15% (DP) in Hessenberg reduction and up to 25% (SP) and 14% (DP) improvement for the bidiagonal reduction over the implementation provided by CUBLAS 5.0. © 2013 Springer-Verlag.},
author_keywords={Bidiagonal Reduction;  GPU Optimizations;  Hessenberg Reduction;  Matrix-Vector Multiplication;  Memory-Bound Operations},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Berka2013385,
author={Berka, T. and Kollias, G. and Hagenauer, H. and Vajteršic, M. and Grama, A.},
title={Concurrent programming constructs for parallel MPI applications: The MPI threads library},
journal={Journal of Supercomputing},
year={2013},
volume={63},
number={2},
pages={385-406},
doi={10.1007/s11227-011-0739-5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886718457&doi=10.1007%2fs11227-011-0739-5&partnerID=40&md5=0aa68c974ee738e8d4f97aff45faf51e},
abstract={Concurrency and parallelism have long been viewed as important, but somewhat distinct concepts. While concurrency is extensively used to amortize latency (for example, in web- and database-servers, user interfaces, etc.), parallelism is traditionally used to enhance performance through execution on multiple functional units. Motivated by an evolving application mix and trends in hardware architecture, there has been a push toward integrating traditional programming models for concurrency and parallelism. Use of conventional threads APIs (POSIX, OpenMP) with messaging libraries (MPI), however, leads to significant programmability concerns, owing primarily to their disparate programming models. In this paper, we describe a novel API and associated runtime for concurrent programming, called MPI Threads (MPIT), which provides a portable and reliable abstraction of low-level threading facilities. We describe various design decisions in MPIT, their underlying motivation, and associated semantics. We provide performance measurements for our prototype implementation to quantify overheads associated with various operations. Finally, we discuss two real-world use cases: an asynchronous message queue and a parallel information retrieval system. We demonstrate that MPIT provides a versatile, low overhead programming model that can be leveraged to program large parallel ensembles. © 2012 Springer Science+Business Media, LLC.},
author_keywords={Concurrent programming;  Parallel programming;  Performance;  Reliability;  Threads},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2013404,
author={Liu, G. and Puri, A. and Neelamegham, S.},
title={Glycosylation Network Analysis Toolbox: A MATLAB-based environment for systems glycobiology},
journal={Bioinformatics},
year={2013},
volume={29},
number={3},
pages={404-406},
doi={10.1093/bioinformatics/bts703},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873283704&doi=10.1093%2fbioinformatics%2fbts703&partnerID=40&md5=28fb89178a863abaaa10f708a18955c4},
abstract={Summary: Systems glycobiology studies the interaction of various pathways that regulate glycan biosynthesis and function. Software tools for the construction and analysis of such pathways are not yet available. We present GNAT, a platform-independent, user-extensible MATLAB-based toolbox that provides an integrated computational environment to construct, manipulate and simulate glycans and their networks. It enables integration of XML-based glycan structure data into SBML (Systems Biology Markup Language) files that describe glycosylation reaction networks. Curation and manipulation of networks is facilitated using class definitions and glycomics database query tools. High quality visualization of networks and their steady-state and dynamic simulation are also supported.Availability: The software package including source code, help documentation and demonstrations are available at http://sourceforge.net/ projects/gnatmatlab/files/. © 2012 The Author.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rosenthal2013,
author={Rosenthal, P. and Dhariwal, K. and Whitehouse, J.},
title={IS’15 - A model curriculum reflecting the emerging IS profession},
journal={2013 Proceedings ISECON: Information Systems Educators Conference and CONISAR 2013, Conference on Information Systems Applied Research},
year={2013},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084016975&partnerID=40&md5=8b4682609aa41c04f0393a15b3946bd5},
abstract={This proposed Model Curriculum and Guidelines for Undergraduate Degree Programs in Information Systems is designed to reflect the actual job market in IS/IT. It is grounded in the requirements of industry for new graduates from business school affiliated information systems degree programs. The application software package orientation of the current IS/IT industry has created three major IS employment areas: Systems Administration/ Support; Project/ Systems/ Architecture Analysis and Implementation; and Systems Development and Programming of personal-level applications. This proposed curriculum recognizes these trends by reinserting and updating two IS 2002 courses and creating a new course based on two now significant IS 2002 learning units, while removing two courses “focused on concepts at a higher level of abstraction.” ©2013 EDSIG (Education Special Interest Group of the AITP).},
author_keywords={IS Model Curriculum;  IS undergraduate curriculum},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zakharova201337,
author={Zakharova, T. and Moskalenko, V.},
title={Information technology for the decision-making process in an investment company},
journal={Lecture Notes in Business Information Processing},
year={2013},
volume={137},
pages={37-48},
doi={10.1007/978-3-642-38370-0_4},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025145793&doi=10.1007%2f978-3-642-38370-0_4&partnerID=40&md5=61aa17b34dc1fe8fb8c2a3316306d6e7},
abstract={This paper covers one of the directions of the Investment Company functioning, namely investments in business projects of the national economy. The information technology in order to justify and make investment decisions has been developed. This technology is the complex of models and techniques of analysis of the investment market conditions, formation of the investment policy, evaluation of effectiveness and risks of investment directions and projects, construction of investment portfolio. The implementation of the Decision Support System for the Investment Company is considered. Main functions and requirements of the system are presented. The software solution structure based on service-oriented architecture (SOA) is considered. The description of packages, interfaces and their interaction is described. The developed application is a complete solution for operating with the internal and external data of the Investment Company, obtaining the results of assessment and report generation. © Springer-Verlag Berlin Heidelberg 2013.},
author_keywords={DSS;  Investment company;  Investment policy;  Investment portfolio;  Modeling;  Service-oriented architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lachenmaier2013193,
author={Lachenmaier, P. and Ott, F. and Koch, M.},
title={Model-driven development of a person-centric mashup for social software},
journal={Social Network Analysis and Mining},
year={2013},
volume={3},
number={2},
pages={193-207},
doi={10.1007/s13278-012-0064-x},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947285611&doi=10.1007%2fs13278-012-0064-x&partnerID=40&md5=3adaf75f7d05a3254218be276be7f54c},
abstract={Based on the success of social software, modern information and communication systems are continuously moving from an information-centric data perspective to a more person-centric view. Easy access to federated activity streams of colleagues and other awareness information that is aggregated from different distributed intra- and extra-organizational systems become more and more important for the daily knowledge work. The increasing number of platforms every person uses requires a flexible data integration solution that keeps track of the connections between the pieces of information and the persons involved in their creation in order to create a unified and aggregated view for work groups, teams and communities. This unified data collection is especially important for social network analysis and data mining as individual profiles and activities are meanwhile typically distributed over various source systems. In this paper we present the CommunityMashup, a person-centric multi-user data integration solution for social software and similar systems that facilitates data aggregation and filtering while retaining the link to the pieces of information in the source systems. To support continuous evolution and flexible integration of frequently changing heterogeneous APIs and interfaces, we apply a model-driven development approach based on a therefore created person-centric data model. In addition to the conceptual design of the CommunityMashup, we present a reference implementation based upon open source components. Our overall goal is to build a multi-user mashup middleware for social software that offers an universal entry point in combination with unified data access for different client devices and can be used in various application scenarios with regard to individually specified service levels, e.g. continuous availability. © 2012, Springer-Verlag.},
author_keywords={CommunityMashup;  Model-driven development;  Person centricity;  Service-oriented architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pickles2013248,
author={Pickles, A.J. and Kilgore, I.M. and Steer, M.B.},
title={Automated creation of complex three-dimensional composite mixtures for use in electromagnetic simulation},
journal={IEEE Access},
year={2013},
volume={1},
pages={248-251},
doi={10.1109/ACCESS.2013.2262014},
art_number={6514917},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923317880&doi=10.1109%2fACCESS.2013.2262014&partnerID=40&md5=70bda08ab5c121d98cc4b9b1c3fc30a8},
abstract={The manual creation of complex 3D structures for use in engineering analysis is a major obstacle to analyzing physically realistic structures. A bias is invariably imposed when a mixture is manually composed, and the structure is rarely representative of the process by which composites are fabricated. Properties such as packing density and anisotropies that seem to easily occur in nature are very difflucult to obtain with manual arrangements. This paper addresses the creation of complex 3D mixtures, comprising crystals embedded in a matrix, for subsequent electromagnetic (EM) analysis. The physically realistic arrangement of the crystals is facilitated by the use of physics engine software, specifically the Bullet physics library, which renders the realistic effects in advanced computer games. A composite mixture of crystals is created by pouring a series of random crystals into a box with the crystals bouncing against each other and aligning just as they do in the real world. Higher packing densities are obtained than can be reasonably obtained with manual construction. The arrangement of the obtained crystals reflects the realworld alignment of asymmetric crystals. A composite is created here and used with EM simulation software to investigate energy localization in materials. © 2013 IEEE.},
author_keywords={Automated crystal modeling;  Electric energy density;  Electromagnetic analysis;  Gaming software;  Mixtures;  Relative permittivity;  Transient simulation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pisu2013489,
author={Pisu, C. and Casu, P.},
title={Cloud GIS and 3D modelling to enhance sardinian late gothic architectural heritage},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2013},
volume={40},
number={5W2},
pages={489-494},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908569285&partnerID=40&md5=ab10f6564837fa9d19a53f73b9f66776},
abstract={This work proposes the documentation, virtual reconstruction and spreading of architectural heritage through the use of software packages that operate in cloud computing. Cloud computing makes available a variety of applications and tools which can be effective both for the preparation and for the publication of different kinds of data. We tested the versatility and ease of use of such documentation tools in order to study a particular architectural phenomenon. The ultimate aim is to develop a multi-scale and multi-layer information system, oriented to the divulgation of Sardinian late gothic architecture. We tested the applications on portals of late Gothic architecture in Sardinia. The actions of conservation, protection and enhancement of cultural heritage are all founded on the social function that can be reached only through the widest possible fruition by the community. The applications of digital technologies on cultural heritage can contribute to the construction of effective communication models that, relying on sensory and emotional involvement of the viewer, can attract a wider audience to cultural content.},
author_keywords={3D modelling;  Cloud computing;  Late Gothic Architecture;  Photo-modelling;  Sardinia;  Web GIS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pei20134797,
author={Pei, W. and Deng, W. and Shen, Z. and Zhao, Z. and Wang, J.},
title={Design and implementation of energy management system for energy hub in smart grid},
journal={Information Technology Journal},
year={2013},
volume={12},
number={18},
pages={4797-4804},
doi={10.3923/itj.2013.4797.4804},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901697567&doi=10.3923%2fitj.2013.4797.4804&partnerID=40&md5=ebc9c357e10f6845aad8663ceb626407},
abstract={Part of distributed smart grid region can formed the energy hub through the energy management and coordination control with its distributed energy supply, energy consumption and energy storage units. The design and implementation of energy management system (EMS) for energy hub in smart grid are presented in this paper. The architecture, function module, subsystem interface and service mechanism of energy hub EMS are proposed in detail. The energy hub prototype and the EMS software are developed. Then the actual energy management tests of energy hub are performed based on EMS. The experimental results show that the design and implementation of EMS for energy hub are practicable and feasible, which demonstrates an effective solution for energy management and coordination control of energy hub in smart grid. © 2013 Asian Network for Scientific Information.},
author_keywords={Distributed generation;  Dynamic link library;  Energy hub;  Energy management system;  Smart grid},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kang201345,
author={Kang, B.-C. and Kang, K.-Y. and Lee, S.-C. and Lee, G. and Lee, K.},
title={A simple and robust network protocol for remote controlled robot under punctuated network environment},
journal={2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2013},
year={2013},
pages={45-50},
doi={10.1109/URAI.2013.6677468},
art_number={6677468},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899124070&doi=10.1109%2fURAI.2013.6677468&partnerID=40&md5=07713e3fba793ffd82bcf40d5a429792},
abstract={Software architecture with RESTful API is proposed for ILI robot system under unstable networking environment in order to increase reliability, scalability, independency, and generality, yet simple to implement. Evaluation of the architecture showed that the system could be flexibly adapted to emergency cases, which is performed for diverse scenarios including sudden shutdown of component, hot-swap of component, network failure, etc. Consequently, the proposed software architecture is conducted to reduce complexity of implementation and maintenance, to increase scalability, and to allow easy rescue and recovery scheme. © 2013 IEEE.},
author_keywords={Network Architecture;  RESTful API;  Unpiggable Robot},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Manos20131011,
author={Manos, G.C. and Petalas, A. and Demosthenous, M.},
title={Numerical and experimental study of the rocking response of unanchored body to horizontal base excitation},
journal={ECCOMAS Thematic Conference - COMPDYN 2013: 4th International Conference on Computational Methods in Structural Dynamics and Earthquake Engineering, Proceedings - An IACM Special Interest Conference},
year={2013},
pages={1011-1024},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898971479&partnerID=40&md5=f6d23465ca809d71ef3969427cad32f9},
abstract={Ancient Greek and Roman structures composed of large heavy members that simply lie on top of each other in a perfect-fit construction without the use of connecting mortar, are distinctly different from relatively flexible contemporary structures. The colonnade (including free standing monolithic columns or columns with drums) is the typical structural form of ancient Greek or Roman temples. The seismic response mechanisms that develop on this solid block structural system during strong ground motions can include sliding and rocking, thus dissipating the seismic energy in a different way from that of conventional contemporary buildings. This paper presents results and conclusions from an experimental study that examines the dynamic response of rigid bodies, representing simple models of ancient columns. These models are subjected to various types of horizontal base motions (including sinusoidal as well as earthquake base motions), reproduced by the Earthquake Simulator Facility of Aristotle University. The employed rigid bodies were made of steel and are assumed to be models of prototype structures 20 times larger. In addition to the experimental study, numerical simulations of the observed dynamic and earthquake rocking response of the free standing steel rigid block specimens were also carried out. These numerical simulations were performed in two different ways. At first, a specific software was developed based on analytical expressions of the equations of motion for this problem together with numerical integration techniques that yielded predictions of the rocking response. Next, the numerical simulation employed a commercial finite element software package (Abaqus) and utilized the capabilities of this software to obtain predictions of the rocking response of the tested in the laboratory free standing steel rigid block specimens. The numerical results obtained from these two different numerical simulations are presented and compared with the experimental response measurements as well as among each other. From this comparison the reliability of these two distinct numerical simulations is presented and discussed.},
author_keywords={Ancient monuments;  Numerical simulation experimental verification;  Rigid body;  Rocking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yin2013561,
author={Yin, Z. and Tang, J. and Schaeffer, S.W. and Bader, D.A.},
title={Streaming breakpoint graph analytics for accelerating and parallelizing the computation of DCJ median of three genomes},
journal={Procedia Computer Science},
year={2013},
volume={18},
pages={561-570},
doi={10.1016/j.procs.2013.05.220},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896925434&doi=10.1016%2fj.procs.2013.05.220&partnerID=40&md5=06ee4571dbafbd8dfbac8a50f4bd0ab9},
abstract={The problem of finding the median of three genomes is the key process in building the most parsimonious phylogenetic trees from genome rearrangement data. The median problem using Double-Cut-and-Join (DCJ) distance is NP-hard and the best exact algorithm is based on a branch-and-bound best-first search strategy to explore sub-graph patterns in Multiple BreakPoint Graph (MBG). In this paper, by taking advantage of the "streaming" property of MBG, we introduce the "footprint-based" data structure to reduce the space requirement of a single search nodes from O(v2) to O(v); minimize the redundant computation in counting cycles/paths to update bounds, which leads to dramatically decrease of workload of a single search node. Additional heuristic of branching strategy is introduced to help reducing the searching space. Last but not least, the introduction of a multi-thread shared memory parallel algorithm with two load balancing strategies bring in additional benefit by distributing search work efficiently among different processors. We conduct extensive experiments on simulated datasets and our results show significant improvement on all datasets. And we test our DCJ median algorithm with GASTS, a state of the art software phylogenetic tree construction package. On the real high resolution Drosophila data set, our exact algorithm run as fast as the heuristic algorithm and help construct a better phylogenetic tree. © 2013 The Authors. Published by Elsevier B.V.},
author_keywords={Double-cut-and-joining median;  Genome rearrangement;  Parallel programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Plumbridge2013,
author={Plumbridge, G. and Audsley, N.C.},
title={Programming FPGA based NoCs with Java},
journal={2013 International Conference on Reconfigurable Computing and FPGAs, ReConFig 2013},
year={2013},
doi={10.1109/ReConFig.2013.6732323},
art_number={6732323},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894503854&doi=10.1109%2fReConFig.2013.6732323&partnerID=40&md5=5edc6fe816dafe944e588da5f74ace68},
abstract={FPGAs enable NoC architecture experimentation, although to be effective they need to be supported by tools and frameworks for construction of the NoC and effective software programming of the NoC. In this paper, we focus upon effective programming of the NoC using Java, complementing previous work which proposes the Blueshell framework for NoC generation for FPGAs. The approach taken is called Network-Chi, providing a number of key extensions to the Chi Java compiler. This includes provision of a networking API within Java giving a mesh based abstraction for network communication, allowing the programmer to send Java objects to other nodes without consideration for the underlying hardware topology or protocols; and a region-based memory management API that enables the definition of transient allocation contexts that discard all objects allocated within them when they reach the end of execution. Results show the approach taken to be efficient and effective. © 2013 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cameron2013,
author={Cameron, A. and Stumptner, M. and Nandagopal, N. and Mayer, W. and Mansell, T.},
title={A rule-based platform for distributed real-time SOA with application in defence systems},
journal={2013 Military Communications and Information Systems Conference, MilCIS 2013 - Proceedings},
year={2013},
doi={10.1109/MilCIS.2013.6694494},
art_number={6694494},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893542664&doi=10.1109%2fMilCIS.2013.6694494&partnerID=40&md5=3c7b35feac4ca1046adcd3d2fc20f780},
abstract={Modularity has been a key issue in the design and development of modern embedded Real-Time Software Systems (RTS), where modularity enables flexibility with respect to changes in platform, environment, and requirements, as well as reuse. In distributed RTS, similar ideas have led to the adoption of COTS components integrated via Service-Oriented Architecture (SOA) principles and technologies that are already well-established in business-oriented information systems. However, current SOA implementations, with respect to service activation and orchestration, do not meet strict time-dependent constraints on scalability and latency required by RTS. We present a novel approach to RTS development where the orchestration of real-time processes is distributed among the services within a fully distributed data-driven process framework. Our framework wraps around COTS components implementing individual processing steps in a distributed real-time process. Our execution model is configurable through message routing policies distributed as a knowledge base containing rule sets, thus maintaining real-time constraints, and therefore dispenses with the need for a central orchestration component which could easily become a bottleneck. Deterministic behaviour can therefore be achieved through the validation of the rule-sets and the use of Modular Performance Analysis. © 2013 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Houssem2013229,
author={Houssem, A. and Philippe, P. and Didier, N. and Pierre, C.J. and Chokri, B.},
title={Integration approach of mechatronics system in PLM systems},
journal={ICAC 2013 - Proceedings of the 19th International Conference on Automation and Computing: Future Energy and Automation},
year={2013},
pages={229-234},
art_number={6662043},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892724167&partnerID=40&md5=bf52b9a9863138ff5227c0e624db68cc},
abstract={The design of mechatronic systems requires the integration of data concerning numerous disciplines (mechanical, electric CAD, automatism, embarked software...). In this context, the PLM (Product Lifecycle Management) systems define a naturally relevant solution to manage those data. However, being built around the CAD, the structure of these systems does not allow a real integration in terms of re-use, reconfiguration or calculation of impact. This article proposes a frame of definition for the global integration of components into an application library (CAD, Automatic, HMI...) within a PLM including the mechatronic systems. This integration approach is based on the construction of a meta-model structured with multi-faceted components. The propositions were experimented in the context of a simple mechatronic system and implemented with the PLM Windchill system. © 2013 Chinese Automation and Computing Society in the UK - CACS.},
author_keywords={Integration;  Mechatronic;  Meta-Model;  PLM},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2013207,
author={Lee, G.A. and Billinghurst, M.},
title={A component based framework for mobile outdoor AR applications},
journal={Proceedings - VRCAI 2013: 12th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
year={2013},
pages={207-210},
doi={10.1145/2534329.2534344},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891550276&doi=10.1145%2f2534329.2534344&partnerID=40&md5=01de1d3145db467346d8c59b32ae1f87},
abstract={In this paper, we describe the design and implementation details of a software development framework for building Outdoor Augmented Reality (AR) applications on mobile platforms. The framework consists of a mobile client software library, web-based server, and a web-based authoring tool. It is developed using component based design making it easy to customize and reuse, and as it provides both low-level functional modules and ready-to-use high level components, both skilled and novice developers can choose the level of abstraction they need to use, or even work together under the same framework. By mixing and matching the ready-to-use framework components, developers can focus on designing and developing the domain content, logic and user interface, and spend less time implementing basic functionality. We demonstrate the validity of our framework with three case studies of mobile Outdoor AR application development. © 2013 ACM.},
author_keywords={development framework;  mobile outdoor augmented reality;  software library},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhdanov201388,
author={Zhdanov, D. and Ershov, S. and Deryabin, N.},
title={Object-oriented model of photorealistic vizualizatioln of complex scenes},
journal={Scientific Visualization},
year={2013},
volume={5},
number={4},
pages={88-117},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890834411&partnerID=40&md5=e86af04e5368037034ff32c5c4405483},
abstract={The article is devoted to design of the object-oriented model of physically accurate visualization of the geometrically and optically complex scenes. The actuality of the object-oriented organization becomes clear if we design complex platforms for accurate light simulations and photorealistic rendering. In this case, all extra efforts spent to object-oriented design of the rendering and its database are overplayed because of time to extend the database are significantly reduced. For example, the addition of new type of geometrical element to the database or modification of the algorithm of photon map processing do not touch other components of the scene and renderer and require any addition modification in the software package. The article considers basic principles of the object-oriented organization to design software for photorealistic visualization of complex scenes and discriminates two main components of the objet-oriented model: scene database and renderer. The article introduces conception of basic interfaces, which restrict all interactions between scene objects and between scene objects and renderer. For scene database, two levels of basic interfaces are proposed. The first level is a level of the end user. This level of interfaces is reduced to interfaces of entities, which are data structures with standard interfaces for self-visualization, user and program data modification and interaction with script. The second level is a level of the light simulations. For basic types of scene objects (geometry, camera, observer, light sources and optical properties), the minimal interface providing accurate and efficient ray tracing and calculation of direct and indirect luminances was designed. Moreover, for scene objects with a special behavior like scattering micro-geometry or special lenses, the conception optical elements is introduced. The interface of the optical elements joins interfaces of all other scene object types. A photorealistic scene visualization applies bi-directional stochastic ray tracing which provides physically correct light transfer from light sources through scene to camera. The ray tracer and renderer are designed on object-oriented principles too and all interactions of ray tracer and renderer with scene database is restricted with the basic interfaces. Object-oriented model of the scene database and photorealistic visualization solution based on the bi-directional stochastic ray tracing were implemented in Lumicept software package. As an example, the article shows a result of computer visualization of LCD display of car dashboard under different conditions of interior and exterior illumination. The main specific of the visualization is physically accurate light simulation, which takes into account all details of the LCD construction, including scattering microstructures, polarization filters and TFT matrix. The developed object-oriented model of photorealistic visualization is based on physically correct scene models including spectral and polarization light ray transformation incorporated to the basic interfaces of the proper object types. The conception of the optical element allows extending this object-oriented scene database with special objects, which interface cannot be expressed in the frames of standard scene object type. As a difference from most of computer solutions, the object-oriented conception was implemented for whole software package that significantly simplifies its extension with new scene and ray tracing models.},
author_keywords={Bidirectional ray tracing;  Optical element;  Photorealistic visualization;  Physically accurate rendering;  Program interface;  Rendering;  Scene;  Scene object;  Stochastic ray tracing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Merson2013153,
author={Merson, P.},
title={Ultimate architecture enforcement custom checks enforced at code-commit time},
journal={SPLASH 2013 - Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, and Applications: Software for Humanity},
year={2013},
pages={153-159},
doi={10.1145/2508075.2508433},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888150645&doi=10.1145%2f2508075.2508433&partnerID=40&md5=8a890b9d0cc0ec1fb88115476be88dae},
abstract={Creating a software architecture is a critical task in the development of software systems. However, the architecture discussed and carefully created is often not entirely followed in the implementation. Unless the architecture is communicated effectively to all developers, divergence between the intended architecture (created by the architect) and the actual architecture (found in the source code) tends to gradually increase. Static analysis tools, which are often used to check coding conventions and best practices, can help. However, the common use of static analysis tools for architecture enforcement has two limitations. One is the fact that design rules specific to a software architecture are not known and hence not enforced by the tool. The other limitation is more of a practical issue: static analysis tools are often integrated to the IDE or to a continuous integration environment; they report violations but the developers may choose to ignore them. This paper reports a successful experience where we addressed these two limitations for a large codebase comprising over 50 Java applications. Using a free open source tool called checkstyle and its Java API, we implemented custom checks for design constraints specified by the architecture of our software systems. In addition, we created a script that executes automatically on the Subversion software configuration management server prior to any code commit operation. This script runs the custom checks and denies the commit operation in case a violation is found. When that happens, the developer gets a clear error message explaining the problem. The architecture team is also notified and can proactively contact the developer to address any lack of understanding of the architecture. This experience report provides technical details of our architecture enforcement approach and recommendations to employ this or similar solutions more effectively. Copyright © 2013 by the Association for Computing Machinery, Inc. (ACM).},
author_keywords={Architecture conformance;  Architecture enforcement;  Checkstyle;  Java;  Software architecture;  Static analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yong-Xian2013244,
author={Yong-Xian, W. and Li-Lun, Z. and Wei, L. and Yong-Gang, C. and Chuan-Fu, X. and Zheng-Hua, W. and Yu, Z.},
title={Efficient parallel implementation of large scale 3D structured grid CFD applications on the Tianhe-1A supercomputer},
journal={Computers and Fluids},
year={2013},
volume={80},
number={1},
pages={244-250},
doi={10.1016/j.compfluid.2012.03.003},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885298967&doi=10.1016%2fj.compfluid.2012.03.003&partnerID=40&md5=dcdd61b1a254f3981a7531d146c3e7c2},
abstract={In this paper, a parallelization for a large scale CFD application with mixed one-to-one multiblock/overset structured grid was implemented into our in-house TH-CFD code running on Tianhe-1A supercomputer system. Strategies at multiple software levels were employed in a mutually supportive way for overall performance enhancement, and they include grid repartition, MPI + OpenMP hybrid programming, use of multiple programming languages adaptively for different software components, and adaptive thread scheduling for load balancing. Numerical testing was carried out and the testing results have shown high efficiency of our parallel implementation, supporting the effectiveness of our parallelization strategies. © 2012 Elsevier Ltd.},
author_keywords={Parallel computing;  Structured grid;  TH-CFD code;  Tianhe-1A},
document_type={Article},
source={Scopus},
}

@ARTICLE{Brandouy2013350,
author={Brandouy, O. and Mathieu, P. and Veryzhenko, I.},
title={On the Design of Agent-Based Artificial Stock Markets},
journal={Communications in Computer and Information Science},
year={2013},
volume={271},
pages={350-364},
doi={10.1007/978-3-642-29966-7_23},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880476259&doi=10.1007%2f978-3-642-29966-7_23&partnerID=40&md5=b04c6f1a8707a44f7582be92d14696e3},
abstract={The purpose of this paper is to define software engineering abstractions that provide a generic framework for stock market simulations. We demonstrate a series of key points and principles that has governed the development of an Agent-Based financial market application programming interface (API). The simulator architecture is presented. During artificial market construction we have faced the whole variety of agent-based modelling issues: local interaction, distributed knowledge and resources, heterogeneous environments, agents autonomy, artificial intelligence, speech acts, discrete or continuous scheduling and simulation. Our study demonstrates that the choices made for agent-based modelling in this context deeply impact the resulting market dynamics and proposes a series of advances regarding the main limits the existing platforms actually meet. © Springer-Verlag Berlin Heidelberg 2013.},
author_keywords={Agents behaviour;  Artificial market;  Market microstructure;  Multi-agent systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2013743,
author={Wang, M.},
title={Design and implementation of flash online recorder},
journal={Advances in Intelligent Systems and Computing},
year={2013},
volume={212},
pages={743-749},
doi={10.1007/978-3-642-37502-6_88},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880390425&doi=10.1007%2f978-3-642-37502-6_88&partnerID=40&md5=3535265f7761c10a7a7235d5269db365},
abstract={Online recorder software is used extensively because it can acquire data directly and conveniently. More and more online recorder software which base on Flash platform is invented, however, all of these need support of Flash Media Server, it increases expense of development. This paper is trying to achieve local coding and storage by Flash ActionScript3.0 and open source library, supplying a new solution for online recorder. © Springer-Verlag Berlin Heidelberg 2013.},
author_keywords={ActionScript3.0;  Flash;  Online recording},
document_type={Article},
source={Scopus},
}

@ARTICLE{Katsanos201370,
author={Katsanos, E.I. and Sextos, A.G.},
title={ISSARS: An integrated software environment for structure-specific earthquake ground motion selection},
journal={Advances in Engineering Software},
year={2013},
volume={58},
pages={70-85},
doi={10.1016/j.advengsoft.2013.01.003},
note={cited By 55},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874711877&doi=10.1016%2fj.advengsoft.2013.01.003&partnerID=40&md5=13f138c677328dd10386978b57755036},
abstract={Current practice enables the design and assessment of structures in earthquake prone areas by performing time history analysis with the use of appropriately selected strong ground motions. This study presents a Matlab-based software environment, which is integrated with a finite element analysis package, and aims to improve the efficiency of earthquake ground motion selection by accounting for the variability of critical structural response quantities. This additional selection criterion, which is tailored to the specific structure studied, leads to more reliable estimates of the mean structural response quantities used in design, while fulfils the criteria already prescribed by the European and US seismic codes and guidelines. To demonstrate the applicability of the software environment developed, an existing irregular, multi-storey, reinforced concrete building is studied for a wide range of seismic scenarios. The results highlight the applicability of the software developed and the benefits of applying a structure-specific criterion in the process of selecting suites of earthquake motions for the seismic design and assessment. © 2013 Elsevier Ltd. All rights reserved.},
author_keywords={API functions;  Earthquake engineering;  Software advancements;  Strong ground motion selection and scaling;  Structural response;  Time history analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yan2013548,
author={Yan, L. and Chang, F. and Qin, W. and Li, B. and Liu, Y.},
title={Design and implementation of testing platform for middleware of wireless sensor networks},
journal={Communications in Computer and Information Science},
year={2013},
volume={334 CCIS},
pages={548-561},
doi={10.1007/978-3-642-36252-1_51},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873955472&doi=10.1007%2f978-3-642-36252-1_51&partnerID=40&md5=44071923ffe4f7eb4837964fea90c52f},
abstract={Middleware for wireless sensor networks (WSNs) is system software between WSNs and applications, which provides a set of common application program interface (API). With the deeper research on the middleware of WSN, the researchers focus on the testing technologies on middleware of WSN and testing platform of middleware of WSN. In this paper, we summarize and propose the standards and common methods of the testing for middleware of WSN, referencing on the ISO/IEC 9126, CSTC and China Mobile standards. We design and implement a testing platform for middleware of WSN based on Web Integration and Eclipse Plug-in technologies, which could conduct the reliability testing, the performance testing and the stress testing for certain specific middle-ware. The platform provides two common testing methods including the manual way and the automatic way, which aims to efficiently verify the performance, the handling capability of error and the handling capability of concurrent for the middleware of WSN. Finally we give the design details and processes of each software module, and show the experimental evaluation of the testing platform. © 2013 Springer-Verlag.},
author_keywords={Functional Testing;  Middleware;  Performance Testing;  Testing Platform;  Wireless Sensor Network},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Murphy201389,
author={Murphy, M. and McGovern, E. and Pavia, S.},
title={Historic Building Information Modelling - Adding intelligence to laser and image based surveys of European classical architecture},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2013},
volume={76},
pages={89-102},
doi={10.1016/j.isprsjprs.2012.11.006},
note={cited By 302},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873262334&doi=10.1016%2fj.isprsjprs.2012.11.006&partnerID=40&md5=2e13585f2f88c331374e42c8ae38c070},
abstract={Historic Building Information Modelling (HBIM) is a novel prototype library of parametric objects, based on historic architectural data and a system of cross platform programmes for mapping parametric objects onto point cloud and image survey data. The HBIM process begins with remote collection of survey data using a terrestrial laser scanner combined with digital photo modelling. The next stage involves the design and construction of a parametric library of objects, which are based on the manuscripts ranging from Vitruvius to 18th century architectural pattern books. In building parametric objects, the problem of file format and exchange of data has been overcome within the BIM ArchiCAD software platform by using geometric descriptive language (GDL). The plotting of parametric objects onto the laser scan surveys as building components to create or form the entire building is the final stage in the reverse engineering process. The final HBIM product is the creation of full 3D models including detail behind the object's surface concerning its methods of construction and material make-up. The resultant HBIM can automatically create cut sections, details and schedules in addition to the orthographic projections and 3D models (wire frame or textured) for both the analysis and conservation of historic objects, structures and environments. © 2012 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author_keywords={Architecture;  Building;  CAD;  Cultural heritage;  Modelling;  Software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chrysos2013,
author={Chrysos, G. and Dagritzikos, P. and Papaefstathiou, I. and Dollas, A.},
title={HC-CART: A parallel system implementation of data mining classification and regression tree (CART) algorithm on a multi-FPGA system},
journal={Transactions on Architecture and Code Optimization},
year={2013},
volume={9},
number={4},
doi={10.1145/2400682.2400706},
art_number={47},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872946632&doi=10.1145%2f2400682.2400706&partnerID=40&md5=f94f17e61e0c8f0064f6ec8769141b38},
abstract={Data mining is a new field of computer science with a wide range of applications. Its goal is to extract knowledge from massive datasets in a human-understandable structure, for example, the decision trees. In this article we present an innovative, high-performance, system-level architecture for the Classification And Regression Tree (CART) algorithm, one of the most important and widely used algorithms in the data mining area. Our proposed architecture exploits parallelism at the decision variable level, and was fully implemented and evaluated on a modern high-performance reconfigurable platform, the Convey HC-1 server, that features four FPGAs and a multicore processor. Our FPGA-based implementation was integrated with the widely used "rpart" software library of the R project in order to provide the first fully functional reconfigurable system that can handle real-world large databases. The proposed system, named HC-CART system, achieves a performance speedup of up to two orders of magnitude compared to well-known singlethreaded data mining software platforms, such as WEKA and the R platform. It also outperforms similar hardware systems which implement parts of the complete application by an order of magnitude. Finally, we show that the HC-CART system offers higher performance speedup than some other proposed parallel software implementations of decision tree construction algorithms. © 2013 ACM.},
author_keywords={Classification and regression tree (CART);  Decision tree classification (DTC);  High-performance computing;  HW architecture;  R project;  Reconfigurable system;  Rpart library;  WEKA platform},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dong201345,
author={Dong, S. and Behzadan, A.H. and Chen, F. and Kamat, V.R.},
title={Collaborative visualization of engineering processes using tabletop augmented reality},
journal={Advances in Engineering Software},
year={2013},
volume={55},
pages={45-55},
doi={10.1016/j.advengsoft.2012.09.001},
note={cited By 84},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868582172&doi=10.1016%2fj.advengsoft.2012.09.001&partnerID=40&md5=8204a0fa0f2169420cc52e60bebaa8a3},
abstract={3D computer visualization has emerged as an advanced problem-solving tool for engineering education and practice. For example in civil engineering, the integration of 3D/4D CAD models in the construction process helps to minimize the misinterpretation of the spatial, temporal, and logical aspects of construction planning information. Yet despite the advances made in visualization, the lack of collaborative problem-solving abilities leaves outstanding challenges that need to be addressed before 3D visualization can become widely accepted in the classroom and in professional practice. The ability to smoothly and naturally interact in a shared workspace characterizes a collaborative learning process. This paper introduces tabletop Augmented Reality to accommodate the need to collaboratively visualize computer-generated models. A new software program named ARVita is developed to validate this idea, where multiple users wearing Head-Mounted Displays and sitting around a table can all observe and interact with dynamic visual simulations of engineering processes. The applications of collaborative visualization using Augmented Reality are reviewed, the technical implementation is covered, and the program's underlying tracking libraries are presented. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={Collaboration;  Planar tracking library;  Simulation;  Tabletop augmented reality;  Validation;  Visualization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yoshida20123994,
author={Yoshida, H. and Wu, Y. and Cai, W. and Brett, B.},
title={Scalable, high-performance 3D imaging software platform: System architecture and application to virtual colonoscopy},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2012},
pages={3994-3997},
doi={10.1109/EMBC.2012.6346842},
art_number={6346842},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870841436&doi=10.1109%2fEMBC.2012.6346842&partnerID=40&md5=bd588fbf439203dff984e82966e9acf3},
abstract={One of the key challenges in three-dimensional (3D) medical imaging is to enable the fast turn-around time, which is often required for interactive or real-time response. This inevitably requires not only high computational power but also high memory bandwidth due to the massive amount of data that need to be processed. In this work, we have developed a software platform that is designed to support high-performance 3D medical image processing for a wide range of applications using increasingly available and affordable commodity computing systems: multi-core, clusters, and cloud computing systems. To achieve scalable, high-performance computing, our platform (1) employs size-adaptive, distributable block volumes as a core data structure for efficient parallelization of a wide range of 3D image processing algorithms; (2) supports task scheduling for efficient load distribution and balancing; and (3) consists of a layered parallel software libraries that allow a wide range of medical applications to share the same functionalities. We evaluated the performance of our platform by applying it to an electronic cleansing system in virtual colonoscopy, with initial experimental results showing a 10 times performance improvement on an 8-core workstation over the original sequential implementation of the system. © 2012 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ishak20121337,
author={Ishak, M.K. and Herrmann, G. and Pearson, M.},
title={FPGA implementation of a simple approach for jitter minimisation in Ethernet for real-time control communication},
journal={Proceedings of the 14th IEEE International Conference on High Performance Computing and Communications, HPCC-2012 - 9th IEEE International Conference on Embedded Software and Systems, ICESS-2012},
year={2012},
pages={1337-1343},
doi={10.1109/HPCC.2012.197},
art_number={6332333},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870406120&doi=10.1109%2fHPCC.2012.197&partnerID=40&md5=d0018f75d29ff51e6b82fce8d481b9a1},
abstract={An approach for cheap and deterministic control communication using Ethernet real-time control communication is presented. Field-programmable gate array (FPGA) technology, i.e Xilinx XC3S500E from the Spartan-3E family, is used to implement the Ethernet communication strategy. The unit is defined in Verilog using Xilinx ISE 11.1 software tools. Data packages are sent at well defined times to avoid collisions. Collisions mainly occur due to jitter of the transmitter system, so that arbitration (similar to CANopen) is necessary. The Binary Exponential Backoff (BEB) scheme is used. This paper analyzes and investigates how the backoff time affects the performance of the Carrier Sense Multiple Access protocol with Collision Detection (CSMA/CD) in a basic Media Access Controller (MAC), in terms of data arrival characteristics, i.e jitter and delay. We propose to assign different minimal backoff times for each of the CSMA/CD controller units to minimize packet collisions. The proposed hardware design shows the advantage of our approach over a standard CSMA/CD setting. © 2012 IEEE.},
author_keywords={Binary Exponential Backoff;  CSMA/CD;  Ethernet;  network model},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Sarkar201285,
author={Sarkar, N.I. and McHaney, R.},
title={Modeling and simulation of IEEE 802.11 wireless LANs: A case study of a network simulator},
journal={Simulation in Computer Network Design and Modeling: Use and Analysis},
year={2012},
pages={85-99},
doi={10.4018/978-1-4666-0191-8.ch005},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898574872&doi=10.4018%2f978-1-4666-0191-8.ch005&partnerID=40&md5=8bde3008afcecc71bc7837d2a0289305},
abstract={Stochastic discrete event simulation methodology is becoming increasingly popular among network researchers worldwide in recent years. This popularity results from the availability of various sophisticated and powerful simulation software packages, and also because of the flexibility in model construction and validation offered by simulation. In this chapter, the authors describe their experience in using the network simulator 2 (ns-2), a discrete event simulation package, as an aid to modeling and simulation of the IEEE 802.11 Wireless Local Area Networks (WLANs). This chapter provides an overview of ns-2 focusing on simulation environment, architecture, model development and parameter setting, model validation, output data collection and processing, and simulation execution. The strengths and weaknesses of ns-2 are discussed. This chapter also emphasizes that selecting a good simulator is crucial in modeling and performance analysis of wireless networks. © 2012, IGI Global.},
document_type={Book Chapter},
source={Scopus},
}

@BOOK{Jabin2012239,
author={Jabin, S. and Mustafa, K.},
title={A survey of semantic web based architectures for adaptive intelligent tutoring system},
journal={Multidisciplinary Computational Intelligence Techniques: Applications in Business, Engineering, and Medicine},
year={2012},
pages={239-256},
doi={10.4018/978-1-4666-1830-5.ch015},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898114064&doi=10.4018%2f978-1-4666-1830-5.ch015&partnerID=40&md5=6a45f9962ebb1201a6fad8a71320248b},
abstract={Most recently, IT-enabled education has become a very important branch of educational technology. Education is becoming more dynamic, networked, and increasingly electronic. Today's is a world of Internet social networks, blogs, digital audio and video content, et cetera. A few clear advantages of Web-based education are classroom independence and availability of authoring tools for developing Web-based courseware, cheap and efficient storage and distribution of course materials, hyperlinks to suggested readings, and digital libraries. However, there are several challenges in improving Webbased education, such as providing for more adaptivity and intelligence. The main idea is to incorporate Semantic Web technologies and resources to the design of artificial intelligence in education (AIED) systems aiming to update their architectures to provide more adaptability, robustness, and richer learning environments. The construction of such systems is highly complex and faces several challenges in terms of software engineering and artificial intelligence aspects. This chapter addresses state of the art Semantic Web methods and tools used for modeling and designing intelligent tutoring systems (ITS). Also it draws attention of Semantic Web users towards e-learning systems with a hope that the use of Semantic Web technologies in educational systems can help the accomplishment of anytime, anywhere, anybody learning, where most of the web resources are reusable learning objects supported by standard technologies and learning is facilitated by intelligent pedagogical agents, that may be adding the essential instructional ingredients implicitly. © 2012, IGI Global.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Gresil201291,
author={Gresil, M. and Giurgiutiu, V. and Shen, Y. and Poddar, B.},
title={Guidelines for using the finite element method for modeling guided Lamb wave propagation in SHM processes},
journal={Proceedings of the 6th European Workshop - Structural Health Monitoring 2012, EWSHM 2012},
year={2012},
volume={1},
pages={91-98},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894768438&partnerID=40&md5=9ac170528872d548d012f3eb6c55ba26},
abstract={The aim of the work presented in this paper is to provide guidelines for extending the modeling capacities and improve quality and reliability of 2-D guided wave propagation models using commercially available finite element method (FEM) packages. Predictive simulation of ultrasonic nondestructive evaluation (NDE) and structural health monitoring (SHM) in realistic structures is challenging. Analytical methods can perform efficiently modeling of wave propagation are limited to simple geometries. Realistic structures with complicated geometries are usually modeled with the finite element method (FEM). Commercial FEM codes offer convenient built-in resources for automated meshing, frequency analysis, as well as time integration of dynamic events. We propose to develop FEM guidelines for 2-D Lamb wave propagation with a high level of accuracy. The proposed 2-D guided wave problem will be the pitch-catch arrangement in a full 3-D geometry plate involving guided waves between a transmitter piezoelectric wafer active sensor (PWAS) and receiver P WAS. In addition, corrosion damage is added to this problem to simulate the detection of damage, and assess the detectability threshold. The general approach is to run a series of FEM models. These FEM models will be compared with the experimental data and with our 1-D analytical homemade software.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brand2012,
author={Brand, W.},
title={Two approaches to one task: A historical case study of the implementation and deployment of two software packages for the design of light-weight structures in architecture and civil engineering},
journal={AISB/IACAP World Congress 2012: Symposium on the History and Philosophy of Programming, Part of Alan Turing Year 2012},
year={2012},
page_count={8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893161194&partnerID=40&md5=ef24b3f1c043cc10f2b79c50c1084843},
abstract={The advent of powerful computers during the 1960s enabled architects and civil engineers for the first time to design and construct light-weight structures never dreamed of before. This historical case study describes the implementation and deployment of two software packages for the design of light-weight structures at the University of Stuttgart, Germany in the context of the software engineering and hardware technology around 1970. Both software packages were used to design the tent-shaped membrane roof of the Munich Olympic Stadium for the 1972 Olympic Games. One software package (ASKA) was based on the Finite Element Method (FEM), the other package relied on the newly developed Force Density Method (FDM). The ASKA package had been under development since the early 1960s, whereas the development of the second package had just started. This environment led to two different design processes and to a rather limited interaction between the two groups carrying out the work. Both applications proved to be successful in creating light-weight structures and the fate of both software packages is traced until today. The design of the two software packages was influenced by the philosophy of structuring problems in a way appropriate to specific high performance computer architecture's. Today, this paradigm is still at the core of software engineering for supercomputers.},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Chakrabarty2012103,
author={Chakrabarty, B.K.},
title={Computer-aided design and computer graphics},
journal={Computer Aided Design: Technology, Types and Practical Applications},
year={2012},
pages={103-149},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891984741&partnerID=40&md5=dd8518717918b8a12d218fd0ba3952d9},
abstract={Computer Aided Design (CAD) is a vital application area of the all-pervasive Information Technology Revolution influencing the way we live and work, shrinking the effects of time and distance, and altering the very nature of work in an organization [20,21,27]. Modern computers (even a PC) can generate and process information on a scope never before possible. Such information, generated in tables of numbers of hundreds of pages long, may be of no worth to a human user unable to comprehend them, but may be very meaningful if presented graphically [11,26, 29, 33,34]. Computer graphics, which is a study of techniques to improve communication between human and machine, is an important component of CAD [11,19,29, 33,34]. Adoption of CAD in various fields has accelerated with recent advances in microcomputer technology, steady decline in cost of computers, and tremendous increase in their computing power and graphic capabilities [11,15,19]. This is equally applicable in Architecture (including Planning), Civil-Building Engineering, and Construction (usually classified as A-E-C) field [11,15,24,29]. Real utility of Computer Aided Design (CAD) is not confined to just producing sophisticated solid geometry's or giving more stress to cosmetic such as producing neater, more attractive drawing packages only [11,15,19,29]. It should cover many other aspects of design adopting an Integrated Computer-Aided Design Approach (covering all its components including graphics, drafting, geometric modeling, optimization and so on), involving hundreds of variables, to enhance Productivity, Social-Responsiveness, and other benefits, particularly in Urban Development and Management, creating scope for informed public-participation in the decision-making process in such operations [9,10,19,21, 29]. The chapter highlights planning and designs as a problem solving process and the vital role of computers and graphics in this process. It outlines the concept of Integrated Computer Aided Design; it's components, and the utility and application of computer graphics in geometric modeling, design analysis, design optimization [1], and in architectural and engineering drafting. It gives illustrative graphic application results of Integrated Computer Aided Design (ICAD) Software developed by the author for design analysis and design optimization in integrated computer-aided structural design and drafting, and integrated computer-aided building design and drafting [2-10]. It presents a case study of participatory planning and design using a Descriptive Model in graphic form for urban development planning policy analysis. It gives an example application of graphics programming using AutoLISP for Computer-Aided Design and Drafting (CADD) of urban layout modules. The need for universal graphic standard programming like GKS [11,19,34] in the AEC sector is stressed, which will greatly facilitate application of Integrated Computer Aided Design in this sector to derive various benefits including Efficiency and Effectiveness. To make such applications feasible, a compulsory addition of "Computer Aided Planning and Design by Optimization", as a full-length subject of study (suggestive course-content in Appendix) for the AEC Professional Degree Curriculum, is suggested. This will help equip AEC Professionals, who commit huge Resources in their Planning and Design Decisions enjoining on them an ethical responsibility for efficient and equitable use of resources so committed, to develop and apply such techniques to discharge their Accountability for a Resource-Efficient and Equitable problem solution in this sector. © 2012 by Nova Science Publishers, Inc. All rights reserved.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Cao2012510,
author={Cao, L. and Li, X. and Tacher, L. and Yang, R.},
title={Three-dimensional geological modeling based on division of engineering geological strata units},
journal={Advanced Science Letters},
year={2012},
volume={5},
number={2},
pages={510-514},
doi={10.1166/asl.2012.1991},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880810195&doi=10.1166%2fasl.2012.1991&partnerID=40&md5=33f263a960c15fd08ae995413354c487},
abstract={It is well known that the correct division of engineering geological strata units is the basis of generalization for geological modeling, as well as site quality evaluation for underground construction. Such a work could also be helpful to show the geological structures more clearly, identify the primary engineering geological problems and study representative geotechnical parameters. With ongoing geological survey in Suzhou City, engineering geological strata units of soil in eastern plains area are divided. Additionally, key bearing strata and weak-sensitive strata are also indentified. Eventually the three-dimensional geological model is established by self-developed software package GEOLEP-3D for stereo visualization, which is based on 665 representative engineering boreholes in the working area. © 2012 American Scientific Publishers All rights reserved.},
author_keywords={Engineering geological strata units;  Geological modeling;  Suzhou eastern plains area},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mcgee2012295,
author={Mcgee, W. and Pigram, D. and Kaczynski, M.P.},
title={Robotic reticulations A method for the integration of multi-axis fabrication processes with algorithmic form-finding techniques},
journal={Beyond Codes and Pixels - Proceedings of the 17th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2012},
year={2012},
pages={295-304},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875274197&partnerID=40&md5=e9a5c8a383463be57b74e94ffe6deb5c},
abstract={This paper addresses the design and fabrication of non-uniform structural shell systems. Structural shells, particularly gridshells, have a long history but due to their complexity and the accompanying high cost of construction, their application has been limited. The research proposes a method for integrating the design and fabrication processes such that complex double curved reticulated frames can be constructed efficiently, from prefabricated components, requiring significantly less formwork than is typical. A significant aspect of the method has been the development of software tools that allow for both algorithmic formfinding and the direct control of robotic fabrication equipment from within the same modelling package. A recent case-study is examined where the methodology has been applied to construct a reticulated shell structure in the form of a partial vault. Components were prefabricated using 6-axis robotic fabrication equipment. Individual parts are designed such that the assembly of components guides the form of the vault, requiring no centring to create the desired shape. Algorithmically generated machine instructions controlled a sequence of three tool changes for each part, using a single modular fixture, greatly increasing accuracy. The complete integration of computational design techniques and fabrication methodologies now enables the economical deployment of non-uniform structurally optimised reticulated frames. © 2012, Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.},
author_keywords={Computational design;  Dynamic relaxation;  Form-finding;  Reticulated frame;  Robotic fabrication},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Franchini2012419,
author={Franchini, S. and Gentile, A. and Sorbello, F. and Vassallo, G. and Vitabile, S.},
title={A dual-core coprocessor with native 4D Clifford algebra support},
journal={Proceedings - 15th Euromicro Conference on Digital System Design, DSD 2012},
year={2012},
pages={419-422},
doi={10.1109/DSD.2012.2},
art_number={6386920},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872915698&doi=10.1109%2fDSD.2012.2&partnerID=40&md5=cb63fc50464cf62f091104e4c028432f},
abstract={Geometric or Clifford Algebra (CA) is a powerful mathematical tool that is attracting a growing attention in many research fields such as computer graphics, computer vision, robotics and medical imaging for its natural and intuitive way to represent geometric objects and their transformations. This paper introduces the architecture of CliffordCoreDuo, an embedded dual-core coprocessor that offers direct hardware support to four-dimensional (4D) Clifford algebra operations. A prototype implementation on an FPGA board is detailed. Experimental results show a 1.6x average speedup of CliffordCoreDuo in comparison with the baseline mono-core architecture. A potential cycle speedup of about 40x over Gaigen 2, a geometric algebra software library generator for general-purpose processors, is also demonstrated. © 2012 IEEE.},
author_keywords={Clifford algebra;  embedded coprocessors;  FPGA prototyping;  medical imaging;  multi-core architectures},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao20122939,
author={Zhao, X.-Y. and Chen, X.-F. and Gui, W.-H.},
title={EBE-based parallel finite element analysis of electric field in aluminum reduction cell},
journal={Proceedings of the World Congress on Intelligent Control and Automation (WCICA)},
year={2012},
pages={2939-2943},
doi={10.1109/WCICA.2012.6358373},
art_number={6358373},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872352147&doi=10.1109%2fWCICA.2012.6358373&partnerID=40&md5=98b2910511e72034273d6a7b153b8611},
abstract={Finite element analysis for large-scale complicated structure such as aluminum reduction cell makes higher demand on memory capacity and calculation speed, which often results in failure or inefficiency of traditional serial computation for such large-scale problems. Finite element EBE-PCG algorithm is proposed on the basis of EBE(Element-By-Element) idea and Jacobi preconditioned conjugate gradient(PCG) method. The main difficulties in parallel implementation of this algorithm are discussed and solved, such as data organization of grid model, mixed elements processing and data communication. Subsequently, parallel program of finite element analysis based on EBE-PCG is developed using C language and MPI standard library, and then applied to numeric simulation of electric field distribution in aluminum reduction cell. The computational accuracy of parallel program developed is verified through comparison with commercial finite element software ANSYS. Experiment results show that the method is of very high parallel efficiency and can greatly shorten the calculation time, which indicates the effectiveness of EBE's use in parallel computation of large-scale complicated structures. © 2012 IEEE.},
author_keywords={aluminum reduction cell;  EBE;  electric field;  finite element analysis;  parallel computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ariño2012,
author={Ariño, J. and Murga, G. and Campo, R. and Eletxigerra, I. and Ampuero, P.},
title={Building information models for astronomy projects},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2012},
volume={8449},
doi={10.1117/12.926180},
art_number={84490D},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871704254&doi=10.1117%2f12.926180&partnerID=40&md5=1f8f247d4b6ca8bdffe190d569226fcd},
abstract={A Building Information Model is a digital representation of physical and functional characteristics of a building. BIMs represent the geometrical characteristics of the Building, but also properties like bills of quantities, definition of COTS components, status of material in the different stages of the project, project economic data, etc. The BIM methodology, which is well established in the Architecture Engineering and Construction (AEC) domain for conventional buildings, has been brought one step forward in its application for Astronomical/Scientific facilities. In these facilities steel/concrete structures have high dynamic and seismic requirements, M&E installations are complex and there is a large amount of special equipment and mechanisms involved as a fundamental part of the facility. The detail design definition is typically implemented by different design teams in specialized design software packages. In order to allow the coordinated work of different engineering teams, the overall model, and its associated engineering database, is progressively integrated using a coordination and roaming software which can be used before starting construction phase for checking interferences, planning the construction sequence, studying maintenance operation, reporting to the project office, etc. This integrated design & construction approach will allow to efficiently plan construction sequence (4D). This is a powerful tool to study and analyze in detail alternative construction sequences and ideally coordinate the work of different construction teams. In addition engineering, construction and operational database can be linked to the virtual model (6D), what gives to the end users a invaluable tool for the lifecycle management, as all the facility information can be easily accessed, added or replaced. This paper presents the BIM methodology as implemented by IDOM with the E-ELT and ATST Enclosures as application examples. © 2012 SPIE.},
author_keywords={4D;  6D;  ATST;  BIM;  Design for construction;  E-ELT;  Virtual construction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20127843,
author={Wang, W.D. and Chang, L.Y. and Xu, Z.H. and Weng, Q.P.},
title={Numerical analysis of a large-scale and deep cylindrical excavation in Shanghai soft deposit},
journal={ECCOMAS 2012 - European Congress on Computational Methods in Applied Sciences and Engineering, e-Book Full Papers},
year={2012},
pages={7843-7857},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871627203&partnerID=40&md5=1a03fcdade01df66fbb379be046a1359},
abstract={The Shanghai 500kV World Expo Underground Transmission and Substation situated in the center district of Shanghai city. The project involved the construction of a four levels cylindrical underground structure with a diameter of 130 m. The excavation depth for the project was 34 m and it was constructed by top-down method in difficult soft soil condition. This paper presents the numerical analysis of the multi-propped deep excavation with undrained effective stress analysis method. Firstly, the geotechnical profile at the construction site and arrangement of the supporting system were introduced. Secondly, an axisymmetric finite element model was setup in the PLAXIS software package. The model included the diaphragm wall, the horizontal props, and the layered soils. The horizontal props were modeled by spring elements, the stiffness of which was obtained through three-dimensional finite element analysis of the horizontal props. The HS-small model was used to present the behavior of the Shanghai soft clays. At last, comparisons between the numerical prediction and the measured results were reported and discussed. It shows that the calculated lateral displacements of the diaphragm wall, the ground surface settlements, the circumferential axial forces in the wall, and the lateral pressure acting on the wall matched well with the measured results.},
author_keywords={Deep cylindrical excavation;  FEM;  HS-small model;  Soft soil;  Undrained analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Adams201290,
author={Adams, A. and Jacobs, D.E. and Dolson, J. and Tico, M. and Pulli, K. and Talvala, E.-V. and Ajdin, B. and Vaquero, D. and Lensch, H.P.A. and Gelfand, N. and Matusik, W. and Horowitz, M. and Park, S.H. and Baek, J. and Levoy, M.},
title={The Frankencamera: An experimental platform for computational photography},
journal={Communications of the ACM},
year={2012},
volume={55},
number={11},
pages={90-98},
doi={10.1145/2366316.2366339},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869415034&doi=10.1145%2f2366316.2366339&partnerID=40&md5=d2a512a89b34e894718792b726d4a541},
abstract={Although there has been much interest in computational photography within the research and photography communities, progress has been hampered by the lack of a portable, programmable camera with sufficient image quality and computing power. To address this problem, we have designed and implemented an open architecture and application programming interface (API) for such cameras: the Frankencamera. It consists of a base hardware specification, a software stack based on Linux, and an API for C++. Our architecture permits control and synchronization of the sensor and image processing pipeline at the microsecond timescale, as well as the ability to incorporate and synchronize external hardware like lenses and flashes. This paper specifies our architecture and API, and it describes two reference implementations we have built. Using these implementations, we demonstrate several computational photography applications: high dynamic range (HDR) viewfinding and capture, automated acquisition of extended dynamic range panoramas, foveal imaging, and inertial measurement unit (IMU)-based hand shake detection. Our goal is to standardize the architecture and distribute Frankencameras to researchers and students, as a step toward creating a community of photographer-programmers who develop algorithms, applications, and hardware for computational cameras.© 2012 ACM.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Teubl201226,
author={Teubl, F. and Kurashima, C. and Cabral, M. and Zuffo, M.},
title={FastFusion: A scalable multi-projector system},
journal={Proceedings - 2012 14th Symposium on Virtual and Augmented Reality, SVR 2012},
year={2012},
pages={26-35},
doi={10.1109/SVR.2012.1},
art_number={6297557},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867826030&doi=10.1109%2fSVR.2012.1&partnerID=40&md5=d8069dd7acc53ce24d63910f809db182},
abstract={Multi-projector systems offer both higher resolution and brightness by using a cluster of projectors, and it can provide better visual quality when compared to traditional systems using a single high performance projector. When we consider the high cost associated with high-end projectors, the use of multiple low cost projectors can reduce considerably the cost of such installation. This article presents the research and development of a scalable multi-projection system that enables the construction of virtual reality systems with a large number of projectors and graphics computers, and that is capable of achieving a high resolution display. We demonstrate the viability of such system with the development of a camera-based multi-projector system called FastFusion, which automatically calibrates casually aligned projectors to properly blend different projections. Our system software improves known algorithms in the literature for projector calibration and image blending. The main improvement is a more efficient distribution of the calibration process. In addition, since our library proposes a new architecture that is able to manage many projectors, it may lead to the development of Immersion Systems with retina resolution. FastFusion has been tested and validated by virtual reality applications. In this work, we analyze the visual performance of FastFusion in a CAVE system with three walls, eighteen projectors and nine computers. © 2012 IEEE.},
author_keywords={Multi-projector System;  Virtual Reality;  Visual Computer},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lilis20121,
author={Lilis, Y. and Savidis, A.},
title={Implementing reusable exception handling patterns with compile-time metaprogramming},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7527 LNCS},
pages={1-15},
doi={10.1007/978-3-642-33176-3_1},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867655908&doi=10.1007%2f978-3-642-33176-3_1&partnerID=40&md5=e9cc1c66b685b38d57e0cfd95ed79b27},
abstract={We investigate in depth the adoption of compile-time metaprogramming to implement exception handling patterns. It is based on logic that is executed at compile-time and outputs source fragments which substitute the meta-code before compilation. Exception patterns are realized as metafunctions capable to transparently generate the invocation details and the appropriate exception handling layout. This way, programmers are relieved from underlying exception handling details, while the handling patterns can be standardized and directly reused. Pattern libraries of directly editable code are enabled, while the adoption of compile-time metaprogramming allows configuring the pattern deployment within the original client source based on application requirements. We examine key exception handling scenarios and we implement them as configurable and reusable pattern libraries in an existing meta-language. © 2012 Springer-Verlag.},
author_keywords={compile-time metaprogramming;  design patterns;  Exception handling patterns;  pattern implementation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tang20122091,
author={Tang, G. and Xuan, H.},
title={Research on measurement of software package dependency based on component},
journal={Journal of Software},
year={2012},
volume={7},
number={9},
pages={2091-2098},
doi={10.4304/jsw.7.9.2091-2098},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867432230&doi=10.4304%2fjsw.7.9.2091-2098&partnerID=40&md5=f9e0f59ae398e744595513cc69cc9efa},
abstract={Dependence between software packages is of importance to influence the extendibility and stability of system. In existing programs, dependence mainly manifests for class and component. Here, it has the important guiding sense to our system construction and programming. This paper analyzed the dependency between software packages, and designed the algorithm for detection existence of the package dependency loop, based on defined dependency, stability, no-responsibility and stability of the System; and then elevated the package design principles. To validate our design methodology in software development, which is valid and can be helpful for the programmers, we developed a software to analyze the dependencies between the software packages and use a graphical method to express this dependency. © 2012 ACADEMY PUBLISHER.},
author_keywords={Component;  Dependency;  Dependency loop;  No-responsibility;  Stability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Samer2012721,
author={Samer, P. and Sampaio, A.H. and Milanés, A. and Urrutia, S.},
title={Designing a multicore graph library},
journal={Proceedings of the 2012 10th IEEE International Symposium on Parallel and Distributed Processing with Applications, ISPA 2012},
year={2012},
pages={721-728},
doi={10.1109/ISPA.2012.106},
art_number={6280366},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867281645&doi=10.1109%2fISPA.2012.106&partnerID=40&md5=2efd8dcfe9cb33e19db77d3204584681},
abstract={Graph Theory provides a set of powerful tools (both theorems and algorithms) for problem modeling and solving in numerous domains. Though there are several libraries implementing graph algorithms and targeting different platforms and users, few of those offer parallel implementations. To the best of our knowledge, there is a particular need for an easier to use and extend library, specifically designed to exploit the multicore architecture trend for high performance parallelism. In this paper we describe Magical, a new OpenMP-based C++ multicore graph library. Our focus is to provide an implementation of graph algorithms which is designed for multicore architectures, by means of an easy to use application programming interface. We describe the library design and evaluate its performance by means of a case study concerning a shortest-paths problem. © 2012 IEEE.},
author_keywords={graph;  library;  magical;  multicore;  parallel},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kushal2012499,
author={Kushal, K.S. and Muttanna Kadal, H.K. and Chetan, S. and Shivaputra},
title={Design and implementation of a RFID based prototype SmArt LibrARY (SALARY) system using wireless sensor networks},
journal={Advances in Intelligent and Soft Computing},
year={2012},
volume={167 AISC},
number={VOL. 2},
pages={499-505},
doi={10.1007/978-3-642-30111-7_47},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865172939&doi=10.1007%2f978-3-642-30111-7_47&partnerID=40&md5=e906f4271573fa4ae6590911bb9edfe6},
abstract={"There is a great deal of difference between an eager man who wants to read a book and a tired man who wants a book to read" - G.K. Chesterton With the colossal collection of traditional and digital including books, journals, audio materials, photographs, e-journals, e-books, web resources and more in recent years, finding an o'nymous item is becoming more and more difficult, resulting in a number of practical conflicts. The accessing and the procurement of the details pertaining to a publication is becoming an ubiquitous problem. Widespread use of wireless technologies paired with the recent advances in the wireless applications, manifests that digital data dissemination could be the key to solve emerging problems. Wireless Sensor Network technology has attracted increased attention and is rapidly emerging due to their enormous application potential in diverse fields. This buoyant field is expected to provide an efficient and cost-effective solution to the effluent problems. This paper proposes a SmArt LibrARY (SALARY) System based on wireless sensor network technology which provides advanced features like automatic update in the addition/deletion of a publication, automated guidance, and item reservation mechanism. The paper describes the overall system architecture of SALARY from hardware to software implementation in the view point of sensor networks. We implemented a full-fledged prototype system for library management to realize the design functionalities and features mentioned. Our preliminary test results show that the performance of this WSN based system can effectively satisfy the needs and requirements of existing integrated library system hassles thereby minimizing the time consumed to find the slot of thou publication, real-time information rendering, and smart reservation mechanisms. © 2012 Springer-Verlag GmbH.},
author_keywords={Context Aware;  Java ME(J2ME);  Radio Frequency Identification(RFID);  Wireless Sensor Networks(WSN)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Brewer2012,
author={Brewer, J. and Popp, T. and Perrin, J.},
title={REDDNET and digital preservation in the open cloud: Research at Texas Tech University libraries on long-term archival storage},
journal={Journal of Digital Information},
year={2012},
volume={13},
number={1},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864686051&partnerID=40&md5=97988443a30de1d6f11d66d3cc99b7fe},
abstract={In the realm of digital data, vendor-supplied cloud systems will still leave the user with responsibility for curation of digital data. Some of the very tasks users thought they were delegating to the cloud vendor may be a requirement for users after all. For example, cloud vendors most often require that users maintain archival copies. Beyond the better known vendor cloud model, we examine curation in two other models: inhouse clouds, and what we call "open" clouds which are neither inhouse clouds nor vendor supported clouds. In open clouds, users come aboard as participants or partners for example, by being invited to participate in development or in hosting hardware. In open cloud systems users can develop their own software and data management, control access, and purchase their own hardware while running securely in the cloud environment. To do so will still require working within the rules of the cloud system, but in some open cloud systems those restrictions and limitations can be walked around easily with surprisingly little loss of freedom. It is in this context that REDDnet (Research and Education Data Depot network) is presented as the place where the Texas Tech University (TTU)) Libraries have been conducting research on long-term digital archival storage. The REDDnet network by year's end will be at 1.2 petabytes (PB) with an additional 1.4 PB for a related project (Compact Muon Soleniod Heavy Ion [CMS-HI]); additionally there are over 200 TB of tape storage. These numbers exclude any disk space which TTU will be purchasing during the year. National Science Foundation (NSF) funding covering REDDnet and CMS-HI was in excess of $850,000 with $850,000 earmarked toward REDDnet. In the terminology we used above, REDDnet is an open cloud system that invited TTU Libraries to participate. This means that we run software which fits the REDDnet structure. We are beginning the final design of our system, and moving into the first stages of construction. And we have made a decision to move forward to purchase one-half PB of disk storage in the initial phase. The concerns, deliberations and testing are presented here along with our initial approach.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hahn201244,
author={Hahn, M.},
title={OpenAccess: Standard and practices},
journal={IEEE Design and Test of Computers},
year={2012},
volume={29},
number={2},
pages={44-52},
doi={10.1109/MDT.2012.2186277},
art_number={6148306},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864228838&doi=10.1109%2fMDT.2012.2186277&partnerID=40&md5=6b3cc7dd33d7a69d41353ae2ef057c98},
abstract={The OpenAccess data enables a variety of tools used for mixed-signal chip designs, spanning custom design, digital implementation, chip integration and finishing, and chip/package codesign, as well as emerging technologies like 3DIC. One part of the OpenAccess data model covers design data, including structural netlist connectivity, hierarchy, placement, and routing. First, a feature called Embedded Module Hierarchy (EMH) unifies and automatically maintains consistency between three different domains. When an application creates a bus net, OpenAccess automatically creates individual nets for each of the bits of the bus, and then maintains the relationship between the two during subsequent editing. In addition to representing design data specific to each chip, OpenAccess plays a central role in Process Design Kits (PDKs), capturing fundamental aspects of each process technology. The latest releases of OpenAccess include two major enhancements, support for multithreaded applications, and compression.},
author_keywords={computer-aided design;  interoperability;  object-oriented databases;  reusable software;  software engineering;  software libraries;  Standards},
document_type={Article},
source={Scopus},
}

@ARTICLE{Villar2012216,
author={Villar, N. and Scott, J. and Hodges, S. and Hammil, K. and Miller, C.},
title={NET gadgeteer: A platform for custom devices},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7319 LNCS},
pages={216-233},
doi={10.1007/978-3-642-31205-2_14},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864210270&doi=10.1007%2f978-3-642-31205-2_14&partnerID=40&md5=094c791936d1750c3507f76bb33bba5f},
abstract={.NET Gadgeteer is a new platform conceived to make it easier to design and build custom electronic devices and systems for a range of ubiquitous and mobile computing scenarios. It consists of three main elements: solder-less modular electronic hardware; object-oriented managed software libraries accessed using a high-level programming language and established development environment; and 3D design and construction tools designed to facilitate a great deal of control over the form factor of the resulting electronic devices. Each of these elements is designed to be accessible to a wide range of people with varying backgrounds and levels of experience and at the same time provide enough flexibility to allow experts to build relatively sophisticated devices and complex systems in less time than they are used to. In this paper we describe the .NET Gadgeteer system in detail for the first time, explaining a number of key design decisions and reporting on its use by new users and experts alike. © 2012 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cao2012279,
author={Cao, Y. and Chen, W. and Yu, L.},
title={Control and simulation of a novel permanent magnet brushless DC wheel motor based on finite element method},
journal={International Journal of Advancements in Computing Technology},
year={2012},
volume={4},
number={13},
pages={279-286},
doi={10.4156/ijact.vol4.issue 13.32},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865343386&doi=10.4156%2fijact.vol4.issue+13.32&partnerID=40&md5=fd94beb61c4cab079b9ee700ca3608c2},
abstract={The electric vehicles have obtained more and more attention from researchers due to their environmental friendliness and high efficiency. The axial-field permanent-magnet (AFPM) brushless DC motor with its compact construction and high power density is very suitable for the in-wheel motor used for electric vehicles. In this paper, the structure and the operation principle of the AFPM brushless DC motor are discussed. Transient field and circuit coupled method is used to model and analyze the operation of the motor based on Ansoft/Maxwell, which is a finite-element package software.},
author_keywords={Axial-field permanent magnet(AFPM);  Brushless DC motor;  Finite element method},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tych201219,
author={Tych, W. and Young, P.C.},
title={A Matlab software framework for dynamic model emulation},
journal={Environmental Modelling and Software},
year={2012},
volume={34},
pages={19-29},
doi={10.1016/j.envsoft.2011.08.008},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859108257&doi=10.1016%2fj.envsoft.2011.08.008&partnerID=40&md5=93561775a6192a74640086795ba65f40},
abstract={The paper describes a software framework for implementing the main stages of the Data Based Mechanistic (DBM) modelling approach to the reduced order emulation (meta-modelling) of large dynamic system computer models, within the Matlab software environment. The framework exploits routines in the CAPTAIN Toolbox to identify and estimate transfer function models that reflect the dominant modes of the dynamic behaviour in the large model. This allows for the 'nominal emulation' and validation of the large model for a single, specified set of parameters; as well as 'stand-alone, full emulation' based on the construction and validation of hyper-dimensional maps between a user-specified range of large model parameters and the parameters of the associated, low order transfer function models. The software framework uses the multivariable structure constructs available within Matlab ™ to form a small library of routines that will become part of the Captain Toolbox. The library is formed around special data structures that facilitate multivariable operations and visualisations which both enhance the efficiency of the emulation modelling analysis and the modeller's interaction with the process of emulation. The nature of the analysis is illustrated by a topical example concerned with the emulation of the OTIS computer simulation model for the transport and dispersion of solutes in a river system. © 2011 Elsevier Ltd.},
author_keywords={DACE;  DBM;  Dominant modes;  Emulation;  Matlab;  Transfer function},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chevitarese201230,
author={Chevitarese, D.S. and Szwarcman, D. and Vellasco, M.},
title={Speeding up the training of neural networks with CUDA technology},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7267 LNAI},
number={PART 1},
pages={30-38},
doi={10.1007/978-3-642-29347-4_4},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861065075&doi=10.1007%2f978-3-642-29347-4_4&partnerID=40&md5=c9d18ef5c38c43fcadaa458f4d91a667},
abstract={Training feed-forward neural networks can take a long time when there is a large amount of data to be used, even when training with more efficient algorithms like Levenberg-Marquardt. Parallel architectures have been a common solution in the area of high performance computing, since the technology used in current processors is reaching the limits of speed. An architecture that has been gaining popularity is the GPGPU (General-Purpose computing on Graphics Processing Units), which has received large investments from companies such as NVIDIA that introduced CUDA (Compute Unified Device Architecture) technology. This paper proposes a faster implementation of neural networks training with Levenberg-Marquardt algorithm using CUDA. The results obtained demonstrate that the whole training time can be almost 30 times shorter than code using Intel Math Library (MKL). A case study for classifying electrical company customers is presented. © 2012 Springer-Verlag Berlin Heidelberg.},
author_keywords={Artificial Neural Networks;  CUDA;  GPGPU;  High Performance Computing;  Software Engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li20121117,
author={Li, C. and Liu, L. and Li, X.},
title={Software networks of Java class and application in fault localization},
journal={Proceedings - 2012 International Conference on Intelligent Systems Design and Engineering Applications, ISDEA 2012},
year={2012},
pages={1117-1120},
doi={10.1109/ISdea.2012.403},
art_number={6173401},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861011635&doi=10.1109%2fISdea.2012.403&partnerID=40&md5=b7778c0bb63379d8fbaef05ec035fee3},
abstract={Complex networks are backbones of complex systems. A lot of empirical analysis demonstrates that software is kind of artificial complex systems that expose the small-world effects and follow scale-free degree distribution. Here analyzed is Java class complex networks construction in binary file with BCEL (Byte Code Engineering Library) of Apache Jakarta Project. This method needn't source code to generate networks, so it can extract all java software's inner structure to assist programmer to understand software macroscopic features. Existing lectures almost verified variety of software networks are complex networks, and some investigators have concluded software networks cannot yet produce factual instruction in software engineering. Here we utilize log information in Java networks to diagnose software fault and exception. Experiments show that software networks can not only visualize the software structure, but also really instruct software fault localization. © 2012 IEEE.},
author_keywords={Complex Networks;  Fault Localization;  Java;  Software Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Espinosa2012,
author={Espinosa, E.G.},
title={Using configuration management and product line software paradigms to support the experimentation process in software engineering},
journal={Proceedings - International Conference on Research Challenges in Information Science},
year={2012},
doi={10.1109/RCIS.2012.6240454},
art_number={6240454},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864975186&doi=10.1109%2fRCIS.2012.6240454&partnerID=40&md5=386f969a6d11da81d85422175bc8ee21},
abstract={There is no empirical evidence whatsoever to support most of the beliefs on which software construction is based. We do not yet know the adequacy, limits, qualities, costs and risks of the technologies used to develop software. Experimentation helps to check and convert beliefs and opinions into facts. This research is concerned with the replication area. Replication is a key component for gathering empirical evidence on software development that can be used in industry to build better software more efficiently. Replication has not been an easy thing to do in software engineering (SE) because the experimental paradigm applied to software development is still immature. Nowadays, a replication is executed mostly using a traditional replication package. But traditional replication packages do not appear, for some reason, to have been as effective as expected for transferring information among researchers in SE experimentation. The trouble spot appears to be the replication setup, caused by version management problems with materials, instruments, documents, etc. This has proved to be an obstacle to obtaining enough details about the experiment to be able to reproduce it as exactly as possible. We address the problem of information exchange among experimenters by developing a schema to characterize replications. We will adapt configuration management and product line ideas to support the experimentation process. This will enable researchers to make systematic decisions based on explicit knowledge rather than assumptions about replications. This research will output a replication support web environment. This environment will not only archive but also manage experimental materials flexibly enough to allow both similar and differentiated replications with massive experimental data storage. The platform should be accessible to several research groups working together on the same families of experiments. © 2012 IEEE.},
author_keywords={experimental replication;  replication package;  software engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Laval2012,
author={Laval, J. and Falleri, J.-R. and Vismara, P. and Ducasse, S.},
title={Efficient retrieval and ranking of undesired package cycles in large software systems},
journal={Journal of Object Technology},
year={2012},
volume={11},
number={1},
doi={10.5381/jot.2012.11.1.a4},
art_number={a4},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861694019&doi=10.5381%2fjot.2012.11.1.a4&partnerID=40&md5=e39c660827568fac0a31b971e4c23bf5},
abstract={Many design guidelines state that a software system architecture should avoid cycles between its packages. Yet such cycles appear again and again in many programs. We believe that the existing approaches for cycle detection are too coarse to assist developers to remove cycles from their programs. In this paper, we describe an efficient algorithm that performs a fine-grained analysis of cycles among application packages. In addition, we define multiple metrics to rank cycles by their level of undesirability, prioritizing cycles that are the more undesired by developers. We compare these multiple ranking metrics on four large and mature software systems in Java and Smalltalk. © JOT 2011.},
author_keywords={Package cycle;  Package dependency;  Software architecture;  Software re-engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu201266,
author={Liu, H. and Cai, C. and Zou, C.},
title={An object-oriented serial implementation of a DSMC simulation package},
journal={Computers and Fluids},
year={2012},
volume={57},
pages={66-75},
doi={10.1016/j.compfluid.2011.12.007},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862795872&doi=10.1016%2fj.compfluid.2011.12.007&partnerID=40&md5=fa0ee86eccd5cbb8782aac7b47223540},
abstract={This paper reports a scalar implementation of a multi-dimensional direct simulation Monte Carlo (DSMC) package named " Generalized Rarefied g. As Simulation Package" (GRASP). This implementation adopts a concept of simulation engine and it utilizes many Object-Oriented Programming features and software engineering design patterns. As a result, this implementation successfully resolves the problem of program functionality and interface conflictions for multi-dimensional DSMC implementations. The package has an open architecture which benefits further development and code maintenance. To reduce engineering time for three-dimensional simulations, one effective implementation is to adopt a hybrid grid scheme with a flexible data structure, which can automatically treat cubic cells adjacent to object surfaces. This package can utilize traditional structured, unstructured or hybrid grids to model multi-dimensional complex geometries and simulate rarefied non-equilibrium gas flows. Benchmark test cases indicate that this implementation has satisfactory accuracy for complex rarefied gas flow simulations. © 2011.},
author_keywords={DSMC implementation;  Hybrid grid;  Object-Oriented Programming;  Rarefied gas flow},
document_type={Article},
source={Scopus},
}

@ARTICLE{deSouza2012157,
author={de Souza, R.A. and Ferrari, V.J.},
title={Automatic design of the flexural strengthening of reinforced concrete beams using fiber reinforced polymers (FRP) [Dimensionamento automático do reforço à flexão de vigas de concreto armado com PRFC]},
journal={Acta Scientiarum - Technology},
year={2012},
volume={34},
number={2},
pages={157-165},
doi={10.4025/actascitechnol.v34i2.8318},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858191273&doi=10.4025%2factascitechnol.v34i2.8318&partnerID=40&md5=fa950b143c9d30897f1872a597c09e28},
abstract={Changing the functions of a building, the presence of some design or construction errors, the incidence of seismic actions and even the updating of design codes may demand the strengthening of certain structures. In the specific case of reinforced concrete structures it is desirable the application of a technique of strengthening which is fast, economic and efficient, in order to provide advantages when an intervention is necessary. The technique of strengthening chosen must provide less disorder as possible as well as the guaranty of safety. Taking into account this scenery, fiber reinforced polymers have been working as a very attractive alternative for rehabilitating in-service structures. In that way, the present study aims at presenting the main properties of this new material as well as the design routines for flexural strengthening of reinforced concrete beams. Finally, a package-software developed into the MATLAB platform is presented, intending to generate a simple tool for the automatic design using fiber reinforced polymers.},
author_keywords={Design;  Fiber reinforced polymers;  Reinforced concrete;  Strengthening},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shahar20121,
author={Shahar, Y.},
title={The "human Cli-Knowme" Project: Building a universal, formal, procedural and declarative clinical knowledge base, for the automation of therapy and research},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={6924 LNAI},
pages={1-22},
doi={10.1007/978-3-642-27697-2_1},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857764261&doi=10.1007%2f978-3-642-27697-2_1&partnerID=40&md5=0482aeb2175cb0987ba31165a25f5a17},
abstract={Currently, most clinical knowledge is in free text and is not easily accessible to clinicians and medical researchers. A major grand challenge for medical informatics is the creation of a distributed, universal, formal, sharable, reusable, and computationally accessible medical knowledge base. The required knowledge consists of both procedural knowledge, such as clinical guidelines, and declarative knowledge, such as context-sensitive interpretations of longitudinal patterns of raw clinical data accumulating from several sources. In this position paper, I first demonstrate the feasibility of such an enterprise, and explain in detail the overall lifecycle of a clinical guideline, by reviewing the main current components and their respective evaluations of one such comprehensive architecture for management of clinical guidelines: The Digital Electronic Guideline Library (DeGeL), a Web-based, modular, distributed architecture that facilitates gradual conversion of clinical guidelines from text to a formal representation in chosen target guideline ontology. The architecture supports guideline classification, semantic markup, context-sensitive search, browsing, run-time application to a specific patient at the point of care, and retrospective quality assessment. The DeGeL architecture operates closely with a declarative-knowledge temporal-abstraction architecture, IDAN. Thus, there is significant evidence that building a distributed, multiple-ontology architecture that caters for the full life cycle of a significant portion of current clinical procedural and declarative knowledge, which I refer to as "the Human Clin-knowme Project," has become a feasible task for a joint, coordinated, international effort involving clinicians and medical informaticians. © 2012 Springer-Verlag.},
author_keywords={Automatic Application;  Clinical Guidelines;  Knowledge Acquisition;  knowledge Representation;  Medical Decision Support Systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Song2012274,
author={Song, G. and Yan, G.},
title={Electronic Commerce Teaching Resources Platform Construction Solution Study},
journal={11th Wuhan International Conference on E-Business, WHICEB 2012},
year={2012},
pages={274-279},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138697385&partnerID=40&md5=218e8b2a7b1be201c33ecb7e9471cc79},
abstract={The paper proposes the solution to teaching resource application platform that applies to electronic commerce teaching. Electronic commerce teaching resource platform system includes hardware system structure and software system structure. It is an open approach based on B/S mode that provides each faculty with local and remote teaching resources via Intranet/Internet and booming cloud technology. It establishes inner-and inter-scholastic supporting teaching resource library to meet the demands in teaching practices, and includes functions such as theory teaching, practice teaching, skill practice, entrepreneurial internship, teaching resource service, laboratory management, teaching management, teacher-student interaction and etc. The paper combines the comprehensive teaching resource platform construction practices of modern business and logistics in Beijing Youth Politics College, and puts forward opinions on and solutions to construction scheme of electronic commerce laboratory and teaching resource system in colleges and universities, including construction purpose, system structure, network laboratory composition, teaching function, function of teaching resource library contents and other aspects. Besides, it proposes the basic solutions to experimental teaching platform, teaching management platform, practice and entrepreneurial platform and teaching resource library construction. © 2012, Association for Information Systems. All rights reserved.},
author_keywords={cloud technology;  electronic commerce teaching;  function and application;  system structure;  teaching resource platform construction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ellis20127,
author={Ellis, I.C. and Agah, A.},
title={Software development paradigms for Artificial Intelligence applications},
journal={Proceedings of the 6th IASTED International Conference on Software Engineering and Applications, SEA 2002},
year={2012},
pages={7-12},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904201440&partnerID=40&md5=c814874bf468f51b31efd1f4a648cce5},
abstract={Functional programming languages like LISP and PROLOG have long dominated the field of classical Artificial Intelligence (AI) programming. One major drawback to functional languages, however, is in their speed performance. Some programmers that are fond of functional languages immediately dismiss alternatives like C++ because they do not support features like tailrecursion or treat functions as first-class objects. While C++ may not have the same features as these other languages, it does provide very good performance and has a number of useful features. The C++ Standard Library is a collection of data Containers, and Algorithms that can be performed on the Containers along with a framework for customizing and enhancing the library. The addition of the Standard Library has made C++ into a language that AI programmers should consider when good performance is needed. It is now possible to write general code that works on any data types, and to write functions that can be treated as first-class objects. In this paper, these techniques are demonstrated with the design and implementation of a path finding AI simulation written in C++. The path finding is demonstrated though a threedimensional graphics program, augmented with a graphical user interface for ease of use.},
author_keywords={Artificial intelligence applications and programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{An2012461,
author={An, Z. and Liu, H.},
title={Design and implementation of hidden key loggers under .net platform},
journal={Proceedings of the 2012 National Conference on Information Technology and Computer Science, CITCS 2012},
year={2012},
pages={461-464},
doi={10.2991/citcs.2012.183},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878150345&doi=10.2991%2fcitcs.2012.183&partnerID=40&md5=f59a76228bff56800d20234bf9a1ceef},
abstract={keyboard recording technology has been widely applied to the system monitoring program.This paper describes the design and implementation keyboard loggers based on the the HOOK, under .NET platform. At the same time, in order to reach the hidden effect, record keyboard function module is inserted into the system processes, through the dynamic link library Insertion technology. © 2012. The authors - Published by Atlantis Press.},
author_keywords={Dynamic link library insertion technology;  Hook;  Key loggers},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Axelsen2012268,
author={Axelsen, E.W. and Sørensen, F. and Krogdahl, S. and Møller-Pedersen, B.},
title={Challenges in the design of the package template mechanism},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7271 LECTURE NO},
pages={268-305},
doi={10.1007/978-3-642-35551-6_7},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872650917&doi=10.1007%2f978-3-642-35551-6_7&partnerID=40&md5=fa75c7e8cd23916c32da52b02ff55abe},
abstract={Package Templates are a mechanism for writing modules meant for reuse, where each module (template) consists of a collection of classes. A package template must be instantiated in a program at compile time to form a set of ordinary classes, and during instantiation, the classes may be adapted by means of renaming, adding attributes (fields and methods), and supplying type parameters. An approach like this naturally gives rise to two distinct dimensions along which classes can be extended. One is the ordinary subclass dimension, while the other is comprised of the ability to adapt classes during instantiations. The latter dimension also allows for a form of static multiple inheritance, in that classes from two or more instantiations may be merged to form a new class. This will be handled entirely at compile time, and the new class will have all the attributes from the merged classes and also the attributes added in the instantiating template. This paper discusses how these two dimensions play together in the different mechanisms that make up the Package Templates approach, and the design considerations involved. The paper also argues that the compromise made in Package Templates between simplicity of the type system on the one hand and expressiveness on the other is, for most purposes, better than similar approaches based on virtual classes. © Springer-Verlag Berlin Heidelberg 2012.},
author_keywords={Inheritance;  Language Constructs;  Modularization;  Modules;  Object-oriented Programming;  Packages;  Programming Languages;  Templates},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pirola2012,
author={Pirola, Y. and Rizzi, R. and Picardi, E. and Pesole, G. and Vedova, G.D. and Bonizzoni, P.},
title={PIntron: A fast method for detecting the gene structure due to alternative splicing via maximal pairings of a pattern and a text},
journal={BMC Bioinformatics},
year={2012},
volume={13},
doi={10.1186/1471-2105-13-S5-S2},
art_number={S2},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867680338&doi=10.1186%2f1471-2105-13-S5-S2&partnerID=40&md5=7ae78f96f962fc55297f6a9071b924ef},
abstract={Background: A challenging issue in designing computational methods for predicting the gene structure into exons and introns from a cluster of transcript (EST, mRNA) sequences, is guaranteeing accuracy as well as efficiency in time and space, when large clusters of more than 20,000 ESTs and genes longer than 1 Mb are processed. Traditionally, the problem has been faced by combining different tools, not specifically designed for this task. Results: We propose a fast method based on ad hoc procedures for solving the problem. Our method combines two ideas: a novel algorithm of proved small time complexity for computing spliced alignments of a transcript against a genome, and an efficient algorithm that exploits the inherent redundancy of information in a cluster of transcripts to select, among all possible factorizations of EST sequences, those allowing to infer splice site junctions that are largely confirmed by the input data. The EST alignment procedure is based on the construction of maximal embeddings, that are sequences obtained from paths of a graph structure, called embedding graph, whose vertices are the maximal pairings of a genomic sequence T and an EST P. The procedure runs in time linear in the length of P and T and in the size of the output. The method was implemented into the PIntron package. PIntron requires as input a genomic sequence or region and a set of EST and/or mRNA sequences. Besides the prediction of the full-length transcript isoforms potentially expressed by the gene, the PIntron package includes a module for the CDS annotation of the predicted transcripts. Conclusions: PIntron, the software tool implementing our methodology, is available at http://www.algolab.eu/PIntron under GNU AGPL. PIntron has been shown to outperform state-of-the-art methods, and to quickly process some critical genes. At the same time, PIntron exhibits high accuracy (sensitivity and specificity) when benchmarked with ENCODE annotations. © 2012.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20121,
author={Zhang, W. and Betz, V. and Rose, O.},
title={Portable and scalable FPGA-based acceleration of a direct linear system solver},
journal={ACM Transactions on Reconfigurable Technology and Systems},
year={2012},
volume={5},
number={1},
pages={1-26},
doi={10.1145/2133352.2133358},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867537292&doi=10.1145%2f2133352.2133358&partnerID=40&md5=ad4bdec97061dc6f577dfcb93e7f193e},
abstract={FPGAs have the potential to serve as a platform for accelerating many computations including scientific applications. However, the large development cost and short life span for FPGA designs have limited their adoption by the scientific computing community. FPGA-based scientific computing and many kinds of embedded computing could become more practical if there were hardware libraries that were portable to any FPGA-based system with performance that scaled with the size of the FPGA. To illustrate this idea we have implemented one common super-computing library function: the LU factorizationmethod for solving systems of linear equations. This paper describes a method for making the design both portable and scalable that should be illustrative if such libraries are to be built in the future. The design is a software-based generator that leverages both the flexibility of a software programming language and the parameters inherent in an hardware description language. The generator accepts parameters that describe the FPGA capacity and external memory capabilities. We compare the performance of our engine executing on the largest FPGA available at the time of this work (an Altera Stratix III 3S340) to a single processor core fabricated in the same 65nm IC process running a highly optimized software implementation from the processor vendor. For single precision matrices on the order of 10,000 × 10,000 elements, the FPGA implementation is 2.2 times faster and the energy dissipated per useful GFLOP operation is a factor of 5 times less. For double precision, the FPGA implementation is 1.7 times faster and 3.5 times more energy efficient. © 2012 ACM.},
author_keywords={Acceleration;  FPGA;  Linear system solver;  LU decomposition;  Portable;  Scalable},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ye2012,
author={Ye, C. and Ma, Z.S. and Cannon, C.H. and Pop, M. and Yu, D.W.},
title={Exploiting sparseness in de novo genome assembly.},
journal={BMC bioinformatics},
year={2012},
volume={13 Suppl 6},
pages={S1},
doi={10.1186/1471-2105-13-S6-S1},
art_number={S1},
note={cited By 122},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866712746&doi=10.1186%2f1471-2105-13-S6-S1&partnerID=40&md5=ce5730f5b228ccfb154f47848e16ba63},
abstract={The very large memory requirements for the construction of assembly graphs for de novo genome assembly limit current algorithms to super-computing environments. In this paper, we demonstrate that constructing a sparse assembly graph which stores only a small fraction of the observed k-mers as nodes and the links between these nodes allows the de novo assembly of even moderately-sized genomes (~500 M) on a typical laptop computer. We implement this sparse graph concept in a proof-of-principle software package, SparseAssembler, utilizing a new sparse k-mer graph structure evolved from the de Bruijn graph. We test our SparseAssembler with both simulated and real data, achieving ~90% memory savings and retaining high assembly accuracy, without sacrificing speed in comparison to existing de novo assemblers.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bosché201290,
author={Bosché, F.},
title={Plane-based registration of construction laser scans with 3D/4D building models},
journal={Advanced Engineering Informatics},
year={2012},
volume={26},
number={1},
pages={90-102},
doi={10.1016/j.aei.2011.08.009},
note={cited By 132},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-83555160969&doi=10.1016%2fj.aei.2011.08.009&partnerID=40&md5=efe3ca2c8238b6c766f22a93ec18adba},
abstract={With the development of building information modelling (BIM) and terrestrial laser scanning (TLS) in the architecture, engineering, construction and facility management (AEC/FM) industry, the registration of site laser scans and project 3D (BIM) models in a common coordinate system is becoming critical to effective project control. The co-registration of 3D datasets is normally performed in two steps: coarse registration followed by fine registration. Focusing on the coarse registration, model-scan registration has been well investigated in the past, but it is shown in this article that the context of the AEC/FM industry presents specific (1) constraints that make fully-automated registration very complex and often ill-posed, and (2) advantages that can be leveraged to develop simpler yet effective registration methods. This paper thus presents a novel semi-automated plane-based registration system for coarse registration of laser scanned 3D point clouds with project 3D models in the context of the AEC/FM industry. The system is based on the extraction of planes from the laser scanned point cloud and project 3D/4D model. Planes are automatically extracted from the 3D/4D model. For the point cloud data, two methods are investigated. The first one is fully automated, and the second is a semi-automated but effective one-click RANSAC-supported extraction method. In both cases, planes are then manually but intuitively matched by the user. Experiments, which compare the proposed system to software packages commonly used in the AEC/FM industry, demonstrate that at least as good registration quality can be achieved by the proposed system, in a simpler and faster way. It is concluded that, in the AEC/FM context, the proposed plane-based registration system is a compelling alternative to standard point-based registration techniques. © 2011 Elsevier Ltd. All rights reserved.},
author_keywords={3D model;  BIM;  Coarse Registration;  Construction;  Laser Scan;  Point Cloud},
document_type={Article},
source={Scopus},
}

@ARTICLE{Taboada20112382,
author={Taboada, G.L. and Touriño, J. and Doallo, R. and Shafi, A. and Baker, M. and Carpenter, B.},
title={Device level communication libraries for high-performance computing in Java},
journal={Concurrency and Computation: Practice and Experience},
year={2011},
volume={23},
number={18},
pages={2382-2403},
doi={10.1002/cpe.1777},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81155150353&doi=10.1002%2fcpe.1777&partnerID=40&md5=7621c462f71956f014dca007f2ca0692},
abstract={Since its release, the Java programming language has attracted considerable attention from the high-performance computing (HPC) community because of its portability, high programming productivity, and built-in multithreading and networking support. As a consequence, several initiatives have been taken to develop a high-performance Java message-passing library to program distributed memory architectures, such as clusters. The performance of Java message-passing applications relies heavily on the communications performance. Thus, the design and implementation of low-level communication devices that support message-passing libraries is an important research issue in Java for HPC. MPJ Express is our Java message-passing implementation for developing high-performance parallel Java applications. Its public release currently contains three communication devices: the first one is built using the Java New Input/Output (NIO) package for the TCP/IP; the second one is specifically designed for the Myrinet Express library on Myrinet; and the third one supports thread-based shared memory communications. Although these devices have been successfully deployed in many production environments, previous performance evaluations of MPJ Express suggest that the buffering layer, tightly coupled with these devices, incurs a certain degree of copying overhead, which represents one of the main performance penalties. This paper presents a more efficient Java message-passing communications device, based on Java Input/Output sockets, that avoids this buffering overhead. Moreover, this device implements several strategies, both in the communication protocol and in the HPC hardware support, which optimizes Java message-passing communications. In order to evaluate its benefits, this paper analyzes the performance of this device comparatively with other Java and native message-passing libraries on various high-speed networks, such as Gigabit Ethernet, Scalable Coherent Interface, Myrinet, and InfiniBand, as well as on a shared memory multicore scenario. The reported communication overhead reduction encourages the upcoming incorporation of this device in MPJ Express (http://mpj-express.org). © 2011 John Wiley & Sons, Ltd.},
author_keywords={high-speed networks;  Java for high-performance computing (HPC);  Message Passing in Java (MPJ);  performance evaluation;  shared memory multicore communication},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pébay20111475,
author={Pébay, P. and Thompson, D. and Bennett, J. and Mascarenhas, A.},
title={Design and performance of a scalable, parallel statistics toolkit},
journal={IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
year={2011},
pages={1475-1484},
doi={10.1109/IPDPS.2011.293},
art_number={6009003},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455246619&doi=10.1109%2fIPDPS.2011.293&partnerID=40&md5=e4b3a7e2cbd51f2e35d2ad9fc3245065},
abstract={Most statistical software packages implement a broad range of techniques but do so in an ad hoc fashion, leaving users who do not have a broad knowledge of statistics at a disadvantage since they may not understand all the implications of a given analysis or how to test the validity of results. These packages are also largely serial in nature, or target multicore architectures instead of distributed-memory systems, or provide only a small number of statistics in parallel. This paper surveys a collection of parallel implementations of statistics algorithm developed as part of a common framework over the last 3 years. The framework strategically groups modeling techniques with associated verification and validation techniques to make the underlying assumptions of the statistics more clear. Furthermore it employs a design pattern specifically targeted for distributed-memory parallelism, where architectural advances in large-scale high-performance computing have been focused. Moment-based statistics (which include descriptive, correlative, and multicorrelative statistics; principal component analysis (PCA); and k-means statistics) scale nearly linearly with the data set size and number of processes. Entropy-based statistics (which include order and contingency statistics) do not scale well when the data in question is continuous or quasi-diffuse but do scale well when the data is discrete and compact. We confirm and extend our earlier results by now establishing near-optimal scalability with up to 10,000 processes. © 2011 IEEE.},
author_keywords={Clustering;  Design patterns;  Informatics;  Parallel computing;  Principal component analysis;  Statistics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lei2011219,
author={Lei, Y. and Dou, Y. and Shen, L. and Zhou, J. and Guo, S.},
title={Special-purposed VLIW architecture for IEEE-754 quadruple precision elementary functions on FPGA},
journal={Proceedings - IEEE International Conference on Computer Design: VLSI in Computers and Processors},
year={2011},
pages={219-225},
doi={10.1109/ICCD.2011.6081400},
art_number={6081400},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455210408&doi=10.1109%2fICCD.2011.6081400&partnerID=40&md5=68bbbb09c3968dcc612e0eb0a71126dc},
abstract={This work explores the feasibility to implement IEEE-754-2008 standard quadruple precision (Quad) elementary functions on recent FPGAs with plenty of embedded memories and DSP blocks. First, we analysis the implementation algorithm of Quad elementary functions in detail. Then, we present a special-purpose Very Large Instruction Word (VLIW) architecture for Quad elementary function (QE-Processor). The proposed processor uses a unified hardware structure, equipped with multiple basic arithmetic units, to implement various Quad algebraic and transcendental functions, in which several tradeoffs between latency and resource usage are carefully planned to avoid unbalanced resource utilization. The performance is improved through the explicitly parallel technology of custom VLIW instruction. Finally, we create a prototype of QE-Processor into Xilinx Virtex-5 and Virtex-6 FPGA chips. The experimental results show that our design can guarantee that the percentage of correct rounding is more than 99.9%. Moreover, the FPGA implementation on Virtex-6 XC6VLX760-2FF1760 FPGA, running at 220 MHz, outperforms the parallel software approach based on OpenMP running on an Intel Xeon E5620 CPU at 2.40GHz by a factor of 13X-20X for special function applications in Boost library. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2011,
title={SOSP'11 - Proceedings of the 23rd ACM Symposium on Operating Systems Principles},
journal={SOSP'11 - Proceedings of the 23rd ACM Symposium on Operating Systems Principles},
year={2011},
page_count={409},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-82655179225&partnerID=40&md5=0be7e4970d7c188275e979b8c36d6aa3},
abstract={The proceedings contain 28 papers. The topics discussed include: design implications for enterprise storage systems via multi-dimensional trace analysis; differentiated storage services; a file is not a file: understanding the I/O behavior of apple desktop applications; intrusion recovery for database-backed web applications; software fault isolation with API integrity and multi-principal modules; windows azure storage: a highly available cloud storage service with strong consistency; an empirical study on configuration errors in commercial and open source systems; cells: a virtual mobile smartphone architecture; breaking up is hard to do: security and functionality in a commodity hypervisor; Atlantis: robust, extensible execution environments for web applications; logical attestation: an authorization architecture for trustworthy computing; and practical software model checking via dynamic interface reduction.},
document_type={Conference Review},
source={Scopus},
}

@BOOK{Chakrabarty20111,
author={Chakrabarty, B.K.},
title={Computer-Aided design and computer graphics},
journal={Computer Graphics},
year={2011},
pages={1-34},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892034948&partnerID=40&md5=0d9d8e90bb25be3e9769f0e71384d0d3},
abstract={Computer Aided Design (CAD) is a vital application area of the all-pervasive Information Technology Revolution influencing the way we live and work, shrinking the effects of time and distance, and altering the very nature of work in an organization [20,21,27]. Modern computers (even a PC) can generate and process information on a scope never before possible. Such information, generated in tables of numbers of hundreds of pages long, may be of no worth to a human user unable to comprehend them, but may be very meaningful if presented graphically [11, 26, 29, 33,34]. Computer graphics, which is a study of techniques to improve communication between human and machine, is an important component of CAD [11,19,29, 33,34]. Adoption of CAD in various fields has accelerated with recent advances in microcomputer technology, steady decline in cost of computers, and tremendous increase in their computing power and graphic capabilities [11,15,19]. This is equally applicable in Architecture (including Planning), Civil-Building Engineering, and Construction (usually classified as A-E-C) field [11,15,24,29]. Real utility of Computer Aided Design (CAD) is not confined to just producing sophisticated solid geometry's or giving more stress to cosmetic such as producing neater, more attractive drawing packages only [11,15,19,29]. It should cover many other aspects of design adopting an Integrated Computer-Aided Design Approach (covering all its components including graphics, drafting, geometric modeling, optimization and so on), involving hundreds of variables, to enhance Productivity, Social-Responsiveness, and other benefits, particularly in Urban Development and Management, creating scope for informed public-participation in the decision-making process in such operations [9,10,19,21, 29]. The chapter highlights planning and designs as a problem solving process and the vital role of computers and graphics in this process. It outlines the concept of Integrated Computer Aided Design; it's components, and the utility and application of computer graphics in geometric modeling, design analysis, design optimization [1], and in architectural and engineering drafting. It gives illustrative graphic application results of Integrated Computer Aided Design (ICAD) Software developed by the author for design analysis and design optimization in integrated computer-aided structural design and drafting, and integrated computer-aided building design and drafting [2-10]. It presents a case study of participatory planning and design using a Descriptive Model in graphic form for urban development planning policy analysis. It gives an example application of graphics programming using AutoLISP for Computer-Aided Design and Drafting (CADD) of urban layout modules. The need for universal graphic standard programming like GKS [11,19,34] in the AEC sector is stressed, which will greatly facilitate application of Integrated Computer Aided Design in this sector to derive various benefits including Efficiency and Effectiveness. To make such applications feasible, a compulsory addition of "Computer Aided Planning and Design by Optimization", as a full-length subject of study (suggestive course-content in Appendix) for the AEC Professional Degree Curriculum, is suggested. This will help equip AEC Professionals, who commit huge Resources in their Planning and Design Decisions enjoining on them an ethical responsibility for efficient and equitable use of resources so committed, to develop and apply such techniques to discharge their Accountability for a Resource-Efficient and Equitable problem solution in this sector. © 2011 by Nova Science Publishers, Inc. All rights reserved.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{García201148,
author={García, L.E.R. and Garcia, A. and Bateman, J.},
title={An ontology-based feature recognition and design rule checker for engineering},
journal={CEUR Workshop Proceedings},
year={2011},
volume={809},
pages={48-59},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891933410&partnerID=40&md5=204d40ea8f263733c22066cf9163433f},
abstract={This paper describes the design and implementation of an Ontology- Based System for Features Recognition and Design Rules Checking in the domain of sheet-metal engineering using Semantic Web technologies. The system was implemented by means of the Protégé Application Programming Interface (API), a rule engine and a reasoner. Using ontology in the core of the system enabled the representation of manufacturing rules and Automatic Features recognition. Rules and Queries were not hard coded in the program, giving the system a high level of maintainability and reusability. Features were classified as general and specific, easing the work of classifying newer features as they appear given their previous classification. Most common sheet metal features referred to in the literature were recognized by the system.},
author_keywords={Cad;  Cam;  Capp;  Owl;  Semantic web;  Sqwrl;  Swrl},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Blumenthal20113554,
author={Blumenthal, S. and Prassler, E. and Fischer, J. and Nowak, W.},
title={Towards identification of best practice algorithms in 3D perception and modeling},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2011},
pages={3554-3561},
doi={10.1109/ICRA.2011.5980106},
art_number={5980106},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871703475&doi=10.1109%2fICRA.2011.5980106&partnerID=40&md5=73bd47b96e9ef20a33a569a9bb103661},
abstract={Robots need a representation of their environment to reason about and to interact with it. Different 3D perception and modeling approaches exist to create such a representation, but they are not yet easily comparable. This work tries to identify best practice algorithms in the domain of 3D perception and modeling with a focus on environment reconstruction for robotic applications. The goal is to have a collection of refactored algorithms that are easily measurable and comparable. The realization follows a methodology consisting of five steps. After a survey of relevant algorithms and libraries, common representations for the core data-types Cartesian point, Cartesian point cloud and triangle mesh are identified for use in harmonized interfaces. Atomic algorithms are encapsulated into four software components: the Octree component, the Iterative Closest Point component, the k-Nearest Neighbors search component and the Delaunay triangulation component. A sample experiment demonstrates how the component structure can be used to deduce best practice. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Arena2011787,
author={Arena, A. and Formica, G. and Lacarbonara, W. and Dankowicz, H.},
title={Nonlinear finite element-based path following of periodic solutions},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2011},
volume={4},
number={PARTS A AND B},
pages={787-793},
doi={10.1115/DETC2011-48673},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863588530&doi=10.1115%2fDETC2011-48673&partnerID=40&md5=54627abea041da971e77e2121eb9f362},
abstract={A computational framework is proposed to path follow the periodic solutions of nonlinear spatially continuous systems and more general coupled multiphysics problems represented by systems of partial differential equations with time-dependent excitations. The set of PDEs is cast in first order differential form (in time) ?u = f(u; s; t; c) where u(s; t) is the vector collecting all state variables including the velocities/time rates, s is a space co-ordinate (here, one-dimensional systems are considered without lack of generality for the space dependence) and t denotes time. The vector field f depends, in general, not only on the classical state variables (such as positions and velocities) but also on the space gradients of the leading unknowns. The space gradients are introduced as part of the state variables. This is justified by the mathematical and computational requirements on the continuity in space up to the proper differential order of the space gradients associated with the unknown position vector field. The path following procedure employs, for the computation of the periodic solutions, only the evaluation of the vector field f. This part of the path following procedure within the proposed combined scheme was formerly implemented by Dankowicz and co-workers in a MATLAB software package called COCO. The here proposed procedure seeks to discretize the space dependence of the variables using finite elements based on Lagrangian polynomials which leads to a discrete form of the vector field f: A concurrent bifurcation analysis is carried out by calculating the eigenvalues of the monodromy matrix. A hinged-hinged nonlinear beam subject to a primary-resonance harmonic transverse load or to a parametric-resonance horizontal end displacement is considered as a case study. Some primary-resonance frequency-response curves are calculated along with their stability to assess the convergence of the discretization scheme. The frequency-response curves are shown to be in close agreement with those calculated by direct integration of the PDEs through the FE software called COMSOL Multiphysics. Besides primary-resonance direct forcing conditions, also parametric forcing causing the principal parametric resonance of the lowest two bending modes is considered through construction of the associated transition curves. The proposed approach integrates algorithms from the finite element and bifurcation domains thus enabling an accurate and effective unfolding of the bifurcation and post-bifurcation scenarios of nonautonomous PDEs with the underlying structures. Copyright © 2011 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Heyn2011227,
author={Heyn, T. and Seidl, A. and Mazhar, H. and Lamb, D. and Tasora, A. and Negrut, D.},
title={Enabling computational dynamics in distributed computing environments using a heterogeneous computing template},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2011},
volume={4},
number={PARTS A AND B},
pages={227-236},
doi={10.1115/DETC2011-48347},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863561916&doi=10.1115%2fDETC2011-48347&partnerID=40&md5=112e2a7ba872e3fd2a539c5f3f149fd8},
abstract={This paper describes a software infrastructure made up of tools and libraries designed to assist developers in implementing computational dynamics applications running on heterogeneous and distributed computing environments. Together, these tools and libraries compose a so called Heterogeneous Computing Template (HCT). The heterogeneous and distributed computing hardware infrastructure is assumed herein to be made up of a combination of CPUs and GPUs. The computational dynamics applications targeted to execute on such a hardware topology include many-body dynamics, smoothed-particle hydrodynamics (SPH) fluid simulation, and fluid-solid interaction analysis. The underlying theme of the solution approach embraced by HCT is that of partitioning the domain of interest into a number of sub-domains that are each managed by a separate core/accelerator (CPU/GPU) pair. Five components at the core of HCT enable the envisioned distributed computing approach to large-scale dynamical system simulation: (a) a method for the geometric domain decomposition and mapping onto heterogeneous hardware; (b) methods for proximity computation or collision detection; (c) support for moving data among the corresponding hardware as elements move from subdomain to subdomain; (d) numerical methods for solving the specific dynamics problem of interest; and (e) tools for performing visualization and post-processing in a distributed manner. In this contribution the components (a) and (c) of the HCT are demonstrated via the example of the Discrete Element Method (DEM) for rigid body dynamics with friction and contact. The collision detection task required in frictional-contact dynamics; i.e., task (b) above, is discussed separately and in the context of GPU computing. This task is shown to benefit of a two order of magnitude gain in efficiency when compared to traditional sequential implementations. Note: Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not imply its endorsement, recommendation, or favoring by the US Army. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Army, and shall not be used for advertising or product endorsement purposes. Copyright © 2011 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Muller201139,
author={Muller, D.},
title={Automod™ - Providing simulation solutions for over 25 years},
journal={Proceedings - Winter Simulation Conference},
year={2011},
pages={39-51},
doi={10.1109/WSC.2011.6147738},
art_number={6147738},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858029213&doi=10.1109%2fWSC.2011.6147738&partnerID=40&md5=dc690fb4d118e3ed05be347c6a492a80},
abstract={Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model components in other models, decreasing the time required to build a model. In addition, recent enhancements to AutoMod's material handling template systems have in-creased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Garcia201175,
author={Garcia, S. and Granado, B.},
title={Task model and online operating system API for hardware tasks in OLLAF platform},
journal={Conference on Design and Architectures for Signal and Image Processing, DASIP},
year={2011},
pages={75-82},
doi={10.1109/DASIP.2011.6136857},
art_number={6136857},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857733076&doi=10.1109%2fDASIP.2011.6136857&partnerID=40&md5=f567935889c58c1bc69d786ad81763ed},
abstract={This article present an original hardware task model and the corresponding online API for Fine Grained Dynamically Reconfigurable Architecture. We cover the integration of this API in the OLLAF platform and more specifically its application to memory access management in a dynamically reconfigurable environment. Methods offered by this platform are compared to existing software and hardware solutions. We also discuss of the design complexity of an application using difference solutions. We demonstrate that our solution cans give application developer the same flexibility than with a software implementation, with a very close design complexity while ensuring the same performance gain a common FPGA based IP would permit. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dongarra201133,
author={Dongarra, J. and Faverge, M. and Ltaief, H. and Luszczek, P.},
title={High performance matrix inversion based on LU factorization for multicore architectures},
journal={MTAGS'11 - Proceedings of the 2011 ACM International Workshop on Many Task Computing on Grids and Supercomputers, Co-located with SC'11},
year={2011},
pages={33-42},
doi={10.1145/2132876.2132885},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857663656&doi=10.1145%2f2132876.2132885&partnerID=40&md5=d4e299138e8ef1cf9f51816baa2dd6dc},
abstract={The goal of this paper is to present an efficient implementation of an explicit matrix inversion of general square matrices on multicore computer architecture. The inversion procedure is split into four steps: 1) computing the LU factorization, 2) inverting the upper triangular U factor, 3) solving a linear system, whose solution yields inverse of the original matrix and 4) applying backward column pivoting on the inverted matrix. Using a tile data layout, which represents the matrix in the system memory with an optimized cache-aware format, the computation of the four steps is decomposed into computational tasks. A directed acyclic graph is generated on the fly which represents the program data flow. Its nodes represent tasks and edges the data dependencies between them. Previous implementations of matrix inversions, available in the state-of-the-art numerical libraries, are suffer from unnecessary synchronization points, which are non-existent in our implementation in order to fully exploit the parallelism of the underlying hardware. Our algorithmic approach allows to remove these bottlenecks and to execute the tasks with loose synchronization. A runtime environment system called QUARK is necessary to dynamically schedule our numerical kernels on the available processing units. The reported results from our LU-based matrix inversion implementation significantly outperform the state-of-the-art numerical libraries such as LAPACK (5x), MKL (5x) and ScaLAPACK (2.5x) on a contemporary AMD platform with four sockets and the total of 48 cores for a matrix of size 24000. A power consumption analysis shows that our high performance implementation is also energy efficient and substantially consumes less power than its competitors. © 2011 ACM.},
author_keywords={LU factorization;  multicore parallel performance;  runtime DAG scheduling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jozwik2011416,
author={Jozwik, K. and Tomiyama, H. and Edahiro, M. and Honda, S. and Takada, H.},
title={Rainbow: An OS extension for hardware multitasking on dynamically partially reconfigurable FPGAs},
journal={Proceedings - 2011 International Conference on Reconfigurable Computing and FPGAs, ReConFig 2011},
year={2011},
pages={416-421},
doi={10.1109/ReConFig.2011.73},
art_number={6128613},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856943299&doi=10.1109%2fReConFig.2011.73&partnerID=40&md5=01f840efa4c85a9a8714df2596721e8d},
abstract={DPR (Dynamic Partial Reconfiguration) capability found in some of modern FPGAs allows implementation of a concept of a HW (Hardware) task, which similarly to its software counterpart has its state and shares time-multiplexed resources with the other tasks. While the new technology presents many advantages for embedded systems where run-time adaptability is an additional requirement, their efficient and easily portable implementations require a control software or an OS which would manage all the complexities of the underlying technology, providing an abstracted interface for the application programmer. This paper presents a novel and robust hardware multitasking extension for a conventional OS, managing task scheduling and configurations, and providing easy-to-use API (Application Programming Interface) for the application programmer. Scheduling is priority-based and takes advantage of task caching. Moreover, the extension is based on a developed design flow and embedded hardware platform allowing efficient task preemption, which can be utilized whenever it presents any benefits to the application. © 2011 IEEE.},
author_keywords={Dynamic Reconfiguration;  FPGA;  Runtime Reconfiguration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma2011,
author={Ma, J. and Shen, J. and Xu, S.},
title={A parallel implementation of Douglas-Peucker algorithm for real-time map generalization of polyline features on multi-core processor computers},
journal={Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University},
year={2011},
volume={36},
number={12},
pages={1423-1426+1494},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455200457&partnerID=40&md5=1b70c0142acdb348a55f87d641ded2b2},
abstract={The Douglas-Peucker polyline simplification algorithm has been widely adopted in map generalization for decades, though it is often criticized for its low performance. As multi-core processor computers become widely available, it might be a good opportunity to improve the performance by converting its sequential implementation to parallel form. We present three different parallel implementations of the Douglas-Peucker algorithm. The first is in either recursive or non-recursive manner using OpenMP. The second is done by splitting a polyline feature into irrelevant segments and distributing segments to parallel threads. The third method is to dispatch each polyline feature to an idle parallel thread in which the conventional sequential method is applied. By utilizing the official China's provincial boundary geospatial data set, and C ++ language for programming, performances on various multi-core processor computers are compared among the three implementations together with the original sequential forms. We prove that with the increment of processor's cores and the number of threads accordingly, the parallel algorithms will efficiently reduce the number of vertex of a polyline and generate multi-resolution polyline data, which dramatically speed up the process of map generalization and thus real-time display effects are achieved.},
author_keywords={Cartographic generalization;  Douglas-Peucker algorithm;  Multi-core processor;  Parallel algorithm;  Sequential algorithm},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kelefouras20116217,
author={Kelefouras, V.I. and Athanasiou, G.S. and Alachiotis, N. and Michail, H.E. and Kritikakou, A.S. and Goutis, C.E.},
title={A methodology for speeding up fast fourier transform focusing on memory architecture utilization},
journal={IEEE Transactions on Signal Processing},
year={2011},
volume={59},
number={12},
pages={6217-6226},
doi={10.1109/TSP.2011.2168525},
art_number={6021384},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81455148136&doi=10.1109%2fTSP.2011.2168525&partnerID=40&md5=49a06aa150c223469998ee0ac2ef3147},
abstract={Several SOA (state of the art) self-tuning software libraries exist, such as the Fastest Fourier Transform in the West (FFTW) for fast Fourier transform (FFT). FFT is a highly important kernel and the performance of its software implementations depends on the memory hierarchy's utilization. FFTW minimizes register spills and data cache accesses by finding a schedule that is independent of the number of the registers and of the number of levels and size of the cache, which is a serious drawback. In this paper, a new methodology is presented, achieving improved performance by focusing on memory hierarchy utilization. The proposed methodology has three major advantages. First, the combination of production and consumption of butterflies' results, data reuse, FFT parallelism, symmetries of twiddle factors and also additions by zeros and multiplications by zeros and ones when twiddle factors are zero or one, are fully and simultaneously exploited. Second, the optimal solution is found according to the number of the registers, the data cache sizes, the number of the levels of data cache hierarchy, the main memory page size, the associativity of the data caches and the data cache line sizes, which are also considered simultaneously and not separate. Third, compilation time and source code size are very small compared with FFTW. The proposed methodology achieves performance gain about 40% (speed-up of 1.7) for architectures with small data cache sizes where memory management has a larger effect on performance and 20% (speed-up of 1.25) on average for architectures with large data cache sizes (Pentium) in comparison with FFTW. © 2011 IEEE.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kim2011219,
author={Kim, J. and Ahn, C. and Choi, S. and Glossner, J.},
title={Implementation of smart antenna API and transceiver API in software communication architecture for a wireless innovation forum standard},
journal={Analog Integrated Circuits and Signal Processing},
year={2011},
volume={69},
number={2-3},
pages={219-226},
doi={10.1007/s10470-011-9759-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755188005&doi=10.1007%2fs10470-011-9759-6&partnerID=40&md5=6d90db9647f4c09161d0b63937672faa},
abstract={This study presents an implementation of the standard smart antenna (SA) application programming interface (API) and Transceiver API developed by the wireless innovation forum's (WINNF) smart antenna working group (SAWG). The API is implemented using the open-source SCA implementation-embedded (OSSIE) developed at Virginia Tech. Our implementation verified that the SA API can be utilized in software communication architecture (SCA)-based software defined radio (SDR) systems. We also verified that the Transceiver API can be realized with a real radio frequency (RF) transceiver module such as universal software radio peripheral2 (USRP2). The SA API enables various functions of multi-antenna systems such as beamforming and multiple input multiple output (MIMO) of spatial multiplexing. These are core technologies prevalent in 4G mobile communication systems. In order to support multi-antenna structures, the Transceiver API has first been extended for multichannel use. The paper details how the API is extended using OSSIE and the current status of the API as a standard within the Wireless Innovation Forum. © 2011 Springer Science+Business Media, LLC.},
author_keywords={SCA;  SDR;  Smart antenna;  Transceiver},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yang20112624,
author={Yang, Y. and Ye, H. and Fei, S.},
title={Design of communication interface for M2M-based positioning and monitoring system},
journal={2011 International Conference on Electronics, Communications and Control, ICECC 2011 - Proceedings},
year={2011},
pages={2624-2627},
doi={10.1109/ICECC.2011.6067754},
art_number={6067754},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81455138144&doi=10.1109%2fICECC.2011.6067754&partnerID=40&md5=28f966fd42dd1b78955745d6473ddcb1},
abstract={The idea of M2M platform proposes a completely new system architecture for positioning and monitoring applications with wider coverage and higher communication efficiency. With the comparison of the original communication mode to the M2M-based one, the communication interface is changed in the new communication solution for the application side accessing to the platform, which is finally designed into two parts according to the interface specification for M2M platform. The one is the local web service which provides methods for the platform to transmit data from the terminal side, and the other is the communication service software developed based on the SDK package from the M2M platform. This interface has been implemented into an actual application for the construction machinery monitoring, and the communication work between the terminals and the control center through M2M platform performs efficiently and reliably. © 2011 IEEE.},
author_keywords={M2M;  positioning and monitoring system;  terminal;  web service},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Basman2011237,
author={Basman, A. and Lewis, C. and Clark, C.},
title={To inclusive design through contextually extended IoC- Infusion IoC, a JavaScript library and mentality for scalable development of accessible and maintainable systems},
journal={SPLASH'11 Compilation - Proceedings of OOPSLA'11, Onward! 2011, GPCE'11, DLS'11, and SPLASH'11 Companion},
year={2011},
pages={237-255},
doi={10.1145/2048147.2048220},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355142068&doi=10.1145%2f2048147.2048220&partnerID=40&md5=195c10a12537e30ff829f79a7559147b},
abstract={Using current software development techniques, code and designs are often unmaintainable from the point of inception. Code is brittle and hard to refactor, hard to press to new purposes, and hard to understand. Here we present a system aimed at creating a model for scalable development, addressing this and several other critical problems in software construction. Such an aim is far from new, and has resembled the aims of each generation of software methodologists over the last 50 years. It deserves comment why these aims have so signally failed to be achieved, and we will present arguments as to why the combination of techniques explained here could expect to lead to novel results. Software products of today are notoriously unadaptable. An application which meets need A generally cannot be extended to meet apparently very similar need A′ without something resembling "software engineering". Applications present users with a "take it or leave it" proposition - if the software doesn't happen to meet a user's needs or preferences, there's no way to change it without writing more code, which is out of reach for most users. Indeed, software regularly fails to be easily adaptable to meet the needs of users with differing needs, such as in the case of accessibility. These "precarious values" - accessibility and usability with different devices, languages, and personal needs - are typically left until the end or ignored, and represent a significant expense in traditional approaches to software development. Often these needs are met by developing a largely unrelated version of the application, requiring maintenance of additional, separate code bases. Our aim is to enable Inclusive Design [3], whose objective is to satisfy the needs and desires of the broadest range of users possible. Every designer sets out with this objective to a certain extent, but as well as limitations of intent, there are also strong limitations placed by the technology and economics of software development. Due to the poor scaling characteristics of current techniques, even meeting one set of relatively inflexible needs can be an expensive undertaking, especially over the long term. To address these problems of adaptability, we present a model for software construction, together with a base library, Fluid Infusion, implemented in the JavaScript language. Fluid Infusion implements an Inversion of Control model, Infusion IoC, which features a notion of context as the basis for adaptability, resolved in a scope modelled in terms of a data structure, a component tree expressing the computation to be performed. In the Context-Oriented Programming community [7], this model of scoping is known as structural scoping. We will also work with a model of transparent state in which all modifiable state of interest to users is held in publicly visible locations, indexed by path strings. This model for state is isomorphic to that modeled by JSON [6], a well-known state model derived from, but not limited to, the JavaScript language. Instantiation in the model is handled by an Inversion of Control system extended from the model of similar system such as the Spring Framework or Pico first developed in the Java language. We relate such systems to goal-directed resolution systems such as Prolog, and show that they have beneficial properties such as homoiconicity [2] which have not been seen in a strong or widespread form since the days of LISP. We exhibit some cases to show how the framework enables, through a simple declarative syntax, types of adaptation and composition that are hard or impossible using traditional models of polymorphism. We also relate Infusion IoC to other software methodologies such as Aspect-Oriented Programming and Context-Oriented Programming which have been found to greatly increase flexibility and expressiveness of designs. We conclude with some remarks on the applicability of the system to the parallelisation of irregular algorithms, and its relationship to upcoming developments in the ECMAScript 6 language specification.},
author_keywords={Accessibility;  Context-oriented programming;  Inversion of control;  JavaScript;  JSON;  Transparent state},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ramirez2011,
author={Ramirez, S. and Karlsson, J. and Trelles, O.},
title={MAPI: Towards the integrated exploitation of bioinformatics Web Services},
journal={BMC Bioinformatics},
year={2011},
volume={12},
doi={10.1186/1471-2105-12-419},
art_number={419},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054949770&doi=10.1186%2f1471-2105-12-419&partnerID=40&md5=2fa9263e906348dc4c3996f63b7714ff},
abstract={Background: Bioinformatics is commonly featured as a well assorted list of available web resources. Although diversity of services is positive in general, the proliferation of tools, their dispersion and heterogeneity complicate the integrated exploitation of such data processing capacity.Results: To facilitate the construction of software clients and make integrated use of this variety of tools, we present a modular programmatic application interface (MAPI) that provides the necessary functionality for uniform representation of Web Services metadata descriptors including their management and invocation protocols of the services which they represent. This document describes the main functionality of the framework and how it can be used to facilitate the deployment of new software under a unified structure of bioinformatics Web Services. A notable feature of MAPI is the modular organization of the functionality into different modules associated with specific tasks. This means that only the modules needed for the client have to be installed, and that the module functionality can be extended without the need for re-writing the software client.Conclusions: The potential utility and versatility of the software library has been demonstrated by the implementation of several currently available clients that cover different aspects of integrated data processing, ranging from service discovery to service invocation with advanced features such as workflows composition and asynchronous services calls to multiple types of Web Services including those registered in repositories (e.g. GRID-based, SOAP, BioMOBY, R-bioconductor, and others). © 2011 Ramirez et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Šaletić2011145,
author={Šaletić, D.Z. and Andelković, M.},
title={A perceptual computer software model applied to hierarchical decision making},
journal={SISY 2011 - 9th International Symposium on Intelligent Systems and Informatics, Proceedings},
year={2011},
pages={145-150},
doi={10.1109/SISY.2011.6034311},
art_number={6034311},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054822825&doi=10.1109%2fSISY.2011.6034311&partnerID=40&md5=ff9f6d23e53586f74b35f504d2fa4fbf},
abstract={Perceptual computer (Per-C) is a concept introduced in year 2001 by Mendel. It employs the Zadeh's theory of Computing With Words to provide a model for computer and software architectures able to aid people making subjective judgments. It can be used for variety of tasks like investment decision making, social judgment making, hierarchical decision making and so on. In this paper the topic of hierarchical decision making and related introductory terms are summarized and partly reinterpreted. A newly developed application programming interface (API) for software implementations of Per-C is presented, along with several new technical details. The API has been tested on the well known problem of missile system selection from the literature. The results have been compared with those from Wu and Mendel. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sagan2011,
author={Sagan, D. and Chee, J.Y. and Finkelstein, K. and Hoffstaetter, G.},
title={Combined charged-particle and x-ray simulations using the Bmad open source software library},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2011},
volume={8141},
doi={10.1117/12.892406},
art_number={81410Y},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054065748&doi=10.1117%2f12.892406&partnerID=40&md5=b99516ad2a179b1078db92e4cf95fba6},
abstract={The open source Bmad software library, developed at Cornell University, has proved to be a useful tool for accelerator simulations owing to its modular, object-oriented design. Bmad has been used to simulate the storage ring CESR for many years, and to design and analyze the proposed Cornell Energy Recovery Linac. Work is ongoing to expand Bmad in a number of directions. In particular, the ability to be able to do a combined simulation of accelerated charged-particle beams and the x-ray beams they create is being developed. Ultimately a complete framework for simulations from the gun cathode (including space-charge) to x-ray generation, to x-ray tracking through to the experimental end-stations is envisioned. The exibility of Bmad is such that multiple propagation algorithms can be accommodated with the user selecting the appropriate algorithm for each individual element. To this end, the integration of Bmad with the SHADOW tracking code is in development. © 2011 SPIE.},
author_keywords={Accelerator;  Bmad;  Charged-particle;  Photon;  SHADOW;  Simulation;  Tracking;  X-ray},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ning201117,
author={Ning, C. and Xiang, Z.-L. and Wang, R.-H. and Wang, Y.},
title={A data-structure used to describe three-dimensional geological bodies based on borehole data},
journal={2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce, AIMSEC 2011 - Proceedings},
year={2011},
pages={17-22},
doi={10.1109/AIMSEC.2011.6010479},
art_number={6010479},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053270038&doi=10.1109%2fAIMSEC.2011.6010479&partnerID=40&md5=c4ca84add6fab78460da8d82952fb909},
abstract={In view of the shortcomings of bore and section data-based modeling, a new kind of data structure is proposed in this paper to describe three-dimensional geological body structure based on borehole data. Based on OpenGL library, the 3D geological modeling and visualization of complex geologic body were achieved in Visual C ++ 6.0. Experimental data test shows that the data-structure not only can improve modeling efficiency and update data automatically, but also can prove the functions of three-dimensional spatial intersection, query, and analysis. It is useful for the design of 3D geological modeling and visualization software and the promotion of the informatization construction in mine. © 2011 IEEE.},
author_keywords={Borehole data;  Data-structure;  Geological modeling;  Object-oriented;  Visual C++ 6.0},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luszczek2011944,
author={Luszczek, P. and Ltaief, H. and Dongarra, J.},
title={Two-stage tridiagonal reduction for dense symmetric matrices using tile algorithms on multicore architectures},
journal={Proceedings - 25th IEEE International Parallel and Distributed Processing Symposium, IPDPS 2011},
year={2011},
pages={944-955},
doi={10.1109/IPDPS.2011.91},
art_number={6012903},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053252490&doi=10.1109%2fIPDPS.2011.91&partnerID=40&md5=3e17939d83e52c95b9f404fa3d7297a4},
abstract={While successful implementations have already been written for one-sided transformations (e.g., QR, LU and Cholesky factorizations) on multicore architecture, getting high performance for two-sided reductions (e.g., Hessenberg, tridiagonal and bidiagonal reductions) is still an open and difficult research problem due to expensive memory-bound operations occurring during the panel factorization. The processor-memory speed gap continues to widen, which has even further exacerbated the problem. This paper focuses on an efficient implementation of the tridiagonal reduction, which is the first algorithmic step toward computing the spectral decomposition of a dense symmetric matrix. The original matrix is translated into a tile layout i.e., a high performance data representation, which substantially enhances data locality. Following a two-stage approach, the tile matrix is then transformed into band tridiagonal form using compute intensive kernels. The band form is further reduced to the required tridiagonal form using a left-looking bulge chasing technique to reduce memory traffic and memory contention. A dependence translation layer associated with a dynamic runtime system allows for scheduling and overlapping tasks generated from both stages. The obtained tile tridiagonal reduction significantly outperforms the state-of-the-art numerical libraries (10X against multithreaded LAPACK with optimized MKL BLAS and 2.5X against the commercial numerical software Intel MKL) from medium to large matrix sizes. © 2011 IEEE.},
author_keywords={Bulge Chasing;  Scheduling;  Tile Algorithms;  Translation Layer;  Tridiagonal Reduction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Eißler20112797,
author={Eißler, T. and Hodges, C.P. and Meier, H.},
title={Ptpan-overcoming memory limitations in oligonucleotide string matching for primer/probe design},
journal={Bioinformatics},
year={2011},
volume={27},
number={20},
pages={2797-2805},
doi={10.1093/bioinformatics/btr483},
art_number={btr483},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053958733&doi=10.1093%2fbioinformatics%2fbtr483&partnerID=40&md5=fd1b2c3172b735c131a28e6a63ec66e2},
abstract={Motivation: Nucleic acid diagnostics has high demands for non-heuristic exact and approximate oligonucleotide string matching concerning in silico primer/probe design in huge nucleic acid sequence collections. Unfortunately, public sequence repositories grow much faster than computer hardware performance and main memory capacity do. This growth imposes severe problems on existing oligonucleotide primer/probe design applications necessitating new approaches based on space-efficient indexing structures.Results: We developed PTPan (spoken Peter Pan, 'PT' is for Position Tree, the earlier name of suffix trees), a space-efficient indexing structure for approximate oligonucleotide string matching in nucleic acid sequence data. Based on suffix trees, it combines partitioning, truncation and a new suffix tree stream compression to deal with large amounts of aligned and unaligned data. PTPan operates efficiently in main memory and on secondary storage, balancing between memory consumption and runtime during construction and application. Based on PTPan, applications supporting similarity search and primer/probe design have been implemented, namely FindFamily, ProbeMatch and ProbeDesign. All three use a weighted Levenshtein distance metric for approximative queries to find and rate matches with indels as well as substitutions. We integrated PTPan in the worldwide used software package ARB to demonstrate usability and performance. Comparing PTPan and the original ARB index for the very large ssu-rRNA database SILVA, we recognized a shorter construction time, extended functionality and dramatically reduced memory requirements at the price of expanded, but very reasonable query times. PTPan enables indexing of huge nucleic acid sequence collections at reasonable application response times. Not being limited by main memory, PTPan constitutes a major advancement regarding rapid oligonucleotide string matching in primer/probe design now and in the future facing the enormous growth of molecular sequence data. © The Author 2011. Published by Oxford University Press. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Russell-Smith2011760,
author={Russell-Smith, S. and Lepech, M.},
title={Dynamic life cycle assessment of building design and retrofit processes},
journal={Congress on Computing in Civil Engineering, Proceedings},
year={2011},
pages={760-767},
doi={10.1061/41182(416)94},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053140316&doi=10.1061%2f41182%28416%2994&partnerID=40&md5=9f09c26bcb5051a2080327fe75524652},
abstract={Designers and managers of buildings and other constructed facilities cannot easily quantify the sustainability impacts of structures for improved analysis, management, or decision-making. This is due in part to the lack of interoperability between design and analysis software and datasets that enable full life cycle assessment (LCA) of constructed facilities. This work develops a computational framework to enable building designers, engineers, contractors, and managers to reliably and efficiently construct dynamic life cycle models that capture environmental impacts associated with every life cycle phase. This includes 3D architectural tools, structural software, and virtual design and construction packages. Use phase impacts can be quantified using distributed sensor networks. This integration provides a dynamic LCA modeling platform for management of facility footprints in real-time during construction and use phases, offering unique analysis opportunities to examine the tradeoffs between design and construction/operation decisions. © 2011 ASCE.},
author_keywords={Building design;  Life cycles;  Rehabilitation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rico-Gallego2011337,
author={Rico-Gallego, J.-A. and Díaz-Martín, J.-C.},
title={Performance evaluation of thread-based MPI in shared memory},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6960 LNCS},
pages={337-338},
doi={10.1007/978-3-642-24449-0_42},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053027077&doi=10.1007%2f978-3-642-24449-0_42&partnerID=40&md5=f44cb231a88bd2541d9ee9058b973e91},
abstract={Although Unified Parallel C and OpenMP are being proposed for supporting more efficiently multicore architectures, the fact is that MPI is still used as a useful model on shared memory machines. Traditional mainstream MPI implementations as MPICH2 and OpenMPI build each MPI task as a process, an approach that presents some disadvantages in shared memory because message passing between processes usually requires two copies. One-copy communication can be achieved with operating system support, through kernel modules like KNEM or Limic, techniques like SMARTMAP, or special system calls like vmsplice, with disadvantages mainly in portability and limited improvements in performance. It is also possible building each MPI task as a thread. This is not a new concept. Implementations such as TOMPI, TMPI, or the newer MPI Actor or MPC-MPI run an MPI node as a thread, each one stressing different goals. AzequiaMPI is a thread-based but still a full conformant open source implementation of the MPI-1.3 standard. AzequiaMPI shows that MPI performance can be significatively improved by fully exploiting a single shared address space. © 2011 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kapitza2011302,
author={Kapitza, P.J.},
title={An implementation of Heegaard diagrams},
journal={Proceedings of the Annual Southeast Conference},
year={2011},
pages={302-303},
doi={10.1145/2016039.2016118},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052364245&doi=10.1145%2f2016039.2016118&partnerID=40&md5=e4e050828897ad26aab61e7132bcfa13},
abstract={A software platform for the construction and study of three dimensional manifold structures and more generally, of two dimensional cell complexes is presented. Interactive operations allow the construction of 3-manifold spaces in planar form. Manifold invariants, based upon the diagrams and their associated presentations are constructed directly. The package incorporates the operations defined by J. Singer. This package enables a user to graphically define and apply 3-manifold operations in the context of Heegaard Diagrams. © 2011 Author.},
author_keywords={cell complex;  Heegaard;  manifold},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2011,
author={Liu, Y. and Du, M. and Cai, G.},
title={Design and implementation of municipal facilities management system based on mobile mapping system and wireless transmission system},
journal={Proceedings - 2011 19th International Conference on Geoinformatics, Geoinformatics 2011},
year={2011},
doi={10.1109/GeoInformatics.2011.5980848},
art_number={5980848},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052350266&doi=10.1109%2fGeoInformatics.2011.5980848&partnerID=40&md5=6bf8d830bb7613ce15f7a3c36583619c},
abstract={This paper focuses on the design and development of Municipal Facilities Management System (MFMS). Street images and videos were collected by Wuhan LD2000-R Mobile Mapping System, and real-time videos on site under emergency conditions were captured by Wireless Transmission System. In MFMS, any street can be queried and located, visual information of facilities on the streets can be viewed, as well as real-time videos on site can be checked quickly. Furthermore, the size of facilities or the distances can be measured directly on the images; the density of facilities can also be calculated, etc. MFMS is a C/S software architecture, C# was selected as programming language and SQL Server 2005 as database server, and TrueMapEngine API and ArcEngine as development tools. To test how MFMS can be effectively used to manage municipal facilities, the system has been deployed in Beijing Garden Bureau of Xicheng District for their actual business. © 2011 IEEE.},
author_keywords={Design and Implementation;  Mobile Mapping System;  Municipal Facilities Management;  Wireless Transmission System},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kwon2011361,
author={Kwon, J. and Hailes, S.},
title={A lightweight, component-based approach to engineering reconfigurable embedded real-time control software},
journal={Proceedings - 9th IEEE International Symposium on Parallel and Distributed Processing with Applications Workshops, ISPAW 2011 - ICASE 2011, SGH 2011, GSDP 2011},
year={2011},
pages={361-366},
doi={10.1109/ISPAW.2011.69},
art_number={5952002},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051991030&doi=10.1109%2fISPAW.2011.69&partnerID=40&md5=ad427fb0d3ba7ae9fcbae05ba77513d9},
abstract={The cost of poor or repeat engineering in complex control systems is extremely high, and flexibility in software design and implementation is one of the key factors in staying competitive in the market. Complexity can be managed most effectively if the underlying software systems support structured, standardised, high-level abstraction layers that encapsulate unnecessary details behind well-defined interfaces. Moreover, since the costs of software maintenance are often as high as that of initial development, the ease with which it is possible flexibly to reconfigure, re-engineer, and replace software components in operational systems is also critical. In this paper, we present a lightweight, component-based approach to engineering embedded real-time control software, which is realized in the form of a middleware system named MIREA. The middleware supports dynamic reconfiguration of components written in C/C++, and addresses variability management in relation to non-functional properties, such as quality-of-service (QoS) and real-time scheduling. Users are allowed to componentize existing libraries easily, such as the standard NIST 4D/Real-time Control Systems (RCS) library, which has been successfully used in many U.S government-driven intelligent control projects, and to reuse them as dynamically reconfigurable components. A realistic illustration is provided showing how control systems are structured and reconfigured using our approach. In fact, we discuss our approach to control using a fusion of NIST RCS as a means of architecting a real time control system and MIREA as a means of realising that architecture. Our progress to date suggests that MIREA is indeed well suited as a middleware facilitating the construction of efficient, lightweight, and scalable real-time embedded control systems. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nascimento201134,
author={Nascimento, M.R. and Rothenberg, C.E. and Salvador, M.R. and Corrêa, C.N.A. and De Lucena, S.C. and Magalhães, M.F.},
title={Virtual routers as a service: The RouteFlow approach leveraging software-defined networks},
journal={Proceedings of the 6th International Conference on Future Internet Technologies, CFI11},
year={2011},
pages={34-37},
doi={10.1145/2002396.2002405},
note={cited By 100},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051567936&doi=10.1145%2f2002396.2002405&partnerID=40&md5=8c6a89a1abd4fa30c2a6682f1ef9e8c8},
abstract={The networking equipment market is being transformed by the need for greater openness and flexibility, not only for research purposes but also for in-house innovation by the equipment owners. In contrast to networking gear following the model of computer mainframes, where closed software runs on proprietary hardware, the software-defined networking approach effectively decouples the data from the control plane via an open API (i.e., OpenFlow protocol) that allows the (remote) control of packet forwarding engines. Motivated by this scenario, we propose RouteFlow, a commodity routing architecture that combines the line-rate performance of commercial hardware with the flexibility of open-source routing stacks (remotely) running on general purpose computers. The outcome is a novel point in the design space of commodity routing solutions with far-reaching implications towards virtual routers and IP networks as a service. This paper documents the progress achieved in the design and prototype implementation of our work and outlines our research agenda that calls for a community-driven approach. © 2011 ACM.},
author_keywords={C.2.1 [Network Architecture and Design];  Packet-switching networks},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Katayama2011,
author={Katayama, T. and Wilkinson, M.D. and Vos, R. and Kawashima, T. and Kawashima, S. and Nakao, M. and Yamamoto, Y. and Chun, H.-W. and Yamaguchi, A. and Kawano, S. and Aerts, J. and Aoki-Kinoshita, K.F. and Arakawa, K. and Aranda, B. and Bonnal, R.J.P. and Fernández, J.M. and Fujisawa, T. and Gordon, P.M.K. and Goto, N. and Haider, S. and Harris, T. and Hatakeyama, T. and Ho, I. and Itoh, M. and Kasprzyk, A. and Kido, N. and Kim, Y.-J. and Kinjo, A.R. and Konishi, F. and Kovarskaya, Y. and von Kuster, G. and Labarga, A. and Limviphuvadh, V. and McCarthy, L. and Nakamura, Y. and Nam, Y. and Nishida, K. and Nishimura, K. and Nishizawa, T. and Ogishima, S. and Oinn, T. and Okamoto, S. and Okuda, S. and Ono, K. and Oshita, K. and Park, K.-J. and Putnam, N. and Senger, M. and Severin, J. and Shigemoto, Y. and Sugawara, H. and Taylor, J. and Trelles, O. and Yamasaki, C. and Yamashita, R. and Satoh, N. and Takagi, T.},
title={The 2nd DBCLS BioHackathon: Interoperable bioinformatics Web services for integrated applications},
journal={Journal of Biomedical Semantics},
year={2011},
volume={2},
number={1},
doi={10.1186/2041-1480-2-4},
art_number={4},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911884509&doi=10.1186%2f2041-1480-2-4&partnerID=40&md5=9583f6b50f18712dce9caa74df53e44a},
abstract={Background: The interaction between biological researchers and the bioinformatics tools they use is still hampered by incomplete interoperability between such tools. To ensure interoperability initiatives are effectively deployed, end-user applications need to be aware of, and support, best practices and standards. Here, we report on an initiative in which software developers and genome biologists came together to explore and raise awareness of these issues: BioHackathon 2009. Results: Developers in attendance came from diverse backgrounds, with experts in Web services, workflow tools, text mining and visualization. Genome biologists provided expertise and exemplar data from the domains of sequence and pathway analysis and glyco-informatics. One goal of the meeting was to evaluate the ability to address real world use cases in these domains using the tools that the developers represented. This resulted in i) a workflow to annotate 100,000 sequences from an invertebrate species; ii) an integrated system for analysis of the transcription factor binding sites (TFBSs) enriched based on differential gene expression data obtained from a microarray experiment; iii) a workflow to enumerate putative physical protein interactions among enzymes in a metabolic pathway using protein structure data; iv) a workflow to analyze glyco-gene-related diseases by searching for human homologs of glyco-genes in other species, such as fruit flies, and retrieving their phenotype-annotated SNPs. Conclusions: Beyond deriving prototype solutions for each use-case, a second major purpose of the BioHackathon was to highlight areas of insufficiency. We discuss the issues raised by our exploration of the problem/solution space, concluding that there are still problems with the way Web services are modeled and annotated, including: i) the absence of several useful data or analysis functions in the Web service "space"; ii) the lack of documentation of methods; iii) lack of compliance with the SOAP/WSDL specification among and between various programming-language libraries; and iv) incompatibility between various bioinformatics data formats. Although it was still difficult to solve real world problems posed to the developers by the biological researchers in attendance because of these problems, we note the promise of addressing these issues within a semantic framework. © 2011 Katayama et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kalibera2011,
author={Kalibera, T. and Pizlo, F. and Hosking, A.L. and Vitek, J.},
title={Scheduling real-time garbage collection on uniprocessors},
journal={ACM Transactions on Computer Systems},
year={2011},
volume={29},
number={3},
doi={10.1145/2003690.2003692},
art_number={8},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052354777&doi=10.1145%2f2003690.2003692&partnerID=40&md5=d67bd1117a4444a6e1020907bac55a16},
abstract={Managed languages such as Java and C# are increasingly being considered for hard real-time applications because of their productivity and software engineering advantages. Automatic memory management, or garbage collection, is a key enabler for robust, reusable libraries, yet remains a challenge for analysis and implementation of real-time execution environments. This article comprehensively compares leading approaches to hard real-time garbage collection. There are many design decisions involved in selecting a real-time garbage collection algorithm. For time-based garbage collectors on uniprocessors one must choose whether to use periodic, slack-based or hybrid scheduling. A significant impediment to valid experimental comparison of such choices is that commercial implementations use completely different proprietary infrastructures. We present Minuteman, a framework for experimenting with real-time collection algorithms in the context of a high-performance execution environment for real-time Java. We provide the first comparison of the approaches, both experimentally using realistic workloads, and analytically in terms of schedulability. © 2011 ACM.},
author_keywords={Joint scheduling;  Real-time garbage collection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20111721,
author={Wang, L. and Lee, F.X.},
title={MathQCDSR: A Mathematica package for QCD sum rules calculations},
journal={Computer Physics Communications},
year={2011},
volume={182},
number={8},
pages={1721-1731},
doi={10.1016/j.cpc.2011.04.017},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957810074&doi=10.1016%2fj.cpc.2011.04.017&partnerID=40&md5=19985600dd0c082d773d22e122a9f0d5},
abstract={We present a software package written in Mathematica for standard QCD sum rules calculations. Two examples are given to demonstrate how to use the package. One is for the mass spectrum of octet baryons from two-point correlation functions; the other for the magnetic moments of octet baryons in the external-field method. The free package FeynCalc is used to handle the gamma-matrix algebra. In addition to two notebooks for the construction of the QCD sum rules, two corresponding notebooks are provided for a Monte Carlo-based numerical analysis, complete with in-line graphical display of sum rule matching, error distributions, and scatter plots for correlations. Program summary: Program title: MathQCDSR Catalogue identifier: AEJA-v1-0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEJA-v1-0.html Program obtainable from: CPC Program Library, Queen?s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 93 897 No. of bytes in distributed program, including test data, etc.: 631 481 Distribution format: tar.gz Programming language: Mathematica Computer: PCs and Workstations Operating system: Any OS that supports Mathematica. The package has been tested under Windows XP, Macintosh OS X, and Linux Classification: 11.5 External routines: FeynCalc (http://www.feyncalc.org/). It is a freely available Mathematica package for high-energy physics calculations. Here it is used primarily to handle gamma-matrix algebra. Nature of problem: The QCD sum rule method is a nonperturbative approach to solving quantum chromodynamics (QCD), the fundamental theory of the strong force. The approach establishes a direct link between hadron phenomenology and the QCD vacuum structure via a few QCD parameters called vacuum condensates and susceptibilities. It has been widely applied in nuclear and particle physics to gain insight into various aspects of strong-interaction physics. Solution method: First, QCD sum rules are constructed by evaluating correlation functions from two perspectives. On the quark level, it leads to a function of QCD parameters and the Borel mass parameter M. On the hadronic level, it leads to a function of phenomenological parameters and the same M. By numerically matching the two sides over a range in M, the phenomenological parameters can be extracted. The construction involves a large amount of gamma-matrix algebra, Fourier transform, and Borel transform. The matching usually involves searching for minimum χ2. We employ a Monte Carlo-based procedure to perform the analysis which allows for realistic error estimates. Restrictions: The package deals with only standard (SVZ) QCD sum rule calculations. It can be easily adapted to handle other variants of the method (like finite-energy sum rules). Due to the use of FeynCalc, two of the notebooks (qcdsr2pt-construction.nb and qcdsr3pt-construction.nb) only run on version 6.0 of Mathematica. The other two can run on any version. Additional comments: The package consists of the following 4 notebooks:qcdsr2pt-construction.nb - This notebook constructs the QCD sum rules for octet baryon masses and outputs them, one particle at a time, to disk in plain text for analysis. For reference, we include all the output files (named Mass-*.txt) as part of the package, totaling 8 files in about 20 lines. The user should generate the outputs on their own computer and check against the supplied ones.qcdsr2pt-analysis.nb - This notebook reads and analyzes the QCD sum rules produced by qcdsr2pt-construction.nb. The user can save the graphics in the analysis to disk in a variety of formats.qcdsr3pt- construction.nb - This notebook constructs the QCD sum rules for the octet baryon magnetic moments and outputs them, one particle at a time, to disk in plain text for analysis. Again, for reference, we include all the output files (named Mag-*.txt), totaling 24 files in about 400 lines. The user should generate the outputs on their own computer and check against the supplied ones to make sure the program is running properly.qcdsr3pt-analysis.nb - This notebook reads and analyzes the QCD sum rules produced by qcdsr3pt-construction. nb. The user can save the graphics in the analysis to disk in a variety of formats. Each notebook can be run separately, apart from the simple interface between the construction and analysis programs via plain text files written to disk. Running time: For mass calculations, qcdsr2ptconstruction.nb and qcdsr2pt-analysis.nb take about a minute each to run on a laptop. For magnetic moment calculations, qcdsr3pt-construction.nb can take up to 10 minutes for a given particle, and qcdsr3pt-analysis.nb typically a few minutes, depending on the number of Monte Carlo samples. © 2011 Elsevier B.V. All rights reserved.},
author_keywords={Baryon masses and magnetic moments;  FeynCalc;  Mathematica;  QCD sum rule method;  Quantum chromodynamics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Falleri2011260,
author={Falleri, J.-R. and Denier, S. and Laval, J. and Vismara, P. and Ducasse, S.},
title={Efficient retrieval and ranking of undesired package cycles in large software systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6705 LNCS},
pages={260-275},
doi={10.1007/978-3-642-21952-8_19},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960158395&doi=10.1007%2f978-3-642-21952-8_19&partnerID=40&md5=ec700c46eeda5735f661a360f5a7e38e},
abstract={Many design guidelines state that a software system architecture should avoid cycles between its packages. Yet such cycles appear again and again in many programs. We believe that the existing approaches for cycle detection are too coarse to assist the developers to remove cycles from their programs. In this paper, we describe an efficient algorithm that performs a fine-grained analysis of the cycles among the packages of an application. In addition, we define a metric to rank cycles by their level of undesirability, prioritizing the cycles that seems the more undesired by the developers. Our approach is validated on two large and mature software systems in Java and Smalltalk. © 2011 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Veryzhenko201174,
author={Veryzhenko, I. and Mathieu, P. and Brandouy, O.},
title={Key points for realistic agent-based financial market simulations},
journal={ICAART 2011 - Proceedings of the 3rd International Conference on Agents and Artificial Intelligence},
year={2011},
volume={2},
pages={74-83},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960134555&partnerID=40&md5=03af919f0bbf98b8fb3055a1f775f787},
abstract={The purpose of this paper is to define software engineering abstractions that provide a generic framework for stock market simulations. We demonstrate a series of key points and principles that has governed the development of an Agent-Based financial market in the form of an API. The simulator architecture is presented. During artificial market construction we have faced the whole variety of agent-based modeling issues and solved them : local interaction, distributed knowledge and resources, heterogeneous environments, agents autonomy, artificial intelligence, speech acts, discrete scheduling and simulation. Our study demonstrates that the choices made for agent-based modeling in this context deeply impact the resulting market dynamics and proposes a series of advances regarding the main limits the existing platforms actually meet.},
author_keywords={Agents interactions;  Artificial market;  Market microstructure;  Multi-agent systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cozzi2011,
author={Cozzi, P. and Ring, K.},
title={Under the hood of virtual globes},
journal={ACM International Conference Proceeding Series},
year={2011},
doi={10.1145/1999320.1999403},
art_number={79},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960080305&doi=10.1145%2f1999320.1999403&partnerID=40&md5=022c3e3ce5872558243c7b8dae994099},
abstract={Virtual globes are a key tool for visualizing large geospatial datasets. This half day course goes under the hood of virtual globes and looks at their implementation techniques from the software developer's perspective. Our focus is not on one particular virtual globe, such as NASA World Wind or Google Earth; instead, we discuss common techniques used by many virtual globes, including coordinate transformations, globe representations, precision, multithreading, and terrain rendering. We also consider the differences between virtual globe 3D engines and game engines. Attendees should have software development experience. A background in computer graphics is useful, but not required.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kalomiros2011496,
author={Kalomiros, J.A. and Lygouras, J.},
title={Design and hardware implementation of a stereo-matching system based on dynamic programming},
journal={Microprocessors and Microsystems},
year={2011},
volume={35},
number={5},
pages={496-509},
doi={10.1016/j.micpro.2011.04.005},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80955176419&doi=10.1016%2fj.micpro.2011.04.005&partnerID=40&md5=2d9ef4f7c65a7026ff0b9dd44d709327},
abstract={A new real-time stereo system is presented based on a hardware implementation of an efficient Dynamic Programming algorithm. A simple state-machine calculates the cost-matrix along the diagonal of the 2-D disparity space for each epipolar pair of image scan-lines. Minimum transition costs are stored in embedded RAM and are used to backtrack disparities at clock rate. All calculations are within a pre-determined slice of the cost plane, representing the useful disparity range. The system is designed as a VHDL library component and is implemented as a SoC in a medium-capacity Field Programmable Gate Array chip. It can process stereo-pairs in full VGA resolution at a rate of 25 Mpixels/s and produces 8-bit dense disparity maps within a range of disparities up to 65 pixels. The design is evaluated comparing to ground truth and in terms of resource usage. It is also compared to a software implementation of the Dynamic Programming algorithm and to other FPGA-based stereo systems. © 2011 Elsevier B.V. All rights reserved.},
author_keywords={Dynamic programming;  Hardware design;  Real-time systems;  Stereo vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2011,
author={Li, J. and Gong, B. and Chen, X. and Liu, T. and Wu, C. and Zhang, F. and Li, C. and Li, X. and Rao, S. and Li, X.},
title={DOSim: An R package for similarity between diseases based on Disease Ontology},
journal={BMC Bioinformatics},
year={2011},
volume={12},
doi={10.1186/1471-2105-12-266},
art_number={266},
note={cited By 73},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959632257&doi=10.1186%2f1471-2105-12-266&partnerID=40&md5=f80e865da07ad90e0dab55beda8a5dc3},
abstract={Background: The construction of the Disease Ontology (DO) has helped promote the investigation of diseases and disease risk factors. DO enables researchers to analyse disease similarity by adopting semantic similarity measures, and has expanded our understanding of the relationships between different diseases and to classify them. Simultaneously, similarities between genes can also be analysed by their associations with similar diseases. As a result, disease heterogeneity is better understood and insights into the molecular pathogenesis of similar diseases have been gained. However, bioinformatics tools that provide easy and straight forward ways to use DO to study disease and gene similarity simultaneously are required.Results: We have developed an R-based software package (DOSim) to compute the similarity between diseases and to measure the similarity between human genes in terms of diseases. DOSim incorporates a DO-based enrichment analysis function that can be used to explore the disease feature of an independent gene set. A multilayered enrichment analysis (GO and KEGG annotation) annotation function that helps users explore the biological meaning implied in a newly detected gene module is also part of the DOSim package. We used the disease similarity application to demonstrate the relationship between 128 different DO cancer terms. The hierarchical clustering of these 128 different cancers showed modular characteristics. In another case study, we used the gene similarity application on 361 obesity-related genes. The results revealed the complex pathogenesis of obesity. In addition, the gene module detection and gene module multilayered annotation functions in DOSim when applied on these 361 obesity-related genes helped extend our understanding of the complex pathogenesis of obesity risk phenotypes and the heterogeneity of obesity-related diseases.Conclusions: DOSim can be used to detect disease-driven gene modules, and to annotate the modules for functions and pathways. The DOSim package can also be used to visualise DO structure. DOSim can reflect the modular characteristic of disease related genes and promote our understanding of the complex pathogenesis of diseases. DOSim is available on the Comprehensive R Archive Network (CRAN) or http://bioinfo.hrbmu.edu.cn/dosim. © 2011 Li et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Malik2011190,
author={Malik, M.Z. and Siddiqi, J.H. and Khurshid, S.},
title={Constraint-based program debugging using data structure repair},
journal={Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation, ICST 2011},
year={2011},
pages={190-199},
doi={10.1109/ICST.2011.65},
art_number={5770608},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958757899&doi=10.1109%2fICST.2011.65&partnerID=40&md5=ac7303deafa8244555ceb287f4e41649},
abstract={Developers have used data structure repair over the last few decades as an effective means to recover on-the-fly from errors in program state. Traditional repair techniques were based on dedicated repair routines, whereas more recent techniques have used invariants that describe desired structural properties as the basis for repair. All repair techniques are designed with one primary goal: run-time error recovery. However, the actions that any such technique performs to repair an erroneous program state are meant to produce the effect of the actions of a (hypothetical) correct program. The key insight in this paper is that repair actions on the program state can guide debugging of code (when the erroneous program execution is due to a fault in the program and not an external event).This paper presents an approach that abstracts concrete repair actions that a routine performs to repair an erroneous state into a sequence of program statements that perform the same actions using variables visible in the scope of the faulty code. Thus, appending the generated statements to the original code is akin to performing the repair from within the program. Our implementation uses the Juzi data structure repair tool as an enabling technology. Experimental results using a library data structure as well as two applications demonstrate the effectiveness of our approach in enabling repair of faulty code. © 2011 IEEE.},
author_keywords={Automated Debugging;  Data Structure Repair;  Error Recovery;  Fault Localization;  Program Repair},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lustig20111,
author={Lustig, Y. and Vardi, M.Y.},
title={Synthesis from recursive-components libraries},
journal={Electronic Proceedings in Theoretical Computer Science, EPTCS},
year={2011},
volume={54},
pages={1-16},
doi={10.4204/EPTCS.54.1},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014946842&doi=10.4204%2fEPTCS.54.1&partnerID=40&md5=cb179559cc546ac61560b9709874cea4},
abstract={Synthesis is the automatic construction of a system from its specification. In classical synthesis algorithms it is always assumed that the system is�constructed from scratch� rather than composed from reusable components. This, of course, rarely happens in real life. In real life, almost every non-trivial commercial software system relies heavily on using libraries of reusable components. Furthermore, other contexts, such as web-service orchestration, can be modeled as synthesis of a system from a library of components. In 2009 we introduced LTL synthesis from libraries of reusable components. Here, we extend the work and study synthesis from component libraries with �call and return� control flow structure. Such control-flow structure is very common in software systems. We define the problem of Nested-Words Temporal Logic (NWTL) synthesis from recursive component libraries, where NWTL is a specification formalism, richer than LTL, that is suitable for �call and return� computations. We solve the problem, providing a synthesis algorithm, and show the problem is 2EXPTIME-complete, as standard synthesis. � Yoad Lustig and Moshe Y. Vardi.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2011,
author={Chen, W.H. and Sun, P.P. and Lu, Y. and Guo, W.W. and Huang, Y.X. and Ma, Z.Q.},
title={MimoPro: A more efficient Web-based tool for epitope prediction using phage display libraries},
journal={BMC Bioinformatics},
year={2011},
volume={12},
doi={10.1186/1471-2105-12-199},
art_number={199},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052876042&doi=10.1186%2f1471-2105-12-199&partnerID=40&md5=1a7628fac919f83556b36e8233ed9ca8},
abstract={Background: A B-cell epitope is a group of residues on the surface of an antigen which stimulates humoral responses. Locating these epitopes on antigens is important for the purpose of effective vaccine design. In recent years, mapping affinity-selected peptides screened from a random phage display library to the native epitope has become popular in epitope prediction. These peptides, also known as mimotopes, share the similar structure and function with the corresponding native epitopes. Great effort has been made in using this similarity between such mimotopes and native epitopes in prediction, which has resulted in better outcomes than statistics-based methods can. However, it cannot maintain a high degree of satisfaction in various circumstances.Results: In this study, we propose a new method that maps a group of mimotopes back to a source antigen so as to locate the interacting epitope on the antigen. The core of this method is a searching algorithm that is incorporated with both dynamic programming (DP) and branch and bound (BB) optimization and operated on a series of overlapping patches on the surface of a protein. These patches are then transformed to a number of graphs using an adaptable distance threshold (ADT) regulated by an appropriate compactness factor (CF), a novel parameter proposed in this study. Compared with both Pep-3D-Search and PepSurf, two leading graph-based search tools, on average from the results of 18 test cases, MimoPro, the Web-based implementation of our proposed method, performed better in sensitivity, precision, and Matthews correlation coefficient (MCC) than both did in epitope prediction. In addition, MimoPro is significantly faster than both Pep-3D-Search and PepSurf in processing.Conclusions: Our search algorithm designed for processing well constructed graphs using an ADT regulated by CF is more sensitive and significantly faster than other graph-based approaches in epitope prediction. MimoPro is a viable alternative to both PepSurf and Pep-3D-Search for epitope prediction in the same kind, and freely accessible through the MimoPro server located at http://informatics.nenu.edu.cn/MimoPro. © 2011 Chen et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Geveler2011113,
author={Geveler, M. and Ribbrock, D. and Mallach, S. and Göddeke, D.},
title={A simulation suite for Lattice-Boltzmann based real-time CFD applications exploiting multi-level parallelism on modern multi- and many-core architectures},
journal={Journal of Computational Science},
year={2011},
volume={2},
number={2},
pages={113-123},
doi={10.1016/j.jocs.2011.01.008},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958766126&doi=10.1016%2fj.jocs.2011.01.008&partnerID=40&md5=474459cfff3863fe2505a3d982c18324},
abstract={We present a software approach to hardware-oriented numerics which builds upon an augmented, previously published set of open-source libraries facilitating portable code development and optimisation on a wide range of modern computer architectures. In order to maximise efficiency, we exploit all levels of parallelism, including vectorisation within CPU cores, the Cell BE and GPUs, shared memory thread-level parallelism between cores, and parallelism between heterogeneous distributed memory resources in clusters. To evaluate and validate our approach, we implement a collection of modular building blocks for the easy and fast assembly and development of CFD applications based on the shallow water equations: We combine the Lattice-Boltzmann method with fluid-structure interaction techniques in order to achieve real-time simulations targeting interactive virtual environments. Our results demonstrate that recent multi-core CPUs outperform the Cell BE, while GPUs are significantly faster than conventional multi-threaded SSE code. In addition, we verify good scalability properties of our application on small clusters. © 2011 Elsevier B.V.},
author_keywords={HPC software development multi- and many-core architectures;  Lattice-Boltzmann methods fluid-structure interaction;  Real-time simulationm},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stoma2011,
author={Stoma, S. and Fröhlich, M. and Gerber, S. and Klipp, E.},
title={STSE: Spatio-Temporal Simulation Environment Dedicated to Biology},
journal={BMC Bioinformatics},
year={2011},
volume={12},
doi={10.1186/1471-2105-12-126},
art_number={126},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955143469&doi=10.1186%2f1471-2105-12-126&partnerID=40&md5=8de975f26e1be39389db405ac618069a},
abstract={Background: Recently, the availability of high-resolution microscopy together with the advancements in the development of biomarkers as reporters of biomolecular interactions increased the importance of imaging methods in molecular cell biology. These techniques enable the investigation of cellular characteristics like volume, size and geometry as well as volume and geometry of intracellular compartments, and the amount of existing proteins in a spatially resolved manner. Such detailed investigations opened up many new areas of research in the study of spatial, complex and dynamic cellular systems. One of the crucial challenges for the study of such systems is the design of a well stuctured and optimized workflow to provide a systematic and efficient hypothesis verification. Computer Science can efficiently address this task by providing software that facilitates handling, analysis, and evaluation of biological data to the benefit of experimenters and modelers.Results: The Spatio-Temporal Simulation Environment (STSE) is a set of open-source tools provided to conduct spatio-temporal simulations in discrete structures based on microscopy images. The framework contains modules to digitize, represent, analyze, and mathematically model spatial distributions of biochemical species. Graphical user interface (GUI) tools provided with the software enable meshing of the simulation space based on the Voronoi concept. In addition, it supports to automatically acquire spatial information to the mesh from the images based on pixel luminosity (e.g. corresponding to molecular levels from microscopy images). STSE is freely available either as a stand-alone version or included in the linux live distribution Systems Biology Operational Software (SB.OS) and can be downloaded from http://www.stse-software.org/. The Python source code as well as a comprehensive user manual and video tutorials are also offered to the research community. We discuss main concepts of the STSE design and workflow. We demonstrate it's usefulness using the example of a signaling cascade leading to formation of a morphological gradient of Fus3 within the cytoplasm of the mating yeast cell Saccharomyces cerevisiae.Conclusions: STSE is an efficient and powerful novel platform, designed for computational handling and evaluation of microscopic images. It allows for an uninterrupted workflow including digitization, representation, analysis, and mathematical modeling. By providing the means to relate the simulation to the image data it allows for systematic, image driven model validation or rejection. STSE can be scripted and extended using the Python language. STSE should be considered rather as an API together with workflow guidelines and a collection of GUI tools than a stand alone application. The priority of the project is to provide an easy and intuitive way of extending and customizing software using the Python language. © 2011 Stoma et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cretu201195,
author={Cretu, L.-G.},
title={Event-driven replication in distributed systems},
journal={Proceedings of the 4th India Software Engineering Conference 2011, ISEC'11},
year={2011},
pages={95-98},
doi={10.1145/1953355.1953367},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953885129&doi=10.1145%2f1953355.1953367&partnerID=40&md5=f95f672cbf754dfa2fc880207403d56d},
abstract={In order to address problems related to geographical distribution of application instances, data distribution has been a hot topic for years. Recently, the spectacular evolution of bandwidth, frameworks and components available to build Rich Internet Applications has brought back data centralization practices. As a consequence, most database-centric applications today (e.g. business management software) are being developed using a four tier architecture: presentation logic (web tier), business logic (services), persistence services (object relational mappings and SQL variants) and database management system (one single database). While plenty of data distribution scenarios can still be found in the real world, existing solutions mainly come from DBMS providers. However, in practice it has been proven that these low level replication techniques are not really easy to use and, in most cases, they do not fit well for real-time data replication, due to the fact that: (1) highly reliable network connections are required (uninterrupted connections, in most cases) and (2) every bit has the same critical importance as any other in order to keep data integrity. Even if object-oriented replication techniques exist, such as Hibernate replication API (the well known ORM framework in Java), this paper will show that they cannot be used alone in real production environments where network connections are highly unreliable and expensive. In this short paper we will present one practical case of multiple, geographically distributed application instances, using their own local databases, while real-time synchronization over a highly unreliable and expensive network is needed. The novelty of the solution described in this paper consists in transferring the replication responsibility from database level to application logic level (business services) using a message oriented model. Unlike DBMS replication tools and techniques, our proposed model does provide the critical requirements of real-world replication needs: 1) fault-tolerance and failure recovery with event-based exception handling while still keeping the system up and running; 2) information-centric instead of bit-centric system; 3) ability to deal with numerous, unpredictable, network connections breakdowns; 4) ability to replicate data over heterogeneous platforms (both application and database level); 5) ability to run within an a multi-version application environment (different application versions on different replication nodes).},
author_keywords={Distributed systems;  Event-driven system;  Message-oriented model;  Replication;  Service oriented architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Theodoropoulos20113,
author={Theodoropoulos, D. and Kuzmanov, G. and Gaydadjiev, G.},
title={A reconfigurable audio beamforming multi-core processor},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6578 LNCS},
pages={3-15},
doi={10.1007/978-3-642-19475-7_3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953185740&doi=10.1007%2f978-3-642-19475-7_3&partnerID=40&md5=1a3f9ca0ad662dbed50460eb6a6109fb},
abstract={Over the last years, the Beamforming technique has been adopted by the audio engineering society to amplify the signal of an acoustic source, while attenuating any ambient noise. Existing software implementations provide a flexible customizing environment, however they introduce performance limitations and excessive power consumption overheads. On the other hand, hardware approaches achieve significantly better performance and lower power consumption compared to the software ones, but they lack the flexibility of a high-level versatile programming environment. To address these drawbacks, we have already proposed a minimalistic processor architecture tailoring audio Beamforming applications to configurable hardware. In this paper, we present its application as a multi-core reconfigurable Beamforming processor and describe our hardware prototype, which is mapped onto a Virtex4FX60 FPGA. Our approach combines software programming flexibility with improved hardware performance, low power consumption and compact program-executable memory footprint. Experimental results suggest that our FPGA-based processor, running at 100 MHz, can extract in real-time up to 14 acoustic sources 2.6 times faster than a 3.0 GHz Core2 Duo OpenMP-based implementation. Furthermore, it dissipates an order of magnitude less energy, compared to the general purpose processor software implementation. © 2011 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tian2011,
author={Tian, Y. and Huang, T. and Gao, W.},
title={iULib: Where UDL and Wikipedia could meet},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2011},
volume={7879},
doi={10.1117/12.876534},
art_number={78790G},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953019862&doi=10.1117%2f12.876534&partnerID=40&md5=7a0ee234f55f624c8effcf3b13a84632},
abstract={Empowering the group collaboration and knowledge-sharing capabilities for the Universal Digital Library (UDL) is definitely an important work after more than 1.5 million digitalized books were open to access online. One motivation of developing such a platform is the emergence of Web 2.0 in recent years, especially with the rapidly increased popularity of Wikipedia. This paper presents our vision, which we call iULib, about where and how UDL and Wikipedia could meet. In the first phase, we directly apply the Wiki architecture and software in UDL to upgrade the digital library as an interactive platform that facilitates community and collaboration. Preliminary implementation shows the feasibility and reliability of our design. Furthermore, as a free encyclopedia that assembles contributions from different users, Wikipedia may also be used as a knowledge base for UDL. As a result, UDL can be upgraded as an intelligent platform for information retrieval and knowledge sharing. Our practice at the WikipediaMM task in the ImgeCLEF 2008 shows that the knowledge network constructed from Wikipedia can be used to effectively expand the query semantics of image retrieval. It is expected that Wikipedia and digital library can integrate each other's valuable results and best practices to benefit each other.},
author_keywords={Digital library;  Group collaboration;  IULib;  Knowledge network;  Knowledge sharing;  Query expansion;  Wikipedia},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alonso2011485,
author={Alonso, P. and Bernabéu, M.O. and García, V.M. and Vidal, A.M.},
title={Implementation and tuning of a parallel symmetric Toeplitz eigensolver},
journal={Journal of Parallel and Distributed Computing},
year={2011},
volume={71},
number={3},
pages={485-494},
doi={10.1016/j.jpdc.2010.10.010},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751648152&doi=10.1016%2fj.jpdc.2010.10.010&partnerID=40&md5=16c03abf3008a06a2d345c605f75cb05},
abstract={In a previous paper (Vidal et al., 2008, [21]), we presented a parallel solver for the symmetric Toeplitz eigenvalue problem, which is based on a modified version of the Lanczos iteration. However, its efficient implementation on modern parallel architectures is not trivial. In this paper, we present an efficient implementation on multicore processors which takes advantage of the features of this architecture. Several optimization techniques have been incorporated to the algorithm: improvement of Discrete Sine Transform routines, utilization of the GohbergSemencul formulas to solve the Toeplitz linear systems, optimization of the workload distribution among processors, and others. Although the algorithm follows a distributed memory parallel programming paradigm that is led by the nature of the mathematical derivation, special attention has been paid to obtaining the best performance in multicore environments. Hybrid techniques, which merge OpenMP and MPI, have been used to increase the performance in these environments. Experimental results show that our implementation takes advantage of multicore architectures and clearly outperforms the results obtained with LAPACK or ScaLAPACK. © 2010 Elsevier Inc. All rights reserved.},
author_keywords={Matrix eigenvalues;  Multicore platforms;  Toeplitz matrices},
document_type={Article},
source={Scopus},
}

@ARTICLE{Boeck2011543,
author={Boeck, S. and Freysoldt, C. and Dick, A. and Ismer, L. and Neugebauer, J.},
title={The object-oriented DFT program library S/PHI/nX},
journal={Computer Physics Communications},
year={2011},
volume={182},
number={3},
pages={543-554},
doi={10.1016/j.cpc.2010.09.016},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650756197&doi=10.1016%2fj.cpc.2010.09.016&partnerID=40&md5=410dfdf5aa64db705621cfe5da3d6308},
abstract={In order to simplify the development and implementation process of quantum mechanical algorithms, we developed a set of object-oriented C++ libraries which can exploit modern computer architectures. The libraries are characterized as follows: (i) State-of-the-art computer science techniques have been applied or developed in this work to provide language elements to express algebraic expressions efficiently on modern computer platforms. (ii) Quantum mechanical algorithms are crucial in the field of materials research. The new libraries support the Dirac notation to implement such algorithms in the native language of physicists. (iii) The libraries are completed by elements to express equations of motions efficiently which is required for implementing structural algorithms such as molecular dynamics. Based on these libraries we introduce the DFT program package S/PHI/nX. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Ab initio calculations;  Computer languages;  Density-functional theory;  Electronic structure calculations;  Electronic structure methods;  High performance computing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shibuya2011,
author={Shibuya, M. and Jun, G. and Iida, K. and Mikami, K.},
title={Construction of a LEGO-brick type ERP system},
journal={21st International Conference on Production Research: Innovation in Product and Production, ICPR 2011 - Conference Proceedings},
year={2011},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923515826&partnerID=40&md5=161cf92bd0ef2ee9542d1dd252b8caa3},
abstract={With enhancement of computer processing capability, constructing business systems has become possible even with a personal computer, which is accelerating the use of information technology by small and medium-sized enterprises. The enterprises with limited management resources have become interested in ERP, recognizing IT systems as the strategic management base for enhancing their competitive power. When introducing ERP, small and medium-sized enterprises face several problems such as high costs, long introduction periods and no idea about how to build the systems. Their management resources are limited, so introduction of the same ERP as the major companies' does not go well. What is important is close analysis of demands and construction of high-realizability plans. To make ERP widespread in smaller business, the author, et al. thought necessary a low-cost system which remodels software to suit the business. Concretely, what is desirable is creation of a low-cost, easy-to-operate, customizable ERP system by making use of the advantages of both in-house programming software and package software and reducing their disadvantages. To achieve the above-mentioned goal, we conducted research into the ERP package software functions, and designed a conception of an architecture which enables a program-less add-on function on the system. It is characteristic of our system to have a LEGO-brick type structure assembling by the block functions necessary for the user. This paper reports on the outlines of our system and a system made on an experimental basis.},
author_keywords={ERP;  LEGO;  MZ platform;  Open software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim201125,
author={Kim, I. and Kim, H. and Choi, J.},
title={Mapping system between property of bim software and material of energy plus for open bim-based energy performance assessment},
journal={Proceedings of the 28th International Symposium on Automation and Robotics in Construction, ISARC 2011},
year={2011},
pages={25-26},
doi={10.22260/isarc2011/0002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863756410&doi=10.22260%2fisarc2011%2f0002&partnerID=40&md5=88dbc9fe88a4b8ae1c37fc39ce0d2e60},
abstract={Recently, AEC industries have been much research BIM-based energy performance and green building, in Korea. However, Current researches have the lacks of standard guideline for BIM data for BIM-based energy performance assessment and the limitation of interoperability between BIM software. Interoperability is required to Open BIM-based energy performance assessment and it can be exchange, sharing, and compatibility of information between all Open BIM-based software. The purpose of this study is to establish a mapping system between IFC of open BIM standard file format and material of Energy Plus. To achieve this purpose, library and material requirements for the BIM-based energy performance assessment are arranged by investigation and analysis of the researches. A mapping system structure is propose which enables the interoperability between library information of BIM modeling software and material information of energy performance assessment software Energy Plus based on the arranged requirements. Through this research, it is feasible to do the energy performance assessment by the practical use of IFC which was judged as the limitations before, and the application and practical use of mapping system are expected more advanced energy performance assessment software.},
author_keywords={Energy performance assessment;  Energy plus;  IFC;  Mapping system;  Open BIM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Huang2011402,
author={Huang, F. and Liu, D. and Li, X. and Wang, L. and Xu, W.},
title={Preliminary study of a cluster-based open-source parallel GID based on the GRASS GIS},
journal={International Journal of Digital Earth},
year={2011},
volume={4},
number={5},
pages={402-420},
doi={10.1080/17538947.2010.543954},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051898465&doi=10.1080%2f17538947.2010.543954&partnerID=40&md5=9c2bda977331744b8f5e782a73428059},
abstract={In response to the problem of how to give geographic information system (GIS) high-performance capabilities for certain specific GIS applications, a new GIS research direction, parallel GIS processing, has emerged. However, traditional research has focused mostly on implementing typical GIS parallel algorithms, with little discussion of how to parallelize an entire GIS package on clusters based on theory. Therefore, the authors have chosen the geographic resources analysis support system (GRASS) GIS as the object of their research and have put forward the concept of a cluster-based open-source parallel GIS (cluster-based OP-GIS) as a tool to support Digital Earth construction. The related theory includes not only the parallel computing mode, architecture, and software framework of such a system, but also various parallelization patterns. From experiments on the prototype system, it can be concluded that the parallel system has better efficiency and performance than the conventional system on certain selected modules. © 2011 Taylor & Francis.},
author_keywords={Cluster;  Digital earth;  Grass gis;  Open-source parallel gis;  Parallel computing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang20111082,
author={Zhang, X. and Zheng, L. and Lu, Y.},
title={The exploration of the core asset library in the software product line},
journal={Proceedings - 2011 8th International Conference on Information Technology: New Generations, ITNG 2011},
year={2011},
pages={1082-1083},
doi={10.1109/ITNG.2011.198},
art_number={5945399},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051503672&doi=10.1109%2fITNG.2011.198&partnerID=40&md5=d28f70ff965e2bb46918b4a34518e5fe},
abstract={Core asset library is the core component of the software product line. With the background of the Data Processing Oriented Software Product Line, this paper describes the design and implementation of the Core asset Library, researches the key technology and provides the information for software companies. © 2011 IEEE.},
author_keywords={Component;  Core asset library;  Software Product Line},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rivero2011330,
author={Rivero, C.R. and Hernández, I. and Ruiz, D. and Corchuelo, R.},
title={A reference architecture for building semantic-web mediators},
journal={Lecture Notes in Business Information Processing},
year={2011},
volume={83 LNBIP},
pages={330-341},
doi={10.1007/978-3-642-22056-2_36},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960333931&doi=10.1007%2f978-3-642-22056-2_36&partnerID=40&md5=b3cf7d3d94b172bba8021488b63acb6a},
abstract={The Semantic Web comprises a large amount of distributed and heterogeneous ontologies, which have been developed by different communities, and there exists a need to integrate them. Mediators are pieces of software that help to perform this integration, which have been widely studied in the context of nested relational models. Unfortunately, mediators for databases that are modelled using ontologies have not been so widely studied. In this paper, we present a reference architecture for building semantic-web mediators. To the best of our knowledge, this is the first reference architecture in the bibliography that solves the integration problem as a whole, contrarily to existing approaches that focus on specific problems. Furthermore, we describe a case study that is contextualised in the digital libraries domain in which we realise the benefits of our reference architecture. Finally, we identify a number of best practices to build semantic-web mediators. © 2011 Springer-Verlag.},
author_keywords={Information Integration;  Mediator;  Semantic-web Technologies},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Amálio2010151,
author={Amálio, N. and Kelsen, P. and Ma, Q. and Glodt, C.},
title={Using VCL as an aspect-oriented approach to requirements modelling},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6210 LNCS},
pages={151-199},
doi={10.1007/978-3-642-16086-8_5},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649848014&doi=10.1007%2f978-3-642-16086-8_5&partnerID=40&md5=605cf4fd3cd9d6201c5d6f0b44752684},
abstract={Software systems are becoming larger and more complex. By tackling the modularisation of crosscutting concerns, aspect orientation draws attention to modularity as a means to address the problems of scalability, complexity and evolution in software systems development. Aspect-oriented modelling (AOM) applies aspect-orientation to the construction of models. Most existing AOM approaches are designed without a formal semantics, and use multi-view partial descriptions of behaviour. This paper presents an AOM approach based on the Visual Contract Language (VCL): a visual language for abstract and precise modelling, designed with a formal semantics, and comprising a novel approach to visual behavioural modelling based on design by contract where behavioural descriptions are total. By applying VCL to a large case study of a car-crash crisis management system, the paper demonstrates how modularity of VCL's constructs, at different levels of granularity, help to tackle complexity. In particular, it shows how VCL's package construct and its associated composition mechanisms are key in supporting separation of concerns, coarse-grained problem decomposition and aspect-orientation. The case study's modelling solution has a clear and well-defined modular structure; the backbone of this structure is a collection of packages encapsulating local solutions to concerns. © 2010 Springer-Verlag.},
author_keywords={aspect-oriented modelling;  design by contract;  modularity;  separation of concerns;  VCL},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ohira201089,
author={Ohira, M. and Ye, Y.},
title={3rd International Workshop on Supporting Knowledge Collaboration in Software Development (KCSD2009)},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6284 LNAI},
pages={89-90},
doi={10.1007/978-3-642-14888-0_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649814441&doi=10.1007%2f978-3-642-14888-0_8&partnerID=40&md5=2b55801f6c66b4916860e639799e6188},
abstract={The creation of modern software systems requires knowledge from a wide range of domains: application domains, computer hardware and operating systems, algorithms, programming languages, vast amount of component libraries, development environments, the history of the software system, and users. Because few software developers have all the required knowledge, the development of software has to rely on distributed cognition by reaching into a complex networked world of information and computer mediated collaboration. The success of software development, therefore, hinges on how various stakeholders are able to share and combine their knowledge through cooperation, collaboration and co-construction. © 2010 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu201065,
author={Liu, X. and Zhou, J. and Zhang, D. and Shen, Y. and Guo, M.},
title={A parallel skeleton library for embedded multicores},
journal={Proceedings of the International Conference on Parallel Processing Workshops},
year={2010},
pages={65-73},
doi={10.1109/ICPPW.2010.21},
art_number={5599219},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649813413&doi=10.1109%2fICPPW.2010.21&partnerID=40&md5=06bc52e71c86f0c052d7bdb06d2fea76},
abstract={Many SoCs adopt multicore architectures. As a result, embedded programmers are also facing the challenge of parallel programming. We propose a parallel skeleton library that can be used on embedded multicores. Our library is implemented in standard C++ using template features. We propose two parallel skeletons to support common program patterns on multicores. In our skeleton library, programmers can easily choose underlying parallel implementations with no code changes. Experimental results show that many applications can take advantage of these two skeletons for performance improvement, sometimes better than hand-parallelized code. © 2010 IEEE.},
author_keywords={Embedded multicore;  Parallel skeleton;  Template library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Woollard2010325,
author={Woollard, D. and Mattmann, C.A. and Popescu, D. and Medvidovic, N.},
title={KADRE: Domain-specific architectural recovery for scientific software systems},
journal={ASE'10 - Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
year={2010},
pages={325-328},
doi={10.1145/1858996.1859062},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649800060&doi=10.1145%2f1858996.1859062&partnerID=40&md5=35ef1eb04b887526faada926abaf476f},
abstract={Scientists today conduct new research via software-based experimentation and validation in a host of disciplines. Scientific software represents a significant investment due to its complexity and longevity yet there is little reuse of scientific software beyond small libraries which increases development and maintenance costs. To alleviate this disconnect, we have developed KADRE, a domain-specific architecture recovery approach and toolset to aid automatic and accurate identification of workflow components in existing scientific software. KADRE improves upon state of the art general cluster techniques, helping to promote component-based reuse within the domain. © 2010 ACM.},
author_keywords={Scientific computing;  Software architecture;  Workflows},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bischoff2010968,
author={Bischoff, R. and Guhl, T. and Prassler, E. and Nowak, W. and Kraetzschmar, G. and Bruyninckx, H. and Soetens, P. and Haegele, M. and Pott, A. and Breedveld, P. and Broenink, J. and Brugali, D. and Tomatis, N.},
title={BRICS - Best practice in robotics},
journal={Joint 41st International Symposium on Robotics and 6th German Conference on Robotics 2010, ISR/ROBOTIK 2010},
year={2010},
volume={2},
pages={968-975},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881452818&partnerID=40&md5=0d53f600959111f447ab57ce60ae4dd3},
abstract={In the past, the process of developing a new robot application has had more of the design of a piece of artwork or of an act of ingenious engineering than of a structured and formalized process. The prime objective of BRICS is to structure and formalize the robot development process itself and to provide tools, models, and functional libraries, which allow reducing the development time by a magnitude. BRICS is working together with academic as well as industrial providers of robotics "components" (hardware and software), to identify and document best practices in the development of complex robotics systems, to refactor (together) the existing components in order to achieve a much higher level of reusability and robustness, and to support the robot development process with a structured tool chain and code repository. BRICS is a joint research project funded by the European Commission ICT Challenge 2 under grant number 231940. First results include the analysis of existing robot development processes, the first steps towards harmonizing robot control interfaces and component models and the set-up of robot systems for best practice analyses.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Buckow2010398,
author={Buckow, H. and Groß, H.-J. and Piller, G. and Prott, K. and Willkomm, J. and Zimmermann, A.},
title={Integration strategies and patterns for SOA and standard platforms},
journal={INFORMATIK 2010 - Service Science - Neue Perspektiven fur die Informatik, Beitrage der 40. Jahrestagung der Gesellschaft fur Informatik e.V. (GI)},
year={2010},
volume={1},
pages={398-403},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874248718&partnerID=40&md5=b2f85242fd624645af9fcdc927947d4d},
abstract={The SOA Innovation Lab presents a holistic approach for the development of a service-oriented enterprise architecture with custom and standard software packages. Starting point is the construction and analysis of company domain maps with respect to characteristics of SOA and standard software. After assessing the SOA ability of standard software packages within an architecture maturity framework, we show how target architectures can be developed with the help of use case analyses, capability maps and integration patterns. Besides methods and related artifacts, we present current adoption issues for standard software packages in service-oriented contexts.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Garcia-Murillo20105477,
author={Garcia-Murillo, M.},
title={Theory construction: Finding your contribution},
journal={16th Americas Conference on Information Systems 2010, AMCIS 2010},
year={2010},
volume={7},
pages={5477-5480},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870272267&partnerID=40&md5=c5e9c82a2a889ed3ec0436e50ed8ed90},
abstract={Computers, digital libraries and the Internet in general have led to an explosion of research that is often difficult to keep up with our fields. The vast amount of academic papers available to researchers makes it difficult to determine what to read or even figure out where we can make a contribution. This workshop focuses on the process of theory development and how to find holes in the literature where a contribution can be made. Participants will be made aware of software applications (some of which are open), that can facilitate the theory development process with visualizations, citation analysis graphs and reference software for example. The workshop is unique because it falls in a niche that is not covered in either methodological or philosophy of science texts. Unlike other contributions on theory development which are much more formally presented, this workshop will be much more practical in nature. It is intended to guide the research and theory construction process to make it easier for scholars to be able to successfully and more effectively make a contribution to their fields. It provides a practical and systematic approach to the research process beyond traditional methods of research design or philosophy. Today, contributions are almost random given that they are, for the most part, relying on articles that scholars find in databases with little or no strategies to identify the most relevant or from papers that advisers recommend.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ponomarenko201097,
author={Ponomarenko, A. and Rubanov, V.},
title={Header-driven generation of sanity API tests for shared libraries},
journal={2010 6th Central and Eastern European Software Engineering Conference, CEE-SECR 2010},
year={2010},
pages={97-102},
doi={10.1109/CEE-SECR.2010.5783158},
art_number={5783158},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959631742&doi=10.1109%2fCEE-SECR.2010.5783158&partnerID=40&md5=1e35d285f6b57ae48894633e7e7e6239},
abstract={There are thousands of various software libraries being developed in the modern world - completely new libraries emerge as well as new versions of existing ones regularly appear. Unfortunately, developers of many libraries focus on developing functionality of the library itself but neglect ensuring high quality and backward compatibility of application programming interfaces (APIs) provided by their libraries. The best practice to address these aspects is having an automated regression test suite that can be regularly (e.g., nightly) run against the current development version of the library. Such a test suite would ensure early detection of any regressions in the quality or compatibility of the library. But developing a good test suite can cost significant amount of efforts, which becomes an inhibiting factor for library developers when deciding QA policy. That is why many libraries do not have a test suite at all. This paper discusses an approach for low cost automatic generation of basic tests for shared libraries based on the information automatically extracted from the library header files and additional information about semantics of some library data types. Such tests can call APIs of target libraries with some correct parameters and can detect typical problems like crashes "out-of-the-box". Using this method significantly lowers the barrier for developing an initial version of library tests, which can be then gradually improved with a more powerful test development framework as resources appear. The method is based on analyzing API signatures and type definitions obtained from the library header files and creating parameter initialization sequences through comparison of target function parameter types with other functions' return values or out-parameters (usually, it is necessary to call some function to get a correct parameter value for another function and the initialization sequence of the necessary function calls can be quite long). The paper also describes the structure of a tool that implements the proposed method for automatic generation of basic tests for Linux shared libraries (for C and C++ languages). Results of practical usage of the tool are also presented. © 2010 IEEE.},
author_keywords={application programming interface;  Linux;  sanity testing;  Shared libraries},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2010233,
author={Xie, Z.-Q. and Qiang, S. and Sun, X. and Shuai, W. and Zheng, R.},
title={Development of simulation and analysis software of temperature control for massive concrete construction},
journal={Proceedings - 2010 2nd WRI World Congress on Software Engineering, WCSE 2010},
year={2010},
volume={1},
pages={233-236},
doi={10.1109/WCSE.2010.85},
art_number={5718302},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952942212&doi=10.1109%2fWCSE.2010.85&partnerID=40&md5=b8f36780e90db46cf09d43df3293aaed},
abstract={According to the shortage of analysis software for temperature control while massive concrete structures were widely used, the special temperature control aided analysis software (TCAS) was developed. A mature Fortran program was applied with a precise method for pipe cooling calculation added during development. Friendly graphic user interface was developed, by using Microsoft foundation classes (MFC). Open graphics library (OpenGL) was applied for the display of the model and results. TCAS has strong practicality; pertinence and easy operation, comparing with general commercial finite element analysis software. Theory of temperature control for concrete construction and mechanism of major measures for temperature control were analyzed. In particular, various concrete cooling simulation methods were compared and the cooling pipe discrete element method using in this software was introduced in detail. Details on the development process, software modules, and functions of major modules were introduced in this paper. With the help of TCAS, the choice of concrete temperature control measures and designing of temperature control scheme will become simpler and more convenient. © 2010 IEEE.},
author_keywords={Concrete;  Finite element method;  Mixed programming;  Object-oriente;  Temperature control},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu2010291,
author={Xu, Z. and Cui, J. and Wang, Z. and Liu, F. and Liu, G.},
title={The architecture and implementation of biodiversity digital library in China},
journal={Proceedings - 2010 2nd WRI World Congress on Software Engineering, WCSE 2010},
year={2010},
volume={1},
pages={291-294},
doi={10.1109/WCSE.2010.167},
art_number={5718315},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952933280&doi=10.1109%2fWCSE.2010.167&partnerID=40&md5=c6fa7d491e37c6888791f12df798d590},
abstract={Digital library is the trend of current traditional library. However, many problems, such as data collection, sharing, cooperation among different libraries and the copyright, disturb the development of digital library. Based on previous work and the cooperation with BHL (Biodiversity Heritage Library), this paper gives a description on lots of affairs in the construction of digital library in biodiversity: the hardware and software, the workflow with IA (Internet Archive), database basis, the architecture and the function. Some development works and future work are also demonstrated in the paper. The application of digital library in biodiversity will largely improve the service of related resources in traditional libraries and also have a pilot effect for many libraries in other disciplines. © 2010 IEEE.},
author_keywords={Biodiversity heritage library;  Biodiversity informatics;  Digital library;  Scientific community},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2010370,
author={Liu, L. and Morozov, O.},
title={A process-oriented streaming system design paradigm for FPGAs},
journal={Proceedings - 2010 International Conference on Reconfigurable Computing and FPGAs, ReConFig 2010},
year={2010},
pages={370-375},
doi={10.1109/ReConFig.2010.39},
art_number={5695334},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951751989&doi=10.1109%2fReConFig.2010.39&partnerID=40&md5=172dd9432225f5cc45fca520c9c6d88b},
abstract={This paper presents a streaming system design paradigm that allows developers to model streaming applications and their FPGA-based many-core hardware architectures as processes and channels. We have developed a programming language called System-Oberon, together with a run-time library, a hardware library implemented on an FPGA, and a compiler to automate the system design flow. In general, the proposed paradigm represents a software-driven approach to the streaming system designs on FPGAs. Compared to the existing solutions, our system design paradigm and its toolchain allow the automatic construction of a completely autonomous system on an FPGA, support the task-level parallelism from both software and hardware levels, and avoid the need for hardware programming work for application developers. These features make the proposed approach advantageous in achieving better results regarding the system's performance, power consumption, design reuse and time-to-market. To prove the applicability of our approach, a monitor for real-time ECG signal analysis was built and analyzed for its performance, size, power consumption and development time. © 2010 IEEE.},
author_keywords={Automatic streaming system design;  FPGA;  Many-core architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fan20102165,
author={Fan, L. and Zhang, F. and Wang, G. and Liu, Z.},
title={Algorithm analysis and efficient parallelization of the single particle reconstruction software package: EMAN},
journal={Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
year={2010},
volume={47},
number={12},
pages={2165-2176},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650951358&partnerID=40&md5=427f1b21826609637f8ac684a186be43},
abstract={Single particle reconstruction is one of the most important technologies for determining three-dimensional structures of macromolecules. In recent years, it has been given more and more attention, because of some of its distinct features. Unfortunately, its application is greatly constrained, due to its extremely long processing time and lack of efficient parallel implementations. This study optimizes and parallelizes one of the most widely-used software packages for single particle reconstruction: EMAN. By analyzing algorithms of its major components, the authors find that the key problem is achieving ideal load balancing with low communication costs. A self-adaptive dynamic scheduling algorithm is introduced to solve this problem. It is not only applicable to EMAN, but also to other similar scheduling problems with independent tasks. Actual experiments show that through optimization, serial execution time of our implementation is 11.50% less than that of EMAN. Besides, thanks to the self-adaptive scheduling algorithm, our implementation produces much higher speedups than EMAN. Speedups of the most time-consuming classification component are close to linearity. Moreover, parallel efficiency of our implementation on 16 CPU cores is 29.8% higher, compared with the implementation of EMAN. Therefore, our implementation is capable of making full use of available computing resources, dramatically reducing the processing time of single particle reconstruction.},
author_keywords={Bioinformation;  EMAN;  Parallel computing;  Scheduling algorithm;  Single particle reconstruction},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rintaluoma201079,
author={Rintaluoma, T. and Silvén, O.},
title={SIMD performance in software based mobile video coding},
journal={Proceedings - 2010 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation, IC-SAMOS 2010},
year={2010},
pages={79-85},
doi={10.1109/ICSAMOS.2010.5642079},
art_number={5642079},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650928300&doi=10.1109%2fICSAMOS.2010.5642079&partnerID=40&md5=6fba6df747091f038ab510a11099ff9e},
abstract={Most video applications use specific application programming interfaces to achieve the desired functionalities. Implementing interface backends with hardware is often too expensive for low-end mobile devices, so most of the devices cope with highly optimized software implementations that employ special instruction sets. The most common approach is the utilization of SIMD processing units such as ARM NEON or Intel WMMX in mobile application processors. Fully utilizing the potential benefits of such instruction sets usually means tedious assembly coding even if vectorizing compilers have improved lately. In addition, low level APIs such as OpenMax DL have been made available to offer a standardized interface for accelerated codec functionalities. In this paper we present optimization methods and results from using a NEON instruction set and OpenMax DL API for MPEG-4 and H.264 video encoding and decoding. Although these technologies provide for significant speed-ups and reduce the burden of application designers, the serial bit stream processing bottleneck remains to be solved. ©2010 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wahba2010249,
author={Wahba, S.K. and Hallstrom, J.O. and Soundarajan, N.},
title={Initiating a design pattern catalog for embedded network systems},
journal={Embedded Systems Week 2010 - Proceedings of the 10th ACM International Conference on Compilers, Architecture and Synthesis for Embedded Systems, EMSOFT'10},
year={2010},
pages={249-258},
doi={10.1145/1879021.1879054},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650663128&doi=10.1145%2f1879021.1879054&partnerID=40&md5=8e29abcb8e0100266e8990840b3bdd33},
abstract={In the domain of desktop software, design patterns have had a profound impact; they are applied ubiquitously across a broad range of applications. Patterns serve both to promulgate expert knowledge and as a vocabulary for documenting software design. The result is higher-quality software, reduced development effort, and improved documentation. This impact has yet to be felt in the emerging domain of embedded network software. The applications are characterized by highly distributed, reactive computations executed over resource-constrained processors; they present a host of new design challenges. Without a coherent repository of expert solutions, best practices do not emerge, design flaws are repeated, and developer communication is hindered. In this paper, we initiate a pattern catalog for embedded network software. The contributions are two-fold. First, we present three new design patterns distilled from a careful analysis of expert-crafted software. Second, we present a formalism for specifying the structural requirements of such patterns. The resulting specifications provide a rigorous foundation for pattern identification and documentation.},
author_keywords={Embedded networks;  nesC;  Patterns;  Sensor networks;  TinyOS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Frenkel2010,
author={Frenkel, Z. and Paux, E. and Mester, D. and Feuillet, C. and Korol, A.},
title={LTC: A novel algorithm to improve the efficiency of contig assembly for physical mapping in complex genomes},
journal={BMC Bioinformatics},
year={2010},
volume={11},
doi={10.1186/1471-2105-11-584},
art_number={584},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649462780&doi=10.1186%2f1471-2105-11-584&partnerID=40&md5=ba2fa35598235e18c63012ebdb29cd65},
abstract={Background: Physical maps are the substrate of genome sequencing and map-based cloning and their construction relies on the accurate assembly of BAC clones into large contigs that are then anchored to genetic maps with molecular markers. High Information Content Fingerprinting has become the method of choice for large and repetitive genomes such as those of maize, barley, and wheat. However, the high level of repeated DNA present in these genomes requires the application of very stringent criteria to ensure a reliable assembly with the FingerPrinted Contig (FPC) software, which often results in short contig lengths (of 3-5 clones before merging) as well as an unreliable assembly in some difficult regions. Difficulties can originate from a non-linear topological structure of clone overlaps, low power of clone ordering algorithms, and the absence of tools to identify sources of gaps in Minimal Tiling Paths (MTPs).Results: To address these problems, we propose a novel approach that: (i) reduces the rate of false connections and Q-clones by using a new cutoff calculation method; (ii) obtains reliable clusters robust to the exclusion of single clone or clone overlap; (iii) explores the topological contig structure by considering contigs as networks of clones connected by significant overlaps; (iv) performs iterative clone clustering combined with ordering and order verification using re-sampling methods; and (v) uses global optimization methods for clone ordering and Band Map construction. The elements of this new analytical framework called Linear Topological Contig (LTC) were applied on datasets used previously for the construction of the physical map of wheat chromosome 3B with FPC. The performance of LTC vs. FPC was compared also on the simulated BAC libraries based on the known genome sequences for chromosome 1 of rice and chromosome 1 of maize.Conclusions: The results show that compared to other methods, LTC enables the construction of highly reliable and longer contigs (5-12 clones before merging), the detection of "weak" connections in contigs and their "repair", and the elongation of contigs obtained by other assembly methods. © 2010 Frenkel et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Marjane2010240,
author={Marjane, A. and Allailou, B.},
title={Vectorial conception of FCSR},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6338 LNCS},
pages={240-252},
doi={10.1007/978-3-642-15874-2_20},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249243865&doi=10.1007%2f978-3-642-15874-2_20&partnerID=40&md5=138a7ad00e339b02e9ea16210bed8d24},
abstract={In this paper, we investigate the structure of FCSR made by Goresky and Klapper. Using a vectorial construction of the objects and of the register, we extend the analysis of FCSRs. We call these registers vectorial FCSRs or VFCSRs. We obtain similar results to those of analysis of FCSRs and of d-FCSRs generating binary sequences or p-ary sequences. In fact, the AFSRs built over finite fields double-struck Fpnwith n ≥ 2 suffer from an very difficult and formal analysis. But if you analyze these registers with a vectorial structure, you can decompose the output sequence into a vector of binary sequences or p-ary sequences. This method allows us to obtain very easily the period, the behavior of memory with interval optimized , the maximal period, the existence of l-sequences and the calculations become explicit and easily implementable. At the end of this paper, we implement the quadratic case (double-struck F22 case) and present the conclusions about pseudorandom properties of quadratic l-sequences which are tested by NIST STS package. In conclusion, VFCSRs are easy to implement in software and hardware and have excellent pseudorandomn property. © 2010 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xylomenos201011,
author={Xylomenos, G. and Cici, B.},
title={Design and evaluation of a socket emulator for publish/subscribe networks},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6369 LNCS},
pages={11-19},
doi={10.1007/978-3-642-15877-3_2},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049389875&doi=10.1007%2f978-3-642-15877-3_2&partnerID=40&md5=e4177821270fcb3733afa582218e878b},
abstract={In order for a Future Internet architecture to be globally deployed, it must ensure that existing applications will continue to operate efficiently on top of it. As part of the Publish Subscribe Internet Routing Paradigm (PSIRP) project, we have explored various options for making endpoint centric applications based on the Sockets Application Programming Interface (API) compatible with the information centric PSIRP prototype implementation. We developed an emulator that mediates between the client/server socket calls and the publish/subscribe PSIRP calls, transforming the exchange of packets to distribution of publications. To assess the overhead of our emulator, we measure the execution time of a simple file transfer application in native socket mode, in emulated socket mode and in native publish/subscribe mode. © 2010 Springer-Verlag Berlin Heidelberg.},
author_keywords={PSIRP;  Publish/Subscribe;  Sockets;  TCP/IP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Greaves201021,
author={Greaves, D. and Singh, S.},
title={Designing application specific circuits with concurrent C# programs},
journal={8th ACM/IEEE International Conference on Formal Methods and Models for Codesign, MEMOCODE 2010},
year={2010},
pages={21-30},
doi={10.1109/MEMCOD.2010.5558627},
art_number={5558627},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957795388&doi=10.1109%2fMEMCOD.2010.5558627&partnerID=40&md5=257684188cd6144b4f55a21ba9d71766},
abstract={This paper presents an investigation into the possibility of using a regular concurrent programming language for modeling and implementing digital circuits. Some of the reasons for using an existing language include the ability to use existing compilers and analysis tools for circuit design and verification. Another important reason is the ever increasing need to model complete systems that comprise interacting software and hardware in a single framework which facilitates easier migration of sub-components between hardware and software implementations compared to multi-model approaches. To this end we present the design of the Kiwi system which models digital circuits with concurrent programs using a standard library in C# for multi-threaded programming. Kiwi models can be executed using a regular C# compiler. Also, the compiled bytecode can be automatically converted into circuits using our Kiwi hardware synthesis system. © 2010 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Winarno2010421,
author={Winarno, S. and Griffith, A. and Stephenson, P.},
title={Reducing earthquake risk to non-engineered buildings in Indonesia},
journal={Construction Innovation},
year={2010},
volume={10},
number={4},
pages={421-434},
doi={10.1108/14714171011083588},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986131517&doi=10.1108%2f14714171011083588&partnerID=40&md5=5a51d5881e46c54d6dc14df8a8e8b967},
abstract={Purpose – The purpose of this paper is to investigate the potential for reducing earthquake risk to nonengineered buildings. “Nonengineered” buildings are, in simple terms, arbitrarily designed and inadequately built structures. Such buildings are indigenous and widespread within Indonesia, and are particularly susceptible to damage and destruction from earthquake events. A range of technical and social elements are significant to reducing earthquake risk to this building type and need to be appreciated within the context of current regulatory, design, and construction practices in Indonesia. A greater awareness for, and better understanding of, both the technical and human elements which impact on the planning, design, and construction of nonengineered buildings will enable more effective earthquake risk reduction measures to be implemented within building practices throughout Indonesia. Design/methodology/approach – Research used a triangulated approach collating qualitative and quantitative data acquired by multiple collection methods of a questionnaire survey, structured and semistructured interviews, case studies, and professional practice and localcommunity workshops. Quantitative data were collated and analyzed using the Statistical Package for the Social Sciences to provide inferential ranking and correlation of responses. Qualitative data were processed using NVivo software to code prominent patterns in the views and opinions of the respondents. Findings – The findings highlight the following: the reasons why earthquakerelated building codes are not applied more readily to nonengineered buildings; elements of practice where problems occur and where change could improve the use of building codes in their application to nonengineered buildings; actions for change to improve planning regulation, design, and construction of nonengineered buildings; and practical and effective methods of disseminating the research to industry, academic, and community stakeholders. Originality/value – The application of the research findings is unquestionably significant and valuable to Indonesia as any measures which can reduce earthquake risk have: the potential to improve the quality and sustainability of buildings; the ability to enhance the protection of property; and the real capability to save lives. Moreover, knowledge and capabilities developed there are highly relevant to earthquakevulnerable zones within the wider AsiaPacific region. © 2010, Emerald Group Publishing Limited},
author_keywords={Building specifications;  Codes;  Design and theory;  Earthquakes;  Indonesia;  Structural analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Selga2010527,
author={Selga, J. and Rodríguez, A. and Gil, M. and Carbonell, J. and Boria, V.E. and Martín, F.},
title={Synthesis of planar microwave circuits through aggressive space mapping using commercially available software packages},
journal={International Journal of RF and Microwave Computer-Aided Engineering},
year={2010},
volume={20},
number={5},
pages={527-534},
doi={10.1002/mmce.20458},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649240364&doi=10.1002%2fmmce.20458&partnerID=40&md5=98971bc48c935e33009552ba29602639},
abstract={This article is focused on the practical implementation of a software tool for the synthesis of planar microwave circuits based on aggressive space mapping (ASM), which makes use of commercially available software packages as the core program (MATLAB) and as the electromagnetic (EM) fine model simulation space (Agilent Momentum). All technical details related to the control of the cited EM commercial software tool, by means of a general purpose and user-friendly environment (MATLAB in our case), are discussed in detail. The new synthesis tool has been designed to cope with the automatic layout generation of planar metamaterial structures, which is a novel practical application of the ASM algorithm. More specifically, the automated synthesis of microstrip lines loaded with complementary split ring resonators has been considered in this work. However, this synthesis tool can be easily customized to deal with the design of many other planar structures. © 2010 Wiley Periodicals, Inc.},
author_keywords={complementary split ring resonator;  microstrip technology metamaterial transmission lines;  planar microwave circuits;  space mapping},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2010,
author={Li, S. and Tang, S. and Gao, D.},
title={An integrated optimization for hypersonic inlet design based on PYTHON},
journal={2010 International Conference on Computer Design and Applications, ICCDA 2010},
year={2010},
volume={4},
pages={V464-V467},
doi={10.1109/ICCDA.2010.5541491},
art_number={5541491},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955966162&doi=10.1109%2fICCDA.2010.5541491&partnerID=40&md5=bd4aa5e785c322585540f094272c854e},
abstract={In this paper, an integrated optimization program, based on PYTHON script programming language, has been developed by integrating CFD software and optimization package for shape optimization of high speed flight vehicles. The program comprises four functions connected with PYTHON scripts: geometry module, flow grid generator, flow solver, and optimizer, and the data of each part can be exchanged and updated automatically until the iterative study is completed. The paper takes two-dimensional hypersonic inlet with planar lip as an example for design and analysis. The numerical results show that the boundary-layer separation caused by shock boundary-layer interaction in the shoulder of inlet is reduced significantly and the total pressure recovery coefficient of the inlet is greatly improved against the model before optimized. With the increase in computer performance, the integrated optimization coupling with CFD technology will be the trend for vehicle design with aerodynamics in the future. © 2010 IEEE.},
author_keywords={Hypersonic inlet;  Numerical simulation;  Optimization;  PYTHON;  Total pressure recovery coefficient},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bainbridge2010149,
author={Bainbridge, D. and Novak, B.J. and Cunningham, S.J.},
title={A user-centered design of a personal digital library for music exploration},
journal={Proceedings of the ACM International Conference on Digital Libraries},
year={2010},
pages={149-158},
doi={10.1145/1816123.1816145},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955115062&doi=10.1145%2f1816123.1816145&partnerID=40&md5=eeb8bdaf9c109f28a3f52f2cccab2d50},
abstract={We describe the evaluation of a personal digital library environment designed to help musicians capture, enrich and store their ideas using a spatial hypermedia paradigm. The target user group is musicians who primarily use audio and text for composition and arrangement, rather than with formal music notation. Using the principle of user-centered design, the software implementation was guided by a diary study involving nine musicians which suggested five requirements for the software to support: capturing, overdubbing, developing, storing, and organizing. Moreover, the underlying spatial data-model was exploited to give raw audio compositions a hierarchical structure, and - to aid musicians in retrieving previous ideas - a search facility is available to support both query by humming and text-based queries. A user evaluation of the completed design with eleven subjects indicated that musicians, in general, would find the hypermedia environment useful for capturing and managing their moments of musical creativity and exploration. More specifically they would make use of the query by humming facility and the hierarchical track organization, but not the overdubbing facility as implemented. © 2010 ACM.},
author_keywords={Music composition;  Personal digital music library;  Spatial hypermedia},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{McIntosh2010405,
author={McIntosh, S.J. and Bainbridge, D.},
title={Integrating greenstone with an interactive map visualizer},
journal={Proceedings of the ACM International Conference on Digital Libraries},
year={2010},
pages={405},
doi={10.1145/1816123.1816203},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955102058&doi=10.1145%2f1816123.1816203&partnerID=40&md5=ae0c888b2a1a5d6ab553b50731f8e1a9},
abstract={This extended abstract describes recent work in combining interactive map functionality with the Greenstone 3 digital library software research framework.1 Figure 1 shows a snapshot from the implemented system, and is discussed further below. The software implementation needed to achieve this involved development of both the build-time and runtime components of Greenstone. For build-time, the main change required place names in the text to be identified and marked up. This was accomplished by linking Greenstone's document processing plugins with ANNIE, the part of the open source text-mining tool GATE2 that is specifically designed for named entity identification problems. Beyond the disambiguation techniques employed to ANNIE, to further differentiate which place name is being referred to in a document (e.g., just which of the dozen or so places in the world called Hamilton), a spatial clustering algorithm was devised. This collated the place names identified in a specific document, and then exploited the hierarchical structure of the gazetteer to assign weights to each place name. The weighted information could then be used to list the place names in order of likelihood, or alternatively (in the case of a clear winner, determined by using an empirically set threshold) a definitive (but potentially error-prone) decision was made. For the runtime system, the design approach taken was to maintain two interlocked views, displayed side-by-side: one the traditional text-based digital library view, the other an interactive map view. At any stage of the user's interaction the two views represent the same information, only displayed differently. The views are interlocked, in that the user can interact with either view, and as a result both views are updated. For instance, Figure 1 captures the moment just after a text search has been issued. The left-hand panel shows the traditional ranked result list, augmented to use different colors for each matching document. The right-hand panel shows the map based view of the same information, where place names in the text of the result set are displayed on the map. The color used to display a place name is color-coded to match that used in the corresponding document in the text view. Clicking on a document presents the text with place names marked up; marked up place names in turn linked to pop-up information displayed in the map view when clicked upon. A fish-eye view is also available to provide a gestalt to the user of place names the document mentions. Spatial searching is also supported. For example, the next thing the user might do from Figure 1 is click out a region in the map and search for only documents that mention place names included in that area. Implementation of the runtime interface is AJAX based. It uses the Google Widget Toolkit to provide the map interface through its API for Google Maps. PostgresGIS is used to support the spatial searching capabilities.},
author_keywords={Digital library integration;  Interactive map visualizer},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tankink2010440,
author={Tankink, C. and Geuvers, H. and McKinna, J. and Wiedijk, F.},
title={Proviola: A tool for proof re-animation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6167 LNAI},
pages={440-454},
doi={10.1007/978-3-642-14128-7_37},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954883426&doi=10.1007%2f978-3-642-14128-7_37&partnerID=40&md5=ab3b03e36c55f0523505ff93cb4cd945},
abstract={To improve on existing models of interaction with a proof assistant (PA), in particular for storage and replay of proofs, we introduce three related concepts, those of: a proof movie, consisting of frames which record both user input and the corresponding PA response; a camera, which films a user's interactive session with a PA as a movie; and a proviola, which replays a movie frame-by-frame to a third party. In this paper we describe the movie data structure and we discuss a prototype implementation of the camera and proviola based on the ProofWeb system [7]. ProofWeb uncouples the interaction with a PA via a web-interface (the client) from the actual PA that resides on the server. Our camera films a movie by "listening" to the ProofWeb communication. The first reason for developing movies is to uncouple the reviewing of a formal proof from the PA used to develop it: the movie concept enables users to discuss small code fragments without the need to install the PA or to load a whole library into it. Other advantages include the possibility to develop a separate commentary track to discuss or explain the PA interaction. We assert that a combined camera+proviola provides a generic layer between a client (user) and a server (PA). Finally we claim that movies are the right type of data to be stored in an encyclopedia of formalized mathematics, based on our experience in filming the Coq standard library. © 2010 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Miranda2010119,
author={Miranda, C. and Dumont, P. and Cohen, A. and Duranton, M. and Pop, A.},
title={ERBIUM: A deterministic, concurrent intermediate representation for portable and scalable performance},
journal={CF 2010 - Proceedings of the 2010 Computing Frontiers Conference},
year={2010},
pages={119-120},
doi={10.1145/1787275.1787312},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954506414&doi=10.1145%2f1787275.1787312&partnerID=40&md5=009c3c5f86ea946887fcea2a53427e2f},
abstract={Optimizing compilers and runtime libraries do not shield programmers from the complexity of multi-core hardware; as a result the need for manual, target-specific optimizations increases with every processor generation. High-level languages are being designed to express concurrency and locality without reference to a particular architecture. But compiling such abstractions into efficient code requires a portable, intermediate representation: this is essential for modular composition (separate compilation), for optimization frameworks independent of the source language, and for just-in-time compilation of bytecode languages. This paper introduces Erbium, an intermediate representation for compilers, a low-level language for efficiency programmers, and a lightweight runtime implementation. It relies on a data structure for scalable and deterministic concurrency, called Event Recording, exposing the data-level, task and pipeline parallelism suitable to a given target. We provide experimental evidence of the productivity, scalability and efficiency advantages of Erbium, relying on a prototype implementation in GCC 4.3. © 2010 author/owner(s).},
author_keywords={intermediate representation;  kpn;  parallelism;  streaming;  synchronization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Järvi2010596,
author={Järvi, J. and Marcus, M. and Smith, J.N.},
title={Programming with C++ concepts},
journal={Science of Computer Programming},
year={2010},
volume={75},
number={7},
pages={596-614},
doi={10.1016/j.scico.2009.01.001},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950867107&doi=10.1016%2fj.scico.2009.01.001&partnerID=40&md5=6ce6c2ee2027479e0138c48394712d74},
abstract={This paper explores the definition, applications, and limitations of concepts and concept maps in C++, with a focus on library composition. We also compare and contrast concepts to adaptation mechanisms in other languages. Efficient, non-intrusive adaptation mechanisms are essential when adapting data structures to a library's API. Development with reusable components is a widely practiced method of building software. Components vary in form, ranging from source code to non-modifiable binary libraries. The Concepts language features, slated to appear in the next version of C++, have been designed with such compositions in mind, promising an improved ability to create generic, non-intrusive, efficient, and identity-preserving adapters. We report on two cases of data structure adaptation between different libraries, and illustrate best practices and idioms. First, we adapt GUI widgets from several libraries, with differing APIs, for use with a generic layout engine. We further develop this example to describe the run-time concept idiom, extending the applicability of concepts to domains where run-time polymorphism is required. Second, we compose an image processing library and a graph algorithm library, by making use of a transparent adaptation layer, enabling the efficient application of graph algorithms to the image processing domain. We use the adaptation layer to realize a few key algorithms, and report little or no performance degradation. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={C++;  Component adaptation;  Concepts;  Generic programming;  Polymorphism;  Software libraries},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wölkl2010635,
author={Wölkl, S. and Shea, K.},
title={A computational product model for conceptual design using SysML},
journal={Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference 2009, DETC2009},
year={2010},
volume={2},
number={PART A},
pages={635-645},
doi={10.1115/DETC2009-87239},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953759634&doi=10.1115%2fDETC2009-87239&partnerID=40&md5=176a5c59d27d40f1d489cf6ee4c40529},
abstract={The importance of the concept development phase in product development is contradictory to the level and amount of current computer-based support for it, especially with regards to mechanical design. Paper-based methods for conceptual design offer a far greater level of maturity and familiarity than current computational methods. Engineers usually work with software designed to address only a single stage of the concept design phase, such as requirements management tools. Integration with software covering other stages, e.g. functional modeling, is generally poor. Using the requirements for concept models outlined in the VDI 2221 guideline for systematic product development as a starting point, the authors propose an integrated product model constructed using the Systems Modeling Language (SysML) that moves beyond geometry to integrate all necessary aspects for conceptual design. These include requirements, functions and function structures, working principles and their structures as well as physical effects. In order to explore the applicability of SysML for mechanical design, a case study on the design of a passenger car's luggage compartment cover is presented. The case study shows that many different SysML diagram types are suitable for formal modeling in mechanical concept design, though they were originally defined for software and control system development. It is then proposed that the creation and use of libraries defining generic as well as more complicated templates raises efficiency in modeling. The use of diagrams and their semantics for conceptual modeling make SysML a strong candidate for integrated product modeling of mechanical as well as mechatronic systems. Copyright © 2009 by ASME.},
author_keywords={Conceptual design;  Formal modeling language;  Integrated product model;  SysML},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jha2010209,
author={Jha, P.C. and Kapur, P.K. and Bali, S. and Kumar, U.D.},
title={Optimal component selection of cots based software system under consensus recovery block scheme incorporating execution time},
journal={International Journal of Reliability, Quality and Safety Engineering},
year={2010},
volume={17},
number={3},
pages={209-222},
doi={10.1142/S0218539310003767},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149264862&doi=10.1142%2fS0218539310003767&partnerID=40&md5=6c870195ec13a73ec2b1c0be56936b70},
abstract={Computer based systems have increased dramatically in scope, complexity, pervasiveness. Most industries are highly dependent on computers for their basic day to day functioning. Safe & reliable software operations are an essential requirement for many systems across different industries. The number of functions to be included in a software system is decided during the software development. Any software system must be constructed in such a way that execution can resume even after the occurrence of failure with minimal loss of data and time. Such software systems which can continue execution even in presence of faults are called fault tolerant software. When failure occurs one of the redundant software modules get executed and prevent system failure. The fault tolerant software systems are usually developed by integrating COTS (commercial off-the-shelf) software components. The motivation for using COTS components is that they will reduce overall system development costs and reduce development time. In this paper, reliability models for fault tolerant consensus recovery blocks are analyzed. In first optimization model, we formulate joint optimization problem in which reliability maximization of software system and execution time minimization for each function of software system are considered under budgetary constraint. In the second model the issue of compatibility among alternatives available for different modules, is discussed. Numerical illustrations are provided to demonstrate the developed models. © 2010 World Scientific Publishing Company.},
author_keywords={COTS products;  execution time;  fault tolerance;  Modular software;  normalization;  optimization;  redundancy;  software reliability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou2010518,
author={Zhou, J. and Ji, Y. and Zhao, D. and Liu, J.},
title={Using AOP to ensure component interactions in component-based software},
journal={2010 The 2nd International Conference on Computer and Automation Engineering, ICCAE 2010},
year={2010},
volume={3},
pages={518-523},
doi={10.1109/ICCAE.2010.5452043},
art_number={5452043},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952654190&doi=10.1109%2fICCAE.2010.5452043&partnerID=40&md5=84b3b1936ac40b1b906ab6c974e8dff3},
abstract={Component-based software development (CBSD) has been got considerable adoption in software industry, but it is still lack of language support to ensure proper interactions among components, i.e. modularity assurance, which usually causes the software hard to maintain and evolve because of the improper dependencies among the components. In this paper, we propose an AOP approach to ensure that the interactions among components are strictly conformed to the sated API usage policies of the components. Also, by using AOP, we can separate the constraints violation checking code from the normal functional code via the so called aspects, thus improving the software quality by separation of concern. Experiment using AspectJ as the AOP implementation technique shows that the performance is comparable to the non embedded code. ©2010 IEEE.},
author_keywords={Aspect-oriented programming;  Component coupling;  Component-based development;  Constraints violation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schmid2010,
author={Schmid, B. and Schindelin, J. and Cardona, A. and Longair, M. and Heisenberg, M.},
title={A high-level 3D visualization API for Java and ImageJ},
journal={BMC Bioinformatics},
year={2010},
volume={11},
doi={10.1186/1471-2105-11-274},
art_number={274},
note={cited By 366},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952447610&doi=10.1186%2f1471-2105-11-274&partnerID=40&md5=015b210188795a4f1824978423b3d7e4},
abstract={Background: Current imaging methods such as Magnetic Resonance Imaging (MRI), Confocal microscopy, Electron Microscopy (EM) or Selective Plane Illumination Microscopy (SPIM) yield three-dimensional (3D) data sets in need of appropriate computational methods for their analysis. The reconstruction, segmentation and registration are best approached from the 3D representation of the data set.Results: Here we present a platform-independent framework based on Java and Java 3D for accelerated rendering of biological images. Our framework is seamlessly integrated into ImageJ, a free image processing package with a vast collection of community-developed biological image analysis tools. Our framework enriches the ImageJ software libraries with methods that greatly reduce the complexity of developing image analysis tools in an interactive 3D visualization environment. In particular, we provide high-level access to volume rendering, volume editing, surface extraction, and image annotation. The ability to rely on a library that removes the low-level details enables concentrating software development efforts on the algorithm implementation parts.Conclusions: Our framework enables biomedical image software development to be built with 3D visualization capabilities with very little effort. We offer the source code and convenient binary packages along with extensive documentation at http://3dviewer.neurofly.de. © 2010 Schmid et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hemel2010224,
author={Hemel, Z. and Visser, E.},
title={PIL: A platform independent language for retargetable DSLs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={5969 LNCS},
pages={224-243},
doi={10.1007/978-3-642-12107-4_17},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951581593&doi=10.1007%2f978-3-642-12107-4_17&partnerID=40&md5=4763ae7d46a199a1debc5809c14dc559},
abstract={Intermediate languages are used in compiler construction to simplify retargeting compilers to multiple machine architectures. In the implementation of domain-specific languages (DSLs), compilers typically generate high-level source code, rather than low-level machine instructions. DSL compilers target a software platform, i.e. a programming language with a set of libraries, deployable on one or more operating systems. DSLs enable targeting multiple software platforms if its abstractions are platform independent. While transformations from DSL to each targeted platform are often conceptually very similar, there is little reuse between transformations due to syntactic and API differences of the target platforms, making supporting multiple platforms expensive. In this paper, we discuss the design and implementation of PIL, a Platform Independent Language, an intermediate language providing a layer of abstraction between DSL and target platform code, abstracting from syntactic and API differences between platforms, thereby removing the need for platform-specific transformations. We discuss the use of PIL in an implemementation of WebDSL, a DSL for building web applications. © 2010 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jugel2010354,
author={Jugel, U.},
title={Generating smart wrapper libraries for arbitrary APIs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={5969 LNCS},
pages={354-373},
doi={10.1007/978-3-642-12107-4_24},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951532610&doi=10.1007%2f978-3-642-12107-4_24&partnerID=40&md5=c6fe885ab0cf7fbf32bf90b30d6496fb},
abstract={"Library design is language design" [1]. The development of a smart program library is very similar to the creation of a domain specific language (DSL). Both are currently created in an ad-hoc manner, taking account of best practices and software patterns. Creating new languages and the tools needed to integrate them can be very cumbersome. We propose a reproducible, model-driven methodology to add automation to the DSL-creation process. Our novel approach presents an easy way to design and generate smart, API-wrapping libraries, similar to internal DSLs. These libraries increase the usability of an existing API and can be easily integrated into existing software development tool chains. To generate these DSLs, we propose an enhanced code generation that applies usability-enhancing software patterns. Our current generator leverages the Expression Builder pattern, which is described in detail. We validate our methodology and our enhanced code generation by applying it to Java APIs resulting in smart Java libraries that we call "dotLings". © 2010 Springer-Verlag.},
author_keywords={API-usability;  Code generation;  Domain specific languages;  Language integration;  Model-driven},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hogg2010,
author={Hogg, J.D. and Scott, J.A.},
title={A fast and robust mixed-precision solver for the solution of sparse symmetric linear systems},
journal={ACM Transactions on Mathematical Software},
year={2010},
volume={37},
number={2},
doi={10.1145/1731022.1731027},
art_number={17},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951891411&doi=10.1145%2f1731022.1731027&partnerID=40&md5=9af6a84cc2971860be77f531e4854744},
abstract={On many current and emerging computing architectures, single-precision calculations are at least twice as fast as double-precision calculations. In addition, the use of single precision may reduce pressure on memory bandwidth. The penalty for using single precision for the solution of linear systems is a potential loss of accuracy in the computed solutions. For sparse linear systems, the use of mixed precision in which double-precision iterative methods are preconditioned by a single-precision factorization can enable the recovery of high-precision solutions more quickly and use less memory than a sparse direct solver run using double-precision arithmetic. In this article, we consider the use of single precision within direct solvers for sparse symmetric linear systems, exploiting both the reduction in memory requirements and the performance gains. We develop a practical algorithm to apply a mixed-precision approach and suggest parameters and techniques to minimize the number of solves required by the iterative recovery process. These experiments provide the basis for our new code HSL-MA79-a fast, robust, mixed-precision sparse symmetric solver that is included in the mathematical software library HSL. Numerical results for a wide range of problems from practical applications are presented. © 2010 ACM.},
author_keywords={FGMRES;  Fortran 95;  Gaussian elimination;  Iterative refinement;  Mixed precision;  Multifrontal method;  Sparse symmetric linear systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Loriot20101127,
author={Loriot, S. and Cazals, F. and Bernauer, J.},
title={ESBTL: Efficient PDB parser and data structure for the structural and geometric analysis of biological macromolecules},
journal={Bioinformatics},
year={2010},
volume={26},
number={8},
pages={1127-1128},
doi={10.1093/bioinformatics/btq083},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951946532&doi=10.1093%2fbioinformatics%2fbtq083&partnerID=40&md5=469a57e14c991c640b4cdece6ec35c34},
abstract={Summary: The ever increasing number of structural biological data calls for robust and efficient software for analysis. Easy Structural Biology Template Library (ESBTL) is a lightweight C++ library that allows the handling of PDB data and provides a data structure suitable for geometric constructions and analyses. The parser and data model provided by this ready-to-use include-only library allows adequate treatment of usually discarded information (insertion code, atom occupancy, etc.) while still being able to detect badly formatted files. The template-based structure allows rapid design of new computational structural biology applications and is fully compatible with the new remediated PDB archive format. It also allows the code to be easy-to-use while being versatile enough to allow advanced user developments. Availability: ESBTL is freely available under the GNU General Public License from http://esbtl.sf.net. The web site provides the source code, examples, code snippets and documentation. Contact: julie.bernauer@inria.fr. © The Author 2010. Published by Oxford University Press. All rights reserved. For Permissions, please email: journals.permissions@oxfordjournals.org.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cho201035,
author={Cho, H. and Gray, J. and White, J.},
title={Ontology support for abstraction layer modularization},
journal={SPLC 2010 - Proceedings of the 14th International Software Product Line Conference},
year={2010},
pages={35-39},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087978925&partnerID=40&md5=4bbc9eaa573f336e07f18e24cc9c6a99},
abstract={Abstraction layers have been widely used to increase the portability of a software system by hiding the implementation details of underlying resources (e.g., OS, hardware, and reusable libraries). Abstraction layers have also been adopted in Software Product Lines (SPLs), which assist in the creation of a family of products by reusing common core assets and managing variants in a family domain. An abstraction layer provides transparent and unified access to the APIs of underlying resources. Abstraction layer APIs are modularizd by generalizing the APIs of underlying resources based on semantic similarity across common resources. Thus, an abstraction layer inherently needs to handle the semantic variants of the underlying APIs. However, the lack of a systematic approach for evolving an abstraction layer in accordance with the evolution of underlying resources may restrict its usage. This paper describes an approach toward ontology-based feature modeling to build and maintain the abstraction layer in a modularized and systematic way. The combination of ontologies and feature modeling can assist in modularizing abstraction layers by identifying the semantic similarities in APIs and provide insight into the variability of the underlying resources. © 2010 SPLC 2010 - Proceedings of the 14th International Software Product Line Conference. All rights reserved.},
author_keywords={Abstraction Layer;  Feature model;  Model-Driven Engineering;  Ontology;  Software Product Lines},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2010898,
author={Lee, K.},
title={Uses of open source remote sensing software for interoperable geo-web implementation},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2010},
volume={38},
pages={898-901},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924087301&partnerID=40&md5=7569ee096897eef1eb7040f49e7b1c92},
abstract={In the geo-spatial domain, new web-based computing technologies for interoperable geo-web are emerged. software as a service, asynchronous particle update, mashup, open sources, rich user experiences, collaborate tagging, open interoperability and structured information. Among them, open source developments or their applications in the geo-spatial community have somewhat long history, and nowadays they are regarded on the maturing stage in both the academics and industries. The main theme in this work is to use of the open source remote sensing (OSRS) software covering satellite image processing. Currently, OSRS software can be categorized into some types: full source codes, application programming interface (API) and development libraries. Furthermore, compared to proprietary remote sensing tools, some OSRS software provides highly advanced functions to fit specific target applications, as well as basic pre-processing or post-processing ones. However, till now, there are a few comparative and guidance studies to OSRS specialties. In this study, OSRS classification and its summary concerning features and especial specifications are presented in the consideration to land applications. Finally, geo-web architecture and system design with several OSRS software is shown in order to land applications to construct remote sensing contents and to provide interoperable web services in geo-web. It is thought that the application and implementation of OSRS software can widen uses-scope of remote sensing applications as IT main stream.},
author_keywords={Design;  Interface;  Internet/web;  Interoperability;  Open systems;  Software;  Standards},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2010,
author={Lee, K.},
title={Intelligent geo-web services based on hybrid-mashup using open source geo-spatial software},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2010},
volume={38},
number={4W13},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923878919&partnerID=40&md5=c0d21fc5dd4174e168175fbaccbb8912},
abstract={In these days, geo-spatial technologies, as components in geo-web, according to extension of web 2.0 or more paradigms are diversified and emerged: Software as a service, asynchronous particle update, mashup, open sources, rich user experiences, collaborate tagging, and structured information. These are closely related and linked to each other, so as to design and implementation stages for a given target application to meet users' needs. In the mean time, many types of applications using some of them have been developed and implemented. The main theme in this study is a sort of hybrid-mashup. This term is distinguished from data-centric mashup, a typical mashup. As well, it focuses on open sources in geo-spatial communities, open APIs provided by web portal or commercial vendors, geo-standards in OGC and ISO and freely-accessed geo-based data. This can be regarded as technological bases towards more intelligent geo-web to fulfil more complex users' demands beyond data level. In this study, some cases are presented; for examples, mashup using Google maps API, smart 3D applications on mobile devices, or data interoperable application using middle ware of open sources. Architecture with layered structure for hybrid-mashup in geo-web is proposed with an example case. Open sources widely used in geo-spatial domain such as Geoserver or Deegree are utilized, and other open sources are also used in design process: PostgreSQL/PostGIS as open DBMS and spatial engine, and OpenLayers or Google maps API as client modules. The beneficial points of hybrid-mashup in the view from intelligent geo-web implementation are system adaptability and portability, and extensibility for user-sided application purposes. © 2010 ISPRS Archives.},
author_keywords={Interface;  Internet/Web;  Metadata;  Ontology;  Open systems;  Software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Valilai20101095,
author={Valilai, O.F. and Houshmand, M.},
title={INFELT STEP: An integrated and interoperable platform for collaborative CAD/CAPP/CAM/CNC machining systems based on STEP standard},
journal={International Journal of Computer Integrated Manufacturing},
year={2010},
volume={23},
number={12},
pages={1095-1117},
doi={10.1080/0951192X.2010.527373},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649527720&doi=10.1080%2f0951192X.2010.527373&partnerID=40&md5=f5eccf1588f0d00da929236d0e84d829},
abstract={Integrated product development is comprised of CAD, process planning and CNC code generation based on an integrated data structure. To make various CAD/CAM software solutions with different internal product data structures interoperable, it is necessary to realise reliable and robust information exchange capability in a manufacturing environment. To enable interoperability amongst integrated product development processes, management of collaboration of diverse CAD/CAM software packages is of outmost importance. In this article, the fundamental requirements for achieving interoperability and collaboration management in an integrated, enterprise-wide, product development process environment are discussed. Based on these requirements, the present prominent integrated, interoperable and collaborative CAD/CAM information system platforms are comparatively reviewed. There are two main paradigms for development of such platforms - viz. neutral-file based and procedure based. The merits and limitations of each paradigm are discussed. To overcome the limitations of the current present platforms, a three-layered integrated and interoperable platform, named INFELT STEP, for collaborative and interoperable product design/development using different and diverse CAD/CAPP/CAM/CNC software is proposed. This platform conjugates the capabilities of both main paradigms to eliminate their limitations. The layered structure of INFELT STEP caters for the requirements of an integrated, interoperable and collaborative computer-based manufacturing and supports the entire range of software packages in the CAD/CAPP/CAM product development chain. In this platform, each software package can send product data based on its own internal data structures to other packages that are linked to and communicate with this platform. Different layers of INFELT STEP convert software packages' processed data to its structured data models, which are based on the STEP standard-and then store them in its database. Conversely, INFELT STEP layers can retrieve structured data stored in its database and convert them to the format acceptable by a specific CAD/CAM/CAPP software package that is cooperating in a production workflow. This proposed platform manages the collaboration of CAD/CAM software packages, maintains the integration of CAD/CAM/CNC operations based on STEP data models, and also enables interoperability of such packages with different local data structures. The capabilities of INFELT STEP are demonstrated by using it in a prototype implementation. © 2010 Taylor & Francis.},
author_keywords={CAD/CAM integration;  distributed product design and production;  ISO 10303 (STEP);  manufacturing collaboration;  manufacturing interoperability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cuntz2010,
author={Cuntz, H. and Forstner, F. and Borst, A. and Häusser, M.},
title={One rule to grow them all: A general theory of neuronal branching and its practical application},
journal={PLoS Computational Biology},
year={2010},
volume={6},
number={8},
doi={10.1371/journal.pcbi.1000877},
art_number={e1000877},
note={cited By 213},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049446478&doi=10.1371%2fjournal.pcbi.1000877&partnerID=40&md5=089413f7137cd745ec23637e693243ac},
abstract={Understanding the principles governing axonal and dendritic branching is essential for unravelling the functionality of single neurons and the way in which they connect. Nevertheless, no formalism has yet been described which can capture the general features of neuronal branching. Here we propose such a formalism, which is derived from the expression of dendritic arborizations as locally optimized graphs. Inspired by Ramón y Cajal's laws of conservation of cytoplasm and conduction time in neural circuitry, we show that this graphical representation can be used to optimize these variables. This approach allows us to generate synthetic branching geometries which replicate morphological features of any tested neuron. The essential structure of a neuronal tree is thereby captured by the density profile of its spanning field and by a single parameter, a balancing factor weighing the costs for material and conduction time. This balancing factor determines a neuron's electrotonic compartmentalization. Additions to this rule, when required in the construction process, can be directly attributed to developmental processes or a neuron's computational role within its neural circuit. The simulations presented here are implemented in an open-source software package, the "TREES toolbox," which provides a general set of tools for analyzing, manipulating, and generating dendritic structure, including a tool to create synthetic members of any particular cell group and an approach for a model-based supervised automatic morphological reconstruction from fluorescent image stacks. These approaches provide new insights into the constraints governing dendritic architectures. They also provide a novel framework for modelling and analyzing neuronal branching structures and for constructing realistic synthetic neural networks. © 2010 Cuntz et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Díaz2010987,
author={Díaz, J. and Hernández, S. and Fontán, A. and Romera, L.},
title={A computer code for finite element analysis and design of post-tensioned voided slab bridge decks with orthotropic behaviour},
journal={Advances in Engineering Software},
year={2010},
volume={41},
number={7-8},
pages={987-999},
doi={10.1016/j.advengsoft.2010.04.005},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955473841&doi=10.1016%2fj.advengsoft.2010.04.005&partnerID=40&md5=a81c93a5d5b3c7feec9005a0c707f841},
abstract={This paper presents a computer software which allows the generation of a complete structural model of a concrete bridge with a voided slab deck, a common design for medium span bridges. The code implements the orthotropic plate paradigm and provides a graphical user interface, which allows both preprocessing and post-processing, linked to a commercial finite element package. A description of the code is presented, along with the formulation of the orthotropic plate. Verification and comparison examples demonstrate the performance and features of the software and also the applicability of the formulation. © 2010 Elsevier Ltd. All rights reserved.},
author_keywords={Bridge design;  Concrete deck;  Finite element analysis;  Orthotropic plate;  Voided slab},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Müller20091411,
author={Müller, C.L. and Baumgartner, B. and Ofenbeck, G. and Schrader, B. and Sbalzarini, I.F.},
title={pCMALib: A parallel Fortran 90 library for the evolution strategy with covariance matrix adaptation},
journal={Proceedings of the 11th Annual Genetic and Evolutionary Computation Conference, GECCO-2009},
year={2009},
pages={1411-1418},
doi={10.1145/1569901.1570090},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-72749089604&doi=10.1145%2f1569901.1570090&partnerID=40&md5=aba5120b1011273a70f6ce2eba413345},
abstract={We present pCMALib, a parallel software library that implements the Evolution Strategy with Covariance Matrix Adaptation (CMA-ES). The library is written in Fortran 90/95 and uses the Message Passing Interface (MPI) for efficient parallelization on shared and distributed memory machines. It allows single CMA-ES optimization runs, embarrassingly parallel CMA-ES runs, and coupled parallel CMA-ES runs using a cooperative island model. As one instance of an island model CMA-ES, the recently presented Particle Swarm CMA-ES (PS-CMA-ES) is included using collaborative concepts from Swarm Intelligence for the migration model. Special attention has been given to an efficient design of the MPI communication protocol, a modular software architecture, and a user-friendly programming interface. The library includes a Matlab interface and is supplemented with an efficient Fortran implementation of the official CEC 2005 set of 25 real-valued benchmark functions. This is the first freely available Fortran implementation of this standard benchmark test suite. We present test runs and parallel scaling benchmarks on Linux clusters and multi-core desktop computers, showing good parallel efficiencies and superior computational performance compared to the reference implementation. Copyright 2009 ACM.},
author_keywords={CMA-ES;  Evolution strategies;  Parallel island model;  Software library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2009294,
author={Li, B.},
title={Construction of application software for enterprise based on semantic SOA and component oriented technology},
journal={2009 2nd International Conference on Intelligent Computing Technology and Automation, ICICTA 2009},
year={2009},
volume={4},
pages={294-297},
doi={10.1109/ICICTA.2009.786},
art_number={5288277},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-71949112041&doi=10.1109%2fICICTA.2009.786&partnerID=40&md5=f66722ad41e713370f11dd663d08f96f},
abstract={The construction of enterprise application software and the dynamic semantic integration based on SOA and component oriented technologies are presented in this paper. After analyzing the EERP (End-to-End Resource Planning) Web service functions and the service requirements in the refinery enterprise in depth, it develops the component libraries and then gives a solution to build the EERP information systems based on semantic SOA and component oriented technologies. Considering of the two aspects about technology and management, this paper investigates the new approach to accomplish the enterprise information system. © 2009 IEEE.},
author_keywords={Component oriented;  EERP(End-to-End Resource Planning);  Semantic SOA;  SOA(Service Oriented Architecture)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pauly2009,
author={Pauly, T. and Higginbottom, I. and Pederson, H. and Malzone, C. and Corbett, J. and Wilson, M.},
title={Keeping pace with technology through the development of an intuitive data fusion, management, analysis & visualization software solution},
journal={OCEANS '09 IEEE Bremen: Balancing Technology with Future Needs},
year={2009},
doi={10.1109/OCEANSE.2009.5278290},
art_number={5278290},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-71249153208&doi=10.1109%2fOCEANSE.2009.5278290&partnerID=40&md5=56986609b42fd52fd4833dd41c6dc12a},
abstract={A significant challenge facing researchers is the bridging of domains between the natural and physical sciences through the integration, analysis & visualization of multivariate environmental data from a variety of sources. Typically the temporal, visual and analytical components are not well handled within a single software application or by existing tools, and the final products are often difficult to share with collaborating researchers and managers. To address these issues, Myriax has launched Eonfusion; the next generation of geospatial analysis software which provides a state-of-the-art 4D-analysis environment that is intuitive and readily extensible. The software significantly enhances the ease with which scientists can integrate diverse data types (raster, vector and media) and share methods across disciplines. The powerful 4D-visualization interface also incorporates a range of analysis and plotting tools as well as a time-navigation capability that allow users to explore the temporal evolution of spatial structure and resolved relationships. Its revolutionary fusion capability allows data at different temporal or spatial scales to be reconciled, attributes merged and topological relationships identified. The software is customizable, through an integrated coding environment for algorithm implementation and an Application Programming Interface (API), to facilitating the development of modules. It is ideal for both undertaking complex analyses and communicating syntheses. This paper introduces the concepts behind fusion of irregular vector data in Eonfusion. Multiple data sets, including modeled and sampled data, can be fused into a single set with shared coordinates, enabling the discovery of topological relationships between coincident data items. These relationships also allow data attributes to be directly compared and transferred between data sets. A corresponding process also allows raster data attributes to be directly compared with vector data attributes through a raster mapping form of fusion. Both the raster and vector data sets can be visualized together, and the attribute transfer results displayed and used for further analysis. Vector data is introduced to the application as sets of spatial features (e.g. points, lines or polygons). Attributes may vary from vertex to vertex across a spatial feature (vertex attributes, e.g. x, y, z, t), or may be constant across a feature (feature attributes). Fusion typically takes place in a space defined by a subset of common vertex attributes. The implementation is sufficiently general to use any such collection of attributes, for example temperature or mean volume backscatter. A simple yet powerful example is the fusion of video data with camera trajectory data for ground truthing acoustic data. This architecture enables user-positioned "probes" in scenes on the camera trajectory line to locate the corresponding video frames. Along track acoustic integration data can also be fused with the trajectory data, providing tools for seabed classification or combined net/acoustic studies. This example represents the simple application of a powerful set of tools for oceanographic and ecological studies. Eonfusion was developed with the support of leading researchers around the world to meet the demands of the latest marine research tools, technology and methodology. Eonfusion moves the technology horizon far beyond simultaneous 4D visualization of coincident data, to the elegant integration of data via fusion. The ability to fuse data from multiple sources in a versatile 4-dimensional visualization environment provides new opportunities for observation insights into marine ecosystems. It will ultimately yield the analysis methodologies of the future. ©2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Leontie2009830,
author={Leontie, E. and Bloom, G. and Narahari, B. and Simha, R. and Zambreno, J.},
title={Hardware containers for software components: A trusted platform for COTS-based systems},
journal={Proceedings - 12th IEEE International Conference on Computational Science and Engineering, CSE 2009},
year={2009},
volume={2},
pages={830-836},
doi={10.1109/CSE.2009.56},
art_number={5283136},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70749140736&doi=10.1109%2fCSE.2009.56&partnerID=40&md5=0f134c88a6ae9c8c8edcfd33b0856394},
abstract={Much of modern software development consists of assembling together existing software components and writing the glue code that integrates them into a unified application. The term COTS-Based System (CBS) is often used to describe such applications, for which the components assembled are understood to be Commercial- Off-The-Shelf (COTS) components written by a multitude of independent third parties. The manner of assembly in CBS includes full-source components that are integrated at compile-time, pure-binary libraries incorporated at loadtime, and plugins that are loaded into the application at execution time by the user. Because components have access to system resources, applications may crash due to faulty components or may be compromised by malicious components. In this paper, we ask the question: can hardware support the development and deployment of CBS by providing applications with a trusted platform for managing components and their interactions? We present an architecture that places each CBS component in a hardware-enforced container. The hardware then detects improper usage of system resources (unauthorized memory accesses or denial-of-service) and enables applications to undertake a hardware-supervised recovery procedure. Furthermore, the hardware also maintains a violation record to enable developers to recreate the violation for the purpose of debugging and further development. Taken together, the purpose of the architecture we propose is to enable executing untrusted CBS code on trusted hardware. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Longshaw2009207,
author={Longshaw, S.M. and Turner, M.J. and Finch, E. and Gawthorpe, R.},
title={Discrete element modelling using a parallelised physics engine},
journal={Theory and Practice of Computer Graphics 2009, TPCG 2009 - Eurographics UK Chapter Proceedings},
year={2009},
pages={207-214},
doi={10.2312/LocalChapterEvents/TPCG/TPCG09/207-214},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878702425&doi=10.2312%2fLocalChapterEvents%2fTPCG%2fTPCG09%2f207-214&partnerID=40&md5=3096a8557d81d45b4af072bb40483ad7},
abstract={Discrete Element Modelling (DEM) is a technique used widely throughout science and engineering. It offers a convenient method with which to numerically simulate a system prone to developing discontinuities within its structure. Often the technique gets overlooked as designing and implementing a model on a scale large enough to be worthwhile can be both time consuming and require specialist programming skills. Currently there are a few notable efforts to produce homogenised software to allow researchers to quickly design and run DEMs with in excess of 1 million elements. However, these applications, while open source, are still complex in nature and require significant input from their original publishers in order for them to include new features as a researcher needs them. Recently software libraries notably from the computer gaming and graphics industries, known as physics engines, have emerged. These are designed specifically to calculate the physical movement and interaction of a system of independent rigid bodies. They provide conceptual equivalents of real world constructions with which an approximation of a realistic scenario can be quickly built. This paper presents a method to utilise the most notable of these engines, NVIDIAs PhysX, to produce a parallelised geological DEM capable of supporting in excess of a million elements. © The Eurographics Association 2009.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{McCallum20091249,
author={McCallum, A. and Schultz, K. and Singh, S.},
title={FACTORIE: Probabilistic programming via imperatively defined factor graphs},
journal={Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference},
year={2009},
pages={1249-1257},
note={cited By 140},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863338363&partnerID=40&md5=75bddc10bb146787d393c7f2f027e7d0},
abstract={Discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their application to complex relational data. The power in relational models is in their repeated structure and tied parameters; at issue is how to define these structures in a powerful and flexible way. Rather than using a declarative language, such as SQL or first-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning. By combining the traditional, declarative, statistical semantics of factor graphs with imperative definitions of their construction and operation, we allow the user to mix declarative and procedural domain knowledge, and also gain significant efficiencies. We have implemented such imperatively defined factor graphs in a system we call FACTORIE, a software library for an object-oriented, strongly-typed, functional language. In experimental comparisons to Markov Logic Networks on joint segmentation and coreference, we find our approach to be 3-15 times faster while reducing error by 20-25%-achieving a new state of the art.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wölkl2009635,
author={Wölkl, S. and Shea, K.},
title={A computational product model for conceptual design using sysml},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2009},
volume={2},
number={PARTS A AND B},
pages={635-645},
doi={10.1115/DETC2009-87239},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155188776&doi=10.1115%2fDETC2009-87239&partnerID=40&md5=abe6c817956ca075df468daa4c773fc6},
abstract={The importance of the concept development phase in product development is contradictory to the level and amount of current computer-based support for it, especially with regards to mechanical design. Paper-based methods for conceptual design offer a far greater level of maturity and familiarity than current computational methods. Engineers usually work with software designed to address only a single stage of the concept design phase, such as requirements management tools. Integration with software covering other stages, e.g. functional modeling, is generally poor. Using the requirements for concept models outlined in the VDI 2221 guideline for systematic product development as a starting point, the authors propose an integrated product model constructed using the Systems Modeling Language (SysML) that moves beyond geometry to integrate all necessary aspects for conceptual design. These include requirements, functions and function structures, working principles and their structures as well as physical effects. In order to explore the applicability of SysML for mechanical design, a case study on the design of a passenger car's luggage compartment cover is presented. The case study shows that many different SysML diagram types are suitable for formal modeling in mechanical concept design, though they were originally defined for software and control system development. It is then proposed that the creation and use of libraries defining generic as well as more complicated templates raises efficiency in modeling. The use of diagrams and their semantics for conceptual modeling make SysML a strong candidate for integrated product modeling of mechanical as well as mechatronic systems. © 2009 by ASME.},
author_keywords={Conceptual Design;  Formal Modeling Language;  Integrated Product Model;  SysML},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{González200961,
author={González, F. and Luaces, A. and Dopico, D. and González, M.},
title={Parallel linear equation solvers and OpenMp in the context of multibody system dynamics},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={2009},
volume={4},
number={PARTS A, B AND C},
pages={61-71},
doi={10.1115/DETC2009-86274},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155166602&doi=10.1115%2fDETC2009-86274&partnerID=40&md5=30bef13ab6044f3ed6911b77e9c0def4},
abstract={Computational efficiency of numerical simulations is a key issue in multibody system (MBS) dynamics, and parallel computing is one of the most promising approaches to increase the computational efficiency of MBS dynamic simulations. The present work evaluates two non-intrusive parallelization techniques for multibody system dynamics: parallel sparse linear equation solvers and OpenMP. Both techniques can be applied to existing simulation software with minimal changes in the code structure; this is a major advantage over MPI (Message Passing Interface), the de facto standard parallelization method in multibody dynamics. Both techniques have been applied to parallelize a starting sequential implementation of a global index-3 augmented Lagrangian formulation combined with the trapezoidal rule as numerical integrator, in order to solve the forward dynamics of a variable number of loops four-bar mechanism. This starting implementation represented a highly optimized code, where the overhead of parallelization would represent a considerable part of the total amount of elapsed time in calculations. Several multi-threaded solvers have been added to the original software. In addition, parallelizable regions of the code have been detected and multi-threaded via OpenMP directives. Numerical experiments have been performed to measure the efficiency of the parallelized code as a function of problem size and matrix filling ratio. Results show that the best parallel solver (Pardiso) performs better than the best sequential solver (CHOLMOD) for multibody problems of large and medium sizes leading to matrix fillings above 10 non-zeros per variable. OpenMP also proved to be advantageous even for problems of small sizes, in despite of the small percentage of parallelizable workload with respect to the total burden of the execution of the code. Both techniques delivered speedups above 70% of the maximum theoretical values for a wide range of multibody problems. © 2009 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Odersky2009427,
author={Odersky, M. and Moors, A.},
title={Fighting bit rot with types (Experience report: Scala collections)},
journal={Leibniz International Proceedings in Informatics, LIPIcs},
year={2009},
volume={4},
pages={427-451},
doi={10.4230/LIPIcs.FSTTCS.2009.2338},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650086118&doi=10.4230%2fLIPIcs.FSTTCS.2009.2338&partnerID=40&md5=19e5f9243afcc9d3d2fd3282bf06e621},
abstract={We report on our experiences in redesigning Scala's collection libraries, focussing on the role that type systems play in keeping software architectures coherent over time. Type systems can make software architecture more explicit but, if they are too weak, can also cause code duplication. We show that code duplication can be avoided using two of Scala's type constructions: higher-kinded types and implicit parameters and conversions. © Odersky, Moors.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{ArqueroHidalgo2009101,
author={Arquero Hidalgo, Á. and Álvarez Alonso, M. and Martínez Izquierdo, E.},
title={Decision management making by AHP (Analytical Hierarchy Process) trought GIS data},
journal={IEEE Latin America Transactions},
year={2009},
volume={7},
number={1},
pages={101-106},
doi={10.1109/TLA.2009.5173471},
art_number={5173471},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955017532&doi=10.1109%2fTLA.2009.5173471&partnerID=40&md5=d836416e12523f7fb4815f4721350158},
abstract={In this work, we propose the use of the AHP (Analytical Hierarchy Process) as a mathematical tool to structure a multiple criteria problem like a visual pattern. The main objective pursued is to determine what is the optimal placing to build a urban construction to locate a university library. In this case, it has been applied for the Campus of Montegancedo of the Polytechnic University of Madrid (Spain), where the Faculty of Computer science is placed. The data come from a Geographical Information System (GIS). Three different profiles of standard users have been considered and they determine the management of the final decision to take, in order to carry out the study. The use of the commercial software Expert Choice facilitates efficiently this management.},
author_keywords={Analytical Hierarchy Process (AHP);  Expert choice;  Geographical Information System (GIS);  Multiple criteria decision-making},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{VanLangeveld2009316,
author={Van Langeveld, M. and Kessler, R.},
title={Educational impact of digital visualization and auditing tools on a digital character production course},
journal={FDG 2009 - 4th International Conference on the Foundations of Digital Games, Proceedings},
year={2009},
pages={316-323},
doi={10.1145/1536513.1536567},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953571299&doi=10.1145%2f1536513.1536567&partnerID=40&md5=55b36905a19aa7eeb00d49c94bb994d2},
abstract={Many Universities and Colleges are building interdisciplinary programs that overlap engineering and fine art departments allowing a focus on games, special effects, animation and other forms of invention that require interdisciplinary efforts. With increasing demands for education linking the Engineering Sciences and Fine Arts, fueled by the competitive nature of the industries that recruit graduates, educators need to become more efficient and effective in their jobs of educating engineering and fine art majors in cross-disciplinary courses. Many of the courses that draw from the disciplines of the engineering sciences and fine arts are neglected in the research of educational best practices and tools for enhancing the learning experiences of the students. The courses are also becoming larger and more unmanageable as presently taught. Digital character production courses can range from predominantly engineering content to one with mostly art content, or a combination in the middle of the two departments depending on curriculum design. Our course at the University of Utah is directly in the center, drawing equally from both disciplines. It gives students an applied experience in 3D graphics and the computer science that helps to produce 3D graphics. It is a prerequisite for our Machinima Class, which extends the blend of art and technology as it immerses students into 3D game engines for creating cinematic short animations. Our particular digital character production curriculum has been taught at many universities and has been refined over the last twelve years. Presented in this paper are two experimental comparisons between traditional visualization tools and digital visualization tools, which are less expensive, easier to distribute, easier to arrange/procure and easier to transport than the traditional tools. Traditional visualization tools include lifelike skeleton reproductions, wooden body mass structures, actual human models and anatomy drawing books. They are used in conjunction with lectures, demonstrations and one-on-one lab instruction. The digital visualization tools that are contrasted in this paper are: a layered anatomically correct, digital human model (skin, muscles, masses and some bones adapted from several sources) and a VisTrails MAYA version of a properly produced human figure (interactive animation). The digital tools are used to replace the traditional visualization tools used in the same educational curriculum, which teaches students to design, model and produce digital characters for games, machinima, and animation. The first experiment assesses if the digital visualization tools are comparable to traditional tools in three areas; for improving a student's understanding of the complex software packages used to produce characters, for improving specific techniques used to model 3D characters, and for improving understanding of 3D form/visual relationships. The second experimental comparison extends the analysis of the third area garnered from the other experiment to determine if the improved understanding of form/visual relationships extends into non-digital medium. Both experiments were designed to measure learning experiences and the ability to adapt modeling processes to a variety of different characters and not to just duplicate the process on similar characters. Copyright 2009 ACM.},
author_keywords={3D character modeling;  Anatomy;  Contour design;  Digital entertainment industry;  Education;  Form;  Interdisciplinary curriculum;  Machinima;  Technique;  Video games},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Takizawa2009408,
author={Takizawa, H. and Sato, K. and Komatsut, K. and Kobayashit, H.},
title={CheCUDA: A checkpoint/restart tool for CUDA applications},
journal={Parallel and Distributed Computing, Applications and Technologies, PDCAT Proceedings},
year={2009},
pages={408-413},
doi={10.1109/PDCAT.2009.78},
art_number={5372771},
note={cited By 69},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950975351&doi=10.1109%2fPDCAT.2009.78&partnerID=40&md5=8a8e3ae584198fe9977d47241a765427},
abstract={In this paper, a tool named CheCUDA is designed to checkpoint CUDA applications that use GPUs as accelerators. As existing checkpoint/restart implementations do not support checkpointing the GPU status, CheCUDA hooks a part of basic CUDA driver API calls in order to record the status changes on the main memory. At checkpointing, CheCUDA stores the status changes in a file after copying all necessary data in the video memory to the main memory and then disabling the CUDA runtime. At restarting, CheCUDA reads the file, re-initializes the CUDA runtime, and recovers the resources on GPUs so as to restart from the stored status. This paper demonstrates that a prototype implementation of CheCUDA can correctly checkpoint and restart a CUDA application written with basic APIs. This also indicates that CheCUDA can migrate a process from one PC to another even if the process uses a GPU. Accordingly, CheCUDA is useful not only to enhance the dependability of CUDA applications but also to enable dynamic task scheduling of CUDA applications required especially on heterogeneous GPU cluster systems. This paper also shows the timing overhead for checkpointing. © 2009 IEEE.},
author_keywords={Checkpoint/restart;  Compute unified device architecture;  Graphics processing units},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2009137,
author={Guo, W. and Wang, Y.},
title={An incident management model for SaaS application in the IT organization},
journal={ICRCCS 2009 - 2009 International Conference on Research Challenges in Computer Science},
year={2009},
pages={137-140},
doi={10.1109/ICRCCS.2009.42},
art_number={5401320},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949947969&doi=10.1109%2fICRCCS.2009.42&partnerID=40&md5=900c8622661e934b228e07624566af19},
abstract={Software as a Service (SaaS) is a new style mode of software deployment whereby a provider licenses an application to customers for use as a service on demand, and it challenges the traditional software mode and makes major changes for IT organizations to deploy application. Incidents are recognized as service disruptions which can have a considerable impact on business capability of IT organizations, thus calling for the implementation of efficient incident management and service restoration processes. Information Technology Infrastructure Library (ITIL) as the best- practice of IT Service Management proposes incident management model. However the original model doesn't adapt for the organizations which change their IT department structures and operation processes owing to adopting SaaS application. This paper firstly describes the background of SaaS, analyzes the issues of existing incident management model based on ITIL, then propose an organization model to manage SaaS operation and maintenance and a notation approach for modeling process. Finally, this paper optimizes the incident process based on above approach. © 2009 IEEE.},
author_keywords={Incident management;  IT service management;  ITIL (Information Technology Infrastructure Librarly);  SaaS (Software as a Service)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kalibera200981,
author={Kalibera, T. and Pizlo, F. and Hosking, A.L. and Vitek, J.},
title={Scheduling hard real-time garbage collection},
journal={Proceedings - Real-Time Systems Symposium},
year={2009},
pages={81-92},
doi={10.1109/RTSS.2009.40},
art_number={5368170},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77649304778&doi=10.1109%2fRTSS.2009.40&partnerID=40&md5=c0f9a3ac00a770c878fa9419c32e332c},
abstract={Managed languages such as Java and C# are increasingly being considered for hard real-time applications because of their productivity and software engineering advantages. Automatic memory management, or garbage collection, is a key enabler for robust, reusable libraries, yet remains a challenge for analysis and implementation of real-time execution environments. This paper comprehensively compares the two leading approaches to hard real-time garbage collection. While there are many design decisions involved in selecting a real-time garbage collection algorithm, for time-based garbage collectors researchers and practitioners remain undecided as to whether to choose periodic scheduling or slack-based scheduling. A significant impediment to valid experimental comparison is that the commercial implementations use completely different proprietary infrastructures. Here, we present Minuteman, a framework for experimenting with real-time collection algorithms in the context of a high-performance execution environment for real-time Java. We provide the first comparison of the two approaches, both experimentally using realistic workloads, and analytically in terms of schedulability. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee20091136,
author={Lee, M. and Jeon, J.-H. and Bae, J. and Jang, H.-S.},
title={Parallel implementation of a financial application on a GPU},
journal={ACM International Conference Proceeding Series},
year={2009},
volume={403},
pages={1136-1141},
doi={10.1145/1655925.1656132},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-74949140725&doi=10.1145%2f1655925.1656132&partnerID=40&md5=d2fd8857932f33e94246b9b326dce534},
abstract={The architecture of the latest Graphic Processing Unit (GPU) has surpassed the previous application-specific stream architecture. They consist of a number of uniform programmable units integrated on the same chip which facilitate the general-purpose computing beyond the graphic processing. With the multiple programmable units executing in parallel, the latest GPU shows superior performance for many different applications. Furthermore, programmers can have a direct control on the GPU pipeline using easy-to-use parallel programming environments, whereas they had to rely on specific graphics API's in the past. These advances in hardware and software make General-Purpose GPU (GPGPU) computing widespread. In this paper, using the latest GPU and its software environment, we parallelize a computationally demanding financial application based on Monte-Carlo methods and optimize its performance. Experimental results show that a GPU can achieve a superior performance, greater than 190x, compared with the CPU-only case. Copyright © 2009 ACM.},
author_keywords={Financial derivatives modeling;  GPU;  High performance computing;  Shared-memory},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ekberg20099,
author={Ekberg, J.-E. and Bugiel, S.},
title={Trust in a small package: Minimized MRTM software implementation for mobile secure environments},
journal={Proceedings of the ACM Conference on Computer and Communications Security},
year={2009},
pages={9-18},
doi={10.1145/1655108.1655111},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049124417&doi=10.1145%2f1655108.1655111&partnerID=40&md5=20216815de233e497818905f1aa5af7b},
abstract={In this paper we present a software-based implementation of a Mobile Remote Owner Trusted Module, using security extensions of contemporary System-On-Chip architectures. An explicit challenge are the constrained resources of such on-chip mechanisms. We expose a software architecture that minimizes the code and data size of the MRTM, applying some novel approaches proposed in recent research. Additionally, we explore alternatives within the specification to further optimize the size of MTMs. We present an analysis of specific new security issues induced by the architecture. Performance figures for an on-the-market mobile handset are provided. The results clearly indicate that a software-based MRTM is feasible on modern embedded hardware with legacy security environments. Copyright 2009 ACM.},
author_keywords={Mobile phones;  Mobile trusted module;  Platform security;  Secure hardware;  Trusted computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Abdeen2009103,
author={Abdeen, H. and Ducasse, S. and Sahraoui, H. and Allouiz, I.},
title={Automatic package coupling and cycle minimization},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2009},
pages={103-112},
doi={10.1109/WCRE.2009.13},
art_number={5328767},
note={cited By 76},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-73449132572&doi=10.1109%2fWCRE.2009.13&partnerID=40&md5=9841b1d3a926fb5c710bafb9c96b0bd1},
abstract={Object-oriented (OO) software is usually organized into subsystems using the concepts of package or module. Such modular structure helps applications to evolve when facing new requirements. However, studies show that as software evolves to meet requirements and environment changes, modularization quality degrades. To help maintainers improve the quality of software modularization we have designed and implemented a heuristic search-based approach for automatically optimizing inter-package connectivity (i.e., dependencies). In this paper, we present our approach and its underlying techniques and algorithm. We show through a case study how it enables maintainers to optimize OO package structure of source code. Our optimization approach is based on Simulated Annealing technique. © 2009 IEEE.},
author_keywords={Re-engineering;  Reverse engineering;  Search algorithms;  Software modularization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Garcia-Ojeda2009707,
author={Garcia-Ojeda, J.C. and DeLoach, S.A. and Robby},
title={agentTool process editor: Supporting the design of tailored agent-based processes},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2009},
pages={707-714},
doi={10.1145/1529282.1529430},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-72949124845&doi=10.1145%2f1529282.1529430&partnerID=40&md5=9828a4968ab555e6d19f9c01f885a503},
abstract={This paper describes the agentTool Process Editor (APE), an Eclipse plug-in based on the Eclipse Process Framework. The aim of APE is to facilitate the design, verification, and management of custom agent-oriented software development processes. APE provides five basic structures. The Library is a repository of agent-oriented method fragments. A Process Editor allows the management of tailored processes. Task Constraints help process engineers specify guidelines to constrain how tasks can be assembled, while a Process Consistency mechanism verifies the consistency of tailored processes against those constraints. Finally, the Process Management integrates APE with the agentTool III development environment and provides a way to measure project progress using Earned Value Analysis. Copyright 2009 ACM.},
author_keywords={agentTool III;  agentTool process editor;  CASE tool;  Method engineering;  O-MaSE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nystrom200923,
author={Nystrom, J.H.},
title={Automatic assessment of failure recovery in erlang applications},
journal={Erlang'09 - Proceedings of the 2009 ACM SIGPLAN Erlang Workshop, Co-located with the International Conference on Functional Programming, ICFP'09},
year={2009},
pages={23-32},
doi={10.1145/1596600.1596604},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70549109143&doi=10.1145%2f1596600.1596604&partnerID=40&md5=d5c373fb66788e7e5a62dd837d56eaaa},
abstract={Erlang is a concurrent functional language, especially tailored for distributed, highly concurrent and fault-tolerant software. An important part of Erlang is its support for failure recovery. A designer implements failure recovery by organising the processes of an Erlang application into tree structures, in which parent processes monitor failures of their children and are responsible for their restart. Libraries support the creation of such structures during system initialisation. We present a technique to automatically analyse that the process structure of an Erlang application is constructed in a way that guarantees recovery from process failures. First, we extract (part of) the process structure by static analysis of the initialisation code of the application. Thereafter, analysis of the process structure checks that it will recover from any process failure. We have implemented the technique in a tool, and applied it to several OTP library applications and to a subsystem of the AXD 301 ATM switch. Copyright © 2009 ACM.},
author_keywords={Erlang;  Fault-tolerance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fu2009189,
author={Fu, C. and Chang, H.},
title={Reusable components retrieval based on faceted classification with sem-library in domain component library},
journal={Proceedings - 2009 International Conference on Interoperability for Enterprise Software and Applications, IESA 2009},
year={2009},
pages={189-194},
doi={10.1109/I-ESA.2009.45},
art_number={5260834},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449633564&doi=10.1109%2fI-ESA.2009.45&partnerID=40&md5=56b8e7f6ed0a1c2e0771d1f0fbff396a},
abstract={Faceted classification with strong extensibility has been widely used in the classification and retrieval of component library. However, owing to its hierarchy structure, it lacks of understanding of the domain knowledge in domain component library. Aimed at solving this problem, this paper proposes a Sem-library to provide effective keyword proximity queries to express abundant domain knowledge in domain component library and release the users from esoteric vocabulary. Sem-Library supports basic semantic relationships and incremental query construction. Moreover, it is feasible and extensible for more complicated semantic relationships.},
author_keywords={Domain component library;  Facet;  Retrieval;  Semantic},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2009127,
author={Wang, Y. and Gaspes, V.},
title={A domain specific approach to network software architecture assuring conformance between architecture and code},
journal={Proceedings - 2009 4th International Conference on Digital Telecommunications, ICDT 2009},
year={2009},
pages={127-132},
doi={10.1109/ICDT.2009.31},
art_number={5205221},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449602416&doi=10.1109%2fICDT.2009.31&partnerID=40&md5=70bf332e8f6d075eaf8fea1cc9ef4f6f},
abstract={Network software is typically organized according to a layered architecture that is well understood. However, writing correct and efficient code that conforms with the architecture still remains a problem. To overcome this problem we propose to use a domain specific language based approach. The architectural constraints are captured in a domain specific notation that can be used as a source for automatic program generation. Conformance with the architecture is thus assured by construction. Knowledge from the domain allows us to generate efficient code. In addition, this approach enforces reuse of both code and designs, one of the major concerns in software architecture. In this paper, we illustrate our approach with PADDLE, a tool that generates packet processing code from packet descriptions. To describe packets we use a domain specific language of dependent types that includes packet overlays. From the description we generate C libraries for packet processing that are easy to integrate with other parts of the code. We include an evaluation of our tool. © 2009 IEEE.},
author_keywords={Dependent types;  Network software;  Program generation;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shoufan200998,
author={Shoufan, A. and Winky, T. and Molterz, G. and Hussz, S. and Strentzkex, F.},
title={A novel processor architecture for McEliece cryptosystem and FPGA platforms},
journal={Proceedings of the International Conference on Application-Specific Systems, Architectures and Processors},
year={2009},
pages={98-105},
doi={10.1109/ASAP.2009.29},
art_number={5200016},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049123271&doi=10.1109%2fASAP.2009.29&partnerID=40&md5=c3d0fce30afd4ce5a6123f1d77e768e9},
abstract={McEliece scheme represents a code-based public-key cryptosystem. So far, this cryptosystem was not employed because of efficiency questions regarding performance and communication overhead. This paper presents a novel processor architecture as a high-performance platform to execute key generation, encryption and decryption according to this cryptosystem. A prototype of this processor is realized on Virtex-5 FPGA and tested via a software API. A comparison with a similar software solution highlights the performance advantage of the proposed hardware solution. © 2009 IEEE.},
author_keywords={Cryptography hardware and implementation;  Cryptoprocessor;  FPGA;  Goppa code;  McEliece cryptosystem},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Olszak200919,
author={Olszak, A. and Jørgensen, B.N.},
title={Remodularizing Java programs for comprehension of features},
journal={ACM International Conference Proceeding Series},
year={2009},
pages={19-26},
doi={10.1145/1629716.1629722},
art_number={1629722},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350650809&doi=10.1145%2f1629716.1629722&partnerID=40&md5=03e936719d71b34c52504cf2eaa6374a},
abstract={Feature-oriented decomposition of software is known to improve a programmer's ability to understand and modify software during maintenance tasks. However, it is difficult to take advantage of this fact in case of object-oriented software due to lack of appropriate feature modularization mechanisms. In absence of these mechanisms, feature implementations tend to be scattered and tangled in terms of object-oriented abstractions, making the code implementing features difficult to locate and comprehend. In this paper we present a semi-automatic method for feature-oriented remodularization of Java programs. Our method uses execution traces to locate implementations of features, and Java packages to establish explicit feature modules. To evaluate usefulness of the approach, we present a case study where we apply our method to two real-world software systems. The obtained results indicate a significant improvement of feature representation in both programs, and confirm the low level of manual effort required by the proposed remodularization method. Copyright © 2009 ACM.},
author_keywords={Feature location;  Features;  Remodularization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Birk2009878,
author={Birk, Y. and Fiksman, E.},
title={Dynamic reconfiguration architectures for multi-context FPGAs},
journal={Computers and Electrical Engineering},
year={2009},
volume={35},
number={6},
pages={878-903},
doi={10.1016/j.compeleceng.2008.11.024},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350573468&doi=10.1016%2fj.compeleceng.2008.11.024&partnerID=40&md5=5f77334d63a9816eeead39f47b07a593},
abstract={Field-programmable gate arrays (FPGAs) are being integrated with processors on the same motherboard or even chip in order to achieve flexible high-performance computing, and this may become main stream in chip multi-core architectures. However, the expensive FPGA area is often used inefficiently, with much of the logic idle at any given time. This work, motivated by the Dynamic-Link Library (DLL) concept in software, explores the possibility of "hardware DLLs" by finding ways for fast dynamic incremental reconfiguration of FPGAs. So doing would, among other things, enable same-function replication at any given time, with functions changing quickly over time, thereby enabling efficient exploitation of data parallelism at no additional hardware cost. We present two new multi-context FPGA architectures based on two different configuration storage architectures: local and centralized. Problems such as configuration storage and reconfiguration (time, power and space) overhead are considered. Well known area and power models are used in evaluating various approaches and in order to provide guidelines for matching architectures to target applications. Lastly, we provide insights into resulting scheduling issues. Our findings provide the foundation and "rules of the game" for subsequent development of reconfiguration schedulers and execution environments. © 2008 Elsevier Ltd. All rights reserved.},
author_keywords={FPGA;  Multi-context architectures;  Reconfigurable computing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ganesan20091586,
author={Ganesan, D. and Keuler, T. and Nishimura, Y.},
title={Architecture compliance checking at run-time},
journal={Information and Software Technology},
year={2009},
volume={51},
number={11},
pages={1586-1600},
doi={10.1016/j.infsof.2009.06.007},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-69549123880&doi=10.1016%2fj.infsof.2009.06.007&partnerID=40&md5=6804891ba4c2ac95a560db34a1ae0cfc},
abstract={In this paper, we report on our experiences with architecture compliance checking - the process of checking whether the planned or specified software architecture is obeyed by the running system - of an OSGi-based, dynamically evolving application in the office domain. To that end, we first show how to dynamically instrument a running system in the context of OSGi in order to collect run-time traces. Second, we explain how to bridge the abstraction gap between run-time traces and software architectures, through the construction of hierarchical Colored Petri nets (CP-nets). In addition, we demonstrate how to design reusable hierarchical CP-nets. In an industry example, we were able to extract views that helped us to identify a number of architecturally relevant issues (e.g., architectural style violations, behavior violations) that would not have been detected otherwise, and could have caused serious problems like system malfunctioning or unauthorized access to sensitive data. Finally, we package valuable experiences and lessons learned from this endeavor. © 2009 Elsevier B.V. All rights reserved.},
author_keywords={Architecture compliance checking;  Hierarchical Colored Petri nets;  Run-time monitoring},
document_type={Article},
source={Scopus},
}

@ARTICLE{Varcholik2009523,
author={Varcholik, P. and Laviola Jr., J.J. and Nicholson, D.},
title={TACTUS: A hardware and software testbed for research in multi-touch interaction},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2009},
volume={5611 LNCS},
number={PART 2},
pages={523-532},
doi={10.1007/978-3-642-02577-8_57},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350327789&doi=10.1007%2f978-3-642-02577-8_57&partnerID=40&md5=69e1874ed072bb436f71adea9496d408},
abstract={This paper presents the TACTUS Multi-Touch Research Testbed, a hardware and software system for enabling research in multi-touch interaction. A detailed discussion is provided on hardware construction, pitfalls, design options, and software architecture to bridge the gaps in the existing literature and inform the researcher on the practical requirements of a multi-touch research testbed. This includes a comprehensive description of the vision-based image processing pipeline, developed for the TACTUS software library, which makes surface interactions available to multi-touch applications. Furthermore, the paper explores the higher-level functionality and utility of the TACTUS software library and how researchers can leverage the system to investigate multi-touch interaction techniques. © 2009 Springer Berlin Heidelberg.},
author_keywords={API;  HCI;  Multi-Touch;  Testbed;  Touch Screen},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheung200939,
author={Cheung, E. and Smith, T.M.},
title={Experience with modularity in an advanced teleconferencing service deployment},
journal={2009 31st International Conference on Software Engineering - Companion Volume, ICSE 2009},
year={2009},
pages={39-49},
doi={10.1109/ICSE-COMPANION.2009.5070962},
art_number={5070962},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349706073&doi=10.1109%2fICSE-COMPANION.2009.5070962&partnerID=40&md5=54553b407c7e0cb9db3db6a811b703d1},
abstract={In this paper, we describe our experience with the design of an advanced teleconferencing service under two different frameworks - an early implementation of the Distributed Feature Composition architecture, and the SIP Servlet API. The usual design goals of software modularity for encapsulation and reuse are pursued. Interestingly, two very different designs resulted. This paper discusses the factors that influenced our design decisions. In particular, we examine the different characteristics of the two frameworks as well as the maturity of project requirements, and illustrate the ways in which these factors affect various mechanisms for achieving software modularity. We also aim to draw on this experience to propose a set of design guidelines for building modular, composable SIP Servlet applications for Voice over IP and converged services. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dale2009864,
author={Dale, C. and Liu, J.},
title={Apt-p2p: A peer-to-peer distribution system for software package releases and updates},
journal={Proceedings - IEEE INFOCOM},
year={2009},
pages={864-872},
doi={10.1109/INFCOM.2009.5061996},
art_number={5061996},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349675558&doi=10.1109%2fINFCOM.2009.5061996&partnerID=40&md5=a537ff8a456802ea1014733a0cb301ab},
abstract={The Internet has become a cost-effective vehicle for software development and release, particular in the free software community. Given the free nature of this software, there are often a number of users motivated by altruism to help out with the distribution, so as to promote the healthy development of this voluntary society. It is thus naturally expected that a peer-topeer distribution can be implemented, which will scale well with large user bases, and can easily explore the network resources made available by the volunteers. Unfortunately, this application scenario has many unique characteristics, which make a straightforward adoption of existing peer-to-peer systems for file sharing (such as BitTorrent) suboptimal. In particular, a software release often consists of a large number of packages, which are difficult to distribute individually, but the archive is too large to be distributed in its entirety. The packages are also being constantly updated by the loosely-managed developers, and the interest in a particular version of a package can be very limited depending on the computer platforms and operating systems used. In this paper, we propose a novel peer-to-peer assisted distribution system design that addresses the above challenges. It enhances the existing distribution systems by providing compatible and yet more efficient downloading and updating services for software packages. Our design leads to apt-p2p, a practical implementation that extends the popular apt distributor. apt-p2p has been used in conjunction with Debian-based distribution of Linux software packages and is also available in the latest release of Ubuntu. We have addressed the key design issues in apt-p2p, including indexing table customization, response time reduction, and multi-value extension. They together ensure that the altruistic users' resources are effectively utilized and thus significantly reduces the currently large bandwidth requirements of hosting the software, as confirmed by our existing real user statistics gathered over the Internet. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lam2009579,
author={Lam, A. and Boehm, B.},
title={Experiences in developing and applying a software engineering technology testbed},
journal={Empirical Software Engineering},
year={2009},
volume={14},
number={5},
pages={579-601},
doi={10.1007/s10664-008-9096-2},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-72449151870&doi=10.1007%2fs10664-008-9096-2&partnerID=40&md5=87333caccb94ff2976e65a944bc65ff8},
abstract={A major problem in empirical software engineering is to determine or ensure comparability across multiple sources of empirical data. This paper summarizes experiences in developing and applying a software engineering technology testbed. The testbed was designed to ensure comparability of empirical data used to evaluate alternative software engineering technologies, and to accelerate the technology maturation and transition into project use. The requirements for such software engineering technology testbeds include not only the specifications and code, but also the package of instrumentation, scenario drivers, seeded defects, experimentation guidelines, and comparative effort and defect data needed to facilitate technology evaluation experiments. The requirements and architecture to build a particular software engineering technology testbed to help NASA evaluate its investments in software dependability research and technology have been developed and applied to evaluate a wide range of technologies. The technologies evaluated came from the fields of architecture, testing, state-model checking, and operational envelopes. This paper will present for the first time the requirements and architecture of the software engineering technology testbed. The results of the technology evaluations will be analyzed from a point of view of how researchers benefitted from using the SETT. The researchers just reported how their technology performed in their original findings. The testbed evaluation showed (1) that certain technologies were complementary and cost-effective to apply; (2) that the testbed was cost-effective to use by researchers within a well-specified domain of applicability; (3) that collaboration in testbed use by researchers and the practitioners resulted comparable empirical data and in actions to accelerate technology maturity and transition into project use, as shown in the AcmeStudio evaluation; and (4) that the software engineering technology testbed's requirements and architecture were suitable for evaluating technologies and accelerating their maturation and transition into project use. © 2008 Springer Science+Business Media, LLC.},
author_keywords={Software adoption;  Software maturity;  Technology evaluation;  Technology transition;  Testbed},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yao20091080,
author={Yao, G. and Xie, W. and Ju, H.},
title={Selection of four hoisting points for large-span steel trusses based on strain energy method},
journal={Chongqing Daxue Xuebao/Journal of Chongqing University},
year={2009},
volume={32},
number={9},
pages={1080-1085},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449119706&partnerID=40&md5=51f40899a84e6cf32e6f48dd4deafb80},
abstract={The strain energy method is applied to hoisting point selection in structural hoisting, and the feasibility of this method is investigated. Based on the number of hoisting points, a structural analysis is carried out for each possible combination of hoisting points in the whole variable space of nodes. The strain energy corresponding to the each combination is obtained based on the member forces from the structural analysis. The combinations are sorted based on their magnitudes of strain energy, and the combination with the smallest strain energy is picked out. The structure needed to be hoisted is in a rational state when it has low strain energy. The combination with the smallest strain energy is treated as the best selection of hoisting points. Hoisting construction of a workshop in Chongqing was taken as an example and three construction conditions were considered. The commercial package ANSYS was used to conduct the structural analysis and obtain the strain energy. The best selections were verified by another software MADIS.},
author_keywords={Four hoisting points;  Steel truss;  Strain energy principle;  Working condition analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lincke2009236,
author={Lincke, D. and Jansson, P. and Zalewski, M. and Ionescu, C.},
title={Generic libraries in c++ with concepts from high-level domain descriptions in haskell a domain-specific library for computational vulnerability assessment},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2009},
volume={5658 LNCS},
pages={236-261},
doi={10.1007/978-3-642-03034-5_12},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-69049097161&doi=10.1007%2f978-3-642-03034-5_12&partnerID=40&md5=921476eba6181cfca581d48200b1baa8},
abstract={A class of closely related problems, a problem domain, can often be described by a domain-specific language, which consists of algorithms and combinators useful for solving that particular class of problems. Such a language can be of two kinds: it can form a new language or it can be embedded as a sublanguage in an existing one. We describe an embedded DSL in the form of a library which extends a general purpose language. Our domain is that of vulnerability assessment in the context of climate change, formally described at the Potsdam Institute for Climate Impact Research. The domain is described using Haskell, yielding a domain specific sublanguage of Haskell that can be used for prototyping of implementations. In this paper we present a generic C++ library that implements a domain-specific language for vulnerability assessment, based on the formal Haskell description. The library rests upon and implements only a few notions, most importantly, that of a monadic system, a crucial part in the vulnerability assessment formalisation. We describe the Haskell description of monadic systems and we show our mapping of the description to generic C++ components. Our library heavily relies on concepts, a C++ feature supporting generic programming: a conceptual framework forms the domain-specific type system of our library. By using functions, parametrised types and concepts from our conceptual framework, we represent the combinators and algorithms of the domain. Furthermore, we discuss what makes our library a domain specific language and how our domain-specific library scheme can be used for other domains (concerning language design, software design, and implementation techniques). © IFIP International Federation for Information Processing 2009.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dechev200948,
author={Dechev, D. and Pirkelbauer, P. and Rouquette, N. and Stroustrup, B.},
title={Semantically enhanced containers for concurrent real-time systems},
journal={Proceedings of the International Symposium and Workshop on Engineering of Computer Based Systems},
year={2009},
pages={48-57},
doi={10.1109/ECBS.2009.12},
art_number={4839231},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650236591&doi=10.1109%2fECBS.2009.12&partnerID=40&md5=2695e6b3dcf6b8c25ca7cbd509599d02},
abstract={Future space missions, such as Mars Science Laboratory, are built upon computing platforms providing a high degree of autonomy and diverse functionality. The increased sophistication of robotic spacecraft has skyrocketed the complexity and cost of its software development and validation. The engineering of autonomous spacecraft software relies on the availability and application of advanced methods and tools that deliver safe concurrent synchronization as well as enable the validation of domainspecific semantic invariants. The software design and certification methodologies applied at NASA do not reach the level of detail of providing guidelines for the development of reliable concurrent software. To achieve effective and safe concurrent interactions as well as guarantee critical domain-specific properties in code, we introduce the notion of a Semantically Enhanced Container (SEC). A SEC is a data structure engineered to deliver the flexibility and usability of the popular ISO C++ Standard Template Library containers, while at the same time it is hand-crafted to guarantee domain-specific policies. We demonstrate the SEC proof-of-concept by presenting a shared nonblocking SEC vector. To eliminate the hazards of the ABA problem (a fundamental problem in lock-free programming), we introduce an innovative library for querying C++ semantic information. Our SEC design aims at providing an effective model for shared data access within the JPL's Mission Data System. Our test results show that the SEC vector delivers significant performance gains (a factor of 3 or more) in contrast to the application of nonblocking synchronization amended with the traditional ABA avoidance scheme. © 2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tripathi20091639,
author={Tripathi, V. and Mahesh, T.S.G. and Srivastava, D.A.},
title={Performance and language compatibility in software pattern detection},
journal={2009 IEEE International Advance Computing Conference, IACC 2009},
year={2009},
pages={1639-1643},
doi={10.1109/IADCC.2009.4809263},
art_number={4809263},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-66249119713&doi=10.1109%2fIADCC.2009.4809263&partnerID=40&md5=0b88a12efaeb19dd6fb84e1e11f186da},
abstract={Re documentation and design recovery are two important areas of reverse engineering. Detection of recurring organizations of classes and communicating objects, called Software Patterns, supports this process [1]. Many approaches to detect Software Patterns which have been published in the past years suffer from the problems of necessity of reference library, performance and language compatibility. This paper presents a model to solve those problems in software pattern detection. The proposed model solves the problem of necessity of reference library by detecting software patterns using Formal Concept Analysis (FCA). The proposed model solves the problem of performance by using the most efficient algorithm CMCG (Concept-Matrix based Concepts Generation) for the construction of concept lattice, which is the core data structure of FCA [3]. The proposed model solves the problem of language compatibility by using the language independent Meta model called MOOSE for taking the input information. The validity of this model was proved in theory and by experiment.© 2009 IEEE.},
author_keywords={Design recovery;  Reengineering.;  Reverse engineering;  Software patterns},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mignolet200934,
author={Mignolet, J.-Y. and Wuyts, R.},
title={Embedded multiprocessor systems-on-chip programming},
journal={IEEE Software},
year={2009},
volume={26},
number={3},
pages={34-41},
doi={10.1109/MS.2009.64},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-65749094399&doi=10.1109%2fMS.2009.64&partnerID=40&md5=eb87ac90325de90033abc81859b97a24},
abstract={Multiprocessor system-on-chip (MPSoC) platforms have found their way into embedded systems. The reason is a balanced combination of power efficiency with flexibility, which makes them cost effective compared to dedicated hardware or general-purpose platforms. However they're notoriously difficult to program because it's hard to develop programs that can exploit the parallel resources in such platforms. To help with cost effectively developing efficient code for an MPSoC system, a team of IMEC researchers has developing an MPSoC programming approach supported by a tool chain that enables the mapping of a single application on multiple cores. The tool chain consists of a tool that cleans C code so that it can be analyzed more efficiently, mapping tools that can manage memory hierarchies and generate source code for parallel threads, and a runtime library that abstracts the underlying hardware. © 2009 IEEE.},
author_keywords={Compilers;  Data mining;  embedded systems;  Guidelines;  Hardware;  Libraries;  Multicore/single chip multiprocessor;  Parallel architectures;  Probability density function;  Programming;  Runtime environments;  Software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aliaga200988,
author={Aliaga, J.I. and Almeida, F. and Badía, J.M. and Barrachina, S. and Blanco, V. and Castillo, M. and Mayo, R. and Quintana, E.S. and Quintana, G. and Remón, A. and Rodríguez, C. and De Sande, F. and Santos, A.},
title={Toward the parallelization of GSL},
journal={Journal of Supercomputing},
year={2009},
volume={48},
number={1},
pages={88-114},
doi={10.1007/s11227-008-0207-z},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349129496&doi=10.1007%2fs11227-008-0207-z&partnerID=40&md5=0169583a7a918a26d1e1fe2bd89845ae},
abstract={In this paper, we present our joint efforts to design and develop parallel implementations of the GNU Scientific Library for a wide variety of parallel platforms. The multilevel software architecture proposed provides several interfaces: a sequential interface that hides the parallel nature of the library to sequential users, a parallel interface for parallel programmers, and a web services based interface to provide remote access to the routines of the library. The physical level of the architecture includes platforms ranging from distributed and shared-memory multiprocessors to hybrid systems and heterogeneous clusters. Several well-known operations arising in discrete mathematics and sparse linear algebra are used to illustrate the challenges, benefits, and performance of different parallelization approaches. © 2008 Springer Science+Business Media, LLC.},
author_keywords={GNU Scientific Library;  Numerical scientific computing;  Parallel algorithms and architectures;  Web services},
document_type={Article},
source={Scopus},
}

@ARTICLE{Peirce2009,
author={Peirce, J.W.},
title={Generating stimuli for neuroscience using PsychoPy},
journal={Frontiers in Neuroinformatics},
year={2009},
volume={2},
number={JAN},
doi={10.3389/neuro.11.010.2008},
art_number={10},
note={cited By 982},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449265811&doi=10.3389%2fneuro.11.010.2008&partnerID=40&md5=64b50b02e8cd548483f689b71198cb80},
abstract={PsychoPy is a software library written in Python, using OpenGL to generate very precise visual stimuli on standard personal computers. It is designed to allow the construction of as wide a variety of neuroscience experiments as possible, with the least effort. By writing scripts in standard Python syntax users can generate an enormous variety of visual and auditory stimuli and can interact with a wide range of external hardware (enabling its use in fMRI, EEG, MEG etc.). The structure of scripts is simple and intuitive. As a result, new experiments can be written very quickly, and trying to understand a previously written script is easy, even with minimal code comments. PsychoPy can also generate movies and image sequences to be used in demos or simulated neuroscience experiments. This paper describes the range of tools and stimuli that it provides and the environment in which experiments are conducted. © 2009 Peirce.},
author_keywords={EEG;  fMRI;  MEG;  Neuroscience;  Psychophysics;  Python;  Software;  Vision},
document_type={Article},
source={Scopus},
}

@BOOK{Gogol-Döring20091,
author={Gogol-Döring, A. and Reinert, K.},
title={Biological sequence analysis using the seqan C++ library},
journal={Biological Sequence Analysis Using the SeqAn C++ Library},
year={2009},
pages={1-311},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030091899&partnerID=40&md5=6d72be64d205f406c142d189d4f1faeb},
abstract={An Easy-to-Use Research Tool for Algorithm Testing and Development Before the SeqAn project, there was clearly a lack of available implementations in sequence analysis, even for standard tasks. Implementations of needed algorithmic components were either unavailable or hard to access in third-party monolithic software products. Addressing these concerns, the developers of SeqAn created a comprehensive, easy-to-use, open source C++ library of efficient algorithms and data structures for the analysis of biological sequences. Written by the founders of this project, Biological Sequence Analysis Using the SeqAn C++ Library covers the SeqAn library, its documentation, and the supporting infrastructure. The first part of the book describes the general library design. It introduces biological sequence analysis problems, discusses the benefit of using software libraries, summarizes the design principles and goals of SeqAn, details the main programming techniques used in SeqAn, and demonstrates the application of these techniques in various examples. Focusing on the components provided by SeqAn, the second part explores basic functionality, sequence data structures, alignments, pattern and motif searching, string indices, and graphs. The last part illustrates applications of SeqAn to genome alignment, consensus sequence in assembly projects, suffix array construction, and more. This handy book describes a user-friendly library of efficient data types and algorithms for sequence analysis in computational biology. SeqAn enables not only the implementation of new algorithms, but also the sound analysis and comparison of existing algorithms. © 2010 by Taylor and Francis Group, LLC.},
document_type={Book},
source={Scopus},
}

@ARTICLE{Sudeikat2009,
author={Sudeikat, J. and Braubach, L. and Pokahr, A. and Renz, W. and Lamersdorf, W.},
title={Systematically engineering self-organizing systems: The SodekoVS approach},
journal={Electronic Communications of the EASST},
year={2009},
volume={17},
page_count={12},
doi={10.14279/tuj.eceasst.17.194.211},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961684586&doi=10.14279%2ftuj.eceasst.17.194.211&partnerID=40&md5=4e0b09afd4223eb6380c4338950b21b1},
abstract={Self-organizing systems promise new software quality attributes that are very hard to obtain using standard software engineering approaches. In accordance with the visions of e.g. autonomic computing and organic computing, self-organizing systems promote self-adaptability as one major property helping to realize software that can manage itself at runtime. In this respect, self-adaptability can be seen as a necessary foundation for realizing e.g. self* properties such as self- configuration or self-protection. However, the systematic development of systems exhibiting such properties challenges current development practices. The SodekoVS project addresses the challenge to purposefully engineer adaptivity by proposing a new approach that considers the system architecture as well as the software development methodology as integral intertwined aspects for system construction. Following the proposed process, self-organizing dynamics, inspired by biological, physical and social systems, can be integrated into applications by composing modules that distribute feedback control structures among system entities. These compositions support hierarchical as well as completely decentralized solutions without a single point of failure. This novel development conception is supported by a reference architecture, a tailored programming model as well as a library of ready to use self-organizing patterns. The key challenges, recent research activities, application scenarios as well as intermediate results are discussed. © 2009, Universitatsbibliothek TU Berlin.},
author_keywords={Decentralized coordination;  Distributed systems;  Self-Organization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Santhi2009344,
author={Santhi, M. and Rajaram, R. and Devi, R.R.},
title={Design and implementation of lcl-resonant push-pull DC-DC converter using pwm controller for aerospace application},
journal={International Journal of Modelling and Simulation},
year={2009},
volume={29},
number={4},
pages={344-354},
doi={10.2316/Journal.205.2009.4.205-4842},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950832233&doi=10.2316%2fJournal.205.2009.4.205-4842&partnerID=40&md5=ef620a92f20c43d75e4c101d617e7030},
abstract={The push-pull converter topology is suitable for unregulated lowvoltage to high-voltage power conversion, as in battery-powered systems where input currents can exceed input voltages by an order of magnitude. The resonant circuit operates at twice the switching frequency allowing for small resonant components. The MOSFET primary switches operate under zero voltage switching (ZVS) conditions, due to commutation of the transformer magnetizing current and the snubbing effect of the inherent drain-source capacitance. Output rectifier turn-off is effectively snubbed by the resonant capacitor. An analytical study on ZVS LCL-resonant push-pull DC-DC converter is presented in detail. Closed loop controlled ZVS LCL-resonant push-pull DC-DC converter using PWM controller is implemented. Circuit simulation and experimental results of both open loop and closed loop operations are presented. The software package PSPICE A/D is used for simulation.},
author_keywords={Closed loop operation;  Dc-dc converter;  Push-pull topology;  Pwm controller;  Zero-voltage switching},
document_type={Article},
source={Scopus},
}

@ARTICLE{DíazMartín2009327,
author={Díaz Martín, J.C. and Rico Gallego, J.A. and Álvarez Llorente, J.M. and Perogil Duque, J.F.},
title={An MPI-1 compliant thread-based implementation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2009},
volume={5759 LNCS},
pages={327-328},
doi={10.1007/978-3-642-03770-2_42},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350460776&doi=10.1007%2f978-3-642-03770-2_42&partnerID=40&md5=44564c2e415dc6733a64bf9ce2475c56},
abstract={This work presents AzequiaMPI, the first full compliant implementation of the MPI-1 standard where the MPI node is a thread. Performance comparisons with MPICH2-Nemesis show that thread-based implementations exploit adequately the multicore architectures under oversubscription, what could make MPI competitive with OpenMP-like solutions. © 2009 Springer Berlin Heidelberg.},
author_keywords={Multicore architectures;  Thread-based MPI implementation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Limsoonthrakul2009633,
author={Limsoonthrakul, S. and Dailey, M.N. and Srisupundit, M. and Tongphu, S. and Parnichkun, M.},
title={A modular system architecture for autonomous robots based on blackboard and publish-subscribe mechanisms},
journal={2008 IEEE International Conference on Robotics and Biomimetics, ROBIO 2008},
year={2009},
pages={633-638},
doi={10.1109/ROBIO.2009.4913075},
art_number={4913075},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349175813&doi=10.1109%2fROBIO.2009.4913075&partnerID=40&md5=1e25387c97cc76a09f96a4dd305a4f87},
abstract={We present a system software architecture for mobile robots such as autonomous vehicles. The system achieves the goals of flexibility, maintainability, testability, and modifiability through a decoupled software architecture based on an asynchronous publish-subscribe mechanism and a blackboard object handling synchronized access to shared data. We report on two implementations using the proposed generic architecture and the POSIX real time API. The first implementation is for an autonomous vehicle using waypoint-based navigation, and the second implementation uses the same high-level modules but replaces the low-level hardware interfaces with a virtual reality simulation. Our experiments and an evaluation indicate that the architecture is suitable for a wide variety of control algorithms and supports the construction of testable, maintainable, and modifiable autonomous robot vehicles at low cost in terms of real-time performance. © 2008 IEEE.},
author_keywords={Autonomous vehicles;  Decoupling;  Mobile robot control;  Real time systems;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Seoy200955,
author={Seoy, S. and Lee, J. and Suraz, Z.},
title={Design and implementation of software-managed caches for multicores with local memory},
journal={Proceedings - International Symposium on High-Performance Computer Architecture},
year={2009},
pages={55-66},
doi={10.1109/HPCA.2009.4798237},
art_number={4798237},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-64949198379&doi=10.1109%2fHPCA.2009.4798237&partnerID=40&md5=00464e5899a9f723e6f81c7909daf05e},
abstract={Heterogeneous multicores, such as Cell BE processors and GPGPUs, typically do not have caches for their accelerator cores because coherence traffic, cache misses, and latencies from different types of memory accesses add overhead and adversely affect instruction scheduling. Instead, the accelerator cores have internal local memory to place their code and data. Programmers of such heterogeneous multicore architectures must explicitly manage data transfers between the local memory of a core and the globally shared main memory. This is a tedious and errorprone programming task. A software-managed cache (SMC), implemented in local memory, can be programmed to automatically handle data transfers at runtime, thus simplifying the task of the programmer. In this paper, we propose a new software-managed cache design, called extended set-index cache (ESC). It has the benefits of both set-associative and fully associative caches. Its tag search speed is comparable to the set-associative cache and its miss rate is comparable to the fully associative cache. We examine various line replacement policies for SMCs, and discuss their trade-offs. In addition, we propose adaptive execution strategies that select the optimal cache line size and replacement policy for each program region at runtime. To evaluate the effectiveness of our approach, we implement the ESC and other SMC designs on the Cell BE architecture, and measure their performance with 8 OpenMP applications. The evaluation results show that the ESC outperforms other SMC designs. The results also show that our adaptive execution strategies work well with the ESC. In fact, our approach is applicable to all cores with access to both local and global memory in a multicore architecture. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang20091,
author={Wang, H. and Akinci, B. and Garrett Jr., J.H. and Nyberg, E. and Reed, K.A.},
title={Semi-automated model matching using version difference},
journal={Advanced Engineering Informatics},
year={2009},
volume={23},
number={1},
pages={1-11},
doi={10.1016/j.aei.2008.05.005},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57549087017&doi=10.1016%2fj.aei.2008.05.005&partnerID=40&md5=824d67d4b35be1681286465d34fced98},
abstract={Interoperability of software is a critical requirement in the architecture, engineering, and construction (AEC) industry, where a number of data exchange standards have been created to enable data exchange among different software packages. To be able to comply with existing data exchange standards, the software developers need to match their internal data schemas to the schema defined in a standard and vice versa. The process of matching two large scale data models is time consuming and cumbersome when performed manually, and becomes even more challenging when a source and/or a target model is being updated frequently to meet the ever expanding real world requirements. While several prior studies discussed the need for approaches toward automated or semi-automated schema matching, an approach that builds on existing matches between two models has rarely been studied. In this paper, we present a semi-automated approach for model matching. This approach leverages a given set of existing matching between two models and upgrades those matching when a new version of a target model is released. The paper describes in detail a list of upgrade patterns generated and validated through a prototype by matching a domain-specific data model to several recent releases of the industry foundation classes. © 2008.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nix2008,
author={Nix, D.A. and Courdy, S.J. and Boucher, K.M.},
title={Empirical methods for controlling false positives and estimating confidence in ChIP-Seq peaks},
journal={BMC Bioinformatics},
year={2008},
volume={9},
doi={10.1186/1471-2105-9-523},
art_number={523},
note={cited By 164},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-60849117520&doi=10.1186%2f1471-2105-9-523&partnerID=40&md5=255a2480ce80dedf6116f7ef367b6748},
abstract={Background: High throughput signature sequencing holds many promises, one of which is the ready identification of in vivo transcription factor binding sites, histone modifications, changes in chromatin structure and patterns of DNA methylation across entire genomes. In these experiments, chromatin immunoprecipitation is used to enrich for particular DNA sequences of interest and signature sequencing is used to map the regions to the genome (ChIP-Seq). Elucidation of these sites of DNA-protein binding/modification are proving instrumental in reconstructing networks of gene regulation and chromatin remodelling that direct development, response to cellular perturbation, and neoplastic transformation. Results: Here we present a package of algorithms and software that makes use of control input data to reduce false positives and estimate confidence in ChIP-Seq peaks. Several different methods were compared using two simulated spike-in datasets. Use of control input data and a normalized difference score were found to more than double the recovery of ChIP-Seq peaks at a 5% false discovery rate (FDR). Moreover, both a binomial p-value/q-value and an empirical FDR were found to predict the true FDR within 2-3 fold and are more reliable estimators of confidence than a global Poisson p-value. These methods were then used to reanalyze Johnson et al.'s neuron-restrictive silencer factor (NRSF) ChIP-Seq data without relying on extensive qPCR validated NRSF sites and the presence of NRSF binding motifs for setting thresholds. Conclusion: The methods developed and tested here show considerable promise for reducing false positives and estimating confidence in ChIP-Seq data without any prior knowledge of the chIP target. They are part of a larger open source package freely available from http://useq.sourceforge.net/. © 2008 Nix et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kaewlai2008143,
author={Kaewlai, P. and Jinuntuya, P. and Kanongchaiyos, P.},
title={Interactive feasibility-based CAAD system for infrastructure and open space planning in housing project design},
journal={CAADRIA 2008 - The Association for Computer-Aided Architectural Design Research in Asia: Beyond Computer-Aided Design},
year={2008},
pages={143-148},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875239656&partnerID=40&md5=7430357ad04fdde1e3f2535f3da7353c},
abstract={The decision support system developed in this research is aimed to the conceptual scheme of project focusing on infrastructure planning and open space design in the architectural context for housing project. Alternative design sets are provided within the limitations, and possibilities to be further evaluated appropriately. This system helps architects and developers to analyze relationships of physical environment, architectural requirements and the overall of project-related factors with real-time cost estimation. Factors for cost estimation derived from the beginning to the end of project will be manipulated simultaneously. Architects and developers can use this design simulation to address the physical data with real-time cost estimation, provide alternative results, and design evaluation for overall project's feasibility. The software of our research is not just a tool for design & planning automation in feasibility analysis. It will be an interactive decision support system for both developers and planners aspects. The system was developed by SketchUp Ruby Application Programming Interface. The results will be presented into two ways. Firstly, 2D and 3D modeling will be used for interactive visualization in design and planning of the beginning process. Subsequently, numbers and additional factors in details will be used to show relationship between architectural environment and feasibilitybased information to help architects and developers collaboratively analyze the land use planning and open space design for housing project. In evaluation process, the developed software is tested with the project preceding and the future project of Bangkok area under constraints and regulations of Building Control Act of Thailand. In conclusion, this system will make effectiveness in design process and management of the construction knowledge. The decision support systems should be designed to makes explicit use of both planning analysis aspect and knowledge-based decision making.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wallick2008370,
author={Wallick, K.},
title={Digital and manual joints},
journal={ACADIA 08: Silicon + Skin: Biological Processes and Computation: Proceedings of the 28th Annual Conference of the Association for Computer Aided Design in Architecture},
year={2008},
pages={370-375},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858816154&partnerID=40&md5=02e276b2d42ee70770ce14842aba4afa},
abstract={Thispaper considersthe problem of detailing jointsbetween manual anddigital construction by tracking the provocationsof KieranTimberlake'sSmartWrapresearch andthe evolution of thatknowledge into practical architectural instrumentsthatcan be deployedinto more traditional construction projects. Over the past several years, KieranTimberlake Associates in Philadelphia has undertaken a path of research focusing on problems of contemporary construction systems and practices. One product of this research was a speculative wall system assembled for a museum exhibit. SmartWrap was to be a digitally prefabricated wall system with embedded technology. While they have yet to wrap a building with SmartWrap, KieranTimberlake have utilized a number of the construction principles and digital tools tested in the SmartWrap exhibit. One of the most important principles, prefabrication, was explored in a fast-track construction project at the Sidwell Friends School. The compressed schedule drove the design of an enclosure system which incorporated performative elements in similar categories to SmartWrap: insulation, an electrical system, view, daylighting, and a rainscreen. Besides being a prefabricated façade system, the rainscreen detailing became a formal system for organizing many other scales of the project including: site systems, thermal systems, daylighting systems, enclosure, and ornament. At a second project, a similar wood rainscreen strategy was used. However, at the Loblolly House the question of prefabrication and digital modeling was tested far more extensively: thermal systems were embedded into prefabricated floor cartridges, entire program elements - a library, kitchen, and bathroom were proposed as prefabricated systems of self-contained volume and infrastructure which were then inserted into the on-site framework. In all three projects the joint between manual-imprecise construction and digital-precise prefabrication became the area of richest invention (Figure 1). SmartWrap may not have yielded flexible, plastic architecture; but its conceptual and practical questions have yielded tangible implications for the design/construction processes and the built product in KieranTimberlake's practice.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2008,
title={Proceedings of the 2008 International Conference on Information and Knowledge Engineering, IKE 2008},
journal={Proceedings of the 2008 International Conference on Information and Knowledge Engineering, IKE 2008},
year={2008},
page_count={631},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857114692&partnerID=40&md5=03681b40368d2561da593025cc345613},
abstract={The proceedings contain 99 papers. The topics discussed include: an algebraic structure for decompositions of relational databases; enabling federated access to information; evaluation of test collection construction methods: a case study; expert finding by means of plausible inferences; improving document classification using anchor text; the effect of normalization when recall really matters; high-availability cluster management tool for monitoring and self-managing of cluster DBMS; finding association rules in large relations for possible normalization; a prototype automated knowledge acquisition system; a study of knowledge organisation system in digital libraries: an IRS perspective; developing a GIS-based system for analyzing medical transportation activities; and an integrated quality function development and Fuzzy regression-based optimization framework for selecting an E-business software system.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Mingjing2008444,
author={Mingjing, G. and Yang, Z.},
title={An architecture for digital library information service in an ambient intelligence environment},
journal={Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008},
year={2008},
volume={6},
pages={444-447},
doi={10.1109/CSSE.2008.1637},
art_number={4723293},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951475982&doi=10.1109%2fCSSE.2008.1637&partnerID=40&md5=431b217162878eb92d7be6db32661599},
abstract={Digital library is the national knowledge infrastructure which provides various kinds of information services for promoting knowledge innovation. Ambient Intelligence is a kind of digital intelligence environment, its emergence and developmemt have changed model, dissemination channel and using mechanism of information and brought great opportunities to the development of digital library. In order to satisfy the users' personalized information requirements better in an Ami environment, an architecture for digital library information service is proposed in this paper, which allows individual users to retrieve, access and share information resources more efficiently. The implementation mechanism of the architecture introduced specific service process and explored the potential of the architecture to support future service applications of digital library. © 2008 IEEE.},
author_keywords={Ambient intelligence;  Context-aware;  Digital library information service;  System architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ardimento2008357,
author={Ardimento, P. and Baldassarre, M.T. and Cimitile, M. and Visaggio, G.},
title={Empirical Experimentation for Validating the Usability of Knowledge Packages in Transferring Innovations},
journal={Communications in Computer and Information Science},
year={2008},
volume={22 CCIS},
pages={357-370},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349259228&partnerID=40&md5=d7e78ff4c0697cbfe94b785483d732ef},
abstract={Transfer of research results following to technological innovation and to the experience collected in applying the innovation within an enterprise is a key success factor. A critical factor in transferring innovations to software processes concerns the knowledge transfer activity which requires the knowledge be explicit and understandable by stakeholders. As so many researchers have been studying alternative ways to conventional approaches i.e. books, papers, reports and other written communication means that favour knowledge acquisition on behalf of users. In this context, we propose the Knowledge Package (KP) structure as alternative. We have carried out an experiment which compared the usability of the proposed approach with conventional ones, along with the efficiency and the comprehensibility of the knowledge enclosed in a KP rather than in a set of Conventional Sources. The experiment has pointed out that knowledge packages are more efficient than conventional ones, for knowledge transfer. The experiment has been described according to guidelines that allow for replications. In this way other researchers can confirm or refute the results and enforce their validity. © Springer-Verlag Berlin Heidelberg 2008.},
author_keywords={Empirical investigation;  Knowledge packaging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Haveraaen200821,
author={Haveraaen, M.},
title={Institutions, property-aware programming and testing},
journal={LCSD 2007 - Proceedings of the 2007 ACM SIGPLAN Symposium on Library-Centric Software Design},
year={2008},
pages={21-30},
doi={10.1145/1512762.1512765},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952359737&doi=10.1145%2f1512762.1512765&partnerID=40&md5=72143bf8ce919c2e88ff9b089dacaa54},
abstract={The institution notion is a general model theoretic framework, explaining how specifications (algebraic axioms) relate to models (mathematical models or even software constructions) in a formalism-independent manner. There is a large set of institution-independent structuring mechanisms for specifications. Property aware programming, as e.g. supported by concepts in C++0X, provides algebraic axioms as part of the code to ensure correctness of generic software composition. Testing is very important for the validation of software, but tests are all too often developed on an ad hoc basis. Here we present a library testing framework, with a basis in structured specifications. The approach will be demonstrated on Sophus, a medium-sized software library for coordinate-free numerics. Sophus was developed using (informal) algebraic specifications in order to improve reusability and reduce development costs. © 2007 ACM.},
author_keywords={Algebraic specifications;  Coordinate-free numerics;  Generic programming;  Institutions;  Sophus software library;  Systematic software library testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Niebler200842,
author={Niebler, E.},
title={Proto: A compiler construction toolkit for DSELs},
journal={LCSD 2007 - Proceedings of the 2007 ACM SIGPLAN Symposium on Library-Centric Software Design},
year={2008},
pages={42-51},
doi={10.1145/1512762.1512767},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952340530&doi=10.1145%2f1512762.1512767&partnerID=40&md5=7dcd50b40cca5d4b64fd817d28a7162d},
abstract={A Domain-Specific Embedded Language (DSEL) is a miniature language-within-a-language for solving problems in a particular domain. One technique for creating efficient and expressive DSELs in C++ is to use expression templates, but this technique is not for the faint of heart. Such libraries are difficult to write and maintain due to the esoteric nature of template meta-programming, and difficult to use because of the often impenetrable compiler error messages they generate. Existing tools help somewhat, but do not provide the support that language designers have come to expect: something like BNF for defining the language's grammar and associated semantic actions. This paper describes Proto, a C++ library that implements a compiler construction toolkit for embedded languages. The benefits of grammar-based DSELs are shown by contrasting them to other existing approaches to DSEL design. The nature of embedded languages with constrained grammars and their implications for a embedded compiler construction toolkit is briefly explored. Some examples are shown where library interfaces can be made more expressive through the use of grammar-based DSELs. © 2007 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yi2008109,
author={Yi, Q. and Whaley, R.C.},
title={Automated transformation for performance-critical kernels},
journal={LCSD 2007 - Proceedings of the 2007 ACM SIGPLAN Symposium on Library-Centric Software Design},
year={2008},
pages={109-119},
doi={10.1145/1512762.1512773},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952326947&doi=10.1145%2f1512762.1512773&partnerID=40&md5=3bf10bcc1a76dced3f6556d097dd791f},
abstract={The performance of many scientific applications depends on a small number of key computational kernels which require a level of efficiency rarely satisfied by existing native compilers. We present a new approach to high performance kernel optimization, where a general-purpose transformation engine automates the production of highly efficient library routines. The library routines are then empirically tested until an implementation with a satisfactory performance level is found. Our framework requires an annotated kernel specification and can automatically produce optimized implementations based on tuning parameters controlled by a search driver. The transformation engine includes an extensive suite of optimizations which can be easily expanded using a custom transformation language. We have applied our framework to generate code for key linear algebra kernels and have achieved similar performance as that achieved by ATLAS's highly tuned kernels. In several cases, our kernels were faster than ATLAS's native kernels; we have made these kernels available to ATLAS, which results in speedups for the ATLAS library, as we show. © 2007 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dobolyi200847,
author={Dobolyi, K. and Weimer, W.},
title={Changing java's semantics for handling null pointer exceptions},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2008},
pages={47-56},
doi={10.1109/ISSRE.2008.59},
art_number={4700309},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249104242&doi=10.1109%2fISSRE.2008.59&partnerID=40&md5=d0dcc21ac23c44369f21f06c22828623},
abstract={We envision a world where no exceptions are raised; instead, language semantics are changed so that operations are total functions. Either an operation executes normally or tailored recovery code is applied where exceptions would have been raised. As an initial step and evaluation of this idea, we propose to transform programs so that null pointer dereferences are handled automatically without a large runtime overhead. We increase robustness by replacing code that raises null pointer exceptions with error-handling code, allowing the program to continue execution. Our technique first finds potential null pointer dereferences and then automatically transforms programs to insert null checks and error-handling code. These transformations are guided by composable, context-sensitive recovery policies. Errorhandling code may, for example, create default objects of the appropriate types, or restore data structure invariants. If no null pointers would be dereferenced, the transformed program behaves just as the original. We applied our transformation in experiments involving multiple benchmarks, the Java Standard Library, and externally reported null pointer exceptions. Our technique was able to handle the reported exceptions and allow the programs to continue to do useful work, with an average execution time overhead of less than 1% and an average bytecode space overhead of 22%. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Young2008,
author={Young, P.J. and Nielsen, J.J. and Roberts, W.H. and Wilson, G.M.},
title={Achieving design reuse: A case study},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2008},
volume={7019},
doi={10.1117/12.787698},
art_number={70192M},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949161105&doi=10.1117%2f12.787698&partnerID=40&md5=1266aa67a0e7f9f4eeeba45ba17efb24},
abstract={The RSAA CICADA data acquisition and control software package uses an object-oriented approach to model astronomical instrumentation and a layered architecture for implementation. Emphasis has been placed on building reusable C++ class libraries and on the use of attribute/value tables for dynamic configuration. This paper details how the approach has been successfully used in the construction of the instrument control software for the Gemini NIFS and GSAOI instruments. The software is again being used for the new RSAA SkyMapper and WiFeS instruments.},
author_keywords={Architecture;  Object-oriented;  Reuse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rodriguez-Ramos2008,
author={Rodriguez-Ramos, J.M. and Castello, E.M. and Conde, C.D. and Valido, M.R. and Marichal-Hernandez, J.G.},
title={2D-FFT implementation on FPGA for wavefront phase recovery from the CAFADIS camera},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2008},
volume={7015},
doi={10.1117/12.789312},
art_number={701539},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-66949129347&doi=10.1117%2f12.789312&partnerID=40&md5=f4fba1cd9796183bc5fb3254d18a227e},
abstract={The CAFADIS camera is a new sensor patented by Universidad de La Laguna (Canary Islands, Spain): international patent PCT/ES2007/000046 (WIPO publication number WO/2007/082975). It can measure the wavefront phase and the distance to the light source at the same time in a real time process. It uses specialized hardware: Graphical Processing Units (GPUs) and Field Programmable Gates Arrays (FPGAs). These two kinds of electronic hardware present an architecture capable of handling the sensor output stream in a massively parallel approach. Of course, FPGAs are faster than GPUs, this is why it is worth it using FPGAs integer arithmetic instead of GPUs floating point arithmetic. GPUs must not be forgotten, as we have shown in previous papers, they are efficient enough to resolve several problems for AO in Extremely Large Telescopes (ELTs) in terms of time processing requirements; in addition, the GPUs show a widening gap in computing speed relative to CPUs. They are much more powerful in order to implement AO simulation than common software packages running on top of CPUs. Our paper shows an FPGA implementation of the wavefront phase recovery algorithm using the CAFADIS camera. This is done in two steps: the estimation of the telescope pupil gradients from the telescope focus image, and then the very novelty 2D-FFT over the FPGA. Time processing results are compared to our GPU implementation. In fact, what we are doing is a comparison between the two different arithmetic mentioned above, then we are helping to answer about the viability of the FPGAs for AO in the ELTs.},
author_keywords={Adaptive optics;  Avefront phase recovery;  FPGA;  GPU;  Real-time processing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kollias2008735,
author={Kollias, G. and Georgiou, K. and Gallopoulos, E.},
title={Jylab meets Eclipse: Integrating PSEs with multicomponent platforms},
journal={Proceedings - 4th IEEE International Conference on eScience, eScience 2008},
year={2008},
pages={735-742},
doi={10.1109/eScience.2008.11},
art_number={4736892},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749154643&doi=10.1109%2feScience.2008.11&partnerID=40&md5=53a6581bcc27dfba18866d1bd034cafa},
abstract={Jylab is a recent PSE architecture emphasizing portable computing over distributed platforms. It captures the idea of reusing some of the best open source software projects' functionality within the context of a single, net-aware, interactive environment The original implementation of this idea resulted in a system built around a portable interpreter supported by a carefully selected suite of libraries spanning a comprehensive set of applications including scripting, numerical linear algebra, distributed/Grid computing and Internet algorithmics. Because Jylab is a multicomponent PSE system, it is quite natural to base its implementation on a robust platform automating the management of complex stacks of software components, i.e. self-describing objects. The Eclipse Platform meets this basic prerequisite, additionally providing many other interesting integration facilities, an extensive set of ready-to-use plug-ins1 and is also embraced by a vibrant community of users, developers and leading software companies. In this paper we describe the design and basic implementation ofaflexible environment resulting from the integration of Jylab into Eclipse. To this effect, we survey relevant aspects of the rich Eclipse ecosystem as well as the Jyl ab approach to PSE construction. To illustrate our environment we present case studies from Grid computing, neural network training and native libraries integration. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Desai2008309,
author={Desai, A. and Singh, J. and Devlin, J.},
title={Configurable toolchain for embedded java environments},
journal={Proceedings of the 2008 International Conference on Embedded Systems and Applications, ESA 2008},
year={2008},
pages={309-314},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-62649108366&partnerID=40&md5=aca1a07f14f961c31bdeb23d0fa954db},
abstract={This paper presents the design and implementation of a complete toolchain for design, implementation and optimization of runtime software for embedded Java environments. The toolchain consists of an array of highly configurable tools enabling: compilation of native code and Java API, profiling for program metrics, bytecode transformation, coverage verification and finally linking of the runtime image for use in embedded Java Virtual Machines. This paper also presents the data distribution analysis of Java class data based on CLDC 1.1 API and industry standard Embedded Java Benchmarks (Grinderbench) which is used for designing the runtime data structures for realizing a compact and efficient runtime memory image. The results show that the linking model and the data structures used offer about 39% reduction in runtime memory image size, while offering simpler and highly efficient handling of complex instructions. The preloading implementation is able to load internalized String objects and helper data such as class name and resource name hash tables for speeding up start-up and providing faster implementations of native functions normally involving indeterministic lookup process.},
author_keywords={Bytecode compaction;  Embedded systems;  Java class file;  Runtime data structures;  Runtime image},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barbier200868,
author={Barbier, F. and Cariou, E.},
title={Component design based on model executability},
journal={EUROMICRO 2008 - Proceedings of the 34th EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2008},
year={2008},
pages={68-75},
doi={10.1109/SEAA.2008.16},
art_number={4725707},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-60349118178&doi=10.1109%2fSEAA.2008.16&partnerID=40&md5=3c224f3045d7d535e9ba811798724753},
abstract={Model-Driven Development (MDD) corresponds to the building of models and their transformation into intermediate models and code. Modeling components and compositions is a natural consequence of MDD. We show in this paper the advantages of using an executable modeling language associated with a Java library which pre-implements the execution semantics of this language. The proposed executable language is based on UML State Machine Diagrams. The semantic variation points linked to these diagrams lead us to manage equivalent variations in the Java implementation of components. The paper offers a comprehensive component design method based on a tailor-made UML profile whose role is the control of the semantic variation points in models. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pattabiraman2008219,
author={Pattabiraman, K. and Grover, V. and Zorn, B.G.},
title={Samurai: Protecting critical data in unsafe languages},
journal={EuroSys'08 - Proceedings of the EuroSys 2008 Conference},
year={2008},
pages={219-232},
doi={10.1145/1352592.1352616},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-59249096511&doi=10.1145%2f1352592.1352616&partnerID=40&md5=9cbc5825a597383ca9a5927da5bd3b57},
abstract={Programs written in type-unsafe languages such as C and C++ incur costly memory errors that result in corrupted data structures, program crashes, and incorrect results. We present a data-centric solution to memory corruption called critical memory, a memory model that allows programmers to identify and protect data that is critical for correct program execution. Critical memory defines operations to consistently read and update critical data, and ensures that other non-critical updates in the program will not corrupt it. We also present Samurai, a runtime system that implements critical memory in software. Samurai uses replication and forward error correction to provide probabilistic guarantees of critical memory semantics. Because Samurai does not modify memory operations on non-critical data, the majority of memory operations in programs run at full speed, and Samurai is compatible with third party libraries. Using both applications, including a Web server, and libraries (an STL list class and a memory allocator), we evaluate the performance overhead and fault tolerance that Samurai provides. We find that Samurai is a useful and practical approach for the majority of the applications and libraries considered. Copyright 2008 ACM.},
author_keywords={Critical memory;  Error recovery;  Memory safety},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pironti2008241,
author={Pironti, A. and Sisto, R.},
title={Formally sound refinement of Spi Calculus protocol specifications into java code},
journal={Proceedings of IEEE International Symposium on High Assurance Systems Engineering},
year={2008},
pages={241-250},
doi={10.1109/HASE.2008.27},
art_number={4708882},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58449090669&doi=10.1109%2fHASE.2008.27&partnerID=40&md5=9ba917855cec0d64ae17ea1dd1321ada},
abstract={Spi Calculus is an untyped high level modeling language for security protocols, used for formal protocols specification and verification. In this paper, a type system for the Spi Calculus and a translation function are formally defined, in order to formalize the refinement of a Spi Calculus specification into a Java implementation. Since the generated Java implementation uses a custom Java library, formal conditions on the custom Java library are also stated, so that, if the library implementation code satisfies such conditions, then the generated Java implementation correctly simulates the Spi Calculus specification. © 2008 IEEE.},
author_keywords={Correctness preserving code generation;  Formal methods;  Model-based software development;  Security protocols},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Al-Gahmi2008507,
author={Al-Gahmi, A. and Cook, J.},
title={Towards a service-based middleware layer for runtime environments},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2008},
pages={507-511},
doi={10.1145/1363686.1363810},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749186479&doi=10.1145%2f1363686.1363810&partnerID=40&md5=79005b59c5ba34309ef1bd5976c67e16},
abstract={Natively compiled, binary-code application programs are typically thought of as executing on the "raw" operating system. However, they do in fact utilize a bare-bones middleware layer-the dynamic linker. This paper presents a service-based view of an expanded run-time environment in which the current dynamic linker is only a core service, and other middleware-type services are available to the application and its components (shared libraries). This paper then describes a prototype implementation of such an environment, called SBRT, or Service-Based Run-Time. SBRT also contains a unified event-based interface that allows for customized middleware services by means of an extension mechanism. Copyright 2008 ACM.},
author_keywords={Dynamic linker;  Middleware;  Runtime environments;  Service-based architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Koch20083159,
author={Koch, M. and Schaefers, L. and Kleinjohann, B.},
title={An efficient dataflow-oriented fuzzy library},
journal={Proceedings of the SICE Annual Conference},
year={2008},
pages={3159-3162},
doi={10.1109/SICE.2008.4655209},
art_number={4655209},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749152137&doi=10.1109%2fSICE.2008.4655209&partnerID=40&md5=dff0344a8127497f9d12b3dad7e1caef},
abstract={Fuzzy logic is often used in intelligent systems in general and especially in learning domains. On this occasion, always returning problem formulations are to be solved like an arithmetic- and memory-efficient implementation by fuzzy operations. We present a software library which aims to an efficient, extendable and comfortable implementation for embedded mechatronlc systems. The data-flow oriented concept will be explained. It enables a modular and extensible system design where the communication path and also the behavior parameters are exchangeable. We summarize the implementation issues and give an overview of the special type of operations on a memory and processing restricted controller. The evaluation is done with a lab robot system having an omnidirectional drive. We implemented the controller in a commercial engineering design environment and compared the behavior of our library with a common library which comes with the commercial solution. The results show that our implementation not only works but also shows good performance on the robot drive. The conclusion shows the portability of the library to an application example of movement analysis by fiber optical sensors. © 2008 SICE.},
author_keywords={Control systems;  Fuzzy systems;  Intelligent systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Brukman20082315,
author={Brukman, O. and Dolev, S. and Kolodner, E.K.},
title={A self-stabilizing autonomic recoverer for eventual Byzantine software},
journal={Journal of Systems and Software},
year={2008},
volume={81},
number={12},
pages={2315-2327},
doi={10.1016/j.jss.2008.04.028},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-53949112684&doi=10.1016%2fj.jss.2008.04.028&partnerID=40&md5=cb56073a3bac0c3ad77f3f44056c558e},
abstract={We suggest modeling software package flaws (bugs) by assuming eventual Byzantine behavior of the package. We assume that if a program is started in a predefined initial state, it will exhibit legal behavior for a period of time but will eventually become Byzantine. We assume that this behavior pattern can be attributed to the fact that the manufacturer had performed sufficient package tests for limited time scenarios. Restarts are useful for recovering such systems. We suggest a general, yet practical, framework and paradigm for the monitoring and restarting of systems where the framework and paradigm are based on a theoretical foundation. An autonomic recoverer that monitors and initiates system recovery is proposed. It is designed to handle a task, given specific task requirements in the form of predicates and actions. A directed acyclic graph subsystem hierarchical structure is used by a consistency monitoring procedure for achieving a gracious recovery. The existence and correct functionality of the autonomic recovery is guaranteed by the use of a self-stabilizing kernel resident (anchor) process. The autonomic recoverer uses a new scheme for liveness assurance via on-line monitoring that complements known schemes for on-line safety assurance. © 2008 Elsevier Inc. All rights reserved.},
author_keywords={Automatic recovery;  Liveness;  Monitor;  Safety;  Self-stabilization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Medina-Domínguez2008283,
author={Medina-Domínguez, F. and Saldaña-Ramos, J. and Mora-Soto, A. and Sanz-Esteban, A. and Sanchez-Segura, M.-I.},
title={A collaborative framework to support software process improvement based on the reuse of process assets},
journal={ICSOFT 2008 - Proceedings of the 3rd International Conference on Software and Data Technologies},
year={2008},
volume={PL},
number={DPS/KE/-},
pages={283-289},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849098617&partnerID=40&md5=3e22d151e70494c47333257d0b6a0c53},
abstract={In order to allow software organizations to reuse their know-how, the authors have defined product patterns artefact. This know-how can be used in combination with software engineering best practices to improve the quality and productivity of their software projects, as well as reduce projects cost. This paper describes the structure of the Process Assets Library (PAL), and the framework developed to encapsulate the know-how in organizations. The PIBOK-PB (Process improvement based on knowledge - pattern based) tool uses the proposed PAL to access the knowledge encapsulated in the product patterns, and to execute software projects more efficiently. This paper also describes PIBOK-PB's features and compares similar tools in the market.},
author_keywords={Knowledge management;  Patterns;  Process Assets;  Product pattern;  Reuse;  Software engineering;  Software Process improvement},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lobato20081365,
author={Lobato, C. and Garcia, A. and Romanovsky, A. and Lucena, C.},
title={An aspect-oriented software architecture for code mobility},
journal={Software - Practice and Experience},
year={2008},
volume={38},
number={13},
pages={1365-1392},
doi={10.1002/spe.873},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-55649105703&doi=10.1002%2fspe.873&partnerID=40&md5=435bf9c6b9eb3e80e2a88f082784e5d5},
abstract={Mobile agents have come forward as a technique for tackling the complexity of open distributed applications. However, the pervasive nature of code mobility implies that it cannot be modularized using only object-oriented (OO) concepts. In fact, developers frequently evidence the presence of mobility scattering in their system's modules. Despite these problems, they usually rely on OO application programming interfaces (APIs) offered by the mobility platforms. Such classical API-oriented designs suffer a number of architectural restrictions, and there is a pressing need for empowering developers with an architectural framework supporting a flexible incorporation of code mobility in the agent applications. This work presents an aspect-oriented software architecture, called ArchM, ensuring that code mobility has an enhanced modularization and variability in agent systems, and is straightforwardly introduced in otherwise stationary agents. It addresses OO APIs' restrictions and is independent of specific platforms and applications. An ArchM implementation also overcomes fine-grained problems related to mobility tangling and scattering at the implementation level. The usefulness and usability of ArchM are assessed within the context of two case studies and through its composition with two mobility platforms. Copyright © 2008 John Wiley & Sons, Ltd.},
author_keywords={Aspect-oriented software development;  Mobile agents;  Reuse},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bao200842,
author={Bao, T. and Jones, M.},
title={Model checking abstract components within concrete software environments},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5156 LNCS},
pages={42-59},
doi={10.1007/978-3-540-85114-1_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249153836&doi=10.1007%2f978-3-540-85114-1_6&partnerID=40&md5=2a84f66f571cfd234b01fe3284ae2570},
abstract={In order to model check a software component which is not a standalone program, we need a model of the software which completes the program. This is typically done by abstracting the surrounding software and the environment in which the entire system will be executed. However, abstracting the surrounding software artifact is difficult when the surrounding software is a large, complex artifact. In this paper, we take a new approach to the problem by abstracting the software component under test and leaving the surrounding software concrete. We compare three abstraction schemes, bitstate hashing and two schemes based on predicate abstraction, which can be used to abstract the components. We show how to generate the mixed abstract-concrete model automatically from a C program and verify the model using the SPIN model checker. We give verification results for three C programs each consisting of hundreds or thousands of lines of code, pointers, data structures and calls to library functions. Compared to the predicate abstraction schemes, bitstate hashing was uniformly more efficient in both error discovery and exhaustive state enumeration. The component abstraction results in faster error discovery than normal code execution when pruning during state enumeration avoids repeated execution of instructions on the same data. © 2008 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Caballé200894,
author={Caballé, S.},
title={Combining generic programming and service-oriented architectures for the effective and timely development of complex e-learning systems},
journal={Proceedings - CISIS 2008: 2nd International Conference on Complex, Intelligent and Software Intensive Systems},
year={2008},
pages={94-100},
doi={10.1109/CISIS.2008.99},
art_number={4606668},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-53149089433&doi=10.1109%2fCISIS.2008.99&partnerID=40&md5=8067e42b511bd3c777622c44bb3d6f91},
abstract={Over the last years, e-Learning needs have been evolving accordingly with more and more demanding pedagogical and technological requirements. On-line learning environments no longer depend on homogeneous groups, static content and resources, and single pedagogies, but high customization and flexibility are a must in this context. As a result, current educational organizations' needs involve extending and moving to highly customized learning and teaching forms in timely fashion, each incorporating its own pedagogical approach, each targeting a specific learning goal, and each incorporating its specific resources. Moreover, organizations' demands include a cost-effective integration of legacy and separated learning systems, from different institutions, departments and courses, which are implemented in different languages, supported by heterogeneous platforms and distributed everywhere, to name some of them. Therefore, e-Learning applications need to be developed in a way that overcome these demanding requirements as well as provide educational organizations with fast, flexible and effective solutions for the enhancement and improvement of the learning performance and outcomes. To this end, in this paper, an innovative engineering software technique is introduced that combines the Generic Programming paradigm and Service-Oriented Architectures in the form of Web-services for the effective and timely construction of flexible, scalable, interoperable and robust applications as key aspects to address the current demanding and changing requirements in software development in general and specifically in the e-Learning domain. This results in a generic, reusable, extensible platform called Collaborative Learning Purpose Library for the systematic development of collaborative learning applications that help meet these demanding requirements. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bafna2008730,
author={Bafna, S. and Humphries, J. and Miranker, D.P.},
title={Schema driven assignment and implementation of life science identifiers (LSIDs)},
journal={Journal of Biomedical Informatics},
year={2008},
volume={41},
number={5},
pages={730-738},
doi={10.1016/j.jbi.2008.05.014},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949117136&doi=10.1016%2fj.jbi.2008.05.014&partnerID=40&md5=948b37f97667aa3638a6fa8193285e4e},
abstract={Life science identifier (LSID) is a global unique identifier standard intended to help rationalize the unique archival requirements of biological data. We describe LSID implementation architecture such that data managed by a relational database management system may be integrated with the LSID protocol as an add-on layer. The approach requires a database administrator (DBA) to specify an export schema detailing the content and structure of the archived data, and a mapping of the existing database to that schema. This specification can be expressed using SQL view syntax. In effect, we define a SQL-like language for implementing LSIDs. We describe the mapping of the view definition to an implementation as a set of databases triggers and a fixed runtime library. Thus a compiler for a domain-specific language could be written that would reduce the implementation of LSIDs to the task of writing SQL view-like definitions. © 2008 Elsevier Inc.},
author_keywords={Biology;  Export schema;  LSID;  Metadata;  RDF;  Resolution;  Systematics;  Trigger;  View},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rector200869,
author={Rector, A. and Horridge, M. and Drummond, N.},
title={Building modular ontologies and specifying ontology joining, binding, localizing and programming interfaces in ontologies implemented in OWL},
journal={AAAI Spring Symposium - Technical Report},
year={2008},
volume={SS-08-07},
pages={69-73},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52349109911&partnerID=40&md5=40e13deece76b9ad63e42a0690fef2f4},
abstract={The notion of an Application Programming Interface (API) proved a breakthrough in software modularization and reuse by allowing developers to separate applications' public interfaces from their detailed internal structure. No comparable notion exists currently for ontologies, although there is considerable other work on issues related to modularization of ontologies. We present four use cases for "Ontology Programming Interfaces" and discuss the existing features of OWL that facilitate this approach and the additional features needed to consolidate it. Copyright © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2008342,
author={Chen, C. and Maza, M.M. and Lemaire, F. and Pan, W. and Li, L. and Xie, Y.},
title={The ConstructibleSetTools and ParametricSystemTools modules of the RegularChains library in Maple},
journal={Proceedings - The International Conference on Computational Sciences and its Applications, ICCSA 2008},
year={2008},
pages={342-352},
doi={10.1109/ICCSA.2008.61},
art_number={4561239},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249086756&doi=10.1109%2fICCSA.2008.61&partnerID=40&md5=76e0c8bb16f076e788780c9b9cb9aed1},
abstract={We present two new modules of the RegularChains library in Maple: ConstructibleSetTools which is the first distributed package dedicated to the manipulation of (parametric or not) constructible sets and Parametric- SystemToois which is the first implementation of comprehensive triangular decomposition. We illustrate the functionalities of these new modules by examples and describe our software design and implementation techniques. Since several existing packages have functionalities related to those of our new modules, we include an overview of the algorithms and software for manipulating constructible sets and solving parametric systems. © 2008 IEEE.},
author_keywords={Comprehensive triangular decomposition;  Constructible set;  Parametric polynomial system;  Regular chain;  Software design;  Triangular decomposition},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2008273,
author={Wang, S. and Avrunin, G.S. and Clarke, L.A.},
title={Plug-and-play architectural design and verification},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5135 LNCS},
pages={273-297},
doi={10.1007/978-3-540-85571-2_12},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849143699&doi=10.1007%2f978-3-540-85571-2_12&partnerID=40&md5=a948687930500f1a824f19c9fd8dd1c3},
abstract={In software architecture, components represent the computational units of a system and connectors represent the interactions among those units. Making decisions about the semantics of these interactions is a key part of the design process. It is often difficult, however, to choose the appropriate interaction semantics due to the wide range of alternatives and the complexity of the system behavior affected by those choices. Techniques such as finite-state verification can be used to evaluate the impact of these design choices on the overall system behavior. This paper presents the Plug-and-Play approach that allows designers to experiment with alternative design choices of component interactions in a plug-and-play manner. With this approach, connectors representing specific interaction semantics are composed from a library of predefined, reusable building blocks. In addition, standard interfaces for components are defined that reduce the impact of interaction changes on the components' computations. This approach facilitates design-time verification by improving the reusability of component models and by providing reusable formal models for the connector building blocks, thereby reducing model-construction time for finite-state verification. © Springer-Verlag Berlin Heidelberg 2008.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao2008,
author={Zhao, Y.-J. and Xu, C.},
title={Design and simulation of human lower extremity exoskeleton},
journal={Xitong Fangzhen Xuebao / Journal of System Simulation},
year={2008},
volume={20},
number={17},
pages={4756-4759+4766},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52349122481&partnerID=40&md5=f5e66ecfa8ab39ab8d66d19946ca399c},
abstract={A Human Lower Extremity Exoskeleton (HLEE) that integrates human and robotic machines into one system and can augment soldier strength was designed. Through analyzing the function, work principle, construction of HLEE, and probing into the correlative construction characteristics of human lower extremity joint, freedom degree, gait of human based on bionics, the structure of HLEE was designed. The modeling and simulation of HLEE were performed with three-dimensional modeling software Pro/E and kinematic modeling package ADAMS. All these work established the foundation for further research in the future.},
author_keywords={Exoskeleton;  Gait;  Simulation;  Structure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Praing2008313,
author={Praing, R. and Schneider, M.},
title={Efficient implementation techniques for topological predicates on complex spatial objects},
journal={GeoInformatica},
year={2008},
volume={12},
number={3},
pages={313-356},
doi={10.1007/s10707-007-0035-y},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46949085280&doi=10.1007%2fs10707-007-0035-y&partnerID=40&md5=397850eb4d5a727ca6e9c84fba535b24},
abstract={Topological relationships like overlap, inside, meet, and disjoint uniquely characterize the relative position between objects in space. For a long time, they have been a focus of interdisciplinary research as in artificial intelligence, cognitive science, linguistics, robotics, and spatial reasoning. Especially as predicates, they support the design of suitable query languages for spatial data retrieval and analysis in spatial database systems and geographical information systems. While, to a large extent, conceptual aspects of topological predicates (like their definition and reasoning with them) as well as strategies for avoiding unnecessary or repetitive predicate executions (like predicate migration and spatial index structures) have been emphasized, the development of robust and efficient implementation techniques for them has been largely neglected. Especially the recent design of topological predicates for all combinations of complex spatial data types has resulted in a large increase of their numbers and stressed the importance of their efficient implementation. The goal of this article is to develop correct and efficient implementation techniques of topological predicates for all combinations of complex spatial data types including two-dimensional point, line, and region objects, as they have been specified by different authors and in different commercial and public domain software packages. Our solution consists of two phases. In the exploration phase, for a given scene of two spatial objects, all topological events like intersection and meeting situations are summarized in two precisely defined topological feature vectors (one for each argument object of a topological predicate) whose specifications are characteristic and unique for each combination of spatial data types. These vectors serve as input for the evaluation phase which analyzes the topological events and determines the Boolean result of a topological predicate (predicate verification) or the kind of topological predicate (predicate determination) by a formally defined method called nine-intersection matrix characterization. Besides this general evaluation method, the article presents an optimized method for predicate verification, called matrix thinning, and an optimized method for predicate determination, called minimum cost decision tree. The methods presented in this article are applicable to all known complete collections of mutually exclusive topological predicates that are formally based on the well known nine-intersection model. © Springer Science+Business Media, LLC 2007.},
author_keywords={Complex spatial object;  Evaluation phase;  Exploration phase;  Matrix thinning;  Minimum cost decision tree;  Nine-intersection matrix characterization;  Predicate determination;  Predicate verification;  Spatial database;  Topological feature vector;  Topological predicate},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kaczmarek2008317,
author={Kaczmarek, P.L.},
title={Ontology supported selection of versions for N-version programming in semantic Web Services},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5101 LNCS},
number={PART 1},
pages={317-326},
doi={10.1007/978-3-540-69384-0_37},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749110973&doi=10.1007%2f978-3-540-69384-0_37&partnerID=40&md5=b51e04fe98bca10ea73e0af98c575b53},
abstract={Web Services environment provides capabilities for effective N-version programming as there exist different versions of software that provide the same functionality. N-version programming, however, faces the significant problem of co-relation of failures in different software versions. This paper presents a solution that attempts to reduce the risk of co-relation of failures by selecting for invocation services having relatively different non-functional features. We use an ontology-driven approach to identify and store information about software features related to differences in software versions, such as: software vendor, design technology or implementation language. We present an algorithm for selection of software versions using the designed ontology. The solution was verified in a prototypical implementation with the use of an existing OWL-S API library. © 2008 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Goto20081,
author={Goto, K. and Van De Geijn, R.},
title={High-performance implementation of the level-3 BLAS},
journal={ACM Transactions on Mathematical Software},
year={2008},
volume={35},
number={1},
pages={1-14},
doi={10.1145/1377603.1377607},
note={cited By 205},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849089104&doi=10.1145%2f1377603.1377607&partnerID=40&md5=4dc984abd7a50b5cf2d136d7f5925bc4},
abstract={A simple but highly effective approach for transforming high-performance implementations on cache-based architectures of matrix-matrix multiplication into implementations of other commonly used matrix-matrix computations (the level-3 BLAS) is presented. Exceptional performance is demonstrated on various architectures. © 2008 ACM.},
author_keywords={Basic linear algebra subprograms;  Libraries;  Linear algebra;  Matrix-matrix operations},
document_type={Article},
source={Scopus},
}

@ARTICLE{Belic2008400,
author={Belic, K. and Surla, D.},
title={User-friendly web application for bibliographic material processing},
journal={Electronic Library},
year={2008},
volume={26},
number={3},
pages={400-410},
doi={10.1108/02640470810879536},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-45349093780&doi=10.1108%2f02640470810879536&partnerID=40&md5=839507404fe530cc15ab5c47b8c0037b},
abstract={Purpose - The purpose of this paper is implementation of a software system for bibliographic material processing which does not require knowledge about any format for bibliographic data input. This means that the input can be done not only by librarians, but also by other persons such as authors of bibliographic units, students, employees, etc. Design/methodology/approach - An object-oriented methodology for developing information systems by means of computer-aided software engineering (CASE) tools and software components is used. The software architecture is multi-layered and web-based. The implementation is done in Java environment. Findings - The result is a web application by which users can catalogue bibliographic material without being familiar with the corresponding format. Nevertheless, the bibliographic record is formed in accordance with the given format for bibliographic material processing (MARC21, UNIMARC, etc.). Research limitations/implications - Automatic generating of screen forms for the chosen set of data for bibliographic material processing is not provided in the presented version of the application. In order to eliminate this limitation, there are preset solutions that can be integrated into the application. Practical implications - The application is primarily intended for research institutions aiming at forming their electronic catalogue and/ or bibliographies of researchers or institutions. Originality/value - The originality of the paper lies in the software architecture of the application related to the middle layer, i.e. the one of business logic. This layer implements a mechanism by which different sets of input data are mapped to persistent data by means of the unique object model of an accepted format for the bibliographic material processing (MARC21, UNIMARC, or others).},
author_keywords={Bibliographies;  Computer software;  Library systems;  Online catalogues;  Worldwide web},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ustymenko20081057,
author={Ustymenko, S. and Schwartz, D.},
title={Adapting software engineering design patterns for ontology construction},
journal={WSEAS Transactions on Information Science and Applications},
year={2008},
volume={5},
number={6},
pages={1057-1066},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959225805&partnerID=40&md5=a8be7a4e2974761809b8f032f7c0907a},
abstract={In this paper, we present an argument for designing metadata schemata with design patterns. Design patterns are structured descriptions of solutions to some class of problems, and are used extensively in various stages of object-oriented software engineering. We present a use case of collaborative construction of metadata for a digital library. We explore design challenges this scenario presents and then adapt a pattern called Composite from a standard software engineering design patterns reference to address parts of these challenges. Additionally, we propose a new design pattern called History suggested by a collaborative metadata construction scenario and applicable to a wider class of problems in metadata design.},
author_keywords={Design patterns;  Knowledge engineering;  Object-Oriented design;  Semantic metadata;  Web ontology},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2008761,
author={Chen, J.-C.},
title={Symmetry partition sort},
journal={Software - Practice and Experience},
year={2008},
volume={38},
number={7},
pages={761-773},
doi={10.1002/spe.851},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949093813&doi=10.1002%2fspe.851&partnerID=40&md5=b1194ac99e52c6479476458eb760261b},
abstract={In this paper, we propose a useful replacement for quicksort-style utility functions. The replacement is called Symmetry Partition Sort, the principle of which is similar to that of Proportion Extend Sort. The main characteristic of the new algorithm is that it always places partially sorted inputs (used as a basis for proportional extensions) at each end of an array when entering the partitioning routine. This is advantageous in speeding up the processing for partitioning. The library function we developed based on the new algorithm is more attractive than the Psort library function introduced in 2004 because of its simple implementation mechanism, clearer source code, and faster operation with a performance guarantee of O(n log n). Increased robustness and adaptivity also make it highly competitive as a library function. Copyright © 2007 John Wiley & Sons, Ltd.},
author_keywords={Algorithm design and implementation;  High-performance sorting;  Library sort functions;  Proportion extend sort},
document_type={Article},
source={Scopus},
}

@ARTICLE{Marzullo20081,
author={Marzullo, F.P. and Porto, R.N. and da Silva, G.Z. and de Souza, J.M. and Blaschek, J.R.},
title={An MDA approach for database profiling and performance assessment},
journal={Studies in Computational Intelligence},
year={2008},
volume={131},
pages={1-10},
doi={10.1007/978-3-540-79187-4_1},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43349104047&doi=10.1007%2f978-3-540-79187-4_1&partnerID=40&md5=a73a8dd6529c19c9358c128c5cdf32ad},
abstract={This paper describes a Model Driven Architecture (MDA) approach for assessing database performance. The increase in the area of component-based development is reaching a point where performance issues are critical to successful system deployment. It is widely accepted that the model development approach is playing an important role on IT projects; therefore the profiling technique discussed here presents a way to assess performance and identify flaws while performing software construction activities. The approach is straightforward: using new defined MDA stereotypes and tagged values in conjunction with profiling libraries, the proposed MDA extension enables code generation to conduct a thorough set of performance analysis elements. This implementation uses the well-known MDA framework AndroMDA [2], the profiling libraries JAMon [11] and InfraRED [15], and creates a Profiling Cartridge, in order to generate the assessment code. © 2008 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Saha2008245,
author={Saha, P. and El-Araby, E. and Huang, M. and Taher, M. and Lopez-Buedo, S. and El-Ghazawi, T. and Shu, C. and Gaj, K. and Michalski, A. and Buell, D.},
title={Portable library development for reconfigurable computing systems: A case study},
journal={Parallel Computing},
year={2008},
volume={34},
number={4-5},
pages={245-260},
doi={10.1016/j.parco.2008.03.005},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43049154799&doi=10.1016%2fj.parco.2008.03.005&partnerID=40&md5=385fc7e741458ee87f4221b28ff36087},
abstract={Portable libraries of highly-optimized hardware cores can significantly reduce the development time of reconfigurable computing applications. This paper presents the tradeoffs and challenges in the design of such libraries. A set of library development guidelines is provided, which has been validated with the RCLib case study. RCLib is a set of portable libraries with over 100 cores, targeting a wide range of applications. RCLib portability has been verified in three major High-Performance reconfigurable computing architectures: SRC6, Cray XD1 and SGI RC100. Compared to full-software implementations, applications using RCLib hardware acceleration cores show speedups ranging from one to four orders of magnitude. © 2008 Elsevier B.V. All rights reserved.},
author_keywords={FPGA;  Hardware design methodologies;  Portable libraries;  Reconfigurable computing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pattabiraman2008219,
author={Pattabiraman, K. and Grover, V. and Zorn, B.G.},
title={Samurai: Protecting critical data in unsafe languages},
journal={Operating Systems Review (ACM)},
year={2008},
volume={42},
number={4},
pages={219-232},
doi={10.1145/1357010.1352616},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952162164&doi=10.1145%2f1357010.1352616&partnerID=40&md5=c3e5858da28ac98089099285fae88480},
abstract={Programs written in type-unsafe languages such as C and C++ incur costly memory errors that result in corrupted data structures, program crashes, and incorrect results. We present a data-centric solution to memory corruption called critical memory, a memory model that allows programmers to identify and protect data that is critical for correct program execution. Critical memory defines operations to consistently read and update critical data, and ensures that other non-critical updates in the program will not corrupt it. We also present Samurai, a runtime system that implements critical memory in software. Samurai uses replication and forward error correction to provide probabilistic guarantees of critical memory semantics. Because Samurai does not modify memory operations on non-critical data, the majority of memory operations in programs run at full speed, and Samurai is compatible with third party libraries. Using both applications, including a Web server, and libraries (an STL list class and a memory allocator), we evaluate the performance overhead and fault tolerance that Samurai provides. We find that Samurai is a useful and practical approach for the majority of the applications and libraries considered. Copyright 2008 ACM.},
author_keywords={Critical memory;  Error recovery;  Memory safety},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DeFlorio2008,
author={De Florio, V. and Blondia, C.},
title={A survey of linguistic structures for application-level fault tolerance},
journal={ACM Computing Surveys},
year={2008},
volume={40},
number={2},
doi={10.1145/1348246.1348249},
art_number={6},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43649103734&doi=10.1145%2f1348246.1348249&partnerID=40&md5=637b516f7ccedfaf8bfb0bf3a4d41f4f},
abstract={Structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. Structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. As such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. This fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. In this text we first define a "base" of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. This result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures. © 2008 ACM.},
author_keywords={Language support for software-implemented fault tolerance;  Reconfiguration and error recovery;  Separation of design concerns;  Software fault tolerance},
document_type={Review},
source={Scopus},
}

@ARTICLE{Oliveira2008524,
author={Oliveira, M.J.T. and Nogueira, F.},
title={Generating relativistic pseudo-potentials with explicit incorporation of semi-core states using APE, the Atomic Pseudo-potentials Engine},
journal={Computer Physics Communications},
year={2008},
volume={178},
number={7},
pages={524-534},
doi={10.1016/j.cpc.2007.11.003},
note={cited By 112},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-40249091764&doi=10.1016%2fj.cpc.2007.11.003&partnerID=40&md5=b03a2ca96a8d6270c25ab2395c7b375a},
abstract={We present a computer package designed to generate and test norm-conserving pseudo-potentials within Density Functional Theory. The generated pseudo-potentials can be either non-relativistic, scalar relativistic or fully relativistic and can explicitly include semi-core states. A wide range of exchange-correlation functionals is included. Program summary: Program title: Atomic Pseudo-potentials Engine (APE). Catalogue identifier: AEAC_v1_0. Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEAC_v1_0.html. Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland. Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html. No. of lines in distributed program, including test data, etc.: 88 287. No. of bytes in distributed program, including test data, etc.: 649 959. Distribution format: tar.gz. Programming language: Fortran 90, C. Computer: any computer architecture, running any flavor of UNIX. Operating system: GNU/Linux. RAM: <5 Mb. Classification: 7.3. External routines: GSL (http://www.gnu.org/software/gsl/). Nature of problem: Determination of atomic eigenvalues and wave-functions using relativistic and nonrelativistic Density-Functional Theory. Construction of pseudo-potentials for use in ab-initio simulations. Solution method: Grid-based integration of the Kohn-Sham equations. Restrictions: Relativistic spin-polarized calculations are not possible. The set of exchange-correlation functionals implemented in the code does not include orbital-dependent functionals. Unusual features: The program creates pseudo-potential files suitable for the most widely used ab-initio packages and, besides the standard non-relativistic Hamann and Troullier-Martins potentials, it can generate pseudo-potentials using the relativistic and semi-core extensions to the Troullier-Martins scheme. APE also has a very sophisticated and user-friendly input system. Running time: The example given in this paper (Si) takes 10 s to run on a Pentium IV machine clocked at 2 GHz. © 2007 Elsevier B.V. All rights reserved.},
author_keywords={Density functional;  Electronic structure;  Pseudo-potential},
document_type={Article},
source={Scopus},
}

@ARTICLE{Buckley2008107,
author={Buckley, J. and LeGear, A.P. and Exton, C. and Cadogan, R. and Johnston, T. and Looby, B. and Koschke, R.},
title={Encapsulating targeted component abstractions using software Reflexion Modelling},
journal={Journal of Software Maintenance and Evolution},
year={2008},
volume={20},
number={2},
pages={107-134},
doi={10.1002/smr.364},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-42649107161&doi=10.1002%2fsmr.364&partnerID=40&md5=28eaa7a17d16620bc566d73fc1685eef},
abstract={Design abstractions such as components, modules, subsystems or packages are often not made explicit in the implementation of legacy systems. Indeed, often the abstractions that are made explicit turn out to be inappropriate for future evolution agendas. This can make the maintenance, evolution and refactoring of these systems difficult. In this publication, we carry out a fine-grained evaluation of Reflexion Modelling as a technique for encapsulating user-targeted components. This process is a prelude to component recovery, reuse and refactoring. The evaluation takes the form of two in vivo case studies, where two professional software developers encapsulate components in a large, commercial software system. The studies demonstrate the validity of this approach and offer several best-use guidelines. Specifically, they argue that users benefit from having a strong mental model of the system in advance of Reflexion Modelling, even if that model is flawed, and that users should expend effort exploring the expected relationships present in Reflexion Models. Copyright © 2008 John Wiley & Sons, Ltd.},
author_keywords={Architecture recovery;  Component recovery;  Re-engineering;  Reflexion;  Software maintenance},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Quante2008171,
author={Quante, J.},
title={Using Library Dependencies for Clustering},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2008},
volume={P-126},
pages={171-175},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134631463&partnerID=40&md5=77354238f6246dcba34f2426db016bc1},
abstract={Software clustering is an established approach to automatic architecture recovery. It groups components that are in some way similar to each other. Usually, the similarity measure is based on the dependencies between components of a system. Library dependencies are intentionally ignored during the clustering process - otherwise, system components would be clustered with library components they use. We propose to particularly look at the dependencies on external components or libraries to learn more about an application's high-level structure. The number of dependencies of a component from different kinds of libraries provides insightful information about the component's functionality. Our case study illustrates the potential of this idea. © 2008 Gesellschaft fur Informatik (GI). All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ardimento2008357,
author={Ardimento, P. and Baldassarre, M.T. and Cimitile, M. and Visaggio, G.},
title={Empirical experimentation for validating the usability of knowledge packages in transferring innovations},
journal={Communications in Computer and Information Science},
year={2008},
volume={22 CCIS},
pages={357-370},
doi={10.1007/978-3-540-88655-6_27},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099427002&doi=10.1007%2f978-3-540-88655-6_27&partnerID=40&md5=058cb00e3e5e7a0b708d29113a3ec8ab},
abstract={Transfer of research results following to technological innovation and to the experience collected in applying the innovation within an enterprise is a key success factor. A critical factor in transferring innovations to software processes concerns the knowledge transfer activity which requires the knowledge be explicit and understandable by stakeholders. As so many researchers have been studying alternative ways to conventional approaches i.e. books, papers, reports and other written communication means that favour knowledge acquisition on behalf of users. In this context, we propose the Knowledge Package (KP) structure as alternative. We have carried out an experiment which compared the usability of the proposed approach with conventional ones, along with the efficiency and the comprehensibility of the knowledge enclosed in a KP rather than in a set of Conventional Sources. The experiment has pointed out that knowledge packages are more efficient than conventional ones, for knowledge transfer. The experiment has been described according to guidelines that allow for replications. In this way other researchers can confirm or refute the results and enforce their validity. © 2008 Springer-Verlag.},
author_keywords={empirical investigation;  Knowledge packaging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wegner2008,
author={Wegner, J.D. and Inglada, J. and Tison, C.},
title={Automatic fusion of SAR and optical imagery based on line features},
journal={Proceedings of the European Conference on Synthetic Aperture Radar, EUSAR},
year={2008},
volume={1-4},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064164168&partnerID=40&md5=210eb02dec2d5b9cb5e9b3c8c0e3e5d6},
abstract={A generic image processing chain for the fusion of very high resolution optical and SAR imagery is developed. It is fast to operate and provides reasonably good accuracy. This method is capable of rapidly registering optical and SAR images based on extracted features rather than on pixels. Indeed, at very high resolution, pixel approaches fail on above ground structures. An approach on a higher semantic level is required. The fusion is generally applicable to many kinds of remote sensing imagery due to it being generic. Its goal is to enable feature based registration of unrectified optical and SAR images with sub metric resolution in urban areas. Important aspects of this fusion approach are the very high resolution of the images, the introduction of a classification, rapid processing and the modular construction of the entire processing chain based on the open source software library ORFEO Toolbox (OTB). © 2008 VDE VERLAG GMBH.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Calvo-Manzano200825,
author={Calvo-Manzano, J.A. and Cuevas, G. and Feliu, T.S. and Serrano, A.},
title={A process asset library to support software process improvement in small settings},
journal={Communications in Computer and Information Science},
year={2008},
volume={16},
pages={25-35},
doi={10.1007/978-3-540-85936-9_3},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012096618&doi=10.1007%2f978-3-540-85936-9_3&partnerID=40&md5=be1e86927feef58ba43f0e3d81878d26},
abstract={A main factor to the success of any organization process improvement effort is the Process Asset Library implementation that provides a central database accessible by anyone at the organization. This repository includes any process support materials to help process deployment. Those materials are composed of organization's standard software process, software process related documentation, descriptions of the software life cycles, guidelines, examples, templates, and any artefacts that the organization considers useful to help the process improvement. This paper describes the structure and contents of the Web-based Process Asset Library for Small businesses and small groups within large organizations. This library is structured using CMMI as reference model in order to implement those Process Areas described by this model. © Springer-Verlag Berlin Heidelberg 2008.},
author_keywords={CMMI;  OPD;  PAL;  Process assets;  Process library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2008435,
author={Chen, P.-H. and Truc, N.T.L.},
title={Automatic 3D modeling development and application for hydraulic construction},
journal={ISARC 2008 - Proceedings from the 25th International Symposium on Automation and Robotics in Construction},
year={2008},
pages={435-439},
doi={10.3846/isarc.20080626.435},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862997367&doi=10.3846%2fisarc.20080626.435&partnerID=40&md5=0674a1789408645283e18ebfeef06ca5},
abstract={Nowadays, the application of 3D models is increasing in almost every field, especially the AEC (Architecture, Engineering, and Construction) industry. Most 3D models are generated through human manipulation with the use of CAD software like 3D Max, AutoCAD or Maya. These software packages are very efficient in 3D modeling, but sometimes they are not easy to use and experience might be required. Moreover, traditional 3D modeling is time-consuming, as manual input is required for each component as well as for the whole scene. Therefore, it is difficult for a person to build 3D models without good 3D modeling skills and the knowledge of 3D modeling tools, although CAD software provides many facilitating functions. In the hydraulic engineering domain, 3D models are used not only to illustrate the realistic view before construction, but also to measure the construction's volume in order to estimate cost based on cut-and-fill volume quantity, concrete volume quantity, etc. Therefore, 3D modeling is crucial and greatly needed in planning hydraulic construction. Since 3D modeling for hydraulic facilities requires accuracy and details, it would need a lot of time, effort and prior experience to manually build the whole model from scratch. To meet this need, an approach of automatic 3D modeling is proposed in this paper. The main purpose is to help the user build a 3D model of hydraulic construction in less time and with less manual operation. Also, the proposed approach aims to provide easy-to-use features for those who do not have much experience in 3D modeling. To demonstrate the proposed approach, an application is developed in the AutoCAD environment to automatically generate a 3D sluice model, a model of artificial passageway for water fitted with a valve or gate to stop or regulate water flow. With this approach, users no longer have to manually build 3D models step by step. A 3D model could be automatically generated shortly after the input of data. As most steps are automatically done by the computer, the result would be of high accuracy. A complete 3D sluice model could be created from a sketch without much effort and time spent comparing to the traditional step-by-step manual input and operation.},
author_keywords={3D modeling;  Automatic;  Sluice;  VisualLISP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhou200873,
author={Zhou, Q. and Li, W. and Yang, J.},
title={A study of construction and share digital resources in an higher education district in China},
journal={ICWL 2008 - Short Paper Proceedings of the 7th International Conference on Web-Based Learning},
year={2008},
pages={73-75},
doi={10.1109/ICWL.2008.18},
art_number={5163796},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949926057&doi=10.1109%2fICWL.2008.18&partnerID=40&md5=8075d8bc38c4610aed9aa25735cd3335},
abstract={This article includes the digital library experience in Xiasha Education District of Hangzhou city in China. The first part is the necessity to construct the digital library center in the changing trends of the higher education in China. It appears that the traditional library could not follow the trends of the higher education in China. The second part is the technology experience for building up the digital library. It introduces the theory structure and construction steps of digital library and resource. In this section, we introduce all levels of the structure theory. Meanwhile, we show the steps of constructing the digital library including organization of the committee, connection of Internet and the hardware of computers, standard and development of software and preservation and building up of digital resource. © 2008 IEEE.},
author_keywords={Digital library;  Digital resources;  Higher education district},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Suarez2008184,
author={Suarez, M. and Sison, R.},
title={Automatic construction of a bug library for object-oriented novice java programmer errors},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={5091 LNCS},
pages={184-193},
doi={10.1007/978-3-540-69132-7_23},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349876594&doi=10.1007%2f978-3-540-69132-7_23&partnerID=40&md5=1a3a2f77210a4c6aeb47ac57b2bfe11c},
abstract={Machine learning techniques have been applied to the task of student modeling, more so in building tutors for acquiring programming skill. These were developed for various languages (Pascal, Prolog, Lisp, C++) and programming paradigms (procedural and declarative) but never for object-oriented programming in Java. JavaBugs builds a bug library automatically using discrepancies between a student and correct program. While other works analyze code snippets or UML diagrams to infer student knowledge of object-oriented design and programming, JavaBugs examines a complete Java program and identifies the most similar correct program to the student's solution among a collection of correct solutions and builds trees of misconceptions using similarity measures and background knowledge. Experiments show that JavaBugs can detect the most similar correct program 97% of the time, and discover and detect 61.4% of student misconceptions identified by the expert. © 2008 Springer-Verlag Berlin Heidelberg.},
author_keywords={Automatic bug library construction;  Java errors;  Multistrategy learning;  Object-oriented programming},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bodik200839,
author={Bodik, R.},
title={Small languages in an undergraduate pl/compiler course},
journal={ACM SIGPLAN Notices},
year={2008},
volume={43},
number={11},
pages={39-44},
doi={10.1145/1480828.1480836},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67049160301&doi=10.1145%2f1480828.1480836&partnerID=40&md5=edfc1815628ed57f578ec5caf5e3ca1d},
abstract={I propose to revive the undergraduate PL/compiler course by making it relevant to software engineers - the bulk of our audience. My premise is that developers are not mere users of programming languages; the better developers regularly design small languages, whether they are frameworks with rich APIs, code generators, configuration and data processing languages, or full scripting languages. Our recent experience at Berkeley shows that we may indeed be able to awaken budding language designers while developing at least a better language "taste" in the remaining students. The proposed approach is to go through the development of small languages: the choice of a programming model (abstractions), design of an intuitive concrete syntax, effective static and dynamic type checking, and efficient implementation. It's hard to fit all this into a single course, of course, especially if the course is to cover enough about both design and implementation of languages. One trick may be to use the language implementation itself as a case study for small language design. For example, regular expressions deserve more attention that we have been giving them in lexical analysis: you could argue that their semantics, as defined in most languages is broken and so is their implementation. Revisiting their semantics is a case study in how to embed a language and their implementation offers an exercise in how to parallelize an algorithm that has been considered inherently sequential. Finally, we can increase the relevance to software developers by focusing on web languages, where new programming models are likely to appear: a well-designed course will prepare students for new web languages, and may encourage them to design these languages themselves. After all, PHP, javascript, Ruby and perl were all designed by language amateurs: the more we teach our undergrads, the better our future languages will be. © 2008 ACM.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Verstraelen20081530,
author={Verstraelen, T. and Van Speybroeck, V. and Waroquier, M.},
title={ZEOBUILDER: A GUI toolkit for the construction of complex molecular structures on the nanoscale with building blocks},
journal={Journal of Chemical Information and Modeling},
year={2008},
volume={48},
number={7},
pages={1530-1541},
doi={10.1021/ci8000748},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449118321&doi=10.1021%2fci8000748&partnerID=40&md5=114b54e34d63e3e57a6076e0c3c41f8e},
abstract={In this paper, a new graphical toolkit, ZEOBUILDER, is presented for the construction of the most complex zeolite structures based on building blocks. Molecular simulations starting from these model structures give novel insights in the synthesis mechanisms of micro- and mesoporous materials. ZEOBUILDER is presented as an open-source code with easy plug-in facilities. This architecture offers an ideal platform for further development of new features. Another specific aspect in the architecture of ZEOBUILDER is the data structure with multiple reference frames in which molecules and molecular building blocks are placed and which are hierarchically ordered. The main properties of ZEOBUILDER are the feasibility for constructing complex structures, extensibility, and transferability. The application field of ZEOBUILDER is not limited to zeolite science but easily extended to the construction of other complex (bio)molecular systems. ZEOBUILDER is a unique user-friendly GUI toolkit with advanced plug-ins allowing the construction of the most complex molecular structures, which can be used as input for all ab initio and molecular mechanics program packages. © 2008 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Santana200875,
author={Santana, F.S. and de Siqueira, M.F. and Saraiva, A.M. and Correa, P.L.P.},
title={A reference business process for ecological niche modelling},
journal={Ecological Informatics},
year={2008},
volume={3},
number={1},
pages={75-86},
doi={10.1016/j.ecoinf.2007.12.003},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-40949128257&doi=10.1016%2fj.ecoinf.2007.12.003&partnerID=40&md5=63ce22e416e0c4fbc779862143242337},
abstract={Ecological niche modelling combines species occurrence points with environmental raster layers in order to obtain models for describing the probabilistic distribution of species. The process to generate an ecological niche model is complex. It requires dealing with a large amount of data, use of different software packages for data conversion, for model generation and for different types of processing and analyses, among other functionalities. A software platform that integrates all requirements under a single and seamless interface would be very helpful for users. Furthermore, since biodiversity modelling is constantly evolving, new requirements are constantly being added in terms of functions, algorithms and data formats. This evolution must be accompanied by any software intended to be used in this area. In this scenario, a Service-Oriented Architecture (SOA) is an appropriate choice for designing such systems. According to SOA best practices and methodologies, the design of a reference business process must be performed prior to the architecture definition. The purpose is to understand the complexities of the process (business process in this context refers to the ecological niche modelling problem) and to design an architecture able to offer a comprehensive solution, called a reference architecture, that can be further detailed when implementing specific systems. This paper presents a reference business process for ecological niche modelling, as part of a major work focused on the definition of a reference architecture based on SOA concepts that will be used to evolve the openModeller software package for species modelling. The basic steps that are performed while developing a model are described, highlighting important aspects, based on the knowledge of modelling experts. In order to illustrate the steps defined for the process, an experiment was developed, modelling the distribution of Ouratea spectabilis (Mart.) Engl. (Ochnaceae) using openModeller. As a consequence of the knowledge gained with this work, many desirable improvements on the modelling software packages have been identified and are presented. Also, a discussion on the potential for large-scale experimentation in ecological niche modelling is provided, highlighting opportunities for research. The results obtained are very important for those involved in the development of modelling tools and systems, for requirement analysis and to provide insight on new features and trends for this category of systems. They can also be very helpful for beginners in modelling research, who can use the process and the experiment example as a guide to this complex activity. © 2008 Elsevier B.V. All rights reserved.},
author_keywords={Ecological niche modeling;  Modelling software packages;  Reference architecture;  Reference business process;  SOA;  Species distribution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Filippi200876,
author={Filippi, S. and Motyl, B. and Bandera, C.},
title={Analysis of existing methods for 3D modelling of femurs starting from two orthogonal images and development of a script for a commercial software package},
journal={Computer Methods and Programs in Biomedicine},
year={2008},
volume={89},
number={1},
pages={76-82},
doi={10.1016/j.cmpb.2007.10.011},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-37549068558&doi=10.1016%2fj.cmpb.2007.10.011&partnerID=40&md5=198ed2b392b1d106f778fed22b96f217},
abstract={Background: At present the interest in medical field about the generation of three-dimensional digital models of anatomical structures increases due to the widespread diffusion of CAS - computer assisted surgery - systems. Most of them are based on CT - computer tomography - or MR - magnetic resonance - data volumes but sometimes this information is not available; there are only few X-ray, ultrasound or fluoroscopic images. Methods: This paper describes the study and the development of a script for a commercial software package (3ds Max) able to reconfigure the template model of a femur starting from two orthogonal images representing the specific patient's anatomy. Results: The script was used in several tests as summarized in this paper and the results appear to be interesting and acceptable, even for the medical experts that evaluated them. Conclusions: The script developed in this work allows the generation of the 3D model of a femur in a very simple way (the user interface has been developed obeying to the main usability guidelines) and using a widespread commercial package. The quality of the results can be compared to the quality of more expensive and specialized systems. © 2007 Elsevier Ireland Ltd. All rights reserved.},
author_keywords={Bone modelling;  Computer aided design;  Free form deformation;  Image-based reconfiguration},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2007,
title={Proceedings of the 2007 International Conference on Artificial Intelligence, ICAI 2007},
journal={Proceedings of the 2007 International Conference on Artificial Intelligence, ICAI 2007},
year={2007},
volume={1},
page_count={880},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866522231&partnerID=40&md5=e545f2962a0e4084162e00dc178729fd},
abstract={The proceedings contain 136 papers. The topics discussed include: automated system for the extraction of themes from news articles; ontology-driven reasoning and prediction; domain ontology construction from biomedical text; networked intelligent agents for distributed decision making; analysis of system behavior through cognitive architectures; confirmatory bias and the sharing of information within social networks; representing definitions and its associated knowledge in a learning program; collaborative learning grouping based on fuzzy clustering; an overview of intelligent tutoring systems; a survey of component-based face recognition approaches; two-stages evaluation algorithm for automatic ranking in information retrieval; fuzzy term-evaluation in a information system to manage airport user rights; second order meta-programming - situatedness, awareness, knowledge; and a discrete-event simulation model for dynamic function personalization in generalized software packages.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{NoAuthor2007,
title={Proceedings of the 2007 International Conference on Artificial Intelligence, ICAI 2007},
journal={Proceedings of the 2007 International Conference on Artificial Intelligence, ICAI 2007},
year={2007},
volume={2},
page_count={880},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866487763&partnerID=40&md5=75dd6699469a8dd0e5354aaf14d55472},
abstract={The proceedings contain 136 papers. The topics discussed include: automated system for the extraction of themes from news articles; ontology-driven reasoning and prediction; domain ontology construction from biomedical text; networked intelligent agents for distributed decision making; analysis of system behavior through cognitive architectures; confirmatory bias and the sharing of information within social networks; representing definitions and its associated knowledge in a learning program; collaborative learning grouping based on fuzzy clustering; an overview of intelligent tutoring systems; a survey of component-based face recognition approaches; two-stages evaluation algorithm for automatic ranking in information retrieval; fuzzy term-evaluation in a information system to manage airport user rights; second order meta-programming - situatedness, awareness, knowledge; and a discrete-event simulation model for dynamic function personalization in generalized software packages.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Short2007214,
author={Short, M.},
title={Efficient implementation of fault-tolerant data structures in PC-based control software},
journal={ICINCO 2007 - 4th International Conference on Informatics in Control, Automation and Robotics, Proceedings},
year={2007},
volume={SPSMC},
pages={214-219},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649909335&partnerID=40&md5=e9d66012cc16406f0805a9093ecf4d2b},
abstract={Recent years have seen an increased interest in the use of open-architecture, PC-based controllers for robotic and mechatronic systems. Although such systems give increased flexibility and performance at low unit cost, the use of commercial processors and memory devices can be problematic from a safety perspective as they lack many of the built-in integrity testing features that are typical of more specialised equipment. Previous research has shown that the rate of undetected corruptions in industrial PC memory devices is large enough to be of concern in systems where the correct functioning of equipment is vital. In this paper the mechanisms that may lead to such corruptions and the level of risk is examined. A simple, portable and highly effective software library is also presented in this paper that can reduce the impact of such memory errors. The effectiveness of the library is verified in a small example.},
author_keywords={Critical systems;  Open architecture controllers;  Software fault tolerance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Balaji20071340,
author={Balaji, V. and Ramakrishna, P.V.},
title={Design studies on the implementation of on board control, signal acquisition and communication (OBCSAC) system on FPGA/ASIC platforms},
journal={2007 International Conference on Intelligent and Advanced Systems, ICIAS 2007},
year={2007},
pages={1340-1344},
doi={10.1109/ICIAS.2007.4658602},
art_number={4658602},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57949116087&doi=10.1109%2fICIAS.2007.4658602&partnerID=40&md5=6bf920f1a0d0ce6d867b25ad24637e12},
abstract={The On Board Control, Signal Acquisition and Communication (OBCSAC) subsystem is a critical component of all satellite systems. This paper presents the results of a feasibility study on realizing the design subsystem first on FPGA, and then on ASIC. Apart from giving the design details and results on a typical basic configuration, a number of extensions of this architecture have been studied and the results are presented. Specifically, starting from the realization of the basic configuration on a commercial antifuse FPGA, the studies carried out include (a) mapping the vital configuration onto an equivalent radiation hardened antifuse FPGA (b) mapping the same configuration onto a commercial FPGA after incorporating 'Triple Modular Redundancy (TMR) manually at the RTL level, (c) altering the basic architecture significantly to conform to the CCSDS standard prescribed for the OBCSAC of any satellite and mapping the design onto to FPGAs as well as on ASICS first without TMR and then separately incorporating TMR for each case. The basic configuration realized in an FPGA had been extensively tested in hardware under various expected environmental conditions and the FPGA utilization details are provided. For the ASIC design, the details up to the layout/GDSII stage are presented. Further, in the case of the ASIC, the test patterns for the netlist were generated based on ATPG software and the details of fault coverage are provided. The FPGA designs were carried out with Actel SX72A and AX1000 device families and the ASIC designs were carried out using the AMIS 0.35u CMOS libraries. The results presented in this work will serve as a design guideline in terms of FPGA/ASIC area, power, speed, and fault coverage and fault tolerance to those working in the area of system design. ©2007 IEEE.},
author_keywords={ASIC design;  CCSDS;  FPGA;  TMR},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liverani200774,
author={Liverani, A. and Carbone, L. and Caligiana, G.},
title={VAM: Video aided modeling for shape reconstruction and re-design},
journal={Proceedings of the 2nd IASTED International Conference on Human-Computer Interaction, HCI 2007},
year={2007},
pages={74-79},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-54949124955&partnerID=40&md5=adfd387df87444912cdf5ff20d485336},
abstract={Although the 3D shape recovery of a real object have been greatly improved in the last few years, modeling a complex virtual object by starting from the real prototype is still a very time-consuming activity. In this paper an originally conceived method and testing software to recover a CAD model from a real object is presented. The developed software tool joins a professional Computer-Aided Design (CAD) and a Mixed Reality (MR) tool in the same interface, enabling the operator to use standard CAD tools and features together with a camera, which provides external image streaming displayed in the workspace background. Moreover, a special programmed library performs a real-time calculation of camera position and other parameters with respect to standard markers in order to drive the CAD 3D virtual camera and align it to external world. In that way rendered virtual models may be superimposed to external images of reality grabbed by the video camera. Thus with MR-CAD tool the operator may easily recover a complex shape directly from the external views of a real object or may start the object re-design from the previous reconstructed geometry. Furthermore the interface is totally integrated in a CAD environment, both avoiding to work with unfamiliar new software and exploiting CAD geometry database and tools. Finally, MR-CAD can be considered a significant step ahead in the bi-directional interaction of virtual and real models, reducing also the gap between real prototypes and CAD data.},
author_keywords={Augmented reality;  Mixed-reality;  Reverse engineering;  Virtual prototyping},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ming-Hu20071343,
author={Ming-Hu, Z. and Yong, W. and Sen, Z. and Shao-Jie, C. and Jing-Yang, B.},
title={Research on application of virtual instrument and grey theory in the fault diagnostic system},
journal={Proceedings of 2007 IEEE International Conference on Grey Systems and Intelligent Services, GSIS 2007},
year={2007},
pages={1343-1346},
doi={10.1109/GSIS.2007.4443492},
art_number={4443492},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249133708&doi=10.1109%2fGSIS.2007.4443492&partnerID=40&md5=036cc8e6a990e85189e75ab621ee1d8b},
abstract={Virtual instrument is an integration of the latest PC technology, advanced testing technique and strong software package, and the development of visual instrument system is, in the domain of the automatic test and fault diagnosis equipment, etc, the trend of modern technology. It is developed, applied virtual instrument technology, grey system theory, and database, the fault diagnostic system for shipboard chemical defense equipment, in order to update entirely the means of the equipment testing. The modularization and universalization are proposed in its database-based design concept, realizing the design of software and hardware. The ODBC technique is applied for the interconnection of databases to ensure the generality and flexibility of the system. The system mode the best of virtual instrument platform and grey diagnosis method, broke through conventional check diagnosis patterns for warships chemical defense equipment, solved the problems of state prediction and trouble-mode recognition of warships chemical defense equipment. It has been proved by experiments that the system has merits both high accuracy and economical practicability. Also it can reduce the application development cycle and cost. ©2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{LeBaron2007210,
author={LeBaron, T. and Jacobsen, C.},
title={The simulation power of automod},
journal={Proceedings - Winter Simulation Conference},
year={2007},
pages={210-218},
doi={10.1109/WSC.2007.4419603},
art_number={4419603},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-49749095098&doi=10.1109%2fWSC.2007.4419603&partnerID=40&md5=212e98e0b78a552648541cbb78da4546},
abstract={Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model objects in other models, decreasing the time required to build a model. In addition, recent enhancements to Automod's material handling template systems have increased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages. © 2007 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2007,
title={Proceedings of SPIE: Current Developments in Lens Desian and Optical Engineering VIII},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2007},
volume={6667},
page_count={238},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149113979&partnerID=40&md5=d27f281b6a2a986ddd8bfecdee89c761},
abstract={The proceedings contain 24 papers. The topics discussed include; new generation of high-resolution panoramic lenses; design and analysis of diffractive astigmatic lens for DVD pickup; large-format telecentric lens; rear landscape on steroids; optical encoder based on a nondiffractive beam; practical guide to saddle-point construction in lens design; predictability and unpredictability in optical system optimization; third order aberration solution using aberration polynomials for a general zoom lens design; vision multiplexing: an optical engineering concept for low-vision aids; modeling of polychromatic MT losses due to secondary effects in diffractive lenses; scene-based sensor modeling using optical design software; spectral response evaluation and computation for pushbroom imaging spectrometers; polarization vortices in optical design; and focal splitting and optical vortex structure induced by stress birefringence.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Järvi200773,
author={Järvi, J. and Marcus, M.A. and Smith, J.N.},
title={Library composition and adaptation using c++ concepts},
journal={GPCE'07 - Proceedings of the Sixth International Conference on Generative Programming and Component Engineering},
year={2007},
pages={73-82},
doi={10.1145/1289971.1289984},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849121252&doi=10.1145%2f1289971.1289984&partnerID=40&md5=3b18425d2eebeb34a85783f8bfdc2dcc},
abstract={Large scale software is composed of libraries produced by different entities. Non-intrusive and efficient mechanisms for adapting data structures from one library to conform to APIs of another are essential for the success of large software projects. Concepts and concept maps, planned features for the next version of C++, have been designed to support adaptation, promising generic, non-intrusive, efficient, and identity preserving adapters. This paper analyses the use of concept maps for library composition and adaptation, comparing and contrasting concept maps to other common adaptation mechanisms. We report on two cases of data structure adaptation between different libraries, indicating best practices and idioms along the way. First, we adapt GUI controls from several frameworks for use with a generic layout engine, extending the application of concepts to run-time polymorphism. Second, we develop a transparent adaptation layer between an image processing library and a graph algorithm library, enabling the efficient application of graph algorithms to the image processing domain. Copyright © 2007 ACM.},
author_keywords={C++;  Generic programming;  Polymorphism;  Software libraries},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Masi200720,
author={Masi, C.G.},
title={All my best ideas are stolen},
journal={PACE - Process and Control Engineering},
year={2007},
volume={60},
number={10},
pages={20-21},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-37349065609&partnerID=40&md5=7e5159874ff271d7b498f516f05d368e},
abstract={Several industry experts of process and control engineering shared their views on improving the control system processes by imitating tips and techniques, which were previously implemented by other industrial plants or companies. Peter Blume, president of Bloomy Controls, advised to prepare for good control-program design and develop a proof of concept to evaluate each specific instrument, hardware component, or software module. Roger Richardson, president of Delta Sigma Corp., expressed that end users can be best information sources and reiterated to carefully listen and understand their problems. Patrick Gallagher, managing partner at Millennium Control Systems, suggested to dynamically calculating motion profiles designed to replicate mechanical parts to improve fault recovery. Michael Gurney, co-owner and principal engineer at Concept Systems, also opined to observe open and modular architecture packages that are capable of the speeds and advanced motion functions required in various applications.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Joshi2007497,
author={Joshi, N. and Riley, W. and Schneider, J. and Tan, Y.-S.},
title={Integration of domain-specific IT processes and tools in IBM Service Management},
journal={IBM Systems Journal},
year={2007},
volume={46},
number={3},
pages={497-511},
doi={10.1147/sj.463.0497},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948892000&doi=10.1147%2fsj.463.0497&partnerID=40&md5=192b5804715a2bdce4481cca742c6987},
abstract={In this paper we focus on the integration of domain-specific information technology (IT) processes and tools in the IBM Service Management architecture, a service-oriented software architecture that automates and simplifies the management of IT services. The IT processes are based on a generalized concept of service management that incorporates best practices, such as those defined by the Information Technology Infrastructure Library® (ITIL®). The IT tools are the operational management tools in various domains, such as monitoring, network management, and provisioning. We refer to implementation of IT processes as Process Managers. We first describe three typical scenarios in which integrating the domain-specific IT processes through the use of PMs increases the level of automation. Then, we illustrate the benefits to be gained from integrating IT processes and tools and describe the design of four PMs: the Service Level Process Manager, the IBM Tivoli Availability Process Manager, the IBM Tivoli Capacity Process Manager, and the IT Service Continuity Process Manager. © Copyright 2007 by International Business Machines Corporation.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2007321,
author={Wang, H. and Akinci, B. and Garrett, J.H.},
title={Formalism for detecting version differences in data models},
journal={Journal of Computing in Civil Engineering},
year={2007},
volume={21},
number={5},
pages={321-330},
doi={10.1061/(ASCE)0887-3801(2007)21:5(321)},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547889957&doi=10.1061%2f%28ASCE%290887-3801%282007%2921%3a5%28321%29&partnerID=40&md5=cfb0e971a055bf7276c4f11ec80293c0},
abstract={In the architecture/engineering/construction (AEC) industry, a large number of data models (e.g., data exchange standards and task-specific data models) have been created and utilized to represent and exchange data in software packages. To meet the ever-expanding requirements for modeling real world information, the data models need to be updated frequently. Accordingly, those who need to implement these data models in their AEC-related software which often requires that they possess civil engineering domain knowledge, have to change their existing implementations for compliance with these models to account for the latest update. Before adopting changes of such data models, those developers working at AEC-related software companies must precisely identify which parts of the data models have been modified in a new release. Given the growing scale and complexity of today's data models involved in the AEC domain, identification of differences in two versions of a data model is a time-consuming and error-prone process, when performed manually. A semiautomated approach that identifies the differences in two versions of a data model could enable a rapid update of existing implementations of the model in AEC-related software. Due to the likelihood of having some commonality between the two versions of a model, it is possible to automatically identify version differences accurately. In this paper, we present an approach for detecting the differences between two releases of the same data model accurately and efficiently. This approach incorporates taxonomy for describing possible differences between two versions of a data model and provides a way to classify these differences. A prototype is implemented and used to validate the approach with the recent releases of some real world data models. The approach developed in this paper can help AEC-related software developers adopt and implement data models in their software systems. © 2007 ASCE.},
author_keywords={Computer software;  Construction industry;  Information management},
document_type={Article},
source={Scopus},
}

@ARTICLE{Karpushkin2007107,
author={Karpushkin, E. and Bogomolov, A. and Zhukov, Y. and Boruta, M.},
title={New system for computer-aided infrared and Raman spectrum interpretation},
journal={Chemometrics and Intelligent Laboratory Systems},
year={2007},
volume={88},
number={1},
pages={107-117},
doi={10.1016/j.chemolab.2006.08.010},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250656122&doi=10.1016%2fj.chemolab.2006.08.010&partnerID=40&md5=67f56aa43c536c537333d98a47de4b9b},
abstract={A new software tool for the interpretation of infrared and Raman spectra has been developed. It makes use of fragment libraries comprising representative, carefully selected and refined data on characteristic vibration frequencies. The information was collected from multiple sources including published correlation tables and reference spectral databases. In an automatic mode, the system performs structure to spectrum verification to test the correspondence between a drawn chemical structure and an experimental spectrum. Computer-aided "manual" interpretation (an expert mode) facilitates the assignment of structural elements to spectral features by means of highlighting the library information over the spectrum. The interpretation performance was significantly improved compared to other systems of this type due to some novel features. These are original fragment structures incorporating the concept of nucleus (vibrating group) and a new system of queries (fuzzy atoms and bonds), as well as the inter-fragment logic, which make fragment formulation extremely flexible. Important methodological aspects of fragment library construction were considered, and the main principles of fragment formulation on the basis of experimental spectra were formalized. Principal component analysis (PCA) was applied to distinguish clusters formed by spectral responses due to a specific structural environment and to refine characteristic frequency regions. © 2006 Elsevier B.V. All rights reserved.},
author_keywords={Computer-aided spectral interpretation;  Infrared spectroscopy;  PCA;  Raman spectroscopy},
document_type={Article},
source={Scopus},
}

@ARTICLE{VanDerSchaaf2007193,
author={Van Der Schaaf, T. and Germans, D. and Bal, H.E. and Koutek, M.},
title={Lessons learned from building and calibrating the ICWall, a stereo tiled display},
journal={Computer Animation and Virtual Worlds},
year={2007},
volume={18},
number={3},
pages={193-210},
doi={10.1002/cav.172},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547313589&doi=10.1002%2fcav.172&partnerID=40&md5=83bf940129c9fa544c0e14667cd6109c},
abstract={Implementation of stereo tiled displays is a rather demanding task. In this article we want to share the lessons we have learned during the design and construction of the ICWall tiled display. This large display, used in a classroom setting, is a high-resolution stereo tiled display (2×8 tiles), built from low-cost commodity components. The overall image is produced by an array of projectors. When building such a system, a key challenge is to align the projector images. We describe our automated approach for alignment/calibration of the left- and right-eye stereo images. We provide measurements that show accuracy of this procedure. We explain and compare two calibration approaches; a single-pass and a two-pass rendering method to align the tiled images. We explain how to provide seamless image on the tiled display and which issues have to be solved. We also discuss the depth perception issues on the ICWall for the large audiences. Another important aspect, is the architecture of the software used for PC-cluster-based rendering. We describe Aura, the parallel scene graph API that is used for rendering on our tiled display. Copyright © 2007 John Wiley & Sons, Ltd.},
author_keywords={Parallel rendering;  Stereo graphics;  Tiled displays;  VR system architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Poletti2007606,
author={Poletti, F. and Poggiali, A. and Bertozzi, D. and Benini, L. and Marchal, P. and Loghi, M. and Poncino, M.},
title={Energy-efficient multiprocessor systems-on-chip for embedded computing: Exploring programming models and their architectural support},
journal={IEEE Transactions on Computers},
year={2007},
volume={56},
number={5},
pages={606-621},
doi={10.1109/TC.2007.1040},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34147141277&doi=10.1109%2fTC.2007.1040&partnerID=40&md5=c35a65b00dba2cb1e16a44299030ef03},
abstract={In today's multiprocessor SoCs (MPSoCs), parallel programming models are needed to fully exploit hardware capabilities and to achieve the 100 Gops/W energy efficiency target required for Ambient Intelligence Applications. However, mapping abstract programming models onto tightly power-constrained hardware architectures imposes overheads which might seriously compromise performance and energy efficiency. The objective of this work is to perform a comparative analysis of message passing versus shared memory as programming models for single-chip multiprocessor platforms. Our analysis is carried out from a hardware-software viewpoint: We carefully tune hardware architectures and software libraries for each programming model. We analyze representative application kernels from the multimedia domain, and identify application-level parameters that heavily influence performance and energy efficiency. Then, we formulate guidelines for the selection of the most appropriate programming model and its architectural support. © 2007 IEEE.},
author_keywords={Embedded multimedia;  Energy efficiency;  Low power;  MPSoCs;  Programming models;  Task-level parallelism},
document_type={Article},
source={Scopus},
}

@ARTICLE{NoAuthor2007,
title={7th IFIP International Conference on e-Business, e-Services, and e-Society, I3E 2007},
journal={IFIP Advances in Information and Communication Technology},
year={2007},
volume={251 VOLUME 1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901784576&partnerID=40&md5=dfeb5693fa91bb67da94f18b54a86bbd},
abstract={The proceedings contain 77 papers. The special focus in this conference is on E-Business, E-Services, and E-Society. The topics include: Virtual organization theory; mining XML frequent query patterns; a group-based trust model in peer-to-peer environment; an optimization handover scheme based on HMIPv6; risk assessment of virtual enterprise based on the fuzzy comprehensive evaluation method; network externality products competition strategy; analysis of the supply-demand value chain in the B2B e-business; an integrated data mining and survival analysis model for customer segmentation; related problems of the vocational qualification standard for online financial marketing; internet virtual money under Chinese payment environment and its efficiency analysis; an analysis on the implementation of electronic supply chain in international trade; research and design of multimedia NSS for E-commerce; research on the interoperability architecture of the digital library grid; the research on enterprise manufacture integrated and collaborative commerce system; findings from Chinese main websites case studies; development of China C2C E-commerce from the perspective of goods delivery; research on the internal cause and correlative factor of virtual enterprises; mobile agents integrity research; research on selection of the third-party logistics service providers; the information architecture for website design; research on indexing systems for enterprise credit evaluation in B2B; knowledge sharing network based trust management for agent-mediated ecommerce; dynamic fair electronic cash model without trustees; customer satisfaction evaluation for mobile commerce services based on grey clustering relational method; contractual versatility in software business; information integration of virtual enterprise based on service-oriented architecture; organization performance model based on evolution of information system; link analysis and web attractiveness evaluation for enterprise websites; the credibility of enterprise's website and its evaluation in the customer's perspective; a dynamic pricing method in e-commerce based on PSO-trained neural network; convergence management system of mobile data services; an efficient mode selection algorithm for H.264; the effects of information asymmetry on consumer driven health plans; a research on Chinese consumers' using intention on 3G mobile phones; construction of China's publishing enterprise agile supply chain; an empirical study of factors related to consumer complaint behavior; adaptive customer profiles for context aware services in a mobile environment; research on anti-tax evasion system based on union-bank online payment mode; information overload on e-commerce; analysis on research and application of China C2C websites evaluating index system; the analysis of bullwhip effect in supply chain based on strategic alliance; knowledge innovation in e-business enterprises; the fuzzy integrated evaluation of enterprise information system security based on EC; the construction of integrated information portal based on innovative development; 3G-based mobile commerce value chain; an approach of personalization for the electronic commerce website based on ontology; knowledge processing and service system for digital education based on semantic web; hierarchy analysis method for management information systems; web-based coordination for e-commerce; analysis and implementation of workflow-based supply chain management system; the simulation and analysis of e-business model; payment scheme for multi-party cascading P2P exchange; a research of value-net based business model and operating of M-commerce; on developing china's third party payment; on Chinese tourism e-business development on current stage; an adaptive agent architecture for automated negotiation; a digital signature scheme in web-based negotiation support system; analysis of e-commerce model in transaction cost economics framework; research on design of analysis-based CRM system for mobile communications industry and its application; information retrieval in Web2.0 and abnormal data detection for an e-business using object-oriented approach.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Hristovska2007335,
author={Hristovska, E.},
title={Presence on cantilever beam with impact dynamic loadings},
journal={Annals of DAAAM and Proceedings of the International DAAAM Symposium},
year={2007},
pages={335-336},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896270198&partnerID=40&md5=fe6c682bcf1ecbfe96381b0c301eaab5},
abstract={In this paper the state on the carrying structure of the working wheel at rotating excavator SRs-630 for dynamic loadings is analyzed. Dynamic loadings on rotating excavators are random with the possibility for improvident impact loadings. With theoretical analysis, variable dynamic loadings are approximate with dimension on loading which is constant for determined working regimes, by methodology with which whole exploitation life of construction stays equal. Dynamic loadings on this carrying structure are analyzed for characteristically working regimes. For this propose regimes is defined as a deformity state of carrying structure and its clamp dogs, with use of software package. Results of this research are presented in this paper.},
author_keywords={Cantilever beam;  Deformity state;  Dynamic loadings;  Rotating excavator},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nebiker2007,
author={Nebiker, S. and Christen, M. and Eugster, H. and Flückiger, K. and Stierli, C.},
title={Integrating mobile geo sensors into collaborative virtual globes - Design and implementation issues},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2007},
volume={36},
number={5C55},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249108203&partnerID=40&md5=e65103f0b9e83eb4f4fc57067e44a189},
abstract={This paper describes the large potential offered by integrating mobile geo sensor data, for example georeferenced video imagery acquired from micro UAVs, into collaborative 3D geoinformation technologies. It identifies some of the key issues to be addressed in the design and implementation of such geospatial collaboration frameworks and describes the main features of a prototype system which is currently being implemented as part of the ViMo (Virtual Monitoring) research project. © 2007 International Society for Photogrammetry and Remote Sensing. All rights reserved.},
author_keywords={3D;  Geo Sensor Web;  Geospatial Collaboration;  Mobile Sensors;  Virtual Globes;  Visualisation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ceri20071,
author={Ceri, S.},
title={Design abstractions for innovative Web applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2007},
volume={4587 LNCS},
pages={1-2},
doi={10.1007/978-3-540-73390-4_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149003135&doi=10.1007%2f978-3-540-73390-4_1&partnerID=40&md5=0347940560b7b89dac45291c93ff2537},
abstract={Web Modelling Language (WebML) [1-2] was defined, about 8 years ago, as a conceptual model for data-intensive Web applications. Early deployment technologies were very unstable and immature; as a reaction, WebML was thought as a high level, implementation-independent conceptual model, and the associated design support environment, called WebRatio [7], has always been platform-independent, so as to adapt to frequent technological changes. WebML is based upon orthogonal separation of concerns: content, interface logics, and presentation logics are defined as separate components. The main innovation in WebML comes from the interface logics, that enables the computation of Web pages made up of logical components (units) interconnected by logical links (i.e., not only the units but also the links have a formal semantics); the computation is associated with powerful defaults so as to associate to simple diagrams all the required semantics for a full deployment, through code generators. While the Web has gone through waves of innovation, new application sectors have developed, and revolutionary concepts - such as enabling the interaction of software artefacts rather than only humans - are opening up. While the foundations of the WebML model and method are still the same, the pragmatics of its interpretation and use has dramatically changed through the last years [3-6]. A retrospective consideration of our work shows that we have addressed every new challenge by using a common approach, which indeed has become evident to us during the course of time, and now is well understood and consolidated. For every new research directions, we had to address four different kinds of extensions, respectively addressing the development process, the content model, the hypertext meta-model, and the tool framework. Extensions of the development process capture the new steps of the design that are needed to address the new functionalities, providing as well the methodological guidelines and best practices for helping designers. Extensions of the content model capture state information associated with providing the new functionalities, in the format of standard model, e.g. a collection of entities and relationship that is common to all applications; this standard model is intertwined with the application model, so as to enable a unified use of all available content. Extension of the hypertext meta-model capture the new abstractions that are required for addressing the new functionalities within the design of WebML specifications, through new kinds of units and links which constitute a functionality-specific "library", which adds to the "previous" ones. Extensions of the tool framework introduce new tools in order to extend those modelling capability falling outside of standard WebRatio components (content, interface logics, presentation logics), or to empower users with new interfaces and wizards to express the semantics of new units and links in terms of existing ones, or to provide direct execution support for new units and links (e.g. invoking a web service). In this talk, I first illustrate the common approach to innovation, and then show such approach at work in two contexts. One of them, dealing with "Service-Oriented Architectures" (SOA), has reached a mature state; the other one, "Semantic Web Services" (SWS), is at its infancy, but promises to deliver very interesting results in the forthcoming years. © Springer-Verlag Berlin Heidelberg 2007.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sia2007568,
author={Sia, S.K. and Soh, C.},
title={An assessment of package-organisation misalignment: Institutional and ontological structures},
journal={European Journal of Information Systems},
year={2007},
volume={16},
number={5},
pages={568-583},
doi={10.1057/palgrave.ejis.3000700},
note={cited By 113},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35648989968&doi=10.1057%2fpalgrave.ejis.3000700&partnerID=40&md5=f741b2fa2965988ead614febe7a1dd28},
abstract={Even with today's 'best practice' software, commercial packages continue to pose significant alignment challenges for many organisations. This paper proposes a conceptual framework, based on institutional theory and systems ontology, to assess the misalignments between package functionality and organisational requirements. We suggest that these misalignments can arise from incompatibility in the externally imposed or voluntarily adopted structures embedded in the organisation and package, as well as differences in the way the meaning of organisational reality is ontologically represented in the deep or surface structure of packages. The synthesis of the institutional-ontological dimensions leads us to identify four types of misalignments with varying degrees of severity - imposed-deep, imposed-surface, voluntary-deep, and voluntary-surface - and to predict their likely resolution. We test the predictions using over 400 misalignments from package implementations at three different sites. The findings support the predictions: the majority of imposed-deep misalignments were resolved via package customisation. Imposed-surface and voluntary-deep misalignments were more often resolved via organisational adaptation and voluntary-surface misalignments were almost always resolved via organisational adaptation. The extent of project success also appeared to be influenced by the number of misalignments and the proportion of imposed-deep misalignments. We conclude by suggesting strategies that implementing organisations and package vendors may pursue. © 2007 Operational Research Society Ltd. All rights reserved.},
author_keywords={Enterprise systems;  Institutional theory;  Misalignments;  Ontology;  Package software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Feather2007,
author={Feather, C.},
title={Electronic resources communications management: A strategy for success},
journal={Library Resources and Technical Services},
year={2007},
volume={51},
number={3},
pages={204-211+228},
doi={10.5860/lrts.51n3.204},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547486888&doi=10.5860%2flrts.51n3.204&partnerID=40&md5=f6b195a122467f3cfbc34eca7f51f2ac},
abstract={Communications in the workflow of electronic resources (e-resources) acquisitions and management are complex and numerous. The work of acquiring and managing e-resources is hampered by the lack of best practices, standards, and adequate personal information management software. The related communications reflect these inadequacies. An e-resource management communications analysis at The Ohio State University Libraries revealed the underlying structure of the communication network and areas that could be improved in terms of efficiency and effectiveness. E-resources management must be responsive to the high expectations of users and other library staff. Efficient management of the related communications network increases the likelihood of a productive and successful operation.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2007425,
author={Chen, W. and Gilson, M.K.},
title={ConCept: De novo design of synthetic receptors for targeted ligands},
journal={Journal of Chemical Information and Modeling},
year={2007},
volume={47},
number={2},
pages={425-434},
doi={10.1021/ci600233v},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247282678&doi=10.1021%2fci600233v&partnerID=40&md5=baa6a8cb298c5c0e4181d16c071779cd},
abstract={Low-molecular-weight receptors that bind targeted guest molecules have a wide range of potential applications but are difficult to design. This paper describes an evolutionary method for computer-aided design of such receptors that works by linking together chemical components from a user-defined library around a stable conformation of the targeted ligand. The software can operate in three modes: de novo design, in which it builds a wide variety of receptors from small components; macrocycle design, in which it builds homopolymeric macrocycles around the ligand; and elaboration of an existing receptor structure. The top candidates generated by the automatic construction process are further studied with detailed affinity calculations whose validity is supported by prior studies of experimentally characterized host-guest systems. All three modes of operation are illustrated here through the design of novel adenine receptors. © 2007 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sarakinos200739,
author={Sarakinos, S.S. and Valakos, I.M. and Nikolos, I.K.},
title={A software tool for generic parameterized aircraft design},
journal={Advances in Engineering Software},
year={2007},
volume={38},
number={1},
pages={39-49},
doi={10.1016/j.advengsoft.2006.06.001},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749123900&doi=10.1016%2fj.advengsoft.2006.06.001&partnerID=40&md5=5618dbe47c1c171699ec57d76026385a},
abstract={In this work a surface generation software named Ge.P.A.S. (for generic parameterized aircraft surface) is presented, designed for the construction of aircraft aerodynamic surfaces. The surface generation procedure is parameterized and different aircraft configurations can be produced in an interactive way. A hierarchical structure of geometric parameters was adopted, resulting in easier manipulation of the shape and a scalable number of control parameters. Additionally, the geometric parameters may serve as design optimization variables in cooperation with an external optimizer. The surface generation is based on the use of NURBS curves and surfaces, which provide the ability to produce complicated geometries with a relative small number of design variables. Standard or user-defined airfoil sections can be used for the wing generation. The surface description is compatible with international input/output standards; IGES and STEP formats are supported for the output files. Consequently, Ge.P.A.S. can serve as a preprocessor for other software packages, which may be used in order to refine the geometry or to generate the grid for numerical simulations. The geometric algorithms, the software features and its basic characteristics are presented in this paper, along with a demonstration of its abilities in sample aircraft configurations. © 2006 Elsevier Ltd. All rights reserved.},
author_keywords={Aircraft design;  NURBS surfaces;  Surface generation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Eldin2006,
author={Eldin, N. and Schilling, L.},
title={Integration of cost and scheduling data: A framework for an automated project control procedure},
journal={Proceedings of the 5th International Conference on Engineering Computational Technology},
year={2006},
page_count={16},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858683385&partnerID=40&md5=f0e7bca3378ed15ee61f6471fc682391},
abstract={This study provides a computer procedure that utilizes the work breakdown structure and earned value concepts to quantify work progress on construction projects and to integrate cost and scheduling data. Two commonly available computer packages were used in developing the procedure. Primavera Project Planner® was selected as the scheduling software and Microsoft® ACCESS was chosen as the database platform. A numerical example is provided to illustrate the procedural steps and output. The payoff of this study includes providing accurate and objective measurement of work progress, which should minimize the opportunities for legal disputes and litigations resulting from disagreements regarding the percentage of completion of construction projects. The procedure also provides an automated process for the routine updates of construction schedules, which should relieve project management teams of such a repetitive mundane task. © 2006 Civil-Comp Press.},
author_keywords={Computer applications in CM;  Cost analysis systems;  Cost-schedule integration;  Database applications;  Project controls;  Project management},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shettar2006328,
author={Shettar, R. and Banakar, Dr.R.M. and Nataraj, Dr.P.S.V.},
title={Design and implementation of interval arithmetic algorithms},
journal={1st International Conference on Industrial and Information Systems, ICIIS 2006},
year={2006},
pages={328-331},
doi={10.1109/ICIINFS.2006.347173},
art_number={4155202},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249165787&doi=10.1109%2fICIINFS.2006.347173&partnerID=40&md5=14bf427ea27702d7a197773b016159e6},
abstract={Interval arithmetic provides an efficient method for monitoring and controlling errors in numerical calculations and can be used to solve problems that cannot be efficiently solved with floating-point arithmetic. However, existing software packages for interval arithmetic are often too slow for numerically intensive calculations. While conventional floating point arithmetic is provided by fast hardware, interval arithmetic is simulated with software routines based on integer arithmetic. Therefore, the hardware design for interval arithmetic can provide a significant performance improvement over software implementations of interval arithmetic. In this paper, we design and implement interval arithmetic algorithms. The proposed method performs interval intersection, hull, minimum, maximum, and comparisons, as well as floating-point minimum, maximum and comparisons. Compared to the corresponding Forte Fortran 95 interval operations, about 80% reduction in the number of instructions is obtained with the proposed method. In short, it is found to greatly speedup the interval operations of hull, intersection, maximum, minimum, and comparisons over the corresponding Forte Fortran 95 interval operations. ©2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DelCorso2006,
author={Del Corso, D. and Forno, L. and Morrone, G. and Signorile, I.},
title={Development of didactic design guidelines for the production of e-courses},
journal={Proceedings - Frontiers in Education Conference, FIE},
year={2006},
doi={10.1109/FIE.2006.322588},
art_number={4117084},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749128053&doi=10.1109%2fFIE.2006.322588&partnerID=40&md5=fafcbe4c3a17908267be9eca680c5c70},
abstract={Several software packages make it easy to add voice and video to the same slides used for face-to-face lessons. This is not the best solution for e-courses, since the different delivery context require specific pedagogical redesign. The paper describes the procedures for design and development of courses distributed over the internet to remote and presence students developed at Politecnico di Torino, focusing on the "pedagogical instructions" provided to authors. A "didactical design manual" describes course organization, hierarchy, granularity of learning units and other information that must be provided to the learner. A "production manual" defines roles of people and the rules for production of support material, taking into account SCORM compliance. This methodology can be used with any tool able to integrate and synchronize slides and audio. Courses designed and developed with the described procedure have replaced previous courses in videotapes, with positive feedback from students. © 2006 IEEE.},
author_keywords={Course design;  E-courses;  E-learning;  SCORM;  Standards for learning technology},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Blumrich20063,
author={Blumrich, M. and Chen, D. and Chiu, G.L.-T. and Cipolla, T. and Coteus, P. and Crumley, P. and Gara, A. and Giampapa, M.E. and Hall, S. and Haring, R.A. and Heidelberger, P. and Hoenicke, D. and Kopcsay, G.V. and Liebsch, T.A. and Mok, L. and Ohmacht, M. and Salapura, V. and Swetz, R. and Takken, T. and Vranas, P.},
title={A holistic approach to system reliability in blue gene},
journal={Proceedings of the Innovative Architecture for Future Generation High-Performance Processors and Systems},
year={2006},
pages={3-9},
doi={10.1109/IWIAS.2006.22},
art_number={4089351},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449130612&doi=10.1109%2fIWIAS.2006.22&partnerID=40&md5=20e9fb54f39082c3399bd4b0764298a6},
abstract={Optimizing supercomputer performance requires a balance between objectives for processor performance, network performance, power delivery and cooling, cost and reliability. In particular, scaling a system to a large number of processors poses challenges for reliability, availability and serviceability. Given the power and thermal constraints of data centers, the BlueGene/L supercomputer has been designed with a focus on maximizing floating point operations per second per Watt (FLOPS/Watt). This results in a drastic reduction in FLOPS/m2 floor space and FLOPS/dollar, allowing for affordable scale-up. The BlueGene/L system has been scaled to a total of 65,536 compute nodes in 64 racks. A system approach was used to minimize power at all levels, from the processor to the cooling plant. A Blue-Gene/L compute node consists of a single ASIC and associated memory. The ASIC integrates all system functions including processors, the memory subsystem and communication, thereby minimizing chip count, interfaces, and power dissipation. As the number of components increases, even a low failure rate per-component will lead to an unacceptable system failure rate. Additional mechanisms will have to be deployed to achieve sufficient reliability at the system level. In particular, the data transfer volume in the communication networks of a massively parallel system poses significant challenges on bit error rates and recovery mechanisms in the communication links. Low power dissipation and high performance, along with reliability, availability and serviceability were prime considerations in BlueGene/L hardware architecture, system design, and packaging. A high-performance software stack, consisting of operating system services, compilers, libraries and middleware, completes the system, while enhancing reliability and data integrity. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lungu2006185,
author={Lungu, M. and Lanza, M. and Gîrba, T.},
title={Package patterns for visual architecture recovery},
journal={Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR},
year={2006},
pages={185-194},
doi={10.1109/CSMR.2006.39},
art_number={1602370},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46149117055&doi=10.1109%2fCSMR.2006.39&partnerID=40&md5=bcaded29c996d4783d76d7cd5550786f},
abstract={Recovering the architecture is the first step towards reengineering a software system. Many reverse engineering tools use top-down exploration as a way of providing a visual and interactive process for architecture recovery. During the exploration process, the user navigates through various views on the system by choosing from several exploration operations. Although some sequences of these operations lead to views which, from the architectural point of view, are mode relevant than others, current tools do not provide a way of predicting which exploration paths are worth taking and which are not. In this article we propose a set of package patterns which are used for augmenting the exploration process with information about the worthiness of the various exploration paths. The patterns are defined based on the internal package structure and on the relationships between the package and the other packages in the system. To validate our approach, we verify the relevance of the proposed patterns for real-world systems by analyzing their frequency of occurrence in six open-source software projects. © 2006 IEEE.},
author_keywords={Architecture recovery;  Program comprehension;  Reverse engineering;  Software exploration;  Visualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2006,
title={2006 IEEE Dallas ICAS Workshop on Design, Applications, Integration and Software, DCAS-06},
journal={2006 IEEE Dallas/CAS Workshop onDesign, Applications, Integration and Software, DCAS-06},
year={2006},
page_count={159},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46149092279&partnerID=40&md5=12a3adea2734e51ba19277b78f76eaec},
abstract={The proceedings contain 29 papers. The topics discussed include: power-supply noise attributed timing jitter in nonoverlapping clock generation circuits; optimal gate size selection for standard cells in a library; exact Toffoli newtowk synthesis of reversible logic using Boolean satisfiability; a built-in tester modulation noise in a wireless transmitter; efficient procedures for analyzing large-scale RF circuits; feedforward interference cancellation in narrow-band receivers; a fixed-point implementation for QR decomposition; design methodology of on-chip power distribution network; phase noise reduction in high frequency divider; noise analysis of time-to-digital converter in all digital PLLs; an approach to interference detection for ultra wideband radio systems; dynamic multi-point rational interpolation for frequency-selective model order reduction; and novel voltage-mode structurely allpass filters without external passive components.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Oltean200690,
author={Oltean, G. and Sipos, E.},
title={Simulink implementation of fuzzy functional models for analog modules},
journal={2006 IEEE International Conference on Automation, Quality and Testing, Robotics, AQTR},
year={2006},
pages={90-95},
doi={10.1109/AQTR.2006.254504},
art_number={4022826},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-45249116812&doi=10.1109%2fAQTR.2006.254504&partnerID=40&md5=8c27c5a4925e78796949aa276c5c178b},
abstract={The CAD of mixed electronic circuits asks for high level functional models of analog modules for efficient simulation. Being universal approximators, fuzzy systems can be successfully involved in this kind of models. Because Simulink has become a widely used software package in academia and industry for modeling and simulating dynamic systems, we chose it for implementation of our functional models. The models, based on two fuzzy logic systems describe the input voltage-output voltage relation in terms of frequency and phase shift and include also the temperature effect. The experimental results provided by the fuzzy functional model of a CMOS transconductance amplifier prove a high modeling accuracy. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{AllenMichalski2006145,
author={Allen Michalski, E. and Buell, D.A.},
title={A scalable architecture for RSA cryptography on large FPGAS},
journal={Proceedings - 2006 International Conference on Field Programmable Logic and Applications, FPL},
year={2006},
pages={145-152},
doi={10.1109/FPL.2006.311207},
art_number={4100969},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049009194&doi=10.1109%2fFPL.2006.311207&partnerID=40&md5=f6fe58944e055aa5eee1abbd5494c4cd},
abstract={The RSA algorithm is the standard for public-key cryptography today, with Montgomery multiplication the most common mechanism of implementation due to modulo operations using a bitwise shift in place of a division operation. Several Montgomery designs have been proposed for ASIC and FPGA implementation based on limited resource availability to satisfy the computational burden. FPGAs are now available that have large configurable logic resources in addition to dedicated high-speed ALU logic for operations such as multiplication. This paper describes an improvement to a limited resource Montgomery multiplier design, the MWR2MM algorithm proposed by Tenca and Koc, which is suitable for implementation on large FPGAs. The design can be scaled to utilize available FPGA multipliers, CLB logic and frequencies of operation. Implementation and design choices are discussed for an RSA core based on this design, and a comparison against the OpenSSL open source cryptographic library is given. Our results show a 1024-bit RSA core on a 100MHz Virtex2 Pro 100 FPGA platform to be 3.13x faster than an equivalent software implementation on a 2.8 GHz Intel Xeon PC workstation. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Allen2006307,
author={Allen, J.N. and Abdel-Aty-Zohdy, H.S. and Ewing, R.L.},
title={Introducing RapidHDL: A new library to design FPGA hardware in Microsoft.Net and automatically generate verilog netlists},
journal={2006 IEEE International Conference on Electro Information Technology},
year={2006},
pages={307-312},
doi={10.1109/EIT.2006.252153},
art_number={4017710},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250894291&doi=10.1109%2fEIT.2006.252153&partnerID=40&md5=ed26034a522b606f1fb4c744c9c6488b},
abstract={RapidHDL is introduced as a new object oriented hardware description library for C# developers. RapidHDL seeks to speed up FPGA development by applying best practices used in software engineering to increase productivity. Logic is rapidly defined in component classes using a structure of truth-table definitions, sink nodes, source nodes, and pass-through nodes. Hardware simulation co-runs with C# programs, and a linked-list of clock events simulates propagation delays. A standardized testing framework allows the developer to write test benches, hardware, and software in a single language. Algorithms are presented to automatically transform RapidHDL objects to Verilog netlist that can be synthesized by calling 3rd party tools.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bishop2006751,
author={Bishop, J.},
title={Multi-platform user interface construction-a challenge for software engineering-in-the-small},
journal={Proceedings - International Conference on Software Engineering},
year={2006},
volume={2006},
pages={751-760},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247188976&partnerID=40&md5=1631e74b4eb8ad423dd77fcff310b59e},
abstract={The popular view of software engineering focuses on managing teams of people to produce large systems. This paper addresses a different angle of software engineering, that of development for re-use and portability. We consider how an essential part of most software products - the user interface - can be successfully engineered so that it can be portable across multiple platforms and on multiple devices. Our research has identified the structure of the problem domain, and we have filled in some of the answers. We investigate promising solutions from the model-driven frameworks of the 1990s, to modern XML-based specification notations (Views, XUL, XIML, XAML), multi-platform toolkits (Qt and Gtk), and our new work, Mirrors which pioneers reflective libraries. The methodology on which Views and Mirrors is based enables existing GUI libraries to be transported to new operating systems. The paper also identifies cross-cutting challenges related to education, standardization and the impact of mobile and tangible devices on the future design of UIs. This paper seeks to position user interface construction as an important challenge in software engineering, worthy of ongoing research. Copyright 2006 ACM.},
author_keywords={.NET;  Graphical user interfaces;  GUI library reuse;  Mirrors;  Mobile devices;  Platform independence;  Portability;  Reflection;  Tangible user interfaces;  Views;  XAML;  XUL},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Krishna2006205,
author={Krishna, A.S. and Gokhale, A.S. and Schmidt, D.C.},
title={Context-specific middleware specialization techniques for optimizing software product-line architectures},
journal={Operating Systems Review (ACM)},
year={2006},
volume={40},
number={4},
pages={205-218},
doi={10.1145/1218063.1217955},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101106690&doi=10.1145%2f1218063.1217955&partnerID=40&md5=747788a2175f94281881a25e2c7e0ef5},
abstract={Product-line architectures (PLAs) are an emerging paradigm for developing software families for distributed real-Time and embedded (DRE) systems by customizing reusable artifacts, rather than hand-crafting software from scratch. To reduce the effort of developing software PLAs and product variants for DRE systems, developers are applying general-purpose-ideally standard-middleware platforms whose reusable services and mechanisms support a range of application quality of service (QoS) requirements, such as low latency and jitter. The generality and flexibility of standard middleware, however, often results in excessive time/space overhead for DRE systems, due to lack of optimizations tailored to meet the specific QoS requirements of different product variants in a PLA.This paper provides the following contributions to the study of middleware specialization techniques for PLA-based DRE systems. First, we identify key dimensions of generality in standard middleware stemming from framework implementations, deployment platforms, and middleware standards. Second, we illustrate how context-specific specialization techniques can be automated and used to tailor standard middleware to better meet the QoS needs of different PLA product variants. Third, we quantify the benefits of applying automated tools to specialize a standard Realtime CORBA middleware implementation. When applied together, these middleware specializations improved our application product variant throughput by ∼65%, average-and worst-case end-To-end latency measures by ∼43% and ∼45%, respectively, and predictability by a factor of two over an already optimized middleware implementation, with little or no effect on portability, standard middleware APIs, or application software implementations, and interoperability. © 2006 Authors.},
author_keywords={middleware;  product lines;  specializations},
document_type={Article},
source={Scopus},
}

@ARTICLE{Frank2006107,
author={Frank, H.F.},
title={Some considerations when selecting digital library software},
journal={OCLC Systems and Services},
year={2006},
volume={22},
number={2},
pages={107-110},
doi={10.1108/10650750610663987},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745103426&doi=10.1108%2f10650750610663987&partnerID=40&md5=bc38fd901ae263b0d7f11afea91be343},
abstract={Purpose - The purpose of this paper is to develop an understanding of the aspects of system software selection for digital library system projects. Design/methodology/approach - This article outlines the aspects of system software selection giving particular emphasis to those points where a digital library project differs from a traditional enterprise-level software selection process. Findings - Digital library system software selection differs in some significant ways from traditional software implementations. In particular, security and authentication issues, long-term cost and maintenance considerations, vendor viability, as well as training and documentation are areas where the software selection team needs to devote greater attention if the project is to be successful. Originality/value - This paper fills a gap in the digital project management literature by helping project managers understand how the software selection process for digital library systems projects differs from the traditional process used in software selection. © Emerald Group Publishing Limited.},
author_keywords={Computer software;  Digital libraries;  Selection;  Training},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alba2006415,
author={Alba, E. and Almeida, F. and Blesa, M. and Cotta, C. and Díaz, M. and Dorta, I. and Gabarró, J. and León, C. and Luque, G. and Petit, J. and Rodríguez, C. and Rojas, A. and Xhafa, F.},
title={Efficient parallel LAN/WAN algorithms for optimization. The mallba project},
journal={Parallel Computing},
year={2006},
volume={32},
number={5-6},
pages={415-440},
doi={10.1016/j.parco.2006.06.007},
note={cited By 56},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748419805&doi=10.1016%2fj.parco.2006.06.007&partnerID=40&md5=47f066cef8136983984d1ee88a44a7ba},
abstract={The mallba project tackles the resolution of combinatorial optimization problems using generic algorithmic skeletons implemented in C++. A skeleton in the mallba library implements an optimization method in one of the three families of generic optimization techniques offered: exact, heuristic and hybrid. Moreover, for each of those methods, mallba provides three different implementations: sequential, parallel for Local Area Networks, and parallel for Wide Area Networks. This paper introduces the architecture of the mallba library, details some of the implemented skeletons, and offers computational results for some classical optimization problems to show the viability of our library. Among other conclusions, we claim that the design used to develop the optimization techniques included in the library is generic and efficient at the same time. © 2006 Elsevier B.V. All rights reserved.},
author_keywords={Combinatorial optimization;  Exact techniques;  Hybridization;  Local and wide area implementations;  mallba library;  Metaheuristics;  Parallel algorithms;  Software engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kettil2006225,
author={Kettil, P. and Wiberg, N.-E.},
title={Development of software for structural analysis adapted to construction engineering applications},
journal={Engineering with Computers},
year={2006},
volume={21},
number={3},
pages={225-236},
doi={10.1007/s00366-005-0007-5},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645733788&doi=10.1007%2fs00366-005-0007-5&partnerID=40&md5=09966faa84dc0c8e53bedb5245e32567},
abstract={The paper presents the development and application of software for structural analysis adapted to construction engineering applications. The software consists of data structures, application subroutine libraries and the built-in Matlab subroutine library. For each application, the user writes an application-specific main program using the data structures and routines in the subroutine libraries. This gives the user flexibility to build a main program that is adapted to the specific needs of the current application. The software has been tested in a number of real-world projects, e.g. tunnels and bridges. The examples show that the software is a useful design tool for design work. © Springer-Verlag London Limited 2005.},
author_keywords={Design;  FEM;  Software;  Structural analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Petrini200655,
author={Petrini, F. and Nieplocha, J. and Tipparaju, V.},
title={SFT: Scalable Fault Tolerance},
journal={Operating Systems Review (ACM)},
year={2006},
volume={40},
number={2},
pages={55-62},
doi={10.1145/1131322.1131336},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646403728&doi=10.1145%2f1131322.1131336&partnerID=40&md5=e56f452ba6700063c0b79ea0a94d5c4b},
abstract={In this paper we will present a new technology that we are currently developing within the SFT: Scalable Fault Tolerance FastOS project which seeks to implement fault tolerance at the operating system level. Major design goals include dynamic reallocation of resources to allow continuing execution in the presence of hardware failures, very high scalability, high efficiency (low overhead), and transparency - requiring no changes to user applications. Our technology is based on a global coordination mechanism, that enforces transparent recovery lines in the system, and TICK, a lightweight, incremental checkpointing software architecture implemented as a Linux kernel module. TICK is completely user-transparent and does not require any changes to user code or system libraries; it is highly responsive: an interrupt, such as a timer interrupt, can trigger a checkpoint in as little as 2.5μs; and it supports incremental and full checkpoints with minimal overhead - less than 6% with full checkpointing to disk performed as frequently as once per minute.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Penedo2006354,
author={Penedo, M. and Lado, M.J. and Tahoces, P.G. and Souto, M. and Vidal, J.J.},
title={Effects of JPEG2000 data compression on an automated system for detecting clustered microcalcifications in digital mammograms},
journal={IEEE Transactions on Information Technology in Biomedicine},
year={2006},
volume={10},
number={2},
pages={354-361},
doi={10.1109/TITB.2005.864381},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645713747&doi=10.1109%2fTITB.2005.864381&partnerID=40&md5=6522b030092022c798ff286dc052b9b3},
abstract={The functionalities of the JPEG2000 standard have led to its incorporation into digital imaging and communications in medicine (DICOM), which makes this compression method available for medical systems. In this study, we evaluated the compression of mammographic images with JPEG2000 (16:1, 20:1, 40:1, 60.4:1, 80:1, and 106:1) for applications with a computer-aided detection (CAD) system for clusters of microcalcifications. Jackknife free-response receiver operating characteristic (JAFROC) analysis indicated that differences in the detection of clusters of microcalcifications were not statistically significant for uncompressed versus 16:1 (T = -0.7780; p = 0.4370), 20:1 (T = 1.0361; p = 0.3007), and 40:1 (T = 1.6966; p = 0.0904); and statistically significant for uncompressed versus 60.4:1 (T = 5.8883; p < 0.008), 80:1 (T= 7.8414; p < 0.008), and 106:1 (T = 17.5034; p = < 0.008). Although there is a small difference in peak signal-to-noise ratio (PSNR) between compression ratios, the true-positive (TP) and false-positive (FP) rates, and the free-response receiver operating characteristic (FROC), figure of merit values considerably decreased from a 60:1 compression ratio. The performance of the CAD system is significantly reduced when using images compressed at ratios greater than 40:1 with JPEG2000 compared to uncompressed images. Mammographic images compressed up to 20:1 provide a percentage of correct detections by our CAD system similar to uncompressed images, regardless of the characteristics of the cluster. Further investigation is required to determine how JPEG2000 affects the detectability of clusters of microcalcifications as a function of their characteristics. © 2006 IEEE.},
author_keywords={Computer-aided detection (CAD);  Digital mammography;  Image compression;  JPEG2000},
document_type={Article},
source={Scopus},
}

@ARTICLE{NoAuthor20061,
title={8th International Conference on Information and Communications Security, ICICS 2006},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4307 LNCS},
pages={1-555},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025696443&partnerID=40&md5=4e44f39989bee6f59824a61ae7470188},
abstract={The proceedings contain 39 papers. The special focus in this conference is on Information and Communications Security. The topics include: Strong and robust RFID authentication enabling perfect ownership transfer; a robust and secure RFID-based pedigree system; a topological condition for solving fair exchange in byzantine environments; an identity based proxy signature scheme from pairings; finding compact reliable broadcast in unknown fixed-identity networks; formal analysis and systematic construction of two factor authentication scheme; hierarchical key assignment for black-box tracing with efficient ciphertext size; trace-driven cache attacks on AES; a construction for general and efficient oblivious commitment based envelope protocols; defining and measuring policy coverage in testing access control policies; distributed credential chain discovery in trust management with parameterized roles and constraints; an operating system design for the security architecture for microprocessors; efficient protocols for privacy preserving matching against distributed datasets; quantifying information leakage in tree-based hash protocols; an anonymous authentication scheme for identification card; a wireless covert channel on smart cards; from proxy encryption primitives to a deployable secure-mailing-list solution; an independent function-parallel firewall architecture for high-speed networks; estimating accuracy of mobile-masquerader detection using worst-case and best-case scenario; provably correct runtime enforcement of non-interference properties; an attack on SMC-based software protection; modular behavior profiles in systems with shared libraries; efficient protection against heap-based buffer overflows without resorting to magic; cryptanalysis of timestamp based password authentication schemes using smart cards and the fairness of perfect concurrent signatures.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Song2006227,
author={Song, H. and Zhou, F. and Li, Y. and Jia, L.},
title={The design and implementation of middleware-based and VR-based software framework for distributed industrial robot application in train maintenance},
journal={2006 IEEE International Conference on Robotics and Biomimetics, ROBIO 2006},
year={2006},
pages={227-232},
doi={10.1109/ROBIO.2006.340380},
art_number={4141869},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249124825&doi=10.1109%2fROBIO.2006.340380&partnerID=40&md5=1d8ed58deb99a8c95e528780a956091a},
abstract={The paper's work is to develop an extensible, scalable and portable software framework providing specific solutions for train maintenance using industrial robots, The core of this framework is C++CORBA, it serves as main bone for communication, scheduling and computing, C++ and Java RMI serves in client side for comprehensive management applications, OpenGL is adopted to help robot's design, job simulation and remote surveillance. Then the paper expatiates at length on the framework's design and implementaion based on the testbed including two Motoman UP6 manipulators and one Pioneer mobile robot, the framework employs two-tier server and the considerations are given. Then specifications for the framework and the system's CORBA IDL are described and explained. Then a virtual reality oriented C++ client application using OpenGL is illustrated, focus is laid on the robot's design, simulation and remote surveillance by means of OpenGL. Then the paper shows how train maintenance job is simulated and managed by client application running within this framework. Finally, concluding remarks for future works are given. ©2006 IEEE.},
author_keywords={Middleware;  Robot;  Virtual reality},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mahboubi200667,
author={Mahboubi, Z. and Clarke, S.},
title={.NET API wrapping for existing C++ haptic APIs},
journal={Proceedings of the 2006 IEEE International Workshop on Haptic Audio Visual Environments and Their Applications, HAVE 2006},
year={2006},
pages={67-71},
doi={10.1109/HAVE.2006.283806},
art_number={4062552},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749098994&doi=10.1109%2fHAVE.2006.283806&partnerID=40&md5=96c9cd704b9f00169b668f16fcb898b6},
abstract={For a long time Haptic devices were expensive and therefore only accessible to a specialized community. But with companies like Novint Technologies introducing a peripheral intended to sell for about US$100, Haptic devices can be expected to be affordable for a wider public. But considering that most Haptic APIs are in C++, a language intended for expert programmers, novice programmers wanting to program haptic devices would face a steep learning curve. However, if the APIs were to be usable from within the .NET Framework, it would allow the more novice users to program using over 20 programming languages and extensive programming solutions and therefore they would be able to easily and efficiently develop software with haptic capabilities. This paper presents a set of guidelines for a design architecture that would allow migrating an existing C++ API to the .NET Framework without having to rewrite it from scratch. The presented architecture was implemented by wrapping the Sensable Ghost SDK 3.0. It was then used in both software and hardware based scenarios. ©2006 IEEE.},
author_keywords={API;  Haptics;  Managed wrappers;  Microsoft .NET framework;  Sensable ghost SDK 3.0},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Renault2006519,
author={Renault, L. and Chou, H.-T. and Chiu, P.-L. and Hill, R.M. and Zeng, X. and Gipson, B. and Zhang, Z.Y. and Cheng, A. and Unger, V. and Stahlberg, H.},
title={Milestones in electron crystallography},
journal={Journal of Computer-Aided Molecular Design},
year={2006},
volume={20},
number={7-8},
pages={519-527},
doi={10.1007/s10822-006-9075-x},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248554933&doi=10.1007%2fs10822-006-9075-x&partnerID=40&md5=1732db79b683469bde2e196c638d38d5},
abstract={Electron crystallography determines the structure of membrane embedded proteins in the two-dimensionally crystallized state by cryo-transmission electron microscopy imaging and computer structure reconstruction. Milestones on the path to the structure are high-level expression, purification of functional protein, reconstitution into two-dimensional lipid membrane crystals, high-resolution imaging, and structure determination by computer image processing. Here we review the current state of these methods. We also created an Internet information exchange platform for electron crystallography, where guidelines for imaging and data processing method are maintained. The server (http://2dx.org) provides the electron crystallography community with a central information exchange platform, which is structured in blog and Wiki form, allowing visitors to add comments or discussions. It currently offers a detailed step-by-step introduction to image processing with the MRC software program. The server is also a repository for the 2dx software package, a user-friendly image processing system for 2D membrane protein crystals. © Springer Science+Business Media, LLC 2006.},
author_keywords={2D crystallization;  2dx;  Electron crystallography;  Electron microscopy;  Image processing;  Membrane proteins},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mizutani20063197,
author={Mizutani, K. and Sakaguchi, K. and Takada, J.-I. and Araki, K.},
title={Development of MIMO-SDR platform and its application to real-time channel measurements},
journal={IEICE Transactions on Communications},
year={2006},
volume={E89-B},
number={12},
pages={3197-3207},
doi={10.1093/ietcom/e89-b.12.3197},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845594510&doi=10.1093%2fietcom%2fe89-b.12.3197&partnerID=40&md5=ef4bc03afb0531c980e6e7a782f8c7fd},
abstract={A multiple-input multiple-output software defined radio (MIMO-SDR) platform was developed for implementation of MIMO transmission and propagation measurement systems. This platform consists of multiple functional boards for baseband (BB) digital signal processing and frequency conversion of 5 GHz-band radio frequency (RF) signals. The BB boards have capability of arbitrary system implementation by rewriting software on reconfigurable devices such as field programmable gate arrays (FPGAs) and digital signal processors (DSPs). The MIMO-SDR platform employs hybrid implementation architecture by taking advantages of FPGA, DSP, and CPU, where functional blocks with the needs for real-time processing are implemented on the FPGAs/DSPs, and other blocks are processed off-line on the CPU. In order to realize the hybrid implementation, driver software was developed as an application program interface (API) of the MIMO-SDR platform. In this paper, hardware architecture of the developed MIMO-SDR platform and its software implementation architecture are explained. As an application example, implementation of a real-time MIMO channel measurement system and initial measurement results are presented. Copyright © 2006 The Institute of Electronics, Information and Communication Engineers.},
author_keywords={Hybrid implementation architecture;  Multiple-input multiple-output (MIMO);  Real-time channel measurement;  Software defined radio (SDR)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Santoro20065,
author={Santoro, A. and Quaglia, F.},
title={Transparent State Management for Optimistic Synchronization in the High Level Architecture},
journal={Simulation},
year={2006},
volume={82},
number={1},
pages={5-20},
doi={10.1177/0037549706065350},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646404057&doi=10.1177%2f0037549706065350&partnerID=40&md5=b8abf90048e447e3c4353bdccbc91eed},
abstract={In this article, the authors present the design and implementation of a software architecture—namely, MAgic State Manager (MASM)—to be employed within a runtime infrastructure (RTI) in support of High Level Architecture (HLA) federations. MASM allows performing checkpointing/recovery of the federate state in a way completely transparent to the federate itself, thus providing the possibility of demanding to the RTI any task related to state management in optimistic synchronization. Different from existing proposals, through this approach, the federate programmer is required neither to supply modules for state management within the federate code nor to explicitly interface the federate code with existing, third-party checkpointing/recovery libraries. Hence, the federate programmer is completely relieved from the burden of facing state management issues. One major application of this proposal is the possibility to employ optimistic synchronization, even in case of federates originally designed for the conservative approach. This can provide a way of improving the simulation system performance in specific scenarios (e.g., in case of poor or zero lookahead within the federation). The authors elaborate on this issue by discussing on how to integrate MASM within the RTI to achieve such a synchronization objective. Some experimental results demonstrating limited runtime overhead introduced by MASM are also reported for two case studies—namely, an interconnection network simulation and a personal communication system simulation. © 2006, Sage Publications. All rights reserved.},
author_keywords={federated simulation systems;  HLA;  middleware;  transparent checkpointing/recovery},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2005,
title={28th International Convention on Telecommunications and Information, MIPRO 2005},
journal={MIPRO 2005 - 28th International Convention Proceedings: Telecommunications and Information},
year={2005},
volume={2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895161068&partnerID=40&md5=4ec22d65053a5a1082738e284f74b585},
abstract={The proceedings contain 49 papers. The special focus in this conference is on Telecommunications and Information. The topics include: Programska implementacija pravnog segmenta poslovnog procesa; Ethernet konvergencija i korporativne mreze; primjena upitnika o ergonomskim aspektima rada za radunalom; multimedia services support protocols for UMTS networks; open source tools for multimedia desktop conferencing and media streaming over IPv6; lntroduction of IPv6 in IMS; engine telephony server with SIP/SIP-T signalling; SIP proxy function as a part of IP softswitch solution; architecture of SIP load balancer; real-time management of firewalls for enabling SIP communication; implementacija VoIP usluge u postojedu fiksnu telekomunikacijsku mrezu; nadzor i odrzavanje ispravnog rada sustava IP telefoniie; end-to-end sender-based packet-loss recovery techniques for VoIP; integrating the software and documentation development processes; managing of software development projects using principles of work package concept; software retiability as one of the key attribute of quality model; architecture for testing a-GPS enabled mobile stations across wireless technologies; dynamic (re-) configuration expressed in RT-UML; modeliranje i simulacija procesa testiranja programskih proizvoda; open and convergent approach to next generation service delivery; methodology for telecommunication service component distribution; network options for carrier-class Ethernet services; classification of call set up problems in WCDMA networks; adapting layered video for transmission over lossy packet networks; slijedeca generacija pristupnih mreza - optidka bezidna mreza s integriranim uslugama; delivery of triple-play services in DSL access based broadband networks; public broadband access transport network; network performance and potential appliances of fixed SMS; optimal MAC packet size in wireless LAN; enabling and monitoring mobile services using over-the-air device management technology; signal degradation in direct broadcast satellites due to atmospheric conditions and coexistence of analog AM broadcast and digital DRM signal.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Kis2005380,
author={Kis, G. and László, Z. and Somogyi, C. and Sulyán, T.},
title={Metaprogramming architecture to support dynamically changing transformation use cases},
journal={WMSCI 2005 - The 9th World Multi-Conference on Systemics, Cybernetics and Informatics, Proceedings},
year={2005},
volume={4},
pages={380-385},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867342610&partnerID=40&md5=aabe05bcdd6cc8bf069ab00f0ddcddb5},
abstract={Meta-programming is generally based on a formal software model. The developer creates a model (analyzer) usually from source code, and then the modified model (transformator) is turned back into source code (generator). The formal model is determined by two factors: the source code of the software and the set of model transforming operations characterized by their use cases. An effective meta-programming system requires a tool, which automates the construction of analyzers and generators. The input of this tool, called "meta-generator", is a representation of a yielding between the grammar syntax of the programming language and the operations related to the elements of a meta-model. The meta-model defines model elements which reflect the use-cases of the model transformations demanded. The output of the meta-generator consists of three programs: the analyzer, the source generator and the transformation library template. This paper introduces the MOFCOM meta-programming architecture.},
author_keywords={C#;  Java;  Metaprogramming;  MOF},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Brutzman200563,
author={Brutzman, D. and Harney, J. and Blais, C.},
title={X3D fundamentals},
journal={Visualizing Information Using SVG and X3D},
year={2005},
pages={63-84},
doi={10.1007/1-84628-084-2_3},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650603640&doi=10.1007%2f1-84628-084-2_3&partnerID=40&md5=2ffdebe096efab4143b8e200ce6483ea},
abstract={This chapter will provide the reader with high-level exposure to the newly proposed Extensible 3D Graphics (X3D) specification. We will demonstrate basic scene construction and behaviour authoring. It is assumed that the reader has a basic understanding of the Extensible Markup Language (XML) to grasp more quickly the concepts introduced. 3D scene authoring experience is not necessary, but will aid the reader in understanding some of the concepts. Chapter 5 will build on concepts introduced in this chapter to show how the Java API,ECMAScript and other Application Program Interfaces (APIs) can be employed in the emerging XML-centric Semantic Web. X3D is an emerging software standard for defining various interactive Web-based 3D content that can be integrated with multimedia across a variety of hardware platforms. X3D is the next-generation International Standards Organization (ISO) standard for Web-based 3D graphics, extending the earlier Virtual Reality Modelling Language (VRML) and VRML97 standards. It improves upon these standards by adding new features that could not be supported easily in the early days of the lowspeed dial-up Internet.Additionally, it incorporates advanced applications programming interfaces, additional data encoding formats, stricter conformance enforcement and a componentized architecture allowing a modular approach extensible across the varieties of hardware to which content authors deploy in the modern Web. Since X3D is represented by the XML, it is intended to be a universal interchange format for integrated 3D graphics and multimedia (X3D ISO Specification).},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{NoAuthor2005,
title={9th Jornadas de Ingenieria del Software y Bases de Datos, JISBD'2004},
journal={IEEE Latin America Transactions},
year={2005},
volume={3},
number={1},
page_count={142},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149363508&partnerID=40&md5=a6db78fea876a16a43d2862188e2df02},
abstract={The proceedings contain 18 papers. The topics discussed include: modeling reusable web business processes from an aspect oriented perspective; text retrieval in the Galician digital library; concurrency and recovery in the Q-tree. transactional approach of a multidimensional index in a web application; using MDA to develop component and aspect based applications; a UML profile for designing secure data warehouses; analysis and visualization of scientific communities with information extracted from the web; an architecture to define graphical metaphors for metamodels; how to specify and validate dependability benchmarks for transactional systems; applying personal construct theory to requirements elicitation; implementing and improving the SEI risk management method in a university software project; modeling reusable web business processes from an aspect oriented perspective; and implementing direct and sequential access to data collections using aspects.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Farooq2005,
author={Farooq, A. and Braungarten, R. and Dumke, R.R.},
title={An empirical analysis of object-oriented metrics for Java technologies},
journal={2005 Pakistan Section Multitopic Conference, INMIC},
year={2005},
doi={10.1109/INMIC.2005.334410},
art_number={4133425},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249139753&doi=10.1109%2fINMIC.2005.334410&partnerID=40&md5=4a77ef6fe5c8f6023f476b5e46f3e4de},
abstract={As a fact, the application of object-oriented approach is of high significance in the area of software development since it can abet efficiency or cost effectiveness and reduce error probability during software design and implementation. In order to quantify, especially qualitative aspects such as potential error hot spots caused by elevated design complexity, software measurement can strongly assist. Particularly, metrics proposed by Chidamber and Kemerer as well as Abreu's MOOD metrics set are presumably most prevalent in practice and provide adequate explanatory power. Especially the object-oriented programming language Java cannot be dismissed from one's thoughts because a lot of Java libraries serve as foundation for contemporary applications. Thus, after initially defining language bindings for the aforementioned metrics, we perform measurement and evaluation of various Java standard libraries like J2SE, J2EE, J2ME, JWSDP and few others concerning different aspects. The results acquired are beneficial to be used by software designers for aligning and orienting their design with common industry practices. Furthermore, our extensive measurements enable us to carry out key metrics correlation studies incorporating many thousand Java classes.},
author_keywords={Chidamber and Kemerer Metrics;  Correlation;  Empirical analysis;  Java;  MOOD metrics;  Object-oriented design metrics;  Object-oriented software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gentile2005211,
author={Gentile, A. and Segreto, S. and Sorbello, F. and Vassallo, G. and Vitabile, S. and Vullo, V.},
title={CliffoSor, an innovative FPGA-based architecture for geometric algebra},
journal={Proceedings of the 2005 International Conference on Engineering of Reconfigurable Systems and Algorithms, ERSA'05},
year={2005},
pages={211-217},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-47749126837&partnerID=40&md5=3036012b47580e5f6c91c1ed56fae6fc},
abstract={Geometric objects representation and their transformations are the two key aspects in computer graphics applications. Traditionally, compute-intensive matrix calculations are involved to model and render 3D scenery. Geometric algebra (a.k.a. Clifford algebra) is gaining growing attention for its natural way to model geometric facts coupled with its being a powerful analytical tool for symbolic calculations. In this paper, the architecture of CliffoSor (Clifford Processor) is introduced. Cliff orSor is an embedded parallel coprocessing core that offers direct hardware support to Clifford algebra operators. A prototype implementation on an FPGA board is detailed. Initial test results show a 20x speedup for 3D vector rotations, a I2x speedup for Clifford sums and differences and more than 4x speedup for Clifford products against the analogous operations in GAIGEN, a standard geometric algebra library generator for general purpose processors.},
author_keywords={Clifford algebra;  Computer graphics;  Embedded coprocessor;  FPGA;  Parallel processing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Griewel2005119,
author={Griewel, A. and Rarey, M.},
title={From greedy to branch & bound and back: Assessing optimization strategies for incremental construction molecular docking tools},
journal={Proceedings of the German Conference on Bioinformatics, GCB 2005},
year={2005},
pages={119-130},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748646045&partnerID=40&md5=ea94b0c1e475aa0332e35084ecb4bc6f},
abstract={A branch & bound approach for the assembly-phase of the incremental construction algorithm of the software package FlexX is presented. For this a local bound for partial solutions has been implemented which estimates the best score achievable for the considered solutions. This estimation is based on scoring values which single components can achieve in certain regions of the active site as well as distance constraints deduced from the composition of the considered partial solution. Furthermore, a time-and space-bounded search strategy specific to the addressed problem has been developed. The implemented algorithm was tested on a dataset containing 169 complexes. In 116 of these cases the calculation was finished in a reasonably defined time frame while the calculation for the remaining complexes was not completed in this period of time. For all calculated complexes, the best solution shows a score better or equal to the solution of standard-FlexX. This, however, is not always associated with an improvement of the RMSD between the calculated placement of the ligand and the crystal structure. The presented algorithm is applicable for thorough virtual screening of small sets of ligands comprising up to nine rotatable, acyclic bonds. Furthermore, the method gives important insights to the k-greedy method and can be used for scientific assessment of new scoring functions within FlexX.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hwang200571,
author={Hwang, S.-M. and Kim, H.-M.},
title={A study on metrics for supporting the software process improvement based on SPICE},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2005},
volume={3647 LNCS},
pages={71-80},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745152702&partnerID=40&md5=be84638c07d2004d1eb1ea80c307f55c},
abstract={Software Process Improvement (SPI) is the set of activities with which an organization attempts to reach better performances on product cost, time-to-market and product quality, by improving the software development process. Changes are made to the process based on 'best practices': experiences of other, not necessarily similar organizations, Within SPI methodologies there is a focus on the software development process, because it is based on the as-sumption that an improved development process positively impacts product quality, productivity, product cost and time-to-market. This paper defines stan-dard metrics for quantitative measurement of quality indicators of processes through Software Process Assessment (SPA) based on SPICE. Through accom-plishment of this, we are able to control and to measure SPI activity, and pro-vide for a basis of quantitative S/W process management. The results of our re-search will represent a circulatory architecture for SPI and support of the risk management through the improvement activities and the Process Asset Library with collected and measured data. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Suleman200533,
author={Suleman, H. and Feng, K. and Mhlongo, S. and Omar, M.},
title={Flexing digital library systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2005},
volume={3815 LNCS},
pages={33-37},
doi={10.1007/11599517_4},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744911148&doi=10.1007%2f11599517_4&partnerID=40&md5=a067c5f7955e4fe1498b413a8fdc7edc},
abstract={Digital library systems with monolithic architectures are rapidly facing extinction as the discipline adopts new practices in software engineering, such as component-based architectures and Web Services. Past projects have attempted to demonstrate and justify the use of components through the construction of systems such as NCSTRL and ScholNet. This paper describes current work to push the boundaries of digital library research and investigate a range of projects made feasible by the availability of suitable components. These projects include: the ability to assemble component-based digital libraries using a visual interface; the design of customisable user interfaces and workflows; the packaging and installation of systems based on formal descriptions; and the shift to a component farm for cluster-like scalability. Each of these sub-projects makes a potential individual contribution to research in architectures, while sharing a common underlying framework. Together, all of these projects support the hypothesis that a consistent component architecture and suite of components can provide the basis for advanced research into flexible digital library architectures. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Beloglavec200520,
author={Beloglavec, S. and Heričko, M. and Jurič, M.B. and Rozman, I.},
title={Analysis of the limitations of multiple client handling in a java server environment},
journal={ACM SIGPLAN Notices},
year={2005},
volume={40},
number={4},
pages={20-28},
doi={10.1145/1064165.1064170},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744825109&doi=10.1145%2f1064165.1064170&partnerID=40&md5=acc141044f6790afbf51c145f2d4a9f7},
abstract={A server infrastructure in web servers, message servers and other parallel systems use a variation of two software architectures for providing concurrency: threaded or event-driven. This paper analyzes the performance limitations of concurrent applications implemented in Java. Both architectures have been evaluated and compared with various design patterns, which combine the best practices from both architectures. For each architecture the suitability for handling a large volume of client requests, the efficient management of a server load, the influence of client request structures, and the physical size of a client request, have been studied. The discussed Java APIs are core technologies for high-level APIs, used in developing web and distributed applications. The research also includes performance comparison on various platforms and discusses performance variation on various versions of a Java runtime. The paper contributes to the understanding of Java-based server architecture capabilities. Core server software architectures and required Java libraries are compared, the reasons for the limitations are identified and guidelines for choosing proper combinations are given.},
author_keywords={Event-Driven Server;  Java Networking;  Threaded Server},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2005,
title={ACM International Conference Proceeding Series},
journal={ACM International Conference Proceeding Series},
year={2005},
page_count={693},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056293969&partnerID=40&md5=fe4b853ad2b52aef372fc535f8abeb48},
abstract={The proceedings contain 59 papers. The topics discussed include: the challenges and realities of merging online banks; maintaining concentration to achieve task completion; the Goldilocks content framework: identifying just-right information; the paradox of the library: information architecture challenges in an interdisciplinary organization; improving color exploration and visualization on the ColorSmart by BEHR application; moving GM to #1 in online satisfaction: techniques for conducting quantitative benchmarking across hundreds of web sites and for prioritizing functionality based on user needs in differing geographies and markets; diamond search: improving the user experience of buying loose diamonds online; messageboard topic tagging: user tagging of collectively owned community content; guidelines are a tool: building a design knowledge management system for programmers; a process for incorporating heuristic evaluation into a software release; early and often: how to avoid the design revision death spiral; viewing visual web site design in context; design of a telescope control system interface; and gospel spectrum.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Gong20052123,
author={Gong, X. and Zhao, J. and Chen, L.},
title={Research and development of computer-aided engineering drawing software for heterogeneous CAD platforms},
journal={Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics},
year={2005},
volume={17},
number={9},
pages={2123-2128},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-25144468359&partnerID=40&md5=d2dd2ac476364d863cfa2c22fa804968},
abstract={The engineering drawings are important documents for industrial manufacture and technical exchange, but it is very difficult and tedious to make a standard drawing in 3D CAD systems. Through analyzing the functional shortage of engineering drawing module and well considering the differences of secondary development on different 3D platforms, the paper presents a general computer-aided engineering drawing software architecture for heterogeneous CAD systems based on ready-made software architecture and advanced development tools. Several key technologies are discussed in detail, including deployment of physical modules, construction of logical objects and maintenance of their instances, management of engineering constraints of attached entities which follow their host entities, access of product assembly model, etc. A software package based on the method mentioned above is developed and used in practice.},
author_keywords={Computer application;  Engineering drawing;  Secondary development;  Software architecture},
document_type={Article},
source={Scopus},
}

@ARTICLE{Perini2005319,
author={Perini, A. and Susi, A.},
title={Agent-oriented visual modeling and model validation for engineering distributed systems},
journal={Computer Systems Science and Engineering},
year={2005},
volume={20},
number={4},
pages={319-329},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644534294&partnerID=40&md5=f1f57010f3e9834d50d5cff2557d0692},
abstract={Agent-Oriented methodologies that have been recently proposed for engineering distributed systems tend to adopt a model-based approach to software development, that is they devise a development process based on the definition of a specific set of models for each steps in the analysis and in the software design phases. To be put into practice, this approach demands clear guidelines for building and refining models along the software development process, as well as flexible modeling tools which integrates automatic verification techniques at support of model validation. In this paper we describe a modeling environment which integrates an Agent-Oriented (AO) modeling tool with other tools, such as a model-checker for the verification of formal properties of the model and a library which implements graph transformation techniques which can be used to support model refinement and model transformations. In designing it we took into account recommendations from the OMG's Model-Driven Architecture initiative. We illustrate the modeling environment architecture, give details on the AO modeling tool and on the components that allows for the integration with other tools. Examples of how modeling and validation can be interleaved and supported by the modeling environment are given. © 2005 CRL Publishing Ltd.},
author_keywords={Distributed systems;  Model validation;  Visual modeling},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2005293,
author={Zhang, H. and Wu, W.},
title={Construction project expense management based on contractual relationships},
journal={Qinghua Daxue Xuebao/Journal of Tsinghua University},
year={2005},
volume={45},
number={3},
pages={293-296},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-17944381653&partnerID=40&md5=18fe263fd77d5e5e695e33f091cc64e0},
abstract={A construction project expense management and control software package was developed with advanced expense management expertise. This paper describes the contract-based structure of the expense management system and the planar expense structural matrix. The system incorporates modules for changes in expense management, expense control and analysis, and a construction project expense management system based on the FIDIC contract system. The construction project expense management system can be tightly linked with contract management and can provide expense control and analysis consistent with international management standards to improve construction project expense management.},
author_keywords={Divisional encode parallel table;  Expense management;  FIDIC-base;  Planar expense structural matrix},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yang2005148,
author={Yang, Y. and Qu, M. and Qian, L.},
title={Lethality test simulation system for conventional warhead based on numerical results},
journal={Asian Simulation Conference 2005, ASC 2005 / 6th International Conference on System Simulation and Scientific Computing, ICSC 2005},
year={2005},
pages={148-152},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902457327&partnerID=40&md5=bbe4c102a5c66de72b4570c63b095752},
abstract={The paper presents design and implementation of lethality test simulation system. The function of the system is analyzed based on lethality simulation method for conventional warhead [1]. The general frame and software structure of the system are established, the key technique is illustrated, and the implementation of the system is analyzed. On the base of the computer simulation, lethality test simulation system is achieved. Using the system, the characteristic parameters and lethality parameters are gained, numerical simulation of the whole process for fragment field formed, fragment movement, and fragment impact on target is achieved. For an aimable fragmentation warhead, a generic simulation example is given, and we analyze the powerful parameters that affect warhead lethality.},
author_keywords={Conventional warhead;  Lethality test simulation;  Numerical simulation;  OpenGL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2005,
title={8th Asia-Pacific Network Operations and Management Symposium, APNOMS 2005},
journal={APNOMS 2005 - 8th Asia-Pacific Network Operations and Management Symposium :Toward Managed Ubiquitous Information Society, Proceedings},
year={2005},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899083298&partnerID=40&md5=59e776449e4f8699ad7e6823dc5e035b},
abstract={The proceedings contain 75 papers. The special focus in this conference is on Network Operations and Management. The topics include: Challenge towards the ubiquitous network society; network operations challenges for ubiquitous converged network; the convergence of public policy and technology in the ubiquitous network environment; the current status and the future direction of DMB (digital multimedia broadcasting) technology; towards a European next generation mobile network; overview of RFID technologies for ubiquitous services; NGN focus group; ubiquitous visual communication; seamless mobility - a compelling blending of ubiquitous computing and autonomic computing; network operations visibility - real time dashboard that connects the field and business management; autonomic architecture to support next generation services; provider provisioned internet VPN for personal communication environment; flow-based application-aware internet traffic monitoring and field trial experiences; methodology of performance evaluation of integrated service systems with timeout control scheme; development of network operation knowledge system; ontology modeling in developing OSSs; dynamic traffic management focusing on users satisfaction in All-IP mobile environment; open API for QoS control in next generation networks; dynamic management of QoS parameters; a content transformation framework for personalization service; autonomic service architecture using virtual services; managing connectivity in wireless ad hoc networks; an efficient distributed security policy management for gateway-less VPNs; reducing service interaction in home network; ubiquitous data management by using word-of-mouth information; an evolvable software architecture for managing ubiquitous systems and devices; data sources in proactive network management; using mixed distribution for modeling end-to-end delay characteristics; a T-entropy analysis of the slammer worm outbreak; a real-time network traffic based worm detection system for enterprise networks; distributed measurement system for the end-to-end network performance using a dynamic job management; database design for performance data in integrated network management system; study on compatibility of diffusion-type flow control and TCP; design and implementation of BcN-NMS; transport network and resource management; a method of network provisioning on layer 2 networks; the realization of work management system based on 2-dimensional distributed data driven architecture; radio resource management under fixed-mobile convergence architecture; the scheduling model and algorithm for workforce management; high-performance intrusion detection in FPGA-based reconfiguring hardware; detection- and prevention-system of DNS query-based distributed denial-of-service attack; risk analysis in communication networks with conditional value-at-risk; application protocol based anomaly detection for high speed network; an SAML based SSO architecture for secure data exchange between user and OSS; a prototype of on demand and scheduled remote access VPN management system; leveraging an information model for user centric service fulfilment and assurance; construction of network monitoring screens using scalable vector graphics; a rule-set based internet application identification and analysis; message processing framework for an integrated NMS; detection of network traffic anomalies; bandwidth management system on Ethernet for applications without signaling networks; OSS provisioning interface for triple play service; web-based management design using CTEM (common type envelope machine); proposal for the policy-based 3rd party access management architecture; creation of configuration data management system based on a distributed data driven architecture and XML; basic design of ontological knowledge base to support network design by unskilled users; an interaction translation method for WBEM/SNMP gateway; management and construction issues for enterprise-wide IP telephony; cross-layering TCP congestion control mechanism in heterogeneous networks; introduction to service-oriented computing, SOA and service management; optical transport systems/networks and control by GMPLS; inter-domain traffic engineering for QoS-guaranteed diffserv provisioning; quality of service in heterogeneous networks and management of ubiquitous sensor network.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Alumbaugh2005485,
author={Alumbaugh, T.J. and Jiao, X.},
title={Compact array-based mesh data structures},
journal={Proceedings of the 14th International Meshing Roundtable, IMR 2005},
year={2005},
pages={485-503},
doi={10.1007/3-540-29090-7_29},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-69849107611&doi=10.1007%2f3-540-29090-7_29&partnerID=40&md5=38c76957d7214e7559de8651ebab56cc},
abstract={In this paper, we present simple and efficient array-based mesh data structures, including a compact representation of the half-edge data structure for surface meshes, and its generalization - a half-face data structure - for volume meshes. These array-based structures provide comprehensive and efficient support for querying incidence, adjacency, and boundary classification, but require substantially less memory than pointer-based mesh representations. In addition, they are easy to implement in traditional programming languages (such as in C or Fortran 90) and convenient to exchange across different software packages or different storage media. In a parallel setting, they also support partitioned meshes and hence are particularly appealing for large-scale scientific and engineering applications. We demonstrate the construction and usage of these data structures for various operations, and compare their space and time complexities with alternative structures. © 2005 Springer-Verlag Berlin Heidelberg.},
author_keywords={Half-edge;  Half-face;  Mesh data structures;  Parallel computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Koval2005237,
author={Koval, V.N. and Savyak, V.V. and Serhienko, I.V.},
title={SCIT - First supercomputer cluster in Ukraine, hardware architecture and software},
journal={Proceedings of the Third Workshop - 2005 IEEE Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2005},
year={2005},
pages={237-240},
doi={10.1109/IDAACS.2005.282977},
art_number={4062128},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43549117725&doi=10.1109%2fIDAACS.2005.282977&partnerID=40&md5=a006f43c5218356b20a838753356673a},
abstract={The paper describes a first supercomputer cluster project in Ukraine, its hardware architecture and a software. There are described main ideas used to choose an architecture, components and an analyzes of competing solutions. The paper shows the performance results received on systems that were built. There are also described software packages made for cluster users support and their task performance by our cluster team. ©2005 IEEE.},
author_keywords={High-performance cluster supercomputer architecture implementation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kootsey2005855,
author={Kootsey, J.M. and McAuley, G. and Bernal, J.},
title={Building interactive simulations in Web pages without programming},
journal={Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
year={2005},
volume={7 VOLS},
pages={855-858},
doi={10.1109/iembs.2005.1616550},
art_number={1616550},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846908095&doi=10.1109%2fiembs.2005.1616550&partnerID=40&md5=7f95fb388e11c25c6f138bd5fd077727},
abstract={A software system is described for building interactive simulations and other numerical calculations in Web pages. The system is based on a new Java-based software architecture named NumberLinX (NLX) that isolates each function required to build the simulation so that a library of reusable objects could be assembled. The NLX objects are integrated into a commercial Web design program for codingfree page construction. The model description is entered through a wizard-like utility program that also functions as a model editor. The complete system permits very rapid construction of interactive simulations without coding. A wide range of applications are possible with the system beyond interactive calculations, including remote data collection and processing and collaboration over a network. © 2005 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ma2005340,
author={Ma, J. and Yi, Y. and Tian, T. and Li, Y.},
title={Retrieving digital artifacts from digital libraries semantically},
journal={Lecture Notes in Computer Science},
year={2005},
volume={3644},
number={PART I},
pages={340-349},
doi={10.1007/11538059_36},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-27144478945&doi=10.1007%2f11538059_36&partnerID=40&md5=c67725f99f96382617aafb8611fba506},
abstract={The techniques for organizing and retrieving the artifacts from digital libraries (DLs) semantically are discussed, which include letting the taxonomies and semantic relations work in tandem to index the artifacts in DLs; integrating the techniques used in natural language processing and taxonomies to help users to start their retrieval processes; and ranking scientific papers on similarity in terms of contents or ranking the relevant papers on multi-factors. These techniques are verified through the design and implementation of a prototype of DLs for scientific paper management. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2005319,
author={Chen, T. and Yang, J. and Wang, Y. and Zhan, C. and Zang, Y. and Qin, J.},
title={Design of recombinant stem cell factor-macrophage colony stimulating factor fusion proteins and their biological activity in vitro},
journal={Journal of Computer-Aided Molecular Design},
year={2005},
volume={19},
number={5},
pages={319-328},
doi={10.1007/s10822-005-5686-x},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-26644439288&doi=10.1007%2fs10822-005-5686-x&partnerID=40&md5=0d5992798ea7f6544a3f855a334b4d63},
abstract={Stem cell factor (SCF) and macrophage colony stimulating factor (M-CSF) can act in synergistic way to promote the growth of mononuclear phagocytes. SCF-M-CSF fusion proteins were designed on the computer using the Homology and Biopolymer modules of the software packages InsightII. Several existing crystal structures were used as templates to generate models of the complexes of receptor with fusion protein. The structure rationality of the fusion protein incorporated a series of flexible linker peptide was analyzed on InsightII system. Then, a suitable peptide GGGGSGGGGSGG was chosen for the fusion protein. Two recombinant SCF-M-CSF fusion proteins were generated by construction of a plasmid in which the coding regions of human SCF (1-165aa) and M-CSF (1-149aa) cDNA were connected by this linker peptide coding sequence followed by subsequent expression in insect cell. The results of Western blot and activity analysis showed that these two recombinant fusion proteins existed as a dimer with a molecular weight of ∼84 KD under non-reducing conditions and a monomer of ∼42 KD at reducing condition. The results of cell proliferation assays showed that each fusion protein induced a dose-dependent proliferative response. At equimolar concentration, SCF/M-CSF was about 20 times more potent than the standard monomeric SCF in stimulating TF-1 cell line growth, while M-CSF/SCF was 10 times of monomeric SCF. No activity difference of M-CSF/SCF or SCF/M-CSF to M-CSF (at same molar) was found in stimulating the HL-60 cell linear growth. The synergistic effect of SCF and M-CSF moieties in the fusion proteins was demonstrated by the result of clonogenic assay performed with human bone mononuclear, in which both SCF/M-CSF and M-CSF/SCF induced much higher number of CFU-M than equimolar amount of SCF or M-CSF or that of two cytokines mixture. © Springer 2005.},
author_keywords={Computer-aided design;  Express in insect cell;  Fusion protein;  M-CSF;  SCF},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Czekster2005226,
author={Czekster, R.M. and De Souza, O.N.},
title={VIZ - A graphical open-source architecture for use in structural bioinformatics},
journal={Lecture Notes in Bioinformatics (Subseries of Lecture Notes in Computer Science)},
year={2005},
volume={3594},
pages={226-229},
doi={10.1007/11532323_29},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444459441&doi=10.1007%2f11532323_29&partnerID=40&md5=0a5d581cb0b9bf9dba002784d41ea8e0},
abstract={Protein structure visualization is crucial for understanding its function inside the cell. Each year, laboratories around the world deposit protein structures on a central database for further analysis and research. The result is a large amount of structures being deposited (approximately 31,000 in may 2005). Visualization is a very powerful tool to help in the analysis, aiding data understanding and interpretation. The present work suggests an architecture to help the rapid construction of visual biomolecular software, specifically designed to be simple, modular and scalable. The architecture, called VIZ, employs high quality open-source libraries offering simple data structures and customizable options. The architecture can be used to start a new visual software project to visualize and represent individual protein structures, as well as multiple conformations from molecular dynamics simulation trajectories. © Springer-Verlag Berlin Heidelberg 2005.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Talpin200433,
author={Talpin, J.-P. and Gamatié, A. and Berner, D. and Le Dez, B. and Le Guernic, P.},
title={Hard real-time implementation of embedded software in JAVA},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={2952},
pages={33-47},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048853501&partnerID=40&md5=7cfc81c78a1e8ac3500841facbb82c31},
abstract={The popular slogan "write once, run anywhere" effectively renders the expressive capabilities of the JAVA programming framework for developing, deploying, and reusing target-independent applets. Its generality and simplicity has driven most attention of the compiler technology community to developing just-in-time and runtime compilation techniques, local and compositional optimization algorithms. When it comes to real-time JAVA and to the implementation of embedded software, this approach is however far from satisfactory, especially in hard real-time system design (e.g. airborne systems) where conformance to real-time specifications is critical. We show that synchronous design tools, and particularly the design workbench POLYCHRONY, allow for a complete modeling of embedded software written in a high-level and general purpose programming language such as JAVA. The synchronous approach provides a formal engineering model and methodology, using global transformation and optimization techniques, that allow for a JAVA program written once to be mapped on any distributed target architecture. We present a technique to import a resource constrained, multithreaded, RT JAVA program, together with its runtime system API, into POLYCHRONY. We put this modeling technique to work by considering a formal, refinement-based, design methodology that allows for a correct by construction remapping of the initial threading architecture of a given JAVA program on either a single-threaded target or a distributed architecture. This technique allows to generate stand-alone (JVM-less) executables and to remap threads onto a given distributed architecture or a prescribed target real-time operating system. As a result, it allows for a complete separation between the virtual threading architecture of the functional-level system design (in JAVA) and its actual, real-time and resource constrained implementation. © Springer-Verlag Berlin Heidelberg 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Emerson20041357,
author={Emerson, M.J. and Sztipanovits, J. and Bapty, T.},
title={A MOF-based metamodeling environment},
journal={Journal of Universal Computer Science},
year={2004},
volume={10},
number={10},
pages={1357-1382},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-23844557795&partnerID=40&md5=7bcef0397ced93c2560327663dcc2de0},
abstract={The Meta Object Facility (MOF) forms one of the core standards of the Object Management Group's Model Driven Architecture. It has several use-cases, including as a repository service for storing abstract models used in distributed object-oriented software development, a development environment for generating CORBA IDL, and a metamodeling language for the rapid specification, construction, and management of domain-specific technology-neutral modeling languages. This paper will focus on the use of MOF as a metamodeling language and describe our latest work on changing the MIC metamodeling environment from UML/OCL to MOF. We have implemented a functional graphical metamodeling environment based on the MOF v1.4 standard using GME and GReAT. This implementation serves as a testament to the power of formally well-defined metamodeling and metamodel-based model transformation approaches.Furthermore, our work gave us an opportunity to evaluate sevaral important features of MOF v1.4 as a metamodeling language: Completeness of MOF v1.4 for defining the abstract syntax for complex (multiple aspect) DSML-s The Package concept for composing and reusing metamodels Facilities for modeling the mapping between the abstract and concrete syntax of DSML-s © J.UCS.},
author_keywords={Graph transformations;  Model Driven Architecture;  Model-Integrated Computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Melchior2004145,
author={Melchior, N.A. and Smart, W.D.},
title={A framework for robust mobile robot systems},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2004},
volume={5609},
pages={145-154},
doi={10.1117/12.571599},
art_number={16},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644400688&doi=10.1117%2f12.571599&partnerID=40&md5=ca40d0d5893e00db07b590112c1003a5},
abstract={Fielded mobile robot systems will inevitably suffer hardware and software failures. Failures in a single subsystem can often disable the entire robot, especially if the controlling application does not consider such failures. Often simple measures, such as a software restart or the use of a secondary sensor, can solve the problem. However, these fixes must generally be applied by a human expert, who might not be present in the field. In this paper, we describe a recovery-oriented framework for mobile robot applications which addresses this problem in two ways. First, fault isolation automatically provides graceful degradation of the overall system as individual software and hardware components fail. In addition, subsystems are monitored for known failure modes or aberrant behavior. The framework responds to detected or immanent failures by restarting or replacing the suspect component in a manner transparent to the application programmer and the robot's operator.},
author_keywords={Autonomic computing;  Fault tolerance;  Mobile robotics;  Recovery-oriented computing;  Software architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Danielyan2004198,
author={Danielyan, G.L.},
title={Multi channel fiber optic bundles and sensors for biomedical application},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2004},
volume={5566},
pages={198-203},
doi={10.1117/12.577574},
art_number={31},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-15944395472&doi=10.1117%2f12.577574&partnerID=40&md5=e122bf3d16ce0cf461af7608888b87da},
abstract={The Special Ordered Structures of Specialty Fiber included into Multifunctional and Multi Channel Fiber Optic Bundles (MFOB) and Sensors are proposed. Optimal construction of fiber optic channels in the MFOB exhibit reduced speckle noise and high intensity transmission resulting from spatial homogeneity and symmetry of radiation. Improved new type of the Fibers: Metal Coated Multimode, Special Plastic Coated, Fibers for UV-VIS, Fibers for VIS- NIR spectral Range, Fibers for NIR and IR spectral range. Hexagonal package of sensitive end of the MFOB structures designed with different type and fiber core diameters fibers are transferred into the different configured input/output optical channels. For fluorescence spectroscopy and FDT Diagnostic described optimal arrangement with 7- 256 Fibers included into MFOB structure. Remote spectroscopic Probes are used for "in Vivo" or "in Vitro" experimental devices. Sensors with MFOB probes bifurcated from two up to seven channels are used for process photometry and for mini- fiber spectrometric devices. Customized Software and flexible numerical simulations for data analysis are based into two levels of programming: - micro program part for ATMEL microprocessor, Visual C++ version 6.0 for PC computers with Windows -98-2000Me Programs. Advanced Applications of MFOB type of probes show some features for Biomedical Remote Sensing Systems:High Optical Throughput for Special Fluorescence Probes; High Stability for fool spectral range; Minimal cross link between fibers into MFOB-M structures; High stability for Endoscopes and sterilization proof tested solutions; Quality Controlled Scattered Reflection MFOB. MFOB structures designed with Mini Fiber Spectrometers show high spectral resolution (7-12 nm) and possibility to combine in one set different function: Normalization function for different light sources, Multi scan measurements with adjusted time duration, Spectral band analysis (including integrated characters for selected wavebands),Fast time resolution for selected types of scanning characters.},
author_keywords={Fiber Optic;  Fluorescence;  IR fiber;  Multi channel fiber bundle;  Nir fiber;  Remote spectroscopy},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kootsey20045166,
author={Kootsey, J.M. and Siriphongs, D. and McAuley, G.},
title={Building interactive simulations in a web page design program},
journal={Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
year={2004},
volume={26 VII},
pages={5166-5168},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144270256&partnerID=40&md5=24babdc65c2fce6301cdf46d13f04e26},
abstract={A new Web software architecture, NumberLinX (NLX), has been integrated into a commercial Web design program to produce a drag-and-drop environment for building interactive simulations. NLX is a library of reusable objects written in Java, including input, output, calculation, and control objects. The NLX objects were added to the palette of available objects in the Web design program to be selected and dropped on a page. Inserting an object in a Web page is accomplished by adding a template block of HTML code to the page file. HTML parameters in the block must be set to user-supplied values, so the HTML code is generated dynamically, based on user entries in a popup form. Implementing the object inspector for each object permits the user to edit object attributes in a form window. Except for model definition, the combination of the NLX architecture and the Web design program permits construction of interactive simulation pages without writing or inspecting code.},
author_keywords={Interactive;  Simulation;  Web page},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Todd2004482,
author={Todd, C.A. and Naghdy, F. and O'Leary, S.J.},
title={Geometric modelling of the temporal bone for cochlea implant simulation},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2004},
volume={5367},
pages={482-490},
doi={10.1117/12.533900},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-5644277661&doi=10.1117%2f12.533900&partnerID=40&md5=b1e0e658595a357f574b9631d014799d},
abstract={The first stage in the development of a clinically valid surgical simulator for training otologic surgeons in performing cochlea implantation is presented. For this purpose, a geometric model of the temporal bone has been derived from a cadaver specimen using the biomedical image processing software package Analyze (AnalyzeDirect, Inc) and its three-dimensional reconstruction is examined. Simulator construction begins with registration and processing of a Computer Tomography (CT) medical image sequence. Important anatomical structures of the middle and inner ear are identified and segmented from each scan in a semi-automated threshold-based approach. Linear interpolation between image slices produces a three-dimensional volume dataset: the geometrical model. Artefacts are effectively eliminated using a semi-automatic seeded region-growing algorithm and unnecessary bony structures are removed. Once validated by an Ear, Nose and Throat (ENT) specialist, the model may be imported into the Reachin Application Programming Interface (API) (Reachin Technologies AB) for visual and haptic rendering associated with a virtual mastoidectomy. Interaction with the model is realized with haptics interfacing, providing the user with accurate torque and force feedback. Electrode array insertion into the cochlea will be introduced in the final stage of design.},
author_keywords={Analyze;  Cochlea implant;  Geometrical model;  Haptic;  Otologic;  Region-growing;  Temporal bone},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dudbridge20042327,
author={Dudbridge, F. and Carver, T. and Williams, G.W.},
title={Pelican: Pedigree editor for linkage computer analysis},
journal={Bioinformatics},
year={2004},
volume={20},
number={14},
pages={2327-2328},
doi={10.1093/bioinformatics/bth231},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-5044250281&doi=10.1093%2fbioinformatics%2fbth231&partnerID=40&md5=172f47af0c535e8c246384819f6c5b20},
abstract={Summary: Linkage analysis software requires an input text file that describes the structure of the pedigrees to be analysed. Manual creation of these files is tedious and error-prone, and a graphical input tool is desirable. This is currently only available in commercial packages that include much greater functionality. We have therefore developed Pelican, a lightweight graphical pedigree editor for rapid construction of linkage pedigree files and diagrams. © Oxford University Press 2004; all rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rizk2004933,
author={Rizk, N.J.},
title={Parallelization of IBD computation for determining genetic disease maps},
journal={Concurrency and Computation: Practice and Experience},
year={2004},
volume={16},
number={9},
pages={933-943},
doi={10.1002/cpe.814},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142664610&doi=10.1002%2fcpe.814&partnerID=40&md5=a33a796cffc1be35690313a3ea1eeb55},
abstract={A number of software packages are available for the construction of comprehensive human genetic maps. In this paper we parallelize the widely used package Genehunter. We restrict our attention to only one function of the package, namely the computations of Identity By Descent (IBD) genes of a family. We use a master-slave model with the Message Passing Interface parallel environment. Our tests are done on two different architectures: a network of workstations and a shared memory multiprocessor. A new and efficient strategy to classify the parallelization of genetic linkage analysis programs results from our experiments. The classification is based on values of parameters which affect the complexity of the computation. Copyright © 2004 John Wiley & Sons, Ltd.},
author_keywords={Genetic linkage analysis;  Identity by Descent;  Parallel computing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gervasi2004703,
author={Gervasi, O. and Laganà, A.},
title={SIMBEX: A portal for the a priori simulation of crossed beam experiments},
journal={Future Generation Computer Systems},
year={2004},
volume={20},
number={5},
pages={703-715},
doi={10.1016/j.future.2003.11.028},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942587350&doi=10.1016%2fj.future.2003.11.028&partnerID=40&md5=8624cecd08cf77e5a3893af7c83adc19},
abstract={The architecture and the computational kernels of Simulation of Crossed Molecular Beam Experiments, an Internet portal managing the simulation of elementary bimolecular processes as those occurring in crossed beam apparatuses, is discussed. The construction of this portal is our contribution to project 003/001 of the COST in Chemistry action D23 (METACHEM: Metalaboratories for complex computational applications in chemistry). The portal is specifically designed to support the collaborative efforts of various Computational Chemistry and Computer Science European laboratories aimed at building an a priori molecular simulator based on a Grid infrastructure. Such an environment makes use of free-software packages and was implemented using Web technologies. © 2004 Elsevier B.V. All rights reserved.},
author_keywords={Computing Grid;  Internet portal;  Molecular simulator;  Reaction dynamics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adalsteinsson2004,
author={Adalsteinsson, D. and McMillen, D. and Elston, T.C.},
title={Biochemical Network Stochastic Simulator (BioNetS): Software for stochastic modeling of biochemical networks},
journal={BMC Bioinformatics},
year={2004},
volume={5},
page_count={21},
doi={10.1186/1471-2105-5-24},
art_number={24},
note={cited By 105},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-2942514591&doi=10.1186%2f1471-2105-5-24&partnerID=40&md5=55aa45eb812727213e1f19c0c366fc90},
abstract={Background: Intrinsic fluctuations due to the stochastic nature of biochemical reactions can have large effects on the response of biochemical networks. This is particularly true for pathways that involve transcriptional regulation, where generally there are two copies of each gene and the number of messenger RNA (mRNA) molecules can be small. Therefore, there is a need for computational tools for developing and investigating stochastic models of biochemical networks. Results: We have developed the software package Biochemical Network Stochastic Simulator (BioNetS) for efficiently and accurately simulating stochastic models of biochemical networks. BioNetS has a graphical user interface that allows models to be entered in a straightforward manner, and allows the user to specify the type of random variable (discrete or continuous) for each chemical species in the network. The discrete variables are simulated using an efficient implementation of the Gillespie algorithm. For the continuous random variables, BioNetS constructs and numerically solves the appropriate chemical Langevin equations. The software package has been developed to scale efficiently with network size, thereby allowing large systems to be studied. BioNetS runs as a BioSpice agent and can be downloaded from http://www.biospice.org. BioNetS also can be run as a stand alone package. All the required files are accessible from http://x.amath.unc.edu/BioNetS. Conclusions: We have developed BioNetS to be a reliable tool for studying the stochastic dynamics of large biochemical networks. Important features of BioNetS are its ability to handle hybrid models that consist of both continuous and discrete random variables and its ability to model cell growth and division. We have verified the accuracy and efficiency of the numerical methods by considering several test systems. © 2004 Adalsteinsson et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fritsch2004621,
author={Fritsch, D. and Kada, M.},
title={Visualisation using game engines},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2004},
volume={35},
pages={621-625},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043288283&partnerID=40&md5=45b00c478beec02361c0c7f21ff91f99},
abstract={Geographic Information Systems (GIS) and Computer Aided Facility Management-Systems (CAFM) are currently undergoing the transition to storing and processing real 3D geospatial data. Applications for this type of data are, among others, location based services, navigation systems and the planning of large-scale construction projects. For presentation purposes and especially when working in the field, powerful visualisation systems are needed that are also capable of running on mobile devices like notebooks, personal digital assistants (PDA) or even cell phones. In such application areas, the free movement of the viewer’s position and the interaction with the data are of great importance. Real-time visualisation of 3D geospatial data is already well established and also commercially successful in the entertainment industry, namely in the market of 3D video games. The development of software in this field is very cost-intensive, so that the packages are often used for several game products and are therefore universally applicable to a certain extend. These so-called game engines include not only visualisation functionality, but also offer physics, sound, network, artificial intelligence and graphical user interfaces to handle user in- and output. As certain portions or sometimes even the whole engine are released as open source software, these engines can be extended to build more serious applications at very little costs. The paper shows how these game engines can be used to create interactive 3D applications that present texture-mapped geospatial data. The integration of 3D data into such systems is discussed. Functionality like thematic queries can be implemented by extending the internal data structures and by modification of the game’s accompanying dynamic link libraries. © 2014 ISPRS. All Rights Reserved.},
author_keywords={GIS;  Modelling;  Real-time;  Virtual reality;  Visualisation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor20041,
title={8th International Conference on Knowledge-Based Intelligent Information and Engineering Systems, KES 2004},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3215},
pages={1-905},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975480370&partnerID=40&md5=01c0c4319818e7ca0034aecb93226174},
abstract={The proceedings contain 117 papers. The special focus in this conference is on Engineering of Ontology, Multi-agent System Design, Intelligent Multimedia Solution, Operations Research Based on Soft Computing and Web Mining. The topics include: Implementing EGAP-based many-valued argument model for uncertain knowledge; ontology revision using the concept of belief revision; a robust rule-based event management architecture for call-data records; adaptive agent integration in designing object-based multiagent system; ontological representations of software patterns; dynamic traffic grooming and load balancing for GMPLS-centric all optical networks; probabilistic model of traffic breakdown with random propagation of disturbance for its application; novel symbol timing recovery algorithm for multi-level signal; improving cam-dh protocol for mobile nodes with constraint computational power; space time code representation in transform domain; a multimedia database system using mobile indexing agent in wireless network; bus arrival time prediction method for its application; RRAM spare allocation in semiconductor manufacturing for yield improvement; a toolkit for constructing virtual instruments for augmenting user interactions and activities in a virtual environment; security requirements for software development; intelligent control model of information appliances; effective solution of a portofolio selection based on a block of shares by a meta-controlled Boltzmann machine; soft computing approach to books allocation strategy for library; possibilistic forecasting model and its application to analyze the economy in Japan; a proposal of chaotic forecasting method based on wavelet transform and retrieval of product reputations from the WWW.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Zhang2004151,
author={Zhang, J.},
title={A software system for automated and visual analysis of functionally annotated haplotypes},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={2983},
pages={151},
doi={10.1007/978-3-540-24719-7_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945251552&doi=10.1007%2f978-3-540-24719-7_31&partnerID=40&md5=835af925570760e91516b10f6393f2df},
abstract={We have developed a software analysis package, HapScope, which includes a comprehensive analysis pipeline and a sophisticated visualization tool for analyzing functionally annotated haplotypes. The HapScope analysis pipeline supports: a) computational haplotype construction with an EM or Bayesian statistical algorithm; b) SNP classification by protein coding change, homology to model organisms or putative regulatory regions; c) minimum SNP subset selection by either a Brute Force Algorithm or a Greedy Partition Algorithm. The HapScope viewer displays genomic structure with haplotype information in an integrated environment, providing eight alternative views for assessing genetic and functional correlation. It has a user-friendly interface for: a) haplotype block visualization; b) SNP subset selection; c) haplotype consolidation with subset SNP markers; d) incorporation of both experimentally determined haplotypes and computational results; e) data export for additional analysis. Comparison of haplotypes constructed by the statistical algorithms with those determined experimentally shows variation in haplotype prediction accuracies in genomic regions with different levels of nucleotide diversity. We have applied HapScope in analyzing haplotypes for candidate genes and genomic regions with extensive SNP and genotype data. We envision that the systematic approach of integrating functional genomic analysis with population haplotypes, supported by HapScope, will greatly facilitate current genetic disease research. © Springer-Verlag Berlin Heidelberg 2004.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Antoniol2004383,
author={Antoniol, G. and Di Penta, M. and Masone, G. and Villano, U.},
title={Compiler hacking for source code analysis},
journal={Software Quality Journal},
year={2004},
volume={12},
number={4},
pages={383-406},
doi={10.1023/B:SQJO.0000039794.29432.7e},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-4444279816&doi=10.1023%2fB%3aSQJO.0000039794.29432.7e&partnerID=40&md5=86aafb6b240915b7d158d936e610ffd5},
abstract={Many activities related to software quality assessment and improvement, such as empirical model construction, data flow analysis, testing or reengineering, rely on static source code analysis as the first and fundamental step for gathering the necessary input information. In the past, two different strategies have been adopted to develop tool suites. There are tools encompassing or implementing the source parse step, where the parser is internal to the toolkit, and is developed and maintained with it. A different approach builds tools on the top of external already-available components such as compilers that output the program abstract syntax tree, or that make it available via an API. This paper discusses techniques, issues and challenges linked to compiler patching or wrapping for analysis purposes. In particular, different approaches for accessing the compiler parsing information are compared, and the techniques used to decouple the parsing front end from the analysis modules are discussed. Moreover, the paper presents an approach and a tool, XOgastan, developed exploiting the gcc/g++ ability to save a representation of the intermediate abstract syntax tree. XOgastan translates the gcc/g++ dumped abstract syntax tree format into a Graph exchange Language representation, which makes it possible to take advantage of currently available XML tools for any subsequent analysis step. The tool is illustrated and its design discussed, showing its architecture and the main implementation choices made.},
author_keywords={Gcc;  GXL;  Source code analysis tools;  XML},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cho2004241,
author={Cho, Y. and Park, K.S. and Moher, T. and Johnson, A.E. and Chang, J. and Whang, M.C. and Lim, J.S. and Rhee, D.-W. and Park, K.R. and Park, H.K.},
title={CLOVES: A virtual world builder for constructing virtual environments for science inquiry learning},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3166},
pages={241-247},
doi={10.1007/978-3-540-28643-1_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048895035&doi=10.1007%2f978-3-540-28643-1_31&partnerID=40&md5=35e509c5a1062de5337da40868397070},
abstract={This paper presents the motivation, design, and a preliminary evaluation of a virtual world builder, CLOVES. CLOVES is designed to support rapid construction of data-rich virtual environments and instruments for young children's science inquiry learning. It provides a layered programming interface such as a visual design environment, scripting layer, and low-level application programming interface targeting for multiple levels of programming expertise. It is also intended to be a collaborative medium among interdisciplinary domain experts such as educators, modelers and software developers. A case study was conducted to evaluate the capabilities and effectiveness of CLOVES. The results showed that designers actively participated in decision making at every stage of the design process and shared knowledge among one another. © IFIP International Federation for Information Processing 2004.},
author_keywords={Interactive learning;  Virtual reality environment},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deconinck2004123,
author={Deconinck, G. and De Florio, V. and Belmans, R.},
title={Architecting distributed control applications based on (re-)configurable middleware},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3069},
pages={123-143},
doi={10.1007/978-3-540-25939-8_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048881012&doi=10.1007%2f978-3-540-25939-8_6&partnerID=40&md5=e04fec545b90da8321f6c7b47536433b},
abstract={Industrial distributed automation applications call for reusable software components, without endangering dependability. The DepAuDE architecture provides middleware to integrate fault tolerance support into such applications based on a library of detection, reconfiguration and recovery functions, and a language for expressing non-functional services, such as configuration and fault tolerance. At run time, a middleware layer orchestrates the execution of recovery actions. The paper further provides a hierarchical model, consisting of a dedicated intra-site local area network and an open inter-site wide area network, to deal with the different characteristics and requirements for dependability and quality-of-service, when such applications rely on off-the-shelf communication technology to exchange management or control information. The middleware can be dynamically reconfigured when the environment changes. This methodology has been integrated in the distributed automation system of an electrical substation. © Springer-Verlag Berlin Heidelberg 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Almási2004405,
author={Almási, G. and Archer, C. and Gunnels, J. and Heidelberger, P. and Martorell, X. and Moreira, J.E.},
title={Architecture and performance of the BlueGene/L message layer},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3241},
pages={405-414},
doi={10.1007/978-3-540-30218-6_55},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048876804&doi=10.1007%2f978-3-540-30218-6_55&partnerID=40&md5=34d08ddfbf0a75790e55bb642749a8b5},
abstract={The BlueGene/L supercomputer is planned to consist of 65,536 dual-processor compute nodes interconnected by high speed torus and tree networks. Compute nodes can only address local memory, making message passing the natural programming model for the machine. In this paper we present the architecture and performance of the BlueGene/L message layer, the software library that makes an efficient MPI implementation possible. We describe the components and protocols of the message layer, and present microbenchmark based performance results for several aspects of the library. © Springer-Verlag 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang2004408,
author={Huang, L. and Chapman, B. and Liu, Z. and Kendall, R.},
title={Efficient translation of OpenMP to distributed memory},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3038},
pages={408-413},
doi={10.1007/978-3-540-24688-6_54},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-25144490392&doi=10.1007%2f978-3-540-24688-6_54&partnerID=40&md5=8b4b878b5c3c48e0fcc547e0ec084163},
abstract={The shared memory paradigm provides many benefits to the parallel programmer, particular with respect to applications that are hard to parallelize. Unfortunately, there are currently no efficient implementations of OpenMP for distributed memory platforms and this greatly diminishes its usefulness for real world parallel application development. In this paper we introduce a basic strategy for implementing OpenMP on distributed memory systems via a translation to Global Arrays. Global Arrays is a library of routines that provides many of the same features as OpenMP yet targets distributed memory platforms. Since it enables a reasonable translation strategy and also allows precise control over the movement of data within the resulting code, we believe it has the potential to provide higher levels of performance than the traditional translation of OpenMP to distributed memory via software distributed shared memory. © Springer-Verlag 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bøttcher20031,
author={Bøttcher, S.G. and Dethlefsen, C.},
title={Deal: A package for learning bayesian networks},
journal={Journal of Statistical Software},
year={2003},
volume={8},
pages={1-19},
note={cited By 87},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544365481&partnerID=40&md5=77290eec5e8044818c4d981ef3a2afa0},
abstract={deal is a software package for use with R. It includes several methods for analysing data using Bayesian networks with variables of discrete and/or continuous types but restricted to conditionally Gaussian networks. Construction of priors for network parameters is supported and their parameters can be learned from data using conjugate updating. The network score is used as a metric to learn the structure of the network and forms the basis of a heuristic search strategy. deal has an interface to Hugin.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Allen2003138,
author={Allen, D.W. and Clough, J.A. and Sohn, H. and Farrar, C.R.},
title={A Software Tool for Graphically Assembling Damage Identification Algorithms},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2003},
volume={5057},
pages={138-144},
doi={10.1117/12.482755},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242551336&doi=10.1117%2f12.482755&partnerID=40&md5=2da63b3758a2fce3ec66c555b3720b5f},
abstract={At Los Alamos National Laboratory (LANL), various algorithms for structural health monitoring problems have been explored in the last 5 to 6 years. The original DIAMOND (Damage Identification And MOdal aNalysis of Data) software was developed as a package of modal analysis tools with some frequency domain damage identification algorithms included. Since the conception of DIAMOND, the Structural Health Monitoring (SHM) paradigm at LANL has been cast in the framework of statistical pattern recognition, promoting data driven damage detection approaches. To reflect this shift and to allow user-friendly analyses of data, a new piece of software, DIAMOND II is under development. The Graphical User Interface (GUI) of the DIAMOND II software is based on the idea of GLASS (Graphical Linking and Assembly of Syntax Structure) technology, which is currently being implemented at LANL. GLASS is a Java based GUI that allows drag and drop construction of algorithms from various categories of existing functions. In the platform of the underlying GLASS technology, DIAMOND II is simply a module specifically targeting damage identification applications. Users can assemble various routines, building their own algorithms or benchmark testing different damage identification approaches without writing a single line of code.},
author_keywords={Damage detection;  Software;  Statistical pattern recognition;  Time series analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fóthi2003815,
author={Fóthi, Á. and Nyéky-Gaizler, J. and Porkoláb, Z.},
title={The Structured Complexity of Object-Oriented Programs},
journal={Mathematical and Computer Modelling},
year={2003},
volume={38},
number={7-9},
pages={815-827},
doi={10.1016/S0895-7177(03)90066-5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0242507445&doi=10.1016%2fS0895-7177%2803%2990066-5&partnerID=40&md5=78d00cbe15aa053d850c1550c4252232},
abstract={There are several methods measuring the complexity of object-oriented programs. Most of them are based on some special object-oriented feature: number of methods/classes, cohesion of classes, inheritance, etc. In practice, however, object-oriented programs are constructed with the help of the same control structures as traditional ones. Moreover, recent ideas of multiparadigm programming (i.e., emerging use of generic programming and aspect-oriented programming) has the effect that in modern programs - and even in class libraries - object-orientation is only one (however major) construction tool among others. An adequate measure therefore should not be based on special features of one paradigm, but on basic language elements and construction rules which could be applied to many different paradigms. In our model discussed here, the complexity of a program is the sum of three components: the complexity of its control structure, the complexity of data types used, and the complexity of the data handling (i.e., the complexity of the connection between the control structure and the data types). We suggest a new complexity measure. First, we show that this measure works well on procedural programs, and then we extend it to object-oriented programs. There is a software tool under development based on gnu g++ compiler which computes our new measure. We can apply this tool to C and C++ sources to gain a number of quantitative results with our measure. © 2003 Elsevier Ltd. All rights reserved.},
author_keywords={Complexity;  Object-oriented programming;  Software metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Owolabi2003201,
author={Owolabi, A. and Anumba, C.J. and El-Hamalawi, A.},
title={Architecture for implementing IFC-based online construction product libraries},
journal={Electronic Journal of Information Technology in Construction},
year={2003},
volume={8},
pages={201-218},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042692836&partnerID=40&md5=e31eb432fcca44fbbab253b746943dfa},
abstract={Construction product information providers have responded to the demand for electronic delivery by providing online access, CD-ROMs and DVDs but these solutions have limited usability and are generally incapable of supporting prevalent and emerging industry practices. The product library implementations attempt to replicate the functionalities of the paper versions, which serve for independent specification and procurement but gives little thought to teams and tools integration through support for automated information exchange and sharing. The IFC standard provides common terminologies, technologies, syntax and semantics necessary to address present and future compatibility and integration issues, hence IFC-based implementation of product libraries have good prospect for meeting the industry requirements. This paper reviews current product information delivery methods and examines the applicability of the IFC and other standards. The requirements for IFC-based construction product libraries are identified and an architecture for realising the requirements was presented.},
author_keywords={Data exchange and sharing;  IFC;  Integration;  Multi-tier architecture;  Product libraries},
document_type={Review},
source={Scopus},
}

@ARTICLE{Gondzio2003561,
author={Gondzio, J. and Sarkissian, R.},
title={Parallel interior-point solver for structured linear programs},
journal={Mathematical Programming},
year={2003},
volume={96},
number={3},
pages={561-584},
doi={10.1007/s10107-003-0379-5},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-13544254921&doi=10.1007%2fs10107-003-0379-5&partnerID=40&md5=19c187e8371bfe9fa939d9c47b432226},
abstract={Issues of implementation of an object-oriented library for parallel interior-point methods are addressed. The solver can easily exploit any special structure of the underlying optimization problem. In particular, it allows a nested embedding of structures and by this means very complicated real-life optimization problems can be modelled. The efficiency of the solver is illustrated on several problems arising in the optimization of networks. The sequential implementation outperforms the state-of-the-art commercial optimization software. The parallel implementation achieves speed-ups of about 3.1-3.9 on 4-processors parallel systems and speed-ups of about 10-12 on 16-processors parallel systems. © 2003 Springer-Verlag.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Glezer200347,
author={Glezer, C.},
title={A conceptual model of an interorganizational intelligent meeting-scheduler (IIMS)},
journal={Journal of Strategic Information Systems},
year={2003},
volume={12},
number={1},
pages={47-70},
doi={10.1016/S0963-8687(02)00034-3},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867940470&doi=10.1016%2fS0963-8687%2802%2900034-3&partnerID=40&md5=83510d98c0b78f73000947a4a48639b4},
abstract={This article proposes and evaluates a comprehensive agent-based architecture for an Interorganizational Intelligent Meeting-Scheduler. The article extends and generalizes the Intelligent Meeting-Scheduler conceptual model [EXPERSYS 95-Proc. Seventh Intl Conf. Artificial Intelligence Expert Syst. Appl. (1995) 279; J. Organizational Comput. Electron. Commerce, 9 (1999) 233] which focused on intraorganizational meeting scenarios. First, the article reviews several academic meeting-scheduling prototypes and commercial software packages. Based on this review, it is demonstrated that only an integrated approach that supports interoperability in all three dimensions of the meeting-scheduling problem (calendar, scheduling, and communication-management) can succeed in achieving interoperability among heterogeneous calendar and scheduling systems. The next part of the article provides a specification of an agent-based system that attempts to address the interoperability challenge. The specification comprises of the following elements: environment, behaviors, symbol-level, and knowledge-level architectures [IEEE Trans. Syst., Man, Cybernet. 25 (1995) 852]. The inter-organizational meeting-scheduling process is articulated as an iterative negotiation process where knowledge and symbol level units ('the IIMS system') interact with the system's end-users ('the environment') by exhibiting behaviors that address end-user requirements. The IIMS conceptual model is evaluated empirically and related to relevant literature on adoption difficulties of inter-organizational systems. It is evident that the IIMS faces a plethora of technological, organizational, sociological, behavioral, and psychological challenges that hinder its successful adoption. The article proposes several implementation tactics and guidelines in order to overcome these obstacles. © 2003 Elsevier Science B.V.},
author_keywords={Calendars;  Group-tasks;  Human resource management (HRM);  Inter-organizational systems (IOS);  Interoperability;  Meeting-scheduling;  Meetings;  Software agents},
document_type={Article},
source={Scopus},
}

@ARTICLE{D'Ornellas2003231,
author={D'Ornellas, M.C. and Van Den Boomgaard, R.},
title={The state of art and future development of morphological software towards generic algorithms},
journal={International Journal of Pattern Recognition and Artificial Intelligence},
year={2003},
volume={17},
number={2},
pages={231-255},
doi={10.1142/S0218001403002344},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038737109&doi=10.1142%2fS0218001403002344&partnerID=40&md5=a4b4161fa3aaa645339cd530682fd15b},
abstract={The goal of this paper is to establish the fundamental requirements of any morphological software in such a way as to produce reliable, precise, and generic algorithm representations in order to overcome the combinatorial explosion of the code needed to deal with all possible kinds of data structures and image types. Algorithms should be built with the guarantee that the desired implementation is written once, process data in an abstract way, and is efficient at the same time. Requirements are derived from a review of some of the most used software packages and image libraries with respect to mathematical morphology including Mmach, Mmorph, Micromorph and Visilog. Such a review makes it possible to characterize their advantages and drawbacks. These aspects call for software for mathematical morphology that has to be generic and should support a large variety of data types.},
author_keywords={Generic programming;  Implementation techniques;  Mathematical morphology;  Morphological software packages;  Software development},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rizk2003,
author={Rizk, N.J.},
title={Parallelisation of IBD computation for determining genetic disease map},
journal={Proceedings - International Parallel and Distributed Processing Symposium, IPDPS 2003},
year={2003},
doi={10.1109/IPDPS.2003.1213293},
art_number={1213293},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947290626&doi=10.1109%2fIPDPS.2003.1213293&partnerID=40&md5=76c405e623686de6f78419a897d2e4af},
abstract={A number of software packages are available for the construction of comprehensive human genetic maps. In this paper we parallelize the widely used package Genehunter. We restrict our attention to only one function of the package, namely the computations of Identity By Descent (IBD) genes of a family. We use a master-slave model with the Message Passing Interface (MPI) parallel environment. Our tests are done on two different architectures: A network of workstations and a shared memory multiprocessor. A new and efficient strategy to classify the parallelization of genetic linkage analysis programs results from our experiments. The classification is based on values of parameters which affect the complexity of the computation. © 2003 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Calafate2003243,
author={Calafate, C.M.T. and Manzoni, P.},
title={A multi-platform programming interface for protocol development},
journal={Proceedings - 11th Euromicro Conference on Parallel, Distributed and Network-Based Processing, Euro-PDP 2003},
year={2003},
pages={243-249},
doi={10.1109/EMPDP.2003.1183595},
art_number={1183595},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946399693&doi=10.1109%2fEMPDP.2003.1183595&partnerID=40&md5=84d60f9e74c245b4398b445ba98d69e0},
abstract={We present a programming interface, called PICA, which aims to reduce the production cycle duration for communication protocols. It offers a user-friendly API that is very intuitive and that covers the major necessities which these protocols may have. Since most research is done in the Linux operating system, it also aims to reduce very significantly the time required to port a protocol implementation to other platforms like Windows NT or Windows CE by providing source code compatibility. We estimated the efficiency of our library and found that the overhead introduced is very small. © 2003 IEEE.},
author_keywords={Computer interfaces;  Linux;  Operating systems;  Production;  Prototypes;  Routing protocols;  Software design;  Software libraries;  Software performance;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kovácsházy2003179,
author={Kovácsházy, T. and Samu, G. and Péceli, G.},
title={Simulink block library for fast prototyping of reconfigurable DSP systems},
journal={2003 IEEE International Symposium on Intelligent Signal Processing: From Classical Measurement to Computing with Perceptions, WISP 2003 - Proceedings},
year={2003},
pages={179-184},
doi={10.1109/ISP.2003.1275835},
art_number={1275835},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946047338&doi=10.1109%2fISP.2003.1275835&partnerID=40&md5=edbb2b6183f7136a8594bd1f2e702073},
abstract={This paper presents a block library for Matlab/Simulink that allows fast prototyping of reconfigurable DSP systems. Up tilt now no similar software package was available. The block library supports the construction of reconfigurable discrete time linear and non-linear systems from reconfigurable digital filters using various filter structures, state-space form implementations, polynomial filters, and PID controllers. The paper lists the requirements for the block library and introduces the main implementation related decisions that allows the block library to meet these requirements. An example illustrates the usage of the block library. ©2003 IEEE.},
author_keywords={Reconfigurable DSP systems;  Simulink block library;  Transient management and reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeCarvalho200355,
author={De Carvalho, F.H., Jr. and Lins, R.D. and Quental, N.C.},
title={On the implementation of SPMD applications using Haskell#},
journal={Proceedings - Symposium on Computer Architecture and High Performance Computing},
year={2003},
volume={2003-January},
pages={55-63},
doi={10.1109/CAHPC.2003.1250321},
art_number={1250321},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944449902&doi=10.1109%2fCAHPC.2003.1250321&partnerID=40&md5=292285bc8fe5e10bb06719004173c3b0},
abstract={Commodities-built clusters, a low cost alternative for distributed parallel processing, brought high-performance computing to a wide range of users. However, the existing widespread tools for distributed parallel programming, such as messaging passing libraries, does not attend new software engineering requirements that have emerged due to increase in complexity of applications. Haskell# is a parallel programming language intending to reconcile higher abstraction and modularity with scalable performance. It is demonstrated the use of Haskell# in the programming of three SPMD benchmark programs, which have lower-level MPI implementations available. © 2003 IEEE.},
author_keywords={Application software;  Computer architecture;  Concurrent computing;  Costs;  Distributed computing;  Parallel processing;  Parallel programming;  Programming profession;  Software engineering;  Software libraries},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gopalakrishnan2003297,
author={Gopalakrishnan, C. and Katkoori, S.},
title={Resource allocation and binding approach for low leakage power},
journal={Proceedings of the IEEE International Conference on VLSI Design},
year={2003},
volume={2003-January},
pages={297-302},
doi={10.1109/ICVD.2003.1183153},
art_number={1183153},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941358756&doi=10.1109%2fICVD.2003.1183153&partnerID=40&md5=6c34d4248f7b8e334d16126b2ae9705e},
abstract={We propose a leakage power minimization approach based on multi-threshold CMOS (MTCMOS) technology. A clique partitioning-based resource allocation and binding algorithm is presented, which maximizes the idle periods of modules in the data-path. Modules with significant idle times are selectively bound to MTCMOS instances. We developed a parameterizable MTCMOS component library, characterized with respect to sleep transistor width. Using this characterization, the leakage power-delay trade-off is analyzed and optimal sleep transistor widths are identified. For three well known HLS benchmarks, we obtain an average leakage power reduction of 22.44%. The main disadvantage of MTCMOS technology is performance degradation. We present a performance recovery technique based on multi-cycling and introduction of slack. With this technique, the performance penalty reduces to as low as 14.28%. We obtain an average leakage power reduction of 17.46% after performance recovery. The average area overhead incurred due to the introduction of MTCMOS modules is 10.21%. Results are presented for 0.18 μm CMOS technology. © 2003 IEEE.},
author_keywords={Circuits;  CMOS technology;  High level synthesis;  Leakage current;  Libraries;  Minimization;  Performance loss;  Resource management;  Sleep;  Very large scale integration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor20031,
title={8th International Workshop on DNA-Based Computers, DNA 2002},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2568},
pages={1-336},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934768144&partnerID=40&md5=0b8c8393f2f522bca69f4857b3a02f6f},
abstract={The proceedings contain 29 papers. The special focus in this conference is on DNA-Based computers. The topics include: Self-assembling DNA graphs; construction and characterization of filaments composed of TX-tile lattice; the design of autonomous DNA nanomechanical devices; cascading whiplash PCR with a nicking enzyme; A PNA-mediated whiplash PCR-based program for in vitro protein evolution; engineering signal processing in cells; temperature gradient-based DNA computing for graph problems with weighted edges; shortening the computational time of the fluorescent DNA computing; how efficiently can room at the bottom be traded away for speed at the top?; hierarchical DNA memory based on nested PCR; binary arithmetic for DNA computers; implementation of a random walk method for solving 3-SATon circular DNA molecules; version space learning with DNA molecules; DNA implementation of theorem proving with resolution refutation in propositional logic; universal biochip readout of directed hamiltonian path problems; algorithms for testing that sets of DNA words concatenate without secondary structure; a PCR-based protocol for in vitro selection of non-crosshybridizing oligonucleotides; on template method for DNA sequence design; a combinatorial approach; stochastic local search algorithms for DNA word design; a sequence design system with multiobjective optimization; a software tool for generating non-crosshybridizing libraries of DNA oligonucleotides; regularity and below and on the computational power of insertion-deletion systems.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Milosavljević2003100,
author={Milosavljević, G. and Perišić, B.},
title={Really rapid prototyping of large-scale business information systems},
journal={Proceedings of the International Workshop on Rapid System Prototyping},
year={2003},
volume={2003-January},
pages={100-106},
doi={10.1109/IWRSP.2003.1207036},
art_number={1207036},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649118573&doi=10.1109%2fIWRSP.2003.1207036&partnerID=40&md5=17aff080504157ab64b0780a7e0a7f5d},
abstract={This paper presents a method and concepts of a supporting tool for really rapid prototyping of large-scale business information systems. Our method is based on the following guidelines: (1) small team of highly skilled members with combined skills; (2) prototype-based development of subsystems and the system as a whole; (3) brainstorming sessions always involving systems analysts, database and application designers, and user representatives (if needed); and (4) development tools providing for efficient prototype development by maximum automation of all design phases. The presented development tool is based on standardization of functional and visual characteristics of an application, a library of high-level, coarse-grained components, and a set of rules for model-to-application mapping enabling automatic application reconfiguration in case of changes in the data model. © 2003 IEEE.},
author_keywords={Data analysis;  Data models;  Design automation;  Guidelines;  Information systems;  Large-scale systems;  Libraries;  Prototypes;  Standardization;  Visual databases},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brukman200320,
author={Brukman, O. and Dolev, S. and Kolodner, E.K.},
title={Self-stabilizing autonomic recoverer for eventual Byzantine software},
journal={Proceedings - IEEE International Conference on Software- Science, Technology and Engineering, SwSTE 2003},
year={2003},
pages={20-29},
doi={10.1109/SWSTE.2003.1245312},
art_number={1245312},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-5044219884&doi=10.1109%2fSWSTE.2003.1245312&partnerID=40&md5=d57eb57cfab4af086c0e71463db4c75e},
abstract={We suggest to model software package flaws (bugs) by assuming eventual Byzantine behavior of the package. In particular, the package has been tested by the manufacturer for limited length scenarios when started in a predefined initial state; the behavior beyond the tested scenario may be Byzantine. Restarts (reboots) are useful for recovering such systems. We suggest a general yet practical framework and paradigm, based on a theoretical foundation, for the monitoring and restarting of systems. An autonomic recoverer that monitors and restarts the system is proposed, where: the autonomic recoverer is designed to handle different tasks given specific task requirements in the form of predicates and actions. DAG subsystem hierarchy structure is used by a consistency monitoring procedure in order to achieve gracious recovery. The existence and correct functionality of the autonomic recovery is guaranteed by the use of a kernel resident (anchor) process, and the design of the process to be self-stabilizing. The autonomic recoverer uses new scheme for liveness assurance via online monitoring that complements known schemes for online ensuring safety. © 2003 IEEE.},
author_keywords={Computer bugs;  Computer industry;  Computer science;  Fault tolerant systems;  Monitoring;  Packaging;  Safety;  Software packages;  Software systems;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{SantaolayaS.2003704,
author={Santaolaya S., R. and Fragoso D., O.G. and Pérez O., J. and Zambrano S., L.},
title={Restructuring conditional code structures using object oriented design patterns},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2667},
pages={704-713},
doi={10.1007/3-540-44839-x_74},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248837859&doi=10.1007%2f3-540-44839-x_74&partnerID=40&md5=13bed35ffe941d75680275d78c6a5dbf},
abstract={Nowadays, software industry demands high quality reusable artifacts that are easy to configure for developing new applications or modifying the existing ones at minimum cost. In this context several approaches have been proposed, as a result of this, libraries with a number of reusable functions and/or classes have been obtained. Such approaches have also proposed guidelines aimed to reuse most of the software developed by programmers. However this goal has not been achieved yet, mainly due to the lack of quality attributes of the reusable software components currently available. This paper introduces an approach known as SR2, which means software reengineering for reuse, it is based on a reengineering process whose input is legacy code written in C language and the output is an object-oriented framework in C++. In this work we employ the Gamma design patterns strategy and state to structure the framework generated by the reengineering process. © Springer-Verlag Berlin Heidelberg 2003.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Modi2003464,
author={Modi, A. and Long, L.N. and Plassmann, P.E.},
title={Real-time visualization of wake-vortex simulations using computational steering and Beowulf clusters},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2565},
pages={464-478},
doi={10.1007/3-540-36569-9_31},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35248824688&doi=10.1007%2f3-540-36569-9_31&partnerID=40&md5=302a63f595f8bd9be3efc6e01b019752},
abstract={In this paper, we present the design and implementation of POSSE, a new, lightweight computational steering system based on a client/server programming model. We demonstrate the effectiveness of this software system by illustrating its use for a visualization client designed for a particularly demanding real-time application - wake-vortex simulations for multiple aircraft running on a parallel Beowulf cluster. We describe how POSSE is implemented as an object-oriented, class-based software library and illustrate its ease of use from the perspective of both the server and client codes. We discuss how POSSE handles the issue of data coherency of distributed data structures, data transfer between different hardware representations, and a number of other implementation issues. Finally, we consider how this approach could be used to augment AVOSS (an air traffic control system currently being developed by the FAA) to significantly increase airport utilization while reducing the risks of accidents. © Springer-Verlag Berlin Heidelberg 2003.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cyganek2003713,
author={Cyganek, B. and Borgosz, J.},
title={An object-oriented software platform for examination of algorithms for image processing and compression},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2658},
pages={713-720},
doi={10.1007/3-540-44862-4_77},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048895069&doi=10.1007%2f3-540-44862-4_77&partnerID=40&md5=b0f6386733c14e13c56e1d8c5b92e7a3},
abstract={This paper presents a design and implementation of the innovative software development system for image processing and compression. This platform allows to make 3D image processing as well as fundamental operations on images. One of the most important features of this system, that we will focus on in this paper, is its ability to evaluate performance of different stereo matching methods. This is accomplished by a special software module that verifies machine computed disparities with values provided by a person. Additionally, due to OOP technology, the software constitutes an open architecture that allows for easy addition of new components for image processing and compression. The presented platform was verified experimentally and also compared with existing commercial packages. © Springer-Verlag Berlin Heidelberg 2003.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kelapure2003263,
author={Kelapure, R. and Gonçalves, M.A. and Fox, E.A.},
title={Scenario-Based generation of digital library services},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2769},
pages={263-275},
doi={10.1007/978-3-540-45175-4_25},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048894894&doi=10.1007%2f978-3-540-45175-4_25&partnerID=40&md5=3b340ba96f5c560d96d49c078ec1f247},
abstract={We describe the development, implementation, and deployment of a new generic digital library generator yielding implementations of digital library services from models of DL "societies" and "scenarios". The distinct aspects of our solution are: 1) approach based on a formal, theoretical framework; 2) use of state-of-the-art database and software engineering techniques such as domain-specific declarative languages, scenario synthesis, componentized and model driven architectures; 3) analysis centered on scenario-based design and DL societal relationships; 4) automatic transformations and mappings from scenarios to workflow designs and from these to Java implementations, 5) special attention paid to issues of simplicity of implementation, modularity, reusability, and extensibility. We demonstrate the feasibility of the approach through a number of examples. © Springer-Verlag 2003.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Dorta2003292,
author={Dorta, I. and León, C. and Rodríguez, C. and Rojas, A.},
title={Parallel skeletons for divide-and-conquer and branch-and-bound techniques},
journal={Proceedings - 11th Euromicro Conference on Parallel, Distributed and Network-Based Processing, Euro-PDP 2003},
year={2003},
pages={292-298},
doi={10.1109/EMPDP.2003.1183602},
art_number={1183602},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745307260&doi=10.1109%2fEMPDP.2003.1183602&partnerID=40&md5=1442457a23012b8c8d5c281e1264fb13},
abstract={The article describes the parallel implementation of skeletons for the divide-and-conquer and branch-and-bound techniques. The user has to choose a paradigm and has to specify for it the type of the problem, the type of solution and the specific characteristics of the technique using the C++ programming language. This information is combined with the provided resolution skeleton to obtain a sequential program and a parallel program. The paper describes the parallel implementation of the skeletons using MPI. Computational results on a Linux-cluster of PCs, Cray T3E and Origin 3000 are presented. © 2003 IEEE.},
author_keywords={Acceleration;  Algorithm design and analysis;  Application software;  Computer languages;  Concurrent computing;  Dynamic programming;  Libraries;  Personal communication networks;  Skeleton;  Software tools},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cechich20031,
author={Cechich, A. and Piattini, M. and Vallecillo, A.},
title={Assessing component-based systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2003},
volume={2693},
pages={1-20},
doi={10.1007/978-3-540-45064-1_1},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-21144451628&doi=10.1007%2f978-3-540-45064-1_1&partnerID=40&md5=f6a9277f085888fd5577b3a62a1de5ae},
abstract={The last decade marked the first real attempt to turn software development into engineering through the concepts of Component-Based Software Development (CBSD) and Commercial Off-The-Shelf (COTS) components. The idea is to create high-quality parts and join them together to form a functioning system. The problem is that the combination of such parts does not necessarily result in a high-quality system. It is clear that CBSD affects software quality in several ways, ranging from introducing new methods for selecting COTS components, to defining a wide scope of testing principles and measurements. Today, software quality staff must rethink the way software is assessed, including all life-cycle phases - from requirements to evolution. Based on cumulated research efforts, the goal of this chapter is to introduce the best practices of current Component-Based Software Assessment (CBSA). We will develop and describe in detail the concepts involved in CBSA and its constituent elements, providing a basis for discussing the different approaches presented later in this book. © Springer-Verlag Berlin Heidelberg 2003.},
document_type={Article},
source={Scopus},
}

@ARTICLE{VanAmerongen200387,
author={Van Amerongen, J. and Breedveld, P.},
title={Modelling of physical systems for the design and control of mechatronic systems},
journal={Annual Reviews in Control},
year={2003},
volume={27 I},
number={1},
pages={87-117},
doi={10.1016/S1367-5788(03)00010-5},
note={cited By 85},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141836260&doi=10.1016%2fS1367-5788%2803%2900010-5&partnerID=40&md5=95649b6eef2fbbdbc291b8bf924283d0},
abstract={Mechatronic design requires that a mechanical system and its control system be designed as an integrated system. This contribution covers the background and tools for modelling and simulation of physical systems and their controllers, with parameters that are directly related to the real-world system. The theory will be illustrated with examples of typical mechatronic systems such as servo systems and a mobile robot. Hands-on experience is realised by means of exercises with the 20-sim software package (a demo version is freely available on the Internet). In mechatronics, where a controlled system has to be designed as a whole, it is advantageous that model structure and parameters are directly related to physical components. In addition, it is desired that (sub-)models be reusable. Common block-diagram- or equation-based simulation packages hardly support these features. The energy-based approach towards modelling of physical systems allows the construction of reusable and easily extendible models. This contribution starts with an overview of mechatronic design problems and the various ways to solve such problems. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission and a mobile robot. The energy-based approach towards modelling is treated in some detail. This will give the reader sufficient insight in order to exercise it with the aid of modelling and simulation software (20-sim). Such a tool allows high level input of models in the form of iconic diagrams, equations, block diagrams or bond graphs and supports efficient symbolic and numerical analysis as well as simulation and visualisation. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a process that can be controlled by block-diagram-based (digital) controllers. This contribution is based on object-oriented modelling: each object is determined by constitutive relations at the one hand and its interface, the power and signal ports to and from the outside world, at the other hand. Other realizations of an object may contain different or more detailed descriptions, but as long as the interface (number and type of ports) is identical, they can be exchanged in a straightforward manner. This allows top-down modelling as well as bottom-up modelling. Straightforward interconnection of (empty) submodels supports the actual decision process of modelling, not just model input and output manipulation. Empty submodel types may be filled with specific descriptions with various degrees of complexity (models can be polymorphic) to support evolutionary and iterative modelling and design approaches. Additionally, submodels may be constructed from other submodels in hierarchical structures. An introduction to the design of controllers based on these models is also given. Modelling and controller design as well as the use of 20-sim may be exercised in hands-on experience assignments, available at the Internet (http://www.ce.utwente.nl/IFACBrief/). A demonstration copy of 20-sim that allows the reader to use the ideas presented in this contribution may be downloaded from the Internet (http://www.20sim.com).},
author_keywords={Bond graphs;  Design and control;  Energy-based modelling;  Mechatronics;  Modelling and simulation;  Physical systems},
document_type={Short Survey},
source={Scopus},
}

@ARTICLE{Jardim-Gonçalves2003105,
author={Jardim-Gonçalves, R. and Steiger-Garção, A.},
title={Integration and adoptability of APs: The role of ISO TC184/SC4 standards},
journal={International Journal of Computer Applications in Technology},
year={2003},
volume={18},
number={1-4},
pages={105-116},
doi={10.1504/ijcat.2003.002131},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0041363089&doi=10.1504%2fijcat.2003.002131&partnerID=40&md5=48626c45d1732aef2cef4acbb618198e},
abstract={Automatic data exchange among computer applications in industrial domains is an issue that has been largely worked out by the scientific and industrial community during last years. In the scope of the several industrial activities, the lack of tools and structured data models ready to be adopted to enable immediate connection of applications in networked-based environments, is one of the main gaps identified when seeking global integrated solutions. ISO TC184/ SC4 standards (e.g., STEP and PLib) seem ready to aid solving this problem, and the adoption of such standards has been opening new possibilities in this area, fulfilling most of the identified requirements. To support those standards, applications must be prepared with suitable mechanisms and interfaces easily adaptable for fast and reliable plug-and-play in the standard-based architecture, to accomplish the foreseen Open Platforms. Therefore, data modelling, data share and exchange, reuse of models, automatic code generators and software libraries, together with the possibility to incorporate expertise and knowledge representation, is a challenge to face when working in environments supported by heterogeneous platforms and concepts, as is the case. Nevertheless, to include mechanisms for conformance testing will faster assure interoperability among the applications joining the platform. The content of this paper intends to provide a broad view of two of the main ISO TC184/SC4 Standards, i.e., ISO 10303-STEP and ISO 13584-PLib, presenting its methodology and architecture, and contributing for a reference to those managing in the area of the integration of data in industrial domains. This text is based on the standard documentation available, and results from the work developed and in progress in the scope of some industrial and research international projects for the furniture and building and construction industrial sectors within CEN/ISSS and ISO TC184/SC4 (e.g., funStep and ECOS ESPRIT projects).},
author_keywords={Information system;  Integration of data;  Interoperability;  Modelling;  Standards},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Coffland2003666,
author={Coffland, J.E. and Pimentel, A.D.},
title={A software framework for efficient system-level performance evaluation of embedded systems},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2003},
pages={666-671},
doi={10.1145/952532.952663},
note={cited By 39},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038675299&doi=10.1145%2f952532.952663&partnerID=40&md5=8afe8f636947ddf7a6528b585e3f64cd},
abstract={The Sesame environment provides modeling and simulation methods and tools for the efficient design space exploration of heterogeneous embedded multimedia systems. In this paper we describe the Sesame software system and demonstrate its capabilities using several examples. We show that Sesame significantly reduces model construction time through the use of modeling component libraries, hierarchy, and advanced model structure description features.},
author_keywords={Co-simulation;  Embedded systems;  Performance evaluation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{VanderHeyden2003431,
author={VanderHeyden, W.B. and Dendy, E.D. and Padial-Collins, N.T.},
title={CartaBlanca - A pure-Java, component-based systems simulation tool for coupled nonlinear physics on unstructured grids - An update},
journal={Concurrency and Computation: Practice and Experience},
year={2003},
volume={15},
number={3-5 SPEC.},
pages={431-458},
doi={10.1002/cpe.662},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037373858&doi=10.1002%2fcpe.662&partnerID=40&md5=4d036f953cd6d981a835451e8a2187ec},
abstract={This paper describes a component-based nonlinear physical system simulation prototyping package written entirely in Java using object-oriented design. The package provides scientists and engineers with a 'developer-friendly' software environment for large-scale computational algorithm and physical model development. The software design centers on the Jacobian-free Newton-Krylov solution method surrounding a finite-volume treatment of conservation equations. This enables a clean component-like implementation. We first provide motivation for the development of the software and then discuss software structure. The discussion includes a description of the use of Java's built-in thread facility that enables parallel, shared-memory computations on a wide variety of unstructured grids with triangular, quadrilateral, tetrahedral and hexahedral elements. We also discuss the use of Java's inheritance mechanism in the construction of a hierarchy of physics systems objects and linear and nonlinear solver objects that simplify development and foster software re-use. We provide a brief review of the Jacobian-free Newton-Krylov nonlinear system solution method and discuss how it fits into our design. Following this, we show results from example calculations and then discuss plans including the extension of the software to distributed-memory computer systems.},
author_keywords={Development environment;  Finite volume;  High performance computing;  Java;  Nonlinear solver;  Object oriented design},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nikitovic200243,
author={Nikitovic, M. and Brorsson, M.},
title={An adaptive chip-multiprocessor architecture for future mobile terminals},
journal={Proceedings of the 2002 International Conference on Compilers, Architecture, and Synthesis for Embedded Systems, CASES '02},
year={2002},
pages={43-49},
doi={10.1145/581630.581638},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144289189&doi=10.1145%2f581630.581638&partnerID=40&md5=2c027e7a6f0143a23c4f81fc0bfec696},
abstract={Power consumption has become an increasingly important factor in the field of computer architecture. It affects issues such as heat dissipation and packaging cost, which in turn affects the design and cost of a mobile terminal. Today, a lot of effort is put into the design of architectures and software implementation to increase performance. However, little is done on a system level to minimize power consumption, which is crucial in mobile systems. We propose an adaptive chip-multiprocessor (CMP) architecture, where the number of active processors is dynamically adjusted to the current workload need in order to save energy while preserving performance. The architecture is suitable in future mobile terminals where we anticipate a bursty and performance demanding workload. We have carried out an evaluation of the performance and power consumption of the proposed architecture using previously validated high-level simulation models. Our experiments show that orders of magnitude in power consumption can be saved compared to a conventional architecture to a negligable performance cost. The method used is complementary to other power saving techniques such as voltage and frequency scaling. Copyright 2002 ACM.},
author_keywords={Chip-multiprocessor (CMP);  Energy-aware scheduling;  Mobile terminals;  Power consumption},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rohrer2002173,
author={Rohrer, M.W. and McGregor, I.W.},
title={Simulating reality using automod},
journal={Winter Simulation Conference Proceedings},
year={2002},
volume={1},
pages={173-181},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036927457&partnerID=40&md5=4306b7a3d6ffa234f64dbcc37464c347},
abstract={Decision making in industry has become more complicated in recent years. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Brooks-PRI Automation has been used on thousands of projects to help engineers and managers make the best decisions possible. With the release of AutoMod 11.0 in 2002, AutoMod now supports hierarchical model construction. This new architecture allows users to reuse model objects in other models, decreasing the time required to build a model. Composite models are just one of the latest advances that make AutoMod one of the most widely used simulation software packages.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Flaxman2002275,
author={Flaxman, M.},
title={Using the Virtual Terrain Project to plan real cities: Alternative futures for Hangzhou, China},
journal={ACM SIGGRAPH 2002 Conference Abstracts and Applications, SIGGRAPH 2002},
year={2002},
pages={275},
doi={10.1145/1242073.1242288},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945953105&doi=10.1145%2f1242073.1242288&partnerID=40&md5=79fa2c4425e64f6d36c7e286c9edc7d8},
abstract={Traditional architectural software tools are well suited to individual building design, but don't scale up well to large landscapes and cities. In city and regional planning applications, it is important to visualize city blocks and neighborhoods, a task which requires integration of data from geographic information systems and 3D surface models from CAD. However, the development of semantically and visually rich real-time representations of cities and large landscapes has proven difficult. Two major problems are the acquisition of appropriate data, and the volume of data once acquired. A typical city or suburban landscape may have thousands of streets, tens of thousands of buildings, and hundreds of thousands of trees and bushes. Three dimensional data specifically describing all of these objects is not usually available, and visual simulations must be based on abstract classifications originally developed for entirely different purposes, such as tax assessor's databases or generalized vegetation maps. Fortunately, several powerful data management techniques are available, including procedural modeling based on GIS attributes and perceptually-based level of detail (LOD) management. While both approaches have been known for some time, advances in technical implementation and hardware have only recently made it possible to generate real-time visualizations of large areas using standard personal computers. The key is to automatically generate the appropriate level of detail from large geographic data sets using representational techniques specific to the class of object and its perceptual characteristics. The Virtual Terrain Project1 (VTP) is an open-source software project whose goal is to foster the creation of tools for easily constructing any part of the real world in interactive, 3D digital form (Discoe, 2002). VTP software is cross platform and requires no specific hardware other than a good OpenGL graphics card. It can render terrain with continuous level of detail simplification, and provides switchable LOD representation for both surface vegetation and buildings. Also, VTP can import data from an enormous variety of CAD and GIS data formats, even converting between multiple spatial coordinate systems.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Arlat2002138,
author={Arlat, J. and Fabre, J.-C. and Rodríguez, M. and Salles, F.},
title={Dependability of COTS microkernel-based systems},
journal={IEEE Transactions on Computers},
year={2002},
volume={51},
number={2},
pages={138-163},
doi={10.1109/12.980005},
note={cited By 98},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036472860&doi=10.1109%2f12.980005&partnerID=40&md5=889ae24b3b7b16acc8f4c6e58b411d3e},
abstract={The commercial offer concerning microkernel technology constitutes an attractive for developing operating systems to suit a wide range of application domains. However, the integration of COTS microkernels into critical embedded computer systems is a problem for system developers, in particular due to the lack of objective data concerning their behavior in the presence of faults. This paper addresses this issue by describing a prototype environment (MAFALDA: Microkernel Assessment by Fault injection AnaLysis and Design Aid) that is aimed at providing objective failure data on a candidate microkernel and also improving its error detection capabilities. The paper first presents the overall architecture of MAFALDA. Then, a case study carried out on an instance of the Chorus microkernel is used to illustrate the benefits that can be obtained with MAFALDA both from the dependability assessment and design-aid viewpoints. Implementation issues are also addressed that account for the specific API of the target microkernel. Some overall insights and lessons learned, gained during the various studies conducted on both Chorus and another target microkernel (LynxOS), are then depicted and discussed. Finally, we conclude the paper by summarizing the main features of the work presented and by identifying future research.},
author_keywords={COTS microkernels;  Dependability characterization;  Error confinement;  Fault injection;  Wrapping},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vachharajani2002271,
author={Vachharajani, M. and Vachharajani, N. and Penry, D.A. and Blome, J.A. and August, D.I.},
title={Microarchitectural exploration with Liberty},
journal={Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
year={2002},
volume={2002-January},
pages={271-282},
doi={10.1109/MICRO.2002.1176256},
art_number={1176256},
note={cited By 105},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983179859&doi=10.1109%2fMICRO.2002.1176256&partnerID=40&md5=ab1db1b733314f9f82c0df47bd71ce0e},
abstract={To find the best designs, architects must rapidly simulate many design alternatives and have confidence in the results. Unfortunately, the most prevalent simulator construction methodology, hand-writing monolithic simulators in sequential programming languages, yields simulators that are hard to retarget, limiting the number of designs explored, and hard to understand, instilling little confidence in the model. Simulator construction tools have been developed to address these problems, but analysis reveals that they do not address the root cause, the error-prone mapping between the concurrent, structural hardware domain and the sequential, functional software domain. This paper presents an analysis of these problems and their solution, the Liberty Simulation Environment (LSE). LSE automatically constructs a simulator from a machine description that closely resembles the hardware, ensuring fidelity in the model. Furthermore, through a strict but general component communication contract, LSE enables the creation of highly reusable component libraries, easing the task of rapidly exploring ever more exotic designs. © 2002 IEEE.},
author_keywords={Analytical models;  Computational modeling;  Computer languages;  Contracts;  Electronic switching systems;  Hardware;  Libraries;  Microarchitecture;  Product design;  Writing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lu2002889,
author={Lu, T. and Wu, G.},
title={The war-gaming training system based on HLA distributed architecture},
journal={Proceedings - International Conference on Computers in Education, ICCE 2002},
year={2002},
pages={889-893},
doi={10.1109/CIE.2002.1186104},
art_number={1186104},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964496586&doi=10.1109%2fCIE.2002.1186104&partnerID=40&md5=da4f6f1583065a4e240acc22a3f3d1bf},
abstract={This paper describes the design of HLA distributed architecture to support an interactive, interoperable, and collaborative war-gaming simulation using Java technology and IEEE standard 1516-High Level Architecture (HLA). Based on the run-time infrastsructure (RTI) services which are specified in HLA and Java application programming interface (API) of the RTI, the proposed distributed virtual environment (DVE) provides a practical foundation to enhance interactivity, portability, and interoperability for distributed simulation. Moreover, we build up a 3D synthetic virtual world for aircraft simulator by means of Java 3D API in which Java 3D technology could support a simple, portable, and flexible programming model for 3D scene construction. From the system implementation and experimental results, we show that the proposed HLA distributed architecture is a practical and scalable design that is applicable for a wide variety of moderate DVEs. © 2002 IEEE.},
author_keywords={Distributed Virtual Environment (DVE);  High Level Architecture (HLA);  Java 3D API},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wong2002449,
author={Wong, S. and Stougie, B. and Cotofana, S.},
title={Alternatives in FPGA-based SAD implementations},
journal={Proceedings - 2002 IEEE International Conference on FieId-Programmable Technology, FPT 2002},
year={2002},
pages={449-452},
doi={10.1109/FPT.2002.1188733},
art_number={1188733},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962877282&doi=10.1109%2fFPT.2002.1188733&partnerID=40&md5=873ee399e47ec052f30cde2440cdad61},
abstract={In multimedia processing, it is well-known that the sum-of-absolute-differences (SAD) operation is the most time-consuming operation when implemented in software running on programmable processor cores. This is mainly due to the sequential characteristic of such an implementation. In this paper, we investigate several hardware implementations of the SAD operation and map the most promising one in FPGA. Our investigation shows that an adder tree based approach yields the best results in terms of speed and area requirements and has been implemented as such by writing high-level VHDL code. The design was functionally verified by utilizing the MAX+plus II 10.1 Baseline software package from Altera Corp. and then synthesized by utilizing the LeonardoSpectrum software package from Exemplar Logic Inc. Preliminary results show that the design can be clocked at 380 MHz. This result translates into a faster than real-time full search in motion estimation for the main profile/main level of the MPEG-2 standard. © 2002 IEEE.},
author_keywords={field-programmable gate array;  hardware synthesis;  sum of absolute difference},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DiPenta2002319,
author={Di Penta, M. and Neteler, M. and Antoniol, G. and Merlo, E.},
title={Knowledge-based library re-factoring for an open source project},
journal={Proceedings - Working Conference on Reverse Engineering, WCRE},
year={2002},
volume={2002-January},
pages={319-328},
doi={10.1109/WCRE.2002.1173089},
art_number={1173089},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952935897&doi=10.1109%2fWCRE.2002.1173089&partnerID=40&md5=4251df3d3144a6f4e6b8ac3d97094ead},
abstract={Software miniaturization is a form of software re-factoring focused on reducing an application to its bare bones. Porting an application on a hand-held device is likely to require a preliminary step of software miniaturization, plus the development of device drivers dedicated to the new environment and hardware architecture. This paper presents the process and lessons learned in re-factoring a large open source application to remove any excess, introduce shared libraries, remove circular dependencies among libraries and, more generally, to minimize inter-library dependencies. While the final goal was to fully exploit shared library capabilities, among the various possibilities we defined a process based on existing knowledge about the application, and aimed to minimize the maintenance effort required by miniaturization activities. © 2002 IEEE.},
author_keywords={clustering;  Re-factoring;  re-modularization;  software evolution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Oh2002145,
author={Oh, S.-C. and Chung, S.-H. and Jang, H.},
title={Design and implementation of CC-NUMA card II for SCI-based PC clustering},
journal={Proceedings - IEEE International Conference on Cluster Computing, ICCC},
year={2002},
volume={2002-January},
pages={145-151},
doi={10.1109/CLUSTR.2002.1137739},
art_number={1137739},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948974704&doi=10.1109%2fCLUSTR.2002.1137739&partnerID=40&md5=63b63a11b324e238ed1a5b72af6cb9f4},
abstract={It is extremely important to minimize network access time when constructing a high-performance PC cluster system. For an SCI-based PC cluster it is possible to reduce the network access time by maintaining network cache in each cluster node. This paper presents the second version CC-NUMA card (CC-NUMA card II) that utilizes network cache for SCI-based PC clustering. The CC-NUMA card II is directly plugged into the PCI slot of each node, and contains shared memory, network cache, a shared memory control module and network control module. The network cache is maintained for shared memory on the PCI bus of cluster nodes. The coherency mechanism between network cache and shared memory is based on the IEEE SCI standard. In previous research, the first version CC-NUMA card (CC-NUMA card I) was developed. The CC-NUMA card I adopting Dolphin's PCI-SCI card as the network control module caused overhead in exchanging data between the remote nodes. In this paper, the overhead is removed by developing the CC-NUMA card II that combines the shared memory control module and network control module in a single card. Throughout the experiment with the SPLASH-2 benchmark suite, the CC-NUMA card II based PC cluster shows better performance than a NUMA system based on Dolphin's PCI-SCI card. © 2002 IEEE.},
author_keywords={Computer networks;  Costs;  Electronic commerce;  Ethernet networks;  High-speed networks;  Maintenance engineering;  Read-write memory;  Software libraries;  Web and internet services;  Web server},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor20021,
title={7th International Conference on Software Reuse, ICSR, 2002},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2002},
volume={2319},
pages={1-352},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948965912&partnerID=40&md5=5e5d688a727872845c1a7f9b2a5653ca},
abstract={The proceedings contain 38 papers. The special focus in this conference is on Software Reuse. The topics include: Integrating and reusing GUI-driven applications; source tree composition; layered development with(unix) dynamic libraries; concurrent execution with sequential reasoning; concepts and guidelines of feature modeling for product line software engineering; domain modeling for world wide web based software product lines with UML; enhancing component reusability through product line technology; modeling variability with the variation point model; the impact of open-source on commercial vendors; integrating reference architecture definition and reuse investment planning; control localization in domain specific translation; model reuse with metamo del-based transformations; generation of text search applications for databases; domain networks in the software development process; supporting reusable use cases; project management knowledge reuse through scenario models; adaptation of coloured petri nets models of software artifacts for reuse; improving hazard classification through the reuse of descriptive arguments; service oriented programming: a new paradigm of software reuse; an empirical user study of an active reuse repository system; towards the formalization of a reusability framework for refactoring; extending abstract factories to decouple advanced dependencies; software fortresses; the case against a grand unification theory; international workshop on reuse economics; workshop on generative programming 2002 (GP2002); industrial experience with product line approaches; workshop on software reuse and agile approaches; software architecture quality analysis methods; tutorial on practical product line scoping and modeling; generative reuse for software generation, maintenance and reengineering; building reusable test assets for a product line; architecture-centric software engineering; practical strategies and techniques for adopting software product lines; methods, techniques, and applications.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Kapetanios200220,
author={Kapetanios, E. and Baer, D. and Groenewoud, P. and Mueller, P.},
title={The design and implementation of a meaning driven data query language},
journal={Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM},
year={2002},
volume={2002-January},
pages={20-23},
doi={10.1109/SSDM.2002.1029702},
art_number={1029702},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948653106&doi=10.1109%2fSSDM.2002.1029702&partnerID=40&md5=615db0e5a621bf2bae8fd97db468386e},
abstract={We present the design and implementation of a meaning driven data query language-MDDQL-which aims at the construction of queries through system made suggestions of natural language based query terms for both scientific application domain terms and operator/operation terms. A query construction blackboard is used where query language terms are suggested to the user in its preferred natural language and in a name centered way, together with their connotation. This helps in understanding the meaning of the terms and/or operators or operations to be included in the query. Furthermore, the construction of the query turns out to be an incremental refinement of the query under construction through semantic constraints, where only those domain language terms and/or operators/operations are suggested which result into meaningful combinations of query terms as related to the scientific application domain semantics. Therefore, semantically meaningless queries can be prevented during the query construction. Such a semantics aware mechanism is not available in conventional database query languages such as SQL, where one is allowed to execute a query calculating, for example, the average of numerical data values whereas they represent the codes of categorical values. Moreover no familiarity with the semantics of complex database schemes or interpretation of the symbols (names of classes/tables/attributes, value codes) underlying the storage model, as well as familiarity with the syntax of a database specific query language are needed by the end-user. © 2002 IEEE.},
author_keywords={Application software;  Computer science;  Data models;  Database languages;  Database systems;  Engines;  Natural languages;  Ontologies;  Software packages;  Vocabulary},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Devai2002273,
author={Devai, F. and Rahman, M.},
title={The suitability of the Java language for computer-graphics and visualisation research},
journal={Proceedings of the International Conference on Information Visualisation},
year={2002},
volume={2002-January},
pages={273-278},
doi={10.1109/IV.2002.1028788},
art_number={1028788},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910063926&doi=10.1109%2fIV.2002.1028788&partnerID=40&md5=52ce44812b1436c76566cf8b75fc80ed},
abstract={The potential and the limitations of the Java language for graphics and visualisation research are evaluated. Evidence is offered that Java is adequate for high-performance computing and still has the added benefit of the portability of its compiled byte code over a wide range of platforms. It is demonstrated that, contrary to popular belief, there is an efficient way for reading and writing individual pixels. It is also demonstrated that graphics primitives can be drawn in one memory access per pixel even without pointer arithmetics. Just-in-time and hot-spot compiler technologies considerably improve the speed of the executable code, though some penalty remains for dynamic binding. Some inconsistencies and flaws in the design and implementations of the Java class libraries are noted. In general, however, Java offers both portability and adequate performance of the executable code, and also increased productivity due to its extensive class libraries, therefore it should be a serious consideration for researchers, developers and educators in graphics and visualisation. © 2002 IEEE.},
author_keywords={Arithmetic;  Computer graphics;  Databases;  Java;  Packaging;  Productivity;  Programming;  Software libraries;  Visualization;  Writing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hwang200265,
author={Hwang, R.Y.},
title={The design and implementation of mobile navigation system for the digital libraries},
journal={Proceedings of the International Conference on Information Visualisation},
year={2002},
volume={2002-January},
pages={65-69},
doi={10.1109/IV.2002.1028757},
art_number={1028757},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898451997&doi=10.1109%2fIV.2002.1028757&partnerID=40&md5=f7bbc2840c5b6ae16db174da0d8fa8e7},
abstract={A mobile navigation system is designed and implemented in this paper. This system can be applied to the digital libraries to make a real time query in the library. The information provided for this system includes data and voice. The major functions of this system include information access and destination guidance. Other value-added functions can also be finished easily. There are three major parts in this system. Mobile navigation device is designed for the visitors of the libraries. In this device, wireless equipments such as bluetooth technology and IrDA device are employed to finish the major function. Man-machine interface is designed to receive the information from the mobile navigation device. Besides, man-machine interface also processes the function of destination guidance. As for the information access, interface PC will pass data to the host computer to get the queried data and then pass the information to the mobile device via wireless module. The third part of the system is host computer with SQL database. The prototype of this system had been almost finished. Now, The test work is in progress. Evidences display that there is a nice effect. © 2002 IEEE.},
author_keywords={Bluetooth;  Computer interfaces;  Databases;  Mobile computing;  Navigation;  Prototypes;  Real time systems;  Software libraries;  Testing;  User interfaces},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Corno2002120,
author={Corno, F. and Cumani, G. and Sonza Reorda, M. and Squillero, G.},
title={Automatic test program generation from RT-level microprocessor descriptions},
journal={Proceedings - International Symposium on Quality Electronic Design, ISQED},
year={2002},
volume={2002-January},
pages={120-125},
doi={10.1109/ISQED.2002.996710},
art_number={996710},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-21444454039&doi=10.1109%2fISQED.2002.996710&partnerID=40&md5=661bb1dfbc262dfb6f269918c491e815},
abstract={The paper addresses the issue of microprocessor and microcontroller testing, and follows an approach based on the generation of a test program. The proposed method relies on two phases: in the first, a library of code fragments (named macros) is generated by hand based on the knowledge of the instruction set, only. In the second phase, an optimization algorithm is run to suitably select macros and values for their parameters. The algorithm only relies on RT-level information, and exploits a suitable RT-level fault model to guide the test program generation. A major advantage of the proposed approach lies in the fact that it does not require any knowledge about the low level implementation of the processor. Experimental results gathered on an i8051 model using a prototypical implementation of the approach show that it is able to generate test programs whose gate-level fault coverage is higher than the one obtained by comparable gate-level ATPG tools, while the computational effort and the length of the generated test program are similar. © 2002 IEEE.},
author_keywords={Automatic test pattern generation;  Automatic testing;  Design for testability;  Inspection;  Libraries;  Microcontrollers;  Microprocessors;  Monitoring;  Performance evaluation;  Prototypes},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ohki200252,
author={Ohki, M. and Akiyama, S. and Kambayashi, Y.},
title={A Verification of Class Structure Evolution Model and its Parameters},
journal={International Workshop on Principles of Software Evolution (IWPSE)},
year={2002},
pages={52-56},
doi={10.1145/512048.512049},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442352337&doi=10.1145%2f512048.512049&partnerID=40&md5=6456a2abdd74cff955cb7950d347662f},
abstract={It is widely accepted that the role of software "architect" that provide frameworks to program developers is important in the object-oriented software development processes. When developers try to extend the base classes given by the architect, they may want some guidelines that tell them how many subclasses and how many methods in one subclass are reasonable. So far we are not aware of such guidelines. Through measurements of Java and Delphi class libraries, we have distilled formulae that forecast the number of methods and the number of subclasses when constructing class trees from the base classes. We propose that we should focus to extract methods and attributes rather than class structure. The formulae we have formulated support this proposition.},
author_keywords={Architect;  Evolution model;  Measurement;  Verification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Moore20021445,
author={Moore, K. and Dongarra, J.},
title={Netbuild: Transparent cross-platform access to computational software libraries},
journal={Concurrency and Computation: Practice and Experience},
year={2002},
volume={14},
number={13-15},
pages={1445-1456},
doi={10.1002/cpe.670},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036870406&doi=10.1002%2fcpe.670&partnerID=40&md5=c510930e080512037d0b81f832fa3fdb},
abstract={NetBuild is a suite of tools which automate the process of selecting, locating, downloading, configuring, and installing computational software libraries from over the Internet, and which aid in the construction and cataloging of such libraries. Unlike many other tools, NetBuild is designed to work across a wide variety of computing platforms, and perform fine-grained matching to find the most suitable version of a library for a given target platform. We describe the architecture of NetBuild and its initial implementation.},
author_keywords={Automatic library selection;  Computational software;  Software libraries},
document_type={Article},
source={Scopus},
}

@ARTICLE{DeSouza2002395,
author={De Souza, M.A.F. and Ferreira, M.A.G.V.},
title={Designing reusable rule-based architectures with design patterns},
journal={Expert Systems with Applications},
year={2002},
volume={23},
number={4},
pages={395-403},
doi={10.1016/S0957-4174(02)00075-1},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036836816&doi=10.1016%2fS0957-4174%2802%2900075-1&partnerID=40&md5=153ccfdf4a1cb97b6e50f802ae8242ce},
abstract={Rule-based systems or production systems still have great importance in the construction of knowledge systems. In these systems, the domain expertise to solve a problem is encoded in the form of 'if-then' rules, enabling a modular description of the knowledge, thus facilitating its maintenance and updating. Although they have been extensively described in the Artificial Intelligence literature, their design process is at times repeated because of the lack of common software architecture and the restrictions offered by some off-the-shelf libraries and systems. This paper proposes a reusable architecture for rule-based systems described through design patterns. The aim of these patterns is to constitute a design catalog that can be used by designers to understand and create new rule-based systems, thus promoting reuse in these systems. Additionally, the use of the described patterns in the design of an intelligent tutoring system architecture is exemplified. © 2002 Elsevier Science Ltd. All rights reserved.},
author_keywords={Design patterns;  Reuse in rule-based systems;  Rule-based systems},
document_type={Short Survey},
source={Scopus},
}

@ARTICLE{Niere2002338,
author={Niere, J. and Schäfer, W. and Wadsack, J.P. and Wendenhals, L. and Welsh, J.},
title={Towards pattern-based design recovery},
journal={Proceedings-International Conference on Software Engineering},
year={2002},
pages={338-348},
doi={10.1145/581380.581382},
art_number={33},
note={cited By 147},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036036578&doi=10.1145%2f581380.581382&partnerID=40&md5=939e19586e40e9b5a0b35907d8ad6ced},
abstract={A method and a corresponding tool is described which assist design recovery and program understanding by recognising instances of design patterns semi-automatically. The approach taken is specifically designed to overcome the existing scalability problems caused by many design and implementation variants of design pattern instances. Our approach is based on a new recognition algorithm which works incrementally rather than trying to analyse a possibly large software system in one pass without any human intervention. The new algorithm exploits domain and context knowledge given by a reverse engineer and by a special underlying data structure, namely a special form of an annotated abstract syntax graph. A comparative and quantitative evaluation of applying the approach to the Java AWT and JGL libraries is also given.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Iribarne200130,
author={Iribarne, L. and Troya, J.M. and Vallecillo, A.},
title={Trading for COTS components in open environments},
journal={Conference Proceedings of the EUROMICRO},
year={2001},
pages={30-37},
doi={10.1109/EURMIC.2001.952435},
art_number={952435},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888994043&doi=10.1109%2fEURMIC.2001.952435&partnerID=40&md5=e00d4b9e03f5de76ffb0cbf2fe271552},
abstract={Trading is a well-known mechanism for searching and locating services in object-oriented systems. However, current traders present some limitations when used in open component-based system environments. In this paper we analyze the required features that COTS components traders should have, and describe the design and implementation of COTStrader; an Internet-based trader for COTS components that handles the heterogeneity, scalability and evolution of COTS traders. © 2001 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Clarke200193,
author={Clarke, D. and Yancey, T.},
title={Twenty-first century tools for vocabulary management & indexing},
journal={Proceedings of the ASIST Annual Meeting},
year={2001},
volume={38},
pages={93-98},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039560941&partnerID=40&md5=dff40f87147aa84e42152ba63ad1b48a},
abstract={For the past four years Synapse, the Knowledge Link Corporation, has been consulting with key companies in the information world to design and develop a universal knowledge management system that can be deployed on intranets and extranets - a highly customizable and scalable software solution for managing taxonomies, thesauri, classification schemes, authority control files and indexes. In 2001 the Synaptica(R) software application has reached a sufficient critical mass of functionality to be able to support a wide variety of vocabulary management and indexing tasks: (i) standard micro-and meta-thesaurus construction in compliance with ANSI/NISO Z39.19-1993; (ii) authority control management including concept clustering of multiple preferred terms; (iii) user-configurable record structures, attributes, relationship types and syndetic rules-bases; (iv) sophisticated control of content access and user permissions; (v) intranet / extranet client-server networking with web-based user-interface for all editorial tasks and systems administration tools; (vi) complete COM library and API interface; (vii) web-based indexing tools that interface with full text document management systems to allow vocabularies, thesauri or authority files, to be searched and browsed and for terminology to be embedded directly back into web-pages, documents or meta-headers. Synaptica has been adopted as the enterprise-wide vocabulary management system by a number of companies from diverse industry sectors and business applications: from traditional publishing companies and web-directory publishers to companies that just need to manage the internal information resources of corporate databases or intranets. Web-based technology offers a unique opportunity to enable teams of in-house staff to work collaboratively with location-independent staff or contractors. What are going to be the new vocabulary management and indexing needs of the twenty first century? Synapse Corporation is attempting to identify these needs through discussions with companies and organizations worldwide. Some topics of current interest include: (i) drill-down taxonomies specifically geared to producers of web-based directories - classification structures that combine top-down hierarchical pathways and multifaceted classification principles; (ii) interfaces with automated or computer-assisted indexing systems. The presentation will be most relevant to the ASIST topics of Classification and Representation and of Information Discovery, Capture and Creation. The presentation will benefit information professionals by providing: (i) a practical demonstration of how a web-based information architecture can support sophisticated thesaurus management, authority control and indexing tasks; and (ii) a forum for audience members to discuss emerging needs for new software tools in the field of vocabulary management, classification and indexing.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kilgore2001262,
author={Kilgore, R.A.},
title={Open-source SML and Silk for java-based, object-oriented simulation},
journal={Winter Simulation Conference Proceedings},
year={2001},
volume={1},
pages={262-268},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035693916&partnerID=40&md5=7ef821240f41e644e8103666274a1f67},
abstract={Silk® and SML are software libraries of Java, C++, C# and VB.Net classes that support object-oriented, discrete-event simulation. SMLTM is a new open-source or "free" software library of simulation classes that enable multi-language development of complex, yet manageable simulations through the construction of usable and reusable simulation objects. These objects are usable because they express the behavior of individual entity-threads from the system object perspective using familiar process-oriented modeling within an object-oriented design supported by a general purpose programming language. These objects are reusable because they can be easily archived, edited and assembled using professional development environments that support multi-language, cross-platform execution and a common component architecture. This introduction supports the tutorial session that describes the fundamentals of designing and creating an SML or Silk model.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shih2001139,
author={Shih, T.K. and Chang, S.-K. and Tsai, J. and Ma, J. and Huang, R.},
title={Supporting Well-Engineered Web Documentation Development - A Multimedia Software Engineering Approach toward Virtual University Courseware Designs},
journal={Annals of Software Engineering},
year={2001},
volume={12},
number={1},
pages={139-165},
doi={10.1023/A:1013366823005},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035565846&doi=10.1023%2fA%3a1013366823005&partnerID=40&md5=65beafd58a8f20ce72b43bb0e427cb2a},
abstract={Distance learning has become a very important mechanism for virtual university operation. In order to realize such an operation smoothly, it is necessary to consider distance learning from three perspectives: administration, awareness, and assessment. We are currently implementing a virtual university environment according to these guidelines. In this paper, we propose part of such a supporting environment of the Multimedia Macro-University project.1 One of the most important focuses is a Web course development paradigm. Web documents are increasingly playing an important role in our daily life, as the Internet has become a new medium for communication and commerce. On the other hand, software development paradigms were developed to support program construction. However, these traditional paradigms do not completely fit the needs of Web document development due to the following reasons. Firstly, computer programs focus on problem solving, but Web documents focus on information delivery. Secondly, computer programs usually have a fixed size, but this is not true for Web documents, because Web documents are always evolving as if it were a living document. It is therefore necessary to investigate a new software development paradigm for developing Web documents. We propose such a new paradigm and its supporting environment, as well as software testing/metrics mechanisms for Web documents. The Web documents developed using our paradigm are stored in a Web documentation database. From a script description, to its implementation as well as testing records, the database and its interface allow the user to design Web documents as virtual courses to be used in a Web-savvy virtual library. The system is implemented as a three-tier architecture, which runs under MS Windows.},
author_keywords={Active documents;  Multimedia database;  Multimedia software engineering;  Software metrics;  Software testing;  Virtual university;  Web documents;  WWW},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Azimi2001943,
author={Azimi, M. and Nasiopoulos, P. and Ward, R.K.},
title={Implementation of MPEG system target decoder},
journal={Canadian Conference on Electrical and Computer Engineering},
year={2001},
volume={2},
pages={943-948},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034851971&partnerID=40&md5=ac09e6cc4f4e45fe82d45e7343288929},
abstract={The MPEG-2 system standard is being widely used as a transport system to deliver compressed video, audio and other multimedia contents in various applications such as digital television broadcasting and video communications. This standard provides methods for multiplexing a number of elementary MPEG streams into a single system stream. It also defines methods to maintain the synchronization and timing of compressed streams. This is achieved by exact definitions of the times at which data arrive to the decoder, timing of data flow in the decoder and timing of decoding and presentation events. For this purpose, the standard defines a conceptual model for a target decoder, called "System Target Decoder" (STD), which is used to model the decoding process. System streams generated by the multiplexer should comply with the specifications imposed by the STD model to guarantee the normal operations of real time decoding and presentation process. Therefore, this model is necessary during the construction and verification of system streams. The multiplexer should observe the behavior of STD to ensure that decoder buffers will not overflow or underflow due to encoding or multiplexing issues when receiving the system stream. To achieve this, the scheduler that coordinates the multiplexing order of system packs should consider the monitored information from STD as one of the scheduling control parameters to follow the specifications imposed by STD. This paper describes the theoretical principles, design considerations and architecture of Program and Transport STDs. The implementations of these target decoders in a software package for verification of MPEG system streams is presented. This implementation uses the Microsoft DirectShow and has many attractive features as modularity, generating full reports of buffer occupancies, and supporting all levels of buffering (Transport level buffering, Multiplex buffering and Elementary Stream buffering) of STD. The results of decoding some sample system streams are also presented.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peña-Mora2001173,
author={Peña-Mora, F. and Vadhavkar, S. and Dirisala, S.K.},
title={Component-based software development for integrated construction management software applications},
journal={Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM},
year={2001},
volume={15},
number={2},
pages={173-187},
doi={10.1017/S0890060401152054},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035300579&doi=10.1017%2fS0890060401152054&partnerID=40&md5=3b8d64d5c0b0f02a708a96a61f2801b3},
abstract={This paper presents a framework and a prototype for designing Integrated Construction Management (ICM) software applications using reusable components. The framework supports the collaborative development of ICM software applications by a group of ICM application developers from a library of software components. The framework focuses on the use of an explicit software development process to capture and disseminate specialized knowledge that augments the description of the ICM software application components in a library. The importance of preserving and using this knowledge has become apparent with the recent trend of combining the software development process with the software application code. There are three main components in the framework: design patterns, design rationale model, and intelligent search algorithms. Design patterns have been chosen to represent, record, and reuse the recurring design structures and associated design experience in object-oriented software development. The Design Recomm endation and Intent Model (DRIM) was extended in the current research effort to capture the specific implementation of reusable software components. DRIM provides a method by which design rationale from multiple ICM application designers can be partially generated, stored, and later retrieved by a computer system. To address the issues of retrieval, the paper presents a unique representation of a software component, and a search mechanism based on Reggia's setcover algorithm to retrieve a set of components that can be combined to get the required functionality is presented. This paper also details an initial, proof-of-concept prototype based on the framework. By supporting nonobtrusive capture as well as effective access of vital design rationale information regarding the ICM application development process, the framework described in this paper is expected to provide a strong information base for designing ICM software.},
author_keywords={Component-based software;  Design patterns;  Design rationale;  Integrated construction management applications;  Intelligent search mechanisms;  Process planning;  Software framework},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Antoniu20011089,
author={Antoniu, G. and Hatcher, P.},
title={Remote object detection in cluster-based java},
journal={Proceedings - 15th International Parallel and Distributed Processing Symposium, IPDPS 2001},
year={2001},
pages={1089-1094},
doi={10.1109/IPDPS.2001.925077},
art_number={925077},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981156827&doi=10.1109%2fIPDPS.2001.925077&partnerID=40&md5=6151386a7d3c1a02771acce18b5073ed},
abstract={Our work combines Java compilation to native code with a run-time library that executes Java threads in a distributed-memory environment with true parallelism. This approach is implemented within the Hyperion system for the distributed execution of compiled Java programs on clusters of PCs. To provide the illusion of a shared memory to Java threads, Hyperion has been built on top of DSM-PM2, a portable implementation platform for multithreaded distributed-shared-memory protocols. We have designed, implemented and experimented with two alternative consistency protocols compliant with the Java Memory Model. The protocols have different mechanisms for access detection: the first one uses explicit locality checks, whereas the second one is based on page faults. We illustrate the effects of the two access-detection techniqueswith five applications run on two clusters with different interconnection networks: BIP/Myrinet and SISCI/SCI. © 2001 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schupp2001109,
author={Schupp, S. and Gregor, D. and Musser, D. and Liu, S.-M.},
title={Library transformations},
journal={Proceedings - 1st IEEE International Workshop on Source Code Analysis and Manipulation},
year={2001},
pages={109-121},
doi={10.1109/SCAM.2001.972672},
art_number={972672},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963808562&doi=10.1109%2fSCAM.2001.972672&partnerID=40&md5=66c97af6803c5ed1f7ee5fdbd241331d},
abstract={While software methodology encourages the use of software libraries and advocates architectures of layered libraries, in practice the composition of libraries is not always seamless and the combination of two well-designed libraries not necessarily well designed, since it could result in suboptimal call sequences, lost functionality, or avoidable overhead. In this paper we introduce Simplicissimus, a framework for rewrite-based source code transformations, that allows for code replacement in a systematic and safe manner. We discuss the design and implementation of the framework and illustrate its functionality with applications in several areas. Simplicissimus is integrated into the Gnu C++ compiler. © 2001 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2001,
title={Proceedings - 3rd International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems, WECWIS 2001},
journal={Proceedings - 3rd International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems, WECWIS 2001},
year={2001},
page_count={222},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963541965&partnerID=40&md5=872c33f8e8a56d7db28766b9baf27570},
abstract={The proceedings contain 30 papers. The topics discussed include: design and implementation of COSIMA - a smart and speaking e-sales assistant; distributed territorial data management and exchange for public organizations; data classification and management in very large data warehouses; data distribution strategies for providing database scalability in e-commerce applications; an evaluation of distributed computing options for a rule-based approach to black-box software component integration; an e-business integration & collaboration platform for b2b e-commerce; development of service integration platform for one-stop service applications; end-to-end transaction management for composite web based services; state recovery in internet transactions; an architecture-centric approach to multi-agent system development and application; an anonymous electronic voting protocol for voting over the internet; customized library of modules for STREAMS-based TCP/LP implementation to support content-aware request processing for web applications; and cache freshness optimization: sliding scale guarantees.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Schönwälder2001423,
author={Schönwälder, J. and Müller, A.},
title={Reverse engineering Internet MIBs},
journal={2001 7th IEEE/IFIP International Symposium on Integrated Network Management Proceedings: Integrated Management Strategies for the New Millennium},
year={2001},
pages={423-436},
doi={10.1109/INM.2001.918057},
art_number={918057},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952319478&doi=10.1109%2fINM.2001.918057&partnerID=40&md5=d71046fc6a0440b107ff53b5e55bb684},
abstract={The Internet-standard management protocol SNMP manipulates data structures that are defined in MIB modules. A large number of MIB modules has been defined over the last decade. Some of them are rather complex and full of technical details, which makes it hard to understand them. Furthermore, the limitations of the current data definition language make it impossible to formally express the conceptual model underlying a well-engineered MIB module. This paper presents a reverse engineering algorithm which extracts conceptual models from the MIB data definitions. The algorithm uses several heuristics that are derived from common MIB naming and registration conventions. The output produced by the algorithm is a graphical representation for conceptual MIB models, which is a slightly customized version of a unified modeling language (UML) class diagram. A prototype implementation of the algorithm is described which has been integrated into the libsmi software package. © 2001 IEEE.},
author_keywords={Data mining;  Data structures;  Heuristic algorithms;  Internet;  Protocols;  Reverse engineering;  Software algorithms;  Software packages;  Software prototyping;  Unified modeling language},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor20011,
title={4th International Conference on Information Security, ISC 2001},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2001},
volume={2200},
pages={1-553},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947230597&partnerID=40&md5=22ec95d39c81a850136b2641f2aa4349},
abstract={The proceedings contain 37 papers. The special focus in this conference is on Protocols, Enhancing Technologies, Privacy, Software Protection and Message Hiding. The topics include: Bounds and constructions for unconditionally secure distributed key distribution schemes for general access structures; privacy amplification theorem for noisy main channel; efficient kerberized multicast in a practical distributed setting; suitability of a classical analysis method for e-commerce protocols; a logical model for privacy protection; an approach to the obfuscation of control-flow of sequential computer programs; a practical and effective approach to large-scale automated linguistic steganography; robust new method in frequency domain watermarking; on the complexity of public-key certificate validation; elliptic curve arithmetic using SIMD; on the hardware implementation of the 3gpp confidentiality and integrity algorithms; efficient implementation of elliptic curve cryptosystems on an arm7 with hardware accelerator; a theoretical DPA-based cryptanalysis of the NESSIE candidates flash and SFLASH; securing elliptic curve point multiplication against side-channel attacks; a flexible role-based access control model for multimedia medical image database systems; a secure publishing service for digital libraries of xml documents; an optimistic non-repudiation protocol with transparent trusted third party; persistent authenticated dictionaries and their applications; efficient optimistic n-party contract signing protocol; efficient sealed-bid auctions for massive numbers of bidders with lump comparison; fingerprinting text in logical markup languages and an auditable metering scheme for web advertisement applications.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Picó2001289,
author={Picó, F.I. and Asensi, S.C. and Córcoles, V.},
title={Accelerating Statistical Texture Analysis with an FPGA-DSP Hybrid Architecture},
journal={Proceedings - 9th Annual IEEE Symposium on Field-Programmable Custom Computing Machines, FCCM 2001},
year={2001},
pages={289-290},
art_number={1420936},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148839192&partnerID=40&md5=5fbf30c1e90901b9b6641ca77e073333},
abstract={Nowadays, most image processing systems are implemented using either MMX-optimized software libraries or, when time requirements are limited, expensive high performance DSP-based boards. In this paper we present a texture analysis co-processor concept that permits the efficient hardware implementation of statistical feature extraction, and hardware-software codesign to achieve high-performance low-cost solutions. We propose a hybrid architecture based on FPGA chips, for massive data processing, and digital signal processor (DSP) for floating-point computations. In our preliminary trials with test images, we achieved sufficient performance improvements to handle a wide range of real-time applications. © 2001 Non IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Calderón2001185,
author={Calderón, A. and García, F. and Carretero, J. and Fernández, J. and Pérez, O.},
title={New techniques for collective communications in clusters: A case study with MPI},
journal={Proceedings of the International Conference on Parallel Processing},
year={2001},
volume={2001-January},
pages={185-192},
doi={10.1109/ICPP.2001.952062},
art_number={952062},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042693039&doi=10.1109%2fICPP.2001.952062&partnerID=40&md5=cdc1f40723c1e15b8175716d6bafee94},
abstract={The paper describes new techniques to increase the performance of collective communication operations in clusters. These techniqnes are based in multithreading operations and on-line data compression. The techniques proposed have been implemented in MiMPI, a thread-safe implementation of MPI. We have evaluated, and compared, the performance of MiMPI with other implementations of MPI available for clusters with Linux and Windows 2000. The benchmark used has been MPBench, a flexible and portable framework to allow benchmarking of MPI implementations. © 2001 IEEE.},
author_keywords={Computer aided software engineering;  Computer architecture;  Computer science;  Data compression;  Libraries;  Linux;  Multithreading;  Operating systems;  Sockets;  Yarn},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Berman2001327,
author={Berman, F. and Chien, A. and Cooper, K. and Dongarra, J. and Foster, I., Sr. and Gannon, D. and Johnsson, L. and Kennedy, K. and Kesselman, C., Sr. and Mellor-Crumme, J. and Reed, D. and Torczon, L. and Wolski, R.},
title={The GrADS project: Software support for high-level grid application development},
journal={International Journal of High Performance Computing Applications},
year={2001},
volume={15},
number={4},
pages={327-344},
doi={10.1177/109434200101500401},
note={cited By 228},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035551798&doi=10.1177%2f109434200101500401&partnerID=40&md5=5712f880d3a26adaf6e02ccec7875f56},
abstract={Advances in networking technologies will soon make it possible to use the global information infrastructure in a qualitatively different way-as a computational as well as an information resource. As described in the recent book The Grid: Blueprint for a New Computing Infrastructure, this Grid will connect the nation's computers, databases, instruments, and people in a seamless web of computing and distributed intelligence, which can be used in an on-demand fashion as a problem-solving resource in many fields of human endeavor-and, in particular, science and engineering. The availability of grid resources will give rise to dramatically new classes of applications, in which computing resources are no longer localized but, rather, distributed, heterogeneous, and dynamic; computation is increasingly sophisticated and multidisciplinary; and computation is integrated into our daily lives and, hence, subject to stricter time constraints than at present. The impact of these new applications will be pervasive, ranging from new systems for scientific inquiry, through computing support for crisis management, to the use of ambient computing to enhance personal mobile computing environments. To realize this vision, significant scientific and technical obstacles must be overcome. Principal among these is usability. The goal of the Grid Application Development Software (GrADS) project is to simplify distributed heterogeneous computing in the same way that the World Wide Web simplified information sharing over the Internet. To that end, the project is exploring the scientific and technical problems that must be solved to make it easier for ordinary scientific users to develop, execute, and tune applications on the Grid. In this paper, the authors describe the vision and strategies underlying the GrADS project, including the base software architecture for grid execution and performance monitoring, strategies and tools for construction of applications from libraries of grid-aware components, and development of innovative new science and engineering applications that can exploit these new technologies to run effectively in grid environments.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rasala2001297,
author={Rasala, R. and Raab, J. and Proulx, V.K.},
title={Java power tools: Model software for teaching object-oriented design},
journal={SIGCSE Bulletin (Association for Computing Machinery, Special Interest Group on Computer Science Education)},
year={2001},
pages={297-301},
doi={10.1145/366413.364606},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034992469&doi=10.1145%2f366413.364606&partnerID=40&md5=552176ff58737ef802410f815a950d9f},
abstract={The Java Power Tools or JPT is a Java toolkit designed to enable students to rapidly develop graphical user interfaces in freshman computer science programming projects. Because it is simple to create GUIs using JPT, students can focus on the more fundamental issues of computer science rather than on widget management. In a separate article, we will discuss with examples how the JPT can help freshman students to learn about the basics of algorithms, data structures, classes, and interface design. In this article, we will focus on how the JPT itself can be used as an extended case study of object-oriented design principles in a more advanced course. The fundamental design principles of the JPT are that the elements of a graphical user interface should be able to be combined recursively as nested views and that the communication between these views and the internal data models should be as automatic as possible. In particular, in JPT, the totality of user input from a complex view can be easily converted into a corresponding data model and any input errors will be detected and corrected along the way. This ease of communication is achieved by using string objects as a lingua franca for views and models and by using parsing when appropriate to automatically check for errors and trigger recovery. The JPT achieves its power by a combination of computer science and software design principles. Recursion, abstraction, and encapsulation are systematically used to create GUI tools of great flexibility. It should be noted that a much simpler pedagogical package for Java IO was recently presented in [9].},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Freund2001187,
author={Freund, E. and Lüdemann-Ravit, B. and Stern, O. and Koch, T.},
title={Creating the architecture of a translator framework for robot programming languages},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2001},
volume={1},
pages={187-192},
doi={10.1109/ROBOT.2001.932551},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034861117&doi=10.1109%2fROBOT.2001.932551&partnerID=40&md5=53a67ea7db8c66005ba35a0ee3ccc0a3},
abstract={This paper presents a novel approach to facilitate the development and maintenance of translators for industrial robot programming languages. Such translators are widely used in robot simulation and offline programming systems to support programming in the respective native robot language. Our method is based upon a software architecture, that is provided as a complete translator framework. For the developer of a new translator, it offers convenient strategies to concentrate on robot specific language elements during the design and implementation process: fill-in templates, libraries for common functionality, design patterns etc., all tied up with a general translation scheme. In contrast to other compiler construction tools, the developers need not care about the complex details of a whole translator. As a matter of principle, the architecture offers a complete default translator (except for the grammar). Robot specific elements can be held in separate units - outside of the actual translator - to facil itate maintenance and feature extension. The most probable changes in the translator product life cycle are restricted to the adaptation of these units. Several translators built upon this framework are in actual use in the commercial robot simulation system COSIMIR® to support native language robot programming, as well as in the widely used robot programming system COSIROP to verify the syntax of robot programs.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vasilko200170,
author={Vasilko, M. and Macháček, L. and Matej, M. and Stepien, P. and Holloway, S.},
title={A rapid prototyping methodology and platform for seamless communication systems},
journal={Proceedings of the International Workshop on Rapid System Prototyping},
year={2001},
pages={70-76},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034841968&partnerID=40&md5=9d92e0372a3263562c6b81fec9f0010f},
abstract={The availability of reconfigurable technologies has enabled the construction of fiexible systems allowing run-time reconfiguration of system hardware and software functions. "Seamless communications" (also known in radio communications worm as "reconfigurable" or "software-defined" radio) is one of the areas where technologies allowing run-time reconfiguration are highly desirable. This paper presents a new rapid prototyping methodology and platform for prototyping generic seamless communication systems. The methodology combines a C-based software design flow targeting host and DSP processors, and a rapid FPGA hardware design flow based on Handel-C a C-like programming language. The hardware design flow also supports the generation of partial FPGA configurations. A library, of parametrised communication modules was developed to facilitate the rapid construction of common communication architectures. A PC-based prototyping platform provides a set of custom hardware interfaces for prototyping systems with radio-frequency (RF), infrared (IR) and generic wide-bandwidth communication links. The feasibility of the presented methodology was tested on several simple demonstrator applications.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Friedrich2001177,
author={Friedrich, J.},
title={A more efficient design and implementation of CAL programs in natural science using object-oriented technology},
journal={Turkish Journal of Electrical Engineering and Computer Sciences},
year={2001},
volume={9},
number={2},
pages={177-188},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034801840&partnerID=40&md5=a5f759aa4c2a53ecef43675ea1dd1e90},
abstract={With the amount and complexity of science topics and applications increasing, the need for appropriate and effective science education is constantly growing. Computer-based education is very promising to help both teachers and learners in their difficult task, which involves complex psychological processes. This complexity is reflected in high demands on the design and implementation methods used to create computer-assisted learning (CAL) programs. Due to their concepts, flexibility, maintainability and extended library resources, object-oriented technology (OOT) is very suitable to producing this type of pedagogical tool. The introduced approach is demonstrated by a basketball simulation program for instruction in Newtonian mechanics covering topics like mass, acceleration, force, and the equation of motion. The overall goal of this work is to expose teachers to OOT and raise their interest to help them to participate more actively in the design and implementation of future CAL tools.},
author_keywords={Basketball simulation program;  Computer-assisted learning;  Newtonian mechanics;  Object-oriented design and implementation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wang2000116,
author={Wang, Xin and Yu, C. and Schulzrinne, Henning and Stirpe, Paul and Wu, Wei},
title={IP multicast fault recovery in PIM over OSPF},
journal={International Conference on Network Protocols},
year={2000},
pages={116-125},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034507141&partnerID=40&md5=e963123fc531f99413d9b4df1d041bd8},
abstract={Little attention has been given to understanding the fault recovery characteristics and performance tuning of native IP multicast networks. This paper focuses on the interaction of the component protocols to understand their behavior in network failure and recovery scenarios. We consider a multicast environment based on the Protocol Independent Multicast (PIM) routing protocol, the Internet Group Management Protocol (IGMP) and the Open Shortest Path First (OSPF) protocol. Analytical models are presented to describe the interplay of all of these protocols in various multicast channel recovery scenarios. Quantitative results for the recovery time of IP multicast channels are given as references for network configurations, and protocol development. Simulation models are developed using the OPNET simulation tool to measure the fault recovery time and the associated protocol control overhead, and study the influence of important protocol parameters. A testbed with five Cisco routers is configured with PIM, OSPF, and IGMP to measure the multicast channel failure and recovery times for a variety of different link and router failures. In general, the failure recovery is found to be light-weight in terms of control overhead and recovery time. Failure recovery time in a WAN is found to be dominated by the unicast protocol recovery process. Failure recovery in a LAN is more complex, and strongly influenced by protocol interactions and implementation specifics. Suggestions for improvement of the failure recovery time via protocol enhancements, parameter tuning, and network configuration are provided.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pollard2000363,
author={Pollard, J.K.},
title={Component-based architecture for simulation of transmission systems},
journal={Proceedings - IEEE Computer Society's International Computer Software and Applications Conference},
year={2000},
pages={363-368},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034503886&partnerID=40&md5=9123ff54b257f2286ee0ebe4e010f808},
abstract={Example transmission system simulations are used to illustrate criteria for quality architecture: Component interoperability, re-usability, reliability and maintainability. Top-level architectural issues such as system partition, encapsulation of components and a Graphical User Interface that is decoupled from the core software are considered. It is suggested that component communication should be: Write a file, signal a `commit' and then read by the recipient. This protocol allows input and output data types and ranges to be checked. An error code on failure allows roll-back to a previously saved state whereas a successful completion signal can be used as a sequential control. The desirable feature of very loosely coupled independent components implies insensitivity to construction technology. This allows the use of legacy and commercial software packages. In addition, components can be deployed on different types and scales of networks and can be fixed on computers and data transferred to them, or vice-versa.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Beuche2000147,
author={Beuche, D. and Frohlichy, A.A. and Meyer, R. and Papajewski, H. and Schony, F. and Schroder-Preikschat, W. and Spinczyk, O. and Spinczyk, U.},
title={On architecture transparency in operating systems},
journal={Proceedings of the 9th Workshop on ACM SIGOPS European Workshop: Beyond the PC: New Challenges for the Operating System, EW 2000},
year={2000},
pages={147-152},
doi={10.1145/566726.566758},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988253337&doi=10.1145%2f566726.566758&partnerID=40&md5=1265cd1df349ab435d3e98caa492ad62},
abstract={A Pure operating system is meant to be an \open operating system". All its abstractions are revealed to a system designer or even application programmer. The entire system is represented as a library, or a set of libraries, of small and \handy" object modules. These modules are small with respect to the number of exported references to functions or variables. This helps, e.g., state-of-the-art binders creating slim-line operating systems that contain only those components used (i.e. referenced) by a given application. Prerequisite however is a highlymodular system architecture and this is achieved by a familybased design and an object-oriented implementation. Pure has much in common with OSKit [4]. Instead of inventing a new system architecture, Pure provides abstractions that allows one to construct many of those architectures. An operating-system architecture is not prescribed by Pure. Rather, a construction set for the development of operating systems is established. Whether an operating system is monolithic or based, e.g., on micro-kernel technology, is up to the actual \mechanic" who uses Pure elements to create a product according to some blueprint. In order to create a tailor-made operating system, the blueprint comes from the application itself. Besides undertaking a stepwise enrichment of Pure by Unix-like system functions as part of student projects, future work also concentrates on providing a family-based Java execution environment. This plan encompasses two sorts of Java-related work: (1) PureJava, to add minimal Java-Extensions to Pure and (2) JavaPure, that aims at developing a family of streamlined Java virtual machines. The works are done in close cooperation with a major German automobil manufacturer to provide a run-time executive that combines the joint processing of soft real-time (Java-based) telematic services and hard real-time (C/C++/assembler-based) engine control functions. Portability, scalability, extensibility, and composability are the main keywords behind Pure to aim at the development of application-oriented operating systems. In this sense, the design and implementation of Pure adopts much of [2] to come up with a highly exible software structure.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shang2000182,
author={Shang, Y. and Shi, H.},
title={IDEAL: An integrated distributed environment for asynchronous learning},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2000},
volume={1830},
pages={182-191},
doi={10.1007/3-540-45111-0_21},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931828957&doi=10.1007%2f3-540-45111-0_21&partnerID=40&md5=c3906715d788b33f62f989de92995267},
abstract={In this paper, we present the design and implementation of IDEAL, an Integrated Distributed Environment for Asynchronous Learning. The learning environment supports a Web-based distributed community for student-centered, self-paced, and highly interactive learning. IDEAL enables the students in the community to learn from each other and enhances their learning experience. Implemented using the prevalent Internet, Web, intelligent agent, and digital library technologies, IDEAL adopts an open architecture design and targets at large-scale, distributed operations. In the initial implementation, a number of prototypes using different Java-based software development environments have been developed. © Springer-Verlag Berlin Heidelberg 2000.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zoraja2000158,
author={Zoraja, I. and Bode, A. and Sunderam, V.},
title={A framework for process migration in software DSM environments},
journal={Proceedings - 8th Euromicro Workshop on Parallel and Distributed Processing, EURO-PDP 2000},
year={2000},
pages={158-165},
doi={10.1109/EMPDP.2000.823407},
art_number={823407},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949642861&doi=10.1109%2fEMPDP.2000.823407&partnerID=40&md5=b496366d2792f03957580de9dc269c08},
abstract={Proves that process migration can successfully be implemented for software distributed shared memory (DSM) environments. We have developed a migration framework that is able to transparently migrate DSM processes, thereby preserving the consistency of running applications. The migration framework is integrated into the CORAL (Cooperative Online monitoRing Actions Layer) system, an online monitoring system that connects parallel tools to a running application. A special emphasis has been put on techniques and mechanisms for the migration of shared resources and communication channels as well as internal monitoring data structures. Currently, the migration framework migrates parallel processes based on the TreadMarks library. The Condor library has been utilized for the state transfer of a single process. In a computing environment consisting of eight nodes running TreadMarks applications, the migration framework brings a 10% overhead to Condor and grows almost linearly with added nodes. Although our first implementation supports TreadMarks applications, both the monitoring system and the migration framework are designed to be reusable and easily adaptable to other software DSM systems. © 2000 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sheard200081,
author={Sheard, T. and Benaissa, Z.-E.-A. and Pasalic, E.},
title={DSL implementation using staging and monads},
journal={SIGPLAN Notices (ACM Special Interest Group on Programming Languages)},
year={2000},
volume={35},
number={1},
pages={81-94},
doi={10.1145/331963.331975},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644417874&doi=10.1145%2f331963.331975&partnerID=40&md5=f79af18c7e01f6383881d294cb5c51ab},
abstract={The impact of Domain Specific Languages (DSLs) on software design is considerable. They allow programs to be more concise than equivalent programs written in a high-level programming languages. They relieve programmers from making decisions about data-structure and algorithm design, and thus allows solutions to be constructed quickly. Because DSL's are at a higher level of abstraction they are easier to maintain and reason about than equivalent programs written in a high-level language, and perhaps most importantly they can be written by domain experts rather than programmers. The problem is that DSL implementation is costly and prone to errors, and that high level approaches to DSL implementation often produce inefficient systems. By using two new programming language mechanisms, program staging and monadic abstraction, we can lower the cost of DSL implementations by allowing reuse at many levels. These mechanisms provide the expressive power that allows the construction of many compiler components as reusable libraries, provide a direct link between the semantics and the low-level implementation, and provide the structure necessary to reason about the implementation. © 2000 ACM.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nath2000177,
author={Nath, S.K. and Shahid, S. and Dewangan, P.},
title={SEISRES - a Visual C++ program for the sequential inversion of seismic refraction and geoelectric data},
journal={Computers and Geosciences},
year={2000},
volume={26},
number={2},
pages={177-200},
doi={10.1016/S0098-3004(99)00086-2},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034016694&doi=10.1016%2fS0098-3004%2899%2900086-2&partnerID=40&md5=66bc4eedff4b1bac0c2bdd73777c5030},
abstract={Refraction seismic and geoelectric methods are usually applied for the delineation of near surface structures in environmental, engineering and hydrogeological investigations. When applied independently, these techniques yield sufficiently accurate subsurface models. But the inversion of these data may also lead to incorrect parameter estimation specially in complicated geological situations, namely, blind zone problems in seismics, suppression and equivalence problems in geoelectrics. Stability and non-uniqueness can be reduced to a great extent by integrating physically different sets of data into a joint or sequential inversion scheme. In the present paper, we aim at introducing one such algorithm, wherein the seismic refraction and DC resistivity inversion routines are amalgamated. Even though the seismic and geoelectric methods may independently see different interfaces due to completely different physical responses, the joint or sequential inversion needs a common parameter, the layer thickness being the one in this situation. The proposed scheme is coded in Visual C++ on Microsoft Windows ’95 environment using the concept of object-oriented programming. The program SEISRES is exclusively menu driven and customised for running on personal computers. It has several options, namely, seismic ray inversion for near-surface estimation, curve dissemination for generating a starting model using seismic depth section, 1D resistivity inversion for Schlumberger and Wenner single electrode arrays using evolutionary programming for global optimisation, 1D forward calculations, creation of resistivity data sets from Wenner multi-electrode pseudo-section and construction of quasi-2D geoelectric section of the subsurface. The software is tested on a variety of synthetic examples with complex litho-stratigraphic relationships. The present paper deals with three such synthetic examples for the delineation of an aquifer in a three-layer setting in the first two examples and the detection of a thin conductive clay lens embedded in an aquifer in a four layered earth model emulating an equivalence problem. The seismic-guided 1D Schlumberger and Wenner inversions and the quasi-2D sections from pseudo-section interpretation led to subsurface information with better precision compared to the direct 2D inversion. A detailed field investigation was undertaken in Midnapur District for ground water prospecting. The results of two test sites at Satkui and Tangasol are presented here for judging the performance of the software package. Available borehole lithologs and geophysical logs validated the findings from SEISRES. The strength of the present scheme lies in its ability to model the subsurface seismically and geoelectrically, even in 2D environment by 1D approximation on a laptop/personal computer at-site cost-effectively. © 2000 Elsevier Science Ltd.},
author_keywords={Aquifer;  Evolutionary programming;  Pseudo-section;  Quasi-2D section;  RINSE},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cross2000641,
author={Cross, Valerie V. and Voss, Clare R.},
title={Fuzzy queries and cross-language ontologies in multilingual document exploitation},
journal={IEEE International Conference on Fuzzy Systems},
year={2000},
volume={2},
pages={641-646},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033700932&partnerID=40&md5=1ef7e19e57d84f676d3123c77e5b3290},
abstract={We report on our progress developing RAVEN, a prototype system that enables English-speaking users to quickly determine whether an individual foreign language (FL) text document is relevant to their interest when they do not understand that FL and no human or high-quality machine translation of the document is available. RAVEN uses a cross-language ontology fuzzy mathematics, and graph-based visualization software during its user-query phase and its document-assessment phase. We introduce a novel fuzzy query-construction method and a graph-based presentation of query results with document relevance measures.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sheard199981,
author={Sheard, T. and Benaissa, Z.-E.-A. and Pasalic, E.},
title={DSL implementation using staging and monads},
journal={Proceedings of the 2nd Conference on Domain-Specific Languages, DSL 1999},
year={1999},
pages={81-94},
doi={10.1145/331960.331975},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947236734&doi=10.1145%2f331960.331975&partnerID=40&md5=c145a3286e7da120de6d0b9923e05d35},
abstract={The impact of Domain Specific Languages (DSLs) on software design is considerable. They allow programs to be more concise than equivalent programs written in a high-level programming languages. They relieve programmers from making decisions about data-structure and algorithm design, and thus allows solutions to be constructed quickly. Because DSL's are at a higher level of abstraction they are easier to maintain and reason about than equivalent programs written in a high-level language, and perhaps most importantly they can be written by domain experts rather than programmers. The problem is that DSL implementation is costly and prone to errors, and that high level approaches to DSL implementation often produce inefficient systems. By using two new programming language mechanisms, program staging and monadic abstraction, we can lower the cost of DSL implementations by allowing reuse at many levels. These mechanisms provide the expressive power that allows the construction of many compiler components as reusable libraries, provide a direct link between the semantics and the low-level implementation, and provide the structure necessary to reason about the implementation.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schmidt1999360,
author={Schmidt, D.A.},
title={A return to elegance: The reapplication of declarative notation to software design},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1999},
volume={1551 LNCS},
pages={360-364},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867466161&partnerID=40&md5=d7b2b5f93ba23f4ff4d73f7a18febcad},
abstract={Software design methodologies were introduced to manage the scale of complex systems built in imperative languages under demanding work conditions. To some degree, declarative notations have been ignored for systems building because they lack similar design methodologies. Methodologies useful to object-orientation, namely, software architectures, design patterns, reusable libraries, and programming frameworks, are proposed as a model to be imitated by the declarative programming community. The resulting "declarative design methodology" would hasten the reapplication of declarative notations to mainstream software design and implementation. © Springer-Verlag Berlin Heidelberg 1998.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lizzi1999152,
author={Lizzi, Christophe},
title={Enabling deadline scheduling for Java real-time computing},
journal={Proceedings - Real-Time Systems Symposium},
year={1999},
pages={152-153},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033319958&partnerID=40&md5=0df0578bfbfa209e996567fdc8477e5e},
abstract={We present a scheduling architecture that enables the implementation of deadline-based scheduling policies. The architecture is used to give real-time capabilities to a custom Java virtual machine. This framework retains a clear separation between generic kernel mechanisms and upper level scheduling classes. A pure earliest deadline first algorithm can be implemented, but enhanced policies such as Dover or Robust EDF are also supported. We present our architecture, its implementation in the ChorusOS microkernel and its use by the Java virtual machine.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu1999306,
author={Liu, Ting and Kalbarczyk, Zbignew and Iyer, Ravishankar K.},
title={Software multilevel fault injection mechanism: Case study evaluating the virtual interface architecture},
journal={Proceedings of the IEEE Symposium on Reliable Distributed Systems},
year={1999},
pages={306-307},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033312093&partnerID=40&md5=57c8a9d7cd1589e3d4917811f9a28eb2},
abstract={The software multilevel fault injection (SMiFI) mechanism to evaluate failure characteristics of networked systems, such as virtual interface architecture (VIA), is proposed. The mechanism covers all software protocol layers of the host interface and corrupts both messages and the computation engines that manipulate messages.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chevochot1999292,
author={Chevochot, Pascal and Puaut, Isabelle},
title={Approach for fault-tolerance in hard real-time distributed systems},
journal={Proceedings of the IEEE Symposium on Reliable Distributed Systems},
year={1999},
pages={292-293},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033310635&partnerID=40&md5=928e3e4a32746369cf2683b934f4e874},
abstract={The presence of hard timing constraints makes the design of fault-tolerant systems difficult, because when tasks are replicated to treat errors, both the tasks replicas and the fault-tolerance building blocks (e.g. consensus) must be taken into account in the feasibility tests. This paper is devoted to tire description of an approach for managing failures in hard real-time distributed systems. Our approach is based on the use of a task replication tool, named Hydra, which makes tasks fault-tolerant off-line through the replication of parts of their code. The contribution of our work is not to provide new replication strategies but rather to provide replication strategies that are simultaneously suited to real-time constraints, transparent to application designers and flexible (i.e. adaptable to application requirements and with low dependence with the underlying run-time support and hardware). Further details on Hydra can be found in [2].},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Oikawa1999111,
author={Oikawa, S. and Rajkumar, R.},
title={Portable RK: A portable resource kernel for guaranteed and enforced timing behavior},
journal={Real-Time Technology and Applications - Proceedings},
year={1999},
pages={111-120},
doi={10.1109/RTTAS.1999.777666},
note={cited By 72},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032593204&doi=10.1109%2fRTTAS.1999.777666&partnerID=40&md5=93b9f28d21be60865aa0421b3cf236f3},
abstract={Portable RK is a portable implementation of a resource kernel, a resource-centric approach to build a real-time kernel that provides explicit timely, guaranteed, and enforced access by applications to system resources. Portable RK is designed to work with widely available operating systems with minimal changes. This facilitates experimentation in familiar software environments and helps the faster deployment of research results. Execution in resource kernels is directly based on OS-enforced resource reservation. As a result, an application can request the reservation of a certain amount of a resource, and the kernel can guarantee that the requested amount is available to that application in timely fashion. We describe the design and implementation of Portable RK called Linux/RK that resides within the Linux kernel. The evaluation results show that Portable RK in the form of Linux/RK gives direct control over timely resource utilization by applications and that its overhead costs are small enough to be negligible. © 1999 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Niu1999,
author={Niu, H. and Park, Y.},
title={An execution-based retrieval of object-oriented components},
journal={Proceedings of the 37th Annual Southeast Regional Conference, ACM-SE 1999},
year={1999},
doi={10.1145/306363.306388},
art_number={a18},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349131449&doi=10.1145%2f306363.306388&partnerID=40&md5=08a3d44d96f11007b680123cde2401ed},
abstract={Software reuse and object-oriented software development is recognized as one of the promising techniques to improve software development. Class components are basic building blocks in objectoriented software development. This paper describes a method of retrieving potentially reusable objected-oriented class components from a class library by executing classes in the class library on a test program based on a user-provided sample behavior of the desired class. The method handles the class hierarchy and uses a strategy - maximum number of matched methods and minimum unmatched methods - to find the classes close to the desired class. Based on the method, a prototype implementation of the class retrieval tool is implemented for Java classes. © 1999 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fromme1999354,
author={Fromme, M. and Hoffmann-Schulz, G. and Litvinenko, E. and Ziem, P.},
title={BEAN-A new standard program for data analysis at BER-II},
journal={SANTA FE 1999 - 11th IEEE NPSS Real Time Conference, Conference Record, RT 1999},
year={1999},
pages={354-358},
doi={10.1109/RTCON.1999.842641},
art_number={842641},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041317352&doi=10.1109%2fRTCON.1999.842641&partnerID=40&md5=a70dfec80a1a7b33df57949d34eeed37},
abstract={A program package BEAN (BENSC Analysis Program) has been developed to provide a standard for the analysis of one-dimensional neutron spectra gathered at the experiment facilities of BENSC. The main purpose was to replace the large number of heterogeneous software packages which has been developed in the past for each neutron spectrometer separately. BEAN runs on a UNIX application server which is embedded in a 3-Tier computing architecture. Data display is performed by means of the PV-WAVE software tools. For easy use the graphical user interface follows the common styling guidelines of Windows programs. © 1999 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor19991,
title={4th International Conference on Reliable Software Technologies, Ada-Europe 1999},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1999},
volume={1622},
pages={1-450},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958639344&partnerID=40&md5=a91de749b91750788bfd872df4caa397},
abstract={The proceedings contain 37 papers. The special focus in this conference is on Ravenscar Profile and High Integrity Systems. The topics include: An ada runtime system implementation of the ravenscar profile for high speed application-layer data switch; re-engineering a safety-critical application using SPARK 95 and GNORT; an ada95 solution for certification of embedded safety critical applications; defining the contents of architectural descriptions; mapping object-oriented designs to ada; efficient and extensible multithreaded remote servers; report on the VERA experiment; acceptance testing of object oriented systems; environment for the development and specification of real-time ada programs; interprocedural symbolic evaluation of ada programs with aliases; automatic verification of concurrent ada programs; translating time petri net structures into ada 95 statements; railway scale model simulator; ada 95 as a foundation language in computer engineering education in Ukraine; yaRTI, an ada 95 HLA run-time infrastructure; an ada95 implementation of a network coordination language with code mobility; on the use of controlled types for fossil collection in a distributed simulation system; ada binding to a shared object layer; the ceiling protocol in multi-moded real-time systems; implementing a new low-level tasking support for the GNAT runtime system; metascribe, an ada-based tool for the construction of transformation engines; sharing and handling of data between heterogeneous applications using persistence; browsing a component library using non-functional information; HW/SW co-design of embedded systems and hardware/software embedded system specification and design using ada and VHDL.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Bertozzi1999199,
author={Bertozzi, M. and Boselli, F. and Conte, G. and Reggiani, M.},
title={An MPI implementation on the top of the virtual interface architecture},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1999},
volume={1697},
pages={199-206},
doi={10.1007/3-540-48158-3_25},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958057790&doi=10.1007%2f3-540-48158-3_25&partnerID=40&md5=75609b957495a7ad9d711e7d847433ea},
abstract={This paper describes an implementation of the LAMMPI suite on the top of the Virtual Interface Architecture. The Virtual Interface Architecture (VIA) is an emerging standard designed by Intel, Microsoft, and Compaq aimed at the reduction of communication latency for cluster of workstations or system area networks. Thanks to M-VIA, a Linux software module that emulates VIA and provides programmers of VIA APIs, it has been possible to develop an MPI implementation even in absence of a hardware VIA interface. Nonetheless, M-VIA module high optimization as well as VIA protocol simplicity permitted a reduction of latency time with respect to the use of the TCP/IP protocol on Ethernet network interfaces. © Springer-Verlag Berlin Heidelberg 1999.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Goswami1999130,
author={Goswami, D. and Singh, A. and Preiss, B.R.},
title={Using object-oriented techniques for realizing parallel architectural skeletons},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1999},
volume={1732},
pages={130-141},
doi={10.1007/10704054_14},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947769575&doi=10.1007%2f10704054_14&partnerID=40&md5=c13930b24db14a83fb1935bf0fbd6f21},
abstract={The concept of design patterns has recently emerged as a new paradigm in the context of object-oriented design methodology. Similar ideas are being explored in other areas of computing. In the parallel computing domain, design patterns describe recurring parallel computing problems and their solution strategies. Starting with the late 1980’s, several pattern-based systems have been built for facilitating parallel application development. However, most of these systems use patterns in ad hoc manners, thus lacking a generic or standard model for using and intermixing different patterns. This substantially hampers the usability of such systems. Lack of flexibility and extensibility are some of the other major concerns associated with most of these systems. In this paper, we propose a generic (i.e., pattern- and application-independent) model for realizing and using parallel design patterns. The term architectural skeleton is used to represent the application independent, reusable set of attributes associated with a pattern. The model can provide most of the functionalities of low level message passing libraries, such as PVM or MPI, plus the benefits of the patterns. This results in tremendous flexibility to the user. It turns out that the model is an ideal candidate for object-oriented style of design and implementation. It is currently implemented as a C++ template-library without requiring any language extension. The generic model, together with the object-oriented and library-based approach, facilitates extensibility. © Springer-Verlag Berlin Heidelberg 1999.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Andreoli1999183,
author={Andreoli, J.-M. and Arregui, D. and Pacull, F. and Rivière, M. and Vion-Dury, J.-Y. and Willamowski, J.},
title={CLF/Mekano: A framework for building virtual-enterprise applications},
journal={Proceedings - 3rd International Enterprise Distributed Object Computing Conference},
year={1999},
volume={1999-January},
number={January},
pages={183-192},
doi={10.1109/EDOC.1999.792062},
art_number={792062},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936935381&doi=10.1109%2fEDOC.1999.792062&partnerID=40&md5=3acb49d9b641e3ddf305e2d55aa1aae4},
abstract={CLF/Mekano is a distributed object infrastructure oriented towards the high-level coordination of coarse grain components. Unlike other infrastructures of the same class, such as CORBA or DCOM, coordination in CLF/Mekano is built-in at the lowest level, namely at the inter-component communication protocol level, and not as a side service (such as the event, transaction or negotiation services of Corba). Although the CLF protocol is "lightweight" (it relies on very few concepts and only 8 communication "verbs"), it makes the design and implementation of components more complex, but also more valuable if it can be reused. The Mekano library has been developed in order to deal with this complexity, targeting reusability. It provides ready-to-use generic classes of customizable components, as well as useful component parts which can be reassembled according to application specific needs. Of course, further layers of domain-specific libraries (so called business object libraries), can then be developed on top of Mekano, to provide ready-to-use components dedicated to specific business needs (in the line of Enterprise Java Beans). © 1999 IEEE.},
author_keywords={Component library;  Coordination;  Distributed objects;  Middleware},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fiutem1999339,
author={Fiutem, R. and Antoniol, G. and Tonella, P. and Merlo, E.},
title={ART: An Architectural Reverse Engineering Environment},
journal={Journal of Software Maintenance and Evolution},
year={1999},
volume={11},
number={5},
pages={339-364},
doi={10.1002/(sici)1096-908x(199909/10)11:5<339::aid-smr196>3.0.co;2-i},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033340014&doi=10.1002%2f%28sici%291096-908x%28199909%2f10%2911%3a5%3c339%3a%3aaid-smr196%3e3.0.co%3b2-i&partnerID=40&md5=1a4615123cb48f306594fa69bc416844},
abstract={When programmers perform maintenance tasks, program understanding is often required. One of the first activities in understanding a software system is identifying its subsystems and their relations, i.e., its software architecture. Since a large part of the effort is spent in creating a mental model of the system under study, tools can help maintainers in managing the evolution of legacy systems by showing them architectural information. This paper describes an environment for the architectural recovery of software systems called the architectural recovery tool (ART). The environment is based on a hierarchical architectural model that drives the application of a set of recognizers, each producing a different architectural view of a system or of some of its parts. Recognizers embody know ledge about architectural clichés and use flow analysis techniques to make their output more accurate. To test the accuracy and effectiveness of the ART, a suite of public domain applications containing interesting architectural organizations was selected as a benchmark. Results are presented by showing ART performance in terms of precision and recall of the architectural concept retrieval process. The results obtained show that cliché-based architectural recovery is feasible and the recovered information can be valuable support in reengineering and maintenance activities. Copyright © 1999 John Wiley & Sons, Ltd.},
author_keywords={Cliché;  Flow analysis;  Matching;  Program understanding;  Reverse architecturing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ben-Shaul1999658,
author={Ben-Shaul, Israel and Gazit, Hovav and Gidron, Yoad and Holder, Ophir and Lavva, Boris},
title={FarGo: A system for mobile component-based application development},
journal={Proceedings - International Conference on Software Engineering},
year={1999},
pages={658-659},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032688682&partnerID=40&md5=9a0081d3950d0790ddc990de97d1ede9},
abstract={The FarGo system presents a novel programming model that is centered around the concept of `dynamic application layout', which permits the manipulation of component location at runtime, thereby enabling to map dynamically the logical components onto physical hosts. Since the emphasis is on components that are part of a larger application, component mobility preserves the validity of incoming and outgoing component references, in addition to the internal state of the component. Thus, FarGo inter-component references can dynamically stretch and shrink, unlike traditional references, which are fixed at design time to be either local or remote.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Heinonen1999569,
author={Heinonen, Tomi and Lahtinen, Antti and Dastidar, Prasun and Ryymin, Pertti and Laarne, Paivi and Malmivuo, Jaakko and Laasonen, Erkki and Frey, Harry and Eskola, Hannu},
title={Applications of magnetic resonance image segmentation in neurology},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1999},
volume={3658},
pages={569-579},
doi={10.1117/12.349469},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032653585&doi=10.1117%2f12.349469&partnerID=40&md5=549f26d1f47c0aba34c6cbc0e2340189},
abstract={After the introduction of digital imaging devices in medicine computerized tissue recognition and classification (i.e., segmentation) have become important in research and clinical applications. Segmented data can be applied among numerous research fields including volumetric analysis of particular tissues and structures, construction of anatomical models, three-dimensional (3D) visualization, and multimodal visualization, hence making segmentation essential in modern image analysis. In this research project several PC based software were developed in order to segment medical images, to visualize raw and segmented images in 3D, and to produce EEG brain maps in which MR images and EEG signals were integrated. The software package was tested and validated in numerous clinical research projects in hospital environment.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dyne1999142,
author={Dyne, Barry and Bernstein, David},
title={Correct-by-construction approach to MEMS design and analysis},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1999},
volume={3680},
number={I},
pages={142-149},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032652860&partnerID=40&md5=6c5a4dac0bd94e5d7b457cc0d3370530},
abstract={The need for general modeling and analysis tools for MEMS devices has been well established, and several software packages designed to address these needs have appeared in the last five years. While being powerful and comprehensive, existing tools often require specialized knowledge of the methods involved in the analysis (e.g., 3D modeling, mesh generation), and lack integration into a complete design environment. This paper describes a new MEMS design and analysis tool that uses a correct-by-construction approach in a tightly integrated environment of layout, 3D modeling, analysis, and system design. The MEMS design tool suite is integrated into an existing integrated circuit design package. Our goal is to automate as many steps of the process as possible, thus lowering the barrier to MEMS design. Device layout is created either manually, or using automatic layout generation macros, then a 3D model is automatically generated from the layout and a process description. Fully integrated finite and boundary element analysis allows the user to specify boundary conditions on the layout or on the 3D model, then automatic mesh generation is performed, taking advantage of the layered structure of the device to reduce mesh density in non-critical areas. Solvers include electrostatic, mechanical, and thermal, and well as coupled domain analysis. The design process ensures that the user does not create a device that cannot be fabricated. Libraries containing standard fabrication processes (e.g., MUMPS) and material properties will be available. Results of the analysis are passed to a behavioral Model Builder which then constructs high-level macro models using model order reduction, table construction and parameter extraction. These models can then be used in system simulations, allowing quick exploration of the design space and system verification.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Song1998408,
author={Song, I.-Y. and Watts, P. and Hassell, L. and Wong, C.P.},
title={Modeling Dynamic Behavior with Object Interaction Diagrams},
journal={Proceedings of the Joint Conference on Information Sciences},
year={1998},
volume={3},
pages={408-411},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1642333276&partnerID=40&md5=b1ea3795bcfa2927e936b8412bc4d2de},
abstract={Object Interaction Diagrams (OIDs) model dynamic behavior by showing how system components interact to complete core tasks defined in use case design. While seemingly intuitive, diagram elements are not consistently defined, and methods for constructing an OID have not been described in the literature. Also there is a lack of understanding about how OIDs relate to each other and to other system diagrams. The goal of the paper is to resolve these issues by systematically examining the structure and role of the OID in object-oriented (OO) systems design. First, we take a look at how prominent developers use OIDs in their designs. Next the structure of an ODD is examined in detail, and recommendations for clarification are presented. Then a heuristic method for OID construction is described. An analysis of a set of library OIDs corroborates the heuristic by reproducing the profile of entity, control, and boundary object stereotypes described in the literature. The heuristic method represents a first effort in the field of OO design to describe how an OID should be constructed. We also show that communicating OIDs are not systematically connected to each other. Relationships can be fully understood if OIDs are organized hierarchically with strategically positioned connector buttons that enable the viewer to both "drill down" to subordinate OIDs and to "roll up" to calling OIDs for a complete, detailed trace through a system. We conclude the paper by bringing together all of our recommendations for syntax changes into a single enhanced OID that will hopefully improve the quality of software design.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saksena199892,
author={Saksena, M. and Ptak, A. and Freedman, P. and Rodziewicz, P.},
title={Schedulability analysis for automated implementations of real-time object-oriented models},
journal={Proceedings - Real-Time Systems Symposium},
year={1998},
pages={92-102},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032304271&partnerID=40&md5=0eecd09a8fc7120be73ba1153a689a8e},
abstract={The increasing complexity of real-time software has led to a recent trend in the use of high-level modeling languages for development of real-time software. One representative example is the modeling language ROOM (real-time object-oriented modeling), which provides features such as object-orientation, state machine description of behaviors, formal semantics for executability of models, and possibility of automated code generation. However, these modeling languages largely ignore the timeliness aspect of real-time systems, and fail to provide any guidance for a designer to a priori predict and analyze temporal behavior. In this paper we consider schedulability analysis for automated implementations of ROOM models, based on the ObjecTime toolset. This work builds on results presented in, where we developed some guidelines for the design and implementation of real-time object-oriented models. Using the guidelines, we have modified the run-time system library provided by the ObjecTime toolset to make it amenable to schedulability analysis. Based on the modified toolset, we show how a ROOM model can be analyzed for schedulability, taking into account the implementation overheads and structure. The analysis is validated experimentally, first using simple periodic models, and then using a large case study of a train tilting system.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeYoung19981156,
author={De Young, D. and Dillow, J. and Corcoran, S. and Andrews, E. and Yellowhair, J. and DeVries, K.},
title={Ground demonstration of an optical control system for a space-based sparse aperture telescope},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1998},
volume={3356},
pages={1156-1167},
doi={10.1117/12.324516},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032299086&doi=10.1117%2f12.324516&partnerID=40&md5=91a6f5eabc5b32e59c77f10fa2f0ff95},
abstract={svs has recently completed a phase II small business innovative research (SBIR) project called Low Cost Space Imager (LCSI). As part of the SB[R project, a sparse aperture telescope design concept was developed. This design includes an optical control system capable of correcting the primary segments to within 38 nm piston and 17 nrad tilt as required by the optical tolerance analysis. The optical system utilizes a common secondary and primaries arranged in a Golay-6 configuration. The primaries are spherical, which eliminates the need for translation and rotation control. A laboratory experiment to validate the controls concept has been completed. This experiment culminated in the demonstration of autonomous capture, alignment, and phasing of an optical system with a three segment primary to tolerances consistent with the space optical system. The implementation of the controls scheme in the laboratory experiment is done using Matlab/Simulink for controller design and code generation. The code is implemented real-time on a VME based computer system. Closed loop piston control, which utilizes a four-bin sensing scheme, of an actuated mirror to 25nm RMS mirror motion has been demonstrated. Additionally, autonomous capture and phasing of three segmented primaries has been demonstrated. The technique for the phasing capture involves real-time implementation of image processing techniques to measure the white light fringe visibility in the far field.},
author_keywords={Automatic phasing;  Optical controls;  Segmented primary;  Test results;  White light fringes},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor1998566,
title={Proceedings of the 1998 24th International Conference for the Resource Management & Performance Evaluation of Enterprise Computing Systems, CMG. Part 2 (of 2)},
journal={CMG Proceedings},
year={1998},
volume={2},
pages={566-1007},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032284256&partnerID=40&md5=3813280458d313f78c25e798b8086c33},
abstract={The proceedings contains 44 papers from the 24th International Conference for the Resource Management and Performance Evaluation of Enterprise Computing Systems. Topics discussed include: system implementation failures; integrated cryptographic facilities; Web searches; response time agreements; storage systems; data integration; Internet; Intranet; processor consumption forecasting; computer architecture; client/server systems; long term file activity; inter-reference periods; large assembler mainframe; and standard UNIX accounting.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Zhang199890,
author={Zhang, Tao and Xu, Xiangdong},
title={General purpose software package for thermal system control},
journal={Qinghua Daxue Xuebao/Journal of Tsinghua University},
year={1998},
volume={38},
number={4},
pages={90-93},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032267486&partnerID=40&md5=09888650a755e1483e1973954ff7c5a4},
abstract={In China, in order to effectively enhance the automation degree of the thermal control systems of the medium and small power stations, and to adapt the different constructions of these thermal systems, the strategy of using modular analysis and programming method is taken. To improve the reliability of the systems, hierarchical multi-tier redundancy error-tolerance structure is presented. Some new control theories and tactics are applied to solve the control problems on which normal control methods can hardly obtain successful control effects. The software package presented has the features of general-purpose, high security, easy-to-use, etc., and it provides a feasible integration tool for thermal system automation reengineering. The details of software configuration, data acquisition control strategy, modular setup and implementation are listed. It has been successfully applied to several thermal systems.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Smith1998242,
author={Smith, G. and Gough, J. and Szyperski, C.},
title={Case for meta-interworking: projecting CORBA meta-data into COM},
journal={Proceedings of the Conference on Technology of Object-Oriented Languages and Systems, TOOLS},
year={1998},
pages={242-253},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032255548&partnerID=40&md5=b150eac285972d3561dd7adbb9be3566},
abstract={The pressure to reduce the time and effort required to produce and update software components, together with the existence of multiple competing component worlds, has forced the introduction of interworking standards. Unfortunately the interworking standards fail to suitably address the need for access to meta information. In the light of meta-data being increasingly important for component environments this inaccessibility of meta-data can lead to components being unusable by their intended clients. What exasperates the situation is that if access to meta-data repositories is provided using an implementation of the interworking standard, the information retrieved is incorrect. This paper describes these difficulties. To exemplify the solution, the construction of an adapter is described. This adapter provides access to the Common Object Request Broker Architectures' (CORBA) Interface Repository via the interfaces of the Component Object Model's (COM) Type Library. This solution provides access to meta-data which is essential if the full benefit of interworking is to be realised.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kaiser19983,
author={Kaiser, G.E. and Dossick, S.E. and Jiang, W. and Yang, J.J. and Ye, S.X.},
title={WWW‐based collaboration environments with distributed tool services},
journal={World Wide Web},
year={1998},
volume={1},
number={1},
pages={3-25},
doi={10.1023/A:1019291009758},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0003019298&doi=10.1023%2fA%3a1019291009758&partnerID=40&md5=8e32ccc67377b8bc3127f04885b562f4},
abstract={We have developed an architecture for a general‐purpose framework for hypermedia collaboration environments that support purposeful work by orchestrated teams. The hypermedia represents all plausible multimedia artifacts concerned with the collaborative task(s) at hand that can be placed or generated on‐line, from application‐specific materials (e.g., source code, chip layouts, blueprints) to formal documentation to digital library resources to informal email and chat transcripts. The framework capabilities support both internal (WWW‐style hypertext) and external (non‐WWW open hypertext link server) links among these artifacts, which can be added incrementally as useful connections are discovered; project‐specific intelligent hypermedia search and browsing; automated construction of artifacts and hyperlinks according to the semantics of the group and individual tasks and the overall workflow among the tasks; application of arbitrary tools to the artifacts; and collaborative work for geographically dispersed teams connected by the Internet and/or an intranet/extranet. We also present a general architecture for a WWW‐based distributed tool launching service compatible with our collaboration environment framework. We describe our prototype realization of the framework in OzWeb. It reuses object‐oriented data management for application‐specific hyperbase organization, and workflow enactment and cooperative transactions as built‐in services, which were originally developed for the Oz non‐hypermedia environment. The tool service is implemented by the generic Rivendell component, which has been integrated into OzWeb as an example “foreign” (i.e., add‐on) service. Rivendell could alternatively be employed in a stand‐alone manner. We have several months experience using an OzWeb hypermedia collaboration environment for our own continuing software development work on the system. © 1998, Kluwer Academic Publishers.},
author_keywords={Collaborative Task;  Digital Library;  Formal Documentation;  Link Server;  Tool Service},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor1998,
title={Proceedings - 1998 Australian Software Engineering Conference, ASWEC 1998},
journal={Proceedings - 1998 Australian Software Engineering Conference, ASWEC 1998},
year={1998},
volume={1998-November},
page_count={210},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052210119&partnerID=40&md5=624cb1b0bf2df3f8776638d18acf429b},
abstract={The proceedings contain 14 papers. The topics discussed include: the influence of organizational factors on the success and quality of a product-line architecture; process capability in the Australian software industry - results from the SPICE trials; design recovery through formal specification; software design: in search of method; morphable objects in Smalltalk; a framework for dynamic checks in C arrays via a C++ class; adding semantics to interface definition languages; the ins and outs of objects; identifying library functions in executable files using patterns; classifying relationships between object-oriented design patterns; building information systems development methods: synthesizing from a basis in both theory and practice; supplementing process-oriented with structure-oriented design explanation within formal object oriented method; issues in automatic software manufacturing in the presence of generators; TGE approach for constructing software engineering tools; implementing process enactment within a process-centred software development environment; on the test allocations for the best lower bound performance of partition testing; toward a method for deriving measures of reuse; and inheritance and reusability.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Worley1998667,
author={Worley, J. and Jordan, R.},
title={A scalable Khoros neural network toolbox},
journal={ITS 1998 Proceedings - SBT/IEEE International Telecommunications Symposium},
year={1998},
volume={2},
pages={667-670},
doi={10.1109/ITS.1998.718476},
art_number={718476},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051146066&doi=10.1109%2fITS.1998.718476&partnerID=40&md5=99cce064203a2312ecd4c286623585da},
abstract={Three problems limit neural network simulation environments. First, the computational complexity of neural network simulation imposes a practical constraint on the size of simulated neural networks. Second, neural network simulation environments are usually designed to accommodate a confined set of neural network models. Third, most neural network simulation environments do not facilitate interoperability with external systems. This project addresses these issues by implementing a simulation environment that scales to support larger neural systems, facilitates the implementation of new network architectures, and interoperates with other software. This problem was approached by examining a representative set of neural network architectures that feature a diversity of characteristics such as topology and learning rules. The result of this research was applied to the design and implementation of an environment consisting of tools and applications to support neural network simulation. This environment, developed under the Khoros system, provides a visual neural network construction tool, an extensible C++ class library that encapsulates network management and object interaction, a file format for storage and retrieval of neural networks, and finally, neural network visualization tools. © 1998 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor19981,
title={25th Conference on Current Trends in Theory and Practice of Informatics, SOFSEM 1998},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1998},
volume={1521},
pages={1-452},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957679389&partnerID=40&md5=40deff7fd1cdaa497d9b082072c5b9f9},
abstract={The proceedings contain 37 papers. The special focus in this conference is on Theory and Practice of Informatics. The topics include: Software architecture for distributed reactive systems; a logical basis for modular software and systems engineering; from quartets to phylogenetic trees; reuse methods for construction of parallel software; compiling horn-clause rules in IBM’s business system 12, an early experiment in declarativeness; models of computation, riemann hypothesis, and classical mathematics; security of electronic money; algorithms based on randomization and linear and semidefinite programming; distributed systems technology for electronic commerce applications; parallel interactive media server systems; online routing problems for broadband networks; some prospects for efficient fixed parameter algorithms; system infrastructure for digital libraries; an introduction to cryptology; authoring structured multimedia documents; engineering software and software engineering; efficient communication schemes; audit of information systems; towards the use of dynamic documents in business processes; computational power of BSP computers; modeling of hypermedia applications; on the klee’s measure problem in small dimensions; yet another modular technique for efficient leader election; regulated grammars with leftmost derivation; some results on the modelling of spatial data; randomized meldable priority queues; the reconstruction of convex polyominoes from horizontal and vertical projections and behavioral safety in a model with multiple class objects.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Bassetti1998,
author={Bassetti, F. and Brown, D. and Davis, K. and Henshaw, W. and Quinlan, D.},
title={Overture: An object-oriented framework for high performance scientific computing},
journal={Proceedings of the International Conference on Supercomputing},
year={1998},
volume={1998-November},
doi={10.1109/SC.1998.10013},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957648088&doi=10.1109%2fSC.1998.10013&partnerID=40&md5=6d32f698679ab6c1215344aac8907955},
abstract={The Overture Framework is an object-oriented environment for solving PDEs on serial and parallel architectures. It is a collection of C++ libraries that enables the use of finite difference and finite volume methods at a level that hides the details of the associated data structures, as well as the details of the parallel implementation. It is based on the A++/P++ array class library and is designed for solving problems on a structured grid or a collection of structured grids. In particular, it can use curvilinear grids, adaptive mesh refinement and the composite overlapping grid methods to represent problems with complex moving geometry. This paper introduces Overture, its motivation, and specifically the aspects of the design central to portability and high performance. In particular we focus on the mechanisms within Overture that permit a hierarchy of abstractions and those mechanisms which permit their efficiency on advanced serial and parallel architectures. We expect that these same mechanisms will become increasingly important within other object-oriented frameworks in the future. © 1998 IEEE.},
author_keywords={Computer software;  Differential equation solvers;  Hydrodynamics;  Object-oriented frameworks},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cysewski19986,
author={Cysewski, G. and Gromadzki, T. and Lyskawa, H. and Piechowka, M. and Szejko, S. and Kozlowski, W.E. and Vahamaki, O.},
title={Reusable framework for telecontrol protocols},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1998},
volume={1429},
pages={6-13},
doi={10.1007/3-540-68383-6_3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889218335&doi=10.1007%2f3-540-68383-6_3&partnerID=40&md5=5870e81215a58cc365a4a96a2f7dd152},
abstract={The paper presents a so-called COMSOFT framework of communication software development model based on object oriented architecture and design patterns. The library of reusable components supports reuse in all phases of the development process; it is intended to be generic with respect to the family of telecontrol protocols. The framework is supplemented with customisation guideline to help in refining the reused library so that deriving a working application becomes easier and more systematic. The elaborated solution is based on experience obtained during the development of the IEC-870-5-101 protocol interface for ABB monitoring and control devices. © Springer-Verlag Berlin Heidelberg 1998.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lou1998156,
author={Lou, J.Z. and Norton, C.D. and Cwik, T.},
title={A robust and scalable library for parallel adaptive mesh refinement on unstructured meshes},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1998},
volume={1457 LNCS},
pages={156-169},
doi={10.1007/bfb0018536},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883482102&doi=10.1007%2fbfb0018536&partnerID=40&md5=7599f90bbdbe18fec84473d15cdb3ea2},
abstract={The design and implementation of a software library for parallel adaptive mesh refinement in unstructured computations on multiprocessor systems are described. This software tool can be used in parallel finite element or parallel finite volume applications on triangular and tetrahedral meshes. It contains a suite of well-designed and efficiently implemented modules that perform operations in a typical P-AMR process. This includes mesh quality control during successive parallel adaptive mesh refinement, typically guided by a local-error estimator, and parallel load-balancing. Our P-AMR tool is implemented in Fortran 90 with a Message-Passing Interface (MPI) library, supporting code efficiency, modularity and portability. The AMR schemes, Fortran 90 data structures, and our parallel implementation strategies are discussed in the paper. Test results of our software, as applied to a few selected engineering finite element applications, will be demonstrated. Performance results of our code on Cray T3E, HP/Convex Exemplar systems, and on a PC cluster (a Beowulf-class system) will also be reported.},
author_keywords={Fortran 90;  Parallel adaptive mesh refinement;  Unstructured mesh},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ooi1998329,
author={Ooi, W.-T. and Smith, B. and Mukhopadhyay, S. and Chan, H.H. and Weiss, S. and Chiu, M. and Song, J.},
title={The Dali multimedia software library},
journal={1998 IEEE 2nd Workshop on Multimedia Signal Processing},
year={1998},
volume={1998-December},
pages={329-334},
doi={10.1109/MMSP.1998.738955},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862610106&doi=10.1109%2fMMSP.1998.738955&partnerID=40&md5=ae65692ceec0eaffac3206742ca3c8eb},
abstract={This paper presents a new approach for constructing libraries for building processing-intensive multimedia software. Such software is currently constructed either "from scratch" or by using high-level libraries. We have found that the second approach produces inefficient code, while the first approach is time-consuming. We therefore designed and implemented Dali, a set of reusable, high-performance primitives and abstractions that are at an intermediate point in this design space. By decomposing common multimedia data types and operations into thin abstractions and primitives, programs written using Ball are shorter and more reusable than hand-tuned C code, yet achieve competitive performance. This paper describes the design and implementation of Dali. © 1998 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Blount199835,
author={Blount, B. and Chatterjee, S.},
title={An evaluation of java for numerical computing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1998},
volume={1505},
pages={35-46},
doi={10.1007/3-540-49372-7_4},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-22444451748&doi=10.1007%2f3-540-49372-7_4&partnerID=40&md5=bc88f9609a82aaff4536bc6a3d89874a},
abstract={We describe the design and implementation of high perfor- mance numerical software in Java. Our primary goals are to characterize the performance of object-oriented numerical software written in Java and to investigate whether Java is a suitable language for such endea- vors. We have implemented JLAPACK, a subset of the LAPACK libr- ary in Java. LAPACK is a high-performance Fortran 77 library used to solve common linear algebra problems. JLAPACK is an object-oriented library using encapsulation, inheritance, and exception handling. It per- forms within a factor of four of the optimized Fortran version for certain platforms and test cases. When used with the native BLAS library, JLA- PACK performs comparably with the Fortran version using the native BLAS library. We conclude that high-performance numerical software could be written in Java if a few concerns about language features and compilation strategies are addressed. © Springer-Verlag Berlin Heidelberg 1998.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Eriksson199849,
author={Eriksson, J. and Gulliksson, M. and Lindström, P. and Wedin, P.-Å.},
title={Regularization tools for training large feed-forward neural networks using automatic differentiation},
journal={Optimization Methods and Software},
year={1998},
volume={10},
number={1},
pages={49-69},
doi={10.1080/10556789808805701},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032291127&doi=10.1080%2f10556789808805701&partnerID=40&md5=16dcf80ebe7fc27608214a30217ea3cf},
abstract={We describe regularization tools for training large-scale artificial feed-forward neural networks. We propose algorithms that explicitly use a sequence of Tikhonov regularized nonlinear least squares problems. For large-scale problems, methods using new special purpose automatic differentiation are used in a conjugate gradient method for computing a truncated Gauss-Newton search direction. The algorithms developed utilize the structure of the problem in different ways and perform much better than a Polak-Ribière based method. All algorithms are tested using benchmark problems and guidelines by Lutz Prechelt in the Proben1 package. All software is written in Matlab and gathered in a toolbox.},
author_keywords={Automatic differentiation;  Large-scale problems;  Neural network training;  Tikhonov regularization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jan1998443,
author={Jan, Song F.},
title={Integrated software system for reinforced concrete design of industrial structures in high seismic zones},
journal={Congress on Computing in Civil Engineering, Proceedings},
year={1998},
pages={443-445},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031641536&partnerID=40&md5=90e0640654c70e986c656ff7e6e174a4},
abstract={Several large scale software systems have been evaluated and GTSTRUDL was found to be the most advanced finite element analysis program having the capability of designing the reinforced concrete structures in high seismic zone. GTSTRUDL can interface with Frameworks for structural modeling to become a highly efficient integrated automation system. This software system is discussed to demonstrate the automation process for concrete design in high seismic risk regions.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sutton199862,
author={Sutton, R.A. and Srini, V.P. and Rabaey, J.M.},
title={A multiprocessor DSP system using PADDI-2},
journal={Proceedings - Design Automation Conference},
year={1998},
pages={62-65},
doi={10.1145/277044.277056},
art_number={724440},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031632301&doi=10.1145%2f277044.277056&partnerID=40&md5=cf48744cddfebb332d6b58387b4e50ec},
abstract={We have integrated an image processing system built around PADDI-2, a custom 48 node MIMD parallel DSP. The system includes image processing algorithms, a graphical SFG tool, a simulator, routing tools, compilers, hardware configuration and debugging tools, application development libraries, and software implementations for hardware verification. The system board, connected to a SPARCstation via a custom Sbus controller, contains 384 processors in 8 VLSI chips. The software environment supports a multiprocessor system under development (VGI-1). The software tools and libraries are modular, with implementation dependencies isolated in layered encapsulations. © 1998 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ota1997964,
author={Ota, Y. and Wilamowski, B.M.},
title={VLSI architecture for analog bidirectional pulse-coupled neural networks},
journal={IEEE International Conference on Neural Networks - Conference Proceedings},
year={1997},
volume={2},
pages={964-968},
doi={10.1109/ICNN.1997.616156},
art_number={616156},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030714384&doi=10.1109%2fICNN.1997.616156&partnerID=40&md5=8cd8efe0d486313601ee744f1b58f16d},
abstract={A compact architecture for analog CMOS VLSI implementation of voltage-mode pulse-coupled neural network (PCNN) is presented. The main feature of the proposed neuron circuit is that the structure is compact, yet exhibiting all the basic properties of natural biological neurons. Another unique feature of the proposed neuron cell is that one node serves as both input and output, mimicking a natural biological neuron, and the neuron cell uses frequency modulated bidirectional pulse-streams. Functionality of the proposed PCNN circuit is verified with SPICE simulations. © 1997 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wakunda1997644,
author={Wakunda, J. and Zell, A.},
title={EvA: A tool for optimization with evolutionary algorithms},
journal={Conference Proceedings of the EUROMICRO},
year={1997},
pages={644-651},
doi={10.1109/EURMIC.1997.617395},
art_number={617395},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030654415&doi=10.1109%2fEURMIC.1997.617395&partnerID=40&md5=06f951103236aac88ef135401082de16},
abstract={We describe the EvA software package which consists of parallel (and sequential) implementations of genetic algorithms (GAs) and evolution strategies (ESs) and a common graphical user interface. We concentrate on the descriptions of the two distributed implementations of GAs and ESs which are of most interest for the future. We present comparisons of different kinds of genetic algorithms and evolution strategies that include implementations of distributed algorithms on the Intel Paragon, a large MIMD computer and massively parallel algorithms on a 16384 processor MasPar MP-1, a large SIMD computer. The results show that parallelization of evolution strategies not only achieves a speedup in execution time of the algorithm, but also a higher probability of convergence and an increase of quality of the achieved solutions. In the benchmark functions we tested, the distributed ESs have a better performance than the distributed GAs. © 1997 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor1997,
title={Proceedings of the 1997 12th Annual Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA'97},
journal={Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
year={1997},
volume={32},
number={10},
page_count={344},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031248335&partnerID=40&md5=1deb3654956ce015191e15005f4574e0},
abstract={The proceedings contains 29 papers from the 1997 Annual Conference on Object-Oriented Programming, Systems, Languages, and Applications. Topics discussed include: class library evolution; reusability; Java programming language; type parameterization; parasitic methods for multi-methods implementation for Java; dynamic interdependent objects collections; object-relational database systems; World Wide Web; call graph construction; dynamic dispatch without virtual functions; efficient type inclusion tests; aspect-oriented programming; software agents and network-based software; distributed systems tools; ephemerons; common-object request broker architecture (CORBA); and software blueprints.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Alshawi1997339,
author={Alshawi, M. and Che Wan Putra, C.W.F. and Faraj, I.},
title={A framework for the object life cycle in integrated environments},
journal={Microcomputers in Civil Engineering},
year={1997},
volume={12},
number={5},
pages={339-351},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031223649&partnerID=40&md5=a9d314627ad0446bcdedd942f46dfdf1},
abstract={Problems related to the implementation of product models are different from those encountered during their theoretical development. They are related mainly to information management and the flow of information within the integrated environment. From the point of view of implementation, this paper addresses the problems experienced with current models, concentrating on two main issues: object definition and the life cycle of objects. It also discusses briefly a proposed strategic framework for an integrated construction environment within which a framework for the object life cycle is introduced. Four phases have been defined for the object life cycle. These are create and amend, supplement object with data, use object, and decommission object. This concept has been implemented in the integrated environment SPACE. © 1997 Microcomputers in Civil Engineering. Published by Blackwell Publishers.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor1997,
title={Proceedings of the 1996 International Workshop on Environments and Tools for Parallel Scientific Computing},
journal={Parallel Computing},
year={1997},
volume={23},
number={1-2},
page_count={270},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031118848&partnerID=40&md5=ab6a393593de5e61824c61e63235dad1},
abstract={The proceedings contains 16 papers on parallel computing. Topics discussed include: realistic parallel performance estimation; scheduling policies using Dimemas; efficient distributed computing on ATM networks; LU factorization; parallelization integrated tools; message-passing library implementation; heterogeneous network computing; Jacobi method parallelization; graph partitioning based methods and tools; parallelizing scientific codes; parallel program performance data; discrete event systems; parallel programming; and loop parallelization algorithm.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Dennis199750,
author={Dennis, J.B.},
title={A parallel program execution model supporting modular software construction},
journal={Proceedings - 3rd Working Conference on Massively Parallel Programming Models, MPPM 1997},
year={1997},
pages={50-60},
doi={10.1109/MPPM.1997.715961},
art_number={715961},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014824479&doi=10.1109%2fMPPM.1997.715961&partnerID=40&md5=916b14aa2b22c0975cbf20401d99d63a},
abstract={A watershed is near in the architecture of computer systems. There is overwhelming demand for systems that support a universal format for computer programs and software components so users may benefit from their use on a wide variety of computing platforms. At present this demand is being met by commodity microprocessors together with standard operating system interfaces. However current systems do not offer a standard API (application program interface) for parallel programming, and the popular interfaces for parallel computing violate essential principles of modular or component-based software construction. Moreover microprocessor architecture is reaching the limit of what can be done usefully within the framework of superscalar and VLIW processor models. The next step is to put several processors (or the equivalent) on a single chip. This paper presents a set of principles for modular software construction and describes a program execution model based on functional programming that satisfies the set of principles. The implications of the principles for computer system architecture are discussed together with a sketch of the architecture of a multithread processing chip which promises to provide efficient execution of parallel computations while providing a sound base for modular software construction. © 1998 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kacarska1997183,
author={Kacarska, M. and Andonov, D.},
title={Optimal module distribution for pipeline digital filter analysis algorithm},
journal={Journal of Computing and Information Technology},
year={1997},
volume={5},
number={3},
pages={183-192},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011717705&partnerID=40&md5=1d048d8b98d940c1722f2bbfe22ff753},
abstract={The development of general techniques for the analyses of digital filters with arbitrary topology is an area of interest in the process of digital filter design, especially in the educational area. A few analyses with a large number of frequency points are required until the desired response is achieved. Therefore, it is necessary to provide fast analysis algorithms. The pipeline implementation of the Crout's algorithm enables parallelized execution of the digital filter analysis. The process distribution must be optimized in order to achieve faster analysis execution and balanced processor performance. This paper presents a program package with a generalized approach to optimize the process distribution, based on an algorithm for element combinations for a set of size L into all subsets of size m arranged in lexicographic order. Two optimization criteria are used: the number of processors and their utilization. To avoid variable execution times on different processors, the number of operations executed at each processor is taken as a measure of processor execution time. Obtained results indicate that optimal distribution is achieved using smaller number of well balanced processors. With a larger N processor efficiency rises to more than 95%.},
author_keywords={Digital filter analysis;  Pipeline architecture;  Process distribution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Feldman199723,
author={Feldman, M.B.},
title={An ada 95 sort race construction set},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1997},
volume={1251},
pages={23-34},
doi={10.1007/3-540-63114-3_3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957634790&doi=10.1007%2f3-540-63114-3_3&partnerID=40&md5=36bd004e871d292a3f015a34c28c0343},
abstract={A “sort race” is a set of sort algorithms, executing concurrently and using some kind of visualization scheme to display the state of the various sorts as they proceed. The sort race is often used in algorithms and data structures courses to illustrate the disparate behavior and time performance of different sort algorithms; it has also served software engineering education, as an interesting, even exciting, example of concurrent programming and separation of concerns. This paper describes a set of Ada 95 packages providing a “sort race construction set,” which allows users to create sort races on various platforms using various techniques for rendering the race display. We have used the construction set with GNAT to implement sort races using the Macintosh user interface and graphics libraries, VGA graphical displays on MS-DOS computers, and standard 24×80 character displays. © Springer-Verlag Berlin Heidelberg 1997.},
author_keywords={Ada 95;  Algorithm animation;  Concurrent programming},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor19971,
title={15th British National Conference on Databases, BNCOD 1997},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1997},
volume={1271},
pages={1-233},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947968103&partnerID=40&md5=37641010c487d7e9dd30ee34e1303e89},
abstract={The proceedings contain 23 papers. The special focus in this conference is on Invited Paper; Transaction Processing; Optimisation; Posters. The topics include: The Role of Intelligent Software Agents in Advanced Information Systems; Location and Replication Independent Recovery in a Highly Available Database; Compensation-Based Query Processing in On-Line Transaction Processing Systems; Detection Arcs for Deadlock Management in Nested Transactions and Their Performance; Improved and Optimized Partitioning Techniques in Database Query Processing; Query Evaluation in CROQUE; Optimisation of Partitioned Temporal Joins; Maintaining Library Catalogues with an RDBMS; Merging an Active Database and a Reflective System: Modelling a New Several Active Meta-Levels Architecture; A Framework for Database Mining; Query Processing Techniques for Partly Inaccessible Distributed Databases; Indexing Multi-Visual Features in Image Database; Customisable Visual Query Interface to a Heterogeneous Database Environment; Automatic Web Interfaces and Browsing for Object-Relational Databases; A Mechanism for Automating Database Interface Design, Based on Extended E-R Modelling; Exploration of the Requirements in a Visual Interface to Tabular Data; The Deductive Object-Oriented Approach to the Development of Adaptive Natural Language Interfaces; An Efficient Indexing Scheme for Objects with Roles; A Prefetching Technique for Object-Oriented Databases; A Warehouse for Internet Data; An Object Versioning System to Support Collaborative Design within a Concurrent Engineering Context; Schema Integration with Integrity Constraints and A Method for Integrating Deductive Databases.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Torres1997429,
author={Torres, M. and Kofuji, S.T.},
title={The barrier synchronization impact on the MPI-programs performance using a cluster of workstations},
journal={3rd International Symposium on Parallel Architectures, Algorithms, and Networks, I-SPAN 1997},
year={1997},
pages={429-432},
doi={10.1109/ISPAN.1997.645132},
art_number={645132},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849101165&doi=10.1109%2fISPAN.1997.645132&partnerID=40&md5=6706b6af4766c46fd298181afb37e3ce},
abstract={The aim of this work is to measure the barrier synchronization influence on the overall performance of sevral application programs. In order to do that, we use the MPICH implementation version 1.1 of the MPI library. Moreover, we choose two barrier synchronization solutions: the MPICH software solution, i.e. the MPI. © 1997 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lauterbach1997593,
author={Lauterbach, T. and Unbehaun, M. and Angebaud, D. and Bache, A. and Groult, T. and Knott, R.P. and Luff, P. and Lebourhis, G. and Bourdeau, M. and Karlsson, P. and Rebhan, R. and Sundström, N.},
title={Using DAB and GSM to provide interactive multimedia services to portable and mobile terminals},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1997},
volume={1242},
pages={593-607},
doi={10.1007/bfb0037378},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-21744435460&doi=10.1007%2fbfb0037378&partnerID=40&md5=e24a1813a2b6944965bcbeed7a5a2d65},
abstract={The objective of the MEMO project is to design, integrate and operate applications which utilise interactive DAB services, i.e. combining the existing GSM mobile telecommunications network and the Digital Audio Broadcasting system. A first trial with this platform for mobile multimedia services was performed to demonstrate the results achieved after one year of work and cooperation of the European partners involved. The MEMO architecture comprises DAB transmitter equipment and PC-based terminals. During the specification of the first generation prototype and the software/hardware development the following results were achieved: Infrastructure and transmitter equipment (Specification and implementation of a protocol for information interchange between contents providers and the DAB network; design and implementation of the multi-network server (MNS) with an interface to the DAB-network-server (DNS) to feed the DAB transmitter; set-up of a DAB test transmitter at CCETT, Rennes; specification of a protocol for broadcasting Multimedia data on DAB) and Terminal (development of prototype DAB receivers with a data interface and PC plug-in card, notebook PCs and interface equipment to connect the PCs to the DAB receiver for data transfer at data rates up to 256 kbit/s; implementation of the GSM interaction channel between the mobile terminal and the MNS; definition and partial implementation of Application Programming Interfaces (APIs); development of terminal software for the network/system/application service layer and adaptation of publishing and construction industry applications to demonstrate mobile, interactive services). For the trial, the transmission chain from GSM request to DAB download was successfully completed and operated, including the connection between information provider and network operator. Three interactive applications were implemented and tested - a specialised software for the building construction industry, a system to download issues of a newspaper and a general software to transmit data objects - representing the contrary edges of services MEMO is targeting. Thus, the approach of interactive mobile services at high data rates was presented for the first time to potential customers and users providing valuable results. © Springer-Verlag Berlin Heidelberg 1997.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu1997167,
author={Liu, L. and Pu, C.},
title={An Adaptive Object-Oriented Approach to Integration and Access of Heterogeneous Information Sources},
journal={Distributed and Parallel Databases},
year={1997},
volume={5},
number={2},
pages={167-205},
doi={10.1023/A:1008641408566},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031118067&doi=10.1023%2fA%3a1008641408566&partnerID=40&md5=9997d98b6baeae3fe8272a2f9e024fe3},
abstract={A large-scale interoperable database system operating in a dynamic environment should provide Uniform access to heterogeneous information sources, Scalability to the growing number of information sources, Evolution and Composability of software and information sources, and Autonomy of participants, both information consumers and information producers. We refer to these set of properties as the USECA properties [29]. To address the research issues presented by such systems in a systematic manner, we introduce the Distributed Interoperable Object Model (DIOM). DIOM promotes an adaptive approach to interoperation via intelligent mediation [46, 47], aimed at enhancing the robustness and scalability of the services provided for integrating and accessing heterogeneous information sources. DIOM's main features include (1) the recursive construction and organization of information access through a network of application-specific mediators, (2) the explicit use of interface composition meta operations (such as specialization, generalization, aggregation, import and hide) to support the incremental design and construction of consumer's domain query model, (3) the deferment of semantic heterogeneity resolution to the query result assembly time instead of before or at the time of query formulation, and (4) the systematic development of the query mediation framework and the procedure of each query processing step from query routing, query decomposition, parallel access planning, query translation to query result assembly. To make DIOM concrete, we outline the DIOM-based information mediation architecture, which includes important auxiliary services such as domain-specific metadata library and catalog functions, object linking databases, and associated query services. Several practical examples and application scenarios illustrate the flavor of DIOM query mediation framework and the usefulness of DIOM in multi-database query processing.},
author_keywords={Distributed data management;  Intelligent integration of heterogeneous information sources;  Interoperability;  Object-oriented data model;  Query mediation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gall199718,
author={Gall, Harald and Jazayeri, Mehdi and Klosch, Rene and Trausmuth, Georg},
title={Architectural style of component programming},
journal={Proceedings - IEEE Computer Society's International Computer Software and Applications Conference},
year={1997},
pages={18-25},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030687419&partnerID=40&md5=d7e6f1c971cf984e5949d1fe37e4bdc1},
abstract={Component programming is a multiparadigm approach to software construction based on highly generic components. Because component programming is concerned with source code components, it is assumed by many to be a low-level approach to software development that affects only the development of source code libraries. On the contrary, this paper shows that the concepts of component programming go beyond library and source code issues, but define a new conceptual attempt to software development with generic components. We show that component programming is an architectural style that supports the building of classes of software architectures in a specific domain. Component programming can be applied in the early stages of software development when architectural issues are to be determined. All the benefits of using an architectural style, therefore, can also be gained by using component programming: it guides the engineer in the problem decomposition towards design and implementation of a system. The paper presents the architectural style of component programming and the insights we gained about component programming as we tried to define it as an architectural style.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kaiser19973,
author={Kaiser, Gail E. and Dossick, Stephen E. and Jiang, Wenyu and Yang, Jack Jingshuang},
title={Architecture for WWW-based hypercode environments},
journal={Proceedings - International Conference on Software Engineering},
year={1997},
pages={3-13},
doi={10.1145/253228.253231},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030615492&doi=10.1145%2f253228.253231&partnerID=40&md5=5ac30b5d1808b43e06ebe0360e4e5c94},
abstract={A hypercode software engineering environment represents all plausible multimedia artifacts concerned with software development and evolution that can be placed or generated on-line, from source code to formal documentation to digital library resources to informal e-mail and chat transcripts. A hypercode environment supports both internal (hypertext) and external (link server) links among these artifacts, which can be added incrementally as useful connections are discovered; project-specific hypermedia search and browsing; automated construction of artifacts and hyperlinks according the software process; application of tools to the artifacts according to the process workflow; and collaborative work for geographically dispersed teams. We present a general architecture for what we call hypermedia subwebs, and groupspace services operating on shared subwebs, based on World Wide Web technology - which could be applied over the Internet or within an intranet. We describe our realization in OzWeb.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Conway19961027,
author={Conway, Daniel G. and Ragsdale, Cliff T.},
title={Guidelines for designing effective spreadsheet models for optimization problems},
journal={Proceedings - Annual Meeting of the Decision Sciences Institute},
year={1996},
volume={2},
pages={1027-1029},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030366005&partnerID=40&md5=1052e564c25dfd5c15240d6e7fdc0bc7},
abstract={Electronic spreadsheets have become the most common software tool managers use to analyze data and model quantitative problems. Increasingly, these software packages are also being used in introductory OR/MS courses as the means for introducing students to a variety of quantitative modeling tools. Because spreadsheets are inherently free-form, they impose no particular guidelines or structure on the way problems may be modeled. Thus, academics and practitioners accustomed to solving problems in very methodical, structured ways are being forced to deal with these problems in the unstructured spreadsheet environment where there might be a variety of ways to implement and solve the same problem. This paper offers some suggestions and guidelines that can be followed to assist in the creation effective spreadsheet models for optimization problems.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Harrison1996417,
author={Harrison, A. and Thomas, P.G.},
title={Integrating multiple and diverse abstract knowledge types in real-time embedded systems},
journal={Knowledge-Based Systems},
year={1996},
volume={9},
number={7},
pages={417-434},
doi={10.1016/S0950-7051(96)01052-0},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030281818&doi=10.1016%2fS0950-7051%2896%2901052-0&partnerID=40&md5=c88130ae9206f07596093dfedc75b000},
abstract={Designers of large-scale real-time systems are increasingly turning to knowledge-based techniques in order to solve complex problems. This paper identifies three essential needs to support the implementation of these systems: first, the need to provide a variety of knowledge-based components that can be used to model the diverse expert domains being encountered; second, the need to provide the user with the means of creating multiple independent instances of the knowledge-based components; and third, the need to provide an integrating environment in which the knowledge-based instances may be controlled. This paper uses ideas derived from the concept of abstract data types and recommends the construction of a library of diverse knowledge-based components, called abstract knowledge types, and that multiple instances of the abstract knowledge types be integrated and controlled using a blackboard architecture. A prototype component library and a blackboard have been implemented in Ada in order to take advantage of a real-time language which supports software engineering principles through a well defined and enforced standard. The use of abstract knowledge types gives a uniform software engineered approach to the development and integration of both conventional and knowledge-based components.},
author_keywords={Abstract knowledge types;  Ada;  Blackboard architecture;  Knowledge-based components;  Knowledge-based techniques;  Real-time embedded system;  Software engineering principles},
document_type={Article},
source={Scopus},
}

@ARTICLE{Surridge19961053,
author={Surridge, M. and Tildesley, D.J. and Kong, Y.C. and Adolf, D.B.},
title={A parallel molecular dynamics simulation code for dialkyl cationic surfactants},
journal={Parallel Computing},
year={1996},
volume={22},
number={8},
pages={1053-1071},
doi={10.1016/S0167-8191(96)00042-7},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030260665&doi=10.1016%2fS0167-8191%2896%2900042-7&partnerID=40&md5=3d078aa1035f88bce769f1e3dd616f43},
abstract={We have developed a new simulation code, COMFORT, for the study of assemblies of flexible surfactant molecules, structured for parallel execution and specialised to surfactants with dialkyl chain geometry. The approach is a hybrid domain-decomposition and systolic-loop algorithm which is suitable for systems composed of long chain molecules and with tens of thousands of atoms in total. The algorithm uses a modified Ewald technique for two dimensionally periodic systems which has been successfully parallelized. The code was designed to be highly portable between machines of different architectures. The code has been tested on a number of platforms including the Intel iPSC/860, the IBM SP1, the CRAY T3D, a SGI Power Challenge and a number of workstation clusters. We demonstrate that scalable parallel computing technology, combined with appropriate software, can provide a commercially viable simulation system for use in the exploration and development of surfactant assemblies.},
author_keywords={Implementation;  Molecular dynamics;  Parallel architectures;  Performance;  Portability;  Simulation;  Workstation clusters},
document_type={Article},
source={Scopus},
}

@ARTICLE{DalCin1996211,
author={Dal Cin, M. and Hessenauer, H. and Hohl, W.},
title={The modular expandable multiprocessor system MEMSY},
journal={Computer Systems Science and Engineering},
year={1996},
volume={11},
number={4},
pages={211-219},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030189964&partnerID=40&md5=1f9648381ccb8751b798934f23c600f4},
abstract={The experimental multiprocessor system MEMSY was built to validate the concept of a scalable multiprocessor architecture based on local shared memory. The main application areas are scientific computations with high demand for processing power and memory capacity. In designing the hardware architecture the extensive use of standard components and fault tolerance were prerequisites. The programming model of MEMSY is custom made, reflecting its true hardware structure, whereas the operating system is a Unix extension. In massively parallel system with its complexity and large number of components the chance of a single or multiple failure is no longer negligible. Therefore, redundance, reconfigurability and diagnosis techniques have been incorporated at the design stage itself and not as a subsequent add-on.},
author_keywords={Fault tolerance;  MIMD;  Multiprocessor;  Scalability},
document_type={Article},
source={Scopus},
}

@ARTICLE{DoValleSimões1996421,
author={Do Valle Simões, E. and Uebel, L.F. and Barone, D.A.C.},
title={Hardware implementation of RAM neural networks},
journal={Pattern Recognition Letters},
year={1996},
volume={17},
number={4 SPEC. ISS.},
pages={421-429},
doi={10.1016/0167-8655(95)00137-9},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030568488&doi=10.1016%2f0167-8655%2895%2900137-9&partnerID=40&md5=a248b487d4be7c79a5f33b5fe5567f99},
abstract={This work describes an alternative technique for hardware and software implementation of RAM-based Boolean neural networks, which describes neurons using the VHDL language. An example of application consisting of the classification problem of the British mail scanned address is attended with a RAM architecture presenting (340 × 12)-input neurons. The weights of each neuron are represented by its truth table and described using simple logic gates (AND, OR, and NOT), aiming to make possible the network logic minimisation and its hardware implementation by the ALTERA MAX + PLUS II fast prototyping package (Altera, 1992). The developed software tool allows the specification and training of the network. Then, its VHDL description is generated to be interpreted and minimised by the ALTERA EPLD design system. If it is not necessary to have high-speed processing or if pre-processing phases are needed, the ANN can be implemented in software. The software strategy makes use of the direct translation of the VHDL description into a simplified C language code. Once the user has specified and taught the network, this approach makes possible automatic prototyping of RAM neural networks in software and hardware.},
author_keywords={Boolean neural networks;  Character recognition;  Fast prototyping;  Image classification;  Neural network hardware implementation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bao1996500,
author={Bao, Y. and Horowitz, E.},
title={A new approach to software tool interoperability},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={1996},
volume={Part F128723},
pages={500-509},
doi={10.1145/331119.331432},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960294371&doi=10.1145%2f331119.331432&partnerID=40&md5=25843466c543258f086533c9f0b494ba},
abstract={Most of the existing interoperability solutions access the functionality of tools through APIs, message interfaces, and command-line options. The inadequacy of those interfaces and lack of access to the source code of tools make it very difficult for users to create sophisticated interoperability solutions. Furthermore these solutions tend to hardwire the policy and behavior of the integration inside the tools, that is, the policy is mixed with the mechanism. Thus when the system evolves and the integration requirements change, major modifications to the integrated system have to be performed to meet the new requirements. This paper presents a flexible integration approach which overcomes the problems mentioned above and offers a new way to solve the interoperability problem. The solution is based upon accessing the tools through their user interface. Thus no additional effort on the part of the developer of the tool is required. The paper describes the design and implementation of Tool Integration Language (TIL) which provides flexible integration mechanism for our approach. By using TIL to model the relationships between tools, our approach provides the capability for users to dynamically bind the relationship, making it possible to change the way tools interact with each other dynamically, thus providing strong support for system evolution. © 1996 ACM.},
author_keywords={CASE;  Software engineering environment (SEE);  Software interoperability;  Tool integration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Santurjian1996563,
author={Santurjian, O. and Kolarow, L.},
title={A spatial fem model of thermal stress state of concrete blocks with creep consideration},
journal={Computers and Structures},
year={1996},
volume={58},
number={3},
pages={563-574},
doi={10.1016/0045-7949(95)00156-B},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030567138&doi=10.1016%2f0045-7949%2895%2900156-B&partnerID=40&md5=745ad52a4e5362fb3ba89fc05dcc552c},
abstract={Specific features of the thermal stress fields in mass concrete structures are their always spatial character and complete dependence on the maturity function deformation properties (elasticity and creep). The extremely complex nature of the latter and of the field of temperature changes is a reason for continuous development and improvement of the models for numerical description of the thermal stress state (TSS) in mass concrete. In the paper a linear, mechanically correct, FEM model of TSS of incremental blocks, accounting for creep by relaxation functions is formulated. On its basis an algorithm for a three-dimensional numerical transient TSS model has been carried out and included in the program "Thermostress-5" for three-dimensional thermal studies of mass concrete blocks. It has been verified by comparison of numerically evaluated temperatures and stresses, with such measured by instruments made by Japanese researchers in a thick experimental wall.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rudgyard1996358,
author={Rudgyard, M. and Schönfeld, T. and D'Ast, I.},
title={A parallel library for CFD and other grid-based applications},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1996},
volume={1067},
pages={358-359},
doi={10.1007/3-540-61142-8_570},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944082500&doi=10.1007%2f3-540-61142-8_570&partnerID=40&md5=729a58ecdd3159044170f959a84363ed},
abstract={We describe the design and implementation of a software library that aims to simplify the task of writing and developing parallel applications based on arbitrary grids. We discuss the need for such tools, both for reducing development costs and for obtaining portable code that remains efficient on a wide-range of HPC platforms. We give examples of the use of the library for CFD simulations using the Meiko CS2 and discuss future developments that axe possible. Although we consider examples taken from the field of Computational Fluid Dynamics, the library is applicable to most numerical simulations that make use of structured, unstructured or hybrid meshes. © Springer-Verlag Berlin Heidelberg 1996.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kortüm1996513,
author={Kortüm, W. and Schwartz, W. and Wentscher, H.},
title={Optimization of active vehicle suspensions with mechatronic software tools [Optimierung aktiver fahrzeugfedemngen durch mechatronik-simulation]},
journal={At-Automatisierungstechnik},
year={1996},
volume={44},
number={11},
pages={513-521},
doi={10.1524/auto.1996.44.11.513},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863126081&doi=10.1524%2fauto.1996.44.11.513&partnerID=40&md5=aba650d2524d0a2c9fa2b2d9d9734e59},
abstract={Costs of development and requirements of safety, but also of comfort, force the industry to perform accompanying Computer simulations during the whole development of the vehicle. The Simulation of modern vehicles in lightweight construction which use electronics in regions which are originally reserved to mechanical engineering (suspensions, brakes, steering) requires an open Simulation Software which can be connected with Software products of other disciplines (CAD, FEM, control, electronics etc.) without problems. This article shows the Cooperation of two Software packages for Simulation of multibody systems (MBS) and multiobjective Parameter optimization to optimize the suspension control during the design process with a complex Simulation model (“design-by-simulation”). This paper gives a short description of the MBS program SIMPACK and shows in two examples the design of (semi) active suspensions with multiobjective parameter optimization using SIMPACK in connection with the Simulation and design environment of ANDECS. The first example demonstrates in a case study the optimization of a Controller with respect to safety and comfort using a complex verified model of an ojf-road vehicle. The second example deals with the design of a semi-active nose landing gear of a transport aircraft to decrease the Vibration due to the elasticity of the aircraft structure (fuselage, wings). © 1996 R. Oldenbourg Verlag.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ott19961,
author={Ott, M. and Reininger, D. and Michelitsch, G. and Bansal, V. and Siracusa, R.J. and Raychaudhuri, D.},
title={Heidi-II: A software architecture for ATM network based distributed multimedia systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1996},
volume={1045},
pages={1-14},
doi={10.1007/3-540-60938-5_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750680002&doi=10.1007%2f3-540-60938-5_1&partnerID=40&md5=b709754e89156e7fadade95a3b65f102},
abstract={In this document, we describe the architecture of a distributed multimedia software prototype (“Heidi-II”) currently under development at our laboratories. This prototype aims to demonstrate a software framework for development of efficient, quality-of-service (QoS) based multimedia applications over ATM networks, using a synergistic combination of several novel approaches. In particular, the software architecture incorporates the following key components: Jodler, a new distributed scripting language for easy-to-use and efficient object oriented implementation of multimedia applications across the network; CockpitView, a new graphical user interface for intuitive access of multimedia services over a network; ATM Service Manager, an advanced ATM API with automatic service provisioning, transport protocol options, ABR/ VBR/CBR services, dynamic bandwidth renegotiation and quality-of-service (QoS) support; VBR+, a dynamically renegotiated variable bit-rate transport mode in ATM networks for efficient support of media streams with networklevel QoS control; and Multimedia Transport Protocol (MTP), a stream-oriented, lightweight media transport protocol customized for delivery of real time video/audio, etc. over ATM networks. © Springer-Verlag Berlin Heidelberg 1996.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Case1996333,
author={Case, M.P. and Lu, S.C.-Y.},
title={Discourse Model for collaborative design},
journal={CAD Computer Aided Design},
year={1996},
volume={28},
number={5},
pages={333-345},
doi={10.1016/0010-4485(95)00053-4},
note={cited By 71},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0042854787&doi=10.1016%2f0010-4485%2895%2900053-4&partnerID=40&md5=22c74c7b9c6f95c4c9b2ddeba8614876},
abstract={A Discourse Model, including a structure and a process, is developed that provides software support for collaborative engineering design. The model shares characteristics of other design systems in the literature, including frames, constraints, semantic networks, and libraries of sharable design objects. It contributes a new model for conflict-aware agents, dynamic identification and dissemination of agent interest sets, a virtual workspace language, automatic detection of conflict, and a unique protocol for negotiation that ensures that interested agents have an opportunity to participate. The model is implementation independent and applicable to many research and commercial design environments currently available. An example scenario is provided in the architecture/engineering/construction domain that illustrates collaboration during the conceptual design of a fire station. Published by Elsevier Science Ltd.},
author_keywords={Agent;  Blackboard architecture;  Concurrent engineering;  Conflict;  Discourse design collaboration;  KQML},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kuo1996427,
author={Kuo, D.},
title={Model and Verification of a Data Manager Based on ARIES},
journal={ACM Transactions on Database Systems},
year={1996},
volume={21},
number={4},
pages={427-479},
doi={10.1145/236711.236712},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030407533&doi=10.1145%2f236711.236712&partnerID=40&md5=82eb3bec06ed7307a79ec72f00473413},
abstract={In this article, we model and verify a data manager whose algorithm is based on ARIES. The work uses the I/O automata method as the formal model and the definition of correctness is defined on the interface between the scheduler and the data manager.},
author_keywords={F.3.1 [Logics and Meanings of Programs]: Specifying and Reasoning about Programs - assertions, invariants, pre- and postconditions;  H.2.2 [Data Management]: Physical Design - restart recovery;  H.2.4 [Data Management]: Systems - transaction processing},
document_type={Article},
source={Scopus},
}

@ARTICLE{East1996267,
author={East, E.W. and Fu, M.C.},
title={Abstracting lessons learned from design reviews},
journal={Journal of Computing in Civil Engineering},
year={1996},
volume={10},
number={4},
pages={267-275},
doi={10.1061/(ASCE)0887-3801(1996)10:4(267)},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030269457&doi=10.1061%2f%28ASCE%290887-3801%281996%2910%3a4%28267%29&partnerID=40&md5=febb201cf9d20dccff37ebfcc66bff6e},
abstract={The application of lessons learned during construction to future designs is a goal of many construction organizations. This paper presents the current status of design review systems within the U.S. Army Corps of Engineers, and describes a new computer program, the Lessons-Learned Generator, that abstracts frequently used design review comments. Frequently used comments in the Reviewer's Assistant system are evaluated based on usefulness, generality, and content stability. Well-formed comments are then abstracted and included in a new Reviewer's Assistant project. The abstraction process keeps the growth of the database at a reasonable level and allows users to identify and distribute sets of repetitive comments. Difficulties associated with the automated abstraction of lessons learned are discussed. Since the Lessons-Learned Generator identifies lessons learned as part of a reviewer's daily business practice, then makes the lessons-learned immediately accessible within the Reviewer's Assistant system, this approach may be more effective than paper checklists or stand-alone automation approaches for applying lessons learned.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Firth1996863,
author={Firth, J.R. and Forsyth, C.H. and Wand, I.C.},
title={The compilation of Ada},
journal={Software - Practice and Experience},
year={1996},
volume={26},
number={8},
pages={863-909},
doi={10.1002/(sici)1097-024x(199608)26:8<863::aid-spe37>3.0.co;2-4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030216426&doi=10.1002%2f%28sici%291097-024x%28199608%2926%3a8%3c863%3a%3aaid-spe37%3e3.0.co%3b2-4&partnerID=40&md5=aadfcf1581e4738d61a4658d5865d489},
abstract={The Ada programming language has been in use for 12 years and a major revision of the language, now called Ada 95, was adopted as a new international standard on 15 February 1995. There have been many accounts of the use of Ada 83 in a wide variety of applications but little has been published about the construction, performance and correctness of Ada compilers. This paper details the design and construction of the York Ada compiler and examines the major technical issues that determined the form of the software constructed. The principal objectives of the compiler were: (a) comprehensive error messages for the user; (b) compiler portability; and (c) convenient use in the UNIX environment. A subsidiary objective was to research the technical issues involved in the compilation of the (then) novel features of Ada (e.g. tasking). The principal technical innovations of the resulting compiler were the use of software tools to construct some of the syntax-based parts of the system, and use of a machine description and tree automata to support portable code generation. The resulting compiler is small, reasonably fast, and easy to move from machine to machine. It will compile very large programs, involving many units and libraries, and integrates well with the UNIX environment. The compiler has been ported to six different architectures and validated under four different versions of the Validation Test Suite; it has been used widely academically and is sold commercially. Based upon this accumulated experience, we examine critically the compiler's design and construction. We conclude that the lack of a rigorous language definition made the design and construction particularly difficult, but that the portability method employed was successful. With care, the compiler for a large language need not be huge; its construction is then possible using a small team. This compiler and its tool set would not, however, pass a rigorous hazard analysis and should not be used for the construction of safety-critical systems.},
author_keywords={Ada;  Code generation;  Compiler;  Correctness;  Implementation;  Portability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vercauteren1996521,
author={Vercauteren, Steven and Lin, Bill and De Man, Hugo},
title={Constructing application-specific heterogeneous embedded architectures from custom HW/SW applications},
journal={Proceedings - Design Automation Conference},
year={1996},
pages={521-526},
doi={10.1145/240518.240617},
note={cited By 47},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029720741&doi=10.1145%2f240518.240617&partnerID=40&md5=1bbb0f113574e943e8d9804f7f06bdf6},
abstract={Deep sub-micron processing technologies have enabled the implementation of new application-specific embedded architectures that integrate multiple software programmable processors (e.g. DSPs, microcontrollers) and dedicated hardware components together onto a single cost-efficient IC. These application-specific architectures are emerging as a key design solution to today's microelectronics design problems, which are being driven by emerging applications in the areas of wireless communication, broadband networking, and multimedia computing. However, the construction of these customized heterogeneous multiprocessor architectures, while ensuring that the hardware and software parts communicate correctly, is a tremendously difficult and highly error proned task with little or no tool support. In this paper, we present a solution to this embedded architecture co-synthesis problem based on an orchestrated combination of architectural strategies, parameterized libraries, and software tool support.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pasley1996725,
author={Pasley, Gregory P. and Roddis, W.M.Kim},
title={Decision support tool for the steel building industry},
journal={Computing in Civil Engineering (New York)},
year={1996},
pages={725-731},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029719191&partnerID=40&md5=875eeeb94b8d4674445f1b76eebf674b},
abstract={SteelTeam is a computer-based tool that integrates knowledge-based systems with a computer-aided design and drafting environment. SteelTeam provides a common representation schema that allows project data, in the form of the project data model, to be shared and transmitted electronically between the parties involved in the design, detailing, fabrication, and construction of steel buildings. In addition, SteelTeam acts as an intelligent assistant allowing informed decision making throughout the design cycle of a steel building. SteelTeam provides expert feedback in the form of knowledge-based systems that help the parties involved avoid downstream problems by addressing various concerns at an early stage in the design. The SteelTeam system allows the automation of routine tasks and contains a costing module that allows comparison of design alternatives.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Williams1996404,
author={Williams, Mike},
title={Graphical simulation for project planning: 4D-PlannerTM},
journal={Computing in Civil Engineering (New York)},
year={1996},
pages={404-409},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029699857&partnerID=40&md5=63411d578f9ee32abcbf1d7a6c672c09},
abstract={Today's complex and schedule driven projects, coupled with increasingly knowledgeable and active project participants, require more effective planning and communication tools than traditional static drawings and complicated network schedules. 4D-PlannerTM was developed in response to project visualization, simulation, and communication needs. This powerful but easy to use planning tool allows the user to combine the 3D CAD model with the project schedule and represent the construction plan graphically. This allows better scenario analyses, quicker understanding of the impact of changes, and improved understanding of the project execution plan by non-technical project participants.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mylopoulos1996238,
author={Mylopoulos, J. and Chaudhri, V. and Plexousakis, D. and Shrufi, A. and Topaloglou, T.},
title={Building knowledge base management systems},
journal={VLDB Journal},
year={1996},
volume={5},
number={4},
pages={238-263},
doi={10.1007/s007780050027},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0010019841&doi=10.1007%2fs007780050027&partnerID=40&md5=76a9091cfd895e197ca39700688424af},
abstract={Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible. © Springer-Verlag 1996.},
author_keywords={Concurrency control;  Constraint enforcement;  Knowledge base management systems;  Rule management;  Storage management},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Deutsch199554,
author={Deutsch, Jeff K. and Gary, Mark R.},
title={Physical Volume Library deadlock avoidance in a striped media environment},
journal={Digest of Papers - IEEE Symposium on Mass Storage Systems},
year={1995},
pages={54-64},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029531331&partnerID=40&md5=9d8aaaea84ec2cbc392cc0d643e1ecd0},
abstract={Most modern high performance storage systems store data in large repositories of removable media volumes. Management of the removable volumes is performed by a software module known as a Physical Volume Library (PVL). To meet performance and scalability requirements, a PVL can be asked to mount multiple removable media volumes for use by a single client for parallel data transfer. Mounting sets of volumes creates an environment in which it is possible for multiple client requests to deadlock while attempting to gain access to storage resources. Scenarios leading to deadlock in a PVL include multiple client requests that contend for the same cartridge(s), and client requests that vie for a limited set of drive resources. These deadlock scenarios are further complicated by the potential for volumes to be mounted out-of-order (for example, by Automatic Cartridge Loaders or human operators). This paper begins by introducing those PVL requirements which create the possibility of deadlock. Next we examine traditional approaches to deadlock resolution and how they might be applied in a PVL. This leads to a design for a PVL that addresses deadlock scenarios. Following the design presentation is a discussion of possible design enhancements. We end with a case study of an actual implementation of the PVL design in the High Performance Storage System (HPSS).},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sreekanth19953426,
author={Sreekanth, Uday and Govindaraj, T. and Mitchell, Christine M. and McGinnis, Leon F.},
title={POEMS - a process and object environment for manufacturing simulation},
journal={Proceedings of the IEEE International Conference on Systems, Man and Cybernetics},
year={1995},
volume={4},
pages={3426-3431},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029484195&partnerID=40&md5=ac040c54b4a39297cb11a4f4ec1e7ce7},
abstract={Existing research efforts in object-oriented manufacturing simulation are structured predominantly on the event-interaction simulation paradigm. An often effective simulation option for discrete-event systems modeling is a process interaction approach, as indicated by the success of commercial manufacturing simulation software that support this paradigm (see [1].) The construction and use of an object-oriented modeling architecture structured on the process-interaction simulation paradigm is described in this paper. Through a real-world example, the creation and re-use of an object and process library for manufacturing entities is illustrated. The software is implemented in C++, using C++/CSIM [11] and runs on UNIX workstations.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kermarrec1995263,
author={Kermarrec, Y. and Pautet, L. and Tardieu, S.},
title={GARLIC: Generic Ada reusable library for interpartition communication},
journal={Proceedings of the Conference on TRI-Ada 1995: Ada's Role in Global Markets: Solutions for a Changing Complex World, TRI-Ada 1995},
year={1995},
pages={263-269},
doi={10.1145/376503.376591},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957894235&doi=10.1145%2f376503.376591&partnerID=40&md5=eeb15d2dd8143f2813a32ab721eb369b},
abstract={This paper presents an implementation of the distributed programming features of Ada 95 within the GNAT system. The work we describe is the result of an international collaboration whose goal is to produce a high level environment for distributed system programming. This paper focuses on issues of interprocessor communication, since this is the core element of our software architecture. We describe the design and implementation of GARLIC, an interface between the network and the application. GARLIC is an extension of the predefined interface specified by System.RPC. © 1995 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bosch1995255,
author={Bosch, V.M. and Hancock-Beaulieu, M.},
title={CDROM user interface evaluation: The appropriateness of GUIS},
journal={Online Information Review},
year={1995},
volume={19},
number={5},
pages={255-270},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029387481&partnerID=40&md5=aeb9409faad07a1dbfe39d5d99f84b98},
abstract={Following a general trend in software development, CDROM applications are increasingly implementing Graphical User Interfaces (GUIs). The general assumption is that GUIs offer advantages in terms of ease of learning and use, especially for non-expert users. Moreover, the adoption of GUIs for CDROMs has been suggested as a means of providing a de facto standard interface. This study assesses the appropriateness of GUIs, more specifically Windows-based interfaces for CDROM. An evaluation model was devised to carry out an expert evaluation of the interfaces of seven CDROMproducts. The model identified two levels of interaction, the dialogue level and task level, and focused on general interface features, search and retrieval tasks, and output and processing options as well as the help facilities. The results are discussed in the light of HCI Usability Criteria and design guidelines (including general interface design guidelines, specific Windows design guidelines and The CDROM Consistent Interface Guidelines) to assess to what extent the applications comply and appropriate recommendations are made.},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Dikaiakos1995365,
author={Dikaiakos, M.D. and Manoussaki, D. and Lin, C. and Woodward, D.E.},
title={The portable parallel implementation of two novel mathematical biology algorithms in ZPL},
journal={Proceedings of the International Conference on Supercomputing},
year={1995},
volume={Part F129361},
pages={365-374},
doi={10.1145/224538.224632},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029181985&doi=10.1145%2f224538.224632&partnerID=40&md5=9e4ae68c253207c5a203568e7c9dab45},
abstract={This paper shows that mathematical models of biological pattern formation are ideally suited to data parallelism. We present two new algorithms, one for simulating the dynamic structure of fibroblasts, and the other for studying the self-organization of motile bacteria. We describe implementations of these algorithms using a high level data parallel language called ZPL, and we give performance results for the Kendall Square Research KSR-2 and the Intel Paragon that include comparisons against sequential Fortran. © 1995 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Benecke19954,
author={Benecke, C. and Grund, R. and Hohberger, R. and Kerber, A. and Laue, R. and Wieland, T.},
title={Chemical isomerism, a challenge for algebraic combinatorics and for computer science},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1995},
volume={948},
pages={4-20},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3743079058&partnerID=40&md5=35fa919b1b95afd0495fb57afd39370c},
abstract={Chemical isomerism means that there do occur different molecules with the same atomic constitutents. For example, about 70 molecules have been found which consist of exactly six carbon and six hydrogen atoms, or, in formal terms, which have the chemical formula C6H6. The existence of chemical isomerism was stated by the end of the eighteenth century, it was verified a quarter of a century later and explained another half of a century afterwards. It stimulated the development of graph theory and gave birth to algebraic combinatorics. It is only now that efficient computers and the helpful methods of computer science can be used in order to solve the basic problem related to chemical isomerism and the corresponding molecular structure elucidation. This problem is the construction of all the molecular graphs which correspond to a given chemical formula and (optional) further conditions on prescribed and forbidden substructures etc. Here we therefore present MOLGEN, a software package which solves this problem in a redundancy free and efficient way. It is designed for research and education, and it finds applications in molecular structure elucidation, where a molecule has to be identified from experimental, usually from spectroscopic data. MOLGEN provides the full wealth of mathematically possible structures (multigraphs with given degree sequence, where the vertices are colored by atom names), from which further chemical tests allow to pick the correct solutions. From the mathematical point of view, MOLGEN is based on the constructive theory of discrete structures, and it clearly shows the success of the combination of algebraic and combinatorial methods for applications in sciences. Moreover, its efficiency is based on a careful use of data structures. MOLGEN is intensively used in chemical industry. © Springer-Verlag Berlin Heidelberg 1995.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sosič1995767,
author={Sosič, R.},
title={A procedural interface for program directing},
journal={Software: Practice and Experience},
year={1995},
volume={25},
number={7},
pages={767-787},
doi={10.1002/spe.4380250704},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029346099&doi=10.1002%2fspe.4380250704&partnerID=40&md5=e9c94f6f838cf94572082538741d9abe},
abstract={Debugging and performance measurement tools are becoming more important in the development and maintenance of increasingly complex software. These tools belong to a class of programs, called directors. Directors are programs that monitor and control other programs. Through monitoring, directors analyze program execution at runtime. Monitoring provides tracing primitives as well as access to program's variables, dynamic data structures, and its internal state. Results of the monitoring analysis can be used by the director itself to change the future program behavior or presented to a human user for an interactive review and possible feedback actions. The future program behavior is changed through controlling primitives which provide an organized way to modify data values as well as the program code. This paper presents a library of directing commands which enable the construction of powerful directors. The interface has been implemented in a Unix environment as a runtime subsystem running in the directed program's address space. The paper provides the description of the interface and the basic programming techniques in building directors. Examples of novel applications, illustrating the use of the directing interface, are demonstrated by the directors for the visualization of program control and structured snapshots. Copyright © 1995 John Wiley & Sons, Ltd},
author_keywords={debugging;  directing;  monitoring;  procedural interface;  program visualization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Franke1995127,
author={Franke, Hubertus and Wu, C.Eric and Riviere, Michel and Pattnaik, Pratap and Snir, Marc},
title={MPI programming environment for IBM SP1/SP2},
journal={Proceedings - International Conference on Distributed Computing Systems},
year={1995},
pages={127-135},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029235581&partnerID=40&md5=97a924ef6d08789ae8c61d4ff66bb093},
abstract={In this paper we discuss an implementation of the Message Passing Interface standard (MPI) for the IBM Scalable Power PARALLEL 1 and 2 (SP1, SP2). Key to a reliable and efficient implementation of a message passing library on these machines is the careful design of a UNIX-Socket like layer in the user space with controlled access to the communication adapters and with adequate recovery and flow control. The performance of this implementation is at the same level as the IBM-proprietary message passing library (MPL). We also show that in the IBM SP1 and SP2 we achieve integrated tracing ability, where both system events, such as context switches and page fault etc., and MPI related activities are traced, with minimal overhead to the application program, thus presenting application programmers the trace of all the events that ultimately affect efficiency of a parallel program.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gaing1995467,
author={Gaing, Z.L. and Lu, C.N. and Chang, B.S. and Cheng, C.L.},
title={Object-oriented approach for implementing power system restoration package},
journal={IEEE Power Industry Computer Applications Conference},
year={1995},
pages={467-473},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029230867&partnerID=40&md5=8bfb8b4c8300921bbcb2d99ad357bd51},
abstract={Due to many unforeseen circumstances that could happen in the today's bulk power system, there is a possibility of a system wide outage. In order to provide aids to the power system dispatchers following a complete collapse of the power system, a prototype package has been developed. Through interactive and friendly graphic interface, the package suggests a guideline for the dispatcher to restore the power system. With an aim to increase the ease of maintenance, object-oriented technique was adopted to implement the package. The development of the software involved three stages: 1) Object oriented analysis, 2) Object oriented design, and 3) Integration and testing. In this paper, the structure and the development procedure of the prototype system are presented.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cooper19951499,
author={Cooper, Thomas E. and Lutz, James D.},
title={Knowledge-based construction claims advisor},
journal={Computing in Civil Engineering (New York)},
year={1995},
volume={2},
pages={1499-1505},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029225788&partnerID=40&md5=841584a1d6da6727759d49770fe0a684},
abstract={The claims-related provisions of the American Institute of Architects (A.I.A.) A201 General Conditions document are incorporated into a knowledge-based system that instructs the user in the evaluation and processing of a construction claim. This interactive system, Claims Advisor, operates using the Windows version of the expert system shell, EXSYS Professional. A total of 21,702 discreet paths through the system's internal logic diagram is available. In each case a specific course of action is recommended to the user. In addition, the system allows for unlimited modification of the user input so that the resulting impact of each of the claim's various parameters may be evaluated. In the analysis of seven typical construction claims cases by the Claims Assistant, the system provided the same conclusion that was obtained by litigation in six of the cases. In the remaining one, the system provided a conclusion that was opposite to that reached by the District Court, but identical to that arrived at by the Court of Appeals.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu1995892,
author={Wu, Pei-Chi and Lin, Jin-Hue and Wang, Feng-Jian},
title={Designing a reusable symbol table library},
journal={IEEE Region 10's Annual International Conference, Proceedings},
year={1995},
volume={2},
pages={892-896},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029219543&partnerID=40&md5=25cf0865a69318598287c05fe62763b6},
abstract={In this paper, we address our design rationale and present the design of a symbol table library. A symbol processing task can be divided into two parts: one for storing/retrieving symbols, and the other for semantics checking. The former is language-independent and can be defined as reusable components. The latter is language-dependent and can be defined as parametric types to symbol table components or as specific semantics rules. We identify general properties of name analysis problems: topology, overload, navigation, etc. The resulting design of the library covers most usages in symbol processing.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lawrence1995139,
author={Lawrence, Philip and Brucker, Beth and Case, Michael and Ganeshan, Rajaram and Golish, Michael and Griffith, Eric and Heckel, Jeffrey},
title={Application of integrated product and process development in facility delivery},
journal={Computing in Civil Engineering (New York)},
year={1995},
volume={1},
pages={139-146},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029217030&partnerID=40&md5=341dfac7e3c93991fb085f62117a1f0e},
abstract={The following is a description of an application of Integrated Product and Process Development (IPPD) principles to the design and construction of Army facilities. The Collaborative Engineering Team at the Construction Engineering Research Laboratories of the US Army Corps of Engineers has developed a hardware and software test bed to demonstrate how IPPD techniques can be applied to the design and construction of facilities. The test bed uses a virtual teaming architecture centered upon an object-oriented representation provided by the Agent Collaboration Environment (ACE) and Design++. The information exchange between these two systems is accomplished using an approach called the Virtual Workspace Language (VWL) which incorporates the Knowledge Interchange Format (KIF) and the Knowledge Query and Manipulation Language (KQML). Common frame libraries for the electronic modeling of product and process information has been developed to provide a shared ontology with which all participants can create objects and communicate information. Agents which use this shared representation to exchange information have been developed for project management, architecture, design, construction, and operation and maintenance of a standard US Army fire station facility. Also, legacy software such as Computer Aided Design (CAD) tools have been integrated into the system.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bamberger19952873,
author={Bamberger, Roberto H. and Evans, Brian L. and Lee, Edward A. and McClellan, James H. and Yoder, Mark A.},
title={Integrating analysis, simulation, and implementation tools in electronic courseware for teaching signal processing},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={1995},
volume={5},
pages={2873-2876},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028996492&partnerID=40&md5=23aa0da6b5645bf2bcd5b7052aeffe23},
abstract={A typical path in learning digital signal processing begins at the theoretical end and progresses toward the practical constraints imposed by implementation in hardware or software. On this path, the student would learn how to convert mathematical theory into algorithms and then algorithms into efficient implementations. In this paper, we first summarize the electronic courseware we have already developed in Mathematica, MATLAB, and Ptolemy to teach DSP theory, algorithms, and implementation, respectively. Then, we discuss ways to integrate our efforts to help students discover the connections between these topics.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dharwadkar1994539,
author={Dharwadkar, Parmanand V. and O'Connor, James T. and Gatton, Thomas M.},
title={3D CAD modeling and graphical simulation of mobile cranes},
journal={Computing in Civil Engineering (New York)},
year={1994},
number={1},
pages={539-546},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028607687&partnerID=40&md5=1a8aa8450997f95bcae457df842c3106},
abstract={This paper describes the research performed to lay a foundation for developing a graphical simulation system which can be used in the planning of heavy lifts using mobile cranes. Mobile crane components are represented as different objects and each object is given a unique name and number. A common terminology is developed to represent objects of different crane configurations. This decomposition of mobile cranes into standard objects is used for developing a standard crane model. A methodology is developed for creating a 3D CAD crane model based on crane specifications obtained from the crane manufacturers. Intergraph's MicroStation (3D CADD Package) is used to create the crane geometry. A procedure is presented for creating a fully animated mobile crane model based on the created 3D CAD crane geometry. Bechtel's Walkthru (Walkthrough Animation Package) is used to create the animated models and perform graphical simulation of mobile cranes. The hierarchical kinematics relationships between different crane objects is defined, and crane motion menus are developed for the nine possible degrees of freedom. A library of crane models is developed which includes crawler cranes, truck carrier cranes, crawler cranes with tower, and all-terrain carrier cranes.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dharwadkar1994759,
author={Dharwadkar, Parmanand V. and Varghese, Koshy and O'Connor, James T. and Gatton, Thomas M.},
title={Graphical visualization for planning heavy lifts},
journal={Computing in Civil Engineering (New York)},
year={1994},
number={1},
pages={759-766},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028599007&partnerID=40&md5=b19c2cff7d94b17790ffe1248be99648},
abstract={The application of graphical visualization to construction can serve a wide variety of objectives and offers many significant benefits. Heavy lifts are an important component of industrial construction. These lifts usually involve the placement or removal of prefabricated plant components. This paper describes a case study of using graphical visualization to assist in planning the replacement of a 145t compressor at an industrial plant. Advanced technologies are used to develop 3D graphical models of the lift site and these models are analyzed to develop a reliable lift plan. The case study was performed using the visualization program Walkthru on a Silicon Graphics workstation. The geometry of the site was obtained from the as-built drawings provided by the plant owner and the equipment specifications were obtained from the lifting contractor. The site model and equipment models were created using the 3D CAD package MicroStation and then translated to a Walkthru format. Basic procedures developed for visualizing heavy lifts are described and these procedures are applied to the compressor replacement project. The paper also presents a discussion on the benefits and costs for this particular case study.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Plank1994686,
author={Plank, James S. and Li, Kai},
title={Performance results of ickp - a consistent checkpointer on the iPSC/860},
journal={Proceedings of the Scalable High-Performance Computing Conference},
year={1994},
pages={686-693},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028584525&partnerID=40&md5=0eac62712cd2742db234b21d41aa89c1},
abstract={This paper presents performance results of ickp, a program that checkpoints applications written on the Intel iPSC/860. ickp is the first implementation of general-purpose checkpointing on a multicomputer. ickp implements three consistent checkpointing algorithms, as well as two optimizations, and recovery. This paper displays the results of testing ickp on many benchmark programs. These benchmarks are useful iPSC/860 programs written by other scientists, and not toy benchmarks used to test the checkpointer. An alpha release of ickp has been made to the Intel community.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Froese199463,
author={Froese, Thomas and Yu, Qiang Kevin},
title={StartPlan: Producing schedule templates using IRMA},
journal={Computing in Civil Engineering (New York)},
year={1994},
number={1},
pages={63-70},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028552090&partnerID=40&md5=a69f72fc296ea95a0e21acfd159c79c6},
abstract={The Information Reference Model for AEC (IRMA) is a generic information modeling standard to support computer-integrated construction (CIC). Although IRMA is still in a conceptual stage, we would like to test some of its basic ideas and capabilities in a realistic application. The StartPlan program will provide such a test-bed. StartPlan is a program that creates `template' construction schedules. Based on a user's description of a construction project, StartPlan will select a construction schedule from a library of `good' schedules for similar projects and will alter it to suit the details of the project at hand. The system will then export this schedule to one of a variety of formats to be used as a `starting template' for creating a new project schedule in traditional scheduling systems. Not only does this save users significant effort in creating a new schedule, but it helps them take advantage of the scheduling expertise of others. Furthermore, it provides a useful test-bed for IRMA since it will use the model both to represent a wide range of project information and to allow the information to be translated between many different types of construction applications.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor1994,
title={Proceedings of the Conference on TRI-Ada 1994},
journal={Proceedings of the Conference on TRI-Ada 1994},
year={1994},
page_count={498},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441788&partnerID=40&md5=2a40cdf540c84965463ee9cf3e554cf6},
abstract={The proceedings contain 48 papers. The topics discussed include: Ada 9x tagged types and their implementation in GNAT; features of the Gnu Ada runtime library; working with Ada 9X classes; quality guidelines = designer metrics; run-time check elimination for Ada 9X; smart recompilation and the GNAT compiler; lessons learned in implementing a team review process; integrating GNAT into GCC; the GNAT compilation model; the GNAT project: a GNU-Ada 9X compiler; an object-oriented approach to software process modeling and definition; software project reporting: management, measurement, and process improvement; the STARS process engine: language and architecture to support process capture and multi-user execution; delegation: dynamic specialization; Ada-based programming language course in Moscow State University; educating educators: lessons in adding Ada to a small school curriculum; profiling in an object-oriented design environment that supports Ada 9x and Ada 83 code generation; selecting a software development process; Easy-Sim: using Ada 9x in a graphics system software architecture; implementing internal program representations with Ada and Ada 9x; and design of GUIs from a programming perspective.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Fogelsong199464,
author={Fogelsong, B.A. and Schoenefeld, D.A.},
title={Graphics class library for plotting (GCLIP), an extensible, reusable graphics framework},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={1994},
volume={Part F129433},
pages={64-69},
doi={10.1145/326619.326664},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039461587&doi=10.1145%2f326619.326664&partnerID=40&md5=57d485a21c3d2c92ff13413dc57da8a8},
abstract={GCLIP is a C++ class library which provides high-level, interactive graphics features under both Turbo C++ Graphics and X Windows/Motif. Its development was motivated by the requirement in technical application software for plotting capabilities-Available at a higher level of abstraction than is normally afforded by in callable graphics libraries. An object oriented software engineering approach resulted in an extensible library of plotting classes which can be reused in C++ client applications. A Motif-based GCLIP Editor provides a visual programming interface by allowing the interactive construction, editing, loading, storing and viewing of plots. The GCLIP architecture includes container service~, customizable plot resources, common protocols, uniform color management, and interactive components. The GCLIP class library is an open framework into which other graphics classes may be readily integrated, complementing the pre-defined set of plot types. © 1994 ACM.},
author_keywords={Design;  Interactive graphics;  Object-oriented analysis;  Programming;  Software reuse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Goldstein1994,
author={Goldstein, T.C. and Sloane, A.D.},
title={The object binary interface - C++ objects for evolvable shared class libraries},
journal={USENIX 6th C++ Technical Conference},
year={1994},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054937643&partnerID=40&md5=4e68d78fe1359728f523d1a62fc7b5d6},
abstract={Object-oriented design and object-oriented languages support the development of independent software components such as class libraries. When using such components, versioning becomes a key issue. While various ad-hoc techniques and coding idioms have been used to provide versioning, all of these techniques have deficiencies - ambiguity, the necessity of recompilation or re-coding, or the loss of binary compatibility of programs. Components from different software vendors are versioned at different times. Maintaining compatibility between versions must be consciously engineered. New technologies such as distributed objects further complicate libraries by requiring multiple implementations of a type simultaneously in a program. This paper describes a new C++ object model called the Shared Object Model for C++ users and a new implementation model called the Object Binary Interface for C++ implementors.These techniques provide a mechanism for allowing multiple implementations of an object in a program. Early analysis of this approach has shown it to have performance broadly comparable to conventional implementations. © USENIX 1994.All right reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{An19941,
author={An, P.E. and Brown, M. and Harris, C.J. and Lawrence, A.J. and Moore, C.G.},
title={Associative memory neural networks: Adaptive modelling theory, software implementations and graphical user interface},
journal={Engineering Applications of Artificial Intelligence},
year={1994},
volume={7},
number={1},
pages={1-21},
doi={10.1016/0952-1976(94)90038-8},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028377961&doi=10.1016%2f0952-1976%2894%2990038-8&partnerID=40&md5=f692c94a446eeaee7aa537bebdaf86d8},
abstract={This paper describes in a unified mathematical framework a class of associative memory neural networks (AMN), that have very fast learning rates, local generalisation, parallel implementation, and guaranteed convergence to the mean squared error, making them appropriate for applications such as intelligent control and on-line modelling of nonlinear dynamical processes. The class of AMN considered include the Albus CMAC, B-spline neural network and classes of fuzzy logic networks. Appropriate instantaneous learning rules are derived and applied to a bench mark nonlinear time series prediction problem. For practical implementation, a network software library and graphical user interface (GUI) is introduced for these networks. The data structure is modular, allowing a natural implementation on a parallel machine. The GUI provides a front end, for high-level procedures, allowing the networks to be designed, trained and analysed within a common environment with a minimum of user effort. The software library is readily integrable into industrial packages such as MATLAB. © 1994.},
author_keywords={Associative memory;  B-spline;  CMAC;  fuzzy logic;  graphical user interface;  neural networks;  non-linear modelling;  parallel implementation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ihlenfeldt1994109,
author={Ihlenfeldt, W.-D. and Takahashi, Y. and Abe, H. and Sasaki, S.},
title={Computation and Management of Chemical Properties in CACTVS: An Extensible Networked Approach toward Modularity and Compatibility},
journal={Journal of Chemical Information and Computer Sciences},
year={1994},
volume={34},
number={1},
pages={109-116},
doi={10.1021/ci00017a013},
note={cited By 136},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028336046&doi=10.1021%2fci00017a013&partnerID=40&md5=5e1fec7f7951ae72bfaafcba4588618d},
abstract={The data model of the CACTVS program suite (Chemical Algorithms Construction, Threading, and Verification System) is presented. CACTVS is an open environment which readily integrates new computational modules and data format descriptions in the domain of computational chemistry. These chemical information (property) computation modules are provided with requested input data, typically transparently computed by other modules. Computational results are managed appropriately with regard to file and data base I/O, transport to display servers, input for high-level routines, and report generation. Definitions of property characteristics which include optionally precompiled objects and source plus additional documentation may be stored in networked data bases for global access. This facilitates module reusal and the exchange of modules between interested parties. The elementary data format handling routines and the property computation routines need not to be part of the core program. They are retrieved from and loaded dynamically at run time from local files or data bases which can be reached on the network. This look-up process can be made completely transparent, resulting in a comparably small core program which grows only on demand to incorporate procedures for the evaluation of requested information. A model for asynchronous distributed client-server computations is supported as an alternative to dynamic linking for expensive calculations. CACTVS is intended to make algorithm development in chemistry more productive and to help in collecting the synergetic benefits from the concerted application of compatible algorithm sets which derive chemical information in all its variety. © 1994, American Chemical Society. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sanders1993674,
author={Sanders, William H. and Obal II, W.Douglas},
title={Dependability evaluation using UltraSAN},
journal={Digest of Papers - International Symposium on Fault-Tolerant Computing},
year={1993},
pages={674-679},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027832981&partnerID=40&md5=ce12d227f7eff7d06646f5799f779761},
abstract={Dependability evaluation is an important, but difficult, aspect of the design of fault-tolerant computer systems. This software demonstration highlights UltraSAN, a stochastic activity network-based package for model-based performance, dependability, and performability evaluation of such systems. The package supports solution by both simulation-based and analytic methods. Of particular importance in the evaluation of dependable systems by analytic means in the control of an often explosive state space growth. UltraSAN has the potential to do this, using recently developed reduced base model construction techniques which exploit symmetries inherent in the SAN structure during construction of the underlying stochastic process representation. Four analytic solvers are available: SOR and LU decomposition for steady-state variables, and uniformization for transparent instant-of-time and interval-of-time variables. For models with characteristics that preclude analytic solution, transient and steady-state simulation can be used. This demonstration illustrates the use of UltraSAN and reduced base model construction by considering the evaluation of a fault-tolerant distributed system. Model generation via an X Windows based graphical interface, stochastic process construction, and model solution, using numerical methods, are illustrated.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pirklbauer1993403,
author={Pirklbauer, K. and Plosch, R. and Weinreich, R.},
title={Libraries and tools for object-oriented distributed automation software},
journal={Proceedings of the IEEE International Conference on Systems, Man and Cybernetics},
year={1993},
volume={4},
pages={403-408},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027816414&partnerID=40&md5=a13025edba8489140b30c68f242c2c4b},
abstract={Object-oriented programming allows the construction of software component libraries and predefined program structures (application frameworks) that significantly improve software quality and reduce software development time. We claim that in addition to such libraries we need tools that both allow the composition of applications using such libraries and offer the possibility to experiment with prototypes of such applications. We present the prototyping tool ProcessBuild, which is based on the application framework ProcessTalk. Both support the implementation of software in the area of process automation, an area that is fairly distinct from the construction of graphic user interfaces, for which such libraries or tools are usually intended.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Diamantaras19932,
author={Diamantaras, K. and Chihoub, A. and Zawadzki, A.},
title={Scalable architectures for image processing},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1993},
volume={2064},
pages={2-13},
doi={10.1117/12.150275},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009636092&doi=10.1117%2f12.150275&partnerID=40&md5=e4bcbaaea7385c15a83cdcab3c84e571},
abstract={Image processing is an area where many applications demand a lot of computational power. In the future such demands are likely to get even higher. Real time processing, the trend towards using larger sizes of images, and the use of color are all factors that will increase the demand for more computing power in the area. In the past the usual approach to resolve the need for more computational power has been to use the latest in high performance machines. Such an approach, however, can be quite expensive. In addition to the cost of acquiring a new machine there is a cost for acquiring/migrating the software to run on such machine. In recent years scalable parallel architectures have emerged as a cost-efficient solution to addressing the problem of ever-increasing computational demands. Such architectures provide the flexibility of increasing the performance while preserving the substantial investment in the software and hardware of a given machine. In this paper we propose to present the results of our investigation of the scalability of a selected number of low-level vision operations on three topologies: (a) 1-D array with wrap-around (ring) (b) 2-D array with wrap-around (torus), and (c) the hypercube. In particular we will present the scalability results of the following algorithms: convolution, mathematical morphology, Fourier transform, DCT, histogram, image translation, rotation, zooming, edge-enhancement operators (e.g. Canny, Sobel, Kirsch, Nevatia-Babu), median filtering, point-wise operations (e.g. thresholding, dithering) on the three topologies. In addition, we will present preliminary results of the design of a scalable parallel processor that performs optimally (performance increases linearly or close to linear in terms of the number of processing elements in the machine) on these operators. Our study consists of three phases: mapping the algorithms on the three topologies, simulating the execution of these algorithms, and design of the array. Our mapping follows the standard methodology ∗2 proposed for the design of systolic arrays, since our application domain is very specific and the selected algorithms very regular. After the mapping is done we simulate the algorithms using the SES/workbench simulation package which allows us to collect statistics on the execution time and efficiency of our mappings and evaluate the performance of the three topologies in our application domain using different array and problem sizes. For each algorithm and topology the range of scalability is determined as a function of image size. In the design phase we propose an SIMD array with 2-D torus interconnection topology as a cost-efficient solution to the scalable implementation of the selected algorithms. Considerations entering the design phase are performance as determined by simulations, cost of implementation, and ease of scaling the machine size. © 1993 SPIE. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gropp199387,
author={Gropp, W. and Smith, B.},
title={Scalable, extensible, and portable numerical libraries},
journal={Proceedings of Scalable Parallel Libraries Conference, SPLC 1993},
year={1993},
pages={87-93},
doi={10.1109/SPLC.1993.365579},
art_number={365579},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041714395&doi=10.1109%2fSPLC.1993.365579&partnerID=40&md5=e2c810c5c2e6f5f5084e94e124c65802},
abstract={Designing a scalable and portable numerical library requires consideration of many factors, including choice of parallel communication technology, data structures, and user interfaces. The PETSc library (Portable Extensible Tools for Scientific computing) makes use of modern software technology to provide a flexible and portable implementation. This paper discusses the use of a meta-communication layer (allowing the user to choose different transport layers such as MPI, p4, pvm, or vendor-specific libraries) for portability, an aggressive data-structure-neutral implementation that minimizes dependence on particular data structures (even vectors), permitting the library to adapt to the user rather than the other way around, and the separation of implementation language from user-interface language. Examples are presented. © 1994 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Libes1993167,
author={Libes, D.},
title={The nist express toolkit - design and implementation},
journal={ASME 1993 International Computers in Engineering Conference and Exposition, CIE 1993},
year={1993},
volume={Part F167954-1},
pages={167-180},
doi={10.1115/EDM1993-0108},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-13444260297&doi=10.1115%2fEDM1993-0108&partnerID=40&md5=a896b56eb7a7b4d7ff533367adb52350},
abstract={The NIST EXPRESS toolkit is a software library for building EXPRESS-related tools. EXPRESS is an ISO language for describing information models. EXPRESS descriptions are neutral to different data storage paradigms and systems on different hardware platforms and networks. This paper describes the design and implementation of the toolkit including its important interfaces, data structures, and algorithms. This paper is recommended for anyone wishing to modify the toolkit or anyone wishing to build their own EXPRESS implementation. The reader is assumed to be familiar with the EXPRESS language, the basics of traditional language implementations, and C - the language with which the toolkit is implemented. As a testbed against which to benchmark the evolving EXPRESS language, conformance to the standard (currently Draft International Standard) is the highest priority in the toolkit. Nonetheless, time/space efficiency, accurate and helpful diagnostics, and ease-of-use are also critical to the success of the toolkit. The paper describes how these concerns are addressed even though EXPRESS is a complex and sophisticated language. The toolkit is available from the National Institute of Standards and Technology. The toolkit is just one of a number of tools for data management in STEP, a family of ISO standards currently in development. All of the NIST tools, including the NIST EXPRESS toolkit, are in the public domain. © 1993 ASME 1993 International Computers in Engineering Conference and Exposition, CIE 1993. All rights reserved.},
author_keywords={compiler;  EXPRESS;  implementation;  National PDES Testbed;  PDES;  STEP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Grimm199361,
author={Grimm, F. and Bunke, H.},
title={An expert system for the selection and application of image processing subroutines},
journal={Expert Systems},
year={1993},
volume={10},
number={2},
pages={61-74},
doi={10.1111/j.1468-0394.1993.tb00303.x},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027594178&doi=10.1111%2fj.1468-0394.1993.tb00303.x&partnerID=40&md5=22969087434efa1da6af013069622e18},
abstract={Abstract: Expert systems are useful tools for the efficient application of image processing software packages. This paper describes an image processing expert system based on the SPIDER package (Tamura et al. 1983). The system supports inexperienced users who wish to solve image processing problems by means of SPIDER subroutines. We give a detailed description of the concepts underlying the construction of the system and present the main components of the knowledge base. The modular structure of the knowledge base can be considered as a generalised approach to software configuration expert systems which can easily be adapted to other image processing software packages and other problem domains. Copyright © 1993, Wiley Blackwell. All rights reserved},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reddy1993146,
author={Reddy, R.R. and Gupta, A. and Singh, R.P.},
title={Expert system for optimum design of concrete structures},
journal={Journal of Computing in Civil Engineering},
year={1993},
volume={7},
number={2},
pages={146-161},
doi={10.1061/(ASCE)0887-3801(1993)7:2(146)},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027575294&doi=10.1061%2f%28ASCE%290887-3801%281993%297%3a2%28146%29&partnerID=40&md5=79a797cb72868bb585e78575db9ebb60},
abstract={Optimization of concrete structures at the conceptual design stage has been based so far on informal methods. The designer would use his expertise, which is often heuristic, to find optimum solutions. The optimization at the design realization stage is usually achieved by making use of numerical techniques. The minimum-weight approach used in steel structures is not valid for concrete structures because of different costs for steel, concrete, form work, and work force. In this paper, a formal method is proposed for the cost optimization of reinforced-concrete structures. A cost function has been derived to estimate the optimum sizes of members at the conceptual design stage. An expert system, called EXFORM, was developed for the cost-optimum design of reinforced-concrete members. EXFORM makes use of cost function along with heuristic knowledge of the designer. The scope of the work is extended to the design of columns, beams, and slabs. © ASCE.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Frankel1992220,
author={Frankel, M.},
title={Analysis / architecture models to ASG models: Enabling the transition},
journal={Proceedings of the Conference on TRI-Ada 1992},
year={1992},
pages={220-231},
doi={10.1145/143557.143722},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032582215&doi=10.1145%2f143557.143722&partnerID=40&md5=aa440fa34dff04af66f1350c2d806e05},
abstract={Once again, we are thrown into the model quagmire, desperately trying to understand what models the methods expect of us and how to effectively build them. As our understanding of a particular model matures in terms of pattern recognition on their construction and evaluation, we start to look for methods for transitioning between models during software development. The maturation of the Practical Ada Design Method (PAdaDM) has led to a level of pattern recognition that allows us to make a methodical transition from an Architecture Data Flow Diagram (DFD) Model to a first cut Ada Structure Graph (ASG) Model. The transition approach involves five major steps: 1) Identification of potential concurrent activities, (and then, for each activity:), 2) Conversion of processing steps (bubbles and Pspecs from DFD) to first cut package choices, based on PAdaDM package taxonomy, 3) Allocation of internal (when to execute) timing requirements to tasks, 4) Allocation of system control processing (Cspecs from DFD) to new or existing packages, and 5) Refinement of ASG model based on further breakdown of detailed design and application of structured Ada design guidelines. Making our way through the jungle of new models and new transition paths is a lot easier when we have the benefit of a tour guide to lead the way. Book your trip today! © 1992 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dolinskij199258,
author={Dolinskij, M.S. and Chernykh, G.V. and Komisarenko, S.A.},
title={On one approach to the construction of architecture-independent program model},
journal={Avtomatika i Vychislitel'naya Tekhnika},
year={1992},
number={4},
pages={58-63},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026884835&partnerID=40&md5=9feca179537593d7b0c46079463bca09},
abstract={Problem of modelling the computer structure functioning in an operating mode is considered. Method of architecture-independent models (AI-models) construction for program execution is suggested. AI-model reflects program properties that are defined by its program algorithm and don't depend on the concrete computer architecture. The software package MARIN is described which implements realization of the method for constructing AI-models for the assembler-language programs. Main properties of AI-models, requirements to them and principles of their construction are described.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ostertag1992205,
author={Ostertag, E. and Hendler, J. and Díaz, R.P. and Braun, C.},
title={Computing Similarity in a Reuse Library System: An AI-Based Approach},
journal={ACM Transactions on Software Engineering and Methodology (TOSEM)},
year={1992},
volume={1},
number={3},
pages={205-228},
doi={10.1145/131736.131739},
note={cited By 113},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026884536&doi=10.1145%2f131736.131739&partnerID=40&md5=382d14138ff913368041355633ef7cff},
abstract={This paper presents an AI based library system for software reuse, called AIRS, that allows a developer to browse a software library in search of components that best meet some stated requirement. A component is described by a set of 1992 pairs. A feature represents a classification criterion, and is defined by a set of related terms. The system allows to represent packages (logical units that group a set of components) which are also described in terms of features. Candidate reuse components and packages are selected from the library based on the degree of similarity between their descriptions and a given target description. Similarity is quantified by a nonnegative magnitude (distance) proportional to the effort required to obtain the target given a candidate. Distances are computed by comparator functions based on the subsumption, closeness, and package relations. We present a formalization of the concepts on which the AIRS system is based. The functionality of a prototype implementation of the AIRS system is illustrated by application to two different software libraries: a set of Ada packages for data structure manipulation, and a set of C components for use in Command, Control, and Information Systems. Finally, we discuss some of the ideas we are currently exploring to automate the construction of AIRS classification libraries. © 1992, ACM. All rights reserved.},
author_keywords={facet classification;  similarity-based retrieval},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Todd1992267,
author={Todd, P.H.},
title={A constructive variational geometry based mechanism design software package},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={1992},
volume={Part F168016-3},
pages={267-273},
doi={10.1115/detc1992-0306},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104206617&doi=10.1115%2fdetc1992-0306&partnerID=40&md5=97ecde9890a8c23e75331569c431febd},
abstract={This paper describes the design and implementation of a variational geometry based mechanism design and analysis software package. A new. constructive approach to variational geometry allows positions and their derivatives to be computed using closed form expressions rather than an iterative procedure. This facilitates an interactive kinematics model. Statics and dynamics capabilities are added by means of the principle of least work and d'Alambert's principle. Examples are presented which highlight the flexibility and ease of use of the system. © 1992 American Society of Mechanical Engineers (ASME). All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Camarinha-Matos1992245,
author={Camarinha-Matos, L.M. and Pinheiro-Pita, H.I.},
title={Intelligent C.A.S.E. for CIM},
journal={IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
year={1992},
volume={1992-August},
pages={245-250},
doi={10.1109/ETFA.1992.683260},
art_number={683260},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067685411&doi=10.1109%2fETFA.1992.683260&partnerID=40&md5=ecefd71b932bc5557438b8bb7d7d7dc0},
abstract={After a brief characterization of the current scenario in CIM developments regarding integration aspects, the need for tools supporting the design of new manufacturing systems, based on existing components, is introduced.The concept of Electronic Catalog of CIM tools is presented as an open library from which subsets, dedicated to particular application domains and fulfilling particular functional requirements, can be selected and integrated. The concept of CIM-CASE is introduced as a system that will support a systems' designer in deriving particular architectures out of a reference model represented by toolbox's metaknowledge base. Implementation aspects of current prototype system are discussed. The design process for a new architecture follows a hierarchical planning paradigm using an IDEFO-like representation for functional modeling. Semantic attachment is done based on a metaknowledge base containing knowledge about tools, CIM activities, application domains, reference information concepts and configuration knowledge. Selection of tools is based on the specified requirements and is driven by a multicriteria reasoning process. Additionally, the derivation of information concepts to feed the information system of the new architecture, as well as the generation of its controller is discussed. This work has been developed in the context of CTM-PLATO, an ESPRIT project of the European Community, involving 14 partners (universities and companies) from 7 countries, © 1992 IEEE.},
author_keywords={Computer aided software engineering;  Computer integrated manufacturing;  Interactive planning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gall199279,
author={Gall, H. and Klösch, R.},
title={Reuse engineering: Software construction from reusable components},
journal={Proceedings - International Computer Software and Applications Conference},
year={1992},
pages={79-86},
doi={10.1109/CMPSAC.1992.217600},
art_number={217600},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031737785&doi=10.1109%2fCMPSAC.1992.217600&partnerID=40&md5=acaf006d2fcae5fe55c55b51704ee751},
abstract={Much effort has been invested in research on software reuse so far, but most of this work was focused on some areas of this problem and important solutions were assumed. This paper describes a comprehensive approach that supports reuse in the software development process, especially the production of software from reusable components. For this purpose well known strategies like reverse engineering and design recovery are combined with a generalization mechanism to develop the "reuse engineering" process. This process includes both the phase of isolating reusable modules from existing software (module extraction) and the phase of retrieving such modules from a software components library and combining them to new software (module interconnection). The paper leads to the definition of a "reuse engineering life-cycle", which integrates reuse into the conventional software life-cycle. © 1992 IEEE.},
author_keywords={Forward engineering;  Module extraction;  Module interconnection;  Reusability;  Reusable components;  Reuse engineering;  Reuse engineering lifecycle;  Reverse engineering;  Software components library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bourdeau1992239,
author={Bourdeau, R.H. and Cheng, B.H.C.},
title={An object-oriented toolkit for constructing specification editors},
journal={Proceedings - International Computer Software and Applications Conference},
year={1992},
pages={239-244},
doi={10.1109/CMPSAC.1992.217561},
art_number={217561},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029528745&doi=10.1109%2fCMPSAC.1992.217561&partnerID=40&md5=bc779f1c29b0d16f8623a6d17e053214},
abstract={Formal software development techniques facilitate the design and implementation of more reliable computer systems. In particular, formal specification languages provide a means for precisely characterizing the behavior of a computer system and its components, and, facilitate the determination of correct implementation using automated reasoning techniques. While formal specifications can be created using a word processor or typesetter, the process is neither easy nor suitable for large scale software specification. Tools supporting the application of formal methods are needed to make these activities easier, and thus more practical to use. This paper discusses Spectacle, an object-oriented library of software components designed for constructing formal specification editing tools; prototype specification editors built from this library are presented. © 1992 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Prat1992184,
author={Prat, A. and Catot, J.M. and Lores, J. and Fletcher, P. and Galmes, J. and Sanjeevan, K.},
title={A separable architecture for the construction of knowledge based front ends},
journal={AI Communications},
year={1992},
volume={5},
number={4},
pages={184-190},
doi={10.3233/AIC-1992-5402},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974753140&doi=10.3233%2fAIC-1992-5402&partnerID=40&md5=c22dbfce14edb2e7a9f0440bb42195f6},
abstract={This paper describes the principles and motives underlying the work of ESPRIT II project 2620 -FOCUS (Front Ends for Open and Closed User Systems) which began in December 1988 and is expected to end after four years in December 1992. In this paper, we state the aim of the FOCUS project, explain the strands of investigation and describe the core element that has emerged from the research into design strategy-a separable, distributed architecture for Knowledge-Based Front Ends (KBFE’s). The objective of FOCUS was to develop the tools and techniques that would aid the construction of KBFE’s. These in turn would provide users of application software packages and libraries with enhanced interfaces and improved levels of co-operative assistance. The tools and techniques have been integrated into a general FOCUS architecture that has been used to develop several prototype applications. © 1992 IOS Press.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Songer1992456,
author={Songer, A.D. and Ibbs, C.W. and Garrett, J.H. and Napier, T.R. and Stumpf, A.L.},
title={Knowledge-based advisory system for public-sector design-build},
journal={Journal of Computing in Civil Engineering},
year={1992},
volume={6},
number={4},
pages={456-471},
doi={10.1061/(ASCE)0887-3801(1992)6:4(456)},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026930220&doi=10.1061%2f%28ASCE%290887-3801%281992%296%3a4%28456%29&partnerID=40&md5=e58d682844fa1f000746d9f6628ff14d},
abstract={Design-build (DB) contracting is becoming an increasingly viable alternative to traditional contracting techniques in the public sector. However, many project managers do not have the experience to use this style of contracting effectively. A knowledge-based expert system called DB ADVISOR has been developed by the writers to assist project managers in designing and administering DB contracts. Assistance provided by DB ADVISOR includes: (1) Selecting projects appropriate for design-build; (2) preparing the request for proposal; and (3) evaluating the proposal. Though the scope of the DB ADVISOR is primarily design-build contracting for the U.S. Army Corps of Engineers Military Construction Army (MCA) program, we have generalized the system for other users. MCA includes acquisition of administrative, training, and maintenance facilities. The primary research issues of DB ADVISOR were identifying, defining, and synthesizing the information necessary for selecting and administering design-build projects, and providing it to a project planner in a flexible, user-friendly manner. Additionally. the research investigated ways to provide user control in a highly integrated process continuum involving many decision makers and their decisions. DB ADVISOR blends hypertext-like interactivity with knowledge-based technology. © ASCE.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rao199143,
author={Rao, M. and Luxhoj, J.T.},
title={Integration framework for intelligent manufacturing processes},
journal={Journal of Intelligent Manufacturing},
year={1991},
volume={2},
number={1},
pages={43-52},
doi={10.1007/BF01471335},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249919982&doi=10.1007%2fBF01471335&partnerID=40&md5=712f62d5ea0191ddd9417922478fe497},
abstract={In this article, we describe a new approach to applying distributed artificial intelligence techniques to manufacturing processes. The construction of intelligent systems is one of the most important techniques among artificial intelligence research. Our goal is to develop an integrated intelligent system for real time manufacturing processes. An integrated intelligent system is a large knowledge integration environment that consists of several symbolic reasoning systems (expert systems) and numerical computation packages. These software programs are controlled by a meta-system which manages the selection, operation and communication of these programs. A meta-system can be implemented in different language environments and applied to many disciplines. This new architecture can serve as a universal configuration to develop high performance intelligent systems for many complicated industrial applications in real world domains. © 1991 Chapman and Hall Ltd.},
author_keywords={Distributed artificial intelligence;  integration;  meta-system},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gowripalan19911225,
author={Gowripalan, S. and Shakil, J. and Singh, G.},
title={Life-cycle studies of a concrete structure using simulation on a microcomputer},
journal={Computers and Structures},
year={1991},
volume={41},
number={6},
pages={1225-1230},
doi={10.1016/0045-7949(91)90258-N},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026383276&doi=10.1016%2f0045-7949%2891%2990258-N&partnerID=40&md5=9dec482ee814f31c321265fb5803dd46},
abstract={This paper describes a means of exploiting the combination of microcomputers and Monte Carlo simulation to perform a systematic life-expectancy and financial analysis. The use of the program Venturer, is illustrated using an example of a footbridge. The service life of the structure is modelled using the times taken for carbonation and visible corrosion. The possible prolongation of the life expectancy due to the application of a coating is considered. The results of sensitivity analyses which indicate the relative importance of the factors considered are also shown. © 1991.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adeli1991773,
author={Adeli, H. and Hawkins, D.W.},
title={A hierarchical expert system for design of floors in highrise buildings},
journal={Computers and Structures},
year={1991},
volume={41},
number={4},
pages={773-788},
doi={10.1016/0045-7949(91)90187-Q},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026369206&doi=10.1016%2f0045-7949%2891%2990187-Q&partnerID=40&md5=33e941fef39ef626077efab59455a311},
abstract={A prototype knowledge-based expert system, called COFDEX (for COmposite Floor Design EXpert), has been developed for integrated design of composite floors in multistory buildings in accordance with the 1986 American Institute of Steel Construction (AISC) Load and Resistance Factor Design (LRFD) Specification. Design options include design for practical minimum cost or for minimum depth. Developed in the expert system programming environment GURU, COFDEX is a coupled system with a hierarchical structure in which procedural modules interact with rule sets and data bases. The expert system COFDEX demonstrates how AI technology complements the traditional numerical processing in automating the complicated process of engineering design and developing highly interactive software packages. © 1991.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chau1991789,
author={Chau, K.W. and Lee, S.T.},
title={Computer-aided design package RCTANK for the analysis and design of reinforced concrete tanks},
journal={Computers and Structures},
year={1991},
volume={41},
number={4},
pages={789-799},
doi={10.1016/0045-7949(91)90188-R},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026367040&doi=10.1016%2f0045-7949%2891%2990188-R&partnerID=40&md5=75569da7c0f62d59b42f69d1e4df84af},
abstract={The design of reinforced concrete liquid retaining structures involves many tedious calculations and/or numerous design charts. Frequently some careless human errors, which are fatal to the design, may be introduced in calculations or in interpolations of design charts. In view of this a computer-aided design package would appear to be necessary. This paper presents the development and verification of a computer program RCTANK for computer-aided design of medium-sized liquid retaining tanks. It is a structural engineering problem capable of performing analysis of the wall, the roof slab and the base slab of the tank, and, according to the analysis results, also giving recommendations about suitable concrete thicknesses and reinforcements entailed for different structural elements. Two most commonly encountered shapes, namely, rectangular and circular configurations, for liquid retaining tanks are catered for. These different shapes, together with the two different tank locations, i.e. locating at roof or below ground surface, constitute four different combinations to be considered in the computer program. Due to the numerous advantages provided by Turbo Pascal version 5.0, it is adopted as the programming language. © 1991.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ishikawa1991347,
author={Ishikawa, M.},
title={Thermal stress analysis of a concrete dam},
journal={Computers and Structures},
year={1991},
volume={40},
number={2},
pages={347-352},
doi={10.1016/0045-7949(91)90360-X},
note={cited By 53},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025791193&doi=10.1016%2f0045-7949%2891%2990360-X&partnerID=40&md5=892619e6724da8bda02858b911ef3278},
abstract={An example will be shown that ADINA was applied to thermal stress analysis of a concrete dam. The characteristics of thermal stress analysis of a mass concrete structures that the finite elements should be added according to the casting schedule of concrete and the elastic modulus of concrete should be increased with time. The user's routine was developed to calculate thermal stress in concrete structures. As an example, temperature and stress distributions were simulated in a concrete dam which was constructed by our company. © 1991.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chandansing19911213,
author={Chandansing, R.A. and Sliedrecht, H. and de Bruijn, W.A. and Vos, Ch.J.},
title={Computer assisted structural concrete education},
journal={Computers and Structures},
year={1991},
volume={40},
number={5},
pages={1213-1221},
doi={10.1016/0045-7949(91)90392-Y},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025724353&doi=10.1016%2f0045-7949%2891%2990392-Y&partnerID=40&md5=90d85d4b8eb5b58525c5fa38b1da3f35},
abstract={A CAD system, called the CAD Structural Concrete Exercise, has been developed by the Concrete Structures section of Delft University of Technology to improve the learning effect of a structural concrete design exercise. AUTOCAD was selected as the basic software. Much work has been carried out to tailor AUTOCAD for this specific exercise, by writing subroutines in AUTOLISP and PASCAL. The main feature of the CAD system is the ability to check the design, the accessory calculations and the drawing by using information which is gathered by the system during the drawing activities of the students. Another feature is the possibility of performing parameter studies to investigate the influence of the different parameters on the design. The CAD system, of which development started in 1987, has recently been used for the first time by 187 students. It was evaluated as quite successful. Apart from educational purposes, the CAD system has the potency to be used as a tool for research activities on CAD applications in the concrete construction industry. The CAD system is running on Apollo DN 3000 workstations under AEGIS. A demonstration version is available for PCs running under MS-DOS. © 1991.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Couvillion1991290,
author={Couvillion, J. and Freire, R. and Johnson, R. and Obal, W.D. and Qureshi, M.A. and Rai, M. and Sanders, W.H. and Tvedt, J.E.},
title={Performability modeling with UltraSAN},
journal={Proceedings of the 4th International Workshop on Petri Nets and Performance Models, PNPM 1991},
year={1991},
pages={290-299},
doi={10.1109/PNPM.1991.238791},
art_number={238791},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0005403153&doi=10.1109%2fPNPM.1991.238791&partnerID=40&md5=1e3bbbd26b3ae6033495a779c1909c9c},
abstract={Stochastic extensions to Petri nets have received growing attention during the past decade as a model for evaluating the performance, dependability, and performability of computer hardware, software, and networks. Their formal structure permits solution by analytic means in many cases. When this is not possible, they an facilitate the automatic generation of a simulation program to estimate system behavior. The paper describes an X-window based software tool for evaluating systems that are represented as stochastic activity networks, a variant of stochastic Petri nets. The tool, known as UltraSAN, incorporates the results of recent research to significantly reduce the size of the state space that is considered for analytic solution, as well as the number of event types that are considered in simulation. Throughout the paper, a simple local area network model is used to illustrate the concepts, user interface, and model construction and solution methods implemented in the package. © 1991 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Prat1990225,
author={Prat, A. and Lores, J. and Fletcher, P. and Catot, J.M.},
title={Back-end manager: an interface between a knowledge-based front end and its application subsystems},
journal={Knowledge-Based Systems},
year={1990},
volume={3},
number={4},
pages={225-229},
doi={10.1016/0950-7051(90)90100-V},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249018199&doi=10.1016%2f0950-7051%2890%2990100-V&partnerID=40&md5=2f60e18a3ae5f380fcee4a8c53764bff},
abstract={Front Ends for Open and Closed User Systems (FOCUS) is an ESPRIT/2 (no. 2620) project aimed at designing tools and techniques for the construction of knowledge-based front ends (KBFEs) for open-user systems (reusable software components, libraries, etc) and closed-user systems (free-standing software, packages, etc). An important part of the project involves the establishment of an architecture for KBFEs and the specification of the KBFE/back-end interface. This paper describes the properties and related issues of such an interface, known as the back-end manager (BEM), and its relationship to the proposed KBFE architecture. © 1990.},
author_keywords={back-end manager;  interface Separability;  knowledge-based front end;  user interface},
document_type={Article},
source={Scopus},
}

@ARTICLE{Brenner199071,
author={Brenner, B.R. and Hawkes, M.},
title={The automated underground: computer-aided design of Boston's Central Artery project},
journal={Microcomputers in Civil Engineering},
year={1990},
volume={5},
number={1},
pages={71-81},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025573744&partnerID=40&md5=07e2c4621818bf8f6fe56d267e146b07},
abstract={Computer applications related to underground construction are described as they are being applied to Boston's Central Artery/Third Harbor Tunnel Project. The project is currently one of the largest highway projects in the country. Computer applications include a geotechnical boring log management system, spreadsheets for tunnel analysis and design, finite-element studies of soil-structure interaction for a construction excavation problem, and software that draws flow nets. Each application is outlined, and problems and future improvements to the methods are discussed. -Authors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bomans1990119,
author={Bomans, L. and Roose, D. and Hempel, R.},
title={The Argonne/GMD macros in FORTRAN for portable parallel programming and their implementation on the Intel iPSC/2},
journal={Parallel Computing},
year={1990},
volume={15},
number={1-3},
pages={119-132},
doi={10.1016/0167-8191(90)90036-9},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025491004&doi=10.1016%2f0167-8191%2890%2990036-9&partnerID=40&md5=1c6018df96d4ee7a13829e5951099571},
abstract={A macro package for expressing message passing functions within parallel FORTRAN program is presented. It makes the user program fully portable among all parallel computers where the macros are implemented. The implementation on the Intel iPSC/2 hypercube is discussed in more detail. New message passing primitives have been added to the iPSC/2 operating system, offering the user a broader functionality at no efficiency loss. The full macro set, using these primitives, works with the same performance as the original Intel primitives. © 1990.},
author_keywords={Efficiency;  FORTRAN macro package;  Implementation features;  Intel iPSC/2 hypercube;  Parallel computers;  Portability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Touffait1990171,
author={Touffait, Y. and Beucher, H. and Guerillot, D.},
title={3D integrated structure for computer-aided reservoir characterization},
year={1990},
pages={171-180},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025065535&partnerID=40&md5=47fcc87594e4be8443183d2474071822},
abstract={This article describes a software which gives 3D images of this architecture using conditional geostatistical simulations and provides tools to study the influence of these lithological descriptions on fluid flow behavior. This software establishes the link between wells data and fluid flow reservoir simulators. Each step of this interactive and graphic system is explained here: (1) geological and geostatistical analysis to characterize a suitable model for the following steps, (2) conditional geostatistical simulations giving a lithological description of the internal heterogeneities on high resolution grids, (3) quantification of the reservoir assigning petrophysical values to each lithofacies grid cell (porosity and absolute permeability tensor), (4) scaling up methods computing in a cost effective way the averaged petrophysical values associated to reservoir simulator grid cells.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Umphress1989541,
author={Umphress, D.A. and Pooch, U.W. and Tanik, M.},
title={Simulation: Fast prototyping of a goal-oriented simulation environment system},
journal={Computer Journal},
year={1989},
volume={32},
number={6},
pages={541-548},
doi={10.1093/comjnl/32.6.541},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024917587&doi=10.1093%2fcomjnl%2f32.6.541&partnerID=40&md5=59fefe1538ff1250dff0dbd8c2711532},
abstract={The goal-oriented Simulation Environment Systems (SES) architecture 'humanizes' the problem solving process by providing a more natural scheme of model construction and experimentation over traditional simulation languages. SES is a collection of integrated tools that allows users to focus on problem solving rather than on the peripheral activities of programming.Interactive software plays a vital role in reducing the burden on the user in describing the various information types. It prompts for information regarding identification of controllable parameters, generation of the goal scenario, and definition of performance criteria. Efforts are made to supply the user with as much information as is currently defined in the model base when eliciting responses. Further, the SES model specification language is specifically designed to support a library of model parts. Such a library serves as a corporate memory of past simulation studies and contains information on component behaviours, transaction sequences, and analysis rules. © 1989 The British Computer Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{CraigHoward1989458,
author={Craig Howard, H.},
title={Prototype integrated environment for AEC data},
journal={Computing in Civil Engineering (New York)},
year={1989},
pages={458-465},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024907948&partnerID=40&md5=702caa52534ee37ddcd7c3f5b1ef3efe},
abstract={To address the problems of effective, automated data exchange in the architecture-engineering-construction (AEC) industry, we are developing a loosely-coupled prototype environment integrating databases, expert systems, and algorithmic applications on a heterogeneous hardware and software base. Our prototype interface (KADBASE) dynamically processes requests from engineering applications to multiple data sources. We have implemented KADBASE on a network of workstations and minicomputers. The paper describes two specific extensions of the KADBASE architecture that are currently in progress.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cooper198956,
author={Cooper, Richard},
title={Persistent languages facilitate the implementation of software version management},
journal={Proceedings of the Hawaii International Conference on System Science},
year={1989},
volume={2},
pages={56-65},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024866790&partnerID=40&md5=c6b97afeaab59be496afa071c3ebec0a},
abstract={The use of persistent programming languages to construct tools for maintaining the complex modular structure of large-scale application programs transforms the problem into a database maintenance problem. The application construction environment can be considered to be a database of software libraries, and standard database techniques can be used to handle the modules. The persistent language PS-Algol provides a good basis for manipulating such libraries, as it has the appropriate graphics facilities, first-class procedures, delayed binding, delayed type-checking, strong type-checking and a compiler callable at run-time. It is shown how these features help by concentrating on two issues: module version management and the control over the binding together of two modules. Some simple facilities for managing a software database are outlined, and implementation techniques equally applicable to more sophisticated systems are described.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hendrickson1989145,
author={Hendrickson, J.B. and Huang, P.},
title={Multiple Constructions in Synthesis Design},
journal={Journal of Chemical Information and Computer Sciences},
year={1989},
volume={29},
number={3},
pages={145-151},
doi={10.1021/ci00063a002},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024715742&doi=10.1021%2fci00063a002&partnerID=40&md5=dc48391de40981732b870e3d8b45dc87},
abstract={Multiple construction reactions that may take place in one laboratory operation are argued to be important keys to rapid creation of the target skeleton in synthesis design. The several paths for such constructions, forming two to four skeletal bonds, are logically formulated. Two of the major classes, double affixation and multiple cyclization, are then articulated as computer programs to find all skeletal variants for a given target and then to apply appropriate reactive functionality to the generated starting skeletons to produce viable routes to the target. © 1989, American Chemical Society. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Roufaiel1989127,
author={Roufaiel, M.S.L. and Monasa, F.F.},
title={Microcomputer-aided analysis and design of steel frames},
journal={Journal of Computing in Civil Engineering},
year={1989},
volume={3},
number={2},
pages={127-142},
doi={10.1061/(ASCE)0887-3801(1989)3:2(127)},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024575776&doi=10.1061%2f%28ASCE%290887-3801%281989%293%3a2%28127%29&partnerID=40&md5=6b80c2ea308a6f7ce7b22df3e41551f5},
abstract={A microcomputer software package, FRAME-AD Version 3.00, has been developed for the analysis and design of moment-resisting steel frames. The main feature of the software is the use of interactive computer graphics. This feature facilitates the construction of the structural model, application of loading conditions, and displaying the results in a clear and concise manner. In addition, the software is menu driven, user friendly, and does not require previous experience with computer operating systems. The analysis is carried out using the stiffness method, and the design can be performed using either the “Load and Resistance Factor Design” or the “Allowable Stress Design” Specifications of the American Institute of Steel Construction. The software is developed for IBM or compatible personal computers, and is written in BASIC language using Microsoft Quick-BASIC 4 compiler. © ASCE.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Umphress1988120,
author={Umphress, D.A. and Pooch, U.W. and Tanik, M.},
title={Fast prototyping of a goal-oriented simulation environment system},
journal={Proceedings of the 1988 ACM 16th Annual Conference on Computer Science, CSC 1988},
year={1988},
pages={120-130},
doi={10.1145/322609.322625},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028829704&doi=10.1145%2f322609.322625&partnerID=40&md5=999c1a99ece646fc8362e889610c91fb},
abstract={The goal-oriented Simulation Environment Systems (SES) architecture "humanizes" the problem solving process by providing a more natural scheme of model construction and experimentation over traditional simulation languages. SES is a collection of integrated tools that allows users to focus on problem solving rather than on the peripheral activities of programming. Interactive software plays a vital role in reducing the burden on the user in describing the various information types. It prompts for information regarding identification of controllable parameters, generation of the goal scenario, and definition of performance criteria. Efforts are made to supply the user with as much information as is currently defined in the model base when eliciting responses. Further, the SES model specification language is specifically designed to support a library of model parts. Such a library serves as a corporate memory of past simulation studies and contains information on component behaviors, transaction sequences, and analysis rules.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Moon1988265,
author={Moon, C.},
title={Computerized personal information systems for research scientists},
journal={International Journal of Information Management},
year={1988},
volume={8},
number={4},
pages={265-273},
doi={10.1016/0268-4012(88)90034-5},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-45449121888&doi=10.1016%2f0268-4012%2888%2990034-5&partnerID=40&md5=86eff6d360ab776ec907424120d0ac26},
abstract={A personal information system is defined as a system for supporting the acquisition, storage and retrieval of information by individuals. For research scientists, such a system is centred around the storage and retrieval of bibliographic references. A survey of the literature, not only of computerized personal indexes, but also of the information needs of research, and the use of information by scientists, provides guidelines for the design of computerized personal information systems. It is concluded that the personal information system should not only help scientists to manage their document collections, but should enable them to represent the structure of the literature by creating and storing links between document records. A computerized personal information system should also be integrated with other personal software, such as electronic mail and word-processing packages. © 1988.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Birdwell1987468,
author={Birdwell, J.D. and Cockett, J.R.B.},
title={Experiences in the design and use of an expert system for computer-aided control system design},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1987},
volume={786},
pages={468-473},
doi={10.1117/12.940658},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958489152&doi=10.1117%2f12.940658&partnerID=40&md5=2187c203f67c244f74d2bd981ee5bbfc},
abstract={An expert system, the Computer-Aided Systems and Control Analysis and Design Environment (CASCADE) lias been implemented for the design of multivariable controllers for linear plants, using the linear quadratic Gaussian/loop transfer recovery (LQG/LTR) design methodology [1, 2, 3]. The expert system was implemented using a novel shell, DECIDE [4, 5], which uses a decision expression methodology and incorporates explanation and backtracking, and linkage with the numeric and graphics software necessary for control system analysis and design. CASCADE was evaluated by generating trial designs for several realistic applications in the following areas: High voltage DC power transmission, robot manipulator path tracking, large structure active stiffening, aircraft stability augmentation, wind power cogeneration, bioreactor control, and steam power plant control. Since these designs were performed by graduate students, this provided an evaluation of the CASCADE implementation as both an expert system-based design package and an aid in education. We found that not only was the expert system useful for computer-aided design and education; it also served to demonstrate several subtle fallacies in the theoretical design methodology. Our surmise is that these fallacies had remained undiscovered because of the significant investment required in the mechanics of the design process in previous work. The expert system allowed attention to be focused on a codification of design knowledge rather than on the algorithmic details. © 1987, SPIE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Frazier1987300,
author={Frazier, J.M. and Cannon Jr., N.P.},
title={Practical guidelines for the design of menus},
journal={Computers and Industrial Engineering},
year={1987},
volume={13},
number={1-4},
pages={300-303},
doi={10.1016/0360-8352(87)90101-X},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023210707&doi=10.1016%2f0360-8352%2887%2990101-X&partnerID=40&md5=4088dfea5aba8db0e6c2d39d24221798},
abstract={It has been said that the greatest challenge for the future of computers in industrial engineering is not the further development of new systems or new architectures, rather it is the expansion of the usefulness of the powerful tools we already possess. The efficient use of a system or software package is dependent on the non-machine interface. The use of menus has long been viewed as a effective way of implementing this interface. But an inappropriate menu design can obscure useful functions and prevent the user from accessing the full power of a system. This paper discusses such menu features as size, shape, content and organization (layout). Cursor control and the use of texture formatting (highlighting, blinking, etc.) are also covered. A survey of the main principles of menu design as presented in the literature is discussed. This survey is followed by a description of an extensive menu-driven system developed by the authors. How the principles proposed by other researchers were integrated or adapted is discussed; as well as the introduction of new concepts. All this is condensed into a set of practical guidelines for menu designers. © 1987.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Terwilliger1986436,
author={Terwilliger, Robert B. and Campbell, Roy H.},
title={ENCOMPASS: A SAGA BASED ENVIRONMENT FOR THE COMPOSITION OF PROGRAMS AND SPECIFICATIONS.},
journal={Proceedings of the Hawaii International Conference on System Science},
year={1986},
volume={2 a},
pages={436-447},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022954704&partnerID=40&md5=6feb406e2a68d6bcf35040aac3d9e53a},
abstract={ENCOMPASS is an example integrated software engineering environment being constructed by the SAGA project. ENCOMPASS supports the specification, design, and construction of efficient, validated, and verified programs in a modular programming language. We present the development paradigm, schema of software configurations, and hierarchical library structure used by ENCOMPASS. We illustrate these with an example of software development. In ENCOMPASS, a development proceeds through the phases planning, requirements definition, validation, refinement, and system integration. The refinement phase may be decomposed into a number of steps, each consisting of a design transformation and its verification. The components in a software system are modeled as entities which have relationships between them. An entity may have different versions and different views of the same project are allowed. The simple entities supported by ENCOMPASS may be combined into modules which may be collected into projects. ENCOMPASS supports multiple programmers and projects using a hierarchical library system containing a workspace for each programmer; a project library for each project, and a global library common to all projects. A prototype implementation of ENCOMPASS is being constructed on the Unix operating system using an existing revision control system.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kung1986118,
author={Kung, S.Y.},
title={On programming languages for VLSI array processors},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1986},
volume={614},
pages={118-133},
doi={10.1117/12.960503},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022862190&doi=10.1117%2f12.960503&partnerID=40&md5=037191681d301bf2a67c2b18c3e8ae88},
abstract={A formal algorithmic notation and programming language will be critical in developing VLSI systems. Due to the difficulty of keeping track of several simultaneously occuring events, parallel programming is significantly more complicated than sequential programming. Therefore, it is desirable to consider new notations or languages which fit better to the array processors, instead of always sticking to conventional languages. The advent of algorithm-oriented VLSI architectures also necessitate new programming language theory to precisely describe concurrency imbedded in computational algorithms. For example, it is of great interest and importance to develop a complete set of programming techniques and software packages for wavefront/systolic type arrays. Some guidelines for algorithmic notations for such VLSI arrays will be addressed in this paper. We shall also discuss in great details several illustrative programming examples based on a wavefront language and Occam language. © 1986 SPIE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Batory1986187,
author={Batory, D.S. and Mannino, M.},
title={Panel: Extensible Database Systems},
journal={ACM SIGMOD Record},
year={1986},
volume={15},
number={2},
pages={187-190},
doi={10.1145/16856.16873},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976813155&doi=10.1145%2f16856.16873&partnerID=40&md5=766cacc453bd8177c00ae7dcf1470d5c},
abstract={New implementation techniques and new capabilities for database systems are being developed and proposed at a rapid rate. Novel file structures and improved algorithms for query optimization, buffer and recovery management, and transaction management have the potential of realizing significant gains in DBMS performance. The proposed integration of design objects, voice, text, rules, vector graphics, and images into databases promises exciting new capabilities for DBMSs. To accommodate advances in database technology and to support new classes of database applications, DBMSs must be extensible (i.e., customizable). To achieve extensibility forces a fundamental rethinking about how DBMSs are built, and how special-purpose features can be integrated into a DBMS with little effort and expense. Customizing DBMSs implies the availability of extensible data models, to allow for the introduction of new object types and operations, and extensible storage structures, to take advantage of special properties of stored data or operations to enhance performance. Although research on extensible DBMSs is still in its infancy, a fundamental concept underlying their construction is now evident. This is the standardization of interfaces and the plug-compatibility of modules. An extensible DBMS will be a ‘software bus’ whereby new modules (and hence new DBMS capabilities) can be added, exchanged, or removed by plugging or unplugging modules. Extensible DBMSs will thus rely on extensive software libraries, where new modules can be added as needed. Furthermore, changes to DBMSs can be made in months rather than years, and the reinvention of established technology is kept to a minimum because of the reusability of modules. The perception of DBMSs as monolithic entities that are difficult to modify will change as extensible DBMS technology becomes better understood. The use of database systems will not change, the ANSI/SPARC roles of database users, who write and execute transactions, and the database administrator (DBA), who designs and writes database schemas, will remain. Extensible DBMSs will require the introduction of an additional party, the database architecture administrator (DDA), who is responsible for the construction and customization of a DBMS. A growing number of researchers are developing extensible DBMSs. The purpose of this panel is to explain and discuss some of the approaches that are now being taken (and those that can be taken), and to survey the problems that confront extensible database technology. Descriptions of the systems and research represented at this panel are given in the following sections. © 1986, ACM. All rights reserved.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Joosten198665,
author={Joosten, R.},
title={Integration of Town Planning, Landscaping and 3D‐Architecture: Results and Future Enhancements since Camp ‘83},
journal={Computer Graphics Forum},
year={1986},
volume={5},
number={1},
pages={65-69},
doi={10.1111/j.1467-8659.1986.tb00269.x},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022675507&doi=10.1111%2fj.1467-8659.1986.tb00269.x&partnerID=40&md5=a7f7d9c2f458e754bf1206c83d1cbd87},
abstract={Through major innovations in software development Graphicomp succeeded in 1983 in creating ILP program (Intelligent Library Program) as a part of the ICAADS package (Integrated Computer Aided Architectural Design System) which enables maintenance of unlimited intelligent library items, in contrast to huge limited library figure files using large amounts of active diskspace unproductively. Further topics will be discussed: • Library Graphics aspects • Relations between library figures and architectural networks. • ACE menus. • Relations between various ACE applications. • Image design and solid modelling in ACE. • Relations towards 3D town planning and management. • Features of 3D‐landscaping with image design. Copyright © 1986, Wiley Blackwell. All rights reserved},
document_type={Article},
source={Scopus},
}

@ARTICLE{Thompson198573,
author={Thompson, R.E.},
title={A room of their own: Optimal setting for patrons and patron computers},
journal={Technical Services Quarterly},
year={1985},
volume={2},
number={3-4},
pages={73-91},
doi={10.1300/J124v02n03_07},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952505495&doi=10.1300%2fJ124v02n03_07&partnerID=40&md5=83b7c6f295b20f2db90bda0b6895e935},
abstract={Discusses rationale for a public library board's decision to construct a room to house patron computers, the chronology for construction, physical characteristics of the room, computer hardware, coin-op devices, computer software, user guidelines, patron scheduling, fee structure, staffing, patron orientation, the library’s role vis-a-vis patron computers, mistakes made, future plans. Appendices include annotated lists of patron software, guidelines, orientation outline, responsibility statement, introductory brochure. © 1985 Taylor & Francis Group, LLC.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cox1985397,
author={Cox, M.G.},
title={Topic libraries for mathematical computation},
journal={Software: Practice and Experience},
year={1985},
volume={15},
number={4},
pages={397-411},
doi={10.1002/spe.4380150408},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022042941&doi=10.1002%2fspe.4380150408&partnerID=40&md5=0f3a50d1a63e21b72f0a70f22303dc91},
abstract={This paper describes software engineering aspects and the scope of three subroutine libraries developed at the U.K. National Physical Laboratory (NPL). The libraries are concerned with three important fields of mathematical computation: data approximation, linear algebra and optimization. A portable subset of Fortran was used in the construction of these libraries, which have a highly modular structure and are supported by full user documentation. Reference is made to Microtext, an NPL software product for displaying structured information. Its value in respect of the subroutine libraries lies in its ability to assist the user in selecting appropriate subroutines for the solution of his problem. Copyright © 1985 John Wiley & Sons, Ltd},
author_keywords={Fortran;  Mathematical computation;  Microtext;  Modularity;  Portability;  Subroutine library design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Johnson1984739,
author={Johnson, J.L.},
title={Anatomy of an educational network database system},
journal={Software: Practice and Experience},
year={1984},
volume={14},
number={8},
pages={739-754},
doi={10.1002/spe.4380140804},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021477949&doi=10.1002%2fspe.4380140804&partnerID=40&md5=6cb966a3b493d545aac9817dff35bbcb},
abstract={The construction of the systems software necessary to support arbitrary network databases is described. The software consists of two packages. First, there is a data description language compiler, which accepts a conceptual description of an arbitrary network and reduces it to a set of specialized data structures. The second component is a set of data manipulation language access routines, which are callable from an application program to store and retrieve data from the structures. The software is essentially an implementation of the CODASYL proposal, although the treatment of sets has been simplified. The main thrust of the discussion is the representation of nodes and sets of an arbitrary network by certain data structures. The software was written to provide a system on which students could practice database design problems and to illustrate systems programming techniques common in database software. Copyright © 1984 John Wiley & Sons, Ltd},
author_keywords={CODASYL model;  Database;  Network model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bryant1982111,
author={Bryant, R.M.},
title={Discrete system simulation in Ada},
journal={Simulation},
year={1982},
volume={39},
number={4},
pages={111-121},
doi={10.1177/003754978203900402},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020192818&doi=10.1177%2f003754978203900402&partnerID=40&md5=13c89b73c6d29c86abff770e03c0dde6},
abstract={Ada, the new Department of Defense standard language, contains many features designed to facilitate the construction of software for embedded computer systems. Two par ticularly important features are tasks (independent threads of execution in a program) and packages (collections of routines and data structures that can be compiled separately and in cluded as a unit in other programs). These features allow the construction of powerful, process-oriented, discrete-system simulation packages in Ada. We provide a design for such a package and illustrate its use. © 1982, Sage Publications. All rights reserved.},
author_keywords={discrete simulation;  government applications;  military applications;  process-oriented;  software engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Denert198165,
author={Denert, E. and Hesse, W. and Neumaier, H.},
title={S/E/TEC – an environment for the production of reliable software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1981},
volume={123 LNCS},
pages={65-84},
doi={10.1007/3-540-10885-8_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019670493&doi=10.1007%2f3-540-10885-8_31&partnerID=40&md5=f3b61de21e9d8db8245b6ac0c8a572b9},
abstract={S/E/TEC is a software engineering environment supporting the software producers' activities as well as management activities. The main components of S/E/TEC are the project model, the production techniques, the management techniques, the product patterns, the development standards, the project library and the terminal system PET/MAESTRO. The conceptuel basis of S/E/TEC and the methodological frame for the other components is the project model. It defines the phases, activities, products, and management actions of software projects. The production techniques cover the whole software production process from analysis and definition through design, implementation, integration and installation to operations and maintenance. Product patterns and development standards give guidelines for the structure of the products and possible ways to establish them. Management gets support from tools for effort estimation and control, priority planning, and quality assurance steps. All development and management documents are collected and administered in the project library. The software development terminal system PET/MAESTRO provides computer support for most of the production and management techniques and is a suitable basis for the project library. © by Springer-Verlag Berlin Heidelberg 1981.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Woody1977449,
author={Woody, C.A. and Fitzgerald, M.P. and Scott, F.J. and Power, D.L.},
title={A subject-content oriented retriever for processing information on-line (SCORPIO)},
journal={AFIPS Conference Proceedings - 1977 National Computer Conference, AFIPS 1977},
year={1977},
pages={449-454},
doi={10.1145/1499402.1499480},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053119946&doi=10.1145%2f1499402.1499480&partnerID=40&md5=722afad5a6f70e73d8ebd590e8bb75d6},
abstract={Traditional obstacles to the development of on-line software for library/information center functions are overcome in the design and implementation of SCORPIO, an on-line information retrieval system developed at the Library of Congress, through the use of an "elastic" data base architecture, the phased development of reusable/disposable software, and the design of an open-ended, non-technical retrieval language. The data base architecture allows arbitrarily large files with no limit on the size of individual records. Software is developed in short time intervals to minimize investment and maintain customer interest; and the software itself is isolated from environmental factors most subject to change. The retrieval language is nontechnical and system responses are people-oriented. © 1977 by the American Federation of Information Processing Societies, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pouzin197380,
author={Pouzin, L.},
title={Presentation and major design aspects of the cyclades computer network},
journal={Data Networks: Analysis and Design - 3rd Data Communications Symposium, DATACOMM 1973},
year={1973},
pages={80-87},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059169404&partnerID=40&md5=bf746f60fb6290c7a71bfd08a22703f6},
abstract={A computer network is being developed in France, under government sponsorship, to link about twenty heterogeneous computers located in universities, research and D.P. Centers. Goals are to set up a prototype network in order to foster experiment in various areas, such as : data communications, computer interaction, cooperative research, distributed data bases. The network is intended to be both, an object for research, and an operational tool. In order to speed up the implementation, standard equipment is used, and modifications to operating systems are minimized. Rather, the design effort bears on a carefully layered architecture, allowing for a gradual insertion of specialized protocols and services tailored to specific application and user classes. A particular objective, for which CYCLADES should be an operational tool, is to provide various departments of the French Administration with access to multiple data bases located in geographically distant areas. Host-host protocols, as well as error and flow control mechanisms are based on a simple message exchange procedure, on top of which various options may be built for the sake of efficiency, error recovery, or convenience. Depending on available computer resources, these options can be implemented as user software, system modules, or front end processor package. For each of them, network-wide interfaces are defined, to conserve consistency in human communications. CYCLADES uses a packet-switching sub-network, which is a transparent message carrier, completely independent of host-host conventions. While in many ways similar to ARPANET, it presents some distinctive differences in address and message handling, intended tofacilitate interconnection with other networks. In particular, addresses can have variable formats, and messages are not delivered in sequence, so that they can flow out of the network through several gates toward an outside target. Terminal concentrators are mini-hosts, and implement whatever services users or applications require, such as sequencing, error recovery, code translation, buffering, etc. Some specialized hosts may be installed to cater for specific services, such as mail, resource allocation, information retrieval, mass storage. A control center is also being installed and will be operated by the French PTT. © 1973 Association for Computing Machinery, Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Woodsford1971335,
author={Woodsford, P.A.},
title={The design and implementation of the GINO 3D graphics software package},
journal={Software: Practice and Experience},
year={1971},
volume={1},
number={4},
pages={335-365},
doi={10.1002/spe.4380010404},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983945006&doi=10.1002%2fspe.4380010404&partnerID=40&md5=485be3aeb714e317aa44b2105d54fd66},
abstract={The design and implementation of a general purpose graphics software package (GINO) is described. GINO provides facilities for 3D graphics (co‐ordinate transformation, clipping, intensity modulation) but is organized so that 2D facilities form a clean subset. It is device independent, permitting use of refresh CRT displays, storage tube displays and plotters. A characteristic feature is the use of small satellite computers attached to a large multiaccess computer (ATLAS 2) GINO takes the form of a subroutine library accessible from FORTRAN and other languages, and the case for this level of graphics software is argued. The reasons for not using a mandatory graphical data structure are also discussed. GINO is not biased towards any particular style of interaction, but two techniques are described; one based on the light pen and the other on teletype command languages Efficiency of implementation is achieved without loss of flexibility by use of a systems programming language (SAL). Copyright © 1971 John Wiley & Sons, Ltd},
author_keywords={2D graphics;  3D graphics;  Data structures;  Graphics;  Interaction;  Satellite computer;  Systems programming language},
document_type={Article},
source={Scopus},
}
