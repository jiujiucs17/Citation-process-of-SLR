@article{HAESAERT201775,
title = {Certified policy synthesis for general Markov decision processes: An application in building automation systems},
journal = {Performance Evaluation},
volume = {117},
pages = {75-103},
year = {2017},
issn = {0166-5316},
doi = {https://doi.org/10.1016/j.peva.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166531617301049},
author = {Sofie Haesaert and Nathalie Cauchi and Alessandro Abate},
keywords = {Verification, Synthesis, General Markov decision processes, Safety, Building automation systems, Temperature control},
abstract = {In this paper, we present an industrial application of new approximate similarity relations for Markov models, and show that they are key for the synthesis of control strategies. Typically, modern engineering systems are modelled using complex and high-order models which make the correct-by-design controller construction computationally hard. Using the new approximate similarity relations, this complexity is reduced and we provide certificates on the performance of the synthesised policies. The application deals with stochastic models for the thermal dynamics in a “smart building” setup: such building automation system set-up can be described by discrete-time Markov decision processes evolving over an uncountable state space and endowed with an output quantifying the room temperature. The new similarity relations draw a quantitative connection between different levels of model abstraction, and allow to quantitatively refine over complex models control strategies synthesised on simpler ones. The new relations, underpinned by the use of metrics, allow in particular for a useful trade-off between deviations over probability distributions on states and distances between model outputs. We develop a software toolbox supporting the application and the computational implementation of these new relations.}
}
@incollection{FRAUBAUM2015527,
title = {Modeling and Sensitivity Analysis of a Medium-Temperature Gas Cleaning Process of Biogenous Synthesis Gas},
editor = {Krist V. Gernaey and Jakob K. Huusom and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {37},
pages = {527-532},
year = {2015},
booktitle = {12th International Symposium on Process Systems Engineering and 25th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63578-5.50083-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444635785500839},
author = {Michaela Fraubaum and Heimo Walter},
keywords = {biomass gasification, tar conversion, methanation, process modelling},
abstract = {Tar removal is the major technical obstacle in the implementation of the gasification technology. This paper deals with the modelling, simulation and design of a small scale production process of biogenous synthetic natural gas (SNG) using a novel catalytic gas cleaning process which operates in a considerably lower temperature range than catalytic hot gas cleaning methods. Mathematical models for the different gas cleaning steps are developed and implemented into an already existing model library of the commercial simulation software IPSEpro. A special focus lies on the tar reforming reactor, which is modelled by applying mass and energy balances, reaction kinetics and chemical equilibrium equations. Energy savings of the individual process steps are identified and the overall process, which is based on an allothermal gasifier, is integrated in the software. Additionally, a sensitivity analysis is performed to show the influence of various operating parameters on the single elements and the whole system. Their effects on the overall process efficiencies as well as on important process factors are evaluated and discussed in this paper. The analysis described in this paper shows that a promising total efficiency for the overall process (with district heating) of approximately 90% can be reached.}
}
@article{GARCIA2004283,
title = {Component Fault Isolation in a Fluid Catalytic Cracking Unit via Analytical Redundancy1},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {21},
pages = {283-288},
year = {2004},
note = {2nd IFAC Symposium on System Structure and Control, Oaxaca, Mexico, December 8-10, 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)30482-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017304822},
author = {Efraín Alcorta García and Plinio de Léon Cantón and Oscar Sotomayor and Darci Odloack},
keywords = {Linear model, observers, fault detection, isolation},
abstract = {This paper considers an approach to detect and isolate component faults for a Fluid Catalytic Cracking (FCC) unit. Model based analytical redundancy is used on an augmented linearized model of the nonlinear FCC unit to the isolation of faults. Some component faults are modeled and implemented in a software package to test the considered approach. The proposed design is tested using a nonlinear simulation of the FCC unit model. The results show that the linear observed-based approach applied to an augmented state of the FCC to component fault detection and isolation can be used with success. The nonlinear behavior of the FCC however has some effect on the residuals. The detection and isolation can be reached with the help of a threshold.}
}
@article{VAIDYA2021101445,
title = {Towards generating a reliable device-specific identifier for IoT devices},
journal = {Pervasive and Mobile Computing},
volume = {76},
pages = {101445},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101445},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000869},
author = {Girish Vaidya and Akshay Nambi and T.V. Prabhakar and Vasanth Kumar T. and Suhas Sudhakara},
keywords = {Device ID, Fingerprinting, PUFs, Edge ML},
abstract = {A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers. We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices. Further, we discuss approaches to evaluate and improve the reliability of the IoT-ID.11This manuscript is an extension of the paper ‘IoT-ID: A Novel Device-Specific Identifier Based on Unique Hardware Fingerprints’ Vaidya et al. (2020) published in 2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI).}
}
@article{LAM1996909,
title = {Portable end-to-end ground system for low-cost mission support},
journal = {Acta Astronautica},
volume = {39},
number = {9},
pages = {909-915},
year = {1996},
note = {IAA International Symposium on Small Satellites for Earth Observation},
issn = {0094-5765},
doi = {https://doi.org/10.1016/S0094-5765(97)00075-1},
url = {https://www.sciencedirect.com/science/article/pii/S0094576597000751},
author = {Barbara Lam},
abstract = {This paper presents a revolutionary architecture of the end-to-end ground system to reduce overall mission support costs. The present ground system of the Jet Propulsion Laboratory (JPL) is costly to operate, maintain, deploy, reproduce, and document. In the present climate of shrinking NASA budgets, this proposed architecture takes on added importance as it should dramatically reduce all of the above costs. Currently, the ground support functions (i.e., receiver, tracking, ranging, telemetry, command, monitor and control) are distributed among several subsystems that are housed in individual rack-mounted chassis. These subsystems can be integrated into one portable laptop system using established Multi Chip Module (MCM) packaging technology and object-based software libraries. The large scale integration of subsystems into a small portable system connected to the World Wide Web (WWW) will greatly reduce operations, maintenance and reproduction costs. Several of the subsystems can be implemented using Commercial Off-The-Shelf (COTS) products further decreasing non-recurring engineering costs. The inherent portability of the system will open up new ways for using the ground system at the “point-of-use” site as opposed to maintaining several large centralized stations. This eliminates the propagation delay of the data to the Principal Investigator (PI), enabling the capture of data in real-time and performing multiple tasks concurrently from any location in the world. Sample applications are to use the portable ground system in remote areas or mobile vessels for real-time correlation of satellite data with earth-bound instruments; thus, allowing near real-time feedback and control of scientific instruments. This end-to-end portable ground system will undoubtedly create opportunities for better scientific observation and data acquisition.}
}
@article{JOOSEN199077,
title = {Dynamic load balancing in transputer applications with geometric parallelism},
journal = {Microprocessing and Microprogramming},
volume = {30},
number = {1},
pages = {77-84},
year = {1990},
note = {Proceedings Euromicro 90: Hardware and Software in System Engineering},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(90)90221-T},
url = {https://www.sciencedirect.com/science/article/pii/016560749090221T},
author = {W Joosen and Y Berbers and P Verbaeten},
keywords = {Dynamic load balancing, data parallelism, geometric parallelism, transputer},
abstract = {In this paper we report on the design of a software package for dynamic load balancing in applications with geometric parallelism. The load controller we propose can be developed and adapted separately from the typical application code. It will be shown that our load balancing mechanism can be implemented on the transputer with minimal overhead.}
}
@article{MORSTYN2020115397,
title = {OPEN: An open-source platform for developing smart local energy system applications},
journal = {Applied Energy},
volume = {275},
pages = {115397},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.115397},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920309090},
author = {Thomas Morstyn and Katherine A. Collett and Avinash Vijay and Matthew Deakin and Scot Wheeler and Sivapriya M. Bhagavathy and Filiberto Fele and Malcolm D. McCulloch},
keywords = {Distributed energy resource, Distribution network, Modelling, Python, Open-source software, Smart local energy system},
abstract = {This paper presents OPEN, an open-source software platform for integrated modelling, control and simulation of smart local energy systems. Electric power systems are undergoing a fundamental transition towards a significant proportion of generation and flexibility being provided by distributed energy resources. The concept of ‘smart local energy systems’ brings together related strategies for localised management of distributed energy resources, including active distribution networks, microgrids, energy communities, multi-energy hubs, peer-to-peer trading platforms and virtual power plants. OPEN provides an extensible platform for developing and testing new smart local energy system management applications, helping to bridge the gap between academic research and industry translation. OPEN combines features for managing smart local energy systems which are not provided together by existing energy management tools, including multi-phase distribution network power flow, energy market modelling, nonlinear energy storage modelling and receding horizon optimisation. The platform is implemented in Python with an object-oriented structure, providing modularity and allowing it to be easily integrated with third-party packages. Case studies are presented, demonstrating how OPEN can be used for a range of smart local energy system applications due to its support of multiple model fidelities for simulation and control.}
}
@article{CHRISOCHOIDES2000621,
title = {Mobile object layer: a runtime substrate for parallel adaptive and irregular computations},
journal = {Advances in Engineering Software},
volume = {31},
number = {8},
pages = {621-637},
year = {2000},
issn = {0965-9978},
doi = {https://doi.org/10.1016/S0965-9978(00)00032-6},
url = {https://www.sciencedirect.com/science/article/pii/S0965997800000326},
author = {N. Chrisochoides and K. Barker and D. Nave and C. Hawblitzel},
keywords = {Parallel, Message passing, Load balancing, Runtime software, Adaptive mesh generation},
abstract = {In this paper we present a parallel runtime substrate, the Mobile Object Layer (MOL), that supports data or object mobility and automatic message forwarding in order to ease the implementation of adaptive and irregular applications on distributed memory machines. The MOL implements a global logical name space for message passing and distributed directories to assist in the translation of logical to physical addresses. Our data show that the latency of the MOL primitives is within 10–14% of the latency of the underlying communication substrate. The MOL is a lightweight, portable library designed to minimize maintenance costs for very large-scale parallel adaptive applications.}
}
@incollection{SALALIZARRAGA20201009,
title = {13 - Exergy in continuous media. Application to equipment design},
editor = {José M P {Sala Lizarraga} and Ana Picallo-Perez},
booktitle = {Exergy Analysis and Thermoeconomics of Buildings},
publisher = {Butterworth-Heinemann},
pages = {1009-1067},
year = {2020},
isbn = {978-0-12-817611-5},
doi = {https://doi.org/10.1016/B978-0-12-817611-5.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128176115000138},
author = {José M P {Sala Lizarraga} and Ana Picallo-Perez},
keywords = {Local exergy balance, Local exergy cost balance, Thermodynamics of continuous media},
abstract = {Classical Thermodynamics analyses systems in a state of equilibrium. To overcome the simplifications that this implies, in this chapter, we will use Thermodynamics of Continuous Media. A review of Thermodynamics of Continuous Media is first presented, making an introduction to multicomponent systems. Then, the Law of Conservation of Mass, the Law of Conservation of Energy and the Second Law are presented for a Control Mass, locally and for a Control Volume. Later, the concept of exergy in continuous media is introduced, thus defining the scalar field of exergy. For each point of a continuous medium and at each instant the generation of entropy is related to exergy destruction. The equation of the local exergy balance is then written, analyzing the exergy destruction in two transport phenomena: heat transfer and flow with friction. Finally a brief introduction to the Theory of Exergy Cost in continuous media is developed. The main aim in this chapter is to present the exergy balance to be implemented in CFD software packages, which will allow to obtain additional information that can only be found through the use of the exergy methodology.}
}
@article{FRANKE201462,
title = {FALCON or how to compute measures time efficiently on dynamically evolving dense complex networks?},
journal = {Journal of Biomedical Informatics},
volume = {47},
pages = {62-70},
year = {2014},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2013.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1532046413001469},
author = {R. Franke and G. Ivanova},
keywords = {Complex network, Brain network, OpenCL, GPGPU, Code optimization, SSE},
abstract = {A large number of topics in biology, medicine, neuroscience, psychology and sociology can be generally described via complex networks in order to investigate fundamental questions of structure, connectivity, information exchange and causality. Especially, research on biological networks like functional spatiotemporal brain activations and changes, caused by neuropsychiatric pathologies, is promising. Analyzing those so-called complex networks, the calculation of meaningful measures can be very long-winded depending on their size and structure. Even worse, in many labs only standard desktop computers are accessible to perform those calculations. Numerous investigations on complex networks regard huge but sparsely connected network structures, where most network nodes are connected to only a few others. Currently, there are several libraries available to tackle this kind of networks. A problem arises when not only a few big and sparse networks have to be analyzed, but hundreds or thousands of smaller and conceivably dense networks (e.g. in measuring brain activation over time). Then every minute per network is crucial. For these cases there several possibilities to use standard hardware more efficiently. It is not sufficient to apply just standard algorithms for dense graph characteristics. This article introduces the new library FALCON developed especially for the exploration of dense complex networks. Currently, it offers 12 different measures (like clustering coefficients), each for undirected-unweighted, undirected-weighted and directed-unweighted networks. It uses a multi-core approach in combination with comprehensive code and hardware optimizations. There is an alternative massively parallel GPU implementation for the most time-consuming measures, too. Finally, a comparing benchmark is integrated to support the choice of the most suitable library for a particular network issue.}
}
@article{XU2020111404,
title = {Development and implementation of EPICS application program of HL-2A host engineering parameters acquisition system},
journal = {Fusion Engineering and Design},
volume = {151},
pages = {111404},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2019.111404},
url = {https://www.sciencedirect.com/science/article/pii/S0920379619309007},
author = {Jie Xu and Peihong Tian and Fanghua Du and Yinxiang Wan and Fangqun Tang and Qingwei Yang},
keywords = {Tokamak, Centralized control, PXI express, Data acquisition, C#, EPICS},
abstract = {Abstact
In order to meet the increasing experimental requirements of HL-2A tokamak device discharge and improve the integration of HL-2A host centralized control system, this paper designs a high-speed data acquisition system based on PXI Express. In order to enable the fast acquisition system to be integrated into the HL-2A host centralized control system under the EPICS platform for centralized control and management, it mainly describes how to realize the communication between fast acquisition signal system based on C# and the upper layer OPI through the PON network under the EPICS platform. The data acquisition system hardware adopts PXIe-based chassis, embedded controller and high-speed synchronous acquisition card, which has the advantages of good mechanical encapsulation, high modularity and high sampling rate. The software adopts a measurement and control system development platform based on Microsoft. NET platform, Visual Studio development environment and Visual C# language. In this paper, the Epics C# library was studied, the EPICS Server application was developed and the remote control of the state and parameters in HL-2A host engineering system was realized. The system runs stably in the HL-2A device 2018 experiment. The result shows that the system can work stably at 10KSps sampling rate, which can better meet the needs of the device operation. This work completed the basic framework of the HL-2A Tokamak host centralized control system. And the technology and method can provide reference for the design of the next-generation device HL-2 M host centralized control system of the, laying a good foundation.}
}
@article{LOSASSO2012427,
title = {Project management techniques used in the European Vacuum Vessel sectors procurement for ITER},
journal = {Fusion Engineering and Design},
volume = {87},
number = {5},
pages = {427-431},
year = {2012},
note = {Tenth International Symposium on Fusion Nuclear Technology (ISFNT-10)},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2011.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0920379611006181},
author = {Marcello Losasso and Maria Ortiz {de Zuniga} and Lawrence Jones and Angel Bayon and Jean-François Arbogast and Joan Caixas and José Fernández and Stefano Galvan and Teresa Jover and Kimihiro Ioki and Michal Lewczanin and Gonzalo Micó and Jose Miguel Pacheco and Joseph Preble and Vassilis Stamos and Alexandru Trentea},
keywords = {ITER Vacuum Vessel, Project management, Tender, Schedule, Risk, WBS, PLM},
abstract = {The contract for the seven European Sectors of the ITER Vacuum Vessel (VV) was placed at the end of 2010 with a consortium of three Italian companies. The task of placing and the initial take-off of this large and complex contract, one of the largest placed by F4E, the European Domestic Agency for ITER, is described. A stringent quality controlled system with a bespoke Vacuum Vessel Project Lifecycle Management system to control the information flow, based on ENOVIA SmarTeam, was developed to handle the storage and approval of Documentation including links to the F4E Vacuum Vessel system and ITER International Organization System interfaces. The VV Sector design and manufacturing schedule is based on Primavera software, which is cost loaded thus allowing F4E to carry out performance measurement with respect to its payments and commitments. This schedule is then integrated into the overall Vacuum Vessel schedule, which includes ancillary activities such as instruments, preliminary design and analysis. The VV Sector Risk Management included three separate risk analyses from F4E and the bidders, utilizing two different methodologies. These efforts will lead to an efficient and effective implementation of this contract, vital to the success of the ITER machine, since the Vacuum Vessel is the biggest single work package of Europe's contribution to ITER and is the largest component of the ITER device.}
}
@article{GERGEL1993305,
title = {A software system for multiextremal optimization},
journal = {European Journal of Operational Research},
volume = {65},
number = {3},
pages = {305-313},
year = {1993},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(93)90109-Z},
url = {https://www.sciencedirect.com/science/article/pii/037722179390109Z},
author = {V.P. Gergel},
abstract = {The system for multiextremal optimization (SYMOP) described here is a program package [7] oriented for using in the computer-aided design (CAD)_as some invariant software tool of making efficient design decisions. Besides, it is possible to use this system independently for solving various optimization problems and systems of nonlinear algebraic equations. The SYMOP package is designed to solve numerically the multicriteria multiextremal multidimensional optimization problems with complicated constraints. These problems can be considered as the mathematical models of decision making problems in various applications. For example, these may be identification problems (defining the functional dependencies by experimental data), approximation problems (choosing the optimal parameters of given functions), etc. As we can see the SYMOP system differs from packages of similar purpose. On the one hand, the SYMOP is oriented upon solving complicated decision-making problems. On the other hand, a wide set of optimization problems can be solved by this system. Also, the decision making process performed by the SYMOP system is similar to the techniques used in the CAD to choose an optimal decision. The system has been implemented on the IBM/360-compatible ES-1035 computer and it runs under the ES OS operating system. It was written in FORTRAN-IV. As minimum the system needs 200 Kb of internal and 2 Mb of external memory. For dialogue the ES-7920 alphabetic displays are used.}
}
@article{LIU19989,
title = {Distributed multilingual applications of the OSI architecture},
journal = {Computer Standards & Interfaces},
volume = {19},
number = {1},
pages = {9-29},
year = {1998},
issn = {0920-5489},
doi = {https://doi.org/10.1016/S0920-5489(97)00004-4},
url = {https://www.sciencedirect.com/science/article/pii/S0920548997000044},
author = {Raymond Liu and John Lions},
keywords = {Distributed applications, External data representation, Unicode implementations, Multibyte encoding and multilingual programming},
abstract = {The merging of ISO 10646 and Unicode standards provides a single Universal Character Set (UCS) that contains all world's written language characters. It will replace ASCII as an international character codeset to design computer systems for supporting universal languages. Further work needs to be done in incorporating the UCS into historical operating systems, programming languages, existing software programs, and network protocols. In this paper, we review internal and external representations of the UCS, and propose a byte encoding for the UCS. The proposed encoding is a better external code for data storage and transmission in terms of the bit usage and code spaces for character representations. It is an ASCII-compatible textual representation of the UCS. A wide character code functions library that is based on a fixed-width of 16-bit encoding is developed in the context of XPG4. We will present a Unicode Programming Platform (UPP). The UPP is built on top of X Window to support codeset-independency applications. Also, we will describe an abstract data type of multilingual data with the UCS and ASN.1 in the presentation layer, and implement a compiler for translating extended ASN.1 data types into corresponding C/C++ data types for data exchange of distributed applications and multilingual communications.}
}
@article{AIRD1983202,
title = {PROTRAN: Problem solving software},
journal = {Advances in Engineering Software (1978)},
volume = {5},
number = {4},
pages = {202-206},
year = {1983},
issn = {0141-1195},
doi = {https://doi.org/10.1016/0141-1195(83)90048-7},
url = {https://www.sciencedirect.com/science/article/pii/0141119583900487},
author = {Thomas J. Aird and John R. Rice},
keywords = {problem-solving, mathematical-software, Fortran-preprocessor, high-level language},
abstract = {PROTRAN is the name of a family of problem-solving software systems designed to provide quick and convinient access to the computational power of the IMSL Library of mathematical and statistical Fortran subroutines. This paper discusses the structure and implementation of PROTRAN using as illustration MATH/PROTRAN which provides access to the mathematical areas of the Library. PROTRAN accepts problem specifications, checks them for consistency, selects the appropriate IMSL routines and generates a Fortran program. All programming requirements such as workspace assignment and error message handling are handled automatically by PROTRAN. The generated Fortran program goes through the usual steps of compilation, linking, and execution to produce a solution to the problem. PROTRAN adds to the problem-solving power of Fortran by adding very high level problem-solving statements and it adds to the reliability of Fortran by providing extensive error checking and automating the details of using the IMSL Library.}
}
@article{DERAKHSHANIAN2018231,
title = {Experimental and numerical investigation for a reliable simulation tool for oblique water entry problems},
journal = {Ocean Engineering},
volume = {160},
pages = {231-243},
year = {2018},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2018.04.080},
url = {https://www.sciencedirect.com/science/article/pii/S0029801818305948},
author = {Mohammad Saeed Derakhshanian and Mobin Haghdel and Mohammad Mahdi Alishahi and Ali Haghdel},
keywords = {Water entry, Oblique impact, Truncated-nose body, Fluid-solid interaction, ALE method},
abstract = {Oblique water entry of different marine structures involve several two-phase fluid dynamics phenomena including splash, asymmetrical cavity formation and cavity collapse that have been observed in experimental work. Correct prediction of the applied forces and moments that define the trajectory during water entry is dependent on precise simulation of the mentioned fluid dynamic effects. This study investigates and compares the performance of three different algorithms. i.e.: a) finite volume Eulerian; b) finite difference Eulerian, c) Arbitrary Lagrangian Eulerian (ALE), applied in different software codes. In order to achieve this goal, an experimental task, including normal and oblique water entry cases, was defined to provide the data set for comparing and verifying performance of these different software packages. These experiments have been designed and implemented, concentrating on oblique water entry of blunted nose models with various moment of inertia, center of gravity, and other dynamical properties that affect the water entry problem. Verification of computational results of the three software are carried out through comparison with the experimental results in two stages. Firstly, the fluid dynamics of splash, cavity formation and its collapse for normal water entry of a sphere are compared between experimental and three software numerical results. It is shown that two of the codes, i.e. “a” and “b” as defined above, cannot even predict the much simpler problem of normal water entry problem, therefore, fail to be a candidate for the more involved problem of oblique water entry, i.e., the second stage of the comparison. Through comparison with the experimental results it is shown that ABAQUS software, i.e. the “c” algorithm, is the most capable software among the three compared codes for the correct simulation of the involved fluid dynamic effects. Therefore, the ABAQUS software is applied for simulation of oblique water entry problem in the second stage and the results are compared with experiments. Upon comparison of various experimental results with this software output, it is shown that the numerical algorithm in this computer program (ALE) can be reliably used to simulate this kind of fluid solid interaction problem and determine the impact forces and moments more precisely even in the extreme angles and velocities of water entry problems.}
}
@article{LAGIN2008530,
title = {Status of the National Ignition Facility Integrated Computer Control System (ICCS) on the path to ignition},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {530-534},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0920379607004942},
author = {L.J. Lagin and R.C. Bettenhausen and G.A. Bowers and R.W. Carey and O.D. Edwards and C.M. Estes and R.D. Demaret and S.W. Ferguson and J.M. Fisher and J.C. Ho and A.P. Ludwigsen and D.G. Mathisen and C.D. Marshall and J.T. Matone and D.L. McGuigan and R.J. Sanchez and E.A. Stout and E.A. Tekle and S.L. Townsend and P.J. {Van Arsdall} and E.F. Wilson},
keywords = {National Ignition Facility, Integrated Computer Control, Automation, CORBA, Software frameworks},
abstract = {The National Ignition Facility (NIF) at the Lawrence Livermore National Laboratory is a stadium-sized facility under construction that will contain a 192-beam, 1.8-MJ, 500-TW, ultraviolet laser system together with a 10-m diameter target chamber with room for multiple experimental diagnostics. NIF is the world's largest and most energetic laser experimental system, providing a scientific center to study inertial confinement fusion (ICF) and matter at extreme energy densities and pressures. NIF's laser beams are designed to compress fusion targets to conditions required for thermonuclear burn, liberating more energy than required to initiate the fusion reactions. NIF is comprised of 24 independent bundles of eight beams each using laser hardware that is modularized into more than 6000 line replaceable units such as optical assemblies, laser amplifiers, and multi-function sensor packages containing 60,000 control and diagnostic points. NIF is operated by the large-scale Integrated Computer Control System (ICCS) in an architecture partitioned by bundle and distributed among over 800 front-end processors and 50 supervisory servers. NIF's automated control subsystems are built from a common object-oriented software framework based on CORBA distribution that deploys the software across the computer network and achieves interoperation between different languages and target architectures. A shot automation framework has been deployed during the past year to orchestrate and automate shots performed at the NIF using the ICCS. In December 2006, a full cluster of 48 beams of NIF was fired simultaneously, demonstrating that the independent bundle control system will scale to full scale of 192 beams. At present, 72 beams have been commissioned and have demonstrated 1.4-MJ capability of infrared light. During the next 2 years, the control system will be expanded in preparation for project completion in 2009 to include automation of target area systems including final optics, target positioners and diagnostics. Additional capabilities to support fusion ignition shots in a National Ignition Campaign (NIC) beginning in 2010 will include a cryogenic target system, target diagnostics, and integrated experimental shot data analysis with tools for data visualization and archiving. This talk discusses the current status of the control system implementation and discusses the plan to complete the control system on the path to ignition.}
}
@article{DAI201547,
title = {TEE: A virtual DRTM based execution environment for secure cloud-end computing},
journal = {Future Generation Computer Systems},
volume = {49},
pages = {47-57},
year = {2015},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14001538},
author = {Weiqi Dai and Hai Jin and Deqing Zou and Shouhuai Xu and Weide Zheng and Lei Shi and Laurence Tianruo Yang},
keywords = {Virtual Machine Monitor (VMM), Dynamic Root of Trust for Measurement (DRTM), Cloud computing, Xen hypervisor},
abstract = {The Internet of Things (IoT) is the incoming generation of information technology. However, the huge amount of data collected by wireless sensors in IoT will impose a big challenge that can only be met by cloud computing. In particular, ensuring security in the cloud-end is necessary. Previous studies have mainly focused on secure cloud-end storage, whereas secure cloud-end computing is much less investigated. The current practice is solely based on Virtual Machines (VM), and cannot offer adequate security because the guest Operating Systems (OS) often can be compromised (e.g., by exploiting their vulnerabilities). This motivates the need of solutions for more secure cloud-end computing. This paper presents the design, implementation and analysis of a candidate solution, called Trusted Execution Environment (TEE), which takes advantage of both virtualization and trusted computing technologies simultaneously. The novelty behind TEE is the virtualization of the Dynamic Root of Trust for Measurement (DRTM).}
}
@article{GENAIDY1988437,
title = {Ergonomic job design in frequent manual lifting tasks: A microcomputer-based model},
journal = {Computers & Industrial Engineering},
volume = {15},
number = {1},
pages = {437-442},
year = {1988},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(88)90123-4},
url = {https://www.sciencedirect.com/science/article/pii/0360835288901234},
author = {A. Genaidy and J. Duggal and M. Ayoub and S. Puppala},
abstract = {The main objective of this paper is to generate ergonomic software that can be used in the design and evaluation of manual materials handling tasks so as to minimize the risk of injury. Specifically, this study is aimed at developing a microcomputer-based model for the design of frequent manual lifting tasks based ont he concept of job saverity index. The microcomputer-based software package is intended to be used by non-experts in the field of MMH. Possible engineering and administrative controls are implemented in the software in case if human lifting abilities are exceeded. The software was written in AutoLISP for the IBM personal computer. The knowledge base of the software is built upon a set of two models which were developed in the present study. The models were based on 2,736 observations.}
}
@article{LOPES20163,
title = {Physically-sound simulation of low-velocity impact on fiber reinforced laminates},
journal = {International Journal of Impact Engineering},
volume = {92},
pages = {3-17},
year = {2016},
note = {Impact Loading on Lightweight Structures},
issn = {0734-743X},
doi = {https://doi.org/10.1016/j.ijimpeng.2015.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0734743X15001128},
author = {C.S. Lopes and S. Sádaba and C. González and J. Llorca and P.P. Camanho},
keywords = {Composite laminates, Low-velocity impact, Numerical simulation, Finite element analysis, Continuum damage modeling, Cohesive models},
abstract = {A high-fidelity virtual tool for the numerical simulation of low-velocity impact damage in unidirectional composite laminates is proposed. A continuum material model for the simulation of intraply damage phenomena is implemented in a numerical scheme as a user subroutine of the commercially available Abaqus finite element package. Delaminations are simulated using of cohesive surfaces. The use of structured meshes, aligned with fiber directions allows the physically-sound simulation of matrix cracks parallel to fiber directions, and their interaction with the development of delaminations. The implementation of element erosion criteria and the application of intraply and interlaminar friction allow for the simulation of fiber splits and their entanglement, which in turn results in permanent indentation in the impacted laminate. It is shown that this simulation strategy gives sound results for impact energies bellow and above the Barely Visible Impact Damage threshold, up to laminate perforation conditions.}
}
@article{COLE198527,
title = {Adoption and adaptation in the use of transaction processing systems: The case of OCLC software},
journal = {Information Processing & Management},
volume = {21},
number = {1},
pages = {27-34},
year = {1985},
issn = {0306-4573},
doi = {https://doi.org/10.1016/0306-4573(85)90125-6},
url = {https://www.sciencedirect.com/science/article/pii/0306457385901256},
author = {Elliot Cole and Katherine W. McCain},
abstract = {Library networks have developed transaction processing software as automation tools for technical and public services, based on generic library models. Transaction processing systems are shown to be a flexible type of design, allowing users to introduce modifications which aid their individual objectives: 92% of user institutions reported they made local modifications to standard OCLC practices and procedures. These findings are discussed in terms of network models for decision making in software use; methodology of software user studies; and diffusion of innovation processes in post-implementation stages of transaction processing software.}
}
@article{BRUIJN198735,
title = {Simulation and Realisation of In-line Control Algorithms},
journal = {IFAC Proceedings Volumes},
volume = {20},
number = {12},
pages = {35-42},
year = {1987},
note = {IFAC Symposium on Simulation of Control Systems, Vienna, Austria, 22-26 September, 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)55603-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017556037},
author = {P.M. Bruijn and J. Cser and A.R.M. Soeterboek},
keywords = {Control engineering computer applications, simulation, real-time control, direct digital control},
abstract = {Two program packages are described for the simulation and the realisation in real time of control structures described by block diagrams. The blocks represent the algorithms, the control functions and the simulation of processes during the design stage. The topology is defined by the connections of the blocks. In the package MUSIC a set of standard blocks is available and the user can write in Fortran his own blocks using a standard software interface. The interaction between the user and the control structure is performed via a separate command interface program. A graphic display facility facilitates the evaluation of results and the comparison of a set of solutions. The whole system is supported by a user-friendly menu and runs on PDP-11 and microVAX computers. The second package, called FOCOP, is written in Forth and offers the possibility to combine the basic blocks in user-defined modules to form more complex transfer functions, which can be repeatedly used. Due to the choice of Forth it is relatively simple to implement the software on different computers. At the moment versions are available on 6502 and Z80 microprocessors and on PDP-11 minicomputers.}
}
@article{CASTRO20111113,
title = {Diagnosing and correcting design inconsistencies in source code with logical abduction},
journal = {Science of Computer Programming},
volume = {76},
number = {12},
pages = {1113-1129},
year = {2011},
note = {Special Issue on Software Evolution, Adaptability and Variability},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2010.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167642310001565},
author = {Sergio Castro and Coen {De Roover} and Andy Kellens and Angela Lozano and Kim Mens and Theo D’Hondt},
keywords = {Inconsistency management, Abductive reasoning, Logic meta programming},
abstract = {Correcting design decay in source code is not a trivial task. Diagnosing and subsequently correcting inconsistencies between a software system’s code and its design rules (e.g., database queries are only allowed in the persistence layer) and coding conventions can be complex, time-consuming and error-prone. Providing support for this process is therefore highly desirable, but of a far greater complexity than suggesting basic corrective actions for simplistic implementation problems (like the “declare a local variable for non-declared variable” suggested by Eclipse). We present an abductive reasoning approach to inconsistency correction that consists of (1) a means for developers to document and verify a system’s design and coding rules, (2) an abductive logic reasoner that hypothesizes possible causes of inconsistencies between the system’s code and the documented rules and (3) a library of corrective actions for each hypothesized cause. This work builds on our previous work, where we expressed design rules as equality relationships between sets of source code artifacts (e.g., the set of methods in the persistence layer is the same as the set of methods that query the database). In this paper, we generalize our approach to design rules expressed as user-defined binary relationships between two sets of source code artifacts (e.g., every state changing method should invoke a persistence method). We illustrate our approach on the design of IntensiVE, a tool suite that enables defining sets of source code artifacts intensionally (by means of logic queries) and verifying relationships between such sets.}
}
@article{BECKER20062340,
title = {Finite element-based analysis of shunted piezoelectric structures for vibration damping},
journal = {Computers & Structures},
volume = {84},
number = {31},
pages = {2340-2350},
year = {2006},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2006.08.067},
url = {https://www.sciencedirect.com/science/article/pii/S0045794906002859},
author = {Jens Becker and Oliver Fein and Matthias Maess and Lothar Gaul},
keywords = {Piezoelectric structure, Passive electrical network, Finite element analysis, Vibration damping},
abstract = {Piezoelectric patches shunted with passive electrical networks can be attached to a host structure for reduction of structural vibrations. This approach is frequently called “shunted piezo damping” and has the advantage of guaranteed stability and low complexity in implementation. For numerical treatment of such structures, a finite element modelling methodology is presented that incorporates both the piezoelectric coupling effects of the patches and the electrical dynamics of the connected passive electrical circuits. It allows direct computation of the achieved modal damping ratios as a major design criterion of interest. The damping ratios are determined from the eigenvalue problem corresponding to the coupled model containing piezoelectric structure and passive electrical circuit. The model includes local stiffening and mass effects as a result of the attached patches and, therefore, enables accurate prediction of the natural frequencies and corresponding modal damping ratios. This becomes crucial for choosing the patch thickness to achieve optimal modal damping for a given host structure. Additionally, structures with complex geometry or spatially varying material properties can easily be handled. Furthermore, the use of a finite element formulation for the coupled model of piezoelectric patches and a host structure facilitates design modifications and systematic investigations of parameter dependencies. In this paper, the impact of parameters of the passive electrical network on modal damping ratios as well as the variation of the patch thickness are studied. An application of this modelling method is realized by commercial software packages by importing fully coupled ANSYS© – models in MATLAB©. Afterwards, modal truncation is applied, the dynamic equations of the passive electrical network are integrated into the piezoelectric model and eigenvalue problems are solved to extract the increase in modal damping ratios. The numerical results are verified by experiments.}
}
@article{DUFOURD201563,
title = {Formal study of functional orbits in finite domains},
journal = {Theoretical Computer Science},
volume = {564},
pages = {63-88},
year = {2015},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2014.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S0304397514008330},
author = {Jean-François Dufourd},
keywords = {Formal specification, Functional orbits, Program correctness, Linked representation, Algebraic data type, Memory shape analysis, Computer-aided proof, Coq system},
abstract = {In computer science, functional orbits – i.e., tracks left by the iterations of a function – in a finite domain naturally appear at a high or a low level. This paper introduces a Coq logical orbit framework, the purpose of which is to help rigorously developing software systems with some complex data structures from specification to implementation. The result is a Coq library of orbit concepts formalized as definitions, lemmas and theorems. Most of them are inspired by our previous work in geometric modelling, where combinatorial hypermaps were used to describe surface subdivisions appearing in computational geometry problems, e.g., building convex hulls or Delaunay diagrams. Now, this domain remains a reference for us, but our results are drastically extended and usable in other computer science areas. The proof of Floyd's cycle-detection algorithm, known as “the tortoise and the hare”, confirms that point. The library contains operations to observe, traverse and update orbits – addition, deletion, mutation, transposition – with proofs of their behavior. It focuses on the important case where the involved function is a partial injection. In this case, it defines a connectivity relationship and evaluates the variation of the number of connected components during updates.}
}
@article{GLEZER200347,
title = {A conceptual model of an interorganizational intelligent meeting-scheduler (IIMS)},
journal = {The Journal of Strategic Information Systems},
volume = {12},
number = {1},
pages = {47-70},
year = {2003},
issn = {0963-8687},
doi = {https://doi.org/10.1016/S0963-8687(02)00034-3},
url = {https://www.sciencedirect.com/science/article/pii/S0963868702000343},
author = {Chanan Glezer},
keywords = {Meetings, Group-tasks, Meeting-scheduling, Interoperability, Human resource management (HRM), Calendars, Software agents, Inter-organizational systems (IOS)},
abstract = {This article proposes and evaluates a comprehensive agent-based architecture for an Interorganizational Intelligent Meeting-Scheduler. The article extends and generalizes the Intelligent Meeting-Scheduler conceptual model [EXPERSYS 95-Proc. Seventh Intl Conf. Artificial Intelligence Expert Syst. Appl. (1995) 279; J. Organizational Comput. Electron. Commerce, 9 (1999) 233] which focused on intraorganizational meeting scenarios. First, the article reviews several academic meeting-scheduling prototypes and commercial software packages. Based on this review, it is demonstrated that only an integrated approach that supports interoperability in all three dimensions of the meeting-scheduling problem (calendar, scheduling, and communication-management) can succeed in achieving interoperability among heterogeneous calendar and scheduling systems. The next part of the article provides a specification of an agent-based system that attempts to address the interoperability challenge. The specification comprises of the following elements: environment, behaviors, symbol-level, and knowledge-level architectures [IEEE Trans. Syst., Man, Cybernet. 25 (1995) 852]. The inter-organizational meeting-scheduling process is articulated as an iterative negotiation process where knowledge and symbol level units (‘the IIMS system’) interact with the system's end-users (‘the environment’) by exhibiting behaviors that address end-user requirements. The IIMS conceptual model is evaluated empirically and related to relevant literature on adoption difficulties of inter-organizational systems. It is evident that the IIMS faces a plethora of technological, organizational, sociological, behavioral, and psychological challenges that hinder its successful adoption. The article proposes several implementation tactics and guidelines in order to overcome these obstacles.}
}
@article{FERSTL199639,
title = {Job- and resource-management systems in heterogeneous clusters},
journal = {Future Generation Computer Systems},
volume = {12},
number = {1},
pages = {39-51},
year = {1996},
note = {Resource Management in Distributed Systems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(96)84677-2},
url = {https://www.sciencedirect.com/science/article/pii/0167739X96846772},
author = {F. Ferstl},
keywords = {Scheduling, Batch, Batch jobs, Parallel jobs, Job management, Resource management, Resource control, Load distribution, Load levelling, Workload management, Workload distribution, Parallel environment, Management framework, Resource management system, Codine},
abstract = {This paper gives an overview about the state-of-the-art in job management systems and about future challenges for such load distribution facilities. A brief history of the evolution of job management systems is followed by a definition of typical features and application areas for such systems. The example of the system Codine is used to outline a suitable architectural approach. Current implementations providing support for serial and parallel batch jobs with and without checkpointing and for interactive jobs are compared. The paper then discusses future requirements for job management software such as application programmer's interfaces (APIs), cluster-wide resource control, interfaces to system management frameworks, complex interdependent job nets, interfaces to mixed shared and distributed memory parallel environments as well as usage in the wide area network (WAN) context.}
}
@article{EKER2001271,
title = {Design and Simulation of Heterogeneous Control Systems using Ptolemy II},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {22},
pages = {271-276},
year = {2001},
note = {IFAC Conference on New Technologies for Computer Control (NTCC 2001), Hong Kong, 19-22 November 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32950-6},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017329506},
author = {Johan Eker and Chamberlain Fong and Jörn W. Janneck and Jie Liu},
keywords = {Simulators, real-time computers, embedded systems},
abstract = {ABSTRACT
Complex control systems are heterogeneous from both an implementation and a modeling perspective. Design and simulation environments for such systems need to integrate different component interaction styles, like differential equations, discrete events, state machines, dataflow networks, and real-time scheduling. This paper motivates the use of Ptolemy II software environment for mode ling and simulation of heterogeneous control systems. Ptolemy II advocates a component-based design methodology, and hierarchically integrates multiple models of computation, which can be used to capture different design perspectives. A Furuta pendulum control system is used as a motivating example. After designing a three-mode hybrid controller under idealized assumptions, implementation effects, like real-time scheduling and network protocols, are taken into consideration to achieve a more realistic simulation. The 3D animation package in Ptolemy II helps designers to visualize the control results. In this process of refining the design, components modeled in early phases can be reused.}
}
@article{NEUGEBAUER2017439,
title = {A parallelization approach for resource-restricted embedded heterogeneous MPSoCs inspired by OpenMP},
journal = {Journal of Systems and Software},
volume = {125},
pages = {439-448},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.08.069},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216301534},
author = {Olaf Neugebauer and Michael Engel and Peter Marwedel},
keywords = {Parallelization, Heterogeneous multiprocessor system-on-chip, Embedded systems},
abstract = {Future low-end embedded systems will make an increased use of heterogeneous MPSoCs. To utilize these systems efficiently, methods and tools are required that support the extraction and implementation of parallelism typically found in embedded applications. Ideally, large amounts of existing legacy code should be reused and ported to these new systems. Existing parallelization infrastructures, however, mostly support parallelization according to the requirements of HPEC systems. For resource-restricted embedded systems, different parallelization strategies are necessary to achieve additional non-functional objectives such as the reduction of energy consumption. HPC-focused parallelization also assumes processor, memory and communication structures different from low-end embedded systems and therefore wastes optimization opportunities essential for improving the performance of resource-constrained embedded systems. This paper describes a new approach and infrastructure inspired by the OpenMP API to support the extraction and implementation of pipeline parallelism, which is commonly found in complex embedded applications. In addition, advanced techniques to extract parallelism from legacy applications requiring only minimal code modifications are presented. Further, the resulting toolflow combines advanced parallelization, mapping and communication optimization tools leading to a more efficient approach to exploit parallelism for typical embedded applications on heterogeneous MPSoCs running distributed real-time operating systems.}
}
@article{KEUCHEL19911209,
title = {Identification and Control with Cadacs-PC},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {3},
pages = {1209-1214},
year = {1991},
note = {9th IFAC/IFORS Symposium on Identification and System Parameter Estimation 1991, Budapest, Hungary, 8-12 July 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)52515-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701752515X},
author = {U. Keuchel and Chr. Schmid},
keywords = {Computer-aided system design, computer software, identification, control system design},
abstract = {CADACS is a comprehensive CAD package designed to cover a wide range of control system engineering tasks. It contains modules for process identification, system analysis, controller design, simulation and controller implementation based on a broad variety of modern as well as classical approaches for SISO and MIMO systems. Classical curve fitting techniques applied to step or frequency responses or different parameter estimation techniques will lead in combination with model reduction to system descriptions which can be used for control system design. A rich set of utilities is supporting the identification task both for continuous- and discrete-time models in different representation forms. The general description of the system shows the ideas and methods standing behind the system. An illustrative example using a turbogenerator set comprises modelling and controller design and the implementation using the real-time suite of this package. The system can run on IBM-compatible personal computers.}
}
@article{LIM1996101,
title = {Advanced motion control of mechatronic systems via a high-speed dsp and a parallel processing transputer network},
journal = {Mechatronics},
volume = {6},
number = {1},
pages = {101-122},
year = {1996},
issn = {0957-4158},
doi = {https://doi.org/10.1016/0957-4158(95)00052-6},
url = {https://www.sciencedirect.com/science/article/pii/0957415895000526},
author = {S.Y. Lim and D.M. Dawson and P. Vedagarbha},
abstract = {A system for implementing an advanced motion controller using a floating-point digital signal processor (DSP) and a transputer-based parallel processing system is presented. The discussion includes a brief look at the advantages of using DSPs in real-time control applications and the architecture for integrating DSPs with parallel processing transputer networks to enhance computing power. The main focus of this paper is on the design of a system that combines the Texas Instruments TMS320C30 floating-point DSP with a parallel processing system based on Intel i860 RISC processors and Inmos transputers to achieve excellent real-time response and superior computational power. The software of the system is hosted in the popular MATLAB program which furnishes a simple user interface and programming environment. The system is ideal for implementing advanced motion control experiments that require high-speed floating-point computations as well as fast sampling rates. In addition to real-time experiment, the system also includes a simulation package to allow rapid verification of the user's program and control algorithm. A complex adaptive motor control experiment is presented to illustrate the application of the system.}
}
@article{FRAZIER1987300,
title = {Practical guidelines for the design of menus},
journal = {Computers & Industrial Engineering},
volume = {13},
number = {1},
pages = {300-303},
year = {1987},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(87)90101-X},
url = {https://www.sciencedirect.com/science/article/pii/036083528790101X},
author = {James M. Frazier and N.P. Cannon},
abstract = {It has been said that the greatest challenge for the future of computers in industrial engineering is not the further development of new systems or new architectures, rather it is the expansion of the usefulness of the powerful tools we already possess. The efficient use of a system or software package is dependent on the non-machine interface. The use of menus has long been viewed as a effective way of implementing this interface. But an inappropriate menu design can obscure useful functions and prevent the user from accessing the full power of a system. This paper discusses such menu features as size, shape, content and organization (layout). Cursor control and the use of texture formatting (highlighting, blinking, etc.) are also covered. A survey of the main principles of menu design as presented in the literature is discussed. This survey is followed by a description of an extensive menu-driven system developed by the authors. How the principles proposed by other researchers were integrated or adapted is discussed; as well as the introduction of new concepts. All this is condensed into a set of practical guidelines for menu designers.}
}
@incollection{HENSEL1986155,
title = {CADOCS—COMPUTER AIDED DESIGN ON MULTIVARIABLE CONTROL SYSTEMS—METHODS AND THE SOFTWARE PACKAGE},
editor = {P. MARTIN LARSEN and N.E. HANSEN},
booktitle = {Computer Aided Design in Control and Engineering Systems},
publisher = {Pergamon},
pages = {155-160},
year = {1986},
isbn = {978-0-08-032557-6},
doi = {https://doi.org/10.1016/B978-0-08-032557-6.50034-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325576500340},
author = {H. Hensel},
abstract = {A CAD-package serving for the design of discrete-time multivariable control systems is presented. Various design methods for different feedback controller types (DB, PID, SC) are described based on discrete-time parametric process models which are obtained by On-line identification methods. The concept of the software package contains an interpreter and a database offering high flexibility in the design and coordinating the design modules. A design example starting with the identification and ending with a DDC-implementation will be shown.}
}
@article{PUJARA2008363,
title = {Loss less real-time data compression based on LZO for steady-state Tokamak DAS},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {363-365},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0920379607005571},
author = {H.D. Pujara and Manika Sharma},
keywords = {Data acquisition, CAMAC, Long pulse, Steady state, Data compression},
abstract = {The evolution of data acquisition system (DAS) for steady-state operation of Tokamak has been technology driven. Steady-state Tokamak demands a data acquisition system which is capable enough to acquire data losslessly from diagnostics. The needs of loss less continuous acquisition have a significant effect on data storage and takes up a greater portion of any data acquisition systems. Another basic need of steady state of nature of operation demands online viewing of data which loads the LAN significantly. So there is strong demand for something that would control the expansion of both these portion by a way of employing compression technique in real time. This paper presents a data acquisition systems employing real-time data compression technique based on LZO. It is a data compression library which is suitable for data compression and decompression in real time. The algorithm used favours speed over compression ratio. The system has been rigged up based on PXI bus and dual buffer mode architecture is implemented for loss less acquisition. The acquired buffer is compressed in real time and streamed to network and hard disk for storage. Observed performance of measure on various data type like binary, integer float, types of different type of wave form as well as compression timing overheads has been presented in the paper. Various software modules for real-time acquiring, online viewing of data on network nodes have been developed in LabWindows/CVI based on client server architecture.}
}
@article{STAINOV1993741,
title = {Dynamic protocol configuration for multimedia networks},
journal = {Microprocessing and Microprogramming},
volume = {38},
number = {1},
pages = {741-748},
year = {1993},
note = {Proceedings Euromicro 93 Open System Design: Hardware, Software and Applications},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(93)90220-F},
url = {https://www.sciencedirect.com/science/article/pii/016560749390220F},
author = {Rumen Stainov},
abstract = {This paper presents an approach and environment for adaptive communication software. In contrast to existing systems the adaptivity is ensured not only by configuring of functional modules (e.g. inside a protocol stack), but also by varying their implementation and therefore their granularity and encapsulation. Thus, different module versions representing different degree of granularity and encapsulation can be designed at the language level and compiled as different alternatives organised in libraries. These libraries are used in run-time for a flexible handling of the trade-off between object encapsulation and its efficient implementation on the target communication system. The proposed environment is a light-weight framework implementing our approach. An example, called DYCE2 represents an adaptive transport system for multimedia networks configuring in run-time its protocol modules to provide an optimal, application-tailored quality of service.}
}
@article{OVERTOOM2020195,
title = {Assessing the impacts of shared autonomous vehicles on congestion and curb use: A traffic simulation study in The Hague, Netherlands},
journal = {International Journal of Transportation Science and Technology},
volume = {9},
number = {3},
pages = {195-206},
year = {2020},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2020.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S2046043020300265},
author = {Irene Overtoom and Gonçalo Correia and Yilin Huang and Alexander Verbraeck},
keywords = {Autonomous vehicles, Shared autonomous vehicles, Urban traffic, Curb use, Simulation},
abstract = {New developments in the automotive world have the power to change mobility, but because of high uncertainties, municipalities are adopting a wait-and-see attitude. Nonetheless, autonomous, connected and shared vehicle technologies are in a far stage of development and it is only a matter of time before shared autonomous vehicles (SAVs) enter urban traffic. This research aims to provide insights into the congestion effects of SAVs on urban traffic, focusing on the differences in microscopic behaviour from conventional cars, and to investigate which easy-to-implement solutions a municipality could apply to facilitate the new mix of traffic. This was researched by performing a simulation study, using the traffic simulation package Vissim and a case study of a network in the city of The Hague during the morning peak in 2040. Several SAV market penetration scenarios were tested: 0%, 3%, 25%, 50% and 100% SAV usage by travellers. Additionally, two network designs were implemented: dedicated lanes for SAVs and kiss & ride (K&R)-facilities. From the results, it was clear that while the autonomous driving capabilities of SAVs help reduce traffic congestion, they also have a negative effect by stopping on the curbside to drop off passengers, forming bottlenecks for other road users, and by circulating on the network using low capacity links. Below the line, this adds up to an overall negative effect on urban traffic congestion according to our results. The dedicated lanes design was unsuccessful at reducing this congestion caused by SAVs. The K&R design, however, was successful at reducing delays, but only for SAV penetration rates higher than 25%. These exact effects are not generalizable due to limitations in network size and simulation software. However, the results can be seen as indicative for planning purposes. Similar effects could be expected in cities where transport network companies (TNCs) such as Uber become exceptionally popular with non-autonomous cars. The advice for municipalities is to closely monitor the situation and to account for SAVs (and TNCs) in each new infrastructural project.}
}
@article{FRIBORG2011304,
title = {Rapid development of scalable scientific software using a process oriented approach},
journal = {Journal of Computational Science},
volume = {2},
number = {3},
pages = {304-313},
year = {2011},
note = {Social Computational Systems},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877750311000263},
author = {Rune Møllegaard Friborg and Brian Vinter},
keywords = {Communicating Sequential Processes, Many-core, Grid computing, Python},
abstract = {Scientific applications are often not written with multiprocessing, cluster computing or grid computing in mind. This paper suggests using Python and PyCSP to structure scientific software through Communicating Sequential Processes. Three scientific applications are used to demonstrate the features of PyCSP and how networks of processes may easily be mapped into a visual representation for better understanding of the process workflow. We show that for many sequential solutions, the difficulty in implementing a parallel application is removed. The use of standard multi-threading mechanisms such as locks, conditions and monitors is completely hidden in the PyCSP library. We show the three scientific applications: kNN, stochastic minimum search and McStas to scale well on multi-processing, cluster computing and grid computing platforms using PyCSP.}
}
@incollection{NOVAKOVIC20121412,
title = {Computer aided delivery of case-based learning activities in EBL within chemical engineering curriculum},
editor = {Ian David Lockhart Bogle and Michael Fairweather},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {30},
pages = {1412-1416},
year = {2012},
booktitle = {22nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-59520-1.50141-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044459520150141X},
author = {Katarina Novakovic and Michael Parr and Jarka Glassey},
keywords = {Enquiry based learning, computer aided learning, problem solving},
abstract = {Enquiry Based Learning (EBL) in chemical engineering education provides an opportunity to develop important professional attributes of innovative problem solving, team work, initiative and communication skills within the subject-specific content. Computer aided learning packages play an important role in this process, as they enable students to explore and gain experience of new software environments in subject specific context. The development of case studies delivered in Stages 1 and 2 and utilising a number of different software packages is described. The Stage 1 case study develops fundamental principles of chemical engineering (mass and energy balances, reaction kinetics, heat transfer, sustainability) whilst Stage 2 case studies concentrate on separation processes and the reactor engineering aspect at the same time requiring the application of the knowledge of statistics and design of experiments, respectively. Student evaluation via questionnaires, focus groups and comments from individual reflective reports, submitted as part of the evaluation of the Stage 1 case study activity, demonstrate the impact of these case studies on student learning and their behaviour. Students consider these case studies very useful in strengthening their knowledge of the relevant areas of chemical engineering curriculum as well as helping them develop skills they consider important from employability point of view. Their suggestions for further improvements have been implemented during summer 2011 and a new, student-centred self-help tutorial on HYSYS will be available for the new academic year.}
}
@article{DELASEN19859,
title = {Modified adaptive controllers for improving the transients in adaptive control},
journal = {Computer-Aided Design},
volume = {17},
number = {1},
pages = {9-14},
year = {1985},
issn = {0010-4485},
doi = {https://doi.org/10.1016/0010-4485(85)90004-1},
url = {https://www.sciencedirect.com/science/article/pii/0010448585900041},
author = {M. {de la Sen}},
keywords = {adaptive controllers, transients, optimization},
abstract = {This paper presents a program for the CAD of modified adoptive controllers for improving the transients in adaptive systems. Two techniques are analysed, based respectively on optimization tools and adaptive sampling. The software package has a modular structure. Each module has a specific task corresponding to the hardware design. In structuring the modules both expected hardware implementation facilities and ease of programming are taken into account.}
}
@article{QUARATI2016403,
title = {Delivering cloud services with QoS requirements: Business opportunities, architectural solutions and energy-saving aspects},
journal = {Future Generation Computer Systems},
volume = {55},
pages = {403-427},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15000539},
author = {Alfonso Quarati and Andrea Clematis and Daniele D’Agostino},
keywords = {Cloud computing, Economics, IT architecture},
abstract = {The flexible and pay-as-you-go computing capabilities offered by Cloud infrastructures are nowadays very attractive, and widely adopted by many organizations and enterprises. In particular this is true for those having periodical or variable tasks to execute, and, choose to not or cannot afford the expenses of buying and managing computing facilities or software packages, that should remain underutilized for most of the time. For their ability to couple the scalability offered by public service providers, with the wider Quality of Service (QoS) provisions and ad-hoc customizations provided by private Clouds, Hybrid Clouds (HC) seem a particularly appealing solution for customers requiring something more than the mere availability of the service. The paper firstly introduces a Cloud brokering system leveraging on a promising architectural approach, based on the use of a gateway toolkit. This approach provides noticeably advantages, both to customers and to Cloud Brokers (CB), for its ability to hide all the intricacies related to the management of powerful, but often complex and heterogeneous infrastructures like the Cloud. Moreover such approach, through customized interfaces, facilitates customers in accessing Cloud resources thus easing the tailored deployment and execution of their applications and workflows. The major contribution of this work is given by the analysis of a set of brokering strategies for Hybrid Clouds, implemented by a brokering algorithm, aimed at the execution of various applications subject to different user requirements and computational conditions. With the objective of firstly maximize both user satisfaction and CB’s revenues the algorithm also pursues profit increases through the reduction of energy costs by adopting energy saving mechanisms. A simulation model is used to evaluate performance, and the results show that differences among strategies depend on type and size of system loads and that the use of turn on and off techniques greatly improves energy savings at low and medium load rates thus indirectly increasing CB revenues without diminishing customers’ satisfaction.}
}
@article{GRIECO2017340,
title = {QuickFuzz testing for fun and profit},
journal = {Journal of Systems and Software},
volume = {134},
pages = {340-354},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302066},
author = {Gustavo Grieco and Martín Ceresa and Agustín Mista and Pablo Buiras},
keywords = {Testing, Fuzzing, Haskell, QuickCheck},
abstract = {Fuzzing is a popular technique to find flaws in programs using invalid or erroneous inputs but not without its drawbacks. At one hand, mutational fuzzers require a set of valid inputs as a starting point, in which modifications are then introduced. On the other hand, generational fuzzing allows to synthesize somehow valid inputs according to a specification. Unfortunately, this requires to have a deep knowledge of the file formats under test to write specifications of them to guide the test case generation process. In this paper we introduce an extended and improved version of QuickFuzz, a tool written in Haskell designed for testing unexpected inputs of common file formats on third-party software, taking advantage of off-the-self well known fuzzers. Unlike other generational fuzzers, QuickFuzz does not require to write specifications for the file formats in question since it relies on existing file-format-handling libraries available on the Haskell code repository. It supports almost 40 different complex file-types including images, documents, source code and digital certificates. In particular, we found QuickFuzz useful enough to discover many previously unknown vulnerabilities on real-world implementations of web browsers and image processing libraries among others.}
}
@article{GHIASISHIRAZI201743,
title = {RSCM technology for developing runtime-reconfigurable telecommunication applications},
journal = {Computer Standards & Interfaces},
volume = {51},
pages = {43-55},
year = {2017},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916301490},
author = {Kamaledin Ghiasi-Shirazi and Mahdi Mohseni and Majid Darvishan and Reza Yousefzadeh},
keywords = {runtime reconfiguration, application management, configuration management, telecommunication, subagent, XML technology},
abstract = {Runtime reconfiguration is a fundamental requirement of many telecommunication applications which also has been addressed by management standards like CMIP, 3GPP TS 32.602, and NETCONF. Two basic commands considered by these standards are CREATE and DELETE which operate on managed objects inside an application. The available configuration management technologies, like JMX, OSGi, and Fractal, do not support the CREATE and DELETE reconfiguration commands of the telecommunication standards. In this paper, we introduce a novel technology, called RSCM, for development of runtime reconfigurable applications complying with the telecommunication standards. The RSCM subagent takes the responsibility of loading the application from the configuration file, executing the runtime reconfiguration commands (including CREATE and DELETE), enforcing validity of the configuration state, and updating the configuration file according to the latest reconfiguration commands. We exploit the modular and object oriented features of the XML technology for storing the configuration state of a program in a configuration file. The software development process is tailored such that the design of XML schemas of managed classes is performed parallel to the design of software classes. In addition, a novel programming approach based on indirect referencing is proposed which allows safe and almost immediate deletion of managed objects at runtime. This indirect referencing mechanism affects the implementation of associations in class diagrams and prevents methods of a class to use the “this” pointer freely. The RSCM technology has been successfully used in several commercial telecommunication applications; including an SMS service center, an SMS gateway, and an SMS hub.}
}
@article{LEFOR1988268,
title = {Analysis of Chromium-51 release assay data using personal computer spreadsheet software},
journal = {Computers and Biomedical Research},
volume = {21},
number = {3},
pages = {268-275},
year = {1988},
issn = {0010-4809},
doi = {https://doi.org/10.1016/0010-4809(88)90032-8},
url = {https://www.sciencedirect.com/science/article/pii/0010480988900328},
author = {Alan T. Lefor and Seth M. Steinberg and Eric A. Wiebke},
abstract = {The Chromium-51 release assay is a widely used technique to assess the lysis of labeled target cells in vitro. We have developed a simple technique to analyze data from Chromium-51 release assays using the widely available LOTUS 1-2-3 spreadsheet software. This package calculates percentage specific cytotoxicity and lytic units by linear regression. It uses all data points to compute the linear regression and can determine if there is a statistically significant difference between two lysis curves. The system is simple to use and easily modified, since its implementation requires neither knowledge of computer programming nor custom designed software. This package can help save considerable time when analyzing data from Chromium-51 release assays.}
}
@article{YARMOHAMMADI201717,
title = {Mining implicit 3D modeling patterns from unstructured temporal BIM log text data},
journal = {Automation in Construction},
volume = {81},
pages = {17-24},
year = {2017},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516302825},
author = {Saman Yarmohammadi and Reza Pourabolghasem and Daniel Castro-Lacouture},
keywords = {Building information modeling, Data mining, Sequential pattern analysis, Design log files, Performance management},
abstract = {Building information modeling is instrumental in documenting design, enhancing customer experience, and improving product functionality in capital projects. However, good building models do not happen by accident, but rather as a result of a managed process that involves several participants from different disciplines and backgrounds. Effective management of this process requires an ability to closely monitor the modeling process and correctly measure modelers' performance. Nevertheless, existing methods of performance monitoring in building design practices lack an objective measurement system to quantify modeling progress. The widespread utilization of Building Information Modeling (BIM) tools presents a unique opportunity to retrieve granular design process data and conduct accurate performance measurements. As a building's 3D model is gradually developed, model generation software packages, such as Autodesk Revit, automatically create log files that record design activities. This paper investigates what information these log files contain and how one can extract and further analyze the information to provide insight into the design modeling process. The specific objectives of this study were to: (1) investigate the presence of implicit patterns in 3-D design log files; and (2) to empirically characterize the performance of modelers based on the time it takes them to execute similar modeling tasks. To fulfill these objectives, design log files provided by an international architecture and design firm were analyzed. Using a tailored text file parser, user-model interaction data including modeler characteristics, command type, and command time were extracted from the journal files. To identify implicit command execution patterns, a sequence mining algorithm based on Generalized Suffix Trees (GST) was implemented. It was shown that there is a statistically significant difference between the average time it takes modelers to execute each command sequence. This study extends the existing knowledge by proposing a novel methodology to extract meaningful patterns from time-stamped unstructured design log data. This research contributes to the state of practice by providing a better understanding of information embedded in design log files.}
}
@article{POO1999523,
title = {Design, implementation and performance study of reliable transactions in X.500 directory service},
journal = {Computer Communications},
volume = {22},
number = {6},
pages = {523-542},
year = {1999},
issn = {0140-3664},
doi = {https://doi.org/10.1016/S0140-3664(99)00019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0140366499000195},
author = {G.-S. Poo and Q. Zeng},
keywords = {X.500 Directory, Transaction processing, Replication, Directory shadowing},
abstract = {Replication of Directory information improves system performance and availability. This is provided in 1993 Standard through a shadowing mechanism. However, the standard specifies only weak consistency requirement. This is not satisfactory as some applications may require strong consistency with atomic (all-or-none) updates. This article describes the effort to incorporate the transaction function into the Directory in order to support the strong consistency requirement. An object-oriented approach is used to design and implement the system. Appropriate object models for the DUA and the DSA are developed to include the transaction capability. The atomic update is ensured by the transaction manager, which controls and coordinates the distributed transaction and the CCR ASE, which tracks and transfers the atomic transaction messages on single transaction branch. This is assisted by a resource manager consisting of a local database with appropriate APIs and the underlying database manager which, ensures the ACID properties of the atomic updates. Performance evaluation of the system implemented indicates a transaction overhead of 31% for commit case and 14% for rollback case. The bottleneck areas include the local database access, the network processing overhead and the size of replicas. The overheads can be reduced by employing advanced hardware and software. Thus, the usefulness of transaction capability outweighs the overhead consideration.}
}
@incollection{SCHMID1980625,
title = {CAD OF ADAPTIVE SYSTEMS},
editor = {M.A. CUENOD},
booktitle = {Computer Aided Design of Control Systems},
publisher = {Pergamon},
pages = {625-630},
year = {1980},
isbn = {978-0-08-024488-4},
doi = {https://doi.org/10.1016/B978-0-08-024488-4.50099-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080244884500991},
author = {Chr. Schmid},
abstract = {The design and realization of discrete adaptive control systems using a stability conception and only process input and output signals are discussed. It is shown that known methods are special cases, which can be obtained from a more general and systematic design based on hyperstability theory. This allows the development of a systematic CAD conception existing of theory, algorithms and software architecture. The realization on a process computer in a general purpose DDC-design and implementation package as an adaptive control software subsystem is sketched.}
}
@article{SALCIC1999385,
title = {IMECO: A Reconfigurable FPGA-based Image Enhancement Co-Processor Framework},
journal = {Real-Time Imaging},
volume = {5},
number = {6},
pages = {385-395},
year = {1999},
issn = {1077-2014},
doi = {https://doi.org/10.1006/rtim.1998.0134},
url = {https://www.sciencedirect.com/science/article/pii/S1077201498901346},
author = {Z. Salcic and J. Sivaswamy},
abstract = {In this paper, we present a way to improve the computational speed of image contrast enhancement using low-cost FPGA-based hardware primarily targeted to X-ray images. In particular, we consider an enhancement method that consists of filtering followed by histogram modification. Filtering is done via the high boost filter (HBF) which is based on unsharp masking, and the histogram modification which is based on global histogram equalization (GHE). An image enhancement co-processor, IMECO, concept is proposed that enables efficient hardware implementation of enhancement procedures and hardware/software co-design to achieve high-performance low-cost solutions. The co-processor runs on an FPGA prototyping ISA-bus board. At this stage it consists of two hardware functional units that implement HBF and GHE and can be downloaded onto the board sequentially or reside on the board at the same time. These units represent an embryo of virtual hardware units that form a library of image enhancement algorithms. These algorithms can be easily integrated into software templates. In our trials with chest X-ray images, performance improvement over software-only implementations is more than two orders of magnitude, thus providing real-time or near-real-time image enhancement as required in target applications.}
}
@article{LOCE19977,
title = {Logically Efficient Spatial Resolution Conversion Using Paired Increasing Operators},
journal = {Real-Time Imaging},
volume = {3},
number = {1},
pages = {7-16},
year = {1997},
issn = {1077-2014},
doi = {https://doi.org/10.1006/rtim.1996.0042},
url = {https://www.sciencedirect.com/science/article/pii/S107720149690042X},
author = {Robert P. Loce and Edward R. Dougherty and Ronald E. Jodoin and Michael S. Cianciosi},
abstract = {In a typical office setting, digital documents may be handled by a variety of devices and software packages that perform scanning, printing, display, transmission, and various forms of image processing. The devices that comprise the system may each operate at a different sampling resolution and spatial resampling must often be performed to allow porting of a digital image among the various devices. The primary purpose here is to design optimal integer resolution-conversion filters based onincreasingoperators rather than the more typically employednon-increasingoperators. Design and implementation of optimal increasing filters possess certain statistical advantages and, more importantly from the perspective of real-time imaging, there is often a great logic-cost savings in implementing an increasing as opposed to a non-increasing filter.}
}
@article{DESSIRIER2018181,
title = {A new scripting library for modeling flow and transport in fractured rock with channel networks},
journal = {Computers & Geosciences},
volume = {111},
pages = {181-189},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0098300417304569},
author = {Benoît Dessirier and Chin-Fu Tsang and Auli Niemi},
keywords = {Channel network, Groundwater flow, Solute transport, Graph theory},
abstract = {Deep crystalline bedrock formations are targeted to host spent nuclear fuel owing to their overall low permeability. They are however highly heterogeneous and only a few preferential paths pertaining to a small set of dominant rock fractures usually carry most of the flow or mass fluxes, a behavior known as channeling that needs to be accounted for in the performance assessment of repositories. Channel network models have been developed and used to investigate the effect of channeling. They are usually simpler than discrete fracture networks based on rock fracture mappings and rely on idealized full or sparsely populated lattices of channels. This study reexamines the fundamental parameter structure required to describe a channel network in terms of groundwater flow and solute transport, leading to an extended description suitable for unstructured arbitrary networks of channels. An implementation of this formalism in a Python scripting library is presented and released along with this article. A new algebraic multigrid preconditioner delivers a significant speedup in the flow solution step compared to previous channel network codes. 3D visualization is readily available for verification and interpretation of the results by exporting the results to an open and free dedicated software. The new code is applied to three example cases to verify its results on full uncorrelated lattices of channels, sparsely populated percolation lattices and to exemplify the use of unstructured networks to accommodate knowledge on local rock fractures.}
}
@article{SCHNEIDER197998,
title = {Design of a multifunctional communications software package},
journal = {Computer Communications},
volume = {2},
number = {3},
pages = {98-104},
year = {1979},
issn = {0140-3664},
doi = {https://doi.org/10.1016/0140-3664(79)90179-8},
url = {https://www.sciencedirect.com/science/article/pii/0140366479901798},
author = {Michael Schneider and Leo May},
abstract = {Data communications and computer networks have undergone phenomenal growth, resulting in the proliferation of hardware, software, and communications protocols. However, as needs change and new technologies are developed existing software frequently becomes incapable of handling the new configurations and therefore obsolete. The design and implementation of a new data communications software package, Commpac, designed to address these problems, is described. The package is modular and multifunctional, and supports the hierarchy of point-to-point communications protocols. In addition, it has been designed to adapt to system changes and to provide the software resources necessary for the development of new modules to cope with these changes.}
}
@article{GENAIDY1989455,
title = {A microcomputer-based ergonomic software for the design and evaluation of frequent manual lifting tasks},
journal = {Computers & Industrial Engineering},
volume = {16},
number = {3},
pages = {455-468},
year = {1989},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(89)90163-0},
url = {https://www.sciencedirect.com/science/article/pii/0360835289901630},
author = {A.M. Genaidy and M.M. Ayoub and J.S. Duggal},
abstract = {The main objective of this paper is to generate ergonomic software that can be used in the design and evaluation of manual lifting tasks so as to minimize the risk of injury. Specifically, this study is aimed at developing a microcomputer-based model for the design and evaluation of frequent manual lifting tasks built upon the concept of job severity index. The microcomputer-based software package is intended to be used by non-experts in the field of manual materials handling. Possible engineering and administrative controls are implemented in the software if human lifting abilities are exceeded. These controls are discussed at length in the main text. The software was written in AutoLISP for the IBM personal computer. The knowledge based of the software is built upon a set of two models which were developed in the present study. The models were based on 2736 observations. An example is given to illustrate the usage of the software.}
}
@article{KYRIAKARAKOS20123785,
title = {A fuzzy cognitive maps–petri nets energy management system for autonomous polygeneration microgrids},
journal = {Applied Soft Computing},
volume = {12},
number = {12},
pages = {3785-3797},
year = {2012},
note = {Theoretical issues and advanced applications on Fuzzy Cognitive Maps},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2012.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1568494612000646},
author = {George Kyriakarakos and Anastasios I. Dounis and Konstantinos G. Arvanitis and George Papadakis},
keywords = {Polygeneration, Microgrids, Fuzzy cognitive maps, Petri nets, Particle swarm optimization},
abstract = {Autonomous polygeneration microgrids (APM) are a relatively new approach in covering specific needs like power, potable water and fuel for transportation, in remote areas. This approach has been proved to be technically feasible nowadays and even present itself as an economically viable investment. The initial management system built for this approach is a simple ON/OFF supervisor which can make the APM operate, but not in an optimal way. The devices cannot be operated in part load and as a consequence there is little room for optimization. A combined fuzzy cognitive maps (FCMs)–petri nets (PN) approach has been developed for the energy management of such a system. The PN is used as an activator in the fuzzy cognitive map structure so as to enable different FCMs to be activated depending on the state of the microgrid. This combination forms an integrated approach to the energy management of the microgrid. Using this approach considerable optimization in the design and operation of the microgrid is possible. A methodology for simultaneous and interactive optimization of the energy management system along with the sizing of the various devices of the actual microgrid is implemented. A software platform consisting of TRNSYS, TRNOPT and GenOPT software packages was used for simulation and optimization. Particle swarm optimization is applied both for the sizing of the system and the optimization of the FCM weights and PN parameters. Two microgrids were designed, one based on the FCM–PN energy management system (FPEMS) and one on the ON/OFF approach. The results show that FPEMS manages the energy flows more effectively throughout the year which leads to a considerable decrease in the sizing of the various components of the microgrid.}
}
@article{RAUH2013570,
title = {Verified Stability Analysis for Interval-Based Sliding Mode and Predictive Control Procedures with Applications to High-Temperature Fuel Cell Systems},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {23},
pages = {570-575},
year = {2013},
note = {9th IFAC Symposium on Nonlinear Control Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130904-3-FR-2041.00145},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016317219},
author = {Andreas Rauh and Luise Senkel and Julia Kersten and Harald Aschemann},
keywords = {Interval arithmetic, sliding mode control, predictive control, fuel cell systems},
abstract = {In previous work, control-oriented models have been derived for solid oxide high-temperature fuel cell systems. In these models, interval variables have been used to describe uncertainty due to a limited knowledge about system parameters and to handle effects of electric load variations on the temperature distribution in the fuel cell stack module as well as bounded measurement uncertainty. To deal with these types of uncertainty both in the design of robust controllers and during their online usage, interval techniques can be employed successfully. These control procedures make use of the basic principles of either sliding mode control or predictive control. The corresponding algorithms and the prerequisites for their real-time capable implementation using software libraries for interval arithmetic and algorithmic differentiation are described in this paper. Experimental results show the efficiency of these control laws for a fuel cell test rig that is available at the Chair of Mechatronics at the University of Rostock.}
}
@article{ODONNCHA2016199,
title = {On the Efficiency of Executing Hydro-environmental Models on Cloud},
journal = {Procedia Engineering},
volume = {154},
pages = {199-206},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816318367},
author = {Fearghal O’Donncha and Emanuele Ragnoli and Srikumar Venugopal and Scott C. James and Kostas Katrinis},
keywords = {Cloud, HPC, Numerical modelling, Containers, Hydrodynamic model},
abstract = {Optimizing high-performance computing applications requires understanding of both the application and its parallelization approach, the system software stack and the target architecture. Traditionally, performance tuning of parallel applications involves consideration of the underlying machine architecture, including floating point performance, memory hierarchies and bandwidth, interconnect architecture, data placement – among others. The shift to the utility computing model through cloud has created tempting economies of scale across IT and domains, not leaving HPC as an exception as a candidate beneficiary. Nevertheless, the infrastructure abstraction and multi-tenancy inherent to cloud offerings poses great challenges to HPC workloads, requiring a dedicated study of applicability of cloud computing as a viable time-to-solution and efficiency platform. In this paper, we present the evaluation of a widely used hydro-environmental code, EFDC, on a cloud platform. Specifically, we evaluate the target parallel application on Linux containers managed by Docker. Unlike virtualization- based solutions that have been widely used for HPC cloud explorations, containers are more fit-for-purpose, sporting among others native execution and lightweight resource consumption. Many-core capability is provided by the OpenMP library in a hybrid configuration with MPI for cross-node data movement, and we explore the combination of these in the target setup. For the MPI part, the work flow is implemented as a data-parallel execution model, with all processing elements performing the same computation, on different sub-domains with thread-level, fine-grain parallelism provided by OpenMP. Optimizing performance requires consideration of the overheads introduced by the OpenMP paradigm such as thread initialization and synchronization. Features of the application make it an ideal test case for deployment on modern cloud architectures, including that it: 1) is legacy code written in Fortran 77, 2) has an implicit solver requiring non-local communication that poses a challenge to traditional partitioning methods, communication optimization and scaling and, 3) is a legacy code across academia, research organizations, governmental agencies, and consulting firms. These technical and practical considerations make this study a representative assessment of migrating legacy codes from traditional HPC systems to the cloud. We finally discuss challenges that stem from the containerized nature of the platform; the latter forms another novel contribution of this paper.}
}
@article{ROUSSEL199086,
title = {CRYStallize: A crystallographic symmetry display and handling subpackage in TOM/FRODO},
journal = {Journal of Molecular Graphics},
volume = {8},
number = {2},
pages = {86-88},
year = {1990},
issn = {0263-7855},
doi = {https://doi.org/10.1016/0263-7855(90)80087-V},
url = {https://www.sciencedirect.com/science/article/pii/026378559080087V},
author = {Alain Roussel and Juan-C. Fontecilla-Camps and Christian Cambillau},
keywords = {crystallographic symmetry, molecular replacement, crystal packing},
abstract = {We have implemented in TOM/FRODO a protein crystallographic symmetry display and handling package, called CRYStallize. This package is designed as an aid in solving protein structures by molecular replacement methods. It allows the rotation/translation solutions provided by molecular replacement programs to be checked in a fast and easy way. Using CRYStallize, approximate solutions can also be improved by manual modifications. Symmetry-related objects, represented as surfaces, can be generated and handled in the same way as the reference molecules, thus permitting an efficient analysis of crystal packing and site accessibility. This program is available in the TOM/FRODO software release, which runs on the Silicon-Graphics workstations.}
}
@article{KUMAR2000583,
title = {An automated design and assembly of interference-free modular fixture setup},
journal = {Computer-Aided Design},
volume = {32},
number = {10},
pages = {583-596},
year = {2000},
issn = {0010-4485},
doi = {https://doi.org/10.1016/S0010-4485(00)00032-4},
url = {https://www.sciencedirect.com/science/article/pii/S0010448500000324},
author = {A.Senthil kumar and J.Y.H. Fuh and T.S. Kow},
keywords = {Modular fixture, Automated fixture design, Machining interference, Cutter swept volume approach},
abstract = {This paper describes an automated modular fixture design system developed using a CAD-based methodology and implemented on a 3-D CAD/CAM software package. The developed automated fixture design (AFD) system automates the fixturing points determination and is integrated on top of the previously developed interactive and semi-automated fixture design systems. The determination of fixturing points is implemented in compliance with the fixturing principles that are formulated as heuristics rules to generate candidate list of points and then select the exact points from the list. Apart from determining the fixturing points automatically, the system is capable of producing cutting tool collision-free fixture design using its machining interference detection sub-module. The machining interference detection is accomplished through the use of cutter swept solid based on cutter swept volume approach. Therefore, using the developed AFD, an interference-free fixture design and assembly can be achieved in the possible shortest design lead-time.}
}
@article{ZWAHLEN1986141,
title = {The design of a user friendly engineering economy analysis package for a microcomputer},
journal = {Computers & Industrial Engineering},
volume = {11},
number = {1},
pages = {141-145},
year = {1986},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(86)90066-5},
url = {https://www.sciencedirect.com/science/article/pii/0360835286900665},
author = {Helmut T. Zwahlen and Gilbert H. Puetz},
keywords = {Engineering economy, software package, microcomputer, rate-of-return, Monte-Carlo simulation, present-worth, optimization, capital allocation, probabilistic analysis, decisions under risk},
abstract = {The design and features of a user friendly engineering economy analysis software package for the Apple II microcomputer is described. The software package is written in Applesoft Basic. An Apple II with 64k RAM, one or two disk drives and a line printer is required to use the software package. The package consists of five complementary programs: (1) Selection of the better of two alternatives using present-worth evaluations. (2) Conventional rate-of-return computations for a single project. (3) Selection from mutually exclusive alternatives using rate-of-return analysis. (4) Optimal capital allocation among several investment opportunities under risk. (5) Probabilistic after-tax economic analysis for a single project. All programs but (4) are based on the time value of money. Program (4) is a chance-constrained optimization model utilizing the Lagrange-multiplier technique. Program (5) implements depreciation, debt/equity financing, and taxes and performs a sequential after-tax Monte Carlo simulation over a specified life-span. The use of this software package does not require any previous computer or programming experience. A menu format is used. The software package allows the user to specify the shapes of the interest and inflation probability distributions on a year by year basis. The user has the option to specify the reinvestment rates. The package presents the results in tables and graphs. User acceptance has been excellent with this software package and this package is well suited for the use in an engineering economy class or for the practicing industrial engineer in industry.}
}
@article{PAPACHRISTOU198573,
title = {Generation and implementation of state machine controllers: A VLSI approach},
journal = {Microprocessing and Microprogramming},
volume = {16},
number = {2},
pages = {73-81},
year = {1985},
note = {Euromicro '85},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(85)90042-0},
url = {https://www.sciencedirect.com/science/article/pii/0165607485900420},
author = {Christos A Papachristou and Danny Cornett},
abstract = {This paper presents an approach to hierarchical design of state machine controllers from a high-level specification into PLA code suitable for VLSI. The idea is to use the Algorithmic State Machine (ASM) model as a basis for representing the state machine and performing state assignments in software. The algorithmic behavior of the controller is defined in block-structured form by means of a high-level design language. This descriptions are then compiled into state machine code via data structures. A PLA generation package is then used to produce state machine designs in silicon.}
}
@incollection{VUKOBRATOVIC1992441,
title = {PROGRAM PACKAGE FOR GENERATION OF CONTROL LAWS FOR ROBOT MANIPULATORS IN SYMBOLIC FORM},
editor = {I. TROCH and K. DESOYER and P. KOPACEK},
booktitle = {Robot Control 1991},
publisher = {Pergamon},
address = {Amsterdam},
pages = {441-446},
year = {1992},
series = {IFAC Symposia Series},
isbn = {978-0-08-041276-4},
doi = {https://doi.org/10.1016/B978-0-08-041276-4.50079-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080412764500790},
author = {M. Vukobratović and N. Kirćanski and A. Timčenko},
abstract = {Abstract
This paper presents a new program package for the generation of control laws for robot manipulators in symbolic form. Since the computational efficiency of the generated control laws is extremely high, the real-time implementation become possible even on low-cost microcomputers. This program package represents an extension of the previously developed program package SYM which is designed to generate efficient manipulator models (Timčenko, Kirćanski, Vukobratović 1991). The basic algorithm belongs to the class of customized algorithms that reduce the computational burden by taking into account the specific characteristics of the manipulator to be controlled. It generates the high-level program code for computing various control laws based on inverse robot dynamics. The program code is highly optimized from the standpoint of numerical complexity. It represents a series of assignment statements with simple float variables and constants. Matrix and vector computations, loops, and conditional branches are avoided. The software modules for generation of various kinds of control laws as well as for simulation of robot moving along a defined path are incorporated into SYM software package and demonstrated in this paper. Generated models and control laws are implemented in several robot controllers designed to drive a 6 degrees of freedom robots.}
}
@article{ALCORTAGARCIA2005231,
title = {ACTUATOR AND COMPONENT FAULT ISOLATION IN A FLUID CATALYTIC CRACKING UNIT},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {231-236},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.01614},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016376261},
author = {Efraín Alcorta-García and Plinio de León-Cantón and Oscar A.Z. Sotomayor and Darci Odloack},
keywords = {Linear model, observers, fault detection and isolation},
abstract = {This paper considers an observer-based approach for detection and isolation of actuator and component faults in a Fluid Catalytic Cracking (FCC) unit. Model based analytical redundancy is used on an augmented linearised model of the non-linear FCC unit to the isolation of faults. Some component faults are modelled and implemented in a software package to show the considered approach. The proposed design is tested using a non-linear simulation of the FCC unit model. The results show that the linear observer-based approach applied to an augmented state of the FCC to actuator and component fault detection and isolation can be used with success even if the non-linear behaviour of the FCC has some effect on the residuals.}
}
@article{LI2021101028,
title = {Stochastic exponential synchronization for delayed neural networks with semi-Markovian switchings: Saturated heterogeneous sampling communication},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {41},
pages = {101028},
year = {2021},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2021.101028},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X21000182},
author = {Xiaoqing Li and Sing Kiong Nguang and Kun She and Jun Cheng and Kaibo Shi and Shouming Zhong},
keywords = {Heterogeneous sampling communication mechanism (HSCM), Semi-Markovian switchings (SMSs), Actuator saturations, Chaotic neural networks (CNNs), Linear matrix inequalities (LMIs)},
abstract = {By implementing heterogeneous sampling communication mechanism, this article addresses the exponential synchronization issue of drive–response chaotic neural networks (CNNs) with interval time-varying delays by simultaneously taking into account the semi-Markovian switchings and saturating actuators. More specifically, a semi-Markovian jumping model whose transition rates (TRs) are not constant but depends on the sojourn time (ST) is introduced to characterize the stochastic changing among the interaction of CNNs, which makes the NNs model under consideration more suitable for some actual circumstances. More particularly, we assume that the sampling intervals are heterogeneous and time-varying, which may be more practical in real-life applications than homogeneous sampling policy. Additionally, by introducing some new terms, one novel time-dependent Lyapunov–Krasovskii function (LKF) is ingeniously constructed, which can fully capture the characteristic information of heterogeneous sampling pattern. Benefitting from the introduced relaxed free-weighting matrices (FWM) and resorting to the formed LKF, some sampling-interval-dependent sufficient conditions for controller design of the resulting semi-MJNNs error system are established and expressed by linear matrix inequalities (LMIs). These LMIs-based constraints can be effectively checked by utilizing the available software packages. Therein, the developed synchronization criteria dependent on both the lower and upper bounds of sampling periods, and the available information about the actual sampling pattern is fully considered. Ultimately, two numerical examples are provided to demonstrate the feasibility and practicability of our theoretical findings.}
}
@article{LI201762,
title = {An Investigation into the Interrelationship between Aircraft Systems and Final Assembly Process Design},
journal = {Procedia CIRP},
volume = {60},
pages = {62-67},
year = {2017},
note = {Complex Systems Engineering and Development Proceedings of the 27th CIRP Design Conference Cranfield University, UK 10th – 12th May 2017},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117300574},
author = {Tao Li and Helen Lockett},
keywords = {Aircraft Systems Integration, Final Assembly Process Design, Integrated CAD},
abstract = {Modern aircraft are more integrated with advanced systems functionalities, which result in ever-increasing aircraft complexity, further development difficulties and development delays. These system complexities are mostly in the form of system interactions that make it difficult to understand the overall system characteristics. At the early stages of final assembly line (FAL) design, one of the most important objectives is to arrange the installation and test tasks from components to sub-systems and systems in the proper sequence to meet the designed functions and prevent hazards from the integration process. Improper sequencing of the final assembly process will cause rework, time delays, cost and potential safety risk in development. In the field of final assembly line design, previous research has mostly focused on assembly line balancing or supply chain design based on structural parts assembly. However, these approaches do not consider the early final assembly line definition or test allocation for system functions. In this paper, the research proposes a method based on a systems engineering view and integrated computer aided design (CAD) to help better understand system interactions and generate viable final assembly process sequencing. This research aims to develop a concept of unified master data for final assembly design, which contains 3D geometrical CAD, system functions and interaction characteristics. The paper will present the methodology framework, key concepts and associated industrial software packages for implementation. The paper concludes with further discussion of an initial case study.}
}
@article{NAKHLE2021100323,
title = {Ready, Steady, Go AI: A practical tutorial on fundamentals of artificial intelligence and its applications in phenomics image analysis},
journal = {Patterns},
volume = {2},
number = {9},
pages = {100323},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100323},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001719},
author = {Farid Nakhle and Antoine L. Harfouche},
keywords = {algorithms, deep learning, explainable artificial intelligence, image analysis, integrated development environments, machine learning, phenomics, programming languages, software frameworks, software libraries},
abstract = {Summary
High-throughput image-based technologies are now widely used in the rapidly developing field of digital phenomics and are generating ever-increasing amounts and diversity of data. Artificial intelligence (AI) is becoming a game changer in turning the vast seas of data into valuable predictions and insights. However, this requires specialized programming skills and an in-depth understanding of machine learning, deep learning, and ensemble learning algorithms. Here, we attempt to methodically review the usage of different tools, technologies, and services available to the phenomics data community and show how they can be applied to selected problems in explainable AI-based image analysis. This tutorial provides practical and useful resources for novices and experts to harness the potential of the phenomic data in explainable AI-led breeding programs.}
}
@article{WALCZUCH199945,
title = {Using individual prefixes in B+-trees},
journal = {Journal of Systems and Software},
volume = {47},
number = {1},
pages = {45-51},
year = {1999},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(99)00023-0},
url = {https://www.sciencedirect.com/science/article/pii/S0164121299000230},
author = {Nikolaus Walczuch and Herbert Hoeger},
keywords = {B-trees, Page prefix, Individual prefix, Information retrieval, Software engineering},
abstract = {In 1985 the authors developed an original tree structure, based on B+-trees, in which each key keeps track of the prefix it shares with its immediate preceding key using only one byte. This is considered to be a substantial space and search-time saving approach. These kind of trees have been successfully implemented and have been used in applications such as information retrieval, general data bases, information systems, library administration systems and statistical analysis. This paper presents the structure of these trees, explains how the prefix is used to save space and time and gives a comparative performance study with respect to similar trees and approaches.}
}
@article{WEIS19879,
title = {Environmental testing of silicon photodetectors},
journal = {Reliability Engineering},
volume = {18},
number = {1},
pages = {9-21},
year = {1987},
issn = {0143-8174},
doi = {https://doi.org/10.1016/0143-8174(87)90048-5},
url = {https://www.sciencedirect.com/science/article/pii/0143817487900485},
author = {E.A. Weis and D. Caldararu and M.M. Snyder and N. Croitoru},
abstract = {The electrical parameters of silicon detectors were measured under various external influences (temperature cycling, humidity, salt atmosphere, etc.). The tests were designed and the data was analyzed by using the randomized block design method (implementing the software package SAS).}
}
@article{ESCALADAHERNANDEZ201987,
title = {Design and evaluation of a prototype of augmented reality applied to medical devices},
journal = {International Journal of Medical Informatics},
volume = {128},
pages = {87-92},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618312954},
author = {Paula Escalada-Hernández and Nelia {Soto Ruiz} and Leticia {San Martín-Rodríguez}},
keywords = {Medical device, Augmented Reality, Medical Informatics Application, Mobile device},
abstract = {Background
According to current legislation, medical devices have to incorporate all the necessary information to eliminate or greatly minimise any problem associated with their use. However, the physical capacity of the actual device's packaging may frequently not be enough to contain all this information. To address this limitation, this study aimed to design and evaluate a prototype app for mobile devices applying augmented reality technology. The main feature of this kind of technology is combining virtual images with images from the real world.
Methods
This work, carried out in Spain, was developed in three different phases. 1) Assessment of users’ needs: Through a focus group and an online questionnaire, information was obtained about the following aspects: type of medical devices likely to be included in the app, relevant information that should be included and format in which this information should be presented. 2) Development of the prototype: Considering all the functional features identified in the previous phase, the software was developed by a team of professionals specialised in AR technology and applying a user-centred model. 3) Evaluation of the software: functionality and usability were assessed by means of the think-aloud method.
Results
1) Assessment of users’ needs: a total of 11 nurses participated in the focus group and 280 healthcare professionals answered the questionnaire. Their findings showed that users consider that information about the following aspects of medical devices should be included in the app: instructions for use, indications for use, brief description of the device, special precautions and biocompatibility, image of the content with its components and meaning of icons. 2) Description of the prototype: Once the app has been launched, when the user scans the medical device with the mobile device camera, access to the home screen is activated, where three sections can be found: name of the medical device, image of the device and four icons which provide access to: a brief description of the device, a detailed description of it, the packaging iconography and a video about use of the device. 3) Evaluation of the software: the app was defined by users as “very intuitive”. They highlighted, as one of its main positive aspects, the chance to obtain information about the medical device just by scanning the object. Additionally, the evaluation performed through the think-aloud method identified potential improvements in the app. These improvements were subsequently implemented to make the prototype more functional.
Conclusion
Working with potential prototype users made it possible to identify information considered relevant for these users and to delve into the format which they consider more appropriate to show this information in the prototype. Our results show that AR technology can be used as support for clinical practice.}
}
@article{TYLUTKI2019103484,
title = {CardiacPBPK: A tool for the prediction and visualization of time-concentration profiles of drugs in heart tissue},
journal = {Computers in Biology and Medicine},
volume = {115},
pages = {103484},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.103484},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519303531},
author = {Zofia Tylutki and Jakub Szlęk and Sebastian Polak},
keywords = {PBPK, Cardiac safety assessment, Heart tissue, Computer application, Prediction},
abstract = {Background and objective
Prediction of drug concentration in heart tissue is important in terms of drug safety and efficacy. This work presents the Open-Source CardiacPBPK platform for the prediction of the time-concentration profile of drugs, which could potentially reduce the risk of drug development failure due to cardiotoxicity. The objective of the CardiacPBPK development is to accelerate and simplify the in-silico toxicological assessment of new drugs, and to provide supportive material for the research community to use.
Methods
The CardiacPBPK software provides a modular implementation of the PBPK model of heart tissue. It can be easily accessed via the Internet or installed locally. The graphical user interface and tabular design are easy to configure and use.
Results
CardiacPBPK is a tool designed to predict and visualize the time-concentration profiles of a parent compound, and one metabolite, in venous plasma and heart tissue after oral or intravenous drug administration. CardiacPBPK is built on the R-environment framework and supports shiny application features such as interactive visualization of the results, and web applications interface by default. A shiny application refers to a computer program created with the use of shiny package in R. The application is freely available at https://github.com/jszlek/CardiacPBPK and https://sourceforge.net/projects/cardiacpbpk/. This open-source application runs on all platforms supporting R-environment (Linux, Windows, Mac OS X, Solaris).
Conclusions
We demonstrate the application of CardiacPBPK by simulating the study of amitriptyline intoxication in the case of CYP2D6 genetic polymorphism.}
}
@article{OPDEBEECK2021111059,
title = {On the practice of semantic versioning for Ansible galaxy roles: An empirical study and a change classification model},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111059},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111059},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001564},
author = {Ruben Opdebeeck and Ahmed Zerouali and Camilo Velázquez-Rodríguez and Coen {De Roover}},
keywords = {Ansible, Infrastructure as code, Semantic versioning, Empirical study, Mining software repositories},
abstract = {Ansible, a popular Infrastructure-as-Code platform, provides reusable collections of tasks called roles. Roles are often contributed by third parties, and like general-purpose libraries, they evolve. Therefore, new releases of roles need to be tagged with version numbers, for which Ansible recommends adhering to the semantic versioning format. However, roles significantly differ from general-purpose libraries, and it is not yet known what constitutes a breaking change or the addition of a feature to a role. Consequently, this can cause confusion for clients of a role and new role contributors. To alleviate this issue, we perform an empirical study on semantic versioning in Ansible roles to uncover the types of changes that trigger certain types of version bumps. Our dataset consists of over 81000 version increments spanning upwards of 8500 Ansible roles. We design a novel structural model for these roles, and implement a domain-specific structural change extraction algorithm to calculate structural difference metrics. Afterwards, we quantitatively investigate the state of semantic versioning in Ansible roles and identify the most commonly changed elements. Then, using the structural difference metrics, we train a Random Forest classifier to predict applicable version bumps for Ansible role releases. Finally, we confirm our empirical findings with a developer survey. Our observations show that although most Ansible role developers follow the semantic versioning format, it appears that they do not always consistently follow the same rules when selecting the version bump to apply. Moreover, we find that the distinction between patch and minor increments is often unclear. Therefore, we use the gained insights to formulate a number of guidelines to apply semantic versioning on Ansible roles. These guidelines can be used by role developers to ensure a clear interpretation of the version increments.}
}
@article{MELSSEN1996431,
title = {Parallel processing of chemical information in a local area network — I. HYDRA: Concept, configuration, and implementation of parallel applications},
journal = {Computers & Chemistry},
volume = {20},
number = {4},
pages = {431-438},
year = {1996},
issn = {0097-8485},
doi = {https://doi.org/10.1016/0097-8485(95)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/0097848595000844},
author = {W.J. Melssen and E.P.P.A. Derks and M.L.M. Beckers and L.M.C. Buydens},
abstract = {Sophisticated software packages put an increasing demand on computer hardware. In local area networks, computational intensive programs can lower the performance of individual workstations to an unacceptable level. However, utilizing in a coarse grained sense the computing power of all hosts in such networks, offers the potential to achieve considerable improvements in execution speed within reasonable cost limits. Since conventional workstations are not designed to be used in a parallel configuration, the program HYDRA is developed to control and synchronize parallel processing in a local area network. Part I of this paper focuses on the technical aspects of HYDRA, i.e. configuration and implementation. The second and third parts describe two applications of the HYDRA package in the field of chemistry: using parallel genetic algorithms for the conformational analysis of nucleic acids, and parallel cross-validation of artificial neural networks.}
}
@article{JIMENEZ2000223,
title = {Supervised Real-time Control with PLCS and SCADA in Ceramic Plant},
journal = {IFAC Proceedings Volumes},
volume = {33},
number = {6},
pages = {223-228},
year = {2000},
note = {6th IFAC Workshop on Algorithms and Architectures for Real-Time Control 2000 (AARTC'2000), Palma de Mallorca, Spain, 15-17 May 2000},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)35474-5},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017354745},
author = {E. Jiménez and J.M. Miruri and F.J. {Martínez de Pisón} and M. Gil},
keywords = {Adaptive Control, Supervisory Control and Data Adquisition (SCADA), Industrial Control, Manufacturing Systems, Process Simulators, Real-Time System},
abstract = {This paper describes the design of an automatic control system in an industrial ceramics production plant of the latest generation. The automatic control is implemented on PLCs and is monitored through a powerful package of SCADA software. A PC supervisory real-time system is designed and installed to optimise the adaptive control devices to their intended purposes, or to take complete control of the automation if it detects system failures. In addition, a supervisory system captures the most significant control parameters, for later off-line analysis. Furthermore, a telecontrol system allows taking total control over the real-time automation via modem.}
}
@article{QIN2010934,
title = {An element implementation of the boundary face method for 3D potential problems},
journal = {Engineering Analysis with Boundary Elements},
volume = {34},
number = {11},
pages = {934-943},
year = {2010},
note = {Special issue on the advances in mesh reduction methods- In honor of Professor Subrata Mukherjee on the occasion of his 65th birthday},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2010.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0955799710001517},
author = {Xianyun Qin and Jianming Zhang and Guangyao Li and Xiaomin Sheng and Qiao Song and Donghui Mu},
keywords = {BEM, Geometric map, Surface element, Boundary face method, CAD software},
abstract = {This work presents a new implementation of the boundary face method (BFM) with shape functions from surface elements on the geometry directly like the boundary element method (BEM). The conventional BEM uses the standard elements for boundary integration and approximation of the geometry, and thus introduces errors in geometry. In this paper, the BFM is implemented directly based on the boundary representation data structure (B-rep) that is used in most CAD packages for geometry modeling. Each bounding surface of geometry model is represented as parametric form by the geometric map between the parametric space and the physical space. Both boundary integration and variable approximation are performed in the parametric space. The integrand quantities are calculated directly from the faces rather than from elements, and thus no geometric error will be introduced. The approximation scheme in the parametric space based on the surface element is discussed. In order to deal with thin and slender structures, an adaptive integration scheme has been developed. An adaptive method for generating surface elements has also been developed. We have developed an interface between BFM and UG-NX(R). Numerical examples involving complicated geometries have demonstrated that the integration of BFM and UG-NX(R) is successful. Some examples have also revealed that the BFM possesses higher accuracy and is less sensitive to the coarseness of the mesh than the BEM.}
}
@article{HORVAI198543,
title = {Software architecture of large distributed process control systems},
journal = {Annual Review in Automatic Programming},
volume = {13},
pages = {43-48},
year = {1985},
note = {13th IFAC/IFIP Workshop on Real Time Programming},
issn = {0066-4138},
doi = {https://doi.org/10.1016/0066-4138(85)90442-2},
url = {https://www.sciencedirect.com/science/article/pii/0066413885904422},
author = {M Horvai and L Gyimesi and A Horváth and G Juhász and K Kovács and I Sári and J Szlankó and E Tóth},
keywords = {Distributed process control, data base management system, event handling, data acquisition, digital control, man-machine communication, local area network},
abstract = {Users of process control applications need two kinds of information: data and events. Data is handled by the process control data base management system (PCDB), while events, i.e., software triggering, by the event handling means (EHM).Data and events are gained by the data acquisition and control package (DCP), and can be represented by the display communication system (DICOM). In a distributed real-time system a local area network interconnects the different control computers, so the system software tools are implemented in a distributed way too. The software architecture of a large distributed process control systems is presented in the paper.}
}
@article{SHAHROM2018191,
title = {A new low power multiplexer based ternary multiplier using CNTFETs},
journal = {AEU - International Journal of Electronics and Communications},
volume = {93},
pages = {191-207},
year = {2018},
issn = {1434-8411},
doi = {https://doi.org/10.1016/j.aeue.2018.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1434841117321635},
author = {Erfan Shahrom and Seied Ali Hosseini},
keywords = {Single Bit Ternary Multiplier, Two Supply Voltages, Low Power, CNTFET, Low PDP},
abstract = {In the following paper, a single bit ternary multiplier utilizing carbon nanotube field-effect transistor (CNTFET) has been presented. Almost in the ternary circuit design, only one supply voltage VDD is used and a voltage division circuit is activated to produce VDD/2 for logic ‘1’, So the direct current from VDD to ground increases the static power considerably. In This paper, using two supply voltages, VDD and VDD/2, the circuit is designed so as VDD/2 could be transmitted to output directly for logic ‘1’ to eliminate direct current from source to ground. This is provided by proper division of truth table and using two level output gates. Also for extending to multi bit multiplier in this way, three type of half adders and one full adder are designed using two supply voltages and removing direct current. The implementation for two bits is reported. The results of simulation, using Hspice software and Stanford 32 nm CNTFET library with the voltage of 0.9 (v), as expected, indicate much lower power dissipation and power delay product (PDP) in comparison with the previous works.}
}
@article{OLIVEIRA20181,
title = {Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions},
journal = {Journal of Systems and Software},
volume = {136},
pages = {1-18},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217302558},
author = {Juliana Oliveira and Deise Borges and Thaisa Silva and Nelio Cacho and Fernando Castor},
keywords = {Exception handling, Android, Robustness, Maintainability},
abstract = {All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs.}
}
@article{JUNG2007711,
title = {A variational level set approach for surface area minimization of triply-periodic surfaces},
journal = {Journal of Computational Physics},
volume = {223},
number = {2},
pages = {711-730},
year = {2007},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2006.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0021999106004700},
author = {Y. Jung and K.T. Chu and S. Torquato},
abstract = {In this paper, we study triply-periodic surfaces with minimal surface area under a constraint in the volume fraction of the regions (phases) that the surface separates. Using a variational level set method formulation, we present a theoretical characterization of and a numerical algorithm for computing these surfaces. We use our theoretical and computational formulation to study the optimality of the Schwartz primitive (P), Schwartz diamond (D), and Schoen gyroid (G) surfaces when the volume fractions of the two phases are equal and explore the properties of optimal structures when the volume fractions of the two phases are not equal. Due to the computational cost of the fully three-dimensional shape optimization problem, we implement our numerical simulations using a parallel level set method software package.}
}
@article{GUO2000403,
title = {Development of a Windows-based indoor air quality simulation software package},
journal = {Environmental Modelling & Software},
volume = {15},
number = {4},
pages = {403-410},
year = {2000},
issn = {1364-8152},
doi = {https://doi.org/10.1016/S1364-8152(00)00020-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364815200000207},
author = {Zhishi Guo},
keywords = {Indoor air, Exposure, Mass transfer, Microenvironment},
abstract = {A Microsoft Windows-based indoor air quality (IAQ) simulation software package has been developed and has completed a small-scale beta test and quality assurance review. Tentatively named Simulation Tool Kit for Indoor Air Quality and Inhalation Exposure, or STKi for short, this package complements and supplements existing IAQ simulation packages and is designed mainly for advanced users. STKi Version 1 consists of a general-purpose simulation program and four stand-alone, special-purpose programs. The general-purpose program performs multi-zone, multi-pollutant simulations and allows gas-phase chemical reactions. With a large collection of models for sources, sinks, and air filters/cleaners, it can perform simulations for a wide range of indoor air pollution scenarios. The special-purpose programs implement fundamentally based models, which are often excluded from existing IAQ simulation programs despite their improved performance over statistical models. In addition to performing conventional IAQ simulation, which generates time–concentration profiles, STKi can estimate the adequate ventilation rate when certain air quality criteria are given, a unique feature useful for product stewardship and risk management. STKi will be developed in a cumulative manner. More special-purpose simulation programs will be added to the package. Key numerical methods used in STKi are discussed. Ways to convert the STKi programs into language-independent simulation modules that can be used by multi-pathway exposure models are also being explored.}
}
@article{GEN1989525,
title = {Algorithms for solving large-scale 0–1 goal programming and its application to reliability optimization problem},
journal = {Computers & Industrial Engineering},
volume = {17},
number = {1},
pages = {525-530},
year = {1989},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(89)90117-4},
url = {https://www.sciencedirect.com/science/article/pii/0360835289901174},
author = {M. Gen and K. Ida and M. Sasaki and J.U. Lee},
abstract = {Goal programming(GP) is one of the most widely used Operations Research/Management Science/Industrial Engineering techniques for solving multiple criteria decision making (MCD M) problems. In the realistic decision making problems, many GP problems are involved a large number of 0–1 decision variables and a special type of system structures. Inthis paper, we develop a computational algorithm for solving 0–1 goal programming with a generalized upper bounding (GUB) structures. From the views of the computational experience and storage requirement, we implemented an efficient software package for UN IX workstations in which we called it micro 0–1 GP(GUB). In the micro 0–1 GP(GUB) developed here, the GUB structures would be effectively handled and we designed user-friendly GP data entry subsystem. As a real-world 0–1 goal programming problem with the GUB structures, we demonstrate an optimization problem of system reliability for allocating redundant units by the micro 0–1 GP(GUB) software package on an UN IX system.}
}
@article{ALLESINA2004337,
title = {WAND: an ecological network analysis user-friendly tool},
journal = {Environmental Modelling & Software},
volume = {19},
number = {4},
pages = {337-340},
year = {2004},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2003.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364815203002147},
author = {Stefano Allesina and Cristina Bondavalli},
keywords = {Network analysis, Food web, Ecosystem, Input–output, ECOPATH, NETWRK},
abstract = {Ecological network analysis is a modelling approach that requires one to represent ecosystems as networks wherein matter/energy enters compartments (species-trophospecies), is exchanged between them and finally leaves the system as dissipation or usable export. The systematic analysis of the ecosystem flow networks is comprised of several techniques, aiming to interpret ecosystems structure and functioning and to estimate their size and developmental stage. The implementation and improvement of this modelling technique depends strongly upon the availability of appropriate software and access to technical expertise to easily and speedily execute the huge amount of computation required. With the package WAND presented in this manuscript the authors intended to promulgate a new, more user-friendly version of some existing environmental software, NETWRK, to analyze networks of trophic exchanges in ecosystems. A Windows—compatible version of NETWRK has been developed, along with ancillary software to facilitate the estimation of new networks and the translation of existing data sets between text and Excel formats.}
}
@incollection{CHEDID20001580,
title = {Chapter 324 - An Educational Laboratory Model on Hybrid Renewable Energy Systems},
editor = {A.A.M. Sayigh},
booktitle = {World Renewable Energy Congress VI},
publisher = {Pergamon},
address = {Oxford},
pages = {1580-1583},
year = {2000},
isbn = {978-0-08-043865-8},
doi = {https://doi.org/10.1016/B978-008043865-8/50324-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008043865850324X},
author = {R.B. Chedid and F.B. Chaaban and H. Kanj},
abstract = {Publisher Summary
This chapter deals with an educational laboratory model on hybrid renewable energy systems presents a laboratory model to help students understand the design and operation of hybrid renewable energy systems composed of wind, solar, diesel and battery units. This educational tool is composed of three modules: the software, the hardware and the interface modules. The software performs two tasks. It uses linear programming to optimize system components, and it ensures optimal energy flow via a designed controller. The hardware is a reduced model of a real green village, and the interface is based on a dynamic-link library using Visual C++ to do the connection between the hardware and the computer. The chapter discusses discuss the design and implementation of a laboratory model to help students understand the design and operation of hybrid renewable energy systems. This educational tool is based on three modules: the software, the hardware and the interface between the computer and the user operated hardware.}
}
@article{HOUSKA20112279,
title = {An auto-generated real-time iteration algorithm for nonlinear MPC in the microsecond range},
journal = {Automatica},
volume = {47},
number = {10},
pages = {2279-2285},
year = {2011},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2011.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0005109811003918},
author = {Boris Houska and Hans Joachim Ferreau and Moritz Diehl},
keywords = {Nonlinear model predictive control, Automatic C-code generation, Real-time algorithms},
abstract = {In this paper, we present an automatic C-code generation strategy for real-time nonlinear model predictive control (NMPC), which is designed for applications with kilohertz sample rates. The corresponding code export module has been implemented within the software package ACADO Toolkit. It is capable of exporting fixed step-size integrators together with their sensitivities as well as a real-time Gauss–Newton method. Here, we employ the symbolic representation of optimal control problems in ACADO in order to auto-generate plain C-code which is optimized for final production. The exported code has been tested for model predictive control scenarios comprising constrained nonlinear dynamic systems with four states and a control horizon of ten samples. The numerical simulations show a promising performance of the exported code being able to provide feedback in much less than a millisecond.}
}
@article{MARTINEZLLARIO20112314,
title = {Design of a Java spatial extension for relational databases},
journal = {Journal of Systems and Software},
volume = {84},
number = {12},
pages = {2314-2323},
year = {2011},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2011.06.072},
url = {https://www.sciencedirect.com/science/article/pii/S0164121211001695},
author = {J. Martinez-Llario and M. Gonzalez-Alcaide},
keywords = {Spatial database, PostGIS, FOSS, Java, GIS, OGC},
abstract = {Jaspa (Java Spatial) is a novel spatial extension for relational database management systems (RDBMSs). It is the result of a research project that aims to accomplish two goals: firstly, to fill the absence in the Free and Open Source Software (FOSS) world of a solid Java-based alternative to PostGIS, the leading spatial extension written in C. Secondly, to exploit the advantages of Java and the Java geospatial libraries over C in terms of portability and easiness to extend. Java programming for geospatial purposes is a flowering field and similar solutions to Jaspa have recently appeared, but none of them can equate with PostGIS due to lack of functionalities. Jaspa currently implements almost all PostGIS functionality. The next step will be the enrichment of the software with more sophisticated features: storage of spatial data in a topological structure within the RDBMS, cluster tolerance and geodetic functionalities. Jaspa is being developed at the Department of Cartographic Engineering, Geodesy and Photogrammetry of the Universidad Politécnica de Valencia and it has been published under an Open Source license on the OSOR.eu repository. This paper has been written by its creators with the aim of introducing users to its main capabilities.}
}
@article{DENEUX2013746,
title = {Establishment of a Model for a Combined Heat and Power Plant with ThermosysPro Library},
journal = {Procedia Computer Science},
volume = {19},
pages = {746-753},
year = {2013},
note = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.06.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913007060},
author = {O. Deneux and B. El Hafni and B. Péchiné and E. {Di Penta} and G. Antonucci and P. Nuccio},
keywords = {Modelling, simulation, combined heat and power plant.},
abstract = {The Simulation and Information Technologies for Power Generation System Department (STEP) has developed a methodology, as part of the framework of an EDF R&D project, to model and optimize energy systems for the associated companies with the aim of highlighting the possibility of using modelling tools to optimize energy systems. Dymola software, a commercial implementation of Modelica, which was developed by Dassault Systemes has been employed. A library, called ThermosysPro and developed by EDF R&D, has in particular been used. The energy system presented in the paper is a Combined Heat and Power plant (CHP), managed by Fenice SpA, which has been designed to supply electric and thermal energy to a food factory near Parma (Italy). This kind of system can be regulated over a large power range and, as a consequence, the CHP plant can supply the total amount of thermal energy required by the user. From a comparison of the experimental data and the simulation results, it can be seen that the behaviour of the turbo gas and the steam turbine approximately follows the results of the performance test at full load over a wide temperature range. As far as the performance at partial load, the gas turbine Heat Rate and the heat recovery steam generator (HRSG) performance are concerned, the simulation results and the actual CHP plant behaviour again appear to be in good agreement.}
}
@article{SAAD201547,
title = {Hybrid Coordination Strategy of a Group of Cooperating Autonomous Underwater Vehicles},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {5},
pages = {47-52},
year = {2015},
note = {3rd IFAC Workshop on Multivehicle Systems},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.06.462},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315007016},
author = {S. Ben Saad and B. Zerr and I. Probst and F. Dambreville},
keywords = {Multi-robot cooperation, coordination, formation control, control architectures, visual control simulator},
abstract = {In the underwater environment, the needs of data acquisition have significantly increased over the last decades. As electromagnetic waves show poor propagation in sea water, acoustical sensing is generally preferred. However, the emergence of small and low cost autonomous underwater vehicles (AUV) allow for rethinking the underwater use of optical sensors. Their small coverage can be significantly improved by using a fleet of coordinated underwater robots. This paper presents a strategy to coordinate a group of robots in order to systematically survey the seabed and detect small objects or singularities. The proposed hybrid coordination strategy is based on two main modes. The first mode relies on a swarm algorithm to organize the team in geometrical formation. In the second mode, the group formation is maintained using a hierarchical coordination. A finite state machine controls the high level hybrid strategy by defining the appropriate coordination mode according to the evolution of the mission. Before the sea validation, the behavior and the performance of the hybrid coordination strategy are first evaluated in simulation. The control of individual robots relies on visual servoing, implemented with the OpenCV library, and the simulation tool is based on Blender software. The dynamics of the robots has been implemented in a realistic way in Blender using the Bullet solver and the estimated hydrodynamic coefficients. This paper presents and discusses the preliminary results of the hybrid coordination strategy applied on a fleet of 3 AUVs.}
}
@article{RUIZ2020104098,
title = {Starviewer and its comparison with other open-source DICOM viewers using a novel hierarchical evaluation framework},
journal = {International Journal of Medical Informatics},
volume = {137},
pages = {104098},
year = {2020},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104098},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619301108},
author = {Marc Ruiz and Adrià Julià and Imma Boada},
keywords = {DICOM viewer, Open-source, Software comparison, Medical imaging},
abstract = {Methods
The aim of the paper is twofold. First, we present Starviewer, a DICOM viewer developed in C++ with a core component built on top of open-source libraries. The viewer supports extensions that implement functionalities and front-ends for specific use cases. Second, we propose an adaptable evaluation framework based on a set of criteria weighted according to user needs. The framework can consider different user profiles and allow criteria to be decomposed in subcriteria and grouped in more general categories making a multi-level hierarchical structure that can be analysed at different levels of detail to make scores interpretation more comprehensible.
Results
Different examples to illustrate Starviewer functionalities and its extensions are presented. In addition, the proposed evaluation framework is used to compare Starviewer with four open-source viewers regarding their functionalities for daily clinical practice. In a range from 0 to 10, the final scores are: Horos (7.7), Starviewer (6.2), Weasis (6.0), Ginkgo CADx (4.1), and medInria (3.8).
Conclusions
Starviewer provides basic and advanced features for daily image diagnosis needs as well as a modular design that enables the development of custom extensions. The evaluation framework is useful to understand and prioritize new development goals, and can be easily adapted to express different needs by altering the weights. Moreover, it can be used as a complement to maturity models.}
}
@article{VANCUTSEM1987387,
title = {Design and Implementation of an Advanced State Estimation Software},
journal = {IFAC Proceedings Volumes},
volume = {20},
number = {6},
pages = {387-392},
year = {1987},
note = {IFAC Symposium on Power Systems and Power Plant Control, Beijing, PRC, 12-15 August 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)59256-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017592563},
author = {Th. {Van Cutsem} and L. Mili and Ph. Vandeloise},
keywords = {Power system control, data processing, state estimation, observability, bad data analysis},
abstract = {This paper describes the design of an advanced state estimation software. The heart of this package is a fast decoupled constant gain matrix estimator. Besides the estimation itself, two satellite functions - viz. observability and bad data analyses - have been developed. The observability analysis is performed on a topological basis and a systematic observability restoration procedure is included. The treatment of bad data is carried out through the “hypothesis testing identification” method which definitely departs from classical schemes. The software has been implemented on a 60-node system monitored by a Belgian area control center.}
}
@article{MARI2009844,
title = {A computational system for uncertainty propagation of measurement results},
journal = {Measurement},
volume = {42},
number = {6},
pages = {844-855},
year = {2009},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2009.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0263224109000153},
author = {Luca Mari},
keywords = {Uncertainty propagation, Automatic differentiation, Computational methods for measurement},
abstract = {This paper discusses some design issues in the implementation of the law of uncertainty propagation according to an automatic differentiation strategy in the context of a simulation engine supporting the construction and the interactive testing of models of dynamic systems. The proposed solution propagates not only the partial derivatives, as usual in automatic differentiation, but also the input uncertainties, so to make their various modifications visible to the user of the evaluation system, and give him the opportunity to analyze the partial contributions to the standard uncertainty of the output measurands. A tool for uncertainty propagation in a general, user-oriented, computational system, instead of a software library or a dedicated system, makes uncertainty propagation transparently computable also for vector/matrix measurands, even in the case of dynamic systems, and makes uncertainty evaluation an inherent component of computational processes instead of an optional, ad hoc, addendum to them.}
}
@article{GALANIN2018483,
title = {Implementation of an iterative algorithm for the coupled heat transfer in case of high-speed flow around a body},
journal = {Computers & Fluids},
volume = {172},
pages = {483-491},
year = {2018},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2018.03.048},
url = {https://www.sciencedirect.com/science/article/pii/S004579301830152X},
author = {M.P. Galanin and V.T. Zhukov and N.V. Klyushnev and K.S. Kuzmina and V.V. Lukin and I.K. Marchevsky and A.S. Rodin},
keywords = {Heat transfer, Coupled problem, Numerical experiment, Supersonic flow, Shock wave},
abstract = {The results of investigation of a numerical technique for coupled heat transfer problem solving for an atmospheric supersonic flying vehicle and flow around it are presented. An iterative numerical algorithm and software package are developed for heat transfer simulation in a flying vehicle structure during its motion in the atmosphere. The convergence of variants of the iterative process for solution matching on the surface with ideal thermal contact is studied. The results of the numerical experiment confirm the obtained theoretical estimates.}
}
@article{BRUNNER1988167,
title = {VIPER: a general-purpose digital image-processing system applied to video microscopy},
journal = {Computer Methods and Programs in Biomedicine},
volume = {26},
number = {2},
pages = {167-181},
year = {1988},
issn = {0169-2607},
doi = {https://doi.org/10.1016/0169-2607(88)90042-9},
url = {https://www.sciencedirect.com/science/article/pii/0169260788900429},
author = {Manfred Brunner and Werner Ittner},
keywords = {Image processing, Video image, Image acquisition, Strobe, Flashlight illumination},
abstract = {This paper describes VIPER, the video image-processing system Erlangen. It consists of a general purpose microcomputer, commercially available image-processing hardware modules connected directly to the computer, video input/output-modules such as a TV camera, video recorders and monitors, and a software package. The modular structure and the capabilities of this system are explained. The software is user-friendly, menu-driven and performs image acquisition, transfers, greyscale processing, arithmetics, logical operations, filtering, display, colour assignment, graphics, and a couple of management functions. More than 100 image-processing functions are implemented. They are available either by typing a key or by a simple call to the function-subroutine library in application programs. Examples are supplied in the area of biomedical research, e.g. in in-vivo microscopy.}
}
@incollection{SILLS2002231,
title = {Chapter 6 - Web Development Using XML and ASP.NET},
editor = {Adam Sills and Mesbah Ahmed and  Dotthatcom.com and Frank Boumphrey and Jonothon Ortiz},
booktitle = {XML .NET Developer's Guide},
publisher = {Syngress},
address = {Rockland},
pages = {231-282},
year = {2002},
isbn = {978-1-928994-47-3},
doi = {https://doi.org/10.1016/B978-192899447-3/50009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781928994473500093},
author = {Adam Sills and Mesbah Ahmed and  Dotthatcom.com and Frank Boumphrey and Jonothon Ortiz},
abstract = {Publisher Summary
This chapter discusses the online aspects of XML using ASP.NET describing how XML is a vital part of online applications through the use of multiple examples such as an online catalog. Rather than just being ASP 4 or an incremental upgrade, ASP.NET is a complete rewrite from the ground up, using all advanced features that .NET makes available. It takes advantage of all that .NET has to offer, including support for around 20 or more .NET languages, from C# to Perl .NET, and the full set of .NET Framework software libraries. Web applications written in ASP.NET are fast, efficient, manageable, scalable, and flexible, and above all, easy to understand and to code. With ASRNET there is a true choice of languages. All the .NET languages have access to the same foundation class libraries, the same type of systems, equal object orientation and inheritance capabilities, and full interoperability with existing COM components. ASRNET incorporates all the important standards, such as XML and SOAR. In addition, with ADO.NET and the foundation class libraries, they are arguably easier to implement than in any other technology, including Java. An ASRNET programmer still only needs a computer with Notepad and the ability to FTP to write ASP code, but now with the .NETFramework command-line tools and the platform's XML-based configuration, this is truer than before. In the past, the limitations of ASP scripting meant components were required for functionality reasons, not just for architectural reasons. ASRNET has access to the same functionality and uses the same languages in which components can be created, so now components are an architectural choice only.}
}
@article{VILLA2000169,
title = {Design of multi-paradigm integrating modelling tools for ecological research},
journal = {Environmental Modelling & Software},
volume = {15},
number = {2},
pages = {169-177},
year = {2000},
issn = {1364-8152},
doi = {https://doi.org/10.1016/S1364-8152(99)00032-8},
url = {https://www.sciencedirect.com/science/article/pii/S1364815299000328},
author = {Ferdinando Villa and Robert Costanza},
keywords = {Multi-paradigm ecological modelling, Remote simulation control, Simulation interface design, Model coordination},
abstract = {Integrating modelling tools allow different modelling paradigms to coexist and cooperate in the same simulation model. The need for such tools in ecological modelling is due to the high level of complexity of ecological and environmental decision-making problems, their multiple scales of description, the diversity of the available approaches, and the size and heterogeneity of the available datasets. This article discusses problems and perspectives in developing integrating modelling tools and introduces the Simulation Network Interface (SNI), a software package for easy coordination of different existing simulation models. The interface allows the coordination of independent simulation models residing on different machines into higher-level, multi-paradigm, distributed simulation, with minimal recoding efforts of existing models. The interface can also be used to easily provide a remote interface to simulation or data retrieval services running on different architectures. As examples of its application, we describe three ongoing projects using the SNI: (1) the integration of Swarm, an agent-based simulation toolkit, with the Spatial Modelling Environment (SME), a process-based spatial simulation toolkit; (2) the straightforward implementation of a GIS-based spatial data repository for network-based data retrieval and manipulation; and (3) a network-based calibration service for complex simulation models.}
}
@article{HOFMANN199423,
title = {Intelligent measurements — new solutions for old problems},
journal = {Measurement},
volume = {13},
number = {1},
pages = {23-37},
year = {1994},
issn = {0263-2241},
doi = {https://doi.org/10.1016/0263-2241(94)90074-4},
url = {https://www.sciencedirect.com/science/article/pii/0263224194900744},
author = {D. Hofmann},
keywords = {Intelligent measurements, Quality measurements, Quality control, Quality assurance, Quality management, Software technology, Object-oriented programming},
abstract = {Aim of the paper is to show that intelligent measurements make measurement problem solutions system-open, cost-effective, flexible and fast. Intelligent measurements have to be supported by challenging intellectual work. The design of the solution procedures of measurement problems is getting priority. Standardized, modularized, windows-driven complex software packages are becoming the core of any problem solutions. Methodically classified a priori knowledge about hardware and software, problems and solutions are a fundamental prerequisite for productive work. Recently object-oriented programming has been applied for logical and transparent software design. From this point of view the paper gives some recommendations for analysis, classification, design specification and implementation of modern intelligent measurements. Examples will be given in the field of dimensional measurements for industrial quality control.}
}
@article{BIEMAN1995271,
title = {Measurement of language-supported reuse in object-oriented and object-based software},
journal = {Journal of Systems and Software},
volume = {30},
number = {3},
pages = {271-293},
year = {1995},
note = {Software Reuse},
issn = {0164-1212},
doi = {https://doi.org/10.1016/0164-1212(94)00138-D},
url = {https://www.sciencedirect.com/science/article/pii/016412129400138D},
author = {James M. Bieman and Santhi Karunanithi},
abstract = {A major benefit of object-oriented software development is the support for reuse provided by object-oriented and object-based languages. Yet, measures and measurement tools that quantify such language-supported reuse have been lacking. Comprehensive reuse measures, particularly for reuse with modifications, are necessary to evaluate the status of reuse in an organization and to monitor improvements. We develop a set of measurable reuse attributes appropriate to object-oriented and object-based systems and a suite of measures that quantify these attributes. One of our major objectives is to measure reuse in software written in the object-based language Ada. A set of suitable primitive reuse measures are expressed in Ada Reuse Tables. These tables support the flexible use of primitive measures in programs with nested packages and subprograms, and Ada generic packages. We designed and implemented a prototype Ada Reuse Measurement Analyzer (ARMA) to generate measurement values from Ada programs. ARMA produces a reuse data representation and a corresponding forest representation of an Ada system that contain the information necessary to produce the primitive measures. Developers can use the representations to produce customized reports to satisfy a wide range of measurement goals. We use ARMA to measure primitive reuse attributes for a set of industrial Ada software. We also show that ARMA can be used to generate a set of component access and package visibility measures.}
}
@article{HORVAI198543,
title = {Software Architecture of Large Distributed Process Control Systems},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {13},
pages = {43-48},
year = {1985},
note = {13th IFAC Workshop on Real Time Programming 1985, West Lafayette, IN, USA, 7-8 October 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-033450-9.50012-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080334509500129},
author = {M. Horvai and L. Gyimesi and A. Horváth and G. Juhász and K. Kovács and I. Sári and J. Szlankó and E. Tóth},
keywords = {Distributed process control, data base management system, event handling, data acquisition, digital control, man-machine communication, local area network},
abstract = {Users of process control applications need two kinds of information: data and events. Data is handled by the process control data base management system (PCDB), while events, i.e., software triggering, by the event handling means (EHM). Data and events are gained by the data acquisition and control package (DCP), and can be represented by the display communication system (DICOM). In a distributed real-time system a local area network interconnects the different control computers, so the system software tools are implemented in a distributed way too. The software architecture of a large distributed process control systems is presented in the paper.}
}
@article{DELAZZARI2021106003,
title = {ECMO Assistance during Mechanical Ventilation: Effects Induced on Energetic and Haemodynamic Variables},
journal = {Computer Methods and Programs in Biomedicine},
volume = {202},
pages = {106003},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106003},
url = {https://www.sciencedirect.com/science/article/pii/S016926072100078X},
author = {Beatrice {De Lazzari} and Attilio Iacovoni and Khosrow Mottaghy and Massimo Capoccia and Roberto Badagliacca and Carmine Dario Vizza and Claudio {De Lazzari}},
keywords = {ECMO, Mechanical ventilation, Pressure volume loop, Lumped parameter model, Software simulation, Clinical environment, Percutaneous left ventricular support, Cannulation},
abstract = {Background and Objective
Simulation in cardiovascular medicine may help clinicians understand the important events occurring during mechanical ventilation and circulatory support. During the COVID-19 pandemic, a significant number of patients have required hospital admission to tertiary referral centres for concomitant mechanical ventilation and extracorporeal membrane oxygenation (ECMO). Nevertheless, the management of ventilated patients on circulatory support can be quite challenging. Therefore, we sought to review the management of these patients based on the analysis of haemodynamic and energetic parameters using numerical simulations generated by a software package named CARDIOSIM©.
Methods
New modules of the systemic circulation and ECMO were implemented in CARDIOSIM© platform. This is a modular software simulator of the cardiovascular system used in research, clinical and e-learning environment. The new structure of the developed modules is based on the concept of lumped (0-D) numerical modelling. Different ECMO configurations have been connected to the cardiovascular network to reproduce Veno-Arterial (VA) and Veno-Venous (VV) ECMO assistance. The advantages and limitations of different ECMO cannulation strategies have been considered. We have used literature data to validate the effects of a combined ventilation and ECMO support strategy.
Results
The results have shown that our simulations reproduced the typical effects induced during mechanical ventilation and ECMO assistance. We focused our attention on ECMO with triple cannulation such as Veno-Ventricular-Arterial (VV-A) and Veno-Atrial-Arterial (VA-A) configurations to improve the hemodynamic and energetic conditions of a virtual patient. Simulations of VV-A and VA-A assistance with and without mechanical ventilation have generated specific effects on cardiac output, coupling of arterial and ventricular elastance for both ventricles, mean pulmonary pressure, external work and pressure volume area.
Conclusion
The new modules of the systemic circulation and ECMO support allowed the study of the effects induced by concomitant mechanical ventilation and circulatory support. Based on our clinical experience during the COVID-19 pandemic, numerical simulations may help clinicians with data analysis and treatment optimisation of patients requiring both mechanical ventilation and circulatory support.}
}
@article{FAILER2018448,
title = {Adaptive time-step control for nonlinear fluid–structure interaction},
journal = {Journal of Computational Physics},
volume = {366},
pages = {448-477},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.04.021},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118302377},
author = {Lukas Failer and Thomas Wick},
keywords = {Nonlinear fluid–structure interaction, Arbitrary Lagrangian–Eulerian approach, Temporal adaptivity, Time step control, Dual-weighted residual method, Truncation error},
abstract = {In this work, we consider time step control for variational-monolithic fluid–structure interaction. The fluid–structure interaction (FSI) system is based on the arbitrary Lagrangian–Eulerian approach and couples the incompressible Navier–Stokes equations with geometrically nonlinear elasticity resulting in a nonlinear PDE system. Based on the monolithic setting, we develop algorithms for temporal adaptivity that are based on a rigorous derivation of dual-weighted sensitivity measures and heuristic truncation-based time step control. The Fractional-Step-theta scheme is the underlying time-stepping method. In order to apply the dual-weighted residual method to our setting, a Galerkin interpretation of the Fractional-Step-theta scheme must be employed. All developments are substantiated with several numerical tests, namely FSI-benchmarks, including appropriate extensions, and a flapping membrane example.}
}
@article{SCHIKORA2003231,
title = {Efficacy of end-user neural network and data mining software for predicting complex system performance},
journal = {International Journal of Production Economics},
volume = {84},
number = {3},
pages = {231-253},
year = {2003},
issn = {0925-5273},
doi = {https://doi.org/10.1016/S0925-5273(03)00003-3},
url = {https://www.sciencedirect.com/science/article/pii/S0925527303000033},
author = {Paul F. Schikora and Michael R. Godfrey},
keywords = {Neural networks, Data mining, Regression analysis, Dial-up modem pools},
abstract = {The performance of a university's dial-up modem pool under various time limit policies and customer behavior patterns was studied. Because the system is very complex, simulation offered the only method to obtain a limited set of steady-state performance measure estimates. A more generalized predictive model must be built from the simulated output. Traditional methods available to practitioners for predicting system performance across a range of environmental and decision variables have typically been limited to linear regression models. However, when the system being studied is highly complex and its performance is nonlinear in nature, the effectiveness of linear models can be limited. While more advanced nonlinear methods, such as neural networks, have been shown to perform better than traditional regression analysis in these situations, the knowledge needed to implement them “from scratch” is beyond most practitioners. Fortunately, these advanced methods are now available in ready-to-use desktop software programs, making them more attainable for practitioner use. The efficacy of these end-user programs compared to more traditional methods in practice is of interest. Multiple variable linear regression models were developed for predicting six output measures in a simulation study and were compared to nonlinear regression models developed using a data mining software package (PolyAnalyst 4.3 Evaluation Software from Megaputer Intelligence) and two commercial neural network software packages (Statistica Neural Networks from Statsoft, and Predict from NeuralWorks). Comparisons of the models’ predictive ability were made on both the data used to design the models and on a test set of data. Statistical analysis shows that predictive performance on the test data was usually best with one of the neural network models, but relative performance of the different models varied widely.}
}
@article{MARTIN20057,
title = {OBJECT-ORIENTED MODELING OF VIRTUAL LABORATORIES FOR CONTROL EDUCATION},
journal = {IFAC Proceedings Volumes},
volume = {38},
number = {1},
pages = {7-12},
year = {2005},
note = {16th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20050703-6-CZ-1902.02274},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016382866},
author = {Carla Martin and Alfonso Urquia and Sebastian Dormido},
keywords = {Software tools, Education, Automatic control, Interactive approaches, Object modelling techniques, PID control, Reactor control modeling, Steam generators},
abstract = {The combined use of Ejs, Matlab/Simulink and Dymola (with Modelica language) has been successfully applied to set up virtual labs for control education. The tasks completed to achieve this goal are discussed in this manuscript: (1) the development of a novel modeling methodology adequate for interactive simulation; (2) the object-oriented design and programming of JARA: a Modelica library of dynamic hybrid models of some fundamental physical-chemical principles; (3) the description of the JARA physical models in a way suitable for interactive simulation; and finally (4) the implementation of the virtual labs. This is illustrated by means of two case studies.}
}
@article{GRANELL2010182,
title = {Service-oriented applications for environmental models: Reusable geospatial services},
journal = {Environmental Modelling & Software},
volume = {25},
number = {2},
pages = {182-198},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2009.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815209002047},
author = {Carlos Granell and Laura Díaz and Michael Gould},
keywords = {Geospatial processing services, Application and service integration, Service reuse, Environmental models, Service-oriented architecture, SOA, Spatial data infrastructure, SDI},
abstract = {Environmental modelling often requires a long iterative process of sourcing, reformatting, analyzing, and introducing various types of data into the model. Much of the data to be analyzed are geospatial data—digital terrain models (DTM), river basin boundaries, snow cover from satellite imagery, etc.—and so the modelling workflow typically involves the use of multiple desktop GIS and remote sensing software packages, with limited compatibility among them. Recent advances in service-oriented architectures (SOA) are allowing users to migrate from dedicated desktop solutions to on-line, loosely coupled, and standards-based services which accept source data, process them, and pass results as basic parameters to other intermediate services and/or then to the main model, which also may be made available on-line. This contribution presents a service-oriented application that addresses the issues of data accessibility and service interoperability for environmental models. Key model capabilities are implemented as geospatial services, which are combined to form complex services, and may be reused in other similar contexts. This work was carried out under the auspices of the AWARE project funded by the European programme Global Monitoring for Environment and Security (GMES). We show results of the service-oriented application applied to alpine runoff models, including the use of geospatial services facilitating discovery, access, processing and visualization of geospatial data in a distributed manner.}
}
@article{COOLEY2004345,
title = {Implementing multilingual touch-screen audio-CASI applications},
journal = {Computers in Human Behavior},
volume = {20},
number = {3},
pages = {345-356},
year = {2004},
issn = {0747-5632},
doi = {https://doi.org/10.1016/S0747-5632(03)00059-1},
url = {https://www.sciencedirect.com/science/article/pii/S0747563203000591},
author = {Philip C Cooley and Laxminarayana Ganapathi and Sheping Li},
keywords = {Multilingual interviews, Audio-CASI, Sensitive topics, Rich Text Format, Surveys},
abstract = {Audio computer-assisted self-interviewing (audio-CASI) technologies have become standard tools for collecting information on sensitive measures. However, while the method is commonly applied in the English language, its use is less prevalent in non-English language data collection efforts. One reason for the low occurrence of non-English audio-CASI applications is the relatively slow conversion of audio-CASI software to the nonstandard fonts required to display some languages (e.g., Russian, Chinese). Every written language requires an alphabet and a variety of fonts designed to display the special symbols and notations used by that language. However, early versions of PC operating systems were not flexible in their capacity to support nonstandard fonts. That is no longer the case with the development of more advanced operating systems such as Windows XP. This paper describes a versatile method for implementing audio-CASI technologies using any defined font and therefore any alphabet for which a font exists. This method relies on the capabilities of Rich Text Format for representing the visual or screen display component of the survey. Rich Text Format is supported by a variety of text editors and word processing packages, including Word and Word Perfect. It can be implemented in any software system with Rich Text Edit Control (RTEC). We describe a general process that exploits the capabilities of RTEC to efficiently implement multilingual audio-CASI applications, and demonstrate that process using four distinct languages.}
}
@article{ZNATY1994365,
title = {BECAUSE European workshop Sophia-Antipolis (France) 14–16 October 1992. General presentation of the BECAUSE Benchmark Set: BBS},
journal = {Future Generation Computer Systems},
volume = {10},
number = {4},
pages = {365-379},
year = {1994},
note = {Because Workshop Part1},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(94)90001-9},
url = {https://www.sciencedirect.com/science/article/pii/0167739X94900019},
author = {E. Znaty},
keywords = {BECAUSE project, Industrial application software, TOSCA, NSTC2D, EVEREST, CALIFE, Benchmarking, BECAUSE Benchmark Set (BBS), Basic kernels, Application kernels, Potential library routines, Communication tests, Contribution loops, Matrix assembly, Linear algebra, Iterative methods, Structured and unstructured meshes, Performance measures and problem size specification},
abstract = {Within the framework of the ESPRIT Project named BECAUSE (5417), entitled “Benchmarking of Concurrent Architectures for their Use in Scientific Engineering”, a set of industrial benchmarks was specified. The BECAUSE Benchmark Set (BBS) is scientifically application oriented. The various Test Programs have been extracted and specified from real industrial software such as electromagnetics software, semi-conductor simulation and computational fluid dynamics. The aim was to provide a general tool to industry in order to evaluate new generations of parallel computers and supercomputers in terms of memory capacity and computational power. Before parallelising a complete application, a scientific developer must answer the questions: ‘What is the performance of a given machine on my application and is it worth spending time to parallelise it?’ The BBS has been designed to provide simple benchmarks, as close as possible to critical parts of real scientific applications. Thus, by implementing various tests selected within the BBS, the user can get information useful to answer these questions. The serial version of the entire set of BBS is public and available on a mail server. In this paper, the objectives of the BECAUSE Project are recalled in order to replace the BBS in the context of this ESPRIT Project. Then, the applications from which test programs have been extracted are presented and all tests are described in detail. The next part is dedicated to the practical organisation of the BBS and is concerned with such aspects as documentation, input data and how to get the BBS, so that the BBS can be used inside and outside of the Project to assess parallel machines. As a conclusion, further developments are presented.}
}