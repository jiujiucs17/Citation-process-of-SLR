@inproceedings{10.1145/3474718.3475718,
author = {Oesch, Sean and Bridges, Robert A. and Verma, Miki and Weber, Brian and Diallo, Oumar},
title = {D2U: Data Driven User Emulation for the Enhancement of Cyber Testing, Training, and Data Set Generation},
year = {2021},
isbn = {9781450390651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474718.3475718},
doi = {10.1145/3474718.3475718},
abstract = {Whether testing intrusion detection systems, conducting training exercises, or creating data sets to be used by the broader cybersecurity community, realistic user behavior is a critical component of a cyber range. Existing methods either rely on network level data or replay recorded user actions to approximate real users in a network. Our work produces generative models trained on actual user data (sequences of application usage) collected from endpoints. Once trained to the user’s behavioral data, these models can generate novel sequences of actions from the same distribution as the training data. These sequences of actions are then fed to our custom software via configuration files, which replicate those behaviors on end devices. Notably, our models are platform agnostic and could generate behavior data for any emulation software package. In this paper we present our model generation process, software architecture, and an investigation of the fidelity of our models. Specifically, we consider two different representations of the behavioral sequences, on which three standard generative models for sequential data—Markov Chain, Hidden Markov Model, and Random Surfer—are employed. Additionally, we examine adding a latent variable to faithfully capture time-of-day trends. Best results are observed when sampling a unique next behavior (regardless of the specific sequential model used) and the duration to take the behavior, paired with the temporal latent variable. Our software is currently deployed in a cyber range to help evaluate the efficacy of defensive cyber technologies, and we suggest additional ways that the cyber community as a whole can benefit from more realistic user behavior emulation.},
booktitle = {Cyber Security Experimentation and Test Workshop},
pages = {17–26},
numpages = {10},
keywords = {user emulation, experimental infrastructure, data driven, data sets},
location = {Virtual, CA, USA},
series = {CSET '21}
}

@inproceedings{10.1145/2077370.2077376,
author = {Bhattacharyya, Shuvra S. and Plishker, William and Gupta, Ayush and Shen, Chung-Ching},
title = {Teaching Cross-Platform Design and Testing Methods for Embedded Systems Using DICE},
year = {2011},
isbn = {9781450310468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2077370.2077376},
doi = {10.1145/2077370.2077376},
abstract = {DICE (the DSPCAD Integrative Command Line Environment) is a package of utilities that facilitates efficient management of software projects. Key areas of emphasis in DICE are cross-platform operation, support for projects that integrate heterogeneous programming languages, and support for applying and integrating different kinds of design and testing methodologies. The package is being developed at the University of Maryland to facilitate the research and teaching of methods for implementation, testing, evolution, and revision of engineering software.The platform- and language-independent focus of DICE makes it an effective vehicle for teaching high-productivity, high-reliability methods for design and implementation of embedded systems for a variety of courses. In this paper, we provide an overview of features of DICE --- particularly as they relate to testing driven design practices --- that are useful in embedded systems education, and discuss examples and experiences of applying the tool in courses at the University of Maryland aimed at diverse groups of students --- undergraduate programming concepts for engineers, graduate VLSI architectures (aimed at research-oriented students), and graduate FPGA system design (aimed at professional Master's students).},
booktitle = {Proceedings of the 6th Workshop on Embedded Systems Education},
pages = {38–45},
numpages = {8},
location = {Taipei, Taiwan},
series = {WESE '11}
}

@inproceedings{10.1145/3437120.3437292,
author = {Maikantis, Theodoros and Tsintzira, Angeliki-Agathi and Ampatzoglou, Apostolos and Arvanitou, Elvira-Maria and Chatzigeorgiou, Alexander and Stamelos, Ioannis and Bibi, Stamatia and Deligiannis, Ignatios},
title = {Software Architecture Reconstruction via a Genetic Algorithm: Applying the Move Class Refactoring},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437292},
doi = {10.1145/3437120.3437292},
abstract = {Modularity is one of the four key principles of software design and architecture. According to this principle, software should be organized into modules that are tightly linked internally (high cohesion), whereas at the same time as independent from other modules as possible (low coupling). However, in practice, this principle is violated due to poor architecting design decisions, lack of time, or coding shortcuts, leading to a phenomenon termed as architectural technical debt (ATD). To alleviate this problem (lack of architectural modularity), the most common solution is the application of a software refactoring, namely Move Class—i.e., moving classes (the core artifact in object-oriented systems) from one module to another. To identify Move Class refactoring opportunities, we employ a search-based optimization process, relying on optimization metrics, through which optimal moves are derived. Given the extensive search space required for applying a brute-force search strategy, in this paper, we propose the use of a genetic algorithm that re-arranges existing software classes into existing or new modules (software packages in Java, or folders in C++). To validate the usefulness of the proposed refactorings, we performed an industrial case study on three projects (from the Aviation, Healthcare, and Manufacturing application domains). The results of the study indicate that the proposed architecture reconstruction is able to improve modularity, improving both coupling and cohesion. The obtained results can be useful to practitioners through an open source tool; whereas at the same point, they open interesting future work directions.},
booktitle = {24th Pan-Hellenic Conference on Informatics},
pages = {135–139},
numpages = {5},
location = {Athens, Greece},
series = {PCI 2020}
}

@inproceedings{10.1145/3275219.3275229,
author = {Zhang, Zejun and Wu, Shaobo and Jiang, Renhe and Pan, Minxue and Zhang, Tian},
title = {A Documentation-Based Constraint Generation Method for Java APIs},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275229},
doi = {10.1145/3275219.3275229},
abstract = {As the efficiency of constraint solvers increases, constraint solving has been widely used in many applications of software engineering, such as test case generation, program synthesis and code search. However, encoding source code into constraints is not an easy task. Particularly, when it comes to complex data structures of library functions, existing work cannot generate effective constraints.In this paper, we propose a documentation-based method to generate constraint for Java APIs. It mainly uses Natural Language Processing (NLP) techniques to construct syntactic trees by extracting the specifications of methods. Then it uses predefined constraint templates to generate constraint models by traversing the syntactic trees. Our approach successfully models 228 methods for 11 Java container classes and string-related classes. We summarize 12 functional behavior and formulate 21 constraint templates for these classes. Compared with other tools, our approach models most of methods about string-related classes, and more importantly, generates constraint models for Java container classes that other tools cannot. In order to evaluate the effectiveness of our constraint models, we apply them into test case generation. The experimental result shows that constraint models we generate have practical use.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {4},
numpages = {10},
keywords = {Sequence theory, Constraint model, NLP techniques},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/800173.809688,
author = {Choy, David M.},
title = {Database for Office Automation},
year = {1983},
isbn = {0897911202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800173.809688},
doi = {10.1145/800173.809688},
abstract = {With a fast growth in the quantity and quality of software applications for office systems and advanced workstations, it is becoming clear that the trend is towards a more integrated software architecture for workstation or office systems. A critical piece in a truly integrated system is a well-designed general purpose database mangement system (DBMS) that supports all applications running on the workstation (or office system). Sophisticated DBMSs have been developed on the main frames, primarily for large data processsing (DP) applications. Is it true, then, that all we need is to transport these systems to the workstation?The first problem that one faces to move a full scale DBMS to a workstation is the size. It is not clear that trimming a big DBMS down is an easy task. Some of the advanced capabilities might be modular and relatively easy to remove. Others could be quite difficult as the code is distributed throughout the system. In some cases, it might not even be possible to “trim” when the general capability is needed but to a lesser extent. In practice, most major DBMSs are built without serious concern on the space restrictions. It might be easier to design and implement a new system with “smallness” in mind than to trim down a huge one. The considerations on smallness include the data structures, control blocks, interfaces, buffer requirements, configuration and packaging of the system, as well as the number of lines of code.Size is only the beginning. Smallness makes the DBMS possible to run on a workstation. Because of the new operating environment, there are new requirements for the DBMS which are different from the traditional DP environment on a main frame. The real challenge is to determine what these requirements are and how to design a system accordingly.On an advanced workstation, one has to support word processing, data processing, engineering and scientific, administrative, as well as},
booktitle = {Proceedings of the 1983 Annual Conference on Computers : Extending the Human Resource},
pages = {15–16},
numpages = {2},
series = {ACM '83}
}

@article{10.1145/3302913,
author = {Pibiri, Giulio Ermanno and Venturini, Rossano},
title = {Handling Massive N-Gram Datasets Efficiently},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3302913},
doi = {10.1145/3302913},
abstract = {Two fundamental problems concern the handling of large n-gram language models: indexing, that is, compressing the n-grams and associated satellite values without compromising their retrieval speed, and estimation, that is, computing the probability distribution of the n-grams extracted from a large textual source.Performing these two tasks efficiently is vital for several applications in the fields of Information Retrieval, Natural Language Processing, and Machine Learning, such as auto-completion in search engines and machine translation.Regarding the problem of indexing, we describe compressed, exact, and lossless data structures that simultaneously achieve high space reductions and no time degradation with respect to the state-of-the-art solutions and related software packages. In particular, we present a compressed trie data structure in which each word of an n-gram following a context of fixed length k, that is, its preceding k words, is encoded as an integer whose value is proportional to the number of words that follow such context. Since the number of words following a given context is typically very small in natural languages, we lower the space of representation to compression levels that were never achieved before, allowing the indexing of billions of strings. Despite the significant savings in space, our technique introduces a negligible penalty at query time.Specifically, the most space-efficient competitors in the literature, which are both quantized and lossy, do not take less than our trie data structure and are up to 5 times slower. Conversely, our trie is as fast as the fastest competitor but also retains an advantage of up to 65% in absolute space.Regarding the problem of estimation, we present a novel algorithm for estimating modified Kneser-Ney language models that have emerged as the de-facto choice for language modeling in both academia and industry thanks to their relatively low perplexity performance. Estimating such models from large textual sources poses the challenge of devising algorithms that make a parsimonious use of the disk.The state-of-the-art algorithm uses three sorting steps in external memory: we show an improved construction that requires only one sorting step by exploiting the properties of the extracted n-gram strings. With an extensive experimental analysis performed on billions of n-grams, we show an average improvement of 4.5 times on the total runtime of the previous approach.},
journal = {ACM Trans. Inf. Syst.},
month = {feb},
articleno = {25},
numpages = {41},
keywords = {algorithm engineering, scalability, Efficiency}
}

@inproceedings{10.1145/1062455.1062637,
author = {Tonella, Paolo},
title = {Reverse Engineering of Object Oriented Code},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062637},
doi = {10.1145/1062455.1062637},
abstract = {During software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. For Object-Oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. Design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all.This tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. The recovered diagrams, represented in UML (Unified Modeling Language), include class, object, interaction (collaboration and sequence), state and package diagrams. A unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. A single running example is referred all over the presentation. Trade-offs (e.g., static vs. dynamic analysis), limitations and expected benefits are also discussed.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {724–725},
numpages = {2},
keywords = {diagram recovery, object oriented programming, static code analysis},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@inproceedings{10.1145/2808719.2816838,
author = {Yin, Junqi and Brook, R. Glenn and Crosby, Lonnie and Horton, Mitchel and Burdyshaw, Chad and Skidmore, Frank and Marstrander, Jon and Anthony, Thomas and Liu, Yuliang},
title = {FMRI Image Registration with AFNI's 3dQwarp},
year = {2015},
isbn = {9781450338530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808719.2816838},
doi = {10.1145/2808719.2816838},
abstract = {In the study of Parkinson's disease (PD), substantial research has shown a variety of findings within brain MR images that differ on a group-wise level between individuals with PD and healthy adults, including resting brain activity and blood flow [1, 2], diffusion tensor fractional anisotropy [3], and brain iron deposition [4, 5]. Despite these promising findings, none of these putative biomarkers have become utilized in the field. We believe this is the case principally because it has been very difficulty to register findings visible at a group level to individual brains because of the vast amount of inter-subject variability within the human population. While specific brain structures are common, the precise location, distribution, and size of these structures can vary dramatically between individuals. MR images essentially represent vast unstructured datasets. In the United States in 2011, over 8.7 million MRI brain scans sessions (each comprised of multiple brain imaging sequences) occurred. It is clear that to make practical use of this massive amount of imaging data, not only for PD but also for analysis of other conditions of the brain, high performance computing solutions will be required. There are several software packages available for image registration. For non-linear image registration, the 3dQwarp code in the Analysis of Functional Neuroimaging (AFNI) package [6] designed by NIH provides the best result in terms of normalized cross correlation, mutual information and sum of squared differences [7].The AFNI 3dQwarp code follows the h-method in the language of the finite element analysis, which divides the image into shrinking patches and warps the source image to the template image in incremental steps. In each step, the patches are optimized in at most four sweeps (one by default). We find that for some irregular images, the results are not satisfactory, related to insufficient convergence of the solution space at each patch size. One solution to insufficient convergence uses is a linear registration step. However, our dataset (Michael J. Fox Parkinson's Progressive Markers Initiative (PPMI), a well characterized multidimensional dataset with substantial imaging data.) indicates that the linear registration in AFNI is not always well performed for PD subjects and introduces unnecessary noises to the downstream analysis. We propose a new warping procedure, which starts with an image in original space, and applies non-linear registration directly. The important part is to make sure the solution has converged at each warping step before moving to the next finer optimization step. With the new procedure, our results show about 20% improvement compared to the original procedure in term of the Pearson correlation. The practical ramifications of this improvement were apparent in the end result; before adopting this process 58 of 161 subjects were judged insufficiently converged for analysis by the research user team (Skidmore, Anthony, Marstrander above). Subsequent to the institution of the new process, while computational load increased, only 3 of 158 subjects were rejected as insufficiently registered.Preliminary scaling studies demonstrate that the current implementation of 3dQwarp with OpenMP directives are not sufficient to take advantage of the number of cores available either on current day CPUs, or the number of cores available on the many-core Intel Xeon Phi. For both clinical and research utilization, registration code can be improved using high performance multi-thread computation. Therefore, a future direction will be to implement a version of 3dQwarp that does scale to a larger number of threads to assist in developing analytic turnaround times that will be more relevant for both research and clinical settings.},
booktitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {559–560},
numpages = {2},
keywords = {MRI, parkinson's disease detection, numerical analysis, image registration},
location = {Atlanta, Georgia},
series = {BCB '15}
}

@inproceedings{10.1145/2503848.2503855,
author = {Afonso, Adriano},
title = {The LibreOffice Portuguese Community: A Researcher's View},
year = {2013},
isbn = {9781450322553},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2503848.2503855},
doi = {10.1145/2503848.2503855},
abstract = {The success of Open Source Software (OSS), along with the importance that their communities have created around their project's and products (Ubuntu, LibreOffice, Android and CyanogenMod, etc) has recently attracted the attention of researchers and commercial companies. While the first ones are learning lessons from the success of OSS and applying some of them, the second ones are also acquiring knowledge. They are now more open minded and import the apprenticeship to develop their systems, implement it to their structure and even help to make them compatible with their products (like HTC, Motorola, Samsung and Sony Ericsson do with CyanogenMod). These OSS communities are normally called communities of practice and they are a group of people who are informally bounded by their common interest and practice in a specific domain. The LibreOffice Portuguese Community has been in great growth and trying to answer the "The Document Foundation" (TDF) requests. The "real time" translations of the package and recently the development of the IT and LibreOffice Open Manual are examples. The author feels that it is time to make a short scientific analysis, through a reading of the creative processes and production, based on international scientific developments and conclusions. The objective of this paper is to make the first step of a study of how LibreOffice is contributing to the sustainable development in Portugal.},
booktitle = {Proceedings of the Workshop on Open Source and Design of Communication},
pages = {35–37},
numpages = {3},
keywords = {OSS community, open source, LibreOffice, LibreOffice Portuguese community},
location = {Lisbon, Portugal},
series = {OSDOC '13}
}

@inproceedings{10.1145/3299869.3320241,
author = {Song, Jie and Alter, George and Jagadish, H. V.},
title = {C2Metadata: Automating the Capture of Data Transformations from Statistical Scripts in Data Documentation},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3320241},
doi = {10.1145/3299869.3320241},
abstract = {Datasets are often derived by manipulating raw data with statistical software packages. The derivation of a dataset must be recorded in terms of both the raw input and the manipulations applied to it. Statistics packages typically provide limited help in documenting provenance for the resulting derived data. At best, the operations performed by the statistical package are described in a script. Disparate representations make these scripts hard to understand for users. To address these challenges, we created Continuous Capture of Metadata (C2Metadata), a system to capture data transformations in scripts for statistical packages and represent it as metadata in a standard format that is easy to understand. We do so by devising a Structured Data Transformation Algebra (SDTA), which uses a small set of algebraic operators to express a large fraction of data manipulation performed in practce. We then implement SDTA, inspired by relational algebra, in a data transformation specification language we call SDTL. In this demonstration, we showcase C2metadata's capture of data transformations from a pool of sample transformation scripts in at least two languages: SPSS and Stata (SAS and R are under development), for social science data in a large academic repository. We will allow the audience to explore C2Metadata using a web-based interface, visualize the intermediate steps and trace the provenance and changes of data at different levels for better understanding of the process.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {2005–2008},
numpages = {4},
keywords = {data provenance, data transformation, data documentation},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.1145/1131322.1131336,
author = {Petrini, Fabrizio and Nieplocha, Jarek and Tipparaju, Vinod},
title = {SFT: Scalable Fault Tolerance},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {0163-5980},
url = {https://doi.org/10.1145/1131322.1131336},
doi = {10.1145/1131322.1131336},
abstract = {In this paper we will present a new technology that we are currently developing within the SFT: Scalable Fault Tolerance FastOS project which seeks to implement fault tolerance at the operating system level. Major design goals include dynamic reallocation of resources to allow continuing execution in the presence of hardware failures, very high scalability, high efficiency (low overhead), and transparency---requiring no changes to user applications. Our technology is based on a global coordination mechanism, that enforces transparent recovery lines in the system, and TICK, a lightweight, incremental checkpointing software architecture implemented as a Linux kernel module. TICK is completely user-transparent and does not require any changes to user code or system libraries; it is highly responsive: an interrupt, such as a timer interrupt, can trigger a checkpoint in as little as 2.5μs; and it supports incremental and full checkpoints with minimal overhead---less than 6% with full checkpointing to disk performed as frequently as once per minute.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {55–62},
numpages = {8}
}

@article{10.1145/264934.264940,
author = {Welch, Lonnie R.},
title = {PRISM: A Reverse Engineering Toolset},
year = {1997},
issue_date = {Nov./Dec. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {XVII},
number = {6},
issn = {1094-3641},
url = {https://doi.org/10.1145/264934.264940},
doi = {10.1145/264934.264940},
abstract = {This paper presents a process for the reengineering of computer-based control systems, and describes tools that automate portions of the process. The intermediate representation (IR) for capturing features of computer-based systems during reverse engineering is presented. A novel feature of the IR is that it incorporates the control system software architecture, a view that enables information to be captured at five levels of granularity: the program level, the task level, the package level, the subprogram level, and the statement level. A reverse engineering toolset that constructs the IR from Ada programs, displays the IR, and computes metrics for concurrency, communication and object-orientedness is also presented.},
journal = {Ada Lett.},
month = {nov},
pages = {39–46},
numpages = {8}
}

@inproceedings{10.1109/PADS.2009.16,
author = {Al-Zoubi, Khaldoon and Wainer, Gabriel},
title = {Using REST Web-Services Architecture for Distributed Simulation},
year = {2009},
isbn = {9780769537139},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PADS.2009.16},
doi = {10.1109/PADS.2009.16},
abstract = {In recent years, Web Services technologies have been successfully used for simplifying interoperability while providing scalability and flexibility in multiple applications, including distributed simulation software. The RESTful-CD++ simulation Server provides Web Services according to the REST principles by exposing services as URIs and consumed via HTTP messages. Therefore, the server becomes a service part of the Web that can be easily mashed-up with other applications and simulation software. In contrast, RPC-style SOAP-based Web Services use the Web as a transmission medium by exposing few URIs and many RPCs. RESTful-CD++ is (to our best knowledge) the only existing RESTful system in this area. Further, this distributed simulation package provides pioneering distributed simulation services using the Web architectural style. We present an overview of the principles, design and implementation of the RESTful-CD++ HTTP server and DCD++ simulation. We show that REST fulfills WS objectives with a much better and easier style than the SOAP-based systems.},
booktitle = {Proceedings of the 2009 ACM/IEEE/SCS 23rd Workshop on Principles of Advanced and Distributed Simulation},
pages = {114–121},
numpages = {8},
keywords = {SOA, Web-Services, REST, Distributed Simulation, DEVS, CD++},
series = {PADS '09}
}

@inproceedings{10.1145/3204919.3204928,
author = {St Clere Smithe, Toby and Potter, Ralph},
title = {Building a Brain with SYCL and Modern C++},
year = {2018},
isbn = {9781450364393},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3204919.3204928},
doi = {10.1145/3204919.3204928},
abstract = {State-of-the art machine learning systems typically depend on energetically costly gradient-descent learning over a curated task-specific data set. Despite their successes, these methods are not well suited to building fully autonomous systems such as may employ energy-efficient accelerators targeted by OpenCL. By contrast, the brain uses low-energy local learning rules to discover the causal structure of an environment, forming semantically rich representations without supervision, and therefore exhibiting the required combination of efficiency and flexibility. To investigate these properties, a paradigm shift to dynamic "spike-based" computation is required. Historically, investigating spiking neural models has been a task for specialists, with software that is tailored to specific scientific projects, or that trades flexibility against performance. Here, we present neurosycl, a high-performance, portable spiking network simulator based on SYCL, with a modern and extensible C++ API. Our aim is to provide the necessary components for non-specialists to build a simulated brain, and to run the constructed models as close to real-time as possible.This bipartite aim leads to two competing considerations -- a simple interface, and portable performance -- which are reconciled using SYCL's single-source programming model. We describe two principal algorithmic challenges that illustrate the different hardware demands of spiking neural networks relative to deep learning networks, and how neurosycl solves them for GPU-like parallel processors via SYCL. Firstly, although the brain is akin to a parallel processor whose cores are neurons, the connections between neurons may have differing temporal delays, which results in a message-passing problem if the neurons are simulated asynchronously. Secondly, because these messages ('spikes') are generated chaotically, then transmitted to arbitrary target neurons with arbitrary transmission delays, a naive implementation even of a synchronous model quickly runs into a highly suboptimal memory access regime.neurosycl's design separates the specification of a model architecture from its simulation, so that once a model has been instantiated, its basic structure is fixed. This simplification enables us to infer the memory access pattern, and thus re-order the connection indices so that adjacent kernels access nearby memory locations. The simplification is also reflected in the API design: users can construct complex connection graphs between arbitrary neuron groups using a simple declarative interface, but runtime interactions with the model, for monitoring or I/O, are mediated by a set of simulated electrodes, combined with hooks into the simulation loop. This design mirrors that of neuroscientific experiments, and permits the user to embed the simulated brain into a virtual environment by integrating with other technologies, exposing implementation details only when necessary to allow this. We describe our API, illustrated by a number of "brain-building" examples, showing how the components compose and map via SYCL onto the hardware. We present performance comparisons across hardware platforms and alternative simulators, demonstrating portability for various network configurations and standard neuron models.},
booktitle = {Proceedings of the International Workshop on OpenCL},
articleno = {9},
numpages = {1},
keywords = {simulation, autonomous systems, neuroscience, AI, spiking neural networks, ACM proceedings, SYCL, artificial intelligence, machine learning},
location = {Oxford, United Kingdom},
series = {IWOCL '18}
}

@inproceedings{10.1145/1827418.1827455,
author = {Eyers, David M. and Vargas, Luis and Singh, Jatinder and Moody, Ken and Bacon, Jean},
title = {Relational Database Support for Event-Based Middleware Functionality},
year = {2010},
isbn = {9781605589275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1827418.1827455},
doi = {10.1145/1827418.1827455},
abstract = {Many of the popular relational database management systems (RDBMS) provide features for operating in a distributed environment, such as remote table queries and updates, and support for distributed transactions. In practice, however, much application software targets a more minimal set of functionality than is offered by the SQL standards. Independently of the database tier, engineering concepts such as the enterprise service bus and service oriented architecture have led to the development of communication middleware to support distributed applications. For applications that require reliable delivery of messages, complex event processing, and integrated archiving of data, impedance mismatches are likely to emerge between the database system and the communications middleware---for example with respect to data-types, event filtering that is based on information in the database, and in terms of coordinating access control policy. This paper describes event-based middleware functionality that is supported directly within the database system. In contrast to previous approaches (e.g. being able to name remote tables in SQL statements), the programming of event-based communication operations within the database is explicit. We present initial performance results that compare an augmented PostgreSQL database system to an environment in which a database and an event-based middleware package are used side-by-side. These results demonstrate the viability of our approach.},
booktitle = {Proceedings of the Fourth ACM International Conference on Distributed Event-Based Systems},
pages = {160–171},
numpages = {12},
keywords = {publish/subscribe, database, queues},
location = {Cambridge, United Kingdom},
series = {DEBS '10}
}

@inproceedings{10.5555/41765.41794,
author = {Dowson, M.},
title = {ISTAR and the Contractual Approach},
year = {1987},
isbn = {0897912160},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {ISTAR is a production-quality project support environment that was first released commercially during 1986. ISTAR is language and method independent, supports distributed projects, is portable and includes a rich and open ended tool set; its architecture and tools are described more fully elsewhere (see [DOWSON87a]). This description concentrates on the underlying approach to software development built in to ISTAR and its relationship to various views of the software process.ISTAR is organized to support a powerful and general approach to software development, the contractual approach. The essence of the contractual approach is that it views every task in a software project as having the nature of a contract; that is, a well defined package of work that can be performed independently by a contractor for a client. Contract specifications, of course, can include acceptance tests for contract deliverables, schedules, requirements s to standards which must be adhered to, obligations to make periodic reports and so on. But the key point is that the contractor is free to decide how to fulfill the contract specification. In general, this freedom includes the ability to let subcontracts to perform all or part of the work, and so on recursively.Support for this approach is built into the structure of ISTAR which creates independent contract databases for the execution of each contract. In addition, ISTAR provides contractual operations for assigning contracts, amending or cancelling them when necessary, accepting their deliverables and communicating formal reports between them.Using ISTAR to support a project, then, results in the creation of a dynamic hierarchy of contracts (and their contract databases) which corresponds to the organization of the project. It is important to note that the hierarchy is organizational, and that its structure is determined by more or less autonomous decsions by contractors (project staff) at various levels. Another way of saying this is that the contractual approach does not prescribe a particular software process — but is capable of providing effective support for a wide variety of processes. For example, the 'root' contract in a project hierarchy (the initial contract for the project as a whole) could let subcontracts for successive phases of the work such as 'specification', 'design', 'code', 'integrate', 'test', following a simple waterfall model of development. Alternatively, the first level subcontracts could follow a functional division and each be for the complete development of a part of the system. Of course, the contractual approach does constrain the process to be hierarchical, and insists that everyone in a project knows what they are supposed to be doing and for whom. But this reflects the observation that most effective processes have these minimum properties.In either case, the specifications of the initial ontracts might or might not include transitive obligations to organize subhierarchies in the same way, or even prescribe or forbid further subcontracting. Thus, the degree of autonomy that can be exercised by project staff is, quite properly, under managerial control, with ISTAR providing flexible mechanisms to support it. In practice, the degree of autonomy allowed is likely to vary throughout the contract hierarchy; the overall project manager will grant his group managers considerable freedom to organize their assigned work, while junior programmers will get clear instructions on how to proceed from their team leaders.As usual, the power of the approach is exposed when examining what happens when things go wrong rather than what happens when they go right. When a contractor cannot complete an assigned task within its schedule, or with the available resources, or at all, some sort of 'iteration' ([DOWNSON87b]) is required. At this stage it is worth noting that the contractual operations, and the contractually required reports that they help provide, are not intended to replace the normal informal or semi-formal (eg project meeting) within-project communication mechanisms, but to supplement them with formal communication paths that are guaranteed to correspond to the formal organization of the project. Contractors will thus get formal notification of subcontract problems (either via one of the reports required in the subcontract specification or by the absence of a scheduled deliverable), but corrective action may well be triggered by prior information acquired by less formal routes.What does the contractor do? There are two options: Deal with the problem. Providing that it can be done while remaining within the constraints of the contract specification, the contractor is free to re-organize the contract work as much as necessary. This might be as simple as finding some additional resources to staff one new subcontract (the problematic subcontract will have to have its specification modified, of course), or as extensive as cancelling all the existing subcontracts, effectively deleting the complete sub-hierarchy, re-planning the work and issuing a new set of subcontracts. Pass the problem up the hierarchy. The client (that is, the contractor at the next level in the hierarchy) will now have the same two options available to resolve the problem.As usual, the power of the approach is exposed when examining what happens when things go wrong rather than what happens when they go right. When a contractor cannot complete an assigned task within its schedule, or with the available resources, or at all, some sort of 'iteration' ([DOWNSON87b]) is required. At this stage it is worth noting that the contractual operations, and the contractually required reports that they help provide, are not intended to replace the normal informal or semi-formal (eg project meeting) within-project communication mechanisms, but to supplement them with formal communication paths that are guaranteed to correspond to the formal organization of the project. Contractors will thus get formal notification of subcontract problems (either via one of the reports required in the subcontract specification or by the absence of a scheduled deliverable), but corrective action may well be triggered by prior information acquired by less formal routes.What does the contractor do? There are two options: Deal with the problem. Providing that it can be done while remaining within the constraints of the contract specification, the contractor is free to re-organize the contract work as much as necessary. This might be as simple as finding some additional resources to staff one new subcontract (the problematic subcontract will have to have its specification modified, of course), or as extensive as cancelling all the existing subcontracts, effectively deleting the complete sub-hierarchy, re-planning the work and issuing a new set of subcontracts. Pass the problem up the hierarchy. The client (that is, the contractor at the next level in the hierarchy) will now have the same two options available to resolve the problem.ISTAR goes further than many environments by retaining a permanent record of all contractual operations executed in the course of a project. Although these operations are essentially local, the record of them constitutes an implicit description of the process followed. This is a step in the right direction. Future environments will need to make these descriptions explicit so that they can be used as a basis for progressively improving the software process.},
booktitle = {Proceedings of the 9th International Conference on Software Engineering},
pages = {287–288},
numpages = {2},
location = {Monterey, California, USA},
series = {ICSE '87}
}

@article{10.1145/2379810.2379811,
author = {Taylor, Simon J. E. and Turner, Stephen J. and Strassburger, Steffen and Mustafee, Navonil},
title = {Bridging the Gap: A Standards-Based Approach to OR/MS Distributed Simulation},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {1049-3301},
url = {https://doi.org/10.1145/2379810.2379811},
doi = {10.1145/2379810.2379811},
abstract = {In Operations Research and Management Science (OR/MS), Discrete Event Simulation (DES) models are typically created using commercial off-the-shelf simulation packages (CSPs) such as AnyLogic™, Arena™, Flexsim™, Simul8™, SLX™, Witness™, and so on. A DES model represents the processes associated with a system of interest. Some models may be composed of submodels running in their own CSPs on different computers linked together over a communications network via distributed simulation software. The creation of a distributed simulation with CSPs is still complex and typically requires a partnership of problem owners, modelers, CSP vendors, and distributed simulation specialists. In an attempt to simplify this development and foster discussion between modelers and technologists, the SISO-STD-006-2010 Standard for COTS Simulation Package Interoperability Reference Models has been developed. The standard makes it possible to capture interoperability capabilities and requirements at a DES modeling level rather than a computing technical level. For example, it allows requirements for entity transfer between models to be clearly specified in DES terms (e.g. the relationship between departure and arrival simulation times and input element (queue, workstation, etc.), buffering rules, and entity priority, instead of using specialist technical terminology. This article explores the motivations for distributed simulation in this area, related work, and the rationale for the standard. The four Types of Interoperability Reference Model described in the standard are discussed and presented (A. Entity Transfer, B. Shared Resource, C. Shared Event, and D. Shared Data Structure). Case studies in healthcare and manufacturing are given to demonstrate how the standard is used in practice.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {nov},
articleno = {18},
numpages = {23},
keywords = {manufacturing, Commercial-off-the-shelf simulation packages, healthcare}
}

@article{10.1145/78928.78931,
author = {Kearfott, R. Baker and Novoa, Manuel},
title = {Algorithm 681: INTBIS, a Portable Interval Newton/Bisection Package},
year = {1990},
issue_date = {June 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/78928.78931},
doi = {10.1145/78928.78931},
abstract = {We present a portable software package for finding all real roots of a system of nonlinear equations within a region defined by bounds on the variables. Where practical, the package should find all roots with mathematical certainty. Though based on interval Newton methods, it is self-contained. It allows various control and output options and does not require programming if the equations are polynomials; it is structured for further algorithmic research. Its practicality does not depend in a simple way on the dimension of the system or on the degree of nonlinearity.},
journal = {ACM Trans. Math. Softw.},
month = {jun},
pages = {152–157},
numpages = {6}
}

@inproceedings{10.1145/3460319.3464836,
author = {Nielsen, Benjamin Barslev and Torp, Martin Toldam and M\o{}ller, Anders},
title = {Modular Call Graph Construction for Security Scanning of Node.Js Applications},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464836},
doi = {10.1145/3460319.3464836},
abstract = {Most of the code in typical Node.js applications comes from third-party libraries that consist of a large number of interdependent modules. Because of the dynamic features of JavaScript, it is difficult to obtain detailed information about the module dependencies, which is vital for reasoning about the potential consequences of security vulnerabilities in libraries, and for many other software development tasks. The underlying challenge is how to construct precise call graphs that capture the connectivity between functions in the modules. In this work we present a novel approach to call graph construction for Node.js applications that is modular, taking into account the modular structure of Node.js applications, and sufficiently accurate and efficient to be practically useful. We demonstrate experimentally that the constructed call graphs are useful for security scanning, reducing the number of false positives by 81% compared to npm audit and with zero false negatives. Compared to js-callgraph, the call graph construction is significantly more accurate and efficient. The experiments also show that the analysis time is reduced substantially when reusing modular call graphs.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {29–41},
numpages = {13},
keywords = {static analysis, JavaScript, modularity},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/581339.581382,
author = {Niere, J\"{o}rg and Sch\"{a}fer, Wilhelm and Wadsack, J\"{o}rg P. and Wendehals, Lothar and Welsh, Jim},
title = {Towards Pattern-Based Design Recovery},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581382},
doi = {10.1145/581339.581382},
abstract = {A method and a corresponding tool is described which assist design recovery and program understanding by recognising instances of design patterns semi-automatically. The approach taken is specifically designed to overcome the existing scalability problems caused by many design and implementation variants of design pattern instances. Our approach is based on a new recognition algorithm which works incrementally rather than trying to analyse a possibly large software system in one pass without any human intervention. The new algorithm exploits domain and context knowledge given by a reverse engineer and by a special underlying data structure, namely a special form of an annotated abstract syntax graph. A comparative and quantitative evaluation of applying the approach to the Java AWT and JGL libraries is also given.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {338–348},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/3109729.3109749,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Extending the Common Variability Language (CVL) Engine: A Practical Tool},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109749},
doi = {10.1145/3109729.3109749},
abstract = {The Common Variability Language (CVL) has become a reference in the specification and resolution of variability in the last few years. Despite the multiple advantages of CVL (orthogonal variability, architecture variability resolution, MOF-compliant, standard proposed,...), several approaches require extending and/or modifying the CVL approach in different ways in order to fulfill the industrial needs for variability modeling in Software Product Lines. However, the community lacks a tool that would enable proposed extensions and the integration of novel approaches to be put into practice. Existing tools that provide support for CVL are incomplete or are mainly focused on the variability model's editor, instead of executing the resolution of the variability over the base models. Moreover, there is no API that allows direct interaction with the CVL engine to extend or use it in an independent application. In this paper, we identify the extension points of the CVL approach with the goal of making the CVL engine more flexible, and to help software architects in the task of resolving the variability of their products. The practical tool presented here is a working implementation of the CVL engine, that can be extended through a proposed API.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {32–37},
numpages = {6},
keywords = {Variability, CVL, Software Product Line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/1894483.1894510,
author = {Chen, Xiaoyu},
title = {Electronic Geometry Textbook: A Geometric Textbook Knowledge Management System},
year = {2010},
isbn = {3642141277},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Electronic Geometry Textbook is a knowledge management system that manages geometric textbook knowledge to enable users to construct and share dynamic geometry textbooks interactively and efficiently. Based on a knowledge base organizing and storing the knowledge represented in specific languages, the system implements interfaces for maintaining the data representing that knowledge as well as relations among those data, for automatically generating readable documents for viewing or printing, and for automatically discovering the relations among knowledge data. An interface has been developed for users to create geometry textbooks with automatic checking, in real time, of the consistency of the structure of each resulting textbook. By integrating an external geometric theorem prover and an external dynamic geometry software package, the system offers the facilities for automatically proving theorems and generating dynamic figures in the created textbooks. This paper provides a comprehensive account of the current version of Electronic Geometry Textbook.},
booktitle = {Proceedings of the 10th ASIC and 9th MKM International Conference, and 17th Calculemus Conference on Intelligent Computer Mathematics},
pages = {278–292},
numpages = {15},
location = {Paris, France},
series = {AISC'10/MKM'10/Calculemus'10}
}

@inproceedings{10.1145/3267494.3267499,
author = {Zhu, Jianping and Song, Wei and Zhu, Ziyuan and Ying, Jiameng and Li, Boya and Tu, Bibo and Shi, Gang and Hou, Rui and Meng, Dan},
title = {CPU Security Benchmark},
year = {2018},
isbn = {9781450359917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267494.3267499},
doi = {10.1145/3267494.3267499},
abstract = {The current electronic-economy is booming, electronic-wallets, encrypted virtual-money, mobile payments, and other new generations of economic instruments are springing up. As the most important cornerstone, CPU is facing serious security challenges. And with the blowout of actual application requirements, the importance of CPU security testing is increasing. However, the actual security threats to computer systems are also becoming increasingly rampant (now attackers often use multiple different types of vulnerabilities to construct complex attack systems, not just a single attack chain). The traditional vulnerability detection model is not capable of comprehensive security assessment. We first proposed a comprehensive CPU Security Benchmark solution with high coverage for existing known vulnerabilities, including Undocumented Instructions detection, Control Flow Integrity test, Memory Errors detection, and Cache Side Channels detection, Out of Order and Speculative execution vulnerabilities (Meltdown and Spectre series) tests, and more. Our benchmark provides meaningful and constructive feedbacks for evading architecture/microarchitecture design flaws, system security (OS and libraries) software patches design, and user programming vulnerabilities tips. We hope that the work of this paper will promote the computer system security testing from the past scatter point and line mode (single specific vulnerability and attack chain testing) to coordinated and whole surface mode (multi-type vulnerabilities and attack network testing), thus creating a new research direction of the comprehensive and balanced CPU Security Benchmark. Our test suite will play an inspiring role in the comprehensive assessment of security in personal computer devices (PC/Mobile Phone) and large server clusters (Servers/Cloud), as well as the construction of more secure Block-Chain nodes (IOT), and many other practical applications.},
booktitle = {Proceedings of the 1st Workshop on Security-Oriented Designs of Computer Architectures and Processors},
pages = {8–14},
numpages = {7},
keywords = {vulnerability detection, cpu security, comprehensive benchmark},
location = {Toronto, Canada},
series = {SecArch'18}
}

@inproceedings{10.1145/3526071.3527521,
author = {Canelas, Paulo and Tavares, Miguel and Cordeiro, Ricardo and Fonseca, Alcides and Timperley, Christopher S.},
title = {An Experience Report on Challenges in Learning the Robot Operating System},
year = {2023},
isbn = {9781450393171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526071.3527521},
doi = {10.1145/3526071.3527521},
abstract = {The Robot Operating System (ROS) was initially introduced to lower the barriers to robots software development by reducing the need for extensive domain knowledge. ROS allows developers to build valuable robots by configuring and reusing off-the-shelf components while writing little, if any, code through its modular design, loosely coupled architecture, and rich package ecosystem. However, despite the advantages of this approach, the lack of documentation can present a challenge to novice users.In this work, we discuss the challenges and experience of learning and using ROS from the perspective of three novice users with little to no prior experience in robotics. We report on the experiences in learning ROS through a popular commercial training course provided by The Construct Sim. Through our analysis, we identify several common misunderstandings, mistakes, and bugs, and we outline possible improvements to ROS to overcome these challenges.Our findings motivate further studies on the development of robotic systems in ROS by novice users and promote the improvement of the ROS ecosystem, on educational and training materials of ROS, and on tooling development to help novices identify and correct simple mistakes.},
booktitle = {Proceedings of the 4th International Workshop on Robotics Software Engineering},
pages = {33–38},
numpages = {6},
keywords = {usability, developer experience, robot operating system, python},
location = {Pittsburgh, Pennsylvania},
series = {RoSE '22}
}

@inproceedings{10.1145/1029632.1029678,
author = {Chatty, St\'{e}phane and Sire, St\'{e}phane and Vinot, Jean-Luc and Lecoanet, Patrick and Lemort, Alexandre and Mertz, Christophe},
title = {Revisiting Visual Interface Programming: Creating GUI Tools for Designers and Programmers},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029678},
doi = {10.1145/1029632.1029678},
abstract = {Involving graphic designers in the large-scale development of user interfaces requires tools that provide more graphical flexibility and support efficient software processes. These requirements were analysed and used in the design of the TkZ-inc graphical library and the IntuiKit interface design environment. More flexibility is obtained through a wider palette of visual techniques and support for iterative construction of images, composition and parametric displays. More efficient processes are obtained with the use of the SVG standard to import graphics, support for linking graphics and behaviour, and a unifying model-driven architecture. We describe the corresponding features of our tools, and show their use in the development of an application for airports. Benefits include a wider access to high quality visual interfaces for specialised applications, and shorter prototyping and development cycles for multidisciplinary teams.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {267–276},
numpages = {10},
keywords = {software architecture, vector graphics, visual design, GUI tools, SVG, model-driven architecture},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/384197.384215,
author = {Kadayif, I. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M. J. and Ramanujam, J.},
title = {Morphable Cache Architectures: Potential Benefits},
year = {2001},
isbn = {1581134258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/384197.384215},
doi = {10.1145/384197.384215},
abstract = {Computer architects have tried to mitigate the consequences of high memory latencies using a variety techniques. An example of these techniques is multi-level caches to counteract the latency that results from having a memory that is slower than the processor. Recent research has demonstrated that compiler optimizations that modify data layouts and restructure computation can be successful in improving memory system performance. However, in many cases, working with a fixed cache configuration prevents the application/compiler from obtaining the maximum performance. In addition, prompted by demands in portability, long battery life, and low-cost packaging, the computer industry has started viewing energy and power as decisive design factors, along with performance and cost. This makes the job of the compiler/user even more difficult as one needs to strike a balance between low power/energy consumption and high performance. Consequently, adapting the code to the underlying cache/memory hierarchy is becoming more and more difficult.In this paper, we take an alternate approach and attempt to adapt the cache architecture to the software needs. We focus on array-dominated applications and measure the potential benefits that could be gained from a morphable (reconfigurable) cache architecture. Our results show that not only different applications work best with different cache configurations, but also that different loop nests in a given application demand different configurations. Our results also indicate that the most suitable cache configuration for a given application or a single nest depends strongly on the objective function being optimized. For example, minimizing cache memory energy requires a different cache configuration for each nest than an objective which tries to minimize the overall memory system energy. Based on our experiments, we conclude that fine-grain (loop nest-level) cache configuration management is an important step for a solution to the challenging architecture/software tradeoffs awaiting system designers in the future.},
booktitle = {Proceedings of the ACM SIGPLAN Workshop on Languages, Compilers and Tools for Embedded Systems},
pages = {128–137},
numpages = {10},
location = {Snow Bird, Utah, USA},
series = {LCTES '01}
}

@article{10.1145/384196.384215,
author = {Kadayif, I. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M. J. and Ramanujam, J.},
title = {Morphable Cache Architectures: Potential Benefits},
year = {2001},
issue_date = {Aug. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/384196.384215},
doi = {10.1145/384196.384215},
abstract = {Computer architects have tried to mitigate the consequences of high memory latencies using a variety techniques. An example of these techniques is multi-level caches to counteract the latency that results from having a memory that is slower than the processor. Recent research has demonstrated that compiler optimizations that modify data layouts and restructure computation can be successful in improving memory system performance. However, in many cases, working with a fixed cache configuration prevents the application/compiler from obtaining the maximum performance. In addition, prompted by demands in portability, long battery life, and low-cost packaging, the computer industry has started viewing energy and power as decisive design factors, along with performance and cost. This makes the job of the compiler/user even more difficult as one needs to strike a balance between low power/energy consumption and high performance. Consequently, adapting the code to the underlying cache/memory hierarchy is becoming more and more difficult.In this paper, we take an alternate approach and attempt to adapt the cache architecture to the software needs. We focus on array-dominated applications and measure the potential benefits that could be gained from a morphable (reconfigurable) cache architecture. Our results show that not only different applications work best with different cache configurations, but also that different loop nests in a given application demand different configurations. Our results also indicate that the most suitable cache configuration for a given application or a single nest depends strongly on the objective function being optimized. For example, minimizing cache memory energy requires a different cache configuration for each nest than an objective which tries to minimize the overall memory system energy. Based on our experiments, we conclude that fine-grain (loop nest-level) cache configuration management is an important step for a solution to the challenging architecture/software tradeoffs awaiting system designers in the future.},
journal = {SIGPLAN Not.},
month = {aug},
pages = {128–137},
numpages = {10}
}

@inproceedings{10.1145/384198.384215,
author = {Kadayif, I. and Kandemir, M. and Vijaykrishnan, N. and Irwin, M. J. and Ramanujam, J.},
title = {Morphable Cache Architectures: Potential Benefits},
year = {2001},
isbn = {1581134266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/384198.384215},
doi = {10.1145/384198.384215},
abstract = {Computer architects have tried to mitigate the consequences of high memory latencies using a variety techniques. An example of these techniques is multi-level caches to counteract the latency that results from having a memory that is slower than the processor. Recent research has demonstrated that compiler optimizations that modify data layouts and restructure computation can be successful in improving memory system performance. However, in many cases, working with a fixed cache configuration prevents the application/compiler from obtaining the maximum performance. In addition, prompted by demands in portability, long battery life, and low-cost packaging, the computer industry has started viewing energy and power as decisive design factors, along with performance and cost. This makes the job of the compiler/user even more difficult as one needs to strike a balance between low power/energy consumption and high performance. Consequently, adapting the code to the underlying cache/memory hierarchy is becoming more and more difficult.In this paper, we take an alternate approach and attempt to adapt the cache architecture to the software needs. We focus on array-dominated applications and measure the potential benefits that could be gained from a morphable (reconfigurable) cache architecture. Our results show that not only different applications work best with different cache configurations, but also that different loop nests in a given application demand different configurations. Our results also indicate that the most suitable cache configuration for a given application or a single nest depends strongly on the objective function being optimized. For example, minimizing cache memory energy requires a different cache configuration for each nest than an objective which tries to minimize the overall memory system energy. Based on our experiments, we conclude that fine-grain (loop nest-level) cache configuration management is an important step for a solution to the challenging architecture/software tradeoffs awaiting system designers in the future.},
booktitle = {Proceedings of the 2001 ACM SIGPLAN Workshop on Optimization of Middleware and Distributed Systems},
pages = {128–137},
numpages = {10},
location = {Snow Bird, Utah, USA},
series = {OM '01}
}

@inproceedings{10.1145/1064092.1064104,
author = {Wein, Ron and van den Berg, Jur P. and Halperin, Dan},
title = {The Visibility--Voronoi Complex and Its Applications},
year = {2005},
isbn = {1581139918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1064092.1064104},
doi = {10.1145/1064092.1064104},
abstract = {We introduce a new type of diagram called the VV(c)-diagram (the Visibility--Voronoi diagram for clearance c), which is a hybrid between the visibility graph and the Voronoi diagram of polygons in the plane. It evolves from the visibility graph to the Voronoi diagram as the parameter c grows from 0 to ∞. This diagram can be used for planning natural-looking paths for a robot translating amidst polygonal obstacles in the plane. A natural-looking path is short, smooth, and keeps --- where possible --- an amount of clearance c from the obstacles. The VV(c)-diagram contains such paths. We also propose an algorithm that is capable of preprocessing a scene of configuration-space polygonal obstacles and constructs a data structure called the VV(c)-complex. The VV(c)-complex can be used to efficiently plan motion paths for any start and goal configuration and any clearance value c, without having to explicitly construct the VV(c)-diagram for that c-value. The preprocessing time is O(n2 log n), where n is the total number of obstacle vertices, and the data structure can be queried directly for any c-value by merely performing a Dijkstra search. We have implemented a Cgal-based software package for computing the VV(c)-diagram in an exact manner for a given clearance value, and used it to plan natural-looking paths in various applications.},
booktitle = {Proceedings of the Twenty-First Annual Symposium on Computational Geometry},
pages = {63–72},
numpages = {10},
keywords = {path optimization, motion planning, visibility graphs, voronoi diagrams},
location = {Pisa, Italy},
series = {SCG '05}
}

@inproceedings{10.1145/2986012.2986019,
author = {Kuramitsu, Kimio},
title = {Nez: Practical Open Grammar Language},
year = {2016},
isbn = {9781450340762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2986012.2986019},
doi = {10.1145/2986012.2986019},
abstract = {Nez is a PEG(Parsing Expressing Grammar)-based open grammar language that allows us to describe complex syntax constructs without action code. Since open grammars are declarative and free from a host programming language of parsers, software engineering tools and other parser applications can reuse once-defined grammars across programming languages. A key challenge to achieve practical open grammars is the expressiveness of syntax constructs and the resulting parser performance, as the traditional action code approach has provided very pragmatic solutions to these two issues. In Nez, we extend the symbol-based state management to recognize context-sensitive language syntax, which often appears in major programming languages. In addition, the Abstract Syntax Tree constructor allows us to make flexible tree structures, including the left-associative pair of trees. Due to these extensions, we have demonstrated that Nez can parse many grammars of practical programming languages. Nez can generate various types of parsers since all Nez operations are independent of a specific parser language. To highlight this feature, we have implemented Nez with dynamic parsing, which allows users to integrate a Nez parser as a parser library that loads a grammar at runtime. To achieve its practical performance, Nez operators are assembled into low-level virtual machine instructions, including automated state modifications when backtracking, transactional controls of AST construction, and efficient memoization in packrat parsing. We demonstrate that Nez dynamic parsers achieve very competitive performance compared to existing efficient parser generators.},
booktitle = {Proceedings of the 2016 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {29–42},
numpages = {14},
keywords = {and grammar specification language, Parsing expression grammars, parser generators, context-sensitive grammars},
location = {Amsterdam, Netherlands},
series = {Onward! 2016}
}

@inproceedings{10.1145/347324.349099,
author = {Wittenberg, Craig H.},
title = {Progress in Testing Component-Based Software (Abstract Only)},
year = {2000},
isbn = {1581132662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/347324.349099},
doi = {10.1145/347324.349099},
abstract = {Software components enable practical reuse of software parts and amortization of investments over multiple applications. Each part or component is well defined and independently deployable. Composition is the key technique by which systems of software components are constructed. The composition step can be done before or after the delivery of the system. It is this late composition (or at least the possibility of it) which yields the greatest challenges from a testing standpoint. That is, a component-based application may be composed out of parts that were never tested together. Thus the most useful and reliable parts are those which have been tested independently in as many ways as possible.The Component Applications Group in Microsoft Research is developing tools, techniques, and a large component library to enable the development of sophisticated office, home and web-based applications. For the past three and a half years we have been working on two main efforts. First, we have created a prototype of a highly factored (i.e., customizable, flexible, etc.) architecture for the construction of the UI of applications. Our work can be applied to traditional window-ed applications as well as to the look and feel of Web applications. During this effort we have developed a variety of design techniques, two different composition mechanisms, a visual tool for compositions, and have built several application prototypes out of the same set of components.Most of our time has been spent on tools and techniques for building reliable components. Certain pieces of our infrastructure formed the domain in which we tried out our ideas. The first component we tested was one of our composition mechanisms. That was followed by the testing of a dynamic, binary, aspect composition mechanism and of a particularly generic implementation of collection classes. Our well-factored, versioned build system will also be described. All of the results of our work are compatible with COM.The talk will focus on our key lessons in composition, specification, processes, and tools with a particular emphasis on our test harness and our results in testing. A discussion of the last few bugs found in each of several projects should prove intersting. Some comparisons will be made with other projects inside and outside Microsoft. Since we can only claim progress, not perfection, there are still many areas for further research. As an example, we are looking at ways we can use language annotations to simplifying whole classes of problems (e.g., tests for reentrancy). One of the points here is that we can improve our ability to create reliable components by improving the languages used to implement them (like Java has popularized the use of a garbage collector). Another example is that we hope to improve the automation of the sequencing of test cases.Finally, as a tribute to the power of standing on other's shoulders, many of the roots of our ideas will be traced to techniques published elsewhere. You might say we only composed together many already good ideas. Our group includes people who developed COM itself (myself and Tony Williams), many people from within Microsoft who have delivered successful component-based products (e.g., in Visual Studio), and world-renowned component-ologist (:-) Clemens Szyperski who wrote Component Software: Beyond Object-Oriented Programming.},
booktitle = {Proceedings of the 2000 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {178},
location = {Portland, Oregon, USA},
series = {ISSTA '00}
}

@article{10.1145/347636.349099,
author = {Wittenberg, Craig H.},
title = {Progress in Testing Component-Based Software (Abstract Only)},
year = {2000},
issue_date = {Sept. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/347636.349099},
doi = {10.1145/347636.349099},
abstract = {Software components enable practical reuse of software parts and amortization of investments over multiple applications. Each part or component is well defined and independently deployable. Composition is the key technique by which systems of software components are constructed. The composition step can be done before or after the delivery of the system. It is this late composition (or at least the possibility of it) which yields the greatest challenges from a testing standpoint. That is, a component-based application may be composed out of parts that were never tested together. Thus the most useful and reliable parts are those which have been tested independently in as many ways as possible.The Component Applications Group in Microsoft Research is developing tools, techniques, and a large component library to enable the development of sophisticated office, home and web-based applications. For the past three and a half years we have been working on two main efforts. First, we have created a prototype of a highly factored (i.e., customizable, flexible, etc.) architecture for the construction of the UI of applications. Our work can be applied to traditional window-ed applications as well as to the look and feel of Web applications. During this effort we have developed a variety of design techniques, two different composition mechanisms, a visual tool for compositions, and have built several application prototypes out of the same set of components.Most of our time has been spent on tools and techniques for building reliable components. Certain pieces of our infrastructure formed the domain in which we tried out our ideas. The first component we tested was one of our composition mechanisms. That was followed by the testing of a dynamic, binary, aspect composition mechanism and of a particularly generic implementation of collection classes. Our well-factored, versioned build system will also be described. All of the results of our work are compatible with COM.The talk will focus on our key lessons in composition, specification, processes, and tools with a particular emphasis on our test harness and our results in testing. A discussion of the last few bugs found in each of several projects should prove intersting. Some comparisons will be made with other projects inside and outside Microsoft. Since we can only claim progress, not perfection, there are still many areas for further research. As an example, we are looking at ways we can use language annotations to simplifying whole classes of problems (e.g., tests for reentrancy). One of the points here is that we can improve our ability to create reliable components by improving the languages used to implement them (like Java has popularized the use of a garbage collector). Another example is that we hope to improve the automation of the sequencing of test cases.Finally, as a tribute to the power of standing on other's shoulders, many of the roots of our ideas will be traced to techniques published elsewhere. You might say we only composed together many already good ideas. Our group includes people who developed COM itself (myself and Tony Williams), many people from within Microsoft who have delivered successful component-based products (e.g., in Visual Studio), and world-renowned component-ologist (:-) Clemens Szyperski who wrote Component Software: Beyond Object-Oriented Programming.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {aug},
pages = {178},
numpages = {1}
}

@inproceedings{10.1145/1449814.1449857,
author = {Anslow, Craig and Noble, James and Marshall, Stuart and Tempero, Ewan},
title = {Visualizing the Word Structure of Java Class Names},
year = {2008},
isbn = {9781605582207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449814.1449857},
doi = {10.1145/1449814.1449857},
abstract = {Large amounts of software have been written since the Java language was created. There is little known about the word structure Java within class names. We have created visualizations of words used in class names from the Java API specification and 91 open-source Java applications. Our visualizations will help expose which words are used in practice.},
booktitle = {Companion to the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications},
pages = {777–778},
numpages = {2},
keywords = {java, software visualization, class names, evolution},
location = {Nashville, TN, USA},
series = {OOPSLA Companion '08}
}

@inproceedings{10.1109/CCGRID.2017.103,
author = {Gugnani, Shashank and Lu, Xiaoyi and Panda, Dhabaleswar K. (DK)},
title = {Swift-X: Accelerating OpenStack Swift with RDMA for Building an Efficient HPC Cloud},
year = {2017},
isbn = {9781509066100},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2017.103},
doi = {10.1109/CCGRID.2017.103},
abstract = {Running Big Data applications in the cloud has become extremely popular in recent times. To enable the storage of data for these applications, cloud-based distributed storage solutions are a must. OpenStack Swift is an object storage service which is widely used for such purposes. Swift is one of the main components of the OpenStack software package. Although Swift has become extremely popular in recent times, its proxy server based design limits the overall throughput and scalability of the cluster. Swift is based on the traditional TCP/IP sockets based communication which has known performance issues such as context-switch and buffer copies for each message transfer. Modern high-performance interconnects such as InfiniBand and RoCE offer advanced features such as RDMA and provide high bandwidth and low latency communication. In this paper, we propose two new designs to improve the performance and scalability of Swift. We propose changes to the Swift architecture and operation design. We propose high-performance implementations of network communication and I/O modules based on RDMA to provide the fastest possible object transfer. In addition, we use efficient hashing algorithms to accelerate object verification in Swift. Experimental evaluations with microbenchmarks, Swift stack benchmark (ssbench), and synthetic application workloads reveal up to 2x and 7.3x performance improvement with our two proposed designs for put and get operations. To the best of our knowledge, this is the first work towards accelerating OpenStack Swift with RDMA over high-performance interconnects in the literature.},
booktitle = {Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {238–247},
numpages = {10},
keywords = {RDMA, Swift, OpenStack, High-performance interconnects},
location = {Madrid, Spain},
series = {CCGrid '17}
}

@article{10.1145/3267419.3267425,
author = {Oginni, Oluwaseyi and Bull, Peter and Wang, Yonghao},
title = {Constraint-Aware Software-Defined Network for Routing Real-Time Multimedia},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
url = {https://doi.org/10.1145/3267419.3267425},
doi = {10.1145/3267419.3267425},
abstract = {Traditional Ethernet-based IP networks do not have the capability to provide the Quality of Service (QoS) required for professional real-time multimedia applications. This is because they operate on a best-effort network service model that does not provide service guarantee. Network operators and service providers require a novel network architecture to efficiently handle the increasing demands of this changing network domain. Software-Defined Networking has emerged as an effective network architecture that decouples the control plane and data plane, which makes it capable of handling the dynamic nature of future network functions and intelligent applications while reducing cost through simplified hardware, software, and management.This paper presents an SDN architecture for real-time low latency applications that offer adaptive path provisioning based on the calculated end-to-end delay, available bandwidth, and traditional shortest path first algorithm. The SDN architecture utilises the Ryu OpenFlow application programming interface (API) to perform real-time monitoring to collect network statistics and computes the appropriate paths by using this information. The experiment to ascertain the feasibility and evaluate the effectiveness of this approach is carried out in an emulated network environment using Mininet.},
journal = {SIGBED Rev.},
month = {aug},
pages = {37–42},
numpages = {6},
keywords = {controller, openflow, real-time, software defined networking, API}
}

@inproceedings{10.1145/974044.974064,
author = {Brataas, Gunnar and Hughes, Peter},
title = {Exploring Architectural Scalability},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974064},
doi = {10.1145/974044.974064},
abstract = {We describe a structured, hierarchic approach to exploring the scalability of IT systems architectures. An architecture is considered to be scalable over a particular set of requirements if the physical resource usage per unit of capacity remains roughly constant. For completeness, both requirements and capacity must be defined in the three dimensions of processing, storage and connectivity. Interactions between the three dimensions are considered, as are various forms of departure from non-uniform scaling. Scalability is explored via a combination of measurement and static and dynamic models. Appropriate scale-invariants are introduced to eliminate congestion effects and packaging issues from the analysis. This paper focuses on processing and to a lesser extent, on storage. The method is applied to a practical case study of Transigo, a J2EE-based software platform used in the Norwegian banking industry. We find that understanding the relationship between replication and upgrade for systems, subsystems and devices is key to guiding the exploration of scalability.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {125–129},
numpages = {5},
keywords = {J2EE, scalability, software architecture, software performance},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@article{10.1145/974043.974064,
author = {Brataas, Gunnar and Hughes, Peter},
title = {Exploring Architectural Scalability},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/974043.974064},
doi = {10.1145/974043.974064},
abstract = {We describe a structured, hierarchic approach to exploring the scalability of IT systems architectures. An architecture is considered to be scalable over a particular set of requirements if the physical resource usage per unit of capacity remains roughly constant. For completeness, both requirements and capacity must be defined in the three dimensions of processing, storage and connectivity. Interactions between the three dimensions are considered, as are various forms of departure from non-uniform scaling. Scalability is explored via a combination of measurement and static and dynamic models. Appropriate scale-invariants are introduced to eliminate congestion effects and packaging issues from the analysis. This paper focuses on processing and to a lesser extent, on storage. The method is applied to a practical case study of Transigo, a J2EE-based software platform used in the Norwegian banking industry. We find that understanding the relationship between replication and upgrade for systems, subsystems and devices is key to guiding the exploration of scalability.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {125–129},
numpages = {5},
keywords = {scalability, J2EE, software architecture, software performance}
}

@inproceedings{10.1145/3021460.3021489,
author = {Barn, Balbir and Barat, Souvik and Clark, Tony},
title = {Conducting Systematic Literature Reviews and Systematic Mapping Studies},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021489},
doi = {10.1145/3021460.3021489},
abstract = {Context: An essential part of conducting software engineering (SE) research is the ability to identify extant research on tools, technologies, concepts and methods in order to evaluate and make rational and scientific decisions. The domain from which such knowledge is extracted is typically existing research literature found in journals, conference proceedings, books and gray literature. Empirical approaches that include various systematic review (SR) methodologies such as systematic literature review (SLR) and systematic mapping study (SMS) are found to be effective in this context. They adopt rigorous planning, follow repeatable and well-defined processes, and produce unbiased and evidence-based outcomes. Despite these significant benefits, the general trend on using these systematic review (SR) methodologies is not encouraging in SE research. The primary reasons emerging are twofold - a) SR methodologies are largely cited as time-consuming activities and b) lack of guidance to conduct systematic reviews. This tutorial discusses these concerns and describes an effective way of using SR methodologies for SE research.Objectives: Attendees will be introduced to the key concepts, methods and processes for conducting systematic literature reviews (SLR) and systematic mapping studies (SMS). The benefits, limitations, guidelines for using SR methodologies in an effective manner will discussed in the session. Attendees will be guided on the appropriate formulation of a research question and sub questions; the development of a review protocol such as inclusion criteria, exclusion criteria, quality criteria and classification structures; and execution of review protocol using digital libraries and syntheses of review data. A web based software tool1, for supporting the systematic literature review process will be demonstrated and attendees will get the opportunity to use the tool to conduct the review to help in identification of relevant research and extraction and synthesis of data.Method: We will use a blend of information presentation, interactive hands-on session and knowledge sharing session. The presentation will introduce the key concepts, benefits, limitations and how to overcome the limitations; hands on session will illustrate a review process with a case study, and finally the knowledge sharing session will discuss the experiences, best practices and the lesson learnt.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {212–213},
numpages = {2},
keywords = {Model Based Literature Review, Systematic Mapping Study, Meta Modeling, Literature Review, Systematic Literature Review},
location = {Jaipur, India},
series = {ISEC '17}
}

@inproceedings{10.1145/2951913.2951933,
author = {Dagand, Pierre-Evariste and Tabareau, Nicolas and Tanter, \'{E}ric},
title = {Partial Type Equivalences for Verified Dependent Interoperability},
year = {2016},
isbn = {9781450342193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2951913.2951933},
doi = {10.1145/2951913.2951933},
abstract = {Full-spectrum dependent types promise to enable the development of correct-by-construction software. However, even certified software needs to interact with simply-typed or untyped programs, be it to perform system calls, or to use legacy libraries. Trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simply-typed values can safely be coerced to dependent types and, conversely, dependently-typed programs can defensively be exported to a simply-typed application. In this paper, we give a semantic account of dependent interoperability. Our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. Specifically, we develop the notion of partial type equivalences as a key foundation for dependent interoperability. Our framework is developed in Coq; it is thus constructive and verified in the strictest sense of the terms. Using our library, users can specify domain-specific partial equivalences between data structures. Our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. It thus becomes possible, as we shall illustrate, to internalize and hand-tune the extraction of dependently-typed programs to interoperable OCaml programs within Coq itself.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Functional Programming},
pages = {298–310},
numpages = {13},
keywords = {interoperability, dependent types, type equivalences},
location = {Nara, Japan},
series = {ICFP 2016}
}

@article{10.1145/3022670.2951933,
author = {Dagand, Pierre-Evariste and Tabareau, Nicolas and Tanter, \'{E}ric},
title = {Partial Type Equivalences for Verified Dependent Interoperability},
year = {2016},
issue_date = {September 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022670.2951933},
doi = {10.1145/3022670.2951933},
abstract = {Full-spectrum dependent types promise to enable the development of correct-by-construction software. However, even certified software needs to interact with simply-typed or untyped programs, be it to perform system calls, or to use legacy libraries. Trading static guarantees for runtime checks, the dependent interoperability framework provides a mechanism by which simply-typed values can safely be coerced to dependent types and, conversely, dependently-typed programs can defensively be exported to a simply-typed application. In this paper, we give a semantic account of dependent interoperability. Our presentation relies on and is guided by a pervading notion of type equivalence, whose importance has been emphasized in recent work on homotopy type theory. Specifically, we develop the notion of partial type equivalences as a key foundation for dependent interoperability. Our framework is developed in Coq; it is thus constructive and verified in the strictest sense of the terms. Using our library, users can specify domain-specific partial equivalences between data structures. Our library then takes care of the (sometimes, heavy) lifting that leads to interoperable programs. It thus becomes possible, as we shall illustrate, to internalize and hand-tune the extraction of dependently-typed programs to interoperable OCaml programs within Coq itself.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {298–310},
numpages = {13},
keywords = {interoperability, dependent types, type equivalences}
}

@inproceedings{10.1145/3425269.3425271,
author = {Nicolodi, Luciane Baldo and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Architectural Feature Re-Modularization for Software Product Line Evolution},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425271},
doi = {10.1145/3425269.3425271},
abstract = {Extensive maintenance leads to the Software Product Line Architecture (PLA) degradation over time. When there is the need of evolving the Software Product Line (SPL) to include new features, or move to a new platform, a degraded PLA requires considerable effort to understand and modify, demanding expensive refactoring activity. In the state of the art, search-based algorithms are used to improve PLA at package level. However, recent studies have shown that the most variability and implementation details of an SPL are described in the level of classes. There is a gap between existing approaches and existing practical needs. In this work, we extend the current state of the art to deal with feature modularization in the level of classes by introducing a new search operator and a set of objective functions to deal with feature modularization in a finer granularity of the architectural elements, namely at class level. We evaluated the proposal in an exploratory study with a PLA widely investigated and a real-world PLA. The results of quantitative and qualitative analysis point out that our proposal provides solutions to properly re-modularize features in a PLA, being preferred by practitioners, in order to support the evolution of SPLs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {Search-based Software Engineering, Software Evolution, Architectural Degradation, Feature Modularization},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/322917.323085,
author = {Olmstead, Roger},
title = {Compilers and Parallel Architectures (Abstract Only): Sequential to Parallel Mapping Strategies},
year = {1987},
isbn = {0897912187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322917.323085},
doi = {10.1145/322917.323085},
abstract = {The parallel optimizing compiler is offered as the only viable means of fully exploiting the power of parallel architectures and applying it to mainstream computing problems. In this context, “mainstream” includes -but should not be limited to- scientific computing, which generally implies the solving of partial differential equations, fast Fourier transforms, matrix manipulations, etc. Basically our motivation is to provide the fast execution of a broad class of programs.The requirements of general parallel computing are examined from an engineering perspective and architectures are evaluated in the context of accepted standards, These standards include cost, reliability/fault tolerance, and performance. To simplify the evaluation process, the standards are applied to classes rather than individual architectures. Two classes of architectures exist: algorithmically and computationally specialized architectures (e.g., systolic arrays, SIMD machines, associative processors) and computationally generalized architectures (e.g., MIMD homogeneous multiprocessors with shared central memory). As each standard is applied, generalized architectures invariably prove superior. Although some algorithmically specialized computers, (e.g., systolic arrays) are admittedly less expensive than their more generalized counterparts (though this is not the case with most SIMD machines), total system cost rapidly becomes prohibitive when one considers the vast number of possible algorithms encountered in solving mainstream computing problems. Reliability also favors the generalized machine. Clearly, a machine rich in data paths and redundancy offers superior fault tolerance. Concerning performance, while it is true that algorithmically specialized architectures can occasionally provide nearly optimal solutions to certain specific problems, generalized architectures can produce both high-throughput and rapid turnaround. Indeed, a generalized machine might make use of one or more algorithmically specialized architectures, much like a standard system maintains a library of specialized software routines. In sum, specialized architectural approaches are fundamentally flawed and not sufficient to solve mainstream computing problems.Having opted for generalized parallel architectures, two approaches for their exploitation are evaluated: parallel languages and parallel optimizing compilers. Languages include both those that were designed for parallel programming, like Occam, as well as those sequential languages with added parallel constructs. Standards, similar to those employed above but modified for application to software, are used to evaluate each approach. These standards also include cost and performance, but in the place of reliability, maintainability, and portability are substituted. When examined in light of these standards, the parallel language approach is shown to be inferior in nearly every case. First, software science has shown, empirically, that increasing the quantity of instructions required to perform a given task has a direct, and negative, effect on both programmer productivity and software quality. Thus, the use of additional constructs required to express an algorithm using a parallel language has an adverse effect on the overall cost of the program; and although hand coded programs often more efficient than their compiled counterparts, industry studies have shown that the gain in performance attributable to hand coding is less than twenty percent. Programs coded in a parallel language are also more difficult to maintain. For example, a program might be modified in such a way that the original process-partitioning is no longer optimal; still worse, it may no longer function at all. This problem is not encountered when utilizing sequential programming languages. Concerning portability, if the source program written in a parallel language is moved to a machine with a different architecture (i.e., one having dissimilar process creation, synchronization, and communication costs), it will be less efficient and, more likely, incorrect. It becomes obvious that the criticisms leveled against parallel languages are similar to those favoring the use of high order, rather than assembly languages. This issue points to an underlying philosophical tenet: that is, humans should be freed, whenever possible, from concern over details that can best be left to the automation. Finally, and most importantly, recent research in psycho-linguistics has demonstrated that the structure of natural language is fundamentally sequential. The human brain is inherently designed to map even parallel ideas onto a sequential medium. Thus, the parallel language approach is inimical to human cognitive processes and its application for widespread use in parallel computing is extremely limited.It is clear then, that our goal should be the development of powerful, generalized parallel architectures and the design of compilers capable of extracting the maximum amount of parallelism inherent in a program.},
booktitle = {Proceedings of the 15th Annual Conference on Computer Science},
pages = {424},
location = {St. Louis, Missouri, USA},
series = {CSC '87}
}

@article{10.1145/1416563.1416566,
author = {Huang, Shan Shan and Zook, David and Smaragdakis, Yannis},
title = {Domain-Specific Languages and Program Generation with Meta-AspectJ},
year = {2008},
issue_date = {November 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1416563.1416566},
doi = {10.1145/1416563.1416566},
abstract = {Meta-AspectJ (MAJ) is a language for generating AspectJ programs using code templates. MAJ itself is an extension of Java, so users can interleave arbitrary Java code with AspectJ code templates. MAJ is a structured metaprogramming tool: a well-typed generator implies a syntactically correct generated program. MAJ promotes a methodology that combines aspect-oriented and generative programming. A valuable application is in implementing small domain-specific language extensions as generators using unobtrusive annotations for syntax extension and AspectJ as a back-end. The advantages of this approach are twofold. First, the generator integrates into an existing software application much as a regular API or library, instead of as a language extension. Second, a mature language implementation is easy to achieve with little effort since AspectJ takes care of the low-level issues of interfacing with the base Java language.In addition to its practical value, MAJ offers valuable insights to metaprogramming tool designers. It is a mature metaprogramming tool for AspectJ (and, by extension, Java): a lot of emphasis has been placed on context-sensitive parsing and error reporting. As a result, MAJ minimizes the number of metaprogramming (quote/unquote) operators and uses type inference to reduce the need to remember type names for syntactic entities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {nov},
articleno = {6},
numpages = {32},
keywords = {domain-specific languages, program synthesis, program transformation, Metaprogramming, language extensions, program verification}
}

@article{10.1145/371254.371258,
author = {Chakrabarty, Krishnendu},
title = {Optimal Test Access Architectures for System-on-a-Chip},
year = {2001},
issue_date = {Jan. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/371254.371258},
doi = {10.1145/371254.371258},
abstract = {Test access is a major problem for core-based system-on-a-chip (SOC) designs. Since embedded cores in an SOC are not directly accessible via chip inputs and outputs, special access mechanisms are required to test them at the system level. An efficient test access architecture should also reduce test cost by minimizing test application time. We address several issues related to the design of optimal test access architectures that minimize testing time., including the assignment of cores to test buses, distribution of test data width between multiple test buses, and analysis of test data width required to satisfy an upper bound on the testing time. Even though the decision versions of all these problems are shown to be NP-complete, they can be solved exactly for practical instances using integer linear programming (ILP). As a case study, the ILP models for two hypothetical but nontrivial systems are solved using a public-domain ILP software package.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = {jan},
pages = {26–49},
numpages = {24}
}

@inproceedings{10.5555/2663370.2663384,
author = {Falessi, Davide and Shull, Forrest},
title = {Towards Flexible Automated Support to Improve the Quality of Computational Science and Engineering Software},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {Continual evolution of the available hardware (e.g. in terms of increasing size, architecture, and computing power) and software (e.g. reusable libraries) is the norm rather than exception. Our goal is to enable CSE developers to spend more of their time finding scientific results by capitalizing on these evolutions instead of being stuck in fixing software engineering (SE) problems such as porting the application to new hardware, debugging, reusing (unreliable) code, and integrating open source libraries. In this paper we sketch a flexible automated solution supporting scientists and engineers in developing accurate and reliable CSE applications. This solution, by collecting and analyzing product and process metrics, enables the application of well-established software engineering best practices (e.g., separation of concerns, regression testing and inspections) and it is based upon the principles of automation, flexibility and iteration.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {88–91},
numpages = {4},
keywords = {computational science and engineering software, empirical software engineering},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@article{10.1145/1498698.1658383,
author = {Loera, Jes\'{u}s A. De and Haws, David C. and Lee, Jon and O'Hair, Allison},
title = {Computation in Multicriteria Matroid Optimization},
year = {2010},
issue_date = {2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
issn = {1084-6654},
url = {https://doi.org/10.1145/1498698.1658383},
doi = {10.1145/1498698.1658383},
abstract = {Motivated by recent work on algorithmic theory for nonlinear and multicriteria matroid optimization, we have developed algorithms and heuristics aimed at practical solution of large instances of some of these difficult problems. Our methods primarily use the local adjacency structure inherent in matroid polytopes to pivot to feasible solutions, which may or may not be optimal. We also present a modified breadth-first-search heuristic that uses adjacency to enumerate a subset of feasible solutions. We present other heuristics and provide computational evidence supporting our techniques. We implemented all of our algorithms in the software package MOCHA.},
journal = {ACM J. Exp. Algorithmics},
month = {jan},
articleno = {8},
numpages = {33},
keywords = {tabu search, nonlinear combinatorial optimization, multiobjective optimization, multicriteria optimization, matroid optimization, local search, Matroids}
}

@article{10.1145/3371119,
author = {Xia, Li-yao and Zakowski, Yannick and He, Paul and Hur, Chung-Kil and Malecha, Gregory and Pierce, Benjamin C. and Zdancewic, Steve},
title = {Interaction Trees: Representing Recursive and Impure Programs in Coq},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {POPL},
url = {https://doi.org/10.1145/3371119},
doi = {10.1145/3371119},
abstract = {Interaction trees (ITrees) are a general-purpose data structure for representing the behaviors of recursive programs that interact with their environments. A coinductive variant of “free monads,” ITrees are built out of uninterpreted events and their continuations. They support compositional construction of interpreters from event handlers, which give meaning to events by defining their semantics as monadic actions. ITrees are expressive enough to represent impure and potentially nonterminating, mutually recursive computations, while admitting a rich equational theory of equivalence up to weak bisimulation. In contrast to other approaches such as relationally specified operational semantics, ITrees are executable via code extraction, making them suitable for debugging, testing, and implementing software artifacts that are amenable to formal verification. We have implemented ITrees and their associated theory as a Coq library, mechanizing classic domain- and category-theoretic results about program semantics, iteration, monadic structures, and equational reasoning. Although the internals of the library rely heavily on coinductive proofs, the interface hides these details so that clients can use and reason about ITrees without explicit use of Coq’s coinduction tactics. To showcase the utility of our theory, we prove the termination-sensitive correctness of a compiler from a simple imperative source language to an assembly-like target whose meanings are given in an ITree-based denotational semantics. Unlike previous results using operational techniques, our bisimulation proof follows straightforwardly by structural induction and elementary rewriting via an equational theory of combinators for control-flow graphs.},
journal = {Proc. ACM Program. Lang.},
month = {dec},
articleno = {51},
numpages = {32},
keywords = {monads, Coq, compiler correctness, coinduction}
}

@inproceedings{10.1145/2598394.2598504,
author = {Bevilacqua, Vitoantonio and Pannarale, Paolo},
title = {A Semantic Expert System for the Evolutionary Design of Synthetic Gene Networks},
year = {2014},
isbn = {9781450328814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598394.2598504},
doi = {10.1145/2598394.2598504},
abstract = {This work tries to cope with the standardization issue by the adoption of model exchange standards like CellML, BioBrick standard biological parts and standard signal carriers for modeling purpose. The BioBricks are easily assemblable [1] standard DNA sequences coding for well-defined structures and functions and represent an effort to introduce the engineering principles of abstraction and standardization in synthetic biology. Web applications as GenoCAD [2] are available and implements an algorithm of syntax check of the circuits designed [3], while some other tools for automatic design and optimization of genetic circuits have appeared [4] and are also specific for BioBrick systems [5]. Our generated models are made of Standard Virtual Parts modular components. Model complexity includes more interaction dynamics than previous works. The inherent software complexity has been handled by a rational use of ontologies and rule engine. The database of parts and interactions is automatically created from publicly available whole system models. We implemented a genetic algorithm searching the space of possible genetic circuits for an optimal circuit meeting user defined input-output dynamics. The tools performing structural optimization usually use stochastic strategies, while those optimizing the parameters or matching the components for a given structure can take advantage of both stochastic and deterministic strategies. In most cases it is however necessary human intervention, for example to set the value of certain kinetic parameters. To our best knowledge no tool exists which does not show a couple of these limitations, then our tool is the only capable of using a library of parts, dynamically generated from other system models available from public databases [6]. The tool automatically infers the chemical and genetic interactions occurring between entities of the repository models and applies them in the target model if opportune. The repository models have to be modeled by a specific CellML standard, the Standard Virtual Parts (SVP) [7] formalism and the components have to be annotated with OWL for unique identifiers. The output is a sequence of readily composable biological components, deposited in the registry of parts, and a complete CellML kinetic model of the system. Accordingly, a model can be generated and simulated from a sequence of BioBrick, without any human intervention. Actual tools present a moderated degree of accuracy in the prediction of the behavior, principally due to the lack of consideration of many cellular factors. Despite the advances in molecular construction, modeling and fine-tuning the behavior of synthetic circuits remains extremely challenging [8]. We tried to cope with this issue of scalability by means of ontologies coupled with a rule engine [9]. Model complexity includes more interaction dynamics than previous works, including gene regulation, interaction between small molecules and proteins but also protein-protein and post-transcriptional regulation. The domain was described by using Ontology Web Language (OWL) ontologies in conjunction with CellML [10], while complex logic was added by Jess rules [11]. The system has been successfully tested on a single test case and looks towards the creation of a web platform [12].},
booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {39–40},
numpages = {2},
keywords = {dna, biological system modeling, expert systems, genetic algorithms, design automation},
location = {Vancouver, BC, Canada},
series = {GECCO Comp '14}
}

@inproceedings{10.1145/199688.199808,
author = {Tewari, Rajiv},
title = {Software Reuse and Object-Oriented Software Engineering in the Undergraduate Curriculum},
year = {1995},
isbn = {089791693X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/199688.199808},
doi = {10.1145/199688.199808},
abstract = {Software engineering education and practice are currently undergoing extensive re-evaluation and analysis in the light of new object-oriented software development techniques as the complexity of software development is rapidly increasing. There is a growing recognition that software reuse can contribute to increased productivity, and the programming paradigm that best supports software reuse is the object-oriented paradigm. Component-based software engineering is currently best facilitated by the object-oriented approach through reuse of available class libraries and application frameworks. We present a comparative analysis of the procedural and object-oriented paradigm from a pedagogic perspective, and show that object-oriented techniques are a logical progression of the well tested structured methodologies. We show that the object-oriented methodology better addresses the fundamental concepts and processes defined in the ACM/IEEE Computing Curricula '91.},
booktitle = {Proceedings of the Twenty-Sixth SIGCSE Technical Symposium on Computer Science Education},
pages = {253–257},
numpages = {5},
location = {Nashville, Tennessee, USA},
series = {SIGCSE '95}
}

@article{10.1145/199691.199808,
author = {Tewari, Rajiv},
title = {Software Reuse and Object-Oriented Software Engineering in the Undergraduate Curriculum},
year = {1995},
issue_date = {March 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/199691.199808},
doi = {10.1145/199691.199808},
abstract = {Software engineering education and practice are currently undergoing extensive re-evaluation and analysis in the light of new object-oriented software development techniques as the complexity of software development is rapidly increasing. There is a growing recognition that software reuse can contribute to increased productivity, and the programming paradigm that best supports software reuse is the object-oriented paradigm. Component-based software engineering is currently best facilitated by the object-oriented approach through reuse of available class libraries and application frameworks. We present a comparative analysis of the procedural and object-oriented paradigm from a pedagogic perspective, and show that object-oriented techniques are a logical progression of the well tested structured methodologies. We show that the object-oriented methodology better addresses the fundamental concepts and processes defined in the ACM/IEEE Computing Curricula '91.},
journal = {SIGCSE Bull.},
month = {mar},
pages = {253–257},
numpages = {5}
}

@inproceedings{10.1145/317500.317521,
author = {Fujita, Shohei},
title = {Self-Organizing Distributed Operating System: Implementation and Problem Using Ada},
year = {1987},
isbn = {0897912438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/317500.317521},
doi = {10.1145/317500.317521},
abstract = {This paper introduces a new concept: “Self-Organization” in distributed operating system. Up to now, the communication facilities in distributed systems were almost based on the assumption: the human intervention. Software systems for C I M (Computer Integrated Manufacturing), however, should be highly intelligent, i.e., highly dependable and dynamically evolving (and/or gracefully degrading) systems.The most serious drawback of Ada faced with developing distributed software is the rendezvous mechanism of tasking which provides one-to-one communication facility. One-to-many communication mechanism should be provided. Furthermore, communication facility in distributed system should be self-organized so as to fit the requirements of the problem such as in neural nets.The main idea of this paper is to “self-organize” the communication facilities according to (physical and/or virtual) structural change of communication networks. The mechanism and implementation strategy for the self-organizing distributed operating system are described. A prototype system-1 is being constructed on ten SUNs network using VERDIX Ada compiler. The advantages of using Ada over other languages in constructing distributed operating system are the dynamic data structure and package supported by separate compilation facility. Problems facing with this project also will be discussed.},
booktitle = {Proceedings of the 1987 Annual ACM SIGAda International Conference on Ada},
pages = {157–158},
numpages = {2},
location = {Boston, Massachusetts, USA},
series = {SIGAda '87}
}

@inproceedings{10.1145/3159450.3162353,
author = {Schocken, Shimon},
title = {Nand to Tetris: Building a Modern Computer System from First Principles (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162353},
doi = {10.1145/3159450.3162353},
abstract = {We present a full semester course in which students build a complete computer system - hardware and software - from the ground up. Also known as "Nand to Tetris", the course is presently taught at 100+ universities, and is listed in Coursera's top-rated courses. The course synthesizes many abstractions, algorithms, and data structures learned in CS courses, and makes them concrete by building a complete computer system from first principles. The methodology is based on guiding students through a set of 12 homework assignments that gradually construct and unit-test a simple hardware platform and a modern software hierarchy, yielding a surprisingly powerful computer system. We'll start the workshop by demonstrating some interactive computer games running on this platform. The hardware projects are done in a simple hardware description language and a hardware simulator supplied by us. The software projects (assembler, VM, compiler and OS) can be done in any language, using API's and test programs supplied by us. The result is a general-purpose computer system, simulated on the student's PC. The course is completely self-contained, requiring only programming as a pre-requisite. Also, it is highly modular, and can be viewed as a set of 12 exciting, stand-along homework assignments. The course and the workshop assume no specific knowledge of, or bent for, computer organization/architecture, and are aimed at any instructor who wishes to strengthen his or her courses with engaging programming projects and with an integrated and hands-on understanding of how modern computer systems work, and how they are built.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1052},
numpages = {1},
keywords = {active learning, computer organization / architecture, operating systems, compilers / programming languages, software engineering},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/3510003.3510199,
author = {Tan, Xin and Gao, Kai and Zhou, Minghui and Zhang, Li},
title = {An Exploratory Study of Deep Learning Supply Chain},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510199},
doi = {10.1145/3510003.3510199},
abstract = {Deep learning becomes the driving force behind many contemporary technologies and has been successfully applied in many fields. Through software dependencies, a multi-layer supply chain (SC) with a deep learning framework as the core and substantial down-stream projects as the periphery has gradually formed and is constantly developing. However, basic knowledge about the structure and characteristics of the SC is lacking, which hinders effective support for its sustainable development. Previous studies on software SC usually focus on the packages in different registries without paying attention to the SCs derived from a single project. We present an empirical study on two deep learning SCs: TensorFlow and PyTorch SCs. By constructing and analyzing their SCs, we aim to understand their structure, application domains, and evolutionary factors. We find that both SCs exhibit a short and sparse hierarchy structure. Overall, the relative growth of new projects increases month by month. Projects have a tendency to attract downstream projects shortly after the release of their packages, later the growth becomes faster and tends to stabilize. We propose three criteria to identify vulnerabilities and identify 51 types of packages and 26 types of projects involved in the two SCs. A comparison reveals their similarities and differences, e.g., TensorFlow SC provides a wealth of packages in experiment result analysis, while PyTorch SC contains more specific framework packages. By fitting the GAM model, we find that the number of dependent packages is significantly negatively associated with the number of downstream projects, but the relationship with the number of authors is nonlinear. Our findings can help further open the "black box" of deep learning SCs and provide insights for their healthy and sustainable development.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {86–98},
numpages = {13},
keywords = {deep learning, software supply chain, software structure, software evolution, open source},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.5555/601671.601727,
author = {Ho, Jeffrey and Lee, Kuang Chih and Kriegman, David},
title = {Compressing Large Polygonal Models},
year = {2001},
isbn = {078037200X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present an algorithm that uses partitioning and gluing to compress large triangular meshes which are too complex to fit in main memory. The algorithm is based largely on the existing mesh compression algorithms, most of which require an 'in-core' representation of the input mesh. Our solution is to partition the mesh into smaller submeshes and compress these submeshes separately using existing mesh compression techniques. Since a direct partition of the input mesh is out of question, instead, we partition a simplified mesh and use the partition on the simplified model to obtain a partition on the original model. In order to recover the full connectivity, we present a simple scheme for encoding/decoding the resulting boundary structure from the mesh partition. When compressing large models with few singular vertices, a negligible portion of the compressed output is devoted to gluing information. On desktop computers, we have run experiments on models with millions of vertices, which could not be compressed using standard compression software packages, and have observed compression ratios as high as 17 to 1 using our technique.},
booktitle = {Proceedings of the Conference on Visualization '01},
pages = {357–362},
numpages = {6},
keywords = {compression algorithms},
location = {San Diego, California},
series = {VIS '01}
}

@article{10.1145/2529998,
author = {Ganesan, Dharmalingam and Lindvall, Mikael},
title = {ADAM: External Dependency-Driven Architecture Discovery and Analysis of Quality Attributes},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2529998},
doi = {10.1145/2529998},
abstract = {This article introduces the Architecture Discovery and Analysis Method (ADAM). ADAM supports the discovery of module and runtime views as well as the analysis of quality attributes, such as testability, performance, and maintainability, of software systems. The premise of ADAM is that the implementation constructs, architecture constructs, concerns, and quality attributes are all influenced by the external entities (e.g., libraries, frameworks, COTS software) used by the system under analysis. The analysis uses such external dependencies to identify, classify, and review a minimal set of key source-code files supported by a knowledge base of the external entities. Given the benefits of analyzing external dependencies as a way to discover architectures and potential risks, it is demonstrated that dependencies to external entities are useful not only for architecture discovery but also for analysis of quality attributes. ADAM is evaluated using the NASA's Space Network Access System (SNAS). The results show that this method offers systematic guidelines for discovering the architecture and locating potential risks (e.g., low testability and decreased performance) that are hidden deep inside the system implementation. Some generally applicable lessons for developers and analysts, as well as threats to validity are also discussed.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {17},
numpages = {51},
keywords = {external entities, quality, testability, Concerns, module and runtime views, software architecture, knowledge base, reverse engineering, maintainability}
}

@inproceedings{10.1145/1512762.1512765,
author = {Haveraaen, Magne},
title = {Institutions, Property-Aware Programming and Testing},
year = {2007},
isbn = {9781605580869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1512762.1512765},
doi = {10.1145/1512762.1512765},
abstract = {The institution notion is a general model theoretic framework, explaining how specifications (algebraic axioms) relate to models (mathematical models or even software constructions) in a formalism-independent manner. There is a large set of institution-independent structuring mechanisms for specifications. Property aware programming, as e.g. supported by concepts in C++0X, provides algebraic axioms as part of the code to ensure correctness of generic software composition. Testing is very important for the validation of software, but tests are all too often developed on an ad hoc basis.Here we present a library testing framework, with a basis in structured specifications. The approach will be demonstrated on Sophus, a medium-sized software library for coordinate-free numerics. Sophus was developed using (informal) algebraic specifications in order to improve reusability and reduce development costs.},
booktitle = {Proceedings of the 2007 Symposium on Library-Centric Software Design},
pages = {21–30},
numpages = {10},
keywords = {systematic software library testing, institutions, coordinate-free numerics, generic programming, algebraic specifications, Sophus software library},
location = {Montreal, Canada},
series = {LCSD '07}
}

@inproceedings{10.1145/41866.41879,
author = {Johnson, Mark B.},
title = {You Cannot Lead a Horse to Water: Taking Charge of Microcomputer Support},
year = {1987},
isbn = {0897912411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/41866.41879},
doi = {10.1145/41866.41879},
abstract = {Service. Performance of labor for the benefit of another. Supporting computing, whether dealing with microcomputers or mainframes, is, of course, providing a service. That service is toiling for the benefit of those who use, or wish to use, computing resources.Support, however, does not mean maintaining a laissez-faire attitude and only reacting to user requests. It means taking charge by providing guidance and direction. It means anticipating user wants and needs, and satisfying them before the user perceives them as such. An organization which falters in this respect is no longer a service organization, especially in today's rapidly expanding computing environments.Support does not mean doing the work for the user, even when it might be the easier and less time-consuming path to follow. It means taking the time to teach the user how and why so that they can understand, or at least begin to grasp, the basic principles involved. It means providing them with the tools, knowledge, and incentive to effectively use the equipment themselves. It means making them self-sufficient.For support to be effective, the users must trust those who offer it. A service organization is constantly faced with the task of building and presenting a positive public image; without it, even the highest quality work will go unnoticed or best advice unheeded. An organization should be perceived as being responsive and effective; it needs to be recognized for its available expertise, resources, and services. This type of image will never just happen. However, it can be built, but only if the attention is not focused on building it.To be considered an expert, one must be an expert. To gain a perception of doing quality work, one must do quality work. If a user services organization concentrates on doing the best possible work it can, then the image will take care of itself. One need not remind those in this profession that an organization's attitude towards this task is crucial. Taking charge means cultivating personal contacts and showing people someone cares about their computing needs and problems. It means attacking these problems with tenacity until the best answers are found, and doing so in an unselfish manner.When dealing with people, it is, of course, necessary to maintain a professional demeanor, but this should not overshadow one's own personality. Support staff must be comfortable in the work environment, and they must integrate themselves, their casualness and approachability, into their work. With the right mixture, personal interaction will be greatly enhanced to benefit both the users and the staff.Many people still envision computing centers as cold, harsh places run by machines, and with the decentralization of computing resources, it is even more pressing to overcome these old stereotypes if computing service organizations are to remain effective. People need to be shown that computer support personnel are indeed equipped with certain social skills, and that they can interact with novices and talk about Pascal, not the language, but the philosopher. An open house, computer fair, and other similar events can be excellent avenues which afford this type of interactivity. Not only do they allow a support staff to show itself and its facilities off, but they also produce an opportunity for disseminating large amounts of information to people who might otherwise never receive it.It is obvious that the more people know about available facilities and support, the more likely they are to take advantage of them. Although special events like a computer fair do attract large crowds, there are many potential users who either cannot attend or might not see how computing involves them. These people are not going to come to the resources, so the resources must be taken to them.As with any organization, public relations play an important role in shaping a computing center's image in the public eye. From this point of view, it is almost always advantageous to volunteer resources, both employee time and equipment, for worthy causes on and off the campus. Simple things like a tour for a scout troop, a hands-on demonstration for a local computer club, or a hardware loan for university events or educational conferences can usually be accomplished without sacrificing the normal level of support provided on a day-to-day basis.When dealing with microcomputers, taking charge encompasses the whole range of support services, from introducing new technologies to new users to following up on the trials and tribulations of more experienced ones. Certain services present special challenges to a support staff, but they are all worthy of attention if an organization desires to provide its users with the best possible support.Computing tends to be an exciting field, and can be even more so when introducing people to the power available to them. Schedule seminars and demonstrations which target specific groups with special interests which may traditionally not involve computing. Introduce those with interests in music and art to the newest tools available, and show them the potential these tools put into their hands. Show educators the possibilities of interactive video and administrators the organizational and presentational power of productivity software and the ability to easily share common information. Most importantly, stress to these people that a microcomputer is a tool which they can use to their benefit, not a machine which will dehumanize them. As anyone who has experienced it can attest, getting people excited about how they can benefit from computing makes the task of supporting them much easier.An obvious area of support is providing computing facilities for those who cannot afford their own. Ideally this covers conveniently located, 24-hour access, multi-vendor labs with various useful hardware configurations and an abundance of popular software. The labs, however, must be staffed with competent monitors or consultants if they are to be a real service.For those who are interested in acquiring their own equipment, a demonstration lab serves as an excellent setting for comparing various hardware and software options without the pressuring presence of a retail salesperson. Vendor donations can often be solicited, and are undoubtedly the most inexpensive way to furnish a lab with a wide variety of machines, configurations, peripherals, and software. Support for this lab with honest and unbiased purchase consulting is an absolute must. If someone does not show a need for purchasing a computer, then discourage them from buying. People will purchase, and purchase whatever they want, regardless of advice, but they are much more likely to listen if they feel they can trust the source of this advice.Pricing is always an issue with educational institutions, and it is sometimes possible to provide a staffed microcomputer sales outlet which can offer very competitive pricing to departments and individuals affiliated with the university—the latter including the student population whenever possible. The ideal computer store, from a support point of view, is service-oriented rather than profit-oriented, and based on providing the highest level of service to the university community at the lowest cost. Avoiding a profit-orientation helps avoid the vendor biases so often associated with retail ventures, and, in turn, provides the prospective buyer with a more accurate presentation of purchase options.In those cases where a sales outlet is not possible, a demonstration lab staff can often work with the university purchasing department and various vendors to establish a mechanism for purchasing directly from a manufacturer or distributor at educational price levels.Microcomputer support can, and often does include such things as help with installation of hardware and software. Installation can be left to the user and the vendor, but in many cases it is worth the time to walk through the installation procedure with those new to computing so that they understand that the machine on their desk is not some black box only to be touched by trained professionals. They need to learn basic diagnostic procedures and common sense techniques for trying to figure out the cause and possible solutions to a problem. If they need it, show them things that most people take for granted such as the correct way to feed paper into a printer or to change a ribbon or toner cartridge. Have them write it down if they cannot remember, and let them know that they should exhaust all of these possibilities and the reference manual before calling for help. If a user can be started off in this fashion, they will usually develop a sense of confidence in their ability to deal with most situations. It is this confidence which is the cornerstone in a completely self-reliant user.Beyond the initial installation, the majority of support deals with software. With the wide variety of machines and packages available, no support staff can be expected to keep track of them all, let alone know them all. A staff can, however, provide a high level of support on those packages it feels should be used on campus, and should have enough general knowledge in the workings of most microcomputers and software packages to be able to find the answers needed to support the others. Without telling the users what hardware and software they should or should not use, a support staff can provide implicit guidance by supporting the same combinations in its own lab and not offering the others. This in no way limits the advanced user who wants something different, but does encourage the more casual user to stay within certain, loosely-defined standards.Software piracy is an issue with which support groups must deal. At the Computing Center, the User Services staff will not provide support if there is cause to believe a software package has been pirated. Federal copyright laws are explained to offending parties, and if proof is available which shows the software was copied from the lab, the application and all accompanying files are destroyed.Software support, whether on a walk-in basis or via telephone, is offered at most institutions, but what about on-site support? This is the real essence of taking the resources to the users, of taking charge of microcomputer support. Although it might initially seem more time consuming, it offers unique advantages for both the support staff and the users. The advantage to the users is obvious: they get the undivided attention of a support person, they get help in their own office with their particular configuration, and they can demonstrate the problem instead of trying to explain every detail over the telephone. What about the support staff? They are able to take their expertise out to the campus and get exposure to other people who might not otherwise see them. This helps to increase the presence and image of a support group throughout the campus. It shows that they care enough about the problem to make a special trip. In addition, they are in touch with a wide variety of people and should be able to understand the real and perceived needs of these users better than if they had not ventured out of the support office.These trips out on campus should also be used to check in on other users in the same general area. While in an office, one can offer possible suggestions for new ways to take advantage of the computer or alternative ways to approach a particular problem. The ideal is to stem problems before they develop and let the users know that someone is there who really cares about trying to help them derive the most benefit from computing resources. This is especially important in a user's perception of a service organization.The real key to support is training and documentation. Although individualized training is very effective, it is just not as efficient as organized group training sessions, especially considering the large user base found at most universities. Ideally these sessions are taught in a structured, yet flexible, format in a designated, well-equipped training lab, but there are advantages of teaching on-site in some environments, such as a networked office, which cannot be duplicated in a lab.Training sessions can encompass a wide variety of topics and may be aimed at specific segments of a user base. Novices might be interested in general courses which cover basic computer literacy, diagnosing common hardware and software problems, and introductions to popular application programs or even operating systems. More advanced users will usually opt for courses which cover more specific topics and advanced features of application programs. On-site sessions can be tailored to a specific audience and possibly accomplishing a specific task. In all cases, the subject matter must be presented in an easily understandable format and should build upon simple, fundamental principles. It is very easy to teach only an application and skirt the issue of the operating system, but a user without a basic understanding of the underlying principles is like a building without a foundation—neither will ever be able to stand alone.With office and departmental training and support, it can be very advantageous to work closely with a few individuals and train them to be local experts. In this manner, it is possible to effectively train and support large numbers of people without spending the time to work directly with them all. The local experts are trained well enough to teach others and to field questions from anyone else in the office. When there are problems or questions these experts cannot answer, even with the help of the manual, then they contact the support group for help.Good documentation may be even more important than training, although the feedback gained through interactive training sessions is quite valuable when formulating a document. A well-written and comprehensive document provides every level of user with easily found reference material and a simple, straightforward procedure for accomplishing specific tasks. Reference manuals are generally the most comprehensive source of documentation, and as such the user should be referred to them in any supplemental documentation produced by a support group. This supplemental documentation will ideally be available to users free of},
booktitle = {Proceedings of the 15th Annual ACM SIGUCCS Conference on User Services},
pages = {67–71},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {SIGUCCS '87}
}

@inproceedings{10.1145/1730874.1730897,
author = {Shenvi, Ajit Ashok},
title = {Medical Software: A Regulatory Process Framework},
year = {2010},
isbn = {9781605589220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1730874.1730897},
doi = {10.1145/1730874.1730897},
abstract = {Healthcare industry is governed by regulations which are region or country specific. Since it deals directly with human lives, if a medical device, be it standalone or a connected system or a Healthcare informatics package, is to be launched in a particular country/region, it has to abide by the corresponding medical regulations. However these regulations are not standardized across the globe and that makes it challenging for a medical device manufacturer to abide by the corresponding country specific regulations where the product is going to be launched. For e.g. US is regulated by Quality Systems requirements of FDA and 510 K approval, Europe is governed by CE marking and ISO 13485 standard, China has its SFDA requirements whereas Japan follows PAL regulations.Software is at the heart of all these medical equipments. More and more of the medical devices and the corresponding clinical applications and workflows are now controlled by software. Right from capturing the images generated by the scanner to the processing of these images and subsequent post processing for clinical decision making is done by software. The Picture Archival and Communication system (PACS) system which is the backbone of healthcare Informatics in hospitals has software at its core. So automatically all the software that is written for these products comes under the purview of these medical regulations. Infact the Global Harmonization Task force specifically includes "software" in its definition of medical devices.Software development in many organizations is governed by "software engineering principles" applied through a SEI-CMMI model which lays strong emphasis on establishing sound process framework to ensure a good software quality. So on one hand we have a CMMI based Quality system and on other hand the development and maintenance of "medical software" has to abide by the regulatory structure posed by FDA, ISO 13485 etc. This raises certain fundamental questions for organizations dealing with development and maintenance of "medical software" -- Are these Quality systems requirements too diverse or is there any overlap? How much exactly is this overlap? Is it possible to marry both these worlds of CMMI and the regulatory standards and have a common process framework for the organization? If there is an already existing CMMI based system then is it possible to extend it to include the ISO and FDA aspects? And if yes how can this be done? As the medical software industry matures, more and more standards will get added to the regulatory requirement -- in such a scenario, how does one keep the scalability and integrity of the Quality system? and other similar questions?.This paper is an attempt to answer these questions by sharing the experience of deploying ISO 13485 and FDA CFR 820 elements in an existing CMMI based Quality Management System for development and maintenance of medical software. It begins by painting the regulatory landscape across the globe -- America, Europe, Asia, goes onto explaining briefly the structure of ISO 13485, and FDA Quality systems requirements and summarizes the approach followed by Philips-Healthcare Bangalore centre to achieve the ISO certification in a CMMI based process framework. The paper finally details out a granular mapping of the ISO 13485 clauses to the CMMI Process areas including Generic and Specific practices. This will help any CMMI based organization dealing with medical software to easily map and extend their existing practices to the required regulatory standards and achieve the corresponding certification.},
booktitle = {Proceedings of the 3rd India Software Engineering Conference},
pages = {119–124},
numpages = {6},
keywords = {ghtf, quality policy, fda, iso, process areas, cmmi, regulations, clauses},
location = {Mysore, India},
series = {ISEC '10}
}

@inproceedings{10.5555/319568.319681,
author = {Sussman, Gerald Jay},
title = {Intelligent Support for the Engineering of Software (Panel Paper)},
year = {1985},
isbn = {0818606207},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {Engineers of large systems must be concerned with both design of new systems and maintenance of existing systems. A great deal of effort goes into arranging things so that systems may be maintained and extended when needed, and that new systems fit into the context of existing structures. Insight about the support of software engineering can thus be derived by examining how other branches of engineering support their version of the processes of engineering.Before embarking on a design project an engineer must first determine the customer's need. This is always a complex process, involving modeling of the customer's situation, to determine exactly what is essential and what is accidental. The specifications derived from this analysis are the input to the design process. The specifications may be expressed in varying degrees of formality. Though mathematical formalism is to be desired because it is unambiguous and because it can be manipulated precisely, existing mathematical technique is usually inadequate to precisely specify the requirements. This means that the specification phase must be part of the debugging loop.The design engineer first attempts to meet the specifications with some existing artifact. If this fails, he attempts to synthesize an artifact that meets the specifications by combining the behaviors of several parts, according to some standard plan. For example, in electrical engineering, a complex signal-processing system can often be composed as a cascade of simpler signal-processing components - an engineer knows that the transfer function of a cascade is the product of the transfer functions of the parts cascaded, if the loading is correct. Such design depends on the ability to compute the behavior of a combination of parts from their specifications and a description of how they are combined. Often such analysis must be approximate, with bugs worked out by simulation and debugging of breadboard prototypes.This design strategy is greatly enhanced by the existence of compatible families of canned parts with agreed-upon interfaces and well-defined specifications. If the family of canned parts is sufficiently universal, the interfaces sufficiently well specified, and if design rules can be formulated to isolate the designer from the details of the implementation of the parts, the family constitutes a design language. For example, TTL is a pretty good design language for relatively slow digital systems. Just as the approximations of analysis are often imperfect, the abstraction barriers of the design language are often violated, for good reasons. Thus there are the inevitable bugs. These may be found in simulation or prototyping.One key observation is that all phases of engineering involve bugs and debugging. Bugs are not evidence of moral turpitude or inadequate preparation on the part of the engineer, they are an essential aspect of effective strategy in the problem-solving process. Real-world problems are usually too complex to specify precisely, even assuming that we have adequate formal language to support such specifications. (Imagine trying to formalize what is meant by “My program plays chess at the Expert level.” or “My music synthesizer circuit makes sounds that are like a fine violin.” in any precise way, yet surely such specifications are the meat of real engineering.) And even where one can write very precise specifications (as in “This circuit makes 1/f noise.”) such specifications are often mathematically intractable in current practice. Even worse, analysis of systems made up of a few simple, precisely-specifiable components such as transistors (here a few exponential equations suffice), or floating-point additions are mathematically intractable. Thus engineers, like physicists, make progress by approximate reasoning. Linearizations are made, and deductions that ignore possibly important interactions are used to plan designs, with the explicit intention of finding the relevant ignored effects in the simulation or prototype debugging phase of design. Bugs thus arise from the deliberate oversimplification of problems inherent in using perturbational methods to develop answers to hard problems. Finally, bugs often arise, in apparently finished products, because of unanticipated changes in the requirements of the customer. Although these are technically not errors in the design, the methods we have for patching a design to accommodate a change in requirements amounts to debugging the installed system.Software engineering needs appropriate tools to support each of the phases of the engineering process. There must be tools to aid with specification and modelling, with synthesis and analysis, with rapid-prototyping and debugging, with documentation, verification, and testing, and with maintenance of finished products. In addition there must be environmental tools to support the engineering process in the large.Our tools must support the natural processes of problem solving. They must provide precise ways to talk about alternate design plans and strategies. There is no doubt that mathematical formalisms, such as logic and abstract algebra are essential ingredients in such an enterprise, but we must be careful to separate our concerns. Mathematical formalisms and technique are rarely strong enough to provide better than very approximate models of interesting physical, economic or engineered systems. Sometimes a system that is readily described as a computer program is not easily formalized in more conventional terms. This suggests the exciting possibility that some difficult theoretical constructs can be formally represented as computational algorithms. We can expect to manipulate these as we now manipulate equations of classical analysis. Such a paradigm shift is already taking place in control theory. Twenty years ago, the dominant method for making control systems was synthesizing them using feedback, and the dominant theory was concerned with the stability of linear feedback systems. In contrast, most modern control systems are microprocessor-based, and the theory is now much more qualitative. Moreover, the program for the microprocessor is often a much simpler description of the strategy of control than the classical equations, and one can thus express much more complex strategies than were previously feasible to analyze and synthesize. An even more striking revolution has occurred in the design of signal-processing systems. The nature of the field has changed completely, because digital filters are algorithms.Artificial intelligence research often uses programs as theoretical constructs, akin to equations and schematic diagrams, but with the added feature that programs that embody parts of a theory of the design of programs can be used as tools in the process of theory construction (or software development). The language Lisp, for example, was initially conceived as a theoretical vehicle recursion theory and for symbolic algebra. Most AI experiments are formulated in Lisp. Lisp has developed into a uniquely powerful and flexible family of software development tools, providing wrap-around support for the rapid-prototyping of software systems. As with other languages, Lisp provides the glue for using a vast library of canned parts, produced by members of the AI community. In Lisp, procedures are first-class data, to be passed as arguments, returned as values, and stored in data structures. This flexibility is valuable, but most importantly, it provides mechanisms for formalizing, naming, and saving the idioms - the common patterns of usage that are essential to engineering design. In addition, Lisp programs can easily manipulate the representations of Lisp programs - a feature that has encouraged the development of a vast structure of program synthesis and analysis tools, such as cross-referencers.},
booktitle = {Proceedings of the 8th International Conference on Software Engineering},
pages = {397–399},
numpages = {3},
location = {London, England},
series = {ICSE '85}
}

@inproceedings{10.1145/2538862.2538892,
author = {Miller, James R.},
title = {Using a Software Framework to Enhance Online Teaching of Shader-Based OpenGL},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2538892},
doi = {10.1145/2538862.2538892},
abstract = {Shader-based OpenGL is a powerful and exciting tool for individuals with data visualization needs, interests in gaming, a need to create and manipulate synthetic environments, and a variety of other high-performance tasks. It is well-known that shader-based OpenGL is difficult to learn for programmers with no prior graphics API experience -- some have even claimed it's impossible to teach to individuals without such prior exposure. In this paper, we report on our experiences developing an educational approach that we believe has contributed to the base of evidence that it is not only possible, but also desirable to do so. Our overriding goal has been to learn and exploit the extent to which mastering a complex graphics API like modern shader-based OpenGL can be enhanced by using a software design framework into which relevant concepts can be placed, thus facilitating more rapid assimilation and mastery of the concepts. We have been using this technique in our Introduction to Computer Graphics course in which we have two primary goals: teach shader-based OpenGL to students who have no prior experience using a graphics API, and present a framework to students that will scale up to medium and large scale applications, both in terms of code size as well as data size. We do not in any way suggest that our architecture is the 'best' for all advanced graphics applications, or even for teaching. Instead we simply claim that use of such a framework helps students master complex OpenGL concepts and develop nontrivial interactive 3D applications. Once students fully understand the basics, they should find it easy to migrate to other perhaps quite different architectures.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {603–608},
numpages = {6},
keywords = {computer-graphics, educational technology, computer-aided learning, mastering shader-based OpenGL, self-paced learning},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/277044.277164,
author = {Yuan, Frank Y.},
title = {Electromagnetic Modeling and Signal Integrity Simulation of Power/Ground Networks in High Speed Digital Packages and Printed Circuit Boards},
year = {1998},
isbn = {0897919645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/277044.277164},
doi = {10.1145/277044.277164},
abstract = {The electromagnetic modeling and parameter extraction of digital packages and PCB boards for system signal integrity applications are presented. A systematic approach to analyze complex power/ground structures and simulate their effects on digital systems is developed. First, an integral equation boundary element algorithm is applied to the electromagnetic modeling of the PCB structures. Then, equivalent circuits of the power/ground networks are extracted from the EM solution. In an integrated simulation scheme, the equivalent circuits are combined with signal nets, package models, device circuits, and other external circuitry for system level signal integrity analysis and simulation. This methodology has been implemented as software tools and applied to practical design problems. Effects related to power/ground networks, such as simultaneous switching noises, crosstalk, and ground discontinuity are analyzed for realistic designs.},
booktitle = {Proceedings of the 35th Annual Design Automation Conference},
pages = {421–426},
numpages = {6},
keywords = {custom sizing, timing optimazation, migration},
location = {San Francisco, California, USA},
series = {DAC '98}
}

@inproceedings{10.1145/1629911.1630038,
author = {Jayaseelan, Ramkumar and Mitra, Tulika},
title = {Dynamic Thermal Management via Architectural Adaptation},
year = {2009},
isbn = {9781605584973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629911.1630038},
doi = {10.1145/1629911.1630038},
abstract = {Exponentially rising cooling/packaging costs due to high power density call for architectural and software-level thermal management. Dynamic thermal management (DTM) techniques continuously monitor the on-chip processor temperature. Appropriate mechanisms (e.g., dynamic voltage or frequency scaling (DVFS), clock gating, fetch gating, etc.) are engaged to lower the temperature if it exceeds a threshold. However, all these mechanisms incur significant performance penalty. We argue that runtime adaptation of micro-architectural parameters, such as instruction window size and issue width, is a more effective mechanism for DTM. If the architectural parameters can be tailored to track the available instruction-level parallelism of the program, the temperature is reduced with minimal performance degradation. Moreover, synergistically combining architectural adaptation with DVFS and fetch gating can achieve the best performance under thermal constraints. The key difficulty in using multiple mechanisms is to select the optimal configuration at runtime for time varying workloads. We present a novel software-level thermal management framework that searches through the configuration space at regular intervals to find the best performing design point that is thermally safe. The central components of our framework are (1) a neural-network based classifier that filters the thermally unsafe configurations, (2) a fast performance prediction model for any configuration, and (3) an efficient configuration space search algorithm. Experimental results indicate that our adaptive scheme achieves 59% reduction in performance overhead compared to DVFS and 39% reduction in overhead compared to DVFS combined with fetch gating.},
booktitle = {Proceedings of the 46th Annual Design Automation Conference},
pages = {484–489},
numpages = {6},
keywords = {dynamic thermal management, architecture adaptation},
location = {San Francisco, California},
series = {DAC '09}
}

@inproceedings{10.1145/3292500.3330701,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-Generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {black-box optimization, Bayesian optimization, machine learning system, hyperparameter optimization},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3560810.3565290,
author = {Cammarota, Rosario},
title = {Intel HERACLES: Homomorphic Encryption Revolutionary Accelerator with Correctness for Learning-Oriented End-to-End Solutions},
year = {2022},
isbn = {9781450398756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560810.3565290},
doi = {10.1145/3560810.3565290},
abstract = {In spite strong advances in confidential computing technologies to protect data, the status is that data is encrypted while temporarily not in use and unencrypted during computation. The inability to keep data encrypted during computation can inhibit the ability to fully share and extract the maximum value out of data, e.g., via statistical and machine learning methods with the additional risk of third-party data leakage. Fully Homomorphic Encryption (FHE) enables users to delegate computation to the cloud by enabling the cloud to process users' encrypted inputs without decryption and return encrypted output to the intended recipients. The adoption of FHE by the industry has been slow. First, processing encrypted data incurs a huge performance tax even for simple operations - ciphertexts operations can be several orders of magnitude slower with respect to cleartext operations on existing hardware. Second, there is lack of automation tools for translating data and applications to enable FHE. Third, there is lack of standards and best practices for FHE secure deployment in combination with other confidential computing techniques. The DARPA DPRIVE program [1] is the first publicly visible program aiming to build a platform to accelerate FHE and a path to their commercialization, for their use in healthcare, communication (5G to XG), and cloud computing. The program represents a steppingstone toward FHE adoption. Under the DARPA DPRIVE program, Intel® is designing a new type of computer architecture to reduce the performance tax currently associated with FHE. Intel® collaborates with Microsoft® Azure Global in realization of the project [2]. The design includes flexible arithmetic circuits for algebraic lattices with unprecedented vector parallelism and data transfer capacity between vector slots, to increase ciphertext processing speed, coupled with near-memory computation, to reduce data movement. The software stack leverages the Microsoft® SEAL library [3] augmented the BGV mechanism including bootstrapping [4], and automatic translation tools to explore trade-offs in algorithmic optimization and data encoding to fit the performance requirements [5]. When fully realized, the HERACLES platform will deliver a massive improvement in executing FHE workloads over existing CPU-driven systems, potentially reducing ciphertext processing time by five orders of magnitude. Beyond the new hardware and software stack, Intel® work with international standard bodies to develop standards for and best practices for FHE secure deployment [6] and invests in academic research to advance both theory and application [7].},
booktitle = {Proceedings of the 2022 on Cloud Computing Security Workshop},
pages = {3},
numpages = {1},
keywords = {asic, bootstrapping, privacy-enhancing cryptography, fully homomorphic encryption},
location = {Los Angeles, CA, USA},
series = {CCSW'22}
}

@article{10.5555/1047846.1047888,
author = {Stringfellow, Catherine V. and Carpenter, Stewart B.},
title = {An Introduction to C# and the .Net Framework},
year = {2005},
issue_date = {April 2005},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {20},
number = {4},
issn = {1937-4771},
abstract = {A new style of programming is developing in which applications are created from building blocks available over the Internet. Integrating software components developed in various languages can be difficult. The .NET framework allows software components written in different languages to interact with each other or with old packaged software written in C/C++, hence supporting interoperability. This framework allows for the management and execution of applications and Web services. The framework provides security, memory management and other programming capabilities. In addition, the architecture is capable of existing on multiple platforms. C# is an event-driven, fully object-oriented, visual programming language that was developed specifically for .NET. Its constructs are familiar enough to enable programmers to migrate from C/C++ and Java easily.This tutorial will present C# and Visual Studio's .NET IDE. The tutorial will start out with examples of simple console applications, pointing out differences to C++ and Java. It will demonstrate how to visually program windows applications using several pre-packaged components. As time allows, it will cover concepts for building applications that interact with databases usingADO.NET and SQL, as well as building interactive web documents that respond to client requests using ASP.NET.},
journal = {J. Comput. Sci. Coll.},
month = {apr},
pages = {271–273},
numpages = {3}
}

@inproceedings{10.1145/17701.255364,
author = {Bittlestone, Robert},
title = {XPL: An Expert Systems Framework in APL},
year = {1985},
isbn = {0897911571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/17701.255364},
doi = {10.1145/17701.255364},
abstract = {It is hardly possible to pick up a computer journal today without seeing references to the Alvey project in the UK, the Esprit project run by the EEC, the Japanese “Fifth generation” initiative, the proliferation of knowledge-based computer companies around Stanford in California and MIT in Massachusetts, and the large amounts of money that are being poured into the development of expert systems. The references abound: as I write this article the September 1984 issue of “International Management” (1) arrives on my desk with a cover depicting a digitized human brain image on a computer screen and a caption saying “Artificial intelligence: the race to make it work for managers”. I like the use of the word “race” here; it seems to imply that this is our last chance now that the prospect of human intelligence in managers has declined asymptotically to zero.Among the savants cited in the international Management article is Sir Clive Sinclair, no less, who has become notorious with the British Computer Society's Expert Systems Specialist Group: notorious for arriving unexpectedly at meetings and giving the rest of the audience something to think about. Sir Clive apparently said in the interview: “Once 'machines of silicon' surpass us, they will be capable of their own design. In a real sense, they will be reproductive, and silicon will have ended carbon's long monopoly. And ours too, I suppose, for we will no longer be able to deem ourselves the finest intelligence in the known universe.”Maybe expert systems will dominate us after all — it's just that there'll be a ten-month delay between quoting your Visa number and becoming an inferior intelligence.All this is quite fascinating since the phenomenon is very recent: before 1983 the phrase “expert systems” was largely unheard of and certainly not in itself viewed as the potential salvation of the Western world (there are as yet few hints of Eastern bloc development apart from in Hungary, where the University of Budapest, along with those of Marseilles, Edinburgh and London's Imperial College, claims to have invented PROLOG). However, by now there has been “Horizon” television coverage of the genre and there seems little doubt that expert systems are here to stay. But before I take up the theme of my title, perhaps I might be allowed to state my position on one or two related matters. First of all: “Expert systems are not the same thing as artificial intelligence”We can see the beginnings of a bandwagon effect here in which technical namedroppers, bored of stating that “UNIX is the coming thing” or “The future of computing lies in networking” are catching up with this latest trend: “Life is just another form of domain specialism”. Expert systems are in fact a very narrowly defined class of computer program with certain in-built inference-drawing capabilities. Artificial intelligence, on the other hand, is a much wider field involving the generic search for synthetic rational life. Much of the most interesting work in artificial intelligence does not involve digital computers at all but concerns the possibility of producing artificial models of brain process, using special fabrication techniques to emulate neurons, axons and synapses. Which brings me to my second definition: “Artificial intelligence will not be implemented on digital computers”It turns out that the use of a digital clocked device, irrespective of speed, imposes a fatal limitation on the sophistication of a genuine search for artificial intelligence, and parallel processing makes no difference. The reason is that in the brain it is the dynamic connections between at least 10 to the power 10 asynchronous independent processing elements that provide the kind of complexity that is required. The processors in the brain are called neurons and the interconnection are called axons, and there is substantial evidence to indicate that to a first approximation, almost any neuron can grow an axon to connect to almost any other neuron.Now the number of connections between 'N' elements increases in proportion to the square of 'N'. If you have three neurons you can connect A to B, A to C, B to C, and the same in the other direction, which is valid since it corresponds to a negative signal (inhibition vs. excitation). Consequently we have 6 possibilities for the network topology: i.e. 3x2. For 4 elements we have 12 options: 4x3. If we had a mere 100 neurons the result of 100x99 is still only about 10,000. However, in the brain we are considering about 10,000,000,000 neurons. So the number of possible connections is 100,000,000,000,000,000,000 — a number of literally astronomic proportions. We haven't yet introduced the complexity of neurons which connect to more than one other neuron (the majority do), nor signal processing within the neurons themselves (estimated as a seventh order differential equation), nor chemical effects at the synapses, which are retransmission points along the axons. When you drink too much alcohol, it is primarily the synaptic effects which alter your responses. I am unconvinced that there yet exists an artificially intelligent program capable of intoxication.If you now consider the implications of asynchronous (but not parallel) operations taking place between arbitrarily connectable elements on this scale, you begin to see that even a billion 64-bit microprocessors would make no difference at all. The emphasis in the brain is on analogue circuits in which feedback and frequency of signal are the determining factor and where there is no single location at which processing can be said to be taking place; the whole of the brain is in this sense intelligent.When I asked the Alvey representative at a recent meeting of the Expert Systems Specialist Group (yes, Sir Clive was there too) why there was so little emphasis on neurophysiology in the Alvey program (I was uncharacteristically diplomatic as in fact there is none at all) his response was that the program concerned the “hard” sciences and engineering: the bridge to biology was too narrow for engineers to cross. I believe he was genuinely rueful. However, if your appetite is in any way whetted for “real” artificial intelligence, you had better go out and buy two rather extraordinary books. One of them is, of course, Hofstadter's “Godel Escher Bach: An Eternal Golden Braid” (2). The other is less well-known: William T. Powers's “Behaviour: The Control of Perception” (3).In Hofstadter's book the notion of recursion is explored in many delightful ways. For those who have missed it, here is a sampler. Our heroes, Achilles and the Tortoise, have found a lamp. They rub it and a Genie appears. The Genie will grant three wishes. This prospect is greeted with glee by Achilles, who has read the 'Arabian Night' and knows the form. He makes his first wish: Achilles: “I wish that I had 100 wishes instead of just three.” Genie: “I am sorry, Achilles, but I don't grant meta-wishes.” Achilles: “I wish you'd tell me what a 'meta-wish' is.” Genie: “But that is a meta-meta-wish, Achilles, and I don't grant them either.” But after some pleading, the Genie agrees to try (meta-wishes are his favorite wishes too) and from inside his cloak he produces a lamp. He rubs it and out pops a MetaGenie: Genie: “I wish for permission for temporary suspension of all type-restrictions on wishes, for the duration of one typeless wish.” MetaGenie:“Half a moment please (produces a lamp, rubs it, etc.).” This continues for a short infinite period until we get to the top-level MetaGenie, also known as GOD, who grants permission for the typeless wish: Achilles: “I wish my wish would not be granted!” (There follows a SYSTEM CRASH: or as Hofstadter puts it, “the land of dead hiccoughs and extinguished lightbulbs”). Earlier, when quizzed on what the word GOD means, the Genie explains that it's an acronym — G.O.D. — standing for “GOD OVER DJINN” (Djinn being a generic type of intermediate Eastern Deity). Does this help? Ah yes, because we can expand it: GOD = GOD OVER DJINN = (GOD OVER DJINN) OVER DJINN = ((GOD OVER DJINN) OVER DJINN) OVER DJINN ………… If this is starting to look familiar, it's because it is. A deep knowledge of the relation of a system to its metasystem and of the closely connected notion-of recursion is an essential pre-requisite for serious work in artificial intelligence. In fact I suspect that: “Artificial intelligence = recursion + superficial incompetence” What about the other book I referred to: “Behaviour: The Control of Perception” (or as they write it in the USA, “Behavior: The Control of Prception”)? In my estimation this book ranks with Norbert Wiener's “Cybernetics” (4) and Stafford Beer's “Brain of the Firm” (5): together they form the twentieth century triumvirate in this field. Powers starts off with some deceptively simple experiments and ends up with a necessary but not sufficient model of human intelligence based on first principles and an appeal to reason. The argument is convincing and is closer than any other I have read to the true core of artificial intelligence. Significantly, I don't recall in the book a single serious mention of digital computer programming.Powers explains that the basic circuit elements in organic brains are not digital at all. They are analogue, but they use frequency not voltage as the yardstick. If 10 neural impulses per second flow in an axon, that may cause (at the lowest level) a certain amount of contraction to be applied to a single muscle fiber. 20 impulses per second might double the contraction force. However, the key point is the perception of all this. Feedback from local sensors is compared against a reference level for the desired state, and the output signal is adjusted accordingly. This compensates for unexpected resistance to the muscle's work — and if you think about it, you've never lifted a weight by applying linear pressure in your life. Literally millions of low-level frequency sensitive feedback circuits create what Powers calls Level 1 — the interface to the outside world at the sensory/motor level.Then we start to encounter hierarchy. Where do the reference levels for the desired states come from? Well, these are the outputs from Level 2, but this time they correspond to a higher-level notion: like that of vertical vs. horizontal movement. By positing a hierarchy of feedback circuits whose outputs become the reference levels (i.e.. control signals) for lower level circuits, Powers will lead you to agree that when you are walking at 2 miles per hour, there is an identifiable, traceable neural frequency in your brain which doubles in frequency when you walk at 4 mph. And if you find that hard to swallow, by the time you reach higher levels of hierarchy we are talking about frequencies which correspond to degrees of emotion. Of hate. Of love. To sleep, perchance to dream… “Do Androids Dream of Electric Sheep?” (6).I suspect I have strayed somewhat from the title of this article. But I do think it's important to set the context. If you want some fun with computers, start playing with expert systems. If you want to learn something useful about artificial intelligence, start reading some books. The two are not the same and I personally doubt that insights from one much help in the assimilation of the other.Perhaps we should focus on the main theme. This gives me a small problem since the arbitrary reader of this article exists in four configurations: you know about APL (or you don't) and you know about expert systems (or you don't). Here I am going to pitch the discussion slightly in favor of someone who knows a bit about APL but nothing about expert systems, although I hope there will be some comfort for the others here and there.Let's recap on APL's strengths in this area. We have a language which supports recursion with no limits apart from workspace size. Local variables and arguments to recursive functions are properly masked and pushed to a stack on each recursive invocation. Logical operations in APL are generally implemented at data word bit level and are very comprehensive and very fast. Arrays of arbitrary dimension are supported and most primitive operations apply element-wise without looping. APL is a threaded interpretive language in which individual building blocks are defined and can be invoked in a free-standing manner (like LISP). If LISP suffers from parentheses, APL suffers from silly specialist symbols, but fortunately this last aspect is starting to disappear in the more enlightened implementations. APL is very good indeed at numeric computation of high complexity in both the scientific and the commercial domains, and in addition it is fast and elegant at string handling.Standard APL disallows non-orthogonal arrays and those of mixed character and numeric type. This is undoubtedly a disadvantage for expert systems work but is overcome in APL2. However, I haven't used APL2 seriously yet, like most other APLers I suspect, and shall confine my remarks to traditional (or as my American friends would put it, “vanilla”) APL. So the question becomes: “What has PROLOG got that APL hasn't?”The answer, surprisingly to APL aficionados, is: quite a lot. However, perhaps surprisingly to PROLOG programmers, it turns out that one can code all the standard PROLOG backtracking and similar logic capability into APL user-defined functions (i.e.. into a subroutine library). You then end up with an APL workspace oriented towards the development of expert systems, but with the great advantage of carrying with it all the “hooks” and existing facilities which have been developed in APL over the last 20 years.Evidently PROLOG users don't mind that they have no proper filing, no error control, no serious peripheral control, no multi-process synchronization, no inter-user communication, no user or file security, no computational intrinsics beyond add, subtract, divide and multiply, no interpreter available on IBM mainframes, no links to COBOL and other languages, no screen control, no graphics capability, no links to devices like “mice” … It makes it fun to develop expert systems in PROLOG, but awfully hard to sell them. So far I keep seeing demonstrations of PROLOG expert systems telling me how to water my houseplants, but the florist down the road isn't selling the package yet.Let's look at the framework of an expert system in APL terms. This path has been trodden before (7) but I'd like to look at the problem in a slightly different way. To make it more practical, let me tell you about an expert system I have been writing and playing with. It's called GENESIS 4. This sounds like a reasonable name for a package: GENESIS Release 4.0; but actually this is to do with the Bible (8). Not that I am particularly religious by nature; it's just that the Bible predates “Behaviour: The Control of Perception” as a functional specification of humanity by about 5000 years, so it's of interest to all artificial intelligence buffs.All expert systems have three chunks.The SITUATION MODEL is a whole load of information about (in this case) Adam's descendants, their marriages, their progeny and the main events in their lives.The KNOWLEDGE BASE is a set of rules which indicate the laws of genealogy. For example, “A is offspring of B if B begat A” and so on.The SYSTEM MANAGER is a set of programs that allows you to explore the situation model, ask it questions, and ask it to draw conclusions: for example, “Is Z descended from A?”. Clever system managers sometimes try to propose rules by looking for patterns in large amounts of data in a situation model.That's all.What I will do now is re-write parts of Genesis Chapter 4 as if I were entering lines into an APL-based expert system, trying to keep it as much like PROLOG as possible. The numbers in the margins are the verses.1. “And Adam knew Eve his wife; and she conceived, and bare Cain, and said 'I have gotten a man from the Lord'.”ADAM IS-MARRIED-TO EVECAIN IS-OFFSPRING-OF ADAMCAIN IS-OFFSPRING-OF EVECAIN IS-SEX MALE2. “And she again bare his brother Abel.”ABEL IS-OFFSPRING-OF EVEABEL IS-SEX MALE8. “Cain rose up against Abel his brother, and slew him.”CAIN KILLS ABEL16. “Cain went out from the presence of the Lord, and dwelt in the land of Nod, on the east of Eden.”CAIN INHABITS NODNOD IS-EAST-OF EDEN17. “And Cain knew his wife, and she conceived, and bare Enoch: and he builded a city, and called the name of the city, after the name of his son, Enoch. And unto Enoch was born Irad: and Irad begat Mehujael: and Mehujael began Methusael: and Methusael begat Lamech.” CAIN IS-MARRIED-TO MRS.X ENOCH IS-OFFSPRING-OF CAIN ENOCH IS-OFFSPRING-OF MRS.X CAIN BUILDS-CITY-CALLED ENOCH IRAD IS-OFFSPRING-OF ENOCH MEHUJAEL IS-OFFSPRING-OF IRAD METHUSAEL IS-OFFSPRING-OF MEHUJAEL LAMECH IS-OFFSPRING-OF METHUSAELThere's a lot more of this sort of thing. I know that the Bible is now available in compressed text form on floppy discs as “THE WORD Processor” (9) but I'll wager that the logical inferences remain unencoded.What can we do with APL so far? Well, we need to encode this information into APL data structures. There are many ways of doing this, of course. An obvious approach is to write a little parser which takes statements like LAMECH IS-OFFSPRING-OF METHUSAEL and divide them into their different text strings (3 in this case). We start building a text matrix with a unique instance of each name. When a statement comes in, we look up the row number of each string or we add the new string to the end of the matrix. So the sequence:MEHUJAEL IS-OFFSPRING-OF IRADMETHUSAEL IS-OFFSPRING-OF MEHUJAELLAMECH IS-OFFSPRING-OF METHUSAELwould be represented by these two matrices:MEHUJAEL 1 2 3IS-OFFSPRING-OF 4 2 1IRAD 5 2 4METHUSAELLAMECHActually it's slightly trickier than this as we shall need a flag bit to indicate which entry is a relationship and which is a noun, and they aren't always binary relationships, and the arguments aren't always on each side. But the basic idea's the same. A simple parsing function in APL is about 5 lines of code, and if we call it ADD it really does look like PROLOG.: ADD 'LAMECH IS-OFFSPRING-OF METHUSAEL'So far, so trivial, but we've done something epoch-making: we've defined our first function in XPL, which is the name I have chosen for this set of functions, written in APL but using ASCII character set throughout. Now we want to interrogate the database:“Is Lamech the offspring of Methusael?” DOES 'LAMECH IS-OFFSPRING-OF METHUSAEL' TRUE“Is Irad the offspring of Mehujael?” DOES 'IRAD IS-OFFSPRING-OF MEHUJAEL' FALSEFunny syntax, isn't it? I'm trying to stick to PROLOG though, and this is it. Now is it mindbogglingly hard to code this in XPL? Not really: function DOES looks to be at least one line long: it has to find a particular row of integers in the matrix. Of course things get tougher in PROLOG quite quickly:“Who is the offspring of Methusael?” WHICH 'X:X IS-OFFSPRING-OF METHUSAEL' LAMECH“Who is the offspring of Lamech?” WHICH 'X:X IS-OFFSPRING-OF LAMECH' NO MATCHThis time not only do we have to find a particular row of integers in the numeric matrix, we also have to work out who X refers to and find him in the list of names. Fiendish stuff. Still, I think we can just about cope. Let's move onto the knowledge base. Here's some useful jargon. You and I are “knowledge engineers” in the context of our translation of the Bible into XPL format. Priests are “domain specialists”: Heaven and Hell is their domain. To this I would add that people who create expert systems languages, as we are doing at this moment, are presumably “metadomain specialists” (I believe this is an original coinage).It gets boring telling XPL everything twice. For example:ADD 'LAMECH IS-OFFSPRING-OF METHUSAEL' ADD 'METHUSAEL IS-PARENT-OF LAMECH'How about defining a rule instead?ADD 'X IS-PARENT OF Y IF Y IS-OFFSPRING-OF X'and while we're about it:ADD 'X IS-OFFSPRING-OF Y IF Y IS-PARENT-OF X'Not too hard to handle either is:ADD 'X IS-HUSBAND OF Y IF X IS-MARRIED-TO Y AND X IS-SEX MALE'Now try:ADD 'X IS-GRANDCHILD-OF Y IF X IS-OFFSPRING-OF Z AND Z IS-OFFSPRING-OF YWe've introduced the concept of “place markers”, which are analogous to the local names of the arguments of functions, except that the functions are relations (data) rather than programs. There are many ways to incorporate these into XPL and your own ideas will undoubtedly be better then mine: I use negative numbers in the integer matrix to denote place markers:MEHUJAEL 1 2 3IS-OFFSPRING-OF 4 2 1IRAD 5 2 4METHUSAEL -1 6 -2LAMECH -1 2 -3IS-GRANDCHILD-OF -3 2 -2The functions DOES and WHICH now become rather more interesting. Suppose we ask the question:DOES 'ENOCH IS-GRANDCHILD-OF ADAM'DOES now has to go scouting around to see if this is a bit of known data. If not, it goes to see if it has a rule-based definition for the grandchild relation. If it does, it has to find the first match for Enoch based on the offspring relation, and then pause while it tests whether this match in turn relates to Enoch. If not, it can resume looking for further matches for Enoch. It goes without saying that an XPL function employing recursion is the elegant (possibly the only) way to implement this process. It isn't very difficult to do.Here's a more interesting rule:ADD 'X IS-DESCENDANT-OF Y IF X IS-CHILD-OF Y' ADD 'X IS-DESCENDANT-OF Y IF X IS-CHILD-OF Z AND Z IS-DESCENDANT-OF Y'Recursion groupies will see the classic syntax: the first line denotes the limiting condition, the second line forms the recursive relation. The extensive use of recursive tests based on goal-seeking requests from the user is termed “backtracking” for obvious reasons. The standard mechanism of the state indicator in APL handles such requests beautifully.I hope that those with some APL experience can see that it is not inordinately tricky to implement these aspects of PROLOG in user-defined APL functions. After a while the imagination starts running riot:ADD 'X PROBABLY IS-CHILD-OF Y IF X IS-CHILD-OF Z AND Y IS-MARRIED-TO Z'I suppose there's always the milkman. Adverbs like DEFINITELY, PROBABLY, POSSIBLY, CONCEIVABLY (oops) and INCONCEIVABLY can conveniently carry probability values like 1.0, 0.75, 0.5, 0.25 and 0; if you want to change the climate of expert opinion you can tinker with the values. And don't forget NOT:ADD 'X IS-SEX FEMALE IF X NOT IS-SEX MALE'Well, we don't all live in San Francisco.What about expert systems themselves? If you recall the three elements:then it turns out in languages like PROLOG that the preceding constructs are used on an intermixed basis to represent data in the situation model and rules in the knowledge base. In XPL one feels it may be cleaner to keep them separate, but that's a matter for personal taste.More importantly, most PROLOG users don't seem to have caught on to the fact that much of the data in the situation model may well be a transform of, for example, data about patients in a medical file whose records have been painstakingly composed over many years. File access to other systems is imperative; most of the facts are already there.Furthermore, although PROLOG is good at relations, it's very bad at numerical tests. An expert system for shop floor control needs to cope with quantified tests based on quasi-real-time input from process sensors: for example:ADD 'TURBINE SWITCH-SETTING OFF IF RPM ABOVE 5000' is as far as I know impossible to handle in PROLOG if RPM is intended to be directly linked to the real world, whereas in APL one simply makes RPM a variable shared with an auxiliary processor which handles direct data capture. For non-APLers this means: variable RPM in the APL workspace is dynamically coupled via programmer-specifiable update protocol with an arbitrary machine code segment, which is itself either handling process input/output or is talking to some other process that is. The point is that a robust and very well established framework for this kind of dynamic linking already exists in APL.I hope I have managed to whet two kinds of appetite. If you know APL already, you may be starting to think about building your own XPL functions (mine are for private consumption and you've had numerous hints already) and writing your own expert system. If you know about expert systems and are toiling with inadequate tools, you might just think about looking at APL instead. An unlikely marriage? I don't think so. No more than Cain's, anyway; the identity of Mrs X is the theological riddle of the millenium. Good luck with such experiments: may thy domain specialties be brought forth and multiply upon the interface of the digital computer.},
booktitle = {Proceedings of the International Conference on APL: APL and the Future},
pages = {173–180},
numpages = {8},
location = {Seattle, Washington, USA},
series = {APL '85}
}

@article{10.1145/255315.255364,
author = {Bittlestone, Robert},
title = {XPL: An Expert Systems Framework in APL},
year = {1985},
issue_date = {May 12, 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {0163-6006},
url = {https://doi.org/10.1145/255315.255364},
doi = {10.1145/255315.255364},
abstract = {It is hardly possible to pick up a computer journal today without seeing references to the Alvey project in the UK, the Esprit project run by the EEC, the Japanese “Fifth generation” initiative, the proliferation of knowledge-based computer companies around Stanford in California and MIT in Massachusetts, and the large amounts of money that are being poured into the development of expert systems. The references abound: as I write this article the September 1984 issue of “International Management” (1) arrives on my desk with a cover depicting a digitized human brain image on a computer screen and a caption saying “Artificial intelligence: the race to make it work for managers”. I like the use of the word “race” here; it seems to imply that this is our last chance now that the prospect of human intelligence in managers has declined asymptotically to zero.Among the savants cited in the international Management article is Sir Clive Sinclair, no less, who has become notorious with the British Computer Society's Expert Systems Specialist Group: notorious for arriving unexpectedly at meetings and giving the rest of the audience something to think about. Sir Clive apparently said in the interview: “Once 'machines of silicon' surpass us, they will be capable of their own design. In a real sense, they will be reproductive, and silicon will have ended carbon's long monopoly. And ours too, I suppose, for we will no longer be able to deem ourselves the finest intelligence in the known universe.”Maybe expert systems will dominate us after all — it's just that there'll be a ten-month delay between quoting your Visa number and becoming an inferior intelligence.All this is quite fascinating since the phenomenon is very recent: before 1983 the phrase “expert systems” was largely unheard of and certainly not in itself viewed as the potential salvation of the Western world (there are as yet few hints of Eastern bloc development apart from in Hungary, where the University of Budapest, along with those of Marseilles, Edinburgh and London's Imperial College, claims to have invented PROLOG). However, by now there has been “Horizon” television coverage of the genre and there seems little doubt that expert systems are here to stay. But before I take up the theme of my title, perhaps I might be allowed to state my position on one or two related matters. First of all: “Expert systems are not the same thing as artificial intelligence”We can see the beginnings of a bandwagon effect here in which technical namedroppers, bored of stating that “UNIX is the coming thing” or “The future of computing lies in networking” are catching up with this latest trend: “Life is just another form of domain specialism”. Expert systems are in fact a very narrowly defined class of computer program with certain in-built inference-drawing capabilities. Artificial intelligence, on the other hand, is a much wider field involving the generic search for synthetic rational life. Much of the most interesting work in artificial intelligence does not involve digital computers at all but concerns the possibility of producing artificial models of brain process, using special fabrication techniques to emulate neurons, axons and synapses. Which brings me to my second definition: “Artificial intelligence will not be implemented on digital computers”It turns out that the use of a digital clocked device, irrespective of speed, imposes a fatal limitation on the sophistication of a genuine search for artificial intelligence, and parallel processing makes no difference. The reason is that in the brain it is the dynamic connections between at least 10 to the power 10 asynchronous independent processing elements that provide the kind of complexity that is required. The processors in the brain are called neurons and the interconnection are called axons, and there is substantial evidence to indicate that to a first approximation, almost any neuron can grow an axon to connect to almost any other neuron.Now the number of connections between 'N' elements increases in proportion to the square of 'N'. If you have three neurons you can connect A to B, A to C, B to C, and the same in the other direction, which is valid since it corresponds to a negative signal (inhibition vs. excitation). Consequently we have 6 possibilities for the network topology: i.e. 3x2. For 4 elements we have 12 options: 4x3. If we had a mere 100 neurons the result of 100x99 is still only about 10,000. However, in the brain we are considering about 10,000,000,000 neurons. So the number of possible connections is 100,000,000,000,000,000,000 — a number of literally astronomic proportions. We haven't yet introduced the complexity of neurons which connect to more than one other neuron (the majority do), nor signal processing within the neurons themselves (estimated as a seventh order differential equation), nor chemical effects at the synapses, which are retransmission points along the axons. When you drink too much alcohol, it is primarily the synaptic effects which alter your responses. I am unconvinced that there yet exists an artificially intelligent program capable of intoxication.If you now consider the implications of asynchronous (but not parallel) operations taking place between arbitrarily connectable elements on this scale, you begin to see that even a billion 64-bit microprocessors would make no difference at all. The emphasis in the brain is on analogue circuits in which feedback and frequency of signal are the determining factor and where there is no single location at which processing can be said to be taking place; the whole of the brain is in this sense intelligent.When I asked the Alvey representative at a recent meeting of the Expert Systems Specialist Group (yes, Sir Clive was there too) why there was so little emphasis on neurophysiology in the Alvey program (I was uncharacteristically diplomatic as in fact there is none at all) his response was that the program concerned the “hard” sciences and engineering: the bridge to biology was too narrow for engineers to cross. I believe he was genuinely rueful. However, if your appetite is in any way whetted for “real” artificial intelligence, you had better go out and buy two rather extraordinary books. One of them is, of course, Hofstadter's “Godel Escher Bach: An Eternal Golden Braid” (2). The other is less well-known: William T. Powers's “Behaviour: The Control of Perception” (3).In Hofstadter's book the notion of recursion is explored in many delightful ways. For those who have missed it, here is a sampler. Our heroes, Achilles and the Tortoise, have found a lamp. They rub it and a Genie appears. The Genie will grant three wishes. This prospect is greeted with glee by Achilles, who has read the 'Arabian Night' and knows the form. He makes his first wish: Achilles: “I wish that I had 100 wishes instead of just three.” Genie: “I am sorry, Achilles, but I don't grant meta-wishes.” Achilles: “I wish you'd tell me what a 'meta-wish' is.” Genie: “But that is a meta-meta-wish, Achilles, and I don't grant them either.” But after some pleading, the Genie agrees to try (meta-wishes are his favorite wishes too) and from inside his cloak he produces a lamp. He rubs it and out pops a MetaGenie: Genie: “I wish for permission for temporary suspension of all type-restrictions on wishes, for the duration of one typeless wish.” MetaGenie:“Half a moment please (produces a lamp, rubs it, etc.).” This continues for a short infinite period until we get to the top-level MetaGenie, also known as GOD, who grants permission for the typeless wish: Achilles: “I wish my wish would not be granted!” (There follows a SYSTEM CRASH: or as Hofstadter puts it, “the land of dead hiccoughs and extinguished lightbulbs”). Earlier, when quizzed on what the word GOD means, the Genie explains that it's an acronym — G.O.D. — standing for “GOD OVER DJINN” (Djinn being a generic type of intermediate Eastern Deity). Does this help? Ah yes, because we can expand it: GOD = GOD OVER DJINN = (GOD OVER DJINN) OVER DJINN = ((GOD OVER DJINN) OVER DJINN) OVER DJINN ………… If this is starting to look familiar, it's because it is. A deep knowledge of the relation of a system to its metasystem and of the closely connected notion-of recursion is an essential pre-requisite for serious work in artificial intelligence. In fact I suspect that: “Artificial intelligence = recursion + superficial incompetence” What about the other book I referred to: “Behaviour: The Control of Perception” (or as they write it in the USA, “Behavior: The Control of Prception”)? In my estimation this book ranks with Norbert Wiener's “Cybernetics” (4) and Stafford Beer's “Brain of the Firm” (5): together they form the twentieth century triumvirate in this field. Powers starts off with some deceptively simple experiments and ends up with a necessary but not sufficient model of human intelligence based on first principles and an appeal to reason. The argument is convincing and is closer than any other I have read to the true core of artificial intelligence. Significantly, I don't recall in the book a single serious mention of digital computer programming.Powers explains that the basic circuit elements in organic brains are not digital at all. They are analogue, but they use frequency not voltage as the yardstick. If 10 neural impulses per second flow in an axon, that may cause (at the lowest level) a certain amount of contraction to be applied to a single muscle fiber. 20 impulses per second might double the contraction force. However, the key point is the perception of all this. Feedback from local sensors is compared against a reference level for the desired state, and the output signal is adjusted accordingly. This compensates for unexpected resistance to the muscle's work — and if you think about it, you've never lifted a weight by applying linear pressure in your life. Literally millions of low-level frequency sensitive feedback circuits create what Powers calls Level 1 — the interface to the outside world at the sensory/motor level.Then we start to encounter hierarchy. Where do the reference levels for the desired states come from? Well, these are the outputs from Level 2, but this time they correspond to a higher-level notion: like that of vertical vs. horizontal movement. By positing a hierarchy of feedback circuits whose outputs become the reference levels (i.e.. control signals) for lower level circuits, Powers will lead you to agree that when you are walking at 2 miles per hour, there is an identifiable, traceable neural frequency in your brain which doubles in frequency when you walk at 4 mph. And if you find that hard to swallow, by the time you reach higher levels of hierarchy we are talking about frequencies which correspond to degrees of emotion. Of hate. Of love. To sleep, perchance to dream… “Do Androids Dream of Electric Sheep?” (6).I suspect I have strayed somewhat from the title of this article. But I do think it's important to set the context. If you want some fun with computers, start playing with expert systems. If you want to learn something useful about artificial intelligence, start reading some books. The two are not the same and I personally doubt that insights from one much help in the assimilation of the other.Perhaps we should focus on the main theme. This gives me a small problem since the arbitrary reader of this article exists in four configurations: you know about APL (or you don't) and you know about expert systems (or you don't). Here I am going to pitch the discussion slightly in favor of someone who knows a bit about APL but nothing about expert systems, although I hope there will be some comfort for the others here and there.Let's recap on APL's strengths in this area. We have a language which supports recursion with no limits apart from workspace size. Local variables and arguments to recursive functions are properly masked and pushed to a stack on each recursive invocation. Logical operations in APL are generally implemented at data word bit level and are very comprehensive and very fast. Arrays of arbitrary dimension are supported and most primitive operations apply element-wise without looping. APL is a threaded interpretive language in which individual building blocks are defined and can be invoked in a free-standing manner (like LISP). If LISP suffers from parentheses, APL suffers from silly specialist symbols, but fortunately this last aspect is starting to disappear in the more enlightened implementations. APL is very good indeed at numeric computation of high complexity in both the scientific and the commercial domains, and in addition it is fast and elegant at string handling.Standard APL disallows non-orthogonal arrays and those of mixed character and numeric type. This is undoubtedly a disadvantage for expert systems work but is overcome in APL2. However, I haven't used APL2 seriously yet, like most other APLers I suspect, and shall confine my remarks to traditional (or as my American friends would put it, “vanilla”) APL. So the question becomes: “What has PROLOG got that APL hasn't?”The answer, surprisingly to APL aficionados, is: quite a lot. However, perhaps surprisingly to PROLOG programmers, it turns out that one can code all the standard PROLOG backtracking and similar logic capability into APL user-defined functions (i.e.. into a subroutine library). You then end up with an APL workspace oriented towards the development of expert systems, but with the great advantage of carrying with it all the “hooks” and existing facilities which have been developed in APL over the last 20 years.Evidently PROLOG users don't mind that they have no proper filing, no error control, no serious peripheral control, no multi-process synchronization, no inter-user communication, no user or file security, no computational intrinsics beyond add, subtract, divide and multiply, no interpreter available on IBM mainframes, no links to COBOL and other languages, no screen control, no graphics capability, no links to devices like “mice” … It makes it fun to develop expert systems in PROLOG, but awfully hard to sell them. So far I keep seeing demonstrations of PROLOG expert systems telling me how to water my houseplants, but the florist down the road isn't selling the package yet.Let's look at the framework of an expert system in APL terms. This path has been trodden before (7) but I'd like to look at the problem in a slightly different way. To make it more practical, let me tell you about an expert system I have been writing and playing with. It's called GENESIS 4. This sounds like a reasonable name for a package: GENESIS Release 4.0; but actually this is to do with the Bible (8). Not that I am particularly religious by nature; it's just that the Bible predates “Behaviour: The Control of Perception” as a functional specification of humanity by about 5000 years, so it's of interest to all artificial intelligence buffs.All expert systems have three chunks.The SITUATION MODEL is a whole load of information about (in this case) Adam's descendants, their marriages, their progeny and the main events in their lives.The KNOWLEDGE BASE is a set of rules which indicate the laws of genealogy. For example, “A is offspring of B if B begat A” and so on.The SYSTEM MANAGER is a set of programs that allows you to explore the situation model, ask it questions, and ask it to draw conclusions: for example, “Is Z descended from A?”. Clever system managers sometimes try to propose rules by looking for patterns in large amounts of data in a situation model.That's all.What I will do now is re-write parts of Genesis Chapter 4 as if I were entering lines into an APL-based expert system, trying to keep it as much like PROLOG as possible. The numbers in the margins are the verses.1. “And Adam knew Eve his wife; and she conceived, and bare Cain, and said 'I have gotten a man from the Lord'.”ADAM IS-MARRIED-TO EVECAIN IS-OFFSPRING-OF ADAMCAIN IS-OFFSPRING-OF EVECAIN IS-SEX MALE2. “And she again bare his brother Abel.”ABEL IS-OFFSPRING-OF EVEABEL IS-SEX MALE8. “Cain rose up against Abel his brother, and slew him.”CAIN KILLS ABEL16. “Cain went out from the presence of the Lord, and dwelt in the land of Nod, on the east of Eden.”CAIN INHABITS NODNOD IS-EAST-OF EDEN17. “And Cain knew his wife, and she conceived, and bare Enoch: and he builded a city, and called the name of the city, after the name of his son, Enoch. And unto Enoch was born Irad: and Irad begat Mehujael: and Mehujael began Methusael: and Methusael begat Lamech.” CAIN IS-MARRIED-TO MRS.X ENOCH IS-OFFSPRING-OF CAIN ENOCH IS-OFFSPRING-OF MRS.X CAIN BUILDS-CITY-CALLED ENOCH IRAD IS-OFFSPRING-OF ENOCH MEHUJAEL IS-OFFSPRING-OF IRAD METHUSAEL IS-OFFSPRING-OF MEHUJAEL LAMECH IS-OFFSPRING-OF METHUSAELThere's a lot more of this sort of thing. I know that the Bible is now available in compressed text form on floppy discs as “THE WORD Processor” (9) but I'll wager that the logical inferences remain unencoded.What can we do with APL so far? Well, we need to encode this information into APL data structures. There are many ways of doing this, of course. An obvious approach is to write a little parser which takes statements like LAMECH IS-OFFSPRING-OF METHUSAEL and divide them into their different text strings (3 in this case). We start building a text matrix with a unique instance of each name. When a statement comes in, we look up the row number of each string or we add the new string to the end of the matrix. So the sequence:MEHUJAEL IS-OFFSPRING-OF IRADMETHUSAEL IS-OFFSPRING-OF MEHUJAELLAMECH IS-OFFSPRING-OF METHUSAELwould be represented by these two matrices:MEHUJAEL 1 2 3IS-OFFSPRING-OF 4 2 1IRAD 5 2 4METHUSAELLAMECHActually it's slightly trickier than this as we shall need a flag bit to indicate which entry is a relationship and which is a noun, and they aren't always binary relationships, and the arguments aren't always on each side. But the basic idea's the same. A simple parsing function in APL is about 5 lines of code, and if we call it ADD it really does look like PROLOG.: ADD 'LAMECH IS-OFFSPRING-OF METHUSAEL'So far, so trivial, but we've done something epoch-making: we've defined our first function in XPL, which is the name I have chosen for this set of functions, written in APL but using ASCII character set throughout. Now we want to interrogate the database:“Is Lamech the offspring of Methusael?” DOES 'LAMECH IS-OFFSPRING-OF METHUSAEL' TRUE“Is Irad the offspring of Mehujael?” DOES 'IRAD IS-OFFSPRING-OF MEHUJAEL' FALSEFunny syntax, isn't it? I'm trying to stick to PROLOG though, and this is it. Now is it mindbogglingly hard to code this in XPL? Not really: function DOES looks to be at least one line long: it has to find a particular row of integers in the matrix. Of course things get tougher in PROLOG quite quickly:“Who is the offspring of Methusael?” WHICH 'X:X IS-OFFSPRING-OF METHUSAEL' LAMECH“Who is the offspring of Lamech?” WHICH 'X:X IS-OFFSPRING-OF LAMECH' NO MATCHThis time not only do we have to find a particular row of integers in the numeric matrix, we also have to work out who X refers to and find him in the list of names. Fiendish stuff. Still, I think we can just about cope. Let's move onto the knowledge base. Here's some useful jargon. You and I are “knowledge engineers” in the context of our translation of the Bible into XPL format. Priests are “domain specialists”: Heaven and Hell is their domain. To this I would add that people who create expert systems languages, as we are doing at this moment, are presumably “metadomain specialists” (I believe this is an original coinage).It gets boring telling XPL everything twice. For example:ADD 'LAMECH IS-OFFSPRING-OF METHUSAEL' ADD 'METHUSAEL IS-PARENT-OF LAMECH'How about defining a rule instead?ADD 'X IS-PARENT OF Y IF Y IS-OFFSPRING-OF X'and while we're about it:ADD 'X IS-OFFSPRING-OF Y IF Y IS-PARENT-OF X'Not too hard to handle either is:ADD 'X IS-HUSBAND OF Y IF X IS-MARRIED-TO Y AND X IS-SEX MALE'Now try:ADD 'X IS-GRANDCHILD-OF Y IF X IS-OFFSPRING-OF Z AND Z IS-OFFSPRING-OF YWe've introduced the concept of “place markers”, which are analogous to the local names of the arguments of functions, except that the functions are relations (data) rather than programs. There are many ways to incorporate these into XPL and your own ideas will undoubtedly be better then mine: I use negative numbers in the integer matrix to denote place markers:MEHUJAEL 1 2 3IS-OFFSPRING-OF 4 2 1IRAD 5 2 4METHUSAEL -1 6 -2LAMECH -1 2 -3IS-GRANDCHILD-OF -3 2 -2The functions DOES and WHICH now become rather more interesting. Suppose we ask the question:DOES 'ENOCH IS-GRANDCHILD-OF ADAM'DOES now has to go scouting around to see if this is a bit of known data. If not, it goes to see if it has a rule-based definition for the grandchild relation. If it does, it has to find the first match for Enoch based on the offspring relation, and then pause while it tests whether this match in turn relates to Enoch. If not, it can resume looking for further matches for Enoch. It goes without saying that an XPL function employing recursion is the elegant (possibly the only) way to implement this process. It isn't very difficult to do.Here's a more interesting rule:ADD 'X IS-DESCENDANT-OF Y IF X IS-CHILD-OF Y' ADD 'X IS-DESCENDANT-OF Y IF X IS-CHILD-OF Z AND Z IS-DESCENDANT-OF Y'Recursion groupies will see the classic syntax: the first line denotes the limiting condition, the second line forms the recursive relation. The extensive use of recursive tests based on goal-seeking requests from the user is termed “backtracking” for obvious reasons. The standard mechanism of the state indicator in APL handles such requests beautifully.I hope that those with some APL experience can see that it is not inordinately tricky to implement these aspects of PROLOG in user-defined APL functions. After a while the imagination starts running riot:ADD 'X PROBABLY IS-CHILD-OF Y IF X IS-CHILD-OF Z AND Y IS-MARRIED-TO Z'I suppose there's always the milkman. Adverbs like DEFINITELY, PROBABLY, POSSIBLY, CONCEIVABLY (oops) and INCONCEIVABLY can conveniently carry probability values like 1.0, 0.75, 0.5, 0.25 and 0; if you want to change the climate of expert opinion you can tinker with the values. And don't forget NOT:ADD 'X IS-SEX FEMALE IF X NOT IS-SEX MALE'Well, we don't all live in San Francisco.What about expert systems themselves? If you recall the three elements:then it turns out in languages like PROLOG that the preceding constructs are used on an intermixed basis to represent data in the situation model and rules in the knowledge base. In XPL one feels it may be cleaner to keep them separate, but that's a matter for personal taste.More importantly, most PROLOG users don't seem to have caught on to the fact that much of the data in the situation model may well be a transform of, for example, data about patients in a medical file whose records have been painstakingly composed over many years. File access to other systems is imperative; most of the facts are already there.Furthermore, although PROLOG is good at relations, it's very bad at numerical tests. An expert system for shop floor control needs to cope with quantified tests based on quasi-real-time input from process sensors: for example:ADD 'TURBINE SWITCH-SETTING OFF IF RPM ABOVE 5000' is as far as I know impossible to handle in PROLOG if RPM is intended to be directly linked to the real world, whereas in APL one simply makes RPM a variable shared with an auxiliary processor which handles direct data capture. For non-APLers this means: variable RPM in the APL workspace is dynamically coupled via programmer-specifiable update protocol with an arbitrary machine code segment, which is itself either handling process input/output or is talking to some other process that is. The point is that a robust and very well established framework for this kind of dynamic linking already exists in APL.I hope I have managed to whet two kinds of appetite. If you know APL already, you may be starting to think about building your own XPL functions (mine are for private consumption and you've had numerous hints already) and writing your own expert system. If you know about expert systems and are toiling with inadequate tools, you might just think about looking at APL instead. An unlikely marriage? I don't think so. No more than Cain's, anyway; the identity of Mrs X is the theological riddle of the millenium. Good luck with such experiments: may thy domain specialties be brought forth and multiply upon the interface of the digital computer.},
journal = {SIGAPL APL Quote Quad},
month = {may},
pages = {173–180},
numpages = {8}
}

@inproceedings{10.1145/3106548.3106596,
author = {Azevedo, Werther and Bidarra, Jos\'{e}},
title = {Blendwave: A Sound Design Tool for Audiovisual Authors},
year = {2017},
isbn = {9781450352734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106548.3106596},
doi = {10.1145/3106548.3106596},
abstract = {This paper presents Blendwave, an online sound design application inspired by the ease of use of samplers and the "sfxr family" of sound creation software. Starting from the hypothesis where current software tools have made the creation of sound inaccessible for non-specialist users, we analyze the problems with the musical bias ingrained in DAWs and Patchers, the standard sound design tools. After identifying the Web Audio API as an appropriate technological backbone for an accessible tool, recent online applications are surveyed, followed by popular software used in a rapid prototyping scenario. By acknowledging the merits and shortcomings of these sound creators, we present Blendwave, the by product of a practice-based research and development effort towards democratizing sound design as a rapid prototyping activity for audiovisual authors. The idea of a sampler architecture is proposed as a way to augment the sound possibilities of sfxr-based tools while keeping their simplicity. Finally, we report on the current state of development and outline the next steps.},
booktitle = {Proceedings of the 8th International Conference on Digital Arts},
pages = {153–156},
numpages = {4},
keywords = {Sound design, sound effects, creativity support tools, web audio, practice-based research, samplers},
location = {Macau, China},
series = {ARTECH2017}
}

@inproceedings{10.1145/1568234.1568250,
author = {Gordon, Thomas F. and Walton, Douglas},
title = {Legal Reasoning with Argumentation Schemes},
year = {2009},
isbn = {9781605585970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568234.1568250},
doi = {10.1145/1568234.1568250},
abstract = {Legal reasoning typically requires a variety of argumentation schemes to be used together. A legal case may raise issues requiring argument from precedent cases, rules, policy goals, moral principles, jurisprudential doctrine, social values and evidence. We present an extensible software architecture which allows diverse computational models of argumentation schemes to be used together in an integrated way to construct and search for arguments. The architecture has been implemented in Carneades, a software library for building argumentation tools. The architecture is illustrated with models of schemes for argument from ontologies, rules, cases and testimonial evidence and compared to blackboard systems for hybrid reasoning.},
booktitle = {Proceedings of the 12th International Conference on Artificial Intelligence and Law},
pages = {137–146},
numpages = {10},
location = {Barcelona, Spain},
series = {ICAIL '09}
}

@inproceedings{10.1145/1536513.1536567,
author = {van Langeveld, Mark and Kessler, Robert},
title = {Educational Impact of Digital Visualization and Auditing Tools on a Digital Character Production Course},
year = {2009},
isbn = {9781605584379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1536513.1536567},
doi = {10.1145/1536513.1536567},
abstract = {Many Universities and Colleges are building interdisciplinary programs that overlap engineering and fine art departments allowing a focus on games, special effects, animation and other forms of invention that require interdisciplinary efforts. With increasing demands for education linking the Engineering Sciences and Fine Arts, fueled by the competitive nature of the industries that recruit graduates, educators need to become more efficient and effective in their jobs of educating engineering and fine art majors in cross-disciplinary courses.Many of the courses that draw from the disciplines of the engineering sciences and fine arts are neglected in the research of educational best practices and tools for enhancing the learning experiences of the students. The courses are also becoming larger and more unmanageable as presently taught.Digital character production courses can range from predominantly engineering content to one with mostly art content, or a combination in the middle of the two departments depending on curriculum design. Our course at the University of Utah is directly in the center, drawing equally from both disciplines. It gives students an applied experience in 3D graphics and the computer science that helps to produce 3D graphics. It is a prerequisite for our Machinima Class, which extends the blend of art and technology as it immerses students into 3D game engines for creating cinematic short animations. Our particular digital character production curriculum has been taught at many universities and has been refined over the last twelve years.Presented in this paper are two experimental comparisons between traditional visualization tools and digital visualization tools, which are less expensive, easier to distribute, easier to arrange/procure and easier to transport than the traditional tools. Traditional visualization tools include lifelike skeleton reproductions, wooden body mass structures, actual human models and anatomy drawing books. They are used in conjunction with lectures, demonstrations and one-on-one lab instruction. The digital visualization tools that are contrasted in this paper are: a layered anatomically correct, digital human model (skin, muscles, masses and some bones adapted from several sources) and a VisTrails MAYA version of a properly produced human figure (interactive animation). The digital tools are used to replace the traditional visualization tools used in the same educational curriculum, which teaches students to design, model and produce digital characters for games, machinima, and animation. The first experiment assesses if the digital visualization tools are comparable to traditional tools in three areas; for improving a student's understanding of the complex software packages used to produce characters, for improving specific techniques used to model 3D characters, and for improving understanding of 3D form/visual relationships. The second experimental comparison extends the analysis of the third area garnered from the other experiment to determine if the improved understanding of form/visual relationships extends into non-digital medium. Both experiments were designed to measure learning experiences and the ability to adapt modeling processes to a variety of different characters and not to just duplicate the process on similar characters.},
booktitle = {Proceedings of the 4th International Conference on Foundations of Digital Games},
pages = {316–323},
numpages = {8},
keywords = {anatomy, 3D character modeling, digital entertainment industry, form, technique, interdisciplinary curriculum, video games, contour design, machinima, academic programs, education},
location = {Orlando, Florida},
series = {FDG '09}
}

@inproceedings{10.1145/3037697.3037754,
author = {Churchill, Berkeley and Sharma, Rahul and Bastien, JF and Aiken, Alex},
title = {Sound Loop Superoptimization for Google Native Client},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037754},
doi = {10.1145/3037697.3037754},
abstract = {Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {313–326},
numpages = {14},
keywords = {equivalence checking, bounded verification, verification, native client, assembly, superoptimization, x86-64, data-driven verification},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{10.1145/3093336.3037754,
author = {Churchill, Berkeley and Sharma, Rahul and Bastien, JF and Aiken, Alex},
title = {Sound Loop Superoptimization for Google Native Client},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093336.3037754},
doi = {10.1145/3093336.3037754},
abstract = {Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {313–326},
numpages = {14},
keywords = {native client, data-driven verification, verification, equivalence checking, x86-64, bounded verification, assembly, superoptimization}
}

@article{10.1145/3093337.3037754,
author = {Churchill, Berkeley and Sharma, Rahul and Bastien, JF and Aiken, Alex},
title = {Sound Loop Superoptimization for Google Native Client},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/3093337.3037754},
doi = {10.1145/3093337.3037754},
abstract = {Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google.},
journal = {SIGARCH Comput. Archit. News},
month = {apr},
pages = {313–326},
numpages = {14},
keywords = {verification, native client, equivalence checking, assembly, bounded verification, x86-64, superoptimization, data-driven verification}
}

@inproceedings{10.1109/ISCA45697.2020.00045,
author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
title = {MLPerf Inference Benchmark},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00045},
doi = {10.1109/ISCA45697.2020.00045},
abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {446–459},
numpages = {14},
keywords = {machine learning, benchmarking, inference},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1145/24533.24541,
author = {Wicklund, Gary A and Roth, Roberta M},
title = {Expert Systems in Insurance Underwriting: Model Development and Application},
year = {1987},
isbn = {0897912225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/24533.24541},
doi = {10.1145/24533.24541},
abstract = {The paper examines the work of Dr. Gary A. Wicklund and doctoral student Ms. Roberta M. Roth of the University of Iowa in the development and implementation of an insurance underwriting expert system for a Midwestern insurance company. The underwriting function reviews applicant data for determination of insurability. The feasibility of utilizing an expert system in this area was examined and determined to be practical, and development of a prototype was initiated.The authors began a broad search for and examination of available expert system tools for a personal computer environment. The review of six expert system software packages narrowed the field to two viable candidates. The Ml package from Teknowledge was selected for the first prototyping stage. The prototype was quickly developed with a very limited set of rules to provide a simple demonstration of the applicability of an expert system in this area. The demonstration of the expert system concepts through the prototype to upper management and underwriting management at a local insurance company was well accepted and authorization to build the expert system was given.The host mainframe at the sponsoring insurance company proved incompatible with the M1 software, which was replaced by RuleMaster from Radian. The prototyping work was transferred to RuleMaster, which proved to be beneficial beyond the compatibility concerns. RuleMaster is a modular system as opposed to the inference network approach used in M1, which more closely resembles LISP or PROLOG in structure. With the modular structure, knowledge and rule base building and modifications proved much easier from a programming standpoint.The interviewing of the domain expert was very valuable for future reference in the next domain to be prototyped. Because of the inexperience of the parties, the interviews were unstructured and the knowledge acquisition was a laborious task for the knowledge engineer and the expert. The next interviews were videotaped and replayed later to allow the knowledge engineer closer scrutiny of the responses and adequate time for formulation of additional inquiries. The domain expert and knowledge engineer reviewed sample applicant data to determine key fields and values in those fields which triggered thought processes in the domain expert. By using this approach, the programming of the modular system was simplified, as the software also uses key fields and values which interact with rules for treatment of possible values and provides classification of these values. This also aided the domain expert in understanding how his reactions would be utilized and assisted him in his responses regarding his thought processes.After the initial prototyping and knowledge and rule base building, the system began its refinement stage. During this period the system ran identical data with the domain expert, for comparison with the conclusions which were reached. Discrepancies between the system and the domain expert identified shortcomings and areas for refinement. RuleMaster again proved beneficial in that it provides reasoning for the conclusions reached, which could then be more closely examined by the domain expert and knowledge engineer for modification. Another programming asset of RuleMaster is its ability to induce rules from examples and their treatment which are entered in a simple table by the knowledge engineer. These tables can be easily modified and all changes are immediately reflected in RuleMaster. The sponsoring insurance company also requested that the conclusions include a conclusion strength factor (in this case numeric) which would be highly recognizable to the user as an additional supporting measurement of the conclusion reached. The sponsor will soon begin to run the system parallel with its entire underwriting staff for further analysis and as the first step toward implementation.},
booktitle = {Proceedings of the Conference on The 1987 ACM SIGBDP-SIGCPR Conference},
pages = {129–139},
numpages = {11},
location = {Coral Gables, Florida, USA},
series = {SIGCPR '87}
}

@inproceedings{10.1145/2600176.2600203,
author = {Durbeck, Lisa J. K. and Athanas, Peter M. and Macias, Nicholas J.},
title = {Secure-by-Construction Composable Componentry for Network Processing},
year = {2014},
isbn = {9781450329071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600176.2600203},
doi = {10.1145/2600176.2600203},
abstract = {Techniques commonly used for analyzing streaming video, audio, SIGINT, and network transmissions, at less-than-streaming rates, such as data decimation and ad-hoc sampling, can miss underlying structure, trends and specific events held in the data[3]. This work presents a secure-by-construction approach [7] for the upper-end data streams with rates from 10- to 100 Gigabits per second. The secure-by-construction approach strives to produce system security through the composition of individually secure hardware and software components. The proposed network processor can be used not only at data centers but also within networks and onboard embedded systems at the network periphery for a wide range of tasks, including preprocessing and data cleansing, signal encoding and compression, complex event processing, flow analysis, and other tasks related to collecting and analyzing streaming data. Our design employs a four-layer scalable hardware/software stack that can lead to inherently secure, easily constructed specialized high-speed stream processing.This work addresses the following contemporary problems:(1) There is a lack of hardware/software systems providing stream processing and data stream analysis operating at the target data rates; for high-rate streams the implementation options are limited: all-software solutions can't attain the target rates[1]. GPUs and GPGPUs are also infeasible: they were not designed for I/O at 10-100Gbps; they also have asymmetric resources for input and output and thus cannot be pipelined[4, 2], whereas custom chip-based solutions are costly and inflexible to changes, and FPGA-based solutions are historically hard to program[6];(2) There is a distinct advantage to utilizing high-bandwidth or line-speed analytics to reduce time-to-discovery of information, particularly ones that can be pipelined together to conduct a series of processing tasks or data tests without impeding data rates;(3) There is potentially significant network infrastructure cost savings possible from compact and power-efficient analytic support deployed at the network periphery on the data source or one hop away;(4) There is a need for agile deployment in response to changing objectives;(5) There is an opportunity to constrain designs to use only secure components to achieve their specific objectives. We address these five problems in our stream processor design to provide secure, easily specified processing for low-latency, low-power 10-100Gbps in-line processing on top of a commodity high-end FPGA-based hardware accelerator network processor. With a standard interface a user can snap together various filter blocks, like Legos™, to form a custom processing chain.The overall design is a four-layer solution in which the structurally lowest layer provides the vast computational power to process line-speed streaming packets, and the uppermost layer provides the agility to easily shape the system to the properties of a given application. Current work has focused on design of the two lowest layers, highlighted in the design detail in Figure 1.The two layers shown in Figure 1 are the embeddable portion of the design; these layers, operating at up to 100Gbps, capture both the low- and high frequency components of a signal or stream, analyze them directly, and pass the lower frequency components, residues to the all-software upper layers, Layers 3 and 4; they also optionally supply the data-reduced output up to Layers 3 and 4 for additional processing.Layer 1 is analogous to a systolic array of processors on which simple low-level functions or actions are chained in series[5]. Examples of tasks accomplished at the lowest layer are: (a) check to see if Field 3 of the packet is greater than 5, or (b) count the number of X.75 packets, or (c) select individual fields from data packets. Layer 1 provides the lowest latency, highest throughput processing, analysis and data reduction, formulating raw facts from the stream;Layer 2, also accelerated in hardware and running at full network line rate, combines selected facts from Layer 1, forming a first level of information kernels. Layer 2 is comprised of a number of combiners intended to integrate facts extracted from Layer 1 for presentation to Layer 3. Still resident in FPGA hardware and hardware-accelerated, a Layer 2 combiner is comprised of state logic and soft-core microprocessors.Layer 3 runs in software on a host machine, and is essentially the bridge to the embeddable hardware; this layer exposes an API for the consumption of information kernels to create events and manage state. The generated events and state are also made available to an additional software Layer 4, supplying an interface to traditional software-based systems.As shown in the design detail, network data transitions systolically through Layer 1, through a series of light-weight processing filters that extract and/or modify packet contents. All filters have a similar interface: streams enter from the left, exit the right, and relevant facts are passed upward to Layer 2. The output of the end of the chain in Layer 1 shown in the Figure 1 can be (a) left unconnected (for purely monitoring activities), (b) redirected into the network (for bent pipe operations), or (c) passed to another identical processor, for extended processing on a given stream (scalability).},
booktitle = {Proceedings of the 2014 Symposium and Bootcamp on the Science of Security},
articleno = {27},
numpages = {2},
keywords = {100 Gbps, hardware-software co-design, embedded hardware, secure-by-construction, stream processing, network processor, line-speed processor},
location = {Raleigh, North Carolina, USA},
series = {HotSoS '14}
}

@inproceedings{10.1145/1181775.1181808,
author = {Lo, David and Khoo, Siau-Cheng},
title = {SMArTIC: Towards Building an Accurate, Robust and Scalable Specification Miner},
year = {2006},
isbn = {1595934685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1181775.1181808},
doi = {10.1145/1181775.1181808},
abstract = {Improper management of software evolution, compounded by imprecise, and changing requirements, along with the "short time to market" requirement, commonly leads to a lack of up-to-date specifications. This can result in software that is characterized by bugs, anomalies and even security threats. Software specification mining is a new technique to address this concern by inferring specifications automatically. In this paper, we propose a novel API specification mining architecture called SMArTIC Specification Mining Architecture with Trace fIltering and Clustering) to improve the accuracy, robustness and scalability of specification miners. This architecture is constructed based on two hypotheses: (1) Erroneous traces should be pruned from the input traces to a miner, and (2) Clustering related traces will localize inaccuracies and reduce over-generalizationin learning. Correspondingly, SMArTIC comprises four components: an erroneous-trace filtering block, a related-trace clustering block, a learner, and a merger. We show through experiments that the quality of specification mining can be significantly improved using SMArTIC.},
booktitle = {Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {265–275},
numpages = {11},
keywords = {specification mining, clustering traces, filtering errors},
location = {Portland, Oregon, USA},
series = {SIGSOFT '06/FSE-14}
}

@inproceedings{10.1145/3190508.3190510,
author = {Kashyap, Sanidhya and Min, Changwoo and Kim, Kangnyeon and Kim, Taesoo},
title = {A Scalable Ordering Primitive for Multicore Machines},
year = {2018},
isbn = {9781450355841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190508.3190510},
doi = {10.1145/3190508.3190510},
abstract = {Timestamping is an essential building block for designing concurrency control mechanisms and concurrent data structures. Various algorithms either employ physical timestamping, assuming that they have access to synchronized clocks, or maintain a logical clock with the help of atomic instructions. Unfortunately, these approaches have two problems. First, hardware developers do not guarantee that the available hardware clocks are exactly synchronized, which they find difficult to achieve in practice. Second, the atomic instructions are a deterrent to scalability resulting from cache-line contention. This paper addresses these problems by proposing and designing a scalable ordering primitive, called Ordo, that relies on invariant hardware clocks. Ordo not only enables the correct use of these clocks, by providing a notion of a global hardware clock, but also frees various logical timestamp-based algorithms from the burden of the software logical clock, while trying to simplify their design. We use the Ordo primitive to redesign 1) a concurrent data structure library that we apply on the Linux kernel; 2) a synchronization mechanism for concurrent programming; 3) two database concurrency control mechanisms; and 4) a clock-based software transactional memory algorithm. Our evaluation shows that there is a possibility that the clocks are not synchronized on two architectures (Intel and ARM) and that Ordo generally improves the efficiency of several algorithms by 1.2--39.7X on various architectures.},
booktitle = {Proceedings of the Thirteenth EuroSys Conference},
articleno = {34},
numpages = {15},
location = {Porto, Portugal},
series = {EuroSys '18}
}

@inproceedings{10.1145/3292500.3330699,
author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
title = {Pythia: AI-Assisted Code Completion System},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330699},
doi = {10.1145/3292500.3330699},
abstract = {In this paper, we propose a novel end-to-end approach for AI-assisted code completion called Pythia. It generates ranked lists of method and API recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code IDE. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 ms. We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices. The offline evaluation results obtained on 2700 Python open source software GitHub repositories show a top-5 accuracy of 92%, surpassing the baseline models by 20% averaged over classes, for both intra and cross-project settings.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2727–2735},
numpages = {9},
keywords = {code completion, naturalness of software, neural networks},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/317500.317529,
author = {Musser, David R. and Stepanov, Alexander A.},
title = {A Library of Generic Algorithms in Ada},
year = {1987},
isbn = {0897912438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/317500.317529},
doi = {10.1145/317500.317529},
abstract = {It is well-known that data abstractions are crucial to good software engineering practice. We argue that algorithmic abstractions, or generic algorithms, are perhaps even more important for software reusability. Generic algorithms are parameterized procedural schemata that are completely independent of the underlying data representation and are derived from concrete, efficient algorithms. We discuss this notion with illustrations from the structure of an Ada library of reusable software components we are presently developing.},
booktitle = {Proceedings of the 1987 Annual ACM SIGAda International Conference on Ada},
pages = {216–225},
numpages = {10},
location = {Boston, Massachusetts, USA},
series = {SIGAda '87}
}

@article{10.5555/2184451.2184454,
author = {Proulx, Viera K.},
title = {Software Testing (in Java) from the Beginning},
year = {2012},
issue_date = {June 2012},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {27},
number = {6},
issn = {1937-4771},
abstract = {Software testing is usually delegated to upper level courses when students work on larger projects and practice design skills. We believe that software testing needs to be an integral part of learning how to design programs from the very beginning. The goal of this workshop is to show how with the proper software support students can practice test-first design from the beginning, and show that this approach helps students to develop good program design habits early. We then show how additional techniques can be used to create a more comprehensive validation and verification of programming projects and help in identifying design flaws in student projects.In the first half of the workshop we will introduce unit testing for the beginners, both in the functional style and in the imperative style, exploring the notion of equality, and examining special cases of unit testing (checking the correctness of constructors, methods that throw exceptions, tests for inexact values, for a range of values, or one-of values, tests that compare structured data sets. We will introduce our tester library that makes test design, evaluation, and reporting accessible to the programmer at his/her current level of knowledge of programming. We will show the syntax for the tests and describe the pedagogy we use to enforce test-first design from the very first program students write. The tester library is freely available on our web site with extensive documentation that shows how to use it with Eclipse, NetBeans, BlueJ, or just command-line Java compilation.In the second half of the workshop we will show how to design randomized tests, stress tests, black box tests and integration tests. We will show how the use of tools that measure code metrics can help students and instructors identify design flaws in student projects that make the design of tests difficult or impossible. We have used these tools in our courses, especially in code reviews and in evaluation of student projects with promising results.},
journal = {J. Comput. Sci. Coll.},
month = {jun},
pages = {7–9},
numpages = {3}
}

@article{10.1145/1041853.1041854,
author = {Kreutzer, Wolfgang},
title = {Computer System Modelling and Simulation},
year = {1979},
issue_date = {Spring-Summer 1979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1–2},
issn = {0163-5999},
url = {https://doi.org/10.1145/1041853.1041854},
doi = {10.1145/1041853.1041854},
abstract = {To evaluate the suitability and limitations of software for computer systems modelling, a basic comprehension of the structure of such tools must be provided. A brief discussion of conceptual requirements for the description of discrete models, and computer system models in particular, is followed by a survey of commercially available computer simulation packages. Special and general purpose discrete event simulation and general purpose programming languages are also analysed for their suitability for this class of applications. The survey closes with some recommendations and guidelines for selection and application of computer system simulation tools.To aid the analyst contemplating a computer system modelling project, a brief list of relevant addresses and annotated references is also included.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {apr},
pages = {9–35},
numpages = {27}
}

@inproceedings{10.5555/3234847.3234906,
author = {Quinlan, Jason},
title = {SDaN: Software Defined Adaptive Networking - IoT and Beyond},
year = {2018},
isbn = {9780994988621},
publisher = {Junction Publishing},
address = {USA},
abstract = {In this position paper, I present my vision on how enhancing the interaction between devices in an Internet of Things (IoT) network and the mechanisms of Software Defined Networking (SDN) can not only improve the efficiency of wirelessly transmitted data but can enhance the viability and manageability of devices in an IoT network and beyond. By expanding the composition of SDN to encompass not only the existing North (controller API), South (device API), East and West interfaces (distributed controller(s) API) - viewed as an X-/Y-axis, but also a holistic adaptive Z-axis which mandates a higher layer abstraction of the root APIs irrespective of the underlying architecture. This adaptive Z-axis improves the global view by offering an abstracted view of the network control and management for all objects in the network. In this proposal all interfaces, irrespective of axis, can be viewed as a single abstraction, by which "inter-operative function calls" can be leveraged adaptively between the device(s) and the network(s). By leveraging these additional constructs, I believe that SDN can improve the capabilities and efficiency of IoT networks.},
booktitle = {Proceedings of the 2018 International Conference on Embedded Wireless Systems and Networks},
pages = {250–251},
numpages = {2},
keywords = {SDN, SDaN, Software Defined adaptive Networking, IoT},
location = {Madrid, Spain},
series = {EWSN ’18}
}

@inproceedings{10.1145/2048147.2048220,
author = {Basman, Antranig M. and Lewis, Clayton H. and Clark, Colin B.D.},
title = {To Inclusive Design through Contextually Extended IoC: Infusion IoC, a JavaScript Library and Mentality for Scalable Development of Accessible and Maintainable Systems},
year = {2011},
isbn = {9781450309424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048147.2048220},
doi = {10.1145/2048147.2048220},
abstract = {Using current software development techniques, code and designs are often unmaintainable from the point of inception. Code is brittle and hard to refactor, hard to press to new purposes, and hard to understand. Here we present a system aimed at creating a model for scalable development, addressing this and several other critical problems in software construction. Such an aim is far from new, and has resembled the aims of each generation of software methodologists over the last 50 years. It deserves comment why these aims have so signally failed to be achieved, and we will present arguments as to why the combination of techniques explained here could expect to lead to novel results.Software products of today are notoriously unadaptable. An application which meets need A generally cannot be extended to meet apparently very similar need A' without something resembling "software engineering". Applications present users with a "take it or leave it" proposition - if the software doesn't happen to meet a user's needs or preferences, there's no way to change it without writing more code, which is out of reach for most users. Indeed, software regularly fails to be easily adaptable to meet the needs of users with differing needs, such as in the case of accessibility. These "precarious values" - accessibility and usability with different devices, languages, and personal needs - are typically left until the end or ignored, and represent a significant expense in traditional approaches to software development. Often these needs are met by developing a largely unrelated version of the application, requiring maintenance of additional, separate code bases.Our aim is to enable Inclusive Design [3], whose objective is to satisfy the needs and desires of the broadest range of users possible. Every designer sets out with this objective to a certain extent, but as well as limitations of intent, there are also strong limitations placed by the technology and economics of software development. Due to the poor scaling characteristics of current techniques, even meeting one set of relatively inflexible needs can be an expensive undertaking, especially over the long term.To address these problems of adaptability, we present a model for software construction, together with a base library, Fluid Infusion, implemented in the JavaScript language. Fluid Infusion implements an Inversion of Control model, Infusion IoC, which features a notion of context as the basis for adaptability, resolved in a scope modelled in terms of a data structure, a component tree expressing the computation to be performed. In the Context-Oriented Programming community [7], this model of scoping is known as structural scoping. We will also work with a model of transparent state in which all modifiable state of interest to users is held in publicly visible locations, indexed by path strings. This model for state is isomorphic to that modeled by JSON [6], a well-known state model derived from, but not limited to, the JavaScript language. Instantiation in the model is handled by an Inversion of Control system extended from the model of similar system such as the Spring Framework or Pico first developed in the Java language.We relate such systems to goal-directed resolution systems such as Prolog, and show that they have beneficial properties such as homoiconicity [2] which have not been seen in a strong or widespread form since the days of LISP. We exhibit some cases to show how the framework enables, through a simple declarative syntax, types of adaptation and composition that are hard or impossible using traditional models of polymorphism. We also relate Infusion IoC to other software methodologies such as Aspect-Oriented Programming and Context-Oriented Programming which have been found to greatly increase flexibility and expressiveness of designs. We conclude with some remarks on the applicability of the system to the parallelisation of irregular algorithms, and its relationship to upcoming developments in the ECMAScript 6 language specification.},
booktitle = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
pages = {237–256},
numpages = {20},
keywords = {json, transparent state, context-oriented programming, accessibility, javascript, inversion of control},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@article{10.1007/s00165-016-0355-5,
author = {Cassel, Sofia and Howar, Falk and Jonsson, Bengt and Steffen, Bernhard},
title = {Active Learning for Extended Finite State Machines},
year = {2016},
issue_date = {Apr 2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {2},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-016-0355-5},
doi = {10.1007/s00165-016-0355-5},
abstract = {We present a black-box active learning algorithm for inferring extended finite state machines (EFSM)s by dynamic black-box analysis. EFSMs can be used to model both data flow and control behavior of software and hardware components. Different dialects of EFSMs are widely used in tools for model-based software development, verification, and testing. Our algorithm infers a class of EFSMs called register automata. Register automata have a finite control structure, extended with variables (registers), assignments, and guards. Our algorithm is parameterized on a particular theory, i.e., a set of operations and tests on the data domain that can be used in guards.Key to our learning technique is a novel learning model based on so-called tree queries. The learning algorithm uses tree queries to infer symbolic data constraints on parameters, e.g., sequence numbers, time stamps, identifiers, or even simple arithmetic. We describe sufficient conditions for the properties that the symbolic constraints provided by a tree query in general must have to be usable in our learning model. We also show that, under these conditions, our framework induces a generalization of the classical Nerode equivalence and canonical automata construction to the symbolic setting. We have evaluated our algorithm in a black-box scenario, where tree queries are realized through (black-box) testing. Our case studies include connection establishment in TCP and a priority queue from the Java Class Library.},
journal = {Form. Asp. Comput.},
month = {apr},
pages = {233–263},
numpages = {31},
keywords = {Data Language, Automaton Learning, Membership Query, Symbolic Execution, Priority Queue}
}

@inproceedings{10.1145/1134285.1134404,
author = {Bishop, Judith},
title = {Multi-Platform User Interface Construction: A Challenge for Software Engineering-in-the-Small},
year = {2006},
isbn = {1595933751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134285.1134404},
doi = {10.1145/1134285.1134404},
abstract = {The popular view of software engineering focuses on managing teams of people to produce large systems. This paper addresses a different angle of software engineering, that of development for re-use and portability. We consider how an essential part of most software products - the user interface - can be successfully engineered so that it can be portable across multiple platforms and on multiple devices. Our research has identified the structure of the problem domain, and we have filled in some of the answers. We investigate promising solutions from the model-driven frameworks of the 1990s, to modern XML-based specification notations (Views, XUL, XIML, XAML), multi-platform toolkits (Qt and Gtk), and our new work, Mirrors which pioneers reflective libraries. The methodology on which Views and Mirrors is based enables existing GUI libraries to be transported to new operating systems. The paper also identifies cross-cutting challenges related to education, standardization and the impact of mobile and tangible devices on the future design of UIs. This paper seeks to position user interface construction as an important challenge in software engineering, worthy of ongoing research.},
booktitle = {Proceedings of the 28th International Conference on Software Engineering},
pages = {751–760},
numpages = {10},
keywords = {portability, platform independence, mirrors, XAML, views, graphical user interfaces, mobile devices, .NET, reflection, GUI library reuse, XUL, tangible user interfaces},
location = {Shanghai, China},
series = {ICSE '06}
}

@inproceedings{10.1145/2949689.2949693,
author = {\v{S}ik\v{s}nys, Laurynas and Pedersen, Torben Bach},
title = {SolveDB: Integrating Optimization Problem Solvers Into SQL Databases},
year = {2016},
isbn = {9781450342155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2949689.2949693},
doi = {10.1145/2949689.2949693},
abstract = {Many real-world decision problems involve solving optimization problems based on data in an SQL database. Traditionally, solving such problems requires combining a DBMS with optimization software packages for each required class of problems (e.g. linear and constraint programming) -- leading to workflows that are cumbersome, complex, inefficient, and error-prone. In this paper, we present SolveDB - a DBMS for optimization applications. SolveDB supports solvers for different problem classes and offers seamless data management and optimization problem solving in a pure SQL-based setting. This allows for much simpler and more effective solutions of database-based optimization problems. SolveDB is based on the 3-level ANSI/SPARC architecture and allows formulating, solving, and analysing solutions of optimization problems using a single so-called solve query. SolveDB provides (1) an SQL-based syntax for optimization problems, (2) an extensible infrastructure for integrating different solvers, and (3) query optimization techniques to achieve the best execution performance and/or result quality. Extensive experiments with the PostgreSQL-based implementation show that SolveDB is a versatile tool offering much higher developer productivity and order of magnitude better performance for specification-complex and data-intensive problems.},
booktitle = {Proceedings of the 28th International Conference on Scientific and Statistical Database Management},
articleno = {14},
numpages = {12},
location = {Budapest, Hungary},
series = {SSDBM '16}
}

@inproceedings{10.1145/952532.952663,
author = {Coffland, Joseph E. and Pimentel, Andy D.},
title = {A Software Framework for Efficient System-Level Performance Evaluation of Embedded Systems},
year = {2003},
isbn = {1581136242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/952532.952663},
doi = {10.1145/952532.952663},
abstract = {The Sesame environment provides modeling and simulation methods and tools for the efficient design space exploration of heterogeneous embedded multimedia systems. In this paper we describe the Sesame software system and demonstrate its capabilities using several examples. We show that Sesame significantly reduces model construction time through the use of modeling component libraries, hierarchy, and advanced model structure description features.},
booktitle = {Proceedings of the 2003 ACM Symposium on Applied Computing},
pages = {666–671},
numpages = {6},
keywords = {embedded systems, performance evaluation, co-simulation},
location = {Melbourne, Florida},
series = {SAC '03}
}

@article{10.1145/202235.202238,
author = {MacNair, E. A. and Gordon, R. F.},
title = {An Introduction to the RESearch Queueing Package for Modeling Contention Systems},
year = {1994},
issue_date = {Winter 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0163-6103},
url = {https://doi.org/10.1145/202235.202238},
doi = {10.1145/202235.202238},
abstract = {A queueing network is an important tool for modeling systems where performance is principally affected by contention for resources. Such systems include computer systems, communication networks and manufacturing lines. In order to effectively use queuing networks as performance models, appropriate software is necessary for definition of the networks to be solved, for solution of the networks and for examination of the performance measures obtained.The RESearch Queueing Package (RESQ) and the RESearch Queueing Modeling Environment (RESQME) form a system for constructing, solving and analyzing extended queueing network models. We refer to the class of RESQ networks as "extended" because of characteristics which allow effective representation of system detail. RESQ incorporates a high level language to concisely describe the structure of the model and to specify constraints on the solution. A main feature of the language is the capability to describe models in a hierarchical fashion, allowing an analyst to define submodels to be used analogously to use of macros in programming languages. RESQ also provides a variety of methods for estimating the accuracy of simulation results and for determining simulation run lengths. RESQME is a graphical interface for RESQ.},
journal = {SIGSIM Simul. Dig.},
month = {dec},
pages = {40–70},
numpages = {31}
}

@inproceedings{10.1145/1597990.1597995,
author = {Museth, Ken},
title = {An Efficient Level Set Toolkit for Visual Effects},
year = {2009},
isbn = {9781605588346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1597990.1597995},
doi = {10.1145/1597990.1597995},
abstract = {Over the past couple of years we have developed various tools for visual effects in movie production based on level sets. While most VFX studios, and even some commercial software packages, have had level sets tools for several years we are, to the best of our knowledge, among the first to employ highly optimized data structures. These compact and very fast volumetric data structures allow us to work at unprecedented grid resolutions, which in turn opens the door to new applications. The last two years we have demonstrated this with level set tools specifically developed for fluid surfacing in "Pirates of the Caribbean: At World's End" [Museth et al. 2007] and fragmentation in "The Mummy: Tomb Of The Dragon Emperor" [Museth and Clive 2008], However, none of these past presentations actually discussed the details of the novel data structure, DB-Grid, which forms the very backbone of our level set implementations. The same was true for many of our fundamental level set tools, like the fast and robust scan-converter of self-intersecting polygonal meshes. These are exactly the topics of the current presentation. We will also show how these tools are tightly integrated with Houdini.},
booktitle = {SIGGRAPH 2009: Talks},
articleno = {5},
numpages = {1},
location = {New Orleans, Louisiana},
series = {SIGGRAPH '09}
}

@inproceedings{10.1145/3510454.3516826,
author = {Liang, Yunkai and Lin, Yun and Song, Xuezhi and Sun, Jun and Feng, Zhiyong and Dong, Jin Song},
title = {GDefects4DL: A Dataset of General Real-World Deep Learning Program Defects},
year = {2022},
isbn = {9781450392235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510454.3516826},
doi = {10.1145/3510454.3516826},
abstract = {The development of deep learning programs, as a new programming paradigm, is observed to suffer from various defects. Emerging research works have been proposed to detect, debug, and repair deep learning bugs, which drive the need to construct the bug benchmarks. In this work, we present gDefects4DL, a dataset for general bugs of deep learning programs. Comparing to existing datasets, gDefects4DL collects bugs where the root causes and fix solutions can be well generalized to other projects. Our general bugs include deep learning program bugs such as (1) violation of deep learning API usage pattern (e.g., the standard to implement cross entropy function y · log(y), y → 0, without NaN error), (2) shape-mismatch of tensor calculation, (3) numeric bugs, (4) type-mismatch (e.g., confusing similar types among numpy, pytorch, and tensorflow), (5) violation of model architecture design convention, and (6) performance bugs.For each bug in gDefects4DL, we describe why it is general and group the bugs with similar root causes and fix solutions for reference. Moreover, gDefects4DL also maintains (1) its buggy/fixed versions and the isolated fix change, (2) an isolated environment to replicate the defect, and (3) the whole code evolution history from the buggy version to the fixed version. We design gDefects4DL with extensible interfaces to evaluate software engineering methodologies and tools. We have integrated tools such as ShapeFlow, DEBAR, and GRIST. gDefects4DL contains 64 bugs falling into 6 categories (i.e., API Misuse, Shape Mismatch, Number Error, Type Mismatch, Violation of Architecture Convention, and Performance Bug). gDefects4DL is available at https://github.com/llmhyy/defects4dl, its online web demonstration is at http://47.93.14.147:9000/bugList, and the demo video is at https://youtu.be/0XtaEt4Fhm4.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
pages = {90–94},
numpages = {5},
keywords = {datasets, bugs, neural networks, defects, deep learning},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.5555/55823.55866,
author = {Pittman, C. W.},
title = {The Space Station Information System and Software Support Environment},
year = {1988},
isbn = {0897912586},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {The Space Station will be a large, permanent, multi-purpose facility with comfortable living quarters for a crew of up to eight people. It will consist of four habitable modules and a truss to which payloads which do not require human involvement can be attached. It is intended to accommodate a wide variety of science and technology projects (astronomical and geophysical observations, materials and life sciences, etc.) and, as it evolves, to serve as an assembly, servicing, and departure point for other space missions. The truss, utility services, and two of the modules will be provided by NASA, one module will be provided by the National Space Development Agency of Japan, and the other by the European Space Agency. The entire manned base will be carried into orbit and assembled in nineteen increments over a period of two years around 1995.A large part of the value of the Space Station will reside in the information exchanged between it and the related ground facilities. to ensure that this exchange is effective, the Space Station Information System (SSIS) has been identified as an explicit element of the program and a management structure has been put in place to oversee the development of the end-to-end system, including those elements not provided by NASA. This will involve identifying all the components of the SSIS and the people responsible for each, comparing their schedules, establishing interface and protocol agreements, and providing for testing the overall system. The SSIS management intention is that the non-NASA participants, including the international partners, will have complete freedom in the design of their hardware and software, providing that they satisfy the agreed upon interfaces.The SSIS is an extensive collection of hardware (computers, networks, facilities) and software, whose primary purpose is to carry data between a spaced-based source and a ground-based user. The data source could be a scientific instrument on the manned base or on a platform; it could be a piece of on-board equipment; or even a crew member. The ground-based user could be an experimenter operating from his home institution or an operator based in a spacecraft control facility. The data itself might be scientific data; housekeeping data used to monitor equipment health and safety; a database query; or audio and video data forming part of a space-ground teleconference. While the situation is not completely symmetrical, most of these data-flows will also occur, simultaneously, from ground to space.The collection of hardware elements involved in this process is large and varied, consisting of more than ninety space and ground elements which combine to form the SSIS. Some of these elements are new and unique the SSP while others (such as the Tracking Data Relay Satellite System (TDRSS)) are extended versions of existing NASA institutional capabilities. some elements are outside of SSP (and even NASA) control, such as customer facilities and public communications networks. However, being connected to the SSIS and using its services, they can be considered as SSIS elements.In the past, space mission data systems tended to be rather inflexible, schedule-driven systems which were labor-intensive to operate. Typically, users' data would de delivered several days (or longer) after reception, frequently in formats unique to NASA. The SSIS concept involves some significant improvements: The trend is away from schedule-driven and towards data-driven systems - within certain prearranged limits, the system will autonomously accommodate changes in data rates and data destinations. This result in operations which are more flexible, less labor-intensive, and give better service to usersA necessary building block in the construction of data-driven system is the autonomous data unit - instead of appearing to the data system as anonymous bit streams, basic data units will be packetized and self-identifying. This allows many of the data system functions, such as routing data to users and removing data transport artifacts, to be entirely automated and generic in nature.Packetized data can be further packed into 'virtual channels' so that many logically distinct data streams can be multiplexed into the same physical medium (for example, the space-ground link via TDRS).In addition to the program benefits of these techniques, the adoption of packetized, data-driven methods has equal benefit to the user community in terms of increased operational flexibility and reduced software development effort.Another way in which the SSIS differs from previous data systems is in the concepts of tele-operations and tele-analysis. The tele-operations concept is often described as the ability for a customer to interact with his payload as if that payload were in a laboratory 'next door'. The realization of this concept imposes some stringent requirements on the intervening data systems: They must provide constant, standard interfaces so that the payload/user interface looks the same regardless of whether the payload is on the testbench in the manufacturer's facility, in an integration facility at the launch site or in orbit attached to the station or a platform.They must provide data transparency - transporting data with minimum interference, regardless of the data content.They must transport data with minimum delay so that he investigator can interactively change the course of an observation based on current informationThey must provide all the above regardless of the customer's physical location: at his home institution, at a NASA facility, or at an international location.Tele-analysis extends the concept to the ground processing of payload (and other) data. It includes the ability to locate useful data in distributed databases and archives, to extract and receive sub-sets regardless of the physical location of the data, and finally, to combine and reprocess datasets (possibly from different investigations) to produce new datasets of increased utility.Similarly, the software for the SSIS will be large and complex, developed by many groups of people at many locations, and expected to operate safely and undergo extensive evolution over the life of the Space Station. The Software Support Environment (SSE) is being developed to control the development and maintenance of the SSIS software. Its purpose is to minimize the cost of the software over its life cycle while ensuring a high degree of safety and reliability. It embodies the “tools and rules” i.e., the computer programs, procedures, and standards for the management, development, configuration control, integration, test and verification, and maintenance of the SSIS software. Virtually all software developed for the Space Station will use the SSE.The SSE will reside in numerous facilities: the Software Support Environment Development Facility (SSEDF); a number of Software Production Facilities (SPF's); and the Multi System Integration Facility (MSIF). As its name implies, the SSEDF, located in Houston, is where the SSE will be developed. The SPF's will be at the locations where SSIS software is being produced. A tailored subset of the total SSE will be assembled at the SSEDF and installed at each SPF. The MSIF is the location at which the software elements produced and tested at the SPF's will be integrated and tested. It will contain sufficient simulations/emulations and/or prototypes of the Space Station Systems to allow the software to be tested in a realistic environment.The SSE will control all phases of the software process, from development project initiation to operation and maintenance. When a software development project is initiated, the Project Manager will interact with the SSE to identify the project, the organization which will execute it, and the lines of authority within the organization. The SSE will subsequently report status and problems appropriately. He will also identify the task within the project and the personnel assigned to the task and the SSE will then control the access and activities of the personnel, i.e., tester can not alter code, designers can not perform formal verification testing, etc. The SSE will not permit the project to proceed until these task/personnel relationships are defined. Also during the project initiation, the SSE will assist the project manager to tailor the software life cycle model to fit the particular project and provide him with templates of the required documentation.The SSE provides an extensive set of tools for the design, development, and integration of the software. For design and development, these include tools for requirements and traceability analyses, preliminary design, coding and documentation standards compliance, code analysis, interactive de-bugging, etc. For test and verification, the SSE will have tools to evaluate requirements and design adherence, verify test completeness and comprehensiveness, configure the system for the test, perform the test and record the results, identify and records deficiencies. Throughout the process, the SSE will use a DBMS to store and retrieve all the data objects produced by the project and it will support configuration management by retaining the currently approved versions of all controlled products.The SSE software can be thought of as a collection of functional elements distributed in an architectural framework. The functional elements are: process management; software management support; software production; modeling, simulation, and software checkout; integration, test, and verification; data reconfiguration; and training. These functional elements are distributed among four architectural elements which are called the Framework; Tools; Host system Software; and SSE Management. the Framework provides the functionality and control of the SSE. Users access the life-cycle products, tools, test, and data through the Framework. The Tools element provides what its name implies. The Host System Software contains functions typical of host operating systems and utilities. These include executive, data base management, host configuration control, and communications. The SSE management element provides non-development management functions such as incorporation of non-SSE inputs, system performance monitoring, system management and resource analyses.In addition, the SSE software includes a Human/Computer Interface; a Test and Tools Harness, which provides the interface between the Framework and the Tools; and a Distributed Ada Interface Set, a standard layer of interface services that bridges between the logical and physical views of the system.Physically, each SPF will consist of one or more host computers, a collection of work stations, a local area network, and access to wide area networks. It will be designed to be hardware-independent and to accommodate technology upgrades transparently. Initially, the host computers will be from the VAX 8000 series and the IBM 3090 series, and the work stations will be Apple Macintosh II, IBM PC/AT compatible, and Apollo Series 3000. The local network will be an Ethernet-based TCP/IP LAN. In addition, there will be a specially designed Simulation Interface Buffer to provide an interface between the SPF and local simulations/emulations/prototypes of other Space Station systems.},
booktitle = {Proceedings of the 10th International Conference on Software Engineering},
pages = {455–458},
numpages = {4},
location = {Singapore},
series = {ICSE '88}
}

@article{10.1145/1278901.1278904,
author = {Heiser, Gernot and Elphinstone, Kevin and Kuz, Ihor and Klein, Gerwin and Petters, Stefan M.},
title = {Towards Trustworthy Computing Systems: Taking Microkernels to the next Level},
year = {2007},
issue_date = {July 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1278901.1278904},
doi = {10.1145/1278901.1278904},
abstract = {As computer systems become increasingly mission-critical, used in life-critical situations, and relied upon to protect intellectual property, operating-system reliability is becoming an ever growing concern. In the past, mission- and life-critical embedded systems consisted of simple microcontrollers running a small amount of software that could be validated using traditional and informal techniques. However, with the growth of software complexity, traditional techniques for ensuring software reliability have not been able to keep up, leading to an overall degradation of reliability. This paper argues that microkernels are the best approach for delivering truly trustworthy computer systems in the foreseeable future. It presents the NICTA operating-systems research vision, centred around the L4 microkernel and based on four core projects. The seL4 project is designing an improved API for a secure microkernel, L4, verified will produce a full formal verification of the microkernel, Potoroo combines execution-time measurements with static analysis to determine the worst case execution profiles of the kernel, and CAmkES provides a component architecture for building systems that use the microkernel. Through close collaboration with Open Kernel Labs (a NICTA spinoff) the research output of these projects will make its way into products over the next few years.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jul},
pages = {3–11},
numpages = {9}
}

@inproceedings{10.1145/41526.41527,
author = {Lindberg, D. A. B.},
title = {In Praise of Computing},
year = {1987},
isbn = {0897912489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/41526.41527},
doi = {10.1145/41526.41527},
abstract = {I shall describe briefly some early work in applying computing systems to clinical laboratory, diagnosis, and medical records problems. If we compare the early work with what can be done now, we shall see many examples of enormous improvements in speed and power of computing techniques. A particularly striking difference in comparing 1960 with 1987 is the very much more widespread use of computing devices now and the shift in the pattern of scientific and business work to depend upon automated information systems for support of ordinary professional functions. In other words, computing systems really work now, and lots of people really use them.Let me first establish this contrast and share with you the pleasure this day brings to all of us in taking pride in these accomplishments. We should do this first, because I shall end by drawing attention to some desirable changes that have not occurred over the 25 years. The organizer of this conference, Bruce Blum, urged us to include personal remembrances, so I shall do so.My own interest in computing arose grudgingly from two requirements, one scientific and one professional. I was working under NIH support on an infectious disease problem when I moved from Columbia Presbyterian to the University of Missouri in 1960, taking my NIH grant with me. This in itself may add an element of nostalgia, since most institutions no longer permit residents to hold grants and NIH can barely fund one-third of the applications from senior faculty nowadays. In any event, my interest turned to a systematic investigation of the (then and now) poor correlation between in vitro and in vivo antibiotic sensitivity of bacteria that infect lung and blood. I did not believe (and still do not believe) that 24-hour disc-diffusion growth inhibition was a reasonable measure of the bacteria's susceptibility to antibiotic killing or growth inhibition. I reasoned that the battle between bacteria and pulmonary macrophages was probably over in 20 minutes to an hour. Hence I wanted to study bacteria growing in liquid media, with and without antibiotics. To do this I built a machine, along with a sophomore undergraduate physics student.1The observations the machine permitted gave a great deal of insight into this problem, although never the ultimate secret.2 Nevertheless we were able to prescribe therapy based on these experiments that unquestioningly cured patients with subacute bacterial endocarditis and other infectious diseases on whom others had given up. In any event, computers were necessary to process and organize the output data, and to model the processes of bacterial growth. The progression of steps we took in developing our data processing was rather pitiable in modern terms. First we learned how to wire nixie tubes to display the numbers that represented optical measurements of light scatter (hence bacterial growth) in the hundred or so test tubes that cycled past the light source. In the next version of the machine we took these signals to a Victor adding machine printer. We pasted up the tapes in rows, then read down the resulting columns to sum growth over an individual tube. Next we output the whole run to paper tape which we read into a Burroughs 205 computer. This was the beginning of wisdom, but the Burroughs was not a nice computer and paper tape was awkward. Our next step was to re-design the machine to output to a card punch so that we could process on the “modern” IBM 1620 computer. In both cases, the computers resided in “open shop” mode in the basement of the University Mathematics Department. The Computer Center Director was a half-time job held by an assistant professor of mathematics who was never given tenure. We usually processed at midnight because there would be no one to object if we vacuumed out the detritus from the tape reader.All this was quite good fun with one exception: namely, I was simultaneously supposed to be directing and supervising clinical chemistry, clinical microbiology, a section of the sophomore path class, and doing frozen sections. The biggest practical problem was getting correct reports out of the clinical laboratories and to the clinics and wards. The blackest day I can remember was when I was called to deal with the potential problem that Clinical Chemistry might not get out any of today's reports before they reached quitting time. Even blacker was the discovery that they also hadn't reported the previous day's AutoAnalyzer runs.My personal love was Microbiology. This was a well run laboratory that always got the work out. Unfortunately the report slips—which pathologists in those days personally signed—usually contained a great and imaginative assortment of spellings of the bacteriological names. This may not have actually impeded patient care, since the clinical house staff receiving them also did not spell with much precision. It did, however, allow the Professor of Microbiology to hold me and the Pathology Department up to ridicule and contumely. This academic paragon did not, of course, know that the Record Room had at all times hundreds of lab reports on which the patient names did not even match the patient unit numbers. These were not permitted to be placed in the charts, and hence were lost forever. Misspelling was only one of our problems.I made a number of managerial changes to put a temporary “fix” on the reporting problem. The next morning was the brightest day I can remember; it suddenly occurred to me that the computing system I was using for the antibiotic sensitivity experiments could be used to solve my clinical laboratory problem as well. After all, I reasoned, computer programs always spelled things the same way day after day, and the tabulating printer certainly was faster than the folks running the laboratory typewriters. In addition, I could establish limits, so that at least some of the ridiculous errors could never again be reported out and truly life-threatening findings could be identified for telephone reporting. Once one yielded to this line of thinking, it immediately became obvious that the product of the clinical labs (with the exception of the Blood Bank) was purely information: hundreds of thousands of items per year. Tables outside the programs could contain editing and limits values, pointers to other medical record elements, pricing and quality control data, etc.That day was my moment “on the road to Damascus.” I had been seriously troubled by my divided loyalties between pushing ahead in the research laboratory, while simultaneously knowing the clinical laboratory needed attention. I would then spend futile hours correcting spelling and dilutions in the clinical labs while feeling neglectful of the research. That day I realized computers were the answer to the common problem in both settings: namely, information control. From that moment forward I have been professionally happy, and committed to working out the best uses of automated information systems.The first electronic laboratory reporting system at Missouri (and I believe the first anywhere) was—as you can easily imagine—a kludge too.3,4 We could not do much better, since we did not even have a computer in the Medical School. I arranged a system of pre-punched cards, in which each card contained a line of the ultimate lab report. These were arranged in a kind of pigeon roost from which the lab secretaries composed the messages. Using punch cards on the wards on which to write the clinical order to accompany the specimen worked out well, and various envelopes kept things together during the specimen processing. Incidentally, I was astounded a few years later to see that Warner Slack at the University of Wisconsin had devised an almost identical pigeon roost to hold the cards that provided the basis for his patient history system.Getting our laboratory results printed on the wards was a problem, since there were no teleprocessing systems even if we had owned a computer. We solved this by converting the cards to paper tape, and using the paper tape to drive circuit codes into what Teletype Corporation called a “Stunt Box.” This in turn activated the print circuits to teletype Model 28 printers which we installed in linen closets at the ward and clinic locations. The whole business was slow—and incredibly noisy—but it worked. We summarized the decks of cards—at midnight as usual—in the IBM 1620. This reduced each report to an individual punched card with massive use of “over punches” into the x and y fields. Most of the resultant cards had so many illegal codes that they looked like lace doilies. An easy by-product of all this was a billing punched card for each transaction. These were interpreted and sent to the Business Office. I took considerable glee in knowing that no one in the Business Office, nor in their auditors Price, Waterhouse, had the slightest idea what to do with punched cards. Actually they read them and entered the numbers into posting machines.The only down-side effect of the new lab system was a deal I made to finance the operation. Since we were printing directly to the patient locations, I discharged 20 pretty co-eds who had been employed to deliver lab reports. That was the first but not the last time I heard colleagues say I was “destroying life as we knew it.”This old original laboratory reporting system did of course get replaced. We brought in an IBM 1410 for the Medical School and implemented what amounted to a fast turnaround batch system using punched card input and output. The memory on the 1410 was 40,000 characters; our languages were Autocoder, Fortran, and COBOL.In this system the input was written to a patient record as well as a batch print file. Consequently patient unit numbers really had to be correct. We added Modulus Eleven self-check digits to all patient numbers, persuaded the Record Room to rearrange the filing system, and persuaded the hospital administrator that all his Addressograph-Multigraph plate-stamping equipment really needed replacing in any case. Self-check digits were also used for lab reports and specimen numbers. I imagine Missouri was first here too. I have never been able to understand how American hospitals got along without self-check digits, since European hospitals use them with an even better algorithm.The 1410 system was implemented in the laboratories with a new kind of input terminal: what became the IBM 1092 Densely Coded Matrix. This was a joint development between IBM and the University of Missouri. Its original purpose was to enter size, shape, color, and cutting instructions for the garment trade in New York, but it seemed to me ideal for lab reporting. One used plastic overlays to redefine the key designations and microswitches to detect unique notches on the individual overlays.The matrix keyboard got me my NIH automation grant. We had a terrific proposal to use the 1410 to do complete lab automation integrated into hospital patient record keeping. The obvious missing piece was some kind of device to input the data. Since the new IBM matrix device—indeed the whole 1050 system—was unpatented and unannounced, I could not include these descriptions in the grant proposal. I knew this would draw a site visit. At that time Melvin Belli had just pulled off his trial scene trick of leaving the leg-shaped package wrapped in butcher paper on the courtroom desk. I took a leaf from his book and draped the sheet metal prototype of our reporting device with an Operating Room sheet at the head of the Dean's Conference Room. Essentially we were daring the site visitors to ask us to breach our confidentiality agreement. It worked. They started, but they never asked. And they approved the grant request.The 1410 system worked quite well but was replaced in time by a true on-line system once the 360/50 became available. The latter got to be a rather extensive system, including a lot of quality control, cost analysis, interpretation, etc.5 I know some of those programs ran for fully 20 years.In the course of this work the University of Missouri formed the Medical Computer Program, and I became its first director beginning in 1962. The commitment of the School to make a serious investigation of computers was made by the Dean, Vernon Wilson. He encouraged us to test the question, “Could computers contribute something to teaching, practice, and research in medicine?”, and he did this with much encouragement, advice and support. He was a really fine academic leader. I saw the task to be to create an institutionally-based information utility, to establish a multidisciplinary group including medical, engineering, and mathematical skills, and to support and encourage creative ventures in the professional services of the School and the Hospital. Gwil Lodwick in Radiology at Missouri was certainly at the head of this list. It also included computer groups in Pathology, Dietary, the Dean's Office, and subsequently Surgery, then Medicine and Pediatrics. I still believe central information groups should be institutionally-based, preferably as academic departments of their own; certainly not as service units within other departments.Incidentally computer science has not been mentioned so far because there was none. I remember vividly some years hence getting a telephone call from the Vice President for Academic Affairs at the University. He asked, “Is there such a thing as computer science? Someone wants to make a new department.” Our group had spent a lot of time convincing the Personnel people to believe in job descriptions and career ladders for computer programmers, system analysts, system programmers, technical writers, etc., so this was not so much of a leap of faith for us as for the Graduate School.Other events must be mentioned briefly. Use at Missouri of the AMA Current Medical Terminology tapes in the Consider system was really fun. We used sets of symptoms and findings from patient cases as search arguments against the AMA structured text that constituted fairly regularized disease definitions. This produced a differential diagnosis: at least diagnoses to “consider.”6 The latter were of course ranked according to disease prevalence at the University of Missouri Medical Center as reflected in its discharge diagnosis file. These searches were first done in batch mode. It was clear, however, that teaching of students and residents would be greatly enhanced if we could go on-line. We did this using the IBM 1410 at a time when IBM had not yet produced even its first cathode ray tube terminal. Our display was a Control Data Corp. CRT kludged onto the 1301 RAMAC disk controller. To his credit, the IBM field engineer showed us how to hook it up so the controller thought it had a second RAMAC unit. This CMT work has been subsequently extended by Scott Blois and his Reconsider collaborators, who have added many improvements in software services. To our mutual regret, AMA has not made comparable improvements to the data base itself.I should mention, however, that even the early version of CMT (later called CMIT) contained literature references. These were part of the tape record, even though they were not part of the printed books. This impressed upon me the tremendous power of the formal scientific literature, as well as the great scope of even the central core of medical knowledge. We frequently demonstrated the system with half a dozen excellent clinicians present, in addition to students and residents. I'm certain there never was a time when the differential diagnoses Consider produced did not far exceed the range of thinking and knowledge of the observers. Our simple reasoning that “you can't diagnose something you don't consider” was compelling then and remains so today.The Regional Medical Program must be mentioned. To me, this was a wonderful development by the Federal Government. It turned out to be short-lived, but during this period we had the privilege of testing the Caceres PHS system for EKG interpretation.7 Luther Terry was PHS Surgeon General at the time. He called to initiate the demonstration and trial. This was the first time the Caceres system had been used “in real life” outside the Federal labs. Now that I am a “Fed,” I can understand better why this was so. We tested it statewide in 22 different kinds of settings. We found it to be the most appealing and understandable of all the RMP-proffered aids to medical care. I should also note that in addition to Dr. Caceres, Hubert Pipburger from the Veterans Administration was most generous in giving me help and advice in this RMP project. I won't describe the Caceres system in detail; I hope he will do this. The bottom line scientifically is that it worked extremely well. In general practice settings, 90% of the tracings were normal; 70% from a group of three cardiologists were normal; and 50% from tertiary care hospitals were normal. Fortunately “normal wave form” was a highly reliable statement, as were many of the 150 interpretations. The statements concerning complex arrhythmias were hampered by the relatively short length of tracing then being studied. The bottom line from the point of view of health services research was that the system, with its costly data acquisition carts, transmission, and processing charges was relatively expensive for a rural area like central Missouri but an instant economic success in cities like St. Louis and San Francisco.8A turning point for me professionally came in 1971 after about ten years of directing the Medical Computer Program. We had by then fully operational information systems in clinical laboratory, radiology, dietary, business office, and an institutional integrated file that combined all these medical data with discharge diagnoses, surgical operating room records, surgical pathology and cytology diagnoses, and EKG interpretations. The Clinical literature system was laggard. We had established an SDI system for scientific journal articles and were prepared to leave the rest to NLM! I realized at the time that the future of the field and my own desires lay in research and teaching, not with hospital service systems. The Computer Program had grown from five persons and $98,000/year in 1962 to 125 people and over $4 million in 1970. Consequently we recruited Dr. Peter Reichertz to become director of the Computer Program, and I formed the Information Science Group. The latter has ever since been dedicated to efforts to train the cadre of scientists and health professionals that are needed to build the theoretical and scientific basis for future advances in this field.This brings me to closing on the topic, what has not happened in 25 years? Briefly, we have not seen the creation of the body of theory and principles that should underly our efforts. That we use better, faster computers and more pleasant display devices is simply due to our coupling with the aerospace and business technology evolution. More importantly, medical informatics has remained reasonably well coupled with advances in understanding of basic biomedical fields. This is a struggle: how to keep current in a technical area and in clinical medicine simultaneously. We can already see that the cutting edge research in molecular biology/ biotechnology is seriously challenging even the best people in medical informatics to make room once again for brand new ideas. Biotechnology is especially challenging because at the moment it benefits so much from mathematical computation at a time when medical informatics has half-way convinced itself it needs only symbolic reasoning.The real value of a day of retrospection such as this one must surely be to see more clearly how to progress. The day is just starting, so we must not draw hasty conclusions. Yet I'll bet we will see and hear lots of reasons today to invest in individual creative investigators, that we will see many examples of the need for an atmosphere of interdisciplinary research on topics of fundamental importance, and that we will end the day wanting scope and support for building the formal academic basis for medical informatics.},
booktitle = {Proceedings of ACM Conference on History of Medical Informatics},
pages = {1–4},
numpages = {4},
location = {Bethesda, Maryland, USA},
series = {HMI '87}
}

@inproceedings{10.1145/322917.323121,
author = {Ahuja, Sanjiv},
title = {On Communication Software Testing (Abstract Only)},
year = {1987},
isbn = {0897912187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322917.323121},
doi = {10.1145/322917.323121},
abstract = {The past few years have seen a lot of activity in defining the functions provided by communication protocols and representing them in the form of layered architectures. (1, 2) The layered protocols have provided a means of separation of function in terms of layers in the communication software. Due to this separation of function the testing can be modularized to test each of the layer in entirety as a separate software package and then combining different layers and testing these as whole.Many communication protocols can be represented in a formal form. (3, 4) Most of these formal representations have been in the form of finite state machines, while some have been in the form of the grammars and some both. (5) Techniques have been suggested for the generation of test cases by using the formal representation of protocol. Specifically, by using some well known properties of finite state machines. Not all protocol functions are fully described by these representations therefore techniques for the generation of other test cases to test the whole layer of a protocol are suggested.Due to increasing complexity of protocols, it is recommended to split the layer of a protocol into various functional units, e.g. the link layer of the X.25, HDLC can be split into the following units i) Bit stuffing ii) Checksumming iii) Acknowledgements iv) Polling, etc. Each of these functional units begins with an initial state or set of states. For each state model the following technique can be used for generation of test cases: We assume that the given fsm for the function has the following characteristics 1) completely specified 2) minimal 3) starts with a fixed initial state and 4) every state is reachable.The test cases consist of set 'I' which covers all possible state transitions in the machine. One of the ways to construct the set 'I' is to build a “testing tree” of the fsm 'S'. All partial paths (a sequence of arcs) in 'S' constitute the set 'I'.A partial path starts at the root of the tree and ends either at a terminal or a non terminal node. Each branch in the tree is labeled with an input command/message. This 'I' is a set of input commands/messages from the lower layer of the protocol. Given an fsm 'S' of a functional unit of a protocol a testing tree 'T' can be constructed as follows: a) label the root of the tree 'T' with the initial state of 'S'. b) given level '1' of the tree to construct level '1+1' examine the nodes from left to right. A node is terminated if its label is the same as a non terminal at some level j where j=1. This avoids infinite loops, etc. If node is not terminated label it as i. If on input Ii the machine goes from state Si to Sj we attach a branch Ii and a lower level node Sj to Si in T. This process always terminates as there are only a finite number of states.Having generated the test cases for the function specified in the form of fsms we still have to generate test cases for the rest of the protocol layer which is represented in the form of abstract programs. It is recommended that path execution technique be applied at the abstract program part of the protocol also as that will generate paths only at the protocol functional level rather than at the program control level, therefore providing a more efficient technique for error detection.},
booktitle = {Proceedings of the 15th Annual Conference on Computer Science},
pages = {460},
location = {St. Louis, Missouri, USA},
series = {CSC '87}
}

@inproceedings{10.1145/3545948.3545997,
author = {Cloosters, Tobias and Paa\ss{}en, David and Wang, Jianqiang and Draissi, Oussama and Jauernig, Patrick and Stapf, Emmanuel and Davi, Lucas and Sadeghi, Ahmad-Reza},
title = {RiscyROP: Automated Return-Oriented Programming Attacks on RISC-V and ARM64},
year = {2022},
isbn = {9781450397049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545948.3545997},
doi = {10.1145/3545948.3545997},
abstract = {Return-oriented programming&nbsp;(ROP) is a powerful run-time exploitation technique to attack vulnerable software. Modern RISC architectures like RISC-V and ARM64 pose new challenges for ROP execution due to the lack of a stack-based return instruction and strict instruction alignment. Further, the large number of caller-saved argument registers significantly reduces the gadget space available to the attacker. Consequently, existing ROP gadget tools for other processor architectures cannot be applied to these RISC architectures. Previous work on RISC-V provides only manual construction of ROP attacks against specially crafted programs, and no analysis of ROP attacks has been conducted for ARM64 yet. In this paper, we address these challenges and present RiscyROP, the first automated ROP gadget finding and chaining toolkit for RISC-V and ARM64. RiscyROP analyzes available gadgets utilizing symbolic execution, and automatically generates complex multi-stage chains to conduct arbitrary function calls. Our approach enables the first investigation of the gadget space on RISC-V and ARM64 real-world binaries. RiscyROP successfully builds ROP chains that enable an attacker to execute arbitrary function calls for the nginx web server as well as any binary that contains the libc library.},
booktitle = {Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {30–42},
numpages = {13},
keywords = {Symbolic Execution, RISC-V, Return-Oriented Programming, ARM64, Exploitation},
location = {Limassol, Cyprus},
series = {RAID '22}
}

@inproceedings{10.5555/800124.804005,
author = {Gingerich, Jeffrey Z.},
title = {Computer Graphics Building Definition System},
year = {1973},
publisher = {IEEE Press},
abstract = {In the Architectural field many computer programs exist for design aid. They vary from space allocation algorithms to structural analysis and computer graphic design. More often than not, these programs are independent. Even though most of the architectural programs use similar data in describing buildings and/or parts of buildings and its systems, they very seldom have compatible formats. Therefore, to run a program the user must redescribe the building and/or its parts.If a firm, college, or the profession (preferably) defines a workable building definition data base for use in all programs, one can see there would be a time and work advantage. A workable data base would by definition have to be flexible enough to handle input, output, and change. It should be inclusive enough to define any and all parts down to the level necessary for the building's final construction.Just having a workable building data base would be ineffective without a powerful language to feed and retrieve the building data. When realizing this, one wonders what to develop first, the data base or the language. The approach of this study was to develop a prototype language using computer graphics.The Computer Graphics Building Definition (CGBD) System is designed with the following ideas: 1) computer graphics input and output to simulate the conventional architectural drafting language; 2) the data base prototype as a module to insure compatibility when a more sophisticated base is designed. The goal is a first step system that will allow for future change and addition.The CGBD System facilitates describing a building in an interactive set of two phases. Phase I is schematics. Here the designer lays out areas (interior and exterior) of the building which he wants to analyze or he is currently designing. (As with the conventional drafting language, design and definition can be one.)This is accomplished using the cathode-ray tube and light pen instead of paper and drafting equipment. The user defines the building (s), two-dimensionally, one level (or floor) at a time. Building area modules can be created, repeated, or manipulated and floors repeated or displayed for tracing and reference. These techniques increase the speed of the building definition process. Erasing areas, parts of areas, or buildings is also available to increase the user's efficiency.Phase II is a three-dimensional inspection package that allows the defined parts of the building to be displayed. The building may be rotated about any of the three axes as an orthographic or perspective projection. Hidden line removal can be used for viewing the building masses as a solid or may be suppressed to view interior floor or interior area configurations. The viewpoint for perspective projection can be adjusted. Scissoring and enlarging are available for closer inspection of building parts.By using the speed of a computer to simulate drafting, sketching, and visual inspection, the user can define and/or design buildings while simultaneously providing a building data base for use in other design programs.},
booktitle = {Proceedings of the 10th Design Automation Workshop},
pages = {109–119},
numpages = {11},
series = {DAC '73}
}

@inproceedings{10.5555/3014904.3015013,
author = {Nguyen, Tan and Unat, Didem and Zhang, Weiqun and Almgren, Ann and Farooqi, Nufail and Shalf, John},
title = {Perilla: Metadata-Based Optimizations of an Asynchronous Runtime for Adaptive Mesh Refinement},
year = {2016},
isbn = {9781467388153},
publisher = {IEEE Press},
abstract = {Hardware architecture is increasingly complex, urging the development of asynchronous runtime systems with advance resource and locality management supports. However, these supports may come at the cost of complicating the user interface while programming remains one of the major constraints to wide adoption of asynchronous runtimes in practice. In this paper, we propose a solution that leverages application metadata to enable challenging optimizations as well as to facilitate the task of transforming legacy code to an asynchronous representation. We develop Perilla, a task graph-based runtime system that requires only modest programming effort. Perilla utilizes metadata of an AMR software framework to enable various optimizations at the communication layer without complicating its API. Experimental results with different applications on up to 24K processor cores show that Perilla can realize up to 1.44x speedup over the synchronous code variant. The metadata enabled optimizations account for 25% to 100% of the performance improvement.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {81},
numpages = {12},
location = {Salt Lake City, Utah},
series = {SC '16}
}

@article{10.5555/3455716.3455816,
author = {Valera, Isabel and Pradier, Melanie F. and Lomeli, Maria and Ghahramani, Zoubin},
title = {General Latent Feature Models for Heterogeneous Datasets},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Latent variable models allow capturing the hidden structure underlying the data. In particular, feature allocation models represent each observation by a linear combination of latent variables. These models are often used to make predictions either for new observations or for missing information in the original data, as well as to perform exploratory data analysis. Although there is an extensive literature on latent feature allocation models for homogeneous datasets, where all the attributes that describe each object are of the same (continuous or discrete) type, there is no general framework for practical latent feature modeling for heterogeneous datasets. In this paper, we introduce a general Bayesian nonparametric latent feature allocation model suitable for heterogeneous datasets, where the attributes describing each object can be arbitrary combinations of real-valued, positive real-valued, categorical, ordinal and count variables. The proposed model presents several important properties. First, it is suitable for heterogeneous data while keeping the properties of conjugate models, which enables us to develop an inference algorithm that presents linear complexity with respect to the number of objects and attributes per MCMC iteration. Second, the Bayesian nonparametric component allows us to place a prior distribution on the number of features required to capture the latent structure in the data. Third, the latent features in the model are binary-valued, which facilitates the interpretability of the obtained latent features in exploratory data analysis. Finally, a software package, called GLFM toolbox, is made publicly available for other researchers to use and extend. It is available at https://ivaleram.github.io/GLFM/. We show the flexibility of the proposed model by solving both prediction and data analysis tasks on several real-world datasets.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {100},
numpages = {49}
}

@article{10.1145/3536165,
author = {Chen, Qiao and Jiao, Xiangmin},
title = {HIFIR: Hybrid Incomplete Factorization with Iterative Refinement for Preconditioning Ill-Conditioned and Singular Systems},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/3536165},
doi = {10.1145/3536165},
abstract = {We introduce a software package called Hybrid Incomplete Factorization with Iterative Refinement (HIFIR) for preconditioning sparse, unsymmetric, ill-conditioned, and potentially singular systems. HIFIR computes a hybrid incomplete factorization (HIF), which combines multilevel incomplete LU factorization with a truncated, rank-revealing QR (RRQR) factorization on the final Schur complement. This novel hybridization is based on the new theory of ϵ-accurate approximate generalized inverse (AGI). It enables near-optimal preconditioners for consistent systems and enables flexible GMRES to solve inconsistent systems when coupled with iterative refinement. In this article, we focus on some practical algorithmic and software issues of HIFIR. In particular, we introduce a new inverse-based rook pivoting (IBRP) into ILU, which improves the robustness and the overall efficiency for some ill-conditioned systems by significantly reducing the size of the final Schur complement for some systems. We also describe the software design of HIFIR in terms of its efficient data structures for supporting rook pivoting in a multilevel setting, its template-based generic programming interfaces for mixed-precision real and complex values in C++, and its user-friendly high-level interfaces in MATLAB and Python. We demonstrate the effectiveness of HIFIR for ill-conditioned or singular systems arising from several applications, including the Helmholtz equation, linear elasticity, stationary incompressible Navier–Stokes (INS) equations, and time-dependent advection-diffusion equation.},
journal = {ACM Trans. Math. Softw.},
month = {sep},
articleno = {32},
numpages = {33},
keywords = {rank-revealing factorization, iterative refinement, multilevel ILU factorization, hybrid incomplete factorization, Preconditioning, approximate generalized inverse, singular systems}
}

@article{10.1145/1233307.1233309,
author = {Fraser, Keir and Harris, Tim},
title = {Concurrent Programming without Locks},
year = {2007},
issue_date = {May 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {0734-2071},
url = {https://doi.org/10.1145/1233307.1233309},
doi = {10.1145/1233307.1233309},
abstract = {Mutual exclusion locks remain the de facto mechanism for concurrency control on shared-memory data structures. However, their apparent simplicity is deceptive: It is hard to design scalable locking strategies because locks can harbor problems such as priority inversion, deadlock, and convoying. Furthermore, scalable lock-based systems are not readily composable when building compound operations. In looking for solutions to these problems, interest has developed in nonblocking systems which have promised scalability and robustness by eschewing mutual exclusion while still ensuring safety. However, existing techniques for building nonblocking systems are rarely suitable for practical use, imposing substantial storage overheads, serializing nonconflicting operations, or requiring instructions not readily available on today's CPUs.In this article we present three APIs which make it easier to develop nonblocking implementations of arbitrary data structures. The first API is a multiword compare-and-swap operation (MCAS) which atomically updates a set of memory locations. This can be used to advance a data structure from one consistent state to another. The second API is a word-based software transactional memory (WSTM) which can allow sequential code to be reused more directly than with MCAS and which provides better scalability when locations are being read rather than being updated. The third API is an object-based software transactional memory (OSTM). OSTM allows a simpler implementation than WSTM, but at the cost of reengineering the code to use OSTM objects.We present practical implementations of all three of these APIs, built from operations available across all of today's major CPU families. We illustrate the use of these APIs by using them to build highly concurrent skip lists and red-black trees. We compare the performance of the resulting implementations against one another and against high-performance lock-based systems. These results demonstrate that it is possible to build useful nonblocking data structures with performance comparable to, or better than, sophisticated lock-based designs.},
journal = {ACM Trans. Comput. Syst.},
month = {may},
pages = {5–es},
numpages = {61},
keywords = {transactional memory, lock-free systems, Concurrency}
}

@inproceedings{10.1145/331960.331975,
author = {Sheard, Tim and Benaissa, Zine-el-abidine and Pasalic, Emir},
title = {DSL Implementation Using Staging and Monads},
year = {2000},
isbn = {1581132557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/331960.331975},
doi = {10.1145/331960.331975},
abstract = {The impact of Domain Specific Languages (DSLs) on software design is considerable. They allow programs to be more concise than equivalent programs written in a high-level programming languages. They relieve programmers from making decisions about data-structure and algorithm design, and thus allows solutions to be constructed quickly. Because DSL's are at a higher level of abstraction they are easier to maintain and reason about than equivalent programs written in a high-level language, and perhaps most importantly they can be written by domain experts rather than programmers.The problem is that DSL implementation is costly and prone to errors, and that high level approaches to DSL implementation often produce inefficient systems. By using two new programming language mechanisms, program staging and monadic abstraction, we can lower the cost of DSL implementations by allowing reuse at many levels. These mechanisms provide the expressive power that allows the construction of many compiler components as reusable libraries, provide a direct link between the semantics and the low-level implementation, and provide the structure necessary to reason about the implementation.},
booktitle = {Proceedings of the 2nd Conference on Domain-Specific Languages},
pages = {81–94},
numpages = {14},
location = {Austin, Texas, USA},
series = {DSL '99}
}

@article{10.1145/331963.331975,
author = {Sheard, Tim and Benaissa, Zine-el-abidine and Pasalic, Emir},
title = {DSL Implementation Using Staging and Monads},
year = {2000},
issue_date = {Jan. 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/331963.331975},
doi = {10.1145/331963.331975},
abstract = {The impact of Domain Specific Languages (DSLs) on software design is considerable. They allow programs to be more concise than equivalent programs written in a high-level programming languages. They relieve programmers from making decisions about data-structure and algorithm design, and thus allows solutions to be constructed quickly. Because DSL's are at a higher level of abstraction they are easier to maintain and reason about than equivalent programs written in a high-level language, and perhaps most importantly they can be written by domain experts rather than programmers.The problem is that DSL implementation is costly and prone to errors, and that high level approaches to DSL implementation often produce inefficient systems. By using two new programming language mechanisms, program staging and monadic abstraction, we can lower the cost of DSL implementations by allowing reuse at many levels. These mechanisms provide the expressive power that allows the construction of many compiler components as reusable libraries, provide a direct link between the semantics and the low-level implementation, and provide the structure necessary to reason about the implementation.},
journal = {SIGPLAN Not.},
month = {dec},
pages = {81–94},
numpages = {14}
}

@article{10.1145/3582492,
author = {Anselmann, Mathias and Bause, Markus},
title = {A Geometric Multigrid Method for Space-Time Finite Element Discretizations of the Navier–Stokes Equations and Its Application to 3d Flow Simulation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0098-3500},
url = {https://doi.org/10.1145/3582492},
doi = {10.1145/3582492},
abstract = {Abstract. We present a parallelized geometric multigrid (GMG) method, based on the cell-based Vanka smoother, for higher order space-time finite element methods (STFEM) to the incompressible Navier–Stokes equations. The STFEM is implemented as a time marching scheme. The GMG solver is applied as a preconditioner for GMRES iterations. Its performance properties are demonstrated for 2d and 3d benchmarks of flow around a cylinder. The key ingredients of the GMG approach are the construction of the local Vanka smoother over all degrees of freedom in time of the respective subinterval and its efficient application. For this, data structures that store pre-computed cell inverses of the Jacobian for all hierarchical levels and require only a reasonable amount of memory overhead are generated. The GMG method is built for the deal.II finite element library. The concepts are flexible and can be transferred to similar software platforms.},
note = {Just Accepted},
journal = {ACM Trans. Math. Softw.},
month = {feb}
}

@inproceedings{10.1145/3511808.3557181,
author = {Nguyen, Hoang and Hamidi Rad, Radin and Bagheri, Ebrahim},
title = {PyDHNet: A Python Library for Dynamic Heterogeneous Network Representation Learning and Evaluation},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557181},
doi = {10.1145/3511808.3557181},
abstract = {Network representation learning and its applications have received increasing attention. Due to their various application areas, many research groups have developed a diverse range of software tools and techniques to learn representation for different types of networks. However, to the best of our knowledge, there are limited works that support representation learning for dynamic heterogeneous networks. The work presented in this demonstration paper attempts to fill the gap in this space by developing and publicly releasing an open-source Python library known as, PyDHNet, a Python Library for Dynamic Heterogeneous Network Representation Learning and Evaluation. PyDHNet consists of two main components: dynamic heterogeneous network representation learning and task-specific evaluation. In our paper, we demonstrate that PyDHNet has an extensible architecture, is easy to install (through PIP) and use, and integrates quite seamlessly with other Python libraries. We also show that the implementation for PyDHNet is efficient and enjoys a competitive execution time.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4936–4940},
numpages = {5},
keywords = {network representation learning, network application, dynamic heterogeneous network},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3480935,
author = {Anzt, Hartwig and Cojean, Terry and Flegar, Goran and G\"{o}bel, Fritz and Gr\"{u}tzmacher, Thomas and Nayak, Pratik and Ribizel, Tobias and Tsai, Yuhsiang Mike and Quintana-Ort\'{\i}, Enrique S.},
title = {Ginkgo: A Modern Linear Operator Algebra Framework for High Performance Computing},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/3480935},
doi = {10.1145/3480935},
abstract = {In this article, we present Ginkgo, a modern C++ math library for scientific high performance computing. While classical linear algebra libraries act on matrix and vector objects, Ginkgo’s design principle abstracts all functionality as “linear operators,” motivating the notation of a “linear operator algebra library.” Ginkgo’s current focus is oriented toward providing sparse linear algebra functionality for high performance graphics processing unit (GPU) architectures, but given the library design, this focus can be easily extended to accommodate other algorithms and hardware architectures. We introduce this sophisticated software architecture that separates core algorithms from architecture-specific backends and provide details on extensibility and sustainability measures. We also demonstrate Ginkgo’s usability by providing examples on how to use its functionality inside the MFEM and deal.ii finite element ecosystems. Finally, we offer a practical demonstration of Ginkgo’s high performance on state-of-the-art GPU architectures.},
journal = {ACM Trans. Math. Softw.},
month = {feb},
articleno = {2},
numpages = {33},
keywords = {multi-core and manycore architectures, High performance computing, healthy software lifecycle}
}

@inproceedings{10.1145/512035.512049,
author = {Ohki, Mikio and Akiyama, Shinjiro and Kambayashi, Yasushi},
title = {A Verification of Class Structure Evolution Model and Its Parameters},
year = {2002},
isbn = {1581135459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512035.512049},
doi = {10.1145/512035.512049},
abstract = {It is widely accepted that the role of software "architect" that provide frameworks to program developers is important in the object-oriented software development processes. When developers try to extend the base classes given by the architect, they may want some guidelines that tell them how many subclasses and how many methods in one subclass are reasonable. So far we are not aware of such guidelines. Through measurements of Java and Delphi class libraries, we have distilled formulae that forecast the number of methods and the number of subclasses when constructing class trees from the base classes. We propose that we should focus to extract methods and attributes rather than class structure. The formulae we have formulated support this proposition.},
booktitle = {Proceedings of the International Workshop on Principles of Software Evolution},
pages = {52–56},
numpages = {5},
keywords = {verification, architect, measurement, evolution model},
location = {Orlando, Florida},
series = {IWPSE '02}
}

@inproceedings{10.5555/381473.381502,
author = {Murphy, Gail C. and Lai, Albert and Walker, Robert J. and Robillard, Martin P.},
title = {Separating Features in Source Code: An Exploratory Study},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most software systems are inflexible. Reconfiguring a system's modules to add or to delete a feature requires substantial effort. This inflexibility increases the costs of building variants of a system, amongst other problems.New languages and tools that are being developed to provide additional support for separating concerns show promise to help address this problem. However, applying these mechanisms requires determining how to enable a feature to be separated from the codebase. In this paper, we investigate this problem through an exploratory study conducted in the context of two existing systems: gnu.regexp and jFTPd. The study consisted of applying three different separation of concern mechanisms—Hyper/J,TM AspectJ,TM and a lightweight, lexically-based approach—to separate features in the two packages. In this paper, we report on the study, providing contributions in two areas. First, we characterize the effect different mechanisms had on the structure of the codebase. Second, we characterize the restructuring process required to perform the separations. These characterizations can help researchers to elucidate how the mechanisms may be best used, tool developers to design support to aid the separation process, and early adopters to apply the techniques.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {275–284},
numpages = {10},
keywords = {aspect-oriented programming, design space, feature separation, hyperspaces},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@inproceedings{10.1145/1413140.1413167,
author = {Sharp, Brian L. and Peterson, Gregory D. and Yan, Lok Kwong},
title = {Extending Hardware Based Mandatory Access Controls for Memory to Multicore Architectures},
year = {2008},
isbn = {9781605580982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1413140.1413167},
doi = {10.1145/1413140.1413167},
abstract = {Current memory architectures do not practice the principle of least privilege, resulting in the exposure of memory to data corruption and usurpation threats, collectively called memory based threats. The most famous of such threats is probably the buffer overflow. It has received considerable attention due to its relative ease of exploitation and the ease of exposure as seen by the sheer number of vulnerabilities.The first great computer infection in 1988, the Morris Worm, made use of a buffer overflow exploit in the fingerd application as one of its methods of propagation [4]. Given the vastness and damage of the worm, one would think that in a short amount of time a solution would be developed that would inoculate the world's computers from such a sickness. However, in 2001, thirteen years after the Morris Worm, the Code-Red and CodeRedII worms would use another buffer overrun that would cost $2.6 billion [2]. This is a testimony to the ineffectiveness of the proposed solutions during that thirteen year period and the overall effectiveness of this attack scheme.The problem that we still face is not the lack of solutions, but of a pervasive one that can be enabled and adapted easily by large populations and that actually solves the underlying problem.For analysis purposes, we define memory based vulnerabilities as vulnerabilities that arise from the way that system memory is organized and used. They can then be binned into four large groups, uninitialized data, memory leaks, data usurpation and data corruption. Uninitialized data problems occur when an application fails to initialize objects to fail-safe defaults. Memory leaks occur when applications allocate memory but lose access to them, although the operating system can retrieve the memory after the application ends. Data usurpation problems occur when memory that should not be readable by an application is. Data corruption vulnerabilities are the ones where memory data is overwritten, and therefore corrupted, when it was not part of the application design to do so. For the rest of the paper, we will consider data corruption and focus on buffer overflows since it is by far the most common memory based vulnerability and poses the highest risk.Data usurpation and corruption vulnerabilities exist because the CPU has ubiquitous access to all of memory, i.e. memory is world read, write, and executable by the CPU, which we consider a hardware issue since the CPU has instructions to read, write, and execute memory data. We consider uninitialized data and memory leak software issues because processors do not have built in constructs to allocate memory on the object level; this is all done at the application or software level. The common problem amongst the four of them is that the task of managing and protecting memory is left to the operating system, through the memory management unit which operates on pages, and the application level programmer, while actual memory accesses and controls are done by the CPU that operates on the native memory addressable size, which is accessible by all levels of applications.This imbalance in the unit of protection in the operating system and the CPU and the reliance on the operating system to offer access control for memory explains many of the memory based vulnerabilities. For example, buffer overflows are successful because an application can overwrite all in-band data as long as the data is within the page and even across pages if the pages are contiguous, even though the write should be restricted to only within the buffer. Unfortunately, operating systems typically can only control access page by page.Although buffer overflows are only one of many types of memory data corruption threats, the prevalence of buffer overflow vulnerabilities and the plethora of solutions make it a prime vehicle to investigate hardware based memory access controls. Hence, we propose adding metadata bits to memory for the purpose of providing access control to memory structures while maintaining backwards compatibility.Two motivating factors are considered. First, current memory architectures, including Intel's new LaGrande Technology, rely on the operating system or a similar piece of software to ensure that processes can only access their own memory pages and on the application programmers to properly manage their memory space. Under such a scheme the operating system grants a process unlimited read and write privileges to a page, including the control flow data that only the processor should have access to. Furthermore, the operating system grants execute permissions by the page, leaving executable slack space that could be exploited. Second, a majority of solutions to memory corruption threats have been proposed as singular solutions and require too much user and end-user expertise, therefore limiting their adoption. For example, a majority of buffer overflow solutions do not address the memory corruption problem. They allow an overflow to occur, but attempt to prevent the intended result, such as control flow redirection.One example of this situation is AMD's No eXecute bit (NX), used to mark pages of memory as non-executable. Intel's version of AMD's NX bit is called the Execute Disable Bit [1]. The capability of the processor to take advantage of this sort of functionality can be queried by the operating system that is running. When activated by setting the bit IA32_EFER.NXE, memory pages can be marked as not being executable. This is done by adjusting bit 63 in the corresponding page table entry for that page of memory. If the protection is running and an instruction fetch to a linear address that translates to a physical address in a memory page that has the execute disable bit set, a page fault exception will be generated. It can be seen from this example that a buffer overflow can allow arbitrary code to be written to memory, but no security breach occurs because the page is marked as non-executable. This sort of protection is very close to what is desired in protecting from memory based vulnerabilities: there is no effort required of the user, it incurs very little memory or performance overhead, and is backwards compatible with existing code.To allow for that backwards compatibility, Intel decided to give the host OS the ability to turn this protection on or off. Windows XP Service Pack 2 and Windows 2003 Service Pack 1 contain patches to take advantage of this hardware feature by using what Microsoft calls Data Execution Prevention (DEP). Shortly after their debut, exploits began to be posted that easily sidestepped the mechanism [3]. In order to bypass DEP, a ret-libC style attack can be used to jump to a section of system code marked as executable that can then further be exploited to disable DEP and return into shell code stored in the original buffer. If a process could not disable NX support at runtime, this exploit would not work. It is this fact that makes it seem likely that a hardware based mandatory access control mechanism would be successful. Protection schemes at the hardware level should not be allowed to be circumvented by the OS. While it is understandable that some security tradeoffs may need to be made to allow for backwards compatibility, some basic security components should not. It is this idea that is at the heart of our concept.In recent work, the idea of marking memory as non-executable has been extended to a finer grained approach. Secure Bit2 extends every memory location by one bit to add semantic meaning to each word of memory [5]. This bit is moved along with its associated word by memory manipulation instructions. Words in buffers between processes get their secure bit set while all others mark the secure bit at the destination register or memory location. Call, return, and jump instructions check the secure bit and if set, generate an interrupt or fault signal. Modifications are required at the kernel level to set the secure bit when passing a buffer across domains. Since the address validation is done in hardware, there is no performance overhead.DIFT (Dynamic Information Flow Tracking) is another hardware-based tainting mechanism [6]. It offers two different security policies depending on the amount of protection needed and overhead willing to be incurred. Data from I/O channels is considered spurious and is marked as such. This data is stored in an extra bit and is propagated to other memory locations as it interacts with other pieces of data. DIFT makes additions to a standard processor to add the extra bits and proposes various schemes to minimize memory and performance overhead. The processor then checks the taint bits before executing an instruction or committing to a branch.However, on some architectures, standard integer arithmetic instructions are indistinguishable from pointer arithmetic instructions. This poses a problem for taint bit propagation. Also, in the case of a properly coded program that first bounds checks a piece of data and then combines it with a trusted pointer, a perfectly safe piece of code could be marked as tainted. In this case, the operating system taint module could be invoked many times, resulting in a hefty overhead. There is also a format string attack that takes advantage of a little known printf feature that can bypass DIFT.These hardware based solutions require the least user intervention and promise to be the most pervasive because changing the CPU or obtaining one through a new computer will mean that the solution is built in and it doesn't matter which operating system is used. The user can be the end-user, but that is only when a new system is purchased. For many people, this is done as often as every two years. For older systems, the user must be an administrator or technician since they must install and configure the new hardware.The only required changes are to provide a tag for each byte of memory and change each processing instruction to enforce a simple policy based on each byte's tag. We concluded that tagging at the byte level is the necessary approach. This was chosen in favor of tagging each word, since memory architectures are byte addressable, even though most compilers use word boundaries. Although it does mean that there will be considerable storage overhead, it is compatible with virtual memory architectures. Given the desire to keep the solution as simple as possible, we determined that enforcing access control at the instruction level would be adequate.Whereas these previously mentioned solutions stood up to many attacks that computer systems today cannot handle without extensive antivirus protections, they do not take into account the changing landscape of the industry. The difficulty in continuously increasing clock speed has caused CPU designers to resort to increasing the number of processor cores on a chip in an effort to maintain exponential growth in computing power. The result of this has been a programming and security nightmare. One processor core may lose track of what the others are doing and illegally access memory. It is a possibility for one core to be hijacked to operate maliciously. The challenge of merely maintaining consistency across memory has proved to be difficult. It is the purpose of this work to adapt byte level tainting to fit multicore architectures.In order to catch input coming from an untrusted source (network, user, etc), modifications must be made to the OS in order to mark incoming data as tainted. The previous solutions vary in the granularity at which they taint data, as well as some of the specific policies they use to propagate this information and alert the user to possible security breaches, but these details are inconsequential when it comes to extending these ideas to multicore. The designers make a point to mention how these systems are orthogonal to memory and transparent to existing programs. All of the above solutions essentially add one or multiple bits to each memory location which is then moved around just as if the processor supported a wider word. No major conceptual changes are made to the memory hierarchy. If a particular multicore architecture is able to handle cache coherence issues, the hardware based solutions should work on it also. In other words, if a particular system is able to maintain data coherence across cores by adding another bit to each memory location, moving the taint bit around with its associated piece of memory (across caches and through memory), and maintaining a coherence protocol, then the multicore architecture should be able to take advantage of a hardware based protection scheme. A system that makes use of a cache coherence protocol would be able to move around data and its associated tag bits without even being aware that it was doing so (other than the odd length words).In this paper, we explore the effectiveness of the various memory access control approaches for protecting applications executing on multicore architectures. Both single-threaded and multi-threaded application behaviors are considered, along with suggestions for how the memory access control approaches can be made relevant to the new multicore processors that are rapidly becoming ubiquitous.},
booktitle = {Proceedings of the 4th Annual Workshop on Cyber Security and Information Intelligence Research: Developing Strategies to Meet the Cyber Security and Information Intelligence Challenges Ahead},
articleno = {23},
numpages = {3},
keywords = {mandatory access control, data corruption, hardware, type safety, data usurpation, secure processor},
location = {Oak Ridge, Tennessee, USA},
series = {CSIIRW '08}
}

