
@article{ WOS:000922848700001,
Author = {Audrito, Giorgio and Terraneo, Federico and Fornaciari, William},
Title = {FCPP plus Miosix: Scaling Aggregate Programming to Embedded Systems},
Journal = {IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS},
Year = {2023},
Volume = {34},
Number = {3},
Pages = {869-880},
Month = {MAR 1},
Abstract = {As the density of nodes capable of sensing, computing and actuation
   increases, it becomes increasingly useful to model an entire network of
   physical devices as a single, continuous space-time computing machine.
   The emergent behaviour of the whole software system is then induced by
   local computations deployed within each node and by the dynamics of the
   information diffusion. A relevant example of this distribution model is
   given by aggregate programming and its minimal set of functional
   constructs used to manipulate distributed data structures evolving over
   space and time, and resulting in robustness to changes. In this paper,
   we propose the first implementation of the aggregate computing paradigm
   targeting microcontrollers, by integrating FCPP, a C++ implementation of
   the paradigm, with Miosix, a modern operating system for
   microcontrollers with full C++ support. To the best of the author's
   knowledge, we are the first to present results on the effectiveness of
   FCPP in an embedded operating system setting as opposed to a simulation
   environment, thus considering tight memory and computational constraints
   and accounting for packet losses due to nonidealities of the radio
   channel. We implemented and tested on a network of WandStem nodes two
   benchmark applications: a network connectivity checker for network
   planning and preventive maintenance, and a decentralised contact tracing
   application. Additionally, we show that common problems in sensor
   networks such as neighbour discovery, construction of a graph of the
   network topology, coarse grain clock synchronisation as well as network
   monitoring and the collection of statistics (such as memory occupation
   data) can be easily performed thanks to the expressive semantics of
   aggregate programming.},
DOI = {10.1109/TPDS.2022.3232633},
ISSN = {1045-9219},
EISSN = {1558-2183},
ResearcherID-Numbers = {Fornaciari, William/U-5352-2018
   },
ORCID-Numbers = {Fornaciari, William/0000-0001-8294-730X
   Terraneo, Federico/0000-0001-7475-6167},
Unique-ID = {WOS:000922848700001},
}

@article{ WOS:000933323900001,
Author = {Khunou, Sisinyana H.},
Title = {Mentoring community service nurses in public health settings: Guidelines
   for nurse managers},
Journal = {HEALTH SA GESONDHEID},
Year = {2023},
Volume = {28},
Month = {FEB 15},
Abstract = {Background: Adequate mentoring and support of community service nurses
   (CSNs) in transitioning from the learning environment to the public
   health setting is pivotal. Despite this notion, the mentoring of CSNs is
   inconsistently implemented. It was therefore imperative that the
   researchers developed the guidelines that can be used by managers to
   mentor the CSNs.Aim: This article shares nine guidelines to ensure
   adequate mentoring of CSNs in public health settings.Setting: The study
   was conducted in public health settings designated for placement of
   CSNs, in South Africa.Methods: This study followed a convergent parallel
   mixed-methods design whereby qualitative data were obtained from
   purposefully selected CSNs and nurse managers. Quantitative data were
   obtained from 224 CSNs and 174 nurse managers, with the use of mentoring
   questionnaires. Semi-structured interviews were used on focus groups of
   nurse managers (n = 27) and CSNs (n = 28). Quantitative data were
   analysed with Statistical Package for Social Science software version
   23, ATLAS.ti 7 software was used to analyse qualitative data.Results:
   The merged results evidenced that CSNs were not adequately mentored. The
   public health setting was not conducive to mentoring CSNs. Mentoring
   activities were not well structured. Monitoring and evaluation of
   mentoring of CSNs were not properly done. Evidence from merged results
   and literature were applied to develop mentoring guidelines for
   operationalising a mentoring programme for CSNs.Conclusion: The
   guidelines were: (1) creation of a positive mentoring environment, (2)
   enhancement of collaboration between stakeholders, (3) attributes of
   CSNs and nurse managers in the mentoring relationship, (4) enhance
   orientation for nurse managers and CSNs, (5) facilitation of
   mentor-mentee matching process, (6) conducting mentoring meetings, (7)
   capacity development for CSNs and nurse managers, (8) monitoring and
   evaluation of mentoring process, and (9) reflections and
   feedback.Contribution: This was the first CSNs' guidelines to be
   developed in the public health setting. These guidelines could
   facilitate adequate mentoring of CSNs.},
DOI = {10.4102/hsag.v28i0.1883},
Article-Number = {a1883},
ISSN = {1025-9848},
EISSN = {2071-9736},
Unique-ID = {WOS:000933323900001},
}

@article{ WOS:000899506700019,
Author = {Martinez-Estevez, I and Dominguez, J. M. and Tagliafierro, B. and
   Canelas, R. B. and Garcia-Feal, O. and Crespo, A. J. C. and
   Gomez-Gesteira, M.},
Title = {Coupling of an SPH-based solver with a multiphysics library},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2023},
Volume = {283},
Month = {FEB},
Abstract = {A two-way coupling between the Smoothed Particle Hydrodynamics-based
   (SPH) code with a multiphysics library to solve complex fluid-solid
   interaction problems is proposed. This work provides full access to the
   package for the use of this coupling by releasing the source code,
   completed with guidelines for its compilation and utilization, and
   self-contained template setups for practical uses of the novel
   implemented features, is provided here. The presented coupling expands
   the applicability of two different solvers allowing to simulate fluids,
   multibody systems, collisions with frictional contacts using either
   non-smooth contact (NSC) or smooth contact (SMC) methods, all integrated
   under the same framework. The fluid solver is the open-source code
   DualSPHysics, highly optimised for simulating free-surface phenomena and
   structure interactions, uniquely positioned as a general-purpose
   Computational Fluid Dynamics (CFD) software with a GPU-accelerated
   solver. Mechanical systems that comprise collision detection and/or
   multibody dynamics are solved by the multiphysics library Project
   Chrono, which uses a Discrete Element Method (DEM). Therefore, this
   SPH-DEM coupling approach can manage interactions between fluid and
   complex multibody systems with relative constraints, springs, or
   mechanical joints.},
DOI = {10.1016/j.cpc.2022.108581},
Article-Number = {108581},
ISSN = {0010-4655},
EISSN = {1879-2944},
ResearcherID-Numbers = {Tagliafierro, Bonaventura/T-1475-2019
   Martínez-Estévez, Iván/CAG-0583-2022
   C. Crespo, Alejandro J./G-9948-2015
   Garcia-Feal, Orlando/T-7721-2018
   Dominguez, Jose M./N-2219-2014},
ORCID-Numbers = {Tagliafierro, Bonaventura/0000-0001-9171-3038
   Martínez-Estévez, Iván/0000-0001-5870-8296
   C. Crespo, Alejandro J./0000-0003-3384-5061
   Garcia-Feal, Orlando/0000-0001-6237-660X
   Dominguez, Jose M./0000-0002-2586-5081},
Unique-ID = {WOS:000899506700019},
}

@article{ WOS:000914792800001,
Author = {Patton, M. Longshithung and Warsi, Syed Bustan Fatima and Adak, Dibyendu},
Title = {Experimental and numerical study on the structural behaviour of HST, RCC
   and CFST stub columns under pure axial compression},
Journal = {INNOVATIVE INFRASTRUCTURE SOLUTIONS},
Year = {2023},
Volume = {8},
Number = {2},
Month = {FEB},
Abstract = {The use of hollow steel tubes (HST) as structural members is gaining in
   popularity due to their excellent properties regarding loading in
   compression, torsion and bending in all directions. Also,
   concrete-filled steel tubes (CFST) as compression members is widely used
   in construction industries due to the enhanced structural properties and
   economic advantages it provides. Compared to reinforced cement concrete
   (RCC) columns, CFST delivers the economies of an RCC column, resulting
   in significant economies in the overall structure of a building project.
   This paper presents a comparison study through an experimental and a
   finite element (FE) study of HST, CFST and RCC stub columns, for which
   there are currently little research data, which are the focus of the
   present study. A total of six specimens was tested under uniform axial
   compression, and the test observations are fully reported. The ultimate
   loads, load-displacement curves and failure modes from the tests were
   used for the validation of finite element (FE) models developed through
   FE software package Abaqus. Parametric finite element analyses were then
   performed. The results showed that filling the HST columns with concrete
   infill increases the strength capacity of the CFST columns by 2 times
   for slender sections and 2.9 and 2.4 times for stocky sections of square
   and circular sections, respectively. It was also observed that the
   strength capacity of the HST columns is about 0.45 times of RCC columns
   for slender sections and 1.35 times of RCC columns for stocky sections.
   And the strength capacity of the CFST columns is about on an average 1.3
   times of RCC columns for slender sections of steel tubes and 2.8 times
   of RCC columns for stocky sections of steel tubes.},
DOI = {10.1007/s41062-022-01025-1},
Article-Number = {74},
ISSN = {2364-4176},
EISSN = {2364-4184},
Unique-ID = {WOS:000914792800001},
}

@inproceedings{ WOS:000871847900009,
Author = {de Battista, Nicholas and Bravo-Haro, Miguel A. and Kechavarzi, Cedric},
Editor = {Rizzo, P and Milazzo, A},
Title = {Fibre Optic Monitoring Systems in the Cambridge University Civil
   Engineering Building},
Booktitle = {EUROPEAN WORKSHOP ON STRUCTURAL HEALTH MONITORING (EWSHM 2022), VOL 2},
Series = {Lecture Notes in Civil Engineering},
Year = {2023},
Pages = {78-87},
Note = {10th European Workshop on Structural Health Monitoring (EWSHM), Univ
   Palermo, Dept Engn, Palermo, ITALY, JUL 04-07, 2022},
Organization = {Univ Palermo; Univ Pittsburgh; Kinemetrics; Vallen Systeme; Confederat
   Francaise Essais Non Destructifs; MDPI, Sensors; Springer; ST; Univ
   Studi Napoli Federico II},
Abstract = {The Civil Engineering Building was built between 2017 and 2019 to
   provide new office and laboratory space for the Engineering Department
   at the University of Cambridge, and to house the National Research
   Facility for Infrastructure Sensing (NRFIS). During the construction,
   monitoring systems using distributed fibre optic sensing (DFOS) and
   fibre Bragg grating (FBG) technologies were installed in four different
   parts of the building: ground source heat pump boreholes, the basement
   raft slab and retaining walls, the structures lab strong floor slab, and
   the steel columns and beams of one of the building's load-bearing
   frames. Software packages to acquire, store, analyse and visualise data
   in real time were also developed as part of the monitoring systems. The
   monitoring systems are described in this paper, along with some of the
   initial data recorded during and soon after construction.},
DOI = {10.1007/978-3-031-07258-1\_9},
ISSN = {2366-2557},
EISSN = {2366-2565},
ISBN = {978-3-031-07258-1; 978-3-031-07257-4},
Unique-ID = {WOS:000871847900009},
}

@inproceedings{ WOS:000870223800009,
Author = {Rossi, Andrea and Deetman, Arjen and Stefas, Alexander and Gobert,
   Andreas and Eppinger, Carl and Ochs, Julian and Tessmann, Oliver and
   Eversmann, Philipp},
Editor = {Gengnagel, C and Baverel, O and Betti, G and Popescu, M and Thomsen, MR and Wurm, J},
Title = {An Open Approach to Robotic Prototyping for Architectural Design and
   Construction},
Booktitle = {TOWARDS RADICAL REGENERATION},
Year = {2023},
Pages = {96-107},
Note = {8th Design Modelling Symposium on Towards Radical Regeneration, Univ
   Arts, Berlin, GERMANY, SEP 26-28, 2022},
Abstract = {Emerging from research in computational design and digital fabrication,
   the use of robot arms in architecture is now making its way in the
   practice of construction. However, their increasing diffusion has not
   yet corresponded to the development of shared approaches covering both
   digital (programming and simulation) and physical (end-effector design,
   system integration, IO communication) elements of robotic prototyping
   suited to the unique needs of architectural research. While parallel
   research streams defined various approaches to robotic programming and
   simulation, they all either (A) rely on custom combinations of software
   packages, or (B) are built on top of advanced robotic programming
   environments requiring a higher skill level in robotics than
   conventionally available in an architectural context. This paper
   proposes an alternative open-source toolkit enabling an intuitive
   approach to the orchestration of various hardware and software
   components required for robotic fabrication, including robot programming
   and simulation, end-effector design and actuation, and communication
   interfaces. The pipeline relies on three components: Robot Components, a
   plug-in for intuitive robot programming; Funken, a serial protocol
   toolkit for interactive prototyping with Arduino; and a flexible
   approach to end-effector design. The paper describes these components
   and demonstrates their use in a series of case studies, showing how they
   can be adapted to a variety of project typologies and user skills, while
   keeping highly complex and specific functionality available as an
   option, yielding good practices for a more intuitive translation from
   design to production.},
DOI = {10.1007/978-3-031-13249-0\_9},
ISBN = {978-3-031-13249-0; 978-3-031-13248-3},
ORCID-Numbers = {Eversmann, Philipp/0000-0003-0200-6433},
Unique-ID = {WOS:000870223800009},
}

@article{ WOS:000882458300011,
Author = {Taghavi, M. and Salarian, H. and Ghorbani, B.},
Title = {Economic Evaluation of a Hybrid Hydrogen Liquefaction System Utilizing
   Liquid Air Cold Recovery and Renewable Energies},
Journal = {RENEWABLE ENERGY RESEARCH AND APPLICATIONS},
Year = {2023},
Volume = {4},
Number = {1},
Pages = {125-143},
Month = {JAN},
Abstract = {Liquefaction systems are among the physical techniques of hydrogen (H2)
   storage with a high specific power consumption (SPC), a high
   manufacturing cost, and inevitable boil-off losses. Liquid air cold
   (LAC) recovery is among the strategies that could be used to reduce the
   energy consumption of these systems. The present work economically
   evaluates a combined hydrogen liquefaction configuration using combined
   heat and power (CHP) system, photovoltaic cell (PVC) unit, and liquid
   air energy recovery for pre-cooling under climatic states of Yazd, Iran.
   The LAC recovery is used to pre-cool hydrogen. Moreover, the cascade
   refrigeration systems with helium and hydrogen refrigerants are employed
   to supply refrigeration and liquefaction. Liquid air along with natural
   gas enters CHP after cold energy recovery and compression, and supplies
   a part of the power demand of the liquefaction structure. The rest of
   the power required for refrigeration cycles to liquefy hydrogen is
   supplied by the PVC unit. This integrated structure generates liquid
   hydrogen by receiving 5559 kW of power from PVC unit, 60.79 kg/h of
   natural gas, 8000 kg/h of liquid air, and 1028 kg/h of gaseous hydrogen
   as the inputs. The annualized cost of the configuration (ACC) is applied
   to economically evaluate the hydrogen liquefaction system using
   renewable energies. The developed integrated structure is economically
   evaluated by the HYSYS V10 software and the m-file code in the MATLAB
   package. The economic research results of the hybrid cycle indicate the
   period of return (POR), prime price of liquid hydrogen production, and
   additive value (AV) are 4.249 years, 5.432 USD/kg LH2, and 1.567 USD/kg
   LH2, respectively. The economic sensitivity examination of the combined
   system reveals POR increases from 2.295 to 13.97 years and net annual
   profit decreases from 32.66 to 5.366 MMUSD/year by increasing the
   gaseous hydrogen cost from 1.4 to 3.4 USD/kg LH2. Moreover, POR
   increases from 2.753 to 25.07 years, and the levelized cost of product
   increases from 5.02 to 7.488 US\$/kg LH2 by increasing the capital cost
   from 52.5 to 217.5 MMUSD.},
DOI = {10.22044/rera.2022.11899.1122},
ISSN = {2717-252X},
EISSN = {2676-7430},
ResearcherID-Numbers = {Taghavi, Masoud/AAQ-7929-2021},
ORCID-Numbers = {Taghavi, Masoud/0000-0001-9794-2608},
Unique-ID = {WOS:000882458300011},
}

@inproceedings{ WOS:000922054500031,
Author = {Zhu, Shuai and Zhou, Xiaojian and Xu, Sheng and Chen, Enguo and Ye, Yun
   and Yan, Qun and Guo, Tailiang},
Editor = {Wang, Y and Kidger, TE and Wu, R},
Title = {Design of Metasurface Skin Cloak Based on A Low-complexity Monitoring
   Model and Deep Learning},
Booktitle = {OPTICAL DESIGN AND TESTING XII},
Series = {Proceedings of SPIE},
Year = {2023},
Volume = {12315},
Note = {Conference on Optical Design and Testing XII, ELECTR NETWORK, DEC 05-11,
   2022},
Organization = {SPIE; Chinese Opt Soc},
Abstract = {This work proposes a research scheme to speed up the design of
   metasurface skin cloak through low-complexity phase monitoring model and
   deep learning.This skin cloak conceals a three-dimensional arbitrarily
   shaped object by complete restoration of the phase of the reflected
   light at specific wavelength. And the possibility of realizing spectral
   prediction by deep learning is analyzed.During the study, a phase
   monitoring system was designed in which the detector, the light source
   and the monitored nano-antenna were sequentially distributed at equal
   distances from the emitted wavelength of the light source, so that the
   monitored phase amount was exactly equal to the phase change before the
   reflected wave, thus eliminating the need for multiple monitors to
   measure and calculate the phase change before and after the
   reflection.The traditional metasurface design is usually constructed by
   manual library construction based on the phase distribution and the
   relationship between phase variation and dimensional variation of the
   cell structure, so this work combines the aforementioned monitoring
   model with deep learning to generate the database required for
   modeling.The two variable parameters of device length and width were
   first defined, and the reflected wavefront phase change used as the
   optical response, and we reprocessed the original data and finally build
   and trained an artificial neural network model for forward prediction of
   optical response.This network can obtain its MSE below 0.001 for the
   test set after the training is completed. Thus the scheme can replace
   the role of simulation software to some extent, and its prediction
   process can be completed in a few milliseconds, improving the efficiency
   of the design metasurface process.},
DOI = {10.1117/12.2642101},
Article-Number = {1231517},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-5697-0; 978-1-5106-5696-3},
Unique-ID = {WOS:000922054500031},
}

@article{ WOS:000906349500001,
Author = {Adekunle, Peter and Aigabvboa, Clinton and Thwala, Wellington and
   Akinradewo, Opeoluwa and Oke, Ayodeji},
Title = {Challenges confronting construction information management},
Journal = {FRONTIERS IN BUILT ENVIRONMENT},
Year = {2022},
Volume = {8},
Month = {DEC 19},
Abstract = {This paper aims to present the results of an investigation of the
   prevailing challenges in construction information management. The study
   implemented a quantitative survey methodology, using a questionnaire to
   gather data from architects, civil engineers, quantity surveyors,
   mechanical and electrical engineers, construction managers, and project
   managers. Data analysis was conducted using the SPSS software package:
   applicable measures of dispersal were computed and inferential
   statistical tests performed. The study revealed that information
   management is a significant aspect of construction procedures and that a
   well-structured information system must be in place to achieve success
   in the construction domain. Additionally, the researchers discovered
   that construction information management could be affected by both
   internal and external factors within an organization. Further findings
   revealed that the major challenges in construction information
   management are long-term reliance on legacy systems, a lack of
   technological equipment, leadership development, poor financial
   investment in infrastructure for data management, and the implementation
   of appropriate policies by management. The primary contribution of this
   study lies in its appraisal of the difficulties facing construction
   information management and its identification of the relevant
   challenges, which can help in the proposal of solutions to improve
   methods of managing construction information, in turn producing improved
   performance and more efficient delivery of services on the part of
   professionals within the construction industry.},
DOI = {10.3389/fbuil.2022.1075674},
Article-Number = {1075674},
EISSN = {2297-3362},
Unique-ID = {WOS:000906349500001},
}

@article{ WOS:000896584500004,
Author = {Zhou, Tianchun and Lin, Yuping and Xu, Feng and Ma, Xiaoxia and Wang, Na
   and Ding, Yan},
Title = {Factors influencing physical inactivity status among chinese pregnant
   women: a cross-sectional study},
Journal = {BMC PUBLIC HEALTH},
Year = {2022},
Volume = {22},
Number = {1},
Month = {DEC 9},
Abstract = {Background: Regular prenatal physical activity provides numerous health
   benefits to both mother and fetus. However, little is known about the
   physical activity status of pregnant women in China and whether they
   meet the current guidelines for prenatal physical activity. The aims of
   the study were to assess physical inactivity status and associated
   factors among pregnant women in Shanghai, China. Methods: A
   cross-sectional study of 1636 pregnant women were recruited at a
   tertiary obstetrics and gynecology hospital in Shanghai. Maternal
   sociodemographic characteristics and health information were obtained
   using structured questionnaires or from the electronic medical records.
   Physical inactivity status was assessed using the International Physical
   Activity Questionnaire-Short Form. Factors pertinent to physical
   inactivity were identified by binary logistic regression and were
   reported with adjusted odds ratios (ORs) and 95\% confidence intervals
   (CIs). All statistical analyses were performed using the SPSS software
   package. Results: In total, the prevalence of physical inactivity was
   47.5\%. Walking was the main form of physical activity and only 2.8\% of
   the pregnant women achieved the goal of at least 150 min of
   moderate-intensity physical activity weekly. Multivariate logistic
   regression identified a significant negative association of physical
   inactivity with personal monthly income (adjusted OR 0.648, 95\% CI
   0.505-0.831), engagement in regular exercise before pregnancy (adjusted
   OR 0.575, 95\% CI 0.464-0.711) and in the second (adjusted OR 0.534,
   95\% CI 0.411-0.693) or third (adjusted OR 0.615, 95\% CI 0.470-0.806)
   trimester of pregnancy. Women with nausea or vomiting during pregnancy
   were more likely to be physically inactive during pregnancy (adjusted OR
   1.307, 95\% CI 1.002-1.705). Conclusion: Physical inactivity is highly
   prevalent among pregnant women in China. Further efforts should be taken
   to overcome the barriers to prenatal physical activity and to promote
   moderate- to vigorous-intensity activities among Chinese pregnant women.},
DOI = {10.1186/s12889-022-14757-7},
Article-Number = {2310},
EISSN = {1471-2458},
Unique-ID = {WOS:000896584500004},
}

@article{ WOS:000893305100001,
Author = {Maganga, Deusdedith Pastory and Taifa, Ismail W. R.},
Title = {The readiness of manufacturing industries to transit to Quality 4.0},
Journal = {INTERNATIONAL JOURNAL OF QUALITY \& RELIABILITY MANAGEMENT},
Abstract = {PurposeThis research provides the essential aspects of the transition
   from traditional quality methods like total quality management, quality
   assurance and quality control to a new quality approach linked with the
   Industry 4.0 era. The purpose of this paper is to address this
   issue.Design/methodology/approachThe study used a survey method to
   obtain the practitioners' perceptions of the Quality 4.0 (Q4.0)
   concepts. Both closed-ended and open-ended structured questionnaires
   assessed the perceptions of respondents regarding manufacturers'
   readiness and Q4.0 awareness to transition to Q4.0. Non-probability and
   purposive sampling tactics selected 15 Tanzanian manufacturing
   industries (TMIs). Garnered data were scrutinised quantitatively and
   qualitatively utilising Minitab (R) 20, SmartPLS 3.3.7 and MAXQADA 2020
   software packages.FindingsThe results indicate that TMIs are equipped to
   deploy the Q4.0 approach because industrialists are familiar with the
   concept's characteristics and dimensions and the benefits of
   implementing Q4.0. Most TMIs utilise a Q3.0 method for managing quality,
   while some manufacturing industries have begun to apply Q4.0 leveraging
   technologies. The study revealed several factors influencing Q4.0
   readiness in TMIs, including leveraged technology adoption, training,
   Q4.0 skills, infrastructures, the government set-up, top management
   support, Q4.0 strategy and vision, collaboration, awareness, knowledge
   of Q4.0, customer and supplier centeredness and organisational
   culture.Research limitations/implicationsThe implication of this study
   is on Q4.0 awareness creation so that industries can grab the advantages
   of Q4.0 leveraged technologies. Another implication is that
   organisational readiness factors identified in this study are critical
   for the effective adoption of Q4.0. The highlighted influences may be
   utilised as indications to determine an organisation's readiness to
   transition to the Q4.0 approach. This research was limited to TMIs,
   excluding service firms, mining, and the building and construction
   industry due to differences in their mode of
   operation.Originality/valueDetermining readiness factors and awareness
   for the Q4.0 study is probably the first amongst the seven East African
   countries, including Tanzania. This study thus bridges a huge gap in
   fulfilling the need of this research type.},
DOI = {10.1108/IJQRM-05-2022-0148},
EarlyAccessDate = {DEC 2022},
ISSN = {0265-671X},
EISSN = {1758-6682},
ResearcherID-Numbers = {Andrea Simões Braga, Francisco/GRS-0157-2022
   },
ORCID-Numbers = {Maganga, Deusdedith Pastory/0000-0003-1714-9525},
Unique-ID = {WOS:000893305100001},
}

@article{ WOS:000870353100004,
Author = {Gong, Xiaogang and Rong, Guang and Wang, Zhiyong and Zhang, Ayuan and
   Li, Xiaoke and Wang, Lepeng},
Title = {Baduanjin exercise for patients with breast cancer: A systematic review
   and meta-analysis},
Journal = {COMPLEMENTARY THERAPIES IN MEDICINE},
Year = {2022},
Volume = {71},
Month = {DEC},
Abstract = {Objective: Baduanjin exercise is a traditional Chinese mind-body
   exercise routine characterized by slow, coor-dinated, and sequential
   movements. We have performed the first meta-analysis on the main effect
   of Baduanjin exercise in patients with breast cancer.Methods: This study
   followed the 2020 PRISMA guideline. We searched for randomized
   controlled trials in PubMed, Embase, Cochrane Library, Web of Science,
   Clinical Trials.gov, Chinese National Knowledge Infra-structure, Wanfang
   Data Information Site, Chinese Biomedical Database, and Chinese Science
   and Technique Journals Database through 31 August 2022. Data were
   analyzed for the outcomes of quality of life, anxiety, and depression.
   Review Manager 5.4 software was used for data analysis.Results: Seven
   randomized controlled trials with a total of 537 patients with breast
   cancer were examined. Compared with the control therapies, Baduanjin
   exercise significantly improved the total quality of life score (SMD =
   0.83; 95 \% CI, 0.58-1.08; P < 0.00001) and in two associated domains:
   emotional well-being (SMD = 0.67; 95 \% CI, 0.26-1.07; P = 0.001),
   functional well-being (SMD = 0.55; 95 \% CI, 0.30-0.79; P < 0.00001) and
   breast cancer subscale (SMD = 0.39; 95 \% CI, 0.02-0.77; P = 0.04).
   Meanwhile, it significantly reduced anxiety score (SMD = -0.60; 95 \%
   CI, -1.15 to -0.05; P = 0.03) and in depression score (SMD = -0.70 95 \%
   CI, -0.97 to -0.42; P < 0.00001). None adverse event was
   reported.Conclusion: The meta-analysis suggests that Baduanjin exercise
   is an effective and safe exercise for improving quality of life and
   alleviating depression and anxiety in patients with breast cancer.
   Significant methodological concerns of the included studies limit the
   interpretation of the results. For future trials of Baduanjin exercise
   on BC, we highlight the importance of adopting more rigorous study
   design in terms of assessor blinding, hy-pothesis/purpose blinding,
   allocation concealment, objective outcome selection and consistent
   reporting of adverse events.},
DOI = {10.1016/j.ctim.2022.102886},
Article-Number = {102886},
ISSN = {0965-2299},
EISSN = {1873-6963},
ResearcherID-Numbers = {Gong, Xiaogang/HLQ-3111-2023
   },
ORCID-Numbers = {Gong, Xiaogang/0000-0002-5209-4534},
Unique-ID = {WOS:000870353100004},
}

@article{ WOS:000902592700001,
Author = {Stajcic, Ivana and Stajcic, Aleksandar and Serpa, Cristina and
   Vasiljevic-Radovic, Dana and Randjelovic, Branislav and Radojevic, Vesna
   and Fecht, Hans},
Title = {Microstructure of Epoxy-Based Composites: Fractal Nature Analysis},
Journal = {FRACTAL AND FRACTIONAL},
Year = {2022},
Volume = {6},
Number = {12},
Month = {DEC},
Abstract = {Polymers and polymer matrix composites are commonly used materials with
   applications extending from packaging materials to delicate electronic
   devices. Epoxy resins and fiber-reinforced epoxy-based composites have
   been used as adhesives and construction parts. Fractal analysis has been
   recognized in materials science as a valuable tool for the
   microstructural characterization of composites by connecting fractal
   characteristics with composites' functional properties. In this study,
   fractal reconstructions of different microstructural shapes in an
   epoxy-based composite were performed on field emission scanning electron
   microscopy (FESEM) images. These images were of glass fiber reinforced
   epoxy as well as a hybrid composite containing both glass and
   electrospun polystyrene fibers in an epoxy matrix. Fractal
   reconstruction enables the identification of self-similarity in the
   fractal structure, which represents a novelty in analyzing the fractal
   properties of materials. Fractal Real Finder software, based on the
   mathematical affine fractal regression model, was employed to
   reconstruct different microstructure shapes and calculate fractal
   dimensions to develop a method of predicting the optimal
   structure-property relations in composite materials in the future.},
DOI = {10.3390/fractalfract6120741},
Article-Number = {741},
EISSN = {2504-3110},
ResearcherID-Numbers = {Vasiljevic-Radovic, Dana/C-8034-2016
   },
ORCID-Numbers = {Vasiljevic-Radovic, Dana/0000-0002-7609-8599
   Stajcic, Ivana/0000-0001-8278-4491},
Unique-ID = {WOS:000902592700001},
}

@article{ WOS:000889052500002,
Author = {Zhang, Yang and Wang, Shuqing and Wang, Chengjun and Luo, Xiaomeng},
Title = {Risk identification and analysis for the green redevelopment of
   industrial brownfields: a social network analysis},
Journal = {ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH},
Abstract = {The green redevelopment of industrial brownfields (GRIB) is an important
   means to solve the shortage of urban land resources and realize
   sustainable urban renewal and green development. The identification and
   analysis of risk factors in GRIB projects are of immense significance
   for completing construction tasks and ensuring the planned benefits. In
   this study, work and risk breakdown structures and expert interviews are
   used to construct the risk network model of a GRIB project, based on
   three dimensions: process, subject, and system sources. The software
   package UCINET is used to conduct social network analysis and determine
   the key risk factors and relationships. The results of this study
   suggest that the four risk factors with the most brokerage roles and the
   highest node betweenness centralities are located at the core of the
   network; the six risk relationships with the highest line betweenness
   centralities are those with the strongest transmission capacities; the
   key risk factors are mostly response and stress risks; the main source
   is the design unit; and the key risk relationships are the influence of
   the decision-making stage on the design stage and of the design stage on
   the construction stage. Surpassing the limitations of traditional linear
   research, this study explains the internal relationship among the risk
   factors of GRIB projects and identifies the risk factors that play a
   brokerage role and the risk relationship that plays a conductive role,
   providing a theoretical basis for introducing social network analysis
   tools into the risk assessment of such complex construction projects.},
DOI = {10.1007/s11356-022-24308-7},
EarlyAccessDate = {NOV 2022},
ISSN = {0944-1344},
EISSN = {1614-7499},
Unique-ID = {WOS:000889052500002},
}

@article{ WOS:000895740900130,
Author = {Cao, Rongkai and Chen, Beibei and Xu, Hui and Cai, Yiyi and Liu, Weicai},
Title = {Accuracy of three-dimensional optical devices for facial soft-tissue
   measurement in clinical practice of stomatology: A PRISMA systematic
   review},
Journal = {MEDICINE},
Year = {2022},
Volume = {101},
Number = {47},
Month = {NOV 25},
Abstract = {Background:The accuracy of 3-dimensional (3D) optical devices for facial
   soft-tissue measurement is essential to the success of clinical
   treatment in stomatology. The aim of the present systematic review was
   to summarize the accuracy of 3D optical devices used for facial
   soft-tissue assessment in stomatology. Methods:An extensive systematic
   literature search was performed in the PubMed/MEDLINE, Embase, Scopus
   and Cochrane Library databases for studies published in the English
   language up to May 2022 in accordance with the Preferred Reporting Items
   for Systematic Reviews and Meta-analyses guidelines. Peer-reviewed
   journal articles evaluating the facial soft-tissue morphology by 3D
   optical devices were included. The risk of bias was performed using the
   Quality Assessment Tool for Diagnostic Accuracy Studies-2 guidelines by
   the 2 reviewers. The potential publication bias was analyzed using the
   Review Manager software. Results:The query returned 1853 results. A
   total of 38 studies were included in this review. Articles were
   categorized based on the principle of devices: laser-based scanning,
   structured-light scanning, stereophotogrammetry and red, green,
   blue-depth camera. Conclusion:Overall, the 3D optical devices
   demonstrated excellent accuracy and reliability for facial soft-tissue
   measurement in stomatology. red, green, blue-depth camera can collect
   accurate static and dynamic 3D facial scans with low cost and high
   measurement accuracy. Practical needs and availability of resources
   should be considered when these devices are used in clinical settings.},
DOI = {10.1097/MD.0000000000031922},
ISSN = {0025-7974},
EISSN = {1536-5964},
Unique-ID = {WOS:000895740900130},
}

@article{ WOS:000884688900001,
Author = {Sharma, Yukti and Silal, Prakrit},
Title = {Mapping the effect of healthy and unhealthy food and beverages
   marketing: two decades of bibliometric analysis},
Journal = {EUROPEAN JOURNAL OF MARKETING},
Year = {2023},
Volume = {57},
Number = {1},
Pages = {149-184},
Month = {JAN 4},
Abstract = {Purpose - With multiple theoretical traditions, diverse topical
   landscape and rapid regulatory advancements galvanising the ongoing
   discourse, the emergent marketing scholarship on healthy and unhealthy
   food and beverages (F \& B) has become exhaustive, fragmented and almost
   non-navigable. Accordingly, this study aims to synthesise and trace two
   decades of research focused on healthy and unhealthy F \& B marketing.
   Design/methodology/approach - This study conducts a bibliometric
   analysis of papers published between 2000 and 2020. The data was
   retrieved from Web of Science (WoS) and Scopus, yielding 338 papers for
   final analysis. Using VOSviewer software and the Biblioshiny package,
   the authors performed a detailed bibliometric analysis comprising
   performance analysis and science mapping. Findings- The study delineated
   the contribution, theoretical and thematic structure of healthy and
   unhealthy F \& B marketing scholarship. The authors also mapped the
   evolution trajectory of the thematic structure, which helped us
   contemplate the research gaps. Research limitations/implications - By
   delving deeper into the ``who ``, ``where ``, ``how ``, ``what `` and
   ``when `` of healthy and unhealthy F \& B marketing, the study enhances
   the current understandings and future developments for both theorists
   and practitioners. However, the selection of literature is confined to
   peer-reviewed papers available in WoS and Scopus. Practical
   implications- The findings delineate the existing scholarship which
   could guide F \& B marketers and policymakers towards designing
   consumer-centric marketing/policy interventions. Originality/value - To
   the best of the authors' knowledge, this study is the first to perform a
   bibliometric analysis of healthy and unhealthy F \& B marketing, likely
   to provide valuable guidelines for future scholars, policymakers and
   practitioners.},
DOI = {10.1108/EJM-09-2021-0717},
EarlyAccessDate = {NOV 2022},
ISSN = {0309-0566},
EISSN = {1758-7123},
ResearcherID-Numbers = {Silal, Prakrit/HMD-6216-2023
   Sharma, Yukti/ADO-5040-2022},
ORCID-Numbers = {Sharma, Yukti/0000-0002-2899-9376},
Unique-ID = {WOS:000884688900001},
}

@article{ WOS:000860229000001,
Author = {Borkowska, Justyna and Roukema, Boudewijn F.},
Title = {Does relativistic cosmology software handle emergent volume evolution?},
Journal = {CLASSICAL AND QUANTUM GRAVITY},
Year = {2022},
Volume = {39},
Number = {21},
Month = {NOV 3},
Abstract = {Several software packages for relativistic cosmological simulations that
   do not fully implement the Einstein equation have recently been
   developed. Two of the free-licensed ones are inhomog and gevolution. A
   key question is whether globally emergent volume evolution that is
   faster than that of a Friedmannian reference model results from the
   averaged effects of structure formation. Checking that emergent volume
   evolution is correctly modelled by the packages is thus needed. We
   numerically replace the software's default random realisation of initial
   seed fluctuations by a fluctuation of spatially constant amplitude in a
   simulation's initial conditions. The average volume evolution of the
   perturbed model should follow that of a Friedmannian expansion history
   that corresponds to the original Friedmannian reference solution
   modified by the insertion of the spatially constant perturbation. We
   derive the equations that convert from the perturbed reference solution
   to the effective solution. We find that inhomog allows emergent volume
   evolution correctly at first order through to the current epoch. For
   initial conditions with a resolution of N = 128(3) particles and an
   initial non-zero extrinsic curvature invariant I- i  = 0.001, inhomog
   matches an exact Friedmannian solution to -0.0058\% (Einstein-de Sitter,
   EdS) or -0.0033\% (?CDM). We find that gevolution models the decaying
   mode to fair accuracy, and excludes the growing mode by construction.
   For N = 128(3) and an initial scalar potential phi = 0.001, gevolution
   is accurate for the decaying mode to 0.012\% (EdS) or 0.013\% (?CDM). We
   conclude that this special case of an exact non-linear solution for a
   perturbed Friedmannian model provides a robust calibration for
   relativistic cosmological simulations.},
DOI = {10.1088/1361-6382/ac8ddb},
Article-Number = {215007},
ISSN = {0264-9381},
EISSN = {1361-6382},
ResearcherID-Numbers = {Roukema, Boudewijn/I-5323-2013},
ORCID-Numbers = {Roukema, Boudewijn/0000-0002-3772-0250},
Unique-ID = {WOS:000860229000001},
}

@article{ WOS:000882403300001,
Author = {Gour, Aditya and Mathews, Tom and Behera, R. P.},
Title = {Approach towards qualification of TCP/IP network components of PFBR},
Journal = {NUCLEAR ENGINEERING AND TECHNOLOGY},
Year = {2022},
Volume = {54},
Number = {11},
Pages = {3975-3984},
Month = {NOV},
Abstract = {Distributed control system architecture is adopted for I\&C systems of
   Prototype Fast Breeder Reactor, where the geographically distributed
   control systems are connected to centralized servers \& display stations
   via switched Ethernet networks. TCP/IP communication plays a significant
   role in the successful operations of this architecture. The
   communication tasks at control nodes are taken care by TCP/IP offload
   modules; local area switched network is realized using layer-2/3
   switches, which are finally connected to network interfaces of
   centralized servers \& display stations. Safety, security, reliability,
   and fault tolerance of control systems used for safety-related
   applications of nuclear power plants is ensured by indigenous design and
   qualification as per guidelines laid down by regulatory authorities. In
   the case of commercially available components, appropriate suitability
   analysis is required for getting the operation clearances from
   regulatory authorities. This paper details the proposed approach for the
   suitability analysis of TCP/IP communication nodes, including control
   systems at the field, network switches, and servers/display stations.
   Development of test platform using commercially available tools and
   diagnostics software engineered for control nodes/display stations are
   described. Each TCP link behavior with impaired packets and multiple
   traffic loads is described, followed by benchmarking of the network
   switch's routing characteristics and security features.(c) 2022 Korean
   Nuclear Society, Published by Elsevier Korea LLC. This is an open access
   article under the CC BY-NC-ND license
   (http://creativecommons.org/licenses/by-nc-nd/4.0/).},
DOI = {10.1016/j.net.2022.06.015},
EarlyAccessDate = {NOV 2022},
ISSN = {1738-5733},
ORCID-Numbers = {Mathews, Tom/0000-0002-4876-8524
   Gour, Aditya/0000-0002-6061-7075},
Unique-ID = {WOS:000882403300001},
}

@article{ WOS:000925067300108,
Author = {Haile, Zewodie and Mengist, Hylemariam Mihiretie and Dilnessa, Tebelay},
Title = {Bacterial isolates, their antimicrobial susceptibility pattern, and
   associated factors of external ocular infections among patients
   attending eye clinic at Debre Markos Comprehensive Specialized Hospital,
   Northwest Ethiopia},
Journal = {PLOS ONE},
Year = {2022},
Volume = {17},
Number = {11},
Month = {NOV 3},
Abstract = {Background External eye infection caused by bacteria can lead to reduced
   vision and blindness. Therefore, pathogen isolation and antimicrobial
   susceptibility testing are vital for the prevention and control of
   ocular diseases.
   Objective The main aim of this study was to assess bacterial isolates,
   their antimicrobial susceptibility pattern, and associated factors of
   external ocular infection (EOI) among patients attended eye clinic at
   Debre Markos Comprehensive Specialized Hospital (DMCSH), Northwest
   Ethiopia.
   Methods We conducted a cross-sectional study in patients with external
   ocular infections from January 1, 2021, to June 30, 2021, at DMCSH.
   Socio-demographic and clinical data were collected using semi-structured
   questionnaires. Following standard protocols, external ocular swabs were
   collected and inoculated onto blood agar, chocolate agar, MacConkey agar
   and mannitol salt agar (MSA). Finally, bacterial isolates were
   identified by Gram stain, colony morphology, and biochemical tests.
   Antimicrobial susceptibility testing was done by using the modified
   Kirby-Bauer disk diffusion technique according to Clinical and
   Laboratory Standards Institute (CLSI) guideline. Cleaned and coded data
   were entered into EpiData version 4.2 software and exported to
   Statistical Packages for Social Sciences (SPSS) version 22 for analysis.
   Bivariate logistic regression was applied to investigate the association
   between predictors and outcome variables. P-values <= 0.05 with 95\%
   confidence interval were considered statistically significant.
   Results Two hundred seven study participants were enrolled in this
   study. More than half of them (57.5\%, 119/207) were males, and 37.7\%
   (78/207) of them were >= 65 years old. A total of 130 (62.8\%) bacterial
   isolates were identified, with Gram-positive bacteria accounting for
   78.5\% (102/130) of the isolates. Staphylococcus aureus was the most
   common isolate with a 46.2\% (60/130) prevalence. Ciprofloxacin was
   comparatively effective against Gram-positive and Gram-negative
   bacteria. The prevalence of culture-confirmed bacteria was significantly
   associated with age groups 15-24 (AOR: 9.18, 95\%CI: 1.01-82.80; P =
   0.049) and 25-64 (AOR: 7.47, 95\%CI: 1.06-52.31; P = 0.043). Being
   farmer (AOR: 5.33, 95\% CI: 1.04-37.33; P = 0.045), previous history of
   eye surgery (AOR: 5.39, 95\% CI: 1.66-17.48; P = 0.005), less frequency
   of face washing (AOR: 5.32, 95\% CI: 1.31-7.23; P = 0.010) and face
   washing once a day (AOR: 3.07, 95\% CI: 1.13-25.13; P = 0.035) were also
   significantly associated with the prevalence of culture-confirmed
   bacteria.
   Conclusion The prevalence of culture-confirmed bacteria among patients
   with EOI was high in the study area. A considerable proportion of
   bacterial isolates exhibited mono and/or multi-drug resistance. Age
   (15-64 years), being farmer, previous history of eye surgery and less
   frequency of face washing were significantly associated with the
   prevalence of culture-confirmed bacteria. Bacterial isolation and
   antibiotic susceptibility testing should be routinely performed in the
   study area to combat the emergence of antibiotic resistance.},
DOI = {10.1371/journal.pone.0277230},
Article-Number = {e0277230},
ISSN = {1932-6203},
ORCID-Numbers = {Dilnessa, Tebelay/0000-0002-3713-6783},
Unique-ID = {WOS:000925067300108},
}

@article{ WOS:000875791500001,
Author = {Kharchenko, Volodymyr and Grekhov, Andrii and Kondratiuk, Vasyl},
Title = {Packet Losses in SAGIN with Artificial Intelligence},
Journal = {INTERNATIONAL JOURNAL OF WIRELESS INFORMATION NETWORKS},
Abstract = {Modern solutions based on Artificial Intelligence (AI) play an important
   role in the management of drone's resources in
   Space-Air-Ground-Integrated-Network (SAGIN). AI can use information
   collected by drone sensors to develop routing protocols, optimize
   communication networks, improve energy efficiency, and predict user
   behavior. In this regard, the analysis of data loss in SAGIN with AI is
   relevant. This work is devoted to the calculation of packet losses in
   SAGIN, containing additional hardware of AI system. Based on the
   original model containing a Base Station (BS), a stratospheric Remotely
   Piloted Air System (RPAS) with an AI system, a low-orbit satellite, a
   low-altitude RPAS and a user of a terrestrial cellular network, data
   traffic was simulated using NetCracker Professional 4.1 software. The AI
   system was simulated by a cloud structure with the ability to change the
   delay and the probability of packet losses. Quantitative characteristics
   of traffic in SAGIN channels with such a model of the AI hardware system
   are obtained. The dependences of packets losses on the size of messages
   and the data transfer rate are calculated. The dependences of BS uplink
   Average Load and the packets travel time on the TS, as well as the
   dependences of the Bit Error Rate (BER) on the Average Load, are
   obtained. The results are valuable in terms of practical guidelines for
   choosing data transfer modes and the necessary hardware parameters for
   an AI system.},
DOI = {10.1007/s10776-022-00579-2},
EarlyAccessDate = {OCT 2022},
ISSN = {1068-9605},
EISSN = {1572-8129},
ORCID-Numbers = {Kondratiuk, Vasyl/0000-0002-5690-8873
   Grekhov, Andrii/0000-0001-7685-8706},
Unique-ID = {WOS:000875791500001},
}

@article{ WOS:000882075600001,
Author = {Huang, Huanhuan and Chen, Zhiyu and Chen, Lijuan and Cao, Songmei and
   Bai, Dingqun and Xiao, Qian and Xiao, Mingzhao and Zhao, Qinghua},
Title = {Nutrition and sarcopenia: Current knowledge domain and emerging trends},
Journal = {FRONTIERS IN MEDICINE},
Year = {2022},
Volume = {9},
Month = {OCT 26},
Abstract = {ObjectiveNon-pharmacological management like nutrient supplements has
   shown positive impacts on muscle mass and strength, which has burgeoned
   clinical and research interest internationally. The aim of this study
   was to analyze the current knowledge domain and emerging trends of
   nutrition-related research in sarcopenia and provide implications for
   future research and strategies to prevent or manage sarcopenia in the
   context of aging societies. Materials and methodsNutrition- and
   sarcopenia-related research were obtained from the Web of Science Core
   Collection (WoSCC) database from its inception to April 1, 2022.
   Performance analysis, science mapping, and thematic clustering were
   performed by using the software VOSviewer and R package
   ``bibliometrix.{''} Bibliometric analysis (BA) guideline was applied in
   this study. ResultsA total of 8,110 publications were extracted and only
   7,510 (92.60\%) were selected for final analysis. The production trend
   in nutrition and sarcopenia research was promising, and 1,357 journals,
   107 countries, 6,668 institutions, and 31,289 authors were identified in
   this field till 2021. Stable cooperation networks have formed in the
   field, but they are mostly divided by region and research topics. Health
   and sarcopenia, metabolism and nutrition, nutrition and exercise, body
   compositions, and physical performance were the main search themes.
   ConclusionsThis study provides health providers and scholars mapped out
   a comprehensive basic knowledge structure in the research in the field
   of nutrition and sarcopenia over the past 30 years. This study could
   help them quickly grasp research hotspots and choose future research
   projects.},
DOI = {10.3389/fmed.2022.968814},
Article-Number = {968814},
EISSN = {2296-858X},
Unique-ID = {WOS:000882075600001},
}

@article{ WOS:000878773400002,
Author = {Wang, Jia-bin and Wang, Yan-xia and Li, Fen and Li, Yan-fei and Li,
   Xing-li and Huang, Pei-yao and Wang, Chen and Wang, Meng and Qiu, Jie
   and Yang, Ke-hu and Liu, Lu and Mao, Bao-hong and Li, Hong-pu and Liu,
   Xu-dong and Li, Fu-yun and Cui, Xu-dong and Wang, Peng-ju and Liu,
   Wen-bo},
Title = {The efficacy of Da Chaihu decoction combined with metformin tablets for
   type 2 diabetes mellitus: A systematic review and meta-analysis},
Journal = {COMPLEMENTARY THERAPIES IN MEDICINE},
Year = {2022},
Volume = {71},
Month = {DEC},
Abstract = {Objective: To assess the efficacy of Da Chaihu decoction combined with
   metformin tablets on patients with type 2 diabetes compared with
   metformin alone.Methods: This systematic review and meta-analysis is
   written based on 2020 PRISMA Extension for Chinese Herbal Medicines 2020
   (PRISMA-CHM 2020) reporting guidelines. We reviewed all the relevant
   studies from a search of the following databases from inception to
   February 2022 without any language restriction: Excerpta Medica Database
   (EMBASE), Google Scholar, PubMed, Cochrane Library, China National
   Knowledge Infra-structure (CNKI), VIP Information, Wanfang Data, and the
   Chinese Biomedical Literature Database(CBM). Data were extracted and the
   quality was independently evaluated by two reviewers, based on the
   inclusion and exclusion criteria. Data were analyzed using the Cochrane
   software RevMan 5.3.Results: Six randomized controlled trials comprising
   516 participants were included. The meta-analysis revealed the Da Chaihu
   decoction combined with metformin tablets group was significantly
   superior to the metformin tablets group in terms of fasting blood
   glucose(FPG) (-0.66 mmol/L; 95 \% CI (con-fidence intervals) {[}-1.28,
   -0.04]), plasma glucose 2 h after meal (2-h PG) (-1.18 mmol/L; 95 \% CI
   {[}-1.94,-0.42]) in six RCTs, body mass index (BMI) (-3.07 mmol/L; 95 \%
   CI {[}-6.89, 0.75]) in three RCTs, glycosylated hemoglobin (HbAlc)
   (-0.36 mmol/L; 95 \% CI {[}-1.04, 0.31]) in three RCTs, and
   triglycerides (TG) (-0.76 mmol/L; 95 \% CI {[}-1.37,-0.15]) in two RCTs.
   In two RCTs, there were significant differences in terms of total
   cholesterol (TC) (-0.97 mmol/L; 95 \% CI {[}-1.18,-0.76]). Conclusions:
   Very low-quality research shows that Da Chaihu decoction combined with
   metformin tablets exert a certain level of efficacy on patients with
   type 2 diabetes compared with metformin alone. However, random sequence
   generation methodology was reported in five studies leading to the low
   quality of the included studies. None of the six studies depicted the
   blinding method, allocation concealment, selective reporting, and
   assessed the purity and potency of the product. This observation
   requires verification through high-quality, multi-center, double-blinded
   randomized controlled trials, and assesses the purity and potency of the
   product.},
DOI = {10.1016/j.ctim.2022.102894},
EarlyAccessDate = {OCT 2022},
Article-Number = {102894},
ISSN = {0965-2299},
EISSN = {1873-6963},
ResearcherID-Numbers = {liu, xudong/HJG-8137-2022},
Unique-ID = {WOS:000878773400002},
}

@article{ WOS:000870625000001,
Author = {Lapina, Evgeniia and Oumaziz, Paul and Bouclier, Robin and Passieux,
   Jean-Charles},
Title = {A fully non-invasive hybrid IGA/FEM scheme for the analysis of localized
   non-linear phenomena},
Journal = {COMPUTATIONAL MECHANICS},
Year = {2023},
Volume = {71},
Number = {2},
Pages = {213-235},
Month = {FEB},
Abstract = {This work undertakes to combine the interests of IsoGeometric Analysis
   (IGA) and standard Finite Element Methods (FEM) for the global/local
   simulation of structures. The idea is to adopt a hybrid
   global-IGA/local-FEM modeling, thereby benefiting from: (i) the superior
   geometric description and per-Degree-Of-Freedom accuracy of IGA for
   capturing global, regular responses, and (ii) the ability of FEM to
   compute local, strongly non-linear or even singular behaviors. For the
   sake of minimizing the implementation effort, we develop a coupling
   scheme that is fully non-invasive in the sense that the initial global
   spline model to be enriched is never modified and the construction of
   the coupling operators can be performed using conventional FE packages.
   The key ingredient is to express the FEM-to-IGA bridge, based on Bezier
   extraction, to transform the initial global spline interface into a FE
   one on which the local FE mesh can be constructed. This allows to resort
   to classic FE trace operators to implement the coupling. It results in a
   strategy that offers the opportunity to simply couple an isogeometric
   code with any robust FE code suitable for the modelling of complex local
   behaviors. The method also easily extends in case the users only have at
   their disposal FE codes. This is the situation that is considered for
   the numerical illustrations. More precisely, we only make use of the FE
   industrial software Code\_Aster to perform efficiently and accurately
   the hybrid global-IGA/local-FEM simulation of structures subjected
   locally to cracks, contact, friction and delamination.},
DOI = {10.1007/s00466-022-02234-2},
EarlyAccessDate = {OCT 2022},
ISSN = {0178-7675},
EISSN = {1432-0924},
ResearcherID-Numbers = {Passieux, Jean-Charles/B-8909-2012},
ORCID-Numbers = {Passieux, Jean-Charles/0000-0001-8387-4122},
Unique-ID = {WOS:000870625000001},
}

@article{ WOS:000874668400001,
Author = {Ribeiro, Tiago and Bernardo, Luis and Carrazedo, Ricardo and De
   Domenico, Dario},
Title = {Eurocode-compliant topology optimisation of steel moment splice
   connections},
Journal = {JOURNAL OF BUILDING ENGINEERING},
Year = {2022},
Volume = {62},
Month = {DEC 15},
Abstract = {To address the meagre use of Topology Optimisation (TO) in the civil
   engineering industry, the case of bolted moment connections in steel
   construction has been investigated. Connections are essential in steel
   structures, contributing up to 25\% of the global weight and
   concentrating the complexity in design and, therefore, the industry's
   added value. It has been found that compliance with code requirements
   and difficulties in employing advanced software packages in real,
   heterogeneous and complex connections are the two major deterrents to
   the widespread use of TO. Hence, a novel code-compliant methodology was
   proposed, applied to the cover flange plate under tension of a bolted
   beam splice moment connection and validated with non-linear Finite
   Element Analyses (NLFEA). It has been found that a 50\% volume reduction
   is possible by employing linear elastic TO and establishing the
   non-exceedance of the steel ultimate stress as a criterion. NLFEA showed
   that if the original joint capacity is to be maintained, the maximum
   optimisation threshold reduces only 45\% of the initial volume. Two
   critical conclusions are that linear elastic TO could not meet the
   safety needs and that the introduced validation stage with NLFEA is an
   essential step, highlighting the novelty and significance of the
   proposed method. Topologically optimised solutions showed a significant
   volume and cost reduction, meaningfully contributing to the steel
   construction decarbonisation goals and leading to joints with a more
   ductile behaviour.},
DOI = {10.1016/j.jobe.2022.105346},
EarlyAccessDate = {OCT 2022},
Article-Number = {105346},
EISSN = {2352-7102},
ResearcherID-Numbers = {Carrazedo, Ricardo/I-7315-2013
   },
ORCID-Numbers = {Carrazedo, Ricardo/0000-0002-9830-7777
   De Domenico, Dario/0000-0003-1279-9529},
Unique-ID = {WOS:000874668400001},
}

@article{ WOS:000871081000007,
Author = {Bazhenov, V. E. and Baranov, I. I. and Titov, A. Yu and Sannikov, V, A.
   and Ozherelkov, D. Yu and Lyskovich, A. A. and Koltygin, V, A. and
   Belov, V. D.},
Title = {Effect of Ti, Sr, and B Addition on the Fluidity of A356.2 Grade
   Aluminum Alloy},
Journal = {RUSSIAN JOURNAL OF NON-FERROUS METALS},
Year = {2022},
Volume = {63},
Number = {5},
Pages = {526-536},
Month = {OCT},
Abstract = {At the present time, aluminum alloys with silicon are the most
   widespread construction materials. In order to increase the mechanical
   properties of aluminum alloys, modifying with Sr, Ti, and B is used.
   However, in the foundries, when using scrap and secondary aluminum
   alloys, the modifying elements are accumulated in alloys in the form of
   intermetallic particles, which can lead to a decrease in the level of
   castability. This is connected with the fact that the used modifiers
   exert a short-term effect and cannot be activated upon remelting. Hence
   it is necessary to add the modifiers without taking into account the
   intermetallic particles already contained in the melt. This paper is
   devoted to studies on the effect of additions of Sr, Ti, and B on the
   fluidity of an A356.2 grade aluminum alloy determined by means of vacuum
   fluidity testing. It is shown that, when AlSr10 and AlTi5B1 commercial
   master alloys are used (containing up to 0.3 wt \% Sr and 0.5 wt \% Ti),
   no decrease in fluidity is observed. However, adding the same amount of
   Ti with the use of a homemade AlTi4 master alloy leads to a considerable
   decrease in the fluidity. With the help of scanning electron microscopy
   (SEM) and energy-dispersive X-ray spectroscopy (EDS), the microstructure
   and phase composition of master alloys and of an A356.2 grade alloy
   after adding the mentioned master alloys have been investigated.
   Additionally, the Thermo-Calc software package has been used to evaluate
   the effect of modifier addition exerted on the phase composition and
   phase transition temperature of the alloy. It has been established that
   the effect of the modifier addition on the fluidity of the A356.2 grade
   alloy is connected with the shape and size of crystals containing the
   modifying elements in the master alloy structure. When there are coarse
   crystals formed by such phases, it is quite possible that the crystals
   are dissolved incompletely, which could hinder the free flow of melt.},
DOI = {10.3103/S1067821222050029},
ISSN = {1067-8212},
EISSN = {1934-970X},
ResearcherID-Numbers = {Bazhenov, V.E./K-9978-2013
   },
ORCID-Numbers = {Bazhenov, V.E./0000-0003-3214-1935
   Belov, Vladimir/0000-0003-3607-8144
   Baranov, Ivan/0000-0002-0465-7865
   Lyskovich, Anastasia/0000-0002-8490-4829},
Unique-ID = {WOS:000871081000007},
}

@article{ WOS:000861167000001,
Author = {Muraru, Denisa and Gavazzoni, Mara and Heilbron, Francesca and Mihalcea,
   Diana J. and Guta, Andrada C. and Radu, Noela and Muscogiuri, Giuseppe
   and Tomaselli, Michele and Sironi, Sandro and Parati, Gianfranco and
   Badano, Luigi P.},
Title = {Reference ranges of tricuspid annulus geometry in healthy adults using a
   dedicated three-dimensional echocardiography software package},
Journal = {FRONTIERS IN CARDIOVASCULAR MEDICINE},
Year = {2022},
Volume = {9},
Month = {SEP 13},
Abstract = {BackgroundTricuspid annulus (TA) sizing is essential for planning
   percutaneous or surgical tricuspid procedures. According to current
   guidelines, TA linear dimension should be assessed using two-dimensional
   echocardiography (2DE). However, TA is a complex three-dimensional (3D)
   structure. AimIdentify the reference values for TA geometry and dynamics
   and its physiological determinants using a commercially available
   three-dimensional echocardiography (3DE) software package dedicated to
   the tricuspid valve (4D AutoTVQ, GE). MethodsA total of 254 healthy
   volunteers (113 men, 47 +/- 11 years) were evaluated using 2DE and 3DE.
   TA 3D area, perimeter, diameters, and sphericity index were assessed at
   mid-systole, early- and end-diastole. Right atrial (RA) and ventricular
   (RV) end-diastolic and end-systolic volumes were also measured by 3DE.
   ResultsThe feasibility of the 3DE analysis of TA was 90\%. TA 3D area,
   perimeter, and diameters were largest at end-diastole and smallest at
   mid-systole. Reference values of TA at end-diastole were 9.6 +/- 2.1
   cm(2) for the area, 11.2 +/- 1.2 cm for perimeter, and 38 +/- 4 mm, 31
   +/- 4 mm, 33 +/- 4 mm, and 34 +/- 5 mm for major, minor, 4-chamber and
   2-chamber diameters, respectively. TA end-diastolic sphericity index was
   81 +/- 11\%. All TA parameters were correlated with body surface area
   (BSA) (r from 0.42 to 0.58, p < 0.001). TA 3D area and 4-chamber
   diameter were significantly larger in men than in women, independent of
   BSA (p < 0.0001). There was no significant relationship between TA
   metrics with age, except for the TA minor diameter (r = -0.17, p <
   0.05). When measured by 2DE in 4-chamber (29 +/- 5 mm) and RV-focused
   (30 +/- 5 mm) views, both TA diameters resulted significantly smaller
   than the 4-chamber (33 +/- 4 mm; p < 0.0001), and the major TA diameters
   (38 +/- 4 mm; p < 0.0001) measured by 3DE. At multivariable linear
   regression analysis, RA maximal volume was independently associated with
   both TA 3D area at mid-systole (R-2 = 0.511, p < 0.0001) and
   end-diastole (R-2 = 0.506, p < 0.0001), whereas BSA (R-2 = 0.526, p <
   0.0001) was associated only to mid-systolic TA 3D area.
   ConclusionsReference values for TA metrics should be sex-specific and
   indexed to BSA. 2DE underestimates actual 3DE TA dimensions. RA maximum
   volume was the only independent echocardiographic parameter associated
   with TA 3D area in healthy subjects.},
DOI = {10.3389/fcvm.2022.1011931},
Article-Number = {1011931},
ISSN = {2297-055X},
ResearcherID-Numbers = {Muraru, Denisa/S-9628-2019
   },
ORCID-Numbers = {Muraru, Denisa/0000-0003-2514-3668
   Radu, Daniela-Noela/0000-0002-2364-7929},
Unique-ID = {WOS:000861167000001},
}

@article{ WOS:000872963000003,
Author = {Zhang, Yong},
Title = {Application of Landscape Architecture 3D Visualization Design System
   Based on AI Technology},
Journal = {INTERNATIONAL TRANSACTIONS ON ELECTRICAL ENERGY SYSTEMS},
Year = {2022},
Volume = {2022},
Month = {SEP 12},
Abstract = {With the vigorous development of computer, two-dimensional data
   acquisition technology and massive data dynamic visualization
   information technology, two-dimensional geographic information system
   (GIS) has gradually matured, and has been transferred from the initial
   visual expression to thematic application. Because 3D geographic
   information system has the characteristics of multidimensional data
   processing, classification, and presentation of information, it breaks
   through the confinement of monotonously displayed information on the
   traditional two-dimensional plane and has made breakthroughs in
   government assisted decision-making, national resource investigation,
   intelligent transportation, and other fields. E-commerce has been widely
   used in the field, and it is also very obvious in the field of spatial
   information socialization, playing an irreplaceable intergenerational
   effect. The visualization of landscape design is one of the key
   technologies to construct 3D GIS. Based on the combination of basic
   research and practical application, this paper deeply analyzes the
   openness of the garden 3D visualization system using artificial
   intelligence technology and completes the overall structure design of
   the 3D visualization system on the basis of studying the practical value
   and feasibility. Combined with the construction of the system, it is
   true to determine the acquisition method and construction method of
   three-dimensional landscape architecture model data, classify the
   landscape architecture information, and establish geometric model type
   libraries and texture model libraries. At the same time, the system
   adopts the object-oriented spatial relational database management mode
   to organize and manage the three-dimensional landscape building data
   efficiently and quickly. Determine the corresponding software and
   hardware configurations, carry out the interface design of the system
   and the division of functional modules, and establish a
   three-dimensional terrain visualization system for landscape
   architecture. In order to improve the operational efficiency of the
   system, the data is rationally organized and managed while taking
   optimization measures for the 3D model. Finally, the system adopts AI
   technology and uses component technology to realize the research of the
   intelligent terminal application system of landscape architecture.},
DOI = {10.1155/2022/9918171},
Article-Number = {9918171},
ISSN = {2050-7038},
Unique-ID = {WOS:000872963000003},
}

@article{ WOS:000850240100001,
Author = {Amato, Domenico and Lo Bosco, Giosue and Giancarlo, Raffaele},
Title = {Standard versus uniform binary search and their variants in learned
   static indexing: The case of the searching on sorted data benchmarking
   software platform},
Journal = {SOFTWARE-PRACTICE \& EXPERIENCE},
Year = {2023},
Volume = {53},
Number = {2},
Pages = {318-346},
Month = {FEB},
Abstract = {Learned Indexes use a model to restrict the search of a sorted table to
   a smaller interval. Typically, a final binary search is done using the
   lower\_bound routine of the Standard C++ library. Recent studies have
   shown that on current processors other search approaches (such as k-ary
   search) can be more efficient in some applications. Using the SOSD
   learned indexing benchmarking software, we extend these results to show
   that k-ary search is indeed a better choice when using learned indexes.
   We highlight how such a choice may be dependent on the computer
   architecture used, for example, Intel I7 or Apple M1, and provide
   guidelines for the selection of the Search routine within the learned
   indexing framework.},
DOI = {10.1002/spe.3150},
EarlyAccessDate = {SEP 2022},
ISSN = {0038-0644},
EISSN = {1097-024X},
ResearcherID-Numbers = {Lo Bosco, Giosuè/C-7644-2011},
ORCID-Numbers = {Lo Bosco, Giosuè/0000-0002-1602-0693},
Unique-ID = {WOS:000850240100001},
}

@article{ WOS:000850312600001,
Author = {Hu, Mingzhe and Zhang, Yu},
Title = {An empirical study of the Python/C API on evolution and bug patterns},
Journal = {JOURNAL OF SOFTWARE-EVOLUTION AND PROCESS},
Year = {2023},
Volume = {35},
Number = {2},
Month = {FEB},
Abstract = {Python is a popular programming language, and a large part of its appeal
   comes from diverse libraries and extension modules. In the bloom of data
   science and machine learning, Python frontend with C/C++ native
   implementation achieves both productivity and performance and has almost
   become the standard structure for many mainstream software systems.
   However, feature discrepancies between two languages such as exception
   handling, memory management, and type system can pose many safety
   hazards in the interface layer using the Python/C API. In this paper, we
   carry out an empirical study of the Python/C API on evolution and bug
   patterns. The evolution analysis includes Python/C API design in CPython
   compilers and its usage in mainstream software. By designing and
   applying a static analysis toolset, we reveal the evolution and usage
   statistics of the Python/C API and provide a summary and classification
   of 9 common bug patterns. In Pillow, a widely used Python imaging
   library, we find 48 bugs, 19 of which are undiscovered before. Our
   toolset can be easily extended to access different types of syntactic
   bug-finding checkers, and our systematical taxonomy to classify bugs can
   guide the construction of more highly automated and high-precision
   bug-finding tools.},
DOI = {10.1002/smr.2507},
EarlyAccessDate = {SEP 2022},
Article-Number = {e2507},
ISSN = {2047-7473},
EISSN = {2047-7481},
ORCID-Numbers = {Zhang, Yu/0000-0001-6638-6442
   Hu, Mingzhe/0000-0001-9808-4967},
Unique-ID = {WOS:000850312600001},
}

@article{ WOS:000853964100006,
Author = {Tsou, Yi-Ta and Chen, Wei-Cheng and Lin, Hsiao-Hsien and Chen,
   Chao-Chien and Fang, Rui-Wen and Hsu, Chin-Hsien},
Title = {Exploring the Impacts of Students in Hospitality Programs of Vocational
   High Schools Watching YouTubers' Travel Programs on Travel Intention
   from the Perspective of Digital Business Opportunities in the
   Postpandemic Era},
Journal = {MOBILE INFORMATION SYSTEMS},
Year = {2022},
Volume = {2022},
Month = {SEP 2},
Abstract = {The purpose of this study is to explore the impacts of students in the
   hospitality programs of vocational high schools watching YouTubers'
   travel programs on their travel intentions, as viewed from the
   perspective of digital business opportunities in the postpandemic era.
   Students in the hospitality programs of vocational high schools were
   taken as the research subjects, and a questionnaire survey was conducted
   with convenience sampling. A total of 350 questionnaires were
   distributed, and 320 questionnaires were recovered for a recovery rate
   of 91\%. After 35 invalid questionnaires were excluded, there were 285
   valid questionnaires for an effective recovery rate of 89\%. The
   software, Statistical Package for the Social Sciences and Analysis of
   Moment Structures, was used for statistical analysis. The research
   results show that (1) there was no significant difference between
   different background variables in travel intention; (2) the degree of
   involvement had a significant impact on travel intention; (3) trust had
   a significant impact on travel intention; (4) perceived value had a
   significant impact on travel intention; and (5) destination image had a
   significant impact on travel intention. Finally, based on the results of
   this study, relevant suggestions were provided for practical
   application.},
DOI = {10.1155/2022/6193685},
Article-Number = {6193685},
ISSN = {1574-017X},
EISSN = {1875-905X},
ResearcherID-Numbers = {Lin, Hsiao Hsien/AEH-2149-2022
   },
ORCID-Numbers = {Lin, Hsiao Hsien/0000-0001-8360-3998
   Fang, Rui-Wen/0000-0002-7078-5486},
Unique-ID = {WOS:000853964100006},
}

@article{ WOS:000856270800001,
Author = {Luo, Lanfang and Jiang, Nan},
Title = {Constrained Mode-Damping Solvent Extraction Combined Method for the Soil
   Incorporation into a Real-Time Hybrid Test of the Soil-Structure System},
Journal = {BUILDINGS},
Year = {2022},
Volume = {12},
Number = {9},
Month = {SEP},
Abstract = {The real-time hybrid test is an effective testing method for
   soil-structure interaction research. Due to the data interaction time
   requirement and formula derivation method, the traditional real-time
   hybrid test of soil-structure interaction mostly employs a simple
   numerical substructure model. This study investigated the model
   construction and numerical simulation of a finite element soil
   substructure with high simulation accuracy and calculation efficiency.
   The soil was subdivided into near-field and far-field zones. A
   constrained mode-damping solvent extraction combined method was applied
   to the latter zone, reducing the soil's computational scale and
   simulating the far-field energy dissipation effect. Then, the basic
   formula of the near-field zone-structure system was derived using the
   branch mode method, and the motion equation of the soil-structure system
   applied to real-time hybrid test was obtained. The soil's numerical
   model was realized by the joint application of ANSYS and MATLAB software
   packages and verified through the real-time hybrid test of the
   soil-structure system. The results show that the proposed constrained
   mode-damping solvent extraction combined method had high calculation
   efficiency and good accuracy. It satisfied the requirements of the soil
   numerical substructure in real-time hybrid tests.},
DOI = {10.3390/buildings12091468},
Article-Number = {1468},
EISSN = {2075-5309},
Unique-ID = {WOS:000856270800001},
}

@article{ WOS:000842026400001,
Author = {Bandara, Madhushi and Rabhi, Fethi A. and Bano, Muneera},
Title = {A knowledge-driven approach for designing data analytics platforms},
Journal = {REQUIREMENTS ENGINEERING},
Abstract = {Big data analytics technologies are rapidly expanding across all
   industry sectors as organisations try to make analytics an integral part
   of their everyday decision-making. Although there are many software
   tools and libraries to assist analysts and software engineers in
   developing solutions, organisations are looking for flexible analytics
   platforms that can address their specific objectives and requirements.
   To minimise costs, such platforms also need to co-exist with existing IT
   infrastructures and reuse knowledge and resources already accumulated
   within the organisation. To address such needs, this paper proposes the
   Data Analytics Solution Engineering (DASE) framework-a knowledge-driven
   approach supported by semantic web technologies for requirements
   engineering, design and development of new data analytics platforms. It
   includes a meta-model that captures data analytics platform requirements
   via a Knowledge Base, a set of guidelines that organisations can follow
   in engineering data analytics platforms and a reference architecture
   that demonstrates how to use these guidelines. We evaluate the DASE
   framework through two case studies and demonstrate how it can facilitate
   knowledge-based and requirements-driven data analytics platform
   engineering. The resulting data analytics platforms are observed to be
   user friendly, easy to maintain and flexible in handling changes to
   requirements. This work contributes to the body of knowledge in
   knowledge-driven requirements engineering, and data analytics platform
   engineering by providing a meta-model and a reference architecture that
   can be tailored to different analytics application domains.},
DOI = {10.1007/s00766-022-00385-5},
EarlyAccessDate = {AUG 2022},
ISSN = {0947-3602},
EISSN = {1432-010X},
ORCID-Numbers = {Bandara, Madhushi/0000-0001-6543-3841},
Unique-ID = {WOS:000842026400001},
}

@article{ WOS:000839641000002,
Author = {Gebre, Deneke and Murugan, Rajalakshmi and Bizuwork, Ketema and Wurjine,
   Teshome Habte},
Title = {Knowledge, practice and perceived barriers towards chemotherapy induced
   nausea and vomiting in prophylaxis guideline adherence among nurses in
   oncology units at selected hospitals, in Addis Ababa, Ethiopia, a
   cross-sectional study},
Journal = {BMC NURSING},
Year = {2022},
Volume = {21},
Number = {1},
Month = {AUG 11},
Abstract = {Background Chemotherapy-induced emesis can be prevented by the use of
   recommended guidelines for antiemetic regimens but a research study
   indicates that in Ethiopia the use of standard antiemetic drug
   guidelines is very limited. Objectives To assess knowledge, practice,
   and perceived barriers towards chemotherapy-induced nausea and vomiting
   in prophylaxis guideline adherence among nurses in oncology units.
   Methods A cross-sectional study design was conducted among 81 oncology
   nurses selected in the two public hospitals of Addis Ababa, from March 1
   to 30, 2020. The study participants were selected by using the
   population census method from the source population of nurses in
   oncology units. Data has collected by using semi-structured
   questionnaires with the self-administrated method. Data were analyzed by
   using Statistical Package for the Social Sciences software version 24.
   Descriptive statistics and logistic regression including bivariate and
   multivariate were conducted to examine the association between
   independent and outcome variables. The level of significance was
   determined at a p-value < 0.05 and a 95\% confidence interval. Result
   Seventy-nine nurses participated with a 96\% of response rate. All
   participants were aged greater than 24 with a mean age of 28.8 +/- 6
   years and nearly two-thirds of the respondents (60.8\%) were females.
   Nurses were not trained in chemotherapy-induced nausea and vomiting
   management shows 54.4\%. nurses' knowledge of chemotherapy-induced
   nausea and vomiting prophylaxis Guidelines was 78.5\%. The means score
   of oncology nurses' practice toward guideline recommendation was 41.8\%.
   Knowledge of nurses associated with the use of chemotherapy-induced
   nausea and vomiting prophylaxis guideline recommendations working in the
   outpatient department, inpatient ward, and chemotherapy administration
   unit has a significant association with chemotherapy-induced nausea and
   vomiting management knowledge. In the multiple logistic regression
   analysis, nurses who have trained for chemotherapy-induced nausea and
   vomiting management were 1.64-fold more aware than those who were not
   trained. Conclusion The study reveals that nurses working in the
   oncology unit of the study hospitals have a poor practice of
   Chemotherapy-Induced Nausea and Vomiting. Therefore, recommended
   providing Training for the Nurses working in the oncology unit and
   encourage them to apply standard guidelines.},
DOI = {10.1186/s12912-022-01009-7},
Article-Number = {223},
ISSN = {1472-6955},
ORCID-Numbers = {Murugan, Rajalakshmi/0000-0001-7571-134X
   habte, teshome/0000-0001-9737-9560},
Unique-ID = {WOS:000839641000002},
}

@article{ WOS:000839716100001,
Author = {Hussain, Mudassar and Shah, Nadir and Amin, Rashid and Alshamrani,
   Sultan S. and Alotaibi, Aziz and Raza, Syed Mohsan},
Title = {Software-Defined Networking: Categories, Analysis, and Future Directions},
Journal = {SENSORS},
Year = {2022},
Volume = {22},
Number = {15},
Month = {AUG},
Abstract = {Software-defined networking (SDN) is an innovative network architecture
   that splits the control and management planes from the data plane. It
   helps in simplifying network manageability and programmability, along
   with several other benefits. Due to the programmability features, SDN is
   gaining popularity in both academia and industry. However, this emerging
   paradigm has been facing diverse kinds of challenges during the SDN
   implementation process and with respect to adoption of existing
   technologies. This paper evaluates several existing approaches in SDN
   and compares and analyzes the findings. The paper is organized into
   seven categories, namely network testing and verification, flow rule
   installation mechanisms, network security and management issues related
   to SDN implementation, memory management studies, SDN simulators and
   emulators, SDN programming languages, and SDN controller platforms. Each
   category has significance in the implementation of SDN networks. During
   the implementation process, network testing and verification is very
   important to avoid packet violations and network inefficiencies.
   Similarly, consistent flow rule installation, especially in the case of
   policy change at the controller, needs to be carefully implemented.
   Effective network security and memory management, at both the network
   control and data planes, play a vital role in SDN. Furthermore, SDN
   simulation tools, controller platforms, and programming languages help
   academia and industry to implement and test their developed network
   applications. We also compare the existing SDN studies in detail in
   terms of classification and discuss their benefits and limitations.
   Finally, future research guidelines are provided, and the paper is
   concluded.},
DOI = {10.3390/s22155551},
Article-Number = {5551},
EISSN = {1424-8220},
ResearcherID-Numbers = {amin, Rashid/H-2389-2017
   Alotaibi, Aziz/GQP-7900-2022
   },
ORCID-Numbers = {amin, Rashid/0000-0002-3143-689X
   Alotaibi, Aziz/0000-0002-3342-6716
   Shah, Nadir/0000-0003-1173-4272
   Hussain, Dr. Mudassar/0000-0002-2723-2253
   RAZA, Syed Mohsan/0000-0002-6327-1196
   Alshamrani, Sultan/0000-0001-8194-9354},
Unique-ID = {WOS:000839716100001},
}

@article{ WOS:000835075400016,
Author = {Sheng, Dan and Wang, Yulong},
Title = {Design of Innovation and Entrepreneurship Education Ecosystem in
   Universities Based on User Experience},
Journal = {MATHEMATICAL PROBLEMS IN ENGINEERING},
Year = {2022},
Volume = {2022},
Month = {JUL 21},
Abstract = {The conventional college innovation and entrepreneurship education
   ecosystem is not considered to be designed for low energy consumption,
   and there is confusion in function switching, which leads to a large
   value of packet loss rate in system transmission. In this paper, we
   analyze the construction of college innovation and entrepreneurship
   education ecosystem and construct the scientific ecosystem of college
   innovation and entrepreneurship education based on user experience so as
   to realize college. At the same time, it takes into account the
   principle of green and low energy consumption of system hardware and
   software design, in which the hardware part selects S3C6410 RISC
   microcontroller as the core component to build the structure of
   educational ecological controller, connects a 16 bit buffer in parallel
   outside the controller, uses the buffer as the actual conversion port,
   specifies the function switching bearing interface, uses a protocol
   conversion chip, and designs the data acquisition circuit structure. The
   software part builds a model of user experience elements, sets to get an
   ecological interaction interface under the interoperability between the
   model and information data, and uses JAVA programming to realize
   functional modules according to the deviation of educational information
   shown by users. The experimental results show that the educational
   ecosystem designed in this paper has high data throughput, high fault
   tolerance, and low packet loss rate and system energy consumption, which
   verifies the effectiveness of the system.},
DOI = {10.1155/2022/3266326},
Article-Number = {3266326},
ISSN = {1024-123X},
EISSN = {1563-5147},
Unique-ID = {WOS:000835075400016},
}

@article{ WOS:000828383400001,
Author = {Giudetti, Goran and Polyakov, Igor and Grigorenko, Bella L. and Faraji,
   Shirin and Nemukhin, Alexander V. and Krylov, Anna I.},
Title = {How Reproducible Are QM/MM Simulations? Lessons from Computational
   Studies of the Covalent Inhibition of the SARS-CoV-2 Main Protease by
   Carmofur},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Abstract = {This work explores the level of transparency in reporting the details of
   computational protocols that is required for practical reproducibility
   of quantum mechanics/molecular mechanics (QM/MM) simulations. Using the
   reaction of an essential SARS-CoV-2 enzyme (the main protease) with a
   covalent inhibitor (carmofur) as a test case of chemical reactions in
   biomolecules, we carried out QM/MM calculations to determine the
   structures and energies of the reactants, the product, and the
   transition state/intermediate using analogous QM/MM models implemented
   in two software packages, NWChem and Q-Chem. Our main benchmarking goal
   was to reproduce the key energetics computed with the two packages. Our
   results indicate that quantitative agreement (within the numerical
   thresholds used in calculations) is difficult to achieve. We show that
   rather minor details of QM/ MM simulations must be reported in order to
   ensure the reproducibility of the results and offer suggestions toward
   developing practical guidelines for reporting the results of
   biosimulations.},
DOI = {10.1021/acs.jctc.2c00286},
EarlyAccessDate = {JUL 2022},
ISSN = {1549-9618},
EISSN = {1549-9626},
ORCID-Numbers = {Grigorenko, Bella/0000-0003-1372-4692
   Krylov, Anna/0000-0001-6788-5016
   Faraji, Shirin/0000-0002-6421-4599
   Giudetti, Goran/0000-0003-3851-670X},
Unique-ID = {WOS:000828383400001},
}

@article{ WOS:000828767300001,
Author = {Luo, Jin and Wu, Feng and Liu, Wenge and Ren, Qiaoyun and Diao, Peiwen
   and Guan, Guiquan and Luo, Jianxun and Yin, Hong and Liu, Guangyuan},
Title = {A Novel MicroRNA and the Target Gene TAB2 Can Regulate the Process of
   Sucking Blood in and the Spawn Rate of Hyalomma asiaticum (Acari:
   Ixodidae) Ticks},
Journal = {FRONTIERS IN IMMUNOLOGY},
Year = {2022},
Volume = {13},
Month = {JUL 5},
Abstract = {Ticks are blood-sucking parasites that are harmful to humans and
   animals. MicroRNAs are a class of conserved small noncoding RNAs that
   play regulatory roles in the expression of many genes at the
   posttranscriptional level. Here, a novel miRNA (nov-miR-17) was
   identified from a small RNA data library of Hyalomma asiaticum by
   next-generation sequencing. PCR was used to obtain precursor nov-miR-17
   by RACE using mature loop primers. The secondary structure was predicted
   with UNAFold. The interaction of nov-miR-17 with its target gene TAB2
   was predicted using RNAhybrid software and identified in vitro by
   luciferase assays. Moreover, the interaction was confirmed in vivo by
   phenotype rescue experiments in which dsTAB2 was used for RNA
   interference (RNAi) and an antagomir of nov-miR-17 was used for miRNA
   silencing. The expression levels of nov-miR-17 and TAB2 in ticks at
   different developmental stages and the expression of nov-miR-17 in
   different tissues were analyzed by real-time qPCR. All data were
   analyzed using GraphPad Prism version 5. Results: The results showed
   that TAB2 was a target gene of nov-miR-17. When the blood-sucking
   process of larval, nymph and adult ticks was prolonged, the expression
   of nov-miR-17 was decreased, and TAB2 expression was increased. However,
   the level of nov-miR-17 in the midgut of engorged ticks was highest at
   all stages. Therefore, nov-miR-17 plays an important role in the
   blood-sucking process. The overexpression of nov-miR-17 indicated that
   this miRNA affected the engorged weight (P < 0.001) and spawn rate (P <
   0.001) of female ticks. RNAi of TAB2 also had the same effect. dsRNA not
   only impacted the weight (P < 0.01) but also reduced the spawn rate (P <
   0.001) of the ticks. Furthermore, significant recovery was observed in
   nov-miR-17-silenced ticks after TAB2 silencing by RNAi. nov-miR-17
   silencing by antagomir not only impacted the engorged weight of the
   female ticks (P < 0.001) but also the number of days that the females
   needed to progress from engorgement to spawning (P < 0.001). The study
   showed that nov-miR-17, as a new miRNA, plays an important role along
   with its target gene TAB2 in the blood-sucking and spawning processes in
   female ticks.},
DOI = {10.3389/fimmu.2022.930532},
Article-Number = {930532},
ISSN = {1664-3224},
ResearcherID-Numbers = {Luo, jian/HGE-7331-2022},
Unique-ID = {WOS:000828767300001},
}

@article{ WOS:000796971500002,
Author = {Kamyab, Amir and Ghasemi-Ghalebahman, Ahmad and Fereidoon, Abdolhossein
   and Khonakdar, Hossein Ali},
Title = {Investigation into the shape memory behavior of peanut-pattern auxetic
   structures},
Journal = {EXPRESS POLYMER LETTERS},
Year = {2022},
Volume = {16},
Number = {7},
Pages = {679-693},
Month = {JUL},
Abstract = {In the last decade, auxetic structures with a negative Poisson's ratio
   have attracted the attention of experts in in-dustries such as
   biomechanics, aerospace, and packaging due to their properties, such as
   energy absorption and high crack resistance. The preparation of these
   structures using smart polymers increases their attractiveness due to
   low energy expen-diture at the destination and the deformation of two
   separate states by applying external stimuli. This study aimed to
   inves-tigate the improved peanut-shaped structure that retains its
   auxetic properties at high elongation, the effects of changes in
   geometry and dimensions of the structure on the Poisson's behavior,
   shape fixity ratio, and shape recovery ratio during the shape memory
   cycle. To this end, several samples were prepared using the 4D printing
   method of shape memory polymers, and laboratory results were compared
   with software simulations of the shape memory cycle. After validating
   the simulation results, more cases were examined by the modeling. The
   results showed that despite the complete recovery of samples, the
   Poisson's ratio was not recovered. The changes in geometry and samples'
   dimensions significantly enhanced their auxetic property. Due to
   obtaining a significantly high Poisson's ratio by changing the geometric
   characteristics, this structure is suggested to be used in industrial
   applications such as aerospace.},
DOI = {10.3144/expresspolymlett.2022.50},
ISSN = {1788-618X},
Unique-ID = {WOS:000796971500002},
}

@article{ WOS:000832296000001,
Author = {Li, Bei and Liu, Yu and Li, Jiaqing and Liu, Bin and Wang, Xingxing and
   Deng, Guanyu},
Title = {Investigation of a Novel Hydrogen Depressurization Structure Constituted
   by an Orifice Plate with Tesla-Type Channels},
Journal = {MATERIALS},
Year = {2022},
Volume = {15},
Number = {14},
Month = {JUL},
Abstract = {A hydrogen depressurization system is required to supply the hydrogen to
   the fuel cell stack from the storage. In this study, a Tesla-type
   depressurization construction is proposed. Parallel Tesla-type channels
   are integrated with the traditional orifice plate structure. A
   computational fluid dynamics (CFD) model is applied to simulate
   high-pressure hydrogen flow through the proposed structure, using a
   commercial software package, ANSYS-Fluent (version 19.2, ANSYS, Inc.
   Southpointe, Canonsburg, PA, USA). The Peng-Robinson (PR) equation of
   state (EoS) is incorporated into the CFD model to provide an accurate
   thermophysical property estimation. The construction is optimized by the
   parametric analysis. The results show that the pressure reduction
   performance is improved greatly without a significant increase in size.
   The flow impeding effect of the Tesla-type orifice structure is
   primarily responsible for the pressure reduction improvement. To enhance
   the flow impeding effect, modifications are introduced to the Tesla-type
   channel and the pressure reduction performance has been further
   improved. Compared to a standard orifice plate, the Tesla-type orifice
   structure can improve the pressure reduction by 237\%. Under low inlet
   mass flow rates, introduction of a secondary Tesla-type orifice
   construction can achieve better performance of pressure reduction.
   Additionally, increasing parallel Tesla-type channels can effectively
   reduce the maximum Mach number. To further improve the pressure
   reduction performance, a second set of Tesla-type channels can be
   introduced to form a two-stage Tesla-type orifice structure. The study
   provides a feasible structure design to achieve high-efficiency hydrogen
   depressurization in hydrogen fuel cell vehicles (HFCVs).},
DOI = {10.3390/ma15144918},
Article-Number = {4918},
EISSN = {1996-1944},
ResearcherID-Numbers = {Li, Jiaqing/HJY-6555-2023
   },
ORCID-Numbers = {Wang, Xingxing/0000-0003-4437-5229
   Li, Jiaqing/0000-0002-7993-7706},
Unique-ID = {WOS:000832296000001},
}

@article{ WOS:000827444300036,
Author = {Ahamad, Shahanawaj},
Title = {Component-Oriented Software Engineering Model for Heterogeneous Internet
   of Things Systems with Connectors using Machine Learning},
Journal = {INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND NETWORK SECURITY},
Year = {2022},
Volume = {22},
Number = {6},
Pages = {680-689},
Month = {JUN 30},
Abstract = {Component reuse has been proven both theoretically and empirically to
   increase software quality and productivity with an economically
   cost-effective option. This necessitates the use of a graphical editor
   for project modeling using component-based architecture and development.
   To aid in the creation of component-oriented software, a graphical
   editor was proposed for practice. Both machine learning and software
   engineering employ models based on components architecture. Aside from
   these smart characteristics, AI models may be able to help with
   prediction and decision-making. Communication between IoT system
   components must adhere to a set of guidelines and protocols for
   effective and predictive perspectives. Components must be able to
   communicate with one another in the deployed system. The heterogeneity
   issue in the Internet of Things arises when different IoT devices
   communicate using distinct sets of rules, features, and contexts.
   Components that can be reused are found in these or other systems or
   commercial off-the-shelf. Component-oriented systems rely on connectors
   to link up their reusable parts with other entities, components, or IoT
   devices through the use of related interfaces. COSE development tools
   provide application-level solutions for connectors and component-based
   development of systems. Linking and hookup ports on connectors are
   designed to work with the attached component and other interfaces. The
   communication protocols' packets are identified and organized by the
   connectors with their installed applications. A simulation feature can
   be added to the tools in order to show that the idea can be implemented
   in effective and efficient ways. Connectors allow moving data between
   different parts of computing systems. ML -based training and prediction
   have been shown in this work for performance analysis.},
DOI = {10.22937/IJCSNS.2022.22.6.86},
ISSN = {1738-7906},
ResearcherID-Numbers = {Ahamad, Shahanawaj/ABH-2662-2021},
Unique-ID = {WOS:000827444300036},
}

@article{ WOS:000835731100005,
Author = {Zhang, Jingbin and Qi, Tianxiang},
Title = {Construction of Educational Resource Metadata Management Platform Based
   on Service-Oriented Architecture},
Journal = {JOURNAL OF SENSORS},
Year = {2022},
Volume = {2022},
Month = {JUN 22},
Abstract = {The existing cloud sharing system of educational resources is in the
   form of centralized management, which has the problems of small number
   of concurrent users and low score of resource retrieval results and
   cannot meet the current needs of implicit educational resource sharing.
   Therefore, a cloud sharing system of college implicit educational
   resources based on service architecture is proposed. At present, the
   sharing of educational resources mainly includes network teaching,
   distance education, and other methods. These schemes alleviate the
   sharing problem of educational data resources to a certain extent.
   However, the sharing and rational integrated utilization of educational
   application software are still under exploration, and there is no mature
   solution at present. At present, there is a lack of unified planning for
   the construction of the educational resource database of the management
   cadre college, and there are problems such as repeated construction,
   complex quantity, and nonstandard form, which directly leads to the
   failure of effective dissemination of resource information in the
   network distance education system, which is an urgent problem to be
   solved in the distance education system of the management cadre college.
   This paper first understands some domestic and foreign standardization
   and development of educational resources. Then, it introduces the
   ontology theory and the basic theory of metadata. At present, the
   construction of educational resource database in colleges and
   universities in China lacks unified planning, and there are problems
   such as repeated construction, complex quantity, and nonstandard form,
   which makes the resource information in the network distance education
   system unable to be shared and disseminated effectively. In order to
   improve the utilization of online educational resources and share the
   data of resource databases with different structures, a metadata
   management platform model of educational resources is designed. As a new
   architecture idea, service-oriented architecture can package existing
   assets and reuse existing assets. It can also minimize the impact of
   demand changes. This paper puts forward the process of metadata
   management under the service architecture and independently develops a
   metadata management platform suitable for the development and use of
   service mode. It is combined with automatic testing and continuous
   integration to effectively improve the data quality and development
   efficiency of service development.},
DOI = {10.1155/2022/2172817},
Article-Number = {2172817},
ISSN = {1687-725X},
EISSN = {1687-7268},
Unique-ID = {WOS:000835731100005},
}

@article{ WOS:000851382600003,
Author = {Liu, Junshan and Willkens, Danielle S. and Foreman, Graham},
Title = {AN INTRODUCTION TO TECHNOLOGICAL TOOLS AND PROCESS OF HERITAGE BUILDING
   INFORMATION MODELING (HBIM)},
Journal = {EGE-REVISTA DE EXPRESION GRAFICA EN LA EDIFICACION},
Year = {2022},
Number = {16},
Pages = {50-65},
Month = {JUN},
Abstract = {Heritage Building Information Modelling (or HBIM) is a
   multi-disciplinary process and a promising tool for the management and
   documentation of heritage structures. HBIM can record the significant
   historic events that have taken place in the built environment and is
   used to track the aging process of the built asset. However, the digital
   re-construction procedures for HBIM development associated with historic
   buildings are very challenging: the objects of the historic models
   consist of components whose heterogeneous, complex, and irregular
   characteristics and morphologies are not represented in the existing BIM
   software libraries. Unlike conventional BIM workflows for new
   constructions, the tried and tested tools and methods must be adapted,
   and even reinvented, for HBIM applications. This article introduces the
   basic concept of HBIM, a set of technological tools of data capture for
   HBIM model development, and a feasible HBIM workflow.},
DOI = {10.4995/ege.2022.17723},
ISSN = {1888-8143},
EISSN = {2605-082X},
ResearcherID-Numbers = {Willkens, Danielle/GVS-1515-2022
   },
ORCID-Numbers = {Liu, Junshan/0000-0003-4476-5673
   Willkens, Danielle/0000-0003-3271-6890},
Unique-ID = {WOS:000851382600003},
}

@article{ WOS:000803132900001,
Author = {Maganga, Deusdedith Pastory and Taifa, Ismail W. R.},
Title = {Quality 4.0 transition framework for Tanzanian manufacturing industries},
Journal = {TQM JOURNAL},
Abstract = {Purpose This research aimed at developing the Quality 4.0 transition
   framework for Tanzanian manufacturing industries.
   Design/methodology/approach The survey method was used in this study to
   gather practitioners' perspectives. The approach included open-ended and
   closed-ended structured questionnaires to assess respondents'
   perceptions of Quality 4.0 awareness and manufacturers' readiness to
   transit to Quality 4.0. The study's objective was to adopt
   non-probability and purposive sampling strategies. The study focused on
   fifteen Tanzanian manufacturing industries. The data were analysed
   qualitatively and quantitatively using MAXQADA 2020 and Minitab 20
   software packages, respectively. Findings The study demonstrated a high
   level of awareness of Quality 4.0 among Tanzanian manufacturing
   industries (i.e. 100\% in Quality 4.0 traditional attributes and 53\% in
   Quality 4.0 modern attributes). Individuals acquire knowledge in various
   ways, including through quality training, work experience, self-reading
   and Internet surfing. The result also revealed that most manufacturing
   industries in Tanzania use Quality 3.0 or a lower approach to manage
   quality. However, Tanzanian manufacturing industries are ready to
   embrace Quality 4.0 since practitioners are aware of the concepts and
   could see benefits such as customer satisfaction, product improvement,
   process and continuous improvement, waste reduction and decision support
   when using the Quality 4.0 approach. The challenges hindering Quality
   4.0 adoption in Tanzania include reliable electricity, high-speed
   Internet and infrastructure inadequacy to support the adoption, skilled
   workforces familiar with Quality 4.0-enabled technologies and a
   financial set-up to support technology investment. Moreover, the study
   developed a transition framework for an organisation to transition from
   traditional quality approaches such as quality control, quality
   assurance and total quality management to Quality 4.0, a modern quality
   approach aligned with the fourth industrial revolution era. Research
   limitations/implications The current study solely looked at
   manufacturing industries, leaving other medical, service, mining and
   construction sectors. Furthermore, no focus was laid on the study's
   Quality 4.0 implementation frameworks. Originality/value This is
   probably the first Quality 4.0 transition framework for Tanzanian
   manufacturing industries, perhaps with other developing countries.},
DOI = {10.1108/TQM-01-2022-0036},
EarlyAccessDate = {MAY 2022},
ISSN = {1754-2731},
EISSN = {1754-274X},
ResearcherID-Numbers = {Maganga, Deusdedith/GON-5144-2022
   Andrea Simões Braga, Francisco/GRS-0157-2022
   /H-4715-2018},
ORCID-Numbers = {Maganga, Deusdedith Pastory/0000-0003-1714-9525
   /0000-0001-5747-5208},
Unique-ID = {WOS:000803132900001},
}

@article{ WOS:000884222100004,
Author = {Soni, Heni and Gandhi, Sahaj A. and Pandya, Alok and Sutariya, Pinkesh
   G.},
Title = {Dansyl driven fluorescence paper-based quencher probe for Pr3+ and I-
   based on calix{[}4]arene},
Journal = {JOURNAL OF PHOTOCHEMISTRY AND PHOTOBIOLOGY A-CHEMISTRY},
Year = {2022},
Volume = {431},
Month = {OCT 1},
Abstract = {We have introduced a novel luminous molecular receptor C4DS based on
   calix{[}4]arene conjugate bearing dansyl fluorophore with hydrazine
   carbonyl linkage and characterized it by different spectroscopic
   methods. This chemosensor has been addressed by fluorescence technique
   for recognizing two ions, viz., Pr3+ and I-. C4DS demonstrates a minimum
   detection limit of 3.571 nM for Pr3+ and 2.439 nM for I-. The C4DS
   sensor has a working concentration range of 0-120 nM for I- and 0-135 nM
   for Pr3+ with a binding constant of 6.698 x 108 M-1 (Pr3+) and 7.377 x
   108 M-1 (I-). We report an easy-to-use, cheap, disposable, paper-based,
   fluorescent wax printing device for the rapid chemical screening of Pr3+
   and I-. The GAUSSIAN 09 software package has been used to optimize C4DS
   and complex of C4DS and Pr3+ and I- through B3LYP exchange-correlation
   functional by the basis set LANL2DZ and also intended time-dependent DFT
   (TDDFT) calculations for supported the fluorescence quenching mechanisms
   of complexes. The molecular docking inspection has been supported out by
   using human serum albumin protein structures (2xsi, 2xvq, 2xvu, 2xvv,
   2xvw, 2xw0, and 2xw1) with C4DS and molecular docking score has been
   calculated using HEX software. To assess its analytical applicability,
   the prepared sensor was effectively used for the determination of two
   different real samples (for the determination of Pr3+ and I- from two
   different samples) for the validation with 95-99\% recovery of
   recognized ions.},
DOI = {10.1016/j.jphotochem.2022.114012},
EarlyAccessDate = {MAY 2022},
Article-Number = {114012},
ISSN = {1010-6030},
EISSN = {1873-2666},
Unique-ID = {WOS:000884222100004},
}

@article{ WOS:000802515500001,
Author = {Chepurnenko, Anton},
Title = {Nonlinear Rheological Processes Modeling in Three-Layer Plates with a
   Polyurethane Foam Core},
Journal = {POLYMERS},
Year = {2022},
Volume = {14},
Number = {10},
Month = {MAY},
Abstract = {Introduction: Three-layer structures with a polyurethane foam filler are
   widely used in construction as roofing and wall panels. The purpose of
   this work is to develop a method for calculating the bending of
   three-layer plates with a polyurethane foam filler, taking into account
   the nonlinear creep of the middle layer. The non-linear Maxwell-Gurevich
   equation is used as the polyurethane foam creep law. Methods: In the
   article, the system of resolving the equations is obtained, and the
   solution is carried out numerically by the finite difference method in
   combination with the Euler method in a MATLAB environment. An analytical
   solution is also obtained for a plate hinged along the contour. Results:
   The developed model and calculation algorithms are verified by
   comparison with the calculation in the ANSYS software package. A
   comparison with the calculation according to the linear theory is also
   carried out, and the effects caused by the non-linear creep of
   polyurethane foam are revealed. Conclusion: It has been established that
   when nonlinear creep is taken into account, in contrast to the linear
   law, the stresses in the plate are not constant in time. In the faces,
   at the initial stage, the stresses increase with a subsequent return to
   the initial values, and in the filler, on the contrary, the stresses at
   the initial stage decrease. These results indicate the need to take into
   account the nonlinear creep of polyurethane foam in the calculation of
   sandwich panels.},
DOI = {10.3390/polym14102093},
Article-Number = {2093},
EISSN = {2073-4360},
ResearcherID-Numbers = {Chepurnenko, Anton/E-4692-2017
   },
ORCID-Numbers = {Chepurnenko, Anton/0000-0002-9133-8546},
Unique-ID = {WOS:000802515500001},
}

@article{ WOS:000876275900001,
Author = {Kulkarni, Rishikesh and Wang, Catherine and Bertozzi, Carolyn},
Title = {Analyzing nested experimental designs-A user-friendly resampling method
   to determine experimental significance},
Journal = {PLOS COMPUTATIONAL BIOLOGY},
Year = {2022},
Volume = {18},
Number = {5},
Month = {MAY},
Abstract = {While hierarchical experimental designs are near-ubiquitous in
   neuroscience and biomedical research, researchers often do not take the
   structure of their datasets into account while performing statistical
   hypothesis tests. Resampling-based methods are a flexible strategy for
   performing these analyses but are difficult due to the lack of
   open-source software to automate test construction and execution. To
   address this, we present Hierarch, a Python package to perform
   hypothesis tests and compute confidence intervals on hierarchical
   experimental designs. Using a combination of permutation resampling and
   bootstrap aggregation, Hierarch can be used to perform hypothesis tests
   that maintain nominal Type I error rates and generate confidence
   intervals that maintain the nominal coverage probability without making
   distributional assumptions about the dataset of interest. Hierarch makes
   use of the Numba JIT compiler to reduce p-value computation times to
   under one second for typical datasets in biomedical research. Hierarch
   also enables researchers to construct user-defined resampling plans that
   take advantage of Hierarch's Numba-accelerated functions.
   Author summaryAn important step in analyzing experimental data is
   quantifying uncertainty in the experimenter's conclusions. One mechanism
   for doing so is by using a statistical hypothesis test, which allows the
   experimenter to control what percentage of the time they make erroneous
   conclusions over the course of their career. Biological experimental
   designs often have hierarchical data-gathering schemes that traditional
   hypothesis tests are not well-suited for (for example, an experimenter
   may make measurements of several tissue samples that were collected from
   subjects who were given a treatment). While traditional tests can be
   adapted to hierarchical experimental designs, we propose a simple
   resampling-based hypothesis test that applies to a variety of
   experimental designs while maintaining control over error rate. In this
   manuscript, we describe Hierarch, the Python package that enables users
   to carry out this test and validate it under several conditions.},
DOI = {10.1371/journal.pcbi.1010061},
Article-Number = {e1010061},
ISSN = {1553-734X},
EISSN = {1553-7358},
ORCID-Numbers = {Kulkarni, Rishikesh/0000-0002-6422-149X
   Wang, Catherine/0000-0001-9855-6194},
Unique-ID = {WOS:000876275900001},
}

@article{ WOS:000797924800005,
Author = {Zhou, Xiaoping and Li, Haoran and Wang, Jia and Zhao, Jichao and Xie,
   Qingsheng and Li, Lei and Liu, Jiayin and Yu, Jun},
Title = {CloudFAS: Cloud-based building fire alarm system using Building
   Information Modelling},
Journal = {JOURNAL OF BUILDING ENGINEERING},
Year = {2022},
Volume = {53},
Month = {AUG 1},
Abstract = {Building fires are a common urban disaster. The emergence of high-rise,
   large-scale and inner-complex buildings bring new challenges for fire
   safety and triggers new demand to upgrade traditional building fire
   alarm system (FAS). Different from current studies by deploying
   enor-mous smart fire sensors to replace FAS, this study addresses this
   issue from a novel perspective and proposes a cloud-based FAS using
   Building Information Modelling (BIM) on top of FAS, termed CloudFAS.
   Firstly, the system framework and the software architecture are
   designed. Secondly, two key technologies are presented to address two
   unresolved technical issues: private fire alarm data sharing and
   alignment of fire sensors with the BIM model. A cloud gateway for fire
   sensors is developed to address the first problem by capturing the fire
   alarm data from the fire alarm control unit through the IEEE 1824
   standard. Noticing that the fire sensor locations are listed in a sensor
   installation spreadsheet using natural language, termed as sensor
   location table (SLT). A natural language processing (NLP)-based
   sensor-BIM alignment algorithm is proposed to automatically match fire
   sensors with the BIM model through SLT, which enables to display fire
   sensor statuses in proper places in the 3D BIM model. Finally, a
   concrete case study from the China Construction Library is presented,
   which verifies the effectiveness of our proposed CloudFAS. Our CloudFAS
   is built on top of traditional FAS. If the fire alarm control unit
   follows the IEEE 1824 standard and an SLT is available, then CloudFAS
   can upgrade the traditional FAS in existing buildings effortlessly with
   its BIM model. Moreover, the cloud gateway for fire sensors contributes
   to addressing the private data sharing problem using IEEE 1824 standard,
   and the NLP-based sensor-BIM alignment algorithm can promote the
   adoption of BIM in the building operation phase.},
DOI = {10.1016/j.jobe.2022.104571},
EarlyAccessDate = {APR 2022},
Article-Number = {104571},
EISSN = {2352-7102},
ResearcherID-Numbers = {liu, jia/HKE-9796-2023
   li, jiawei/HOA-5023-2023},
Unique-ID = {WOS:000797924800005},
}

@article{ WOS:000803847900006,
Author = {Rheima, Ahmed Mahdi and Khadom, Anees A. and Kadhim, Mustafa M.},
Title = {Removal of Cibacron Blue P-6B dye from aqueous solution using
   synthesized anatase titanium dioxide nanoparticles: Thermodynamic,
   kinetic, and theoretical investigations},
Journal = {JOURNAL OF MOLECULAR LIQUIDS},
Year = {2022},
Volume = {357},
Month = {JUL 1},
Abstract = {Dye pollution is a serious issue in the water, especially since textile
   mills are the primary source. Nanomaterials have been used to solve this
   problem in many ways, resulting in several researches. Titanium dioxide
   nanoparticles (TiO2 NPs) are successfully produced in this study using
   ultraviolet (UV) irradiation process with a maximum intensity wavelength
   of 365 nm. Transmission electron microscopy (TEM) and scanning electron
   microscope (SEM) measurements are used to examine the morphology and
   form of nano-synthesized. The anatase phase crystal structure was proved
   by X-ray diffraction (XRD). The uptake of Cibacron Blue P-6B dye was
   improved when TiO2 NPs were added. At a contact duration of 70 min,
   Cibacron Blue P-6B dye adsorption onto TiO2 nanoparticles was faster.
   The adsorption process was best described by Freundlich isotherm (R-2 >
   0.981) and pseudo-second-order (R-2 > 0.9866) models. The thermodynamic
   parameters were calculated using the experimental van't Hoff equation.
   The TiO2-Dye adsorption process was simulated theoretically by the
   principles of the Density Functional Theory (DFT) using the Gaussian 09
   package software. Furthermore, the regeneration process of TiO2 was
   addressed. It was found that the breaking bond energy was lower than the
   activation energy required for TiO2 recovery. (C) 2022 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.molliq.2022.119102},
EarlyAccessDate = {APR 2022},
Article-Number = {119102},
ISSN = {0167-7322},
EISSN = {1873-3166},
ResearcherID-Numbers = {M. Kadhim, Mustafa M./AAQ-2456-2020
   Rheima, Ahmed/AAM-7734-2020
   Rheima, Ahmed Mahdi/GQQ-5973-2022},
ORCID-Numbers = {M. Kadhim, Mustafa M./0000-0001-5272-1801
   Rheima, Ahmed/0000-0003-3533-3393
   },
Unique-ID = {WOS:000803847900006},
}

@article{ WOS:000793324000001,
Author = {Wu, Zeyu and Ji, Xiaowei and Shan, Chao and Song, Jie and Zhao, Jin},
Title = {Exploring the pharmacological components and effective mechanism of Mori
   Folium against periodontitis using network pharmacology and molecular
   docking},
Journal = {ARCHIVES OF ORAL BIOLOGY},
Year = {2022},
Volume = {139},
Month = {JUL},
Abstract = {Objectives: To investigate the main active components, potential targets
   of action and analyze the potential molecular mechanisms of Mori Folium
   in preventing and treating periodontitis using network pharmacology and
   molecular docking methods.Materials and methods: The main components and
   action targets of Mori Folium were obtained in TCMSP and ETCM databases,
   and then the action targets of Mori Folium components were inversing
   screening using Swiss Target Prediction and BATMAN-TCM databases.
   Targets associated with periodontitis were retrieved from OMIM,
   Genecard, DrugBank, NCBI Gene and DisGeNET databases. Intersectional
   targets of Mori Folium and periodontitis were obtained by Venn analysis.
   Construction of an ``active components-targets{''} network to prevent
   and treat periodontitis in Mori Folium using Cytoscape 3.8.0. The STRING
   database was used to construct the protein-protein interaction (PPI)
   network of intersecting targets, and the core network was screened using
   CytoNCA and MCODE plug-ins. Gene ontology (GO) and Kyoto encyclopedia of
   genes and genomes (KEGG) enrichment analyses were performed using the
   ClusterProfile package of R software, and then the ``Mori Folium active
   components-targets-signaling pathway{''} network was constructed using
   Cytoscape software. Molecular docking was performed using AutoDock Vina
   software, and Pymol and LigPlus visualized the results.Results: Sixteen
   active components and 1048 targets were screened from Mori Folium, of
   which 164 were intersectional with periodontitis targets and were
   considered potential therapeutic targets. The ``Mori Folium active
   components-action targets{''} network identified Quercetin, Moracin D,
   Moracin E, Moracin G, Moracin H and Moracin B as the main active
   ingredients of Mori Folium for the prevention and treatment of
   periodontitis. PPI network analysis revealed interleukin 6 (IL6),
   albumin (ALB), tumor necrosis factor (TNF), vascular endothelial growth
   factor A (VEGFA), RAC-alpha serine/threonine-protein kinase (AKT1),
   cellular tumor antigen p53 (TP53), prostaglandin G/H synthase 2 (PTGS2),
   pro-epidermal growth factor (EGF), matrix metalloproteinase 9 (MMP9) and
   interleukin 6 (IL10) as the top 10 core potential targets. GO and KEGG
   enrichment analyses showed that the action targets of Mori Folium
   against periodontitis were mainly related to the response to bacterium
   and their lipopolysaccharide, angiogenesis and reactive oxygen species
   metabolic process, as well as through signaling pathways that regulate
   processes related to the accumulation of advanced glycation end products
   (AGEs), response to oxidative stress, response to inflammatory, and
   osteoclast differentiation during the development of the disease.
   Molecular docking revealed that Quercetin, Moracin D, Moracin E, Moracin
   G, Moracin H and Moracin B were able to bind stably to AKT1, PTGS2 and
   ESR1 targets, with Moracin E showing the most stable structure after
   binding to AKT1.Conclusions: In conclusion, this study revealed the
   active components, potential targets of action and the potential
   molecular mechanisms and pharmacological activities involved in the
   prevention and treatment of periodontitis in Mori Folium, providing a
   reference for the development of drugs from Mori Folium for the
   prevention and treatment of periodontitis.},
DOI = {10.1016/j.archoralbio.2022.105391},
EarlyAccessDate = {APR 2022},
Article-Number = {105391},
ISSN = {0003-9969},
EISSN = {1879-1506},
ORCID-Numbers = {Wu, Zeyu/0000-0002-4634-0663
   Zhao, Jin/0000-0002-5207-4680},
Unique-ID = {WOS:000793324000001},
}

@article{ WOS:000806193600006,
Author = {Alzahrani, Bander and Chaudhry, Shehzad Ashraf},
Title = {An Identity-Based Encryption Method for SDN-Enabled Source Routing
   Systems},
Journal = {SECURITY AND COMMUNICATION NETWORKS},
Year = {2022},
Volume = {2022},
Month = {APR 13},
Abstract = {In this study, we consider endpoints communicating over a
   software-defined networking (SDN)-based architecture using source
   routing, i.e., packets are routed through a path selected by the packet
   sender, and we provide a security solution that enforces the selected
   path. In particular, our solution allows a sender to select the path
   that a packet should go through using a constant-size cryptographic
   construction which is referred to as the authenticator. A recipient can
   examine an authenticator and verify that the received packet has
   followed the path selected by the sender. Additionally, any intermediate
   ``programmable{''} switch can verify whether or not it is included in
   the path of a packet. Our solution can be used even for paths that
   include multiple recipients (e.g., multicast paths), as well as multiple
   parallel paths (e.g., multipath transmissions). We implement our
   solution by leveraging identity-based encryption (IBE), so it can be
   used by any sender that knows the identifiers of the links that compose
   the desired path, i.e., information that the sender usually already
   knows as part of the source routing protocol. Our solution is realistic
   since it can be implemented over a variety of platforms with tolerable
   overhead.},
DOI = {10.1155/2022/1942097},
Article-Number = {1942097},
ISSN = {1939-0114},
EISSN = {1939-0122},
ResearcherID-Numbers = {ALZAHRANI, BANDER/C-6585-2018
   Chaudhry, Shehzad Ashraf/Y-3430-2019
   },
ORCID-Numbers = {Chaudhry, Shehzad Ashraf/0000-0002-9321-6956
   Alzahrani, Bander/0000-0002-7118-0761},
Unique-ID = {WOS:000806193600006},
}

@article{ WOS:000789656500011,
Author = {Poole, David and Vallejo, Jorge L. Galvez and Gordon, Mark S.},
Title = {A Task-Based Approach to Parallel Restricted Hartree-Fock Calculations},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Year = {2022},
Volume = {18},
Number = {4},
Pages = {2144-2161},
Month = {APR 12},
Abstract = {In recent years, parallelism via multithreading has become extremely
   important to the optimization of high-performance electronic structure
   theory codes. Such multithreading is generally achieved via OpenMP
   constructs, using a fork-join threading model to enable thread-level
   data parallelism within the code. An alternative approach to
   multithreading is task-based parallelism, which displays multiple
   benefits relative to fork-join thread parallelism. A novel Restricted
   Hartree-Fock (RHF)algorithm, utilizing task-based parallelism to achieve
   optimal performance, wasdeveloped and implemented into the JuliaChem
   electronic structure theory softwarepackage. The new RHF algorithm
   utilizes a unique method of shell quartet batchcreation, enabling
   construction and distribution offine-grained shell quartet batches in a
   load-balanced manner using the Julia taskconstruct. These shell quartet
   batches are then distributed statically across message-passing interface
   (MPI) ranks and dynamicallyacross threads within an MPI rank, requiring
   no explicit inter-rank or interthread synchronization to do so. Compared
   to the hybrid MPI/OpenMP RHF algorithm present in the GAMESS software
   package, the task-based algorithm demonstrates speedups of up to similar
   to 40\% for systems in the S22(3) test set of molecules, with system
   sizes up to similar to 1000 basis functions. The Julia Chem
   algorithmdemonstrates the viability of both the task-based parallelism
   model and the Julia programming language for construction of performant
   electronic structure theory codes targeting systems of a size of
   chemical interest},
DOI = {10.1021/acs.jctc.1c00820},
ISSN = {1549-9618},
EISSN = {1549-9626},
Unique-ID = {WOS:000789656500011},
}

@article{ WOS:000783991200003,
Author = {Nallakukkala, Sirisha and Abulkhair, Hani and Alsaiari, Abdulmohsen and
   Ahmad, Iqbal and Almatrafi, Eydhah and Bamaga, Omar and Lal, Bhajan and
   Shariff, Azmi Mohd},
Title = {Suitable Binary and Ternary Thermodynamic Conditions for Hydrate
   Mixtures of CH4, CO2, and C3H8 for Gas Hydrate-Based Applications},
Journal = {ACS OMEGA},
Year = {2022},
Volume = {7},
Number = {13},
Pages = {10877-10889},
Month = {APR 5},
Abstract = {The selection of suitable hydrate formers and their respective gas
   composition for high hydrate formation, driving force is critical to
   achieve high water recovery and metal removal efficiency in the
   hydrate-based desalination process. This study presents a feasibility
   analysis on the possible driving force and subcooling temperatures for
   the binary and ternary mixtures of methane, carbon dioxide, and propane
   for hydrates-based desalination process. The driving force and
   subcooling for the gas systems was evaluated by predicting their hydrate
   formation phase boundary conditions in 2 wt \% NaCI systems at pressure
   ranges from 2.0-4.0 MPa and temperatures of 1-4 degrees C using modified
   Peng-Robinson equation of state in the PVTSim software package. The
   results suggest that the driving force of CH4 + C3H8 and CO2 + C3H8
   binary systems are similar to their ternary systems. Thus, the use of
   binary systems is preferable and simpler than the ternary systems. For
   binary gas composition, CO2 + C3H8 (70:30) exhibited a higher subcooling
   temperature of 8.07 degrees C and driving force of 1.49 MPa in the
   presence of 2 wt \% aqueous solution. In the case of the ternary system,
   CH4-C3H8-CO2 gas composition of 10:80:10 provided a good subcooling
   temperature of 12.86 degrees C and driving force of 1.657 MPa for
   hydrate formation. The results favor CO2-C3H8 as a preferred hydrate
   former for hydrate-based desalination. This is attributed to the
   formation of sII structure and it constitutes 136 water molecules which
   signifies a huge potential of producing more quantities of treated
   water.},
DOI = {10.1021/acsomega.1c06186},
ISSN = {2470-1343},
ResearcherID-Numbers = {abulkhair, hani/GMX-4736-2022
   Abulkhair, Hani/AAY-2108-2020
   Alsaiari, Abdulmohsen/AAY-3815-2020
   Almatrafi, Eydhah/AAG-8955-2020
   nallakukkala, sirisha/F-1298-2018},
ORCID-Numbers = {abulkhair, hani/0000-0003-1775-9243
   Almatrafi, Eydhah/0000-0003-2009-8335
   Lal, Bhajan/0000-0002-1731-4466
   Alsaiari, Abdulmohsen/0000-0001-9697-9433
   nallakukkala, sirisha/0000-0001-8738-8594},
Unique-ID = {WOS:000783991200003},
}

@article{ WOS:000786106700001,
Author = {Ben Mahmoud, Basma and Lehoux, Nadia and Blanchet, Pierre and Cloutier,
   Caroline},
Title = {Barriers, Strategies, and Best Practices for BIM Adoption in Quebec
   Prefabrication Small and Medium-Sized Enterprises (SMEs)},
Journal = {BUILDINGS},
Year = {2022},
Volume = {12},
Number = {4},
Month = {APR},
Abstract = {Prefabricated construction has long faced problems due to the industry's
   fragmentation. Building Information Modeling (BIM) has thus appeared as
   an efficient solution to provide a favorable environment for efficient
   completion of projects. Despite its benefits, implementing BIM
   successfully in small and medium-sized enterprises (SMEs), which
   represent the vast majority of manufacturers in Quebec, requires deep
   risk analysis and rigorous strategies. Hence, this work aims to study
   BIM implementation barriers, strategies, and best practices in wood
   prefabrication for SMEs through a literature review, semi-structured
   interviews, and an online survey. After qualitative content analysis, 30
   critical barriers, 7 strategic milestones, and 31 best practices to
   maximize BIM benefits were revealed. One of the critical barriers
   concerns the effort required to develop BIM software libraries and
   programs to translate information from the BIM model to production
   equipment. Among the best strategies, it is essential to start by
   analyzing the current business model of the SMEs and to appoint a small
   BIM committee whose main responsibilities are management, coordination,
   and modeling. The prevalent best practices were to support the
   implementation team and encourage communication and collaboration.
   Previous studies show that BIM is not fully exploited in prefabrication
   for various reasons. This study highlights the critical barriers,
   strategies, and best practices for BIM adoption and proposes a framework
   for BIM implementation in prefabrication SMEs in Quebec, Canada. It also
   provides a summary of current knowledge and guidelines to promote BIM
   adoption in this sector.},
DOI = {10.3390/buildings12040390},
Article-Number = {390},
EISSN = {2075-5309},
ResearcherID-Numbers = {Blanchet, Pierre/C-2455-2014
   },
ORCID-Numbers = {Blanchet, Pierre/0000-0002-6348-0289
   Ben Mahmoud, Basma/0000-0002-3299-9387},
Unique-ID = {WOS:000786106700001},
}

@article{ WOS:000770306000007,
Author = {Jha, Ishan and Pathak, Krishna K. and Jha, Mrigank and Ranjan, Ashutosh},
Title = {A Comparative Study of Gradient Descent Method and a Novel Non-Gradient
   Method for Structural Shape Optimization},
Journal = {INTERNATIONAL JOURNAL OF MATHEMATICAL ENGINEERING AND MANAGEMENT
   SCIENCES},
Year = {2022},
Volume = {7},
Number = {2},
Pages = {258-271},
Month = {APR},
Abstract = {Motivated by the works on non-gradient techniques in the domain of shape
   optimization of the structure, the present work intends to suggest a
   novel non-gradient procedure for shape optimization of structures and
   compare it to an existing gradient-based method. The presented technique
   optimizes the shape of structural parts using a fuzzy controlled
   integrated zero-order methodology incorporating the notion of design
   elements and automated mesh construction with mesh refinement at each
   iteration. The movement of nodes and convergence monitoring is taken
   care of using the triangular fuzzy membership function. The changes in
   shape occur according to the selected target maximum shear stress
   (sigma(t)) with a view of reaching as near to the target as possible at
   all the points. The present methodology is packaged in a piece of
   software termed GSO (Gradientless shape optimization) coded in FORTRAN
   language. To explain the efficacy of the current approach, a few basic
   structural shapes have been optimized under various constraints, and the
   results of the same are compared to those obtained using Optistruct (a
   part of software suite HyperWorks from Altair engineering), which works
   on gradient descent method. The proposed approach works well and
   produces more industry fabricable results than what is produced by the
   gradient descent method in Optistruct.},
DOI = {10.33889/IJMEMS.2022.7.2.017},
ISSN = {2455-7749},
ORCID-Numbers = {Jha, Ishan/0000-0002-2381-0482},
Unique-ID = {WOS:000770306000007},
}

@article{ WOS:000785453200001,
Author = {Saadat, Nima P. and van Aalst, Marvin and Ebenhoeh, Oliver},
Title = {Network Reconstruction and Modelling Made Reproducible with moped},
Journal = {METABOLITES},
Year = {2022},
Volume = {12},
Number = {4},
Month = {APR},
Abstract = {Mathematical modeling of metabolic networks is a powerful approach to
   investigate the underlying principles of metabolism and growth. Such
   approaches include, among others, differential-equation-based modeling
   of metabolic systems, constraint-based modeling and metabolic network
   expansion of metabolic networks. Most of these methods are well
   established and are implemented in numerous software packages, but these
   are scattered between different programming languages, packages and
   syntaxes. This complicates establishing straight forward pipelines
   integrating model construction and simulation. We present a Python
   package moped that serves as an integrative hub for reproducible
   construction, modification, curation and analysis of metabolic models.
   moped supports draft reconstruction of models directly from
   genome/proteome sequences and pathway/genome databases utilizing GPR
   annotations, providing a completely reproducible model construction and
   curation process within executable Python scripts. Alternatively,
   existing models published in SBML format can be easily imported. Models
   are represented as Python objects, for which a wide spectrum of
   easy-to-use modification and analysis methods exist. The model structure
   can be manually altered by adding, removing or modifying reactions, and
   gap-filling reactions can be found and inspected. This greatly supports
   the development of draft models, as well as the curation and testing of
   models. Moreover, moped provides several analysis methods, in particular
   including the calculation of biosynthetic capacities using metabolic
   network expansion. The integration with other Python-based tools is
   facilitated through various model export options. For example, a model
   can be directly converted into a CobraPy object for constraint-based
   analyses. moped is a fully documented and expandable Python package. We
   demonstrate the capability to serve as a hub for integrating
   reproducible model construction and curation, database import, metabolic
   network expansion and export for constraint-based analyses.},
DOI = {10.3390/metabo12040275},
Article-Number = {275},
EISSN = {2218-1989},
ResearcherID-Numbers = {Ebenhoeh, Oliver/C-8974-2011
   },
ORCID-Numbers = {Ebenhoeh, Oliver/0000-0002-7229-7398
   van Aalst, Marvin/0000-0002-7434-0249
   Saadat, Nima Philipp/0000-0002-1262-6591},
Unique-ID = {WOS:000785453200001},
}

@article{ WOS:000780555900001,
Author = {Tian, Zhenyu and You, Jiali and Ni, Hong},
Title = {M-Emu: A Platform for Multicast Emulation},
Journal = {ELECTRONICS},
Year = {2022},
Volume = {11},
Number = {7},
Month = {APR},
Abstract = {Network layer multicast research is an important field of network
   research that requires simulators or emulators to support
   Software-Defined Networking (SDN) as well as to provide a specific
   structure at the network layer to facilitate packet forwarding, such as
   a multicast tree. The existing emulation platforms cannot effectively
   support the emulation of certain key multicast technologies, such as the
   Grafting Point (GP)-selection method and Rendezvous Point (RP)-selection
   method, for the following reasons: First, the programmable data plane of
   the existing emulation platform has many defects, such as the inability
   to process packet scheduling tasks, the prohibition of dynamic memory
   allocation and loops with unknown iteration counts, which make it
   difficult to deploy complex multicast protocols and algorithms.
   Secondly, at present, no emulation platform integrates network layer
   multicast emulation functions. As a result, users need to develop the
   multicast tree construction and maintenance mechanism in advance, which
   makes experiments laborious. To solve the above problems, based on NS4,
   we designed a multicast emulation platform, M-Emu. M-Emu presents a
   Service-Forwarding Architecture, which enables the data plane to deploy
   arbitrary complex protocols and algorithms. Based on the
   Service-Forwarding Architecture, M-Emu integrates a Multicast-Emulation
   Framework, which has a complete multicast tree construction and
   maintenance mechanism. We explain in detail how the various parts of
   M-Emu cooperate to complete the multicast emulation with an example and
   prove that M-Emu is efficient in CPU and memory consumption, etc.,
   through a large number of experiments.},
DOI = {10.3390/electronics11071152},
Article-Number = {1152},
EISSN = {2079-9292},
Unique-ID = {WOS:000780555900001},
}

@article{ WOS:000804196500046,
Author = {Mathema, Vivek Bhakta and Duangkumpha, Kassaporn and Wanichthanarak,
   Kwanjeera and Jariyasopit, Narumol and Dhakal, Esha and
   Sathirapongsasuti, Nuankanya and Kitiyakara, Chagriya and
   Sirivatanauksorn, Yongyut and Khoomrung, Sakda},
Title = {CRISP: a deep learning architecture for GC x GC-TOFMS contour ROI
   identification, simulation and analysis in imaging metabolomics},
Journal = {BRIEFINGS IN BIOINFORMATICS},
Year = {2022},
Volume = {23},
Number = {2},
Month = {MAR 10},
Abstract = {Two-dimensional gas chromatography-time-of-flight mass spectrometry (GC
   x GC-TOFMS) provides a large amount of molecular information from
   biological samples. However, the lack of a comprehensive compound
   library or customizable bioinformatics tool is currently a challenge in
   GC x GC-TOFMS data analysis. We present an open-source deep learning
   (DL) software called contour regions of interest (ROI) identification,
   simulation and untargeted metabolomics profiler (CRISP). CRISP
   integrates multiple customizable deep neural network architectures for
   assisting the semi-automated identification of ROIs, contour synthesis,
   resolution enhancement and classification of GC x GC-TOFMS-based contour
   images. The approach includes the novel aggregate feature representative
   contour (AFRC) construction and stacked ROIs. This generates an unbiased
   contour image dataset that enhances the contrasting characteristics
   between different test groups and can be suitable for small sample
   sizes. The utility of the generative models and the accuracy and
   efficacy of the platform were demonstrated using a dataset of GC x
   GC-TOFMS contour images from patients with late-stage diabetic
   nephropathy and healthy control groups. CRISP successfully constructed
   AFRC images and identified over five ROIs to create a deepstacked
   dataset. The high fidelity, 512 x 512-pixels generative model was
   trained as a generator with a Frechet inception distance of 0.96 and a
   classification accuracy of >95.00\% for datasets with and without column
   bleed. Overall, CRISP demonstrates good potential as a DL-based approach
   for the rapid analysis of 4-D GC x GC-TOFMS untargeted metabolite
   profiles by directly implementing contour images. CRISP is available at
   https://github.com/vivekmathema/GCxGC- CRISP.
   {[}GRAPHICS]
   .},
DOI = {10.1093/bib/bbab550},
Article-Number = {bbab550},
ISSN = {1467-5463},
EISSN = {1477-4054},
ORCID-Numbers = {MATHEMA, VIVEK BHAKTA/0000-0003-3916-9949
   Khoomrung, Sakda/0000-0001-9461-8597},
Unique-ID = {WOS:000804196500046},
}

@inproceedings{ WOS:000777601300086,
Author = {Feder, Maddalena and Giusti, Andrea and Vidoni, Renato},
Editor = {Longo, F and Affenzeller, M and Padovano, A},
Title = {An approach for automatic generation of the URDF file of modular robots
   from modules designed using SolidWorks},
Booktitle = {3RD INTERNATIONAL CONFERENCE ON INDUSTRY 4.0 AND SMART MANUFACTURING},
Series = {Procedia Computer Science},
Year = {2022},
Volume = {200},
Pages = {858-864},
Note = {3rd International Conference on Industry 4.0 and Smart Manufacturing
   (ISM), Upper Austria Univ Appl Sci, Hagenberg Campus, Linz, AUSTRIA, NOV
   17-19, 2021},
Abstract = {Modular robot manipulators promise to enhance current automation
   practice by providing higher flexibility and quick recovery in case of
   failures with respect to fixed-structure robots. The configuration of
   the modular robot control for each new assembly, to account for the new
   system kinematics and dynamics, can be time consuming and requires robot
   modelling expertise. We propose an approach for automatically generating
   the Unified Robot Description Format (URDF) file of modular robot
   manipulators, starting from the kinematic and dynamic descriptions
   expressed following the URDF of the single modules they can be composed
   of. The approach has been implemented and numerically verified by
   exploiting off-the-shelf software tools from Robot Operating System
   (ROS) libraries. (C) 2022 The Authors. Published by Elsevier B.V.},
DOI = {10.1016/j.procs.2022.01.283},
EarlyAccessDate = {MAR 2022},
ISSN = {1877-0509},
Unique-ID = {WOS:000777601300086},
}

@article{ WOS:000812123000001,
Author = {Stair, Nicholas H. and Evangelista, Francesco A.},
Title = {QFORTE: An Efficient State-Vector Emulator and Quantum Algorithms
   Library for Molecular Electronic Structure},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Year = {2022},
Volume = {18},
Number = {3},
Pages = {1555-1568},
Month = {MAR 8},
Abstract = {We introduce a novel open-source software package QFORTE, a
   comprehensive development tool for new quantum simulation algorithms.
   QFORTE incorporates functionality for handling molecular Hamiltonians,
   Fermionic encoding, ansatz construction, time evolution, and
   state-vector emulation, requiring only a classical electronic structure
   package as a dependency. QFORTE also contains black-box implementations
   of a wide variety of quantum algorithms, including variational and
   projective quantum eigensolvers, adaptive eigensolvers, quantum
   imaginary time evolution, and quantum Krylov methods. We highlight two
   features of QFORTE: (i) how the Python class structure of QFORTE enables
   the facile implementation of new algorithms, and (ii) how existing
   algorithms can be executed in just a few lines of code.},
DOI = {10.1021/acs.jctc.1c01155},
ISSN = {1549-9618},
EISSN = {1549-9626},
Unique-ID = {WOS:000812123000001},
}

@article{ WOS:000765644300004,
Author = {Temirbekova, Zh E. and Pyrkova, A. Yu},
Title = {Improving teachers' skills to integrate the microcontroller technology
   in computer engineering education},
Journal = {EDUCATION AND INFORMATION TECHNOLOGIES},
Year = {2022},
Volume = {27},
Number = {6},
Pages = {8381-8412},
Month = {JUL},
Abstract = {Currently, the study of microcontroller and microcircuits by students is
   becoming very important and in demand when acquiring competencies in the
   computer engineering educational program. This knowledge and skills will
   be necessary for the employment of graduates in industry, science and
   education. There are more and more large technical systems that require
   an understanding of the architecture of these systems and the skills to
   use these systems in a safe and reliable way. This article discusses
   technologies of fully homomorphic encryption that allow performing
   operations on encrypted data without disclosing them, so they have a
   huge potential for use in solving problems of storing and processing
   personal data. The growing interest in such technologies has led to the
   emergence of software tools and libraries that support fully homomorphic
   encryption. However, due to the relatively young age of this field of
   cryptography, standards and guidelines for the use of fully homomorphic
   encryption schemes are still in development. Thus, the use of these
   libraries without paying attention to the issues of cryptographic
   strength of the schemes used may have significant information security
   risks. In this work, algorithms, principles and methods for creating
   libraries on the Arduino platform are investigated and developed, which
   can be used as guidelines for the development of complexes of this kind.
   Arduino is increasingly being used for training in technical educational
   programs. As a result, every year at various conferences on engineering
   education and in journals, many articles are published on the
   integration of Arduino in teaching.},
DOI = {10.1007/s10639-021-10875-8},
EarlyAccessDate = {MAR 2022},
ISSN = {1360-2357},
EISSN = {1573-7608},
Unique-ID = {WOS:000765644300004},
}

@article{ WOS:000764915100001,
Author = {Walker, Logan A. and Williams, Jennifer S. and Li, Ye and Roossien,
   Douglas H. and Lee, Wei Jie and Michki, Nigel S. and Cai, Dawen},
Title = {nGauge: Integrated and Extensible Neuron Morphology Analysis in Python},
Journal = {NEUROINFORMATICS},
Year = {2022},
Volume = {20},
Number = {3},
Pages = {755-764},
Month = {JUL},
Abstract = {The study of neuron morphology requires robust and comprehensive methods
   to quantify the differences between neurons of different subtypes and
   animal species. Several software packages have been developed for the
   analysis of neuron tracing results stored in the standard SWC format.
   The packages, however, provide relatively simple quantifications and
   their non-extendable architecture prohibit their use for advanced data
   analysis and visualization. We developed nGauge, a Python toolkit to
   support the parsing and analysis of neuron morphology data. As an
   application programming interface (API), nGauge can be referenced by
   other popular open-source software to create custom informatics analysis
   pipelines and advanced visualizations. nGauge defines an extendable data
   structure that handles volumetric constructions (e.g. soma), in addition
   to the SWC linear reconstructions, while remaining lightweight. This
   greatly extends nGauge's data compatibility.},
DOI = {10.1007/s12021-022-09573-8},
EarlyAccessDate = {MAR 2022},
ISSN = {1539-2791},
EISSN = {1559-0089},
ORCID-Numbers = {Michki, Nigel/0000-0003-0403-0648
   Li, Ye/0000-0002-8647-384X
   Williams, Jennifer/0000-0002-3541-2756
   Walker, Logan/0000-0002-5378-3315},
Unique-ID = {WOS:000764915100001},
}

@inproceedings{ WOS:000766311100056,
Author = {Gusev, G. and Shardakov, I},
Editor = {Moreira, P and Tavares, P},
Title = {Research and prediction of the stress-strain state of construction
   facilities in the undermined territories},
Booktitle = {4TH INTERNATIONAL CONFERENCE ON STRUCTURAL INTEGRITY (ICSI 2021)},
Series = {Procedia Structural Integrity},
Year = {2022},
Volume = {37},
Pages = {425-430},
Note = {4th International Conference on Structural Integrity (ICSI), ELECTR
   NETWORK, AUG 30-SEP 02, 2021},
Abstract = {Deformations of the Earth's surface in the territories of potash
   deposits last for decades. The processes of deformation of the Earth's
   surface are caused by the displacement of layers of soil mass lying
   under the surface. Shifts in the layers of the soil massif are caused by
   the organization of mine workings. All this determines the regularity of
   the evolution of the stress-strain state of structures located in the
   mine construction zone. All this makes the task of studying the patterns
   of such behavior urgent. The influence of deformation of the soil mass
   on the load-bearing elements of various structures is studied. The level
   of deformations in the soil layer, which corresponds to the critical
   condition of the structure, has been determined. The data are presented
   on the basis of numerical solutions of boundary value problems on the
   interaction of the structure with the soil mass. The simulation was
   carried out using the ANSYS software package. The simulation results
   contain information about the distribution of the parameters of the
   stress-strain state of the structure depending on the deformation of the
   undermined zone. The limiting parameters of the deformation state of
   both the foundation array and the structure itself are determined. (C)
   2022 The Authors. Published by Elsevier B.V.},
DOI = {10.1016/j.prostr.2022.01.105},
EarlyAccessDate = {FEB 2022},
ISSN = {2452-3216},
Unique-ID = {WOS:000766311100056},
}

@article{ WOS:000796435800041,
Author = {Solomon, Damtew and Bekele, Kebebe and Atlaw, Daniel and Mamo, Ayele and
   Gezahegn, Habtamu and Regasa, Tadele and Negash, Getahun and Nigussie,
   Eshetu and Zenbaba, Demissu and Teferu, Zinash and Nugusu, Fikadu and
   Atlie, Gela},
Title = {Prevalence of anemia and associated factors among adult diabetic
   patients attending Bale zone hospitals, South-East Ethiopia},
Journal = {PLOS ONE},
Year = {2022},
Volume = {17},
Number = {2},
Month = {FEB 18},
Abstract = {Background
   Anemia found in diabetes patients is often unrecognized like many other
   chronic diseases. The occurrence of anemia is also an additional burden
   to the micro vascular complications of patients with diabetes. In the
   selected study structure no published data were found on the prevalence
   of anemia and associated factors in diabetic patients. Hence, the
   findings of this study are very fruitful as an input for further studies
   and after the repetition of similar studies in different frameworks. It
   is helpful as input for the development of guidelines at diabetes
   clinics to request the laboratory assessment of hemoglobin as a routine
   activity.
   Objective
   This study aimed to assess the prevalence of anemia and its associated
   factors among diabetic patients who attended Bale zone hospitals.
   Methods
   A cross-sectional study design was conducted from September 2020-to
   January 2021 GC among adult diabetic patients who had follow-up at Bale
   zone hospitals. A total of 238 study participants were determined by
   single population proportion sample size calculation formula taking
   prevalence of anemia among adult diabetic patients 19.0\%. Systematic
   random sampling technique was used to select the study participants.
   Information on demographic and associated factors of anemia in diabetic
   patients was collected using an interviewer-administered questionnaire.
   Blood sample collection was performed under aseptic conditions by a
   licensed medical laboratory professional. Data were entered into EpiData
   version 3.1, cleaned and exported to statistical package for the social
   sciences (SPSS) version 25 software tools. Logistic regression was used
   to assess factors associated with anemia in diabetic patients. P-value
   less than 0.05 and 95\% CI were considered as statistically significant.
   The odds ratios were reported to indicate the strength of associations.
   Frequencies, percentages, charts and tables were used to summarize the
   characteristics of study participants.
   Results
   In this study anemia among adult diabetic patients is 18.1\% (95\% CI
   (13.2, 23.0\%). Multivariable logistic regression analysis revealed that
   the sex of the study participants and the type of diabetes mellitus were
   found to be statistically significant to associate with anemia. The odds
   of having anemia among females are nearly three times higher when
   compared with males (AOR 2.78, 95\% CI 1.40-5.52). In addition, the odds
   of having anemia among adult diabetic patients who had type II diabetes
   mellitus (AOR 2.18, 95\%CI 1.04-4.54) were 2.18 times higher than those
   who had type I diabetes mellitus patients.
   Conclusion
   Nearly one out of five adult diabetic patients had anemia. Sex of the
   patients and the type of diabetes are associated with anemia among adult
   diabetic patients.},
DOI = {10.1371/journal.pone.0264007},
Article-Number = {e0264007},
ISSN = {1932-6203},
ResearcherID-Numbers = {Zenbaba, Demisu/AEA-6930-2022
   Atlaw, Daniel/ACX-8977-2022},
ORCID-Numbers = {Zenbaba, Demisu/0000-0002-7733-7627
   Atlaw, Daniel/0000-0002-2968-4958},
Unique-ID = {WOS:000796435800041},
}

@article{ WOS:000769415500001,
Author = {Eremina, Tatiana and Korolchenko, Dmitry and Minaylov, Denis},
Title = {Experimental Evaluation of Fire Resistance Limits for Steel
   Constructions with Fire-Retardant Coatings at Various Fire Conditions},
Journal = {SUSTAINABILITY},
Year = {2022},
Volume = {14},
Number = {4},
Month = {FEB},
Abstract = {The experimental evaluation of fire resistance limits for steel
   constructions with fire-retardant coatings consists of a lot of
   experiments on the heating of steel structures of buildings by solving a
   heat engineering problem at various fire conditions. Building design
   implies the assessment of compliance of actual fire resistance limits
   for steel constructions with the required limits. Fire resistance limits
   for steel constructions are determined for ``standard{''} temperature
   mode, and this can lead to overestimated fire resistance and
   underestimated heat influence for a real fire. Estimation of the
   convergence for ``standard{''} temperature mode and possible ``real{''}
   fire mode, as well as of the compliance of actual fire resistance limits
   with real fire conditions, was realized in the following stages:
   mathematical modeling of real fire development by the field model in
   software package Fire Dynamics Simulation (FDS) with various fire loads
   and mathematical modeling of steel construction heating for the standard
   temperature mode obtained by modeling ``real{''} fire modes (the finite
   difference method of solving the Fourier heat conduction equation at
   external and internal nonlinearities was used for modeling the process
   of steel structure heating with the implementation in the ANSYS
   mechanical software package). Experiments of the assessment of
   fire-protective paint's effectiveness were carried out for standard
   temperature mode and obtained by modeling ``real{''} fire modes. The
   equivalent fire duration dependence on fire load type was determined.
   This dependence can be taken into account in determination of fire
   resistance limits for steel constructions in warehouse building roofing.
   Fire-protective paint effectiveness was estimated for ``standard{''}
   temperature mode and various other temperature modes.},
DOI = {10.3390/su14041962},
Article-Number = {1962},
EISSN = {2071-1050},
ORCID-Numbers = {Minailov, Denis/0000-0001-9811-3908
   Eremina, Tatiana/0000-0003-1427-606X
   Korolchenko, Dmitriy Alexandrovich/0000-0002-2361-6428},
Unique-ID = {WOS:000769415500001},
}

@article{ WOS:000780137000011,
Author = {Garcia, Lionel J. and Timmermans, Mathilde and Pozuelos, Francisco J.
   and Ducrot, Elsa and Gillon, Michael and Delrez, Laetitia and Wells,
   Robert D. and Jehin, Emmanuel},
Title = {PROSE: a PYTHON framework for modular astronomic al images processing},
Journal = {MONTHLY NOTICES OF THE ROYAL ASTRONOMICAL SOCIETY},
Year = {2022},
Volume = {509},
Number = {4},
Pages = {4817-4828},
Month = {FEB},
Abstract = {To reduce and analyse astronomical images, astronomers can rely on a
   wide range of libraries providing low-level implementations of legacy
   algorithms. However, combining these routines into robust and functional
   pipelines requires a major effort that often ends up in
   instrument-specific and poorly maintainable tools, yielding products
   that suffer from a low level of reproducibility and portability. In this
   context, we present PROSE, a PYTHON framework to build modular and
   maintainable image processing pipelines. Built for astronomy, it is
   instrument-agnostic and allows the construction of pipelines using a
   wide range of building blocks, pre-implemented or user-defined. With
   this architecture, our package provides basic tools to deal with common
   tasks, such as automatic reduction and photometric extraction. To
   demonstrate its potential, we use its default photometric pipeline to
   process 26 TESS candidates follow-up observations and compare their
   products to the ones obtained withASTROIMAGEJ, the reference software
   for such endeavours. We show that PROSE produces light curves with lower
   white and red noise while requiring less user interactions and offering
   richer functionalities for reporting.},
DOI = {10.1093/mnras/stab3113},
ISSN = {0035-8711},
EISSN = {1365-2966},
ORCID-Numbers = {Pozuelos, Francisco J./0000-0003-1572-7707
   Delrez, Laetitia/0000-0001-6108-4808
   Garcia, Lionel/0000-0002-4296-2246
   Ducrot, Elsa/0000-0002-7008-6888},
Unique-ID = {WOS:000780137000011},
}

@article{ WOS:000769413100001,
Author = {Kolli, Meena Kumari and Opp, Christian and Karthe, Daniel and Kumar,
   Nallapaneni Manoj},
Title = {Web-Based Decision Support System for Managing the
   Food-Water-Soil-Ecosystem Nexus in the Kolleru Freshwater Lake of Andhra
   Pradesh in South India},
Journal = {SUSTAINABILITY},
Year = {2022},
Volume = {14},
Number = {4},
Month = {FEB},
Abstract = {Most of the world's freshwater lake ecosystems are endangered due to
   intensive land use conditions. They are subjected to anthropogenic
   stress and severely degraded because of large-scale aquafarming,
   agricultural expansion, urbanization, and industrialization. In the case
   of India's largest freshwater lake, the Kolleru freshwater ecosystem,
   environmental resources such as water and soil have been adversely
   impacted by an increase in food production, particularly through
   aquaculture. There are numerous instances where aqua farmers have
   indulged in constructing illegal fishponds. This process of aquafarming
   through illegal fishponds has continued even after significant
   restoration efforts, which started in 2006. This underlines the
   necessity of continuous monitoring of the state of the lake ecosystem in
   order to survey the effectiveness of restoration and protection
   measures. Hence, to better understand the processes of ecosystem
   degradation and derive recommendations for future management, we
   developed a web mapping application (WMA). The WMA aims to provide
   fishpond data from the current monitoring program, allowing users to
   access the fishpond data location across the lake region, demanding lake
   digitization and analysis. We used a machine learning algorithm for
   training the composite series of Landsat images obtained from Google
   Earth Engine to digitize the lake ecosystem and further analyze current
   and past land use classes. An open-source geographic information system
   (GIS) software and JavaScript library plugins including a PostGIS
   database, GeoServer, and Leaflet library were used for WMA. To enable
   the interactive features, such as editing or updating the latest
   construction of fishponds into the database, a client-server
   architecture interface was provided, finally resulting in the web-based
   model application for the Kolleru Lake aquaculture system. Overall, we
   believe that providing expanded access to the fishpond data using such
   tools will help government organizations, resource managers,
   stakeholders, and decision makers better understand the lake ecosystem
   dynamics and plan any upcoming restoration measures.},
DOI = {10.3390/su14042044},
Article-Number = {2044},
EISSN = {2071-1050},
ResearcherID-Numbers = {Karthe, Daniel/AAS-5562-2020
   Manoj Kumar, Nallapaneni/R-8605-2018
   },
ORCID-Numbers = {Karthe, Daniel/0000-0003-0288-0593
   Manoj Kumar, Nallapaneni/0000-0002-7382-7784
   Kolli, Meena Kumari/0000-0001-9795-9215
   Opp, Christian/0000-0001-7034-6945},
Unique-ID = {WOS:000769413100001},
}

@article{ WOS:000763569200001,
Author = {Melesse, Misganaw Fikrie and Aynalem, Bewket Yeserah},
Title = {Latrine utilization and associated factors in East Gojjam Zone,
   North-West Ethiopia: A community-based cross-sectional study},
Journal = {SAGE OPEN MEDICINE},
Year = {2022},
Volume = {10},
Month = {FEB},
Abstract = {Objective: This research was aimed at assessing latrine utilization and
   associated factors in East Gojjam Zone, North West Ethiopia.
   Methods: A community-based cross-sectional study was conducted on
   households of East Gojjam Zone, from 1 February to 30 May 2021.
   Multistage cluster sampling technique was used to select 806 study
   participants into the study. Data were collected through pretested
   structured interview questionnaires and direct observation. Collected
   data were entered and cleaned using EPI info version 7.2 and analysed
   using SPSS version 23 software package. Bivariable and multivariable
   logistic regression was employed to assess association of the variables
   and controlling the effect of confounders, respectively. P value less
   than 0.05 was taken as statistically significant.
   Results: The overall latrine utilization in East Gojjam Zone was found
   to be 45.4\% (95\% confidence interval = 42.2-49.1). Occupation
   (adjusted odds ratio = 2.248, 95\% confidence interval = 1.037-4.876),
   participating in model family training (adjusted odds ratio = 2.481,
   95\% confidence interval = 1.802-3.415), water availability (adjusted
   odds ratio = 2.456, 95\% confidence interval = 1.514-3.983), and type of
   latrine (adjusted odds ratio = 2.013, 95\% confidence interval
   1.648-2.972) had statistically significant association with latrine
   utilization.
   Conclusion: Latrine utilization in East Gojjam Zone was found to be low
   relative to other studies and the country's plan. It is very far apart
   from the Ethiopian latrine coverage and utilization plan (100\%).
   Occupational status, participated in the model family training, water
   availability, and type of toilet were significantly associated with
   toilet utilization. Encouraging private latrine construction with
   accessibility of water and all households participating in model family
   training may increase latrine utilization in East Gojjam Zone. Further
   observational study triangulated with qualitative research should be
   conducted to provide more strong evidence for further improvement of
   household latrine utilization status in East Gojjam Zone.},
DOI = {10.1177/20503121221074780},
Article-Number = {20503121221074780},
ISSN = {2050-3121},
ORCID-Numbers = {Aynalem, Bewket/0000-0003-2847-4841
   Melesse, Misganaw/0000-0002-6343-7651},
Unique-ID = {WOS:000763569200001},
}

@article{ WOS:000752732000001,
Author = {Muliokela, Rosemary and Uwayezu, Gilbert and Ngoc, Candide Tran and
   Barreix, Maria and Tamrat, Tigest and Kashoka, Andrew and Chizuni, Caren
   and Nyirenda, Muyereka and Ratanaprayul, Natschja and Malumo, Sarai and
   Mutabazi, Vincent and Mehl, Garrett and Munyana, Edith and Sayinzoga,
   Felix and Tuncalp, Ozge},
Title = {Integration of new digital antenatal care tools using the WHO SMART
   guideline approach: Experiences from Rwanda and Zambia},
Journal = {DIGITAL HEALTH},
Year = {2022},
Volume = {8},
Month = {FEB},
Abstract = {Objectives Digital tools for decision-support and health records can
   address the protracted process of guideline adoption at local levels and
   accelerate countries' implementation of new health policies and
   programmes. World Health Organization (WHO) launched the SMART
   Guidelines approach to support the uptake of clinical, public health,
   and data recommendations within digital systems. SMART guidelines are a
   package of tools that include Digital Adaptation Kits (DAKs), which
   distill WHO guidelines into a format that facilitates translation into
   digital systems. SMART Guidelines also include reference software
   applications known as digital modules. Methods This paper details the
   structured process to inform the adaptation of the WHO antenatal care
   (ANC) digital module to align with country-specific ANC packages for
   Zambia and Rwanda using the DAK. Digital landscape assessments were
   conducted to determine potential integrations between the ANC digital
   module and existing systems. A multi-stakeholder team consisting of
   Ministry of Health technical officers representing maternal health, HIV,
   digital health, and monitoring and evaluation at district and national
   levels was assembled to review existing guidelines to adapt the DAK.
   Results The landscape analysis resulted in considerations for
   integrating the ANC module into the broader digital ecosystems of both
   countries. Adaptations to the DAK included adding national services not
   reflected in the generic DAK and modification of decision support logic
   and indicators. Over 80\% of the generic DAK content was consistent with
   processes for both countries. The adapted DAK will inform the
   customization of country-specific ANC digital modules. Conclusion Both
   countries found that coordination between maternal and digital health
   leads was critical to ensuring requirements were accurately reflected
   within the ANC digital module. Additionally, DAKs provided a structured
   process for gathering requirements, reviewing and addressing gaps within
   existing systems, and aligning clinical content.},
DOI = {10.1177/20552076221076256},
Article-Number = {20552076221076256},
ISSN = {2055-2076},
ORCID-Numbers = {Tamrat, Tigest/0000-0001-8579-5698
   Ratanaprayul, Natschja/0000-0003-0085-2900},
Unique-ID = {WOS:000752732000001},
}

@article{ WOS:000742659300002,
Author = {Saxena, Anurag and Trivedi, Mayur and Shroff, Zubin Cyrus and Sharma,
   Manas},
Title = {Improving hospital-based processes for effective implementation of
   Government funded health insurance schemes: evidence from early
   implementation of PM-JAY in India},
Journal = {BMC HEALTH SERVICES RESEARCH},
Year = {2022},
Volume = {22},
Number = {1},
Month = {JAN 15},
Abstract = {Background Government-sponsored health insurance schemes (GSHIS) aim to
   improve access to and utilization of healthcare services and offer
   financial protection to the population. India's Ayushman Bharat Pradhan
   Mantri Jan Arogya Yojana (PM-JAY) is one such GSHIS. This paper aims to
   understand how the processes put in place to manage hospital-based
   transactions, from the time a beneficiary arrives at the hospital to
   discharge are being implemented in PM-JAY and how to improve them to
   strengthen the scheme's operation. Methods Guidelines were reviewed for
   the processes associated with hospital-based transactions, namely,
   beneficiary authentication, treatment package selection,
   preauthorization, discharge, and claims payments. Across 14 hospitals in
   Gujarat and Madhya Pradesh states, the above-mentioned processes were
   observed, and using a semi-structured interview guide fifty-three
   respondents were interviewed. The study was carried out from March 2019
   to August 2019. Results Average turn-around time for claim reimbursement
   is two to six times higher than that proposed in guidelines and tender.
   As opposed to the guidelines, beneficiaries are incurring out-of-pocket
   expenditure while availing healthcare services. The training provided to
   the front-line workers is software-centric. Hospital-based processes are
   relatively more efficient in hospitals where frontline workers have a
   medical/paramedical/managerial background. Conclusions There is a need
   to broaden capacity-building efforts from enabling frontline staff to
   operate the scheme's IT platform to developing the technical,
   managerial, and leadership skills required for them. At the hospital
   level, an empowered frontline worker is the key to efficient
   hospital-based processes. There is a need to streamline back-end
   processes to eliminate the causes for delay in the processing of claim
   payment requests. For policymakers, the most important and urgent need
   is to reduce out-of-pocket expenses. To that end, there is a need to
   both revisit and streamline the existing guidelines and ensure adherence
   to the guidelines.},
DOI = {10.1186/s12913-021-07448-3},
Article-Number = {73},
EISSN = {1472-6963},
ResearcherID-Numbers = {Trivedi, Mayur/AFU-4877-2022
   Saxena, Anurag/HJB-0833-2022
   Trivedi, Mayur/AHA-6256-2022},
ORCID-Numbers = {Saxena, Anurag/0000-0002-0814-3538
   Trivedi, Mayur/0000-0002-4532-2340},
Unique-ID = {WOS:000742659300002},
}

@article{ WOS:000748087500001,
Author = {Casas, Alexis and Bultelle, Matthieu and Motraghi, Charles and Kitney,
   Richard},
Title = {Removing the Bottleneck: Introducing cMatch-A Lightweight Tool for
   Construct-Matching in Synthetic Biology},
Journal = {FRONTIERS IN BIOENGINEERING AND BIOTECHNOLOGY},
Year = {2022},
Volume = {9},
Month = {JAN 10},
Abstract = {We present a software tool, called cMatch, to reconstruct and identify
   synthetic genetic constructs from their sequences, or a set of
   sub-sequences-based on two practical pieces of information: their
   modular structure, and libraries of components. Although developed for
   combinatorial pathway engineering problems and addressing their quality
   control (QC) bottleneck, cMatch is not restricted to these applications.
   QC takes place post assembly, transformation and growth. It has a simple
   goal, to verify that the genetic material contained in a cell matches
   what was intended to be built - and when it is not the case, to locate
   the discrepancies and estimate their severity. In terms of
   reproducibility/reliability, the QC step is crucial. Failure at this
   step requires repetition of the construction and/or sequencing steps.
   When performed manually or semi-manually QC is an extremely
   time-consuming, error prone process, which scales very poorly with the
   number of constructs and their complexity. To make QC frictionless and
   more reliable, cMatch performs an operation we have called
   ``construct-matching{''} and automates it. Construct-matching is more
   thorough than simple sequence-matching, as it matches at the functional
   level-and quantifies the matching at the individual component level and
   across the whole construct. Two algorithms (called CM\_1 and CM\_2) are
   presented. They differ according to the nature of their inputs. CM\_1 is
   the core algorithm for construct-matching and is to be used when input
   sequences are long enough to cover constructs in their entirety (e.g.,
   obtained with methods such as next generation sequencing). CM\_2 is an
   extension designed to deal with shorter data (e.g., obtained with Sanger
   sequencing), and that need recombining. Both algorithms are shown to
   yield accurate construct-matching in a few minutes (even on hardware
   with limited processing power), together with a set of metrics that can
   be used to improve the robustness of the decision-making process. To
   ensure reliability and reproducibility, cMatch builds on the highly
   validated pairwise-matching Smith-Waterman algorithm. All the tests
   presented have been conducted on synthetic data for challenging, yet
   realistic constructs - and on real data gathered during studies on a
   metabolic engineering example (lycopene production).},
DOI = {10.3389/fbioe.2021.785131},
Article-Number = {785131},
ISSN = {2296-4185},
ORCID-Numbers = {Kitney, Richard/0000-0002-6499-5209},
Unique-ID = {WOS:000748087500001},
}

@article{ WOS:000750887200001,
Author = {van Zyl, Llewellyn E. and ten Klooster, Peter M.},
Title = {Exploratory Structural Equation Modeling: Practical Guidelines and
   Tutorial With a Convenient Online Tool for Mplus},
Journal = {FRONTIERS IN PSYCHIATRY},
Year = {2022},
Volume = {12},
Month = {JAN 7},
Abstract = {Critics of positive psychology have questioned the validity of positive
   psychological assessment measures (PPAMs), which negatively affects the
   credibility and public perception of the discipline. Psychometric
   evaluations of PPAMs have shown that various instruments produce
   inconsistent factor structures between groups/contexts/times frames,
   that their predictive validity is questionable, and that popular PPAMs
   are culturally biased. Further, it would seem positive psychological
   researchers prioritize date-model-fit over measurement quality. To
   address these analytical challenges, more innovative and robust
   approaches toward the validation and evaluation of PPAMs are required to
   enhance the discipline's credibility and to advance positive
   psychological science. Exploratory Structural Equation Modeling (ESEM)
   has recently emerged as a promising alternative to overcome some of
   these challenges by incorporating the best elements from exploratory-
   and confirmatory factor analyses. ESEM is still a relatively novel
   approach, and estimating these models in statistical software packages
   can be complex and tedious. Therefore, the purpose of this paper is to
   provide novice researchers with a practical tutorial on how to estimate
   ESEM with a convenient online tool for Mplus. Specifically, we aim to
   demonstrate the use of ESEM through an illustrative example by using a
   popular positive psychological instrument: the Mental Health
   Continuum-SF. By using the MHC-SF as an example, we aim to provide (a) a
   brief overview of ESEM (and different ESEM models/approaches), (b)
   guidelines for novice researchers on how to estimate, compare, report,
   and interpret ESEM, and (c) a step-by-step tutorial on how to run ESEM
   analyses in Mplus with the De Beer and Van Zy ESEM syntax generator. The
   results of this study highlight the value of ESEM, over and above that
   of traditional confirmatory factor analytical approaches. The results
   also have practical implications for measuring mental health with the
   MHC-SF, illustrating that a bifactor ESEM Model fits the data
   significantly better than any other theoretical model.},
DOI = {10.3389/fpsyt.2021.795672},
Article-Number = {795672},
ISSN = {1664-0640},
ResearcherID-Numbers = {van Zyl, Llewellyn Ellardus/AAH-7668-2021
   Klooster, Peter M. ten/D-2905-2009},
ORCID-Numbers = {van Zyl, Llewellyn Ellardus/0000-0003-3088-3820
   Klooster, Peter M. ten/0000-0002-2565-5439},
Unique-ID = {WOS:000750887200001},
}

@inproceedings{ WOS:000893237400025,
Author = {Ait-Ameur, Yamine and Kissi, Salim Yahia and Ameur-Boulifa, Rabea and
   Seladji, Yassamin},
Editor = {Ait-Ameur, Y and Craciun, F},
Title = {Security Vulnerabilities Detection Through Assertion-Based Approach},
Booktitle = {THEORETICAL ASPECTS OF SOFTWARE ENGINEERING, TASE 2022},
Series = {Lecture Notes in Computer Science},
Year = {2022},
Volume = {13299},
Pages = {381-387},
Note = {16th International Symposium on Theoretical Aspects of Software
   Engineering (TASE), Cluj Napoca, ROMANIA, JUL 08-10, 2022},
Organization = {Babes Bolyai Univ},
Abstract = {Organizations and companies develop very complex software today. Errors
   and flaws can be introduced at different phases of the software
   development life cycle and can lead to exploitable vulnerabilities.
   Furthermore, considering that most systems are exposed to multiple users
   and environments, such flaws can lead to attacks (or actions) with
   unpredictable consequences in terms of damage and costs.
   Most research that deals with security-related issues of software
   focuses their efforts on coding errors and flaws, regardless of the
   infrastructure and platforms that run the software applications. Often,
   such analyses of software applications vulnerabilities may lack
   sufficient specification details, thus possibly miss larger systematic
   flaws, and consequently obscure the existence of serious
   vulnerabilities. Our research aims at developing a technique capable of
   discovering the security weaknesses, specifically buffer overflow
   vulnerabilities in C/C++ programs, through the analysis of source code
   combined with architecture specifications. The proposed approach relies
   on the notion of platform assertions that is, a collection of logical
   relationships used to characterize a platform (execution environment).
   In this paper, we focus on such assertions and show how vulnerabilities
   analysis of software applications can be performed with our
   assertion-based approach. Furthermore, the generation of assertion
   specifications as well as the construction of an assertion library
   including various platforms are explored.},
DOI = {10.1007/978-3-031-10363-6\_25},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-031-10363-6; 978-3-031-10362-9},
ORCID-Numbers = {Kissi, Salim Yahia/0000-0002-9222-0291},
Unique-ID = {WOS:000893237400025},
}

@article{ WOS:000709807900002,
Author = {Arzo, Sisay Tadesse and Scotece, Domenico and Bassoli, Riccardo and
   Barattini, Daniel and Granelli, Fabrizio and Foschini, Luca and Fitzek,
   Frank H. P.},
Title = {MSN: A Playground Framework for Design and Evaluation of
   MicroServices-Based sdN Controller},
Journal = {JOURNAL OF NETWORK AND SYSTEMS MANAGEMENT},
Year = {2022},
Volume = {30},
Number = {1},
Month = {JAN},
Abstract = {Software-defined networking decouples control and data plane in
   softwarized networks. This allows for centralized management of the
   network, but complete centralization of the controller functions raises
   potential issues related to failure, latency, and scalability.
   Distributed controller deployment is adopted to optimize scalability and
   latency problems. However, existing controllers are monolithic,
   resulting in code inefficiency for distributed deployment. Some seminal
   ongoing efforts have been proposed with the idea of disaggregating the
   SDN controller architecture into an assembly of various subsystems, each
   of which can be responsible for a certain controller task. These
   subsystems are typically implemented as microservices and deployed as
   virtual network functions, in particular as Docker Containers. This
   enables flexible deployment of controller functions. However, these
   proposals (e.g., mu ONOS) are still in their early stage of design and
   development, so that a full decomposition of the SDN controller is not
   been available yet. To fill that gap, this article derives some
   important design guidelines to decompose an SDN controller into a set of
   microservices. Next, it also proposes a microservices-based decomposed
   controller architecture, foreseeing communications issues between the
   controller sub-functions. These design and performance considerations
   are also proven via the implementation of the proposed architecture as a
   solution, called Micro-Services based SDN controller (MSN), based on the
   Ryu SDN controller. Moreover, MSN includes different network
   communication protocols, such as gRPC, WebSocket, and REST-API. Finally,
   we show experimental results that highlight the robustness and latency
   of the system on a networking testbed. Collected results prove the main
   pros and cons of each network communication protocol and an evaluation
   of our proposal in terms of system resilience, scalability and latency.},
DOI = {10.1007/s10922-021-09631-7},
Article-Number = {19},
ISSN = {1064-7570},
EISSN = {1573-7705},
ResearcherID-Numbers = {Arzo, Sisay Tadesse/AAU-8436-2020
   Foschini, Luca/H-6876-2015
   Corradi, Antonio/L-7480-2015
   },
ORCID-Numbers = {Arzo, Sisay Tadesse/0000-0001-9062-8499
   Foschini, Luca/0000-0001-9062-3647
   Corradi, Antonio/0000-0002-5107-1023
   Bassoli, Riccardo/0000-0002-6132-7985
   Scotece, Domenico/0000-0003-3824-197X},
Unique-ID = {WOS:000709807900002},
}

@inproceedings{ WOS:000855983300137,
Author = {Bo, Sun and Mao, Xinjun and Yang, Shuo and Chen, Long},
Editor = {Leong, HV and Sarvestani, SS and Teranishi, Y and Cuzzocrea, A and Kashiwazaki, H and Towey, D and Yang, JJ and Shahriar, H},
Title = {Towards An Efficient Searching Approach of ROS Message by Knowledge
   Graph},
Booktitle = {2022 IEEE 46TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE
   (COMPSAC 2022)},
Year = {2022},
Pages = {934-943},
Note = {46th Annual IEEE-Computer-Society International Computers, Software, and
   Applications Conference (COMPSAC) - Computers, Software, and
   Applications in an Uncertain World, ELECTR NETWORK, JUN 27-JUL 01, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {The Robot Operating System (ROS) has become the most popular robot
   development framework in the last few years, which has loosely coupled
   structure and provides remote communications between different component
   nodes. The ROS messages are critical to bridge the communication
   channels and clearly define the data structures. The developers can use
   the standardized or user-customized ROS message types to construct a
   communication channel between two component nodes uniquely. However, it
   becomes increasingly difficult for developers to find the required ROS
   message type from thousands of diverse ROS message types in ROS-based
   robotic software development. Finding the proper ROS message type is a
   non-trivial task because developers may hardly know the exact names of
   required ROS messages but only has a rough knowledge of the task domain
   features. To tackle this challenge, we construct a novel ROS Message
   Knowledge Graph (RMKG) with 4543 entities and 14320 relationships,
   including all ROS message types and message packages. We take the
   shortest path algorithm to search ROS message in RMKG by searching with
   ROS message feature or ROS message package and visualize the subgraph
   structure of the search results. Moreover, we develop a ROS message
   package library that supports fuzzy queries to find the required message
   package. A comprehensive evaluation of RMKG shows the high accuracy of
   our knowledge construction approach. A user study indicates that RMKG is
   promising in helping developers find suitable ROS message types for
   robotics software development tasks. An effect evaluation of message
   package fuzzy query shows the good effects of our fuzzy query method
   under different situations.},
DOI = {10.1109/COMPSAC54236.2022.00145},
ISBN = {978-1-6654-8810-5},
Unique-ID = {WOS:000855983300137},
}

@inproceedings{ WOS:000870300100009,
Author = {Brown, Kristopher and Patterson, Evan and Hanks, Tyler and Fairbanks,
   James},
Editor = {Behr, N and Struber, D},
Title = {Computational Category-Theoretic Rewriting},
Booktitle = {GRAPH TRANSFORMATION, ICGT 2022},
Series = {Lecture Notes in Computer Science},
Year = {2022},
Pages = {155-172},
Note = {15th International Conference on Graph Transformation (ICGT) Held as
   Part of Conference on Software Technologies - Applications and
   Foundations (STAF), Nantes, FRANCE, JUL 07-08, 2022},
Organization = {European Assoc Theoret Comp Sci; European Assoc Software Sci \& Technol;
   IFIP Working Grp 1 3 Fdn Syst Specificat},
Abstract = {We demonstrate how category theory provides specifications that can
   efficiently be implemented via imperative algorithms and apply this to
   the field of graph transformation. By examples, we show how this
   paradigm of software development makes it easy to quickly write correct
   and performant code. We provide a modern implementation of graph
   rewriting techniques at the level of abstraction of finitely-presented
   C-sets and clarify the connections between C-sets and the typed graphs
   supported in existing rewriting software. We emphasize that our
   opensource library is extensible: by taking new categorical
   constructions (such as slice categories, structured cospans, and
   distributed graphs) and relating their limits and colimits to those of
   their underlying categories, users inherit efficient algorithms for
   pushout complements and (final) pullback complements. This allows one to
   perform double-, single-, and sesquipushout rewriting over a broad class
   of data structures. Graph transformation researchers, scientists, and
   engineers can then use this library to computationally manipulate
   rewriting systems and apply them to their domains of interest.},
DOI = {10.1007/978-3-031-09843-7\_9},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-031-09843-7; 978-3-031-09842-0},
ORCID-Numbers = {Patterson, Evan/0000-0002-8600-949X},
Unique-ID = {WOS:000870300100009},
}

@article{ WOS:000770363400011,
Author = {Cesar Morales-Guzman, Carlos},
Title = {Experimental Articulated Node Designs for Folding Roof Systems},
Journal = {REVISTA DE ARQUITECTURA-BOGOTA},
Year = {2022},
Volume = {24},
Number = {1},
Pages = {106-114},
Month = {JAN-JUN},
Abstract = {The present research focuses on the design of an articulated node, based
   on the constructive test of Dr. Felix Escrig, who provided the
   constructive concepts to generate a proposal that aims to develop
   faster-folding systems, for which a series of prototypes were built to
   verify and validate the different constructive scopes that could
   generate a transformable structural system. This justified the
   simulation of the models with Solid Work software that validated said
   folding structures and helped us to verify the construction models,
   which validated such foldable structural connections and helped us to
   verify the constructive models, which is the purpose of this research.
   Consequently, such proposals were approached with the purpose of
   designing the industrial connection details by means of Cad Computer
   aided-Design (CAD), since the software in its package has the capacity
   to develop experimental prototypes and perform the details with better
   results for the constructive connections and thus follow a more optimal
   industrial phase for the models.},
DOI = {10.14718/RevArq.2022.24-1.4188},
ISSN = {1657-0308},
EISSN = {2357-626X},
Unique-ID = {WOS:000770363400011},
}

@article{ WOS:000784300100003,
Author = {Ermakova, V. A. and Gasperovich, V, E. and Ermakov, I, A. and Litvyak,
   V. V.},
Title = {Study of Strength Characteristics of Products Produced by 3D-Printing
   from PLA},
Journal = {SCIENCE \& TECHNIQUE},
Year = {2022},
Volume = {21},
Number = {2},
Pages = {107-113},
Abstract = {In modem mechanical engineering, along with reducing the metal
   consumption of structures, the main task is to increase the reliability
   and durability of parts, assemblies of mechanisms. This problem never
   loses its relevance due to the ever-increasing requirements for
   manufacture of products, with the need to save scarce expensive metals
   and alloys and, as a result, replace them with economically more
   profitable options. One of the most promising, from the point of view of
   ecology, ways to replace metals, is to use in the construction of
   plastic assemblies, in particular polylactic acid (PLA). PLA is a
   biodegradable thermoplastic used in 3D-printing. The paper presents
   results of an experimental study on the rupture of samples obtained by
   3D-printing from PLA under various technological operating modes of
   3D-printer. The following parameters have been chosen as variable
   parameters: the form of filling samples (triangle, hexagon (honeycomb),
   line, edge), nozzle temperature (190-205 degrees C), sample filling
   factor (from 10 up to 40 \%). Tensile testing of the samples have been
   carried out on a hydraulic tensile machine with a measuring software
   package in the Kason WDW-5 set. When performing tests, the conditional
   yield strength was considered the main out put indicator. The analysis
   of experimental data has shown a significant effect of the investigated
   technological parameters on the conventional yield point, which varied
   from 16.5 to 22.42 MPa. The most rational forms and sample filling
   factor, as well as the temperature of nozzle when printing with PLA,
   have been determined.},
DOI = {10.21122/2227-1031-2022-21-2-107-113},
ISSN = {2227-1031},
Unique-ID = {WOS:000784300100003},
}

@article{ WOS:000890749400015,
Author = {Fayzullina, V, Elvira and Parshikova, Mariya A. and Gulin, Denis A. and
   Sultanmagomedov, Timur S. and Sultanmagomedov, Sultanmagomed M. and
   Nasibullin, Timur R.},
Title = {STRESS-STRAIN STATE OF A PIPELINE BURIED IN AREAS OF DISTRIBUTION OF
   HEAVY SOILS},
Journal = {BULLETIN OF THE TOMSK POLYTECHNIC UNIVERSITY-GEO ASSETS ENGINEERING},
Year = {2022},
Volume = {333},
Number = {9},
Pages = {168-177},
Abstract = {The relevance. The article deals with the problem of construction and
   operation of pipelines in the areas of distribution of heaving soils.
   The problem under consideration is relevant. since one of the main
   reasons complicating the process of laying pipeline systems in the
   northern regions is frost heaving, which consists in a multiple increase
   in the volume of wet soils during freezing. The metal of the pipes of
   underground trunk pipelines during their operation in the areas of the
   distribution of heaving soils is subjected to significant deformations
   due to uncontrolled movements of the soil because of frost heaving. This
   force interaction of the underground pipeline and frozen ground can
   cause emergency situations.
   The main aim of the research is to create a 3D model of the interaction
   of the pipeline with frozen heaving soil in order to simulate soil
   heaving and to reveal graphs of the dependence of soil movement on
   stresses and strains that occur in the pipeline
   Objects: section of the pipeline buried in the areas of distribution of
   heaving soils
   Methods. The literature and regulatory documents on the construction and
   operation of structures in permafrost soils were studied and the
   influence of heaving soils on buildings and structures was analyzed. as
   well as modeling in the Ansys software package was carried out. The
   paper analyzes the interaction of permafrost soils with an underground
   pipeline in the AnsysWorkbench software package.
   Results. The authors have modeled the process of soil heaving and
   evaluated the stress-strain state of the pipeline buried in the areas of
   the distribution of heaving soils in the Ansys Workbench software
   package using digital twin technology. With the help of the developed
   model. the values of stresses and total deformations of the pipe wall in
   contact with frozen soil are obtained and analyzed. and a graphical
   dependence of the observed values is constructed with an increase in the
   heaving process and an increase in the heaving area.},
DOI = {10.18799/24131830/2022/9/3595},
ISSN = {2500-1019},
EISSN = {2413-1830},
ResearcherID-Numbers = {Sultanmagomedov, Timur/AAY-9987-2021},
ORCID-Numbers = {Sultanmagomedov, Timur/0000-0002-4969-2808},
Unique-ID = {WOS:000890749400015},
}

@inproceedings{ WOS:000911467600024,
Author = {Haris, Muhammad and Stocker, Markus and Auer, Soeren},
Editor = {Tseng, Y and Katsurai, M and Nguyen, HN},
Title = {Scholarly Knowledge Extraction from Published Software Packages},
Booktitle = {FROM BORN-PHYSICAL TO BORN-VIRTUAL: AUGMENTING INTELLIGENCE IN DIGITAL
   LIBRARIES, ICADL 2022},
Series = {Lecture Notes in Computer Science},
Year = {2022},
Volume = {13636},
Pages = {301-310},
Note = {24th International Conference on Asian Digital Libraries (ICADL),
   Vietnam Natl Univ, Hanoi, VIETNAM, NOV 30-DEC 02, 2022},
Organization = {Asia Pacific Chapter iSchools},
Abstract = {A plethora of scientific software packages are published in
   repositories, e.g., Zenodo and figshare. These software packages are
   crucial for the reproducibility of published research. As an additional
   route to scholarly knowledge graph construction, we propose an approach
   for automated extraction of machine actionable (structured) scholarly
   knowledge from published software packages by static analysis of their
   (meta)data and contents (in particular scripts in languages such as
   Python). The approach can be summarized as follows. First, we extract
   metadata information (software description, programming languages,
   related references) from software packages by leveraging the Software
   Metadata Extraction Framework (SOMEF) and the GitHub API. Second, we
   analyze the extracted metadata to find the research articles associated
   with the corresponding software repository. Third, for software
   contained in published packages, we create and analyze the Abstract
   Syntax Tree (AST) representation to extract information about the
   procedures performed on data. Fourth, we search the extracted
   information in the full text of related articles to constrain the
   extracted information to scholarly knowledge, i.e. information published
   in the scholarly literature. Finally, we publish the extracted machine
   actionable scholarly knowledge in the Open Research Knowledge Graph
   (ORKG).},
DOI = {10.1007/978-3-031-21756-2\_24},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-031-21755-5; 978-3-031-21756-2},
ORCID-Numbers = {Stocker, Markus/0000-0001-5492-3212
   Auer, Soren/0000-0002-0698-2864},
Unique-ID = {WOS:000911467600024},
}

@inproceedings{ WOS:000908025100012,
Author = {Hoff, Adrian and Gerling, Lea and Seidl, Christoph},
Book-Group-Author = {IEEE Comp Soc},
Title = {Utilizing Software Architecture Recovery to Explore Large-Scale Software
   Systems in Virtual Reality},
Booktitle = {2022 WORKING CONFERENCE ON SOFTWARE VISUALIZATION (IEEE VISSOFT)},
Series = {IEEE International Workshop on Visualizing Software for Understanding
   and Analysis VISSOFT},
Year = {2022},
Pages = {119-130},
Note = {10th IEEE Working Conference on Software Visualization (IEEE VISSOFT),
   Limassol, CYPRUS, OCT 02-03, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {Exploring an unfamiliar large-scale software system is challenging,
   especially when based solely on source code. While software
   visualizations help in gaining an overview of a system, they generally
   neglect architecture knowledge in their representations, e.g., by
   arranging elements along package structures rather than functional
   components or locking users in a specific abstraction only slightly
   above the source code. In this paper, we introduce an automated approach
   for software architecture recovery and use its results in an immersive
   3D virtual reality software visualization to aid accessing and relating
   architecture knowledge. We further provide a semantic zoom that allows a
   user to access and relate information both horizontally on the same
   abstraction level, e.g., by following method calls, and vertically
   across different abstraction levels, e.g., from a class to its
   containing component. We evaluate our contribution in a controlled
   experiment contrasting the usefulness regarding software exploration and
   comprehension of our concepts with those of the established CityVR
   visualization and the Eclipse IDE.},
DOI = {10.1109/VISSOFT55257.2022.00020},
ISBN = {978-1-6654-8092-5},
Unique-ID = {WOS:000908025100012},
}

@article{ WOS:000836546100004,
Author = {Jagadish, Arjun R. and Pereira, Amram and Thorat, Alisha and Kumar,
   Pankaj},
Title = {Finite element simulation of liquid nitrogen temperature rolling of
   marine grade aluminium alloy 5754},
Journal = {MATERIALS TODAY-PROCEEDINGS},
Year = {2022},
Volume = {62},
Number = {10},
Pages = {5861-5866},
Note = {3rd International Conference on Processing and Characterization of
   Materials (ICPCM), ELECTR NETWORK, DEC 07-08, 2021},
Abstract = {Now days, numerical approaches are becoming an efficient practical tool
   for the analysis of elastoplastic behavior of various engineering
   materials. Present study elaborates about the numerical investigation of
   liquid nitrogen temperature (LNT) plate rolling operation of marine
   grade aluminium alloy 5754 (AA 5754) by finite element method. To serve
   this purpose, commercially available ANSYS software package has been
   utilized to perform LNT-rolling simulation. In literature, it is well
   documented that LNT-rolling immensely increases mechanical strength
   through grain refinement technique. During rolling at liquid nitrogen
   temperature, the dynamic recovery process is inhibited which leads in
   accumulation of high rate of dislocation defects. Consequently, with due
   rolling these accumulated dislocations attributed towards formation of
   ultrafine grain (UFG) structure. The aim of this work is to formulate an
   efficient method to numerically investigate the deformation
   characteristics of AA 5754 during LNT-rolling. Geometrical and
   experimental parameters like roller diameters, dimensions of flat plate,
   roller speed, friction coefficient etc. are incorporated in the
   preprocessor module of ANSYS software. An optimized meshed domain is
   selected for simulation of 40\% thickness reduction of AA 5754. The
   temperature effect and corresponding stress distribution in the flat
   plate of AA 5754 is well captured and comparative analysis with the
   experimental practices has been successfully carried out. Numerical
   results obtained in the form of uniform thickness reduction,
   elastoplastic stress and strain contour plots, pressure force exerted by
   roller etc. are in good agreement with the experimental rolling
   operation. Copyright (C) 2022 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.matpr.2022.04.618},
ISSN = {2214-7853},
Unique-ID = {WOS:000836546100004},
}

@inproceedings{ WOS:000838715700018,
Author = {Konersmann, Marco and Kaplan, Angelika and Kuehn, Thomas and Heinrich,
   Robert and Koziolek, Anne and Reussner, Ralf and Juerjens, Jan and
   Al-Doori, Mahmood and Boltz, Nicolas and Ehl, Marco and Fuchss, Dominik
   and Grosser, Katharina and Hahner, Sebastian and Keim, Jan and Lohr,
   Matthias and Saglam, Timur and Schulz, Sophie and Toeberg, Jan-Philipp},
Book-Group-Author = {IEEE Comp Soc},
Title = {Replication Package of ``Evaluation Methods and Replicability of
   Software Architecture Research Objects{''}},
Booktitle = {2022 IEEE 19TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE
   COMPANION (ICSA-C 2022)},
Year = {2022},
Pages = {58},
Note = {19th IEEE International Conference on Software Architecture (ICSA),
   ELECTR NETWORK, MAR 12-15, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {In ``Evaluation Methods and Replicability of Software Architecture
   Research{''} {[}1], we present a systematic literature review to assess
   the state-of-practice of evaluating software architecture research
   objects and providing replication artifacts in 153 full technical
   conference papers published at the International and European Conference
   on Software Architecture (ICSA respectively ECSA) from 2017 to 2021.
   Context: Software architecture (SA) as research area experienced an
   increase in empirical research. Empirical research builds a sound
   foundation for the validity and comparability of the research. A current
   overview on the evaluation and replicability of SA research objects
   could help to discuss our empirical standards as a community. However,
   no such current overview exists.
   Objective: We aim at assessing the current state of practice of
   evaluating SA research objects and replication artifact provision in
   full technical ICSA and ECSA conference papers from 2017 to 2021.
   Method: We first create a categorization of papers regarding their
   evaluation and provision of replication artifacts. In a systematic
   literature review (SLR) with 153 papers we then investigate how SA
   research objects are evaluated and how artifacts are made available.
   Results: We found that technical experiments (28\%) and case studies
   (29\%) are the most frequently used evaluation methods over all research
   objects. Functional suitability (46\% of evaluated properties) and
   performance (29\%) are the most evaluated properties. 17 papers (11\%)
   provide replication packages and 97 papers (63\%) explicitly state
   threats to validity. 17\% of papers reference guidelines for evaluations
   and 14\% of papers reference guidelines for threats to validity.
   Conclusions: Our results indicate that the generalizability and
   repeatability of evaluations could be improved to enhance the maturity
   of the field; although, there are valid reasons for contributions to not
   publish their data. We derive from our findings a set of four proposals
   for improving the state of practice in evaluating software architecture
   research objects. Researchers can use our results to find
   recommendations on relevant properties to evaluate and evaluation
   methods to use and to identify reusable evaluation artifacts to compare
   their novel ideas with other research. Reviewers can use our results to
   compare the evaluation and replicability of submissions with the state
   of the practice.},
DOI = {10.1109/ICSA-C54293.2022.00021},
ISBN = {978-1-6654-9493-9},
ResearcherID-Numbers = {Keim, Jan/HHS-7693-2022
   },
ORCID-Numbers = {Keim, Jan/0000-0002-8899-7081
   Grosser, Katharina/0000-0003-4532-0270},
Unique-ID = {WOS:000838715700018},
}

@inproceedings{ WOS:000838691200015,
Author = {Konersmann, Marco and Kaplan, Angelika and Kuehn, Thomas and Heinrich,
   Robert and Koziolek, Anne and Reussner, Ralf and Juerjens, Jan and
   al-Doori, Mahmood and Boltz, Nicolas and Ehl, Marco and Fuchss, Dominik
   and Grosser, Katharina and Hahner, Sebastian and Keim, Jan and Lohr,
   Matthias and Saglam, Timur and Schulz, Sophie and Toberg, Jan-Philipp},
Book-Group-Author = {IEEE COMP SOC},
Title = {Evaluation Methods and Replicability of Software Architecture Research
   Objects},
Booktitle = {IEEE 19TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE (ICSA 2022)},
Year = {2022},
Pages = {157-168},
Note = {19th IEEE International Conference on Software Architecture (ICSA),
   ELECTR NETWORK, MAR 12-15, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {Context: Software architecture (SA) as research area experienced an
   increase in empirical research, as identified by Galster and Weyns in
   2016 In Empirical research builds a sound foundation for the validity
   and comparability of the research. A current overview on the evaluation
   and replicability of SA research objects could help to discuss our
   empirical standards as a community. However, no such current overview
   exists.
   Objective: We aim at assessing the current state of practice of
   evaluating SA research objects and replication artifact provision in
   full technical conference papers from 2017 to 2021.
   Method: We first create a categorization of papers regarding their
   evaluation and provision of replication artifacts. In a systematic
   literature review (SLR) with 153 papers we then investigate how SA
   research objects are evaluated and how artifacts are made available.
   Results: We found that technical experiments (28\%) and case studies
   (29\%) are the most frequently used evaluation methods over all research
   objects. Functional suitability (46\% of evaluated properties) and
   performance (29\%) are the most evaluated properties. 17 papers (11\%)
   provide replication packages and 97 papers (63 \%) explicitly state
   threats to validity. 17\% of papers reference guidelines for evaluations
   and 14\% of papers reference guidelines for threats to validity.
   Conclusions: Our results indicate that the generalizability and
   repeatability of evaluations could be improved to enhance the maturity
   of the field; although, there are valid reasons for contributions to not
   publish their data. We derive from our findings a set of four proposals
   for improving the state of practice in evaluating software architecture
   research objects. Researchers can use our results to find
   recommendations on relevant properties to evaluate and evaluation
   methods to use and to identify reusable evaluation artifacts to compare
   their novel ideas with other research. Reviewers can use our results to
   compare the evaluation and replicability of submissions with the state
   of the practice.},
DOI = {10.1109/ICSA53651.2022.00023},
ISBN = {978-1-6654-1728-0},
ResearcherID-Numbers = {Keim, Jan/HHS-7693-2022
   },
ORCID-Numbers = {Keim, Jan/0000-0002-8899-7081
   Grosser, Katharina/0000-0003-4532-0270
   Corallo, Sophie/0000-0002-1531-2977},
Unique-ID = {WOS:000838691200015},
}

@article{ WOS:000748749400002,
Author = {Kulkarni, Keertivardhan D. and Mahesh, P. A.},
Title = {Left Ventricular and Right Ventricular Functional Changes in Cases of
   COPD and its Corelation with Severity-A Cross-sectional Study},
Journal = {JOURNAL OF CLINICAL AND DIAGNOSTIC RESEARCH},
Year = {2022},
Volume = {16},
Number = {1},
Pages = {OC20-OC23},
Month = {JAN},
Abstract = {Introduction: Chronic Obstructive Pulmonary Disease (COPD) is a global
   health problem, mainly in developing countries. It affects pulmonary
   blood vessels, right ventricle, and also left ventricle leading to
   pulmonary hypertension, Cor pulmonale and right and left ventricular
   dysfunction.
   Aim: To assess the cardiac, right and left ventricular changes in
   subjects with increasing COPD severity staged according to Global
   initiative for chronic Obstructive Lung Disease (GOLD) guidelines and to
   compare Arterial Blood Gases (ABG), St George Respiratory Questionnaire
   (SGRQ) percentages and BODE (Body-mass index, airflow Obstruction,
   Dyspnea, and Exercise) scores to cardiac changes in COPD.
   Materials and Methods: The present study was a cross-sectional study
   conducted at tertiary care hospital in Southern Karnataka, India. The
   sample size was 60. A structured questionnaire was administered which
   included demographic, clinical variables followed by a detailed clinical
   examination, spirometry. Electrocardiograph (ECG), ABG, chest
   radiograph, echocardiography and a 6-Minute Walk Test (6MWT). Data
   collected was analysed using Statistical Package for the Social Sciences
   (SPSS) and Epi INFO software for mean, Standard Deviation (SD) and
   multivariate analysis.
   Results: All the patients diagnosed with COPD (using GOLD criteria) were
   included in study and assessed for right and left ventricular changes.
   Out of 60 patients, 58 were males and 02 were females, with mean age
   being 64.71 +/- 28.28 years. Among the study population, 45 (75\%)
   patients had one or the other cardiac condition. Cardiac changes
   included left ventricular diastolic dysfunction (58.3\%), right
   ventricular dilatation (33.3\%), right ventricular hypertrophy, right
   atrial dilation, tricuspid regurgitation and pulmonary hypertension and
   left heart changes included left ventricular hypertrophy.
   Conclusion: The study highlights the need for early and active cardiac
   screening of all COPD patients. This will help in early treatment and
   good prognosis, and will further contribute in reducing the morbidity
   and mortality.},
DOI = {10.7860/JCDR/2022/50524.15907},
ISSN = {2249-782X},
EISSN = {0973-709X},
Unique-ID = {WOS:000748749400002},
}

@article{ WOS:000729728900006,
Author = {Mahmoud, Sayed and Salman, Alaa},
Title = {Cost Estimate and Input Energy of Floor Systems in Low Seismic Regions},
Journal = {CMC-COMPUTERS MATERIALS \& CONTINUA},
Year = {2022},
Volume = {71},
Number = {2},
Pages = {2159-2173},
Abstract = {Reinforced concrete (RC) as a material is most commonly used for
   buildings construction. Several floor systems are available following
   the structural and architectural requirements. The current research
   study provides cost and input energy comparisons of RC office buildings
   of different floor systems. Conventional solid, ribbed, flat plate and
   flat slab systems are consid-ered in the study. Building models in
   three-dimensional using extended three-dimensional analysis of building
   systems (ETABS) and in two-dimensional using slab analysis by the finite
   element (SAFE) are developed for analysis purposes. Analysis and design
   using both software packages and manual calculations are employed to
   obtain the optimum sections and reinforcements to fit cities of low
   seismic intensities for all the considered building systems. Two ground
   motion records of low peak ground acceleration (PGA) levels are used to
   excite the models to measure the input energies. Uniformat cost
   esti-mating system is adopted to categorize building components
   according to 12 divisions. Also, Microsoft (MS) Project is utilized to
   identify the construction cost and duration of each building system. The
   study shows that floor system significantly causes changes in the input
   energy to structures. In addition, the slight increase in the PGA
   increases the amount of input energy particularly flat plate system.
   Estimated cost of the flat plate system that the flat slab system is of
   higher value as compared to ribbed and conventional slab systems. The
   use of drop panels increases this value as well. Moreover, the estimated
   cost of the ribbed slab system exceeds that of conventional system.},
DOI = {10.32604/cmc.2022.022357},
ISSN = {1546-2218},
EISSN = {1546-2226},
ResearcherID-Numbers = {Salman, Alaa/ABH-2584-2021},
ORCID-Numbers = {Salman, Alaa/0000-0002-0304-2295},
Unique-ID = {WOS:000729728900006},
}

@inproceedings{ WOS:000754461000007,
Author = {Naik, Pushpraj and Chatterjee, Urbi},
Editor = {Batina, L and Picek, S and Mondal, M},
Title = {Network Data Remanence Side Channel Attack on SPREAD, H-SPREAD and
   Reverse AODV},
Booktitle = {SECURITY, PRIVACY, AND APPLIED CRYPTOGRAPHY ENGINEERING, SPACE 2021},
Series = {Lecture Notes in Computer Science},
Year = {2022},
Volume = {13162},
Pages = {129-147},
Note = {11th International Conference on Security, Privacy, and Applied
   Cryptography Engineering (SPACE), IIT Kharagpur, ELECTR NETWORK, DEC
   10-13, 2021},
Abstract = {Side Channel Attacks (SCAs) was first introduced by Paul Kocher in 1996
   to break the secret key of cryptographic algorithms using the inherent
   property of the implementation along with the mathematical structure of
   the cipher. These categories of attacks become more robust as they do
   not require any mathematical cryptanalysis to retrieve the key. Instead,
   they exploit the timing measurements, power consumption, leaked
   electromagnetic radiation of the software/hardware platforms to execute
   key-dependent operations for the cipher. This, in turn, aids the
   adversary to gather some additional information about the computation.
   The overall concept of leaking secrets through side channel has been
   extended for Wireless Sensor Networks (WSNs) that implements Secret
   Sharing (SS) Scheme to exchange secrets between two nodes across
   multiple paths. Now in the idealized network model, it is assumed that
   for such SS schemes, all the paths between the two communicating nodes
   are atomic and have the same propagation delay. However, in the real
   implementation of TCP/IP networks, the shares propagate through every
   link and switch sequentially. Hence the attacker can probe any number of
   paths or switches to get the residual shares from previous messages that
   still exist in the network even when a new message is being sent. This
   kind of side channel vulnerability is known as Network Data Remanence
   (NDR) attacks. In this paper, we specifically target two SS schemes
   named Secure Protocol for Reliable Data Delivery (SPREAD) and
   Hybrid-Secure Protocol for Reliable Data Delivery (H-SPREAD), and an
   on-demand routing protocol named Path Hopping Based on Reverse AODV
   (PHR-AODV) to launch NDR based side channel attacks on the WSNs. We then
   show two specific categories of NDR attacks; a) NDR Blind and b) NDR
   Planned on the schemes mentioned above. We use an in-house C++ library
   to simulate our proposed attacks, and the experimental results reveal
   that the impact of NDR Blind attacks is negligible for these schemes,
   whereas the probability of data recovery for NDR Planned attacker
   proportionally increases with the path length.},
DOI = {10.1007/978-3-030-95085-9\_7},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-030-95085-9; 978-3-030-95084-2},
ORCID-Numbers = {chatterjee, urbi/0000-0002-4631-2208},
Unique-ID = {WOS:000754461000007},
}

@article{ WOS:000876441900013,
Author = {Protosenya, Anatoly G. and Alekseev, V, Alexander and Verbilo, Pavel E.},
Title = {Prediction of the stress-strain state and stability of tunnel face at
   the intersection of disturbed zones of the soil mass},
Journal = {JOURNAL OF MINING INSTITUTE},
Year = {2022},
Volume = {254},
Pages = {252-260},
Abstract = {The article presents a numerical solution of the spatial elastic-plastic
   problem of determining tunnel face stability at the intersection of
   disturbed zones of the soil. The relevance of the study is related to
   the need to take into account the zones of disturbed soils when
   assessing the face stability to calculate the parameters of the support.
   Based on the finite element method implemented in the PLAXIS 3D software
   package, the design of a finite element system ``soil - disturbance -
   tunnel face support{''} and modeling of the intersection of the
   disturbed zones of the soil were performed. To assess the stability of
   soils, deformation and strength criteria are taken. The deformation
   criterion is expressed by the value of the calculated displacement of
   the tunnel face surface, and the strength criterion - by the safety
   factor until the maximum values of the stress state are reached
   according to the Coulomb - Mohr criterion. The results of the study are
   presented in the form of histograms of the safety factor dependences on
   the distance to the disturbance at different bending stiffness of the
   tunnel face support structure, as well as the isofields of deformation
   development. The parameters of rockfall formation in the tunnel face
   zone at the intersection of zones of disturbed soils were determined.
   The local decrease in strength and deformation properties in the rock
   mass along the tunnel track should be taken into account when assessing
   the stability of the tunnel face and calculating the parameters of the
   sup-port. Within the framework of the constructed closed system, a
   qualitative agreement of the simulation results with the case of a
   collapse in the tunnel face during the construction of the
   Vladimirskaya-2 station of the Saint Petersburg Metro was obtained.},
DOI = {10.31897/PMI.2022.26},
ISSN = {2411-3336},
EISSN = {2541-9404},
Unique-ID = {WOS:000876441900013},
}

@inproceedings{ WOS:000799480000134,
Author = {Sun, Xu and Yu, Hao and Solvang, Wei Deng},
Book-Group-Author = {IEEE},
Title = {System Integration for Smart Reverse Logistics Management},
Booktitle = {2022 IEEE/SICE INTERNATIONAL SYMPOSIUM ON SYSTEM INTEGRATION (SII 2022)},
Series = {IEEE/SICE International Symposium on System Integration},
Year = {2022},
Pages = {821-826},
Note = {IEEE/SICE International Symposium on System Integration (SII), ELECTR
   NETWORK, JAN 09-12, 2022},
Organization = {IEEE; SICE Syst Integrat Div; IEEE Robot \& Automat Soc; IEEE Ind Elect
   Soc},
Abstract = {To maximize the value and material recovery from waste products, smart
   reverse logistics aims at managing the complex flows of physical items,
   cash, data, and information. The effective management of these flows
   requires optimal decision making at strategic, tactical, and operational
   levels. To support the decision making, predictive, prescriptive, and
   descriptive analytics have been proved to be valuable at all three
   levels. However, because these analytical tools require different
   software packages, different coding languages, and different structures
   of data, the decision support for complex problems combining various
   analytical methods is usually an ad-hoc process and requires thus
   significant efforts. There is a lack of standardized solutions that
   comprise all the necessary modules for smart reverse logistics
   management. Thus, this paper proposes a conceptual framework with the
   purpose of guiding the next-generation system integration for smart
   reverse logistics management. It goes further with the design of six
   criteria for evaluating the integration maturity of a system. The
   initial concept is shown with existing software solutions through a case
   study in Norway, and several challenges are identified for future
   improvements.},
DOI = {10.1109/SII52469.2022.9708743},
ISSN = {2474-2317},
ISBN = {978-1-6654-4540-5},
ResearcherID-Numbers = {Solvang, Wei Deng/HLQ-0212-2023},
ORCID-Numbers = {Solvang, Wei Deng/0000-0002-3723-9205},
Unique-ID = {WOS:000799480000134},
}

@inproceedings{ WOS:000838715700016,
Author = {Timperley, Christopher S. and Durschmid, Tobias and Schmerl, Bradley and
   Garlan, David and Le Goues, Claire},
Book-Group-Author = {IEEE Comp Soc},
Title = {ROSDiscover: Statically Detecting Run-Time Architecture
   Misconfigurations in Robotics Systems Artifact Paper},
Booktitle = {2022 IEEE 19TH INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE
   COMPANION (ICSA-C 2022)},
Year = {2022},
Pages = {56},
Note = {19th IEEE International Conference on Software Architecture (ICSA),
   ELECTR NETWORK, MAR 12-15, 2022},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {This is the replication package for the paper, ROSDiscover: Statically
   Detecting Run-Time Architecture Misconfigurations in Robotics Systems,
   which is published at the International Conference on Software
   Architecture (ICSA), 2022.
   The artifact contains (a) the tool ROSDiscover, which is a component and
   connector architecture recovery tool that recovers and checks robotics
   systems built in the Robot Operating System (ROS) 1; (b) data set of
   architecture misconfiguration bugs of real-world open-source ROS 1
   systems on GitHub; and (c) scripts and instructions for replicating the
   results produced in the paper that show that it is both possible to
   accurately recover run-time architectures of ROS 1 systems, and that
   these architectures can be used to detect misconfiguration bugs that
   were found in real systems.},
DOI = {10.1109/ICSA-C54293.2022.00055},
ISBN = {978-1-6654-9493-9},
Unique-ID = {WOS:000838715700016},
}

@article{ WOS:000738854300004,
Author = {Peng, Benli and Sheng, Wenlong and He, Zhengyu and Wang, Hong and Su,
   Fengmin and Wang, Shikuan},
Title = {Systematic investigations on charging/discharging performances
   improvement of phase change materials by structured network fins},
Journal = {ENERGY},
Year = {2022},
Volume = {242},
Month = {MAR 1},
Abstract = {Improving the effectiveness of solar energy utilization and waste heat
   recovery by ameliorating charging/ discharging performances of latent
   heat thermal energy storage (LHTES) unit, is essential to reduce fossil
   fuel consumption thus achieve the goal of carbon neutralization.
   Structured network fins are developed to improve charging/discharging
   performances of paraffin wax-based LHTES unit. Simulations are conducted
   by integrating enthalpy-porosity method and Ansys Fluent software
   package. Influences of horizontal fin number n(h) and distance d between
   bottom horizontal fin and bottom wall of enclosure on
   charging/discharging performances are discussed. Results illustrate that
   when n(h) increases from one to six, the maximum enhancement factor for
   charging approaches to 1.6921 but it decreases to 1.0590 when n(h)
   increases from six to ten. The corresponding maximum enhancement factors
   for discharging are 10.5571 and 1.0382, respectively. An appropriate
   n(h) exists. Moreover, an optimum d exists when n(h) <= 3 for charging
   and when n(h) <= 6 for discharging. Charging/discharging rates are
   further enhanced by 10.38\% and 13.84\% respectively through optimizing
   d. When n(h) > 3 for charging and n(h) > 6 for discharging, a smaller d
   is more profitable. Results suggest n(h) = 6 and d <= 3 mm are
   appropriate to achieve satisfactory enhancements for
   charging/discharging performances of paraffin wax-based LHTES
   simultaneously and introduce minimum extra manufacture complexity. (C)
   2021 Published by Elsevier Ltd.},
DOI = {10.1016/j.energy.2021.122963},
EarlyAccessDate = {DEC 2021},
Article-Number = {122963},
ISSN = {0360-5442},
EISSN = {1873-6785},
ResearcherID-Numbers = {su, fengmin/HDO-7218-2022},
Unique-ID = {WOS:000738854300004},
}

@article{ WOS:000671743400001,
Author = {Muriuki, Susan W. and Rengan, Michael S. and Budambula, Nancy L. M.},
Title = {Prokaryotic diversity and potentially pathogenic bacteria in vended
   foods and environmental samples},
Journal = {ANNALS OF MICROBIOLOGY},
Year = {2021},
Volume = {71},
Number = {1},
Month = {DEC},
Abstract = {Purpose Ready-to-eat fast food vending outlets provide a cheap and
   readily available food. Foodborne diseases have been previously reported
   in Embu, Kenya, but data on the prokaryotic metagenome in vended foods
   is scanty. This study aimed to determine the prokaryotic diversity in
   fruits, vegetable salad, African sausage, chips (potato fries), fried
   fish, roasted beef (meat), smokies, samosa, soil, and water collected
   from food vendors and the surrounding environment in Embu Town and
   Kangaru Market. Methods The study used 454 pyrosequencing, Illumina
   high-throughput sequencing of 16S rRNA gene in the analysis of total
   community DNA extracted from samples using the phenol-chloroform method.
   The 16S rRNA gene variable region (V4-V7) of the extracted DNA was
   amplified and library construction performed. Sequence analysis was done
   using QIIME2. Hierarchical clustering of samples, diversity indices,
   rarefaction curves, and Venn diagrams were generated using the R
   programming language in R software version 3.6.3. Results Bacterial
   operational taxonomic units (OUTs) were distributed in Proteobacteria
   (52.81\%), Firmicutes (31.16\%), and Lentisphaerae (0.001\%). The OTUs
   among archaea were Candidatus Nitrososphaera (63.56\%) and
   Nitrososphaera spp. (8.77\%). Brucella spp. and Bacillus cereus
   associated with foodborne diseases were detected. Potential pathogens,
   Rickettsia spp. in risk group 2 and Brucella spp. in risk group 3, were
   detected. Uncultured Candidatus Koribacter and Candidatus Solibacter
   were also detected in the food samples. There was a significant
   difference in the microbial community structure among the sample types
   (P<0.1). Conclusion The results demonstrated the presence of some
   prokaryotes that are associated with food spoilage or foodborne diseases
   in vended foods and environmental samples. This study also detected
   uncultured prokaryotes. The presence of potential pathogens calls for
   stringent hygiene measures in food vending operations.},
DOI = {10.1186/s13213-021-01640-w},
Article-Number = {27},
ISSN = {1590-4261},
EISSN = {1869-2044},
ResearcherID-Numbers = {Budambula, Nancy/W-2391-2019},
ORCID-Numbers = {Budambula, Nancy/0000-0001-8093-2702},
Unique-ID = {WOS:000671743400001},
}

@article{ WOS:000728924600005,
Author = {Paz, Andres and El Boussaidi, Ghizlane and Mili, Hafedh},
Title = {checsdm: A Method for Ensuring Consistency in Heterogeneous
   Safety-Critical System Design},
Journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
Year = {2021},
Volume = {47},
Number = {12},
Pages = {2713-2739},
Month = {DEC 1},
Abstract = {Safety-critical systems are highly heterogeneous, combining different
   characteristics. Effectively designing such systems requires a complex
   modelling approach that deals with diverse components (e.g., mechanical,
   electronic, software)-each having its own underlying domain theories and
   vocabularies-as well as with various aspects of the same component
   (e.g., function, structure, behaviour). Furthermore, the regulated
   nature of such systems prescribes the objectives for their design
   verification and validation. This paper proposes checsdm, a systematic
   approach, based on Model-Driven Engineering (MDE), for assisting
   engineering teams in ensuring consistency of heterogeneous design of
   safety-critical systems. The approach is developed as a generic
   methodology and a tool framework, that can be applied to various design
   scenarios involving different modelling languages and different design
   guidelines. The methodology comprises an iterative three-phased process.
   The first phase, elicitation, aims at specifying requirements of the
   heterogeneous design scenario. Using the proposed tool framework, the
   second phase, codification, consists in building a particular tool set
   that supports the heterogeneous design scenario and helps engineers in
   flagging consistency errors for review and eventual correction. The
   third phase, operation, applies the tool set to actual system designs.
   Empirical evaluation of the work is presented through two executions of
   the checsdm approach for the specific cases of a design scenario
   involving a mix of UML, Simulink and Stateflow, and a design scenario
   involving a mix of AADL, Simulink and Stateflow. The operation phase of
   the first case was performed over three avionics systems and the
   identified inconsistencies in the design models of these systems were
   compared to the results of a fully manual verification carried out by
   professional engineers. The evaluation also includes an assessment
   workshop with industrial practitioners to examine their perceptions
   about the approach. The empirical validation indicates the feasibility
   and ``cost-effectiveness{''} of the approach. Inconsistencies were
   identified in the three avionics systems with a greater recall rate over
   the manual verification. The assessment workshop shows the practitioners
   found the approach easy to understand and gave an overall likelihood of
   adoption within the context of their work.},
DOI = {10.1109/TSE.2020.2966994},
ISSN = {0098-5589},
EISSN = {1939-3520},
ResearcherID-Numbers = {Paz, Andrés/GQH-9815-2022
   },
ORCID-Numbers = {Paz, Andrés/0000-0002-0743-769X
   El Boussaidi, Ghizlane/0000-0001-6145-774X},
Unique-ID = {WOS:000728924600005},
}

@article{ WOS:000735530200041,
Author = {Wang, Fei and Shi, Wei-Xing and Chen, Jie and He, Kang and Fang, Wei},
Title = {Clinical therapeutic effects of combined diacerein and glucosamine in
   the treatment of osteoarthritis A protocol for systematic review and
   meta-analysis},
Journal = {MEDICINE},
Year = {2021},
Volume = {100},
Number = {47},
Month = {NOV 24},
Abstract = {Background: Osteoarthritis (OA) has been identified as a common
   musculoskeletal condition. As a chronic condition, OA adversely impact
   the hip and knee joints. Surgical treatment for hip and knee
   osteoarthritis is associated with high financial and long recovery
   processes. Therefore, patients are continually searching for alternative
   methods of treatment. Diacerein is regarded as symptom-modifying,
   slow-acting drug that could most likely change the disease structure of
   OA. The present systematic review protocol explains methods utilized to
   evaluate the clinical therapeutic effects of combining diacerein and
   glucosamine to treat OA. Methods: The authors will conduct a search for
   randomized controlled trials comparing diacerein plus glucosamine with
   diacerein alone, glucosamine alone, or another treatment in patients
   with OA. The search will be done in the following online-based
   databases: EMBASE, MEDLINE, Cochrane Library, Web of Science, China
   National Knowledge Infrastructure, and WanFang Database. All related
   RCTs included from inception to September 29, 2021 are included. Two
   authors will independently conduct data abstraction and quality
   assessment, and the comparative analysis will compare the results. The
   present meta-analysis will be performed with the RevMan software
   (version 5.3), where the results will be expressed as relative risk,
   mean differences, or standardized mean differences with 95\% confidence
   intervals. Results: This study will be conducted to evaluate the
   clinical therapeutic effects of combined diacerein and glucosamine in
   the treatment of OA. Conclusion: The summary presented in the study will
   ascertain whether diacerein plus glucosamine intervention is an
   efficient and feasible method of treatment for OA patients.},
DOI = {10.1097/MD.0000000000027583},
Article-Number = {e27583},
ISSN = {0025-7974},
EISSN = {1536-5964},
Unique-ID = {WOS:000735530200041},
}

@article{ WOS:000758143800009,
Author = {Wu, Shumei},
Title = {Intelligent Communication Management Terminal in the Construction of
   Human Resource Management Mode},
Journal = {WIRELESS COMMUNICATIONS \& MOBILE COMPUTING},
Year = {2021},
Volume = {2021},
Month = {NOV 23},
Abstract = {With the rapid development of the economy, the integration of corporate
   strategic management and human resource management has become an issue
   of concern. This research mainly discusses the role of intelligent
   communication management terminal in the construction of human resource
   management mode. In this research, the system development process of
   this research mainly uses the class library in the software architecture
   layer to support the software development process. The main development
   language of Android, JAVA, is to install the Android Develop Tools
   plug-in on eclipse and install the Android SDK in the computer operating
   system to build the Android development environment. The development and
   application of the system not only make the enterprise managers more
   convenient and efficient in the process of managing the enterprise but
   also smooth the operation of the enterprise while reducing the human
   resource investment and also gives the employees more right to know and
   the right to participate in the enterprise construction. By creating
   more value while reducing human resource input, enterprises will enable
   it to obtain more benefits, and thus enter a cycle of good development
   and contribute to society. The system has the functions of personnel
   management, recruitment management, attendance management, training
   management, work management, and salary management. The recruitment
   management function of the system is mainly composed of recruitment plan
   management, recruitment information management, and talent pool. In the
   system's recruitment plan management function, important information
   such as the recruitment part, the number of recruits, personnel
   requirements, and the specific arrival time of the personnel must be
   clarified. The personnel in charge of the enterprise personnel
   department shall conduct corresponding regulations according to the
   specific needs of the enterprise and shall be experienced by the
   personnel department. The review is carried out, and all parts of the
   enterprise are coordinated and completed at the same time. In the
   platform performance test, when the number of concurrent users reaches
   50000, the request time is about 6 seconds, which meets the requirement
   that the response time of 10000 people per second is less than 10
   seconds. This research puts forward suggestions and countermeasures for
   the optimization of human resource management, which can not only
   improve the efficiency of Y company's human resource management but also
   provide useful reference and reference for other enterprises facing the
   same problem.},
DOI = {10.1155/2021/7106104},
Article-Number = {7106104},
ISSN = {1530-8669},
EISSN = {1530-8677},
Unique-ID = {WOS:000758143800009},
}

@article{ WOS:000725180100001,
Author = {Hood, Rebecca J. and Maltby, Steven and Keynes, Angela and Kluge,
   Murielle G. and Nalivaiko, Eugene and Ryan, Annika and Cox, Martine and
   Parsons, Mark W. and Paul, Christine L. and Garcia-Esperon, Carlos and
   Spratt, Neil J. and Levi, Christopher R. and Walker, Frederick R.},
Title = {Development and Pilot Implementation of TACTICS VR: A Virtual
   Reality-Based Stroke Management Workflow Training Application and
   Training Framework},
Journal = {FRONTIERS IN NEUROLOGY},
Year = {2021},
Volume = {12},
Month = {NOV 11},
Abstract = {Delays in acute stroke treatment contribute to severe and negative
   impacts for patients and significant healthcare costs. Variability in
   clinical care is a contributor to delayed treatment, particularly in
   rural, regional and remote (RRR) areas. Targeted approaches to improve
   stroke workflow processes improve outcomes, but numerous challenges
   exist particularly in RRR settings. Virtual reality (VR) applications
   can provide immersive and engaging training and overcome some existing
   training barriers. We recently initiated the TACTICS trial, which is
   assessing a ``package intervention{''} to support advanced CT imaging
   and streamlined stroke workflow training. As part of the educational
   component of the intervention we developed TACTICS VR, a novel VR-based
   training application to upskill healthcare professionals in optimal
   stroke workflow processes. In the current manuscript, we describe
   development of the TACTICS VR platform which includes the VR-based
   training application, a user-facing website and an automated back-end
   data analytics portal. TACTICS VR was developed via an extensive and
   structured scoping and consultation process, to ensure content was
   evidence-based, represented best-practice and is tailored for the target
   audience. Further, we report on pilot implementation in 7 Australian
   hospitals to assess the feasibility of workplace-based VR training. A
   total of 104 healthcare professionals completed TACTICS VR training.
   Users indicated a high level of usability, acceptability and utility of
   TACTICS VR, including aspects of hardware, software design, educational
   content, training feedback and implementation strategy. Further, users
   self-reported increased confidence in their ability to make improvements
   in stroke management after TACTICS VR training (post-training mean +/-
   SD = 4.1 +/- 0.6; pre-training = 3.6 +/- 0.9; 1 = strongly disagree, 5 =
   strongly agree). Very few technical issues were identified, supporting
   the feasibility of this training approach. Thus, we propose that TACTICS
   VR is a fit-for-purpose, evidence-based training application for stroke
   workflow optimisation that can be readily deployed on-site in a clinical
   setting.},
DOI = {10.3389/fneur.2021.665808},
Article-Number = {665808},
ISSN = {1664-2295},
ResearcherID-Numbers = {HOOD, REBECCA/O-6933-2019
   },
ORCID-Numbers = {HOOD, REBECCA/0000-0001-7485-4883
   Levi, Christopher/0000-0002-9474-796X
   Maltby, Steven/0000-0003-1240-5964},
Unique-ID = {WOS:000725180100001},
}

@article{ WOS:000716103300001,
Author = {Zhang, Sen and Guo, Xiao-Wei and Li, Chao and Liu, Yi and Fan, Sijiang
   and Zhao, Ran and Yang, Canqun},
Title = {A large scale parallel fluid-structure interaction computing platform
   for simulating structural responses to a detonation shock},
Journal = {SOFTWARE-PRACTICE \& EXPERIENCE},
Year = {2023},
Volume = {53},
Number = {1, SI},
Pages = {211-240},
Month = {JAN},
Abstract = {Due to the intrinsic nature of multi-physics, it is prohibitively
   complex to design and implement a simulation software platform for study
   of structural responses to a detonation shock. In this article, a
   partitioned fluid-structure interaction computing platform is designed
   for parallel simulating structural responses to a detonation shock. The
   detonation and wave propagation are modeled in an open-source
   multi-component solver based on OpenFOAM and blastFoam, and the
   structural responses are simulated through the finite element library
   deal.II. To capture the interaction dynamics between the fluid and the
   structure, both solvers are adapted to preCICE. For improving the
   parallel performance of the computing platform, the inter-solver data is
   exchanged by peer-to-peer communications and the intermediate server in
   conventional multi-physics software is eliminated. Furthermore, the
   coupled solver with detonation support has been deployed on a computing
   cluster after considering the distributed data storage and
   load-balancing between solvers. The 3D numerical result of structural
   responses to a detonation shock is presented and analyzed. On 256
   processor cores, the speedup ratio of the simulations for a detonation
   shock reach 178.0 with 5.1 million of mesh cells and the parallel
   efficiency achieve 69.5\%. The results demonstrate good potential of
   massively parallel simulations. Overall, a general-purpose
   fluid-structure interaction software platform with detonation support is
   proposed by integrating open source codes. And this work has important
   practical significance for engineering application in fields of
   construction blasting, mining, and so forth.},
DOI = {10.1002/spe.3051},
EarlyAccessDate = {NOV 2021},
ISSN = {0038-0644},
EISSN = {1097-024X},
ResearcherID-Numbers = {Li, Chao/ABG-7849-2021},
ORCID-Numbers = {Li, Chao/0000-0001-8721-4826},
Unique-ID = {WOS:000716103300001},
}

@article{ WOS:000713355900126,
Author = {Lou, Qun and Liu, Ting and Wang, Xudong and Wu, Dandan and Wang, Guomin
   and Chen, Yang and Wan, Teng},
Title = {An Observational Study to Evaluate Association Between Velopharyngeal
   Anatomy and Speech Outcomes in Adult Patients With Severe Velopharyngeal
   Insufficiency},
Journal = {JOURNAL OF CRANIOFACIAL SURGERY},
Year = {2021},
Volume = {32},
Number = {8},
Pages = {2753-2757},
Month = {NOV-DEC},
Abstract = {Objective: By measuring velopharyngeal structure and evaluating speech
   intelligibility, to explore and observe the association between
   velopharyngeal anatomy and speech outcomes in these patients. Methods:
   Thirty-one adult patients with velopharyngeal insufficiency after the
   primary palatoplasty aged 18 to 35 years (mean 22.03 years) were
   enrolled as the study group. The patients had significant hypernasality
   and audible nasal emission. The degree of velopharyngeal closure
   assessed by electronic nasopharyngeal fiberoptic endoscopy was grade
   III. Cephalometric analysis was performed on lateral cephalograms to
   measure velopharyngeal structure, including hard palate length
   (ANS-PNS), velar length (PNS-U), pharyngeal depth (PNS-PPW), and
   oropharyngeal airway space (U-MPW). Their speech intelligibility was
   evaluated through the Mandarin Chinese speech intelligibility test, and
   each speech sample was examined by 2 speech and language pathologists.
   The results were assessed with the SPSS 23.0 software package, and
   regression analysis was used to examine the relationship between
   velopharyngeal structure and speech outcomes. Results: A significant
   negative correlation was confirmed between speech intelligibility and
   pharyngeal depth. Pharyngeal depth also showed a linear relationship
   with speech intelligibility, and there was no significant correlation
   between speech intelligibility and other measures (hard palate length,
   velar length, oropharyngeal airway space). Conclusions: In the
   velopharyngeal anatomy, only pharyngeal depth was associated with speech
   intelligibility in adult patients with severe velopharyngeal
   insufficiency, this is consistent with our clinical observation. It
   suggests that appropriate reduction of pharyngeal depth during
   palatopharyngoplasty may have a good effect on the speech recovery in
   patients with cleft palate and patients with velopharyngeal
   insufficiency after palatorrhaphy.},
DOI = {10.1097/SCS.0000000000007853},
ISSN = {1049-2275},
EISSN = {1536-3732},
Unique-ID = {WOS:000713355900126},
}

@article{ WOS:000763274600001,
Author = {Qiu, Fu-Sheng and Wang, Bo-Wen and Du, Yi-Ming and Qian, Hui-Yuan},
Title = {Numerical investigation on the flow characteristics of model dandelion
   seeds with angles of attitude},
Journal = {PHYSICS OF FLUIDS},
Year = {2021},
Volume = {33},
Number = {11},
Month = {NOV},
Abstract = {A stable and axisymmetric ``separated vortex ring{''} formed above the
   pappus can greatly improve the aerodynamic efficiency of dandelion seeds
   and reduce the material requirement for flight. Since the dandelion seed
   is not always in a ``regular{''} state (i.e., the pappus plane parallel
   to the horizontal plane) when flying, this paper uses a simplified rigid
   pappus model to simulate and analyze the influence of flight attitude on
   the flow patterns and the resulting changes in aerodynamic
   characteristics under different porosities. The numerical results are
   obtained by solving the three-dimensional incompressible steady RANS
   equations with the Spalart-Allmaras turbulence model in ANSYS Fluent
   software package. The results show that when there is an angle of
   attitude, the separated vortex ring will break with one of the vortices,
   losing its original recirculating structure, which leads to a
   considerable loss of the drag performance. In addition, the
   high-pressure region below the pappus plane shifting to one side will
   produce a horizontal force and a recovery moment against the rolling
   direction. The horizontal force component generated by a large-porosity
   pappus disk is much larger than that of a solid disk, which shows the
   good flight efficiency of dandelion seeds. At the same time, the
   inclined pappus will produce a recovery moment to keep the seed from
   rolling away from the horizontal plane. These conclusions show that the
   pappus structure has the potential to adjust the flight attitude and
   aerodynamic characteristics, which provides a preliminary understanding
   for further study on the flight dynamics of dandelion seeds.},
DOI = {10.1063/5.0069472},
Article-Number = {113107},
ISSN = {1070-6631},
EISSN = {1089-7666},
ResearcherID-Numbers = {DU, Yiming/AGE-1101-2022
   },
ORCID-Numbers = {Du, Yiming/0000-0002-9413-4899},
Unique-ID = {WOS:000763274600001},
}

@article{ WOS:000705073600006,
Author = {Zhang, Weijia and Li, Jiuyong and Liu, Lin},
Title = {A Unified Survey of Treatment Effect Heterogeneity Modelling and Uplift
   Modelling},
Journal = {ACM COMPUTING SURVEYS},
Year = {2021},
Volume = {54},
Number = {8},
Month = {NOV},
Abstract = {A central question in many fields of scientific research is to determine
   how an outcome is affected by an action, i.e., to estimate the causal
   effect or treatment effect of an action. In recent years, in areas such
   as personalised healthcare, sociology, and online marketing, a need has
   emerged to estimate heterogeneous treatment effects with respect to
   individuals of different characteristics. To meet this need, two major
   approaches have been taken: treatment effect heterogeneity modelling and
   uplifting modelling. Researchers and practitioners in different
   communities have developed algorithms based on these approaches to
   estimate the heterogeneous treatment effects. In this article, we
   present a unified view of these two seemingly disconnected yet closely
   related approaches under the potential outcome framework. We provide a
   structured survey of existing methods following either of the two
   approaches, emphasising their inherent connections and using unified
   notation to facilitate comparisons. We also review the main applications
   of the surveyed methods in personalised marketing, personalised
   medicine, and sociology. Finally, we summarise and discuss the available
   software packages and source codes in terms of their coverage of
   different methods and applicability to different datasets, and we
   provide general guidelines for method selection.},
DOI = {10.1145/3466818},
Article-Number = {162},
ISSN = {0360-0300},
EISSN = {1557-7341},
ResearcherID-Numbers = {Liu, Lin/ABD-1224-2020
   Li, Jiuyong/A-8134-2008
   },
ORCID-Numbers = {Liu, Lin/0000-0003-2843-5738
   Li, Jiuyong/0000-0002-9023-1878
   Zhang, Weijia/0000-0001-8103-5325},
Unique-ID = {WOS:000705073600006},
}

@article{ WOS:000807178800001,
Author = {Zhao, Yikun and Jiang, Bin and Huo, Yongxue and Yi, Hongmei and Tian,
   Hongli and Wu, Haotian and Wang, Rui and Zhao, Jiuran and Wang, Fengge},
Title = {A High-Performance Database Management System for Managing and Analyzing
   Large-Scale SNP Data in Plant Genotyping and Breeding Applications},
Journal = {AGRICULTURE-BASEL},
Year = {2021},
Volume = {11},
Number = {11},
Month = {NOV},
Abstract = {A DNA fingerprint database is an efficient, stable, and automated tool
   for plant molecular research that can provide comprehensive technical
   support for multiple fields of study, such as pan-genome analysis and
   crop breeding. However, constructing a DNA fingerprint database for
   plants requires significant resources for data output, storage,
   analysis, and quality control. Large amounts of heterogeneous data must
   be processed efficiently and accurately. Thus, we developed plant SNP
   database management system (PSNPdms) using an open-source web server and
   free software that is compatible with single nucleotide polymorphism
   (SNP), insertion-deletion (InDel) markers, Kompetitive Allele Specific
   PCR (KASP), SNP array platforms, and 23 species. It fully integrates
   with the KASP platform and allows for graphical presentation and
   modification of KASP data. The system has a simple, efficient, and
   versatile laboratory personnel management structure that adapts to
   complex and changing experimental needs with a simple workflow process.
   PSNPdms internally provides effective support for data quality control
   through multiple dimensions, such as the standardized experimental
   design, standard reference samples, fingerprint statistical selection
   algorithm, and raw data correlation queries. In addition, we developed a
   fingerprint-merging algorithm to solve the problem of merging
   fingerprints of mixed samples and single samples in plant detection,
   providing unique standard fingerprints of each plant species for
   construction of a standard DNA fingerprint database. Different
   laboratories can use the system to generate fingerprint packages for
   data interaction and sharing. In addition, we integrated genetic
   analysis into the system to enable drawing and downloading of
   dendrograms. PSNPdms has been widely used by 23 institutions and has
   proven to be a stable and effective system for sharing data and
   performing genetic analysis. Interested researchers are required to
   adapt and further develop the system.},
DOI = {10.3390/agriculture11111027},
Article-Number = {1027},
EISSN = {2077-0472},
Unique-ID = {WOS:000807178800001},
}

@article{ WOS:000763063000001,
Author = {Lei, Di and Kim, Sae-Hoon},
Title = {Design of 3D Modeling Face Image Library in Multimedia Film and
   Television},
Journal = {JOURNAL OF SENSORS},
Year = {2021},
Volume = {2021},
Month = {OCT 15},
Abstract = {The development of 3D modeling technology has promoted the development
   of the multimedia film and television industry. This article is aimed at
   studying the design of 3D modeling facial image library in multimedia
   film and television, at providing a more comprehensive facial image
   library for the multimedia film and television industry, at breaking the
   shackles of the traditional film and television industry with 3D
   technology, and at continuously surpassing traditional film and
   television media forms. This article deeply explores the background
   development of multimedia film and television and the characteristics of
   the development of new media. Starting from 3D technology, it extracts
   facial features of characters, transforms image data through deep
   autoencoders, and uses local binarization mode to perform the original
   facial image is subjected to texture feature extraction. In this paper,
   a number of experimental subjects were selected, and the subjects were
   photographed from the left, front, and right from multiple angles.
   Through the pinhole camera projection imaging process, the internal and
   external parameters of the camera were adjusted. In the process of 3D
   image construction, the image is first selected for feature detection,
   then the corresponding vector information and geometric conditions are
   matched to construct a 3D matrix, and the facial structure image is
   obtained by triangulation. This article compares the 3D production
   software on the market and selects the Maya platform suitable for
   building this system. The global constraint information is obtained by
   training some sample images. When searching the test image, find the
   appropriate feature point position according to the structural matching
   degree of the local image. When each search is completed, the global
   information will be used for constraint, so as to output reasonable
   feature information. The average residual range of the human face image
   constructed in this paper is 0.25-0.45, and the maximum residual error
   does not exceed 4.0. The experimental method in this paper has good
   stability and robustness. Using the COM transmission model can make
   experimenters not need to think too much about the underlying details.
   This face animation-driven simulation scheme can achieve more vivid
   facial expressions.},
DOI = {10.1155/2021/2221893},
Article-Number = {2221893},
ISSN = {1687-725X},
EISSN = {1687-7268},
Unique-ID = {WOS:000763063000001},
}

@article{ WOS:000715227000001,
Author = {Josbert, Nteziriza Nkerabahizi and Ping, Wang and Wei, Min and Li, Yong},
Title = {Industrial Networks Driven by SDN Technology for Dynamic Fast Resilience},
Journal = {INFORMATION},
Year = {2021},
Volume = {12},
Number = {10},
Month = {OCT},
Abstract = {Software-Defined Networking (SDN) provides the prospect of logically
   centralized management in industrial networks and simplified programming
   among devices. It also facilitates the reconfiguration of connectivity
   when there is a network element failure. This paper presents a new
   Industrial SDN (ISDN) resilience that addresses the gap between two
   types of resilience: the first is restoration while the second is
   protection. Using a restoration approach increases the recovery time
   proportionally to the number of affected flows contrarily to the
   protection approach which attains the fast recovery. Nevertheless, the
   protection approach utilizes more flow rules (flow entries) in the
   switch which in return increments the lookup time taken to discover an
   appropriate flow entry in the flow table. This can have a negative
   effect on the end-to-end delay before a failure occurs (in the normal
   situation). In order to balance both approaches, we propose a Mixed Fast
   Resilience (MFR) approach to ensure the fast recovery of the primary
   path without any impact on the end-to-end delay in the normal situation.
   In the MFR, the SDN controller establishes a new path after failure
   detection and this is based on flow rules stored in its memory through
   the dynamic hash table structure as the internal flow table. At that
   time, it transmits the flow rules to all switches across the appropriate
   secondary path simultaneously from the failure point to the destination
   switch. Moreover, these flow rules which correspond to secondary paths
   are cached in the hash table by considering the current minimum path
   weight. This strategy leads to reduction in the load at the SDN
   controller and the calculation time of a new working path. The MFR
   approach applies the dual primary by considering several metrics such as
   packet-loss probability, delay, and bandwidth which are the Quality of
   Service (QoS) requirements for many industrial applications. Thus, we
   have built a simulation network and conducted an experimental testbed.
   The results showed that our resilience approach reduces the failure
   recovery time as opposed to the restoration approaches and is more
   scalable than a protection approach. In the normal situation, the MFR
   approach reduces the lookup time and end-to-end delay than a protection
   approach. Furthermore, the proposed approach improves the performance by
   minimizing the packet loss even under failing links.},
DOI = {10.3390/info12100420},
Article-Number = {420},
EISSN = {2078-2489},
Unique-ID = {WOS:000715227000001},
}

@article{ WOS:000698885200001,
Author = {Bender, Brian J. and Gahbauer, Stefan and Luttens, Andreas and Lyu,
   Jiankun and Webb, Chase M. and Stein, Reed M. and Fink, Elissa A. and
   Balius, Trent E. and Carlsson, Jens and Irwin, John J. and Shoichet,
   Brian K.},
Title = {A practical guide to large-scale docking},
Journal = {NATURE PROTOCOLS},
Year = {2021},
Volume = {16},
Number = {10},
Pages = {4799-4832},
Month = {OCT},
Abstract = {Structure-based docking screens of large compound libraries have become
   common in early drug and probe discovery. As computer efficiency has
   improved and compound libraries have grown, the ability to screen
   hundreds of millions, and even billions, of compounds has become
   feasible for modest-sized computer clusters. This allows the rapid and
   cost-effective exploration and categorization of vast chemical space
   into a subset enriched with potential hits for a given target. To
   accomplish this goal at speed, approximations are used that result in
   undersampling of possible configurations and inaccurate predictions of
   absolute binding energies. Accordingly, it is important to establish
   controls, as are common in other fields, to enhance the likelihood of
   success in spite of these challenges. Here we outline best practices and
   control docking calculations that help evaluate docking parameters for a
   given target prior to undertaking a large-scale prospective screen, with
   exemplification in one particular target, the melatonin receptor, where
   following this procedure led to direct docking hits with activities in
   the subnanomolar range. Additional controls are suggested to ensure
   specific activity for experimentally validated hit compounds. These
   guidelines should be useful regardless of the docking software used.
   Docking software described in the outlined protocol (DOCK3.7) is made
   freely available for academic research to explore new hits for a range
   of targets.
   Structure-based docking screens of compound libraries are common in
   early drug and probe discovery. This protocol outlines best practices
   and control calculations to evaluate docking parameters prior to
   undertaking a large-scale prospective screen.},
DOI = {10.1038/s41596-021-00597-z},
EarlyAccessDate = {SEP 2021},
ISSN = {1754-2189},
EISSN = {1750-2799},
ResearcherID-Numbers = {Carlsson, Jens/HJI-8377-2023
   },
ORCID-Numbers = {Carlsson, Jens/0000-0003-4623-2977
   Lyu, Jiankun/0000-0002-0461-5454
   Balius, Trent/0000-0002-6811-4667
   Luttens, Andreas/0000-0003-2915-7901
   Gahbauer, Stefan/0000-0002-3115-9757},
Unique-ID = {WOS:000698885200001},
}

@article{ WOS:000707413800002,
Author = {Zhang, Jiahui and Fakharzadeh, Ashkan and Pan, Feng and Roland,
   Christopher and Sagui, Celeste},
Title = {Construction of DNA/RNA Triplex Helices Based on GAA/TTC Trinucleotide
   Repeats},
Journal = {BIO-PROTOCOL},
Year = {2021},
Volume = {11},
Number = {18},
Month = {SEP 20},
Abstract = {Atypical DNA and RNA secondary structures play a crucial role in simple
   sequence repeat (SSR) diseases, which are associated with a class of
   neurological and neuromuscular disorders known as ``anticipation
   diseases,{''} where the age of disease onset decreases and the severity
   of the disease is increased as the intergenerational expansion of the
   SSR increases. While the mechanisms underlying these diseases are
   complex and remain elusive, there is a consensus that stable, non-B-DNA
   atypical secondary structures play an important - if not causative -
   role. These structures include single-stranded DNA loops and hairpins,
   G-quartets, Z-DNA, triplex nucleic acid structures, and others. While
   all of these structures are of interest, structures based on nucleic
   acid triplexes have recently garnered increased attention as they have
   been implicated in gene regulation, gene repair, and gene engineering.
   Our work here focuses on the construction of DNA triplexes and RNA/DNA
   hybrids formed from GAA/TTC trinucleotide repeats, which underlie
   Friedreich's ataxia. While there is some software, such as the Discovery
   Studio Visualizer, that can aid in the initial construction of DNA
   triple helices, the only option for the triple helix is constrained to
   be that of an antiparallel pyrimidine for the third strand. In this
   protocol, we illustrate how to build up more generalized DNA triplexes
   and DNA/RNA mixed hybrids. We make use of both the Discovery Studio
   Visualizer and the AMBER simulation package to construct the initial
   triplexes. Using the steps outlined here, one can - in principle - build
   up any triple nucleic acid helix with a desired sequence for large-scale
   molecular dynamics simulation studies.},
DOI = {10.21769/BioProtoc.4155},
Article-Number = {e4155},
EISSN = {2331-8325},
Unique-ID = {WOS:000707413800002},
}

@article{ WOS:000703600100001,
Author = {Han, Yanqiang and Ali, Imran and Wang, Zhilong and Cai, Junfei and Wu,
   Sicheng and Tang, Jiequn and Zhang, Lin and Ren, Jiahao and Xiao, Rui
   and Lu, Qianqian and Hang, Lei and Luo, Hongyuan and Li, Jinjin},
Title = {Machine learning accelerates quantum mechanics predictions of molecular
   crystals},
Journal = {PHYSICS REPORTS-REVIEW SECTION OF PHYSICS LETTERS},
Year = {2021},
Volume = {934},
Number = {SI},
Pages = {1-71},
Month = {NOV 3},
Abstract = {Quantum mechanics (QM) approaches (DFT, MP2, CCSD(T), etc.) play an
   important role in calculating molecules and crystals with a high
   accuracy and acceptable efficiency. In recent years, with the
   development of artificial intelligence technology, machine learning (ML)
   has played an increasingly essential role in accelerating the QM
   calculations and predictions of molecular crystals, as well as the
   discovery of novel materials. This review provides state-of-the-art
   information and prospects for QM theories, fragment based methods and ML
   methods, as well as their up-to-date applications in predicting small
   inorganic molecules, large drug molecules and relevant molecular
   crystals. The discussed applications include ML potential energy surface
   (PES) construction, crystal structure prediction (CSP), chemical
   reaction prediction and predictions of a series of properties, such as
   structure, energy, atomic force, bond length, chemical shift,
   superconductivity, super-hardness, vibrational spectra, phase transition
   and diagram. This work also reviews software and packages built recently
   based on ML methods for property predictions and PES constructions in
   the field of physics and chemistry. For the three discussed methods, the
   most time-consuming one is the high-level all-atom QM method, which is
   capable of describing electronic structures with high accuracy and thus
   predicts properties that are consistent with the experimental results.
   The second one, fragment-based QM method, requires less computational
   time than all atom QM, which can accelerate all-atom QM calculations for
   large systems by dividing the entire system into subsystems, presenting
   a considerable efficiency increase. The computational complexities for
   fragment-based QM and all-atom QM are N N2 and N5 -N7 (N is the size of
   the system), respectively. A well-trained ML model can make the above
   predictions within seconds while ensuring a high prediction accuracy,
   where its prediction cost and accuracy are determined by the training
   data and the training process. Therefore, it is challenging for ML
   applications in physics and chemistry to generate highly accurate and
   powerful ML models while ensuring sufficient datasets. This work not
   only provides an overview of the recent progress in QM theories,
   fragment-based methods, ML methods and several ML-based software
   programs and applications on small inorganic molecules, large drug
   molecules and relevant crystals, but also shed light on ML methods in
   accelerating QM prediction, optimization and novel crystal material
   design. (C) 2021 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.physrep.2021.08.002},
EarlyAccessDate = {SEP 2021},
ISSN = {0370-1573},
EISSN = {1873-6270},
ResearcherID-Numbers = {Wang, Zhilong/AAU-1200-2020
   },
ORCID-Numbers = {Wang, Zhilong/0000-0002-1910-3654
   Han, Yanqiang/0000-0001-5454-0617},
Unique-ID = {WOS:000703600100001},
}

@article{ WOS:000696635600001,
Author = {Rosiak, Kamil and Schlie, Alexander and Linsbauer, Lukas and
   Vogel-Heuser, Birgit and Schaefer, Ina},
Title = {Custom-tailored clone detection for IEC 61131-3 programming languages},
Journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
Year = {2021},
Volume = {182},
Month = {DEC},
Abstract = {Automated production systems (aPS) are highly customized systems that
   consist of hardware and software. Such aPS are controlled by a
   programmable logic controller (PLC), often in accordance with the IEC
   61131-3 standard that divides system implementation into so-called
   program organization units (POUs) as the smallest software unit and is
   comprised of multiple textual (Structured Text (ST)) and graphical
   (Function Block Diagram (FBD), Ladder Diagram (LD), and Sequential
   Function Chart(SFC)) programming languages that can be arbitrarily
   nested.
   A common practice during the development of such systems is reusing
   implementation artifacts by copying, pasting, and then modifying code.
   This approach is referred to as code cloning. It is used on a
   fine-granular level where a POU is cloned within a system variant. It is
   also applied on the coarse-granular system level, where the entire
   system is cloned and adapted to create a system variant, for example for
   another customer. This ad hoc practice for the development of variants
   is commonly referred to as clone-and-own. It allows the fast development
   of variants to meet varying customer requirements or altered regulatory
   guidelines. However, clone-and-own is a non-sustainable approach and
   does not scale with an increasing number of variants. It has a
   detrimental effect on the overall quality of a software system, such as
   the propagation of bugs to other variants, which harms maintenance.
   In order to support the effective development and maintenance of such
   systems, a detailed code clone analysis is required. On the one hand, an
   analysis of code clones within a variant (i.e., clone detection in the
   classical sense) supports experts in refactoring respective code into
   library components. On the other hand, an analysis of commonalities and
   differences between cloned variants (i.e., variability analysis)
   supports the maintenance and further reuse and facilitates the migration
   of variants into a software productline (SPL).
   In this paper, we present an approach for the automated detection of
   code clones within variants (intra variant clone detection) and between
   variants (inter variant clone detection) of IEC61131-3 control software
   with arbitrary nesting of both textual and graphical languages. We
   provide an implementation of the approach in the variability analysis
   toolkit (VAT) as a freely available prototype for the analysis of IEC
   61131-3 programs. For the evaluation, we developed a meta-model-based
   mutation framework to measure our approach's precision and recall.
   Besides, we evaluated our approach using the Pick and Place Unit (PPU)
   and Extended Pick and Place Unit (xPPU) scenarios. Results show the
   usefulness of intra and inter clone detection in the domain of automated
   production systems. (C) 2021 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.jss.2021.111070},
EarlyAccessDate = {SEP 2021},
Article-Number = {111070},
ISSN = {0164-1212},
EISSN = {1873-1228},
Unique-ID = {WOS:000696635600001},
}

@article{ WOS:000702535900002,
Author = {Cerasoli, Frank T. and Supka, Andrew R. and Jayaraj, Anooja and Costa,
   Marcio and Siloi, Ilaria and Slawinska, Jagoda and Curtarolo, Stefano
   and Fornari, Marco and Ceresoli, Davide and Nardelli, Marco Buongiorno},
Title = {Advanced modeling of materials with PAOFLOW 2.0: New features and
   software design},
Journal = {COMPUTATIONAL MATERIALS SCIENCE},
Year = {2021},
Volume = {200},
Month = {DEC},
Abstract = {Recent research in materials science opens exciting perspectives to
   design novel quantum materials and devices, but it calls for
   quantitative predictions of properties which are not accessible in
   standard first principles packages. PAOFLOW, is a software tool that
   constructs tight-binding Hamiltonians from self consistent electronic
   wavefunctions by projecting onto a set of atomic orbitals. The
   electronic structure provides numerous materials properties that
   otherwise would have to be calculated via phenomenological models. In
   this paper, we describe recent re-design of the code as well as the new
   features and improvements in performance. In particular, we have
   implemented symmetry operations for unfolding equivalent k-points, which
   drastically reduces the runtime requirements of first principles
   calculations, and we have provided internal routines of projections onto
   atomic orbitals enabling generation of real space atomic orbitals.
   Moreover, we have included models for non-constant relaxation time in
   electronic transport calculations, doubling the real space dimensions of
   the Hamiltonian as well as the construction of Hamiltonians directly
   from analytical models. Importantly, PAOFLOW has been now converted into
   a Python package, and is streamlined for use directly within other
   Python codes. The new object oriented design treats PAOFLOW's
   computational routines as class methods, providing an API for explicit
   control of each calculation.},
DOI = {10.1016/j.commatsci.2021.110828},
EarlyAccessDate = {SEP 2021},
Article-Number = {110828},
ISSN = {0927-0256},
EISSN = {1879-0801},
ResearcherID-Numbers = {Slawinska, Jagoda/H-6679-2016
   Fornari, Marco/C-8848-2012
   Curtarolo, Stefano/ABD-7672-2021
   Costa, Marcio/J-1890-2016},
ORCID-Numbers = {Slawinska, Jagoda/0000-0003-1446-1812
   Fornari, Marco/0000-0001-6527-8511
   Curtarolo, Stefano/0000-0003-0570-8238
   Siloi, Ilaria/0000-0002-3806-2034
   Ceresoli, Davide/0000-0002-9831-0773
   Supka, Andrew/0000-0002-9043-1597
   Costa, Marcio/0000-0003-1029-8202},
Unique-ID = {WOS:000702535900002},
}

@article{ WOS:000683878200005,
Author = {Liu, Zhen and Gu, Xingyu and Dong, Qiao and Tu, Shanshan and Li, Shuwei},
Title = {3D Visualization of Airport Pavement Quality Based on BIM and WebGL
   Integration},
Journal = {JOURNAL OF TRANSPORTATION ENGINEERING PART B-PAVEMENTS},
Year = {2021},
Volume = {147},
Number = {3},
Month = {SEP 1},
Abstract = {Building information modeling (BIM) is well-known for improving the
   efficiency of the Architecture, Engineering, and Construction (AEC)
   industries. Currently, the support for web displays using web graphics
   library (WebGL) technology in BIM has gained popularity. To address
   issues of insufficient display for airport pavements by two-dimensional
   (2D) methods and the huge amount of data storage requirements for
   large-scale BIM scenes, this paper presents a visualization method for
   differential expressions for airport pavement quality on web-based
   three-dimensional (3D) models combined with Revit and Three.js software.
   An airport is used as an example to discuss the effects of quality
   displays by integrating BIM and WebGL. The results indicate that Revit
   software has good adaptability in establishing an airport pavement
   model, and the BIM browser can display the quality of the airport
   pavement intuitively, displaying such parameters as pavement condition
   index (PCI), cracking, and patching. These findings provide compelling
   evidence for 3D displaying and rendering of quality conditions for
   transportation infrastructure. Moreover, this research suggests that
   this approach is effective in improving the information and intelligence
   levels of the Airport Pavement Management System (APMS).},
DOI = {10.1061/JPEODX.0000280},
Article-Number = {04021024},
EISSN = {2573-5438},
ResearcherID-Numbers = {Liu, Zhen/CAJ-3474-2022},
ORCID-Numbers = {Liu, Zhen/0000-0001-8012-7682},
Unique-ID = {WOS:000683878200005},
}

@article{ WOS:000685266800005,
Author = {Seyoum, Tewodros and Alemayehu, Mekuriaw and Christensson, Kyllike and
   Lindgren, Helena},
Title = {Provider-perceived benefits and constraints of complete adherence to
   antenatal care guideline among public health facilities, Ethiopia: A
   qualitative study},
Journal = {PLOS ONE},
Year = {2021},
Volume = {16},
Number = {8},
Month = {AUG 9},
Abstract = {Background In Ethiopia, health care providers' level of adherence to the
   national Antenatal Care (ANC) guideline is relatively low. The reasons
   why they do not follow the guidelines are not well known. Therefore,
   this study aimed to explore the provider-perceived benefits and
   constraints associated with using the guideline for ANC in public health
   facilities in Gondar town. Methods A qualitative study was conducted
   using a semi-structured interview guide. The interview was conducted
   among a purposive sample of nine health care providers working in four
   public health facilities in Gondar town. After the interviews were
   transcribed and coded, a content analysis was done using Atlas ti
   version 7.5 software packages. Result Decreasing provider's workload and
   maximizing performance, improving safe motherhood, and improving the
   process of service delivery were reported as the perceived benefits of
   following ANC guideline. Organizational problems, care providers'
   existing knowledge, attitude, and skills and availability of training
   and mentorship were the three main identified groups of factors that
   hinder complete providers' adherence to ANC guideline. Conclusion
   Although providers acknowledged the benefits of following ANC guideline,
   the guideline is not fully implemented. Refresher training should be
   given at the start of the updated eight-contact ANC guideline and
   continuing education and supervision throughout the implementation
   process. Health care providers call for profound and urgent revisions of
   the supply chain system for supplies and equipment.},
DOI = {10.1371/journal.pone.0255297},
Article-Number = {e0255297},
ISSN = {1932-6203},
ORCID-Numbers = {Seyoum, Tewodros/0000-0002-9915-6368},
Unique-ID = {WOS:000685266800005},
}

@article{ WOS:000674548800002,
Author = {Zhang, Sherong and Jiang, Peiqi},
Title = {Implementation of BIM plus WebGIS Based on Extended IFC and Batched 3D
   Tiles Data: An Application in RCC Gravity Dam for Republication of
   Design Change Model},
Journal = {KSCE JOURNAL OF CIVIL ENGINEERING},
Year = {2021},
Volume = {25},
Number = {11},
Pages = {4045-4064},
Month = {NOV},
Abstract = {The construction of hydropower project is a complex management process,
   whose management covers the entire construction site. Building
   information modeling (BIM) + Web Geographic Information System (WebGIS)
   is an effective means to solve the management of the construction site
   and main structures. Industry foundation classes (IFC) is commonly used
   in architecture, engineering and construction (AEC) industry as a
   standard for BIM data exchange. However, as the components of hydropower
   projects have not been included in IFC standards, IFC needs to be
   extended in application. At the same time, WebGIS pays more attention to
   display whole scene and ignores the processing of high-precision models.
   There will be deficiencies in network transmission and browser rendering
   in WebGIS environment of elaborate BIM model. Therefore, the batched
   three dimensions (3D) Tiles data format, which uses Web Graphics Library
   (WebGL) as the standard for 3D model rendering, has been widely
   considered in recent years. At present, in the field of BIM and GIS
   fusion, the conversion of extended IFC to 3D Tiles data is still blank,
   and none of the current BIM modeling software has the ability to export
   the components of hydropower projects. This paper proposes an extended
   IFC export method based on Autodesk Revit and a data exchange from
   extended IFC to batched 3D Tiles. Finally, the effectiveness of this
   method is verified by a design modification case of roller compacted
   concrete (RCC) gravity dam.},
DOI = {10.1007/s12205-021-0115-9},
EarlyAccessDate = {JUL 2021},
ISSN = {1226-7988},
EISSN = {1976-3808},
Unique-ID = {WOS:000674548800002},
}

@article{ WOS:000679997600001,
Author = {Zopes, Jonathan and Platscher, Moritz and Paganucci, Silvio and Federau,
   Christian},
Title = {Multi-Modal Segmentation of 3D Brain Scans Using Neural Networks},
Journal = {FRONTIERS IN NEUROLOGY},
Year = {2021},
Volume = {12},
Month = {JUL 14},
Abstract = {Anatomical segmentation of brain scans is highly relevant for
   diagnostics and neuroradiology research. Conventionally, segmentation is
   performed on T-1-weighted MRI scans, due to the strong soft-tissue
   contrast. In this work, we report on a comparative study of automated,
   learning-based brain segmentation on various other contrasts of MRI and
   also computed tomography (CT) scans and investigate the anatomical
   soft-tissue information contained in these imaging modalities. A large
   database of in total 853 MRI/CT brain scans enables us to train
   convolutional neural networks (CNNs) for segmentation. We benchmark the
   CNN performance on four different imaging modalities and 27 anatomical
   substructures. For each modality we train a separate CNN based on a
   common architecture. We find average Dice scores of 86.7 +/- 4.1\%
   (T-1-weighted MRI), 81.9 +/- 6.7\% (fluid-attenuated inversion recovery
   MRI), 80.8 +/- 6.6\% (diffusion-weighted MRI) and 80.7 +/- 8.2\% (CT),
   respectively. The performance is assessed relative to labels obtained
   using the widely-adopted FreeSurfer software package. The segmentation
   pipeline uses dropout sampling to identify corrupted input scans or
   low-quality segmentations. Full segmentation of 3D volumes with more
   than 2 million voxels requires < 1s of processing time on a graphical
   processing unit.},
DOI = {10.3389/fneur.2021.653375},
Article-Number = {653375},
ISSN = {1664-2295},
Unique-ID = {WOS:000679997600001},
}

@article{ WOS:000671340700001,
Author = {Privratsky, Jan and Novak, Jiri},
Title = {MassSpecBlocks: a web-based tool to create building blocks and sequences
   of nonribosomal peptides and polyketides for tandem mass spectra
   analysis},
Journal = {JOURNAL OF CHEMINFORMATICS},
Year = {2021},
Volume = {13},
Number = {1},
Month = {JUL 7},
Abstract = {Nonribosomal peptides and polyketides are natural products commonly
   synthesized by microorganisms. They are widely used in medicine,
   agriculture, environmental protection, and other fields. The structures
   of natural products are often analyzed by high-resolution tandem mass
   spectrometry, which becomes more popular with its increasing
   availability. However, the characterization of nonribosomal peptides and
   polyketides from tandem mass spectra is a nontrivial task because they
   are composed of many uncommon building blocks in addition to
   proteinogenic amino acids. Moreover, many of them have cyclic and
   branch-cyclic structures. Here, we introduce MassSpecBlocks - an
   opensource and web-based tool that converts the input chemical
   structures in SMILES format into sequences of building blocks. The
   structures can be searched in public databases PubChem, ChemSpider,
   ChEBI, NP Atlas, COCONUT, and Norine and edited in a user-friendly
   graphical interface. Although MassSpecBlocks can serve as a stand-alone
   database, our primary goal was to enable easy construction of custom
   sequence and building block databases, which can be used to annotate
   mass spectra in CycloBranch software. CycloBranch is an open-source,
   cross-platform, and standalone tool that we recently released for
   annotating spectra of linear, cyclic, branched, and branch-cyclic
   nonribosomal peptides and polyketide siderophores. The sequences and
   building blocks created in MassSpecBlocks can be easily exported into a
   plain text format used by CycloBranch. MassSpecBlocks is available
   online or can be installed entirely offline. It offers a REST API to
   cooperate with other tools.},
DOI = {10.1186/s13321-021-00530-2},
Article-Number = {51},
ISSN = {1758-2946},
ResearcherID-Numbers = {Novák, Jiří/J-8071-2014},
ORCID-Numbers = {Novák, Jiří/0000-0003-1902-4304},
Unique-ID = {WOS:000671340700001},
}

@article{ WOS:000679941500022,
Author = {Canales, Lea and Menke, Sebastian and Marchesseau, Stephanie and
   D'Agostino, Ariel and Del Rio-Bermudez, Carlos and Taberna, Miren and
   Tello, Jorge},
Title = {Assessing the Performance of Clinical Natural Language Processing
   Systems: Development of an Evaluation Methodology},
Journal = {JMIR MEDICAL INFORMATICS},
Year = {2021},
Volume = {9},
Number = {7},
Month = {JUL},
Abstract = {Background: Clinical natural language processing (cNLP) systems are of
   crucial importance due to their increasing capability in extracting
   clinically important information from free text contained in electronic
   health records (EHRs). The conversion of a nonstructured representation
   of a patient's clinical history into a structured format enables medical
   doctors to generate clinical knowledge at a level that was not possible
   before. Finally, the interpretation of the insights gained provided by
   cNLP systems has a great potential in driving decisions about clinical
   practice. However, carrying out robust evaluations of those cNLP systems
   is a complex task that is hindered by a lack of standard guidance on how
   to systematically approach them.
   Objective: Our objective was to offer natural language processing (NLP)
   experts a methodology for the evaluation of cNLP systems to assist them
   in carrying out this task. By following the proposed phases, the
   robustness and representativeness of the performance metrics of their
   own cNLP systems can be assured.
   Methods: The proposed evaluation methodology comprised five phases: (1)
   the definition of the target population, (2) the statistical document
   collection, (3) the design of the annotation guidelines and annotation
   project, (4) the external annotations, and (5) the cNLP system
   performance evaluation. We presented the application of all phases to
   evaluate the performance of a cNLP system called ``EHRead Technology{''}
   (developed by Savana, an international medical company), applied in a
   study on patients with asthma. As part of the evaluation methodology, we
   introduced the Sample Size Calculator for Evaluations (SLiCE), a
   software tool that calculates the number of documents needed to achieve
   a statistically useful and resourceful gold standard.
   Results: The application of the proposed evaluation methodology on a
   real use-case study of patients with asthma revealed the benefit of the
   different phases for cNLP system evaluations. By using SLiCE to adjust
   the number of documents needed, a meaningful and resourceful gold
   standard was created. In the presented use-case, using as little as 519
   EHRs, it was possible to evaluate the performance of the cNLP system and
   obtain performance metrics for the primary variable within the expected
   CIs.
   Conclusions: We showed that our evaluation methodology can offer
   guidance to NLP experts on how to approach the evaluation of their cNLP
   systems. By following the five phases, NLP experts can assure the
   robustness of their evaluation and avoid unnecessary investment of human
   and financial resources. Besides the theoretical guidance, we offer
   SLiCE as an easy-to-use, open-source Python library.},
DOI = {10.2196/20492},
Article-Number = {e20492},
EISSN = {2291-9694},
ORCID-Numbers = {Taberna, Miren/0000-0002-2446-186X
   Del Rio-Bermudez, Carlos/0000-0002-1036-1673
   Menke, Sebastian/0000-0002-2588-6405
   Tello, Jorge/0000-0002-3999-1572},
Unique-ID = {WOS:000679941500022},
}

@article{ WOS:000676292500001,
Author = {Chen, Long and Wu, Binxin},
Title = {Research Progress in Computational Fluid Dynamics Simulations of
   Membrane Distillation Processes: A Review},
Journal = {MEMBRANES},
Year = {2021},
Volume = {11},
Number = {7},
Month = {JUL},
Abstract = {Membrane distillation (MD) can be used in drinking water treatment, such
   as seawater desalination, ultra-pure water production, chemical
   substances concentration, removal or recovery of volatile solutes in an
   aqueous solution, concentration of fruit juice or liquid food, and
   wastewater treatment. However, there is still much work to do to
   determine appropriate industrial implementation. MD processes refer to
   thermally driven transport of vapor through non-wetted porous
   hydrophobic membranes, which use the vapor pressure difference between
   the two sides of the membrane pores as the driving force. Recently,
   computational fluid dynamics (CFD) simulation has been widely used in MD
   process analysis, such as MD mechanism and characteristics analysis,
   membrane module development, preparing novel membranes, etc. A series of
   related research results have been achieved, including the solutions of
   temperature/concentration polarization and permeate flux enhancement. In
   this article, the research of CFD applications in MD progress is
   reviewed, including the applications of CFD in the mechanism and
   characteristics analysis of different MD structures, in the design and
   optimization of membrane modules, and in the preparation and
   characteristics analysis of novel membranes. The physical phenomena and
   geometric structures have been greatly simplified in most CFD
   simulations of MD processes, so there still is much work to do in this
   field in the future. A great deal of attention has been paid to the
   hydrodynamics and heat transfer in the channels of MD modules, as well
   as the optimization of these modules. However, the study of momentum
   transfer, heat, and mass transfer mechanisms in membrane pores is rarely
   involved. These projects should be combined with mass transfer, heat
   transfer and momentum transfer for more comprehensive and in-depth
   research. In most CFD simulations of MD processes, some physical
   phenomena, such as surface diffusion, which occur on the membrane
   surface and have an important guiding significance for the preparation
   of novel membranes to be further studied, are also ignored. As a result,
   although CFD simulation has been widely used in MD process modeling
   already, there are still some problems remaining, which should be
   studied in the future. It can be predicted that more complex mechanisms,
   such as permeable wall conditions, fouling dynamics, and multiple ionic
   component diffusion, will be included in the CFD modeling of MD
   processes. Furthermore, users' developed routines for MD processes will
   also be incorporated into the existing commercial or open source CFD
   software packages.},
DOI = {10.3390/membranes11070513},
Article-Number = {513},
EISSN = {2077-0375},
Unique-ID = {WOS:000676292500001},
}

@article{ WOS:000646889400001,
Author = {Chen, Zhao and Chen, Meng and Wang, Jiangjiang and Liu, Rongzheng and
   Shao, Youlin and Liu, Bing and Tang, Yaping and Liu, Malin and Yang,
   Wuqiang},
Title = {Imaging irregular structures using electrical capacitance tomography},
Journal = {MEASUREMENT SCIENCE AND TECHNOLOGY},
Year = {2021},
Volume = {32},
Number = {7},
Month = {JUL},
Abstract = {Electrical capacitance tomography (ECT) is widely used in research for
   different applications, e.g. in the petroleum, pharmaceutical and power
   industries, due to its nonintrusive and noninvasive features and low
   cost. So far, ECT has been successfully used with regular structures,
   such as circular and square shapes. However, in some cases, the
   measurement electrodes cannot be mounted directly on an irregular
   structure, high temperature objects or corrosive objects. In this work,
   a method is proposed for ECT to be used with an irregular structure, by
   filling the gap between the irregular structure and a regular ECT sensor
   wall. Accordingly, a split matrix method is proposed for image
   reconstruction. The characteristics of sensitivity maps with the filling
   method and different structures are investigated. To calculate
   sensitivity maps, two methods are used: the perturbation method and the
   potential method, and their effects on image quality are compared. The
   results show that there is no obvious difference between these two
   methods for imaging construction, but the potential method is more
   efficient than the perturbation method. Among different shapes, an image
   of a triangular object has a large deformation due to its sharp angle
   and hence the filling method is not suitable for imaging a triangular
   object. The split matrix method modified by relaxation factors is
   suitable for irregular structures in general and can generate
   satisfactory images. A hardware system based on an impedance analyser
   and a software package have been developed and used to validate the
   proposed methods. The experimental results show that the proposed method
   is promising for imaging two-phase flows in irregular structures.},
DOI = {10.1088/1361-6501/abdfa0},
Article-Number = {075006},
ISSN = {0957-0233},
EISSN = {1361-6501},
Unique-ID = {WOS:000646889400001},
}

@article{ WOS:000747050200024,
Author = {Ferreira, Agnelo Menino and Chodankar, Shradha U. and Vaz, Frederick
   Satiro and D'souza, Delia Basil and Kulkarni, Manojkumar S.},
Title = {Risk factors for colorectal cancer in Goa, India: A Hospital-based
   Case-Control study},
Journal = {INDIAN JOURNAL OF COMMUNITY MEDICINE},
Year = {2021},
Volume = {46},
Number = {3},
Pages = {474-478},
Month = {JUL-SEP},
Abstract = {Introduction: Colorectal cancer is the third most common cancer in men
   and the second in women worldwide. The objective of the present study
   was to determine and quantify important modifiable risk factors
   attributable to colorectal cancer, in order to explore the ways to
   reduce the incidence of colorectal cancer in this region. Materials and
   Methods: A case-control study was conducted at a tertiary care hospital
   in Goa, India. The study subjects were group matched for age and sex so
   as to include 110 cases and 110 controls. Only incident cases of
   colorectal cancer were recruited in the study. Predesigned structured
   questionnaire was utilized for data collection, while anthropometric
   measurements and laboratory investigations were conducted. Statistical
   analysis was conducted using SPSS software package. The study was
   approved by the Institutional Ethics Committee of the institute.
   Informed written consent was obtained from the study participants.
   Results: Smoking, smokeless tobacco use, alcohol consumption, red meat
   consumption, high body mass index (BMI), and the presence of Type 2
   diabetes mellitus were found to be the risk factors for colorectal
   cancer on univariate analysis, while fruit and vegetable consumption
   were found to be the protective factors. Multiple logistic regression
   analysis identified Type 2 diabetes mellitus and high BMI as risk
   factors for colorectal cancer and consumption of fruits and vegetables
   as protective factors. Conclusion: Identification of risk factors for
   colorectal cancer would help in setting of colorectal cancer screening
   guidelines as well as for creating awareness regarding prevention of
   colorectal cancer among the general population.},
DOI = {10.4103/ijcm.IJCM\_848\_20},
ISSN = {0970-0218},
EISSN = {1998-3581},
ResearcherID-Numbers = {Vaz, FS/AAC-2299-2022
   },
ORCID-Numbers = {Vaz, Frederick/0000-0003-3113-5920},
Unique-ID = {WOS:000747050200024},
}

@article{ WOS:000682125100002,
Author = {Neumann, Andy and Laranjeiro, Nuno and Bernardino, Jorge},
Title = {An Analysis of Public REST Web Service APIs},
Journal = {IEEE TRANSACTIONS ON SERVICES COMPUTING},
Year = {2021},
Volume = {14},
Number = {4},
Pages = {957-970},
Month = {JUL-AUG},
Abstract = {Businesses are increasingly deploying their services on the web, in the
   form of web applications, SOAP services, message-based services, and,
   more recently, REST services. Although the movement towards REST is
   widely recognized, there is not much concrete information regarding the
   technical features being used in the field, such as typical data
   formats, how HTTP verbs are being used, or typical URI structures, just
   to name a few. In this paper, we go through the Alexa.com top 4000 most
   popular sites to identify precisely 500 websites claiming to provide a
   REST web service API. We analyze these 500 APIs for key technical
   features, degree of compliance with REST architectural principles (e.g.,
   resource addressability), and for adherence to best practices (e.g., API
   versioning). We observed several trends (e.g., widespread JSON support,
   software-generated documentation), but, at the same time, high diversity
   in services, including differences in adherence to best practices, with
   only 0.8 percent of services strictly complying with all REST
   principles. Our results can help practitioners evolve guidelines and
   standards for designing higher quality services and also understand
   deficiencies in currently deployed services. Researchers may also
   benefit from the identification of key research areas, contributing to
   the deployment of more reliable services.},
DOI = {10.1109/TSC.2018.2847344},
ISSN = {1939-1374},
ResearcherID-Numbers = {Bernardino, Jorge/K-6437-2014
   },
ORCID-Numbers = {Bernardino, Jorge/0000-0001-9660-2011
   Neumann, Andy/0000-0002-8024-0041
   Laranjeiro, Nuno/0000-0003-0011-9901},
Unique-ID = {WOS:000682125100002},
}

@article{ WOS:000664672200049,
Author = {Luo, Wenping and Yan, Huifen and Guo, Sijie},
Title = {Evaluation of the efficiency of antibiotics in treating adult patients
   with symptomatic apical periodontitis A protocol for systematic review
   and meta-analysis},
Journal = {MEDICINE},
Year = {2021},
Volume = {100},
Number = {25},
Month = {JUN 25},
Abstract = {Background: When a person feels dental pain, it brings great discomfort
   and damages the quality of life. Symptomatic apical periodontitis is
   identified as the most frequent cause that triggers dental pain.
   Symptomatic apical periodontitis arises from an infection or
   inflammation in the pulpless root canal structure. According to clinical
   guidelines, the primary form of therapy for such teeth entails removing
   the inflammation or infection source through local surgical procedures.
   Presently, systemic antibiotics are recommended only for cases where
   there is clear indication of an infectious spread or a systemic
   involvement. Therefore, this study aims to assess the efficacy and level
   of safety of using antibiotics to treat adult symptomatic apical
   periodontitis patients.
   Methods: The present protocol study will conduct a search on electronic
   databases to look for randomized controlled trials (RCTs) that have
   evaluated the effectiveness and safety of antibiotics when used to treat
   adult patients with symptomatic apical periodontitis. The databases will
   be search from their beginning to April 2021. The search is not bound by
   publication status or language restrictions. The following databases
   will be searched: Web of Science, PubMed, the Cochrane Library, Chinese
   National Knowledge Infrastructure, and EMBASE. This study will employ
   ZETOC Conference Proceedings and OpenGrey to identify potential grey
   literature. Afterwards, 2 independent authors will select the studies,
   extract data from the studies, and conduct a risk assessment to check
   for bias. All discrepancies between the authors will be resolute via
   discussion involving a third independent author. The data synthesis and
   statistical analysis of this study will be done with the RevMan software
   (Version: 5.3).
   Results: The present protocol report will provide high-quality evidence
   related to the efficacy and level of safety when using antibiotics to
   treat mature symptomatic apical periodontitis patients.
   Conclusion: The outcomes of the present study will update the evidence
   available for assessing the efficacy and safeness of using antibiotics
   to treat mature symptomatic apical periodontitis patients.
   Ethics and dissemination: This study does not require an ethical
   approval since individual patient data is not included in any form.},
DOI = {10.1097/MD.0000000000026405},
ISSN = {0025-7974},
EISSN = {1536-5964},
Unique-ID = {WOS:000664672200049},
}

@article{ WOS:000683465000008,
Author = {Meng, Xianbin and Li, Lijie and Wang, Xiayan},
Title = {An integrated strategy for the construction of a species-specific glycan
   library for mass spectrometry-based intact glycopeptide analyses},
Journal = {TALANTA},
Year = {2021},
Volume = {234},
Month = {NOV 1},
Abstract = {Mass spectrometry (MS)-based strategies and related software tools using
   glycan mass lists have greatly facilitated the analysis of intact
   glycopeptides. Most glycan mass lists are derived from normal glycans of
   mammals and contain limited monosaccharides, which has significantly
   hindered high throughput studies of unusual glycosylation events
   observed in other species. In this work, an integrated strategy was
   developed for the construction of a species-specific glycan mass list
   from glycan structure databases and published papers. We developed a
   computational tool called LibGlycan, which could process the different
   formats of glycans. Then, the software tool generated a glycan library
   that contained the monoisotope mass, average mass, isotope distribution,
   and glycan mass list for input into Byonic software. This strategy was
   applied to analyze the N-glycosylation of rice roots and O-glycosylation
   of Acinetobacter baumannii ATCC17978, leading to the identification of
   296 and 145 intact glycopeptides respectively. Combined, these results
   show that this strategy is a robust computational approach for the
   determination of glycan diversity within different complex biological
   systems.},
DOI = {10.1016/j.talanta.2021.122626},
EarlyAccessDate = {JUN 2021},
Article-Number = {122626},
ISSN = {0039-9140},
EISSN = {1873-3573},
ResearcherID-Numbers = {li, li/HII-4157-2022
   Li, Li/AEM-3636-2022},
Unique-ID = {WOS:000683465000008},
}

@article{ WOS:000684201900003,
Author = {Truong, Dac Dung and Jang, Beom-Seon and Ju, Han-Baek},
Title = {Development of simplified method for prediction of structural response
   of stiffened plates under explosion loads},
Journal = {MARINE STRUCTURES},
Year = {2021},
Volume = {79},
Month = {SEP},
Abstract = {During their lifetime, marine structures may be exposed to accidental
   loadings such as from collisions or explosions, as well as environmental
   loadings such as from slamming, sloshing and green water. Such loadings
   can cause damage to structures. Therefore, to minimize such damage,
   advanced and robust design guidelines should be formulated. Among those
   loads, in this study, explosions imparting an impulsive pressure loading
   containing a rapid increase in pressure and a short duration that can
   cause serious casualties, property losses, and marine pollution were
   considered. In this paper, a practical and robust method for damage
   assessment of marine structures exposed to explosion loads based on a
   single degree of freedom (SDOF) system and numerical simulations is
   proposed. The SDOF method was improved by introduction of new and better
   idealization resistance for the system and consideration of the effect
   of strain-rate, and subsequently was verified by a numerical method
   developed using the commercial ABAQUS software package. The numerical
   method was itself validated by comparison with relevant pulse pressure
   test data available in the open literature (good correlation was shown).
   Based on the validated numerical models, a rigorous parametric study of
   the structural response of stiffened plates having actual scantlings of
   offshore structures was performed. The numerically obtained maximum
   deformations were compared with the results from the improved SDOF
   method in a parametric study, and the variation of both methods was
   verified. Finally, simple yet accurate and reliable formulations for
   prediction of structural response were empirically derived. These
   formulations are expected to be usefully employed as a first-hand tool
   for prediction of damage extent of marine structures (including offshore
   structures) due to explosion loads.},
DOI = {10.1016/j.marstruc.2021.103039},
EarlyAccessDate = {JUN 2021},
Article-Number = {103039},
ISSN = {0951-8339},
EISSN = {1873-4170},
ResearcherID-Numbers = {shi, danyang/ABG-6569-2021
   Truong, Dac Dung/AAR-2973-2020},
ORCID-Numbers = {Truong, Dac Dung/0000-0002-9831-0861},
Unique-ID = {WOS:000684201900003},
}

@article{ WOS:000659204800055,
Author = {Chen, Lu and Chen, Yanping},
Title = {Effects of omalizumab in children with asthma A protocol for systematic
   review and meta-analysis},
Journal = {MEDICINE},
Year = {2021},
Volume = {100},
Number = {22},
Month = {JUN 4},
Abstract = {Background: It is still controversial in the current literature whether
   omalizumab is beneficial for children with asthma. Given that there is
   no high-quality meta-analysis to incorporate existing evidence, the
   purpose of this protocol is to design a systematic review and
   meta-analysis of the level I evidence to ascertain whether omalizumab is
   beneficial and safe for children with asthma. Methods: The systematic
   literature review is structured to adhere to Preferred Reporting Items
   for Systematic Reviews and Meta-analyses guidelines. The following
   search terms will be used in PUBMED, Scopus, EMBASE, and Cochrane
   Library databases on June, 2021, as the search algorithm: (omalizumab)
   AND (asthma) AND (children). The primary outcome is the long-term safety
   and tolerability of omalizumab. The other outcomes include asthma
   control, quality of life, use of asthma controller medications, and
   spirometry measurements and emergency room visits due to asthma, and
   serum trough concentrations of omalizumab, free and total immunoglobulin
   E measured. Review Manager software (v 5.3; Cochrane Collaboration) will
   be used for the meta-analysis. Results: The review will add to the
   existing literature by showing compelling evidence and improved guidance
   in clinic settings. Registration number: 10.17605/OSF.IO/G6N3P.},
DOI = {10.1097/MD.0000000000026155},
Article-Number = {e26155},
ISSN = {0025-7974},
EISSN = {1536-5964},
Unique-ID = {WOS:000659204800055},
}

@article{ WOS:000672366900002,
Author = {Ferroudji, Fateh and Khelifi, Cherif},
Title = {Structural Strength Analysis and Fabrication of a Straight Blade of an
   H-Darrieus Wind Turbine},
Journal = {JOURNAL OF APPLIED AND COMPUTATIONAL MECHANICS},
Year = {2021},
Volume = {7},
Number = {3},
Pages = {1276-1282},
Month = {SUM},
Abstract = {Small H-Darrieus wind turbines have become popular in the wind power
   market because of their many advantages, which include simplicity of
   design, low construction costs, and they are thought to represent an
   adequate solution even in unconventional installation regions. The blade
   is generally considered as the most important component of the wind
   turbine system because it controls the efficiency of the turbine. The
   blade structure must be designed to support the difficult environmental
   conditions (e.g., wind, and snow) encountered during the operational
   life of the wind turbine. This current study uses three-dimensional (3D)
   modeling and structural strength analysis to fabricate two straight
   blades (aluminum and galvanized steel) for a small H-Darrieus wind
   turbine. The 3D modeling of the blade structure is performed using
   SolidWorks, a computer-aided design (CAD) software package, and the
   structural strength analysis uses the Finite Element Analysis (FEA)
   technique to identify the stiffness, resistance, and reliability of the
   blade structure. The simulation results obtained indicate that no
   structural failures are predicted for either of the two structures
   tested because the factors of safety are larger than one, and the all
   maximum deflections are within the allowable deformation limits for the
   materials. Manufacturing processes for the two structures are described.},
DOI = {10.22055/JACM.2020.31452.1876},
ISSN = {2383-4536},
Unique-ID = {WOS:000672366900002},
}

@article{ WOS:000659550200036,
Author = {Jung, Seunghwa and Choi, Jihwan P.},
Title = {Reliability of Small Satellite Networks With Software-Defined Radio and
   Enhanced Multiple Access Protocol},
Journal = {IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS},
Year = {2021},
Volume = {57},
Number = {3},
Pages = {1891-1902},
Month = {JUN},
Abstract = {Space missions exploiting small satellite networks with a use of
   software-defined radio (SDR) and advanced random access (RA) protocols
   have attracted an increased amount of attentions given their low costs,
   low latency levels, low complexity, and yet competitive data rates for
   global network services. In this article, we derive a mathematical model
   to demonstrate the reliability of a small satellite network with respect
   to SDR structures, the transmitted signal power on the uplink/downlink
   channels, code rates, and packet collisions through an enhanced RA
   protocol. Our model provides quantitative network reliability with
   respect to SDR system failure rates, feasible communication parameters,
   and packet loss ratios. Our analysis suggests a methodology to evaluate
   network reliability differences according to changes of communication
   parameters, and a guideline to sustain a reliable network system with
   appropriate parameter values. We find out that a robust SDR structure
   with a state-of-the-art analog-to-digital converter can provide reliable
   network services effectively with reduced power consumption, even with
   high packet traffic loads, to meet operator-required reliability levels
   for small satellite networks.},
DOI = {10.1109/TAES.2021.3050652},
ISSN = {0018-9251},
EISSN = {1557-9603},
Unique-ID = {WOS:000659550200036},
}

@article{ WOS:000658823600003,
Author = {Garces, Lina and Martinez-Fernandez, Silverio and Oliveira, Lucas and
   Valle, Pedro and Ayala, Claudia and Franch, Xavier and Nakagawa, Elisa
   Yumi},
Title = {Three decades of software reference architectures: A systematic mapping
   study},
Journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
Year = {2021},
Volume = {179},
Month = {SEP},
Abstract = {Software reference architectures have played an essential role in
   software systems development due to the possibility of knowledge reuse.
   Although increasingly adopted by industry, these architectures are not
   yet completely understood. This work presents a panorama on existing
   software reference architectures, characterizing them according to their
   context, goals, perspectives, application domains, design approaches,
   and maturity, as well as the industry involvement for their
   construction. For this, we planned and conducted a systematic mapping
   study. During last decade, the number of reference architectures in very
   diverse application domains has increased, resulting from efforts of
   industry, academia, and through their collaborations. Academic reference
   architectures are oriented to facilitate the reuse of architectural and
   domain knowledge. The industry has focused on architectures for
   standardization with certain maturity level. However, the great amount
   of architectures studied in this work have been designed without
   following a systematic process, and they lack the maturity to be used in
   real software projects. Further investigations can be oriented to
   gathering empirical evidences, from different sources than academic data
   libraries, that allow to understand how references architectures have
   been constructed, utilized, and maintained during the whole software
   life-cycle. (C) 2021 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.jss.2021.111004},
EarlyAccessDate = {MAY 2021},
Article-Number = {111004},
ISSN = {0164-1212},
EISSN = {1873-1228},
ResearcherID-Numbers = {Martínez-Fernández, Silverio/P-5615-2014
   Oliveira, Lucas/AHE-6766-2022
   Garces, Lina/Y-5730-2018},
ORCID-Numbers = {Martínez-Fernández, Silverio/0000-0001-9928-133X
   Dias Valle, Pedro Henrique/0000-0002-6929-7557
   Garces, Lina/0000-0002-4990-6562},
Unique-ID = {WOS:000658823600003},
}

@article{ WOS:000658877300001,
Author = {Kiadehi, Katayoun Bakhshi and Rahmani, Amir Masoud and Molahosseini,
   Amir Sabbagh},
Title = {Increasing fault tolerance of data plane on the internet of things using
   the software-defined networks},
Journal = {PEERJ COMPUTER SCIENCE},
Year = {2021},
Month = {MAY 27},
Abstract = {Considering the Internet of Things (IoT) impact in today's world,
   uninterrupted service is essential, and recovery has received more
   attention than ever before. Fault-tolerance (FT) is an essential aspect
   of network resilience. Fault-tolerance mechanisms are required to ensure
   high availability and high reliability in systems. The advent of
   software-defined networking (SDN) in the IoT plays a significant role in
   providing a reliable communication platform. This paper proposes a data
   plane fault-tolerant architecture using the concepts of software-defined
   networks for IoT environments. In this work, a mathematical model called
   Shared Risk Link Group (SRLG) calculates redundant paths as the primary
   and backup nonoverlapping paths between network equipment. In addition
   to the fault tolerance, service quality was considered in the proposed
   schemes. Putting the percentage of link bandwidth usage and the rate of
   link delay in calculating link costs makes it possible to calculate two
   completely non-overlapping paths with the best condition. We compare our
   two proposed dynamic schemes with the hybrid disjoint paths (Hybrid\_DP)
   method and our previous work. IoT developments, wireless and wired
   equipment are now used in many industrial and commercial applications,
   so the proposed hybrid dynamic method supports both wired and wireless
   devices; furthermore multiple link failures will be supported in the two
   proposed dynamic schemes. Simulation results indicate that, while
   reducing the error recovery time, the two proposed dynamic designs lead
   to improved service quality parameters such as packet loss and delay
   compared to the Hybrid\_DP method. The results show that in case of a
   link failure in the network, the proposed hybrid dynamic scheme's
   recovery time is approximately 12 ms. Furthermore, in the proposed
   hybrid dynamic scheme, on average, the recovery time, the packet loss,
   and the delay improved by 22.39\%, 8.2\%, 5.66\%, compared to the
   Hybrid\_DP method, respectively.},
DOI = {10.7717/peerj-cs.543},
Article-Number = {e543},
EISSN = {2376-5992},
ResearcherID-Numbers = {Rahmani, Amir Masoud/K-2702-2013},
ORCID-Numbers = {Rahmani, Amir Masoud/0000-0001-8641-6119},
Unique-ID = {WOS:000658877300001},
}

@article{ WOS:000636259600003,
Author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden,
   V},
Title = {Learning from reproducing computational results: introducing three
   principles and the Reproduction Package},
Journal = {PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL
   AND ENGINEERING SCIENCES},
Year = {2021},
Volume = {379},
Number = {2197},
Month = {MAY 17},
Abstract = {We carry out efforts to reproduce computational results for seven
   published articles and identify barriers to computational
   reproducibility. We then derive three principles to guide the practice
   and dissemination of reproducible computational research: (i) Provide
   transparency regarding how computational results are produced; (ii) When
   writing and releasing research software, aim for ease of
   (re-)executability; (iii) Make any code upon which the results rely as
   deterministic as possible. We then exemplify these three principles with
   12 specific guidelines for their implementation in practice. We
   illustrate the three principles of reproducible research with a series
   of vignettes from our experimental reproducibility work. We define a
   novel Reproduction Package, a formalism that specifies a structured way
   to share computational research artifacts that implements the guidelines
   generated from our reproduction efforts to allow others to build,
   reproduce and extend computational science. We make our reproduction
   efforts in this paper publicly available as exemplar Reproduction
   Packages. This article is part of the theme issue `Reliability and
   reproducibility in computational science: implementing verification,
   validation and uncertainty quantification in silico'.},
DOI = {10.1098/rsta.2020.0069},
Article-Number = {20200069},
ISSN = {1364-503X},
EISSN = {1471-2962},
Unique-ID = {WOS:000636259600003},
}

@article{ WOS:000656128100001,
Author = {Razavi, Foroogh and Raminfard, Samira and Kalantar Hormozi, Hadis and
   Sisakhti, Minoo and Batouli, Seyed Amir Hossein},
Title = {A Probabilistic Atlas of the Pineal Gland in the Standard Space},
Journal = {FRONTIERS IN NEUROINFORMATICS},
Year = {2021},
Volume = {15},
Month = {MAY 17},
Abstract = {Pineal gland (PG) is a structure located in the midline of the brain,
   and is considered as a main part of the epithalamus. There are numerous
   reports on the facilitatory role of this area for brain function;
   hormone secretion and its role in sleep cycle are the major reports.
   However, reports are rarely available on the direct role of this
   structure in brain cognition and in information processing. A suggestion
   for the limited number of such studies is the lack of a standard atlas
   for the PG; none of the available MRI templates and atlases has provided
   parcellations for this structure. In this study, we used the
   three-dimensional (3D) T1-weighted MRI data of 152 healthy young
   volunteers, and provided a probabilistic map of the PG in the standard
   Montreal Neurologic Institute (MNI) space. The methods included
   collecting the data using a 64-channel head coil on a 3-Tesla Prisma MRI
   Scanner, manual delineation of the PG by two experts, and robust
   template and atlas construction algorithms. This atlas is freely
   accessible, and we hope importing this atlas in the well-known
   neuroimaging software packages would help to identify other probable
   roles of the PG in brain function. It could also be used to study pineal
   cysts, for volumetric analyses, and to test any associations between the
   cognitive abilities of the human and the structure of the PG.},
DOI = {10.3389/fninf.2021.554229},
Article-Number = {554229},
EISSN = {1662-5196},
ORCID-Numbers = {Batouli, Seyed Amir Hossein/0000-0002-9157-4522},
Unique-ID = {WOS:000656128100001},
}

@article{ WOS:000663768100005,
Author = {Xin, Yufeng and Zhang, Dongliang and Qiu, Guopeng},
Title = {Real-Time Animation Complexity of Interactive Clothing Design Based on
   Computer Simulation},
Journal = {COMPLEXITY},
Year = {2021},
Volume = {2021},
Month = {MAY 11},
Abstract = {With the innovation of computer, virtual clothing has also emerged. This
   research mainly discusses the real-time animation complex of interactive
   clothing design based on computer simulation. In the process of
   realizing virtual clothing, the sample interpolation synthesis method is
   used, and the human body sample library is constructed using the above
   two methods (primitive construction method and model reconstruction
   method) first, and then, the human body model is obtained by
   interpolation calculation according to the personalized parameters.
   Building a clothing model is particularly important for the effect of
   trying on. The clothing that needs to be displayed can be scanned and
   then input into the computer to build the model. The model can be
   directly built in 3DMAX and other software and then its surface texture
   can be mapped, or the clothing model can be directly built. The 3D model
   in the 3ds file is loaded by the loop body nested switch branch
   selection structure. Correspondingly, the write-back operation of 3ds
   files is similar. Just follow the general structure of the 3ds file and
   write the root block, version information block, edit information block,
   key frame information block, etc. to a brand new file in sequence. The
   main reason for this article to perform the 3ds file write-back
   operation is that, after the clothing model is dynamically simulated
   through the dynamic principle, the deformed key animation frame needs to
   be saved as a 3ds file so that it can be further imported into the
   3DSMAX software and generated by the renderer, form high-quality picture
   information, and finally get high-definition animation video. In the
   CPU-GPU hybrid method, modules such as force calculation, collision
   processing, and position update use the GPU method, while overstretching
   is processed by the CPU method, making the overall performance 10 times
   higher than the pure CPU method. This research helps to promote the
   development of 3D virtual clothing design.},
DOI = {10.1155/2021/9988623},
ISSN = {1076-2787},
EISSN = {1099-0526},
ORCID-Numbers = {Xin, Yufeng/0000-0002-6625-9598},
Unique-ID = {WOS:000663768100005},
}

@article{ WOS:000648830800003,
Author = {Geary, Cody and Grossi, Guido and McRae, Ewan K. S. and Rothemund, Paul
   W. K. and Andersen, Ebbe S.},
Title = {RNA origami design tools enable cotranscriptional folding of
   kilobase-sized nanoscaffolds},
Journal = {NATURE CHEMISTRY},
Year = {2021},
Volume = {13},
Number = {6},
Pages = {549+},
Month = {JUN},
Abstract = {RNA origami can be used for the modular design of RNA nanoscaffolds but
   can be challenging to design. Newly developed computer-aided design
   software has now been shown to improve the folding yield of
   kilobase-sized RNA origami. These structures fold from a single strand
   during transcription by an RNA polymerase, and are able to position
   small molecules and protein components with nanoscale precision.
   RNA origami is a framework for the modular design of nanoscaffolds that
   can be folded from a single strand of RNA and used to organize molecular
   components with nanoscale precision. The design of genetically
   expressible RNA origami, which must fold cotranscriptionally, requires
   modelling and design tools that simultaneously consider thermodynamics,
   the folding pathway, sequence constraints and pseudoknot optimization.
   Here, we describe RNA Origami Automated Design software (ROAD), which
   builds origami models from a library of structural modules, identifies
   potential folding barriers and designs optimized sequences. Using ROAD,
   we extend the scale and functional diversity of RNA scaffolds, creating
   32 designs of up to 2,360 nucleotides, five that scaffold two proteins,
   and seven that scaffold two small molecules at precise distances.
   Micrographic and chromatographic comparisons of optimized and
   non-optimized structures validate that our principles for strand routing
   and sequence design substantially improve yield. By providing efficient
   design of RNA origami, ROAD may simplify the construction of custom RNA
   scaffolds for nanomedicine and synthetic biology.},
DOI = {10.1038/s41557-021-00679-1},
EarlyAccessDate = {MAY 2021},
ISSN = {1755-4330},
EISSN = {1755-4349},
ResearcherID-Numbers = {Andersen, Ebbe S/O-3936-2014
   },
ORCID-Numbers = {Andersen, Ebbe S/0000-0002-6236-8164
   Grossi, Guido/0000-0003-1936-8676
   McRae, Ewan/0000-0001-9105-2575
   Geary, Cody/0000-0003-2083-4259
   Rothemund, Paul/0000-0002-1653-3202},
Unique-ID = {WOS:000648830800003},
}

@article{ WOS:000730511500038,
Author = {Ghariq, M. and Van Bodegom-Vos, L. and Brignole, M. and Peeters, S. Y.
   G. and de Groot, B. and Kaal, E. C. A. and Hemels, M. E. W. and de
   Lange, F. J. and van Dijk, J. G. and Thijs, R. D. and SYNERGY Consortium},
Title = {Factors facilitating and hindering the implementation of the European
   Society of Cardiology Syncope Guidelines at the Emergency Department: A
   nationwide qualitative study},
Journal = {INTERNATIONAL JOURNAL OF CARDIOLOGY},
Year = {2021},
Volume = {333},
Pages = {167-173},
Month = {JUN 15},
Abstract = {Aims: Syncope care is often fragmented and inefficient. Structuring
   syncope care through implementation of guidelines and Syncope Units has
   been shown to improve diagnostic yield, reduce costs and improve quality
   of life. We implemented the European Society of Cardiology (ESC) 2018
   syncope guidelines at the Emergency Departments (ED) and established
   Syncope Units in five Dutch hospitals. We evaluated the implementation
   process by identifying factors that hinder ('barriers') and facilitate
   ('facilitators') the implementation.
   Methods and results: We conducted, recorded and transcribed
   semi-structured interviews with 19 specialists and residents involved in
   syncope care from neurology, cardiology, internal medicine and emergency
   medicine. Two researchers independently classified the reported barriers
   and facilitators, according to the framework of qualitative research
   (Flottorp), which distinguished several separate fields ('levels').
   Software package Atlas.ti was used for analysis.
   We identified 31 barriers and 22 facilitators. Most barriers occurred on
   the level of the individual health care professional (e.g. inexperienced
   residents having to work with the guideline at the ED) and the
   organizational context (e.g. specialists not relinquishing preceding
   procedures). Participants reported most facilitators at the level of
   innovation (e.g. structured work -flow at the ED). The multidisciplinary
   Syncope Unit was welcomed as useful solution to a perceived need in
   clinical practice.
   Conclusion: Implementing ESC syncope guidelines at the ED and
   establishing Syncope Units facilitated a structured multidisciplinary
   work-up for syncope patients. Most identified barriers related to the
   individual health care professional and the organizational context.
   Future implementation of the multidisciplinary guideline should be
   tailored to address these barriers.
   (c) 2021 The Authors. Published by Elsevier B.V. This is an open access
   article under the CC BY license (http://
   creativecommons.org/licenses/by/4.0/).},
DOI = {10.1016/j.ijcard.2021.02.067},
EarlyAccessDate = {MAY 2021},
ISSN = {0167-5273},
EISSN = {1874-1754},
ResearcherID-Numbers = {Thijs, Roland D/AGE-5180-2022},
ORCID-Numbers = {Thijs, Roland D/0000-0003-1435-8970},
Unique-ID = {WOS:000730511500038},
}

@article{ WOS:000644362400003,
Author = {Gurevich, Leonid and Danenko, Vladimir and Bogdanov, Artem and Kulevich,
   Vitaliy},
Title = {Analysis of the stress-strain state of steel closed ropes under tension
   and torsion},
Journal = {INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY},
Year = {2022},
Volume = {118},
Number = {1-2},
Pages = {15-22},
Month = {JAN},
Abstract = {The analysis of the stress-strain state of the closed rope elements
   under axial tension and torsion was carried out by the finite element
   method using the licensed software package SIMULIA/Abaqus. The closed
   rope consists of an outer layer of Z-profile wires, a subsurface layer
   of alternating round and H-profile wires, an intermediate layer, and a
   core of the structure 1 + 7 + 7/7 + 14 of round wires. Modeling made it
   possible to determine the values of the axial force P, the torque M, the
   relative elongation epsilon, and the relative angle of torsion theta of
   the rope sample at the given values of the axial displacement and the
   rotation angle the cross-section. The results of the analytical
   calculation of the stress-strain state of a spiral rope using
   traditional approaches were compared with those obtained in the course
   of computer finite element modeling. The results of modeling the rope
   deformation under tension were verified by experimental data obtained by
   stretching the rope sample on a universal horizontal hydraulic test
   machine LabTest 6.2000N.7. The results of computer modeling of the rope
   pure tension correlate well with the results of the calculation by the
   method of M.F. Glushko, which more accurately takes into account the
   real construction of the ropes. Computer simulation of the stress-strain
   state of the closed rope elements made it possible to determine the
   contact stresses between shaped wires in layers and between layers at
   different values of the gap between shaped wires; therefore, it can be
   used to optimize the gaps. Computer simulation of the stress-strain
   state of the rope elements of a closed structure makes it possible to
   assess the consistency of the layers narrowing during axial tension by
   analyzing the contact stresses between adjacent wires in the
   cross-section of the rope.},
DOI = {10.1007/s00170-021-07128-w},
EarlyAccessDate = {APR 2021},
ISSN = {0268-3768},
EISSN = {1433-3015},
ResearcherID-Numbers = {KULEVICH, VITALIY/G-2215-2019
   Гуревич, Леонид/AAR-9719-2021
   Bogdanov, Artem/AAA-7995-2022
   Гуревич, Леонид/AAH-9622-2021},
ORCID-Numbers = {KULEVICH, VITALIY/0000-0001-6041-8423
   Bogdanov, Artem/0000-0003-3918-775X
   Гуревич, Леонид/0000-0003-3805-6977},
Unique-ID = {WOS:000644362400003},
}

@article{ WOS:000658956900029,
Author = {Shen, Chuanwen and Tan, Shuying and Yang, Jun},
Title = {Effects of continuous use of metformin on cardiovascular outcomes in
   patients with type 2 diabetes after acute myocardial infarction A
   protocol for systematic review and meta-analysis},
Journal = {MEDICINE},
Year = {2021},
Volume = {100},
Number = {15},
Month = {APR 16},
Abstract = {Background: To our knowledge, no meta-analyses or reviews have
   investigated the efficacy and safety of metformin on cardiovascular
   outcomes after acute myocardial infarction (AMI) in patients with type 2
   diabetes mellitus (T2DM). We thus conduct a high-quality systematic
   review and meta-analysis to assess the efficacy and safety of metformin
   on cardiovascular outcomes after AMI in patients with T2DM. Methods: In
   this systematic review and meta-analysis, we will search PUBMED, Scopus,
   EMBASE, and Cochrane Library databases through April, 2021. The study is
   structured to adhere to PRISMA guidelines (i.e., Preferred Reporting
   Items for Systematic Reviews and Meta-analyses). The literature search,
   data extraction, and quality assessments are conducted independently by
   2 authors. Outcome measures include all-cause mortality; complications
   such as acute kidney injury, lactic acidosis, hospitalization for AMI or
   stroke, or death. Where disagreement in the collection of data occurs,
   this is resolved through discussion. Review Manager Software (v 5.3;
   Cochrane Collaboration) is used for the meta-analysis. Two independent
   reviewers will assess the risk of bias of the included studies at study
   level. Results: It is hypothesized that metformin use at the post-AMI is
   associated with decreased risk of cardiovascular disease and death in
   patients with T2DM. Conclusions: This study expects to provide credible
   and scientific evidence for the efficacy and safety of metformin on
   cardiovascular outcomes after AMI in patients with T2DM. Registration
   number: 10.17605/OSF.IO/S3MBP.},
DOI = {10.1097/MD.0000000000025353},
Article-Number = {e25353},
ISSN = {0025-7974},
EISSN = {1536-5964},
Unique-ID = {WOS:000658956900029},
}

@article{ WOS:000644615200001,
Author = {Glaser, Martin and Deb, Sourav and Seier, Florian and Agrawal, Amay and
   Liedl, Tim and Douglas, Shawn and Gupta, Manish K. and Smith, David M.},
Title = {The Art of Designing DNA Nanostructures with CAD Software},
Journal = {MOLECULES},
Year = {2021},
Volume = {26},
Number = {8},
Month = {APR},
Abstract = {Since the arrival of DNA nanotechnology nearly 40 years ago, the field
   has progressed from its beginnings of envisioning rather simple DNA
   structures having a branched, multi-strand architecture into creating
   beautifully complex structures comprising hundreds or even thousands of
   unique strands, with the possibility to exactly control the positions
   down to the molecular level. While the earliest construction
   methodologies, such as simple Holliday junctions or tiles, could
   reasonably be designed on pen and paper in a short amount of time, the
   advent of complex techniques, such as DNA origami or DNA bricks, require
   software to reduce the time required and propensity for human error
   within the design process. Where available, readily accessible design
   software catalyzes our ability to bring techniques to researchers in
   diverse fields and it has helped to speed the penetration of methods,
   such as DNA origami, into a wide range of applications from biomedicine
   to photonics. Here, we review the historical and current state of CAD
   software to enable a variety of methods that are fundamental to using
   structural DNA technology. Beginning with the first tools for predicting
   sequence-based secondary structure of nucleotides, we trace the
   development and significance of different software packages to the
   current state-of-the-art, with a particular focus on programs that are
   open source.},
DOI = {10.3390/molecules26082287},
Article-Number = {2287},
EISSN = {1420-3049},
ResearcherID-Numbers = {Douglas, Shawn/GVU-2004-2022
   Deb, Sourav/GQV-8013-2022
   Liedl, Tim/O-7539-2014
   Gupta, Manish/F-6645-2013
   },
ORCID-Numbers = {Douglas, Shawn/0000-0001-5398-9041
   Deb, Sourav/0000-0001-7173-6339
   Liedl, Tim/0000-0002-0040-0173
   Gupta, Manish/0000-0001-7067-8085
   Smith, David/0000-0002-8344-850X},
Unique-ID = {WOS:000644615200001},
}

@article{ WOS:000687751400003,
Author = {Boguslawski, Katharina and Leszczyk, Aleksandra and Nowak, Artur and
   Brzek, Filip and Zuchowski, Piotr Szymon and Kedziera, Dariusz and
   Tecmer, Pawel},
Title = {Pythonic Black-box Electronic Structure Tool (PyBEST). An open-source
   Python platform for electronic structure calculations at the interface
   between chemistry and physics},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2021},
Volume = {264},
Month = {JUL},
Abstract = {Pythonic Black-box Electronic Structure Tool (PyBEST) represents a
   fully-fledged modern electronic structure software package developed at
   Nicolaus Copernicus University in Torun. The package provides an
   efficient and reliable platform for electronic structure calculations at
   the interface between chemistry and physics using unique electronic
   structure methods, analysis tools, and visualization. Examples are the
   (orbital-optimized) pCCD-based models for ground- and excited-states
   electronic structure calculations as well as the quantum entanglement
   analysis framework based on the single-orbital entropy and orbital-pair
   mutual information. PyBEST is written primarily in the Python
   programming language with additional parts written in C++, which are
   interfaced using Pybind11, a lightweight header-only library. By
   construction, PyBEST is easy to use, to code, and to interface with
   other software packages. Moreover, its modularity allows us to
   conveniently host additional Python packages and software libraries in
   future releases to enhance its performance. The electronic structure
   methods available in PyBEST are tested for the half-filled 1-D model
   Hamiltonian. The capability of PyBEST to perform large-scale electronic
   structure calculations is demonstrated for the model vitamin B-12
   compound. The investigated molecule is composed of 190 electrons and 777
   orbitals for which an orbital optimization within pCCD and an orbital
   entanglement and correlation analysis are performed for the first time.
   Program summary
   Program title: PyBEST
   CPC Library link to program files: https://doi.org/10.17632/xf9kb7yfwr.1
   Developer's repository link:
   https://zenodo.org/record/3925278\#.X5KAZS8Rq6s
   Licensing provisions: GNU General Public License 3
   Programming language: Python, C++
   Nature of problem: Efficient and reliable modeling of electronic
   structures featuring both weakly- and strongly-correlated electrons.
   Small- and large-scale quantum-mechanical problems at the interface
   between chemistry and physics comprising both quantum chemical and model
   Hamiltonians. Specifically, modeling potential energy surfaces of
   complex electronic structures including bond breaking/formation,
   elucidating complex electronic structures through the picture of
   interacting orbitals, describing noncovalent interactions, ultra-cold
   trapped quantum gases, and a variety of applications in
   interdisciplinary quantum mechanical-based problems.
   Solution method: Modular implementation of a series of unconventional
   (and conventional) electronic structure models based on the pCCD ansatz
   to solve the electronic Schrodinger equation. These include the
   description of both ground- and excited-states, the determination of
   interaction energies, and the analysis and interpretation of electronic
   wavefunctions. All modules are implemented in the modern Python
   programming language, where bottleneck operations are handled by C++
   code interfaced by the Pybind11 header-only library. The implemented
   (wavefunction) modules and modular code structure make PyBEST a very
   efficient alternative to existing electronic structure packages.
   Additional comments including restrictions and unusual features: PyBEST
   features unconventional electronic structure methods (pCCD and post-pCCD
   methods) that are not available in any other quantum chemistry/physics
   software package. It also includes a general orbital entanglement and
   correlation module that supports both pCCD and selected post-pCCD
   methods. PyBEST is designed to be easy to use and code in. Due to its
   modularity (for instance of the tensor contraction engine), new Python
   modules and features can be straightforwardly imported and exploited
   without changing any wavefunction modules directly.
   References: http://pybest.fizyka.umk.pl (c) 2021 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.cpc.2021.107933},
EarlyAccessDate = {MAR 2021},
Article-Number = {107933},
ISSN = {0010-4655},
EISSN = {1879-2944},
ResearcherID-Numbers = {Zuchowski, Piotr/AAW-7213-2021
   Kedziera, Dariusz/GLN-6646-2022
   Tecmer, Pawel/B-4366-2015
   Brzek, Filip/D-2885-2019
   Leszczyk, Aleksandra/X-9959-2018
   Boguslawski, Katharina/B-4491-2015},
ORCID-Numbers = {Zuchowski, Piotr/0000-0001-9267-3090
   Kedziera, Dariusz/0000-0002-9100-5370
   Tecmer, Pawel/0000-0001-6347-878X
   Brzek, Filip/0000-0002-8292-0761
   Leszczyk, Aleksandra/0000-0001-9765-1587
   Boguslawski, Katharina/0000-0001-7793-1151},
Unique-ID = {WOS:000687751400003},
}

@article{ WOS:000628116200001,
Author = {Bazaluk, Oleg and Sai, Kateryna and Lozynskyi, Vasyl and Petlovanyi,
   Mykhailo and Saik, Pavlo},
Title = {Research into Dissociation Zones of Gas Hydrate Deposits with a
   Heterogeneous Structure in the Black Sea},
Journal = {ENERGIES},
Year = {2021},
Volume = {14},
Number = {5},
Month = {MAR},
Abstract = {Ukraine is an energy-dependent country, with less that 50\% of its
   energy consumption fulfilled by its own resources. Natural gas is of
   paramount importance, especially for industry and society. Therefore,
   there is an urgent need to search for alternative and potential energy
   sources, such as gas hydrate deposits in the Black Sea, which can reduce
   the consumption of imported gas. It is necessary to refine the process
   parameters of the dissociation of gas hydrate deposits with a
   heterogeneous structure. The analyzed known geological-geophysical data
   devoted to the study of the offshore area and the seabed give grounds to
   assert the existence of a significant amount of hydrate deposits in the
   Black Sea. An integrated methodological approach is applied, which
   consists of the development of algorithms for analytical and laboratory
   studies of gas volumes obtained during the dissociation of deposits with
   a heterogeneous structure. These data are used for the computer
   modelling of the dissociation zone in the Surfer-8.0 software package
   based on the data interpolation method, which uses three methods for
   calculating the volumes of modelling bodies. A 3D grid-visualization of
   the studied part of the gas hydrate deposit has been developed. The
   dissociation zone parameters of gas hydrate deposits with different
   shares of rock intercalation, that is, the minimum and maximum
   diameters, have been determined, and the potentially recoverable gas
   volumes have been assessed. The effective time of the process of gas
   hydrate deposit dissociation has been substantiated. The obtained
   research results of the dissociation process of gas hydrate deposits can
   be used in the development of new technological schemes for gas recovery
   from the deep-water Black Sea area.},
DOI = {10.3390/en14051345},
Article-Number = {1345},
EISSN = {1996-1073},
ResearcherID-Numbers = {Lozynskyi, Vasyl/C-1812-2015
   Sai, Kateryna/O-4905-2018
   Petlovanyi, Mykhailo/AAJ-9599-2021
   Petlovanyi, Mykhailo/I-2375-2018
   Bazaluk, Oleg/F-7904-2017},
ORCID-Numbers = {Lozynskyi, Vasyl/0000-0002-9657-0635
   Sai, Kateryna/0000-0003-1488-3230
   Petlovanyi, Mykhailo/0000-0002-8911-4973
   Petlovanyi, Mykhailo/0000-0002-8911-4973
   Bazaluk, Oleg/0000-0002-1623-419X},
Unique-ID = {WOS:000628116200001},
}

@article{ WOS:000636371200001,
Author = {Kebede, Bekalu and Chelkeba, Legese and Dessie, Bekalu},
Title = {Rate of blood pressure control and its determinants among adult
   hypertensive patients at Jimma University Medical Center, Ethiopia:
   Prospective cohort study},
Journal = {SAGE OPEN MEDICINE},
Year = {2021},
Volume = {9},
Month = {MAR},
Abstract = {Introduction:
   Despite the fact that the goals for the management of hypertension are
   well-defined and effective therapies are available, control of
   hypertension remains poor in countries with low resources including
   Ethiopia. This study aimed to determine blood pressure control rate and
   its determinants among ambulatory adult hypertensive patients at Jimma
   University Medical Center.
   Methods:
   A general prospective cohort study was conducted among adult
   hypertensive patients who had regular follow-up at Jimma University
   Cardiac Clinic from 20 March to 20 June 2018. Hypertensive patients who
   fulfilled the inclusion criteria were selected in the first month of the
   data collection period. Then, only those patients who visited the clinic
   at the first month were consequently followed-up for the next 3 months.
   The Eighth Joint National Committee guideline was used to categorize
   controlled and uncontrolled blood pressures. Patients' specific data
   were collected using a structured data collection tool. Data were
   analyzed using the statistical software package SPSS version 21.0.
   Bivariate and multivariable logistic regression analysis was used to
   identify independent variables influencing blood pressure control.
   p-values of less than 0.05 were considered statistically significant.
   Results:
   From a total of 416 patients, 237 (57.0\%) were male with a mean age of
   56.50 +/- 11.96 years. Two hundred and fifty eight (62.0\%) participants
   had comorbid conditions and 275 (66.1\%) were on combination therapy.
   The rate of blood pressure control was 42.8\%. Age > 60 years was
   negatively associated with uncontrolled blood pressure (adjusted odd
   ratio = 0.52, confidence interval = 0.31-0.88, p = 0.015). Medication
   non-adherence (adjusted odd ratio = 1.64, confidence interval =
   1.04-2.58, p = 0.034) and non-adherence to international guidelines
   (adjusted odd ratio = 2.33, confidence interval = 1.49-3.64, p < 0001)
   were positively associated with uncontrolled blood pressure.
   Conclusion:
   The rate of blood pressure control among hypertensive patients was
   suboptimal. Age, clinicians' non-adherence to international guidelines,
   and patients' non-adherence to medications were independent predictors
   of blood pressure control. Physicians and clinical pharmacists should
   adhere to guidelines for better treatment and care of hypertensive
   patients.},
DOI = {10.1177/20503121211006000},
Article-Number = {20503121211006000},
ISSN = {2050-3121},
ResearcherID-Numbers = {Simegn, Bekalu Kebede/AAJ-4586-2021
   Alamirew, Bekalu/AAV-2894-2020
   },
ORCID-Numbers = {Alamirew, Bekalu/0000-0002-0201-6829
   Kumsa, Legese/0000-0002-0323-5011},
Unique-ID = {WOS:000636371200001},
}

@article{ WOS:000649028000004,
Author = {Shitikov, V. K. and Zinchenko, T. D. and Golovatyuk, V, L.},
Title = {Models of joint distribution of species with benthic communities from
   the small rivers of the Volga basin as a case study},
Journal = {ZHURNAL OBSHCHEI BIOLOGII},
Year = {2021},
Volume = {82},
Number = {2},
Pages = {143-154},
Month = {MAR-APR},
Abstract = {Considered are the theoretical and practical aspects of building joint
   species distribution models, which are a modern tool for the analysis of
   ecological communities. It is shown that in the case when observational
   data are quantitative indicators of population density (in particular,
   the number of species in hydrobiological studies), it is inappropriate
   to use the MaxEnt method and others based on the concept of
   ``pseudo-absence{''} points. Contemporary multidimensional models of the
   joint distribution of communities should include a set of parameters
   that assess the impact of the following groups of fixed and random
   factors on the species occurrence: (a) covariates and categorical
   variables describing environmental conditions and characteristics of
   biotopes, (b) main indicators characterizing each species, and
   phylogenetic structure of communities, (c) functions of spatial
   autocorrelation of data at observation points, (d) residual (i.e., not
   caused by external factors) associativity of species. The analysis of
   published data and the practical examples of implementation showed that
   the mentioned requirements, in general, are satisfied by the
   methodological platform and Rpackage HMSC (Hierarchical Modelling of
   Species Communities), on the basis of which multidimensional
   hierarchical generalized linear models with mixed parameters, estimated
   by Bayesian procedure, are constructed. The main concepts and blocks of
   the HMSC platform are described and the results of models construction
   based on the authors' data long-term hydrobiological studies of benthic
   communities in 132 small and medium-sized rivers in the Middle and Lower
   Volga basin are discussed. The parameters of a set of one-dimensional
   candidate models for the abundance distribution of the subfamily
   Prodiamesinae (Diptera, Chironomidae) are analyzed and a forecast map of
   its range within the region is built. To illustrate the multidimensional
   case, a model of the joint spatial distribution of 31 species of
   chironomids is constructed and its coefficients are analyzed. A residual
   correlation graph of statistically significant interspecies interactions
   has been built. The conclusion is made that the HMSC method and software
   package can be effectively used to solve fundamental problems of
   communities ecology: how the areas of individual populations, the
   structure of their communities and the nature of interspecific
   interactions depend on environmental conditions, as well as to predict
   future trends of these processes in response to global changes.},
DOI = {10.31857/S0044459621020068},
ISSN = {0044-4596},
ResearcherID-Numbers = {РАН, СамНЦ/AAO-2584-2020},
Unique-ID = {WOS:000649028000004},
}

@article{ WOS:000633134700002,
Author = {Hughes, Daryl and Birkinshaw, Stephen and Parkin, Geoff},
Title = {A method to include reservoir operations in catchment hydrological
   models using SHETRAN},
Journal = {ENVIRONMENTAL MODELLING \& SOFTWARE},
Year = {2021},
Volume = {138},
Month = {APR},
Abstract = {Reservoir construction and operation have significant impacts on
   catchment hydrology, flood risk and fluvial processes. However, few
   available hydrological modelling packages can simulate complex, dynamic,
   manually operated reservoir control structures. We present
   SHETRAN-Reservoir, a physically-based spatially distributed modelling
   tool to simulate catchment hydrology, including reservoir operations. We
   also propose a method for deriving parsimonious reservoir operation
   rules from real-world observations. Application of SHETRANReservoir to
   the Upper Cocker catchment in the Lake District National Park, UK, is
   shown to improve modelling of hydrological response. Modelling combined
   climate change and water resource management scenarios demonstrates the
   influence of operational control rules on hydrological impacts,
   especially during droughts. We discuss how SHETRAN-Reservoir can be
   applied to other reservoir-containing catchments to guide decisions
   concerning water resources, ecology and flood risk. We also discuss
   potential future software developments.},
DOI = {10.1016/j.envsoft.2021.104980},
EarlyAccessDate = {FEB 2021},
Article-Number = {104980},
ISSN = {1364-8152},
EISSN = {1873-6726},
ORCID-Numbers = {Birkinshaw, Stephen/0000-0003-4989-7915
   Hughes, Daryl/0000-0002-7733-6499},
Unique-ID = {WOS:000633134700002},
}

@article{ WOS:000613974600001,
Author = {Sadi, Mahsa H. and Yu, Eric},
Title = {RAPID: a knowledge-based assistant for designing web APIs},
Journal = {REQUIREMENTS ENGINEERING},
Year = {2021},
Volume = {26},
Number = {2},
Pages = {185-236},
Month = {JUN},
Abstract = {With the rise in initiatives such as software ecosystems and Internet of
   Things (IoT), developing web Application Programming Interfaces (web
   APIs) has become an increasingly common practice. One main concern in
   developing web APIs is that they expose back-end systems and data toward
   clients. This exposure threatens critical non-functional requirements,
   such as the security of back-end systems, the performance of provided
   services, and the privacy of communications with clients. Although
   dealing with non-functional requirements during software design has been
   long studied, there is still no framework to specifically assist
   software developers in addressing these requirements in web APIs. In
   this paper, we introduce Rational API Designer (RAPID), an open-source
   assistant that advises on designing non-functional requirements in the
   architecture of web APIs. We have equipped RAPID with a broad range of
   expert knowledge about API design, systematically collected and
   extracted from the literature. The API design knowledge has been encoded
   as a set of 156 rules using the Non-Functional Requirements (NFR)
   multi-valued logic, a formal framework commonly used to describe
   non-functional and functional requirements of software systems. RAPID
   uses the encoded knowledge in a stepwise inference procedure to arrive
   from a given requirement, to a set of design alternatives to a final
   recommendation for a given API design specification. Seven
   well-experienced software engineers have blindly evaluated the accuracy
   of RAPID's consultations over seven different cases of web API design
   and on providing design guidelines for thirty design questions. The
   results of the evaluation show that RAPID's recommendations meet
   acceptable standards of the majority of the evaluators 73.3\% of the
   time. Moreover, analysis of the evaluators' comments suggests that more
   than one-third of the unacceptable ratings (33.8\%) given to RAPID's
   answers are due to valid but incomplete design guidelines. We thus
   expect that the accuracy of the consultations will increase as RAPID's
   knowledge of API design is extended and refined.},
DOI = {10.1007/s00766-020-00342-0},
EarlyAccessDate = {FEB 2021},
ISSN = {0947-3602},
EISSN = {1432-010X},
Unique-ID = {WOS:000613974600001},
}

@article{ WOS:000612473800002,
Author = {Khassaf, Saleh Issa and Chkheiwr, Aqeel Hatem and Jasim, Mohammed
   Abdulrahim},
Title = {EFFECT OF CREST SHAPE ON THE STRUCTURAL PERFORMANCE OF THE DOUBLE
   CURVATURE CONCRETE DAM},
Journal = {INTERNATIONAL JOURNAL OF GEOMATE},
Year = {2021},
Volume = {20},
Number = {78},
Pages = {9-19},
Month = {FEB},
Abstract = {Arch dam stability is one of the main engineering challenges that must
   be met in the early stages of investigations and design of the dam. The
   highest degree of stability can be achieved if the dam `s structure
   formed in best layouts and the free ends of the dam body, defined by the
   crest, restricts especially if the dam is affected seismically. This
   research study falls under the auspices of the second phase of concrete
   dam construction, i.e. design stage. Nonlinear finite element analysis
   is performed in order to investigate the structural behaviors of the
   four models of double curvature concrete arch dam. Three-dimensional
   models of arch dams are prepared to utilizing Revit, the building
   information modeling software. Subsequently, the four models are linked
   to the finite element's tools package of Abaqus 6.13 for the purposes of
   discretization and structural analyses. The models have the same
   mechanical properties and basic dimensions and differ only in the form
   and physical appearance of a crest (crest shape). Amplified of crest
   shape caused a slight movement of the dam center of the mass to be
   optimally positioned, and thus the new center of mass supports the
   stability of the dam. The varying effects of crest configurations on the
   whole dam structure response were discovered according to the results of
   maximum and minimum principal stresses along the crown cantilever path
   as well as to the displacements response along the crest path of the
   arch dam. Strengthening or amplified crest dimensions could decrease
   arch dam thickness for a certain value and can reduce the response of
   the displacement to about 30 percent. The results indicated a reduction
   in the undesired movement between the adjacent cantilevers at the
   contraction joint zone, so a marvelous response was remarked in both of
   triangle upward edges crest and triangle downward edges crest models.},
DOI = {10.21660/2021.78.14601},
ISSN = {2186-2982},
EISSN = {2186-2990},
Unique-ID = {WOS:000612473800002},
}

@article{ WOS:000599925000003,
Author = {Klymenko, M. V. and Vaitkus, J. A. and Smith, J. S. and Cole, J. H.},
Title = {NanoNET : An extendable Python framework for semi-empirical
   tight-binding models},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2021},
Volume = {259},
Month = {FEB},
Abstract = {We present a novel open-source Python framework called NanoNET
   (Nanoscale Non-equilibrium Electron Transport) for modeling electronic
   structure and transport. Our method is based on the tight-binding method
   and non-equilibrium Green's function theory. The core functionality of
   the framework is providing facilities for efficient construction of
   tight-binding Hamiltonian matrices from a list of atomic coordinates and
   a lookup table of the two-center integrals in dense, sparse, or
   blocktridiagonal forms. The framework implements a method based on
   kd-tree nearest-neighbor search and is applicable to isolated atomic
   clusters and periodic structures. A set of subroutines for detecting the
   block-tridiagonal structure of a Hamiltonian matrix and splitting it
   into series of diagonal and off-diagonal blocks is based on a new greedy
   algorithm with recursion. Additionally the developed software is
   equipped with a set of programs for computing complex band structure,
   self-energies of elastic scattering processes, and Green's functions.
   Examples of usage and capabilities of the computational framework are
   illustrated by computing the band structure and transport properties of
   a silicon nanowire as well as the band structure of bulk bismuth.
   Program summary
   Program Title: NanoNET
   CPC Library link to program files: https://doi.org/10.17632/b9p7kyzdj9.1
   Developer's repository link: https://github.com/freude/NanoNet
   Licensing provisions: MIT
   Programming language: Python
   Nature of problem: The framework NanoNET solves a problem which is,
   having a set of atomic coordinates and tight-binding parameters, to
   construct Hamiltonian matrices in one of several desired forms. In
   particular, some applications require those matrices to have a reduced
   bandwidth and/or to possess a block-tridiagonal structure.
   Solution method: The problem is solved using a combination of
   kd-tree-based fast nearest-neighbor search and atomic coordinate
   sorting. Furthermore, a new greedy recursive algorithm is proposed for
   detecting block-tridiagonal structure of a matrix in a non-optimal way.
   Additionally, we propose an algorithm of a polynomial time for
   optimizing block sizes.
   Additional features: Although the resulting matrices can be processed by
   many existing software packages, the framework also has built-in
   standard tools for diagonalizing Hamiltonian matrices and computing
   Green's functions that make it an independent tool for solving
   electronic structure and transport problems. (c) 2020 The Author(s).
   Published by Elsevier B.V. This is an open access article under the CC
   BY license (http://creativecommons.org/licenses/by/4.0/).},
DOI = {10.1016/j.cpc.2020.107676},
Article-Number = {107676},
ISSN = {0010-4655},
EISSN = {1879-2944},
ResearcherID-Numbers = {Cole, Jared H/G-2992-2010
   Smith, Jackson/I-1413-2013
   },
ORCID-Numbers = {Cole, Jared H/0000-0002-8943-6518
   Smith, Jackson/0000-0002-7769-7438
   Klymenko, Mykhailo/0000-0002-4641-8977},
Unique-ID = {WOS:000599925000003},
}

@article{ WOS:000624706200001,
Author = {Raza, Syed M. and Jeong, Jaeyeop and Kim, Moonseong and Kang, Byungseok
   and Choo, Hyunseung},
Title = {Empirical Performance and Energy Consumption Evaluation of Container
   Solutions on Resource Constrained IoT Gateways},
Journal = {SENSORS},
Year = {2021},
Volume = {21},
Number = {4},
Month = {FEB},
Abstract = {Containers virtually package a piece of software and share the host
   Operating System (OS) upon deployment. This makes them notably light
   weight and suitable for dynamic service deployment at the network edge
   and Internet of Things (IoT) devices for reduced latency and energy
   consumption. Data collection, computation, and now intelligence is
   included in variety of IoT devices which have very tight latency and
   energy consumption conditions. Recent studies satisfy latency condition
   through containerized services deployment on IoT devices and gateways.
   They fail to account for the limited energy and computing resources of
   these devices which limit the scalability and concurrent services
   deployment. This paper aims to establish guidelines and identify
   critical factors for containerized services deployment on resource
   constrained IoT devices. For this purpose, two container orchestration
   tools (i.e., Docker Swarm and Kubernetes) are tested and compared on a
   baseline IoT gateways testbed. Experiments use Deep Learning driven data
   analytics and Intrusion Detection System services, and evaluate the time
   it takes to prepare and deploy a container (creation time), Central
   Processing Unit (CPU) utilization for concurrent containers deployment,
   memory usage under different traffic loads, and energy consumption. The
   results indicate that container creation time and memory usage are
   decisive factors for containerized micro service architecture.},
DOI = {10.3390/s21041378},
Article-Number = {1378},
EISSN = {1424-8220},
ResearcherID-Numbers = {Raza, Syed Muhammad/AAJ-6376-2020},
ORCID-Numbers = {Raza, Syed Muhammad/0000-0001-6580-3232},
Unique-ID = {WOS:000624706200001},
}

@article{ WOS:000621404200009,
Author = {Schad, Daniel J. and Betancourt, Michael and Vasishth, Shravan},
Title = {Toward a Principled Bayesian Workflow in Cognitive Science},
Journal = {PSYCHOLOGICAL METHODS},
Year = {2021},
Volume = {26},
Number = {1},
Pages = {103-126},
Month = {FEB},
Abstract = {Experiments in research on memory, language, and in other areas of
   cognitive science are increasingly being analyzed using Bayesian
   methods. This has been facilitated by the development of probabilistic
   programming languages such as Stan, and easily accessible front-end
   packages such as brms. The utility of Bayesian methods, however,
   ultimately depends on the relevance of the Bayesian model, in particular
   whether or not it accurately captures the structure of the data and the
   data analyst's domain expertise. Even with powerful software, the
   analyst is responsible for verifying the utility of their model. To
   demonstrate this point, we introduce a principled Bayesian workflow
   (Betancourt, 2018) to cognitive science. Using a concrete working
   example, we describe basic questions one should ask about the model:
   prior predictive checks, computational faithfulness, model sensitivity,
   and posterior predictive checks. The running example for demonstrating
   the workflow is data on reading times with a linguistic manipulation of
   object versus subject relative clause sentences. This principled
   Bayesian workflow also demonstrates how to use domain knowledge to
   inform prior distributions. It provides guidelines and checks for valid
   data analysis, avoiding overfitting complex models to noise, and
   capturing relevant data structure in a probabilistic model. Given the
   increasing use of Bayesian methods, we aim to discuss how these methods
   can be properly employed to obtain robust answers to scientific
   questions.},
DOI = {10.1037/met0000275},
ISSN = {1082-989X},
EISSN = {1939-1463},
ResearcherID-Numbers = {Schad, Daniel/HIZ-8036-2022
   },
ORCID-Numbers = {Betancourt, Michael/0000-0002-2900-0931
   Schad, Daniel/0000-0003-2586-6823
   Vasishth, Shravan/0000-0003-2027-1994},
Unique-ID = {WOS:000621404200009},
}

@article{ WOS:000618182300004,
Author = {Khuzin, Airat and Ibragimov, Ruslan},
Title = {Processes of structure formation and paste matrix hydration with
   multilayer carbon nanotubes additives},
Journal = {JOURNAL OF BUILDING ENGINEERING},
Year = {2021},
Volume = {35},
Month = {MAR},
Abstract = {The creation of high-strength materials with targeted structure
   formation and hydration to obtain building composites with a given set
   of properties represents a relevant area of the modern construction
   sector. This paper investigates promising nanomodifying additives i.e.
   multilayer carbon nanotubes along with an effective method to obtain a
   comprehensive nanomodifying additive through pre-ultrasonic dispersion
   of MWCNT globules of varying dispersive capacity in an isopropyl alcohol
   medium. The comprehensive nanomodifying additive obtained is
   characterized by high stability as well as a uniform distribution of
   carbon nanotubes in the additive. The Structure software package
   developed by the authors was used to study the structure of paste matrix
   at the mesastructural level. The modification of paste matrix by means
   of the developed comprehensive nanostructure additive reduces the crack
   length index to 113\% with the crack length ratio and the crack shape
   ratio going down to 61\% and 34\%, respectively. The increase in the
   strength and durability of the compositions modified by the developed
   comprehensive nanomodifying additive is due to a more complete filling
   of the intergranular void with newly formed structures in the form of
   C-S-H (I), tobermorite and gel phases which enhance the degree of
   packing and density of the composite under examination. However, the
   share of nano - and micropores goes up by 1.21 and 1.31 times,
   respectively, while the share of meso - and macropores decreases by 1.51
   and 5.25 times.},
DOI = {10.1016/j.jobe.2020.102030},
EarlyAccessDate = {JAN 2021},
Article-Number = {102030},
EISSN = {2352-7102},
ResearcherID-Numbers = {Ибрагимов, Руслан/O-5968-2017},
ORCID-Numbers = {Ибрагимов, Руслан/0000-0001-8879-1190},
Unique-ID = {WOS:000618182300004},
}

@article{ WOS:000608571600001,
Author = {Warren, Dan L. and Matzke, Nicholas J. and Cardillo, Marcel and
   Baumgartner, John B. and Beaumont, Linda J. and Turelli, Michael and
   Glor, Richard E. and Huron, Nicholas A. and Simoes, Marianna and
   Iglesias, Teresa L. and Piquet, Julien C. and Dinnage, Russell},
Title = {ENMTools 1.0: an R package for comparative ecological biogeography},
Journal = {ECOGRAPHY},
Year = {2021},
Volume = {44},
Number = {4},
Pages = {504-511},
Month = {APR},
Abstract = {The ENMTools software package was introduced in 2008 as a platform for
   making measurements on environmental niche models (ENMs, frequently
   referred to as species distribution models or SDMs), and for using those
   measurements in the context of newly developed Monte Carlo tests to
   evaluate hypotheses regarding niche evolution. Additional functionality
   was later added for model selection and simulation from ENMs, and the
   software package has been quite widely used. ENMTools was initially
   implemented as a Perl script, which was also compiled into an executable
   file for various platforms. However, the package had a number of
   significant limitations; it was only designed to fit models using
   Maxent, it relied on a specific Perl distribution to function, and its
   internal structure made it difficult to maintain and expand.
   Subsequently, the R programming language became the platform of choice
   for most ENM studies, making ENMTools less usable for many
   practitioners. Here we introduce a new R version of ENMTools that
   implements much of the functionality of its predecessor as well as
   numerous additions that simplify the construction, comparison and
   evaluation of niche models. These additions include new metrics for
   model fit, methods of measuring ENM overlap, and methods for testing
   evolutionary hypotheses. The new version of ENMTools is also designed to
   work within the expanding universe of R tools for ecological
   biogeography, and as such includes greatly simplified interfaces for
   analyses from several other R packages.},
DOI = {10.1111/ecog.05485},
EarlyAccessDate = {JAN 2021},
ISSN = {0906-7590},
EISSN = {1600-0587},
ResearcherID-Numbers = {Simoes, Marianna/C-3369-2019
   Piquet, Julien Christophe/AAN-1913-2020
   Dinnage, Russell/L-9892-2013
   Matzke, Nicholas/L-4109-2015
   Glor, Richard/N-4656-2013
   Warren, Dan/B-3821-2010},
ORCID-Numbers = {Simoes, Marianna/0000-0003-4401-5530
   Piquet, Julien Christophe/0000-0001-8329-7126
   Dinnage, Russell/0000-0003-0846-2819
   Huron, Nicholas/0000-0001-6835-1390
   Matzke, Nicholas/0000-0002-8698-7656
   Glor, Richard/0000-0002-3359-1631
   Warren, Dan/0000-0002-8747-2451},
Unique-ID = {WOS:000608571600001},
}

@article{ WOS:000612658600001,
Author = {Chenrai, Piyaphong},
Title = {Case Study on Geoscience Teaching Innovation: Using 3D Printing to
   Develop Structural Interpretation Skill in Higher Education Levels},
Journal = {FRONTIERS IN EARTH SCIENCE},
Year = {2021},
Volume = {8},
Month = {JAN 15},
Abstract = {Applying three-dimensional (3D) printing technology to a geoscience
   classroom provides an alternative way to teach students. This brief
   report describes an educational innovation for the geoscience classroom
   by 3D printing technology to develop structural interpretation skill in
   high education level in Thailand. In comparison to traditional
   classrooms, this teaching method enables students to more easily
   comprehend how geological structures and features occur in nature
   through a project-based learning in seismic interpretation course. 3D
   printing models are constructed based on student interpretation through
   three different software packages. The observations in this study
   indicate that the ability to create the 3D models based on digital
   seismic data can enhance structural interpretation skill of students.
   The benefit of freely orientating and viewing in different angles of the
   3D models leads to a construction of cognitive abstract space and
   spatial visualization ability. Therefore, 3D printing technology plays
   an important role in changing and developing the geoscience education
   system in Thailand at present and in the future. This teaching method
   could potentially benefit any science classroom and have applications in
   other disciplines requiring similar skill.},
DOI = {10.3389/feart.2020.590062},
Article-Number = {590062},
EISSN = {2296-6463},
Unique-ID = {WOS:000612658600001},
}

@article{ WOS:000600044300004,
Author = {Flinders, Bryn and Morrell, Josie and Marshall, Peter S. and Ranshaw,
   Lisa E. and Heeren, Ron M. A. and Clench, Malcolm R.},
Title = {Monitoring the three-dimensional distribution of endogenous species in
   the lungs by matrix-assisted laser desorption/ionization mass
   spectrometry imaging},
Journal = {RAPID COMMUNICATIONS IN MASS SPECTROMETRY},
Year = {2021},
Volume = {35},
Number = {1},
Month = {JAN 15},
Abstract = {Rationale Matrix-assisted laser desorption/ionization mass spectrometry
   imaging (MALDI-MSI) is routinely employed to monitor the distribution of
   compounds in tissue sections and generate two-dimensional (2D) images.
   Whilst informative the images do not represent the distribution of the
   analyte of interest through the entire organ. The generation of 3D
   images is an exciting field that can provide a deeper view of the
   analyte of interest throughout an entire organ.
   Methods Serial sections of mouse and rat lung tissue were obtained at
   120 mu m depth intervals and imaged individually. Homogenate
   registration markers were incorporated in order to aid the final 3D
   image construction. Using freely available software packages, the images
   were stacked together to generate a 3D image that showed the
   distribution of endogenous species throughout the lungs.
   Results Preliminary tests were performed on 16 serial tissue sections of
   mouse lungs. A 3D model showing the distribution of phosphocholine at
   m/z 184.09 was constructed, which defined the external structure of the
   lungs and trachea. Later, a second experiment was performed using 24
   serial tissue sections of the left lung of a rat. Two molecular markers,
   identified as {[}PC (32:1) + K](+) at m/z 770.51 and {[}PC (36:4) +
   K](+) at m/z 820.52, were used to generate 3D models of the parenchyma
   and airways, respectively.
   Conclusions A straightforward method to generate 3D MALDI-MS images of
   selected molecules in lung tissue has been presented. Using freely
   available imaging software, the 3D distributions of molecules related to
   different anatomical features were determined.},
DOI = {10.1002/rcm.8957},
Article-Number = {e8957},
ISSN = {0951-4198},
EISSN = {1097-0231},
ResearcherID-Numbers = {Heeren, Ron/ABA-4858-2020
   },
ORCID-Numbers = {Heeren, Ron/0000-0002-6533-7179
   Clench, Malcolm/0000-0002-0798-831X},
Unique-ID = {WOS:000600044300004},
}

@article{ WOS:000605527000002,
Author = {Trofimov, Alexander Vladimirovich},
Title = {Inverse problem analysis for nondestructive evaluation of structural
   characteristics of multilayered foundations},
Journal = {ARCHIVE OF APPLIED MECHANICS},
Year = {2021},
Volume = {91},
Number = {4},
Pages = {1773-1792},
Month = {APR},
Abstract = {The evaluation of the parameters of multilayered foundations (pavements,
   runway strips, etc.) plays an important role in ensuring the safe
   movement of vehicles. An approach of model construction for estimating
   the mechanical and geometric parameters of such foundations based on the
   solutions of inverse problems for multilayered elastic packets is
   proposed. As input data for such problems the measured displacements (or
   velocities) of certain points on the package surface are used. The
   proposed approach is based on informational-probabilistic paradigm for
   inverse problem analysis, whose task is to obtain a posteriori
   probability density in the space of unknown parameters. The essence of
   the approach is the block-parametric approximation of the a priori
   probability density and likelihood function in the spaces of parameters
   and model data of the problem. The method allows estimating the
   parameters of the a priori distribution of unknown variable parameters,
   identifying and excluding outliers of the measured data from the created
   model, and constructing a posteriori estimation of the unknown
   parameters' probability density with acceptable resolution. Proposed
   method can be used to create a new generation of equipment intended for
   nondestructive monitoring and estimating of the condition of pavements,
   runways and foundations of artificial structures. The appropriate
   software for such high-speed scanning devices that allow on-the-fly
   display of the diagnosed layered foundation parameters can be developed.},
DOI = {10.1007/s00419-020-01854-5},
EarlyAccessDate = {JAN 2021},
ISSN = {0939-1533},
EISSN = {1432-0681},
Unique-ID = {WOS:000605527000002},
}

@article{ WOS:000605515800001,
Author = {Bakhshi Kiadehi, Katayoun and Rahmani, Amir Masoud and Sabbagh
   Molahosseini, Amir},
Title = {A fault-tolerant architecture for internet-of-things based on
   software-defined networks},
Journal = {TELECOMMUNICATION SYSTEMS},
Year = {2021},
Volume = {77},
Number = {1},
Pages = {155-169},
Month = {MAY},
Abstract = {One of the most critical challenges of the Internet of Things (IoT) is
   to provide real-time services. Therefore, to provide a secure,
   efficient, and stable communication platform in the Internet of Things,
   emerging architectures such as software-defined networks (SDN) being
   significant. This paper proposes a comprehensive SDN based
   fault-tolerant architecture in IoT environments. In the proposed scheme,
   a mathematical model called Shared Risk Link Group (SRLG) calculates
   redundant paths as the main and backup non-overlapping paths between
   network equipment. In addition to the fault tolerance (FT) discussion in
   the proposed scheme, service quality is considered in the proposed
   scheme. Putting the percentage of link usage and the rate of link delay
   in calculating link costs makes it possible to calculate two completely
   non-overlapping paths. The end-to-end delay and the degree of link
   congestion are minimal. We compare our proposed scheme with two policies
   for building routes from source to destination. The simulation results
   indicate that, while reducing the error recovery time, the proposed
   method leads to improved services quality parameters such as packet
   loss, delay, and packet jitter. The results show that in case of a link
   failure in the network, the recovery time in large scenarios is a
   maximum of 16 ms, which improved by 20\%, compared to the disjoint paths
   (DP) method. Our Approach can decrease packet loss by approximately 30\%
   compared to the Dijkstra's algorithm and roughly 24\% compared to the DP
   method. Also, SLRG reduces latency by approximately 36\% compared to the
   Dijkstra's algorithm and roughly 19\% compared to the DP method. Last,
   the proposed scheme reduces Jitter by approximately 49\% compared to the
   Dijkstra's algorithm, and roughly 26\% compared to the DP method.},
DOI = {10.1007/s11235-020-00750-1},
EarlyAccessDate = {JAN 2021},
ISSN = {1018-4864},
EISSN = {1572-9451},
ResearcherID-Numbers = {Rahmani, Amir Masoud/K-2702-2013},
ORCID-Numbers = {Rahmani, Amir Masoud/0000-0001-8641-6119},
Unique-ID = {WOS:000605515800001},
}

@article{ WOS:000609383600009,
Author = {Yin, Shuwei and Tian, Xiao and Zhang, Jingjing and Sun, Peisen and Li,
   Guanglin},
Title = {PCirc: random forest-based plant circRNA identification software},
Journal = {BMC BIOINFORMATICS},
Year = {2021},
Volume = {22},
Number = {1},
Month = {JAN 6},
Abstract = {BackgroundCircular RNA (circRNA) is a novel type of RNA with a
   closed-loop structure. Increasing numbers of circRNAs are being
   identified in plants and animals, and recent studies have shown that
   circRNAs play an important role in gene regulation. Therefore,
   identifying circRNAs from increasing amounts of RNA-seq data is very
   important. However, traditional circRNA recognition methods have
   limitations. In recent years, emerging machine learning techniques have
   provided a good approach for the identification of circRNAs in animals.
   However, using these features to identify plant circRNAs is infeasible
   because the characteristics of plant circRNA sequences are different
   from those of animal circRNAs. For example, plants are extremely rich in
   splicing signals and transposable elements, and their sequence
   conservation in rice, for example is far less than that in mammals. To
   solve these problems and better identify circRNAs in plants, it is
   urgent to develop circRNA recognition software using machine learning
   based on the characteristics of plant circRNAs.ResultsIn this study, we
   built a software program named PCirc using a machine learning method to
   predict plant circRNAs from RNA-seq data. First, we extracted different
   features, including open reading frames, numbers of k-mers, and splicing
   junction sequence coding, from rice circRNA and lncRNA data. Second, we
   trained a machine learning model by the random forest algorithm with
   tenfold cross-validation in the training set. Third, we evaluated our
   classification according to accuracy, precision, and F1 score, and all
   scores on the model test data were above 0.99. Fourth, we tested our
   model by other plant tests, and obtained good results, with accuracy
   scores above 0.8. Finally, we packaged the machine learning model built
   and the programming script used into a locally run circular RNA
   prediction software, Pcirc
   (https://github.com/Lilab-SNNU/Pcirc).ConclusionBased on rice circRNA
   and lncRNA data, a machine learning model for plant circRNA recognition
   was constructed in this study using random forest algorithm, and the
   model can also be applied to plant circRNA recognition such as
   Arabidopsis thaliana and maize. At the same time, after the completion
   of model construction, the machine learning model constructed and the
   programming scripts used in this study are packaged into a localized
   circRNA prediction software Pcirc, which is convenient for plant circRNA
   researchers to use.},
DOI = {10.1186/s12859-020-03944-1},
Article-Number = {10},
ISSN = {1471-2105},
Unique-ID = {WOS:000609383600009},
}

@article{ WOS:000727972600001,
Author = {Ali, Babar and Zahoor, Hafiz and Aibinu, Ajibade and Nasir, Abdur Rehman
   and Tariq, Ali and Imran, Umair and Khan, Rashid Mehmood},
Title = {BIM AIDED INFORMATION AND VISUALIZATION REPOSITORY FOR MANAGING
   CONSTRUCTION DELAY CLAIMS},
Journal = {JOURNAL OF INFORMATION TECHNOLOGY IN CONSTRUCTION},
Year = {2021},
Volume = {26},
Pages = {1023-1040},
Abstract = {Delays in construction result in a multitude of negative effects on
   project performance, and severe dismays among participating parties.
   This study aims to digitize the traditional process of recording and
   managing the construction delays using Building Information Modeling
   (BIM). Extensive literature review followed by semi structure interviews
   of 21 industry experts were carried out to identify the issues faced by
   construction stakeholders in managing construction delays. To resolve
   these issues, a plugin named BIM-based Construction Delays Recorder
   (BIM-CDR) is developed using Application Programming Interface (API) of
   the most commonly used BIM software i.e. Autodesk Revit. BIM-CDR
   provides a centralized repository, encompassing detailed information
   related to delays, which can be retrieved and visualized to analyze
   their impact on delay claims. To assess the effectiveness of BIM-CDR, a
   feasibility study is conducted with the experts' review panel. The
   results revealed that BIM-CDR can record wide-ranging information
   related to all the significant issues causing delays on construction
   sites, and can help in effectively managing their corresponding claims.
   The advantages of the developed prototype include visualization of
   delays' location, facilitation of delay analysis and effective delays
   management. Moreover, it also promotes transparency and speedy
   settlement of delay related claims without any unwanted disputes.},
DOI = {10.36680/j.itcon.2021.054},
ISSN = {1874-4753},
ResearcherID-Numbers = {Nasir, Dr.-Ing. Abdur Rehman/HJZ-3157-2023
   Zahoor, Hafiz/B-4403-2018},
ORCID-Numbers = {Nasir, Dr.-Ing. Abdur Rehman/0000-0001-6152-9280
   TARIQ, ALI/0000-0002-9487-9102
   Zahoor, Hafiz/0000-0003-0560-6784},
Unique-ID = {WOS:000727972600001},
}

@article{ WOS:000614619200004,
Author = {Ali, Naveed and Lai, Richard},
Title = {GLOBAL SOFTWARE DEVELOPMENT: A REVIEW OF ITS PRACTICES},
Journal = {MALAYSIAN JOURNAL OF COMPUTER SCIENCE},
Year = {2021},
Volume = {34},
Number = {1},
Pages = {82-129},
Abstract = {Global Software Development (GSD) is multi-site software development
   with software teams scattered across different places around the world.
   To gain the benefits of the lower cost of software development and
   access to international talents, many organizations are using GSD.
   However, earlier studies reported that achieving these benefits can be
   difficult and GSD involves risks. This paper presents a systematic
   review of GSD and aim to provide the findings about the current
   practices, benefits, associated risks, and difficulties associated with
   it.The review was conducted in accordance with the systematic review
   procedures and processes defined by Kitchenham. By following these
   guidelines, six digital libraries to gather information on the prevalent
   trends and practices in GSD were searched. The outcomes presented in
   this study are based on 204 studies, published in peer-reviewed
   conferences and journals. Our findings will enable readers to understand
   the current practices, benefits, risks and difficulties associated with
   GSD. As a result, they can form realistic expectations before making a
   decision to engage in GSD or not, and formulate better pre-contract and
   post-contract planning in order to increase the chances of project
   success. A merit of this paper as compared to previous survey papers on
   GSD is that it reports the issues related to cultural diversity,
   requirements engineering and software architecture.},
DOI = {10.22452/mjcs.vol34no1.5},
ISSN = {0127-9084},
Unique-ID = {WOS:000614619200004},
}

@article{ WOS:000648689700002,
Author = {Asamoah, Moses Kumi},
Title = {ICT officials' opinion on deploying Open Source Learning Management
   System for teaching and learning in universities in a developing society},
Journal = {E-LEARNING AND DIGITAL MEDIA},
Year = {2021},
Volume = {18},
Number = {1},
Pages = {18-38},
Month = {JAN},
Abstract = {Information and Communication Technology specialists, working within
   universities play important roles in the deployment of educational
   technologies for teaching and learning. Given the centrality of these
   specialists and the woeful dearth of empirics on this subject-matter in
   Sub-Saharan Africa, this paper interrogates the perspectives of ICT
   specialists working within universities in Ghana, on the deployment of
   Moodle (R) /Sakai (R) Learning Management System (LMS) by universities
   in the country for teaching and learning and the challenges involved.
   Data collection entailed semi-structured interviews with twenty
   informants. Thematic analysis was used for data analysis. It emerged
   that there has been impressive formulation of e-learning policy,
   construction of computer laboratories, Staff Resource Centre, Electronic
   Support Unit in the Balm Library, installation of Internet facility,
   Learning Management System, Enterprise Solution Software, Library
   Solution Software to promote ICT-mediated teaching and learning in the
   face of barriers. The paper adds knowledge to the extant literature in
   the field, impacts practice and policy along the pathway for ensuring
   sustainable deployment of LMS in universities in Sub-Saharan Africa.},
DOI = {10.1177/2042753020946280},
EISSN = {2042-7530},
ORCID-Numbers = {Asamoah, Moses/0000-0002-6698-7528},
Unique-ID = {WOS:000648689700002},
}

@article{ WOS:000681444500005,
Author = {Azmi, Nur Farhana and Ali, Azlan Shah and Ahmad, Faizah},
Title = {Exploring the challenges in protecting the identity of small historic
   towns in Malaysia},
Journal = {OPEN HOUSE INTERNATIONAL},
Year = {2021},
Volume = {46},
Number = {1},
Pages = {64-80},
Abstract = {Purpose Built heritage constitutes the uniqueness and identity of a
   place. While being overlooked in existing research, built heritage in
   small towns is increasingly threatened by dilapidation, exhaustion and
   disappearance. Therefore, this study aims to examine the challenges in
   protecting the unique features and identity of small towns with regard
   to building regulations, guidelines and policies.
   Design/methodology/approach A semi-structured interview was conducted
   amongst a purposive sample of nine stakeholders from different
   organizations involved in heritage matters in Malaysia to investigate
   the issues and challenges facing the protection of small towns' identity
   with regard to existing protection mechanisms impacting development of
   the towns. The data were then analyzed using the qualitative software
   package NVivo 8.0. The summary models imported from NVivo were created
   to visualize the connections between various dimensions of constructs,
   concepts and categories identified in the interviews.
   Findings The study found that the current issues of built heritage
   protection at the local level can be classified into five emergent
   themes: legislative, institutional, economic, social and technical
   issues. While absence of specific guideline for identifying local
   cultural places has been identified as the most fundamental issues
   facing the protection of unique and distinct resources in small Malaysia
   towns, the paper concludes with a framework of measures that can then be
   used for identification and protection of small town identity.
   Originality/value This study is the first of many fruitful contributions
   that examine heritage identification and protection mechanisms at the
   local level.},
DOI = {10.1108/OHI-05-2020-0028},
ISSN = {0168-2601},
EISSN = {2633-9838},
ResearcherID-Numbers = {Ali, Azlan Shah/B-5214-2010},
ORCID-Numbers = {Ali, Azlan Shah/0000-0003-1923-7473},
Unique-ID = {WOS:000681444500005},
}

@inproceedings{ WOS:000682922000039,
Author = {Basyuk, Taras and Vasyliuk, Andrii},
Editor = {Sharonova, N and Lytvyn, V and Cherednichenko, O and Kupriianov, Y and Kanishcheva, O and Hamon, T and Grabar, N and Vysotska, V and KowalskaStyczen, A and JonekKowalska, I},
Title = {Approach to a Subject Area Ontology Visualization System Creating},
Booktitle = {COLINS 2021: COMPUTATIONAL LINGUISTICS AND INTELLIGENT SYSTEMS, VOL I},
Series = {CEUR Workshop Proceedings-Series},
Year = {2021},
Volume = {2870},
Note = {5th International Conference on Computational Linguistics and
   Intelligent Systems (COLINS), Kharkiv, UKRAINE, APR 22-23, 2021},
Organization = {Natl Tech Univ Kharkiv Polytechn Inst; Lviv Polytechn Natl Univ; Univ
   Paris 13, Inst Galilee; Politechnika Slaska; Ukrainian Sci \& Educ IT
   Soc},
Abstract = {This article analyzes the existing technologies and systems for
   displaying ontologies, presents the features and shows the need to
   create a new web-based visualization system of this data structure. The
   display structures analysis is performed and the ontology representation
   peculiarities in the form of frame models and application of Simple
   Knowledge Organization System (SKOS) for the Network router concept are
   shown. The ontology construction stages of the subject area are outlined
   and described, which include: formation of the list of concepts included
   in the ontological structure of the upper level; ordering the list of
   concepts; creation of a glossary; mapping arcs between created concepts.
   An example of building an ontology for the subject area ``information
   security{''} in describing the concept features of ``network computer
   attack{''}. The main functions of the system are highlighted and the
   stage of its design is performed using the object-oriented approach and
   UML language tools. The diagram of precedents which visualizes the basic
   variants of the developed system usage is shown. The construction
   architecture was chosen in favor of three-tiered client-server
   architecture and the peculiarities of interaction between levels are
   described. The development language choice peculiarities are given and
   the choice of libraries and frameworks is proved, which ensure its
   effective functioning. The result of the work is a prototype of a
   software system for visualization of ontologies, which can be used to
   visualize the specified data structure. The study creates the
   prerequisites for building a full-featured system of visualization of
   ontologies, to be part of the decision support system for a particular
   subject area.},
ISSN = {1613-0073},
ResearcherID-Numbers = {Basyuk, Taras/P-8535-2016},
ORCID-Numbers = {Basyuk, Taras/0000-0003-0813-0785},
Unique-ID = {WOS:000682922000039},
}

@inproceedings{ WOS:000760875500131,
Author = {Bikku, Thulasi and Karthik, Jayavarapu and Rao, Ganga Rama Koteswara and
   Sree, K. P. N. V. Satya and Srinivas, P. V. V. S. and Prasad, Chitturi},
Book-Group-Author = {IEEE},
Title = {Brain Tissue Segmentation via Deep Convolutional Neural Networks},
Booktitle = {PROCEEDINGS OF THE 2021 FIFTH INTERNATIONAL CONFERENCE ON I-SMAC (IOT IN
   SOCIAL, MOBILE, ANALYTICS AND CLOUD) (I-SMAC 2021)},
Year = {2021},
Pages = {757-763},
Note = {5th International Conference on IoT in Social, Mobile, Analytics and
   Cloud (I-SMAC), ELECTR NETWORK, NOV 11-13, 2021},
Organization = {IEEE; SCAD Inst Technol},
Abstract = {Deep convolutional neural networks were used to successfully segment
   several important neural tissue classes in MRI brain images, and
   approaches for integrating prior information into the networks to
   increase their performance on this task were investigated. Regrettably,
   only the first of them is addressed in this paper. To make the
   implementation of nonstandard architectures, which was expected to be
   required for the second goal, it was determined to provide a framework
   for defining and training networks by only using fundamental components.
   While this was an educational experience, the amount of progress
   accomplished was far less than if a conventional network package had
   been utilized instead. The requirement to deal with all of the lowest
   level aspects of network construction, from initialization schemes to
   adaptive learning rates and all the other components of the optimizer
   pipeline has left no time for utilizing this infrastructure to do
   something which has not been accomplished by the existing frameworks,
   and of course in all other respects it is far more limited than they
   are. It would in-stead be focused on a clear and detailed analysis of
   the full pipeline, which is required to build a network for solving the
   first problem. Despite several difficulties tracking down bugs in the
   optimizer, GPU memory allocation, and the last-minute accidental
   deletion of a large portion of the experimental results, the software
   implementation made available at: achieves DICE results of 0:8, which,
   while not class leading, would still place well in many benchmarks.},
DOI = {10.1109/I-SMAC52330.2021.9640635},
ISBN = {978-1-6654-2642-8},
ResearcherID-Numbers = {jayavarapu, karthik/ABC-7085-2020
   Rao, Ganga Rama Koteswara/ADX-9162-2022
   S, Srinivas P V V/ABA-2329-2021
   SREE, Dr K P N V SATYA/I-5316-2013},
ORCID-Numbers = {jayavarapu, karthik/0000-0001-5762-340X
   Rao, Ganga Rama Koteswara/0000-0003-0972-1165
   S, Srinivas P V V/0000-0002-4479-2594
   SREE, Dr K P N V SATYA/0000-0003-4567-3447},
Unique-ID = {WOS:000760875500131},
}

@article{ WOS:000613224400001,
Author = {Chen, Long and Xia, Chunhe and Lei, Shengwei and Wang, Tianbo},
Title = {Detection, Traceability, and Propagation of Mobile Malware Threats},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {14576-14598},
Abstract = {In recent years, the application of smartphones, Android operating
   systems and mobile applications have become more prevalent worldwide. To
   study the traceability, propagation, and detection of the threats, we
   perform research on all aspects of the end-to-end environment. With
   machine learning based on the mobile malware detection algorithms that
   integrate the dynamic and static research of the identification
   algorithm, application software samples are collected to study
   sentences. Through knowledge labeling and knowledge construction, the
   association relationship of knowledge is extracted to realize the
   research of knowledge map construction. Flooding is closely correlated
   with the complexity of the Android mobile version of the kernel and
   malicious programs. A static dynamic analysis of the mobile malicious
   program is carried out, and the social network social diagram is
   constructed to model the propagation of the mobile malicious program. We
   extended the approach of deriving common malware behavior through graph
   clustering. On this basis, Android behavior analysis is performed
   through our virtual machine execution engine. We extend the family
   characteristics to the concept of DNA race genes. By studying SMS/MMS,
   Bluetooth, 5G base station networks, metropolitan area networks, social
   networks, homogeneous communities, telecommunication networks, and
   application market ecosystem propagation scenarios, we discovered the
   law of propagation. In addition, we studied the construction of the
   mobile Internet big data knowledge graph. Quantitative data for the main
   family chronology of mobile malware are obtained. We conducted detailed
   research and comprehensive analysis of Android application package (APK)
   details and behavior, relationship, resource-centric, and syntactic
   aspects. Furthermore, we summarized the architecture of mobile malware
   security analysis. We also discuss encryption of malware traffic
   discrimination. These precise modeling and quantified research results
   constitute the architecture of mobile malware analysis.},
DOI = {10.1109/ACCESS.2021.3049819},
ISSN = {2169-3536},
ORCID-Numbers = {Chen, Long/0000-0002-2535-4978},
Unique-ID = {WOS:000613224400001},
}

@article{ WOS:000654905700019,
Author = {Chiesa, Marco and Kamisinski, Andrzej and Rak, Jacek and Retvari, Gabor
   and Schmid, Stefan},
Title = {A Survey of Fast-Recovery Mechanisms in Packet-Switched Networks},
Journal = {IEEE COMMUNICATIONS SURVEYS AND TUTORIALS},
Year = {2021},
Volume = {23},
Number = {2},
Pages = {1253-1301},
Abstract = {In order to meet their stringent dependability requirements, most modern
   packet-switched communication networks support fast-recovery mechanisms
   in the data plane. While reactions to failures in the data plane can be
   significantly faster compared to control plane mechanisms, implementing
   fast recovery in the data plane is challenging, and has recently
   received much attention in the literature. This survey presents a
   systematic, tutorial-like overview of packet-based fast-recovery
   mechanisms in the data plane, focusing on concepts but structured around
   different networking technologies, from traditional link-layer and
   IP-based mechanisms, over BGP and MPLS to emerging software-defined
   networks and programmable data planes. We examine the evolution of
   fast-recovery standards and mechanisms over time, and identify and
   discuss the fundamental principles and algorithms underlying different
   mechanisms. We then present a taxonomy of the state of the art,
   summarize the main lessons learned, and propose a few concrete future
   directions.},
DOI = {10.1109/COMST.2021.3063980},
EISSN = {1553-877X},
ORCID-Numbers = {Rak, Jacek/0000-0001-7276-6097},
Unique-ID = {WOS:000654905700019},
}

@article{ WOS:000606240000001,
Author = {Cooper, Izaak and Hotchkiss, Rollin H. and Williams, Gustavious Paul},
Title = {Extending Multi-Beam Sonar with Structure from Motion Data of Shorelines
   for Complete Pool Bathymetry of Reservoirs},
Journal = {REMOTE SENSING},
Year = {2021},
Volume = {13},
Number = {1},
Month = {JAN},
Abstract = {Bathymetric mapping is an important tool for reservoir management,
   typically completed before reservoir construction. Historically,
   bathymetric maps were produced by interpolating between points measured
   at a relatively large spacing throughout a reservoir, typically on the
   order of a few, up to 10, meters or more depending on the size of the
   reservoir. These measurements were made using traditional survey methods
   before the reservoir was filled, or using sonar surveys after filling.
   Post-construction issues such as sedimentation and erosion can change a
   reservoir, but generating updated bathymetric maps is difficult as the
   areas of interest are typically in the sediment deltas and other
   difficult-to-access areas that are often above water or exposed for part
   of the year. We present a method to create complete reservoir
   bathymetric maps, including areas above the water line, using small
   unmanned aerial vehicle (sUAV) photogrammetry combined with multi-beam
   sonar data-both established methods for producing topographic models.
   This is a unique problem because the shoreline topographic models
   generated by the photogrammetry are long and thin, not an optimal
   geometry for model creation, and most images contain water, which causes
   issues with image-matching algorithms. This paper presents methods to
   create accurate above-water shoreline models using images from sUAVs,
   processed using a commercial software package and a method to accurately
   knit sonar and Structure from Motion (SfM) data sets by matching slopes.
   The models generated by both approaches are point clouds, which consist
   of points representing the ground surface in three-dimensional space.
   Generating models from sUAV-captured images requires ground control
   points (GCPs), i.e., points with a known location, to anchor model
   creation. For this study, we explored issues with ground control
   spacing, masking water regions (or omitting water regions) in the
   images, using no GCPs, and incorrectly tagging a GCP. To quantify the
   effect these issues had on model accuracy, we computed the difference
   between generated clouds and a reference point cloud to determine the
   point cloud error. We found that the time required to place GCPs was
   significantly more than the time required to capture images, so
   optimizing GCP density is important. To generate long, thin shoreline
   models, we found that GCPs with a similar to 1.5-km (similar to 1-mile)
   spacing along a shoreline are sufficient to generate useful data. This
   spacing resulted in an average error of 5.5 cm compared to a reference
   cloud that was generated using similar to 0.5-km (similar to 1/4-mile)
   GCP spacing. We found that we needed to mask water and areas related to
   distant regions and sky in images used for model creation. This is
   because water, objects in the far oblique distance, and sky confuse the
   algorithms that match points among images. If we did not mask the
   images, the resulting models had errors of more than 20 m. Our sonar
   point clouds, while self-consistent, were not accurately georeferenced,
   which is typical for most reservoir surveys. We demonstrate a method
   using cross-sections of the transition between the above-water clouds
   and sonar clouds to geo-locate the sonar data and accurately knit the
   two data sets. Shore line topography models (long and thin) and
   integration of sonar and drone data is a niche area that leverages
   current advances in data collection and processing. Our work will help
   researchers and practitioners use these advances to generate accurate
   post-construction reservoir bathometry maps to assist with reservoir
   management.},
DOI = {10.3390/rs13010035},
Article-Number = {35},
EISSN = {2072-4292},
ResearcherID-Numbers = {Williams, Gustavious/P-7252-2014
   },
ORCID-Numbers = {Williams, Gustavious/0000-0002-2781-0738
   Hotchkiss, Rollin/0000-0002-1391-6101},
Unique-ID = {WOS:000606240000001},
}

@inproceedings{ WOS:000883027100007,
Author = {Cuvilliez, Sam and Li, Jia and Kong, Zichen and Rudolph, Juergen and
   Billon, Francois and Xie, Hai},
Book-Group-Author = {Amer Soc Mech Engineers},
Title = {AFCEN FATIGUE CALCULATIONS BENCHMARK: IMPLEMENTATION OF THE RCC-M RULES
   IN PROBATIONARY PHASE FOR ENVIRONMENTALLY ASSISTED FATIGUE (EAF)
   ASSESSMENT ON A SIMPLE TEST CASE},
Booktitle = {PROCEEDINGS OF ASME 2021 PRESSURE VESSELS AND PIPING CONFERENCE
   (PVP2021), VOL 1},
Year = {2021},
Note = {ASME Pressure Vessels and Piping Conference (PVP), ELECTR NETWORK, JUL
   13-15, 2021},
Organization = {Amer Soc Mech Engineers; Amer Soc Mech Engineers, Pressure Vessels \&
   Piping Div},
Abstract = {In the framework of fatigue assessment of Nuclear Power Plant (NPP)
   components, Environmentally Assisted Fatigue (EAF) is nowadays receiving
   an increased level of attention by regulatory bodies, code committees
   and utilities worldwide. This concerns particularly the Long Term
   Operation (LTO) of NPPs, where EAF may impact significantly the stress
   reports that are revised for Periodic Safety Reviews (PSRs), but also
   nuclear new builds in some cases. In this context, several guidance
   documents were developed and issued, the most well-known example being
   the NUREG/CR-6909 report {[}1] prepared in the US, which prescribes
   inter alia an update of the Cumulative Usage Factor (CUF) through an
   environmental correction factor ``F-en{''} that accounts for the
   detrimental effect of Light-Water Reactor (LWR) coolant environments on
   fatigue life and besides the water chemistry mainly depends on the
   temperature and the strain rate. Several nuclear codes have already
   incorporated this type of approach, including the AFCEN RCC-M code since
   its 2016 edition {[}2]{[}3] through the introduction of two code cases
   or ``Rules in Probationary Phase{''} (RPPs), entitled ``RPP-2{''} and
   ``RPP-3{''}. RPP-2 consists of an update of the design fatigue curve for
   austenitic and cast duplex stainless steels as well as Nickel based
   alloys, and is also a prerequisite for RPP-3 which provides guidelines
   for incorporating an F-en factor in fatigue usage factor calculations.
   RPP-3 describes a different method to consider EAF in fatigue analysis,
   that applies to austenitic and cast duplex stainless steel locations
   subjected to a Pressurized Water Reactor (PWR) primary circuit
   environment. It is an alternative to the straightforward application of
   the NUREG/CR-6909 methodology, where the F-en factor is alleviated by a
   factor of 3. This allowance, also known as the ``Fen-integrated{''}
   approach, is possible because of an over-conservative quantification of
   the effect of surface finish under a PWR environment, which is accounted
   for through the design fatigue curve. This has been demonstrated on the
   basis of numerous fatigue tests led in a PWR primary circuit
   environment, on small scale fatigue specimens with a rough surface
   finish {[}4]{[}5]{[}6]{[}7].
   While RPPs 2 and 3 have been applied in several stress report
   calculations for the fourth decennial inspection of the 900 MWe French
   PWRs fleet, these rules are still a novelty and could be further
   improved in their practical implementation. From this perspective, AFCEN
   has then launched a benchmark exercise at the end of 2019, involving
   several nuclear stakeholders in Europe and China. This benchmark
   consisted in solving a sample problem really close to one already used
   in an earlier EPRI benchmark {[}8] which aimed at the implementation of
   the ASME code case N-792 {[}9]. The geometry studied represents a vessel
   nozzle with an attached piping, and the structure is exemplarily
   subjected to three thermal and mechanical transients. The sample problem
   has been solved through Finite Element Analysis (FEA) calculations with
   various Finite Element (FE) software packages. The benchmark was divided
   into two stages. The first step consisted in achieving thermal and
   mechanical FE calculations, which were kept as simple as possible in
   such a way as to avoid discrepancies between participants' results. On
   the basis of the stresses obtained, the remaining part of the first
   phase consisted in calculating a CUF in air according to the RCC-M
   methodology. The second step was related to the calculation of a CUF in
   PWR primary circuit environment using the RPP-3, and more particularly
   to the calculation of F-en factors for the transient combinations
   identified during the previous stage. Since there is no mandatory rule
   (only guidance) in RPP-3 for calculating an equivalent strain rate, and
   since strain rate calculation constitutes a crucial step in the EAF
   concept, this is the area where discrepancies could be expected between
   participants' results. Based on these results, improvements could be
   proposed to the content of RPP-3, or additional guidelines could be
   added.},
Article-Number = {V001T01A007},
ISBN = {978-0-7918-8531-4},
Unique-ID = {WOS:000883027100007},
}

@article{ WOS:000664523600017,
Author = {Grigorieva, Anna L. and Grigoriev, Yan Y. and Khromov, I, Alexander and
   Kanashin, V, Ilya},
Title = {Modeling of deformation processes of elements of complex structures
   under conditions of low-cycle deformation},
Journal = {MARINE INTELLECTUAL TECHNOLOGIES},
Year = {2021},
Number = {2, 2},
Pages = {123-128},
Abstract = {Shipbuilding is one of the leading industries in Khabarovsk Krai. The
   relevance of the work. The need to be able to analytically assess the
   state of structural elements manufactured at industrial enterprises of
   the region (PJSC ``Amur shipbuilding plant{''}), to predict the state of
   these elements in advance, as well as to avoid problems associated with
   the operation of large technical systems, such as river and sea vessels.
   The production of shipbuilding products is due to the use of shell
   structures in large quantities. This provides unlimited shaping
   possibilities. It is required to have a mathematical basis that allows
   simulating the behavior of such structural elements during production
   and operation under the influence of loads. To simulate the described
   processes, a thin-walled deformable shell at finite stresses is
   considered.
   The problem of stretching a rigid-plastic strip is considered as a
   generalized mathematical model of the process. When modeling, the
   possibility of using various construction materials is considered. When
   solving such a problem, it is possible to trace the process of cracking
   and destruction of the material, and also to determine the optimal value
   of the main value of the Almansi tensor E1, which depends on the
   material from which the sample is made.
   An intermediate result is a system of equations for determining the
   velocity field. The resulting system takes into account the change in
   material density during deformation.
   The practical implementation of the results of the work is possible in
   all sectors of economic activity (shipbuilding, construction of various
   industrial and civil facilities). Verification of the results obtained
   is carried out using numerical computer modeling in the MSC.Patran
   software package. When obtaining analytical relationships, it becomes
   possible to develop a module that supplements the numerical-analytical
   packages, which allows to significantly reduce the computing power when
   solving problems of designing shell structures.},
DOI = {10.37220/MIT.2021.52.2.062},
ISSN = {2073-7173},
Unique-ID = {WOS:000664523600017},
}

@article{ WOS:000604280600008,
Author = {Hassanin, I, Ahmed and Fawzy, Hesham M. and Elsheikh, I, Ahmed},
Title = {Fatigue loading characteristic for the composite steel-concrete beams},
Journal = {FRATTURA ED INTEGRITA STRUTTURALE},
Year = {2021},
Number = {55},
Pages = {110-118},
Month = {JAN},
Abstract = {In the last few decades, composite beams, steel I beam, and concrete
   slab, have had a wide range of usage in bridges construction. This is
   due to its relatively low economic constructional cost compared to
   individual steel structures or reinforced concrete structures. This type
   of bridges in particular and many similar industrial structures in
   general are repeatedly subjected to fatigue loads, and that is
   frequently, as a result of the vehicles passing on these bridges or the
   vibrations caused by the machines in the industrial facilities. Previous
   investigation of fatigue behavior of composite steel-concrete structures
   has become ever more important, specially, steel concrete composite
   beams. These studies focused mainly on the external structural behavior
   of these beams such as a load -deflection relation, observing of cracks
   appeared during the loading and failure stage, and the strain in the
   steel and concrete flanges. Hence, in this study several factors
   affecting the mode of failure of these beams under the fatigue loads.
   ANSYS (Workbench) software package was used to construct a finite
   element model subjected to cyclic loading, and obtain members' cyclic
   behavior at a positive bending moment area. The results of the F.E.
   analysis were compared with experimental tests to obtain the validity of
   the finite element model and its accuracy. A parametric study was also
   conducted to investigate the effect of varying degrees of shear
   connection on the cyclic loading performance for each part of the
   composite section.},
DOI = {10.3221/IGF-ESIS.55.08},
ISSN = {1971-8993},
ResearcherID-Numbers = {Hassanin, Ahmed Ibrahim/ADI-1094-2022},
Unique-ID = {WOS:000604280600008},
}

@article{ WOS:000648758500022,
Author = {Hussain, Omar F. and Al-Kaseem, Bilal R. and Akif, Omar Z.},
Title = {Smart Flow Steering Agent for End-to-End Delay Improvement in
   Software-Defined Networks},
Journal = {BAGHDAD SCIENCE JOURNAL},
Year = {2021},
Volume = {18},
Number = {1},
Pages = {163-174},
Abstract = {To ensure fault tolerance and distributed management, distributed
   protocols are employed as one of the major architectural concepts
   underlying the Internet. However, inefficiency, instability and
   fragility could be potentially overcome with the help of the novel
   networking architecture called software-defined networking (SDN). The
   main property of this architecture is the separation of the control and
   data planes. To reduce congestion and thus improve latency and
   throughput, there must be homogeneous distribution of the traffic load
   over the different network paths. This paper presents a smart flow
   steering agent (SFSA) for data flow routing based on current network
   conditions. To enhance throughput and minimize latency, the SFSA
   distributes network traffic to suitable paths, in addition to
   supervising link and path loads. A scenario with a minimum spanning tree
   (MST) routing algorithm and another with open shortest path first (OSPF)
   routing algorithms were employed to assess the SFSA. By comparison, to
   these two routing algorithms, the suggested SFSA strategy determined a
   reduction of 2\% in packets dropped ratio (PDR), a reduction of 15-45\%
   in end-to-end delay according to the traffic produced, as well as a
   reduction of 23\% in round trip time (RTT). The Mininet emulator and POX
   controller were employed to conduct the simulation. Another advantage of
   the SFSA over the MST and OSPF is that its implementation and recovery
   time do not exhibit fluctuations. The smart flow steering agent will
   open a new horizon for deploying new smart agents in SDN that enhance
   network programmability and management.},
DOI = {10.21123/bsj.2021.18.1.0163},
ISSN = {2078-8665},
EISSN = {2411-7986},
ResearcherID-Numbers = {Al-Kaseem, Bilal R./X-5616-2019
   },
ORCID-Numbers = {Al-Kaseem, Bilal R./0000-0001-8264-6339
   Akif, Omar/0000-0003-1773-103X},
Unique-ID = {WOS:000648758500022},
}

@article{ WOS:000719112800021,
Author = {Koshevyi, O. P. and Levkivskyi, V, D. and Kosheva, V. O. and
   Mozharovskyi, A. S.},
Title = {COMPUTER MODELING AND OPTIMIZATION OF ENERGY EFFICIENCY POTENTIALS IN
   CIVIL ENGINEERING},
Journal = {OPIR MATERIALIV I TEORIA SPORUD-STRENGTH OF MATERIALS AND THEORY OF
   STRUCTURES},
Year = {2021},
Number = {106},
Pages = {274-281},
Abstract = {The paper presents the results of creating a software package for
   optimizing the calculation of potentials of alternative energy sources
   in the regions of Ukraine based on BIM technologies (AutoCAD, ArchiCAD,
   Revit), which are combined using the IFC format. The software package
   uses mathematical and graph-analytical models of climate and energy
   zoning in the regions of Ukraine, and with the help of MS Excel
   visualizes the research process and automates, accelerates optimal
   decision in design, reconstruction and construction. The process of
   forming a database for traditional energy sources (electricity, oil
   products, natural gas, coal, firewood) and a database of energy
   potentials of alternative energy sources (solar energy, wind energy,
   geothermal energy, hydropower of small rivers, potentials of livestock
   and crop biomass potential of excess pressure of natural gas, potentials
   of heat of soil, ground and sewage, potentials of energy of peat and
   forest waste) for all regions of Ukraine. The structure of the software
   package and a block diagram has been developed, all indicators are
   reduced to a single unit of measurement (MW{*}h / year per 1000 people).
   To analyze and make optimal decisions, informative-illustrative bar and
   sector pie charts are built in MS Excel on five main areas of energy
   consumption, taking into account alternative energy sources for each
   region of Ukraine. The general analysis of energy consumption and
   optimization calculations are carried out with the help of
   informative-illustrative diagram SANKEY, which is created with the help
   of ``SankeyDiagramGenerator{''}, and visualizes the whole process of
   graph-analytical modeling of energy consumption in Ukraine.},
DOI = {10.32347/2410-2547.2021.106.274-281},
EISSN = {2410-2547},
Unique-ID = {WOS:000719112800021},
}

@article{ WOS:000774703200006,
Author = {Kuleshova, L. S. and Fattakhov, I. G. and Sultanov, Sh Kh and Rabaev, R.
   U. and Mukhametshin, V. V. and Siraeva, G. M.},
Title = {Experience in Conducting Multi-Zone Hydraulic Fracturing on the Oilfield
   of PJSC ``Tatneft{''}},
Journal = {SOCAR PROCEEDINGS},
Year = {2021},
Number = {1},
Pages = {68-76},
Abstract = {The paper presents the possibilities of expanding production
   opportunities in the oil company PJSC Tatneft. For this purpose, the
   well No.xxx7g with an inclined pilot borehole was drilled at the
   Bavlinskoye oil field and oriented core samples were taken to study the
   lithological cross-section and the geological structure of the
   subsurface horizons. The horizontal wellbore itself is located in the
   dankovo-lebedyansky horizon, where multi-zone hydraulic fracturing was
   carried out through ports with packers there. The following methods will
   increase the share of recoverable oil reserves in the oldest
   oil-producing Volga region by starting the development of new productive
   horizons and increase the oil recovery factors for these reservoirs. The
   methods used in this work will reduce the unit costs of increasing oil
   production and achieve a cost-effective level of work on wells of this
   type. The work had its own peculiarities. One of the reasons for the
   difficulty in interpreting the hydraulic fracturing Minifrac (Meyer
   software package) was the rather long time of closing fractures in
   domanic deposits during the registration of pressure drop. In turn,
   during the minifrac analysis of the Nolte G Time Test graph showed that
   the fracture did not close, and therefore it is impossible to determine
   the closing pressure (the pressure gradient of the gap) with reliable
   accuracy. Note that when interpreting the flow test results, the best
   match of the experimental and calculated curves is achieved when using
   the model of a horizontal well operating a homogeneous reservoir. Also,
   the deterioration of the bottom-hole zone may be associated with a weak
   opening of the created fractures. (c) 2021
   ``OilGasScientificResearchProject{''} Institute. All rights reserved.},
DOI = {10.5510/OGP2021SI100511},
ISSN = {2218-6867},
EISSN = {2218-8622},
ResearcherID-Numbers = {Kuleshova, Lyubov Sergeevna/ABE-3322-2021
   Kuleshova, Lyubov Sergeevna/ABA-6008-2020
   Mukhametshin, Vyacheslav Vyacheslavovich/AAM-4164-2020
   },
ORCID-Numbers = {Kuleshova, Lyubov Sergeevna/0000-0003-2975-3666
   Mukhametshin, Vyacheslav Vyacheslavovich/0000-0003-3283-1047
   Sultanov, Shamil/0000-0003-3481-9519},
Unique-ID = {WOS:000774703200006},
}

@article{ WOS:000615081500001,
Author = {Li, Bin and Feng, Feng and Chen, Xiaojie and Cao, Yan},
Title = {Reconfigurable and High-Efficiency Password Recovery Algorithms Based on
   HRCA},
Journal = {IEEE ACCESS},
Year = {2021},
Volume = {9},
Pages = {18085-18111},
Abstract = {Cryptographic algorithms are widely used in information security fields
   such as network protocol authentication and commercial encryption
   software. Password recovery based on the hash algorithm is an important
   means of electronic forensics, encrypted information restoration,
   illegal information filtering, and network security maintenance. The
   traditional password recovery system is based mainly on the CPU and GPU
   and has a low energy efficiency ratio and cracking efficiency and cannot
   meet high-performance computing requirements. To further improve the
   computational efficiency and application flexibility of password
   recovery algorithms, this paper proposes a reconfigurable computing
   kernel design method based on a hybrid reconfigurable computing array
   (HRCA). Through in-depth analysis of the hash algorithm, the basic
   computing kernel set is extracted, and the combination design is carried
   out from the unit kernel, interconnection and storage structure to
   reconstruct the hash algorithm to match the application with the
   appropriate structure. Second, combined with the pipeline technology,
   the full pipeline hash and high-speed password attack algorithms are
   optimized and implemented to meet the needs of high-performance
   computing. Finally, an advanced computing kernel library is established,
   and the combination of a computing kernel map from the control and
   communication levels to achieve multidimensional reconfigurable
   computing and an overall placement strategy is used to make full use of
   the chip resources to improve computational efficiency. The experimental
   results and analysis show that compared with traditional CPU and GPU
   methods, the password recovery algorithm designed in this paper has the
   highest cracking speeds at 78.22 times and 2.65 times that of the CPU
   and GPU, respectively, and the highest energy efficiency ratio is 25.88
   times and 3.16 times that of the CPU and GPU, respectively. Furthermore,
   the recovery efficiency has been significantly improved and meets the
   requirements of high-performance password recovery computing.},
DOI = {10.1109/ACCESS.2021.3053068},
ISSN = {2169-3536},
ORCID-Numbers = {Chen, Xiaojie/0000-0002-8955-7133
   Li, Bin/0000-0003-3455-4901},
Unique-ID = {WOS:000615081500001},
}

@article{ WOS:000614604900015,
Author = {Medvedeva, Oksana N. and Penenko, Valeriya D.},
Title = {ROUTES OF LAYING GAS SUPPLY SYSTEM PIPELINE},
Journal = {BULLETIN OF THE TOMSK POLYTECHNIC UNIVERSITY-GEO ASSETS ENGINEERING},
Year = {2021},
Volume = {332},
Number = {1},
Pages = {153-163},
Abstract = {Relevance. Designing a gas pipeline route is an important step in
   planning and building gas pipeline. There are various methods,
   dependencies and algorithms characterizing the functional component of
   the gas pipelines, various methods of laying. The most important part in
   the design process is the choice of an optimal route, which application
   effectiveness will be high and at the same time the project will have
   minimal capital investments in construction. Despite many proposed
   approaches, at present there is no a uniform method for determining the
   optimal costs for construction and reconstruction of the gas
   distribution networks.
   The aim of the research is to study assumes solution of the problem of
   choosing the optimal pipeline route.
   Methods. Our program is based on wave algorithms, or search by width. It
   means the search strategy, used in solving the problem, does not use
   additional information, but only that presented in the definition. The
   essence of this method is development successors and distinguish between
   a target and a non-target state.
   Results and conclusions. Submitted description of a software package
   allows calculating the position of a linear structure on the area, based
   on a one-factor approach, linking the cost criterion to the complexity
   of developing different groups of soil.},
DOI = {10.18799/24131830/2021/1/3008},
ISSN = {2500-1019},
EISSN = {2413-1830},
ResearcherID-Numbers = {Медведева, Оксана/S-8351-2016},
ORCID-Numbers = {Медведева, Оксана/0000-0002-0861-0335},
Unique-ID = {WOS:000614604900015},
}

@inproceedings{ WOS:000668942500006,
Author = {Nestor, Natalia and Belej, Olexander and Tomyuk, Vasyl},
Book-Group-Author = {IEEE},
Title = {Application of Hybridization Methods to Detect Network Attacks in
   Wireless Sensor Networks},
Booktitle = {2021 IEEE 16TH INTERNATIONAL CONFERENCE ON THE EXPERIENCE OF DESIGNING
   AND APPLICATION OF CAD SYSTEMS (CADSM)},
Series = {Experience of Designing and Application of CAD Systems in
   Microelectronics-CADSM},
Year = {2021},
Note = {IEEE 16th International Conference on the Experience of Designing and
   Application of CAD Systems (CADSM), Lviv, UKRAINE, FEB 22-26, 2021},
Organization = {IEEE; IEEE Ukraine Sect W MTT ED AP EP SSC Soc Joint Chapter; Lviv
   Polytechn Natl Univ, Inst Comp Sci \& Informat Technologies, Dept Comp
   Aided Design; Lodz Univ Technol, Dept Microelectron \& Comp Sci; IEEE
   Ukraine Sect},
Abstract = {The study proposes the structure of a network distributed attack
   detection system is two-tier: the first level provides primary analysis
   of individual packets and network connections by signature analysis, the
   second level processes aggregated network data streams using adaptive
   classifiers. The method of detection of anomalous network connections on
   the basis of hybridization of methods of computational intelligence for
   construction of multilevel schemes with arbitrary embedding of
   classifiers in each other and their passive connection in the process of
   analysis of the input vector is developed in the article. As a result of
   the signature analysis, it was found that the combination of detectors
   experimentally increased the detection of attacks by 1.28\%. Software
   implementation of the proposed technique in the form of basic
   intelligent classifiers of network attacks can be used separately from
   the attack detection system in addressing the classification of attacks.},
DOI = {10.1109/CADSM52681.2021.9385215},
ISSN = {2572-7583},
EISSN = {2572-7591},
ISBN = {978-1-6654-3894-0},
ORCID-Numbers = {Tomyuk, Vasyl/0000-0003-4999-7019
   Nestor, Nataliia/0000-0003-4391-2563},
Unique-ID = {WOS:000668942500006},
}

@inproceedings{ WOS:000722628800004,
Author = {Nielsen, Benjamin Barslev and Torp, Martin Toldam and Moller, Anders},
Editor = {Cadar, C and Zhang, X},
Title = {Modular Call Graph Construction for Security Scanning of Node.js
   Applications},
Booktitle = {ISSTA `21: PROCEEDINGS OF THE 30TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM
   ON SOFTWARE TESTING AND ANALYSIS},
Year = {2021},
Pages = {29-41},
Note = {30th ACM SIGSOFT International Symposium on Software Testing and
   Analysis (ISSTA), ELECTR NETWORK, JUL 11-17, 2021},
Organization = {ACM SIGSOFT; Assoc Comp Machinery; Google; AWS; Dragon Testing;
   Microsoft Res; Facebook; KBR; NASA; JetBeans},
Abstract = {Most of the code in typical Node.js applications comes from third-party
   libraries that consist of a large number of interdependent modules.
   Because of the dynamic features of JavaScript, it is difficult to obtain
   detailed information about the module dependencies, which is vital for
   reasoning about the potential consequences of security vulnerabilities
   in libraries, and for many other software development tasks. The
   underlying challenge is how to construct precise call graphs that
   capture the connectivity between functions in the modules.
   In this work we present a novel approach to call graph construction for
   Node.js applications that is modular, taking into account the modular
   structure of Node.js applications, and sufficiently accurate and
   efficient to be practically useful. We demonstrate experimentally that
   the constructed call graphs are useful for security scanning, reducing
   the number of false positives by 81\% compared to npm audit and with
   zero false negatives. Compared to js-callgraph, the call graph
   construction is significantly more accurate and efficient. The
   experiments also show that the analysis time is reduced substantially
   when reusing modular call graphs.},
DOI = {10.1145/3460319.3464836},
ISBN = {978-1-4503-8459-9},
ORCID-Numbers = {Moller, Anders/0000-0003-1333-2314},
Unique-ID = {WOS:000722628800004},
}

@article{ WOS:000656707300003,
Author = {Ogundele, Sunday Oladunjoye and Ajibare, Adeola Olubunmi and Amisu,
   Mumuni A. and Dada, Akinola Olusola},
Title = {Prescription pattern and blood pressure control among patients on
   antihypertensive medications attending a tertiary hospital in Lagos: A
   cross-sectional study},
Journal = {NIGERIAN JOURNAL OF BASIC AND CLINICAL SCIENCES},
Year = {2021},
Volume = {18},
Number = {1},
Pages = {9-13},
Month = {JAN-JUN},
Abstract = {Context: Hypertension is a disease of global public health importance
   affecting an estimated 1 billion people worldwide. Hypertension accounts
   for about 10.4 million deaths in 2017. Many guidelines are available for
   the management of hypertension, but despite this, there are wide
   variations in physicians' choice of antihypertensive medications for
   blood pressure (BP) control. Aims: We reviewed the prescription pattern
   of antihypertensive medication and the level of BP control in patients
   with hypertension. Settings and Design: This is a cross-sectional study
   among hypertensive patients attending follow-up clinics of a tertiary
   hospital. Methodology: Structured questionnaires were used to collect
   information on the antihypertensive medications and clinical profiles of
   the study participants. Questionnaires were administered to participants
   during the attendance of the follow-up clinic to capture information
   relevant to the study. Information retrieved from patients includes
   details about their baseline demographic characteristics, clinical
   history, antihypertensive drug history and blood BP recordings. The
   study protocol was reviewed and approved by the institutional ethics
   committee. Statistical Analysis: Data were analysed using Microsoft
   Excel 2003 and the Statistical Package for the Social Sciences version
   22.0 software. Results: A total of 489 hypertensive patients took part
   in the study. The study found that the rate of BP control was 44.6\%
   among the participants. Calcium channel blockers (CCBs) are the most
   common antihypertensive medication prescribed in our clinics. There was
   no significant relationship between how long a patient had been
   attending the follow-up clinic and the level of BP control in patients
   who have attended the clinic for a minimum period of 3 months.
   Conclusion: BP control was less than optimal in more than half of the
   participants, and the most commonly prescribed antihypertensive
   medication were CCBs.},
DOI = {10.4103/njbcs.njbcs\_12\_20},
ISSN = {0331-8540},
EISSN = {2320-477X},
ResearcherID-Numbers = {Ajibare, Adeola Olubunmi/AAE-7250-2021},
ORCID-Numbers = {Ajibare, Adeola Olubunmi/0000-0001-6779-6216},
Unique-ID = {WOS:000656707300003},
}

@inproceedings{ WOS:000723843000104,
Author = {Sajkowski, Maciej and Frania, Krystian and Stenzel, Tomasz and Goral,
   Mateusz},
Book-Group-Author = {IEEE},
Title = {Image Processing Based Control System for Wheeled Mobile Robots},
Booktitle = {2021 IEEE 19TH INTERNATIONAL POWER ELECTRONICS AND MOTION CONTROL
   CONFERENCE (PEMC)},
Series = {IEEE International Power Electronics and Motion Control Conference IPEMC},
Year = {2021},
Pages = {731-736},
Note = {IEEE 19th International Power Electronics and Motion Control Conference
   (PEMC), Silesian Univ Technol, ELECTR NETWORK, APR 25-29, 2021},
Organization = {Inst Elect \& Elect Engineers; IEEE Ind Elect Soc; IEEE Ind Applicat Soc},
Abstract = {The paper presents the design and tests of a wheeled mobile robot. The
   construction of the machine is based on a differential drive. A
   three-layer structure of the robot control system has been developed\_
   The lowest layer is based on an 8-bit microcontroller and its purpose is
   to control motors and sensors of the robot. The medium layer is based on
   a 32-hit ARM Cortex M4 microcontroller. There has been implemented an
   x86 architecture computational unit as the main part of the highest
   layer of the control system. A control software developed for this
   computer is responsible tar the acquisition and processing of an image
   stream from the camera mounted on the robot board. The software has been
   implemented using Aforge.NET library. Selected image processing filters
   have been tested in order to enable object tracking and gesture
   recognition in the mobile robot control system. Finally, a power
   consumption of the computational unit operating in several developed
   control modes has been measured.},
DOI = {10.1109/PEMC48073.2021.9432545},
ISBN = {978-1-7281-5660-6},
Unique-ID = {WOS:000723843000104},
}

@inproceedings{ WOS:000775555100009,
Author = {Taieb, Salma Hadj and Loukil, Taicir Moalla and El Mhamedi, Abderrahman
   and Hani, Yasmina},
Editor = {Dolgui, A and Bernard, A and Lemoine, D and VonCieminski, G and Romero, D},
Title = {Two Variants of Bi-objective Vehicle Routing Problem in Home
   (Health)-Care Fields},
Booktitle = {ADVANCES IN PRODUCTION MANAGEMENT SYSTEMS: ARTIFICIAL INTELLIGENCE FOR
   SUSTAINABLE AND RESILIENT PRODUCTION SYSTEMS, PT V},
Series = {IFIP Advances in Information and Communication Technology},
Year = {2021},
Volume = {634},
Pages = {77-86},
Note = {International-Federation-of-Information-Processing-Working-Group-5.7
   (IFIP WG 5.7) International Conference on Advances in Production
   Management Systems (APMS), ELECTR NETWORK, SEP 05-09, 2021},
Organization = {Int Fed Informat Proc Working Grp 5 7 Advances Prod Management Syst; IMT
   Atlantique, Campus Nantes; Univ Nantes, Centrale Nantes; Rennes Business
   Sch; Audecia Business Sch},
Abstract = {In this research paper, we consider two bi-objectives models, the
   Vehicle Routing Problem Preferences with Time windows, temporal
   dependencies, multiple structures and multiple specialties
   ``VRPPTW-TD-2MS{''} and the Vehicle Routing Problem Balance with Time
   windows, temporal dependencies, multiple structures and multiple
   specialties ``VRPBTW-TD-2MS{''}, in Home (Health)-Care ``HHC{''} service
   field. In this context, The HHC services is mostly assigned to certain
   persons such as people who have a chronic disease or who have difficulty
   leaving their houses by offering them medical, social, and/or
   medico-social services like nursing, dressing, bathing, toileting,
   feeding, shopping, hygiene assistance, etc. These several services are
   provided by different public and private home care structures. These
   structures work together in coordination to provide high HHC quality for
   patients. Therefore, patients are not in a single structure anymore, but
   rather at home and need to be served by caregivers. As a result, the
   problem is no longer limited to the care organization, but also to the
   construction of optimal tours for the caregivers, the delivery of
   medicines, and the equipment needed for care. According to our
   knowledge, our work is different from those made in the literature. Our
   problem consists to determine the caregivers' tours while optimizing
   bi-objective function, which aims to minimize the total traveling time
   and the total negative preference or the maximal difference workload of
   caregivers under set of constraints such as time window,
   synchronization, precedence and disjunction constraints with multiple
   structures and multiple caregivers' skills. A benchmark of instances is
   considered from literature. Cplex (Optimization Software package) is
   used to solve our problems to obtain efficient solutions.},
DOI = {10.1007/978-3-030-85914-5\_9},
ISSN = {1868-4238},
EISSN = {1868-422X},
ISBN = {978-3-030-85914-5; 978-3-030-85913-8},
ORCID-Numbers = {El mhamedi, Abderrahman/0000-0002-1334-6057},
Unique-ID = {WOS:000775555100009},
}

@article{ WOS:000620610400010,
Author = {Timokhin, P. Yu and Mikhaylyuk, V, M. and Vozhegov, E. M. and Panteley,
   K. D.},
Title = {Technology and Methods for Deferred Synthesis of 4K Stereo Clips for
   Complex Dynamic Virtual Scenes},
Journal = {PROGRAMMING AND COMPUTER SOFTWARE},
Year = {2021},
Volume = {47},
Number = {1},
Pages = {67-75},
Month = {JAN},
Abstract = {We consider the task of capturing the result of a researcher-driven
   stereo visualization of a complex dynamic virtual scene into a video
   sequence of stereo pairs (stereo clip) of ultrahigh resolution. An
   efficient technology of deferred synthesis of stereo clips is proposed.
   It allows one to create stereo clips without interfering with real-time
   visualization. The technology includes real-time construction of
   visualization scenario and offline transformation of the scenario into a
   stereo clip. Methods for implementing these stages for the task of
   stereo visualization of the saturation isosurface of displacing fluid
   are considered. For this purpose, a special file format << scr >> for
   the visualization scenario is developed on the basis of chunk data
   structures. This format provides a compact representation of neighboring
   repeated frames. The scenario file is transformed into a sequence of 4K
   stereo pairs by means of the offscreen rendering technology of the
   virtual scene, and stereo pairs are added to the stereo clip using a
   number of open-source FFmpeg libraries designed for processing digital
   video content. The generated stereo clip is placed within an MP4
   container, and the video compressing standard H.264 is used. The
   proposed technologies and methods of 4K stereo clip deferred synthesis
   are implemented in a software designed for visualizing simulation
   results of the unstable displacement of oil from porous media. Using
   this software, a 4K stereo clip is created, which illustrates the
   evolution of the saturation isosurface during the process of unstable
   oil displacement. Testing confirmed the validity of the solution. The
   software can be used in virtual laboratories, for designing virtual
   environment systems and scientific visualization systems, in educational
   applications etc.},
DOI = {10.1134/S0361768820070063},
ISSN = {0361-7688},
EISSN = {1608-3261},
ResearcherID-Numbers = {Timokhin, Petr/ABA-4770-2020},
ORCID-Numbers = {Timokhin, Petr/0000-0002-0718-1436},
Unique-ID = {WOS:000620610400010},
}

@inproceedings{ WOS:000844009000023,
Author = {Vorisek, Jan and Patzak, Borek and Dvorakova, Edita and Rypl, Daniel},
Editor = {Padevet, P},
Title = {AUTOMATED BIM ENTITY RECONSTRUCTION FROM UNSTRUCTURED 3D POINTCLOUDS},
Booktitle = {NANO \& MACRO MECHANICS (NMM 2020)},
Series = {Acta Polytechnica CTU Proceedings},
Year = {2021},
Volume = {30},
Pages = {126-130},
Note = {11th Conference on Nano and Macro Mechanics (NMM), Prague, CZECH
   REPUBLIC, SEP 17, 2020},
Organization = {Czech Tech Univ, Fac Civil Engn, Dept Mech},
Abstract = {Laser scanning is used widely in architecture and construction to
   document existing buildings by providing accurate data for creating a 3D
   model. The output is a set of data points in space, so-called point
   cloud. While point clouds can be directly rendered and inspected, they
   do not hold any semantics. Typically, engineers manually obtain floor
   plans, structural models, or the whole BIM model, which is a very
   time-consuming task for large building projects.
   In this contribution, we present the design and concept of a
   Pointayud2RIM library {[}1]. It provides a set of algorithms for
   automated or user assisted detection of fundamental entities from
   scanned point cloud data sets, such as floors, rooms, walls, and
   openings, and identification of the mutual relationships between them.
   The entity detection is based on a reasonable degree of human
   interaction (i.e., expected wall thickness). The results reside in a
   platform-agnostic JSON database allowing future integration into any
   existing BIM software.},
DOI = {10.14311/APP.2021.30.0126},
ISSN = {2336-5382},
ISBN = {978-80-01-06840-3},
ResearcherID-Numbers = {Patzák, Bořek/HJI-2027-2023},
ORCID-Numbers = {Patzák, Bořek/0000-0002-3373-9333},
Unique-ID = {WOS:000844009000023},
}

@inproceedings{ WOS:000696765400405,
Author = {Wang, Hao and Cheung, Sen-Ching S.},
Book-Group-Author = {IEEE},
Title = {Augmented Reality Circuit Learning},
Booktitle = {2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)},
Series = {IEEE International Symposium on Circuits and Systems},
Year = {2021},
Note = {IEEE International Symposium on Circuits and Systems (IEEE ISCAS),
   Daegu, SOUTH KOREA, MAY 22-28, 2021},
Organization = {IEEE},
Abstract = {Building electronic circuits is one of the most common hands-on
   activities in learning STEM subjects. Beginning students often find it
   difficult to translate a circuit schematic into the construction of the
   physical circuit on a breadboard. In this paper, we describe an
   Augmented Reality circuit learning software that can provide students
   with step-by-step instructions by placing virtual circuit components on
   a physical breadboard. We propose a novel image processing pipeline that
   can robustly identify the planar structure of a breadboard, even in the
   presence of occluding circuit components on the breadboard. Using a
   commercially available library of 3D circuit component models, the
   estimated 3D structure of the breadboard allows us to render arbitrary
   circuit components on it in real time. Experimental results demonstrate
   that our algorithms are accurate and can produce realistic-looking
   virtual circuits.},
DOI = {10.1109/ISCAS51556.2021.9401464},
ISSN = {0271-4302},
ISBN = {978-1-7281-9201-7},
Unique-ID = {WOS:000696765400405},
}

@article{ WOS:000659332800016,
Author = {Zeylik, Boris and Arshamov, Yalkunzhan and Baratov, Refat and
   Bekbotayeva, Alma},
Title = {New technology for mineral deposits prediction to identify prospective
   areas in the Zhezkazgan ore region},
Journal = {MINING OF MINERAL DEPOSITS},
Year = {2021},
Volume = {15},
Number = {2},
Pages = {134-142},
Abstract = {Purpose. Exploration and predicting the prospective areas in the
   Zhezkazgan ore region to set up detailed prospecting and evaluation
   works using new integrated technologies of prediction constructions in
   the mineral deposits geology.
   Methods. An integrated methodological approach is used, including
   methods for deciphering the Earth's remote sensing (ERS) data, the use
   of geophysical data and methods of analogy and actualism. All
   constructions are made in accordance with the principles of
   shock-explosive tectonics (SET). Prediction constructions are started
   with the selection of remote sensing data for the studied region and
   interpretation based on the processing of radar satellite images
   obtained from the Radarsat-1 satellite. The radar satellite images are
   processed in the Erdas Imagine software package.
   Findings. New local prospective areas have been identified, within which
   it is expected to discover the deposits. Their reserves are to replenish
   the depleted ore base in the Zhezkazgan region. Area of the gravity
   maximum 1 (the Near), considered to be the most promising, is located in
   close proximity to the city of Zhezkazgan; area of the gravity maximum 2
   (the Middle); area of the gravity maximum 3 (the Distant-Tabylga); area
   of the gravity maximum 6 (the Central). A prospective area has been also
   revealed, overlaid by a loose sediment cover and located inside the
   Terekty ring structure, as well as the area of a thick stratum of
   pyritized grey sandstones, which is adjacent to the Sh-2 well drilled to
   the south of the Zhezkazgan field.
   Originality. The use of a new prediction technology, in contrast to the
   known ones, is conditioned by the widespread use of the latest remote
   information from satellite images, which increases the accuracy of
   identifying the prospective areas of fields.
   Practical implications. The new technology for predicting mineral
   deposits makes it possible to significantly reduce the areas exposed to
   priority prospecting, which provides significant cost savings.},
DOI = {10.33271/mining15.02.134},
ISSN = {2415-3435},
EISSN = {2415-3443},
ResearcherID-Numbers = {Zeilik, Boris/AAH-5061-2021
   Bekbotayeva, Alma/P-3302-2017
   Arshamov, Yalkunzhan/X-7249-2018
   },
ORCID-Numbers = {Zeilik, Boris/0000-0003-0307-1018
   Bekbotayeva, Alma/0000-0003-2670-3604
   Arshamov, Yalkunzhan/0000-0003-0527-6797
   Baratov, Refat/0000-0002-0627-1536},
Unique-ID = {WOS:000659332800016},
}

@article{ WOS:000596104300001,
Author = {Savary, Paul and Foltete, Jean-Christophe and Moal, Herve and Vuidel,
   Gilles and Garnier, Stephane},
Title = {graph4lg: A package for constructing and analysing graphs for landscape
   genetics in R},
Journal = {METHODS IN ECOLOGY AND EVOLUTION},
Year = {2021},
Volume = {12},
Number = {3},
Pages = {539-547},
Month = {MAR},
Abstract = {In landscape genetics, habitat connectivity and population genetic
   structure have been analysed using graph-theoretic approaches to
   understand how landscape features influence demography (i.e. dispersal
   and population size).
   Despite substantial advances in enhancing both genetic and landscape
   graph use, a software tool bringing together a large range of
   construction and analysis parameters for these two types of graphs was
   lacking in the landscape genetic toolbox. Moreover, although these two
   types of graphs appear complementary for answering landscape genetic
   questions, methods for comparing them have not been forthcoming.
   We have developed an R package to improve and encourage the use of these
   graphs. It includes functions for converting and importing genetic data
   and for genetic distance computing. It also implements time-efficient
   geodesic and cost-distance calculations from spatial data. A large range
   of parameters can be used to create genetic and landscape graphs from
   these data, including several graph pruning methods. We made available
   to R users the command-line facilitaties of Graphab software to easily
   model landscape graphs in R. The package functions perform preliminary
   analysis to adapt methodological choices to research questions.
   Landscape and genetic graphs created can be analysed with node-level
   metrics as well as link-level and modularity analyses. Users can compare
   and visualise these graphs and export them to shapefiles to facilitate
   interpretation and subsequent analyses.
   graph4lg contributes to expanding landscape and genetic graph potential
   for analysing ecological connectivity while encouraging further
   investigations on methodological implications related to these tools.},
DOI = {10.1111/2041-210X.13530},
EarlyAccessDate = {DEC 2020},
ISSN = {2041-210X},
EISSN = {2041-2096},
ORCID-Numbers = {Foltete, Jean-Christophe/0000-0003-4864-5660
   Garnier, Stephane/0000-0002-8938-9440
   Savary, Paul/0000-0002-2104-9941},
Unique-ID = {WOS:000596104300001},
}

@article{ WOS:000595844300001,
Author = {Lamptey, Theophilus and Owusu-Manu, De-Graft and Acheampong, Alex and
   Adesi, Michael and Ghansah, Frank Ato},
Title = {A framework for the adoption of green business models in the Ghanaian
   construction industry},
Journal = {SMART AND SUSTAINABLE BUILT ENVIRONMENT},
Year = {2021},
Volume = {10},
Number = {3, SI},
Pages = {536-553},
Month = {NOV 10},
Abstract = {Purpose Despite the amount of considerable investigations on business
   models, much studies have not been undertaken in the construction
   industry emphasising the adoption of green business models to drive
   sustainable construction. Construction activities continue to increase
   the carbon footprint and eject contaminated materials into the
   ecological environment with dire consequences for economic and social
   sustainability. As a result of the adverse impacts of construction
   activities, it is necessary for construction firms to rethink their
   approach to the use of conventional business models. The purpose of this
   study is to explore a framework for the adoption of green business
   models to drive sustainability in the construction industry of Ghana.
   Design/methodology/approach This research is exploratory due to its
   focus on emerging economies in which there is a perceptible gap in the
   adoption of green business models. As a result of this, this paper is
   entrenched in the interpretivist philosophical stance, which led to the
   adoption of the qualitative approach. Semi-structured interviews were
   undertaken involving 13 senior managers of construction firms. A
   thematic analysis was used with the aid of qualitative data analysis
   computer software package to code the interview transcripts. Findings
   The results demonstrate the six definitions of green business models
   among the managers of construction firms. The study also shows the need
   for developing green business models to address the issues of
   circularity and sustainability goals to reduce carbon footprints in the
   construction industry. Similarly, the paper found various sources of
   information to drive the awareness, understanding and adoption of the
   components for green business models. These sources include
   international conferences and training workshops on green business
   models. Finally, the study presents a framework that integrates the
   building information modelling (BIM) and the Internet of things (IoT)
   into the components for green business models adoption in construction
   firms. Research limitations/implications There is a need to use the
   quantitative approach to undertake further empirical studies, as this
   paper focuses mostly on the qualitative approach to ascertain the nature
   of the relationship between green business model and the various
   components of the circular economy in the construction industry.
   Originality/value The study contributes to the existing knowledge on
   green business models by demonstrating six key pillars of green business
   models by the inclusion of digital technologies such as BIM and IoT,
   which hitherto this investigation have not been considered in the
   adoption of green business models in the construction industry. This
   study extends the existing knowledge on green business models, which has
   the potential to increase the awareness and understanding of
   practitioners and managers of construction firms.},
DOI = {10.1108/SASBE-08-2020-0130},
EarlyAccessDate = {DEC 2020},
ISSN = {2046-6099},
EISSN = {2046-6102},
Unique-ID = {WOS:000595844300001},
}

@article{ WOS:000683353100017,
Author = {Cauley, Kate},
Title = {Banned Books behind Bars: Prototyping a Data Repository to Combat
   Arbitrary Censorship Practices in US Prisons},
Journal = {HUMANITIES-BASEL},
Year = {2020},
Volume = {9},
Number = {4},
Month = {DEC},
Abstract = {``Banned Books Behind Bars{''} is a social justice project that aims to
   shed light on the complex problem of information access in prison and to
   explore potential prototypes for possible solutions to some of these
   obstacles, in particular access to books and printed information. The
   United States is home to five percent of the world's population but a
   staggering twenty-five percent of the world's total prisoners. For many
   incarcerated individuals, access to information is a struggle:
   censorship, book banning, and lack of adequate library facilities or
   collections are common. Over the course of conducting preliminary
   research, this project evolved through the research process of ideation.
   Through the participatory action research method, qualitative interviews
   with volunteers from banned books organizations helped to identify
   potential digital tools meant to aid in the fight against the First
   Amendment violations that incarcerated individuals face daily.
   Furthermore, the interviews clarified that the first step toward
   creating an impactful digital project involves converting various forms
   of unstructured data, including newspaper articles, prison censorship
   forms, and state published banned book lists, into structured data.
   Through this discovery, ``Banned Books Behind Bars{''} became an
   endeavor to standardize practices of data aggregation amongst banned
   books organizations throughout the country. Gathering concrete data
   about the practice of banning books within prisons requires an elevated
   level of transparency. Incarcerated individuals, their families, and
   prison reform activists need a platform for reporting data on censorship
   practices, and, ultimately, for bringing awareness to the arbitrary
   application of censorship guidelines within the complex world of
   incarceration. The final prototype is a digital repository, created with
   Airtable software, which offers authoritative dataset consolidation for
   activists and organizations working to deliver books to prisoners.},
DOI = {10.3390/h9040131},
Article-Number = {131},
EISSN = {2076-0787},
Unique-ID = {WOS:000683353100017},
}

@article{ WOS:000597110600001,
Author = {Diara, Filippo and Rinaudo, Fulvio},
Title = {IFC Classification for FOSS HBIM: Open Issues and a Schema Proposal for
   Cultural Heritage Assets},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {23},
Month = {DEC},
Abstract = {The IFC (Industry Foundation Classes) open format has been developed by
   BuildingSMART and regularized through ISO standards. It has been
   implemented into a BIM (Building Information Modeling) informative
   system for the AEC industry (Architecture Engineering and Construction).
   The IFC format has changed interoperability processes concerning
   architectural and technical entities in a semantic way. However, because
   this standard open format was specifically designed for the modern AEC
   industry, it may not cater to the demands of cultural heritage assets.
   Since IFC classification is fundamental for informative systems, it
   should become a standard also concerning heritage assets, even if
   nowadays there is no regularized IFC classification for historical
   existing buildings. Specific cultural heritage peculiarities therefore
   need semantic classification based on historical asset families. For
   this reason, this work is based on a proposal and experimental IFC
   classification implemented inside an HBIM open source software
   (FreeCAD), whereby limitations of IFC standards can be overcome thanks
   to the freedom of access to libraries and codes. Moreover, this work is
   based on IFC objects management outside the platform for
   interoperability purposes.},
DOI = {10.3390/app10238320},
Article-Number = {8320},
EISSN = {2076-3417},
ResearcherID-Numbers = {Rinaudo, Fulvio/M-5926-2019
   },
ORCID-Numbers = {Rinaudo, Fulvio/0000-0002-9592-1341
   DIARA, FILIPPO/0000-0002-7491-7079},
Unique-ID = {WOS:000597110600001},
}

@article{ WOS:000615095400010,
Author = {Ma, Chang-li and Cheng, He and Zuo, Tai-sen and Jiao, Gui-sheng and Han,
   Ze-hua and Qin, Hong},
Title = {NeuDATool: an Open Source Neutron Data Analysis Tools, Supporting GPU
   Hardware Acceleration, and across-Computer Cluster Nodes Parallel},
Journal = {CHINESE JOURNAL OF CHEMICAL PHYSICS},
Year = {2020},
Volume = {33},
Number = {6},
Pages = {727-732},
Month = {DEC},
Abstract = {Empirical potential structure refinement is a neutron scattering data
   analysis algorithm and a software package. It was developed by the
   disordered materials group in the British spallation neutron source
   (ISIS) in 1980s, and aims to construct the most-probable atomic
   structures of disordered materials in the field of chemical physics. It
   has been extensively used during the past decades, and has generated
   reliable results. However, it implements a shared-memory architecture
   with open multi-processing (OpenMP). With the extensive construction of
   supercomputer clusters and the widespread use of graphics processing
   unit (GPU) acceleration technology, it is now possible to rebuild the
   EPSR with these techniques in the effort to improve its calculation
   speed. In this study, an open source framework NeuDATool is proposed. It
   is programmed in the object-oriented language C++, can be paralleled
   across nodes within a computer cluster, and supports GPU acceleration.
   The performance of NeuDATool has been tested with water and amorphous
   silica neutron scattering data. The test shows that the software can
   reconstruct the correct microstructure of the samples, and the
   calculation speed with GPU acceleration can increase by more than 400
   times, compared with CPU serial algorithm at a simulation box that has
   about 100 thousand atoms. NeuDATool provides another choice to implement
   simulation in the (neutron) diffraction community, especially for
   experts who are familiar with C++ programming and want to define
   specific algorithms for their analysis.},
DOI = {10.1063/1674-0068/cjcp2005077},
ISSN = {1674-0068},
EISSN = {2327-2244},
Unique-ID = {WOS:000615095400010},
}

@article{ WOS:000755785100010,
Author = {Stennikov, V. A. and Edeleva, O. A. and Barakhtenko, E. A. and Sokolov,
   D. V.},
Title = {Methodological Approach to the Integrated Optimization of the
   Heat-Source Structure in the Problems of Developing Heat-Supply Systems},
Journal = {THERMAL ENGINEERING},
Year = {2020},
Volume = {67},
Number = {12},
Pages = {935-946},
Month = {DEC},
Abstract = {The question on the optimal development of the heat sources in the
   developing heat-supply systems (HSSs) is considered. The current state
   of research in the field in question is analyzed. It is noted that this
   issue has aroused increased interest in recent years. The description
   and mathematical formulation of the problem of optimizing the structure
   of the heat sources in the developing HSSs are provided. The best
   distribution of the load between the former is determined and the choice
   of the optimal parameters of the heat-supply networks under permissible
   technical, economic, and ecological restrictions is made. For the first
   time, the problem has been formulated and implemented using the method
   of directed search for possible variants of the heat source's power and
   equipment content in a preplanned redundant heat-source configuration
   that was previously used to search for the best configuration of the
   heat-supply network. To devise redundant configurations and subsequently
   calculate them, methodological approaches oriented towards the
   construction of P-graphs and energy hubs that have been developed in
   recent years can be successfully applied. Every heat-source structure
   variant outlined within the framework of the redundant configuration
   competes with other variants not only in terms of the minimum costs but
   also in terms of the minimum impact on the environment. The proposed
   methodology was implemented in the form of a software package that
   enables the user to interactively communicate with the digital model of
   the system to obtain a solution for the development and reconstruction
   of the HSS under investigation. The practical application of the
   methodological and computational tools to solve the problem of the
   development of heat-supply systems is exemplified by the configuration
   of the heat-supply system of the settlement of Magistralnyi, Irkutsk
   region.},
DOI = {10.1134/S0040601520120083},
ISSN = {0040-6015},
EISSN = {1555-6301},
ResearcherID-Numbers = {Sokolov, Dmitry/K-6737-2018},
ORCID-Numbers = {Sokolov, Dmitry/0000-0002-4068-7770},
Unique-ID = {WOS:000755785100010},
}

@article{ WOS:000579783600008,
Author = {Taylor, Ned Thaddeus and Davies, Francis Huw and Rudkin, Isiah Edward
   Mikel and Price, Conor Jason and Chan, Tsz Hin and Hepplestone, Steven
   Paul},
Title = {ARTEMIS: Ab initio restructuring tool enabling the modelling of
   interface structures},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2020},
Volume = {257},
Month = {DEC},
Abstract = {A program, ARTEMIS, has been developed for the study of interface
   structures. This software allows for the generation of interfaces by
   identifying lattice matches between two parent crystal structures. To
   allow for further exploration of the energetic space of the interface,
   multiple surface terminations parallel to the Miller plane, interface
   alignments and intermixings are used to generate sets of potential
   interfaces for each lattice match. These interface structures can then
   be used in atomic simulations to determine the most energetically
   favourable interface. The software reported here can help to both
   drastically reduce the work of generating and exploring interfaces, and
   aid in understanding of how the interface structure influences the
   subsequent properties. Using several test cases, we demonstrate how
   ARTEMIS can both identify the location of an interface in existing
   structures, and also predict an optimum interface separation based upon
   the parents' atomic structures, which aims to accelerate and inform the
   study of interface science.
   Program summary
   Program Title: ARTEMIS
   CPC Library link to program files:
   http://dx.doi.org/10.17632/5bcrh67xty.1
   Developer's repository link: http://www.artemis-materials.co.uk/
   Licensing provisions: CC BY NC 3.0
   Programming language: Fortran 2003 Nature of problem: Construction and
   identification of the interface between any two crystals. Complicating
   factors include the choice of Miller planes, alignment of the two
   crystals and potential intermixing of them.
   Solution method: This problem is tackled by generating sets of interface
   structures that allow the user to explore the energy space using atomic
   simulations in order to identify the most favourable interface to form
   between two such crystals.
   Additional comments: The source code and working examples can be found
   in the compressed file obtainable from
   http://www.artemis-materials.co.uk/. The code has been tested and
   developed using the GNU 7.2.0 and the Intel 17.0.4 Fortran compilers on
   Unix/Linux operating systems. (C) 2020 The Authors. Published by
   Elsevier B.V.},
DOI = {10.1016/j.cpc.2020.107515},
Article-Number = {107515},
ISSN = {0010-4655},
EISSN = {1879-2944},
ORCID-Numbers = {Price, Conor/0000-0002-1430-3294
   Chan, Tsz Hin/0000-0003-1126-6579
   Davies, Francis/0000-0003-0786-2773
   Rudkin, Isiah/0000-0003-4696-5310
   Hepplestone, Steven/0000-0002-2528-1270
   Taylor, Ned/0000-0002-9134-9712},
Unique-ID = {WOS:000579783600008},
}

@article{ WOS:000661074700045,
Author = {West, Jeffrey and Ma, Yongqian and Kaznatcheev, Artem and Anderson,
   Alexander R. A.},
Title = {IsoMaTrix: a framework to visualize the isoclines of matrix games and
   quantify uncertainty in structured populations},
Journal = {BIOINFORMATICS},
Year = {2020},
Volume = {36},
Number = {22-23},
Pages = {5542-5544},
Month = {DEC 1},
Abstract = {A Summary: Evolutionary game theory describes frequency-dependent
   selection for fixed, heritable strategies in a population of competing
   individuals using a payoff matrix. We present a software package to aid
   in the construction, analysis and visualization of three-strategy matrix
   games. The IsoMaTrix package computes the isoclines (lines of zero
   growth) of matrix games, and facilitates direct comparison of well-mixed
   dynamics to structured populations on a lattice grid. IsoMaTrix computes
   fixed points, phase flow, trajectories, (sub)velocities and uncertainty
   quantification for stochastic effects in spatial matrix games. We
   describe a result obtained via IsoMaTrix's spatial games functionality,
   which shows that the timing of competitive release in a cancer model
   (under continuous treatment) critically depends on the initial spatial
   configuration of the tumor.},
DOI = {10.1093/bioinformatics/btaa1025},
ISSN = {1367-4803},
EISSN = {1460-2059},
ResearcherID-Numbers = {Anderson, Alexander R.A./A-2713-2011
   West, Jeffrey/AAR-3091-2020
   },
ORCID-Numbers = {Anderson, Alexander R.A./0000-0002-2536-4383
   West, Jeffrey/0000-0001-9579-4664
   Kaznatcheev, Artem/0000-0001-8063-2187},
Unique-ID = {WOS:000661074700045},
}

@article{ WOS:000589604000001,
Author = {Kanaani, Hakimeh and Azarmi, Yadollah and Dastmalchi, Siavoush and
   Zarei, Omid and Hamzeh-Mivehroud, Maryam},
Title = {Investigation of intestinal transportation of peptide-displaying
   bacteriophage particles using phage display method},
Journal = {JOURNAL OF PEPTIDE SCIENCE},
Year = {2021},
Volume = {27},
Number = {3},
Month = {MAR},
Abstract = {To investigate whether peptide sequences with specific translocation
   across the gastrointestinal barrier can be identified as drug delivery
   vehicles, in vivo phage display was conducted. For this purpose, a
   random library of 12-mer peptides displayed on M13 bacteriophage was
   orally administered to mice followed by recovery of the phage particles
   from the blood samples after three consecutive biopanning rounds. The
   obtained peptide sequences were analyzed using bioinformatics tools and
   software. The results demonstrated that M13 bacteriophage bearing
   peptides translocate nonspecifically across the mice intestinal mucosal
   barrier deduced from random distribution of amino acids in different
   positions of the identified peptide sequences. The most probable reason
   for entering the phage particles into systemic circulation after oral
   administration of the peptide library can be related to the nanoscale
   nature of their structures which provides a satisfying platform for the
   purpose of designing nanocarriers in pharmaceutical applications.},
DOI = {10.1002/psc.3292},
EarlyAccessDate = {NOV 2020},
ISSN = {1075-2617},
EISSN = {1099-1387},
ResearcherID-Numbers = {Dastmalchi, Siavoush/P-7497-2016
   },
ORCID-Numbers = {Dastmalchi, Siavoush/0000-0001-9427-0770
   Hamzeh-Mivehroud, Maryam/0000-0002-1257-0102},
Unique-ID = {WOS:000589604000001},
}

@article{ WOS:000596863000117,
Author = {Odetti, Angelo and Bruzzone, Gabriele and Altosole, Marco and Viviani,
   Michele and Caccia, Massimo},
Title = {SWAMP, an Autonomous Surface Vehicle expressly designed for extremely
   shallow waters},
Journal = {OCEAN ENGINEERING},
Year = {2020},
Volume = {216},
Month = {NOV 15},
Abstract = {Wetlands, the geographic areas where water meets the earth, are
   ecosystems essential for life and an increasing number of conventions,
   directives and research projects recognise the necessity of protecting
   them. Nevertheless the number, quality and spacial resolution of surveys
   is modest due to the absence of expressly addressed tools. In this paper
   the design, construction and testing of the first prototype of an
   innovative class of Autonomous Surface Vehicles (ASVs) for the extremely
   shallow water and remote areas of wetlands is presented. SWAMP (Shallow
   Water Autonomous Multipurpose Platform) is a full y electric, modular,
   portable, lightweight, and highly-controllable ASV. It is a catamaran,
   equipped with four thrusters azimuth Pump-Jet thrusters that are flush
   with the hull and specifically designed for this vehicle. SWAMP is also
   characterised by small draft soft-foam, unsinkable hull structure with
   high modularity and a flexible hardware/software architecture. Each hull
   can be considered as a single vehicle, being equipped with a full
   navigation, guidance and control package, as well as propulsion, power
   and communication systems. The extreme modularity is also guaranteed
   thanks to the adoption of on-board Wi-Fi communication architecture: the
   two mono-hull vehicles are connected via Wi-Fi as well as all the
   modules aboard each of them.},
DOI = {10.1016/j.oceaneng.2020.108205},
Article-Number = {108205},
ISSN = {0029-8018},
EISSN = {1873-5258},
ResearcherID-Numbers = {Bruzzone, Gabriele/B-5771-2015
   Viviani, Michele/O-9211-2019
   Odetti, Angelo/O-2931-2015
   ALTOSOLE, MARCO/M-6422-2015},
ORCID-Numbers = {Bruzzone, Gabriele/0000-0002-9569-1160
   Viviani, Michele/0000-0001-6212-1586
   Odetti, Angelo/0000-0003-0338-0742
   Caccia, Massimo/0000-0002-4482-4541
   ALTOSOLE, MARCO/0000-0002-7387-7546},
Unique-ID = {WOS:000596863000117},
}

@article{ WOS:000588610000003,
Author = {Obukhov, Artem D. and Krasnyanskiy, Mikhail N.},
Title = {Automated organization of interaction between modules of information
   systems based on neural network data channels},
Journal = {NEURAL COMPUTING \& APPLICATIONS},
Year = {2021},
Volume = {33},
Number = {12},
Pages = {7249-7269},
Month = {JUN},
Abstract = {The automation of the process of information systems construction is an
   important and urgent problem, as it allows reducing the negative
   influence of a person during decision making and developing software,
   releasing additional time and material resources to solve more complex
   and creative problems. Most modern information systems are developed on
   a modular base; therefore, a significant design stage is the
   implementation of links between system components. The purpose of the
   study is to automate the organization of intermodular interaction in
   information systems, which will reduce the complexity, time and cost of
   this implementing process. In order to achieve the result, a method is
   proposed for the organization of interaction between modules of
   information systems based on neural network data channels, realized
   within the general concept of a neural network architecture. The
   structure of neural network channels, the principles of their
   functioning, theoretical substantiation, mathematical and algorithmic
   support and area of application are considered in detail. A
   classification of neural network channels is presented, based on two of
   their characteristics: categories and degrees. As a result of the
   conducted research, the practical implementation of two neural network
   data channels is realized (transmission and adaptation), the structure
   of the program code, the used tools and libraries are analyzed. Based on
   a set of metrics for the complexity of the program code (Halstead,
   Jilb), the estimation of the computational complexity of algorithms,
   time and material costs for implementation, a comparative analysis of
   neural network data transmission channels and adaptation with classical
   approaches in the form of a set of network data transmission protocol
   and the required algorithmic support for data processing is carried out.
   The obtained experimental results confirm the lower complexity of neural
   network channels (reduction by at least 20\% according to Halstead
   metrics and cyclometric complexity), reduction in time (by 12-32\%) and
   cost (by 36-63\%) of implementation and increase in the accuracy of the
   problem solving (by 11.8-15.5\%). This demonstrates the effectiveness of
   using neural network data channels to automate the organization of
   intermodular interaction in information systems.},
DOI = {10.1007/s00521-020-05491-5},
EarlyAccessDate = {NOV 2020},
ISSN = {0941-0643},
EISSN = {1433-3058},
ORCID-Numbers = {Obukhov, Artem/0000-0002-3450-5213},
Unique-ID = {WOS:000588610000003},
}

@article{ WOS:000583044700001,
Author = {Beglaibter, Nahum and Zelekha, Orly and Keinan-Boker, Lital and Sakran,
   Nasser and Mahajna, Ahmad},
Title = {Practices and attitudes of bariatric surgeons in Israel during the first
   phase of the COVID-19 pandemic},
Journal = {ISRAEL JOURNAL OF HEALTH POLICY RESEARCH},
Year = {2020},
Volume = {9},
Number = {1},
Month = {OCT 30},
Abstract = {Introduction Israel ranks very high globally in performing bariatric
   surgery (BS) per capita. In the first phase of the COVID-19 pandemic the
   bariatric surgeons' community faced many concerns and challenges,
   especially in light of a decree issued by the Ministry of Health (MOH)
   on March 22nd, to ban all elective surgery in public hospitals. The aim
   of this study is to portray the practices and attitudes of Israeli
   bariatric surgeons in the first phase of the pandemic. Methods Anonymous
   web-based questionnaire sent to all active bariatric surgeons in Israel.
   Statistical analysis was performed using SAS software package. Results
   53 out of 63 (84\%) active surgeons responded to the survey. 18\%
   practice in the public sector only, 4\% in the private sector only and
   78\% in both sectors. 76\% practice BS for more than 10 years and 68\%
   perform more than 100 procedures a year. Almost all the surgeons (98\%)
   experienced a tremendous decrease in operations. Nevertheless, there
   were substantial differences by sectors. In the public sector, 86\% of
   the surgeons ceased to operate while 14\% did not comply with the
   government's decree. In the public sector 69\% of the surgeons were
   instructed by the administrators to stop operating. The majority of
   surgeons who continued to operate (77\%) changed nothing in the
   indications or contra-indications for surgery. Among the surgeons who
   opted to refrain from operating on special sub-groups, the most frequent
   reasons were pulmonary disease (82\%), age above 60 (64\%), Ischemic
   heart disease (55\%) and living in heavily affected communities. Roughly
   only half (57\%) of the surgeons implemented changes in informed consent
   and operating room (OR) measures, contrary to guidelines and
   recommendations by leading professional societies. When asked about
   future conditions for reestablishing elective procedures, the reply
   frequencies were as follows: no special measures - 40\%; PCR negativity
   - 27\%; IgG positivity - 15\%; waiting until the end of the pandemic-
   9\%. Conclusions We showed in this nation-wide survey that the variance
   between surgeons, regarding present and future reactions to the COVID-19
   pandemic, is high. There were substantial differences between the
   private and the public sectors. Although the instructions given by the
   MOH for the public sector were quite clear, the compliance by surgeons
   and administrators was far from complete. The administrators in the
   public sector, but more so in the private sector were ambiguous in
   instructing staff, leading surgeons to a more ``personal
   non-structured{''} practice in the first phase of the pandemic. These
   facts must be considered by regulators, administrators and surgeons when
   planning for reestablishing elective BS or in case a second wave of the
   pandemic is on its way.},
DOI = {10.1186/s13584-020-00420-2},
Article-Number = {59},
ISSN = {2045-4015},
ORCID-Numbers = {Sakran, Nasser/0000-0002-6658-8981},
Unique-ID = {WOS:000583044700001},
}

@article{ WOS:000586580400001,
Author = {Rodrigues, Fernanda and Antunes, Flavio and Matos, Raquel},
Title = {Safety plugins for risks prevention through design resourcing BIM},
Journal = {CONSTRUCTION INNOVATION-ENGLAND},
Year = {2021},
Volume = {21},
Number = {2},
Pages = {244-258},
Month = {MAR 20},
Abstract = {Purpose
   The use of building information modelling (BIM) methodology has been
   increasing in the architecture, engineering, construction and operation
   sector, driven to a new paradigm of work with the use of
   three-dimensional (3D) parametric models. However, building information
   modelling (BIM) has been mostly used for as-built models of a building,
   not yet been widely used by designers during project and construction
   phases for occupational risks prevention and safety planning. This paper
   aims to show the capacity of developing tools that allow adding
   functionalities to Revit software to improve safety procedures and
   reduce the time spent on modelling them during the design phase.
   Design/methodology/approach
   To reach this objective, a structural 3D model of a building is used to
   validate the developed tools. A plugin prototype based on legal
   regulations was developed, allowing qualitative safety assessment
   through the application of job hazard analysis (JHA), SafeObject and
   checklists. These tools allow the automated detection of falls from
   height situations and the automated placement of the correspondent
   safety systems.
   Findings
   Revit application programming interface allowed the conception and
   addition of several functionalities that can be used in BIM methodology,
   and more specifically in the prevention of occupational risks in
   construction, contributing this paper to the application of a new
   approach to the prevention through design.
   Originality/value
   This paper is innovative and important because the developed plugins
   allowed: automated detection of potential falls from heights in the
   design stage; automated introduction of safety objects from a BIM Safety
   Objects Library; and the intercommunication between a BIM model and a
   safety database, bringing JHA integration directly on the project. The
   prototype of this work was validated for fall from height hazards but
   can be extended to other potentials hazards since the initial design
   stage.},
DOI = {10.1108/CI-12-2019-0147},
EarlyAccessDate = {OCT 2020},
ISSN = {1471-4175},
EISSN = {1477-0857},
ResearcherID-Numbers = {Matos, Raquel/AAG-8897-2019
   Rodrigues, Maria Fernanda/N-1792-2013},
ORCID-Numbers = {Matos, Raquel/0000-0002-0171-7842
   Rodrigues, Maria Fernanda/0000-0001-9127-7766},
Unique-ID = {WOS:000586580400001},
}

@article{ WOS:000591585900002,
Author = {Zhang, Li-li and Zhao, Qi and Wang, Li and Zhang, Ling-yu},
Title = {Research on Urban Traffic Signal Control Systems Based on Cyber Physical
   Systems},
Journal = {JOURNAL OF ADVANCED TRANSPORTATION},
Year = {2020},
Volume = {2020},
Month = {OCT 23},
Abstract = {In this paper, we present a traffic cyber physical system for urban road
   traffic signal control, which is referred to as UTSC-CPS. With this
   proposed system, managers and researchers can realize the construction
   and simulation of various types of traffic scenarios, the rapid
   development, and optimization of new control strategies and can apply
   effective control strategies to actual traffic management. The
   advantages of this new system include the following. Firstly, the fusion
   architecture of private cloud computing and edge computing is proposed
   for the first time, which effectively improves the performance of
   software and hardware of the urban road traffic signal control system
   and realizes information security perception and protection in cloud and
   equipment, respectively, within the fusion framework; secondly, using
   the concept of parallel system, the depth of real-time traffic control
   subsystem and real-time simulation subsystem is realized. Thirdly, the
   idea of virtual scene basic engine and strategy agent engine is put
   forward in the system design, which separates data from control strategy
   by designing a general control strategy API and helps researchers focus
   on control algorithm itself without paying attention to detection data
   and basic data. Finally, considering China, the system designs a general
   control strategy API to separate data from control strategy. Most of the
   popular communication protocols between signal controllers and detectors
   are private protocols. The standard protocol conversion middleware is
   skillfully designed, which decouples the field equipment from the system
   software and achieves the universality and reliability of the control
   strategy. To further demonstrate the advantages of the new system, we
   have carried out a one-year practical test in Weifang City, Shandong
   Province, China. The system has been proved in terms of stability,
   security, scalability, practicability and rapid practice, and
   verification of the new control strategy. At the same time, it proves
   the superiority of the simulation subsystem in the performance and
   simulation scale by comparing the different-scale road networks of
   Shunyi District in Beijing and Weifang City in Shandong Province.
   Further tests were conducted using real intersections, and the results
   were equally valid.},
DOI = {10.1155/2020/8894812},
Article-Number = {8894812},
ISSN = {0197-6729},
EISSN = {2042-3195},
ORCID-Numbers = {zhang, li li/0000-0002-1980-1858},
Unique-ID = {WOS:000591585900002},
}

@article{ WOS:000578102000001,
Author = {Murray, Riley and Chandrasekaran, Venkat and Wierman, Adam},
Title = {Signomial and polynomial optimization via relative entropy and partial
   dualization},
Journal = {MATHEMATICAL PROGRAMMING COMPUTATION},
Year = {2021},
Volume = {13},
Number = {2},
Pages = {257-295},
Month = {JUN},
Abstract = {We describe a generalization of the Sums-of-AM/GM-Exponential (SAGE)
   methodology for relative entropy relaxations of constrained signomial
   and polynomial optimization problems. Our approach leverages the fact
   that SAGE certificates conveniently and transparently blend with convex
   duality, in a way which enables partial dualization of certain
   structured constraints. This more general approach retains key
   properties of ordinary SAGE relaxations (e.g. sparsity preservation),
   and inspires a projective method of solution recovery which respects
   partial dualization. We illustrate the utility of our methodology with a
   range of examples from the global optimization literature, along with a
   publicly available software package.},
DOI = {10.1007/s12532-020-00193-4},
EarlyAccessDate = {OCT 2020},
ISSN = {1867-2949},
EISSN = {1867-2957},
ORCID-Numbers = {Murray, Riley/0000-0003-1461-6458},
Unique-ID = {WOS:000578102000001},
}

@article{ WOS:000580449200009,
Author = {Dave, Keyur and Boorman, Rhonda J. and Walker, Rachel M.},
Title = {Management of a critical downtime event involving integrated electronic
   health record},
Journal = {COLLEGIAN},
Year = {2020},
Volume = {27},
Number = {5},
Pages = {542-552},
Month = {OCT},
Abstract = {Background: There are few descriptions of management of unplanned
   hospital-wide digital downtime and impact on patient care in health
   literature.
   Aim: The aim of this study was to undertake a qualitative review of a
   prolonged critical technology downtime event in an Australian hospital
   in 2017.
   Methods: Inductive content analysis was conducted on data collected
   through face-to-face, semi-structured, individual interviews conducted
   with nine hospitals employees (five nurses with direct-care/operational
   responsibilities, and four executive staff, including nursing) who
   played a role in the incident.
   Findings: Analysis of the data using an open-source R package led to the
   extraction of 139 codes, 13 first-level categories, and 4 main
   categories. Main categories extracted were: impact of event, response to
   the event, resilience and institutional reserve, and challenges and
   learnings.
   Discussion: The overall experience for interview participants was
   positive. Effective communication methods, particularly vertical
   communication, enabled multi-disciplinary teams (comprising nursing,
   medical and pharmacy personnel) to safely transition back from downtime
   paper records to the integrated electronic health record with no harm to
   patients. Participants identified teamwork contributed to a sense of
   comradery with clinical colleagues and executive staff. Contingency
   planning and training are essential for ensuring safe and effective
   management of technology downtime events.
   Conclusion: The prolonged digital disruption and subsequent recovery was
   managed effectively using a face-to-face communication and support
   approach. This approach reduced the impact of the digital downtime and
   ensured patient safety. The data analysis strategy was enhanced using an
   computer-assisted qualitative data analysis software. (C) 2020
   Australian College of Nursing Ltd. Published by Elsevier Ltd.},
DOI = {10.1016/j.colegn.2020.02.002},
ISSN = {1322-7696},
EISSN = {1876-7575},
ResearcherID-Numbers = {Boorman, Rhonda/AAH-7101-2020
   },
ORCID-Numbers = {Boorman, Rhonda/0000-0002-4025-9741
   Walker, Rachel/0000-0002-6089-8225},
Unique-ID = {WOS:000580449200009},
}

@article{ WOS:000579307800004,
Author = {Hawari, Mohamme and Cordero-Fuertes, Juan-Antonio and Clausen, Thomas},
Title = {High-Accuracy Packet Pacing on Commodity Servers for Constant-Rate Flows},
Journal = {IEEE-ACM TRANSACTIONS ON NETWORKING},
Year = {2020},
Volume = {28},
Number = {5},
Pages = {1953-1967},
Month = {OCT},
Abstract = {This paper addresses the problem of high-quality packet pacing for
   constant-rate packet consumption systems, with strict buffering
   limitations. A mostly-software pacing architecture is developed, which
   has minimal hardware requirements, satisfied by commodity servers -
   rendering the proposed solution easily deployable in existing
   (data-centre) infrastructures. Two algorithms (free-running and
   frequency-controlled pacing, for explicitly and implicitly indicated
   target rates, respectively) are specified, and formally analysed. The
   proposed solution, including both algorithms, is implemented, and is
   tested on real hardware and under real conditions. The performance of
   these implementations is experimentally evaluated and compared to
   existing mechanisms, available in general-purpose hardware. Results of
   both exhaustive experiments, and of an analytical modeling, indicate
   that the proposed approach is able to perform low-jitter packet pacing
   on commodity hardware, being thus suitable for constant rate
   transmission and consumption in media production scenarios.},
DOI = {10.1109/TNET.2020.3001672},
ISSN = {1063-6692},
EISSN = {1558-2566},
ResearcherID-Numbers = {CORDERO FUERTES, Juan Antonio/HOH-8103-2023
   Clausen, Thomas/R-5859-2019
   },
ORCID-Numbers = {CORDERO FUERTES, Juan Antonio/0000-0001-5771-3122
   Clausen, Thomas/0000-0002-7400-8887
   HAWARI, Mohammed/0000-0003-4045-2522},
Unique-ID = {WOS:000579307800004},
}

@article{ WOS:000572634400108,
Author = {Jiang, Zhongyuan and Chen, Xi and Ma, Jianfeng and Zhang, Yasheng and
   Gu, Jujuan},
Title = {Traffic Dynamics Evaluation for the Future NFV Deployment},
Journal = {IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS},
Year = {2020},
Volume = {67},
Number = {10},
Pages = {2214-2218},
Month = {OCT},
Abstract = {To adapt the rapid upgradation of the communication technology (e.g.,
   from 5G to 6G) and reduce the redeployment cost, the Network Functions
   Virtualization (NFV) is regarded as a key technology to reduce the
   capital expenditures (CAPEX) and operating expenses (OPEX) by decoupling
   traditional network functions from proprietary hardware appliances to
   software appliances. However, with increasing data transmission
   requirements, the limited traffic capacity is still the bottleneck of
   NFV network and strongly related to three aspects: 1) NFV network
   structure construction; 2) resource allocation; 3) routing design for
   packet navigation. In this brief, we first present a preferential link
   attachment mechanism to model the growth evolution of the NFV network.
   Secondly, we propose to efficiently allocate virtual resource for every
   Virtualized Network Fucntion (VNF). Thirdly, we propose a new routing
   mechanism to fully make use of the network resource and enhance the
   traffic capacity. Finally, extensive experiments demonstrate that our
   integrated solution systematically formulates the traffic dynamics for
   the future NFV deployment.},
DOI = {10.1109/TCSII.2019.2962249},
ISSN = {1549-7747},
EISSN = {1558-3791},
ResearcherID-Numbers = {Ma, Jianfeng/GZB-0110-2022
   Jiang, Zhong-Yuan/B-3847-2012},
ORCID-Numbers = {Jiang, Zhong-Yuan/0000-0001-8249-9711},
Unique-ID = {WOS:000572634400108},
}

@article{ WOS:000603475100011,
Author = {Seifi, Hasti and Chun, Matthew and Gallacher, Colin and Schneider,
   Oliver Stirling and MacLean, Karon E.},
Title = {How Do Novice Hapticians Design? A Case Study in Creating Haptic
   Learning Environments},
Journal = {IEEE TRANSACTIONS ON HAPTICS},
Year = {2020},
Volume = {13},
Number = {4},
Pages = {791-805},
Month = {OCT},
Abstract = {Access to haptic technology is on the rise, in smartphones, virtual
   reality gear, and open-source education kits. However, engineers and
   interaction designers are often inexperienced in designing with haptics,
   and rarely have tools and guidelines for creating multisensory
   experiences. To examine the impact of this deficit, we supplied a haptic
   design kit, custom software, and technical support to nine teams (25
   students) for an innovation challenge at a major haptics conference.
   Teams (predominantly undergraduate engineers with little haptics,
   interaction design, or education training) designed and built haptic
   environments to support learning of science topics. Qualitative analysis
   of surveys, interviews, team blogs, and expert assessments of teams'
   final demonstrations exposed three themes in these design efforts. 1)
   Novice teams tended to ignore many of ten design choices that experts
   navigate, such as explicitly choosing whether haptic and graphic
   feedback should reinforce versus complement one other. 2) Their design
   activities differed in timing and inclusion from the ten activities
   observed in expert process. 3) We identified three success strategies in
   how teams devised useful and engaging interactions and interpretable
   multimodal experiences, and communicated about their designs. We compare
   novice and expert design needs and highlight where future haptic design
   tools and theory need to support novice practice and training.},
DOI = {10.1109/TOH.2020.2968903},
ISSN = {1939-1412},
EISSN = {2329-4051},
ORCID-Numbers = {MacLean, Karon/0000-0003-2969-4627},
Unique-ID = {WOS:000603475100011},
}

@article{ WOS:000569001200011,
Author = {Song, Ping and Hou, Zhenglin and Sukumar, Shravan and Herman, Rod A.},
Title = {Template-based peptide modeling for celiac risk assessment of newly
   expressed proteins in GM crops},
Journal = {REGULATORY TOXICOLOGY AND PHARMACOLOGY},
Year = {2020},
Volume = {116},
Month = {OCT},
Abstract = {Newly expressed proteins in genetically modified (GM) crops are subject
   to celiac disease risk assessment according to EFSA guidelines. Amino
   acid identity matches between short peptides (9aa) and known celiac
   restricted epitopes are required to be further evaluated through peptide
   modeling; however, validated methods and criteria are not yet available.
   In this investigation, several structures of HLA-DQ2.5/peptide/TCR
   (T-cell receptor) complexes were analyzed and two template-based peptide
   molding software packages were evaluated using various peptides
   including ones not associated with celiac disease. Structural
   characterization indicates that residues at P(position)1, P2, P5, P8,
   and P9 in the 9aa restricted epitopes also contribute to the binding of
   celiac peptides to the HLA-DQ2.5 antigen in addition to the presence of
   the motif Q/EX1PX2 starting at P4 or P6. The recognition of the
   HLA-DQ2.5/peptide complex by TCR is through specific interactions
   between the residues in the restricted epitopes and some loop structures
   in the TCR. The template-based software package GalaxyPepDock seems to
   be suitable for the application of peptide modeling when an estimated
   accuracy value of >0.95 combined with >160 interaction similarity score
   are used as a threshold for biologically meaningful in silico binding.
   Nevertheless, caution should be exercised when applying peptide modeling
   to celiac disease risk assessment until methods are rigorously validated
   and further evaluated to demonstrate its value in the risk assessment of
   newly expressed proteins in GM crops.},
DOI = {10.1016/j.yrtph.2020.104715},
Article-Number = {104715},
ISSN = {0273-2300},
EISSN = {1096-0295},
Unique-ID = {WOS:000569001200011},
}

@article{ WOS:000558812000064,
Author = {Spuhler, Dorothee and Germann, Verena and Kassa, Kinfe and Ketema,
   Atekelt Abebe and Sherpa, Anjali Manandhar and Sherpa, Mingma Gyalzen
   and Maurer, Max and Luthi, Christoph and Langergraber, Guenter},
Title = {Developing sanitation planning options: A tool for systematic
   consideration of novel technologies and systems},
Journal = {JOURNAL OF ENVIRONMENTAL MANAGEMENT},
Year = {2020},
Volume = {271},
Month = {OCT 1},
Abstract = {To provide access to sustainable sanitation for the entire world
   population, novel technologies and systems have been developed. These
   options are often independent of sewers, water, and energy and therefore
   promise to be more appropriate for fast-growing urban areas. They also
   allow for resource recovery and and are adaptable to changing
   environmental and demographic conditions what makes them more
   sustainable. More options, however, also enhance planning complexity.
   Structured decision making (SDM) can help balance opposing interests.
   Yet, most of the current research focuses on the selection of a
   preferred option, assuming that a set of appropriate options is
   available. There is a lack of reproducible methods for the
   identification of sanitation system planning options that can consider
   the growing number of available technology and the many possible system
   configurations. Additionally, there is a lack of data, particularly for
   novel options, to evaluate the various sustainability criteria for
   sanitation.To overcome this limitation, we present a novel software
   supported approach: the SANi-tation sysTem Alternative GeneratOr
   (Santiago). To be optimally effective, Santiago is required to be
   integrated into an SDM approach. In this paper, we present all the
   elements that such an integration requires and illustrate these methods
   at the case of Arba Minch, a fast growing town in Ethiopia. Based on
   this example and experiences from other cases, we discuss the lessons
   learnt and present the advantages potentially brought by Santiago for
   sanitation planning The integration requires four elements: a set of
   technologies to be looked at, decision objectives for sustainable
   sanitation, screening criteria to evalute technology appropriateness,
   and about the technologies and the casea. The main output is a set of
   sanitation system options that is locally appropriate, diverse in order
   to reveal trade-offs, and of a manageable size. To support the
   definition of decision objectives, we developed a generic objective
   hierarchy for sustainable sanitation. Because one of the main challenges
   lies in the quantification of screening criteria, we established the
   data for 27 criteria and 41 technologies in a library. The case studies
   showed, that if the integration is successful, then Santiago can provide
   substantial benefits: (i) it is systematic and reproducible; (ii) it
   opens up the decision space with novel and potentially more appropriate
   solutions; (iii) it makes international data accessible for more
   empirical decision making; (iv) it enables decisions based on strategic
   objectives in line with the sustainable development goals; (v) it allows
   to prioritise appropriate and resource efficient systems right from the
   beginning (vi) and it contributes to a more citywide inclusive approach
   by birding strategic objectives with an area-based appropriateness
   assessment. The here presented approach enables the prioritisation of
   appropriate and resource efficient sanitation technologies and systems
   in strategic planning. Thereby this approach contributes to SDG 6.2,
   6.3, and 11, sustainable sanitation for all.},
DOI = {10.1016/j.jenvman.2020.111004},
Article-Number = {111004},
ISSN = {0301-4797},
EISSN = {1095-8630},
ResearcherID-Numbers = {Maurer, Max/B-5579-2009
   },
ORCID-Numbers = {Maurer, Max/0000-0002-5326-6035
   Spuhler, Dorothee/0000-0002-1379-6146},
Unique-ID = {WOS:000558812000064},
}

@article{ WOS:000572015800001,
Author = {Amin, M. N. and Niazi, M. U. K.},
Title = {Finite-Element Analysis of Isolated and Integrated Structural Steel
   Members Exposed to Fire},
Journal = {IRANIAN JOURNAL OF SCIENCE AND TECHNOLOGY-TRANSACTIONS OF CIVIL
   ENGINEERING},
Year = {2020},
Volume = {44},
Number = {SUPPL 1, 1},
Pages = {35-49},
Month = {OCT},
Abstract = {Ambiguities in current fire design codes have triggered conflicting
   responses from researchers and professionals about steel buildings in
   fire. This is because fire codes do not consider the integrated
   structure of the building, but are based on the tests and models of the
   individual structural members of a construction subjected to a standard
   fire (ISO 834 or ASTM E 119). This study designed a four-story steel
   frame structure containing five bays with a bay span of 12 m and a
   height of 3.5 m according to Eurocode 3, considering all joints as being
   simply supported. Four different fire scenarios were modeled with
   reference to members of the assembled structure utilizing the Vulcan
   software package considering standard and parametric fire curves on
   exposed and protected sections. The results were compared with those
   obtained by exposing to the same fire curves an isolated beam member
   equal to those belonging to the frame under investigation. The
   comparison shows that the assembled members in the modeled structure had
   a better fire performance than did the isolated members. Because
   connections are important components of structures, we believe that the
   effects of connections on the fire performance of structures should be
   studied in the future.},
DOI = {10.1007/s40996-020-00464-z},
EarlyAccessDate = {SEP 2020},
ISSN = {2228-6160},
EISSN = {2364-1843},
Unique-ID = {WOS:000572015800001},
}

@article{ WOS:000698779700012,
Author = {Brighenti, Attilio and Duranti, Davide and Quintaba, Debora},
Title = {TGSim Plus (TM)-Real-Time Dynamic Simulation Suite of Gas Turbine
   Systems for the MATLAB((R))/Simulink((R)) Environment},
Journal = {INTERNATIONAL JOURNAL OF TURBOMACHINERY PROPULSION AND POWER},
Year = {2020},
Volume = {5},
Number = {3},
Month = {SEP},
Abstract = {Dynamic simulation of turbomachinery by Hardware in the Loop (HIL)
   real-time systems has become an essential practice, due to the high cost
   of real equipment testing and the need to verify the control and
   diagnostic systems' reaction to emergency situations. The authors
   developed a full model of a power generation Gas Turbine Plant,
   including liquid and gaseous auxiliaries, and the electrical generator
   and starter motor, integrated in a MATLAB((R))/Simulink((R)) simulation
   suite: TGSim Plus (TM). This allows assembling models of various gas
   turbine (GT) architectures by customised Simulink((R)) library blocks
   and simulating steady state and transient conditions, such as complete
   start-up and shutdown operations as well as emergency, contingent
   operations and artificially injected fault scenarios. The model solver
   runs real-time steps at milliseconds scale. The paper describes the main
   modelling characteristics and typical results of steady state and
   transient simulations of a heavy-duty gas turbine under development by
   Doosan Heavy Industries and Construction (Changwon, South Korea).
   Comparison with benchmark design simulations obtained by a reference non
   real-time software shows a good match between the two environments, duly
   taking into account some differences in the GT models setting affecting
   parts of the sequence. The paper discusses also the bleed streams
   warm-up influence on GT performance and the start-up states trajectories
   dependency on control logic and on the starter helper motor torque
   envelope.},
DOI = {10.3390/ijtpp5030024},
Article-Number = {24},
EISSN = {2504-186X},
ORCID-Numbers = {Brighenti, Attilio/0000-0003-0740-8538
   Quintaba, Debora/0000-0001-5724-6309
   Duranti, Davide/0000-0002-0948-9370},
Unique-ID = {WOS:000698779700012},
}

@article{ WOS:000546907800002,
Author = {Ghorbani, Bahram and Javadi, Zahra and Zendehboudi, Sohrab and Amidpour,
   Majid},
Title = {Energy, exergy, and economic analyses of a new integrated system for
   generation of power and liquid fuels using liquefied natural gas
   regasification and solar collectors},
Journal = {ENERGY CONVERSION AND MANAGEMENT},
Year = {2020},
Volume = {219},
Month = {SEP 1},
Abstract = {Simultaneous design of units and process integration reduce the number
   of unnecessary equipment and energy consumption. In this paper, an
   integrated structure is developed and analyzed for liquid fuels and
   power co-generation, using the air separation unit, Fischer-Tropsch
   synthesis unit, steam power plant, and organic Rankine power generation
   unit. Liquefied natural gas regasification and solar dish collectors are
   employed to supply the cooling and heating for the integrated structure,
   respectively. The integrated process is simulated and designed using
   MATLAB, HYSYS, and TRNSYS software packages, where a systematic
   sensitivity analysis is performed to investigate the effects of
   operating conditions on the process behaviors/performance. This
   integrated plant generates 200.6 MW power and 78.88 kgmol/h liquid
   fuels. The overall thermal energy and total exergy efficiencies of the
   integrated approach are 42.36\% and 64.72\%, respectively. According to
   the exergy analysis, the collectors are responsible for 32.98\% of the
   total exergy destruction, followed by the heat exchangers causing
   28.39\% exergy destruction. Conducting economic analysis with the
   annualized cost of system (ACS) method, the period of return and the
   prime cost of the product are 2.186 years and 443.9 USD per m(3) of
   liquid fuels, respectively. This research study offers useful
   guidelines/tips for the theoretical and practical implications of an
   effective integrated structure for production of liquid fuels and power
   at various thermodynamic and process conditions.},
DOI = {10.1016/j.enconman.2020.112915},
Article-Number = {112915},
ISSN = {0196-8904},
EISSN = {1879-2227},
ResearcherID-Numbers = {Zendehboudi, Sohrab/AAU-4366-2020
   Zendehboudi, Sohrab/ABG-7741-2021
   Amidpour, Majid/M-3769-2013
   Ghorbani, Bahram/AAF-2784-2019
   },
ORCID-Numbers = {Amidpour, Majid/0000-0003-2073-6106
   Ghorbani, Bahram/0000-0002-1743-0766
   Zendehboudi, Sohrab/0000-0001-8527-9087
   Javadi, Zahra/0000-0003-1869-7830},
Unique-ID = {WOS:000546907800002},
}

@article{ WOS:000562902500001,
Author = {Udawatta, Nilupa and Zuo, Jian and Chiveralls, Keri and Zillante, George},
Title = {From green buildings to living buildings? Rating schemes and waste
   management practices in Australian educational buildings},
Journal = {ENGINEERING CONSTRUCTION AND ARCHITECTURAL MANAGEMENT},
Year = {2021},
Volume = {28},
Number = {4},
Pages = {1278-1294},
Month = {APR 28},
Abstract = {Purpose There is an increasing level of recognition of the pressing
   issues associated with climate change and resource depletion. As a
   result, it is well recognised that higher education institutions bear
   responsibilities to promote ``sustainable development{''}. Many
   universities have adopted green building practices in the construction
   of their building infrastructure. A variety of Green Building Rating
   Tools (GBRTs) have been designed to facilitate green building
   developments. Thus, the aim of this research is to identify mechanisms
   to improve current GBRTs in terms of waste management (WM) practices by
   using green star accredited educational buildings in Australia.
   Design/methodology/approach A qualitative approach was adopted in this
   study to achieve the research aim by conducting three case studies of
   educational buildings in South Australia. Thirty three interviews were
   carried out in a face-to-face, semi-structured manner and project
   documentations were reviewed. The participants were asked to provide
   their expert opinions on the GS initiative and its ability to minimise
   waste generation, the impact of the GS initiative on solid WM practices
   and problems associated with the implementation process of the GS
   initiative. Data was analysed using code-based content analysis using
   the NVivo software package. Tables and figures were used as the
   visualization technique to present an expedient understanding in a
   holistic manner. Findings Findings showed that the Green Star (GS)
   initiative drives change in the way current practices are performed in
   the Australian construction industry. However, this study revealed that
   WM targets outlined in the GS initiative are not challenging enough.
   Thus, suggestions are provided in this research to improve the WM
   aspects of GS initiatives by looking beyond a focus on
   ``sustainability{''} and ``waste minimisation{''} towards a focus on
   regenerative environments. Originality/value These findings are valuable
   for practitioners and policymakers seeking to improve WM practices and
   to address issues associated with climate change and resource depletion.},
DOI = {10.1108/ECAM-03-2019-0177},
EarlyAccessDate = {AUG 2020},
ISSN = {0969-9988},
EISSN = {1365-232X},
ResearcherID-Numbers = {Hopeward, Keri/F-4183-2013
   },
ORCID-Numbers = {Hopeward, Keri/0000-0002-7465-4934
   Zuo, Jian/0000-0002-8279-9666
   Udawatta, Nilupa/0000-0002-0290-3065},
Unique-ID = {WOS:000562902500001},
}

@article{ WOS:000540708300006,
Author = {Xie, Lixia and Ding, Ying and Yang, Hongyu and Hu, Ze},
Title = {Mitigating LFA through segment rerouting in IoT environment with
   traceroute flow abnormality detection},
Journal = {JOURNAL OF NETWORK AND COMPUTER APPLICATIONS},
Year = {2020},
Volume = {164},
Month = {AUG 15},
Abstract = {The Internet of Things (IoT) provides tremendous smart devices that are
   always connected to and interacting with the Internet. However, the
   development of IoT also promotes the threat of network attacks due to
   the billions of IoT devices vulnerable to hackers. Link-flooding attack
   (LFA) is a new type of DDoS attack used to flood the crucial network
   links. In IoT environment, LFA can be more easily launched by
   large-scale low-rate legitimate data flows with quite a low cost and is
   difficult to detect. Target areas in an enterprise network can be easily
   isolated since the crucial links are unavailable. Software defined
   network (SDN) architecture provides new opportunities to address this
   network security problem with the separation of data plane and control
   plane. Recently, segment routing (SR), which is an evolution of source
   routing, has been viewed as a promising technique for flow rerouting and
   failure recovery. SR is a lightweight easy-deployed scheme known for its
   flexibility, scalability, and applicability. Therefore, in this paper,
   we try to mitigate LFA with segment rerouting within the SDN
   architecture. With the comprehensive network-wide view of the data flows
   and links, we first design a monitoring mechanism to detect LFA based on
   the availability of the crucial links and traceroute flows. We consider
   the traceroute packet flows as time series with white Gaussian noise. A
   machine-learning-based auto-regression scheme is proposed to detect the
   abnormal increase in traceroute packets which indicates the launch of
   LFA. Then we use segment routing to detour the congested flows and
   alleviate the burden on the crucial links. Finally. the LFA bots will be
   identified and the malicious traffic will be blocked. Sufficient
   evaluations demonstrate that our LFA defense can efficiently detect LFA
   and preserve the network services, while only introduce a little
   signaling overhead between the control and data plane.},
DOI = {10.1016/j.jnca.2020.102690},
Article-Number = {102690},
ISSN = {1084-8045},
ResearcherID-Numbers = {Xie, li/HGE-6052-2022},
Unique-ID = {WOS:000540708300006},
}

@article{ WOS:000561035500003,
Author = {Mohamadi, Efat and Takian, Amirhossein and Olyaeemanesh, Alireza and
   Rashidian, Arash and Hassanzadeh, Ali and Razavi, Moaven and Ghazanfari,
   Sadegh},
Title = {Health insurance benefit package in Iran: a qualitative policy process
   analysis},
Journal = {BMC HEALTH SERVICES RESEARCH},
Year = {2020},
Volume = {20},
Number = {1},
Month = {AUG 6},
Abstract = {Background Insufficient transparency in prioritization of health
   services, multiple health insurance organizations with various and
   not-aligned policies, plus limited resources to provide comprehensive
   health coverage are among the challenges to design appropriate Health
   Insurance Benefit Package (HIBP) in Iran. This study aims to analyze
   Policy Process of Health Insurance Benefit Package in Iran. Method Data
   were collected through semi-structured interviews with 25 experts, plus
   document analysis and observation, from February 2014 until October
   2016. Using both deductive and inductive approaches, two independent
   researchers conducted data content analysis. We used MAXQDA.11 software
   for data management. Results We identified 10 main themes, plus 81
   sub-themes related to development and implementation of HIBP. These
   included: lack of transparent criteria for inclusion of services within
   HIBP, inadequate use of scientific evidence to determine the HIBP, lack
   of evaluation systems, and weak decision-making process. We propose 11
   solutions and 25 policy options to improve the situation. Conclusion The
   design and implementation of HIBP did not follow an evidence-based and
   logical algorithm in Iran. Rather, political and financial influences at
   the macro level determined the decisions. This is rooted in social,
   cultural, and economic norms in the country, whereby political and
   economic factors had the greatest impact on the implementation of HIBP.
   To define a cost-effective HIBP in Iran, it is pivotal to develop
   transparent and evidence-based guidelines about the processes and the
   stewardship of HIBP, which are in line with upstream policies and
   societal characteristics. In addition, the possible conflict of
   interests and its harms should be minimized in advance.},
DOI = {10.1186/s12913-020-05592-w},
Article-Number = {722},
EISSN = {1472-6963},
ResearcherID-Numbers = {Olyaeemanesh, Alireza/ABB-8563-2020
   Mohamadi, Efat/ABD-8458-2020
   },
ORCID-Numbers = {Rashidian, Arash/0000-0002-4005-5183},
Unique-ID = {WOS:000561035500003},
}

@article{ WOS:000557608200001,
Author = {Wilhelm, M. E. and Stuber, M. D.},
Title = {EAGO.jl: easy advanced global optimization in Julia},
Journal = {OPTIMIZATION METHODS \& SOFTWARE},
Year = {2022},
Volume = {37},
Number = {2},
Pages = {425-450},
Month = {MAR 4},
Abstract = {An extensible open-source deterministic global optimizer (EAGO)
   programmed entirely in the Julia language is presented. EAGO was
   developed to serve the need for supporting higher-complexity
   user-defined functions (e.g. functions defined implicitly via
   algorithms) within optimization models. EAGO embeds a first-of-its-kind
   implementation of McCormick arithmetic in an Evaluator structure
   allowing for the construction of convex/concave relaxations using a
   combination of source code transformation, multiple dispatch, and
   context-specific approaches. Utilities are included to parse
   user-defined functions into a directed acyclic graph representation and
   perform symbolic transformations enabling dramatically improved solution
   speed. EAGO is compatible with a wide variety of local optimizers, the
   most exhaustive library of transcendental functions, and allows for easy
   accessibility through the JuMP modelling language. Together with Julia's
   minimalist syntax and competitive speed, these powerful features make
   EAGO a versatile research platform enabling easy construction of novel
   meta-solvers, incorporation and utilization of new relaxations, and
   extension to advanced problem formulations encountered in engineering
   and operations research (e.g. multilevel problems, user-defined
   functions). The applicability and flexibility of this novel software is
   demonstrated on a diverse set of examples. Lastly, EAGO is demonstrated
   to perform comparably to state-of-the-art commercial optimizers on a
   benchmarking test set.},
DOI = {10.1080/10556788.2020.1786566},
EarlyAccessDate = {AUG 2020},
ISSN = {1055-6788},
EISSN = {1029-4937},
Unique-ID = {WOS:000557608200001},
}

@article{ WOS:000564898700001,
Author = {Christodoulides, Paul and Vieira, Ana and Lenart, Stanislav and Maranha,
   Joao and Vidmar, Gregor and Popov, Rumen and Georgiev, Aleksandar and
   Aresti, Lazaros and Florides, Georgios},
Title = {Reviewing the Modeling Aspects and Practices of Shallow Geothermal
   Energy Systems},
Journal = {ENERGIES},
Year = {2020},
Volume = {13},
Number = {16},
Month = {AUG},
Abstract = {Shallow geothermal energy systems (SGES) may take different forms and
   have recently taken considerable attention due to energy geo-structures
   (EGS) resulting from the integration of heat exchange elements in
   geotechnical structures. Still, there is a lack of systematic design
   guidelines of SGES. Hence, in order to contribute towards that
   direction, the current study aims at reviewing the available SGES
   modeling options along with their various aspects and practices. This is
   done by first presenting the main analytical and numerical models and
   methods related to the thermal behavior of SGES. Then, the most
   important supplementary factors affecting such modeling are discussed.
   These include: (i) the boundary conditions, in the form of temperature
   variation or heat flow, that majorly affect the predicted thermal
   behavior of SGES; (ii) the spatial dimensions that may be crucial when
   relaxing the infinite length assumption for short heat exchangers such
   as energy piles (EP); (iii) the determination of SGES parameters that
   may need employing specific techniques to overcome practical
   difficulties; (iv) a short-term vs. long-term analysis depending on the
   thermal storage characteristics of GHE of different sizes; (v) the
   influence of groundwater that can have a moderating effect on fluid
   temperatures in both heating and cooling modes. Subsequently,
   thermo-mechanical interactions modeling issues are addressed that may be
   crucial in EGS that exhibit a dual functioning of heat exchangers and
   structural elements. Finally, a quite lengthy overview of the main
   software tools related to thermal and thermo-hydro-mechanical analysis
   of SGES that may be useful for practical applications is given. A
   unified software package incorporating all related features of all SGES
   may be a future aim.},
DOI = {10.3390/en13164273},
Article-Number = {4273},
EISSN = {1996-1073},
ResearcherID-Numbers = {Lenart, Stanislav/R-1693-2019
   Christodoulides, Paul/I-2660-2014
   Christodoulides, Paul/ABF-1363-2020
   },
ORCID-Numbers = {Lenart, Stanislav/0000-0003-4818-5668
   Christodoulides, Paul/0000-0002-2229-8798
   Christodoulides, Paul/0000-0002-2229-8798
   Aresti, Lazaros/0000-0002-9426-7114
   Vieira, Ana/0000-0002-7680-3868
   Florides, Georgios/0000-0001-9079-1907
   Vidmar, Gregor/0000-0003-1577-8395},
Unique-ID = {WOS:000564898700001},
}

@article{ WOS:000550401500001,
Author = {De Gaetani, Carlo Iapige and Mert, Mertkan and Migliaccio, Federica},
Title = {Interoperability Analyses of BIM Platforms for Construction Management},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {13},
Month = {JUL},
Abstract = {It is incontrovertible that an exchange of files is essentially required
   at several stages of the workflow in the architecture, engineering, and
   construction (AEC) industry. Therefore, investigating and detecting the
   capabilities/inabilities of building information modeling (BIM) software
   packages with respect to interoperability can be informative to
   stakeholders who exchange data between various BIM packages. The work
   presented in this paper includes a discussion on the interoperability of
   different software platforms commonly used in the AEC industry.
   Although, in theory, flawless interoperability of some types of files
   between different BIM platforms is ensured, in practical applications,
   this is not always the case. Hence, this research aims to identify
   faults in data exchange by assessing different possible scenarios where
   a sample Industry Foundation Classes (IFC) four-dimensions (4D) BIM
   model and related Gantt charts are exchanged. Throughout the
   interoperability analysis of both IFC file and Gantt charts, the
   following checks were carried out: geometrical and nongeometrical
   information exchange through IFC files, 4D information correct
   readability, and presence of missing schedule information in Gantt
   charts after their import/export procedure. The results show that
   interoperability between the analyzed platforms is not always ensured,
   providing useful insight into realistic scenarios.},
DOI = {10.3390/app10134437},
Article-Number = {4437},
EISSN = {2076-3417},
ResearcherID-Numbers = {DE GAETANI, CARLO IAPIGE/ABB-6399-2020
   DE GAETANI, CARLO IAPIGE/GQH-7164-2022
   },
ORCID-Numbers = {DE GAETANI, CARLO IAPIGE/0000-0002-9214-2588
   MIGLIACCIO, Federica/0000-0002-1350-6946},
Unique-ID = {WOS:000550401500001},
}

@article{ WOS:000593228500005,
Author = {Erofeeva, V, I. and Melnik, G. S. and Zaykina, N. M.},
Title = {The Media Image of the Republic of Crimea: The Russian Student's World
   Model},
Journal = {REGIONOLOGIYA-REGIONOLOGY RUSSIAN JOURNAL OF REGIONAL STUDIES},
Year = {2020},
Volume = {28},
Number = {3},
Pages = {516-542},
Month = {JUL-SEP},
Abstract = {Introduction. In the context of geopolitical confrontation between
   Russia and the West, the task of purposeful construction of the media
   image of the Republic of Crimea is of relevance. The objective of this
   paper is to reveal the communicative and functional as well as the
   structural and semiotic resources of mass media used in the construction
   of the media image of the Republic of Crimea on the basis of the study
   conducted and to obtain a holistic characteristic of this image in the
   Russian student's world model
   Materials and Methods. The data of a sociological survey conducted in
   the spring of 2020 among college students majoring in the humanities and
   technical fields, were used as the research materials. The structural
   and functional approach was adopted, which made it possible to identify
   the structure of the media image, its axiological and pragmatic
   potential. The interpretation of the data obtained was carried out using
   the SOPS 17 software package. Linguocultural analysis, based on the
   indissoluble unity of the language and culture of society, made it
   possible to identify an axiological characteristic of a media image as a
   component of the student's world model.
   Results. The category of media image has been considered as a summation
   of ideas about the key moments of life and value priorities, conditioned
   by information from the media and by the recipient's personal
   interpretation. The data obtained in the study indicate that the main
   sources of information about the Republic of Crimea are the media,
   broadcasting a predominantly one-sided discourse, which prevents
   creation of a holistic image of the peninsula in the world model of a
   person.
   Discussion and Conclusion. The study has revealed the need to adjust the
   media image of Crimea in accordance with modern circumstances and to
   increase its attractive characteristics, primarily for younger
   audiences. Changes in the information policy of the state in relation to
   Crimea are required as well as restructuring of the work of editorial
   offices and provision of a targeted flow at the federal and regional
   levels to improve the reputation characteristics of the region and to
   adjust popular attitudes towards and perceptions of the image of the
   republic. The materials of the article can be used in fundamental
   interdisciplinary research into to the images of Russia and Crimea, and
   in professional activities of PR specialists and journalists.},
DOI = {10.15507/2413-1407.112.028.202003.516-542},
ISSN = {2413-1407},
EISSN = {2587-8549},
ResearcherID-Numbers = {Ерофеева, Irina/J-9218-2013
   },
ORCID-Numbers = {Ерофеева, Irina/0000-0001-5653-2792
   Melnik, Galina/0000-0001-5653-8668},
Unique-ID = {WOS:000593228500005},
}

@article{ WOS:000553022400006,
Author = {Ilyushin, G. D.},
Title = {Intermetallic Compounds NakMn(\textbackslash{}M =K, Cs, Ba, Ag, Pt, Au,
   Zn, Bi, Sb): Geometrical and Topological Analysis, Cluster Precursors,
   and Self-Assembly of Crystal Structures},
Journal = {CRYSTALLOGRAPHY REPORTS},
Year = {2020},
Volume = {65},
Number = {4},
Pages = {539-545},
Month = {JUL},
Abstract = {A geometrical and topological analysis has been performed, and
   self-assembly of the crystal structures of intermetallic compounds
   formed in the Na-M (M = K, Cs, Ba, Ag, Pt, Au, Zn, Bi, Sb) systems has
   been simulated using computer methods (the ToposPro software). The NakMn
   precursor metal clusters of the crystal structures are determined based
   on the algorithms of structural-graph decomposition in cluster
   structures and construction of the basic net of the structure in the
   form of a graph whose nodes correspond to the cluster centers. The
   tetrahedral metal clusters M-4, forming packets in the Na3Bi-hP8,
   Na2Bi2-tP4, and Na2Sb2-mP16 crystal structures; tetrahedral metal
   clusters M-4 and spacers for the Na-2(Au-4)-cF24 and Ba-2(Na-4)-hP12
   framework structures; octahedral clusters M-6 for the Na4Au2-tI12
   structure; octahedral M-6 and tetrahedral M-4 metal clusters for the
   Na-2(Na-4)(Ba-6)-cF96 and Au-2(In-4)(Na-6)-cF96 structures; and
   icosahedral metal clusters M-13 for the Na(Zn-13)-cF116 structure are
   found. The symmetry and topology code of the self-assembly of crystal
   structures of NakMn intermetallic compounds has been reconstructed from
   precursor metal clusters S-3(0) in the following form: primary chain
   S-3(1) -> microlayer S-3(2) -> microframework S-3(3).},
DOI = {10.1134/S1063774520030128},
ISSN = {1063-7745},
EISSN = {1562-689X},
Unique-ID = {WOS:000553022400006},
}

@article{ WOS:000552134500016,
Author = {Karmakar, Supriya},
Title = {Simulator of semiconductor devices for multivalued logic},
Journal = {IET CIRCUITS DEVICES \& SYSTEMS},
Year = {2020},
Volume = {14},
Number = {4},
Pages = {528-536},
Month = {JUL},
Abstract = {Multivalued logic (MVL) is an effective way to increase device
   integration in semiconductor circuits. The major problem for MVL
   implementation is the availability of proper semiconductor devices.
   Different quantum structures and their application in different
   semiconductor devices help them implement the MVL circuit. In this work,
   the designed graphic user interface will provide more flexibility to the
   user to highlight the application of different semiconductor devices for
   MVL implementation. Users can experience the variation of device's
   characteristics and their feasibility in the application of MVL
   effectively with this software package.},
DOI = {10.1049/iet-cds.2019.0415},
ISSN = {1751-858X},
EISSN = {1751-8598},
Unique-ID = {WOS:000552134500016},
}

@article{ WOS:000547051300001,
Author = {Wang, Kewen and Wang, Xue and Xu, Zhenzhen and Yang, Shuming},
Title = {Simultaneous determination of multi-class antibiotics and steroid
   hormones drugs in livestock and poultry faeces using liquid
   chromatography-quadrupole time-of-flight mass spectrometry},
Journal = {FOOD ADDITIVES AND CONTAMINANTS PART A-CHEMISTRY ANALYSIS CONTROL
   EXPOSURE \& RISK ASSESSMENT},
Year = {2020},
Volume = {37},
Number = {9},
Pages = {1467-1480},
Month = {SEP 1},
Abstract = {A method for simultaneous determination of multi-class antibiotics and
   steroid hormone analysis in faeces of livestock and poultry was
   developed using liquid chromatography-quadrupole time-of-flight mass
   spectrometry (LC-QTOF MS). An in-house database was built for 156
   detected drugs using Personal Compound Database Library software (PCDL)
   including compound name, monoisotopic mass, chemical formula, RT,
   chemical structure and three CID MS/MS spectra. The linearity result
   showed that all the drugs exhibited good linearity with determination
   coefficients (R-2) higher than 0.99. The drug recoveries and their RSDs
   for all three faeces samples (pig, cattle and chicken) were tested and
   81, 96 and 92 drugs were chosen for analysis in pig, cattle and chicken
   faeces, respectively. Further validation showed that 73 veterinary drugs
   in all three kinds of faeces samples can be quantified in one analytical
   run. This work shows that qualitative and quantitative analysis using
   LC-QTOF MS represents a simple, sensitive, low-cost and high-throughput
   methodology in routine laboratory analyses.},
DOI = {10.1080/19440049.2020.1776900},
EarlyAccessDate = {JUN 2020},
ISSN = {1944-0049},
EISSN = {1944-0057},
ORCID-Numbers = {Wang, Kewen/0000-0001-5139-0596},
Unique-ID = {WOS:000547051300001},
}

@article{ WOS:000531020300007,
Author = {Kouzapas, Dimitrios and Skitsas, Constantinos and Saeed, Taqwa and
   Soteriou, Vassos and Lestas, Marios and Philippou, Anna and Abadal,
   Sergi and Liaskos, Christos and Petrou, Loukas and Georgiou, Julius and
   Pitsillides, Andreas},
Title = {Towards fault adaptive routing in metasurface controller networks},
Journal = {JOURNAL OF SYSTEMS ARCHITECTURE},
Year = {2020},
Volume = {106},
Month = {JUN},
Note = {11th International Workshop on Network on Chip Architectures (NoCArc),
   Fukuoka, JAPAN, OCT 20, 2018},
Abstract = {HyperSurfaces (HSFs) comprise structurally reconfigurable metasurfaces
   whose electromagnetic properties can be changed via a software
   interface, using an embedded miniaturized network of controllers,
   enabling novel capabilities in wireless communications, including 5G
   applications. Resource constraints associated with a hardware testbed of
   this breakthrough technology, currently under development, necessitate
   an interconnect architecture of a Network of Controllers (CN) that is
   distinct from, yet reminiscent to, those of conventional Network-on-Chip
   (NoC) architectures. To meet the purposes of our HSF testbed, we
   rationalize the construction of an irregular topology where its
   controllers are interconnected in a Manhattan-like geometry, with the
   flow of control directives conducted in a handshaking mode, and routing
   operated by an XY-YX algorithm that is agnostic of the CN connectivity,
   determined following the results of model specification and model
   checking techniques. With such controllers prone to the appearance of
   permanent faults, threatening the operation of such HSFs, we propose,
   develop and evaluate two fault adaptive routing algorithms aiming to
   enhance the successful delivery of packetized control directives to
   their recipients: (1) Loop Free Algorithm (LFA), and (2) Reliable
   Delivery Algorithm (RDA) of deterministic and probabilistic variants.
   LFA and RDA are developed based on utilizing said topology-agnostic
   XY-YX routing algorithm as a base, along with an appropriate adoption of
   routing turn rules, to address said HSF CN challenges that deviate from
   traditional fault tolerant routing algorithms seen in NoCs. Experimental
   evaluation results obtained using a custom developed simulator, show
   that probabilistic RDA exhibits top performance in terms of successful
   packet delivery ratio and topology coverage, albeit at the expense of a
   higher path hop count. Pointers in addressing tradeoffs between HSF CN
   performance and resource utilization are also provided.},
DOI = {10.1016/j.sysarc.2019.101703},
Article-Number = {101703},
ISSN = {1383-7621},
EISSN = {1873-6165},
ResearcherID-Numbers = {Liaskos, Christos/Q-7351-2017
   Petrou, Loukas/ABF-3781-2021
   Abadal, Sergi/L-6004-2014},
ORCID-Numbers = {Liaskos, Christos/0000-0002-1271-8613
   Petrou, Loukas/0000-0003-1999-5733
   Abadal, Sergi/0000-0003-0941-0260},
Unique-ID = {WOS:000531020300007},
}

@article{ WOS:000551495800151,
Author = {Munoz-Ruiperez, Carmelo and Fiol Olivan, Francisco and Calderon
   Carpintero, Veronica and Santamaria-Vicario, Isabel and Rodriguez Saiz,
   Angel},
Title = {Mechanical Behavior of a Composite Lightweight Slab, Consisting of a
   Laminated Wooden Joist and Ecological Mortar},
Journal = {MATERIALS},
Year = {2020},
Volume = {13},
Number = {11},
Month = {JUN},
Abstract = {The investigation reported in this paper is an evaluation of the
   mechanical behavior of full-scale ecological mortar slabs manufactured
   with a mixture of expanded clay and recycled concrete aggregates. The
   composite mortars form a compressive layer over laminated wooden joists
   to form a single construction unit. To do so, full-scale flexural tests
   are conducted of the composite laminated wood-ecological mortar slabs
   with different types of mortar designs: reference mortar (MR),
   lightweight mortar dosed with recycled concrete aggregates (MLC), and
   lightweight mortar dosed with recycled mixed aggregates (MLM). The test
   results showed that the mortar forming the compression layer and the
   laminated wooden joists worked in unison and withstood a higher maximum
   failure load under flexion than the failure load of the wooden joists in
   isolation. Moreover, the laboratory test results were compared with the
   simulated values of the theoretical model, generated in accordance with
   the technical specifications for structural calculations contained in
   the Spanish building code, and with the results calculated by a computer
   software package. From the analysis of the results of the calculation
   methods and the full-scale laboratory test results, it was concluded
   that the safety margin yielded by the calculations validated the use of
   those methods on this type of composite slab. In this way, a strong
   mixed wood-mortar slab was designed, contributing little dead-load to
   the building structure and its manufacture with recycled aggregate, also
   contributes to the circular economy of construction materials.},
DOI = {10.3390/ma13112575},
Article-Number = {2575},
EISSN = {1996-1944},
ResearcherID-Numbers = {Ruiperz, Carmelo Muñoz/AAG-1310-2021
   Calderon, Veronica/P-3428-2019
   },
ORCID-Numbers = {Calderon, Veronica/0000-0001-6863-5473
   Rodriguez Saiz, Angel/0000-0002-3607-2167
   Santamaria-Vicario, Isabel/0000-0002-3576-2107},
Unique-ID = {WOS:000551495800151},
}

@article{ WOS:000544103200007,
Author = {van Binsbergen, L. Thomas and Scott, Elizabeth and Johnstone, Adrian},
Title = {Purely functional GLL parsing},
Journal = {JOURNAL OF COMPUTER LANGUAGES},
Year = {2020},
Volume = {58},
Month = {JUN},
Abstract = {Generalised parsing has become increasingly important in the context of
   software language design and several compiler generators and language
   workbenches have adopted generalised parsing algorithms such as GLR and
   GLL. The original GLL parsing algorithms are described in low-level
   pseudo -code as the output of a parser generator. This paper explains
   GLL parsing differently, defining the FUN-GLL algorithm as a collection
   of pure, mathematical functions and focussing on the logic of the
   algorithm by omitting implementation details. In particular, the data
   structures are modelled by abstract sets and relations rather than
   specialised implementa- tions. The description is further simplified by
   omitting lookahead and adopting the binary subtree representation of
   derivations to avoid the clerical overhead of graph construction.
   Conventional parser combinators inherit the drawbacks from the recursive
   descent algorithms they imple- ment. Based on FUN-GLL, this paper
   defines generalised parser combinators that overcome these problems. The
   algorithm is described in the same notation and style as FUN-GLL and
   uses the same data structures. Both algorithms are explained as a
   generalisation of basic recursive descent algorithms. The generalised
   parser combinators of this paper have several advantages over combinator
   libraries that generate internal grammars. For example, with the
   generalised parser combinators it is possible to parse larger
   permutation phrases and to write parsers for languages that are not
   context -free. The ?BNF combinator library? is built around the
   generalised parser combinators. With the library, embedded and
   executable syntax specifications are written. The specifications contain
   semantic actions for interpreting programs and constructing syntax
   trees. The library takes advantage of Haskell?s type -system to type
   -check semantic actions and Haskell?s abstraction mechanism enables
   ?reuse through abstraction?. The practicality of the library is
   demonstrated by running parsers obtained from the syntax descriptions of
   several software languages.},
DOI = {10.1016/j.cola.2020.100945},
Article-Number = {100945},
ISSN = {2590-1184},
EISSN = {2665-9182},
ORCID-Numbers = {van Binsbergen, L. Thomas/0000-0001-8113-2221},
Unique-ID = {WOS:000544103200007},
}

@article{ WOS:000519653400009,
Author = {Chiacchio, Ferdinando and Ignacio Aizpurua, Jose and Compagno, Lucio and
   D'Urso, Diego},
Title = {SHyFTOO, an object-oriented Monte Carlo simulation library for the
   modeling of Stochastic Hybrid Fault Tree Automaton},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2020},
Volume = {146},
Month = {MAY 15},
Abstract = {Dependability assessment is a crucial activity to ensure the correct
   operation of complex systems. The output of dependability assessment
   activities include the quantification of reliability, availability,
   maintenance and safety related metrics. These metrics can assist in the
   identification of the system weak points or in the conception of
   mitigation strategies to increase the system dependability level. The
   development of advanced computer-aided methodologies to support
   dependability assessment activities is essential to automate and reduce
   the efforts implied by this process and similarly, the development of
   accurate dependability assessment methods is very important to increase
   the quality of the results. In this context, it is possible to identify
   different contributions that improve the dependability assessment
   through general-purpose modeling methodologies. However, existing
   solutions are ad-hoc applications specified with low-level stochastic
   formalisms and this complicates their adoption in the industry.
   Accordingly, this paper presents Stochastic Hybrid Fault Tree Automaton
   (SHyFTA) based simulation algorithm that allows the accurate
   dependability analysis of repairable multi-state systems. SHyFTA
   integrates the stochastic and deterministic operation of the system
   under study as well as their interactions. The algorithm is formalized
   through an object-oriented software architecture, which is developed as
   a software library for the modeling and simulation of repairable SHyFTA
   models. Following the proposed architecture, a Matlab implementation of
   this library, SHyFTOO, has been developed and validated with a thorough
   test campaign. In order to provide a guideline to the end-users and show
   the potential of the SHyFTOO library, the case study of a feed-water
   pumping system is implemented in detail and it is used to evaluate
   different preventive maintenance policies. The SHyFTOO library can open
   the way to further investigations that address the interactions between
   the failure behavior and the functional operation of a system and their
   combined effect on system dependability. (C) 2019 Elsevier Ltd. All
   rights reserved.},
DOI = {10.1016/j.eswa.2019.113139},
Article-Number = {113139},
ISSN = {0957-4174},
EISSN = {1873-6793},
ResearcherID-Numbers = {Aizpurua, Jose Ignacio/B-7110-2019
   D'Urso, Diego/GRN-8256-2022
   },
ORCID-Numbers = {Aizpurua, Jose Ignacio/0000-0002-8653-6011
   D'Urso, Diego/0000-0002-1448-5956
   chiacchio, ferdinando/0000-0002-0281-1291},
Unique-ID = {WOS:000519653400009},
}

@article{ WOS:000543421400313,
Author = {Ali, Jehad and Lee, Gyu-Min and Roh, Byeong-Hee and Ryu, Dong Kuk and
   Park, Gyudong},
Title = {Software-Defined Networking Approaches for Link Failure Recovery: A
   Survey},
Journal = {SUSTAINABILITY},
Year = {2020},
Volume = {12},
Number = {10},
Month = {MAY},
Abstract = {Deployment of new optimized routing rules on routers are challenging,
   owing to the tight coupling of the data and control planes and a lack of
   global topological information. Due to the distributed nature of the
   traditional classical internet protocol networks, the routing rules and
   policies are disseminated in a decentralized manner, which causes
   looping issues during link failure. Software-defined networking (SDN)
   provides programmability to the network from a central point.
   Consequently, the nodes or data plane devices in SDN only forward
   packets and the complexity of the control plane is handed over to the
   controller. Therefore, the controller installs the rules and policies
   from a central location. Due to the central control, link failure
   identification and restoration becomes pliable because the controller
   has information about the global network topology. Similarly, new
   optimized rules for link recovery can be deployed from the central
   point. Herein, we review several schemes for link failure recovery by
   leveraging SDN while delineating the cons of traditional networking. We
   also investigate the open research questions posed due to the SDN
   architecture. This paper also analyzes the proactive and reactive
   schemes in SDN using the OpenDayLight controller and Mininet, with the
   simulation of application scenarios from the tactical and data center
   networks.},
DOI = {10.3390/su12104255},
Article-Number = {4255},
EISSN = {2071-1050},
ResearcherID-Numbers = {Roh, Byeong-hee/V-4966-2018
   Ali, Jehad/V-9707-2019
   },
ORCID-Numbers = {Ali, Jehad/0000-0002-0589-7924
   Roh, Byeong-hee/0000-0003-2509-4210},
Unique-ID = {WOS:000543421400313},
}

@article{ WOS:000527751000001,
Author = {Sherif, Mohtady and Othman, Hesham and Marzouk, Hesham and Aoude, Hassan},
Title = {Design guidelines and optimization of ultra-high-performance
   fibre-reinforced concrete blast protection wall panels},
Journal = {INTERNATIONAL JOURNAL OF PROTECTIVE STRUCTURES},
Year = {2020},
Volume = {11},
Number = {4},
Pages = {494-514},
Month = {DEC},
Abstract = {Ultra-high-performance fibre-reinforced concrete is the latest
   generation of structural concrete, having outstanding fresh and hardened
   properties; this includes the ease of placement and consolidation with
   ultra-high mechanical properties, as well as toughness, volume
   stability, durability, higher flexural and tensile strength, and
   ductility. As more research is being focused on it, the material
   behaviour and characteristics are getting more understood, and the
   research demand for the special applications of the
   ultra-high-performance fibre-reinforced concrete is growing higher. One
   special application that ultra-high-performance fibre-reinforced
   concrete is thought to have an outstanding performance at is in the
   field of protective structures, specifically against blast loads. This
   article presents part of a study that is concerned with the behaviour
   and response of ultra-high-performance fibre-reinforced concrete wall
   panels under blast load. Size and shape optimization techniques were
   combined in this study to optimize the design of a 200-MPa
   ultra-high-performance fibre-reinforced concrete under blast loads using
   finite element modelling. This design optimization aims to maximize
   stiffness and minimize the cost while satisfying both design stresses
   and construction requirements. The design variable to be optimized for
   are the thickness ranging from 100 to 300 mm at 25 mm increments, in
   addition to the reinforcement ratio of 0\%, 0.2\%, 1\% and 3\%, and
   aspect ratio of 1, 1.5 and 2; the boundary condition is four edges fixed
   and restrained. The numerical simulation has been performed using an
   explicate finite element software package. The complete behaviour of an
   ultra-high-performance fibre-reinforced concrete is defined using the
   concrete damaged plasticity model. The concrete constitutive model has
   been developed considering the contribution of tensile hardening
   response, fracture energy and crack-band width approaches to accurately
   represent the tensile behaviour and guarantee mesh independence of
   results. The blast load is applied using the Conventional Weapons method
   of the US Army Corps of Engineers that is readily available in the
   finite element software. The validity of the numerical model used is
   verified by comparing numerical results to experimental data.},
DOI = {10.1177/2041419620912751},
EarlyAccessDate = {APR 2020},
Article-Number = {2041419620912751},
ISSN = {2041-4196},
EISSN = {2041-420X},
Unique-ID = {WOS:000527751000001},
}

@article{ WOS:000526313000034,
Author = {De Santis, Matteo and Storchi, Loriano and Belpassi, Leonardo and
   Quiney, Harry M. and Tarantelli, Francesco},
Title = {PyBERTHART: A Relativistic Real-Time Four-Component TDDFT Implementation
   Using Prototyping Techniques Based on Python},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Year = {2020},
Volume = {16},
Number = {4},
Pages = {2410-2429},
Month = {APR 14},
Abstract = {We present a real-time time-dependent four-component Dirac-Kohn-Sham
   (RT-TDDKS) implementation based on the BERTHA code. This new
   implementation takes advantage of modern software engineering, including
   the prototyping techniques. The software design follows a three step
   approach: (i) the prototype implementation of a time-propagation
   algorithm in nonrelativistic real-time TDDFT within the Psi4NumPy
   framework, which provides a suitable environment for the creation of a
   clear, readable, and easy to test reference code in Python, (ii) the
   design of an original Python application programming interface for the
   relativistic four-component code BERTHA (PyBERTHA), which has an
   efficient computational kernel for relativistic integrals written in
   FORTRAN, and (iii) the porting of the time-propagation scheme enveloped
   within the Psi4NumPy framework to PyBERTHA. The propagation scheme
   consequently resides in a single readable Python computer code that is
   easy to maintain and in which the key quantities, such as the
   Dirac-Kohn-Sham and dipole matrices, can be accessed directly from the
   PyBERTHA module. For linear algebra operations (matrix-matrix
   multiplications and diagonalization) we use the highly optimized
   procedures implemented in the popular NumPy library. The overhead
   introduced by the Python interface to BERTHA is almost negligible (less
   than 1\% evaluated on the SCF procedure), and the interoperability
   between different programming languages (FORTRAN, C, and Python) does
   not affect the numerical stability of the time-propagation scheme. Our
   new RT-TDDKS implementation has been employed to investigate the
   stability of the time-propagation procedure in combination with a
   density-fitting algorithm (both for the Coulomb and for the
   exchange-correlation matrix construction), which are employed in BERTHA
   to speed up the Dirac-Kohn-Sham matrix evaluation. On the basis of
   systematic calculations, employing several density-fitting basis sets of
   increasing accuracy, we showed that quantitative agreement can be
   achieved in combination with extended-fitting basis sets, with an error
   in the Coulomb energy below 1 mu-hartree. Convergence of the transition
   energies increasing of quality of the fitting basis sets has been also
   observed. Our data suggest that the error in the Coulomb energy may also
   represent a good estimate of the fitting basis set quality for real-time
   electron dynamics simulations. Further, we study the applicability of
   the RT-TDDKS method in combination with both weak- and extreme
   strong-field regime. Numerical results of excited-state transitions for
   the Group 12 atoms are reported and compared with a previous real-time
   Dirac-Kohn-Sham implementation (Repisky et al. J. Chem. Theory Comput.
   2015, 11, 980-991). Finally, calculations of high harmonic generation in
   the hydrogen molecule and Au dimer have been also carried out. We were
   able to generate high harmonics with relatively well-defined peaks up to
   the 21st and 13th order in the case of H-2 and Au-2, respectively. Our
   findings show that the four-component structure of the Dirac-Kohn-Sham
   Hamiltonian provides a suitable theoretical framework, with no intrinsic
   unfavorable features, to study molecules in the strong-field regime.},
DOI = {10.1021/acs.jctc.0c00053},
ISSN = {1549-9618},
EISSN = {1549-9626},
ResearcherID-Numbers = {belpassi, Leonardo/AAU-2760-2020
   Quiney, Harry/GWD-0987-2022
   Storchi, Loriano/K-3557-2018
   Tarantelli, Francesco/H-5798-2013
   Belpassi, Leonardo/Q-1153-2016},
ORCID-Numbers = {Storchi, Loriano/0000-0001-5021-7759
   Tarantelli, Francesco/0000-0002-1285-0606
   Belpassi, Leonardo/0000-0002-2888-4990},
Unique-ID = {WOS:000526313000034},
}

@article{ WOS:000518368800048,
Author = {Burkert, V. D. and Elouadrhiri, L. and Adhikari, K. P. and Adhikari, S.
   and Amaryan, M. J. and Anderson, D. and Angelini, G. and Antonioli, M.
   and Atac, H. and Aune, S. and Avakian, H. and Gayoso, C. Ayerbe and
   Baltzell, N. and Barion, L. and Battaglieri, M. and Baturin, V and
   Bedlinskiy, I and Benmokhtar, F. and Bianconi, A. and Biselli, A. S. and
   Bonneau, P. and Bossu, F. and Boyarinov, S. and Briscoe, W. J. and
   Brooks, W. K. and Bruhwel, K. and Carman, D. S. and Celentano, A. and
   Charles, G. and Chatagnon, P. and Chetry, T. and Christiaens, G. and
   Christo, S. and Ciullo, G. and Clary, B. A. and Cole, P. L. and
   Contalbrigo, M. and Cook, M. and Crede, V and Cruz-Torres, R. and
   Cuevas, C. and D'Angelo, A. and Dashyan, N. and Defurne, M. and Deur, A.
   and De Vita, R. and Diehl, S. and Djalali, C. and Dodge, G. and Dupre,
   R. and Ehrhart, M. and El Fassi, L. and Eng, B. and Ewing, T. and Fair,
   R. and Fedotov, G. and Filippi, A. and Forest, T. A. and Garcon, M. and
   Gavalian, G. and Ghoshal, P. and Gilfoyle, G. P. and Giovanetti, K. and
   Girod, F. X. and Glazier, I, D. and Golovatch, E. and Gothe, R. W. and
   Gotra, Y. and Griffioen, K. A. and Guidal, M. and Gyurjyan, V and
   Hafidi, K. and Hakobyan, H. and Hanretty, C. and Harrison, N. and
   Hattawy, M. and Hauenstein, F. and Hayward, T. B. and Heddle, D. and
   Hemler, P. and Hen, O. A. and Hicks, K. and Hobart, A. and Hogan, J. and
   Holtrop, M. and Ilieva, Y. and Illari, I. and Insley, D. and Ireland, D.
   G. and Ishkhanov, B. S. and Isupov, E. L. and Jacobs, G. and Jo, H. S.
   and Johnston, R. and Joo, K. and Joosten, S. and Kageya, T. and Kashy,
   D. and Keith, C. and Keller, D. and Khachatryan, M. and Khanal, A. and
   Kim, A. and Kim, C. W. and Kim, W. and Kubarovsky, V and Kuhn, S. E. and
   Lanza, L. and Leffel, M. and Lucherini, V and Lung, A. and Kabir, M. L.
   and Leali, M. and Lee, S. and Lenisa, P. and Livingston, K. and Lowry,
   M. and MacGregor, I. J. D. and Mandjavidze, I and Marchand, D. and
   Markov, N. and Mascagna, V and McKinnon, B. and McMullen, M. and Mealer,
   C. and Mestayer, M. D. and Meziani, Z. E. and Miller, R. and Milner, R.
   G. and Mineeva, T. and Mirazita, M. and Mokeev, V and Moran, P. and
   Movsisyan, A. and Camacho, C. Munoz and Naidoo, P. and Nanda, S. and
   Newton, J. and Niccolai, S. and Niculescu, G. and Osipenko, M. and
   Paolone, M. and Pappalardo, L. L. and Paremuzyan, R. and Pastor, O. and
   Pasyuk, E. and Phelps, W. and Pogorelko, O. and Poudel, J. and Price, J.
   W. and Price, K. and Procureur, S. and Prok, Y. and Protopopescu, D. and
   Rajput-Ghoshal, R. and Raue, B. A. and Raydo, B. and Ripani, M. and
   Ritman, J. and Rizzo, A. and Rosner, G. and Rossi, P. and Rowley, J. and
   Roy, B. J. and Sabatie, F. and Salgado, C. and Schadmand, S. and
   Schmidt, A. and Segarra, E. P. and Sergeyeva, V and Sharabian, Y. G. and
   Shrestha, U. and Skorodumina, Iu and Smith, G. D. and Smith, L. C. and
   Sokhan, D. and Soto, O. and Sparveris, N. and Stepanyan, S. and Stoler,
   P. and Strauch, S. and Tan, J. A. and Taylor, M. and Tilles, D. and
   Turisini, M. and Tyler, N. and Ungaro, M. and Venturelli, L. and
   Voskanyan, H. and Voutier, E. and Watts, D. and Wei, X. and Weinstein,
   L. B. and Wiggins, C. and Wiseman, M. and Wood, M. H. and Yegneswaran,
   A. and Young, G. and Zachariou, N. and Zarecky, M. and Zhang, J. and
   Zhao, Z. W. and Ziegler, V},
Title = {The CLAS12 Spectrometer at Jefferson Laboratory},
Journal = {NUCLEAR INSTRUMENTS \& METHODS IN PHYSICS RESEARCH SECTION
   A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
Year = {2020},
Volume = {959},
Month = {APR 11},
Abstract = {The CEBAF Large Acceptance Spectrometer for operation at 12 GeV beam
   energy (CLAS12) in Hall B at Jefferson Laboratory is used to study
   electro-induced nuclear and hadronic reactions. This spectrometer
   provides efficient detection of charged and neutral particles over a
   large fraction of the full solid angle. CLAS12 has been part of the
   energy-doubling project of Jefferson Lab's Continuous Electron Beam
   Accelerator Facility, funded by the United States Department of Energy.
   An international collaboration of 48 institutions contributed to the
   design and construction of detector hardware, developed the software
   packages for the simulation of complex event patterns, and commissioned
   the detector systems. CLAS12 is based on a dual-magnet system with a
   superconducting torus magnet that provides a largely azimuthal field
   distribution that covers the forward polar angle range up to 35 degrees,
   and a solenoid magnet and detector covering the polar angles from 35
   degrees to 125 degrees with full azimuthal coverage. Trajectory
   reconstruction in the forward direction using drift chambers and in the
   central direction using a vertex tracker results in momentum resolutions
   of <1\% and <3\%, respectively. Cherenkov counters, time-of-flight
   scintillators, and electromagnetic calorimeters provide good particle
   identification. Fast triggering and high data-acquisition rates allow
   operation at a luminosity of 10(35) cm(-2) s(-1). These capabilities are
   being used in a broad program to study the structure and interactions of
   nucleons, nuclei, and mesons, using polarized and unpolarized electron
   beams and targets for beam energies up to 11 GeV. This paper gives a
   general description of the design, construction, and performance of
   CLAS12.},
DOI = {10.1016/j.nima.2020.163419},
Article-Number = {163419},
ISSN = {0168-9002},
EISSN = {1872-9576},
ResearcherID-Numbers = {Mascagna, Valerio/AAC-9914-2019
   Gyurjyan, Vardan/GSD-5715-2022
   Mascagna, Valerio/HLQ-1103-2023
   Turisini, Matteo/AAK-3160-2020
   Pappalardo, Luciano/AAB-2380-2021
   Lanza, Lucilla/E-6479-2017
   Jo, Hyon-Suk/HGC-7070-2022
   D'Angelo, Annalisa/A-2439-2012
   Deur, Alexandre/H-9778-2019
   Celentano, Andrea/J-6190-2012
   Brooks, William K/C-8636-2013
   Rosner, Guenther/ABB-5516-2021
   Rizzo, Alessandro/C-6397-2014
   Lanza, Lucilla/AAA-3006-2021
   Ciullo, Giuseppe/X-6539-2018
   Ireland, David/E-8618-2010
   Bashkanov, Mikhail/R-1333-2018
   Filippi, Alessandra/I-9530-2012
   },
ORCID-Numbers = {Mascagna, Valerio/0000-0003-3612-1608
   Mascagna, Valerio/0000-0003-3612-1608
   Turisini, Matteo/0000-0002-5422-1891
   Lanza, Lucilla/0000-0002-1280-532X
   D'Angelo, Annalisa/0000-0003-3050-4907
   Deur, Alexandre/0000-0002-2203-7723
   Celentano, Andrea/0000-0002-7104-2983
   Brooks, William K/0000-0001-6161-3570
   Rizzo, Alessandro/0000-0001-5597-8514
   Lanza, Lucilla/0000-0002-1280-532X
   Battaglieri, Marco/0000-0001-5002-8771
   POUDEL, JIWAN/0000-0001-6345-2360
   Soto, Orlando/0000-0002-8613-0310
   Ehrhart, Mathieu/0000-0002-8076-9690
   Benmokhtar, Fatiha/0000-0003-3670-4045
   Paolone, Michael/0000-0002-8980-6670
   Ciullo, Giuseppe/0000-0001-8297-2206
   Contalbrigo, Marco/0000-0002-8612-7998
   Schmidt, Axel/0000-0002-1109-2954
   Ireland, David/0000-0001-7713-7011
   Isupov, Evgeny/0000-0001-7524-2408
   ATAC, HAMZA/0000-0002-8945-5305
   Bashkanov, Mikhail/0000-0001-9822-9433
   Meziani, Zein-Eddine/0000-0001-9450-2914
   Keith, Christopher/0000-0002-5467-2983
   Filippi, Alessandra/0000-0003-4715-8748
   Lee, Sangbaek/0000-0001-6038-0736
   Hen, Or/0000-0002-4890-6544
   Ayerbe Gayoso, Carlos/0000-0001-8640-5380},
Unique-ID = {WOS:000518368800048},
}

@article{ WOS:000524607500001,
Author = {Abdel-Galil, Emad and Ibrahim, Ahmed H. and Alborkan, Ahmed},
Title = {Assessment of transaction costs for construction projects},
Journal = {INTERNATIONAL JOURNAL OF CONSTRUCTION MANAGEMENT},
Year = {2022},
Volume = {22},
Number = {9},
Pages = {1618-1631},
Month = {JUL 4},
Abstract = {The main premise of transaction cost economics is that project cost is
   not only production costs but there are also additional costs resulting
   from transactions between parties. Transaction costs include the costs
   of preparing the bidding package, estimating and drawing up a contract,
   administering the contract, dealing with any deviations from contract
   conditions and any contractual problems include claims, change orders,
   and disputes. The main objectives of this paper are to establish the
   factors that affecting transaction costs in construction projects in
   Egypt, assessing its value in Egyptian construction projects, and to
   provide a mathematical model that can predict transaction costs value
   accurately. The study was conducted on various types of construction
   projects in Egypt, which formed the sample size. To collect data, a
   questionnaire was used that was personally administered to the
   respondents, using emails and conducting semi-structured interviews. The
   collected data were analyzed by using SPSS program, and a software
   package named IBM. The findings indicated that: the most important
   factors affecting positively (decreasing transaction cost value) on
   transaction cost value in construction projects in Egypt are: the
   financial position of the contractor, claims filed by the contractor,
   material substitutions by the contractor, contractor experience in
   similar type projects, contractor`s relationship with previous clients,
   contractor qualification to do the job, and procurement method.
   Transaction cost in the pre-contract phase of construction projects in
   Egypt is approximately (3-4) \% of the contract value on average, and
   the average value for post-contract transaction cost is approximately
   (8-9) \% of the contract value. All factors accredited as influential
   factors were taken into account and developed in an account model, the
   study devolved accounting models that can predict transaction cost value
   as a percentage of the project capital for construction projects in
   Egypt, two techniques were used, first is regression analysis with two
   methods (enter-stepwise), and second is the neural network, all models
   showed a high accuracy up to 95\% in predicting transaction cost value
   for construction projects in Egypt.},
DOI = {10.1080/15623599.2020.1738204},
EarlyAccessDate = {APR 2020},
ISSN = {1562-3599},
EISSN = {2331-2327},
Unique-ID = {WOS:000524607500001},
}

@article{ WOS:000532593200003,
Author = {Frimpong, Ivy Priscilla and Asante, Matilda and Maduforo, Aloysius
   Nwabugo},
Title = {Dietary intake as a cardiovascular risk factor: a cross-sectional study
   of bank employees in Accra},
Journal = {SOUTH AFRICAN JOURNAL OF CLINICAL NUTRITION},
Year = {2020},
Volume = {33},
Number = {2},
Pages = {44-50},
Month = {APR 2},
Abstract = {Objectives: To determine the dietary and nutrient intakes of bank
   employees in Accra in relation to recommended dietary intake for the
   control of cardiovascular diseases (CVD). Design: The study was a
   cross-sectional study. Methodology: A structured validated questionnaire
   was used to obtain demographics. A 3-day 24-hour dietary recall and
   usual food intake questionnaire were used to obtain information on food
   intakes and dietary pattern of the respondents. Food models, household
   measures and photos of common dishes as well as household cups and
   measures were used in order to obtain accurate information regarding the
   types and quantities of foods and beverages consumed. Subjects:
   Convenient sampling was used to select bank branches while simple random
   sampling by ballot without replacement was used to select 119 bank
   employees who consented to participate in the study. Outcome measures:
   Data were analysed using Statistical Package for the Social Sciences
   software. Esha FPro software was used to analyse food nutrients. The
   nutrient intakes were compared with a standard dietary guideline for
   adults. Results: Findings showed significant (p < 0.05) differences in
   mean energy intake between males and females. Also, the bank employees
   were consuming more fat (32\%) compared with the recommended intake. The
   average intake of dietary sodium and cholesterol was within the
   recommended intake levels. All the participants in the study had a mean
   intake of dietary potassium, fibre, fruits and vegetables below the
   recommended levels. The major cooking oils used in meal preparation were
   vegetable and palm oil (46\%), followed by vegetable oil (31.9\%) and
   palm oil (21.8\%). Conclusions: The study evaluated the dietary intake
   of bank employees, which is an important risk factor for chronic
   diseases. There is a need to develop plans to provide nutrition
   education and counselling for adequate nutrient intake and prevention of
   chronic diseases among bank employees. Layman's summary: Obtaining an
   adequate diet and avoiding overconsumption of food helps to maintain
   health. Intake of foods containing high energy and fats without
   corresponding energy expenditure through physical activity could result
   in obesity, heart diseases, diabetes and other health challenges.
   Inadequate intake of micronutrients as well could result in deficiency
   diseases, and depresses the immune system and health of individuals.
   Bank employees and all other individuals need to pay attention to what
   they eat to live a healthy and longer life. Regular physical activity,
   following the dietary approaches to avoid hypertension and regular
   medical check-up are essential for cardiovascular health. Strong lay
   message: Inadequate or overconsumption of nutrients can cause chronic
   heart diseases.},
DOI = {10.1080/16070658.2018.1515394},
ISSN = {1607-0658},
EISSN = {2221-1268},
ResearcherID-Numbers = {Maduforo, Aloysius/AGE-8188-2022
   },
ORCID-Numbers = {Maduforo, Aloysius/0000-0001-7290-5632
   Asante, Matilda/0000-0003-3488-7087},
Unique-ID = {WOS:000532593200003},
}

@article{ WOS:000520385500015,
Author = {Delavar, Mohammad and Bitsuamlak, Girma T. and Dickinson, John K. and
   Costa, Leandro Malveira F.},
Title = {Automated BIM-based process for wind engineering design collaboration},
Journal = {BUILDING SIMULATION},
Year = {2020},
Volume = {13},
Number = {2},
Pages = {457-474},
Month = {APR},
Abstract = {Building information modeling (BIM) can be considered a collaborative
   design process that allows all project stakeholders in different
   disciplines to contribute during the design phase of a construction
   project. However, interoperability issues between wind, structural
   engineering tools, and BIM design authoring software platforms have
   acted as a barrier to such collaborative design processes. This research
   pursued an evaluation-based approach to propose and develop a workflow
   for resolving the interoperability issues as well as automating
   significant parts of the collaborative design process. In this paper,
   the development of an automated modelling system to facilitate
   integrated structural design and wind engineering analysis using BIM is
   presented. The research was focused on pre-engineered building (PEB) as
   a case study. This research introduces some novel BIM concepts to
   facilitate the implementation of automation in the model development
   processes. These concepts facilitate engineering analysis integration
   and overcome challenges associated with creating and working with
   different level of development (LOD) models. The proposed system uses a
   central element level database and outputs a 3D model of the building
   and the computational domain for use by the computational fluid dynamics
   software. A BIM-based application program interface (API) and
   stand-alone software was developed to evaluate the proposed concepts and
   process and their feasibility. The results suggest a successful
   integration that could significantly improve the building design quality
   and further facilitate wind, or other, engineering design
   collaborations. It is also observed that the resulting process could be
   applied (extended) to the general architecture, engineering, and
   construction (AEC) industry.},
DOI = {10.1007/s12273-019-0589-2},
ISSN = {1996-3599},
EISSN = {1996-8744},
Unique-ID = {WOS:000520385500015},
}

@article{ WOS:000526392100005,
Author = {Philip, Milu Mary and Natarajan, Karthik and Ramanathan, Anithkumar and
   Balakrishnan, Vijayakumar},
Title = {Composite pattern to handle variation points in software architectural
   design of evolving application systems},
Journal = {IET SOFTWARE},
Year = {2020},
Volume = {14},
Number = {2},
Pages = {98-105},
Month = {APR},
Note = {7th International Conference on Software Process Improvement (CIMPS),
   CUCEI, Guadalajara, MEXICO, OCT 17-19, 2018},
Organization = {IEEE; Centro Investigacion Matematicas A C},
Abstract = {The variation points in software architecture arise as a result of the
   availability of large number of filters and component libraries. An
   integration of different architectural styles is crucial and necessary
   in the development of large-scale software application systems to handle
   the variation points. This article proposes a composite software
   architectural style for building application systems involving data
   streams, user interactivity, and dynamic mode. It uses a pattern within
   a pattern approach for combining the architectural styles. This approach
   provides flexibility to add or delete any filter or component at run
   time. In addition, the changes in the order of processing of the
   different filters or components can also be incorporated. The software
   architectural specification for any combination of input components and
   their order of processing is generated automatically. This specification
   acts as a baseline for the subsequent design and implementation phases
   of the application system. This model is generic and has been
   successfully validated for a prototype application system involving all
   the three modes of operation.},
DOI = {10.1049/iet-sen.2019.0006},
ISSN = {1751-8806},
EISSN = {1751-8814},
Unique-ID = {WOS:000526392100005},
}

@article{ WOS:000526923200002,
Author = {Phung, Toan K. and Zacchi, Lucia F. and Schulz, Benjamin L.},
Title = {DIALib: an automated ion library generator for data independent
   acquisition mass spectrometry analysis of peptides and glycopeptides},
Journal = {MOLECULAR OMICS},
Year = {2020},
Volume = {16},
Number = {2},
Pages = {100-112},
Month = {APR 1},
Abstract = {Data Independent Acquisition (DIA) Mass Spectrometry (MS) workflows
   allow unbiased measurement of all detectable peptides from complex
   proteomes, but require ion libraries for interrogation of peptides of
   interest. These DIA ion libraries can be theoretical or built from
   peptide identification data from Data Dependent Acquisition (DDA) MS
   workflows. However, DDA libraries derived from empirical data rely on
   confident peptide identification, which can be challenging for peptides
   carrying complex post-translational modifications. Here, we present
   DIALib, software to automate the construction of peptide and
   glycopeptide Data Independent Acquisition ion Libraries. We show that
   DIALib theoretical ion libraries can identify and measure diverse N- and
   O-glycopeptides from yeast and mammalian glycoproteins without prior
   knowledge of the glycan structures present. We present
   proof-of-principle data from a moderately complex yeast cell wall
   glycoproteome and a simple mixture of mammalian glycoproteins. We also
   show that DIALib libraries consisting only of glycan oxonium ions can
   quickly and easily provide a global compositional glycosylation profile
   of the detectable ``oxoniome{''} of glycoproteomes. DIALib will help
   enable DIA glycoproteomics as a complementary analytical approach to DDA
   glycoproteomics.},
DOI = {10.1039/c9mo00125e},
EISSN = {2515-4184},
ResearcherID-Numbers = {Zacchi, Lucia/F-7494-2016
   Schulz, Benjamin/B-3911-2009
   },
ORCID-Numbers = {Zacchi, Lucia/0000-0001-6217-1380
   Schulz, Benjamin/0000-0002-4823-7758
   Phung, Toan/0000-0002-2964-6070},
Unique-ID = {WOS:000526923200002},
}

@article{ WOS:000519324700001,
Author = {Souliman, Mena I. and Gc, Hemant and Isied, Mayzan and Walubita, Lubinda
   F. and Sousa, Jorge and Bastola, Nitish Raj},
Title = {Mechanistic analysis and cost-effectiveness evaluation of asphalt rubber
   mixtures},
Journal = {ROAD MATERIALS AND PAVEMENT DESIGN},
Year = {2020},
Volume = {21},
Number = {1, SI},
Pages = {S76-S90},
Month = {SEP 14},
Abstract = {Traffic loading has been associated with different main distresses
   occurring in the pavements. Fatigue cracking and rutting can be
   considered as the main distresses occurring in the pavement structure.
   Studies conducted in the past have shown improved mechanical performance
   of asphalt rubber mixtures, but only few of these studies are inclined
   in investigating the long-term mechanistic performance and the
   cost-effectiveness of the addition of rubber into asphalt mixtures. A
   mechanistic analysis utilising 3D Move software package was performed to
   assess the long-term fatigue performance and rutting performance of two
   different mixtures, namely: conventional hot mixed asphalt (HMA) and
   asphalt rubber (AR). Two different pavement construction scenarios were
   investigated as a newly constructed pavement structure and a
   rehabilitated overlay. Each mixture was analysed for two different
   vehicle speeds and two different pavement thicknesses that totalled 16
   different 3D Move mechanistic analysis scenarios (2 asphalt mixtures x 2
   construction scenarios x 2 vehicle speeds x 2 pavement thicknesses). The
   maximum tensile strain at the bottom of the asphalt layer for each
   scenario was utilised to establish a fatigue ratio that compared AR to
   the corresponding conventional HMA pavement sections. In addition, a
   cost-effectiveness analysis was carried out for the same 16 scenarios.
   Overall, the mechanistic analysis showed that the fatigue life of an AR
   was six times greater than that of conventional HMA layer for a new
   construction, and for the overlay, AR had 40 times more fatigue life
   than a conventional HMA. The rutting life was also increased with the
   use of asphalt rubber. In the case of cost-effectiveness analysis, AR
   exhibited four times lower cost per 1000 fatigue life cycles and three
   times lower cost per cycle of rutting when compared to a conventional
   HMA in a new construction. For an overlay scenario, AR exhibited 23
   times lower cost per 1000 fatigue life cycles and around four times
   lower cost per cycle of rutting compared to a conventional HMA overlay.},
DOI = {10.1080/14680629.2020.1735492},
EarlyAccessDate = {MAR 2020},
ISSN = {1468-0629},
EISSN = {2164-7402},
Unique-ID = {WOS:000519324700001},
}

@article{ WOS:000504781200011,
Author = {Balta, Musa and Ozcelik, Ibrahim},
Title = {A 3-stage fuzzy-decision tree model for traffic signal optimization in
   urban city via a SDN based VANET architecture},
Journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
Year = {2020},
Volume = {104},
Pages = {142-158},
Month = {MAR},
Abstract = {One of the main challenges for developed cities is vehicle traffic and
   its management. Because of physical structures and cross roads of urban
   intersections, traffic flows may cause time delay, congestion, traffic
   accidents and more fuel/time consumption. For these reasons, there are
   many studies on traffic management systems which are a part of smart
   city applications in both literature and practice. In recent years, it
   seems that VANET (Vehicular Ad Hoc Networks) architecture has been
   preferred in these studies, which enables vehicles to easily communicate
   with each other or devices on the edge of the road and also to transfer
   the related traffic data to the center. On the other hand, there are
   many studies on a new network paradigm called SDN (Software Defined
   Networks) that solves problems such as performance, programmability,
   scalability, security and management difficulties in today's traditional
   networks. Depending on all this, in this study, we will present an
   architectural proposal about how to use SDN and VANET network paradigms
   together in traffic management systems in order to use both existing
   intersection management system modules more adaptively and future
   traffic based services to be added in desired service quality without
   changing the communication infrastructure. Also in this study, a 3-stage
   fuzzy-decision tree model was proposed for traffic signaling which is
   the most important problems of urban traffic by focusing environmental
   factors (accident situation, priority vehicle transition etc.) and
   activity information (road construction, meeting, train and bus schedule
   etc.) not only vehicle density unlike other signaling studies in
   literature. The proposed model also generates/sends a dynamic flow input
   to SDN agent vehicles and SDN agent RSU (Road Side Unit) in intersection
   for making VANET routing protocols (AODV-ad hoc on-demand distance
   vector, DSDV-destination-sequenced distance vector) selection
   automatically according to traffic incident and density. In order to
   measure the functionality of the SDN based VANET architecture and 3
   stage fuzzy-decision tree model on it for the transmission
   infrastructure of the traffic management, performance tests were
   performed on different traffic and network scenarios on Adapazari City
   Centre Model. SDN based VANET architecture was compared with traditional
   systems according to transmission infrastructure and also proposed
   3-stage fuzzy-decision model was compared with signaling techiques in
   literature such as fixed-time signaling, Webster equation, ant colony
   algorithm and particle swarm optimization under same traffic and network
   scenarios. The results obtained from performance tests shows that the
   proposed 3-stage fuzzy-decision model has \%15-17 better performance
   than fixed-time signaling and webster equation and also \%9-11 better
   performance that particle swarm and ant colony optimizations in low
   density traffic scenarios. In high density and dynamic traffic scenarios
   these rates reach \%20-22 compared to fixed-time signaling and webster
   equation and \%14-15 compared to particle swarm and ant colony
   optimizations. In addition in this study, proposed SDN based VANET
   architecture was compared to traditional VANET ITS architectures within
   the same traffic and network scenarios. According to obtained results,
   it is shown that the proposed SDN based VANET has \%30-40 better
   performance than traditional systems in end-end delay, throughput and
   packet loss rate criteria. (C) 2019 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.future.2019.10.020},
ISSN = {0167-739X},
EISSN = {1872-7115},
ResearcherID-Numbers = {, Ibrahim OZCELIK/AAA-3304-2020},
ORCID-Numbers = {, Ibrahim OZCELIK/0000-0001-9985-5268},
Unique-ID = {WOS:000504781200011},
}

@article{ WOS:000542478400008,
Author = {Bastola, Nitish Raj and Souliman, I, Mena and Tripathi, Ashish and
   Pearson, Alexander},
Title = {Mechanistic Analysis and Economic Benefits of Fiber-Reinforced Asphalt
   Overlay Mixtures},
Journal = {JOURNAL OF MATERIALS AND ENGINEERING STRUCTURES},
Year = {2020},
Volume = {7},
Number = {1},
Pages = {83-96},
Month = {MAR},
Abstract = {Among the various distresses in flexible pavement structures, rutting
   and fatigue cracking can be accounted as two of the major distresses
   that need to be addressed by pavement engineers. Laboratory tests, such
   as four-point bending beam and flow number are utilized to characterize
   the rutting and cracking resistance of flexible pavements. Various
   construction practices are introduced to reduce the effect of fatigue
   and rutting in pavement structures. One of such methods is applying
   fibers to the asphalt mixture to prolong the serviceability and the
   performance of the pavement structures. The use of fibers is applicable
   to freshly constructed pavements as well as in the pavement
   rehabilitation and maintenance work, such as overlay. This paper
   primarily analyses the application of fibers in the overlay of
   pavements. The two major cases of the pavement with original asphalt
   overlay and the one with fibers mixed asphalt overlay is considered
   utilizing a developed testing program where the mechanistic analysis as
   well as the economic effectiveness is evaluated. 3D move analysis
   software package is utilized extensively as a means of mechanistic
   analysis tool. It is found that the fiber mixture pavement overlay had a
   higher pavement life than the ordinary asphalt overlay. In addition, the
   cost effectiveness in terms of fatigue and rutting of fiber-reinforced
   overlay structures were 4.4 and 4.1 times the unmodified mixtures,
   respectively. The use of fibers in the overlay of pavement resulted in
   higher pavement life with a high cost effectiveness.},
ISSN = {2170-127X},
Unique-ID = {WOS:000542478400008},
}

@article{ WOS:000530736800012,
Author = {Szemat, W.},
Title = {Innovative digital well construction planning solution enhances
   coherency and efficiency of the drilling design process},
Journal = {OIL GAS-EUROPEAN MAGAZINE},
Year = {2020},
Volume = {46},
Number = {1},
Pages = {40-42},
Month = {MAR},
Abstract = {The oil and gas industry recognizes that there are significant gains to
   be had by the implementation of new digital technologies. For well
   construction planning, the goal is to bring together all domains, all
   data, and all engineering requirements in a seamlessly interconnected
   solution. The use of traditional workflows and solutions often requires
   a lengthy, disconnected, and iterative processes, which can introduce a
   lack of coherency between the different teams and engineering processes
   involved, increasing the planning risks. Given the complexity and the
   number of disciplines involved in the well engineering process, a
   high-level collaboration is needed in new planning systems. Considering
   this as the basis for a change, a novel approach to connect all the well
   planning workflows demonstrates how the process can be optimized by
   combining automatic engineering and validation, big data analytics,
   concurrent engineering, and project orchestration.
   The main benefit of a native cloud designed and deployed solution is the
   simplicity for the user because the architecture handles the
   interconnection of different workflows, data, and people, resulting in a
   more effective process. In addition, the solution always accounts for
   the latest data versions and automatically adjusts when any design
   change is made to the plan. Moreover, the cloud deployment enables
   complex big data analytics for offset well data analyses, thus
   increasing the accuracy of the results and democratizing the company
   knowledge. Automatic engineering validations are immediately confirmed
   against changes to the wellbore geometries, trajectory and drilling
   parameters among others, to deliver a more accurate and coherent result
   when compared to a disconnected model application workflow. As a result
   of this innovative solution, a significant reduction of individual
   software packages required to plan a well can be achieved. The system
   delivers a step change in in well engineering performance, with case
   study examples delivering both high quality results and efficiency gains
   of 50\% and more.},
DOI = {10.19225/200306},
ISSN = {0342-5622},
Unique-ID = {WOS:000530736800012},
}

@article{ WOS:000519820500006,
Author = {Kallay, Mihaly and Nagy, Peter R. and Mester, David and Rolik, Zoltan
   and Samu, Gyula and Csontos, Jozsef and Csoka, Jozsef and Szabo, P.
   Bernat and Gyevi-Nagy, Laszlo and Hegely, Bence and Ladjanszki, Istvan
   and Szegedy, Lorant and Ladoczki, Bence and Petrov, Klara and Farkas,
   Mate and Mezei, Pal D. and Ganyecz, Adam},
Title = {The MRCC program system: Accurate quantum chemistry from water to
   proteins},
Journal = {JOURNAL OF CHEMICAL PHYSICS},
Year = {2020},
Volume = {152},
Number = {7},
Month = {FEB 21},
Abstract = {MRCC is a package of ab initio and density functional quantum chemistry
   programs for accurate electronic structure calculations. The suite has
   efficient implementations of both low- and high-level correlation
   methods, such as second-order MOller-Plesset (MP2), random-phase
   approximation (RPA), second-order algebraic-diagrammatic construction
   {[}ADC(2)], coupled-cluster (CC), configuration interaction (CI), and
   related techniques. It has a state-of-the-art CC singles and doubles
   with perturbative triples {[}CCSD(T)] code, and its specialties, the
   arbitrary-order iterative and perturbative CC methods developed by
   automated programming tools, enable achieving convergence with regard to
   the level of correlation. The package also offers a collection of
   multi-reference CC and CI approaches. Efficient implementations of
   density functional theory (DFT) and more advanced combined DFT-wave
   function approaches are also available. Its other special features, the
   highly competitive linear-scaling local correlation schemes, allow for
   MP2, RPA, ADC(2), CCSD(T), and higher-order CC calculations for extended
   systems. Local correlation calculations can be considerably accelerated
   by multi-level approximations and DFT-embedding techniques, and an
   interface to molecular dynamics software is provided for quantum
   mechanics/molecular mechanics calculations. All components of MRCC
   support shared-memory parallelism, and multi-node parallelization is
   also available for various methods. For academic purposes, the package
   is available free of charge. Published under license by AIP Publishing.},
DOI = {10.1063/1.5142048},
Article-Number = {074107},
ISSN = {0021-9606},
EISSN = {1089-7690},
ResearcherID-Numbers = {Nagy, Péter R/ABA-3331-2021
   Kallay, Mihaly/H-2813-2012
   Farkas, Máté/HHN-0241-2022
   },
ORCID-Numbers = {Nagy, Péter R/0000-0001-6692-0879
   Kallay, Mihaly/0000-0003-1080-6625
   Mester, David/0000-0001-6570-2917
   Szabo, Peter Bernat/0000-0003-1824-8322
   Hegely, Bence/0000-0002-8672-2201
   Gyevi-Nagy, Laszlo/0000-0002-4757-3098},
Unique-ID = {WOS:000519820500006},
}

@article{ WOS:000506172700017,
Author = {Ladines, A. N. and Hammerschmidt, T. and Drautz, R.},
Title = {BOPcat software package for the construction and testing of
   tight-binding models and bond-order potentials},
Journal = {COMPUTATIONAL MATERIALS SCIENCE},
Year = {2020},
Volume = {173},
Month = {FEB 15},
Abstract = {Atomistic models like tight-binding (TB), bond-order potentials (BOP)
   and classical potentials describe the interatomic interaction in terms
   of mathematical functions with parameters that need to be adjusted for a
   particular material. The procedures for constructing TB/BOP models
   differ from the ones for classical potentials. We developed the BOPcat
   software package as a modular python code for the construction and
   testing of TB/BOP parameterizations. It makes use of atomic energies,
   forces and stresses obtained by TB/BOP calculations with the BOPfox
   software package. It provides a graphical user interface and flexible
   control of raw reference data, of derived reference data like defect
   energies, of automated construction and testing protocols, and of
   parallel execution in queuing systems. We demonstrate the concepts and
   usage of the BOPcat software and illustrate its key capabilities by
   exemplary constructing and testing a parameterization of a magnetic BOP
   for Fe. We provide a parameterization protocol with a successively
   increasing set of reference data that leads to good transferability to a
   variety of properties of the ferromagnetic bcc groundstate and to
   crystal structures which were not part of the training set.},
DOI = {10.1016/j.commatsci.2019.109455},
Article-Number = {109455},
ISSN = {0927-0256},
EISSN = {1879-0801},
ResearcherID-Numbers = {Hammerschmidt, Thomas/A-3343-2009
   Hammerschmidt, Thomas/M-1488-2019},
ORCID-Numbers = {Hammerschmidt, Thomas/0000-0002-2270-4469
   Hammerschmidt, Thomas/0000-0002-2270-4469},
Unique-ID = {WOS:000506172700017},
}

@article{ WOS:000513309000001,
Author = {Ponisio, Lauren C. and de Valpine, Perry and Michaud, Nicholas and
   Turek, Daniel},
Title = {One size does not fit all: Customizing MCMC methods for hierarchical
   models using NIMBLE},
Journal = {ECOLOGY AND EVOLUTION},
Year = {2020},
Volume = {10},
Number = {5},
Pages = {2385-2416},
Month = {MAR},
Abstract = {Improved efficiency of Markov chain Monte Carlo facilitates all aspects
   of statistical analysis with Bayesian hierarchical models. Identifying
   strategies to improve MCMC performance is becoming increasingly crucial
   as the complexity of models, and the run times to fit them, increases.
   We evaluate different strategies for improving MCMC efficiency using the
   open-source software NIMBLE (R package nimble) using common ecological
   models of species occurrence and abundance as examples. We ask how MCMC
   efficiency depends on model formulation, model size, data, and sampling
   strategy. For multiseason and/or multispecies occupancy models and for
   N-mixture models, we compare the efficiency of sampling discrete latent
   states vs. integrating over them, including more vs. fewer hierarchical
   model components, and univariate vs. block-sampling methods. We include
   the common MCMC tool JAGS in comparisons. For simple models, there is
   little practical difference between computational approaches. As model
   complexity increases, there are strong interactions between model
   formulation and sampling strategy on MCMC efficiency. There is no
   one-size-fits-all best strategy, but rather problem-specific best
   strategies related to model structure and type. In all but the simplest
   cases, NIMBLE's default or customized performance achieves much higher
   efficiency than JAGS. In the two most complex examples, NIMBLE was 10-12
   times more efficient than JAGS. We find NIMBLE is a valuable tool for
   many ecologists utilizing Bayesian inference, particularly for complex
   models where JAGS is prohibitively slow. Our results highlight the need
   for more guidelines and customizable approaches to fit hierarchical
   models to ensure practitioners can make the most of occupancy and other
   hierarchical models. By implementing model-generic MCMC procedures in
   open-source software, including the NIMBLE extensions for integrating
   over latent states (implemented in the R package nimbleEcology), we have
   made progress toward this aim.},
DOI = {10.1002/ece3.6053},
EarlyAccessDate = {FEB 2020},
ISSN = {2045-7758},
ResearcherID-Numbers = {Ponisio, Lauren/C-3785-2019},
ORCID-Numbers = {Turek, Daniel/0000-0002-1453-1908
   Ponisio, Lauren/0000-0002-3838-7357},
Unique-ID = {WOS:000513309000001},
}

@article{ WOS:000642283800001,
Author = {Igwe, Ogbonnaya and Umbugadu, Allu Augustine},
Title = {Characterization of structural failures founded on soils in Panyam and
   some parts of Mangu, Central Nigeria},
Journal = {GEOENVIRONMENTAL DISASTERS},
Year = {2020},
Volume = {7},
Number = {1},
Month = {FEB 4},
Abstract = {Structural failure of buildings, roads, and other infrastructures has
   led to the loss of lives and monumental damage to the economy. In
   developing countries such as Nigeria, the failures are always attributed
   to the nature/type of soils in the area without considering other
   factors. With the increase in civil engineering constructions and the
   scramble for a limited portion of competent soils for such
   constructions, land, therefore, is becoming man's most priced commodity
   for agriculture, residential and industrial purposes. No works of such
   have been done in the area, it is, therefore, imperative to study the
   cause(s) to prevent further loss of lives and the negative effects of
   such on the economy. The study was carried out at Panyam and some
   surrounding localities on latitudes N9(0)21 ` and N9(0)26 ` and
   longitudes 9(0)11 ` E and 9(0)16 ` E an area of 72km(2). A total of
   forty samples were collected out of which twenty were selected for
   swelling potentials determination due to their cohesive nature within
   the 1.5m depth. To characterize the swelling of the soils, inferential
   testing methods; direct method comprising Liquid Limit and Plasticity
   Index with values ranging from 30\% - 70\% and 10\% - 27\% respectively,
   and the indirect method (Free Swell Index, FSI) whose values range from
   9.1\% - 90\% were used. Most of the soils analyzed based on the
   geotechnical standards were categorized relatively safe for civil
   infrastructure. Structural studies employed the direct measurements of
   surface structures to get the orientations of structures measured.
   Landsat image and Total Magnetic Intensity (TMI) map of the area was
   acquired from the Nigerian Geological Survey Agency (NGSA) for
   subsurface structural analysis. The RTP-TMI (reduce to pole) grid data
   of the area were processed to improve magnetic anomalies associated with
   edges of surface/near-surface geological structures using the MAGMAP
   two-dimensional fast fourier transform (2D-FFT) filters package in Oasis
   Montaj v7.2 software. Geophysical studies using aeromagnetic data at a
   near-surface depth of 40m and fractures measured on the surface both
   revealed a major trend in the NNE - SSW. It was observed that structures
   constructed across the major trend suffered little to moderate damage
   while those constructed along the major fractures trend suffered severe
   damage. It was concluded that both soils and fractures played individual
   roles in the collapse of infrastructure in the area.},
DOI = {10.1186/s40677-020-0141-9},
Article-Number = {7},
EISSN = {2197-8670},
Unique-ID = {WOS:000642283800001},
}

@article{ WOS:000515503100115,
Author = {Kik, Tomasz},
Title = {Computational Techniques in Numerical Simulations of Arc and Laser
   Welding Processes},
Journal = {MATERIALS},
Year = {2020},
Volume = {13},
Number = {3},
Month = {FEB 1},
Abstract = {The article presents a comparison of modern computational techniques
   used in numerical analyses of welding processes. The principles of the
   ``transient{''} technique calculations with a moving heat source, the
   ``macro-bead{''} (MBD) technique, with an imposed thermal cycle on a
   selected weld bead section and the ``local-global{''} approach with
   shrinkage calculation technique were described. They can be used,
   depending on the variant chosen, both for individual, simple weld joints
   and those made of many beads or constructions containing dozens of welds
   and welded elements. Differences in the obtained results and time needed
   to perform calculations with four different calculation examples of
   single and multipass arc and laser beam welding processes were
   presented. The results of calculations of displacements and stresses
   distributions in the welded joints using various computational
   techniques were compared, as well as the calculation times with the
   described techniques. The numerical analyses in the SYSWELD software
   package have shown the differences between the described computational
   techniques, as well as an understanding of the benefits and
   disadvantages of using each of them. This knowledge allows preparing an
   efficient and fast optimization of the welding processes, often aimed at
   minimizing deformations in the first place, as well as detection of
   potential defects of both simple and complex welded structures. In
   general, the possibilities and flexibility of modern numerical
   calculation software have been presented.},
DOI = {10.3390/ma13030608},
Article-Number = {608},
EISSN = {1996-1944},
ResearcherID-Numbers = {Kik, Tomasz/A-4443-2017},
ORCID-Numbers = {Kik, Tomasz/0000-0001-8153-4853},
Unique-ID = {WOS:000515503100115},
}

@article{ WOS:000525305900312,
Author = {Lai, Hsin-hung},
Title = {Applicability of a Design Assessment and Management for the Current
   Ammunition Depots in Taiwan},
Journal = {APPLIED SCIENCES-BASEL},
Year = {2020},
Volume = {10},
Number = {3},
Month = {FEB},
Note = {4th IEEE International Conference on Applied System Invention (IEEE
   ICASI), Tokyo, JAPAN, APR 13-17, 2018},
Organization = {IEEE; IEEE Tainan Sec Sensors Council; Taiwanese Inst Knowledge Innovat},
Abstract = {In Taiwan, many ammunition depots have become outdated after having been
   in service for a long period of time, and if they are not properly
   managed, then accidental explosions might erupt inside. Leakage pressure
   after an explosion is closely related to the opening of the structure
   and the thickness of the wall. In order to reduce the risk of implosion,
   it is necessary to design a new structure or strengthen the existing
   ammunition libraries for the storage of ammunition required for combat.
   In order to evaluate the applicability of an existing ammunition depot
   design, making management simpler and safer, this study integrates the
   scale model experiment of an ammunition depot with computer simulation,
   the arbitrary Lagrangian-Eulerian (ALE) algorithm in ANSYS/LS-DYNA
   software, and it compares the results with the UFC3-340-02 specification
   in order to verify its applicability. The results show that computer
   simulation can verify that the data related to an implosion of an
   ammunition depot is similar to the specification. Therefore, the design
   results of the ammunition depot optimized by computer simulation can be
   used as a reference for the construction or strengthening of ammunition
   depots.},
DOI = {10.3390/app10031041},
Article-Number = {1041},
EISSN = {2076-3417},
ORCID-Numbers = {lai, xin hong/0000-0002-5300-0165},
Unique-ID = {WOS:000525305900312},
}

@article{ WOS:000517664900002,
Author = {Wu, Jie and Lepech, Michael D.},
Title = {Incorporating multi-physics deterioration analysis in building
   information modeling for life-cycle management of durability performance},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2020},
Volume = {110},
Month = {FEB},
Abstract = {Extending the application of Building Information Modeling (BIM) tools
   into the operation and management phases of a building's life cycle is a
   significant advance in recent years. However, few studies have
   investigated or developed tools for the life-cycle management of
   durability performance of built structures leveraging BIM. This gap is
   partly due to (i) the lack of an accessible interface that connects BIM
   software with durability models of built structures, and (ii) no
   available durability models that are fundamental in nature and thus can
   integrate with other tools. To begin addressing this gap, this paper
   presents a framework that uses the Application Programming Interface
   (API) within BIM to integrate physics-based durability performance
   models, which extends the application of BIM into the post-construction
   phases by enabling more accurate predictions of life cycle performance.
   An application of the framework is presented for analysis and management
   of a reinforced concrete structure.},
DOI = {10.1016/j.autcon.2019.103004},
Article-Number = {103004},
ISSN = {0926-5805},
EISSN = {1872-7891},
Unique-ID = {WOS:000517664900002},
}

@article{ WOS:000522751600005,
Author = {Wang, Zhitao and Li, Hui and Yan, Shichen and Fang, Qianrong},
Title = {Synthesis of a Two-dimensional Covalent Organic Framework with the
   Ability of Conducting Proton along Skeleton},
Journal = {ACTA CHIMICA SINICA},
Year = {2020},
Volume = {78},
Number = {1},
Pages = {63-68},
Month = {JAN 15},
Abstract = {Nitrogen heterocyclic compound like imidazole and triazole are often
   loaded in porous material for proton conduction. Inspired by this, we
   employ 5,5'-diamino-3,3'-bis(1H-1,2,4-triazole) (BTDA) containing
   triazole fragments in the structure as the construction unit to react
   with 2,4,6-triformylphloroglucinol (THP) through Schiff-base
   condensation reaction to synthesize a novel two-dimensional covalent
   organic framework named TFP-BTDA-COF. The theoretical results were
   simulated using the Accelrys Material Studios 7.0 software package and
   compared with the powder X-ray diffraction (PXRD) test data to confirm
   the crystal structure of TFP-BTDA-COF. The porosity and pore structure
   of TFP-BTDA-COF were characterized by N2 adsorption-desorption at 77 K.
   The condensation reaction was confirmed by Fourier transform infrared
   spectroscopy (FTIR). Due to the pi-pi accumulation of the 2D-COF, the
   N-H bond of the triazole in BTDA connecting unit is periodically and
   regularly arranged on each layer of the COF to form an ordered array.
   Under certain humidity conditions, the protons can be transmitted along
   the array in the one-dimensional pore channel by the intermediary of
   water molecules. Therefore, the TFP-BTDA-COF has the ability to conduct
   proton through the skeleton. The proton conductivity of TFP-BTDA-COF is
   tested by the AC impedance method. The results show that the proton
   conductivity of the material is gradually enhanced with the increase of
   the ambient humidity, and the maximum value is 1.4 X 10(-3) S.cm(-1) at
   98\% relative humidity. The PXRD of TFP-BTDA-COF in boiling water for 2
   h and after 12 h AC impedance test were compared with the original
   experimental value to evaluate its tolerance under the working
   conditions of the proton membrane fuel cell. The PXRD diffraction peak
   intensity did not change obviously compared with that of the original
   experimental value. The thermogravimetric analysis results show that the
   thermal stability of TFP-BTDA-COF can reach high to 400 C. The above
   evidence proves that it has the potential to be used in proton membrane
   fuel cells.},
DOI = {10.6023/A19110397},
ISSN = {0567-7351},
ResearcherID-Numbers = {Fang, Qianrong/D-6756-2019},
Unique-ID = {WOS:000522751600005},
}

@article{ WOS:000506205800001,
Author = {Herbst, Michael F. and Scheurer, Maximilian and Fransson, Thomas and
   Rehn, Dirk R. and Dreuw, Andreas},
Title = {adcc: A versatile toolkit for rapid development of
   algebraic-diagrammatic construction methods},
Journal = {WILEY INTERDISCIPLINARY REVIEWS-COMPUTATIONAL MOLECULAR SCIENCE},
Year = {2020},
Volume = {10},
Number = {6},
Month = {NOV},
Abstract = {ADC-connect (adcc) is a hybrid python/C++ module for performing excited
   state calculations based on the algebraic-diagrammatic construction
   scheme for the polarization propagator (ADC). Key design goal is to
   restrict adcc to this single purpose and facilitate connection to
   external packages, for example, for obtaining the Hartree-Fock
   references, plotting spectra, or modeling solvents. Interfaces to four
   self-consistent field codes have already been implemented, namely pyscf,
   psi4, molsturm, and veloxchem. The computational workflow, including the
   numerical solvers, is implemented in python, whereas the working
   equations and other expensive expressions are done in C++. This equips
   adcc with adequate speed, making it a flexible toolkit for both rapid
   development of ADC-based computational spectroscopy methods as well as
   unusual computational workflows. This is demonstrated by three examples.
   Presently, ADC methods up to third order in perturbation theory are
   available in adcc, including the respective core-valence separation and
   spin-flip variants. Both restricted or unrestricted Hartree-Fock
   references can be employed. This article is categorized under: Software
   > Simulation Methods Electronic Structure Theory > Ab Initio Electronic
   Structure Methods Theoretical and Physical Chemistry > Spectroscopy
   Software > Quantum Chemistry},
DOI = {10.1002/wcms.1462},
EarlyAccessDate = {JAN 2020},
Article-Number = {e1462},
ISSN = {1759-0876},
EISSN = {1759-0884},
ORCID-Numbers = {Scheurer, Maximilian/0000-0003-0592-3464
   Herbst, Michael/0000-0003-0378-7921},
Unique-ID = {WOS:000506205800001},
}

@article{ WOS:000579085000001,
Author = {Ainechi, Shahrzad and Valibeig, Nima},
Title = {Comparative and contrast study of Karbandie's Masonry ribs brick
   arrangements},
Journal = {CURVED AND LAYERED STRUCTURES},
Year = {2020},
Volume = {7},
Number = {1},
Pages = {166-177},
Month = {JAN},
Abstract = {Karbandies' masonry ribs construction technology is influenced by form
   and size, on one hand, and traditional architects' creativity, on the
   other hand. Some of the exclusive characteristics of construction
   techniques of Karbandies' ribs are revealed, after investigating and
   identifying of some of traditional construction ways of them,
   investigating contemporary studies conducted in the field and comparing
   the methods. The masonry ribs have been built in different forms over
   time and such different forms result in formation of different
   construction technologies. The masonry ribs' form and size and the
   bricks' arrangement way have direct effect on their construction
   process. The aim of the present paper is to determine construction
   technology of the masonry ribs used in different Karbandies. Hence, in
   line with achieving this goal, it also tries to investigate geometric
   arrangement of bricks in masonry ribs. The present study, for the first
   time, presents a comparative structure to study different implementation
   ways of Karbandies' masonry ribs and reasons for similarities and
   differences among different samples. Moreover, the present study tries
   to draw implementation way of different types of ribs with the aid of 2D
   and 3D software and by doing library studies, field understandings and
   interviewing the related masters and then the collected data were
   studied, comparatively. Data-gathering of this research included library
   studies which included written, listening and visual methods of books,
   articles and dissertations. Interviewing traditional masters, who are
   expertise in the field, were used in the samples analysis. Then
   according to the master builders' advice, structural form of karbandi
   ribs was modelled using three-dimensional software type. In this study
   karbandie's masonry ribs were divided into three categories of radial
   brick vault, pitched brick vault and a combination of them based on
   their brick arrangement way. Investigation of the results indicated that
   the bricks due to their geometrical form and variety in shape can result
   in various combinations. Different factors including materials,
   centering, dimensions, bay size and other factors are influential in
   bricks arrangement of the ribs.},
DOI = {10.1515/cls-2020-0013},
ISSN = {2353-7396},
ResearcherID-Numbers = {Valibeig, Nima/AEX-9206-2022},
ORCID-Numbers = {Valibeig, Nima/0000-0002-7562-0701},
Unique-ID = {WOS:000579085000001},
}

@inproceedings{ WOS:000786468800069,
Author = {Barbosa, Paulo and Figueiredo, Alex and Souto, Sabrina and Gaeta,
   Eugenio and Araujo, Eriko and Teixeira, Tiago},
Editor = {DeHerrera, AGS and Gonzalez, AR and Santosh, KC and Temesgen, Z and Kane, B and Soda, P},
Title = {An Open Source Software Architecture and Ready-To-Use Components for
   Health IoT},
Booktitle = {2020 IEEE 33RD INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL
   SYSTEMS(CBMS 2020)},
Series = {IEEE International Symposium on Computer-Based Medical Systems},
Year = {2020},
Pages = {374-379},
Note = {33rd IEEE International Symposium on Computer-Based Medical Systems
   (CBMS), ELECTR NETWORK, JUL 28-30, 2020},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {This paper presents the definition of a software architecture and
   implementation as open-source components specially designed for the
   state-of-art of Health IoT technologies. These activities were conducted
   through a H2020 project that developed a smart IoT solution for
   childhood obesity and two contracted Health IoT projects for the leading
   brazilian clinics in wound treatment. The software components and the
   guidelines presented in this paper are fully available as github
   repositories and contributors can find immediate application in Health
   IoT projects. A survey with contributors in four different countries was
   conducted about the way the realized software architecture addresses
   quality requirements for Health IoT. The results shows that the
   integration of components was the main concern of this team due to the
   distributed nature of the projects and the implementation of patterns
   such as API Gateway using open source technologies are very likely to be
   reused in the future. The initiative continues collecting user's
   experiences for continuous improvements and research goals.},
DOI = {10.1109/CBMS49503.2020.00077},
ISSN = {2372-9198},
ISBN = {978-1-7281-9429-5},
Unique-ID = {WOS:000786468800069},
}

@article{ WOS:000605260300009,
Author = {Beskrovny, A. S. and Bessonov, V, L. and Lvanov, V, D. and Kirillova, V,
   I. and Kossovich, L. Yu},
Title = {Using the Mask-RCNN Convolutional Neural Network to Automate the
   Construction of Two-Dimensional Solid Vertebral Models},
Journal = {IZVESTIYA SARATOVSKOGO UNIVERSITETA NOVAYA SERIYA-MATEMATIKA MEKHANIKA
   INFORMATIKA},
Year = {2020},
Volume = {20},
Number = {4},
Pages = {502-516},
Abstract = {Biomechanical modeling requires the construction of an accurate solid
   model of the object under study based on the data of a particular
   patient. This problem can be solved manually using modern software
   packages for medical data processing or using computer-aided design
   systems. This approach is used by many researchers and allows you to
   create accurate solid models, but is time consuming. In this regard, the
   automation of the construction of solid models suitable for performing
   biomechanical calculations is an urgent task and can be carried out
   using neural network technologies. This study presents the
   implementation of one of the methods for processing computed tomography
   data in order to create two-dimensional accurate solid models of
   vertebral bodies in a sagittal projection. An artificial neural network
   Mask-RCNN was used for automatic recognition of vertebrae. The
   assessment of the quality of the automatic recognition performed by the
   neural network was carried out on the basis of comparison with the
   Sorensen measure with manual segmentation performed by practitioners.
   Application of the method makes it possible to significantly speed up
   the process of modeling bone structures of the spine in 2D mode. The
   implemented technique was used in the development of a solid-state model
   module, which is included in the SmartPlan Ortho 2D medical decision
   support system developed at Saratov State University within the
   framework of the Advanced Research Foundation project.},
DOI = {10.18500/1816-9791-2020-20-4-502-516},
EISSN = {2541-9005},
ResearcherID-Numbers = {Kirillova, Irina Vasilevna/ABC-7392-2021
   Bessonov, Leonid/G-4699-2015
   Ivanov, Dmitriy/D-4750-2013},
ORCID-Numbers = {Kirillova, Irina Vasilevna/0000-0001-6745-4144
   Beskrovnyi, Aleksandr/0000-0002-1724-4058
   Bessonov, Leonid/0000-0002-5636-1644
   Kirillova, Irina/0000-0001-8053-3680
   Ivanov, Dmitriy/0000-0003-1640-6091},
Unique-ID = {WOS:000605260300009},
}

@inproceedings{ WOS:000723870900077,
Author = {Bolme, David S. and Srinivas, Nisha and Brogan, Joel and Cornett, David},
Book-Group-Author = {IEEE},
Title = {Face Recognition Oak Ridge (FaRO): A Framework for Distributed and
   Scalable Biometrics Applications},
Booktitle = {IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020)},
Year = {2020},
Note = {IEEE/IAPR International Joint Conference on Biometrics (IJCB), ELECTR
   NETWORK, SEP 28-OCT 01, 2020},
Organization = {IEEE; IAPR; IEEE Biometr Council; Google; Qualcomm; NSF},
Abstract = {The facial biometrics community has seen a recent abundance of
   high-accuracy facial analytic models become freely available. Although
   these models' capabilities in facial detection, landmark detection,
   attribute analysis, and recognition are ever-increasing, they aren't
   always straightforward to deploy in a real-world environment. In
   reality, the use of the field's ever growing collection of models is
   becoming exceedingly difficult as library dependencies update and
   deprecate. Researchers often encounter headaches when attempting to
   utilize multiple models requiring different or conflicting software
   packages. Face Recognition Oak Ridge (FaRO) is an open-source project
   designed to provide a highly modular, flexible framework for unifying
   facial analytic models through a compartmentalized plug-and-play
   paradigm built on top of the gRPC (Google Remote Procedure Call)
   protocol. FaRO's server-client architecture and flexible portability
   allows easy construction of modularized and heterogeneous face analysis
   pipelines, distributed over many machines with differing hardware and
   software resources. This paper outlines FaRO's architecture and current
   capabilities, along with some experiments in model testing and
   distributed scaling through FaRO.},
ISBN = {978-1-7281-9186-7},
ResearcherID-Numbers = {Brogan, Joel/GWU-7739-2022
   },
ORCID-Numbers = {Cornett, David/0000-0002-2291-0860},
Unique-ID = {WOS:000723870900077},
}

@inproceedings{ WOS:000626725600001,
Author = {Correia, Andreia and Felber, Pascal and Ramalhete, Pedro},
Book-Group-Author = {ACM},
Title = {Persistent Memory and the Rise of Universal Constructions},
Booktitle = {PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS
   (EUROSYS'20)},
Year = {2020},
Note = {15th European Conference on Computer Systems (EuroSys), ELECTR NETWORK,
   APR 27-30, 2020},
Organization = {VMWare; RedHat; Microsoft; Facebook; Google; Oracle},
Abstract = {Non-Volatile Main Memory (NVMM) has brought forth the need for data
   structures that are not only concurrent but also resilient to
   non-corrupting failures. Until now, persistent transactional memory
   libraries (PTMs) have focused on providing correct recovery from
   non-corrupting failures without memory leaks. Most PTMs that provide
   concurrent access do so with blocking progress.
   The main focus of this paper is to design practical PTMs with wait-free
   progress based on universal constructions. We first present CX-PUC, the
   first bounded wait-free persistent universal construction requiring no
   annotation of the underlying sequential data structure. CX-PUC is an
   adaptation to persistence of CX, a recently proposed universal
   construction. We next introduce CX-PTM, a PTM that achieves better
   throughput and supports transactions over multiple data structure
   instances, at the price of requiring annotation of the loads and stores
   in the data structure-as is commonplace in software transactional
   memory. Finally, we propose a new generic construction, Redo-PTM, based
   on a finite number of replicas and Herlihy's wait-free consensus, which
   uses physical instead of logical logging. By exploiting its capability
   of providing wait-free ACID transactions, we have used Redo-PTM to
   implement the world's first persistent key-value store with bounded
   wait-free progress.},
DOI = {10.1145/3342195.3387515},
ISBN = {978-1-4503-6882-7},
ORCID-Numbers = {Felber, Pascal/0000-0003-1574-6721},
Unique-ID = {WOS:000626725600001},
}

@inproceedings{ WOS:000643732600032,
Author = {Covaciu, Florin and Pisla, Adrian and Vaida, Calin and Gherman, Bogdan
   and Pisla, Doina},
Editor = {Miclea, L and Stoian, I and Enyedi, S and Stan, O and Stefan, I and Raica, P and Abrudean, M and Valean, H and Sanislav, T and Gota, D},
Title = {Development of a Virtual Reality Simulator for a Lower Limb
   Rehabilitation Robot},
Booktitle = {PROCEEDINGS OF 2020 IEEE INTERNATIONAL CONFERENCE ON AUTOMATION, QUALITY
   AND TESTING, ROBOTICS (AQTR)},
Series = {IEEE International Conference on Automation Quality and Testing Robotics},
Year = {2020},
Pages = {178-183},
Note = {22nd IEEE International Conference on Automation, Quality and Testing,
   Robotics (AQTR), Cluj-Napoca, ROMANIA, MAY 21-23, 2020},
Organization = {Inst Elect \& Elect Engineers; IEEE Comp Soc Test Technol Tech Council;
   Tech Univ Cluj Napoca; IPA R \& D Insti Automat, Ctr Technol Transfer;
   ARQES; Bosch; Tech Univ Cluj Napoca, Dept Automat; IPA R \& D Insti
   Automat, Cluj Napoca Subsidiary; Romanian Soc Automat Control \& Tech
   Informat},
Abstract = {The paper describes the design of a virtual reality simulator destined
   to a robotic system designed for patient's lower limb medical recovery.
   The VR simulator is created in Unity 5 environment. In this environment
   was imported the mechanical design of the robotic structure, together
   with a virtual human model (an avatar) that latter was coupled with the
   robotic structure. The robotic mechanical structure is designed in the
   Siemens NX environment, a professional mechatronics design software and
   the human virtual model was created using the MakeHuman program, an open
   source 3D computer graphics middleware. In the first step for creating
   the virtual reality simulator, an enclosure was defined where the
   robotic structure used to recover the lower limb has been introduced. In
   the second stage, the human virtual model was loaded into Unity 5
   environment and ``attached{''} to the robotic mechanical system. In the
   last stage was defined a Robotic system User Interface (RUI) for the
   robotic system control. The RUI was created using C Sharp (C\#)
   programming language that exists within the Visual Studio software
   package.},
ISSN = {1844-7872},
ISBN = {978-1-7281-7166-1},
ResearcherID-Numbers = {Gherman, Bogdan/ADB-1418-2022
   Covaciu, Florin/AAW-6796-2021
   Vaida, Calin/B-4548-2015
   },
ORCID-Numbers = {Gherman, Bogdan/0000-0002-4427-6231
   Covaciu, Florin/0000-0002-0079-431X
   Vaida, Calin/0000-0003-2822-9790
   Pisla, Doina/0000-0001-7014-9431
   Pisla, Adrian/0000-0002-5531-6913},
Unique-ID = {WOS:000643732600032},
}

@inproceedings{ WOS:000611540300106,
Author = {Dahalan, N. H. and Tutur, N. and Noor, R. H. R. M. and Yunus, J. N. and
   Yusoff, N. N. M.},
Editor = {Nazri, FM},
Title = {Empirical Study on Relationship Between Customer Satisfaction and
   QLASSIC Performance Score in Northern Area},
Booktitle = {PROCEEDINGS OF AICCE'19: TRANSFORMING THE NATION FOR A SUSTAINABLE
   TOMORROW},
Series = {Lecture Notes in Civil Engineering},
Year = {2020},
Volume = {53},
Pages = {1377-1385},
Note = {AWAM International Conference on Civil Engineering (AICCE), MALAYSIA,
   AUG 21-22, 2019},
Organization = {AWAM; Univ Sains Malaysia, Sch Civil Engn; Construct Ind Dev Board},
Abstract = {There are many factors that may contribute to the successful of
   producing quality product for construction project in Malaysia and one
   of the factors is by using legitimate monitoring checklist as
   assessment. In Malaysia, Quality Assessment System in Construction
   (QLASSIC) had been implemented and most of the construction project had
   obtained highest percentage of QLASSIC performance score. This research
   is carried out to identify the relationship between customer
   satisfaction and QLASSIC performance score. The aim of this study is to
   collect data by using sets of questionnaires. The frequency analyses are
   carried out by using Statistical Package for Social Science (SPSS)
   software. The result shows that customers of the building with higher
   QLASSIC performance score is highly satisfied with their condition of
   the building and there is significant relationship between QLASSIC
   performance score with customer's satisfaction especially in
   architecture aspects.},
DOI = {10.1007/978-3-030-32816-0\_106},
ISSN = {2366-2557},
EISSN = {2366-2565},
ISBN = {978-3-030-32816-0; 978-3-030-32815-3},
ORCID-Numbers = {Tutur, Nuraini/0000-0002-9205-8669},
Unique-ID = {WOS:000611540300106},
}

@article{ WOS:000553561100038,
Author = {Gupta, Shubham Babu and Wadhwa, Pankaj},
Title = {ASSESSMENT OF RATIONAL USE OF ANTIBIOTICS IN ORTHOPEDIC PROCEDURES},
Journal = {INTERNATIONAL JOURNAL OF LIFE SCIENCE AND PHARMA RESEARCH},
Year = {2020},
Number = {9},
Pages = {154-162},
Month = {JAN},
Abstract = {Appropriate use of antimicrobial agents is vitally important from
   clinical perspectives and is essential if the usefulness of antibiotic
   is to be preserved and the further spread of resistance is to be
   limited. Antibiotics are one of the pillars of modern medical care and
   play a major role both in prophylaxis and treatment of infectious
   diseases. However, their misuse is a worldwide problem with the extent
   of the problem being greater in the developing countries. To assess
   rational use of antibiotics for prophylaxis and treatment among patients
   in orthopedic department who undergone surgery. A prospective and
   observational study was conducted on 130 medical records of orthopedic
   patients who undergone surgery. Stratified random sampling technique was
   used for this study. Data was collected by using a pre-tested structured
   questionnaire from September 2019 to December 2019 and then analyzed
   using statistical package for social science (SPSS) version 20.0
   software. Out of 130 patients who undergone surgery 70 patients were
   male and 60 patients were females. Based on age group classification 30
   40 years of age 50 (38.46\%) was mostly affected. The most common
   diagnosis was ORIF procedures accounts for (43.07\%) followed by total
   hip replacement (20\%) of cases. The most frequently prescribed
   antibiotics class was cephalosporins (76.43\%), followed by Penicillin's
   (15.51\%) for prophylaxis Cefuroxime (67.40\%)followed by amikacin
   (14.91\%) and for treatment Cefuroxime (58.17\%)followed by amikacin
   (14.90\%). The most commonly used antibiotics combination for
   prophylaxis is Ceftriazone + Amikacin + Cefuroxime (20.68\%) and
   treatment includes Ceftriaxone + Cefuroxime + clavulanic acid (34.04\%).
   It was found that 50\% of cases for prophylaxis was inappropriate where
   as 73.07 \% of treatment antibiotics failed to adhere to guidelines.
   Generally, this study indicated that orthopedic patients underwent
   surgical procedures by majority of patients were ORIF and Total hip
   replacement. Mostly prescribed antibiotics in the patients who undergone
   major surgery were cefuroxime followed by Amikacin. In general, this
   study result indicated some level of inappropriateness which high light
   need for intervention.},
ISSN = {2250-0480},
Unique-ID = {WOS:000553561100038},
}

@inproceedings{ WOS:000617734800020,
Author = {Ham, Tae Jun and Bruns-Smith, David and Sweeney, Brendan and Lee, Yejin
   and Seo, Seong Hoon and Song, U. Gyeong and Oh, Young H. and Asanovic,
   Krste and Lee, Jae W. and Wills, Lisa Wu},
Book-Group-Author = {IEEE},
Title = {Genesis: A Hardware Acceleration Framework for Genomic Data Analysis},
Booktitle = {2020 ACM/IEEE 47TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER
   ARCHITECTURE (ISCA 2020)},
Series = {ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE},
Year = {2020},
Pages = {254-267},
Note = {47th ACM/IEEE Annual International Symposium on Computer Architecture
   (ISCA), ELECTR NETWORK, MAY 30-JUN 03, 2020},
Organization = {IEEE; ACM; IEEE Comp Soc},
Abstract = {In this paper, we describe our vision to accelerate algorithms in the
   domain of genomic data analysis by proposing a framework called Genesis
   (genome analysis) that contains an interface and an implementation of a
   system that processes genomic data efficiently. This framework can be
   deployed in the cloud and exploit the FPGAs-as-a-service paradigm to
   provide cost-efficient secondary DNA analysis. We propose
   conceptualizing genomic reads and associated read attributes as a very
   large relational database and using extended SQL as a domain-specific
   language to construct queries that form various data manipulation
   operations. To accelerate such queries, we design a Genesis hardware
   library which consists of primitive hardware modules that can be
   composed to construct a dataflow architecture specialized for those
   queries.
   As a proof of concept for the Genesis framework, we present the
   architecture and the hardware implementation of several genomic analysis
   stages in the secondary analysis pipeline corresponding to the best
   known software analysis toolkit, GATK4 workflow proposed by the Broad
   Institute. We walk through the construction of genomic data analysis
   operations using a sequence of SQL-style queries and show how Genesis
   hardware library modules can be utilized to construct the hardware
   pipelines designed to accelerate such queries. We exploit parallelism
   and data reuse by utilizing a dataflow architecture along with the use
   of on-chip scratchpads as well as non-blocking APIs to manage the
   accelerators, allowing concurrent execution of the accelerator and the
   host. Our accelerated system deployed on the cloud FPGA performs up to
   19.3x better than GATK4 running on a commodity multi-core Xeon server
   and obtains up to 15x better cost savings. We believe that if a software
   algorithm can be mapped onto a hardware library to utilize the
   underlying accelerator(s) using an already-standardized software
   interface such as SQL, while allowing the efficient mapping of such
   interface to primitive hardware modules as we have demonstrated here, it
   will expedite the acceleration of domain-specific algorithms and allow
   the easy adaptation of algorithm changes.},
DOI = {10.1109/ISCA45697.2020.00031},
ISSN = {0884-7495},
ISBN = {978-1-7281-4661-4},
Unique-ID = {WOS:000617734800020},
}

@inproceedings{ WOS:000568240800050,
Author = {Hu, Mingzhe and Zhang, Yu},
Editor = {Kontogiannis, K and Khomh, F and Chatzigeorgiou, A and Fokaefs, ME and Zhou, M},
Title = {The Python/C API: Evolution, Usage Statistics, and Bug Patterns},
Booktitle = {PROCEEDINGS OF THE 2020 IEEE 27TH INTERNATIONAL CONFERENCE ON SOFTWARE
   ANALYSIS, EVOLUTION, AND REENGINEERING (SANER `20)},
Year = {2020},
Pages = {532-536},
Note = {27th IEEE International Conference on Software Analysis, Evolution, and
   Reengineering (SANER), London, CANADA, FEB 18-21, 2020},
Organization = {IEEE; IEEE Comp Soc; Western Univ; IEEE Tech Council Software Engn},
Abstract = {Python has become one of the most popular programming languages in the
   era of data science and machine learning, especially for its diverse
   libraries and extension modules. Python front-end with C/C++ native
   implementation 3 achieves both productivity and performance, almost
   becoming the standard structure for many mainstream software systems.
   However, feature discrepancies between two languages can pose many
   security hazards in the interface layer using the Python/C API. In this
   paper, we applied static analysis to reveal the evolution and usage
   statistics of the Python/C API, and provided a summary and
   classification of its 10 bug patterns with empirical bug instances from
   Pillow, a widely used Python imaging library. Our toolchain can be
   easily extended to access different types of syntactic bug -finding
   checkers. And our systematical taxonomy to classify bugs can guide the
   construction of more highly automated and high -precision bug-finding
   tools.},
ISBN = {978-1-7281-5143-4},
ResearcherID-Numbers = {Hu, Mingzhe/AAS-1874-2021
   Zhang, Yu/AAQ-1832-2021
   },
ORCID-Numbers = {Zhang, Yu/0000-0001-6638-6442
   Hu, Mingzhe/0000-0001-9808-4967},
Unique-ID = {WOS:000568240800050},
}

@inproceedings{ WOS:000565215600065,
Author = {Kazemzadeh, Hessam},
Editor = {Soules, JG},
Title = {Full Scale Structural Test Program for Prefab Modular Construction},
Booktitle = {STRUCTURES CONGRESS 2020},
Year = {2020},
Pages = {709-719},
Note = {Structures Congress, St Louis, MO, APR 05-08, 2020},
Organization = {Amer Soc Civil Engineers, Struct Engn Inst},
Abstract = {As the prefab modular construction is gaining more popularity, there is
   a growing need to accurately predict the performance of the ``modules
   stack{''} and their interconnectivity under extreme loading conditions
   such as strong earthquakes or high winds. Due to the uniqueness of the
   assemblies and connections used in the volumetric modules, especially in
   a high-rise context, it is essential to establish a framework in which
   both experimental and analytical models are used in tandem to fully
   evaluate the overall structural system's performance. Together they can
   help the engineering team gain more confidence in the structural
   system's integrity under severe excitations. In this case study, a
   full-scale test program is discussed which was integrated into the
   design process of a thirty story prefab modular highrise in Oakland, CA.
   Twenty-two story residential floors over eight-story podium. Monotonic
   and cyclic tests of several structural sub-assemblies were carried out
   in collaboration with independent structural testing laboratories. Once
   the full-scale test results were gathered and processed, analytical
   models in high-fidelity FEA software packages were calibrated to
   accurately represent the experimental results. The ``acceptance
   criteria{''} for the modular system's performance was established on
   multiple levels. Using the ``sub-structuring technique{''} a typical
   modular level was isolated from the superstructure to conduct analytical
   simulations on module stacks. This approach helped with reduced
   computational runtime and enabled faster module design iterations
   without losing accuracy. The sub-assemblies were modeled using
   combinations of line elements and semi-rigid hinges. Utilizing
   performance based design (PBD) in conjunction with full-scale testing of
   critical structural components helped to verify the capability and
   performance of modular assemblies to withstand 2.75\% inter-story drift
   under MCE and their continuous elastic behavior under extreme building
   deformation.},
ISBN = {978-0-7844-8289-6},
Unique-ID = {WOS:000565215600065},
}

@inproceedings{ WOS:000612835700004,
Author = {Khodajou-Chokami, Hamidreza and Bitarafan, Adeleh and Dylov, V, Dmitry
   and Baghshah, Mahdieh Soleymani and Hosseini, Seyed Abolfazl},
Book-Group-Author = {IEEE},
Title = {Personalized Computational Human Phantoms via a Hybrid Model-based Deep
   Learning Method},
Booktitle = {2020 IEEE INTERNATIONAL SYMPOSIUM ON MEDICAL MEASUREMENTS AND
   APPLICATIONS (MEMEA)},
Series = {IEEE International Symposium on Medical Measurements and Applications
   Proceedings-MeMeA},
Year = {2020},
Note = {15th IEEE International Symposium on Medical Measurements and
   Applications (MeMeA), Bari, ITALY, JUN 01-03, 2020},
Organization = {IEEE; IEEE Instrumentat \& Measurement Soc; IEEE Sensors Council Italy
   Chapter; Politecnico Torino; Soc Italiana Analisi Movimento Clinica;
   Politecnica Bari},
Abstract = {Computed tomography (CT) simulators are versatile tools for scanning
   protocol evaluation, optimization of geometrical design parameters,
   assessment of image reconstruction algorithms, and evaluation of the
   impact of future innovations attempting to improve the performance of CT
   scanners. Computational human phantoms (CHPs) play a key role in
   simulators for the radiation dosimetry and assessment of image quality
   tasks in the medical x-ray systems. Since the construction of
   patient-specific CHPs can be both difficult and time-consuming, nominal
   standard/reference CHPs have been established, yielding significant
   discrepancies in the special design and optimization demands of patient
   dose and imaging protocols for most medical applications. Therefore, the
   aim of this work was to develop a personalized Monte-Carlo (MC) CT
   simulator equipped with a fast and well-structured tool-kit called
   DeepSegNet for automatic generation of patient-specific CHPs based on
   MRI images, working under two principal algorithms. To this end, we
   first developed a 3D convolutional neural network (3DCNN) for the
   automated segmentation of 3D MRI images to detect anatomical
   organs/tissues. Then, a 3D voxel merging (3DVM) algorithm constructing
   CHPs and making fast MC calculations were developed. The proposed 3DCNN
   benefits from the main merit of residual networks by designing a
   15-layer model. Next, the 3DVM algorithm utilizes the segmented data
   acquired from the former step, to create realistic and optimized CHPs by
   material mapping and voxel size manipulating. The performance of our
   3DCNN model on 20 patients as test cases was 84.54\% and 74.52 \% in
   terms of average accuracy and Dice-Coefficient, respectively,
   outperforming SegNet, as a comparable method by 2\%. Finally, we
   developed an MC CT simulator by implementing a set of our generated
   CHPs. The efficiency of our 3DVM algorithm in constructing CHPs was
   assessed in terms of MC execution time and the number of merged voxels
   representing occupied storage memory and compared to the existing
   lattice method. Besides, the accuracy of our 3DVM investigated through
   the estimation of patient dose maps and image reconstruction. Results
   demonstrated a significant reduction of about 96 \% in the number of
   voxels and a 15\% reduction in MC execution time for x-ray photon
   transportation while keeping the same accuracy. Therefore, this software
   package has a strong potential in the optimization of therapeutic and
   radiological imaging procedures.},
ISBN = {978-1-7281-5386-5},
ORCID-Numbers = {Dylov, Dmitry V./0000-0003-2251-3221},
Unique-ID = {WOS:000612835700004},
}

@inproceedings{ WOS:000773388000102,
Author = {Kovalev, V, I. and Saramud, V, M. and Testoyedov, N. A. and Kovalev, I,
   D. and Kuznetsov, A. S. and Koltashev, A. A.},
Book-Group-Author = {IOP},
Title = {Software package for the implementation of bioinspired algorithms for
   the design of fault-tolerant control systems},
Booktitle = {II INTERNATIONAL SCIENTIFIC CONFERENCE ON APPLIED PHYSICS, INFORMATION
   TECHNOLOGIES AND ENGINEERING 25, PTS 1-5},
Series = {Journal of Physics Conference Series},
Year = {2020},
Volume = {1679},
Note = {2nd International Scientific Conference on Applied Physics, Information
   Technologies and Engineering (APITECH), Krasnoyarsk, RUSSIA, SEP 25-OCT
   04, 2020},
Abstract = {The article considers a software package that allows you to form the
   composition of multiversion software for fault-tolerant control systems.
   The software implementation is based on a modified ant colony algorithm,
   which belongs to the class of bioinspired algorithms. The algorithm
   implements the search direction in order to minimize the cost of a
   program consisting of a set of modules. For each module, there are
   several versions of its implementation, developed within the framework
   of the multiversion programming methodology. The search direction is
   also implemented in order to maximize the reliability of the modular
   program in multiversion execution. The software implementation of the
   algorithm allows a number of experiments to be performed to obtain data
   that can be used to compare the performance of the standard and modified
   ant colony algorithms.},
DOI = {10.1088/1742-6596/1679/3/032001},
Article-Number = {032001},
ISSN = {1742-6588},
EISSN = {1742-6596},
ResearcherID-Numbers = {Kovalev, Igor/S-9633-2017
   },
ORCID-Numbers = {Kovalev, Igor/0000-0003-2128-6661
   Kovalev, Dmitry/0000-0001-5308-308X},
Unique-ID = {WOS:000773388000102},
}

@inproceedings{ WOS:000629086600126,
Author = {Lee, Jihyun and Kim, Taeyoung and Kang, Sungwon},
Editor = {Chan, WK and Claycomb, B and Takakura, H and Yang, JJ and Teranishi, Y and Towey, D and Segura, S and Shahriar, H and Reisman, S and Ahamed, SI},
Title = {Recovering Software Product Line Architecture of Product Variants
   Developed with the Clone-and-Own Approach},
Booktitle = {2020 IEEE 44TH ANNUAL COMPUTERS, SOFTWARE, AND APPLICATIONS CONFERENCE
   (COMPSAC 2020)},
Series = {Proceedings International Computer Software and Applications Conference},
Year = {2020},
Pages = {985-990},
Note = {44th Annual IEEE-Computer-Society International Conference on Computers,
   Software, and Applications (COMPSAC), ELECTR NETWORK, JUL 13-17, 2020},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {Software products developed with the clone-and-own approach pose
   difficulties in maintenance. Migrating to software product line can
   solve this problem. This paper proposes an approach to recover software
   product line architecture (PLA) from a family of products developed with
   the clone-and-own approach. The proposed approach decomposes all source
   code of the family of products and analyses cloned copy classes, cloned
   modification classes and product-specific classes. From the results, it
   recovers a PLA. For determining common and variable classes, the
   proposed approach uses Harmonized Total Constant Commonality Indices of
   packages or classes of a product line (HTCCIpL). We apply our approach
   to recover the PLA of the Apo-Games developed with the clone-and-own
   approach. The results show that our approach recovers the Apo-Games PLA
   with a set of guidelines that can assist product line engineers in
   making decisions on commonality and variability of architectural
   elements of a PLA.},
DOI = {10.1109/COMPSAC48688.2020.0-143},
ISSN = {0730-3157},
ISBN = {978-1-7281-7303-0},
Unique-ID = {WOS:000629086600126},
}

@article{ WOS:000597194000001,
Author = {Lee, Ki-Seong and Lee, Chan-Gun},
Title = {Identifying Semantic Outliers of Source Code Artifacts and Their
   Application to Software Architecture Recovery},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {212467-212477},
Abstract = {Understanding software architecture is essential to software
   maintenance. There has been much effort to derive software architecture
   views from source code artifacts. Typically, along with structural
   information, the semantic information derived from an identifier name
   and comments are helpful. However, because code vocabulary choice
   depends on a developer's subjective decision, some source code may have
   semantically low text quality, leading to an inaccurate architecture
   recovery. This paper aims to improve the architecture recovery of a
   software system by identifying and removing the semantic outliers of
   source code artifacts. Accordingly, we propose a novel measure
   Conceptual Conformity (CC), which computes the similarity between two
   latent topic distributions obtained from both the source code and its
   package. We use CC to identify source code that is not relevant to the
   package's semantic context and define it as a semantic outlier. Because
   the semantic outliers may cause inaccurate architecture recovery, we
   remove them during the recovery process. We apply our approach to three
   open-source projects. The results demonstrate that, for projects with
   low recovery performance, removing outliers leads to higher recovery
   accuracy.},
DOI = {10.1109/ACCESS.2020.3040024},
ISSN = {2169-3536},
ResearcherID-Numbers = {Lee, Kiseong/O-2824-2015},
ORCID-Numbers = {Lee, Kiseong/0000-0002-0906-9552},
Unique-ID = {WOS:000597194000001},
}

@inproceedings{ WOS:000675598100485,
Author = {Liu, Xiaoyuan and Bu, Fanliang and Sun, Tianyang and Qin, Hang},
Book-Group-Author = {IEEE},
Title = {The analysis on the role of social network in the field of
   anti-terrorism-take the ``East Turkistan{''} organization as an example},
Booktitle = {2020 5TH INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER
   ENGINEERING (ICMCCE 2020)},
Year = {2020},
Pages = {2282-2285},
Note = {5th International Conference on Mechanical, Control and Computer
   Engineering (ICMCCE), Harbin, PEOPLES R CHINA, DEC 25-27, 2020},
Organization = {Xijing Univ; Acad Exchange Informat Ctr},
Abstract = {In order to analyze the network structure of the ``East Turkistan{''}
   organization and identify members with higher individual threats, so as
   to conduct a better attack on the ``East Turkistan{''} organization. We
   use Gephi, a network analysis software, and collect open source
   information on three batches of 25 terrorists announced by the Ministry
   of Public Security and use networkX, a Python package to study the
   structure of complex networks, to conduct social network analysis(SNA)
   of the ``East Turkistan{''} organization personnel. The ``East
   Turkistan{''} organization presents a combination of scale-free network
   and hierarchical network in the topological structure. To manifest the
   analysis of social network methods' importance in combating terrorism,
   we simulate the effect of removing the core members of the ``East
   Turkistan{''} organization . The construction of SNA requires a large
   deal of open source information. How to obtain intelligence is an
   important factor that restricts precision strikes. In addition, how to
   combine social network analysis with computer deep learning knowledge is
   also a new hot spot and direction in the field of anti-terrorism.},
DOI = {10.1109/ICMCCE51767.2020.00493},
ISBN = {978-1-6654-2314-4},
Unique-ID = {WOS:000675598100485},
}

@inproceedings{ WOS:000637244600058,
Author = {Martinez Sune, Agustin E.},
Book-Group-Author = {IEEE},
Title = {Formalization and analysis of quantitative attributes of distributed
   systems},
Booktitle = {2020 ACM/IEEE 42ND INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING:
   COMPANION PROCEEDINGS (ICSE-COMPANION 2020)},
Series = {International Conference on Software Engineering},
Year = {2020},
Pages = {210-213},
Note = {42nd ACM/IEEE International Conference on Software Engineering -
   Companion Proceedings (ICSE-Companion), ELECTR NETWORK, JUN 27-JUL 19,
   2020},
Organization = {Assoc Comp Machinery; IEEE; IEEE Comp Soc; IEEE Comp Soc Tech Comm
   Software Engn; ACM Special Interest Grp Software Engn; Korean Inst
   Informat Scientists \& Engineers; Natl Sci Fdn; Facebook; N Carolina
   State Univ; Microsoft; Samsung; LG Elect; KAIST; SK Hynix; NAVER;
   Suresoft; HITACHI; Google},
Abstract = {While there is not much discussion on the importance of formally
   describing and analyzing quantitative requirements in the process of
   software construction; in the paradigm of API-based software systems, it
   could be vital. Quantitative attributes can be thought of as attributes
   determining the Quality of Service - QoS provided by a software
   component published as a service. In this sense, they play a determinant
   role in classifying software artifacts according to specific needs
   stated as requirements.
   In this work, we present a research program consisting of the
   development of formal languages and tools to characterize and analyze
   the Quality of Service attributes of software components in the context
   of distributed systems. More specifically, our main motivational
   scenario lays on the execution of a service-oriented architecture.},
DOI = {10.1145/3377812.3381387},
ISSN = {0270-5257},
Unique-ID = {WOS:000637244600058},
}

@inproceedings{ WOS:000681699104080,
Author = {Mueller, Sascha and Hoeflinger, Kilian and Smisek, Michal and Gerndt,
   Andreas},
Book-Group-Author = {IEEE},
Title = {Towards an FDIR Software Fault Tree Library for Onboard Computers},
Booktitle = {2020 IEEE AEROSPACE CONFERENCE (AEROCONF 2020)},
Series = {IEEE Aerospace Conference Proceedings},
Year = {2020},
Note = {IEEE Aerospace Conference, Big Sky, MT, MAR 06-13, 2020},
Organization = {IEEE; AIAA; PHM Soc; IEEE AESS; Cornell Tech Serv; Adv Inst Ind Technol;
   Motiv; IFT},
Abstract = {The increasing complexity of space missions, their software
   architectures, and hardware that has to meet the demands for those
   missions, imposes numerous new challenges for many engineering
   disciplines such as reliability engineering. Affected by the ever
   growing demand for more onboard computation power are the onboard
   computers. They in return require Fault Detection, Isolation, and
   Recovery (FDIR) architectures to support their fault tolerant operation
   in the harsh environment of space. Especially high performance
   commercial processing units face the challenge of dealing with negative
   radiation effects, which may significantly degrade their operation. To
   design performant and fault tolerant onboard computers, it is of high
   interest to assess the effectiveness of the FDIR architecture in the
   early phase of system design. This can be achieved using Fault Tree
   Analysis (FTA). However, to create complete fault trees manually is an
   error prone and labor intensive task.
   In this paper, the methodology for assessing the FDIR design of onboard
   computers in space systems, presented in In is refined by introducing a
   library of FDIR routines. The routines are modeled using fault trees and
   are composed into a software system fault tree using a basic fault model
   and a design configuration chosen by the reliability engineer. To assess
   the configurations, we give a heuristic based on a factor-criteriametric
   model. We demonstrate the feasability of our approach on the basis of a
   case study on the rover of the Martian Moons eXploration (MMX) mission.
   Several FDIR configurations are studied and fault trees are generated
   for them. For the chosen case study, we obtain a reduction of up to 80\%
   in terms of modeling effort.},
ISSN = {1095-323X},
ISBN = {978-1-7281-2734-7},
ResearcherID-Numbers = {Gerndt, Andreas/AAO-2644-2021},
ORCID-Numbers = {Gerndt, Andreas/0000-0002-0409-8573},
Unique-ID = {WOS:000681699104080},
}

@inproceedings{ WOS:000664604600018,
Author = {Oya, Igor and CTA Observ},
Editor = {Angeli, GZ and Dierickx, P},
Title = {Supporting the coordination of a software work-package of the Cherenkov
   Telescope Array via model-driven methodologies},
Booktitle = {MODELING, SYSTEMS ENGINEERING, AND PROJECT MANAGEMENT FOR ASTRONOMY IX},
Series = {Proceedings of SPIE},
Year = {2020},
Volume = {11450},
Note = {Conference on Modeling, Systems Engineering, and Project Management for
   Astronomy IX, ELECTR NETWORK, DEC 14-22, 2020},
Organization = {SPIE},
Abstract = {The Cherenkov Telescope Array (CTA) is the next-generation atmospheric
   Cherenkov gamma-ray observatory. CTA will be deployed as two
   installations, one in the Northern and the other in the Southern
   Hemisphere, containing telescopes of three different sizes, for covering
   different energy domains, with varying designs. The CTA Observatory
   (CTAO) is building a complex and distributed software system for the
   efficient operation of the arrays and the management and scientific
   exploitation of the CTA data. The largest fraction of the construction
   budget of CTA will come as in-kind contributions (IKCs) from CTA
   Shareholders (participating countries and research centres), with the
   prominent IKC example of the telescopes themselves. Most of the effort
   to build the CTA software will be provided by IKCs as well, with CTAO
   personnel managing the coordination between partners, ensuring common
   development practices, and facilitating that the requirements,
   integration, quality level, and deadlines are properly met. This
   contribution presents the management plan of one of the main CTA
   software work packages, the Array Control and Data Acquisition (ACADA).
   The ACADA software will be responsible for the coordination of the
   control and data acquisition of telescopes, as well as many auxiliary
   instruments. The ACADA team will be composed of about 30 developers
   distributed in eight IKC teams, CTAO personnel, and company contracts.
   The contribution shows how model-driven software architecture practices
   are essential for the management of such a distributed team, which is
   predominantly composed of IKCs.},
DOI = {10.1117/12.2560929},
Article-Number = {1145011-1},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-3688-0},
ResearcherID-Numbers = {Oya, Igor/GYU-6580-2022},
Unique-ID = {WOS:000664604600018},
}

@inproceedings{ WOS:000648432000107,
Author = {Polyankin, A. G. and Potokina, A. and Kulikova, E. Yu},
Book-Group-Author = {IOP Publishing},
Title = {Geotechnical risk assessment during the construction of international
   crossing under the runways of Sheremetyevo airport},
Booktitle = {INTERNATIONAL CONFERENCE ON CONSTRUCTION, ARCHITECTURE AND TECHNOSPHERE
   SAFETY (ICCATS 2020)},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2020},
Volume = {962},
Note = {International Conference on Construction, Architecture and Technosphere
   Safety (ICCATS), Sochi, RUSSIA, SEP 06-12, 2020},
Organization = {S Ural State Univ; Irkutsk Natl Res Tech Univ; B N Yeltsin Ural Fed Univ},
Abstract = {The surrounding buildings, topographical, engineering-geological and
   hydrogeological conditions of the construction site area are factors of
   potential geotechnical risks and influence the occurrence of emergencies
   during the building of underground structures. It is irrefutable fact,
   that while the development of the underground space of cities due
   attention is not given to geotechnical risks, considering only issues
   related to economic damage. In addition, there is no comprehensive risk
   control methodology for the construction of complex multifunctional
   facilities. In order to prevent the risk of accidents, during the
   construction of the Inter-terminal transition under the runways of
   Sheremetyevo Airport in Moscow method of geotechnical risk control was
   developed, which proved to be an effective method in the prevention of
   accidents during construction. Recommendations were developed to reduce
   the level of risk at the considered facility, including verification
   calculations carried out by an independent organization in a certified
   geotechnical software package; geophysical surveys of runway foundation;
   filling voids before sinking under the runway; exploratory drilling and
   geophysical surveys before sinking hazardous areas.},
DOI = {10.1088/1757-899X/962/3/032017},
Article-Number = {032017},
ISSN = {1757-8981},
ORCID-Numbers = {Kulikova, Elena/0000-0002-9290-671X},
Unique-ID = {WOS:000648432000107},
}

@article{ WOS:000607658400027,
Author = {Quan-Hoang Vuong and Viet-Phuong La and Minh-Hoang Nguyen and Manh-Toan
   Ho and Trung Tran and Manh-Tung Ho},
Title = {Bayesian analysis for social data: A step-by-step protocol and
   interpretation},
Journal = {METHODSX},
Year = {2020},
Volume = {7},
Abstract = {The paper proposes Bayesian analysis as an alternative approach for the
   conventional frequentist approach in analyzing social data. A
   step-by-step protocol of how to implement Bayesian multilevel model
   analysis with social data and how to interpret the result is presented.
   The article used a dataset regarding religious teachings and behaviors
   of lying and violence as an example. An analysis is performed using R
   statistical software and a bayesvl R package, which offers a
   network-structured model construction and visualization power to
   diagnose and estimate results.
   The paper provides guidance for conducting a Bayesian multilevel
   analysis in social sciences through constructing directed acyclic graphs
   (DAGs, or ``relationship trees{''}) for different models, basic and more
   complex ones.
   The method also illustrates how to visualize Bayesian diagnoses and
   simulated posterior.
   The interpretations of visualized diagnoses and simulated posteriors of
   Bayesian inference are also discussed. (C) 2020 The Author(s). Published
   by Elsevier B.V.},
DOI = {10.1016/j.mex.2020.100924},
Article-Number = {100924},
EISSN = {2215-0161},
ResearcherID-Numbers = {Nguyen, Minh-Hoang/GQO-9183-2022
   Nguyen, Minh-Hoang/N-8135-2019
   La, Viet-Phuong/U-7155-2018
   Tran, Trung/W-4450-2019
   Toan, Ho Manh/T-5435-2019
   Vuong, Hoang/F-2115-2010
   },
ORCID-Numbers = {Nguyen, Minh-Hoang/0000-0002-7520-3844
   Nguyen, Minh-Hoang/0000-0002-7520-3844
   La, Viet-Phuong/0000-0002-4301-9292
   Tran, Trung/0000-0002-0459-7284
   Toan, Ho Manh/0000-0002-8292-0120
   Vuong, Hoang/0000-0003-0790-1576
   Ho, Tung/0000-0002-4432-9081},
Unique-ID = {WOS:000607658400027},
}

@inproceedings{ WOS:000652190900050,
Author = {Sadeghi, Marjan and Elliott, Jonathan W. and Mehany, Mohammed S. Hashem
   M.},
Editor = {Tang, PB and Grau, D and Elasmar, M},
Title = {Automatic Verification of Facilities Management Handover Building
   Information Models},
Booktitle = {CONSTRUCTION RESEARCH CONGRESS 2020: COMPUTER APPLICATIONS},
Year = {2020},
Pages = {488-497},
Note = {Construction Research Congress (CRC) on Construction Research and
   Innovation to Transform Society, Arizona State Univ, Del E Webb Sch
   Construct, Tempe, AZ, MAR 08-10, 2020},
Organization = {Construct Res Council; Amer Soc Civil Engineers, Construct Inst; Amer
   Soc Civil Engineers; Arizona State Univ},
Abstract = {Although building information models (BIM) are increasingly recognized
   as a compelling tool for project delivery, the challenges of seamless
   model validation and the transition of BIM data to FM remains
   unresolved. BIM data interoperability issues, including lack of a clear
   understanding of owner's exchange requirements (ERs), inconsistent
   syntax in developing discipline-specific models, and software
   limitations contribute to the difficulty of using architecture,
   engineering, and construction-purposed BIMs for FM handover. This
   research explores a case-based example of implementing computational BIM
   for automatic verification of an FM-handover BIM at project close out.
   The relationships between, and properties of, model objects used to
   capture the required geometric and non-geometric information for the
   owner's needs at project close out are specified in accordance with the
   applicable IFC concepts. The terminologies and taxonomies are formalized
   in conjunction with the buildingSMART Data Dictionary (bsDD) schema.
   Dynamo is used as a visual programming platform to work with the
   Autodesk Revit application programming interface (API) to extend its
   parametric capabilities for automatic verification of BIM data in
   compliance with the specified requirements. The results of the automated
   verification procedure highlight completeness, uniqueness, correctness,
   usefulness, and unambiguity of BIM data as critical quality dimensions
   for the purpose of project delivery.},
ISBN = {978-0-7844-8286-5},
Unique-ID = {WOS:000652190900050},
}

@inproceedings{ WOS:000587897600014,
Author = {Santos, Joanna C. S. and Moshtari, Sara and Mirakhorli, Mehdi},
Book-Group-Author = {IEEE},
Title = {An Automated Approach to Recover the Use-case View of an Architecture},
Booktitle = {2020 IEEE INTERNATIONAL CONFERENCE ON SOFTWARE ARCHITECTURE COMPANION
   (ICSA-C 2020)},
Year = {2020},
Pages = {63-66},
Note = {17th IEEE International Conference on Software Architecture Companion
   (ICSA-C), Salvador, BRAZIL, MAR 16-20, 2020},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {Many tools and techniques were described in the literature for automated
   recovery of software architecture from software artifacts, such as code.
   These approaches generate models of software architecture at different
   levels of granularity and notations. Whereas there is a vast literature
   in recovering components, packages, and interactions between them, we
   lack automated approaches for recovering the use-case view of an
   architecture. In the 4+1 view of the architecture, use-case view or
   scenarios represents a view of the architecture in terms of the systems'
   core functionalities provided to end-users. This view is essential in
   understanding the system and its underlying architectural decisions.
   Manually recovering and documenting the application's scenarios from
   source code is time-consuming, as large-scale enterprise systems can
   have a large number of scenarios. In this NEMI paper, we present a novel
   automated approach for recovering the scenarios from Web applications
   source code. These scenarios are shown in use case diagrams alongside
   with sequence diagrams that further describe how each use case is
   implemented in the system. Our approach works under the assumption that
   the URLs (endpoints) of a Web application can give us clues to the
   system's use cases. Therefore, our technique combines a set of
   heuristics and static analysis in order to detect the endpoints in a
   Java Web application as well as the backend classes and methods that
   will process the request. Subsequently, it uses Natural Language
   Processing (NLP) techniques to extract use cases from these identified
   endpoints and uses the computed program slices to generate sequence
   diagrams for each identified use case. We conducted an initial
   evaluation of our approach by detecting endpoints in Sagan, an existing
   open-source Web application. We then demonstrate the use cases generated
   and how their implementation looks like through sequence diagrams.},
DOI = {10.1109/ICSA-C50368.2020.00020},
ISBN = {978-1-7281-7415-0},
ResearcherID-Numbers = {da Silva Santos, Joanna Cecilia/AAF-7544-2021
   },
ORCID-Numbers = {da Silva Santos, Joanna Cecilia/0000-0001-8743-2516
   Moshtari, Sara/0000-0002-7918-0467},
Unique-ID = {WOS:000587897600014},
}

@article{ WOS:000652593000505,
Author = {Scheuermann, Tobias M. and Kotyczka, Paul and Martens, Christian and
   Louati, Haithem and Maschke, Bernhard and Zanota, Marie-Line and
   Pitault, Isabelle},
Title = {An Object-Oriented Library for Heat Transfer Modelling and Simulation in
   Open Cell Foams},
Journal = {IFAC PAPERSONLINE},
Year = {2020},
Volume = {53},
Number = {2},
Pages = {7575-7580},
Note = {21st IFAC World Congress on Automatic Control - Meeting Societal
   Challenges, ELECTR NETWORK, JUL 11-17, 2020},
Organization = {Int Federat Automat Control; Siemens; Bayer; ABB; MathWorks; Phoenix
   Contact; Ifak Technol; Berlin Heart; Elsevier; De Gruyter; Tele Medi
   GmbH},
Abstract = {Metallic open cell foams have multiple applications in industry, e. g.
   as catalyst supports in chemical processes. Their regular or
   heterogeneous microscopic structure determines the macroscopic
   thermodynamic and chemical properties. We present an object-oriented
   python library that generates state space models for simulation and
   control from the microscopic foam data, which can be imported from the
   image processing tool iMorph. The foam topology and the 3D geometric
   data are the basis for discrete modeling of the balance laws using the
   cell method. While the material structure imposes a primal chain complex
   to define discrete thermodynamic driving forces, the internal energy
   balance is evaluated on a second chain complex, which is constructed by
   topological duality. The heat exchange between the solid and the fluid
   phase is described based on the available surface data. We illustrate in
   detail the construction of the dual chain complexes, and we show how the
   structured discrete model directly maps to the software objects of the
   python code. As a test case, we present simulation results for a foam
   with a Kelvin cell structure, and compare them to a surrogate finite
   element model with homogeneous parameters. Copyright (C) 2020 The
   Authors.},
DOI = {10.1016/j.ifacol.2020.12.1354},
ISSN = {2405-8963},
Unique-ID = {WOS:000652593000505},
}

@inproceedings{ WOS:000865915900056,
Author = {Sotiropoulos, Stefanos and Kazakis, Georgios and Lagaros, Nikos D.},
Editor = {Lagaros, ND and Abdalla, KM and Marano, GC and Phocas, MC and AlRousan, R},
Title = {High performance topology optimization computing platform},
Booktitle = {1ST INTERNATIONAL CONFERENCE ON OPTIMIZATION-DRIVEN ARCHITECTURAL DESIGN
   (OPTARCH 2019)},
Series = {Procedia Manufacturing},
Year = {2020},
Volume = {44},
Pages = {441-448},
Note = {1st International Conference on Optimization-Driven Architectural Design
   (OPTARCH), Amman, JORDAN, NOV 05-07, 2019},
Organization = {Natl Tech Univ Athens; Jordan Univ Sci \& Technol; Politecnico Torino;
   Univ Cyprus; McGill Univ; Fuzhou Univ},
Abstract = {One of the most challenging tasks in the construction industry nowadays,
   is to reduce the material demands and distribute, in the same time, the
   material among the structural system in the best possible way. Topology
   optimization is a design procedure that is increasingly used, to
   generate optimized forms of structures in several engineering fields.
   The current paper presents the Topology Optimization (TO) module of the
   High-Performance Optimization Computing Platform (HP-OCP) which focuses
   on civil engineering problems. More specifically the SIMP method {[}1]
   is implemented and the topology optimization problem is solved by using
   the OC algorithm. The HP-OCP is a platform which evaluates several
   objective functions, such as the volume of the structure, the compliance
   etc. and can solve constrained or unconstrained structural optimization
   problems. The above libraries are developed in C\#. The core of the
   platform is created in such way that it can be integrated with any CAE
   program that has OAPI, XML or any other type of data exchange format. In
   the proposed work the structural analysis and design software SAP2000 is
   used. Theoretical aspects are discussed in order to implement the
   mathematical formulation in a commercial software. Basic and specific
   features are applied and representative examples are performed. One of
   the highlights of the proposed work is that the above module can be used
   for all kind of finite elements. Benchmark tests are presented with
   structures that are simulated by 2D plane-stress elements, 3D-solid
   elements and shell elements. Furthermore, it is independent of the type
   of the mesh, structured or unstructured, so both examples are presented.
   In the proposed work a powerful tool for both architects and civil
   engineers is introduced. The analysis and design of the structures are
   performed in SAP2000 software, in order to achieve a realistic result
   that could be a solution for a real-world structure. (C) 2020 The
   Authors. Published by Elsevier B.V.},
DOI = {10.1016/j.promfg.2020.02.272},
ISSN = {2351-9789},
ORCID-Numbers = {SOTIROPOULOS, STEFANOS/0000-0001-5669-1320},
Unique-ID = {WOS:000865915900056},
}

@inproceedings{ WOS:000671077600046,
Author = {Suri, Guga and Fu, Jianming and Zheng, Rui and Liu, Xinying},
Editor = {Wang, GJ and Ko, R and Bhuiyan, MZA and Pan, Y},
Title = {Family Identification of AGE-Generated Android Malware Using Tree-Based
   Feature},
Booktitle = {2020 IEEE 19TH INTERNATIONAL CONFERENCE ON TRUST, SECURITY AND PRIVACY
   IN COMPUTING AND COMMUNICATIONS (TRUSTCOM 2020)},
Series = {IEEE International Conference on Trust Security and Privacy in Computing
   and Communications},
Year = {2020},
Pages = {386-393},
Note = {19th IEEE International Conference on Trust, Security and Privacy in
   Computing and Communications (IEEE TrustCom), Guangzhou, PEOPLES R
   CHINA, DEC 29-JAN 01, 2020-2021},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {Application Generation Engine(AGE) is a development tool that can
   automatically generate simple Android applications by utilizing some
   boilerplate codes. People with little software programming background
   could also develop Android applications by using this tool based on
   their requirements. The emergence of AGE dramatically improves the ease
   of developing essential software and lowers the level of programming
   skills required for app developers. However, it also provides easy
   access for attackers to quickly develop a large number of malicious
   applications, which will seriously affect the device and data security
   of regular users.
   Since AGE mainly generates applications based on some boilerplate codes,
   the code structures of malicious apps created by AGE have a high degree
   of similarity when these apps belong to the same family. Based on the
   assumption that the package directory structures of the software from
   the same family are also similar, we designed a novel feature
   construction method to describe the application. Using this method, we
   extracted features from the leaf nodes of the smali tree, while each
   smali tree corresponds to the smali directory of the application. Unlike
   traditional static feature extraction of applications, the tree-based
   feature proposed in this paper can effectively counteract problems such
   as code obfuscation or reflection cause it can adequately reflect the
   semantic features of the small files. To prove the effectiveness of
   tree-based features, we also conducted some experiments based on a
   dataset provided by the enterprise. This dataset contains 1792
   AGE-generated applications, and these applications belong to 17
   malicious families. We demonstrated that the feature construction method
   proposed in this paper is usable and can be applied to machine learning
   classification algorithms for the identification of malicious
   applications.},
DOI = {10.1109/TrustCom50675.2020.00060},
ISSN = {2324-898X},
ISBN = {978-1-6654-0392-4},
Unique-ID = {WOS:000671077600046},
}

@article{ WOS:000685202400052,
Author = {Xia, Li-yao and Zakowski, Yannick and He, Paul and Hur, Chung-Kil and
   Malecha, Gregory and Pierce, Benjamin C. and Zdancewic, Steve},
Title = {Interaction Trees Representing Recursive and Impure Programs in Coq},
Journal = {PROCEEDINGS OF THE ACM ON PROGRAMMING LANGUAGES-PACMPL},
Year = {2020},
Volume = {4},
Month = {JAN},
Abstract = {Interaction trees (ITrees) are a general-purpose data structure for
   representing the behaviors of recursive 5 programs that interact with
   their environments. A coinductive variant of ``free monads,{''} ITrees
   are built out of uninterpreted events and their continuations. They
   support compositional construction of interpreters from event handlers,
   which give meaning to events by defining their semantics as monadic
   actions. ITrees are expressive enough to represent impure and
   potentially nonterminating, mutually recursive computations, while
   admitting a rich equational theory of equivalence up to weak
   bisimulation. In contrast to other approaches such as relationally
   specified operational semantics. ITrees re executable via code
   extraction, making them suitable for debugging, testing, and
   implementing software artifacts that are amenable to formal
   verification.
   We have implemented ITrees and their associated theory as a Coq library,
   mechanizing classic domain- and category-theoretic results about program
   semantics, iteration, monadic structures, and equational reasoning.
   Although the internals of the library rely heavily on coinductive
   proofs, the interface hides these details so that clients can use and
   reason about ITrees without explicit use of Coq's coinduction tactics.
   To showcase the utility of our theory, we prove the
   termination-sensitive correctness of a compiler from a simple imperative
   source language to an assembly-like target whose meanings are given in
   an ITree-based denotational semantics. Unlike previous results using
   operational techniques, our bisimulation proof follows straightforwardly
   by structural induction and elementary rewriting via an equational
   theory of combinators for control-flow graphs.},
DOI = {10.1145/3371119},
Article-Number = {51},
EISSN = {2475-1421},
ORCID-Numbers = {Malecha, Gregory/0000-0003-3952-0807
   Xia, Li-yao/0000-0003-2673-4400},
Unique-ID = {WOS:000685202400052},
}

@inproceedings{ WOS:000681746000017,
Author = {Yeoh, C. E. and Kim, D. B. and Won, Y. B. and Lee, S. R. and Yi, H.},
Book-Group-Author = {IEEE},
Title = {Constructing ROS Package for Legged Robot in Gazebo Simulation from
   Scratch},
Booktitle = {2020 20TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS
   (ICCAS)},
Series = {International Conference on Control Automation and Systems},
Year = {2020},
Pages = {94-99},
Note = {20th International Conference on Control, Automation and Systems
   (ICCAS), Busan, SOUTH KOREA, OCT 13-16, 2020},
Organization = {IEEE IES; IEEE Robot \& Automat Soc; IEEE CSS; ECTI Assoc; CAC; ACA;
   ISA; Inst Control Robot \& Syst},
Abstract = {Robot Operating System, (ROS) is one of the open-source, meta-operating
   system, which is now widely used as the robotic software platform and
   can be applicable for anyone who wanted to build their robot from
   scratch. For the credit of the beneficial of ROS, the work of this paper
   describes all the process and structure of the package construction for
   the legged robot simulation in Gazebo. There are five mains folders
   consisted in the package, which are configuration file (config), launch
   file (launch), meshes folder (meshes), script folder (script), Universal
   robot definition format folder (urdf), and worlds folder (worlds). In
   this research, Pseudo-inverse Jacobian was implemented to obtain the
   optimal angular joint for every step walking during the simulation.
   Result of the walking robot simulation are shown to have the least error
   range around 0.0365 m to 0.0867 m differ from the actual target
   position.},
ISSN = {2093-7121},
ISBN = {978-89-93215-19-9},
Unique-ID = {WOS:000681746000017},
}

@article{ WOS:000525390500016,
Author = {Zhu, Yanchao and Liu, Yi and Zhang, Guozhen},
Title = {FT-PBLAS: PBLAS-Based Fault-Tolerant Linear Algebra Computation on
   High-performance Computing Systems},
Journal = {IEEE ACCESS},
Year = {2020},
Volume = {8},
Pages = {42674-42688},
Abstract = {As high-performance computing (HPC) systems have scaled up, resilience
   has become a great challenge. To guarantee resilience, various kinds of
   hardware and software techniques have been proposed. However, among
   popular software fault-tolerant techniques, both the checkpoint-restart
   approach and the replication technique face challenges of scalability in
   the era of peta- and exa-scale systems due to their numerous processes.
   In this situation, algorithm-based approaches, or algorithm-based fault
   tolerance (ABFT) mechanisms, have become attractive because they are
   efficient and lightweight. Although the ABFT technique is
   algorithm-dependent, it is possible to implement it at a low level
   (e.g., in libraries for basic numerical algorithms) and make it
   application-independent. However, previous ABFT approaches have mainly
   aimed at achieving fault tolerance in integrated circuits (ICs) or at
   the architecture level and are therefore not suitable for HPC systems;
   e.g., they use checksums of rows and columns of matrices rather than
   checksums of blocks to detect errors. Furthermore, they cannot deal with
   errors caused by node failure, which are common in current HPC systems.
   To solve these problems, this paper proposes FT-PBLAS, a PBLAS-based
   library for fault-tolerant parallel linear algebra computations that can
   be regarded as a fault-tolerant version of the parallel basic linear
   algebra subprograms (PBLAS), because it provides a series of
   fault-tolerant versions of interfaces in PBLAS. To support the
   underlying error detection and recovery mechanisms in the library, we
   propose a block-checksum approach for non-fatal errors and a scheme for
   addressing node failure, respectively. We evaluate two fault-tolerant
   mechanisms and FT-PBLAS on HPC systems, and the experimental results
   demonstrate the performance of our library.},
DOI = {10.1109/ACCESS.2020.2975832},
ISSN = {2169-3536},
ORCID-Numbers = {Zhang, Guozhen/0000-0002-9956-8119},
Unique-ID = {WOS:000525390500016},
}

@article{ WOS:000517857100001,
Author = {{[}Anonymous]},
Title = {General Practice and the Community: Research on health service, quality
   improvements and training. Selected abstracts from the EGPRN Meeting in
   Vigo, Spain, 17-20 October 2019 Abstracts},
Journal = {EUROPEAN JOURNAL OF GENERAL PRACTICE},
Year = {2020},
Volume = {26},
Number = {1},
Pages = {42-50},
Abstract = {Background: Social isolation, loneliness and anxiety-depressive states
   are emerging health conditions in the elderly. Research question: To
   assess whether a 4-month programme of physical activity in a group
   improves the emotional, social and quality of life situation in a sample
   of subjects over 64 years old people. Methods: Multi-centre randomized
   clinical trial of two groups. Study population: Patients older than 64
   years assigned to three primary care teams from different locations.
   Inclusion criteria: Submit a score <32 on the DUKE-UNC-11 social support
   scale, or >12 on the Beck Depression Scale, or >10 on the Generalized
   Anxiety Scale (GAD-7), at the start of the study. The intervention group
   participated in a group physical activity program for 4-months that
   consisted of progressively walking sessions two days a week, 60-150
   minutes long depending on the physical condition of each participant.
   Results: Enrolled were 94 patients who met the inclusion criteria. Mean
   age was 74 years (SD 5.18) and 76.6\% were women. No significant
   differences were found at the beginning of the study between the two
   groups in relation to the outcome of the scales evaluated. Once the
   intervention was completed, improvement in the quality of life and
   social support was detected in the intervention group (p<.05). Both
   groups improved the depression and anxiety clinic but the improvement in
   the participants of the intervention group was higher. Those with
   initial depression improved 8.6 points on the scale, compared to the
   control that improved 3.3 points, with the final average of 17.4. Those
   who presented initial anxiety improved 8 points (final average: 7.5
   points, cut-off point for the diagnosis of anxiety 10), compared to the
   control that improved 5.1 points. Conclusion: The results of this study
   indicate that the program developed has positive effects on improving
   the quality of life, social support and depression and anxiety clinic.
   Background: Tourism represents 45\% gross domestic product in Balearic
   Islands. Working as a hotel housekeeper (HH) has been associated with
   important morbidity, especially musculoskeletal, chronic pain, a
   significant number of sick leaves, a high consumption of medication,
   poor psychological well-being and worse quality of life. Research
   question: Explore perceptions and opinions regarding the HH's work and
   health problems. Estimate and evaluate HH's health determinants, the
   exposition to several occupational risk factors, their lifestyles and
   health problems and their quality of life. Methods: Design: mixed
   methods: (1) exploratory qualitative study (QS) including 10
   semi-structured interviews and six focus groups; (2). descriptive study
   (DS): individual interviews and clinical medical records. Inclusion
   criteria: older than 18 years, had worked during the last summer season
   in the Balearic Islands. Analysis: QS: transcription and content
   analysis; DS: descriptive statistical analysis. Results: QS: Identified
   positive aspects of their work: timetables, relationship with
   co-workers, attending clients. Highlighted negative aspects: working
   conditions, hard physical workload, stressful duties and insufficiently
   rewarded. HH associated their health problems with their work; coping
   strategies: self-medication or visiting their general practitioner. DS:
   1.043 HH included. Mean age 43.3 years, mean working years as HH 10.7
   years. Mean rooms/day: 18.1 (+/- 6.5); mean beds/day: 44.6 (+/- 20.7).
   HH reported often pain during the last summer season: 68.2\% (IC 95\%
   65.3-71.0) low back pain; 60.9\% (IC 95\% 57.8-63.8) wrist and hands;
   55.3\% (IC 95\% 52.2-58.3) cervical. 41.6\% and 35.1\% self-reported
   regular and poor health status, respectively. Conclusion: HH perceived
   hard and stressful working conditions, partly justified by the number of
   rooms and beds made per day. They also perceived health problems related
   to their work. HH frequently reported pain during the last summer
   season. Moreover, they perceive regular or poor health status, weaker
   than women from the same social class do.
   Background: Gender-based violence (GBV) is a public health and human
   rights issue, being highly prevalent (12-51\%), repetitive and having a
   severe impact on women's health, with a high sanitary and social cost.
   Primary care has a key role in detection and management. There is low
   detection and delay in diagnosis. There is a lack of preparation to
   recognize abuse, especially in the approach and action after detection.
   Greater awareness and sensitization is required. Research question: Can
   a brief specific training intervention in GBV imparted to primary health
   care professionals in their primary health centre increase knowledge,
   improve attitudes and skills? Methods: A cluster-randomized clinical
   trial was carried out in Vigo area primary health centres with at least
   20 health care professionals. A basal evaluation was made through a
   validated inquiry (PREMIS), which they had to retake after three months.
   In the intervention centres, a clinical session was imparted. pResults:
   Out of 264 primary health care professionals, 145 participated. There
   was a 63.5\% loss out of 145 professionals. A statistically significant
   difference was detected in the field of knowledge, increasing an average
   of two points on a scale from 0 to 5 in these aspects: how to make
   appropriate questions; connections between GBV and pregnancy; why do not
   they leave their partners; risk determination and phases of GBV. There
   was also a decrease in the idea that if the patient does not recognize
   gender violence, there is very little that can be done. No significant
   differences were detected in the detection and follow-up. Conclusion:
   Significant differences were found in the knowledge and attitude
   sections after performing the intervention to the professionals. The
   results support the implementation of continuous brief training on GBV
   in primary care.
   Background: Out-of-hours (OOH) primary care is a topic of great interest
   in European countries. Reasons for this are similar across borders: to
   guarantee continuity of care with decreasing numbers of health care
   workers and to guard equity in OOHcare for all patients. In OOHcare
   research, valid and accessible research data are needed to fill the
   knowledge gap. iCAREdata aims to offer valid and immediately available
   information from OOHcare. Research question: How feasible is it to
   collect, store and link data of different OOH services in Belgium and to
   improve data quality registration? How useful are aggregated data to
   inform stakeholders, to evaluate (the quality of) services in OOH care
   and the effects of interventions? Methods: As a first achievement, data
   flows, encryption and encoding were carefully designed and implemented.
   Solid cooperation with the federal eHealth web services as a trusted
   third party was crucial. Ethical approval and approval by the data
   protection authority was obtained. Clear agreements were established
   concerning access control. A strict code of conduct was agreed upon. A
   steering committee was established to guard the procedures. Results:
   First data were collected in 2015. iCAREdata now receives +/- 3000
   unique patient contacts per weekend, spread over 14 general practice
   cooperatives, and covering about a quarter of the Flemish population.
   Aggregated data, directly processed, are provided weekly on . This
   portal site offers an overview of, among others, the latest diagnostics,
   drug prescriptions and workload. iCAREdata project also collects data
   from emergency departments in hospitals and community pharmacists and
   link them to evaluate further OOH primary care. Conclusion: Developing a
   research database on OOHcare is feasible. The iCAREdata project succeeds
   in an automated output every week, offering insights on the evolution of
   morbidity, services and effects of interventions. Careful validation and
   interpretation of the data is a crucial ongoing challenge.
   Background: More than half of decompensations of heart failure are
   attended in primary care setting. No score that helps to ascertain the
   short-term prognosis in these patients. Research question: To develop
   and validate a short-term score (30 days) to predict hospitalizations or
   death in patients attended in primary care as a consequence of
   decompensation of heart failure, based on variables easily measurable in
   primary care setting Methods: Prospective multinational cohort study
   including patients treated because of a heart failure decompensation in
   primary care setting. There were a derivation (Spain) and a validation
   cohort (nine European countries). Results: The derivation cohort
   included 561 patients, women were 56\%, mean age was 82.2 (SD 8.03)
   years and 31.5\% of patients were hospitalized or died in the first
   month. In the validation cohort, 238 patients were included, women were
   54\%, mean age was 79.0 (10.4) years and 26.9\% of patients were
   hospitalized or died in the first month. According to the multivariate
   models, sex, age, hospital admission due to heart failure the previous
   year, and a heart rate greater than 100 beats/minute, orthopnoea,
   paroxysmal nocturnal dyspnoea, NYHA functional stage III or IV,
   saturation of oxygen lower than 90\% or an increase in the dyspnoea at
   the consultation with the General practitioner were included in the
   HEFESTOS-SCORE. The multivariate model including these variables showed
   a good calibration (Hosmer-Lemeshow p=.35) and discrimination (AUC 0.81,
   95\% CI 0.77-0.85). In the validation cohort, the model presented an
   adequate external validation with good calibration (Hosmer-Lemeshow
   p=.35) and discrimination (AUC 0.74, 95\% CI 0.67-0.82). Conclusion: The
   HEFESTOS-SCORE, based on clinical and demographical variables easily
   measurable in primary care is a useful tool to stratify the short-term
   hospitalization and mortality in patients attended because of a heart
   failure decompensation.
   Background: Despite recommendations against long-term benzodiazepine
   (BZD) use, they are often prescribed during months or years in primary
   care. Research question: To determine facilitators and barriers that
   explain the variation in implementation of a primary care educational
   and feedback intervention targeted to general practitioners (GPs) to
   reduce BZDs prescriptions. Methods: A hybrid type I clinical trial:
   qualitative data to evaluate the implementation outcomes. Three health
   districts of Spain: Balearic Islands, Tarragona-Reus district
   (Catalonia) and Arnau de Vilanova lliria district (Valencia). Forty
   stakeholders (GPs) participated in five focus groups; they were selected
   based on their effectiveness of the intervention results: high (three
   groups) or low (two groups) and individual interviews to two GP of low
   efficiency. The Consolidated Framework for Implementation Research
   (CFIR) was used to guide collection and analysis of qualitative data.
   Two researchers evaluated the qualitative data of the focus groups by
   the Codebook and Rating Rules of CFIR, independently. Results: Of the 31
   CFIR constructs assessed, three constructs strongly distinguished
   between GPs with low versus high success of the intervention
   (intervention complexity, individual state of change, key stakeholders
   engaging), seven additional constructs weakly distinguished
   (adaptability, external policy and incentives, implementation climate,
   compatibility, relative priority, self-efficacy, formally appointed
   internal implementation leaders), 10 had insufficient data to assess and
   11 were non-related to the success of the intervention. Conclusion: We
   identified the constructs that explain the variation in the
   effectiveness of the intervention; this information is relevant to
   redesign successful implementation strategies focused on these
   constructs to implement the BENZORED intervention in health services.
   Background: Despite recommendations against long-term benzodiazepine
   (BZD) use, they are often prescribed during months or years in primary
   care. Research question: To evaluate the effectiveness of a primary care
   educational and feedback intervention targeted to general practitioners
   (GPs) to reduce BZDs prescriptions. Methods: Design: A two-arm parallel
   cluster randomized clinical trial. Settings: Primary Healthcare centres
   from three health districts of Spain: Balearic Islands (IbSalut),
   Catalonia (Institut Catala de la Salut; Tarragona-Reus district) and
   Community of Valencia (Conselleria de Salut Universal; Arnau de Vilanova
   lliria district). Participants: All GPs from the health districts
   included were invited to participate. Ninety percent of the GPs accepted
   to participate. Intervention: GPs received an educational two hours
   workshop training about the rationale for prescribing BZDs and
   deprescribing strategies for long-term BZD users, audit and monthly
   feedback about their prescription and access to a support web page with
   information to help them and leaflets to give to the patients. Control
   group: GPs did not receive any component of the intervention. Outcomes:
   Defined daily dose (DDD)/1000 inhabitants/year (DHD) of BZDs prescribed
   by GP at 12 months. Proportion of long-term BZD users (>6 months) and in
   patients aged 65 or more at 12 months. Statistical analysis: Generalized
   mixed linear random effect models to account for clustering at the level
   of healthcare centre and all analyses were based on an intention to
   treat principle. Results: We included 749 GPs and 49 (6.5\%) were lost
   to follow-up. Adjusted difference between groups in DHD at 12 months was
   -3.26 (-4.87;-1.65), p<.001. The differences in the proportion of
   long-term BZD users was -0.39 (-0.58;-0.19), p<.001 and in patients
   older than 65 was -0.87 (-1.35;-0.26), p=.004. Conclusion: An
   educational and feedback intervention targeted to GPs is effective to
   reduce BZD prescription in primary care.
   Background: Patients who might also go to the general practitioner (GP)
   frequently consult emergency departments (ED). This leads to decreased
   efficiency, high workload at the ED and additional costs for both
   government and patient. Research question: The primary outcome is the
   proportion of patients who enter the ED and are handled by the GP after
   triage. Secondary outcomes: Referral rate to the ED by the GP,
   proportion of patients not following the triage advice, compliance of
   the nurse to the triage-instructions and health insurance expenditures.
   Furthermore, facilitators and barriers will be studied and an incident
   analysis will be performed. Methods: This is a randomised controlled
   trial with weekends serving as clusters. Patients presenting at the ED
   during OOH are triaged and allocated to either ED or GP by a trained
   nurse using an extension to the Manchester Triage System (MTS). During
   control clusters, all patients remain at the ED. Data are collected
   using a database for OOH care (iCAREdata). Results: So far, 296 out of
   2733 (11\%) patients were allocated to the GP. Two-thirds (194) of these
   patients did go to the GP leading to a primary outcome of 7\% for 14
   intervention weekends. Only eight patients were referred back to the ED.
   Compliance of the nurse to the extended MTS was 93\%, in 6\% of the
   cases the nurse chose ED instead of GPC and in less than one percent GPC
   instead of ED. The nurses chose higher urgency categories and more
   discriminators, leading to the GP during intervention clusters. Using an
   automated system, these results are updated weekly, on our poster, we
   will show more results that are complete. Conclusion: These first
   results reveal a low efficiency but a high safety of the intervention.
   More prolonged data collection combined with a process analysis and cost
   efficiency study is necessary before definitive conclusions can be
   drawn.
   Background: Low back pain is a multifactorial condition with individual
   and societal impact. Psychosocial factors play a larger prognostic roll.
   Therefore, earlier multidisciplinary treatment strategy (physical,
   psychological and social/occupational) could be applied to search
   improvement in fear-avoidance beliefs with positive effect in the
   evolution of low back pain. Research question: Evaluate the
   effectiveness of a biopsychosocial multidisciplinary intervention
   (physiotherapy, cognitive-behavioural and pharmacological therapy)
   through the changes in fear-avoidance beliefs (FABs), in working
   population with sub-acute non-specific LBP, compared to usual clinical
   care at 3 and 12 months. Methods: A cluster randomised clinical trial,
   conducted in 39 Primary Health Care Centres (PHCC) in Barcelona.
   Participants between 18 and 65 years old (n = 369; control group =188,
   PHCC 26 and intervention group =181, PHCC 13). Control group received
   usual care, according to guidelines. Intervention group received usual
   care plus a biopsychosocial multidisciplinary intervention (sessions 10
   hours/total). The main outcome was the Fear-Avoidance Beliefs
   questionnaire (FABQ). Other outcomes: Evolution to chronicity.
   Assessment at baseline, 3 and 12 months. Analysis was by intention to
   treat and analyst blinded. Multiple imputations. Results: Of the 369
   enrolled patients with LBP, 421 (84.0\%) provided data at the three
   months of follow-up, and 387 (77.2\%) at 12 months. Mean age of study
   subjects at baseline was 45.1 (SD: 10.4) years-old and 61.2\% were
   women. At baseline, there were no differences. Both groups showed a
   decrease in FABQ (FAB physical and FAB-work) at three months and twelve
   months, with a significant difference at long-term. At FAB-physical
   performance, there was no significant difference over the follow-up time
   and at FAB-Work, a substantial difference at 12 months between groups.
   Conclusion: A multidisciplinary biopsychosocial intervention showed a
   positive effect in FABs by improving fear behaviours and avoidance at
   work.
   Community participation in primary healthcare is enshrined in
   international policies since the 1970s and has been re-emphasised since
   then, most recently in the 2018 WHO Astana Declaration (). The concept
   comes from a social justice perspective. It emphasises that the
   participation of communities who experience poverty and social exclusion
   is essential to the development of primary health care services shaping
   these services and making them relevant to those with the greatest need.
   This is important if we are to address the well-documented Inverse Care
   Law. There is, however, a translational gap between policy and practice.
   The stability of policies for community participation in primary
   healthcare is patchy. The implementation of policies into conventional
   ways of working is patchy. Where implementation has occurred, the
   coverage of community participation initiatives can be patchy - not all
   community members are involved. The literature shows a pattern of
   exclusion whereby so-called `hard to reach' groups are not adequately
   involved in primary healthcare decision-making. This is the case for
   refugees and migrants who arrive to settle and integrate into host
   countries in Europe. The recent WHO Strategy and Action Plan for Refugee
   and Migrant Health (2016; ) is a call for action to disrupt this pattern
   of exclusion and improve the health of refugees and migrants. Drawing on
   the rich tradition of participatory health research is a valuable way
   forward because it provides important concepts, tools and techniques for
   research that is more inclusive and primary care practice. This
   presentation will describe innovative examples of success in family
   practice settings from around Europe. These have brought together
   refugees and migrants with primary care stakeholders and enabled them to
   work together to introduce and sustain changes in clinical practice.
   This evidence can be used to guide and strengthen community
   participation in primary healthcare, for all.
   Background: Community participation is essential for effective
   implementation of research programmes in primary healthcare (PHC) but
   also appropriate interpretation of results and optimal delivery of
   subsequent care. Stakeholder engagement undertaken under defined and
   evaluated frameworks may be key for the establishment of concrete
   collaboration and communication between communities and other parties
   involved in research. This abstract aims to report on community and
   stakeholder engagement methodologies, plans and activities of European
   research projects conducted in Crete, Greece. Research question: Could a
   consensus be reached regarding the methods and tools for enhancing
   stakeholder engagement in community-oriented PHC research? Methods:
   Examined programmes included RESTORE (FP7), FRESH AIR (Horizon2020) and
   VIGOUR (Health Programme). Identified methodologies included
   Normalisation Process Theory, Participatory Learning and Action, Five
   Steps of Stakeholders' Engagement, establishment of Stakeholder
   Engagement Groups under the 9 C's model (commissioners, customers,
   collaborators, contributors, channels, commentators, consumers,
   champions, competitors) and Structured Democratic Dialogue. These were
   implemented to a range of stakeholders, including community members,
   patients, migrants, Roma populations, healthcare professionals and
   policy-makers. Qualitative research (focus groups, individual
   interviews) and Thematic Content Analysis were used for design and
   analysis of engagement activities. Results: In RESTORE, migrants and
   other stakeholders selected guidelines and training supporting
   cross-cultural communication in PHC consultations, based on their own
   needs and expectations. Community members, healthcare professionals and
   healthcare authorities were actively involved in FRESH AIR by
   identifying local priorities and contextual factors for designing
   project interventions, providing access to communities and supporting
   dissemination of project achievements. In VIGOUR, multidisciplinary
   stakeholders were brought together and formulated a joint ambition
   statement for the future of integrated care in Crete. Conclusion:
   Various stakeholder engagement methods with documented effects are
   currently available. Their systematic identification, appraisal,
   synthesis and consolidation may serve with enhancing community
   participation in PHC, sustaining research results and translating
   findings into appropriate actions.
   Background: Screening for prostate cancer remains controversial,
   implying a trade-off between benefits and harms, and a shared
   decision-making process has been advocated. Decision aids are
   evidence-based tools that improve decision quality. For limited-resource
   countries, translating and making cultural adaptations to high-quality
   decision aids is a reasonable alternative to developing new ones.
   Research question: We aimed to translate and culturally adapt an English
   language patient decision aid addressing prostate cancer screening, so
   that Portuguese men can use it. Methods: We followed the European Centre
   for Disease Prevention and Control's (ECDC) five-step, stakeholder-based
   approach to adapting health communication materials: (1) selection of
   materials and process coordinators; (2) early review; (3) translation
   and back translation; (4) comprehension testing with cognitive
   semi-structured interviews; (5) proofreading. Cognitive interviews were
   conducted with 15 men, ages 55-69, from the Oporto district local
   community to refine the decision aid after its translation. Content
   analysis was performed using Ligre (TM) software. Results: Five main
   themes are presented: informational content, information comprehension,
   socio-cultural appropriateness, feelings and primary message, and
   personal perspective concerning prostate cancer screening. For each
   theme, illustrative quotes extracted from men's interviews are
   presented. Most men found the translated version of the decision aid to
   be clear, comprehensive and appropriate for its target population,
   albeit some suggested that medical terms could be a barrier. The data
   collected from men's interviews allowed the researchers to clarify
   concepts and expand existing content. Conclusion: The final version of
   the decision aid can be used in the real world clinical setting and our
   ECDC based approach can be replicated by other workgroups to translate
   and culturally adapt decision aids.
   What are we talking about when we talk about value? In 2006, Michael
   Porter and Elizabeth O. Teisberg published; Redefining Health Care,
   Creating Value-Based Competition on Results, Harvard Business School
   Press. Affirming that payers and providers, including doctors and
   nurses, are very concerned in demonstrating that they work a lot, and
   very little, or nothing, in assessing what their work contributes to the
   health of people and communities. Michael Porter is famous in the
   business world for his work on competitiveness based on the value of
   products and services. He has introduced this concept in the provision
   of health services, all summarised in a phrase: Health systems should
   seek to obtain the maximum possible value for the health of people for
   every dollar they spend. However, to define the value in healthcare, the
   patient must be introduced into the equation, so, in Porterian terms,
   the value is the perception that people have about clinical
   effectiveness and the costs of therapeutic processes. Clinical
   effectiveness is measurable from epidemiology (to be readmitted to a
   hospital fewer times or living longer); value, on the other hand, is
   reflected by people's experience. We need to ask questions such as do
   patients with advanced diseases want to live longer, or they want to
   enjoy the highest quality of life possible. Depending on the response,
   we can develop different delivery models. What is value-based
   healthcare? According to NEJM Catalyst, Value-based healthcare is a
   healthcare delivery model in which providers, including hospitals and
   physicians, are paid based on patient health outcomes (). Under
   value-based care agreements, providers are rewarded for helping patients
   improve their health, reduce the effects and incidence of chronic
   disease, and live healthier lives in an evidence-based way. How to
   achieve a Value-Based Healthcare Model The following six drivers are the
   key to make a primary health care system a value-based healthcare model:
   1. Prioritising patient-centred care. 2. From clinical pathways to care
   delivery value chains. 3. Promoting the right care and reducing medical
   overuse. 4. Turning a fragmented model into another integrated model. 5.
   Creating the enabling environment for healthcare transformation. 6.
   Fostering community health. How to develop a Value-Based Community
   Health Michael Marmot states that if the determinants of health are
   mostly social, solutions must also be social, so to improve the quality
   of community life, political systems require economic, housing,
   education, security and infrastructure programs (Am J Public Health.
   2014;104:S517-S519). Nevertheless, the healthcare system must know how
   to adjust resources according to the social circumstances of each
   community and to understand how to provide a health-oriented vision of
   all the social programs. On the other hand, community health is an
   intervention model that aims to improve the health of a defined
   community that should operate from primary care services to adjust their
   actions to the social reality of each territory.
   Background: Geriatric care needs to be increased with growing elderly
   populations. The Borgholm jurisdiction in the Baltic Island of oland
   (Kalmar region) has an older than average senior population and had
   difficulties recruiting primary care physicians (PCPs) resulting in high
   elderly hospital care consumption. Research question: Could a new model
   of geriatric care be able to decrease the hospital care needs of
   Borgholm as compared to the rest of Kalmar region? Methods: A new model
   of care was developed in Borgholm 2016-2017 where the PCP list was
   limited to 1000 patients, daily slots for PCP home care visits could be
   booked by community nurses or ambulance nurses and PCPs had daily
   anticipatory care planning contacts with Kalmar hospital staff. Results:
   Between 2014 and 2018, Borgholm home care patients >75 years old
   increased by 70\% vs. a 2\% decrease for the rest of Kalmar region.
   Similarly, Kalmar emergency department visits decreased by 19\% in
   Borgholm vs. 9\% increase for the rest of Kalmar. Also, Kalmar hospital
   care episodes decreased 7\% in Borgholm vs. 13\% increase for the rest
   of Kalmar; Kalmar hospital outpatient visits decreased 8\% in Borgholm
   vs. 21\% increase for the rest of Kalmar; total care consumption for >75
   years old decreased 4\% in Borgholm vs. a 10\% increase for the rest of
   Kalmar region. Conclusion: A new geriatric care model consisting of a
   comprehensive collaboration between strengthened primary care and
   community care, hospital care and ambulance care was associated with a
   reduction in total care consumption for senior citizens in a rural
   Swedish jurisdiction.
   Background: Primary health care, the general practitioner, plays a
   critical role for early identification and care of patients with
   dementia. Early diagnosis of dementia allows starting therapy and
   improving the quality of life of the patients. Research question: To
   estimate the prevalence and care of patients with dementia in North
   Macedonia. Methods: Forty-six general practitioners (GPs) surgeries from
   20 cities in Macedonia took part in the project. All individuals age
   over 65 years with a diagnosis of dementia were identified from GP
   electronic disease registers. Results: Based on the diagnosis, 450
   (3.5\%) patients were identified from a total population of 12,926 over
   65s. The most common dementia was Alzheimer's dementia 294 (65.3\%)
   followed by vascular dementia 27.11\%. The average age of respondents in
   the study was 77.5 +/- 8.2 years, with 50\% patients under the age of 79
   years, 65.6\% were female and 68.4\% were with elementary school. In the
   entire sample, most of the patients diagnosed with dementia 195 (43.3\%)
   said they lived with another family member. The most common risk factor
   was hypertension (85.1\%), followed by stroke/ transitory ischemic
   attacks (29.3\%) and equal percentage, i.e. 26.4\% of patients had high
   levels of cholesterol and diabetes. To 242 (53.8\%) acetylcholinesterase
   inhibitors were prescribed (donepezil, rivastigmine, galantamine), 77
   (17.1\%) memantine, while 247 (54.9\%) another OTC therapy. 227 (50.4\%)
   reported that they did not receive treatment. An additional analysis of
   the reasons for not receiving treatment was made on this sample of
   patients who did not receive treatment. It was found that in the
   majority of these patients (more than 50\%) the reason for not receiving
   therapy was that it was not prescribed, in 142 (62.6\%). Conclusion:
   This is the first national representative study of dementia prevalence
   in North Macedonia. Those data can provide information for healthcare
   needs people with dementia.
   Background: Emotional experience for medical students during clinical
   internships is often ignored. Yet, its influence on professional skills
   is certain. Research question: `What is the emotional experience of
   second and third-year medical students during their first clinical
   internship? How do they perceive the management of their experience by
   their supervisors?' Methods: A qualitative study was conducted with 12
   students in their second or third year of medical training at the
   University of Lille, in France, between 2016 and 2019. Interviews were
   carried out comprehensively for a total of 17 hours. Following a
   grounded theory approach, the analysis terminated when data were
   sufficient to offer a conclusive model. Results: Emotional experience
   during clinical internship was rich and intense. It was most often
   ignored and was not taken into account in the development of
   professional skills. The organized management was deficient. Informal
   training existed: when a wilful student met a dedicated teacher.
   Students would have welcomed a possibility to experience intense
   emotions in a protective environment, and only then in an empowering
   environment. They expressed the same desire about early exchanges on the
   experiences of the internship. A modelling of the informants' emotional
   experiences was realized in the form of three diagrams. Conclusion:
   Students ask to be challenged to face patients, and then to be listened
   to about it. Possible interventions are trauma prevention and detection
   of malaise in the workplace; teaching of humanist values; providing
   experience and reflexivity through new pedagogical means (such as
   cinema, theatre, literature, writing), or relational means (such as
   exchange groups, companionship, solidarity commitment, immersive
   internships and tutoring); and training supervisors.
   Background: Oral anticoagulants (OAC) reduce the risk for stroke and
   death from all causes in patients with non-valvular atrial fibrillation
   (NVAF). Research question: To explore adherence rates to OAC among
   patients with NVAF and to compare head-to-head adherence rate of
   different medications in long-term chronic use. Methods: We conducted a
   population-based cohort study Clalit Health Services, Israel. All
   patients, 30 years and over, with a diagnosis of NVAF before 2016 and
   were treated with OAC were included. We included patients that filled at
   least one prescription per year in the three consecutive years
   2016-2018. We analysed all prescriptions that were filled for the
   medications from 1 January 2017 to 31 December 2017. We considered
   purchasing of at least nine monthly prescriptions during 2017 as `good
   medication adherence.' Results: Twenty-six thousand and twenty-nine
   patients with NVAF who were treated with OAC were identified. Ten
   thousand and two hundred and eighty-four (39.5\%) were treated with
   apixaban, 6321 (24.3\%) were treated with warfarin, 6290 (24.1\%) were
   treated with rivaroxaban 3134 (12.0\%) were treated with dabigatran.
   Rates of good medication adherence were 88.9\% for rivaroxaban, 84.9\%
   for apixaban, 83.6\% for dabigatran and 55.8\% for warfarin (p<.0001).
   Good adherence with OAC was associated with lower LDL cholesterol and
   glucose levels. Advanced age was associated with higher adherence rates
   (p<.001). SES was not associated with medication adherence. Conclusion:
   Adherence rates to DOAC among patients with NVAF are high and are higher
   than the adherence rate to warfarin. It should be taken into
   consideration when choosing OAC treatment for NVAF.
   Background: In France, cervical cancer screening by pap-smears should be
   conducted triennially. Screening statistics are based on the number of
   cytology examinations of smears reimbursed by the Health Insurance
   appearing in the claim databases. The percentage of screened women is
   lower based on these data than on declarative surveys. If surveys are
   overestimating the number of screened women, it is likely that claim
   databases underestimate it. Research question: The primary objective was
   to determine the underestimation of screened women in claim databases.
   The secondary purpose was to estimate the proportion of female patients
   not reachable by their GP for a cervical cancer screening in an
   organized screening trial. Methods: The population was the 6327 female
   patients aged 30-65 years of the 24 GP investigators of the PaCUDAHL-Ge
   trial. We compared the lists of their female patients that had no
   cytology of Pap test reimbursed during the three prior years, extracted
   from the Health Insurance claim databases in 2015 and 2018. We selected
   the patients appearing on both lists meaning they had not responded to
   the invitation of their GP to be screened in the trial. We searched in
   the GPs' records valid reasons not to be screened (hysterectomy, history
   of cervical lesion, pregnancy, other conditions making screening
   irrelevant) or evidence of screening. Results: The total number of
   `unscreened' women in 2018 was 2731, 1737 patients appeared on both
   lists, 1522 could be included for analysing, 65 had been screened, 95
   had hysterectomy, three had a history of cervical lesion, nine were
   pregnant and 10 had other conditions making screening irrelevant, 166
   patients were lost to view. Conclusion: Based on GPs' records, health
   insurance claim databases underestimate the number of screened women by
   7.6\%. The percentage of patients not responding to the invitation of
   their GP to be screened in the PaCUDAHL-Ge trial is 24.18\%.
   Background: Over the past decade, the amount of digital data created by
   humans with or without connected tools has grown exponentially. The
   field of primary care (PC) did not escape this digitization, nor the use
   of Big Data algorithms. To evaluate the results of Big Data research in
   PC it seemed useful to identify which algorithms are used. Research
   question: What are the algorithms used for Big Data research in PC
   research and how are they described? Methods: Systematic review of the
   literature according to the recommendations of the PRISMA guide. A
   search equation using the following MeSH terms `big data, data mining,
   Algorithms, Artificial Intelligence, Machine learning, Deep Learning,
   Neural Networks Natural Language Processing, general practice,
   electronic health records, health records' has been applied to the
   PUBMED database. After a selection of the titles and article summaries
   according to the inclusion criteria, the full versions of the eligible
   articles were read and analysed. Referenced articles of the sources
   articles were added to the analysis. The algorithms described in the
   articles were extracted and analysed. Results: In total, 778 articles
   were identified, 169 were eligible for full reading and 26 articles were
   finally selected. The algorithms listed in the articles are poorly
   described. The description is usually limited to a general explanation
   about how the algorithm works. Seven articles gave a partial description
   of the algorithm; a logic diagram was given in four articles and the
   codes in only two. Actually, only one article fully describes the
   algorithm with its mathematical description, its code and its logic
   diagram. Conclusion: Big Data algorithms in PC are not satisfactorily
   described. The lack of reproducibility is not compatible with a
   consistent scientific approach. Researchers should provide more
   information about the way they extract and analyse their data to give
   their readers more confidence in Big Data.
   Background: As a collaborative project of the Family Practice Depression
   and Multimorbidity group of European General Practice Research Network,
   the Hopkins Symptom Checklist-25 (HSCL-25) scale was identified as
   valid, reproducible, effective and easy to use. Subsequently, it has
   been translated and adapted to 13 languages, including Castilian.
   Currently, the scale is being validated in different languages. Research
   question: What are the psychometric properties of the Spanish version of
   HSCL-25 (HSCL-25e) for depression detection in Primary Care? Methods:
   HSCL-25e was administered to outpatients recruited by their physicians
   in six health centres involved in Spanish EIRA3 study, a trial to
   promote healthy behaviours in people aged from 45 to 75. Patients
   complimented HSCL-25 themselves. Sample size was calculated with R
   package (pROC). Statistical analysis: responsiveness was analysed with
   missing data and detecting ceiling and floor effects for the items.
   Principal component analysis (PCA) was done to determine the dimensions
   of HSCL-25e. Item-total correlation, Cronbach's alpha (global and
   dimensions coefficient) and squared multiple correlation were carried
   out to calculate internal consistency. Results: Seven hundred and
   sixty-nine patients out of 806 complimented HSCL-25e, 738 answered to
   all of the items. No patterns of missing answers were found. No ceiling
   effects, expected floor effect in item 18. Item 17 was the most
   consistent one and item 24 was the lower one. All items showed positive
   discrimination index for both cut-off points (1.55 and 1.75). PCA
   indicated two factors; 13 items corresponding to depression dimension
   and the other 12 items corresponding to anxiety subscale. Global
   Cronbach's alpha was 0.92 (0.88 calculated for depression dimension and
   0.84 for anxiety dimension). Conclusion: The HSCL-25e has excellent
   psychometric properties when applied to Primary Care population. It has
   two dimensions as the original version, although the items included are
   not exactly the same. There are more item coincidences with the French
   version.
   Background: Cardiovascular diseases (CVDs) are the first mortality cause
   worldwide with 17.5 million death in 2012. Spices (Scaling-up Packages
   of Interventions for CVD prevention in selected sites in Europe and
   Sub-Saharan Africa) gathered five countries around CVD primary
   prevention interventions, especially for populations with low access to
   prevention and health care system. In France, a rural area where people
   were more deprived and with a low settlement of general practitioners
   (GPs) fitted with the project. Research question: What are the barriers
   and the facilitators for cardiovascular primary prevention
   implementation from caregivers and patients' point of view of a deprived
   rural area? Methods: Semi-structured interviews were conducted until
   theoretical saturation of data. Purposive samplings of GPs, patients,
   patients' families, nurses and pharmacists were designed. Five interview
   guides explored cardiovascular prevention, cardiovascular health
   promotion in the setting, actors of CVD prevention, capacities for CVD
   prevention, patients' and healthcare professionals' representations,
   barriers and facilitators in implementing CVD prevention, possible
   solutions. Guides were adapted concurrently to the analysis. A blinded
   thematic analysis and a mind-mapping were achieved for each group.
   Results: Thirteen GPS, 11 pharmacists', 14 nurses, 12 patients' and 12
   patients' family members' interviews were achieved. Professionals
   highlighted a disconnection between them and national prevention
   programs, lack of time, payment and training for CVD prevention.
   Countryside was either protective or aggressive regarding CVD risk
   balancing gardening and space against isolation and lack of structures.
   GPs had poor connections with the community. Patients described their
   recklessness and feeling of invulnerability until their CVD appeared.
   Families could be a barrier to CVD prevention and lifestyle change.
   Risky behaviours were handed down from one generation to another.
   Conclusion: Innovative interventions for Spices should focus on these
   community specificities and individual behavioural strategies in
   contrast with the six national plans addressing CVD in France. These
   plans solely concentrate on dissemination of prevention messages and
   knowledge, which is of little use according to this survey.},
DOI = {10.1080/13814788.2020.1719994},
ISSN = {1381-4788},
EISSN = {1751-1402},
ResearcherID-Numbers = {Fazli, Ghazal/AAE-8320-2022
   Campos, Caroline/AAT-5847-2021
   Freienberg, Selina/AAV-8829-2021},
Unique-ID = {WOS:000517857100001},
}

@article{ WOS:000513493800004,
Author = {Yalcin, Sercan and Erdem, Ebubekir},
Title = {A mobile fault detection algorithm in heterogeneous wireless sensor
   networks: a bio-inspired approach},
Journal = {SADHANA-ACADEMY PROCEEDINGS IN ENGINEERING SCIENCES},
Year = {2019},
Volume = {45},
Number = {1},
Month = {DEC 20},
Abstract = {This paper puts forth a novel mobile fault detection algorithm for
   wireless sensor networks (WSNs) based on bacterial-inspired
   optimization. We introduce a bio-swarm intelligence approach to mobile
   fault detection in WSNs by using voltage values. At certain times, the
   sensor nodes in the clustered network send data packets containing
   health-fitness information to cluster heads (CHs) selected by the
   proposed CH selection algorithm. A mobile sink (MS) collects the health
   status via data from all the nodes as they reach the intersection point
   of the CHs. After this stage, the data packets are analyzed by the MS,
   and hardware or software faults are detected by assessing the fitness
   values of the nodes. The faulty nodes are eventually discarded from the
   network, and recovery of the rest of the nodes in the network is
   satisfied. Inspired by the interaction of bacteria for feed collection,
   their response to chemicals, and their interaction and communication
   with one another, we bring an innovative approach to finding node
   failures or software faults in WSNs, and these failures are removed from
   the network to help its operation and to take measures to maintain the
   electrical structures. In fact, we adapt our algorithm to low energy
   harvesting electrical components as an example. We compare our novel
   algorithm with existing studies through extensive simulations in NS 2
   environment based on fault detection accuracy, false alarm rate, and
   false positive rate criteria versus fault probability, number of nodes,
   and sink speed. Considering detection accuracy, the simulation results
   validate that our algorithm shows better performance as compared with
   others.},
DOI = {10.1007/s12046-019-1241-7},
Article-Number = {4},
ISSN = {0256-2499},
EISSN = {0973-7677},
ResearcherID-Numbers = {YALÇIN, Sercan/V-9134-2018
   YALÇIN, Sercan/AAM-2618-2020
   },
ORCID-Numbers = {YALÇIN, Sercan/0000-0003-1420-2490
   YALÇIN, Sercan/0000-0003-1420-2490
   ERDEM, Ebubekir/0000-0001-7093-7016},
Unique-ID = {WOS:000513493800004},
}

@article{ WOS:000501452000001,
Author = {King, C. and Shafi, A. and Burke, E.},
Title = {Optimising the management of concurrent symphyseal/parasymphyseal and
   bilateral extracapsular condylar fractures using three-dimensional
   printing},
Journal = {ORAL AND MAXILLOFACIAL SURGERY-HEIDELBERG},
Year = {2020},
Volume = {24},
Number = {2},
Pages = {217-219},
Month = {JUN},
Abstract = {Purpose Three-dimensional (3D) printing plays an important role in the
   diagnosis and treatment planning of many elective procedures in oral and
   maxillofacial surgery (OMFS). 3D printers and the associated print
   materials are now within the price range of most maxillofacial units,
   requiring less work to be sent out to commercial printers. Whilst their
   use in the planning of elective procedures is commonplace, acute trauma
   is an area where 3D printing remains underutilised. The successful
   management of complex fracture patterns such as concomitant
   symphyseal/parasymphyseal and bilateral condylar fractures often
   warrants this approach. Methods Freeware digital processing and
   manipulation software packages were used to view and segment structures
   from computed tomography (CT) data. Thereafter, fractures were digitally
   reduced. 3D printed models were produced from the digitally reduced
   models, allowing preoperative custom adaptation of osteosynthesis
   plates, facilitating accurate fracture fixation intraoperatively.
   Results For less than one hundred pounds sterling (STG), a 3D printer
   (with print material) capable of producing a model of sufficient quality
   can be purchased. The use of freeware digital processing software allows
   digital manipulation of CT data. Production of 3D models and plate
   adaptation can be carried out within hours after CT examination.
   Conclusions The construction of digitally reduced 3D models and custom
   adapted plates enables the surgeon to achieve accurate fixation of
   complex fracture patterns in theatre which is clearly of benefit to
   patients. The potential for reduced theatre time also renders this
   approach more desirable, making this a worthwhile investment despite the
   additional non-clinical time associated with training and initial
   expenditure.},
DOI = {10.1007/s10006-019-00820-y},
EarlyAccessDate = {DEC 2019},
ISSN = {1865-1550},
EISSN = {1865-1569},
Unique-ID = {WOS:000501452000001},
}

@article{ WOS:000504767500001,
Author = {Busienei, P. J. and Ogendi, G. M. and Mokua, M. A.},
Title = {Latrine Structure, Design, and Conditions, and the Practice of Open
   Defecation in Lodwar Town, Turkana County, Kenya: A Quantitative Methods
   Research},
Journal = {ENVIRONMENTAL HEALTH INSIGHTS},
Year = {2019},
Volume = {13},
Month = {DEC},
Abstract = {Background: Poor latrine conditions, structure, and design may deter
   latrine use and provoke reversion to open defecation (OD). Statistics
   show that only 18\% of the households in Turkana County, Kenya, have
   access to a latrine facility with most of these facilities in poor
   structural designs and poor hygienic conditions, which encourages
   rampant OD practices. Aim: This article reports on quantitative aspects
   of a larger cross-sectional survey to assess latrine structure, design,
   and conditions, and the practice of OD in Lodwar. Methods: An
   observational study was carried out to examine latrine conditions,
   structure, and design in Lodwar, Kenya. A standardized questionnaire was
   also used to collect quantitative data. Stratified random sampling
   technique was employed to select respondents for this study with the
   sample drawn from 4 administrative units of Lodwar town covering the
   low-, medium-, and high-income households. Data were managed using
   Statistical Packages for Social Science (SPSS) software. Results:
   Nineteen percent of the sampled households did not possess a latrine
   facility at their homesteads with 73\% of the latrines constructed using
   poor materials (mud, mats, polythene bags, and grass). Twenty percent of
   the respondents were scared of using a latrine with the main reason
   being loose soils that do not support strong constructions. Eighty-seven
   percent of the respondents agreed that the presence of feces on the
   latrine floor encouraged the practice of OD and 321 (80\%) respondents
   stated that the latrine construction materials influenced latrine
   ownership and its subsequent use. Conclusions: Respondents attributed
   rampant OD practices to poor latrine structure, design, and conditions.
   In addition, rampant cases of latrine sharing result in latrine
   filthiness, which eventually encourages OD practice. Inequality in
   sanitation, among counties, should be addressed in Kenya. The government
   should take charge of provision of good-quality communal latrines to the
   less-privileged societies like Turkana. Community empowerment and
   introduction of a small fee for cleaning and maintenance of these
   facilities will also improve their conditions. Ending the practice of OD
   will lead to increased positive public health and environmental outcomes
   in the study area},
DOI = {10.1177/1178630219887960},
Article-Number = {1178630219887960},
ISSN = {1178-6302},
ORCID-Numbers = {Jepkorir, Phylis/0000-0002-7719-7994},
Unique-ID = {WOS:000504767500001},
}

@article{ WOS:000500381900005,
Author = {Rasmussen, Mads Holten and Lefrancois, Maxime and Pauwels, Pieter and
   Hviid, Christian Anker and Karlshoj, Jan},
Title = {Managing interrelated project information in AEC Knowledge Graphs},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2019},
Volume = {108},
Month = {DEC},
Abstract = {In the architecture, engineering and construction (AEC) industry
   stakeholders from different companies and backgrounds collaborate in
   realising a common goal being some physical structure. The exact goal is
   typically not known from the beginning, and throughout all design
   stages, new decisions are made - similarly to other design industries
   {[}1]. As a result, the design must adapt and subsequent consequences
   follow. With working methods being predominantly document-centric,
   highly interrelated and rapidly changing design data in a complex
   network of decisions, requirements and product specifications is
   primarily captured in static documents. In this paper, we consider a
   purely data-driven approach based on semantic web technologies and an
   earlier proposed Ontology for Property Management (OPM). The main
   contribution of this work consists of extensions for OPM to account for
   new competency questions including the description of property
   reliability and the reasoning logic behind derived properties. The
   secondary contribution is the specification of a homogeneous way to
   generate parametric queries for managing an OPM-compliant AEC Knowledge
   Graph (AEC-KG). A software library for operating an OPM-compliant AEC-KG
   is further presented in the form of an OPM Query Generator (OPM-QG). The
   library generates SPARQL 1.1 queries to query and manipulate
   construction project Knowledge Graphs represented using OPM. The OPM
   ontology aligns with latest developments in the W3C Community Group on
   Linked Building Data and suggests an approach to working with design
   data in a distributed environment using separate graphs for explicit
   facts and for materialised, deduced data. Finally, we evaluate the
   suggested approach using an open-source software artefact developed
   using OPM and OPM-QG, demonstrated online with an actual building
   Knowledge Graph. The particular design task evaluated is performing heat
   loss calculations for spaces of a future building using an AEC-KG
   described using domain- and project specific extensions of the Building
   Topology Ontology (SOT) in combination with OPM. With this work, we
   demonstrate how a typical engineering task can be accomplished and
   managed in an evolving design environment, thereby providing the
   engineers with insights to support decision making as changes occur. The
   application uses a strict division between the client viewer and the
   actual data model holding design logic, and can easily be extended to
   support other design tasks.},
DOI = {10.1016/j.autcon.2019.102956},
Article-Number = {102956},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Karlshoj, Jan/H-9532-2017
   Pauwels, Pieter/I-8256-2015},
ORCID-Numbers = {Hviid, Christian Anker/0000-0002-8340-7222
   Karlshoj, Jan/0000-0001-5735-3032
   Pauwels, Pieter/0000-0001-8020-4609},
Unique-ID = {WOS:000500381900005},
}

@article{ WOS:000506712400043,
Author = {El-hoshoudy, A. N. and Soliman, F. S. and Mansour, E. M. and Zaki, T.
   and Desouky, S. M.},
Title = {Experimental and theoretical investigation of quaternary ammonium-based
   deep eutectic solvent for secondary water flooding},
Journal = {JOURNAL OF MOLECULAR LIQUIDS},
Year = {2019},
Volume = {294},
Month = {NOV 15},
Abstract = {Deep eutectic solvents (DES) have recently emerged as green ionic liquid
   candidates for several applications, including electrochemistry,
   nanomaterials, and petrochemical processing. However, there are few
   reports on DES as enhanced oil recovery (EOR) agents. In this research,
   four quaternary ammonium-based DES with urea, thiourea, ethylene glycol
   and glycerol were synthesized and characterized. The H- bond amount was
   monitored through infrared spectroscopy, and, simulated infrared spectra
   conducted through Gaussian\_09W-9.5-Revision-D.01-SMP calculation to
   confirm the experimental spectra. The rheology and interfacial tension
   reducingability of the four DES were investigated in relation to the
   reservoir environment. The data indicate that the four solvents can
   enhance oil recovery in the underground reservoir, as further verified
   through modeling analysis and the viscosity-shearing profiles. Since
   theoretical computation provides a fruitful tool to simulate molecular
   geometry and physicochemical criteria of compounds. The chemical
   structure and geometry optimization conducted by COSMO-RS and TURBOMOLE
   package to predict screening charge density (sigma profile) and
   localized charges of the synthesized eutectic mixture, then
   functionalize the obtained data in predicting DES behavior through oil
   recovery applications. Moreover, molecular dynamic (MD) simulation
   conducted through DS-BIOVIA Materials Studio 2017 to investigate the
   affinity and interaction energies between the oil and the four DES
   solvents. Water flooding was carried out on a linear assembly, followed
   by DES displacement at a nearby coarse reservoir environment, where the
   results were prosperous as incremental oil recovery reach 77.4\%. On a
   field scale, a simulation test was done on a three- dimensional
   reservoir model by using CMG software to evaluate the applicability of
   DES solvents in oil displacement techniques. To the best of our
   knowledge, there are no previous reports on the quantum and
   computational chemistry of DES for application in the field of enhanced
   oil recovery. (C) 2019 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.molliq.2019.111621},
Article-Number = {111621},
ISSN = {0167-7322},
EISSN = {1873-3166},
ResearcherID-Numbers = {Soliman, Fathi S./E-2360-2017
   Mansour, Ass. Prof. Eman/AAI-3425-2021
   El-hoshoudy, Abdelaziz/R-1280-2017},
ORCID-Numbers = {Soliman, Fathi S./0000-0002-0002-3275
   El-hoshoudy, Abdelaziz/0000-0002-5475-4647},
Unique-ID = {WOS:000506712400043},
}

@article{ WOS:000501745700194,
Author = {Lin, Weiliang},
Title = {Analysis on the effect of assignment system of track and field web
   course based on software programming method},
Journal = {CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS},
Year = {2019},
Volume = {22},
Number = {6},
Pages = {15023-15036},
Month = {NOV},
Abstract = {The paper was to improve the assignment module's construction and
   application effect of the track and field (T\&F) web course in physical
   education schools. Literature study, expert interview, system design
   method, software programming method, questionnaire survey. First, the
   editing and feedback of the assignment module are strongly practical and
   the effects of which are very obvious; Second, the content and form of
   the assignments are various, which initiate the extending thinking of
   the students; Third, the structure of assignment module helps with the
   assigning, submitting, prompting and correcting and the comprehensive
   statistics of the assignments and it is easy to operate with its
   multi-function and it also reflects the teaching progress subjectively.
   This research is applicable to the web-based assignment pattern of track
   and field teaching for physical education majors (selective course,
   required course, minor course), the regular maintenance and
   complimentary administration of assignment library resources shall be
   further optimized; the editing and update of assignments shall be
   adjusted and arranged timely according to the teaching practice
   problems.},
DOI = {10.1007/s10586-018-2494-3},
ISSN = {1386-7857},
EISSN = {1573-7543},
ResearcherID-Numbers = {Lin, Wei-Liang/GRX-4137-2022},
Unique-ID = {WOS:000501745700194},
}

@article{ WOS:000503381500074,
Author = {Silva, Mauricio R. and Souza, Elitelma S. and Alsina, Pablo J. and
   Leite, Deyvid L. and Morais, Mateus R. and Pereira, Diego S. and
   Nascimento, Luis B. P. and Medeiros, Adelardo A. D. and Cunha Junior,
   Francisco H. and Nogueira, Marcelo B. and Albuquerque, Glauberto L. A.
   and Dantas, Joao B. D.},
Title = {Performance Evaluation of Multi-UAV Network Applied to Scanning Rocket
   Impact Area},
Journal = {SENSORS},
Year = {2019},
Volume = {19},
Number = {22},
Month = {NOV},
Abstract = {This paper presents a communication network for a squadron of unmanned
   aerial vehicles (UAVs) to be used in the scanning rocket impact area for
   Barreira do Inferno Launch Center-CLBI (Rio Grande do Norte, Brazil),
   aiming at detecting intruder boats. The main features of communication
   networks associated with multi-UAV systems are presented. This system
   sends information through Wireless Sensor Networks (WSN). After
   comparing and analyzing area scanning strategies, it presents the
   specification of a data communication network architecture for a
   squadron of UAVs within a sensor network using XBee Pro 900HP S3B
   modules. A brief description is made about the initial information from
   the construction of the system. The embedded hardware and the design
   procedure of a dedicated communication antenna to the XBee modules are
   presented. In order to evaluate the performance of the proposed
   architecture in terms of robustness and reliability, a set of
   experimental tests in different communication scenarios is carried out.
   Network management software is employed to measure the throughput,
   packet loss and other performance indicators in the communication links
   between the different network nodes. Experimental results allow
   verifying the quality and performance of the network nodes, as well as
   the reliability of the communication links, assessing signal received
   quality, range and latency.},
DOI = {10.3390/s19224895},
Article-Number = {4895},
EISSN = {1424-8220},
ResearcherID-Numbers = {Alsina, Pablo Javier/AAH-2555-2020
   Medeiros, Adelardo/W-9496-2019
   },
ORCID-Numbers = {Medeiros, Adelardo/0000-0001-8520-1888
   Alsina, Pablo Javier/0000-0002-2882-5237
   Nascimento, Luis/0000-0002-1699-4535
   R Silva, Mauricio/0000-0002-7453-4051},
Unique-ID = {WOS:000503381500074},
}

@article{ WOS:000494009700001,
Author = {Ponton, Hazel and Osborne, Allan and Thompson, Neill and Greenwood,
   David},
Title = {The power of humour to unite and divide: a case study of design
   coordination meetings in construction},
Journal = {CONSTRUCTION MANAGEMENT AND ECONOMICS},
Year = {2020},
Volume = {38},
Number = {1},
Pages = {32-54},
Month = {JAN 2},
Abstract = {Design coordination meetings are the formal discussion venues that
   support interdisciplinary group interaction during the construction
   process. Social behaviour needs to be recognized, understood, and
   evaluated by group members if meetings are to be productive. The role of
   humour during the practice of coordinating building design has not
   previously been studied. A non-participant observation method was used
   to collect qualitative data from consecutive contractor-led design team
   meetings during a live building project. Using a 360 degrees panoramic
   video-recording camera, episodes of humour were captured and collated
   into packets of rich data. These packets were then organized,
   structured, and analysed using NVivo computer-assisted qualitative data
   analysis software. The results of the analysis showed that instances of
   humour do not happen at random but at specific times when they performed
   distinct functions to facilitate the design coordination process. One
   notable example was the role of humour in helping to form a cohesive
   team that was able to manage conflict successfully and thus engender a
   positive cultural environment. The inclusive findings of the study have
   demonstrated that humour is a functional aspect of group dynamics during
   the coordination of design in construction that can influence social
   interaction and task-related performance.},
DOI = {10.1080/01446193.2019.1656339},
EarlyAccessDate = {OCT 2019},
ISSN = {0144-6193},
EISSN = {1466-433X},
ORCID-Numbers = {Thompson, Neill/0000-0002-5492-8926
   greenwood, david/0000-0003-1615-2036
   Ponton, Hazel/0000-0003-0840-5187},
Unique-ID = {WOS:000494009700001},
}

@article{ WOS:000492979600001,
Author = {Dyomin, Alexander and Galkina, Svetlana and Fillon, Valerie and Cauet,
   Stephane and Lopez-Roques, Celine and Rodde, Nathalie and Klopp,
   Christophe and Vignal, Alain and Sokolovskaya, Anastasia and
   Saifitdinova, Alsu and Gaginskaya, Elena},
Title = {Structure of the intergenic spacers in chicken ribosomal DNA},
Journal = {GENETICS SELECTION EVOLUTION},
Year = {2019},
Volume = {51},
Number = {1},
Month = {OCT 26},
Abstract = {Background Ribosomal DNA (rDNA) repeats are situated in the nucleolus
   organizer regions (NOR) of chromosomes and transcribed into rRNA for
   ribosome biogenesis. Thus, they are an essential component of eukaryotic
   genomes. rDNA repeat units consist of rRNA gene clusters that are
   transcribed into single pre-rRNA molecules, each separated by intergenic
   spacers (IGS) that contain regulatory elements for rRNA gene cluster
   transcription. Because of their high repeat content, rDNA sequences are
   usually absent from genome assemblies. In this work, we used the
   long-read sequencing technology to describe the chicken IGS and fill the
   knowledge gap on rDNA sequences of one of the key domesticated animals.
   Methods We used the long-read PacBio RSII technique to sequence the BAC
   clone WAG137G04 (Wageningen BAC library) known to contain chicken NOR
   elements and the HGAP workflow software suit to assemble the PacBio RSII
   reads. Whole-genome sequence contigs homologous to the chicken rDNA
   repetitive unit were identified based on the Gallus\_gallus-5.0 assembly
   with BLAST. We used the Geneious 9.0.5 and Mega software, maximum
   likelihood method and Chickspress project for sequence evolution
   analysis, phylogenetic tree construction and analysis of the raw
   transcriptome data. Results Three complete IGS sequences in the White
   Leghorn chicken genome and one IGS sequence in the red junglefowl contig
   AADN04001305.1 (Gallus\_gallus-5.0) were detected. They had various
   lengths and contained three groups of tandem repeats (some of them being
   very GC rich) that form highly organized arrays. Initiation and
   termination sites of rDNA transcription were located within small and
   large unique regions (SUR and LUR), respectively. No functionally
   significant sites were detected within the tandem repeat sequences.
   Conclusions Due to the highly organized GC-rich repeats, the structure
   of the chicken IGS differs from that of IGS in human, apes, Xenopus or
   fish rDNA. However, the chicken IGS shares some molecular organization
   features with that of the turtles, which are other representatives of
   the Sauropsida clade that includes birds and reptiles. Our current
   results on the structure of chicken IGS together with the previously
   reported ribosomal gene cluster sequence provide sufficient data to
   consider that the complete chicken rDNA sequence is assembled with
   confidence in terms of molecular DNA organization.},
DOI = {10.1186/s12711-019-0501-7},
Article-Number = {59},
ISSN = {0999-193X},
EISSN = {1297-9686},
ResearcherID-Numbers = {Vignal, Alain/AAJ-5697-2021
   Vignal, Alain/I-6253-2017
   Demin, Aleksandr G./N-6526-2015
   Fillon, Valerie/GWR-2196-2022
   Klopp, Christophe/J-3164-2019
   Galkina, Svetlana/AAK-7179-2021
   Saifitdinova, Alsu F/C-1104-2011
   Galkina, Svetlana/J-2493-2013},
ORCID-Numbers = {Vignal, Alain/0000-0002-6797-2125
   Vignal, Alain/0000-0002-6797-2125
   Klopp, Christophe/0000-0001-7126-5477
   Saifitdinova, Alsu F/0000-0002-1221-479X
   LOPEZ-ROQUES, Celine/0000-0003-2529-7958
   Galkina, Svetlana/0000-0002-7034-2466},
Unique-ID = {WOS:000492979600001},
}

@article{ WOS:000486307100032,
Author = {Beitzke, Dietrich and Wielandner, Alice and Wollenweber, Tim and Vraka,
   Chrysoula and Pichler, Verena and Uyanik-Uenal, Keziban and Zuckermann,
   Andreas and Greiser, Andreas and Hacker, Marcus and Loewe, Christian},
Title = {Assessment of sympathetic reinnervation after cardiac transplantation
   using hybrid cardiac PET/MRI: A pilot study},
Journal = {JOURNAL OF MAGNETIC RESONANCE IMAGING},
Year = {2019},
Volume = {50},
Number = {4},
Pages = {1326-1335},
Month = {OCT},
Abstract = {Background Sympathetic reinnervation after heart transplantation (HTX)
   is a known phenomenon, which has an impact on patient heart rate
   variability and exercise capacity. The impact of reinnervation on
   myocardial structure has not been evaluated yet. Propose To evaluate the
   feasibility of simultaneous imaging of cardiac reinnervation and cardiac
   structure using a hybrid PET/MRI system. Study type Prospective / pilot
   study. Subjects Ten patients, 4-21 years after cardiac transplantation.
   Field Strength/Sequence 3 T hybrid PET/MRI system. Cine SSFP, T-1
   mapping (modified Look-Locker inversion recovery sequence)
   pre/postcontrast as well as dynamic {[}C-11]meta-hydroxyephedrine
   ({[}C-11]mHED) PET. Assessment All MRI and PET parameters were evaluated
   by experienced readers using dedicated postprocessing software packages
   for cardiac MRI and PET. For all parameters a 16-segment model for the
   left ventricle was applied. Statistical Tests Mann-Whitney U-test;
   Spearman correlations. Results Thirty-six of 160 myocardial segments
   showed evidence of reinnervation by PET. On a segment-based analysis,
   mean native T-1 relaxation times were nonsignificantly altered in
   segments with evidence of reinnervation (1305 +/- 151 msec vs. 1270 +/-
   112 msec; P = 0.1), whereas mean extracellular volume (ECV) was
   significantly higher in segments with evidence of reinnervation (35.8
   +/- 11\% vs. 30.9 +/- 7\%; P = 0.019). There were no significant
   differences in wall motion (WM) and wall thickening (WT) between
   segments with or without reinnervation (mean WM: 7.6 +/- 4 mm vs. group
   B: 9.3 +/- 7 mm {[}P = 0.13]; WT: 79 +/- 63\% vs. 94 +/- 74\% {[}P =
   0.27]) under resting conditions. Data Conclusion The assessment of
   cardiac reinnervation using a hybrid PET/MRI system is feasible.
   Segments with evidence of reinnervation by PET showed nonsignificantly
   higher T-1 relaxation times and a significantly higher ECV, suggesting a
   higher percentage of diffuse fibrosis in these segments, without
   impairment of rest WM and WT. Technical Efficacy: Stage 3 J. Magn.
   Reson. Imaging 2019;50:1326-1335.},
DOI = {10.1002/jmri.26722},
ISSN = {1053-1807},
EISSN = {1522-2586},
ResearcherID-Numbers = {Loewe, Christian/GSI-8660-2022
   Hacker, Marcus/GRJ-2825-2022
   Pichler, Verena/AAB-4494-2020
   },
ORCID-Numbers = {Pichler, Verena/0000-0003-4544-2438
   Beitzke, Dietrich/0000-0003-3179-3827
   Loewe, Christian/0000-0003-2502-0676
   Zuckermann, Andreas/0000-0002-8054-8150
   Vraka, Chrysoula/0000-0003-2065-6093},
Unique-ID = {WOS:000486307100032},
}

@article{ WOS:000497657100001,
Author = {Liebschner, Dorothee and Afonine, Pavel V. and Baker, Matthew L. and
   Bunkoczi, Gabor and Chen, Vincent B. and Croll, Tristan I. and Hintze,
   Bradley and Hung, Li-Wei and Jain, Swati and McCoy, Airlie J. and
   Moriarty, Nigel W. and Oeffner, Robert D. and Poon, Billy K. and
   Prisant, Michael G. and Read, Randy J. and Richardson, Jane S. and
   Richardson, David C. and Sammito, Massimo D. and Sobolev, Oleg V. and
   Stockwell, Duncan H. and Terwilliger, Thomas C. and Urzhumtsev,
   Alexandre G. and Videau, Lizbeth L. and Williams, Christopher J. and
   Adams, Paul D.},
Title = {Macromolecular structure determination using X-rays, neutrons and
   electrons: recent developments in Phenix},
Journal = {ACTA CRYSTALLOGRAPHICA SECTION D-STRUCTURAL BIOLOGY},
Year = {2019},
Volume = {75},
Number = {10},
Pages = {861-877},
Month = {OCT 1},
Abstract = {Diffraction (X-ray, neutron and electron) and electron cryo-microscopy
   are powerful methods to determine three-dimensional macromolecular
   structures, which are required to understand biological processes and to
   develop new therapeutics against diseases. The overall
   structure-solution workflow is similar for these techniques, but nuances
   exist because the properties of the reduced experimental data are
   different. Software tools for structure determination should therefore
   be tailored for each method. Phenix is a comprehensive software package
   for macromolecular structure determination that handles data from any of
   these techniques. Tasks performed with Phenix include data-quality
   assessment, map improvement, model building, the
   validation/rebuilding/refinement cycle and deposition. Each tool caters
   to the type of experimental data. The design of Phenix emphasizes the
   automation of procedures, where possible, to minimize repetitive and
   time-consuming manual tasks, while default parameters are chosen to
   encourage best practice. A graphical user interface provides access to
   many command-line features of Phenix and streamlines the transition
   between programs, project tracking and re-running of previous tasks.},
DOI = {10.1107/S2059798319011471},
ISSN = {2059-7983},
ResearcherID-Numbers = {Sammito, Massimo Domenico/P-3501-2019
   Poon, Billy/AAT-5234-2020
   Poon, Billy/AAH-7065-2020
   Liebschner, Dorothee/AAH-2189-2019
   Read, Randy J./L-1418-2013
   Adams, Paul David/A-1977-2013
   Terwilliger, Thomas/AEF-4827-2022
   Terwilliger, Thomas/AAM-4079-2020
   Moriarty, Nigel/AAH-2190-2019
   },
ORCID-Numbers = {Sammito, Massimo Domenico/0000-0002-8346-9247
   Poon, Billy/0000-0001-9633-6067
   Read, Randy J./0000-0001-8273-0047
   Adams, Paul David/0000-0001-9333-8219
   Terwilliger, Thomas/0000-0001-6384-0320
   Ourjoumtsev, Alexandre/0000-0002-1002-3041
   Oeffner, Robert/0000-0003-3107-2202
   Liebschner, Dorothee/0000-0003-3921-3209
   Moriarty, Nigel/0000-0001-8857-9464
   Croll, Tristan/0000-0002-3514-8377},
Unique-ID = {WOS:000497657100001},
}

@article{ WOS:000498052200016,
Author = {Maher, Gabriel and Wilson, Nathan and Marsden, Alison},
Title = {Accelerating cardiovascular model building with convolutional neural
   networks},
Journal = {MEDICAL \& BIOLOGICAL ENGINEERING \& COMPUTING},
Year = {2019},
Volume = {57},
Number = {10},
Pages = {2319-2335},
Month = {OCT},
Abstract = {The objective of this work is to reduce the user effort required for 2D
   segmentation when building patient-specific cardiovascular models using
   the SimVascular cardiovascular modeling software package. The proposed
   method uses a fully convolutional neural network (FCNN) to generate 2D
   cardiovascular segmentations. Given vessel pathlines, the neural network
   generates 2D vessel enhancement images along the pathlines. Thereafter,
   vessel segmentations are extracted using the marching-squares algorithm,
   which are then used to construct 3D cardiovascular models. The neural
   network is trained using a novel loss function, tailored for partially
   labeled segmentation data. An automated quality control method is also
   developed, allowing promising segmentations to be selected. Compared
   with a threshold and level set algorithm, the FCNN method improved 2D
   segmentation accuracy across several metrics. The proposed quality
   control approach further improved the average DICE score by 25.8\%. In
   tests with users of SimVascular, when using quality control, users
   accepted 80\% of segmentations produced by the best performing FCNN. The
   FCNN cardiovascular model building method reduces the amount of manual
   segmentation effort required for patient-specific model construction, by
   as much as 73\%. This leads to reduced turnaround time for
   cardiovascular simulations. While the method was used for cardiovascular
   model building, it is applicable to general tubular structures.},
DOI = {10.1007/s11517-019-02029-3},
ISSN = {0140-0118},
EISSN = {1741-0444},
ResearcherID-Numbers = {Marsden, A/C-4037-2011},
ORCID-Numbers = {Marsden, A/0000-0003-1902-171X},
Unique-ID = {WOS:000498052200016},
}

@article{ WOS:000488228600012,
Author = {Kozaric, Ljiljana and Purcar, Martina Vojnic and Zivkovic, Smilja},
Title = {Vibrations of Cross-Laminated Timber Floors},
Journal = {DRVNA INDUSTRIJA},
Year = {2019},
Volume = {70},
Number = {3},
Pages = {307-312},
Month = {SEP},
Abstract = {This paper investigates the vibrations caused by human action
   offive-layer cross-laminated timber panels with a height of 14 cm.
   Analysis is made of threefloar panels of identical height, but with
   different combinations of thicknesses of the laminas in cross-layers,
   varying their spans. The longitudinal layers of the panels have better
   physical and mechanical characteristics than transverse layers. The
   relevant criteria to be observed in floor construction at the designing
   stage are specified in order to ensure acceptable behavior regarding the
   dynamic load. Limit values are also given. As it is very difficult to
   determine the threshold of human acceptability, since vibrations that
   someone finds disturbing do not have to be disturbing for others, the
   relevant criteria used in the work has been chosen on the basis of the
   highest representation in the literature: Natural Frequency Limit, Unit
   Load Deflection Limit and the Combined Criterion. Calculation of the
   effective bending stiffness of the panel was performed using Gamma
   method, K-method and Kreuzinger analogy. The natural frequency of each
   panel was determined analytically, and so was the maximum deflection due
   to unit static force and the obtained results were compared with the
   values obtained by modal and static analysis in the Ansys software
   package. All the obtained data were compared with the valid criteria,
   and panels that meet the criteria in terms of the serviceability limit
   slate were selected. The results showed that the analyzed floor panels
   of the span up to 4.5 m are acceptable in terms of adequate dynamic
   behavior related to human action according to all criteria. Also, the
   results showed that the Combined Criterion and the Unit Load Deflection
   Limit are significantly stricter due to the unit static force compared
   to the Natural Frequency Limit. If the minimum required natural
   frequency of the CLT panel were accepted with a value of 8 Hz, which
   corresponds to the milder recommendations in the available literature,
   the spans could go up to 6 m.
   On the basis of the obtained results, it can be concluded that in the
   design offloor structures, in addition to static stability, an adequate
   dynamic response to the initiative caused by everyday human activities
   must be provided. The results also show that the analyzed floor CLT
   panels can be very successfully applied in the floor constructions of
   residential and commercial buildings, provided that the required dynamic
   calculations are made.},
DOI = {10.5552/drvind.2019.1822},
ISSN = {0012-6772},
EISSN = {1847-1153},
ResearcherID-Numbers = {Kozaric, Ljiljana/A-1617-2019
   Živković, Smilja/AAC-6943-2020},
ORCID-Numbers = {Kozaric, Ljiljana/0000-0002-1350-3286
   },
Unique-ID = {WOS:000488228600012},
}

@article{ WOS:000486877700125,
Author = {Lin, Pao-Hung and Chang, Chin-Chuan and Lin, Yu-Hui and Lin, Wei-Liang},
Title = {Green BIM Assessment Applying for Energy Consumption and Comfort in the
   Traditional Public Market: A Case Study},
Journal = {SUSTAINABILITY},
Year = {2019},
Volume = {11},
Number = {17},
Month = {SEP 1},
Abstract = {This study focused on the energy consumption and environmental comfort
   of the traditional Xindian Central Public Retail Market. Established for
   more than 30 years, the market has been a crucial role in the daily life
   of local residents. Thus, the energy consumption and comfort level of
   the market are subjects of great concern. By using green building
   information modeling (BIM) simulation, which is an innovative assessment
   process that combines green buildings and BIM for the architecture,
   engineering, and construction (AEC) industries to achieve
   sustainability, this study explored the current situation of energy
   efficiency and comfort level of the market. A green BIM model of the
   market and surrounding area was constructed in Autodesk Revit.
   Subsequently, nine items pertaining to energy consumption and
   environmental comfort were selected from the green BIM model to conduct
   simulation by using the software package Integrated Environmental
   Solutions Virtual Environment (IES VE). Based on the IES VE simulation
   results, heat radiation was identified as one of the main causes of
   energy consumption in the market. Moreover, the results indicated
   problems of ventilation and insufficient sunlight inside the market.
   These analytical outcomes and optimization suggestions can be provided
   as references for retrofitting to obtain sustainable architectures in
   future.},
DOI = {10.3390/su11174636},
Article-Number = {4636},
EISSN = {2071-1050},
ResearcherID-Numbers = {Lin, Wei-Liang/GRX-4137-2022},
Unique-ID = {WOS:000486877700125},
}

@article{ WOS:000480341500001,
Author = {Mao, Bomin and Tang, FengXiao and Fadlullah, Zubair Md. and Kato, Nei},
Title = {An Absorbing Markov Chain Based Model to Solve Computation and
   Communication Tradeoff in GPU-Accelerated MDRUs for Safety Confirmation
   in Disaster Scenarios},
Journal = {IEEE TRANSACTIONS ON COMPUTERS},
Year = {2019},
Volume = {68},
Number = {9},
Pages = {1256-1268},
Month = {SEP},
Abstract = {The fast increasing chip processing capacities driven by the Moore's Law
   have encouraged the academia and industry to consider more about general
   hardware architectures since they allow the repeated use for multiple
   purposes through the installations of applications. Some techniques
   utilizing the general hardware architectures have been developed to
   improve the flexibility of computer networks, such as the Software
   Defined Networking (SDN) and the Network Functions Virtualization (NFV).
   For these networks, the applications are required to be
   computation/communication-efficient since the installed applications
   share the hardware. In this paper, we study the resource-limited
   disaster recovery networks constructed by the Movable and Deployable
   Resource Units (MDRUs) which consist of various general computation
   platforms. We propose an efficient safety confirmation method through
   the photo sharing by the survivors. In the proposal, the Absorbing
   Markov Chain is utilized to model the safety confirmation process,
   transition matrix of which can be adopted to choose the suitable photo
   size for optimizing the traffic overhead and buffer consumption. Through
   periodical update of the photo database, unnecessary packet
   transmissions can be further avoided with reasonable sacrifice of the
   computation overhead. To expedite the computation, the GPU-accelerated
   MDRU is considered to conduct the matrix calculations in a parallel
   fashion.},
DOI = {10.1109/TC.2019.2906881},
ISSN = {0018-9340},
EISSN = {1557-9956},
ResearcherID-Numbers = {Tang, Fengxiao/ABD-9673-2021
   tang, fengxiao/T-5881-2019
   Tang, Fengxiao/AFN-7960-2022
   Mao, Bomin/AFI-2258-2022
   Fadlullah, Zubair/T-8659-2019
   Mao, Bomin/AAG-8851-2019
   KATO, NEI/T-5892-2019},
ORCID-Numbers = {tang, fengxiao/0000-0003-2414-4802
   Mao, Bomin/0000-0001-7780-5972
   KATO, NEI/0000-0001-8769-302X},
Unique-ID = {WOS:000480341500001},
}

@article{ WOS:000488826400029,
Author = {Zhang, Xuequan and Zhong, Ming and Liu, Shaobo and Zheng, Luoheng and
   Chen, Yumin},
Title = {Template-Based 3D Road Modeling for Generating Large-Scale Virtual Road
   Network Environment},
Journal = {ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION},
Year = {2019},
Volume = {8},
Number = {9},
Month = {SEP},
Abstract = {The 3D road network scene helps to simulate the distribution of road
   infrastructure and the corresponding traffic conditions. However, the
   existing road modeling methods have limitations such as inflexibility in
   different types of road construction, inferior quality in visual effects
   and poor efficiency for large-scale model rendering. To tackle these
   challenges, a template-based 3D road modeling method is proposed in this
   paper. In this method, the road GIS data is first pre-processed before
   modeling. The road centerlines are analyzed to extract topology
   information and resampled to improve path accuracy and match the
   terrain. Meanwhile, the road network is segmented and organized using a
   hierarchical block data structure. Road elements, including roadbeds,
   road facilities and moving vehicles are then designed based on
   templates. These templates define the geometric and semantic information
   of elements along both the cross-section and road centerline. Finally,
   the road network scene is built by the construction algorithms, where
   roads, at-grade intersections, grade separated areas and moving vehicles
   are modeled and simulated separately. The proposed method is tested by
   generating large-scale virtual road network scenes in the World Wind, an
   open source software package. The experimental results demonstrate that
   the method is flexible and can be used to develop different types of
   road models and efficiently simulate large-scale road network
   environments.},
DOI = {10.3390/ijgi8090364},
Article-Number = {364},
EISSN = {2220-9964},
ORCID-Numbers = {Zhang, Xuequan/0000-0002-8755-2443},
Unique-ID = {WOS:000488826400029},
}

@article{ WOS:000471700400006,
Author = {Ma, Liyang and Bocchini, Paolo},
Title = {Hysteretic Model of Single-Bolted Angle Connections for Lattice Steel
   Towers},
Journal = {JOURNAL OF ENGINEERING MECHANICS},
Year = {2019},
Volume = {145},
Number = {8},
Month = {AUG 1},
Abstract = {Single-bolted angle joints are widely used on secondary bracings of
   lattice steel towers due to their low cost and ease of construction. It
   has been observed that the hysteretic behavior of these joints has a
   significant impact on the dynamic performance of the tower structure and
   the nonstructural components supported by the tower. However, the
   various phases of this hysteretic cycle have never been investigated in
   detail. Therefore, there are no accurate numerical models for these
   types of joints in the popular software packages for structural
   analysis. This in turn hinders advanced studies on the performance of
   these critical infrastructure components (utility towers) under strong
   wind or seismic loads. This paper first explains the mechanics and the
   various stages of the hysteretic behavior of the joint, including
   friction, slippage, bolt bearing, and plasticity. Finite-element models
   are built and validated for the analytical modeling of the joint under
   monotonic loading. The model for hysteretic behavior is then presented,
   considering cyclic joint slippage and bolt-hole elongation (damage
   accumulation). A novel algorithm is developed to efficiently incorporate
   the analytical model into a computer program. In particular, the
   analytical model is the basis for a new zero-length finite element that
   can be used in the OpenSees framework. The methodology is applied to
   single-bolted angle joints with 10 typical configurations. The proposed
   analytical zero-length element is shown to agree well with brick element
   simulation results under both monotonic and cyclic loading. Therefore,
   the hysteretic behavior of the single-bolted angle connections can be
   incorporated into the dynamic analysis of lattice steel towers by
   inputting easily obtained physical properties, for example, plate
   thickness and width, bolt and bolt hole diameter, and material strength.
   This proposed element will enable engineers and researchers to
   efficiently study the cyclic performance of lattice tower structures
   capturing well the joint behavior.},
DOI = {10.1061/(ASCE)EM.1943-7889.0001630},
Article-Number = {04019052},
ISSN = {0733-9399},
EISSN = {1943-7889},
ResearcherID-Numbers = {Bocchini, Paolo/A-9907-2011},
ORCID-Numbers = {ma, liyang/0000-0002-6456-1793
   Bocchini, Paolo/0000-0002-5685-2283},
Unique-ID = {WOS:000471700400006},
}

@article{ WOS:000476648500005,
Author = {Shah Mohammadi, Mohammad Reza and Craveiro, Helder D. and Rebelo, Carlos},
Title = {Numerical study of monopile offshore foundation dynamic behaviour using
   coupled simulation},
Journal = {PROCEEDINGS OF THE INSTITUTION OF CIVIL ENGINEERS-STRUCTURES AND
   BUILDINGS},
Year = {2019},
Volume = {172},
Number = {8},
Pages = {580-589},
Month = {AUG},
Abstract = {Offshore wind turbines (OWTs) are highly dynamically loaded by sea waves
   and wind forces and they operate in a narrow range of rotation
   frequencies close to the eigenfrequencies of the support structure. The
   foundation stiffness and damping are therefore very important parameters
   for a reliable estimation of the dynamic behaviour and cost-effective
   design of a wind turbine's supporting structure. However, design
   guidelines for offshore structures usually ignore any soil damping. This
   paper describes the implementation of a simple geotechnical model in the
   software package Ashes, which is a servo-hydro-aeroelastic code for wind
   turbine simulations. In order to investigate the role of the foundation
   damping, a 20 m monopile supporting structure with a National Renewable
   Energy Laboratory 5 MW OWT was considered as a case study. The OWT was
   subjected to standard design load cases and the results were obtained at
   the mudline of the wind turbine. The parametric study undertaken showed
   that, when soil damping is considered, ultimate limit state loads can be
   reduced by 8\% and the number of fatigue cycles can be reduced by 1.5\%.},
DOI = {10.1680/jstbu.17.00191},
ISSN = {0965-0911},
ResearcherID-Numbers = {rebelo, carlos/I-4740-2012
   Craveiro, Hélder/S-9685-2018},
ORCID-Numbers = {rebelo, carlos/0000-0003-2543-0114
   Craveiro, Hélder/0000-0001-8590-5885},
Unique-ID = {WOS:000476648500005},
}

@article{ WOS:000477703600080,
Author = {Simoncini, David and Zhang, Kam Y. J. and Schiex, Thomas and Barbe,
   Sophie},
Title = {A structural homology approach for computational protein design with
   flexible backbone},
Journal = {BIOINFORMATICS},
Year = {2019},
Volume = {35},
Number = {14},
Pages = {2418-2426},
Month = {JUL 15},
Abstract = {Motivation Structure-based Computational Protein design (CPD) plays a
   critical role in advancing the field of protein engineering. Using an
   all-atom energy function, CPD tries to identify amino acid sequences
   that fold into a target structure and ultimately perform a desired
   function. Energy functions remain however imperfect and injecting
   relevant information from known structures in the design process should
   lead to improved designs.
   Results We introduce Shades, a data-driven CPD method that exploits
   local structural environments in known protein structures together with
   energy to guide sequence design, while sampling side-chain and backbone
   conformations to accommodate mutations. Shades (Structural Homology
   Algorithm for protein DESign), is based on customized libraries of
   non-contiguous in-contact amino acid residue motifs. We have tested
   Shades on a public benchmark of 40 proteins selected from different
   protein families. When excluding homologous proteins, Shades achieved a
   protein sequence recovery of 30\% and a protein sequence similarity of
   46\% on average, compared with the PFAM protein family of the target
   protein. When homologous structures were added, the wild-type sequence
   recovery rate achieved 93\%.
   Availability and implementation Shades source code is available at
   https://bitbucket.org/satsumaimo/shades as a patch for Rosetta 3.8 with
   a curated protein structure database and ITEM library creation software.
   Supplementary information
   Supplementary data are available at Bioinformatics online.},
DOI = {10.1093/bioinformatics/bty975},
ISSN = {1367-4803},
EISSN = {1460-2059},
ResearcherID-Numbers = {Simoncini, David/AAA-5204-2021
   Barbe, Sophie/ABD-9530-2020
   Zhang, Kam Y. J./B-3552-2012
   },
ORCID-Numbers = {Simoncini, David/0000-0002-3772-5473
   Zhang, Kam Y. J./0000-0002-9282-8045
   Barbe, Sophie/0000-0003-2581-5022},
Unique-ID = {WOS:000477703600080},
}

@article{ WOS:000475538100003,
Author = {Farronato, Marco and Maspero, Cinzia and Lanteri, Valentina and Fama,
   Andrea and Ferrati, Francesco and Pettenuzzo, Alessandro and Farronato,
   Davide},
Title = {Current state of the art in the use of augmented reality in dentistry: a
   systematic review of the literature},
Journal = {BMC ORAL HEALTH},
Year = {2019},
Volume = {19},
Month = {JUL 8},
Abstract = {BackgroundThe aim of the present systematic review was to screen the
   literature and to describe current applications of augmented
   reality.Materials and methodsThe protocol design was structured
   according to PRISMA-P guidelines and registered in PROSPERO. A review of
   the following databases was carried out: Medline, Ovid, Embase, Cochrane
   Library, Google Scholar and the Gray literature. Data was extracted,
   summarized and collected for qualitative analysis and evaluated for
   individual risk of bias (R.O.B.) assessment, by two independent
   examiners. Collected data included: year of publishing, journal with
   reviewing system and impact factor, study design, sample size, target of
   the study, hardware(s) and software(s) used or custom developed, primary
   outcomes, field of interest and quantification of the displacement error
   and timing measurements, when available. Qualitative evidence synthesis
   refers to SPIDER.ResultsFrom a primary research of 17,652 articles, 33
   were considered in the review for qualitative synthesis. 16 among
   selected articles were eligible for quantitative synthesis of
   heterogenous data, 12 out of 13 judged the precision at least as
   acceptable, while 3 out of 6 described an increase in operation timing
   of about 1h. 60\% (n=20) of selected studies refers to a camera-display
   augmented reality system while 21\% (n=7) refers to a head-mounted
   system. The software proposed in the articles were self-developed by 7
   authors while the majority proposed commercially available ones. The
   applications proposed for augmented reality are: Oral and maxillo-facial
   surgery (OMS) in 21 studies, restorative dentistry in 5 studies,
   educational purposes in 4 studies and orthodontics in 1 study. The
   majority of the studies were carried on phantoms (51\%) and those on
   patients were 11 (33\%).ConclusionsOn the base of literature the current
   development is still insufficient for full validation process, however
   independent sources of customized software for augmented reality seems
   promising to help routinely procedures, complicate or specific
   interventions, education and learning. Oral and maxillofacial area is
   predominant, the results in precision are promising, while timing is
   still very controversial since some authors describe longer preparation
   time when using augmented reality up to 60min while others describe a
   reduced operating time of 50/100\%.Trial registrationThe following
   systematic review was registered in PROSPERO with RN: CRD42019120058.},
DOI = {10.1186/s12903-019-0808-3},
Article-Number = {135},
ISSN = {1472-6831},
ResearcherID-Numbers = {Fama, Andrea/AAM-3446-2021
   Farronato, Marco/AAH-2806-2019
   Farronato, Davide/AAK-7934-2020
   Maspero, Cinzia Maria Norma/AAH-9542-2019
   Lanteri, Valentina/AAD-8947-2019},
ORCID-Numbers = {Fama, Andrea/0000-0002-6746-3726
   Farronato, Marco/0000-0002-6209-9873
   Farronato, Davide/0000-0003-0328-470X
   Maspero, Cinzia Maria Norma/0000-0002-5930-1698
   Lanteri, Valentina/0000-0003-2191-8673},
Unique-ID = {WOS:000475538100003},
}

@article{ WOS:000481779500010,
Author = {{[}Anonymous]},
Title = {Research on multimorbidity in primary care. Selected abstracts from the
   EGPRN meeting in Tampere, Finland, 9-12 May 2019 All abstracts of the
   conference can be found at the EGPRN website:
   www.egprn.org/page/conference-abstracts},
Journal = {EUROPEAN JOURNAL OF GENERAL PRACTICE},
Year = {2019},
Volume = {25},
Number = {3},
Pages = {164-175},
Month = {JUL 3},
Abstract = {Current primary care in Finland is based on the Primary Health Care Act
   (1972), which addressed numerous new tasks to all municipalities. All of
   them had to find a new health centre organization, which provides a wide
   range of health services, including prevention and public health
   promotion. Multiple tasks require multiprofessional staff, and thus, the
   Finnish health centre personnel consisted not only of GPs but of public
   health nurses, midwives, physiotherapists, psychologists, social
   workers, dentists, etc. During the next decade, there have been some
   changes but the idea of multiprofessional structure has remained.
   According to the QUALICOPC study (2012) Finnish GPs are still co-located
   with several other healthcare professionals compared to most of the
   European countries; even compared to other Nordic countries which
   otherwise have many similarities in their primary healthcare. During the
   last 10 or 15 years, healthcare providers and researchers have
   recognized a new challenge: our current systems do not meet the needs of
   patients with multiple health and social problems-and the proportion of
   these patients is increasing all the time as the population is getting
   older. One could suppose that preconditions of handling multimorbidity
   would be excellent in multiprofessional surroundings like ours, but
   actually, a person with multiple problems is a challenge there, too.
   Multiprofessional organization in primary care does not guarantee proper
   care of patients with multiple diseases, if we do not acknowledge the
   challenge and revise our systems. We have to develop new ways of
   collaboration and new models of integrated care. The problematic part is
   secondary care, which is organized with logic of one medical speciality
   per visit. In Tampere University Hospital district, we have created a
   care pathway model, which defines the roles of primary healthcare and
   secondary care. Nationwide, we have recently started to prepare national
   guidelines for the care of patients with multimorbidity. What we need
   more in the future is more research on new practices and models.
   Background: Most patients with antihypertensive medication do not
   achieve their blood pressure (BP) target. Several barriers to successful
   hypertension treatment are well identified but we need novel ways of
   addressing them. Research question: Can using a checklist improve the
   quality of care in the initiation of new antihypertensive medication?
   Methods: This non-blinded, cluster-randomized, controlled study was
   conducted in eight primary care study centres in central Finland,
   randomized to function as either intervention (n = 4) or control sites
   (n = 4). We included patients aged 30-75 years who were prescribed
   antihypertensive medication for the first time. Initiation of medication
   in the intervention group was carried out with a nine-item checklist,
   filled in together by the treating physician and the patient. The
   treating physician managed hypertension treatment in the control group
   without a study-specific protocol. Results: In total, 119 patients were
   included in the study, of which 118 were included in the analysis (n =
   59 in the control group, n = 59 in the intervention group). When
   initiating medication, an adequate BP target was set for 19\% of the
   patients in the control group and for 68\% in the intervention group.
   Shortly after the appointment, only 14\% of the patients in the control
   group were able to remember the adequate BP target, compared with 32\%
   in the intervention group. The use of the checklist was also related to
   more regular agreement on the next follow-up appointment (64\% in the
   control group vs 95\% in the intervention group). Conclusion: Even
   highly motivated new hypertensive patients in Finnish primary care have
   significant gaps in their treatment-related skills. The use of a
   checklist for initiation of antihypertensive medication was related to
   substantial improvement in these skills. Based on our findings, the use
   of a checklist might be a practical tool for clinicians initiating new
   antihypertensive medications.
   Background: Immediate feedback is underused in the French medical
   education curriculum, specifically with video-recorded consultation.
   Research question: The objective of this study was to evaluate the
   feasibility and the interest in this teaching method as a training and
   assessment tool in the learning process of general practitioner (GP)
   trainees. Methods: During the period November 2017 to October 2018,
   trainees in ambulatory training courses collected quantitative data
   about recording consultations with a video camera: numbers of
   recordings, feedback, patients' participation refusals, and information
   about the learning process and competencies. The trainees' level of
   satisfaction was measured by means of a questionnaire at the end of
   their traineeship. Results: Sixty-seven trainees were recruited and 44
   of them 65.7\% actively participated in the study; 607 video recordings
   and 243 feedback with trainers were performed. Few patients (18.5\%)
   refused the video-recording. Most trainees considered video recording
   with immediate feedback to be a relevant learning tool. It made it
   possible for the participants to observe their difficulties and their
   achievements. `Relation, communication, patient-centred care' was the
   most built competency, non-verbal communication, in particular. Time was
   the main limiting factor of this teaching method. Most trainees were in
   favour of its generalization in their university course. Conclusion:
   Video recording with immediate feedback in real-time consultation needs
   to be adapted to training areas and depends on time and logistics. This
   teaching method seems to be useful in the development of communication
   skills. It could lift the barriers of the trainer's physical presence
   near GP trainees during immediate feedback in real-time consultation. It
   could help trainees to build their competencies while enhancing the
   place of immediate feedback in the general practice curriculum. It could
   also constitute an additional tool for the certification of GP trainees.
   Background: Perinatal depression has been associated with psychiatric
   morbidity in mothers and their offspring. This study assessed the
   prevalence of perinatal depressive symptoms in a large population of
   women and investigated associations of these symptoms with demographic
   and clinical factors. Research question: Which factors (including
   sociodemographic, medical, lifestyle, and laboratory test) are
   associated with perinatal depression? Methods: All members of Maccabi
   Health Services who completed the Edinburgh Postnatal Depression Scale
   (EPDS) during 2015-2016 were included in the study. Odds ratios (ORs)
   were calculated for associations of sociodemographic, medical,
   lifestyle, and laboratory test factors with perinatal depressive
   symptoms, according to a score >10 on the EPDS. Results: Of 27 912 women
   who filled the EPDS, 2029 (7.3\%) were classified as having peripartum
   depression. In a logistic regression analysis, the use of antidepressant
   medications, particularly for a period greater than three months, Arab
   background, current or past smoking, a diagnosis of chronic diabetes and
   age under 25 years were all associated with increased ORs for perinatal
   depression; while Orthodox Jewish affiliation, residence in the
   periphery and higher haemoglobin level were associated with lower ORs.
   Incidences of depression were 17.4\% in women with a history of
   antidepressant medication, 16\% among women with diabetes, and 11.8\%
   among current smokers. Conclusion: Several demographic, medical, and
   lifetime factors were found to be substantially more prevalent among
   women with symptoms of perinatal depression than those without.
   Encouraging women to complete the EPDS during and following pregnancy
   may help identify women in need of support.
   Background: Regulating the quality and effectiveness of the work of
   general practitioners is essential for a sound healthcare system. In the
   Republic of Macedonia this is regulated by the Health Insurance Fund
   through a system of penalties/sanctions. Research question: The goal of
   this study is to evaluate the types and effectiveness of the sanctions
   used on primary care practitioners. Methods: This is a quantitative
   research study for which we used an anonymous survey with 18 questions.
   This survey was distributed to 443 randomly selected general
   practitioners from different parts of Macedonia and 438 of them
   responded. For the quantitative data, we used the Pearson's chi-squared
   test, correlation and descriptive statistics. Part of the survey is
   qualitative, consisting of comments and opinions of the general
   practitioners. Results: From the participants, 336 were female and 102
   were male. The doctors' gender was not associated with sanctioning. Most
   general practitioners were in the age categories of 30-39 and 40-49
   years. The participants' age had a significant influence on
   sanctioning-older doctors were sanctioned more frequently. Out of 438
   participants, 33.3\% were specialists in family medicine and 66.7\%
   general practitioners. Specialists in family medicine were sanctioned
   significantly more frequently than general practitioners. Doctors that
   worked in the hospital or 19 km from the nearest hospital were
   significantly more frequently sanctioned. The three most common reasons
   for sanctions were financial consumption of prescriptions and referrals
   above the agreed amount, higher rate of sick leaves and/or justification
   of sick leaves and unrealized preventative goals or education.
   `Financial sanction by scale' was the most common type of sanction:
   49.8\% of participants. Doctors who followed the guidelines, but who
   were exposed to violence were sanctioned significantly more frequently.
   Conclusion: We can observe that age, speciality, the distance of the
   workplace from the nearest hospital and violence influence sanctioning.
   Background: Biases are major barriers to external validity of studies,
   reducing evidence. Among these biases, the definition and the reality of
   the Hawthorne effect (HE) (or observation bias) remains controversial.
   According to McCambridge in a review from 2013, the Hawthorne effect is
   a behaviour change occurring when the subject is being observed during a
   scientific study. This effect would be multifactorial, and he suggests
   the term `effects of research participation.' However, the reviewed
   studies were conflicting and evidence is sparse. Research question: We
   updated McCambridge's review to actualize the definition of the HE.
   Methods: McCambridge's most recent article dated back to January 3,
   2012. We focused on the articles published between January 1, 2012 and
   August 10, 2018 searching Medline. We used the sole keyword `Hawthorne
   Effect.' The search was filtered based on the dates, the availability of
   an abstract and the languages English and French. We included articles
   defining or evaluating the HE. Articles citing the effect without
   defining it or irrelevant to the topic were excluded. Two independent
   readers searched and analysed the articles. Discrepancies were solved by
   consensus. Results: Out of 106 articles, 42 articles were included. All
   the articles acknowledged an observation bias, considered as significant
   or not, depending on the population (education, literacy), the methods
   and the variable of interest. It was a psychological change, limited in
   time. The HE was defined as a change of behaviour related to direct or
   indirect observation of the subjects or the investigators, to their
   previous selection and commitment in the study (written agreement) and
   to social desirability. Despite observations, articles were conflicting.
   Some do confirm the existence of the HE, others deny it. Meta-analysis
   is ongoing. Conclusion: No formal consensus regarding the definition of
   the effect has been reached so far. However, the authors agree on its
   implication as an experimental artefact.
   Background: Polypharmacy and multimorbidity are on the rise.
   Consequently, general practitioners (GPs) treat an increasing number of
   multimorbid patients with polypharmacy. To limit negative health
   outcomes, GPs should search for inappropriate medication intake in such
   patients. However, systematic medication reviews are time-consuming.
   Recent eHealth tools, such as the `systematic tool to reduce
   inappropriate prescribing' (STRIP) assistant, provide an opportunity for
   GPs to get support when conducting such medication reviews. Research
   question: Can the STRIP assistant as electronic decision support help
   GPs to optimize medication appropriateness in older, multimorbid
   patients with polypharmacy? Methods: This cluster randomized controlled
   trial is conducted in 40 Swiss GP practices, each recruiting 8-10
   patients aged >= 65 years, with >= 3 chronic conditions and >= 5 chronic
   medications (320 patients in total). We compare the effectiveness of
   using the STRIP assistant for optimizing medication appropriateness to
   usual care. The STRIP assistant is based on the STOPP/START criteria
   (version 2) and, for this trial, it is implemented in the Swiss eHealth
   setting where some GPs already share routine medical data from their
   electronic medical records in a research database (FIRE). Patients are
   followed-up for 12 months and the change in medication appropriateness
   is the primary outcome. Secondary outcomes are the numbers of falls and
   fractures, quality of life, health economic parameters, patients'
   willingness to deprescribe as well as implementation barriers and
   enablers for GPs when using the STRIP assistant. Results: Patient
   recruitment started in December 2018. This presentation focuses on the
   study protocol and the challenges faced when testing this new software
   in Swiss primary care. Conclusion: Finding out whether the STRIP
   assistant is an effective tool and beneficial for older and multimorbid
   patients, who are usually excluded from trials, will have an impact on
   the coordination of chronic care for multimorbid patients in Swiss
   primary care in this new eHealth environment.
   Background: Workplace violence (WPV) towards healthcare staff is
   becoming a common problem in different healthcare settings worldwide.
   Moreover, the prevalence is 16 times higher than in other professions.
   How often it happened towards young doctors working as general
   practitioners (GPs) at the beginning of their careers has been rarely
   studied. Research question: To investigate the frequency and forms of
   WPV, experienced by the young Croatian GPs from their patients, and
   violence reporting pattern to the competent institutions. Methods: The
   cross-sectional study was carried out on 74 GP residents, during their
   postgraduate study in family medicine in May 2018. A specially designed
   anonymous questionnaire, developed by Association of Family Physicians
   of South Eastern Europe, was used to investigate the prevalence and
   forms of WPV, the narrative description of the traumatic event itself
   and the process of reporting it. Results: The response rate was 91.9\%,
   female 87\%, the median of years working as a GP was 3.5 years. Most of
   the residents were working in an urban practice (63\%), others in the
   rural and the suburban once (27\%, 10\%). All GP residents experienced
   patients' and caregivers' violent behaviour directed towards them.
   High-intensity violence (e.g. physical violence, sexual harassment) was
   experienced by 44\%, middle intensity (e.g. intimidation, visual sexual
   harassment) by 84\% while all residents experienced verbal violence.
   Only 13.2\% residents reported WPV to the competent institutions. Most
   of GP residents reported the appearance of the new form of violence: the
   one over the internet. Conclusion: The high prevalence of all types of
   violence towards young Croatian doctors is worrisome, as is the fact
   that violent acts are seldom reported to the competent institutions.
   Those alarming facts could become a threat to GPs career choosing.
   Background: About 50\% of patients adhere to chronic therapy in France.
   Improving adherence should improve their care. Identifying the patient's
   difficulties in taking medication is complex for the physician, because
   there is no gold standard for measuring adherence to medications. How
   can the general practitioner in his/her practice identify patient
   compliance? Research question: Analyse studies that develop or validate
   scales used to estimate adherence in primary care. Methods: A systematic
   review of the literature from PubMed, the Cochrane Library and PsycINFO
   databases. The search terms used were the MeSH terms (or adapted to the
   database's vocabulary): questionnaire, compliance and primary care. All
   articles were retained whatever the language of writing. Selection
   criteria were: assessment of the development, validation or reliability
   of one or more compliance scales; taking place in primary care. One
   reviewer screened titles, which included the term adherence then
   abstracts and full text. Only articles evaluating the development,
   validity or reliability of a primary care adherence rating scale were
   included in analysis. Results: In total 1022 articles were selected and
   18 articles were included. Seventeen adherence scales were identified in
   primary care, most of which targeted a single pathology, especially
   hypertension. The most cited scale is the MMAS Morisky medication
   adherence scale. Three scales were developed for patients with multiple
   chronic diseases. One scale was developed for patients older than 65
   years-the Strathclyde compliance risk assessment tool (SCRAT)-and two
   scales were developed for adult patients whatever their age-the
   instrument developed by Sidorkiewicz et al., and the DAMS, diagnostic
   adherence to medication scale. Conclusion: Two scales have been
   developed and validated in primary care to assess patient adherence with
   multiple chronic diseases: the DAMS and the instrument developed by
   Sidorkiewicz et al. A simple, reliable, reproducible primary care scale
   would assess the impact of actions developed to improve adherence:
   motivational interviewing, patient therapeutic education, and the ASALeE
   protocol.
   Background: Multimorbidity prevalence increases with age while declining
   quality of life (QoL) is one of its major consequences. Research
   question: The study aims to: (1) Assess the relationship between
   increasing number of diseases and QoL. (2) Identify the most frequently
   occurring patterns of diseases and how they relate to QoL. (3) Observe
   how these associations differ across different European countries and
   regions. Methods: Cross-sectional data analysis performed on wave six of
   the population-based survey of health, ageing and retirement in Europe
   (SHARE) (n = 68 231). Data were collected in 2015 among population 50+
   years old in 17 European countries and Israel. Multimorbidity is defined
   as the co-occurrence of two or more chronic conditions. Conditions were
   self-declared and identified through an open-end questionnaire
   containing 17 prelisted conditions plus conditions added by
   participants. Control, autonomy, self-realization and pleasure
   questionnaire (CASP-12v) was used to evaluate QoL. Association between
   increasing number of diseases and QoL was assessed with linear
   regression. Factor analysis is being conducted to identify patterns of
   diseases to evaluate their impact on QoL further. Multilevel analysis
   will take into account differences between countries and regions.
   Confounding was searched with directed acyclic graph (DAG) method and
   included age, sex, education, socio-economic status, behavioural habits,
   social support and healthcare parameters. Results: Participants
   (49.09\%) had two or more diseases. Maximum number of diseases per
   person was 13, mean number was 1.9. Unadjusted preliminary analysis
   showed that on average QoL decreases by -1.27 (95\%CI: -1.29, -1.24)
   with each added new condition across Europe. The decline appears to be
   the steepest in Spain, -1.61 (95\%CI: -1.71, -1.51), and the least so in
   Israel, -0.67 (95\%CI: -0.82, -0.52). Conclusion: Ongoing analysis will
   identify disease patterns, which may have the highest impact on QoL, as
   well as to elucidate the role of confounders in the relationship between
   increasing number of diseases and disease patterns with QoL.
   Background: The burden and preventive potential of disease is typically
   estimated for each non-communicable disease (NCD) separately but NCDs
   often co-occur, which hampers reliable quantification of their overall
   burden and joint preventive potential in the population. Research
   questions: What is the lifetime risk of developing any NCD? Which
   multimorbidity clusters of NCDs cause the greatest burden? To what
   extent do three key shared risk factors, namely smoking, hypertension
   and being overweight, influence this risk, life-expectancy and
   NCD-multimorbidity? Methods: Between 1990 and 2012 we followed NCD-free
   participants aged >= 45 years at baseline from the Dutch prospective
   Rotterdam study for incidents of stroke, heart disease, diabetes,
   chronic respiratory disease, cancer, and neurodegenerative disease. We
   quantified (co-)occurrence and remaining lifetime risk of NCDs in a
   competing risk framework, and studied the effects of smoking,
   hypertension, and being overweight on lifetime risk and life expectancy.
   Results: During follow-up of 9061 participants, 814 participants were
   diagnosed with stroke, 1571 with heart disease, 625 with diabetes, 1004
   with chronic respiratory disease, 1538 with cancer, and 1065 with
   neurodegenerative disease. Among those, 1563 participants (33.7\%) were
   diagnosed with multiple diseases. The lifetime risk of any NCD from the
   age of 45 onwards was 94.0\% (95\%CI: 92.9-95.1) for men and 92.8\%
   (95\%CI: 91.8-93.8) for women. Absence of shared risk factors was
   associated with a 9.0-year delay (95\%CI: 6.3-11.6) in the age at onset
   of any NCD. Furthermore, overall life expectancy for participants
   without risk factors was 6.0 years (95\%CI: 5.7-7.9) longer than those
   with these risk factors. Participants without these risk factors spent
   21.6\% of their remaining lifetime with NCDs, compared to 31.8\% for
   those with risk factors. Conclusion: Nine out of 10 individuals aged 45
   years and older will develop at least one NCD during their remaining
   lifetime. A third was diagnosed with multiple NCDs during follow-up.
   Absence of three common shared risk factors related to compression of
   morbidity of NCDs.
   Background: This study examined if using electronic reminders increases
   the rate of diagnosis recordings in the patient chart system following
   visits to a general practitioner (GP). The impact of electronic
   reminders was studied in the primary care of a Finnish city. Research
   question: How effective is the reminder of the information system in
   improving the diagnostic level of primary care? Which is better and how:
   financial incentives or reminders? Methods: This was an observational
   retrospective study based on a before-and-after design and was carried
   out by installing an electronic reminder in the computerized patient
   chart system to improve the recording of diagnoses during GP visits. The
   quality of the recorded diagnoses was observed before and after the
   intervention. The effect of this intervention on the recording of
   diagnoses was also studied. Results: Before intervention, the level of
   recording diagnoses was about 40\% in the primary care units. After four
   years, the recording rate had risen to 90\% (p < 0.001). The rate of
   change in the recording of diagnoses was highest during the first year
   of intervention. In the present study, most of the visits concerned mild
   respiratory infections, elevated blood pressure, low back pain and type
   II diabetes. Conclusion: An electronic reminder improved the recording
   of diagnoses during the visits to GPs. The present intervention produced
   data, which reflects the distribution of diagnoses in real clinical life
   in primary care and thus provides valid data about the public.
   Background: Child abuse is widespread, occurs in all cultures and
   communities and remains undiscovered in 90\% of the cases. In total,
   80\% of reported child abuse concerns emotional ill-treatment. In the
   Netherlands, at least 3\% (118 000) of children are victims of child
   abuse resulting in 50 deaths each year. Only 1-3\% of abuse cases are
   reported by general practitioners (GPs) to the Child Protective Services
   agency (CPS). To explain this low reporting rate, we examined GPs'
   experiences with child abuse. Research question: How does the suspicion
   of child abuse arise in GPs' diagnostic reasoning? How do they act upon
   their suspicion and what kind of barriers do they experience in their
   management? Methods: In total 26 GPs (16 female) participated in four
   focus groups. We used purposive sampling to include GPs with different
   levels of experience in rural and urban areas spread over the
   Netherlands. We used NVivo for thematic content analysis. Results:
   Suspected child abuse arose based on common triggers and a gut feeling
   that `something is wrong here'. GPs acted upon their suspicion by
   gathering more data by history taking and physical examination. They
   often found it challenging to decide whether a child was abused because
   parents, despite their good intentions, may lack parenting skills and
   differ in their norms and values. GPs reported clear signs of sexual
   abuse and physical violence to CPS. However, in less clear-cut cases
   they followed-up and built a supporting network around the family. Most
   GPs highly valued the patient-doctor relationship while recognizing the
   risk of pushing boundaries. Conclusion: A low child abuse reporting rate
   by GPs to CPS does not mean a low detection rate. GPs use patients'
   trust in their doctor to improve a child's situation by involving other
   professionals.
   Background: The number of people suffering from multiple chronic
   conditions, multimorbidity, is rising. For society, multimorbidity is
   known to increase healthcare expenses through more frequent contacts,
   especially with the primary sector. For the individual, an increasing
   number of medical conditions are associated with lower quality of life
   (QoL). However, there is no statistically validated condition-specific
   patient-reported outcome measure (PROM) for the assessment of QoL among
   patients with multimorbidity. A validated PROM is essential in order to
   measure effect in intervention studies for this patient group. Research
   question: (1) To identify items covering QoL among patients with
   multimorbidity in a Danish context. (2) To develop and validate a PROM
   for assessment of QoL among patients with multimorbidity. (3) To utilize
   the final PROM in a large group of patients with multimorbidity to
   measure their QoL when living with different combinations and severity
   of multimorbidity. Methods: Phase 1: qualitative individual and focus
   group interviews with patients with multimorbidity to identify relevant
   QoL items. Phase 2: validation of the items through a draft
   questionnaire sent by email to around 200-400 patients with
   multimorbidity. Phase 3: psychometric validation of the draft
   questionnaire securing items with the highest possible measurement
   quality. Phase 4: assessment of QoL among approximately 2000 patients
   with multimorbidity from the Danish Lolland-Falster study. Results:
   There are no results yet. Currently, the interview guide is under
   development. Conclusion: Despite the rising number of patients with
   multimorbidity and the known inverse relationship between a patient's
   number of medical conditions and their quality of life, there is no
   statistically validated condition-specific PROM for assessment of QoL
   among this group. Our aim is that this project's developed and validated
   PROM will be used in future intervention studies as a valid measure of
   QoL among patients with multimorbidity.
   Background: Through a systematic review of the literature and
   qualitative research across Europe, the European General Practitioners
   Research Network (EGPRN) has designed and validated a comprehensive
   definition of multimorbidity. It is a concept considering all the
   biopsychosocial conditions of a patient. This concept encompasses more
   than 50 variables and is consequently difficult to use in primary care.
   Consideration of adverse outcomes (such as death or acute
   hospitalization) could help to distinguish which variables could be risk
   factors of decompensation within the definition of multimorbidity.
   Research question: Which criteria in the EGPRN concept of multimorbidity
   could detect outpatients at risk of death or acute hospitalization (i.e.
   decompensation) in a primary care cohort at 24-months of follow-up?
   Methods: Primary care outpatients (131) answering to EGPRN's
   multimorbidity definition were included by GPs, during two periods of
   inclusion in 2014 and 2015. At 24 months follow-up, the status
   `decompensation' or `nothing to report' was collected. A logistic
   regression following a Cox model was performed to achieve the survival
   analysis and to identify potential risk factors. Results: At 24 months
   follow-up, 120 patients were analysed. Three different clusters were
   identified. Forty-four patients, representing 36.6\% of the population,
   had either died or been hospitalized more than seven consecutive days.
   Two variables were significantly associated with decompensation: Number
   of GPs encounters per year (HR: 1.06; 95\%CI: 1.03-1.10, p <0.001), and
   total number of diseases (HR: 1.12; 95\%CI: 1.03-1.33; P = 0.039).
   Conclusion: To prevent death or acute hospitalization in multimorbid
   outpatients, GPs may be alert to those with high rates of GP encounters
   or a high number of illnesses. These results are consistent with others
   in medical literature.
   Background: A study of casual versus causal comorbidity in family
   medicine in three practice populations from the Netherlands, Malta and
   Serbia. Research question: (1) What is the observed comorbidity of the
   20 most common episodes of care in three countries? (2) How much of the
   observed comorbidity is likely to be casual versus causal? Methods:
   Participating family doctors (FDs) in the Netherlands, Malta and Serbia
   recorded details of all patient contacts in an episode of care structure
   using electronic medical records based on the International
   Classification of Primary Care, collecting data on all elements of the
   doctor-patient encounter, including the diagnostic labels (episode of
   care labels, EoCs). Comorbidity was measured using the odds ratio of
   both conditions being incident or rest-prevalent in the same patient in
   one-year data frames, as against not. Results: Comorbidity in family
   practice expressed as odds ratios between the 41 most prevalent (joint
   top 20) episode titles in the three populations. Specific associations
   were explored in different age groups to observe the changes in odds
   ratios with increasing age as a surrogate for a temporal or biological
   gradient. Conclusion: After applying accepted criteria for testing the
   causality of associations, it is reasonable to conclude that most of the
   observed primary care comorbidity is casual. It would be incorrect to
   assume causal relationships between co-occurring diseases in family
   medicine, even if such a relationship might be plausible or consistent
   with current conceptualizations of the causation of disease. Most
   observed comorbidity in primary care is the result of increasing illness
   diversity.
   Background: The concept of therapeutic alliance emerged in the beginning
   of the twentieth century and came from psychoanalysis. This notion was
   then extended to the somatic field and aims to replace the paternalistic
   model in the doctor-patient relationship. The EGPRN TATA group selected
   the WAI SR as the most reliable and reproducible scale to assess
   therapeutic alliance. To use it within Europe, it was necessary to
   translate it into most European languages. The following study aimed to
   assess the linguistic homogeneity of five of these translations.
   Research question: Are the translations of the WAI SR homogeneous
   between Spain, Poland, Slovenia, France and Italy? Methods:
   Forward-backward translations were achieved in five participating
   countries (Spain, Poland, France, Slovenia and Italy). Using a Delphi
   procedure, a global homogeneity check was then performed by comparing
   the five backward translations during a physical meeting involving GP
   teachers/researchers from many European countries; the heterogeneity of
   the participants' origins was a token of reliability. Results: In the
   assessment of the five translations, 107 experts participated. A
   consensus was obtained in one to two Delphi rounds for each. During the
   `homogeneity check,' some discrepancies were noted with the original
   version and were discussed with the local teams. This last stage
   permitted to highlight cultural discrepancies and real translation
   issues and to correct if needed. Conclusion: Five homogeneous versions
   of the WAI SR are now available in five European languages. They will be
   helpful to evaluate therapeutic alliance at different levels: for GPs in
   daily practice, for students during the initial and continuous training,
   and for further research in these five countries.
   Background: The patient enablement instrument (PEI) is an established
   patient-reported outcome measure (PROM) that reflects the quality of a
   GP appointment. It is a six-item questionnaire, addressed to the patient
   immediately after a consultation. Research question: The study aimed to
   evaluate whether a single-item measure (the Q1), based on the PEI, or a
   single question extracted from the PEI itself (the Q2) could replace the
   PEI when measuring patient enablement among Finnish healthcare centre
   patients. Methods: The study design included (1) a pilot study with
   brief interviews with the respondents, (2) a questionnaire study before
   and after a single appointment with a GP, and (3) a telephone interview
   two weeks after the appointment. The correlations between the measures
   were examined. The sensitivity, specificity and both positive and
   negative predictive values for the Q1 and the Q2 were calculated, with
   different PEI score cut-off points. Results: Altogether 483 patients
   with completed PEIs were included in the analyses. The correlations
   between the PEI and the Q1 or the Q2 were 0.48 and 0.84, respectively.
   Both the Q1 and the Q2 had high sensitivity and negative predictive
   value in relation to patients with lower enablement scores. The
   reliability coefficients were 0.24 for the Q1 and 0.76 for the Q2.
   Conclusion: The Q2 seems to be a valid and reliable way to measure
   patient enablement. The Q1 seems to be less correlated with the PEI, but
   it also has high negative predictive value in relation to low enablement
   scores.
   Multimorbidity challenges existing healthcare organization and research,
   which remains disease and single-condition focused. Basic science
   approaches to multimorbidity have the potential to identify important
   shared mechanisms by which diseases we currently think of as distinct
   might arise, but there is a pressing need for more applied and health
   services research to understand better and manage multimorbidity now.
   There are several recent clinical guidelines, which make recommendations
   for managing multimorbidity or related issues for patients such as
   polypharmacy and frailty. However, the evidence base underpinning these
   recommendations is often weak, and these guidelines, therefore, also
   help define a research agenda. A key problem for researchers and health
   services is that multimorbidity is very heterogeneous, in that
   `intermittent low back pain plus mild eczema' presents very different
   challenges to researchers and health services compared to `active
   psychosis plus severe heart failure'. Identifying important but
   tractable research questions is therefore not always straightforward.
   This presentation will identify important gaps in the evidence, and
   illustrate how they might be filled. The focus will be on two areas
   where there is consensus that better evidence is needed to inform care
   design and delivery: (1) organizational interventions to implement more
   coordinated and holistic care; and (2) interventions to improve
   medicines management in people with multimorbidity and polypharmacy.
   These illustrate both the potential for imaginative research, but also
   the scale.
   Background: The accumulation of multiple chronic diseases
   (multimorbidity) and multiple prescribed medications (polypharmacy) over
   time may influence the extent to which an individual maintains health
   and well-being in later life. Research question: This research aims to
   describe the patterns (sequence and timing) of multimorbidity and
   polypharmacy that accumulate over time among primary healthcare patients
   in Canada. Methods: Data are derived from the Canadian primary care
   sentinel surveillance network (CPCSSN) electronic medical record (EMR)
   database that holds >= 1 million longitudinal, de-identified records.
   Multimorbidity will be identified with 20 categories, cut-off points of
   >= 2 and >= 3 chronic conditions and the International Classification of
   Disease (ICD) classification system. Polypharmacy will be identified
   using the cut-off points of >= 5 and >= 10 medication classes and the
   Anatomical Therapeutic Chemical (ATC) classification system. Analyses
   will be conducted using Java and Stata 14.2 software. Results: The
   prevalence of chronic diseases and prescribed medications will be
   presented, as well as the patterns that are observed among adults and
   older adults in Canada. The most frequent patterns (combinations and
   permutations) of multimorbidity and polypharmacy will be presented,
   stratified by sex and age category. The relationships with other
   factors, such as the presence of frailty, disability or increased health
   service use, will be examined. As well, the methodological challenges to
   identifying the presence and sequence of multimorbidity and polypharmacy
   in national, longitudinal data will be discussed. Conclusion: This
   research will explore the profiles of multimorbidity and polypharmacy in
   mid- and late-life using a national, longitudinal database. These
   findings can be used strategically to inform healthcare delivery and to
   contribute to the understanding of multimorbidity and polypharmacy in
   the international literature. Reducing the burden of prescribed
   medications and the harms of polypharmacy are key tasks within the
   context of multimorbidity.
   Background: Multimorbidity and polypharmacy have become the norm for
   general practitioners (GPs). Ideally, GPs search for inappropriate
   medication and, if necessary, deprescribe. However, it remains
   challenging to deprescribe given time constraints and little backup from
   guidelines. Furthermore, barriers and enablers to deprescribing among
   patients have to be accounted for. Research question: To identify
   barriers and enablers to deprescribing in older patients with
   polypharmacy. Methods: We surveyed among patients >70 years with
   multimorbidity (>2 chronic conditions) and polypharmacy (>4 regular
   medicines). We invited Swiss GPs to recruit eligible patients, each of
   whom completed a paper-based survey on demography, medications and
   chronic conditions. We applied the revised patients' attitudes towards
   deprescribing (rPATD) questionnaire and added 12 additional questions
   and two open questions to assess barriers and enablers towards
   deprescribing. Results: We analysed the first 221 responses received so
   far and full results will be presented at the conference. Participants
   were 79.3 years in mean (SD 5.8) and 48\% female. Thirty-one percent
   lived alone, and 85\% prepared their medication themselves, all others
   required help. Seventy-six percent of participants took 5-9 regular
   medicines and 24\% took >= 10 up to 22 medicines. Participants (76\%)
   were willing to deprescribe one or more of their medicines and 78\% did
   not have any negative experience with deprescribing. Age and gender were
   not associated with their willingness to deprescribe. Important barriers
   to deprescribing were satisfaction with drugs (96\%), long-term drugs
   (56\%) and noticing positive effects when taking them (92\%). When it
   comes to deprescribing, 89\% of participants wanted as much information
   as possible on their medicines. Having a good relationship with their GP
   was a further key factor to them (85\%). Conclusion: Most older adults
   are willing to deprescribe. They would like to be informed about their
   medicines and want to discuss deprescribing to achieve shared
   decision-making with the GP they trust.
   Background: With growing populations of patients with multimorbidity,
   general practitioners need insight into which patients in their practice
   are most in need for person-centred integrated care ('high-need'
   patients). Using data from electronic primary care medical records to
   automatically create a list of possible `high need' patients could be a
   quick and easy first step to assist GPs in identifying these patients.
   Research question: Can `high need' patients with multimorbidity be
   identified automatically from their primary care medical records?
   Methods: Pseudonymized medical records of patients with multimorbidity
   (>= 2 chronic diseases) were analysed. Data was derived from the Nivel
   primary care database, a large registry containing data routinely
   recorded in electronic health records. This includes data on healthcare
   use, health problems and treatment. Logistic regression analysis was
   conducted to predict outcomes (frequent contact with the general
   practice, ER visits and unplanned hospital admissions). Predictors were
   age, sex, healthcare use in the previous year, morbidity and medication
   use. Results: In total, 245 065 patients with multimorbidity were
   identified, of which 48\% were above the age of 65 and 57\% female. More
   than 42\% had five GP contacts in the previous year and 62\% used five
   or more different medications. Frequent contact with the general
   practice could be reliably predicted using only the number of contacts
   in the previous year (AUC: 0.82). Adding all other predictors (including
   specific chronic conditions) only improved the predictive value of the
   model marginally (AUC: 0.84). Identifying patients with a high risk for
   ER visits and unplanned hospital admissions proved more difficult (AUC:
   0.67 and 0.70, respectively). Conclusion: `High need' patients with
   multimorbidity can be automatically selected from primary care medical
   records using only the number of contacts with the general practice in
   the previous year. Composing a list of these patients can help GPs to
   identify those eligible for person-centred integrated care.
   Background: Chronic diseases usually have a long duration and slow
   progression and, as a result, they tend to aggregate in multimorbidity
   patterns (MPs) during the life course and/or due to shared underlying
   pathophysiological pathways. Knowledge of how MPs progress over time is
   necessary to develop effective prevention management strategies.
   Research question: What are the most likely MPs over time? Which
   longitudinal shifts from one pattern to another occur during follow-up?
   Methods: A prospective longitudinal study based on electronic health
   records was conducted during 2012-2016 in Catalonia, Spain. For people
   aged >= 65 years, we extracted data on demographics and diagnostic codes
   for chronic diseases (ICD-10). Machine-learning techniques were applied
   for the identification of disease clusters using fuzzy c-means analysis
   to obtain initial clusters. To estimate longitudinal MPs and their
   progression for each individual a hidden Markov model was fitted,
   estimating: (1) the transition probability matrix between clusters; (2)
   the initial cluster probability; (3) the most likely trajectory for each
   individual. The prevalence of disease in each cluster, observed/expected
   ratios (O/E ratios) and disease exclusivity was determined for each MP.
   Criteria used to designate cluster: O/E ratio >= 2. Results: In total,
   916 619 individuals were included. Ten MPs were identified. The cluster
   including the most prevalent diseases was designated non-specific
   (42.0\% of individuals). The remaining nine clusters included the
   following anatomical systems: ophthalmologic and mental diseases
   (19.3\%), osteometabolic (7.9\%), cardio-circulatory (6.6\%), and
   others. Most patients, minimum 59.2\%, remained in the same cluster
   during the study period. The highest transitions to the mortality state
   were observed in the cardio-circulatory (37.1\%) and nervous (31.8\%)
   MPs. Conclusion: Ten significant longitudinal MPs were found. The
   application of sophisticated statistical techniques ideally suited the
   study of the MPs and allowed for characterization over time. This method
   is useful to establish a probabilistic evolution of MPs.
   Background: Quality of life is an essential theme for quantitative
   surveys in primary care. Treatments and procedures need to be assessed
   on whether they change patients' quality of life. This has led to the
   creation of evaluation scales. The purpose of this study was to
   determine reproducibility and efficiency of 11 previously selected
   quality of life scales (selected with a systematic review) for the
   general population. Research question: What is the best possible
   reproducible and efficient quality of life scale for the general
   population? Methods: The search was conducted from November 2017 to
   April 2018 in PubMed and Cochrane databases, according to the PRISMA
   (preferred reporting items for systematic reviews and meta-analyses)
   protocol. The inclusion criteria were the psychometric qualities for
   each of the 11 scales studied. Articles dealing with subpopulations or
   those not written in IMRAD format were excluded. The collected values
   were reproducibility and efficiency. Results: Out of 206, 46 selected
   articles were included. Cronbach's alpha by domain and Pearson's
   coefficient were the most analysed psychometrics. No valid efficiency
   data was obtained. The internal consistency was over 0.7 for the SF-36,
   SF12v2 and EQ-5D scales. The Pearson coefficient was over 0.4 for the
   SF36v2, SF-12 and SF-12v2 scales. The Cohen's kappa ranged from 0.4 to
   0.80 for the EQ-5D questionnaire. Conclusion: No scale is fully
   validated. Reproducibility values were incomplete (Cronbach's alpha and
   Pearson's most expressed). No efficiency data was found. The most
   validated scales are the SF family and the EQ-5D. Researchers and
   clinicians should be aware of these limitations when choosing a quality
   of life scale. They should return to the scales' designs to choose the
   one that underlines the type of quality of life they want to assess as
   no external validity is available.
   Background: Previous studies have shown an increased rate of infection
   among patients with diabetes; however, it is unclear from these studies
   if the level of HbA1c is correlated with infection. Research question:
   This study aimed to examine the association between glycaemic control of
   type 2 diabetes patients and the incidence of infections. Methods: An
   HMO database was used to identify all DM patients. The first HbA1c test
   during the period of the study was selected for each patient; then an
   infection diagnosis was searched in the 60 days that followed the test.
   We compared the HbA1c test results that were followed by an infection to
   those that were not. After applying exclusion criteria: having cancer,
   receiving immunosuppressive medication, undergoing dialysis treatment,
   anaemia less than 9 mg\%, and G6PD deficiency, there remained 33 637
   patients in the cohort. The study period was October 2014 to September
   2017. The following information was collected: age, gender,
   socio-economic index, BMI, use of hypoglycaemic and steroid medication
   in the 90 days before infection, and comorbid conditions (IHD, PVD, CVA,
   CCF, asthma, COPD, Parkinson's disease, dementia, CRF). Results: In
   total, 804 patients had an infection within 60 days following an HbA1c
   test. For cellulitis, cholecystitis, herpes zoster, pneumonia and
   sinusitis the HbA1c was higher than those patients that had no infection
   (for cellulitis 7.603 vs 7.243). When factored into logistic regression
   analysis, we found that other chronic diseases increased the risk of
   infection between 29 and 60\%. Each increase of a gram of HbA1c
   increased the risk by 8.5\%. Use of steroids in the 90 days before the
   infection increases the chance of infection by 734\%. Conclusion:
   Increasing HbA1c and comorbidity both increase the risk of infection
   among type 2 diabetics but use of oral or injectable steroids is a much
   more significant risk factor.},
DOI = {10.1080/13814788.2019.1643166},
ISSN = {1381-4788},
EISSN = {1751-1402},
ResearcherID-Numbers = {Blondeel, Sofie/AAE-5307-2022
   Fazli, Ghazal/AAE-8320-2022
   DSILVA, BROOKE/HCI-4879-2022
   Baldissera, Annalisa/AHD-6334-2022},
Unique-ID = {WOS:000481779500010},
}

@article{ WOS:000477714100001,
Author = {Angqvist, Mattias and Munoz, William A. and Rahm, J. Magnus and
   Fransson, Erik and Durniak, Celine and Rozyczko, Piotr and Rod, Thomas
   H. and Erhart, Paul},
Title = {ICET - A Python Library for Constructing and Sampling Alloy Cluster
   Expansions},
Journal = {ADVANCED THEORY AND SIMULATIONS},
Year = {2019},
Volume = {2},
Number = {7},
Month = {JUL},
Abstract = {Alloy cluster expansions (CEs) provide an accurate and computationally
   efficient mapping of the potential energy surface of multi-component
   systems that enables comprehensive sampling of the many-dimensional
   configuration space. Here, integrated cluster expansion toolkit (ICET),
   a flexible, extensible, and computationally efficient software package,
   is introduced for the construction and sampling of CEs. ICET is largely
   written in Python for easy integration in comprehensive workflows,
   including first-principles calculations for the generation of reference
   data and machine learning libraries for training and validation. The
   package enables training using a variety of linear regression algorithms
   with and without regularization, Bayesian regression, feature selection,
   and cross-validation. It also provides complementary functionality for
   structure enumeration and mapping as well as data management and
   analysis. Potential applications are illustrated by two examples,
   including the computation of the phase diagram of a prototypical
   metallic alloy and the analysis of chemical ordering in an inorganic
   semiconductor.},
DOI = {10.1002/adts.201900015},
Article-Number = {1900015},
EISSN = {2513-0390},
ResearcherID-Numbers = {Erhart, Paul/ABA-4772-2020
   Rod, Thomas/AAE-4572-2020
   },
ORCID-Numbers = {Erhart, Paul/0000-0002-2516-6061
   Rod, Thomas/0000-0002-6227-8062
   Fransson, Erik/0000-0001-5262-3339},
Unique-ID = {WOS:000477714100001},
}

@article{ WOS:000470042900012,
Author = {Creus, P. K. and Basson, I. J. and Koegelenberg, C. K. and Ekkerd, J.
   and de Graaf, P. J. H. and Bester, M. and Mokele, T.},
Title = {3D Fabric analysis of Venetia Mine, South Africa: Using structural
   measurements and implicitly-modelled surfaces for improved pit slope
   design and risk management},
Journal = {JOURNAL OF AFRICAN EARTH SCIENCES},
Year = {2019},
Volume = {155},
Pages = {137-150},
Month = {JUL},
Abstract = {The geometry and structural history of high-grade granite-gneiss units,
   which host the Venetia diatremes in South Africa, have been modelled in
   3D using an implicit, rules-based, conditional geometrical technique.
   The volume contains a pervasive, dominant foliation, which forms the
   main plane of strength anisotropy in ongoing design and mining of the
   Venetia pit, which has a planned final depth of approximately 465m below
   surface. The well-developed S-2 foliation is a continuous feature that
   was locally re-oriented during D-3 and D-4 and localised `wrapping'
   around competent amphibolite/hornblendite lenses or boudins. These local
   variations have important implications for pit design and operational
   risk management, particularly where they dip adversely out of the slope,
   or occur at the base of bench stacks, or in inter-ramp areas. These
   structures, when undercut by dips steeper than their friction angle, may
   result in planar sliding, presenting an operational safety risk. Best
   practice requires early identification of these occurrences, to make
   design adjustments where warranted. Lithological contacts, which have
   been transposed into parallelism with the S-2 foliation, and a robust
   set of structural data collected over the last 10 years, are analysed to
   quantify and represent the variable orientation of S-2 in 3D space. This
   contribution describes several methods, using leading software packages
   such as Leapfrog Geo (TM) Micromine (TM) and Python, to generate
   representative S-2 form surfaces and anisotropy block models, both of
   which are used for downstream geotechnical engineering analysis. Outputs
   are also translated into apparent dip ``heat{''} maps that show the
   angular interaction between S-2 foliation and a pit design surface.
   These methods augment, and lessen the dependence on, typical 2D
   wedge-shaped design sectors that are commonly employed in pit design.
   This approach has important safety and economic benefits. As the rock
   mass fabric is characterised, it supports quick identification of
   potentially higher-risk areas (e.g. adverse undercut fabric with greater
   likelihood of slope instability) in current mine plans, which justifies
   more detailed analysis and/or additional monitoring controls to ensure
   safe and efficient mining. It also allows rapid mine design
   optimisation.},
DOI = {10.1016/j.jafrearsci.2019.04.009},
ISSN = {1464-343X},
EISSN = {1879-1956},
ORCID-Numbers = {Basson, Ian/0000-0002-4512-5080},
Unique-ID = {WOS:000470042900012},
}

@article{ WOS:000477677900010,
Author = {Vinogradov, L. V. and Oshchepkov, P. P. and Mamaev, V. K.},
Title = {CAD System for Automatic Design of C1 Compressor Profile},
Journal = {CHEMICAL AND PETROLEUM ENGINEERING},
Year = {2019},
Volume = {55},
Number = {3-4},
Pages = {248-257},
Month = {JUL},
Abstract = {A system for computer-aided design (CAD) for the construction of a C1
   compressor profile (CAD\_C1) in the integrated Mathcad package is
   developed. Results of the design of a symmetric initial C1 profile and a
   curved C1-based profile CAD-C1 are presented. CAD-C1 is constructed
   according to a modular principle, i.e., it constitutes a complex of
   information-bound computational software modules. The principal
   advantage of such a structure of the system is that each module is
   solved as an individual complete problem and, independently of the other
   modules, may be altered in the course of development and improvement of
   the system. Independent connection (complementation) of other modules
   and systems is also possible. Designs of two variants of curved profiles
   with identical (practically maximal) turning angle of the flow is
   implemented in corresponding software modules, using a method of growing
   thickness of the profile along the normal to the center line of the
   profile and a method based on nonlinear Hearst transformations. The
   geometric characteristics of the different design variants are
   calculated, statistical estimation of the correlation indices of the
   approximation results is performed, graphical construction of the
   variant profiles is realized, and the geometric parameters of the
   profiles are varied along the length of the profile chord. CAD\_System
   C1 may be considered as an autonomous system and as a subsystem.},
DOI = {10.1007/s10556-019-00612-x},
ISSN = {0009-2355},
EISSN = {1573-8329},
ResearcherID-Numbers = {Oshchepkov, Petr P/T-7384-2017},
ORCID-Numbers = {Oshchepkov, Petr P/0000-0001-5504-106X},
Unique-ID = {WOS:000477677900010},
}

@article{ WOS:000474061200018,
Author = {Ashwood, Christopher and Pratt, Brian and MacLean, Brendan X. and
   Gundry, Rebekah L. and Packer, Nicolle H.},
Title = {Standardization of PGC-LC-MS-based glycomics for sample specific
   glycotyping},
Journal = {ANALYST},
Year = {2019},
Volume = {144},
Number = {11},
Pages = {3601-3612},
Month = {JUN 7},
Abstract = {Porous graphitized carbon (PGC) based chromatography achieves
   high-resolution separation of glycan structures released from
   glycoproteins. This approach is especially valuable when resolving
   structurally similar isomers and for discovery of novel and/or
   sample-specific glycan structures. However, the implementation of
   PGC-based separations in glycomics studies has been limited because
   system-independent retention values have not been established to
   normalize technical variation. To address this limitation, this study
   combined the use of hydrolyzed dextran as an internal standard and
   Skyline software for post-acquisition normalization to reduce retention
   time and peak area technical variation in PGC-based glycan analyses.
   This approach allowed assignment of system-independent retention values
   that are applicable to typical PGC-based glycan separations and
   supported the construction of a library containing >300 PGC-separated
   glycan structures with normalized glucose unit (GU) retention values. To
   enable the automation of this normalization method, a spectral MS/MS
   library was developed of the dextran ladder, achieving confident
   discrimination against isomeric glycans. The utility of this approach is
   demonstrated in two ways. First, to inform the search space for
   bioinformatically predicted but unobserved glycan structures, predictive
   models for two structural modifications, core-fucosylation and bisecting
   GlcNAc, were developed based on the GU library. Second, the
   applicability of this method for the analysis of complex biological
   samples is evidenced by the ability to discriminate between cell culture
   and tissue sample types by the normalized intensity of N-glycan
   structures alone. Overall, the methods and data described here are
   expected to support the future development of more automated approaches
   to glycan identification and quantitation.},
DOI = {10.1039/c9an00486f},
ISSN = {0003-2654},
EISSN = {1364-5528},
ResearcherID-Numbers = {Ashwood, Christopher/AAA-3800-2019
   },
ORCID-Numbers = {Ashwood, Christopher/0000-0001-5944-6179
   Gundry, Rebekah L./0000-0002-9263-833X
   Packer, Nicolle/0000-0002-7532-4021},
Unique-ID = {WOS:000474061200018},
}

@article{ WOS:000467714000039,
Author = {Feyissa, Garumma Tolu and Woldie, Mirkuzie and Munn, Zachary and
   Lockwood, Craig},
Title = {Exploration of facilitators and barriers to the implementation of a
   guideline to reduce HIV-related stigma and discrimination in the
   Ethiopian healthcare settings: A descriptive qualitative study},
Journal = {PLOS ONE},
Year = {2019},
Volume = {14},
Number = {5},
Month = {MAY 13},
Abstract = {Background
   The barriers to uptake of guidelines underscore the importance of going
   beyound the mere synthesis of evidence to tailoring the synthesized
   evidence into local contexts and situations. This requires in-depth
   exploration of local factors. This project aimed to assess contextual
   barriers and facilitators to the implementation of a guideline developed
   to reduce HIV-related stigma and discrimination (SAD) in the Ethiopian
   healthcare setting.
   Methods
   A descriptive qualitative research study was conducted using a
   semi-structured interview guide informed by the Registered Nurses
   Association of Ontario (RNAO) framework. The interview was conducted
   among a purposive sample of seven key informants from Jimma University
   and Jimma Zone HIV Prevention and Control Office. The interviews were
   transcribed, coded and analysed using Atlas ti version 7.5 software
   packages.
   Results
   Guideline attributes, provider-related factors and organizational and
   practice-related were identified as factors that can potentially affect
   the implementation of the guideline. The presence of expert patients
   were identified as agents for guideline implementation, whilst regular
   health education programs in addition to initiatives related to service
   quality improvement, were identified as suitable platforms to assist
   with the implementation of this guideline. Study participants
   recommended that the guideline should be disseminated through
   multi-disciplinary team (MDT) meetings, gate keepers such as opinion
   leaders and unit heads, one-to-five networks and mentorship programs, as
   well as training, workshops and posters. The current study also
   indicated that continuous monitoring, evaluation and mentorship are
   critical elements in the integration of the guideline into the system of
   the hospital.
   Conclusions
   This study identified that guideline implementation can make use of
   existing structures and pathways such as MDT meetings, service quality
   improvement initiatives, one-to-five networks, training and workshops.
   Teamwork and partnership with stakeholders should be strengthened to
   strengthen facilitators and tackle barriers related to the
   implementation of the guideline. Effective implementation of the
   guideline also requires establishing an implementation structure.
   Moreover, indicators developed to track the implementation of stigma
   reduction guideline should be integrated into mentorship, MDT meetings
   and evaluation programs of the hospital to improve performance and to
   assist data collection on implementation experiences.},
DOI = {10.1371/journal.pone.0216887},
Article-Number = {e0216887},
ISSN = {1932-6203},
ResearcherID-Numbers = {Woldie, Mirkuzie/S-4086-2017
   Feyissa, Garumma/AAP-9097-2020
   Munn, Zachary/AAV-9760-2021},
ORCID-Numbers = {Woldie, Mirkuzie/0000-0002-1777-0816
   Feyissa, Garumma/0000-0001-6179-0024
   Munn, Zachary/0000-0002-7091-5842},
Unique-ID = {WOS:000467714000039},
}

@article{ WOS:000475483200019,
Author = {Habib, Aqib and Ashfaq, Areeha and Ullah, Ateeqa Mujeeb},
Title = {A CROSS-SECTIONAL STUDY TO DETERMIINE THE FREQUENCY OF OBESITY AND ITS
   RELATION WITH PHYSICAL ACTIVITY},
Journal = {INDO AMERICAN JOURNAL OF PHARMACEUTICAL SCIENCES},
Year = {2019},
Volume = {6},
Number = {5},
Pages = {8894-8897},
Month = {MAY},
Abstract = {Objectives: To determine the prevalence of obesity among the adult male
   population and to assess the relation of obesity with physical activity,
   life style and caloric intake.
   Materials and Methods: A cross-sectional study design was conducted from
   January 2019 to March 2019, in THQ Hospital Shah Pur, Sargodha. The
   sample size was 400 according to WHO sample size calculation with 50\%
   prevalence, 95\% confidence interval and 5\% precision. Adult males of
   age group 20-60 years were included in the study while male age either
   less than 20 or more than were excluded from the study population. A
   multistage sampling technique was followed in this study. A pretested
   structured questionnaire was used for data collection of various
   variables like age, occupation, and average daily food intake, moderate
   to severe physical activity causing calories burn. Height and weight
   were measured and Body Mass Index was calculated for each of the
   individual by means of standard measuring protocols. Data analysis and
   interpretation were done using Statistical Package for Social Sciences
   and Microsoft Excel software programs and results were evaluated in the
   light of the proposed hypothesis and presented in forms of frequency and
   percentage.
   Results: Our study results showed 20\% prevalence of obesity. The
   average calories intake per person was more in approximately 75\% of the
   study population. The average calories burns per day were 1442.85
   calories which were less as compared to international food guidelines.
   Among the obese, 95\% had decreased physical activity and more calories
   intake as compared to normal non obese men. Our results showed that
   decreased physical activity/ sedentary life style showed positive
   correlation with obesity.
   Conclusion: Sedentary lifestyles, high caloric intake, and reduce
   physical activity the significant factors associated with obesity in
   adult males.},
DOI = {10.5281/zenodo.2667509},
ISSN = {2349-7750},
Unique-ID = {WOS:000475483200019},
}

@article{ WOS:000464751600004,
Author = {Vishart, Andreas Lynge and Ree, Nicolai and Mikkelsen, V, Kurt},
Title = {Graphical user interface for an easy and reliable construction of input
   files to CP2K},
Journal = {JOURNAL OF MOLECULAR MODELING},
Year = {2019},
Volume = {25},
Number = {5},
Month = {MAY},
Abstract = {Creating input files to atomistic simulations and quantum chemical
   calculations in the CP2K software package can be a challenge. Here, we
   present a new graphical user interface to reduce the complexity of the
   work needed to run a CP2K calculation as well as the risk for making
   mistakes. The program is called CP2K Editor, and it provides a
   user-friendly interface for both new and experienced users. CP2K Editor
   keeps the construction of the input file simple and manageable. The
   input files are similarly structured, so they are easy to recognize and
   adjust if more advanced configurations are needed. Furthermore, we have
   implemented several methods for analyzing the output data, and a routine
   to test the best cut-off values. In our group, CP2K Editor has clearly
   been of great help when creating input files to the CP2K software
   package.},
DOI = {10.1007/s00894-019-3987-6},
Article-Number = {115},
ISSN = {1610-2940},
EISSN = {0948-5023},
ResearcherID-Numbers = {Ree, Nicolai/O-9188-2019
   Mikkelsen, K./E-6765-2015},
ORCID-Numbers = {Ree, Nicolai/0000-0001-9900-5730
   Mikkelsen, K./0000-0003-4090-7697},
Unique-ID = {WOS:000464751600004},
}

@article{ WOS:000497156400001,
Author = {{[}Anonymous]},
Title = {ISPO 17th world congress Kobe, Hyogo, Japan 5-8 Oct 2019 Abstracts},
Journal = {PROSTHETICS AND ORTHOTICS INTERNATIONAL},
Year = {2019},
Volume = {43},
Number = {1S, 1},
Pages = {1-600},
Month = {MAY},
Abstract = {Background Spinal cord injury (SCI) is an inherently serious condition
   that have a profound physical, psychological, and socioeconomic impact
   on affected individuals. Recently, the incidence of incomplete SCIs are
   increasing, and the central nervous system of incomplete injury has the
   potential of plastic change. Hybrid assistive limb (HAL) is an
   exoskeleton robot suit that supports walking of persons with motor
   dysfunction. The effectiveness of HAL training on walking ability and
   balance in individuals with chronic SCI is still unlear. Aim To
   investigate the effctiveness of walking tarining with robot suit HAL for
   walking ability and balance in individuals with chronic spinal cord
   injury. Method Fifteen patients with chronic SCI (11 men, 4 women)
   classified by the American Spinal Cord Injury Association (ASIA) D (zone
   of partial preservation C4-L3), who received full rehabilitation in the
   acute and subacute phase of SCI were enrolled. A single case
   experimental A-B(pre-post) design study by repeated assessments of the
   same patients. The subjects performed 3 to 6 months (30 minutes per
   session, 2 times per week) HAL training with walker and supported by
   physical therapists. Functional outcome measures included 10-m walk
   test(10MWT), 6-minute walk test(6MWT), Berg balance scale(BBS), and the
   walking index for SCIII(WISCIII), which were evaluated without HAL.
   Results Twelve patients completed 3 to 6 months HAL training (3 patients
   dropped out after 6 or 8 sessions). Functional outcome measures were
   analyzed in those who completed HAL training. The mean walking speed in
   10MWT increased from 0.46 +/- 0.37 m/s to 0.73 +/- 0.50 m/s (p<0.01).
   The improvement corresponded to a 59\% faster walking than in initial
   evaluation. Walking distance in 6MWT increased from 127.8 +/- 112.7m to
   217.5 +/- 159.1m (p<0.05). Balance evaluated by BBS also improved from
   28.1 +/- 13.4 to 39.8 +/- 11.8 (p<0.01), and WISCIII from 13.8 +/- 5.1
   to 17.0 +/- 2.6 (p<0.05). Discussion and Conclusion Our results suggest
   that HAL training could improve walking ability and balance in
   individuals with chronic SCI. In the past, rehabilitation for SCI
   focused not on functional improvement, but on compensation for
   impairment. HAL would facilitate motor recovery and improve mobility in
   chronic SCI, and may play an important role in rehabilitation for SCI in
   future. Further controlled studies should be needed.
   It is very important that people who are in need of assistive products
   utilise those products in their daily life. In the first part of this
   symposium a keynote lecture will be made by a Japanese opinion leader
   who is an AT development director of a national institute, and is
   recommended by The Rehabilitation Engineering Society of Japan (RESJA).
   He will talk about the current situation of assistive products
   development and use in Japan which is the most aged society in the
   world. In his speech a research project is introduced which focuses
   relationship of assistive products use and daily activities of older
   people including nonagenarians and centenarians. A panel discussion will
   be held in the second part of the symposium. Remarkable panelists from
   WHO and Japan make short presentations about the circumstances of people
   with disabilities or the elders and usage status of assistive products
   in Southeast Asian and other countries. After the short reports, the
   organisers from WHO and NRCPD develop discussions with the panelists
   about better practice of AT use or methods to improve access to
   high-quality affordable AT. Statement of the objective / learning
   objectives The aim is to bring deeper understanding among opinion
   leaders in P\&O / Assistive Technology fields. The audience can learn
   the current situations of AT and be inspired to support AT development
   and dissemination. The Symposium `Assistive Technologies against Ageing
   Society' on Sunday, 6 October 2019, consists of two presentations with
   the first one taking place from 13:00 - 14:15 and the second one
   continuing from 14:30 - 15:45.
   Background The Australian Orthotic Prosthetic Association (AOPA)
   regulates the profession of orthotist/prosthetists in Australia through
   the establishment, maintenance and application of professional
   standards, such as competency standards. While portfolios are commonly
   used to assess professional competency, little research has investigated
   how reliable these assessments are, or how reliability can be improved.
   Aim To evaluate how reliable AOPA's competency portfolio assessment is
   and examine causes of variation to improve reliability. Method Overseas
   trained practitioners wishing to have their qualifications and
   experience recognised by AOPA submitted a portfolio for assessment.
   Portfolios were randomly assigned to two independent assessors who
   determined whether the applicant's portfolio demonstrates competency
   against each of the 68 AOPA Competency Standards (2014) and recorded
   their determination as ``satisfied{''} or ``not satisfied{''}. An
   administrator collated the independent evaluations and identified any
   points of disagreement for discussion at a consensus meeting. To
   quantify interassessor reliability, the Kappa statistic was used for
   each of the 68 competency standards. To better understand the reasons
   for disagreement, semi-structured interviews were conducted with each
   assessor. Results Results from this study show high rates of agreement
   between assessors upon independent assessment. Points of disagreement
   were commonly resolved during the consensus meeting between assessors,
   disagreement was rare. The standards which experienced the highest rates
   of disagreement were identified. Discussion and Conclusion Evidence
   based standards should be supported by evidence-based processes.
   Competency assessment via a portfolio of evidence can be conducted and
   assessed by two assessors with high levels of agreement. Factors that
   may increase the reliability of the assessment include; appropriate
   assessor training, the presence of a consensus meeting and clear
   standards. There are opportunities to further improve the clarity of
   standards identified as having the lowest level of agreement.
   Background Evidence highlights that a major factor associated with
   inappropriate wheelchair distribution is the global shortage of
   wheelchair service provision education and training.{[}1-4] The World
   Health Organization Guidelines on the provision of manual wheelchairs in
   less-resourced settingsrecommend integrating wheelchair service
   provision content into existing academic rehabilitation programs.{[}1]
   However, a 2017 study reported limited training time allocated to
   wheelchair service provision in some professional rehabilitation
   programs in low-, middle- and high-income countries.(4) Aim To develop
   an evidence-based online Seating and Mobility Academic Resource Toolkit
   (SMART) through a mixed-methods approach to identify necessary
   components and investigate preliminary impact. Method Data was collected
   from 3 primary data sources, including an international wheelchair
   education survey, qualitative interview transcripts with representatives
   from health care professional university programs and wheelchair
   education integration `pilot sites'. Data sources were analyzed using a
   deductive content analysis approach to produce the Seating and Mobility
   Academic Resource Toolkit (SMART). Subsequent analyses of SMART
   analytics and purposive interviews were performed to identify how the
   toolkit was used in a variety of settings across different types of
   academic rehabilitation programs and contexts. Results We received 72
   responses on our international wheelchair education survey, conducted 14
   qualitative interviews with health care professional university program
   representatives, and hosted 16 presentations from wheelchair education
   integration `pilot sites' to identify integration challenges. Themes
   emerged related to having minimal resources (human, financial, clinical,
   equipment), unawareness of open-source educational materials, and lack
   of trained personnel to name a few. Therefore, SMART was developed and
   includes educational resources (e.g., syllabi, presentations, online
   modules, evidence-based training packages, labs, evaluations) and
   strategies to overcome barriers to integrating wheelchair content into
   curricula. Analytics reveal SMART pages have been accessed 11,703 times
   globally, and since its launch in April 2018, the average number of
   pages viewed per session have increased by 7.4\%. The interview findings
   suggest SMART has potential to improve the wheelchair education provided
   in terms of pedagogic approach, content and resources used. Discussion
   and Conclusion SMART may facilitate the integration of wheelchair
   content into health care professional university curricula. Ultimately,
   improved education in this important area of practice may enhance the
   participation and quality of life of those individuals who use a
   wheelchair for mobility. Prosthetics and orthotics training programs may
   consider using globally available resources, such as SMART, to identify
   strategies to improve wheelchair education and to advocate for their
   implementation.
   Background Nowadays is well known that voice-activated wheelchairs help
   the quadriplegic get around, and houses can be equipped with similar
   systems to operate lights and appliances; but which are the options when
   besides quadriplegic there is no way to give voice commands to the
   wheelchair or to communicate our desires to other people. The first
   approach of this project was presented in the International Workshop on
   Wearable Robotics (WeRob 2014) and a new version in WeRob 2017{[}2]. Aim
   Provide an alternative to quadriplegic persons in order to give commands
   from eye blinking and communicate with outside world through Internet of
   Things (IoT) capabilities incorporated in a wheelchair. Method To
   fulfill the aim of the project, the engineering design method was
   followed which as first step includes the definition of a clear problem
   statement, then pertinent information was gathered in order to generate
   multiple solutions. Afterwards, the information was analyzed, a solution
   was selected, tested and implemented. Two approaches have been generated
   previously (WeRob 2014, 2017), the first one was developed with a neural
   signal reading device and a Bluetooth communication board; afterwards a
   new alternative was evaluated including eye blinking commands. Now, a
   faster microcontroller with IoT capabilities has been incorporated in
   the design process to include IoT capabilities. Results Taking a
   decision considering commands coming from eye blinking, to give mobility
   to a wheelchair, is not a simple task, bad decisions can end up in
   moving a person in a wrong direction which will give more difficulties
   instead of solutions. In the actual study a faster microcontroller with
   embedded software and hardware for IoT is used, this device can manage
   multiple sensors as inputs and multiple actuators as outputs. The
   raspberry Pi 3 was selected because it is single-board computer with
   wireless LAN and Bluetooth Low Energy (BLE) on board. The developed
   system discriminates an involuntary blinking from a low motion voluntary
   blinking and take a decision to move forward a model wheelchair. The
   position and given commands are sent to an IoT platform to save the
   wheelchair movement data. Discussion and Conclusion A single board
   computer with on board wireless LAN and Bluetooth gives more
   possibilities to incorporate IoT capabilities for decision-making about
   wheelchair movement, and besides that voluntary blinking recognition is
   detected with precision and faster. The developed system fulfils the aim
   to provide an alternative to quadriplegic persons in order to give
   commands from eye blinking and communicate with outside world through
   IoT capabilities incorporated in a wheelchair.
   Background Most cervical cord injury (CCI) people need to use a
   wheelchair in daily life {[}1]. Especially, C5-C6 level of CCI patients
   can't usually perform the extension movement of elbow joint so that they
   can't operate the wheelchair strongly. Previously, we have developed the
   motion assist robot, which is named Active Cast (AC) for the upper limbs
   to transmit their residual function around shoulder to hand operations
   {[}2]. Aim The purpose of this study is to clarify the influence of
   wearing AC on the musculoskeletal of their residual function during
   wheelchair operation by people with different symptoms of CCI. Method
   Two subjects with C6B1(right), C5B(left) and C6B2(both) of the Zancolli
   classification were participated in this evaluation experiment. The
   treadmill was set to reproduce the running on a slope. The inclination
   was set to 3\% for the C5 level CCI subject, and 4\% for the C6 level
   CCI subject in order to confirm that the musculoskeletal of their
   residual functions better. We used the 3D motion analysis system and a
   myoelectric potentiometer for the analysis during 20 {[}s] wheelchair
   operation. From the above, the effect of AC on residual muscles of CCI
   subjects with different symptoms was analyzed. Results Based on the
   experimental results without AC, we confirmed that the C5 level subject
   operated the wheelchair using the elbow flexion force, and the C6 level
   subject used the force of the pronation of forearm. On the other hand,
   from the results with AC, in the case of C5 level subject, the scapula
   elevated greatly due to locking the elbow joint, and the amount of
   activity of the upper trapezius muscle also increased comparing with the
   case without AC. In the case of C6 level subject, we confirmed that the
   amount of scapula abduction increased, and the serratus anterior muscle
   was also exerted. Discussion and Conclusion It was revealed that wearing
   AC induced motions and residual muscles around the shoulder of CCI
   subjects with different symptoms. In C5 level CCI, it was confirmed that
   the elevation movement of the scapula was induced, and in the C6 level
   CCI, the abduction motion of the scapula was induced. It was confirmed
   that the effect of AC differs by symptoms with the pronation of forearm.
   Background Patients with complete paraplegia after spinal cord injury
   (SCI) are difficult to walk by themselves. Gait training with
   conventional orthoses for complete paraplegia requires excessive upper
   limb usage and is difficult to train knee extensor during ambulation. We
   developed a new method of HAL using remaining muscle activation as a
   trigger for paralyzed limb, and we called heterotopic HAL method
   (T-HAL). Aim This study aims to describe voluntary gait and voluntary
   knee extension using Heterotopic Triggered HAL method in patients with
   complete quadri/paraplegia after chronic SCI. Method Nine patients,
   20-67 years old, C6-T11, AIS A-B, were enrolled in this study. HAL
   session consisted of two parts: the first session was voluntary
   ambulation using upper limb triggered HAL (UT-HAL method). In addition,
   for cases who could contract hip flexor, the second session was done for
   active knee extension using hip flexor activation. Surface
   electromyography (EMG) was used to evaluate muscle activity of hip
   flexor and quadriceps femoris (Quad) in synchronization with a Vicon
   motion capture system. The modified Ashworth scale (MAS) score was
   evaluated before and after each session. Results There were not seen
   severe complications such as hypoautonomic hyperreflexia, or symptomatic
   venous vein thrombosis, etc. The only observed adverse event was redness
   caused by contact between the skin and harness in cases 1, 2, and 3
   which was avoidable by using a cushion. In three cases, from C6 to T6,
   MAS score significantly decreased after each session. In all cases, EMG
   before the intervention showed no activation in either Quad. However,
   periodic activation of the lower limb muscles was seen during UT-HAL
   ambulation. In two cases, in T10-11, there was observed active
   contraction in both Quads and hip flexors after intervention. Discussion
   and Conclusion We focused on residual upper limb muscle activity and the
   coordination between the upper limbs and contralateral lower limbs in
   natural gait. We observed neurological positive changes in two cases
   after the intervention. Moreover, patients with cervical cord or upper
   thoracic cord injuries, who have difficulty performing conventional gait
   training with orthoses, may experience decreased spasticity and
   activation of paralyzed muscles. These findings suggest that T-HAL
   method is a feasible option for rehabilitation for complete
   quadri/paraplegia with chronic SCI.
   Background Using a seat cushion, we feel the situation where the
   protrusion of the ischium hits the bottom of the cushion. At the
   pressure measuring time, the phenomenon that the peak pressure value
   gradually increases is observed. In prevention of pressure sores, it is
   important to elucidate the mechanism of bottoming phenomenon. The
   pressure distribution response characteristics when continuously using
   various seat cushions were studied in the human body and human body
   model. Aim It was aimed to investigate whether the bottom was due to a
   change in the condition in the seat cushion or a change in the internal
   pressure in the human body. Method Seven seat cushions were prepared on
   the wheelchair. Subjects were two healthy adult male and a biological
   model. The biological model was made from plastic bone specimens and
   silicone materials (170 cm, 32 kg). Each cushion was placed on the plate
   seating surface of a wheelchair (Netti em, Alu Rehab AS) set at a tilt
   angle of 0 degrees and a reclining angle of 95 degrees, and a pressure
   distribution measuring device was laid on top(Xsensor X 3 Pro, Xsensor
   Technology Corp.). Measurements were carried out, with the human body
   and human body model sitting still at the pelvis intermediate position,
   randomly on each seat cushion (3 minutes / 4861 frames). Results In each
   seat cushion, there was a gradual trend of peak pressure after sitting
   of the human body. On the other hand, this tendency was not observed in
   the human body model. The difference between the maximum and the minimum
   value of the peak pressure was significantly higher for each human body
   than for the human body model. There was no significant difference
   between the maximum and minimum values of the contact area between human
   body and human body model. Discussion and Conclusion Local concentration
   of pressure can not be said to be caused by sinking of humans in the
   seat cushion or compression of the material. Rather, it is presumed that
   biological changes occur over time. If compressive stress affects blood
   flow, in order to distribute body pressure, wheelchair users are
   required to improve the movement such as push-up and use the wheelchair
   posture conversion function.
   Background We are developing Qolo, a new personal mobility device for
   those with motor disability in their lower limbs. It assists
   sit-to-stand and stand-to-sit postural transitions as well as navigation
   in standing posture with hands-free operation. Its mechanism to assist
   the postural transition is implemented with passive gas springs without
   using electric actuators, making it compact, light-weight and low cost.
   Aim The purpose of this study is to report clinical assessment of the
   device after modification of its design to realize voluntary
   sit-to-stand posture transition of people with thoracic level spinal
   cord injury (SCI). Method In the improved design of the device,
   reviewing our preceding clinical experiments, the assistive force of the
   knee springs was doubled and lumbar assist was added. Six participants
   with SCI (age: 30-52y, 4 males and 3 females, neurological level: T4-L3,
   AIS: A-C, MMT Hip Ext.: 0-1, Knee Ext.: 0-3) were asked to conduct
   stand-up and sit-down postural transitions using the improved version of
   the device. Feasibility of the assisted motions were evaluated. Results
   Six out of the seven participants were able to smoothly conduct
   voluntary stand-up and sit-down motions using the device by themselves,
   without additional assistance. Two of the them (T10A, T6A, MMT Knee<=1)
   achieved voluntary stand-up motion only with the improved version of the
   device. One participant (T4A, MMT Knee=0) needed additional manual
   assistance to achieve voluntary stand-up motion using the device.
   Discussion and Conclusion The improvement of the device extended its
   capability to assist sit-to-stand and stand-to-sit posture transition of
   people with lumbar or thoracic level SCI. At the same time, necessity of
   stronger support for middle to upper thoracic level SCI was
   investigated. For the next step, we plan to introduce stronger assistive
   force, and improve of harnessing and attachment and detachment processes
   to facilitate its daily use.
   Background Gait enables human beings to move forward, and most people
   consider gait to be natural skill. But gait disturbances are very
   prevalent to the patients with burn injury. A major cause of functional
   impairment including pain and joint contractures. Contractures at the
   lower extremities such as the hip, knee and ankle significantly limit
   gait. Recent attention of the application of artificial intelligence in
   rehabilitation, in particularly lower limbs robot assisted
   rehabilitation has become a hot issue of studies. Aim This study is
   purposed to find the efficacy and study the mechanism of motor recovery
   after robot assisted gait training on patients with lower extremity
   burn. Method 7 patients with lower extremity burn were included. All
   patients received 4 weeks robot assisted training. Functional scores of
   functional ambulation category (FAC) and 6-minute walking distances, and
   pain score of visual analog score (VAS) during robot assisted gait
   training were measured. Results FAC and VAS scores after training were
   better than the scores before training. 6-min walking distance after
   training were significantly increased than the measure before
   training(p<0.05). Lower limbs robot assisted rehabilitation training
   improves patient's pain, and increases walking speed on patients with
   lower extremity burn. Discussion and Conclusion Lower limbs robot
   assisted rehabilitation improves the walking speed, and prolongs their
   walking distances. The use of robot assisted rehabilitation may
   facilitate early recovery from burn injury.
   Background The infant cerebral palsy is one of the most wide-spread
   reasons of disability at child's age. The success of rehabilitation
   depends on intensity of kinesitherapy, quality of movements and
   motivation of the child that is ensured with modern robotized systems
   with biological feedback. Aim to estimate efficiency of use of robotized
   systems with biological feedback in rehabilitation of children with
   infant cerebral palsy. Method The basic and control groups of children
   (26 persons in each) with spastic paraparesis, flexion positioning of
   lower extremities, impairment of supportability and walking, matched in
   functions, age and gender were formed. The course of rehabilitation
   included: massage, apparatus physiotherapy, therapeutic exercises. The
   patients of the basic group additionally had mechanotherapy on a
   robotized system ``Lokomat Pediatric{''} for training strength of
   muscle, for increasing of amount of movements in joints. The period of
   observation - 1 year (3 courses of treatment). Efficiency of the given
   treatment was evaluated with a method of functional testing (Ashworth,
   GMFCS, GMFM), program testing of a robotized system (L-FORCE, L-ROM)
   before and after the course of rehabilitation. Results For the period of
   observation the spasticity of muscles (Ashworth) of lower extremities in
   patients of both groups reduced, on the average, for 1 point. The level
   of limitation of the gross motor functions (GMFCS) in both groups didn't
   change essentially. Motor possibilities (GMFM) of the patients of the
   basic group increased, on the average, on 25 +/- 7 \%, of the control
   group - on 12 +/- 7 \%. The isometric strength of muscles - flexors of a
   knee and a hip (L-FORCE) in the basic group increased, on the average,
   on 3 Nm, the strength of muscles-extensors of a knee and a hip - on 2
   Nm, in the control group - didn't change. The amount of active movements
   in knee and hip joints (L-ROM) achieved physiological norm in the basic
   group and in 25 \% of children of the control group. Discussion and
   Conclusion The mechanotherapy on a robotized system with biological
   feedback ``Lokomat Pediatric{''} in rehabilitation of children with
   infant cerebral palsy allows to increase the strength of muscles
   trained, amount of movements in joints, results in improvement of
   indexes of functional testing, to increase motivation of children for
   performing active exercises in game. Motor rehabilitation of patients
   with infant cerebral palsy with the use of mechanotherapy on a robotized
   system ``Lokomat Pediatric{''} has shown high efficiency in comparison
   with traditional methods.},
DOI = {10.1177/0309364619883197},
ISSN = {0309-3646},
EISSN = {1746-1553},
ResearcherID-Numbers = {de Jager, Aimee/AAL-1264-2021},
Unique-ID = {WOS:000497156400001},
}

@article{ WOS:000464744000008,
Author = {Wang, Ziwei and Azar, Ehsan Rezazadeh},
Title = {BIM-based draft schedule generation in reinforced concrete-framed
   buildings},
Journal = {CONSTRUCTION INNOVATION-ENGLAND},
Year = {2019},
Volume = {19},
Number = {2},
Pages = {280-294},
Month = {APR 17},
Abstract = {Purpose Project schedules have a vital role in the effective management
   of time, cost, scope and resources in construction projects, and
   creating schedules requires schedulers with construction knowledge and
   experience. The increase in the complexity of building projects and the
   emergence of building information modeling (BIM) in the architecture,
   engineering and construction industry have encouraged researchers to
   explore BIM capabilities for automated schedule generation. The scope
   and capabilities of the developed systems, however, are limited and the
   link between design and scheduling is still underdeveloped. This paper
   aims to investigate methods to develop a BIM-based framework to
   automatically generate schedules for concrete-framed buildings.
   Design/methodology/approach This system first extracts the required data
   from the building information model, including elements' dimensions,
   quantities, spatial information, materials and other related attributes.
   It then applies construction rules, prior knowledge and production rate
   data to create project work-packages, calculate their durations and
   determine their relationships. Finally, it organizes these results into
   a schedule using project management software. Findings This system
   provides an automated and easy-to-use approach to generate schedules for
   concrete-framed buildings that are modeled in a BIM platform. It
   provides two schedules for each project, both a sequential and an
   overlapped solution, which the schedulers can modify into a practical
   schedule based on conditions and available resources. Originality/value
   This research project presents an innovative approach to use BIM-based
   attributes of structural elements to develop list of work-packages and
   estimate their durations, and then it uses a combination of rule-based
   and case-based reasoning to generate the schedules.},
DOI = {10.1108/CI-11-2018-0094},
ISSN = {1471-4175},
EISSN = {1477-0857},
ORCID-Numbers = {Rezazadeh Azar, Ehsan/0000-0002-9711-2679},
Unique-ID = {WOS:000464744000008},
}

@article{ WOS:000473691900011,
Author = {Wiegreffe, Daniel and Alexander, Daniel and Stadler, Peter F. and
   Zeckzer, Dirk},
Title = {RNApuzzler: efficient outerplanar drawing of RNA-secondary structures},
Journal = {BIOINFORMATICS},
Year = {2019},
Volume = {35},
Number = {8},
Pages = {1342-1349},
Month = {APR 15},
Abstract = {Motivation: RNA secondary structure is a useful representation for
   studying the function of RNA, which captures most of the free energy of
   RNA folding. Using empirically determined energy parameters, secondary
   structures of nucleic acids can be efficiently computed by recursive
   algorithms. Several software packages supporting this task are readily
   available. As RNA secondary structures are outerplanar graphs, they can
   be drawn without intersection in the plane. Interpretation by the
   practitioner is eased when these drawings conform to a series of
   additional constraints beyond outerplanarity. These constraints are the
   reason why RNA drawing is difficult. Many RNA drawing algorithms
   therefore do not always produce intersection-free (outerplanar)
   drawings.
   Results: To remedy this shortcoming we propose here the RNApuzzler
   algorithm which is guaranteed to produce intersection-free drawings. It
   is based on a drawing algorithm respecting constraints based on
   nucleotide distances (RNAturtle). We investigate relaxations of these
   constraints allowing for intersection-free drawings. Based on these
   relaxations, we implemented a fully automated, simple, and robust
   algorithm that produces aesthetic drawings adhering to previously
   established guidelines. We tested our algorithm using the RFAM database
   and found that we can compute intersection-free drawings of all RNAs
   therein efficiently.
   Availability and implementation: The software can be accessed freely at:
   https://github.com/dwiegreffe/RNApuzzler.},
DOI = {10.1093/bioinformatics/bty817},
ISSN = {1367-4803},
EISSN = {1460-2059},
ResearcherID-Numbers = {Wiegreffe, Daniel/AAJ-7722-2020
   Stadler, Peter F./L-7857-2015},
ORCID-Numbers = {Wiegreffe, Daniel/0000-0002-5356-6732
   Stadler, Peter F./0000-0002-5016-5191},
Unique-ID = {WOS:000473691900011},
}

@article{ WOS:000457339000002,
Author = {Sutariya, Pinkesh G. and Soni, Heni and Gandhi, Sahaj A. and Pandya,
   Alok},
Title = {Novel luminescent paper based calix{[}4]arene chelation enhanced
   fluorescence- photoinduced electron transfer probe for Mn2+, Cr3+ and F-},
Journal = {JOURNAL OF LUMINESCENCE},
Year = {2019},
Volume = {208},
Pages = {6-17},
Month = {APR},
Abstract = {A novel structurally simple calix{[}4]arene attached
   1-aminoanthraquinone associated lower rim calix{[}4]arene conjugate was
   synthesized and has been used as turn on/off/on fluorescence probe for
   Mn2+, Cr3+ and F-. This chelation enhanced fluorescence - photoinduced
   electron transfer (CHEF-PET) based TAAC probe has been applied for its
   analytical application in real samples such as Mn2+ from blood serum,
   Cr3+ and F- from industrial effluent with 94-99\% recovery. The limit of
   detection of this sensor is found to be 11 nM for Mn2+, 4 nM for Cr3+
   and 19 nM for F- with the concentration range of 0-120 nM. Further, we
   report an easy-to-use, low cost and disposable paper-based sensing
   device for rapid chemical screening of Mn2+, Cr3+ and F-. The device
   comprises fluorescent sensing probes embedded into a nitrocellulose
   matrix where the resonance energy transfer phenomenon seems to be the
   sensing mechanism. It opens up new opportunities for simple and fast
   screening in remote settings where sophisticated instrumentation is not
   always available. The MOPAC-2016 software package has been used to
   optimize the TAAC using PM7 well established method and calculates the
   HOMO-LUMO energy band gap for structure TAAC and TAAC with Mn2+, Cr3+
   and F- ion based structures.},
DOI = {10.1016/j.jlumin.2018.12.009},
ISSN = {0022-2313},
EISSN = {1872-7883},
ResearcherID-Numbers = {Sutariya, Dr. Pinkeshkumar/L-9648-2015
   },
ORCID-Numbers = {Sutariya, Dr. Pinkeshkumar/0000-0003-0750-903X
   Soni, Heni/0000-0003-3548-3308},
Unique-ID = {WOS:000457339000002},
}

@article{ WOS:000461008800002,
Author = {Shukurov, Anvar and Rodrigues, Luiz Felippe S. and Bushby, Paul J. and
   Hollins, James and Rachen, Jorg P.},
Title = {A physical approach to modelling large-scale galactic magnetic fields},
Journal = {ASTRONOMY \& ASTROPHYSICS},
Year = {2019},
Volume = {623},
Month = {MAR 13},
Abstract = {Context. A convenient representation of the structure of the large-scale
   galactic magnetic field is required for the interpretation of
   polarization data in the sub-mm and radio ranges, in both the Milky Way
   and external galaxies.
   Aims. We develop a simple and flexible approach to construct
   parametrised models of the large-scale magnetic field of the Milky Way
   and other disc galaxies, based on physically justifiable models of
   magnetic field structure. The resulting models are designed to be
   optimised against available observational data.
   Methods. Representations for the large-scale magnetic fields in the
   flared disc and spherical halo of a disc galaxy were obtained in the
   form of series expansions whose coefficients can be calculated from
   observable or theoretically known galactic properties. The functional
   basis for the expansions is derived as eigenfunctions of the mean-field
   dynamo equation or of the vectorial magnetic diffusion equation.
   Results. The solutions presented are axially symmetric but the approach
   can be extended straightforwardly to non-axisymmetric cases. The
   magnetic fields are solenoidal by construction, can be helical, and are
   parametrised in terms of observable properties of the host object, such
   as the rotation curve and the shape of the gaseous disc. The magnetic
   field in the disc can have a prescribed number of field reversals at any
   specified radii. Both the disc and halo magnetic fields can separately
   have either dipolar or quadrupolar symmetry. The model is implemented as
   a publicly available software package GALMAG which allows, in
   particular, the computation of the synchrotron emission and Faraday
   rotation produced by the model's magnetic field.
   Conclusions. The model can be used in interpretations of observations of
   magnetic fields in the Milky Way and other spiral galaxies, in
   particular as a prior in Bayesian analyses. It can also be used for a
   simple simulation of a time-dependent magnetic field generated by dynamo
   action.},
DOI = {10.1051/0004-6361/201834642},
Article-Number = {A113},
ISSN = {1432-0746},
ResearcherID-Numbers = {Rachen, Jörg Paul/AAR-4369-2020
   Rodrigues, Luiz Felippe S./J-4311-2015
   Shukurov, Anvar/AAH-4929-2019
   },
ORCID-Numbers = {Rachen, Jörg Paul/0000-0002-6310-3889
   Shukurov, Anvar/0000-0001-6200-4304
   Santiago Rodrigues, Luiz Felippe/0000-0002-3860-0525
   Bushby, Paul/0000-0002-4691-6757},
Unique-ID = {WOS:000461008800002},
}

@article{ WOS:000457728500003,
Author = {Iglesias, I. and Venancio, S. and Pinho, J. L. and Avilez-Valente, P.
   and Vieira, J. M. P.},
Title = {Two Models Solutions for the Douro Estuary: Flood Risk Assessment and
   Breakwater Effects},
Journal = {ESTUARIES AND COASTS},
Year = {2019},
Volume = {42},
Number = {2},
Pages = {348-364},
Month = {MAR},
Abstract = {Estuarine floods are one of the most harmful and complex extreme events
   occurring in coastal environments. To predict the associated effects,
   characterize areas of risk, and promote population safety, numerical
   modeling is essential. This work performs a comparison and a combination
   of two 2-dimensional depth-averaged estuarine models (based on
   openTELEMAC-MASCARET and Delft3D hydrodynamic software packages), to
   develop a two-model ensemble approach that will improve forecast
   robustness when compared to a one-model approach. The ensemble was
   applied to one of the main Portuguese estuaries, the Douro river
   estuary, to predict the expected water levels associated with extreme
   river discharges in the present-day configuration with the new
   breakwaters. This is a region that is periodically under heavy flooding,
   which entails economic losses and damage to protected landscape areas
   and hydraulic structures. Both models accurately simulated water levels
   and currents for tidal- and flood-dominated validation simulations, with
   correlation values close to 1, RMSE below 15\%, and small Bias and Skill
   coefficient close to 1. The two-model ensemble results revealed that the
   present-day estuarine mouth configuration will produce harsher effects
   for the riverine populations in case identical historical river floods
   take place. This is mainly due to the increase in the area and volume of
   the estuary's sand spit related to the construction of the new
   breakwaters.},
DOI = {10.1007/s12237-018-0477-5},
ISSN = {1559-2723},
EISSN = {1559-2731},
ResearcherID-Numbers = {Iglesias, Isabel/G-8407-2015
   Avilez-Valente, Paulo/A-3452-2008
   Pinho, Jose/A-7183-2013},
ORCID-Numbers = {Iglesias, Isabel/0000-0002-4900-135X
   Avilez-Valente, Paulo/0000-0002-2562-6603
   Pinho, Jose/0000-0003-2070-8009},
Unique-ID = {WOS:000457728500003},
}

@article{ WOS:000462236000007,
Author = {Lenhard, Jorg and Blom, Martin and Herold, Sebastian},
Title = {Exploring the suitability of source code metrics for indicating
   architectural inconsistencies},
Journal = {SOFTWARE QUALITY JOURNAL},
Year = {2019},
Volume = {27},
Number = {1},
Pages = {241-274},
Month = {MAR},
Abstract = {Software architecture degradation is a phenomenon that frequently occurs
   during software evolution. Source code anomalies are one of the several
   aspects that potentially contribute to software architecture
   degradation. Many techniques for automating the detection of such
   anomalies are based on source code metrics. It is, however, unclear how
   accurate these techniques are in identifying the architecturally
   relevant anomalies in a system. The objective of this paper is to shed
   light on the extent to which source code metrics on their own can be
   used to characterize classes contributing to software architecture
   degradation. We performed a multi-case study on three open-source
   systems for each of which we gathered the intended architecture and data
   for 49 different source code metrics taken from seven different code
   quality tools. This data was analyzed to explore the links between
   architectural inconsistencies, as detected by applying reflexion
   modeling, and metric values indicating potential design problems at the
   implementation level. The results show that there does not seem to be a
   direct correlation between metrics and architectural inconsistencies.
   For many metrics, however, classes more problematic as indicated by
   their metric value seem significantly more likely to contribute to
   inconsistencies than less problematic classes. In particular, the
   fan-in, a classes' public API, and method counts seem to be suitable
   indicators. The fan-in metric seems to be a particularly interesting
   indicator, as class size does not seem to have a confounding effect on
   this metric. This finding may be useful for focusing code restructuring
   efforts on architecturally relevant metrics in case the intended
   architecture is not explicitly specified and to further improve
   architecture recovery and consistency checking tool support.},
DOI = {10.1007/s11219-018-9404-z},
ISSN = {0963-9314},
EISSN = {1573-1367},
ResearcherID-Numbers = {Herold, Sebastian/N-3557-2015
   },
ORCID-Numbers = {Herold, Sebastian/0000-0002-3180-9182
   Lenhard, Jorg/0000-0002-0107-2108},
Unique-ID = {WOS:000462236000007},
}

@article{ WOS:000462890300019,
Author = {Martinengo, Laura and Yeo, Natalie Jia Ying and Tang, Zheng Qiang and
   Markandran, Kasturi D. O. and Kyaw, Bhone Myint and Car, Lorainne Tudor},
Title = {Digital Education for the Management of Chronic Wounds in Health Care
   Professionals: Protocol for a Systematic Review by the Digital Health
   Education Collaboration},
Journal = {JMIR RESEARCH PROTOCOLS},
Year = {2019},
Volume = {8},
Number = {3},
Month = {MAR},
Abstract = {Background: Digital education is ``the act of teaching and learning by
   means of digital technologies.{''} Digital education comprises a wide
   range of interventions that can be broadly divided into offline digital
   education, online digital education, digital game-based learning,
   massive open online courses (MOOCs), psychomotor skills trainers,
   virtual reality environments, virtual patient simulations, and
   m-learning. Chronic wounds pose an immense economic and psychosocial
   burden to patients and the health care system, as caring for them
   require highly specialized personnel. Current training strategies face
   significant barriers, such as lack of time due to work commitments,
   distance from provider centers, and costs. Therefore, there is an
   increased need to synthesize evidence on the effectiveness of digital
   education interventions on chronic wounds management in health care
   professionals.
   Objective: Our main objective is to assess the effectiveness of digital
   education as a stand-alone approach or as part of a blended-learning
   approach in improving pre- and postregistration health care
   professionals' knowledge, attitudes, practical skills, and behavior in
   the management of chronic wounds, as well as their satisfaction with the
   intervention. Secondary objectives are to evaluate patient-related
   outcomes, cost-effectiveness of the interventions, and any unfavorable
   or undesirable outcomes that may arise.
   Methods: This systematic review will follow the methodology as described
   in the Cochrane Handbook for Systematic Reviews of Interventions. As our
   systematic review is one of a series of reviews on digital education for
   health professionals' education, we will use a previously developed
   search strategy. This search includes the following databases: the
   Cochrane Central Register of Controlled Trials (CENTRAL) (Cochrane
   Library), MEDLINE (Ovid), Embase (Ovid), Web of Science, the Educational
   Resource Information Centre (ERIC) (Ovid), PsycINFO (Ovid), the
   Cumulative Index to Nursing and Allied Health Literature (CINAHL)
   (EBSCO), the ProQuest Dissertation and Theses database, and trial
   registries. Databases will be searched for studies published from
   January 1990 to August 2018. Two independent reviewers will screen the
   library for included studies. We will describe the screening process
   using a flowchart as per the Preferred Reporting Items for Systematic
   Reviews and Meta-Analyses (PRISMA) guidelines. We will extract the data
   using a previously developed, structured data extraction form. Included
   studies will be quality-assessed using the Risk of Bias tool from
   Cochrane. We will narratively summarize the data and, if possible, we
   will conduct a meta-analysis. We will use Cochrane's RevMan 5.3 software
   for data analysis.
   Results: We have completed the screening of titles and abstracts for
   this systematic review and are currently selecting papers against our
   inclusion and exclusion criteria through full-text revision. We are
   expecting to complete our review by the end of April 2019.
   Conclusions: This systematic review will provide an in-depth analysis of
   digital education strategies to train health care providers in the
   management of chronic wounds. We consider this topic particularly
   relevant given the current challenges facing health care systems
   worldwide, including shortages of skilled personnel and a steep increase
   in the population of older adults as a result of a prolonged life
   expectancy.},
DOI = {10.2196/12488},
Article-Number = {e12488},
ISSN = {1929-0748},
ResearcherID-Numbers = {Martinengo, Laura/AAY-8201-2020
   Car, Lorainne Tudor/E-1205-2017
   },
ORCID-Numbers = {Martinengo, Laura/0000-0003-3539-7207
   Car, Lorainne Tudor/0000-0001-8414-7664
   Yeo, Natalie/0000-0002-1158-1432
   Markandran, Kasturi/0000-0002-4522-7407
   Kyaw, Bhone Myint/0000-0002-1750-0330},
Unique-ID = {WOS:000462890300019},
}

@article{ WOS:000474861800012,
Author = {Pibiri, Giulio Ermanno and Venturini, Rossano},
Title = {Handling Massive N-Gram Datasets Efficiently},
Journal = {ACM TRANSACTIONS ON INFORMATION SYSTEMS},
Year = {2019},
Volume = {37},
Number = {2},
Month = {MAR},
Abstract = {Two fundamental problems concern the handling of large n-gram language
   models: Indexing, that is, compressing the n-grams and associated
   satellite values without compromising their retrieval speed, and
   estimation, that is, computing the probability distribution of the
   n-grams extracted from a large textual source.
   Performing these two tasks efficiently is vital for several applications
   in the fields of Information Retrieval, Natural Language Processing, and
   Machine Learning, such as auto-completion in search engines and machine
   translation.
   Regarding the problem of indexing, we describe compressed, exact, and
   lossless data structures that simultaneously achieve high space
   reductions and no time degradation with respect to the state-of-the-art
   solutions and related software packages. In particular, we present a
   compressed trie data structure in which each word of an n-gram following
   a context of fixed length k, that is, its preceding k words, is encoded
   as an integer whose value is proportional to the number of words that
   follow such context. Since the number of words following a given context
   is typically very small in natural languages, we lower the space of
   representation to compression levels that were never achieved before,
   allowing the indexing of billions of strings. Despite the significant
   savings in space, our technique introduces a negligible penalty at query
   time.
   Specifically, the most space-efficient competitors in the literature,
   which are both quantized and lossy, do not take less than our trie data
   structure and are up to 5 times slower. Conversely, our trie is as fast
   as the fastest competitor but also retains an advantage of up to 65\% in
   absolute space.
   Regarding the problem of estimation, we present a novel algorithm for
   estimating modified Kneser-Ney language models that have emerged as the
   de-facto choice for language modeling in both academia and industry
   thanks to their relatively low perplexity performance. Estimating such
   models from large textual sources poses the challenge of devising
   algorithms that make a parsimonious use of the disk.
   The state-of-the-art algorithm uses three sorting steps in external
   memory: we show an improved construction that requires only one sorting
   step by exploiting the properties of the extracted n-gram strings. With
   an extensive experimental analysis performed on billions of n-grams, we
   show an average improvement of 4.5 times on the total runtime of the
   previous approach.},
DOI = {10.1145/3302913},
Article-Number = {25},
ISSN = {1046-8188},
EISSN = {1558-2868},
ORCID-Numbers = {Pibiri, Giulio Ermanno/0000-0003-0724-7092},
Unique-ID = {WOS:000474861800012},
}

@article{ WOS:000457818700014,
Author = {Robson, Reid C. and Pham, Ba' and Hwee, Jeremiah and Thomas, Sonia M.
   and Rios, Patricia and Page, Matthew J. and Tricco, Andrea C.},
Title = {Few studies exist examining methods for selecting studies, abstracting
   data, and appraising quality in a systematic review},
Journal = {JOURNAL OF CLINICAL EPIDEMIOLOGY},
Year = {2019},
Volume = {106},
Pages = {121-135},
Month = {FEB},
Abstract = {Objectives: The aim of the article was to identify and summarize studies
   assessing methodologies for study selection, data abstraction, or
   quality appraisal in systematic reviews.
   Study Design and Setting: A systematic review was conducted, searching
   MEDLINE, EMBASE, and the Cochrane Library from inception to September 1,
   2016. Quality appraisal of included studies was undertaken using a
   modified Quality Assessment of Diagnostic Accuracy Studies 2, and key
   results on accuracy, reliability, efficiency of a methodology, or impact
   on results and conclusions were extracted.
   Results: After screening 5,600 titles and abstracts and 245 full-text
   articles, 37 studies were included. For screening, studies supported the
   involvement of two independent experienced reviewers and the use of
   Google Translate when screening non-English articles. For data
   abstraction, studies supported involvement of experienced reviewers
   (especially for continuous outcomes) and two independent reviewers, use
   of dual monitors, graphical data extraction software, and contacting
   authors. For quality appraisal, studies supported intensive training,
   piloting quality assessment tools, providing decision rules for poorly
   reported studies, contacting authors, and using structured tools if
   different study designs are included.
   Conclusion: Few studies exist documenting common systematic review
   practices. Included studies support several systematic review practices.
   These results provide an updated evidence-base for current knowledge
   synthesis guidelines and methods requiring further research. (C) 2018
   Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.jclinepi.2018.10.003},
ISSN = {0895-4356},
EISSN = {1878-5921},
ResearcherID-Numbers = {Tricco, Andrea/B-9920-2011
   },
ORCID-Numbers = {Tricco, Andrea/0000-0002-4114-8971
   Thomas, Sonia M./0000-0002-4930-2275
   Page, Matthew/0000-0002-4242-7526},
Unique-ID = {WOS:000457818700014},
}

@article{ WOS:000454945800025,
Author = {Sutariya, Pinkesh G. and Soni, Heni and Gandhi, Sahaj A. and Pandya,
   Alok},
Title = {Single-step fluorescence recognition of As3+, Nd3+ and Br- using
   pyrene-linked calix{[}4]arene: application to real samples,
   computational modelling and paper-based device},
Journal = {NEW JOURNAL OF CHEMISTRY},
Year = {2019},
Volume = {43},
Number = {2},
Pages = {737-747},
Month = {JAN 14},
Abstract = {A new fluorescence sensor, namely, TDPC, which is composed of a pyrene
   group connected to calix{[}4]arene as the fluorogenic unit, has been
   synthesized, characterized and analyzed with regard to its selective
   sensing ability for As3+ (0-150 nM), Nd3+ and Br- (0-120 nM), with
   detection limits of 11.53 nM, 0.65 nM and 11.25 nM, respectively. The
   binding constant (K-s) was found to be 7.842 x 10(8) M-1 for As3+, 6.877
   x 10(8) M-1 for Nd3+ and 6.311 x 10(8) M-1 for Br-. Furthermore, we
   report an easy-to-use, low-cost and disposable paper-based sensing
   device for rapid chemical screening of As3+, Nd3+ and Br-. The device
   comprises a luminescent sensing probe embedded in a cellulose matrix in
   which resonance energy transfer seems to be the sensing mechanism. It
   opens up new opportunities for simple and fast screening in remote
   settings where sophisticated instrumentation is not always available.
   The MOPAC2016 software package was used to optimize TDPC using the
   well-established PM7 method, and the HOMO-LUMO energy band gap was
   calculated for TDPC and TDPC with structures based on As3+, Nd3+ and Br-
   ions. To determine the influence of different metal ions on TDPC, a
   molecular docking study was carried out using HEX software. Furthermore,
   to assess its analytical applicability, the prepared sensor was
   successfully used for the analysis of two different real samples (an
   industrial soil sample for Nd3+ and industrial waste water for As3+ and
   Br-) to validate the proposed method, with recoveries of ions of
   95-99\%.},
DOI = {10.1039/c8nj03506g},
ISSN = {1144-0546},
EISSN = {1369-9261},
ResearcherID-Numbers = {Sutariya, Dr. Pinkeshkumar/L-9648-2015
   },
ORCID-Numbers = {Sutariya, Dr. Pinkeshkumar/0000-0003-0750-903X
   Soni, Heni/0000-0003-3548-3308},
Unique-ID = {WOS:000454945800025},
}

@inproceedings{ WOS:000536018101017,
Author = {Alonso Trigueros, J. M. and Canton, A. and Castrillon, M. and Fox, D. J.
   and Gil, O. and Ortega, D. and Perez, S. and Rosado, E. and
   Vazquez-Gallo, M. J.},
Editor = {Chova, LG and Martinez, AL and Torres, IC},
Title = {3D EXPLORA: A GEOGEBRA BOOK FOR VISUALIZING CURVES AND SURFACES IN 3D},
Booktitle = {13TH INTERNATIONAL TECHNOLOGY, EDUCATION AND DEVELOPMENT CONFERENCE
   (INTED2019)},
Series = {INTED Proceedings},
Year = {2019},
Pages = {1007-1011},
Note = {13th International Technology, Education and Development Conference
   (INTED), Valencia, SPAIN, MAR 11-13, 2019},
Abstract = {The project 3D Explora, supported by the Universidad Politecnica de
   Madrid, originated in 2018 in the interests and concerns of the
   professors participating in the Innovative Educational Group MAMI (MAMI
   is a Spanish acronym from ``Improvement of Mathematical Learning in
   Engineering Education{''}). The main goal of this educational research
   project is to provide an online didactical resource for the
   visualization of curves and surfaces in three-dimensional space. It aims
   to facilitate the understanding of abstract and applied geometrical
   notions, especially for objects that vary dynamically in terms of some
   parameters. The resource is intended for students and professors of
   Engineering and Architecture, areas of knowledge in which geometrical
   intuition is much needed. Although there exist many computational
   resources for visualizing curves and surfaces in 3D, in particular CAD
   (Computer Aided Design) software, generally this software is not focused
   on developing abstract geometrical understanding. On the other hand, in
   many cases a good understanding of the mathematical properties of
   geometrical objects helps in making better use of this sort of software,
   so common in professional work. There are also available
   three-dimensional interactive visualization resources linked to formal
   mathematical descriptions, but they often lack the accompanying
   explanations and references needed to understand them fully. In this
   context, the project 3D Explora aims to supply a self-explanatory online
   visualization resource. The resource consists of a Geogebra book of
   interactive and dynamic geometrical constructions accompanied by
   mathematical descriptions and short explanations suitable for blended
   learning. Geogebra is an open source platform for noncommercial purposes
   with a large community of users sharing apps and designs. It provides
   tools for linear algebra, geometry, statistics, and calculus. The 3D
   Explora digital library is already being successfully employed in
   several math courses taught in degree programs of the Universidad
   Politecnica de Madrid and other universities linked to teachers
   participating in this educational research project.},
ISSN = {2340-1079},
ISBN = {978-84-09-08619-1},
ResearcherID-Numbers = {FOX, DANIEL J. F./E-1712-2013
   Vazquez-Gallo, Maria-Jesus/ABF-6604-2021},
ORCID-Numbers = {FOX, DANIEL J. F./0000-0002-8568-6931
   Vazquez-Gallo, Maria-Jesus/0000-0002-1338-3149},
Unique-ID = {WOS:000536018101017},
}

@inproceedings{ WOS:000553304905023,
Author = {Atanasov, H.},
Editor = {Chova, LG and Martinez, AL and Torres, IC},
Title = {ACCESSING RELIABLE INFORMATION: AN INFORMATION PORTAL ON ARCHIVAL
   RECORDS FOR TRAINING BA STUDENTS IN ARCHIVAL AND DOCUMENTARY STUDIES
   SPECIALTY},
Booktitle = {EDULEARN19: 11TH INTERNATIONAL CONFERENCE ON EDUCATION AND NEW LEARNING
   TECHNOLOGIES},
Series = {EDULEARN Proceedings},
Year = {2019},
Pages = {10572-10579},
Note = {11th International Conference on Education and New Learning Technologies
   (EDULEARN), Palma, SPAIN, JUL 01-03, 2019},
Abstract = {In the context of the information society the profession of archivist
   requires knowledge of new technologies and working in the new
   information environment, which is constantly changing. The profession
   requires new qualities and abilities, and therefore the education should
   change in order to ensure successful realization of the students. In our
   daily activity in huge information flows, the archivist is the one who
   should make the archival information accessible to users. This
   information should be available and user-friendly in an environment of
   large dissemination of false information. It is quite difficult to judge
   to what extent the information on the Internet is reliable and whether
   consumers are aware of the possibility of being misled by false one.
   Among the main goals of the report is to present good practice on the
   creation of Internet portals leading to easier access to archival
   information. As best practice we could underlined the Archival
   Collections Portal (Columbia University Libraries) and Archives Portal
   Europe. The second aim is to show the extent to which the new archival
   methods are available to Bulgarian students through the courses
   ``Information and communication technologies in archives{''} and
   ``Digital archives{''}, which are part of the program educating
   ``Archival and Documentary Studies{''} at the University of Library
   Studies and Information Technologies, Bulgaria, Sofia. The final goal of
   the education is students to be able to acquire a high level
   qualification for the professions of archivist and records manager and
   to prepare them for work with contemporary records and archives of
   state, municipal and private institutions, as well as to ensure their
   competencies to deal with old documents stored in historical archives in
   Bulgaria and abroad.
   The report also will introduce how the students will be involved in
   building an ``Information Portal for Archival-Documentary Heritage of
   the Bulgarian Revival{''}, thus combining the theory with their
   practical training. In this way, students will take part in one
   interdisciplinary fundamental research that will leadto the creation of
   a portal, which will collect, structure and systemise our knowledge on
   archival records from the period of the Bulgarian Revival (18th and 19th
   centuries).
   In conclusion, the benefits of such a combination of theory and practice
   will be presented - students are simultaneously acquainted with new
   technologies - computers, hardware and software, ``smart programs{''},
   some useful theoretical knowledge about the Internet. At the same time,
   they are trained to be classical archivists - to keep information
   unchangeable in time, to promote it and make it fully accessible to
   people. A very crucial role for any modern archivist and archive - to
   provide easy access to true and reliable information through new
   technologies.},
ISSN = {2340-1117},
ISBN = {978-84-09-12031-4},
ResearcherID-Numbers = {Atanasov, Hristiyan/AAW-6367-2020},
ORCID-Numbers = {Atanasov, Hristiyan/0000-0003-3701-0387},
Unique-ID = {WOS:000553304905023},
}

@inproceedings{ WOS:000521746600009,
Author = {Azhevsky, Y. A. and Arutyunyan, G. A.},
Editor = {Mikrin, EA and Rogozin, DO and Aleksandrov, AA and Sadovnichy, VA and Fedorov, IB and Mayorova, VI},
Title = {Application of Composite Energy-absorbing Structures in ``Formula
   Student{''} Racecar},
Booktitle = {XLIII ACADEMIC SPACE CONFERENCE, DEDICATED TO THE MEMORY OF ACADEMICIAN
   S P KOROLEV AND OTHER OUTSTANDING RUSSIAN SCIENTISTS - PIONEERS OF SPACE
   EXPLORATION},
Series = {AIP Conference Proceedings},
Year = {2019},
Volume = {2171},
Note = {43rd Academic Space Conference Dedicated to the Memory of Academician S
   P Korolev and Other Outstanding Russian Scientists - Pioneers of Space
   Exploration - Pioneers of Space Exploration, Bauman Moscow State Tech
   Univ, Moscow, RUSSIA, JAN 28-FEB 01, 2019},
Abstract = {The development of passive safety structures is an essential part of the
   design of any vehicle. The main assignments of energy-absorbing
   structures are to minimize the impact on the passenger and to maintain
   the operability of the main vehicles units. Current study is dedicated
   to the problem of increasing the passive safety level of a racecar by
   using a composite energy-absorbing structure. The front impact energy
   absorber for ``Formula Student{''} racecar that meets the requirements
   of the competition regulations was developed in the scope of current
   research. The geometrical parameters of the construction were selected
   for the further layout within the framework of the racecar design. A
   finite element structural analysis was carried out in the Altair
   Hypercrash software package.},
DOI = {10.1063/1.5133172},
Article-Number = {030006},
ISSN = {0094-243X},
ISBN = {978-0-7354-1918-6},
Unique-ID = {WOS:000521746600009},
}

@article{ WOS:000467756100002,
Author = {Bensalah, Mounir and Elouadi, Abdelmajid and Mharzi, Hassan},
Title = {Overview: the opportunity of BIM in railway},
Journal = {SMART AND SUSTAINABLE BUILT ENVIRONMENT},
Year = {2019},
Volume = {8},
Number = {2, SI},
Pages = {103-116},
Abstract = {Purpose - The authors will give an overview of the railway market, with
   a focus on Morocco, before seeing the challenges to face, before listing
   some benefits of rail links in terms of development, ecology, security,
   space management, etc. The authors will then give an overview of the
   development of BIM, its benefits, risks and issues. The purpose of this
   paper is to verify that the BIM can provide the railway with the tools
   to face some of its challenges and improve its productivity.
   Design/methodology/approach - This paper is part of our research project
   on the integration of BIM in railway, which is the result of a
   partnership between Colas Rail Maroc and the ENSAK of the Ibn Tofail
   University of Kenitra. The objective of this paper is mainly to confirm
   that the integration of BIM with the railway, through a theoretical and
   practical study, can have positive impacts. To do this, our methodology
   consists in studying briefly the development of the railway, the need to
   improve the budgets and schedules of the projects, to increase the
   productivity, before showing the advantages of the BIM in the sector of
   the Architecture, Engineering and Construction (AEC). The study of
   feedback from railway projects (chosen for their date of completion
   -beyond 2014, their size, their geographical situation in several
   countries and for the availability of literature in a new field) will
   confirm the initial hypotheses. Among the projects studied will be a
   project that has been the subject of an article written by the authors
   of this paper. In the discussion of the results, the authors will focus
   on the benefits, risks and limitations of integrating BIMinto the
   railway. In conclusion, the authors are laying the groundwork for future
   research in the field.
   Findings -The cases study discussed in this paper and previous research
   confirms the hypotheses of the literature. The integration of BIM into
   railway projects can have several advantages: collaboration, time
   saving, cost optimization, prevention of conflicts between networks,
   construction before construction, optimization of facility management,
   improvement of the quality of works, prefabrication. They also allowed
   us to illustrate the risks (status and appropriation of the BIM model,
   lack of standardization of versions or software and lack of
   understanding of the basics of schedules and specifications) and
   limitations (lack of feedback, lack of adaptability and convergence of
   tools). These experiences have also shown that the use of BIM is not
   just a technological transition, but a revolution in the project
   management process, which requires several key success factors
   (participation of all, commitment, change management and adoption of the
   collaborative approach). Visualization, collaboration and conflict
   elimination are the three main chapters where the benefits of BIM can be
   organized. In fact, there is a lot of intersection between these
   chapters, but they have been chosen as the main ideas around which all
   the benefits can be better understood. Visualization primarily addresses
   the benefits to an individual and improving one's personal understanding
   as a result of using BIM. The collaboration refers to the cooperative
   action of several team members, which is encouraged and facilitated by
   BIM. Conflict elimination mainly concerns project-related benefits, such
   as conflict reduction, waste, risks, costs and time. For railway
   infrastructure projects, the main purpose of using BIM is to improve the
   design integration process, internal project team communication and
   collision detection to eliminate risk of rehabilitation.
   Research limitations/implications - The application of the BIM process
   in railway infrastructure requires constant improvement. This concerns
   the development of libraries and the models available to all users in
   order to encourage the development of this methodology and,
   consequently, its use of information throughout the life cycle of an
   infrastructure work.
   Practical implications -The case study of real projects incorporating
   BIM confirms the results of the literature review. The benefits of
   integrating BIM into rail projects are multiple and proven: cost
   control, decision support, avoids extra work due to design errors,
   improves detection of interface problems, improves planning of vision,
   help with prefabrication and facility management, etc. Finally, the BIM
   process is able to overcome delays in procedures slowing the development
   of the construction industry in many countries, especially in Morocco,
   because of the slowness of design (or downright bad design).
   Social implications - The integration of BIM into rail is becoming a
   global trend. This integration requires government decisions and a
   maturation of technology and tools. The authorities of some developed
   countries studied (Sweden, UK, France, Germany) in the railways, at
   different stages of implementation, are adopting BIM in the process of
   setting up new railway projects. This political impulse is still behind
   in southern countries, such as Morocco. The trend and the data collected
   indicate an adoption between 2020 and 2030 of BIM in all/some AEC
   projects in developed countries. This will have an impact on other
   countries that will soon be doing the same, especially in the railway
   sector to adopt the BIM.
   Originality/value - As part of the realization of this paper, we
   proceeded to the implementation of an electrical substation as part of
   the project to build 40 electric traction substations built by Colas
   Rail on behalf of ONCF.},
DOI = {10.1108/SASBE-11-2017-0060},
ISSN = {2046-6099},
EISSN = {2046-6102},
ResearcherID-Numbers = {Bensalah, Mounir/AAC-8017-2021},
ORCID-Numbers = {Bensalah, Mounir/0000-0001-6374-0426},
Unique-ID = {WOS:000467756100002},
}

@inproceedings{ WOS:000534477300034,
Author = {Breidokaite, Simona and Stankunas, Gediminas},
Editor = {Adliene, D},
Title = {MCNP MODEL OF MEDICAL LINEAR ACCELERATOR TREATMENT HEAD},
Booktitle = {MEDICAL PHYSICS IN THE BALTIC STATES},
Series = {Medical Physics in the Baltic States},
Year = {2019},
Pages = {140-145},
Note = {14th International Conference on Medical Physics, Kaunas, LITHUANIA, NOV
   07-09, 2019},
Organization = {Kaunas Univ Technol; Lund Univ, Skane Univ Hosp; Med Physicists Soc;
   Lithuanian Univ Hlth Sci Kauno klinikos, Univ Hosp; Rentgenas; LRSD,
   IRPA; Radiacines Saugos Centras; C RAD; Graina; Medicinos Fizika;
   Tradintek},
Abstract = {This work analyzes the construction of linear accelerator treatment
   head. Ways of photon interaction with the material heads' material are
   discussed, such as: photoelectric effect, Compton effect, pair creation
   as well as photon absorption properties of the materials used in the
   treatment head structures. MCNP-VIS and MCNP6 software packages are
   proved to be capable of describing the necessary Linac treatment head
   model aspects such as: description of geometry, source information,
   libraries used to describe cross-sections of photon interactions, and
   factors of flux to dose power.},
ISSN = {1822-5721},
ResearcherID-Numbers = {Stankunas, Gediminas/AAD-1781-2019},
ORCID-Numbers = {Stankunas, Gediminas/0000-0002-4996-4834},
Unique-ID = {WOS:000534477300034},
}

@inproceedings{ WOS:000467286000141,
Author = {Bukhari, JanFizza and Yoon, Wonyong},
Editor = {ZafarUzZaman, M},
Title = {Simulated view of SDN based multicasting over D2D enabled heterogeneous
   cellular networks},
Booktitle = {PROCEEDINGS OF 2019 16TH INTERNATIONAL BHURBAN CONFERENCE ON APPLIED
   SCIENCES AND TECHNOLOGY (IBCAST)},
Series = {International Bhurban Conference on Applied Sciences and Technology},
Year = {2019},
Pages = {926-929},
Note = {16th International Bhurban Conference on Applied Sciences and Technology
   (IBCAST), Islamabad, PAKISTAN, JAN 08-12, 2019},
Abstract = {In order to provide high quality multimedia services to a number of
   cellular users in a heterogeneous network, multicasting is a promising
   mechanism transmitting the content in a spectrally efficient way. To
   further enhance the heterogeneous spectrum utilization, Device-to-Device
   (D2D) communication can be enabled which offers wireless peer-to-peer
   services, reducing the traffic stress at the backhaul. However, the
   delivery of multicast content is not reliable enough to achieve
   effective throughput. We introduce OpenFlow based Software-defined
   Networking (SDN) technology in heterogeneous networks for reliable
   dissemination of multicast content to interested D2D users. It provides
   simplified and efficient multicast signaling due to well-organized
   centralized construction of multicast trees, multicast group membership
   management and central allocation of spectral resources. In this paper,
   we present SDN based heterogeneous LTE/WLAN network architecture that
   logically supports D2D enabled flexible multicast communication where
   direct links are established among proximate users. We simulate
   heterogeneous network architecture in NS3 simulator and test the
   provisioning of multicast video frames to real smartphones by jointly
   exploiting network information collection and network abstraction with
   the help of SDN. In order to achieve that, we connect our simulation
   testbed with a real video server to traverse the video packets to
   smartphones over the simulated LTE/WLAN network controlled by a real SDN
   controller. The functionality of our testbed is verified by reliably
   delivering video to connected smartphones through LTE and WLAN. The D2D
   users can fully enjoy flexible transmission of multicast video content
   with less multicast delay (due to neighboring users) and control
   overhead (due to SDN).},
ISSN = {2151-1403},
ISBN = {978-1-5386-7729-2},
Unique-ID = {WOS:000467286000141},
}

@inproceedings{ WOS:000562099100032,
Author = {Duarte, Carlos and Morais, Antonio},
Book-Group-Author = {IOP},
Title = {Paving the Way to NZEB on two Historical Blocks in Lisbon Pombaline
   Quarter},
Booktitle = {4TH WORLD MULTIDISCIPLINARY CIVIL ENGINEERING-ARCHITECTURE-URBAN
   PLANNING SYMPOSIUM - WMCAUS},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2019},
Volume = {603},
Note = {4th World Multidisciplinary Civil Engineering-Architecture-Urban
   Planning Symposium ( WMCAUS), Prague, CZECH REPUBLIC, JUN 17-21, 2019},
Organization = {LAMA Energy Grp; LAMA Gas \& Oil; Prague City Tourism},
Abstract = {The 1758 Lisbon Pombaline Quarter reconstruction plan is made of compact
   rectangular shaped residential blocks, built with a system that complies
   solid mass construction elements with a light wooden structure. If we
   take into consideration both constructive and architectural inherent
   features of the original Pombaline block, it shows the potential to
   achieve NZEB level if an energy retrofit strategy at a block scale is
   implemented, instead of the usual single building or fraction approach.
   The retrofit of historic buildings has raised questions regarding
   interventions depth and efficiency, as the impact on the built heritage
   value has to be residual or null while energy-related improvements must
   be noticeable. With this in mind, this paper intends to analyse and
   compare the result on energy demand and primary energy consumption of
   passive, active and BIST/PV systems packages implementation on two
   original blocks, with the challenge of minimizing the impact on case
   studies appearance. A Building Energy Simulation methodology is applied
   using the whole-of-building dynamic simulation software EnergyPlus. The
   results show that exterior envelope improvements can reduce up to 50\%
   heating and cooling energy demands increasing thermal comfort at the
   same time. Finally, a combined VRF/Biomass heating solution display the
   best results on primary energy consumption while photovoltaic and solar
   thermal systems proved to have an essential role to achieve NZEB
   performance.},
DOI = {10.1088/1757-899X/603/2/022032},
Article-Number = {022032},
ISSN = {1757-8981},
ORCID-Numbers = {Duarte, Carlos/0000-0001-7952-4402},
Unique-ID = {WOS:000562099100032},
}

@inproceedings{ WOS:000571051500020,
Author = {Faria, Heitor and Solis, Priscila and Bordim, Jarcir and Hagstrom,
   Rodrigo},
Editor = {Munoz, VM and Ferguson, D and Helfert, M and Pahl, C},
Title = {A Backup-as-a-Service (BaaS) Software Solution},
Booktitle = {CLOSER: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON CLOUD
   COMPUTING AND SERVICES SCIENCE},
Year = {2019},
Pages = {225-232},
Note = {9th International Conference on Cloud Computing and Services Science
   (CLOSER), Heraklion, GREECE, MAY 02-04, 2019},
Abstract = {Backup is a replica of any data that can be used to restore its original
   form. However, the total amount of digital data created worldwide more
   than doubles every two years and is expected to reach 44 trillions of
   gigabytes in 2020, bringing constant new challenges to backup processes.
   Enterprise backup is one of the oldest and most performed tasks by
   infrastructure and operations professionals. Still, most backup systems
   have been designed and optimized for outdated environments and use
   cases. That fact, generates frustration over currently backup challenges
   and leads to a greater willingness to modernize and to consider new
   technologies. Traditional backup and archive solutions are no longer
   able to meet users current needs. The ideal modern backup and recovery
   software should not only provide features to attend a traditional data
   center, but also allow the integration and exploration of the growing
   Cloud, including ``backup client as a service{''} and ``backup storage
   as a service{''}. The present study proposed and deploys a Backup as a
   Service software solution. For that, the cloud/backup parameters, cloud
   backup challenges, researched architectures and Backup-as-a-Service
   (BaaS) system requirements are specified. Then, a selected set of BaaS
   desired features are developed, resulting in the first truly cloud REST
   API based Backup-as-a-Service interface, namely ``bcloud{''}. Finally,
   this work conducts an on-line usability poll with a significant number
   of users. The analysis of results in an overall average objective zero
   to ten questions evaluation was 8.29\%, indicating a very satisfactory
   user perception of the bcloud BaaS interface prototype.},
DOI = {10.5220/0007250902250232},
ISBN = {978-989-758-365-0},
ORCID-Numbers = {Solis, Priscila/0000-0002-9407-7943},
Unique-ID = {WOS:000571051500020},
}

@inproceedings{ WOS:000548532600011,
Author = {Gavrilov, Dmitriy and Lazarenko, Lyubov and Zakirov, Emil},
Editor = {Prokhorov, S},
Title = {AI recognition in skin pathologies detection},
Booktitle = {2019 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE: APPLICATIONS
   AND INNOVATIONS (IC-AIAI 2019)},
Year = {2019},
Pages = {54-56},
Note = {International Conference on Artificial Intelligence - Applications and
   Innovations (IC-AIAI), Vrdnik Banja, SERBIA, SEP 30-OCT 04, 2019},
Organization = {Moscow Inst Phys \& Technol},
Abstract = {Skin cancer is the most common type of cancer {[}1]. Between different
   malignant skin pathology melanoma is the most fleeting and mortality.
   Despite the superficial location of pathologies, only half of patients
   seek medical assistance on the early stages{[}2]. Treatment on the early
   (epidermal) stage provides a significantly higher chance of recovery.
   To assist a wide range of people in the early skin cancer detection, a
   software package was developed. The software based on deep convolutional
   neural networks technology. This complex allows to classify normal and
   malignant pathology on the uploaded photos.
   In clinical practice doctors use the ABCDE symptom's complex. This
   complex characterizes the observation of pigment spot asymmetry, border
   irregularities, color unevenness, diameter, and evolution {[}3]. The
   machine learning approach involves the computer evaluating similar
   factors when processing multiple images of different skin formations.
   The paper presents an algorithm for classification of skin lesions into
   pathology and norm using convolutional neural network architecture
   Xception with prior images segmentation.
   The upper classifying layers were frozen and new ones were added to
   classify skin diseases in the pre-trained neural network Xception. As a
   result, the classification of benign and malignant skin tumors provided
   at least 89\% accuracy.
   At the moment, the result of research work is designed in form of
   application software that allows to download the image of pigmented skin
   spots from the camera.},
DOI = {10.1109/IC-AIAI48757.2019.00017},
ISBN = {978-1-7281-4326-2},
Unique-ID = {WOS:000548532600011},
}

@inproceedings{ WOS:000518192300027,
Author = {Ghosal, Radhika and Rana, Bhavika and Kapur, Ishan and Parnami, Aman},
Book-Group-Author = {ACM},
Title = {Rapid Prototyping of Pneumatically Actuated Inflatable Structures},
Booktitle = {ADJUNCT PUBLICATION OF THE 32ND ANNUAL ACM SYMPOSIUM ON USER INTERFACE
   SOFTWARE AND TECHNOLOGY (UIST'19 ADJUNCT)},
Year = {2019},
Pages = {78-80},
Note = {32nd Annual ACM Symposium on User Interface Software and Technology
   (UIST), New Orleans, LA, OCT 20-23, 2019},
Organization = {Assoc Comp Machinery; ACM Special Interest Grp Computer Human Interact;
   ACM Special Interest Grp Comp Graph},
Abstract = {Fabricating and actuating inflatables for shape-changing interfaces and
   soft robotics is challenging and time-consuming, requiring knowledge in
   diverse domains such as pneumatics, manufacturing processes for
   elastomers, and embedded systems. We propose in this poster a scheme for
   rapid prototyping and pneumatically actuating piecewise multi-chambered
   inflatables, using balloons as our building blocks. We provide a
   construction kit containing pneumatic control boards, pneumatic
   components, and balloons for constructing simple actuated balloon
   models. We also provide various primitives of actuation and locomotion
   to help the user put together their desired actuator, along with an
   Android app and software API for controlling it via Bluetooth. Finally,
   we demonstrate the construction and actuation of these inflatable
   structures using three sample applications.},
DOI = {10.1145/3332167.3357121},
ISBN = {978-1-4503-6817-9},
Unique-ID = {WOS:000518192300027},
}

@inproceedings{ WOS:000542989200041,
Author = {Hernandez, Leonel and Jimenez, Genett and Pranolo, Andri and Rios,
   Carlos Uc},
Editor = {Pratomo, AH and Pranolo, A and Hernandez, L and Drewzewski, R and Voliansky, R and Zakaria, MS and Akbar, BM and Saifullah, S and Akbar, AT and Husaini, R and Heriyanto and Suryotomo, AP and Permadi, VA and Tahalea, SP},
Title = {Comparative Performance Analysis Between Software-Defined Networks and
   Conventional IP Networks},
Booktitle = {2019 5TH INTERNATIONAL CONFERENCE ON SCIENCE ININFORMATION TECHNOLOGY
   (ICSITECH): EMBRACING INDUSTRY 4.0 - TOWARDS INNOVATION IN CYBER
   PHYSICAL SYSTEM},
Year = {2019},
Pages = {235-240},
Note = {5th International Conference on Science in Information Technology
   (ICSITech), Univ Pembangunan Nasl Veteran Yogyakarta, Yogyakarta,
   INDONESIA, OCT 23-24, 2019},
Organization = {Univ Ahmad Dahlan; Univ Mulawarman; Univ Teknologi Malaysia Big Data
   Ctr; Univ Pendidikan Indonesia; Univ Muhammadiyah Surakarta; Univ Putra
   Malaysia; Univ Malaysia Sabah; Univ Budi Luhur; Politeknik Negeri
   Samarinda; Politeknik Negeri Padang; Univ Negeri Malang; Univ Teknikal
   Malaysia Melaka; ITSA Univ Colombia; Telkom Univ},
Abstract = {Conventional IP networks connect places at great distances and meet the
   connectivity needs of their users. To perform each of these operations,
   each packet must pass through various network devices, which make
   individual routing decisions that make centralized network management
   difficult. These networks have been growing both in size and complexity,
   each day at a higher rate, which has generated a series of difficulties
   in personalization, integration, security, and optimization of these. As
   a solution, the Software-Defined Networking (SDN) architecture {[}1] was
   created, which promises to be a dynamic, manageable, profitable and
   adaptable architecture, thus becoming an ideal tool to handle large
   bandwidths and the development and implementation of customized
   applications, for different types of needs on communication networks.
   This document shows a performance analysis between SDN and a
   conventional IP network configured with the EIGRP and BGP routing
   protocols, establishing a configuration scenario with physical network
   equipment and with an SDN emulator called Mininet. The research
   methodology is based on the guidelines of the Cisco PPDIOO methodology
   and is developed in the following phases: 1. Elaboration of physical
   network topology with Cisco equipment, performing experiments with IPv4
   and IPv6, measuring variables such as Jitter, Delay and Throughput. 2.
   Carrying out the same experiments and tests with SDN, in a network
   topology with similar characteristics to those already mentioned, but
   with OpenFlow switches. 3. Analysis of results, for which the behavior
   of jitter, delay and throughput variations of both scenarios is examined
   to make a series of comparisons (made with statistical analysis)
   concerning protocol, addressing, packet size among others. Finally, it
   was obtained as a result that SDN has a lower delay and jitter than the
   conventional IP network in some cases, as well as a more favorable
   throughput.},
ISBN = {978-1-7281-2380-6},
ResearcherID-Numbers = {Uc Rios, Carlos Eduardo/K-6624-2018
   Pranolo, Andri/M-6117-2016},
ORCID-Numbers = {Uc Rios, Carlos Eduardo/0000-0003-1321-019X
   Pranolo, Andri/0000-0002-3677-2788},
Unique-ID = {WOS:000542989200041},
}

@inproceedings{ WOS:000524690200184,
Author = {Hiep Binh Nguyen and Ngoc-Thanh Dinh and Oh, Jaewook and Kim, Younghan},
Book-Group-Author = {IEEE},
Title = {An Openflow-based Scheme for Service Chaining's High Availability in
   Cloud Network},
Booktitle = {2019 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION
   TECHNOLOGY CONVERGENCE (ICTC): ICT CONVERGENCE LEADING THE AUTONOMOUS
   FUTURE},
Series = {International Conference on Information and Communication Technology
   Convergence},
Year = {2019},
Pages = {805-807},
Note = {10th International Conference on Information and Communication
   Technology Convergence (ICTC) - ICT Convergence Leading the Autonomous
   Future, Jeju, SOUTH KOREA, OCT 16-18, 2019},
Organization = {Korean Inst Commun \& Informat Sci; IEEE Commun Soc; IEICE Commun Soc;
   Minist Sci \& ICT; Elect \& Telecommunicat Res Inst; Korean Federat Sci
   \& Technol Soc; Samsung Elect; SK Telecom; LG Elect; LGU+; KT; LG
   ERICSSON; Huawei; Qualcomm; SOLID; Korea Elect Technol Inst;
   Telecommunicat Technol Assoc; ICT Convergence Korea Forum; Multi Screen
   Serv Forum Soc Safety Syst Forum; Jeju Convent \& Visitors Bur; Jeju
   Special Self Governing Prov},
Abstract = {High availability (HA) is one of the important requirements of many
   end-to-end services built on network function virtualization (NFV) and
   NFV-enabled service function chaining (SFC) due to hard- and soft-ware
   failures. Once a virtual network function (VNF) fails, the entire SFC
   operations are broken down. Existing HA solutions for SFC in NFV
   environments require a significant delay for fault notification to the
   NFV-MANO (Management and Orchestration) and restoration notification of
   NFV-MANO to affected controllers. This leads to a significant load at
   the NFV-MANO, delays the recovery action of the SFC, and causes packet
   loss during the recovery time. In addition, in Software-Defined
   Networking (SDN) architecture, controller-based HA solutions are not
   efficient because studies have shown that SDN centralized architecture
   may not be able to achieve failure recovery within a 50 ms interval. To
   address above issue, this paper presents a proposal and implementation
   of an efficient controller-independent HA scheme based on OpenFlow. In
   particular, we exploit the OpenFlow group table to design protection
   plans and a fast failover mechanism, which enable a quick fault recovery
   for SFCs without requiring fault notification to the NFV-MANO or
   controllers. Experimental results using OpenFlow with OpenStack show
   that our proposed scheme achieves a significant improvement in terms of
   SFC link throughput and packet loss compared to existing schemes.},
ISSN = {2162-1233},
ISBN = {978-1-7281-0893-3},
Unique-ID = {WOS:000524690200184},
}

@inproceedings{ WOS:000528677800273,
Author = {Hsieh, Hsiao-Hu and Wang, Kuochen},
Book-Group-Author = {IEEE},
Title = {A Simulated Annealing-based Efficient Failover Mechanism for
   Hierarchical SDN Controllers},
Booktitle = {PROCEEDINGS OF THE 2019 IEEE REGION 10 CONFERENCE (TENCON 2019):
   TECHNOLOGY, KNOWLEDGE, AND SOCIETY},
Series = {TENCON IEEE Region 10 Conference Proceedings},
Year = {2019},
Pages = {1483-1488},
Note = {IEEE Region 10 Conference on Technology, Knowledge, and Society
   (TENCON), Kochi, INDIA, OCT 17-20, 2019},
Organization = {IEEE Kerala Sect; IEEE Humanitarian Activities Comm; Nissan Digital;
   Nest; Kerala State IT Miss; Cochin Shipyard Ltd; Terumo Penpol; Oracle
   Acad; InApp; Natl Instruments; Mentor; UST Global; I3; ICFOSS; Kerala
   Startup Miss; IEEE Reg 10},
Abstract = {The multi-controller architecture is a must to improve scalability and
   reliability of software defined networks (SDNs). A hierarchical
   multi-controller architecture allows a global controller to handle rare
   events, such as routing events from local controllers. A failover
   mechanism, which includes failure detection and recovery, is necessary
   for the multi-controller architecture in case of a controller failure.
   Existing controller failure detection and recovery mechanisms are
   performed in local controllers, which may degrade the performance of the
   local controllers. In controller failure recovery, existing mechanisms
   did not consider the time-varying load standard deviation metric, which
   may be used to reduce the flow setup time. To address these problems, we
   propose a Simulated Annealing-based Efficient Failover (SAEF) mechanism,
   run as an app in global controllers, to relieve the burden of local
   controllers for high availability hierarchical SDN controllers. The SAEF
   uses multiple global controllers to detect a local controller failure
   and compute a switch migration plan to redo switch-controller
   association for controller failure recovery. In controller failure
   detection, global controllers exchange their failure detection results
   of local controllers to jointly make a local controller failure decision
   to reduce the mistake rate. In controller failure recovery, the SAEF
   considers two metrics: switch-controller propagation delay and load
   standard deviation among local controllers with adaptive weights to
   adapt to time-varying local controllers' loads. In addition, in our
   design, a switch is connected to two local controllers so that
   asynchronous messages, such as packet-in and port-status messages, will
   not be lost during controller failover. Experiment results using EstiNet
   show that, the SAEF can achieve efficient failover with the best load
   balance result in term of load balance metric (LBM) of 1.035, while the
   best LBM of related works is 1.087. The SAEF also has the lowest flow
   setup time of 7.837 ms, while the best of related works is 10.418 ms.
   The lower flow setup time implies that the SDN data plane can receive
   the response message of a packet-in message faster.},
ISSN = {2159-3442},
ISBN = {978-1-7281-1895-6},
Unique-ID = {WOS:000528677800273},
}

@inproceedings{ WOS:000622970800074,
Author = {Ibe, Ekaterina and Shibaeva, Galina and Portnyagin, Denis and
   Afanasyeva, Elena},
Editor = {Murgul, V and Pasetti, M},
Title = {Thermal Protection of Multi-layer Exterior Walls with an Expanded
   Polystyrene Core},
Booktitle = {INTERNATIONAL SCIENTIFIC CONFERENCE ENERGY MANAGEMENT OF MUNICIPAL
   FACILITIES AND SUSTAINABLE ENERGY TECHNOLOGIES EMMFT 2018, VOL 2},
Series = {Advances in Intelligent Systems and Computing},
Year = {2019},
Volume = {983},
Pages = {758-767},
Note = {20th Annual International Scientific Conference on Energy Management of
   Municipal Facilities and Sustainable Energy Technologies (EMMFT),
   Voronezh State Tech Univ, Voronezh, RUSSIA, DEC 10-13, 2018},
Abstract = {In the article, the results of the study of temperature and humidity
   conditions of a multilayered structure in climatic conditions of
   Southern Siberia are stated. The complex analysis of a heat-shielding of
   a building is made. The humidity condition of multilayered external
   walls is studied. The basic construction units with the purpose of
   revealing of cold bridges are researched. Fields of temperature
   deformations of external walls are received. Unsuccessful experience of
   application of 3D-panels is shown, and ways of their elimination from
   the point of view of increase of a heat-shielding are offered. Methods
   of research represent modeling by means of ELCUT and SCAD Office
   software packages. Recommendations for the design of construction units
   of a building using 3D-panel technology are proposed in accordance with
   the requirements of not only normative documents, but also from the
   perspective of a comfortable thermal environment of the rooms.},
DOI = {10.1007/978-3-030-19868-8\_74},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-030-19868-8; 978-3-030-19867-1},
ORCID-Numbers = {Portnyagin, Denis/0000-0002-1209-1849
   Ibe, Ekaterina/0000-0002-3273-8160},
Unique-ID = {WOS:000622970800074},
}

@inproceedings{ WOS:000450908300076,
Author = {Jovic, A. and Kukolja, D. and Friganovic, K. and Jozic, K. and Cifrek,
   M.},
Editor = {Lhotska, L and Sukupova, L and Lackovic, I and Ibbott, GS},
Title = {MULTISAB: A Web Platform for Analysis of Multivariate Heterogeneous
   Biomedical Time-Series},
Booktitle = {WORLD CONGRESS ON MEDICAL PHYSICS AND BIOMEDICAL ENGINEERING 2018, VOL 1},
Series = {IFMBE Proceedings},
Year = {2019},
Volume = {68},
Number = {1},
Pages = {411-415},
Note = {IUPESM World Congress on Medical Physics and Biomedical Engineering,
   Prague, CZECH REPUBLIC, JUN 03-08, 2018},
Organization = {CSBMEMI; Czech Assoc Med Physicists; Varian; RaySearch Labs; Elekta; Int
   Union Phys \& Engn Sci Med; Czech Med Assoc J E Purkyne; Int Org Med
   Phys; Int Federat Med \& Biol Engn},
Abstract = {There is a growing need for efficient and accurate biomedical software
   in healthcare community. In this paper, we present MULTISAB, a web
   platform whose goal is to provide users with detailed analysis
   capabilities for heterogeneous biomedical time series. We describe the
   system architecture, including its subprojects: fron-tend, backend and
   processing. Emphasis is placed on the processing subproject, implemented
   in Java, which incorporates data analysis methods. The subproject is
   divided into several frameworks: record input handling, preprocessing,
   signal visualization, general time series features extraction, specific
   (domain) time series features extraction, expert system recommendations,
   data mining, and reporting. Common signal features extraction framework
   includes a great number of features in time (both linear and nonlinear),
   frequency and time-frequency domain. Currently, domain specific
   frameworks for heart rate variability, ECG and EEG feature extraction
   are supported, which also include preprocessing techniques for noise
   reduction and detection methods for characteristic waveforms (like QRS
   complexes, P and T waves in ECG). Parallelization is implemented for
   feature extraction to increase performance. It is realized using
   multi-threading on several levels: for multiple records, traces, and
   segments. Expert system is implemented, which provides automatic
   recommendation of the set of significant expert features that should be
   extracted from the analyzed signals, depending on the analysis scenario.
   The expert system, apart from the role in recommending features, can
   also participate in automatic diagnosis, after the features are
   extracted. Current expert system prototype contains diagnostic rules for
   acute myocardial ischemia, based on medical guidelines. Data mining
   framework contains dimensionality reduction methods and machine learning
   classifiers used to construct accurate and interpretable disorder
   models. A report is produced at the end of the process using openly
   available libraries. The platform includes best practices from medicine,
   biomedical engineering, and computer science in order to deliver
   detailed biomedical time series analysis services to its users.},
DOI = {10.1007/978-981-10-9035-6\_76},
ISSN = {1680-0737},
ISBN = {978-981-10-9035-6; 978-981-10-9034-9},
ResearcherID-Numbers = {Jovic, Alan/AAB-7865-2020
   Cifrek, Mario/S-9753-2018
   Friganovic, Kresimir/K-2091-2019
   Cifrek, Mario/N-5182-2019
   Friganović, Krešimir/AAX-9785-2021
   },
ORCID-Numbers = {Cifrek, Mario/0000-0002-7554-0824
   Friganovic, Kresimir/0000-0002-6053-5539
   Cifrek, Mario/0000-0002-7554-0824
   Jovic, Alan/0000-0003-3821-8091},
Unique-ID = {WOS:000450908300076},
}

@inproceedings{ WOS:000598539700057,
Author = {Kalaidov, A. N. and Mitryakov, V. V. and Tatarinov, V. V.},
Editor = {Tsvetkov, YB and Romanova, IK},
Title = {Modeling Characteristics of Technogenic Incidents in an Urban
   Environment},
Booktitle = {INTERNATIONAL SCIENTIFIC AND PRACTICAL CONFERENCE MODELING IN EDUCATION
   2019},
Series = {AIP Conference Proceedings},
Year = {2019},
Volume = {2195},
Note = {International Scientific and Practical Conference on Modeling in
   Education, Bauman Moscow State Tech Univ, Moscow, RUSSIA, JUN 19-21,
   2019},
Abstract = {The primary task of the state is to ensure the welfare of citizens, as
   the main indicator of the development of the state, therefore, the
   volume of construction of production facilities in Russia is growing.
   State authorities and local governments take activities to protect the
   population from natural and technogenic emergencies, as well as to
   reduce the risk of their occurrence on the territory of the Russian
   Federation. The Concept of building and developing the ``Safe City{''}
   Hardware and Software Package (HSP) is being introduced in Russia, which
   allows monitoring the condition of buildings and structures, ensuring
   the operation of message receiving and processing systems, emergency
   call systems and other municipal services of various directions,
   monitoring systems, forecasting, warning and managing all types of risks
   and threats inherent in the municipality. To automate the processes, the
   authors have developed various algorithms. The algorithm for recognizing
   the characteristics of technogenic incidents to increase the accuracy of
   identifying accidents when incomplete initial information about the
   current situation is received, as well as the structure of the
   information systems for decision support when managing funds and forces
   of emergency response based on the HSP ``Safe City{''}, will minimize
   the risks of emergencies of natural and technogenic character.},
DOI = {10.1063/1.5140157},
Article-Number = {020057},
ISSN = {0094-243X},
ISBN = {978-0-7354-1946-9},
ORCID-Numbers = {Tatarinov, Victor/0000-0002-6261-6156
   KALAYDOV, Alexander/0000-0003-0358-4286},
Unique-ID = {WOS:000598539700057},
}

@article{ WOS:000478976700030,
Author = {Khan, Mohammad Arsalan and Irfan, Shah and Rizvi, Zarghaam. and Ahmad,
   Jamal},
Title = {A numerical study on the validation of thermal formulations towards the
   behaviour of RC beams},
Journal = {MATERIALS TODAY-PROCEEDINGS},
Year = {2019},
Volume = {17},
Number = {1},
Pages = {227-234},
Note = {International Conference on Advanced Materials, Energy and Environmental
   Sustainability (ICAMEES), UPES, Dehradun, INDIA, DEC 14-15, 2018},
Abstract = {Thermal analysis of building components is done to predict thermal loads
   and deformations that affect the overall design criteria of any building
   subjected to thermal loading. However due to complexity of the problem
   specially in case of composite structures, this is mostly done using
   numerical software packages and no clear guidelines are available in
   structure design codes which are being constantly updated. For example,
   the British and Indian codes for design do not provide a clear guideline
   for designing structural components at elevated temperatures having
   non-constant profile distribution. The paper gives an insight towards a
   rational design of beams subjected to temperature changes through
   describing a detailed numerical validation of the mathematical equations
   for thermal stresses in a beam subjected to a temperature change T(y).
   The beams are subjected to different constant, linear, and non-linearly
   varying temperature profiles along their depth and the behavior is
   checked for the different beam conditions. The boundary conditions in
   this study include perfectly clamped ends, simply supported and a range
   of intermediate cases providing various levels of axial and flexural
   restraints. The theoretical formulations have been accordingly
   manipulated. For each case, thermal stress curves are extracted from the
   analysis against the validation of the mathematically derived equations.
   It is found that the choice of support conditions of the flexural
   elements have a considerable influence over their behaviour in thermal
   analysis in terms of thermal stress/strain due to axial strain, bending
   and curvature. (C) 2019 Elsevier Ltd. All rights reserved.},
ISSN = {2214-7853},
ResearcherID-Numbers = {Khan, Mohammad Arsalan/ACA-2807-2022
   Rizvi, Zarghaam Haider/O-7288-2017},
ORCID-Numbers = {Khan, Mohammad Arsalan/0000-0003-2841-6473
   Rizvi, Zarghaam Haider/0000-0002-7553-8310},
Unique-ID = {WOS:000478976700030},
}

@inproceedings{ WOS:000482103500257,
Author = {Kravchenko, Igor and Glinskiy, Maxim and Kuznetsov, Yury and Cheha,
   Tatiana},
Editor = {Malinovska, L and Osadcuks, V},
Title = {CONCEPTION OF CAE SYSTEM SUPPORT FOR PROTECTIVE COATING DEPOSITION
   PROCESS DESIGN IN AGROINDUSTRIAL COMPLEX},
Booktitle = {18TH INTERNATIONAL SCIENTIFIC CONFERENCE ENGINEERING FOR RURAL
   DEVELOPMENT},
Series = {Engineering for Rural Development},
Year = {2019},
Pages = {1761-1771},
Note = {18th International Scientific Conference on Engineering for Rural
   Development (ERD), Jelgava, LATVIA, MAY 22-24, 2019},
Organization = {Latvia Univ Life Sci \& Technologies, Fac Engn; Latvian Acad Agr \&
   Forestry Sci},
Abstract = {Currently in agricultural complex there exists a significant requirement
   in life time improving of the different processing equipment. One of the
   most promising and advanced ways to improve the resource of
   technological equipment is the protective and functional coatings
   thermal spray deposition for reconditioning and hardening of the core
   parts and movable operating elements, such as: parts of mixers of
   variable constructions and purposes, blades and scrapers of different
   machines, calenders, sliding bearing and mechanical seal parts, tank
   equipment and heat exchanger work surfaces, high speed centrifugal
   separator and decanter parts, conveyors and draw plates. Using of the
   proposed intelligent automated design system (CAE-system) enables
   considerably decrease costs and time for development of effective
   resource-saving technological processes of manufacturing coated
   equipment parts, improve quality and increase designers and technologist
   labour productivity. The software package being implemented at the same
   time allows to effectively carry out computational experiments for a
   comprehensive study and prediction of the resulting coatings of various
   functional purposes with enhanced physical, mechanical and operational
   properties. The paper presents the concept, architecture, principles of
   design and approach to development of the proposed CAE-system and
   highlights the potential for its further implementation in the practice
   of processing industries repair facilities and specialized service
   centres of the agroindustrial complex.},
DOI = {10.22616/ERDev2019.18.N497},
ISSN = {1691-3043},
EISSN = {1691-5976},
ResearcherID-Numbers = {Kuznetsov, Yury/AAR-2572-2021
   Kravchenko, Igor/B-9463-2018
   },
ORCID-Numbers = {Kuznetsov, Yury/0000-0003-3699-8231},
Unique-ID = {WOS:000482103500257},
}

@inproceedings{ WOS:000465803100048,
Author = {Mohsen, A. and Yunus, R. and Handan, R. and Kasim, N. and Hussain, K.},
Editor = {Yaacob, H and Yunus, NZM and Ibrahim, IS and Jamaludin, N and Abdullah, MH and Ahmad, IS and Ismail, DSA and Sirat, QAS and Anuar, UHM},
Title = {Determining factors for enhanced skilled worker requirements in IBS
   construction projects in Malaysia},
Booktitle = {12TH INTERNATIONAL CIVIL ENGINEERING POST GRADUATE CONFERENCE (SEPKA) /
   3RD INTERNATIONAL SYMPOSIUM ON EXPERTISE OF ENGINEERING DESIGN (ISEED),
   2018},
Series = {IOP Conference Series-Earth and Environmental Science},
Year = {2019},
Volume = {220},
Note = {12th International Civil Engineering Post Graduate Conference (SEPKA) /
   3rd International Symposium on Expertise of Engineering Design (ISEED),
   Johor, MALAYSIA, AUG 27-28, 2018},
Organization = {Univ Teknologi Malaysia , Fac Civil Engn; Natl Inst Technol, Kagoshima
   Coll},
Abstract = {Industrialized Building System (IBS) refers to prefabrication and
   offsite construction in Malaysia. The construction industry embraces IBS
   for better construction quality with reduced failure risks and cost
   effectiveness. This method is highly supported by the Malaysian
   government through Construction Industry Development Board (CIDB)
   Malaysia. However, IBS demands workforce that is skilled enough to
   produce and install construction components either on-site or off-site.
   The workers are deemed to be knowledgeable in handling materials,
   methods, and tools for constructing structures like houses, buildings,
   highways and roads. In fact, workers involved in construction projects
   are often found with low knowledge, skills, and accustomed to
   conventional methods instead of IBS. This research aimed at determining
   factors that may contribute enhancing skilled worker requirements for
   improving the implementation of IBS construction projects in Malaysia.
   The questionnaires were used to collect data from contractors involved
   in IBS construction projects in Malaysia. Statistical Package for the
   Social Science (SPSS) version 22.0 software was used to analyse the
   collected data. The results show that skilled labour with awareness of
   IBS implementation can handle and implement IBS projects properly. The
   results of this work revealed that the skilled workers also ensure the
   projects can be accomplished on time with better quality. The usage of
   foreign workforce can be minimized by encouraging local labours to
   participate in IBS as skilled workers, which enhances the market share
   of construction industry in Malaysian economy. This research provides
   valuable information regarding the importance of skilled workers and
   addresses the role of stakeholders in improving knowledge in IBS
   construction.},
DOI = {10.1088/1755-1315/220/1/012048},
Article-Number = {012048},
ISSN = {1755-1307},
ResearcherID-Numbers = {AL-AWAG, AAWAG MOHSEN MOHAMMED/AEZ-4020-2022
   Kasim, Narimah/AAD-6973-2019
   Yunus, Riduan/Q-1652-2017},
ORCID-Numbers = {Yunus, Riduan/0000-0002-2488-7262},
Unique-ID = {WOS:000465803100048},
}

@inproceedings{ WOS:000557266600021,
Author = {Monev, Venelin and Hristova, Maya},
Editor = {Vassilev, T and Smrikarov, A},
Title = {OAtools - Software package for investigation of orthogonal arrays},
Booktitle = {COMPUTER SYSTEMS AND TECHNOLOGIES},
Year = {2019},
Pages = {137-140},
Note = {20th Bulgarian International Computer Science and Technologies
   Conference (CompSysTech), Ruse, BULGARIA, JUN 21-22, 2019},
Organization = {Bulgarian Acad Soc Comp Syst \& Informat Technologies; Bulgarian Union
   Automat \& Informat; Assoc Comp Machinery Inc; Bulgarian Minist Educ\&
   Sci; John Atanasoff Union Automat \& Informat; Univ Ruse},
Abstract = {Orthogonal arrays are important combinatorial structures, both from a
   theoretical and a practical point of view. This fact is based on their
   close relation with other combinatorial structures and their practical
   application for testing and planning of experiments. For this reason,
   the orthogonal arrays are of a great research interest for many authors
   who explore different methods for construction, classification of
   orthogonal arrays, etc. In this paper we present a software package for
   investigation of orthogonal arrays. One of its functionalities is to
   check whether a matrix (or a set of matrices) satisfies the conditions
   for orthogonal array with fixed parameters. Another functionality of
   this package is the identification of isomorphic orthogonal arrays.
   Furthermore, OAtools provides the ability to calculate distance
   distribution.},
DOI = {10.1145/3345252.3345293},
ISBN = {978-1-4503-7149-0},
Unique-ID = {WOS:000557266600021},
}

@inproceedings{ WOS:000530109203029,
Author = {Moreno, J. J. and Puertas-Martin, S. and Orts, F. and Cruz, N. C. and
   Redondo, J. L. and Garzon, E. and Ortigosa, P. M.},
Editor = {Chova, LG and Martinez, AL and Torres, IC},
Title = {ON SIMULATING AN ARM PROCESSOR FOR TEACHING COMPUTER STRUCTURE},
Booktitle = {12TH INTERNATIONAL CONFERENCE OF EDUCATION, RESEARCH AND INNOVATION
   (ICERI 2019)},
Series = {ICERI Proceedings},
Year = {2019},
Pages = {3103-3108},
Note = {12th Annual International Conference of Education, Research and
   Innovation (ICERI), Seville, SPAIN, NOV 11-13, 2019},
Abstract = {Traditionally, the final part of the subject Structure and Computers
   Technology at the University of Almeria introduced students the
   architecture and assembly of MIPS processors. However, students
   perceived this architecture antiquated and with little relevance in
   current Computer Science. For this reason, the program of this subject
   has been updated to replace that part with ARM processors, which are of
   great interest nowadays. This work explains how this architecture has
   been included in the mentioned subject during the academic year 2018/19.
   A basic ARM processor has been designed for this purpose. Its name is
   ARM-Simple and aims to simplify learning for first-year students. For
   its implementation and simulation, the software package Digital has been
   used. It is an open-source circuit simulator which provides an
   interesting set of features that simplify the design, construction,
   analysis, and validation of different components. Several activities
   have been proposed for students. They are presented and divided into
   iterations so that students build the processor incrementally by adding
   a new component at each iteration. At the last iteration, students are
   provided with a high-level code. Students are then first required to
   convert it into assembly code and finally asked to transform it into
   machine code that can be executed by the processor built.},
ISSN = {2340-1095},
ISBN = {978-84-09-14755-7},
ResearcherID-Numbers = {Ortigosa, Pilar M./A-3129-2013
   Martín, Savíns Puertas/B-5859-2017
   Orts, Francisco/X-6626-2019},
ORCID-Numbers = {Ortigosa, Pilar M./0000-0001-6514-6543
   Martín, Savíns Puertas/0000-0001-8956-1733
   Orts, Francisco/0000-0002-4312-3671},
Unique-ID = {WOS:000530109203029},
}

@inproceedings{ WOS:000552229800041,
Author = {Moyeen, M. A. and Tang, Fangye and Saha, Dipon and Haque, Israat},
Editor = {Lutfiyya, H and Diao, YX and Zincir-Heywood, N and Badonnel, R and Madeira, E},
Title = {SD-FAST: A Packet Rerouting Architecture in SDN},
Booktitle = {2019 15TH INTERNATIONAL CONFERENCE ON NETWORK AND SERVICE MANAGEMENT
   (CNSM)},
Series = {International Conference on Network and Service Management},
Year = {2019},
Note = {15th Int Conf on Network and Serv Management (CNSM) / 1st Int Workshop
   on Analyt for Serv and Application Management (AnServApp) / Int Workshop
   on High-Precision Networks Operat and Control, Segment Routing and Serv
   Function Chaining (HiPNet+SR/SFC), Halifax, CANADA, OCT 21-25, 2019},
Organization = {IFIP; IEEE Comp Soc; IEEE Commun Soc; ACM; Dalhousie Univ; Western Univ;
   Juniper Networks; Mitacs; 2Keys Corp; Moogsoft; GoSecure; Discover
   Halifax; Murphys; Digital Nova Scotia; Cisco},
Abstract = {Communication link failure is common in any network. In Software Defined
   Network (SDN), protection-based recovery scheme reduces the failure
   recovery delay by installing alternative routes at the data plane
   switches. We can deploy Fast Failure Group (FFG) of OpenFlow protocol if
   a switch has an alternative path towards the destination; otherwise, the
   switch can use crankback approach to send the affected traffic towards
   the traversed route to find an alternative path. These existing recovery
   schemes force every packet to traverse a chain of matching tables even
   in the absence of a link failure, which impacts packet processing time
   and end-to-end delay. In this paper, we propose a packet rerouting
   architecture, called SD-FAST, that invokes recovery scheme only after
   facing failure and reduces both the packet processing and crankback
   backtracking time. We evaluate SD-FAST in Mininet, considering real and
   simulated traffic on real network topologies. The evaluation results
   confirm that SD-FAST can reduce around 73\% crankback backtracking time
   and 64\% delay compared to its counterparts.},
ISSN = {2165-9605},
ISBN = {978-3-903176-24-9},
Unique-ID = {WOS:000552229800041},
}

@article{ WOS:000487290800013,
Author = {Nguyen, C. T. and Aniskin, N. A.},
Title = {Temperature regime during the construction massive concrete with pipe
   cooling},
Journal = {MAGAZINE OF CIVIL ENGINEERING},
Year = {2019},
Volume = {89},
Number = {5},
Pages = {156-166},
Abstract = {Pipe cooling is one of the effective measures in order to reduce the
   exothermic heating of massive concrete structures. In this paper,
   analyzing the influence parameters of the pipe cooling system on the
   temperature regime and the thermal stress state in mass concrete to be
   built. The concrete mass was considered as a pillar with dimensions in
   plan (10x10) m and a final height of 30.0 m. The effect of the following
   parameters was investigated: the height of the concrete column from the
   elevation of the foundation is used for the cooling pipe system; the
   step of cooling pipe system according to the height and width of the
   concrete block; the temperature of water supplied to the pipe cooling
   system. Then, numerical studies were carried out by using the Midas
   civil software package based on the finite element method in order to
   solve the temperature problem and determine the thermal stress state of
   the block. From that will be collected the numerical results from
   pictures of changes in the temperature regime and the thermally stressed
   state in mass concrete during construction. The influence of each of
   those factors is considered in order to evaluate the change in
   temperature regime and thermal stress of the concrete mass. So, the
   results obtained are of practical importance and can be used to assign
   parameters of the pipe cooling system.},
DOI = {10.18720/MCE.89.13},
ISSN = {2071-4726},
EISSN = {2071-0305},
Unique-ID = {WOS:000487290800013},
}

@inproceedings{ WOS:000659174900002,
Author = {Ornhag, Marcus Valtonen and Wadenback, Marten},
Editor = {DeMarsico, M and DiBaja, GS and Fred, A},
Title = {Planar Motion Bundle Adjustment},
Booktitle = {ICPRAM: PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON PATTERN
   RECOGNITION APPLICATIONS AND METHODS},
Year = {2019},
Pages = {24-31},
Note = {8th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM), Prague, CZECH REPUBLIC, FEB 19-21, 2019},
Abstract = {In this paper we consider trajectory recovery for two cameras directed
   towards the floor, and which are mounted rigidly on a mobile platform.
   Previous work for this specific problem geometry has focused on locally
   minimising an algebraic error between inter-image homographies to
   estimate the relative pose. In order to accurately track the platform
   globally it is necessary to refine the estimation of the camera poses
   and 3D locations of the feature points, which is commonly done by
   utilising bundle adjustment; however, existing software packages
   providing such methods do not take the specific problem geometry into
   account, and the result is a physically inconsistent solution. We
   develop a bundle adjustment algorithm which incorporates the planar
   motion constraint, and devise a scheme that utilises the sparse
   structure of the problem. Experiments are carried out on real data and
   the proposed algorithm shows an improvement compared to established
   generic methods.},
DOI = {10.5220/0007247700240031},
ISBN = {978-989-758-351-3},
ORCID-Numbers = {Wadenback, Marten/0000-0002-0675-2794},
Unique-ID = {WOS:000659174900002},
}

@article{ WOS:000484353200010,
Author = {Petrov, Andrey N. and Kiselevl, Valery V. and Romanova, Elena K. and
   Sivtseva, I, Alena},
Title = {ON THE ISSUE OF SAFE OPERATION OF THE UNDERGROUND TRAVEL COMPLEX ``THE
   KINGDOM OF ETERNAL FROST{''}},
Journal = {BULLETIN OF THE TOMSK POLYTECHNIC UNIVERSITY-GEO ASSETS ENGINEERING},
Year = {2019},
Volume = {330},
Number = {8},
Pages = {94-104},
Abstract = {Relevance. Investigations of the thermal regime and development of
   effective ways of controlling it in underground structures of various
   purposes have received considerable attention in recent years. This is
   related not only to intensification of underground space development,
   but also to the variety of thermal conditions and the degree of
   influence of the temperature factor on the efficiency and safety of
   construction and operation of underground structures. As it is known
   from the mining practice in the North, the thawed rock mass, especially
   above the roof of the excavations, creates significant loads on the
   supports, sometimes exceeding permissible, which causes irreversible
   deformations, and sometimes leads to its collapse.
   Objects: galleries of the tourist complex The Kingdom of Eternal Frost
   located on the 5th km of the Vilyuisk tract of Yakutsk.
   The aim of the research is to develop the measures to ensure the
   required technical condition of the thermal regime complex and
   recommendations for securing underground mining workings providing safe
   conditions for tourists to stay.
   Methods: studies of the temperature regime, visual inspection of the
   mining slope and underground mine workings of the tourist complex,
   selection of the type of fastening for the required conditions,
   calculation of the fastening parameters.
   The paper introduces the results of the studies of the thermal regime of
   underground mine workings during winter operation period, the results of
   a visual survey of the mining slope and underground mine workings. The
   authors give recommendations to ensure the stability and anchoring of
   the zones for formation of underground mining excavations of the tourist
   complex The Kingdom of Eternal Frost. The calculation of the temperature
   mode of the digital computer and the required power of the refrigerating
   machines was corned out using the MuseumCVM software package developed
   at the Laboratory of Mining Thermophysics of the Institute of Mining of
   the North named after N.V. Chersky. To reduce energy consumption for
   producing artificial cold in summer, it is recommended to hold annual
   refrigerated charge in autumn and spring.},
DOI = {10.18799/24131830/2019/8/2216},
ISSN = {2500-1019},
EISSN = {2413-1830},
ResearcherID-Numbers = {Romanova, Elena/ABA-5991-2020},
Unique-ID = {WOS:000484353200010},
}

@article{ WOS:000486981900007,
Author = {Poloprutsky, Zdenek and Frastia, Marek and Marcis, Marian},
Title = {3D DIGITAL RECONSTRUCTION BASED ON ARCHIVED TERRESTRIAL PHOTOGRAPHS FROM
   METRIC CAMERAS},
Journal = {ACTA POLYTECHNICA},
Year = {2019},
Volume = {59},
Number = {4},
Pages = {384-398},
Abstract = {The paper deals with the possibilities and limitations of the image
   processing of digitalized terrestrial photographs from analogue metric
   cameras. By the end of the 20th century, analogue metric cameras such as
   Zeiss UMK were used for documentation purposes in the fields of
   industrial and investment construction, nature and landscape
   preservation, heritage preservation, etc. Currently, the collections of
   photographs are stored at specialized archives of many different
   institutions, such as libraries, museums, universities, etc. These sets
   of photographs provide a material background for a 3D digital
   reconstruction of a subject of interest at the time of taking the
   photographs.
   The digital image processing of old photographs may be more difficult
   due to unknown parameters of the used camera, such as the focal length,
   image coordinates of the fiducial marks and distortion parameters of the
   lens system, etc. In this case, it may be difficult to process these
   photographs in a photogrammetric software.
   The paper presents a methodology for the digital photogrammetric
   processing of analogue terrestrial photographs. The data processing is
   based on the parameters of the used metric cameras, which are described
   in their calibration reports. The image processing was tested in two
   commercial photogrammetric software tools that utilize the technology of
   Structure-from-Motion (SfM) or multi-image intersection photogrammetry
   to process image datasets.},
DOI = {10.14311/AP.2019.59.0384},
ISSN = {1210-2709},
EISSN = {1805-2363},
ResearcherID-Numbers = {Fraštia, Marek/AAC-8666-2020
   },
ORCID-Numbers = {Fraštia, Marek/0000-0003-3519-7504
   Poloprutsky, Zdenek/0000-0002-9305-4897},
Unique-ID = {WOS:000486981900007},
}

@inproceedings{ WOS:000569985400133,
Author = {Radescu, Radu and Dragu, Mihaela},
Book-Group-Author = {IEEE},
Title = {Automatic Analysis of Potential Hazard Events Using Unmanned Aerial
   Vehicles},
Booktitle = {PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON ELECTRONICS,
   COMPUTERS AND ARTIFICIAL INTELLIGENCE (ECAI-2019)},
Series = {International Conference on Electronics Computers and Artificial
   Intelligence},
Year = {2019},
Note = {11th International Conference on Electronics, Computers and Artificial
   Intelligence (ECAI), Pitesti, ROMANIA, JUN 27-29, 2019},
Organization = {IEEE; IEEE Romania Sect; IEEE Ind Applicat Soc; Univ Pitesti, Fac Elect,
   Commun \& Comp; Guvernul Romaniei, Ministerul Educatiei Cercetarii
   Stiintifice},
Abstract = {This paper is motivated by the possibility of developing a wide variety
   of applications and domains in which Unmanned Aerial Vehicles (UAVs) can
   be used globally for various purposes. UAVs are currently used by public
   administrations and security forces such as police, fire brigades, civil
   protection, research institutions, construction, and agriculture
   entities. The purpose of this paper is to facilitate the handling of
   UAVs to retrieve various data from the environment. The drone (UAV)
   visits some points to collect data (image and/or video input) from
   sensors like GPS, camera, gyroscope, and accelerometer. GPS sensor
   coordinates are used to compare the data taken with subsequent results
   through processing with specialized software. The drone is used as an
   access gate with built-in sensors. Certain hazard events (fires, floods,
   avalanches, landslides) are not limited to narrow geographical areas,
   but can impact the environment by triggering negative chain events. 3D
   modeling offers a wide range of possibilities to prevent potential
   hazard events, or, if such an event has occurred, makes it possible to
   monitor the affected area and assess the damage by comparing the area in
   the pre-event configuration with the after-event one. After image
   processing and data acquisition, a report is generated that includes the
   map and the 3D model of the analyzed object. A hazard is an agent that
   has the potential to cause damage to a particular target. Terms such as
   risk or danger can be used in similar contexts. TensorFlow is an open
   source software library in high-performance computing. Flexible
   architecture allows easy deployment of computing on a variety of
   platforms (CPU, GPU, TPU), from desktop to server or mobile devices. We
   used the learning transfer: at first we used a model that was already
   prepared for another problem, and then we re-qualified it on a similar
   problem. Deep learning from scratch can take several days, but learning
   transfer can be done shortly. We applied Python along with TensorFlow to
   train an image classifier and classify images with it. We formed a
   consistent set of training pictures, using three labels: fire, flood
   (detectable hazards) and nature (non-hazard images). We then
   re-qualified an efficient, small-sized neural network by (re)training
   the image set in order to get the best results in the hazards prediction
   selection process with a progressive higher accuracy as (re) training
   evolves at optimal rating. With Python and OpenCV technologies, we used
   four decision algorithms to generate prediction of hazard: Support
   Vector Machine, Naive Bayes, Logistic Regression, and Decision Tree
   Classifier. Each generated report includes precision, recall, f1-score,
   and support indices, depending on the class and intervals used. We also
   used the confusion matrix as an alternative method to evaluate the
   classification accuracy. Analyzing the 4 algorithms we noticed that they
   behave differently. Training using TensorFlow generated better results
   than the other methods. For the main classes tested hazard is recognized
   up to 99\%.},
ISSN = {2378-7147},
ISBN = {978-1-7281-1624-2},
ResearcherID-Numbers = {Rădescu, Radu/A-9859-2010},
ORCID-Numbers = {Rădescu, Radu/0000-0003-3756-067X},
Unique-ID = {WOS:000569985400133},
}

@inproceedings{ WOS:000519019600014,
Author = {Rehman, Mufaiz and Sakalle, Rashmi},
Editor = {Bhadauria, SS and Rawat, A},
Title = {Finite Elemental Analysis of Industrial Structure using Cold Formed
   Steel},
Booktitle = {PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SUSTAINABLE MATERIALS AND
   STRUCTURES FOR CIVIL INFRASTRUCTURES (SMSCI2019)},
Series = {AIP Conference Proceedings},
Year = {2019},
Volume = {2158},
Note = {International Conference on Sustainable Materials and Structures for
   Civil Infrastructures (SMSCI), Rajiv Gandhi Proudyogiki Vishwavidyalaya,
   Univ Inst Technol, Dept Civil En, Bhopal, INDIA, MAR 14-15, 2019},
Organization = {Madhya Pradesh Council Sci \& Technol; AIMIL Ltd; MyCem; MP Chapter;
   Indian Concrete Inst, MP Chapter; Indian Green Building Council},
Abstract = {Increasing world population and natural resource limitations has led to
   a growing demand for more efficient structural systems to achieve a
   sustainable economy and society. Cold-formed steel CFS) Structural
   systems are increasingly adopted as primary or secondary structural
   members in modern building construction because of their light weight,
   speed of construction, recyclability, and sustainability. However, the
   inherently low buckling resistance of thin sections results in
   relatively low strength and ductility in CFS elements, which limits
   their performance in tall buildings and under extreme loading events.
   The pre-engineered steel building system construction has great
   advantages to the single storey buildings, practical and efficient
   alternative to conventional buildings, the System representing one
   central model within multiple disciplines. Pre-engineered building
   creates and maintains in real time multidimensional, data rich views
   through a project support is currently being implemented by Staad pro
   software packages for design and engineering. In this research work we
   will design Industry using analysis tool Staad.pro and use novel cold
   formed steel structure and compare it with general steel available in
   Indian market. Here we will compare both in terms of strength and weight
   of structure with bolted and welded connections.},
DOI = {10.1063/1.5127138},
Article-Number = {020014},
ISSN = {0094-243X},
ISBN = {978-0-7354-1903-2},
Unique-ID = {WOS:000519019600014},
}

@article{ WOS:000500466700005,
Author = {Sadeghi, Marjan and Elliott, Jonathan Weston and Porro, Nick and Strong,
   Kelly},
Title = {Developing building information models (BIM) for building handover,
   operation and maintenance},
Journal = {JOURNAL OF FACILITIES MANAGEMENT},
Year = {2019},
Volume = {17},
Number = {3},
Pages = {301-316},
Abstract = {Purpose - This paper aims to represent the results of a case study to
   establish a building information model (BIM)-enabled workflow to capture
   and retrieve facility information to deliver integrated handover
   deliverables.
   Design/methodology/approach - The Building Handover Information Model
   (BHIM) framework proposed herein is contextualized given the
   Construction Operation Information Exchange (COBie) and the level of
   development schema. The process uses Autodesk Revit as the primary
   BIM-authoring tool and Dynamo as an add-in for extending Revit's
   parametric functionality, BHIM validation, information retrieval and
   documentation in generating operation and maintenance (O\&M)
   deliverables in the end-user requested format.
   Findings - Given the criticality of semantics for model elements in the
   BHIM and for appropriate interoperability in BIM collaboration, each
   discipline should establish model development and exchange protocols
   that define the elements, geometrical and non-geometrical information
   requirements and acceptable software applications early in the design
   phase. In this case study, five information categories (location,
   specifications, warranty, maintenance instructions and Construction
   Specifications Institute MasterFormat division) were identified as
   critical for model elements in the BHIM for handover purposes.
   Originality/value - Design- and construction-purposed BIM is a standard
   platform in collaborative architecture, engineering and construction
   practice, and the models are available for many recently constructed
   facilities. However, interoperability issues drastically restrict
   implementation of these models in building information handover and
   O\&M. This study provides essential input regarding BIM exchange
   protocols and collaborative BIM libraries for handover purposes in
   collaborative BIM development.},
DOI = {10.1108/JFM-04-2018-0029},
ISSN = {1472-5967},
EISSN = {1741-0983},
ResearcherID-Numbers = {Sadeghi, Marjan/AAA-9123-2021
   },
ORCID-Numbers = {Elliott, Jonathan/0000-0002-1705-545X},
Unique-ID = {WOS:000500466700005},
}

@inproceedings{ WOS:000570241300221,
Author = {Shadura, Oksana and Vassilev, Vassil and Bockelman, Brian Paul},
Editor = {Forti, A and Betev, L and Litmaath, M and Smirnova, O and Hristov, P},
Title = {Continuous Performance Benchmarking Framework for ROOT},
Booktitle = {23RD INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY AND NUCLEAR
   PHYSICS (CHEP 2018)},
Series = {EPJ Web of Conferences},
Year = {2019},
Volume = {214},
Note = {23rd International Conference on Computing in High Energy and Nuclear
   Physics (CHEP), Sofia, BULGARIA, JUL 09-13, 2018},
Organization = {Inst Adv Phys Studies; Sofia Univ; Plovdiv Univ; New Bulgarian Univ ,
   IICT-BAS; Burgas Free Univ; BAS, INRNE; BAS, IICT; RHEA Grp; T Syst;
   Intel; Chaos Grp},
Abstract = {Foundational software libraries such as ROOT are under intense pressure
   to avoid software regression, including performance regressions.
   Continuous performance benchmarking, as a part of continuous integration
   and other code quality testing, is an industry best-practice to
   understand how the performance of a software product evolves. We present
   a framework, built from industry best practices and tools, to help to
   understand ROOT code performance and monitor the efficiency of the code
   for several processor architectures. It additionally allows historical
   performance measurements for ROOT I/O, vectorization and parallelization
   sub-systems.},
DOI = {10.1051/epjconf/201921405003},
Article-Number = {05003},
ISSN = {2100-014X},
Unique-ID = {WOS:000570241300221},
}

@inproceedings{ WOS:000618061700065,
Author = {Shutova, M. N. and Evtushenko, I, S. and Kalafatov, D. A.},
Editor = {Mangushev, R and Zhussupbekov, A and Iwasaki, Y and Sakharov, I},
Title = {Analyzing efficiency of two-layer foundations for a power transmission
   line portal based on a numerical experiment},
Booktitle = {GEOTECHNICS FUNDAMENTALS AND APPLICATIONS IN CONSTRUCTION: NEW
   MATERIALS, STRUCTURES, TECHNOLOGIES AND CALCULATIONS},
Series = {Proceedings in Earth and Geosciences},
Year = {2019},
Volume = {2},
Pages = {335-340},
Note = {International Scientific-Technical Conference on Geotechnical
   Fundamentals and Applications in Construction - New Materials,
   Structures, Technologies and Calculations (GFAC), St Petersburg State
   Univ Architecture \& Civil Engn, Saint Petersburg, RUSSIA, FEB 06-08,
   2019},
Organization = {GRF; KGS; Russian Soc Soil Mech Geotech \& Fdn Engn},
Abstract = {The article addresses determination of economic efficiency from the use
   of a two-layer foundation for transmission line portal towers in
   comparison with traditional design solutions. Three types of foundations
   for towers were considered as competing options: a monolithic
   reinforced-concrete foundation, a prefabricated foundation according to
   the serial design of the contractor and a two-layer foundation with a
   pyramidal base according to patent IPC E02D27/01. Construction is
   performed in an area with high seismic activity (8 points). Structural
   design of the foundations was carried out with account for the following
   loads: self-weight, wind in X-direction, wind in Y-direction, tension
   without account for atmospheric ice, tension with account for
   atmospheric ice, weight of a service man, turbulence of wind in
   X-direction, turbulence of wind in Y-direction, seismicity in
   X-direction, seismicity in Y-direction. A numerical experiment was
   carried out with the use of the ANSYS software package; the foundation
   structures and the soil mass were simulated. The design process revealed
   that there was no substantial difference in the settlement between three
   types of models, and that the settlement did not exceed the acceptable
   limit. During structural analysis with account for a special combination
   of forces in the principal reinforcement of the bottom grid, the optimal
   cross-section for each model was selected. Economy of principal
   reinforcement when using a two-layer foundation was 49\% as compared to
   a monolithic foundation, and 30\% as compared to the serial design.},
ISSN = {2639-7749},
EISSN = {2639-7757},
ISBN = {978-0-429-05888-2; 978-0-367-17983-0},
ResearcherID-Numbers = {Evtushenko, Sergej/C-5249-2013},
ORCID-Numbers = {Evtushenko, Sergej/0000-0003-3708-380X},
Unique-ID = {WOS:000618061700065},
}

@inproceedings{ WOS:000468572100059,
Author = {Sinenko, Sergei and Poznakhirko, Tatiana and Obodnikov, Vyacheslav},
Editor = {Wirahadikusumah, RD and Hasiholan, B and Kusumaningrum, P},
Title = {Automation of visualization process for organizational and technological
   design solutions},
Booktitle = {2ND CONFERENCE FOR CIVIL ENGINEERING RESEARCH NETWORKS (CONCERN-2 2018)},
Series = {MATEC Web of Conferences},
Year = {2019},
Volume = {270},
Note = {2nd Conference for Civil Engineering Research Networks (ConCERN), Inst
   Teknologi Bandung, Fac Civil \& Environm Engn, Bandung, INDONESIA, NOV
   27-29, 2018},
Organization = {Univ Teknologi Petronas; Japan Int Cooperat Agcy; Minist Publ Works \&
   Housing, Directorate Gen Construct Dev; Inst Teknologi Bandung, Civil
   Engn Alumni Assoc; Natl Construct Serv Dev Board Indonesia; ITB, Tim
   AKSI; PT Wijaya Karya Beton Tbk; PT Aditya Engn Consultant; PT Mitra
   Plan Enviratama; PT Marga Graha Penta; PT Rayakonsult; PT Lapi
   Ganeshatama; PT Marga Sarana Jabar; PT Metanna Engn Consultant; PT
   Intimulya Multikencana; PT Multimera Harapan},
Abstract = {This article studies modern software packages used in drawing
   construction master plans and their elements. A special emphasis is
   placed on increasing the level of design works, improving their quality,
   and expanding the community of technically unskilled users. The article
   describes approaches to solving the task of visualization of
   organizational and technological solutions and gives a comparative
   analysis thereof. It presents a visualization diagram of an
   organizational and technological solution for the construction of a
   building. It also highlights the most promising direction in graphic
   modeling of a construction process for buildings and structures with
   visualization seen as the most objective solution to address the
   assigned task.},
DOI = {10.1051/matecconf/201927005008},
Article-Number = {05008},
ISSN = {2261-236X},
ResearcherID-Numbers = {Sinenko, Sergey/AAF-6668-2021
   Tatiana, Poznakhirko/AAM-6678-2020},
ORCID-Numbers = {Tatiana, Poznakhirko/0000-0003-3597-2558},
Unique-ID = {WOS:000468572100059},
}

@article{ WOS:000517750400005,
Author = {Skliarov, V},
Title = {State of the art of using model solutions at leading metrological
   institutes of the world},
Journal = {UKRAINIAN METROLOGICAL JOURNAL},
Year = {2019},
Number = {4},
Pages = {33-40},
Abstract = {The article discusses the application of modem digital software
   technologies in leading national metrological institutes (NMI).
   Practical examples of the use of calculation systems for solving
   individual tasks of measuring systems and addressing everyday scientific
   problems are given.
   Based on the NIST fmal report on the destruction of the World Trade
   Center towers in New York on September 11, 2001, the practical use of
   modern computer simulation packages is shown. Model solutions of
   individual components (building structure, aircraft model, factor of
   mechanical and gravitational effects, temperature environment) made it
   possible to reconstruct and explain the processes of their destruction.
   The experience of using software packages for engineering analysis in
   PTB is presented by the example of calculating gas pressure in order to
   determine the Boltzmann constant. In addition, the (mite element method
   is the main method used in the EURAMET and EMPIR projects. The above
   studies suggest a number of calculations to solve problems arising in
   the leading metrological organizations of the world.
   The aim of the work is to analyze and justify the principles of using
   computer modeling in the construction and improvement of elements of the
   national measurement standards. The relevance of the topic of work is
   determined by the Concept for the Development of the Digital Economy and
   Society of Ukraine for 2018-2020 and the Plan of Measures for its
   Implementation (the Order of the Cabinet of Ministers of Ukraine No.
   67-r dated January 17, 2018) approved by the Government of Ukraine. The
   main goal of the Concept is to implement an action plan to remove
   barriers to the digital transformation of Ukraine in the most promising
   areas. The Concept for the Development of the Digital Economy of Ukraine
   is in the use of digital technologies in industrial sectors in order to
   provide their effectiveness, competitiveness and stable development,
   growth in the production of high-tech products and the well-being of the
   population.},
ISSN = {2306-7039},
EISSN = {2522-1345},
ResearcherID-Numbers = {Skliarov, Volodymyr/AGP-8349-2022},
ORCID-Numbers = {Skliarov, Volodymyr/0000-0002-6671-3420},
Unique-ID = {WOS:000517750400005},
}

@article{ WOS:000453247000009,
Author = {Sohu, Samiullah and Chandio, Abdul Fattah and Kaleemullah},
Title = {Identification of Causes and Minimization of Delays in Highway Projects
   of Pakistan},
Journal = {MEHRAN UNIVERSITY RESEARCH JOURNAL OF ENGINEERING AND TECHNOLOGY},
Year = {2019},
Volume = {38},
Number = {1},
Pages = {103-112},
Month = {JAN},
Abstract = {The problem of delay in construction industry is a regular phenomenon
   worldwide, and construction industry of Pakistan has no exception,
   particularly in highways projects. Delay can be described as the
   extension of time to complete the construction project. The aim of this
   paper is to identify main causes of delays in highway projects of
   Pakistan, and to determine mitigating measures for the identified
   causes. The research method of this study is based on literature review,
   questionnaire survey and semi structured interview. From in-depth
   literature review, twenty-six common causes of delay were found. A
   questionnaire survey was carried out among construction professionals of
   highway projects. The causes of delay in highways projects were ranked
   referring to their Mean values. A semi structured interview was carried
   out to determine mitigation measures for the top ten causes of delays.
   The data gathered from questionnaire survey was analyzed using SPSS
   (Statistical Package for the Social Sciences) while, data collected
   through semi structured interviews was analyzed using Nvivo software.
   The findings of this study are expected to be useful for construction
   parties, to mitigate the delays in highway construction projects of
   Pakistan.},
DOI = {10.22581/muet1982.1901.09},
ISSN = {0254-7821},
EISSN = {2413-7219},
ResearcherID-Numbers = {Ullah, Kaleem/Z-5882-2019},
Unique-ID = {WOS:000453247000009},
}

@inproceedings{ WOS:000525951600004,
Author = {Stenta, Aaron and Panzarella, Charles},
Editor = {Lefebvre, F and Cetim, PS},
Title = {Probabilistic Fatigue Software Platform (ProbFat) for Optimizing
   Life-Cycle Decisions},
Booktitle = {FATIGUE DESIGN 2019, INTERNATIONAL CONFERENCE ON FATIGUE DESIGN, 8TH
   EDITION},
Series = {Procedia Structural Integrity},
Year = {2019},
Volume = {19},
Pages = {27-40},
Note = {8th International Conference on Fatigue Design (Fatigue Design), Cetim,
   FRANCE, NOV 20-21, 2019},
Organization = {Prenscia; MRX, X Raybot; Elsevier; ATMA; DVM; Reg Hauts France; AFM; Soc
   Francaise Mettallurgie Materiaux; Univ Technologie Compiegne; JSMS;
   Cetim Tech Ctr Mech Ind},
Abstract = {Fatigue is inherently uncertain. Thus, the use of technology for
   prediction of fatigue damage to ensure the long-term reliability of
   equipment must embrace such uncertainty. The proposed probabilistic
   fatigue software platform (ProbFat) accomplishes just that. The
   stochastic engine behind ProbFat is a collection of Bayesian Decision
   Networks (BDN) that assess risk to recommend the optimal life-cycle
   strategy for maximizing overall return (i.e. return = benefit - cost).
   Benefit is a measure of profitability (e.g. annualized revenue) and cost
   is a measure of expense (e.g. design, inspection, repair, replacement,
   failure, et cetera). To optimize the return, the BDN must incorporate
   all aspects of LCM including: (i) design and construction, (ii)
   monitoring, inspection, and maintenance scheduling, (iii) identification
   of damage, (iv) assessment of observed damage, and (v) decision
   strategies to either run, repair, rerate, replace, or retire the
   component(s) of interest at the best possible times. BDNs are structured
   as graphical cause-effect relationships, represented by probabilistic
   nodes with directional links. Populating conditional probabilities
   associated with nodes is accomplished through assessment of laboratory
   and site-specific data, contributions from subject matter experts and
   simulations from in-house deterministic and stochastic fatigue models.
   In this paper, two industry examples are used to demonstrate the
   effectiveness of ProbFat: (i) low cycle thermo-mechanical fatigue of a
   coke drum, and (ii) very-high cycle vibration fatigue of a piping
   system. For both applications the fatigue methods available in API
   579-1/ASME FFS-1 are utilized for deterministic/probabilistic prediction
   of fatigue damage accumulation and remaining life. Methods available in
   API RBI are used for inspection planning and maintenance scheduling. (C)
   2019 The Authors. Published by Elsevier B.V.},
DOI = {10.1016/j.prostr.2019.12.005},
ISSN = {2452-3216},
Unique-ID = {WOS:000525951600004},
}

@inproceedings{ WOS:000622970800066,
Author = {Tarasenko, Aleksandr and Chepur, Petr and Gruchenkova, Alesya},
Editor = {Murgul, V and Pasetti, M},
Title = {Substantiation of Technological Solutions for the Repair of the Anodic
   Tank Grounding},
Booktitle = {INTERNATIONAL SCIENTIFIC CONFERENCE ENERGY MANAGEMENT OF MUNICIPAL
   FACILITIES AND SUSTAINABLE ENERGY TECHNOLOGIES EMMFT 2018, VOL 2},
Series = {Advances in Intelligent Systems and Computing},
Year = {2019},
Volume = {983},
Pages = {679-685},
Note = {20th Annual International Scientific Conference on Energy Management of
   Municipal Facilities and Sustainable Energy Technologies (EMMFT),
   Voronezh State Tech Univ, Voronezh, RUSSIA, DEC 10-13, 2018},
Abstract = {The article reviews the features of applying the technology of repairing
   the bottom and anodic earthing of a vertical steel cylindrical tank
   without dismantling the floating roof during its reconstruction. The
   main non-trivial task of this project was to carry out work inside the
   tank in the cramped conditions of the sub-pontoon space. To replace the
   bottom of the tank, along with the extended anodic earthing devices,
   located at a depth of 0.9 m from the surface, a technological corridor
   free from floating roof racks was proposed, as well as a technological
   corridor for moving dismantled materials, soil, new bottom sheets
   towards the installation opening in the wall. As part of the proposed
   installation diagram, the floating roof was in the installation position
   provided for by the tank construction project with the provision of a
   2.2 m high sub-pontoon space. To solve the problem of ensuring the
   strength of the structure, a finite element model of a floating roof was
   built in the ANSYS software package, which allows it to be analyzed at
   different types of mounting loads. The results obtained using the model
   formed the basis of the adopted design decisions. Using FEM {[}4-6,
   8-10, 12-15], the design of a temporary reinforcement frame was
   theoretically justified and proposed, which ensured a fivefold safety
   margin sufficient for construction and installation works.},
DOI = {10.1007/978-3-030-19868-8\_66},
ISSN = {2194-5357},
EISSN = {2194-5365},
ISBN = {978-3-030-19868-8; 978-3-030-19867-1},
ResearcherID-Numbers = {Chepur, Petr/L-1887-2015
   Tarasenko, Alexandr/E-5305-2017
   Gruchenkova, Alesya/T-6178-2017},
ORCID-Numbers = {Chepur, Petr/0000-0002-6722-459X
   Tarasenko, Alexandr/0000-0002-3903-0632
   Gruchenkova, Alesya/0000-0002-5376-5221},
Unique-ID = {WOS:000622970800066},
}

@article{ WOS:000454578700008,
Author = {Vasiliev, Eugene},
Title = {AGAMA: action-based galaxy modelling architecture},
Journal = {MONTHLY NOTICES OF THE ROYAL ASTRONOMICAL SOCIETY},
Year = {2019},
Volume = {482},
Number = {2},
Pages = {1525-1544},
Month = {JAN},
Abstract = {AGAMA is a publicly available software library for a broad range of
   applications in the field of stellar dynamics. It provides methods for
   computing the gravitational potential of arbitrary analytic density
   profiles or N-body models, orbit integration and analysis,
   transformations between position/velocity and action/angle variables,
   distribution functions expressed in terms of actions and their moments,
   and iterative construction of self-consistent multicomponent galaxy
   models. Applications include the inference about the structure of Milky
   Way or other galaxies from observations of stellar kinematics,
   preparation of equilibrium initial conditions for N-body simulations,
   and analysis of snapshots from simulations. The library is written in
   C++, provides a PYTHON interface, and can be coupled to other
   stellar-dynamical software: AMUSE, GALPY, and NEMO. It is hosted at
   http://github.com/GalacticDynamics-Oxford/Agama.},
DOI = {10.1093/mnras/sty2672},
ISSN = {0035-8711},
EISSN = {1365-2966},
ResearcherID-Numbers = {Vasiliev, Eugene/M-3699-2015},
ORCID-Numbers = {Vasiliev, Eugene/0000-0002-5038-9267},
Unique-ID = {WOS:000454578700008},
}

@inproceedings{ WOS:000481604500167,
Author = {Zenkov, E. V.},
Book-Group-Author = {IOP},
Title = {Calculation-experimental analysis of the ultimate strength of a material
   under biaxial tension using the finite element method},
Booktitle = {XII INTERNATIONAL SCIENTIFIC AND TECHNICAL CONFERENCE APPLIED MECHANICS
   AND SYSTEMS DYNAMICS},
Series = {Journal of Physics Conference Series},
Year = {2019},
Volume = {1210},
Note = {12th International Scientific and Technical Conference on Applied
   Mechanics and Systems Dynamics, Omsk, RUSSIA, NOV 13-15, 2018},
Abstract = {The article emphasizes the importance of taking into account the
   rigidity of the stress-strain state (SSS) when evaluating the static
   strength of parts and elements of highly loaded structures. The
   description of the universal characteristic of the SSS allowing to
   estimate the SSS rigidity in the construction zone is given. The
   estimation of the SSS rigidity in the zone of destruction of the
   experimental model of a choke unit with a spherical body under its
   quasi-static loading by internal pressure is considered. A calculation
   and experimental technique for estimating the limiting state of
   spring-steel 50CrV4 steel under conditions of biaxial stretching based
   on the use of laboratory prismatic samples for evaluating the strength
   of a material under a biaxial stress state is described. The results of
   numerical finite element analysis of elastoplastic deformation of
   prismatic samples performed in the MSC Marc software package are
   presented. The obtained results show a significant influence of the SSS
   rigidity on the ultimate characteristics of the material of the steel
   used in comparison with its traditional strength limit.},
DOI = {10.1088/1742-6596/1210/1/012167},
Article-Number = {012167},
ISSN = {1742-6588},
EISSN = {1742-6596},
ResearcherID-Numbers = {Zenkov, Evgeniy/E-4990-2017
   },
ORCID-Numbers = {Zen'kov, Evgenii/0000-0003-4414-0307},
Unique-ID = {WOS:000481604500167},
}

@inproceedings{ WOS:000489756800061,
Author = {Zhang, Lu and Swanson, Steven},
Book-Group-Author = {USENIX Assoc},
Title = {Pangolin: A Fault-Tolerant Persistent Memory Programming Library},
Booktitle = {PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE},
Year = {2019},
Pages = {897-911},
Note = {USENIX Annual Technical Conference, Renton, WA, JUL 10-12, 2019},
Organization = {USENIX Assoc; Facebook; VmWare; Nutanix; Oracle; Bloomberg; Google;
   Microsoft; NetApp},
Abstract = {Non-volatile main memory (NVMM) allows programmers to build complex,
   persistent, pointer-based data structures that can offer substantial
   performance gains over conventional approaches to managing persistent
   state. This programming model removes the file system from the critical
   path which improves performance, but it also places these data
   structures out of reach of file system-based fault tolerance mechanisms
   (e.g., block-based checksums or erasure coding). Without
   fault-tolerance, using NVMM to hold critical data will be much less
   attractive.
   This paper presents Pangolin, a fault-tolerant persistent object library
   designed for NVMM. Pangolin uses a combination of checksums, parity, and
   micro-buffering to protect an application's objects from both media
   errors and corruption due to software bugs. It provides these
   protections for objects of any size and supports automatic, online
   detection of data corruption and recovery. The required storage overhead
   is small (1\% for gigabyte-sized pools of NVMM). Pangolin provides
   stronger protection, requires orders of magnitude less storage overhead,
   and achieves comparable performance relative to the current
   state-of-the-art fault-tolerant persistent object library.},
ISBN = {978-1-939133-03-8},
Unique-ID = {WOS:000489756800061},
}

@inproceedings{ WOS:000584318800012,
Author = {Zhao, Ziang and Kang, Yunfan and Magdy, Amr and Cowger, Win and Gray,
   Andrew},
Book-Group-Author = {IEEE},
Title = {A Data-driven Approach for Tracking Human Litter in Modem Cities},
Booktitle = {2019 IEEE 35TH INTERNATIONAL CONFERENCE ON DATA ENGINEERING WORKSHOPS
   (ICDEW 2019)},
Series = {IEEE International Conference on Data Engineering Workshop},
Year = {2019},
Pages = {69-73},
Note = {IEEE 35th International Conference on Data Engineering (ICDE), Macau,
   PEOPLES R CHINA, APR 08-11, 2019},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {In the recent years, human litter, such as food waste, diapers,
   construction materials, used motor oil, hypodermic needles, etc, is
   causing growing problems for the environment and quality of life in
   modern cities. Data about this waste has a significant importance in the
   field of environmental sciences due to its important use cases that span
   saving marine life, reducing the risk from natural hazards, community
   cleaning efforts, etc. In addition, such litter spreads several diseases
   in urban areas with high populations such as undeveloped neighborhoods
   in large modern cities. In this paper, we introduce a data-driven
   approach that enables environmental scientists and organizations to
   track, manage, and model human litter data at a large scale through
   smart technologies. We make a major on-going effort to collect and
   maintain this data worldwide from different sources through a community
   of environmental scientists and partner organizations. With the
   increasing volume of collected datasets, existing software packages,
   such as GIS software, do not scale to process, query, and visualize such
   data. To overcome this, we provide a scalable data management and
   visualization framework that digests datasets from different sources,
   with different formats, in a scalable backend that cleans, integrates,
   and unifies them in a structured form. On top of this backend, frontend
   applications are built to visualize litter data at multiple spatial
   levels, from continents and oceans to street level, to enable new
   opportunities for both environmental scientists and organizations to
   track, model, and clean up litter data. The framework is currently
   managing thirty real datasets and provide different interfaces for
   different kinds of users.},
DOI = {10.1109/ICDEW.2019.00-33},
ISSN = {1943-2895},
ISBN = {978-1-7281-0890-2},
ResearcherID-Numbers = {Foster, Ian/GNH-1877-2022
   Zhang, Zhao/AAX-9485-2021
   },
ORCID-Numbers = {Foster, Ian/0000-0003-2129-5269
   Zhang, Zhao/0000-0001-5921-0035},
Unique-ID = {WOS:000584318800012},
}

@article{ WOS:000465500500001,
Author = {Zhu, Hanbo and Miao, Changqing and Zhuang, Meiling},
Title = {Analysis of Office-Teaching Comprehensive Buildings Using a Modified
   Seismic Performance Evaluation Method},
Journal = {CMES-COMPUTER MODELING IN ENGINEERING \& SCIENCES},
Year = {2019},
Volume = {118},
Number = {3},
Pages = {471-491},
Abstract = {Current building design codes allow the appearance of structural and
   nonstructural damage under design basis earthquakes. The research
   regarding probabilistic seismic loss estimation in domestic building
   structure is urgent. The evaluation in this paper is based on a 11-story
   reinforced concrete office building, incremental dynamic analysis (IDA)
   is conducted in Perform 3D program using models capable to simulate all
   possible limit states up to collapse. Next, the probability distribution
   of post-earthquake casualties, rebuild costs repair costs and business
   downtime loss are calculated in PACT software for the studied building
   considering the modified component vulnerability groups and population
   flow models. The evaluation procedure can also shed light on other types
   of buildings in China. For non-typical functional building structures,
   this article proposes to build a finite element model of structural
   components and to classify the vulnerability groups based on the
   construction drawings, and to supply and improve the vulnerability
   library of appendages in FEMA P-58 according to the actual situation. In
   this way, the application scope of building seismic performance
   evaluation can be expanded.},
DOI = {10.31614/cmes.2019.04382},
ISSN = {1526-1492},
EISSN = {1526-1506},
ResearcherID-Numbers = {Zhuang, Meiling/AAC-1507-2020},
Unique-ID = {WOS:000465500500001},
}

@article{ WOS:000454342900001,
Author = {Ajmal, P. C. Hisham and Mohammed, Althaf},
Title = {Finite element analysis based fatigue life evaluation approach for
   railway bridges: a study in Indian scenario},
Journal = {STRUCTURAL MONITORING AND MAINTENANCE},
Year = {2018},
Volume = {5},
Number = {4},
Pages = {429-443},
Month = {DEC},
Abstract = {Fatigue is a principal failure mode for steel structures, and it is
   still less understood than any other modes of failure. Fatigue life
   estimation of metal bridges is a major issue for making cost effective
   decisions on the rehabilitation or replacement of existing
   infrastructure. The fatigue design procedures given by the standard
   codes are either empirical or based on nominal stress approach. Since
   the fatigue life estimation through field measurements is difficult and
   costly, more researches are needed to develop promising techniques in
   the fatigue analysis of bridges through Finite Element Analysis (FLA).
   This paper aims to develop a methodology for the Fatigue life estimation
   of railway steel bridge using FEA. The guidelines of IIW-1823-07 were
   used in the development of the methodology. The Finite Element (FE)
   package ANSYS and the programming software MATLAB were used to implement
   this methodology on an Indian Railway Standard (IRS) welded plate girder
   bridge. The results obtained were compared with results from published
   literature and found satisfactory.},
DOI = {10.12989/smm.2018.5.4.429},
ISSN = {2288-6605},
EISSN = {2288-6613},
Unique-ID = {WOS:000454342900001},
}

@article{ WOS:000454221200022,
Author = {Liberato, Alextian and Martinello, Magnos and Gomes, Roberta L. and
   Beldachi, Arash F. and Salas, Emilio and Villaca, Rodolfo and Ribeiro,
   Moises R. N. and Kondepu, Koteswararao and Kanellos, George and
   Nejabati, Reza and Gorodnik, Alexander and Simeonidou, Dimitra},
Title = {RDNA: Residue-Defined Networking Architecture Enabling Ultra-Reliable
   Low-Latency Datacenters},
Journal = {IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT},
Year = {2018},
Volume = {15},
Number = {4},
Pages = {1473-1487},
Month = {DEC},
Abstract = {Datacenter (DC) design has been moved toward the edge computing paradigm
   motivated by the need of bringing cloud resources closer to end users.
   However, the software defined networking (SDN) architecture offers no
   clue to the design of micro DCs (MDCs) for meeting complex and stringent
   requirements from next generation 5G networks. This is because canonical
   SDN lacks a clear distinction between functional network parts, such as
   core and edge elements. Besides, there is no decoupling between the
   routing and the network policy. In this paper, we introduce residue
   defined networking architecture (RDNA) as a new approach for enabling
   key features like ultra-reliable and low-latency communication in MDC
   networks. RDNA explores the programmability of residues number system as
   a fundamental concept to define a minimalist forwarding model for core
   nodes. Instead of forwarding packets based on classical table lookup
   operations, core nodes are tableless switches that forward packets using
   merely remainder of the division (modulo) operations. By solving a
   residue congruence system representing a network topology, we found out
   the algorithms and their mathematical properties to design RDNA's
   routing system that: 1) supports unicast and multicast communication; 2)
   provides resilient routes with protection for the entire route; and 3)
   is scalable for 2-tier Clos topologies. Experimental implementations on
   Mininet and NetFPGA SUME show that RDNA achieves 600 ns switching
   latency per hop with virtually no jitter at core nodes and
   sub-millisecond failure recovery time.},
DOI = {10.1109/TNSM.2018.2876845},
ISSN = {1932-4537},
ResearcherID-Numbers = {Hugues-Salas, Emilio/AAZ-1830-2020
   Kondepu, Koteswararao/AAK-1066-2021
   Kondepu, Koteswararao/J-9184-2014
   Ribeiro, Moises R.N./O-7024-2017
   VILLACA, RODOLFO/N-1899-2016
   },
ORCID-Numbers = {Kondepu, Koteswararao/0000-0003-0184-1218
   Ribeiro, Moises R.N./0000-0002-9149-2391
   Bartholomeu Liberato, Alextian/0000-0001-8592-455X
   VILLACA, RODOLFO/0000-0002-8051-3978
   Nejabati, Reza/0000-0003-4664-9369},
Unique-ID = {WOS:000454221200022},
}

@article{ WOS:000454567700013,
Author = {Oses, Corey and Gossett, Eric and Hicks, David and Rose, Frisco and
   Mehl, Michael J. and Perim, Eric and Takeuchi, Ichiro and Sanvito,
   Stefano and Scheffler, Matthias and Lederer, Yoav and Levy, Ohad and
   Toher, Cormac and Curtarolo, Stefano},
Title = {AFLOW-CHULL: Cloud-Oriented Platform for Autonomous Phase Stability
   Analysis},
Journal = {JOURNAL OF CHEMICAL INFORMATION AND MODELING},
Year = {2018},
Volume = {58},
Number = {12, SI},
Pages = {2477-2490},
Month = {DEC},
Abstract = {A priori prediction of phase stability of materials is a challenging
   practice, requiring knowledge of all energetically competing structures
   at formation conditions. Large materials repositories-housing properties
   of both experimental and hypothetical compounds-offer a path to
   prediction through the construction of informatics-based, ab initio
   phase diagrams. However, limited access to relevant data and software
   infrastructure has rendered thermodynamic characterizations largely
   peripheral, despite their continued success in dictating
   synthesizability. Herein, a new module is presented for autonomous
   thermodynamic stability analysis, implemented within the open source, ab
   initio framework AFLOW. Powered by the AFLUX Search-API, AFLOW-CHULL
   leverages data of more than 1.8 million compounds characterized in the
   AFLOW.org repository, and can be employed locally from any UNIX-like
   computer. The module integrates a range of functionality: the
   identification of stable phases and equivalent structures, phase
   coexistence, measures for robust stability, and determination of
   decomposition reactions. As a proof of concept, thermodynamic
   characterizations have been performed for more than 1300 binary and
   ternary systems, enabling the identification of several candidate phases
   for synthesis based on their relative stability criterion-including 17
   promising C15(b)-type structures and 2 half-Heuslers. In addition to a
   full report included herein, an interactive, online web application has
   been developed showcasing the results of the analysis and is located at
   aflow.org/aflow-chull.},
DOI = {10.1021/acs.jcim.8b00393},
ISSN = {1549-9596},
EISSN = {1549-960X},
ResearcherID-Numbers = {Sanvito, Stefano/ABF-4353-2020
   Curtarolo, Stefano/ABD-7672-2021
   },
ORCID-Numbers = {Curtarolo, Stefano/0000-0003-0570-8238
   Sanvito, Stefano/0000-0002-0291-715X
   Hicks, David/0000-0001-5813-6785
   Oses, Corey/0000-0002-3790-1377
   Mehl, Michael/0000-0001-9402-6591},
Unique-ID = {WOS:000454567700013},
}

@article{ WOS:000577186800001,
Author = {Ayele, Yohanes and Mamu, Mulu},
Title = {Assessment of knowledge, attitude and practice towards disposal of
   unused and expired pharmaceuticals among community in Harar city,
   Eastern Ethiopia},
Journal = {JOURNAL OF PHARMACEUTICAL POLICY AND PRACTICE},
Year = {2018},
Volume = {11},
Number = {1},
Month = {NOV 15},
Abstract = {BackgroundPharmaceuticals are produced and consumed in increasing volume
   every year. Unfortunately, not all medications that go in to the hand of
   the consumers get consumed; large quantities remain unused or expire.
   The accumulation of medication at household and unsafe disposal of
   unwanted medicines could lead to inappropriate medicine sharing,
   accidental childhood poisonings and diversion of medicines to illicit
   use.MethodsA descriptive cross-sectional study was conducted among 695
   residents in kebele(ward) 16 of Jinela woreda(district), Harar city from
   February 27-April 27. A multi-stage sampling technique was used to
   select individual households. Face-to-face interview using structured
   questionnaires were conducted to collect data from each study subject.
   The cleaned data was entered in to epidata analyzed using SPSS version
   20 software. Descriptive statistics on sample characteristics was
   computed including frequencies and percentage and presented using tables
   and figures.ResultsMost participants displayed correct understanding
   toward medication waste (72.9\%) and its effect on environmental if
   disposed improperly (86\%). A large portion of the respondents did not
   know about drug-take-back system 464 (66.9\%). In order to minimize the
   entry of pharmaceuticals into environment, 68.6\% of the participants
   suggested the need for proper guidance to the consumer. Majority of the
   respondents believed risk related to the presence of unwanted drug in
   home, potential harm to children, lack of adequate information on safe
   disposal practice and need for take-back program. Approximately 66\% of
   the respondents had unused medicine stored at home and the common types
   of medicines kept in households were analgesics (62.7\%) and antibiotics
   (24\%). Preferred ways of disposal of both unused and expired medicine
   was throwing away in household garbage (53.2\%) and two third of them
   disposed the pharmaceuticals in its original package and dosage
   form.ConclusionIn present study, there was high practice of keeping
   medication at home and most disposal approach indicated by the
   participants was not recommended methods. Awareness about proper
   disposal of unused and expired medicines among the public should be
   created. Guidelines on safe disposal are required and an organized
   method of collecting unused and expired pharmaceuticals needs to be
   introduced.},
DOI = {10.1186/s40545-018-0155-9},
Article-Number = {27},
EISSN = {2052-3211},
ORCID-Numbers = {Ayele, Yohanes/0000-0002-2869-7917},
Unique-ID = {WOS:000577186800001},
}

@article{ WOS:000450530000006,
Author = {Akhoundan, Majid Reza and Khademi, Kia and Bahmanoo, Sam and Wakil,
   Karzan and Mohamad, Edy Tonnizam and Khorami, Majid},
Title = {Practical use of computational building information modeling in
   repairing and maintenance of hospital building- case study},
Journal = {SMART STRUCTURES AND SYSTEMS},
Year = {2018},
Volume = {22},
Number = {5},
Pages = {575-586},
Month = {NOV},
Abstract = {Computational Building Information Modeling (BIM) is an intelligent 3D
   model-based process that provides architecture, engineering, and
   construction professionals the insight to plan, design, construct, and
   manage buildings and infrastructure more efficiently. This paper aims at
   using BIM in Hospitals configurations protection. Infrastructure
   projects are classified as huge structural projects taking advantage of
   many resources such as finance, materials, human labor, facilities and
   time. Immense expenses in infrastructure programs should be allocated to
   estimating the expected results of these arrangements in domestic
   economy. Hence, the significance of feasibility studies is inevitable in
   project construction, in this way the necessity in promoting the
   strategies and using global contemporary technologies in the process of
   construction maintenance cannot be neglected. This paper aims at using
   the building information modeling in covering Imam Khomeini Hospital's
   equipment. First, the relationship between hospital constructions
   maintenance and repairing, using the building information modeling, is
   demonstrated. Then, using library studies, the effective factors of
   constructions' repairing and maintenance were collected. Finally, the
   possibilities of adding these factors in Revit software, as one of the
   most applicable software within BIM is investigated and have been
   identified in some items, where either this software can enter or the
   software for supporting the repairing and maintenance phase lacks them.
   The results clearly indicated that the required graphical factors in
   construction information modeling can be identified and applied
   successfully.},
DOI = {10.12989/sss.2018.22.5.575},
ISSN = {1738-1584},
ResearcherID-Numbers = {Khorami, Majid/AAN-1515-2021},
Unique-ID = {WOS:000450530000006},
}

@article{ WOS:000446539400020,
Author = {Formisano, Antonio and Vaiano, Generoso and Fabbrocino, Francesco and
   Milani, Gabriele},
Title = {Seismic vulnerability of Italian masonry churches: The case of the
   Nativity of Blessed Virgin Mary in Stellata of Bondeno},
Journal = {JOURNAL OF BUILDING ENGINEERING},
Year = {2018},
Volume = {20},
Pages = {179-200},
Month = {NOV},
Abstract = {In this paper the problem of seismic vulnerability of masonry churches
   is analysed with reference to the Nativity of Blessed Virgin Mary
   ecclesiastic complex in Stellata of Bondeno (Italy). This religious
   construction is composed of a church, two bell towers, a cloister and
   the Saint Domenico's oratory. The church, made of masonry brick stones,
   is characterized by a single hall with chapels, which is divided into
   three parts by arches, and has an apse elevated by a few steps with
   reference to the hall. The study herein presented is carried out
   according to the Italian Standards and Guidelines on Cultural Heritage.
   In a first step, the seismic risk coefficients both at Damage Limit
   State (a DLS) and Ultimate Limit State (a ULS) are evaluated by using
   the 3Muri calculation software for masonry structures. These
   coefficients indicate the ratio between the ground acceleration leading
   towards attainment of the two mentioned limit states and the PGAs of the
   site referred to a given reference return period. Afterwards, the
   variability of such coefficients is examined by changing both the
   masonry type and the seismic zone in order to detect the worst
   situations on the Italian land.
   In a second analysis step a comparison among the seismic risk
   coefficients a ULS and the damage index calculated through a fast method
   provided in a suitable form by the Italian Civil Protection Department
   is proposed.
   Moreover, overturning mechanisms of facades are checked by using the
   3Muri software with the ultimate goal to compare the predictive
   theoretical results with the real damages detected after the 2012 Emilia
   Romagna earthquake.
   Finally, in order to obtain a more precise assessment of the seismic
   behaviour of the church under study, linear and non-linear dynamic
   analyses are performed on a 3D FEM model setup through the ABAQUS
   software package.},
DOI = {10.1016/j.jobe.2018.07.017},
ISSN = {2352-7102},
ResearcherID-Numbers = {Formisano, Antonio/AAR-9942-2020
   Fabbrocino, Francesco/AFR-4646-2022},
ORCID-Numbers = {Formisano, Antonio/0000-0003-3592-4011
   },
Unique-ID = {WOS:000446539400020},
}

@article{ WOS:000447575300003,
Author = {Stachtiari, Emmanouela and Mavridou, Anastasia and Katsaros, Panagiotis
   and Bliudze, Simon and Sifakis, Joseph},
Title = {Early validation of system requirements and design through
   correctness-by-construction},
Journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
Year = {2018},
Volume = {145},
Pages = {52-78},
Month = {NOV},
Abstract = {Early validation of requirements aims to reduce the need for the
   high-cost validation testing and corrective measures at late development
   stages. This work introduces a systematic process for the unambiguous
   specification of system requirements and the guided derivation of formal
   properties, which should be implied by the system `s structure and
   behavior in conjunction with its external stimuli. This rigorous design
   takes place through the incremental construction of a model using the
   BIP (Behavior-Interaction-Priorities) component framework. It allows
   building complex designs by composing simpler reusable designs enforcing
   given properties. If some properties are neither enforced nor verified,
   the model is refined or certain requirements are revised. A validated
   model provides evidence of requirements' consistency and design
   correctness. The process is semi-automated through a new tool and
   existing verification tools. Its effectiveness was evaluated on a set of
   requirements for the control software of the CubETH nanosatellite and an
   extract of software requirements for a Low Earth Orbit observation
   satellite. Our experience and obtained results helped in identifying
   open challenges for applying the method in industrial context. These
   challenges concern with the domain knowledge representation, the
   expressiveness of used specification languages, the library of reusable
   designs and scalability.},
DOI = {10.1016/j.jss.2018.07.053},
ISSN = {0164-1212},
EISSN = {1873-1228},
ResearcherID-Numbers = {Bliudze, Simon/ABD-8558-2021
   },
ORCID-Numbers = {Bliudze, Simon/0000-0002-7900-5271
   Katsaros, Panagiotis/0000-0002-4309-5295},
Unique-ID = {WOS:000447575300003},
}

@article{ WOS:000445717000008,
Author = {Zapalowski, Vanius and Nunes, Ingrid and Nunes, Daltro Jose},
Title = {The WGB method to recover implemented architectural rules},
Journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
Year = {2018},
Volume = {103},
Pages = {125-137},
Month = {NOV},
Abstract = {Context: The identification of architectural rules, which specify
   allowed dependencies among architectural modules, is a key challenge in
   software architecture recovery. Existing approaches either retrieve a
   large set of rules, compromising their practical use, or are limited to
   supporting the understanding of such rules, which are manually
   recovered.
   Objective: To propose and evaluate a method to recover architectural
   rules, focusing on those implemented in the source code, which may
   differ from planned or conceptual rules.
   Method: We propose the WGB method, which analyzes dependencies among
   architectural modules as a graph, adding weights that correspond to the
   proposed module dependency strength (MDS) metric and identifies the set
   of implemented architectural rules by solving a mathematical
   optimization problem. We evaluated our method with a case study and an
   empirical study that compared rules extracted by the method with the
   conceptual architecture and source code dependencies of six systems.
   These comparisons considered efficiency and effectiveness of our method.
   Results: Regarding efficiency, our method took 45.55 s to analyze the
   largest system evaluated. Considering effectiveness, our method captured
   package dependencies as extracted rules with a reduction of 87.6\%, on
   average, to represent this information. Using allowed architectural
   dependencies as a reference point (but not a gold standard), provided
   rules achieved 37.1\% of precision and 37.8\% of recall.
   Conclusion: Our empirical evaluation shows that the implemented
   architectural rules recovered by our method consist of abstract
   representations of (a large number of) module dependencies, providing a
   concise view of dependencies that can be inspected by developers to
   identify occurrences of architectural violations and undocumented rules.},
DOI = {10.1016/j.infsof.2018.06.012},
ISSN = {0950-5849},
EISSN = {1873-6025},
ResearcherID-Numbers = {Nunes, Ingrid/A-3715-2014},
ORCID-Numbers = {Nunes, Ingrid/0000-0002-6879-5829},
Unique-ID = {WOS:000445717000008},
}

@article{ WOS:000445858900012,
Author = {Failla, Giuseppe and Santangelo, Fabio and Foti, Gabriele and Scali,
   Fabio and Arena, Felice},
Title = {Response-Spectrum Uncoupled Analyses for Seismic Assessment of Offshore
   Wind Turbines},
Journal = {JOURNAL OF MARINE SCIENCE AND ENGINEERING},
Year = {2018},
Volume = {6},
Number = {3},
Month = {SEP},
Abstract = {According to International Standards and Guidelines, the seismic
   assessment of offshore wind turbines in seismically-active areas may be
   performed by combining two uncoupled analyses under wind-wave and
   earthquake loads, respectively. Typically, the separate earthquake
   response is calculated by a response-spectrum approach and, for this
   purpose, structural models of various degrees of complexity may be used.
   Although response-spectrum uncoupled analyses are currently allowed as
   alternative to time-consuming fully-coupled simulations, for which
   dedicated software packages are required, to date no specific studies
   have been presented on whether accuracy may vary depending on key
   factors as structural modelling, criteria to calculate wind-wave and
   earthquake responses, and other relevant issues as the selected support
   structure, the considered environmental states and earthquake records.
   This paper will investigate different potential implementations of
   response-spectrum uncoupled analyses for offshore wind turbines, using
   various structural models and criteria to calculate the wind-wave and
   earthquake responses. The case study is a 5-MW wind turbine on two
   support structures in intermediate waters, under a variety of wind-wave
   states and real earthquake records. Numerical results show that
   response-spectrum uncoupled analyses may provide non-conservative
   results, for every structural model adopted and criteria to calculate
   wind-wave and earthquake responses. This is evidence that appropriate
   safety factors should be assumed when implementing response-spectrum
   uncoupled analyses allowed by International Standards and Guidelines.},
DOI = {10.3390/jmse6030085},
Article-Number = {85},
ISSN = {2077-1312},
ORCID-Numbers = {Arena, Felice/0000-0002-0517-1859
   FAILLA, Giuseppe/0000-0003-4244-231X},
Unique-ID = {WOS:000445858900012},
}

@article{ WOS:000447365100094,
Author = {Floresta, Giuseppe and Apirakkan, Orapan and Rescifina, Antonio and
   Abbate, Vincenzo},
Title = {Discovery of High-Affinity Cannabinoid Receptors Ligands through a
   3D-QSAR Ushered by Scaffold-Hopping Analysis},
Journal = {MOLECULES},
Year = {2018},
Volume = {23},
Number = {9},
Month = {SEP},
Abstract = {Two 3D quantitative structure-activity relationships (3D-QSAR) models
   for predicting Cannabinoid receptor 1 and 2 (CB1 and CB2) ligands have
   been produced by way of creating a practical tool for the drug-design
   and optimization of CB1 and CB2 ligands. A set of 312 molecules have
   been used to build the model for the CB1 receptor, and a set of 187
   molecules for the CB2 receptor. All of the molecules were recovered from
   the literature among those possessing measured K-i values, and Forge was
   used as software. The present model shows high and robust predictive
   potential, confirmed by the quality of the statistical analysis, and an
   adequate descriptive capability. A visual understanding of the
   hydrophobic, electrostatic, and shaping features highlighting the
   principal interactions for the CB1 and CB2 ligands was achieved with the
   construction of 3D maps. The predictive capabilities of the model were
   then used for a scaffold-hopping study of two selected compounds, with
   the generation of a library of new compounds with high affinity for the
   two receptors. Herein, we report two new 3D-QSAR models that comprehend
   a large number of chemically different CB1 and CB2 ligands and well
   account for the individual ligand affinities. These features will
   facilitate the recognition of new potent and selective molecules for CB1
   and CB2 receptors.},
DOI = {10.3390/molecules23092183},
Article-Number = {2183},
EISSN = {1420-3049},
ResearcherID-Numbers = {Abbate, Vincenzo/K-6204-2019
   Rescifina, Antonio/E-7412-2012
   Floresta, Giuseppe/X-6629-2019
   SPENCER, John/P-5492-2019
   },
ORCID-Numbers = {Rescifina, Antonio/0000-0001-5039-2151
   SPENCER, John/0000-0001-5231-8836
   Floresta, Giuseppe/0000-0002-0668-1260
   Apirakkan, Orapan/0000-0002-1189-3428
   Abbate, Vincenzo/0000-0002-3300-0520},
Unique-ID = {WOS:000447365100094},
}

@article{ WOS:000452645000002,
Author = {Loianno, Giuseppe and Mulgaonkar, Yash and Brunner, Chris and Ahuja,
   Dheeraj and Ramanandan, Arvind and Chari, Murali and Diaz, Serafin and
   Kumar, Vijay},
Title = {Autonomous flight and cooperative control for reconstruction using
   aerial robots powered by smartphones},
Journal = {INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH},
Year = {2018},
Volume = {37},
Number = {11},
Pages = {1341-1358},
Month = {SEP},
Abstract = {Advances in consumer electronics products and the technology seen in
   personal computers, digital cameras, and smartphones phones have led to
   the price/performance ratio of sensors and processors falling
   dramatically over the last decade. In particular, many consumer products
   are packaged with small cameras, gyroscopes, and accelerometers, all
   sensors that are needed for autonomous robots in GPS-denied
   environments. The low mass and small form factor make them particularly
   well suited for autonomous flight with small flying robots. In this
   work, we present the first fully autonomous smartphone-based system for
   quadrotors. We show how multiple quadrotors can be stabilized and
   controlled to achieve autonomous flight in indoor buildings with
   application to smart homes, search and rescue, monitoring construction
   projects, and developing models for architecture design. In our work,
   the computation for sensing and control runs on an off-the-shelf
   smartphone, with all the software functionality embedded in a smartphone
   app. No additional sensors or processors are required for autonomous
   flight. We are also able to use multiple, coordinated autonomous aerial
   vehicles to improve the efficiency of our mission. In our framework,
   multiple vehicles are able to plan safe trajectories avoiding
   inter-robot collisions, while concurrently building in a cooperative
   manner a three-dimensional map of the environment. The work allows any
   consumer with any number of robots equipped with smartphones to
   autonomously drive a team of quadrotor robots, even without GPS, by
   downloading our app and cooperatively build three-dimensional maps.},
DOI = {10.1177/0278364918774136},
ISSN = {0278-3649},
EISSN = {1741-3176},
ResearcherID-Numbers = {Kumar, Vijay/AAM-8545-2020
   Loianno, Giuseppe/E-7473-2017},
ORCID-Numbers = {Kumar, Vijay/0000-0002-3902-9391
   Loianno, Giuseppe/0000-0002-3263-5401},
Unique-ID = {WOS:000452645000002},
}

@article{ WOS:000440114400006,
Author = {Nair, Nitish and Koelman, J. Vianney},
Title = {An Ising-Based Simulator for Capillary Action in Porous Media},
Journal = {TRANSPORT IN POROUS MEDIA},
Year = {2018},
Volume = {124},
Number = {2},
Pages = {413-437},
Month = {SEP},
Abstract = {Multiphase flows in porous media are encountered in several
   contexts-e.g., hydrocarbon recovery operations, battery electrodes,
   microfluidic devices, etc. Capillary-dominated flows are interesting due
   to the complex interplay of interfacial properties and pore geometries.
   Conventional hydrodynamic flow solvers are computationally inefficient
   in the capillary-dominated regime, particularly in complex pore
   structures. The algorithm developed here specifically targets this
   regime to reduce simulation times. We minimise the fluid-fluid and
   fluid-solid interaction energies through an approach inspired by the
   ferromagnetic Ising model. We validate the algorithm on (1) model pore
   geometries with analytical solutions for capillary action, and (2) rocks
   with available mercury porosimetry data. We validate its predictions for
   model geometries and sandstones using (1) curvatures calculated from
   theories developed by Mayer-Stowe-Princen, Ma and Morrow, and Mason and
   Morrow; (2) predictions from GeoDict, a commercial software package,
   which also includes a state-of-the-art drainage simulator; (3) mercury
   porosimetry data. Drainage capillary pressure curves predicted for
   Bentheimer and Fontainebleau rocks reasonably match porosimetry data.},
DOI = {10.1007/s11242-018-1075-5},
ISSN = {0169-3913},
EISSN = {1573-1634},
Unique-ID = {WOS:000440114400006},
}

@article{ WOS:000438294800021,
Author = {Zhang, Lu and Li, Yuli and Li, Yangping and Yang, Zhihui and Li, Yuqiang
   and Wang, Yangfan and Wang, Shi and Bao, Zhenmin},
Title = {A Population Genetic Analysis of Continuously Selected Chlamys farreri
   Populations},
Journal = {JOURNAL OF OCEAN UNIVERSITY OF CHINA},
Year = {2018},
Volume = {17},
Number = {4},
Pages = {913-919},
Month = {AUG},
Abstract = {This study applied an optimized one-step 2b-RAD library construction
   strategy and performed simplified genome sequencing of 539 individuals
   from three continuously selected Chlamys farreri populations. SNP
   screening was performed using RAD typing software and population genetic
   parameters for the continuously selected populations from three
   generations (G1, G2, G3) were determined. The results showed that the
   optimized one-step 2b-RAD library construction strategy greatly
   simplified the experimental process, making it suitable for efficiently
   constructing a large number of libraries. A total of 18450 SNP markers
   were identified, which evenly distributed throughout the genome.
   Population genetic analysis of these three generations showed that the
   mean value of observed heterozygosity was 0.275 +/- 0.177, 0.272 +/-
   0.181 and 0.275 +/- 0.166, respectively. Meanwhile, the mean value of
   expected heterozygosity was 0.275 +/- 0.141, 0.274 +/- 0.145 and 0.280
   +/- 0.133, respectively. The Wright's fixation index (F) was 0.04291,
   0.04976 and 0.06685, respectively. Markers deviated from Hardy-Weinberg
   equilibrium accounted for 10.34\%, 12.64\%, and 23.11\%, and the Shannon
   diversity index was 0.0999 +/- 0.0404, 0.0921 +/- 0.0388 and 0.0733 +/-
   0.0308. F (IS) (also known as the inbreeding coefficient) of the three
   populations was 0.0256, 0.0323 and 0.0468, respectively. We suggested
   that the 2b-RAD method is well suited to population genetic studies of
   aquacultured organisms. Moreover, our results indicated that the
   continuous selection affected the population genetic structure of the
   cultured Penglai-Red scallop, but the change was not significant;
   therefore, population selection should continue.},
DOI = {10.1007/s11802-018-3539-1},
ISSN = {1672-5182},
EISSN = {1993-5021},
Unique-ID = {WOS:000438294800021},
}

@article{ WOS:000438637700005,
Author = {Sharples, Wendy and Zhukov, Ilya and Geimer, Markus and Goergen, Klaus
   and Luehrs, Sebastian and Breuer, Thomas and Naz, Bibi and Kulkarni,
   Ketan and Brdar, Slavko and Kollet, Stefan},
Title = {A run control framework to streamline profiling, porting, and tuning
   simulation runs and provenance tracking of geoscientific applications},
Journal = {GEOSCIENTIFIC MODEL DEVELOPMENT},
Year = {2018},
Volume = {11},
Number = {7},
Pages = {2875-2895},
Month = {JUL 13},
Abstract = {Geoscientific modeling is constantly evolving, with next-generation
   geoscientific models and applications placing large demands on
   high-performance computing (HPC) resources. These demands are being met
   by new developments in HPC architectures, software libraries, and
   infrastructures. In addition to the challenge of new massively parallel
   HPC systems, reproducibility of simulation and analysis results is of
   great concern. This is due to the fact that next-generation
   geoscientific models are based on complex model implementations and
   profiling, modeling, and data processing workflows. Thus, in order to
   reduce both the duration and the cost of code migration, aid in the
   development of new models or model components, while ensuring
   reproducibility and sustainability over the complete data life cycle, an
   automated approach to profiling, porting, and provenance tracking is
   necessary. We propose a run control framework (RCF) integrated with a
   workflow engine as a best practice approach to automate profiling,
   porting, provenance tracking, and simulation runs. Our RCF encompasses
   all stages of the modeling chain: (1) preprocess input, (2) compilation
   of code (including code instrumentation with performance analysis
   tools), (3) simulation run, and (4) postprocessing and analysis, to
   address these issues. Within this RCF, the workflow engine is used to
   create and manage benchmark or simulation parameter combinations and
   performs the documentation and data organization for reproducibility. In
   this study, we outline this approach and highlight the subsequent
   developments scheduled for implementation born out of the extensive
   profiling of ParFlow. We show that in using our run control framework,
   testing, benchmarking, profiling, and running models is less time
   consuming and more robust than running geoscientific applications in an
   ad hoc fashion, resulting in more efficient use of HPC resources, more
   strategic code development, and enhanced data integrity and
   reproducibility.},
DOI = {10.5194/gmd-11-2875-2018},
ISSN = {1991-959X},
EISSN = {1991-9603},
ResearcherID-Numbers = {Naz, Bibi/AAH-1916-2021
   Goergen, Klaus/A-4655-2017
   },
ORCID-Numbers = {Goergen, Klaus/0000-0002-4208-3444
   Luhrs, Sebastian/0000-0001-8496-8630
   Geimer, Markus/0000-0002-5575-8287
   Breuer, Thomas/0000-0003-3979-4795},
Unique-ID = {WOS:000438637700005},
}

@article{ WOS:000431470100015,
Author = {Andriamamonjy, Ando and Saelens, Dirk and Klein, Ralf},
Title = {An automated IFC-based workflow for building energy performance
   simulation with Modelica},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2018},
Volume = {91},
Pages = {166-181},
Month = {JUL},
Abstract = {Steadily increasing use of Building Information Modeling (BIM) in all
   phases of building's lifecycle, together with more attention for openBIM
   and growing software support for the most recent version of the Industry
   Foundation Classes (IFC 4) have created a very promising context for an
   even broader application of Building Energy Performance Simulation
   (BEPS). At the same time, an urgent need for modeling guidelines and
   standardization becomes evident. A well-defined BIM-based workflow and a
   set of tools that fully exploit and extend the possibilities of the
   openBIM-technology can make the difference when it comes to reliability
   and cost of BEPS to design, build and operate high-performance
   buildings. This paper describes the essential elements of this
   integrated workflow, explains why openBIM comprises much more than just
   a standardized file-format and what is achieved with the already
   available technology, namely the Information Delivery Manual (IDM) and a
   newly developed Model View Definition. This MVD is tailored to the needs
   of Building Energy Performance Simulation (BEPS) that uses the Modelica
   language together with a specific library (IDEAS) and can easily be
   adapted to other libraries. In this project, several tools have been
   developed to closely integrate BEPS and IFC4. The simulation engine now
   gets the vast majority of the required input directly from the
   IFC4-file. For the implementation of the tools, the PYTHON language and
   the open source library IfcOpenShell are used. A case study is
   presented, that was used for extensive tests of the proposed approach
   and the implemented tools. The essential benefits of this new workflow
   are illustrated, and the feasibility is demonstrated. Opportunities and
   remaining bottlenecks are identified to encourage further development of
   BIM software to fully support IFC4 as an information source for BEPS.
   Besides some improvements of the proprietary class structure and
   functionality, enabling the export of IFC4 files based on custom MVDs is
   one required key feature.},
DOI = {10.1016/j.autcon.2018.03.019},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Saelens, Dirk/AAA-6463-2020
   },
ORCID-Numbers = {Saelens, Dirk/0000-0003-3450-5448},
Unique-ID = {WOS:000431470100015},
}

@article{ WOS:000432881400009,
Author = {Khomh, Foutse and Abtahizadeh, S. Amirhossein},
Title = {Understanding the impact of cloud patterns on performance and energy
   consumption},
Journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
Year = {2018},
Volume = {141},
Pages = {151-170},
Month = {JUL},
Abstract = {Cloud patterns are abstract solutions to recurrent design problems in
   the cloud. Previous work has shown that these patterns can improve the
   Quality of Service (QoS) of cloud applications but their impact on
   energy consumption is still unknown. In this work, we conduct an
   empirical study on two multi-processing and multi-threaded applications
   deployed in the cloud, to investigate the individual and the combined
   impact of six cloud patterns (Local Database Proxy, Local Sharding Based
   Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and
   Filters) on the energy consumption. We measure the energy consumption
   using Power-API; an application programming interface (API) written in
   Java to monitor the energy consumed at the process-level. Results show
   that cloud patterns can effectively reduce the energy consumption of a
   cloud-based application, but not in all cases. In general, there appear
   to be a trade-off between an improved response time of the application
   and the energy consumption. Moreover, our findings show that migrating
   an application to a microservices architecture can improve the
   performance of the application, while significantly reducing its energy
   consumption. We summarize our contributions in the form of guidelines
   that developers and software architects can follow during the
   implementation of a cloud-based application. (C) 2018 Elsevier Inc. All
   rights reserved.},
DOI = {10.1016/j.jss.2018.03.063},
ISSN = {0164-1212},
EISSN = {1873-1228},
Unique-ID = {WOS:000432881400009},
}

@article{ WOS:000429756000010,
Author = {Faik, Steffen and Tauschwitz, Anna and Iosilevskiy, Igor},
Title = {The equation of state package FEOS for high energy density matter},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2018},
Volume = {227},
Pages = {117-125},
Month = {JUN},
Abstract = {Adequate equation of state (EOS) data is of high interest in the growing
   field of high energy density physics and especially essential for
   hydrodynamic simulation codes. The semi-analytical method used in the
   newly developed Frankfurt equation of state (FEOS) package provides an
   easy and fast access to the EOS of - in principle - arbitrary materials.
   The code is based on the well known QEOS model (More et al., 1988; Young
   and Corey, 1995) and is a further development of the MPQeos code (Kemp
   and Meyer-ter Vehn, 1988; Kemp and Meyer-ter Vehn, 1998) from
   Max-Planck-Institut fur Quantenoptik (MPQ) in Garching Germany. The list
   of features contains the calculation of homogeneous mixtures of chemical
   elements and the description of the liquid-vapor two-phase region with
   or without a Maxwell construction. Full flexibility of the package is
   assured by its structure: A program library provides the EOS with an
   interface designed for Fortran or C/C++ codes. Two additional software
   tools allow for the generation of EOS tables in different file output
   formats and for the calculation and visualization of isolines and
   Hugoniot shock adiabats. As an example the EOS of fused silica (SiO2) is
   calculated and compared to experimental data and other EOS codes.
   Program summary
   Program Title: FEOS Frankfurt equation of state P
   rogram Files doi: http://dx.doi.org/10.17632/6vjsv6v48p.1
   Licensing provisions: GNU General Public License version 3
   Programming language: C++
   Supplementary material: Documentation/manual, exemplary input files for
   aluminum (Al)and fused silica (SiO2)
   Nature of problem: The description of a thermodynamic system e.g. by the
   solution of the hydrodynamic conservation equations presumes in the most
   cases reliable equation of state (EOS) data for different materials and
   for a wide range in temperature-density space. The FEOS code provides
   the required thermodynamic quantities like the pressure or the specific
   internal energy per unit mass as functions of density and temperature
   for, in principle, arbitrary materials, for single elements as well as
   for homogeneous mixtures of elements. FEOS is based on the well known
   QEOS model 121 and is a further development of the MPQeos code 111 from
   Max-Planck-Institut fur Quantenoptik in Garching. Since the model was
   designed for the high energy density matter regimes, it can be applied
   e.g. to high-power laser or ion beam and inertial fusion science
   applications. The most important advantage of the FEOS package is an
   easy and fast access to materials which may not be available by more
   complex EOS codes.
   Solution method: In the QEOS model the thermodynamic quantities are
   derived from the specific Helmholtz free energy f = is an element of-
   Ts, which is composed of three contributions f = f(e) + f(i) + f(b): (1)
   the uncorrected electronic part fe, calculated by a numerical scheme
   based on the simple Thomas-Fermi statistical model, (2) the ionic part
   fi, using the Cowan model {[}2], which employs analytical formulas to
   smoothly interpolate between the Debye solid, the normal solid and the
   liquid states, and (3) the semi empirical bonding correction fb, which
   is included to compensate for the negligence of bonding forces in the
   simple Thomas-Fermi model. Although the total EOS is calculated for a
   single temperature T = T-e = T-i, the user is free to calculate the
   ionic and the corrected electronic contributions independently with
   different temperatures. For homogeneous mixtures of elements the partial
   volumes of all element species k are iteratively adjusted in order to
   equilibrate the Thomas-Fermi pressures p(e,k) and to fulfill an additive
   volume rule for the electronic contribution. Furthermore, in the
   liquid-vapor two-phase region the model provides the (metastable) EOS
   with its characteristic features like van-der-Waals loops. The fully
   equilibrium EOS inside the two-phase region can be calculated by an
   iterative Maxwell construction scheme. Finally, despite the existence of
   the bonding correction, pressures near the critical point are often
   overestimated. Therefore, an improved cold curve can be applied to fit
   the location of the critical point to theoretical or experimental data.
   Additional comments: Besides the material's composition, the user must
   specify a reference density rho(0) and the bulk modulus K-0= rho(partial
   derivative p/partial derivative rho)(s) at a reference point (p, T) =
   (0, T-0) for a new material. The reference temperature T-0 is usually
   chosen such that at p = 0 the studied material is in the solid state.
   All fixed material parameters are stored in a material parameter
   database file which can be easily exchanged between users. The code is
   designed to calculate the equation of state within the following density
   and temperature limits: 10(-7) <= rho/rho(0) < 10(6),10(-4) eV <= T <=
   10(6) eV. Homogeneous mixtures with more than three elements may
   implicate numerical difficulties and/or uneconomical computing times.
   For temperatures close to and below the critical point one must be
   careful to check the accuracy of the model. If available, a more complex
   EOS in this regime is preferable.
   The FEOS package was designed to provide the best possible flexibility
   and therefore consists of three parts: (1) the FEOS library which
   contains all the routines for the calculation of the EOS and which
   provides a C/C++ as well as a Fortran interface for this purpose, (2)
   the FEOS table generation tool which accesses the FEOS library in order
   to generate EOS table files (e.g. in the SESAME database {[}3] format),
   and (3) the SHOWEOS table visualization tool which was developed to
   provide isotherms, isochores, isentropes, and Hugoniot curves from the
   FEOS or SESAME tables. Application of the code was first demonstrated in
   a publication on liquid-vapor metastable states in volumetrically heated
   matter {[}4].
   {[}1] A. J. Kemp, J. Meyer-ter Vehn, An equation of state code for hot
   dense matter, based on the QEOS description, Nuclear Instruments and
   Methods in Physics Research Section A: Accelerators, Spectrometers,
   Detectors and Associated Equipment 415 (3) (1998) 674-676.
   doi:10.1016/S0168-9002(98)00446-X.
   {[}2] R. M. More, K. H. Warren, D. A. Young, G. B. Zimmerman, A new
   quotidian equation of state (QEOS) for hot dense matter, Physics of
   Fluids 31(1988) 3059. doi:10.1063/1.866963.
   {[}3] S. P. Lyon, J. D. Johnson, SESAME: The Los Alamos National
   Laboratory equation of state database, Tech. Rep. LA-UR-92-3407, Los
   Alamos National Laboratory (1992).
   {[}4] S. Faik, M. M. Basko, A. Tauschwitz, I. losilevskiy, J. A. Maruhn,
   Dynamics of volumetrically heated matter passing through the
   liquid-vapor metastable states, High Energy Density Physics 8 (4) (2012)
   349-359. doi:10.1016/j.hedp.2012.08.003. (C) 2018 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.cpc.2018.01.008},
ISSN = {0010-4655},
EISSN = {1879-2944},
Unique-ID = {WOS:000429756000010},
}

@article{ WOS:000432866800002,
Author = {Stoecker, Bianca K. and Koester, Johannes and Zamir, Eli and Rahmann,
   Sven},
Title = {Modeling and simulating networks of interdependent protein interactions},
Journal = {INTEGRATIVE BIOLOGY},
Year = {2018},
Volume = {10},
Number = {5},
Pages = {290-305},
Month = {MAY},
Abstract = {Protein interactions are fundamental building blocks of biochemical
   reaction systems underlying cellular functions. The complexity and
   functionality of these systems emerge not only from the protein
   interactions themselves but also from the dependencies between these
   interactions, as generated by allosteric effects or mutual exclusion due
   to steric hindrance. Therefore, formal models for integrating and
   utilizing information about interaction dependencies are of high
   interest. Here, we describe an approach for endowing protein networks
   with interaction dependencies using propositional logic, thereby
   obtaining constrained protein interaction networks (constrained
   networks). The construction of these networks is based on public
   interaction databases as well as text-mined information about
   interaction dependencies. We present an efficient data structure and
   algorithm to simulate protein complex formation in constrained networks.
   The efficiency of the model allows fast simulation and facilitates the
   analysis of many proteins in large networks. In addition, this approach
   enables the simulation of perturbation effects, such as knockout of
   single or multiple proteins and changes of protein concentrations. We
   illustrate how our model can be used to analyze a constrained human
   adhesome protein network, which is responsible for the formation of
   diverse and dynamic cell-matrix adhesion sites. By comparing protein
   complex formation under known interaction dependencies versus without
   dependencies, we investigate how these dependencies shape the resulting
   repertoire of protein complexes. Furthermore, our model enables
   investigating how the interplay of network topology with interaction
   dependencies influences the propagation of perturbation effects across a
   large biochemical system. Our simulation software CPINSim (for
   Constrained Protein Interaction Network Simulator) is available under
   the MIT license at ; http://github.com/BiancaStoecker/cpinsim and as a
   Bioconda package (; https://bioconda.github.io).},
DOI = {10.1039/c8ib00012c},
ISSN = {1757-9694},
EISSN = {1757-9708},
Unique-ID = {WOS:000432866800002},
}

@article{ WOS:000429999700001,
Author = {Puetz, Peter and Kneib, Thomas},
Title = {A penalized spline estimator for fixed effects panel data models},
Journal = {ASTA-ADVANCES IN STATISTICAL ANALYSIS},
Year = {2018},
Volume = {102},
Number = {2},
Pages = {145-166},
Month = {APR},
Abstract = {Estimating nonlinear effects of continuous covariates by penalized
   splines is well established for regressions with cross-sectional data as
   well as for panel data regressions with random effects. Penalized
   splines are particularly advantageous since they enable both the
   estimation of unknown nonlinear covariate effects and inferential
   statements about these effects. The latter are based, for example, on
   simultaneous confidence bands that provide a simultaneous uncertainty
   assessment for the whole estimated functions. In this paper, we consider
   fixed effects panel data models instead of random effects specifications
   and develop a first-difference approach for the inclusion of penalized
   splines in this case. We take the resulting dependence structure into
   account and adapt the construction of simultaneous confidence bands
   accordingly. In addition, the penalized spline estimates as well as the
   confidence bands are also made available for derivatives of the
   estimated effects which are of considerable interest in many application
   areas. As an empirical illustration, we analyze the dynamics of life
   satisfaction over the life span based on data from the German
   Socio-Economic Panel. An open-source software implementation of our
   methods is available in the R package pamfe.},
DOI = {10.1007/s10182-017-0296-1},
ISSN = {1863-8171},
EISSN = {1863-818X},
ORCID-Numbers = {Kneib, Thomas/0000-0003-3390-0972
   Putz, Peter/0000-0002-2948-0189},
Unique-ID = {WOS:000429999700001},
}

@article{ WOS:000446340900003,
Author = {Souliman, Mena I. and Abd El-Hakim, Ragaa and Davis, Mark and Hemant, Gc
   and Walubita, Lubinda},
Title = {Mechanistic and Economic Impacts of Using Asphalt Rubber Mixtures at
   Various Vehicle Speeds},
Journal = {ADVANCES IN CIVIL ENGINEERING MATERIALS},
Year = {2018},
Volume = {7},
Number = {3, SI},
Pages = {347-359},
Month = {APR},
Abstract = {In the application of hot-mix asphalt pavement (HMA), tension at the
   bottom of the HMA layer creates one of the most challenging distresses
   to pavement structures, fatigue cracking. Adding rubber to the asphalt
   mix can extend the life of a pavement and provide an end use for old
   tires that would otherwise end up in a landfill. It is already known
   that the initial construction cost of an asphalt rubber mix will be
   higher than that of a conventional mix. However, the purpose of this
   article is to investigate if the reduced layer thickness and improved
   fatigue life will offset the initial cost. After completing a
   mechanistic analysis using the FHWA software package named 3D Move
   (University of Nevada, Reno, NV), the pavement thickness required to
   last for 50,000,000 cycles (the estimated endurance limit) was found to
   be much less for asphalt rubber mixes as opposed to the reference HMA.
   The cost to construct one lane mile of the reference mix pavement
   designed for 113 kph traffic was \$190,031, while the cost for asphalt
   rubber mix at the same speed came out to be \$187,629. This is a \$2,402
   difference. Additionally, the cost to construct 1.6 km of lane of the
   reference mix and asphalt rubber mix for four more different vehicle
   speeds was calculated. Overall, analysis showed that Asphalt Rubber (AR)
   modified asphalt mixtures exhibited significantly lower cost of pavement
   per 1,000 cycles of fatigue life per mile compared to the conventional
   HMA mixture.},
DOI = {10.1520/ACEM20170104},
ISSN = {2165-3984},
ResearcherID-Numbers = {El-Hakim, Ragaa Abd/AAD-4803-2021
   },
ORCID-Numbers = {Souliman, Mena/0000-0001-6204-7857
   Abd El-Hakim, Ragaa/0000-0003-4148-6855},
Unique-ID = {WOS:000446340900003},
}

@article{ WOS:000430619300004,
Author = {Tian, Kuo and Lou, Fangrui and Gao, Tianxiang and Zhou, Yongdong and
   Miao, Zhenqing and Han, Zhiqiang},
Title = {De novo assembly and annotation of the whole transcriptome of Sepiella
   maindroni},
Journal = {MARINE GENOMICS},
Year = {2018},
Volume = {38},
Pages = {13-16},
Month = {APR},
Abstract = {As an important cephalopods species, Sepiella maindroni fishery has
   suffered a severe decline due to over-fishing since the 1980s. Stock
   enhancement has been applied to the resource recovery of this species,
   but a sexual precocity appeared in breeding process. In order to
   understand the regulatory mechanism of this phenomenon, we generated the
   whole transcriptome of S. maindroni based on the total RNA of tissue
   samples (eyestalk, peduncle, tentacle, gill, muscle and ovary) using
   Illumina RNA-seq technology. De novo assembly was performed using
   Trinity software and a total of 31,979,244 high-quality clean reads were
   randomly assembled to produce 74,245 contigs. All contigs were further
   assembled and clustered into 58,224 unigenes. Among the predictable
   unigenes, a total of 14,346 unigenes were annotated based on protein
   databases. We assessment the annotation completeness using BUSCO
   software package and the result showed that 91.5\% protein-coding genes
   were found in our assembled transcripts. At last, we predicted the
   structure of all unigenes using TransDecoder software and MicroSAtellite
   identification tool, respectively. Result showed that a total of 26,037
   nucleotide sequences of coding regions (direction of the sequences is 5'
   -> 3') were confirmed to the protein database and a total of 13,471
   simple sequence repeats were identified. Our goal was to provide an
   important foundation for future genomic research on the cephalopod and
   further evaluate the effectiveness of stock enhancement. (C) 2017
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.margen.2017.06.004},
ISSN = {1874-7787},
EISSN = {1876-7478},
Unique-ID = {WOS:000430619300004},
}

@article{ WOS:000427352700001,
Author = {Dagand, Pierre-Evariste and Tabareau, Nicolas and Tanter, Eric},
Title = {Foundations of dependent interoperability},
Journal = {JOURNAL OF FUNCTIONAL PROGRAMMING},
Year = {2018},
Volume = {28},
Month = {MAR 13},
Abstract = {Full-spectrum dependent types promise to enable the development of
   correct-by-construction software. However, even certified software needs
   to interact with simply-typed or untyped programs, be it to perform
   system calls, or to use legacy libraries. Trading static guarantees for
   runtime checks, the dependent interoperability framework provides a
   mechanism by which simply-typed values can safely be coerced to
   dependent types and, conversely, dependently-typed programs can
   defensively be exported to a simply-typed application. In this article,
   we give a semantic account of dependent interoperability. Our
   presentation relies on and is guided by a pervading notion of type
   equivalence, whose importance has been emphasized in recent work on
   homotopy type theory. Specifically, we develop the notions of
   type-theoretic partial Galois connections as a key foundation for
   dependent interoperability, which accounts for the partiality of the
   coercions between types. We explore the applicability of both
   type-theoretic Galois connections and anticonnections in the setting of
   dependent interoperability. A partial Galois connection enforces a
   translation of dependent types to runtime checks that are both sound and
   complete with respect to the invariants encoded by dependent types.
   Conversely, picking an anticonnection instead lets us induce weaker,
   sound conditions that can amount to more efficient runtime checks.
   Our framework is developed in Coq; it is thus constructive and verified
   in the strictest sense of the terms. Using our library, users can
   specify domain-specific partial connections between data structures. Our
   library then takes care of the (sometimes, heavy) lifting that leads to
   interoperable programs. It thus becomes possible, as we shall
   illustrate, to internalize and hand-tune the extraction of
   dependently-typed programs to interoperable OCaml programs within Coq
   itself.},
DOI = {10.1017/S0956796818000011},
Article-Number = {e9},
ISSN = {0956-7968},
EISSN = {1469-7653},
ORCID-Numbers = {tabareau, nicolas/0000-0003-3366-2273},
Unique-ID = {WOS:000427352700001},
}

@article{ WOS:000427244700009,
Author = {Adarme, Marco and Jabba Molinares, Daladier},
Title = {SEED: A software tool and an active-learning strategy for data
   structures courses},
Journal = {COMPUTER APPLICATIONS IN ENGINEERING EDUCATION},
Year = {2018},
Volume = {26},
Number = {2},
Pages = {302-313},
Month = {MAR},
Abstract = {SEED is a software tool designed for Java consisting of a class library
   and a set of simulators to facilitate learning of the main data
   structures. The educational component is based on an active case-solving
   methodology accompanied by a pedagogical strategy. This strategy allows
   for the development of a multilayer-model-programming work for the
   construction of basic and advanced applications in real domains and for
   integration of its use, development and operation assessment with data
   structures. The evaluation results of SEED as a pedagogical mediator in
   the issue of binary trees is presented. The evaluation indicates that
   students prefer to use SEED due to its simplicity and attractive GUIs
   which facilitate data structures learning.},
DOI = {10.1002/cae.21885},
ISSN = {1061-3773},
EISSN = {1099-0542},
ResearcherID-Numbers = {Jabba, Daladier/ABD-1709-2021
   },
ORCID-Numbers = {Jabba, Daladier/0000-0001-5876-2559
   Adarme Jaimes, Marco Antonio/0000-0002-2121-1208},
Unique-ID = {WOS:000427244700009},
}

@article{ WOS:000425525400003,
Author = {Bruderer, Tobias and Varesio, Emmanuel and Hidasi, Anita O. and
   Duchoslav, Eva and Burton, Lyle and Bonner, Ron and Hopfgartner, Gerard},
Title = {Metabolomic spectral libraries for data-independent SWATH liquid
   chromatography mass spectrometry acquisition},
Journal = {ANALYTICAL AND BIOANALYTICAL CHEMISTRY},
Year = {2018},
Volume = {410},
Number = {7},
Pages = {1873-1884},
Month = {MAR},
Abstract = {High-quality mass spectral libraries have become crucial in mass
   spectrometry-based metabolomics. Here, we investigate a workflow to
   generate accurate mass discrete and composite spectral libraries for
   metabolite identification and for SWATH mass spectrometry data
   processing. Discrete collision energy (5-100 eV) accurate mass spectra
   were collected for 532 metabolites from the human metabolome database
   (HMDB) by flow injection analysis and compiled into composite spectra
   over a large collision energy range (e.g., 10-70 eV). Full scan response
   factors were also calculated. Software tools based on accurate mass and
   predictive fragmentation were specially developed and found to be
   essential for construction and quality control of the spectral library.
   First, elemental compositions constrained by the elemental composition
   of the precursor ion were calculated for all fragments. Secondly, all
   possible fragments were generated from the compound structure and were
   filtered based on their elemental compositions. From the discrete
   spectra, it was possible to analyze the specific fragment form at each
   collision energy and it was found that a relatively large collision
   energy range (10-70 eV) gives informative MS/MS spectra for library
   searches. From the composite spectra, it was possible to characterize
   specific neutral losses as radical losses using in silico fragmentation.
   Radical losses (generating radical cations) were found to be more
   prominent than expected. From 532 metabolites, 489 provided a signal in
   positive mode {[}M+H](+) and 483 in negative mode {[}M-H](-). MS/MS
   spectra were obtained for 399 compounds in positive mode and for 462 in
   negative mode; 329 metabolites generated suitable spectra in both modes.
   Using the spectral library, LC retention time, response factors to
   analyze data-independent LC-SWATH-MS data allowed the identification of
   39 (positive mode) and 72 (negative mode) metabolites in a plasma pool
   sample (total 92 metabolites) where 81 previously were reported in HMDB
   to be found in plasma.},
DOI = {10.1007/s00216-018-0860-x},
ISSN = {1618-2642},
EISSN = {1618-2650},
ORCID-Numbers = {Bonner, Ron/0000-0003-0532-8323
   Hopfgartner, Gerard/0000-0002-9087-606X},
Unique-ID = {WOS:000425525400003},
}

@article{ WOS:000429258700010,
Author = {Dan, Saikat and Chaudhary, Manpreet and Barai, Sudhirkumar V.},
Title = {Punching shear behavior of recycled aggregate concrete},
Journal = {COMPUTERS AND CONCRETE},
Year = {2018},
Volume = {21},
Number = {3},
Pages = {321-333},
Month = {MAR},
Abstract = {Flat-slabs, being a significant structural component, not only reduce
   the dead load of the structure but also reduce the amount of concrete
   required for construction. Moreover the use of recycled aggregates
   lowers the impact of large scale construction to nearby ecosystems.
   Recycled aggregate based concrete being a quasi-brittle material shows
   enormous cracking during failure. Crack growth in flat-slabs is mostly
   in sliding mode (Mode II). Therefore sufficient sections need to be
   provided for resistance against such failure modes. The main objective
   of the paper is to numerically determine the ultimate load carrying
   capacity of two self-similar flat-slab specimens and validate the
   results experimentally for the natural aggregate as well as recycled
   aggregate based concrete. Punching shear experiments are carried out on
   circular flat-slab specimen on a rigid circular knife-edge support built
   out of both normal (NAC) and recycled aggregate concrete (RAC, with full
   replacement). Uniaxial compression and bending tests have been conducted
   on cubes, cylinders and prisms using both types of concrete (NAC and
   RAC) for its material characterization and use in the numerical scheme.
   The numerical simulations have been conducted in ABAQUS (a known finite
   element software package). Eight noded solid elements have been used to
   model the flat slab and material properties have been considered from
   experimental tests. The inbuilt Concrete Damaged Plasticity model of
   ABAQUS has been used to monitor crack propagation in the specimen during
   numerical simulations.},
DOI = {10.12989/cac.2018.21.3.321},
ISSN = {1598-8198},
EISSN = {1598-818X},
ResearcherID-Numbers = {Barai, Sudhirkumar/AAI-5698-2020},
ORCID-Numbers = {Barai, Sudhirkumar/0000-0001-5100-0607},
Unique-ID = {WOS:000429258700010},
}

@article{ WOS:000427698100004,
Author = {Jacoski, Claudio Alcides and Hoffmeister, Lissandro Machado},
Title = {POTENTIAL USE OF BIM FOR AUTOMATED UPDATING OF BUILDING MATERIALS VALUES},
Journal = {BRAZILIAN JOURNAL OF OPERATIONS \& PRODUCTION MANAGEMENT},
Year = {2018},
Volume = {15},
Number = {1},
Pages = {35-43},
Month = {MAR},
Abstract = {This study proposes an artifact motivated by improved assertiveness in
   building design budgets. Building Information Modeling (BIM), with the
   structure of the parametric objects created in a file format with the
   Industry Foundation Classes (IFC) extension, can provide the data for
   the object, facilitating the design's control and monitoring process.
   Through the adoption of the IFC standard in the creation of these
   objects, the exchange of information between the tools of different
   software providers becomes viable, allowing interoperability between
   systems. This is a desired situation in the construction industry, which
   incurs significant losses due to this problem. An important condition
   that can significantly contribute to the update of the information of
   the objects and the budget process is the incorporation of the
   possibility of updating the value information (price) of the BIM objects
   that are shared in repositories (object libraries). In this context,
   this study presents an alternative to updating and retrieving the values
   of BIM objects based on the IFC standard. An artifact (web environment)
   was produced linked to a model to meet the proposed objective. This
   method is presented by computing services, enabling the automated
   retrieval of the object value between the owners, the price repository
   and also the designers. The performed tests reveal the practicality of
   its implementation, with no extensive knowledge of the IFC structure
   being necessary. It suffices to simply follow the fill out pattern of
   the custom properties in IFC, defined during the creation of the object.
   The submission of the construction design to the repository allows for
   the retrieval of the values and the quantification of objects present in
   the design. This process is carried out in a simple manner, maintaining
   the synchrony and traceability of the object with the designer and the
   owners of the objects making up the architectural and complementary
   design.},
DOI = {10.14488/BJOPM.2018.v15.n1.a4},
ISSN = {2237-8960},
ResearcherID-Numbers = {Jacoski, Claudio Alcides/AAK-3254-2021
   Jacoski, Claudio Alcides/G-3620-2012},
ORCID-Numbers = {Jacoski, Claudio Alcides/0000-0003-3418-8155
   Jacoski, Claudio Alcides/0000-0003-3418-8155},
Unique-ID = {WOS:000427698100004},
}

@article{ WOS:000428749700017,
Author = {Din, Muhammad and Ghani, Mamuna},
Title = {Problems in Sentence Construction at HSSC Level in Pakistan},
Journal = {INTERNATIONAL JOURNAL OF ENGLISH LINGUISTICS},
Year = {2018},
Volume = {8},
Number = {1},
Pages = {200-211},
Month = {FEB},
Abstract = {This study entitled ``Problems in the Construction of Sentence at HSSC
   Level in Pakistan{''} strives to unearth the problems faced by the
   students in learning sentence structure through literature and the facts
   regarding the role of literature as a teaching tool in teaching English
   as a second/foreign language with reference to the construction of
   sentence at Higher Secondary School Certificate (HSSC) level in
   Pakistan. This study also investigates how much the students learn
   English sentence structure through literature. To achieve the set
   objectives of this study, the researcher went for the quantitative
   research methodology. So, a questionnaire comprising of 30 items
   encompassing the different aspects of sentence structure was designed to
   collect data from 600 subjects (male/female) of HSSC (Higher Secondary
   School Certificate) level. The researcher has also conducted an
   achievement test so that a correlation might be drawn between their
   attitude towards ``teaching English sentence structure through
   literature{''} and the score of their achievement test. The collected
   data were analyzed through software package (SPSS XX) which is commonly
   used in applied linguistics. The findings of this study explicitly
   reveal that the EFL learners remain unable to learn and develop both the
   contraction of sentence and syntactic skills when they are taught
   English through literature. This study recommends that the teaching of
   English should be application oriented and task-based strategies and
   activities should be resorted to by the FL educators.},
DOI = {10.5539/ijel.v8n1p200},
ISSN = {1923-869X},
EISSN = {1923-8703},
Unique-ID = {WOS:000428749700017},
}

@article{ WOS:000426842000002,
Author = {Potterton, Liz and Agirre, Jon and Ballard, Charles and Cowtan, Kevin
   and Dodson, Eleanor and Evans, Phil R. and Jenkins, Huw T. and Keegan,
   Ronan and Krissinel, Eugene and Stevenson, Kyle and Lebedev, Andrey and
   McNicholas, Stuart J. and Nicholls, Robert A. and Noble, Martin and
   Pannu, Navraj S. and Roth, Christian and Sheldrick, George and Skubak,
   Pavol and Turkenburg, Johan and Uski, Ville and von Delft, Frank and
   Waterman, David and Wilson, Keith and Winn, Martyn and Wojdyr, Marcin},
Title = {CCP4i2: the new graphical user interface to the CCP4 program suite},
Journal = {ACTA CRYSTALLOGRAPHICA SECTION D-STRUCTURAL BIOLOGY},
Year = {2018},
Volume = {74},
Number = {2},
Pages = {68-84},
Month = {FEB},
Abstract = {The CCP4 (Collaborative Computational Project, Number 4) software suite
   for macromolecular structure determination by X-ray crystallography
   groups brings together many programs and libraries that, by means of
   well established conventions, interoperate effectively without adhering
   to strict design guidelines. Because of this inherent flexibility, users
   are often presented with diverse, even divergent, choices for solving
   every type of problem. Recently, CCP4 introduced CCP4i2, a modern
   graphical interface designed to help structural biologists to navigate
   the process of structure determination, with an emphasis on pipelining
   and the streamlined presentation of results. In addition, CCP4i2
   provides a framework for writing structure-solution scripts that can be
   built up incrementally to create increasingly automatic procedures.},
DOI = {10.1107/S2059798317016035},
ISSN = {2059-7983},
ResearcherID-Numbers = {Wojdyr, Marcin/N-5686-2015
   },
ORCID-Numbers = {Wojdyr, Marcin/0000-0003-3980-4092
   Noble, Martin/0000-0002-3595-9807
   Cowtan, Kevin Douglas/0000-0002-0189-1437
   von Delft, Frank/0000-0003-0378-0017
   Agirre, Jon/0000-0002-1086-0253
   Turkenburg, Johan/0000-0001-6992-6838
   Jenkins, Huw/0000-0002-3302-6966
   Keegan, Ronan/0000-0002-9495-0431},
Unique-ID = {WOS:000426842000002},
}

@article{ WOS:000456883600002,
Author = {Bongomin, George Okello Candiya and Munene, John C. and Ntayi, Joseph
   Mpeera and Malinga, Charles Akol},
Title = {Nexus between financial literacy and financial inclusion Examining the
   moderating role of cognition from a developing country perspective},
Journal = {INTERNATIONAL JOURNAL OF BANK MARKETING},
Year = {2018},
Volume = {36},
Number = {7},
Pages = {1190-1212},
Abstract = {Purpose - Premised on the argument that cognition structures the way how
   individuals think and make decisions, the purpose of this paper is to
   test the interaction effect of cognition in the relationship between
   financial literacy and financial inclusion of the poor in rural Uganda.
   Design/methodology/approach - The study used cross-sectional research
   design and quantitative data were collected and analyzed using
   Statistical Package for Social Sciences. Baron and Kenny guidelines were
   adopted to test for existence of moderating effect of cognition in the
   relationship between financial literacy and financial inclusion of the
   poor in rural Uganda. Furthermore, ModGraph excel software was used to
   establish the magnitude of moderating effect of cognition in the
   relationship between financial literacy and financial inclusion of the
   poor in rural Uganda.
   Findings - The results revealed that cognition significantly moderate
   the relationship between financial literacy and financial inclusion of
   the poor in rural Uganda. In addition, both cognition and financial
   literacy also have direct effects on financial inclusion of the poor in
   rural Uganda.
   Research limitations/implications - The study adopted cross-sectional
   research design and data were collected by use of only questionnaires.
   Future studies through longitudinal research design may be employed.
   Besides, further studies using interviews may be adopted. Furthermore,
   this study collected data from only tier 3 financial institutions, thus,
   ignoring the other financial institutions. Future studies could focus on
   financial institutions under the other tiers.
   Practical implications - The findings from the study enlightens
   policy-makers, managers of financial institutions, and financial
   inclusion advocates on the importance of cognition in enhancing
   financial literacy among the poor, especially in rural Uganda. Cognition
   combined with financial literacy helps the poor to make wise financial
   decisions and choices toward consuming financial services and products
   provided by formal financial institutions. This leads to increased scope
   of financial inclusion of the poor in rural Uganda. Therefore, advocates
   of financial literacy should assess community cultural cognition and
   utilize them to design and fashion effective financial literacy
   interventions that can promote financial inclusion.
   Originality/value - The study uses Baron and Kenny and ModGraph excel
   software to test for the interaction effect of cognition in the
   relationship between financial literacy and financial inclusion of the
   poor in rural Uganda. While several studies exist worldwide on financial
   inclusion, this study is the first to test the interaction effect of
   cognition in the relationship between financial literacy and financial
   inclusion of the poor in rural areas in a developing country context.},
DOI = {10.1108/IJBM-08-2017-0175},
ISSN = {0265-2323},
EISSN = {1758-5937},
ResearcherID-Numbers = {Ntayi, Joseph/HKW-6871-2023},
Unique-ID = {WOS:000456883600002},
}

@inproceedings{ WOS:000567656900090,
Author = {Bulushev, Sergey and Bunov, Artem},
Editor = {Andreev, V and Matseevich, T and TerMartirosyan, A and Adamtsevich, A},
Title = {Dynamic analysis of strength and human-comfort level of the football
   stadium structures at coordinated movements of audience},
Booktitle = {XXVII R-S-P SEMINAR, THEORETICAL FOUNDATION OF CIVIL ENGINEERING (27RSP)
   (TFOCE 2018)},
Series = {MATEC Web of Conferences},
Year = {2018},
Volume = {196},
Note = {27th Russian-Polish-Slovak (R-S-P) Seminar on Theoretical Foundation of
   Civil Engineering (TFoCE), Don State Tech Univ, Rostov on Don, RUSSIA,
   SEP 17-21, 2018},
Organization = {Moscow State Univ Civil Engn; Wroclaw Univ Technol, Civil Engn Dept;
   Univ Zilina, Fac Civil Engn; Warsaw Univ Technol, Civil Engn Dept;
   Samara State Tech Univ},
Abstract = {In the design of stadiums and other sports facilities it is necessary to
   take into account the dynamic performance caused by coordinated movement
   of groups of people. National standard technical documents do not
   contain guidance on the setting of dynamic loads caused by people, as
   well as admissible dynamic parameters of grandstands' structures in the
   stadiums caused by such actions that should be taken in consideration in
   the course of design. Therefore, it is necessary to develop appropriate
   recommendations. Based on the developed recommendations, analysis of
   stadium structures for dynamic loads caused by the audience movement was
   performed in the time domain by direct integration of motion equation
   using certified software package Lira 10.4. It was obtained that the
   dynamic human-comfort level in the grandstands and in the
   under-grandstand facilities was ensured. Recommendations were made based
   on the analysis of international and Russian standard technical
   documents, generalization of available experience in design,
   construction, and operation of such objects, including similar unique
   large span structures. Obtained accelerations of under-grandstand
   facilities were analyzed in accordance with the construction standards
   SN 2.2.4/2.1.8.566-96. It was revealed that the limit values for some
   structure points have been exceeded.},
DOI = {10.1051/matecconf/201819602032},
Article-Number = {02032},
ISSN = {2261-236X},
ResearcherID-Numbers = {Bulushev, Sergey/AAD-6627-2019},
ORCID-Numbers = {Bulushev, Sergey/0000-0001-7239-4668},
Unique-ID = {WOS:000567656900090},
}

@inproceedings{ WOS:000458687200048,
Author = {Cassee, Nathan and Pinto, Gustavo and Castor, Fernando and Serebrenik,
   Alexander},
Book-Group-Author = {IEEE},
Title = {How Swift Developers Handle Errors},
Booktitle = {2018 IEEE/ACM 15TH INTERNATIONAL CONFERENCE ON MINING SOFTWARE
   REPOSITORIES (MSR)},
Series = {IEEE International Working Conference on Mining Software Repositories},
Year = {2018},
Pages = {292-302},
Note = {ACM/IEEE 15th International Conference on Mining Software Repositories
   (MSR), Gothenburg, SWEDEN, MAY 28-29, 2018},
Organization = {IEEE Comp Soc; Assoc Comp Machinery; SIGSOFT; IEEE Tech Council Software
   Engn},
Abstract = {Swift is a new programming language developed by Apple as a replacement
   to Objective-C. It features a sophisticated error handling (EH)
   mechanism that provides the kind of separation of concerns afforded by
   exception handling mechanisms in other languages, while also including
   constructs to improve safety and maintainability. However, Swift also
   inherits a software development culture stemming from Objective-C being
   the de-facto standard programming language for Apple platforms for the
   last 15 years. It is, therefore, a priori unclear whether Swift
   developers embrace the novel EH mechanisms of the programming language
   or still rely on the old EH culture of Objective-C even working in
   Swift.
   In this paper, we study to what extent developers adhere to good
   practices exemplified by EH guidelines and tutorials, and what are the
   common bad EH practices particularly relevant to Swift code.
   Furthermore, we investigate whether perception of these practices
   differs between novices and experienced Swift developers.
   To answer these questions we employ a mixed-methods approach and combine
   10 semi-structured interviews with Swift developers and quantitative
   analysis of 78,760 Swift 4 files extracted from 2,733 open-source GitHub
   repositories. Our findings indicate that there is ample opportunity to
   improve the way Swift developers use error handling mechanisms. For
   instance, some recommendations derived in this work are not well spread
   in the corpus of studied Swift projects. For example, generic catch
   handlers are common in Swift (even though it is not uncommon for them to
   share space with their counterparts: non empty catch handlers), custom,
   developer-defined error types are rare, and developers are mostly
   reactive when it comes to error handling, using Swift's constructs
   mostly to handle errors thrown by libraries, instead of throwing and
   handling application-specific errors.},
DOI = {10.1145/3196398.3196428},
ISSN = {2160-1852},
ISBN = {978-1-4503-5716-6},
ResearcherID-Numbers = {Castor, Fernando/V-4011-2018
   },
ORCID-Numbers = {Castor, Fernando/0000-0002-6389-3630
   Cassee, Nathan/0000-0002-6511-918X},
Unique-ID = {WOS:000458687200048},
}

@inproceedings{ WOS:000446160800178,
Author = {Chunyuk, Dmitriy and Rasskazova, Daria},
Editor = {Askadskiy, A and Pustovgar, A and Matseevich, T and Adamtsevich, A},
Title = {On the influence of modeling the increase of loads from the building
   frame in the analysis of stress-strain state of the of the foundation
   base},
Booktitle = {XXI INTERNATIONAL SCIENTIFIC CONFERENCE ON ADVANCED IN CIVIL ENGINEERING
   CONSTRUCTION - THE FORMATION OF LIVING ENVIRONMENT (FORM 2018)},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2018},
Volume = {365},
Note = {21st International Scientific Conference on Advanced in Civil
   Engineering Construction - The Formation of Living Environment (FORM),
   Moscow State Univ Civil Engn, Moscow, RUSSIA, APR 25-27, 2018},
Abstract = {Design of bases and foundations is a complex task dependent on many
   different factors. Currently gained particular importance to the problem
   of the influence on the change of stress-strain state of the base of the
   foundation, increasing the load in the process of construction of
   buildings or structures. In this work, we performed a series of
   numerical experiments and the analysis of stress-strain state of the
   foundation soil, depending on the stress distribution at the base of the
   foundation. The basis set of single-layer elastic half-space with a
   limited depth of compressible strata. The calculations were performed
   using the PLAXIS 3D software package. The obtained data allowed to
   evaluate the selected parameters and note some features of the
   ``base-foundation{''} system. The results of the series of calculations
   performed are analyzed by identifying the features in the distribution
   and the values of the parameters, depending on the characteristic
   features of the calculated cases, and are expressed by the following:
   the difference between phased and ``instant{''} downloads, the main
   parameters influenced by the experiment, the influence of the
   construction modeling method on the magnitude and localization of the
   extremums of the functions of the parameters considered. As a result of
   the study, conclusions were drawn about the effect of the loading method
   on the stress-strain state of the base and recommendations were given on
   its modeling.},
DOI = {10.1088/1757-899X/365/4/042038},
Article-Number = {042038},
ISSN = {1757-8981},
ResearcherID-Numbers = {CHUNYUK, DMITRIY/AAD-7168-2022},
ORCID-Numbers = {CHUNYUK, DMITRIY/0000-0002-5081-0082},
Unique-ID = {WOS:000446160800178},
}

@article{ WOS:000435942900003,
Author = {Epifanova, Ekaterina A. and Strokova, Lyudmila A.},
Title = {EVALUATION OF DEFORMATION OF THE HISTORIC BUILDING IN TOMSK BY THE
   INTEGRATED APPROACH BASED ON TERRESTRIAL LASER SCANNER AND FINITE
   ELEMENT MODELING},
Journal = {BULLETIN OF THE TOMSK POLYTECHNIC UNIVERSITY-GEO ASSETS ENGINEERING},
Year = {2018},
Volume = {329},
Number = {5},
Pages = {27-41},
Abstract = {The topic is relevant due to the necessity to improve approaches to
   estimation and prediction of deformations of historical constructions.
   The article deals with the study of engineering-geological conditions of
   the area during reconstruction of the building. It is necessary to
   introduce new technologies, such as laser scanning technology for
   construction of an accurate three-dimensional model of the object,
   finite element method for prediction of soil behaviour.
   This study alms to assess the effects of soil settlement on a
   structure's stress-strain state and the value of laser scanning
   techniques on structure analysis in obtaining correct data of
   deformation.
   Object The interaction of soil, foundations and structure of the Tomsk
   Youth Theatre or ``Teatr Yunogo zritelya{''} is considered in conditions
   of dense development on the territory of Tomsk city.
   Methods. The initial data for simulation the behaviour of the soil
   massif were obtained through regional geological works and field study
   in different years. A reconnaissance survey of the site was completed. A
   program complex based on finite element model was used to forecast the
   stress-strain state of soils. Laser scanning technology allow an
   accurate defenition of deformations lying on every side of the structure
   and make an accurate three-dimensional model of the object. The
   terrestrial laser scanning objects held by 3D laser scanner Leica
   Scanstation C10; horizontal, vertical justification and binding study
   points to a local coordinate system using Total Station LEICA TS15 and
   GNSS receiver LEICA GS10; handling an cloud of points held in the
   software package Leica Cyclone 8.0; three-dimensional object modeling
   was carried out in the software package Solid Works.
   The results. The authors have estimated the engineering geological
   conditions of a research site and developed a digital design model. The
   forecast of stress-strain state of the soils in reconstruction of the
   building is made. The soil profile and general crack formation together
   with terrestrial laser scanning measurement indicate that the structure
   is deformed diagonally along the river Tom. The concentration of cracks
   is associated with the western and eastern annexes to the building,
   which are supported by strip foundations lying at lower depth than the
   main building. The continued deformations are probably associated with
   creep of soft organic soils with a thickness of up to two meters.},
ISSN = {2500-1019},
EISSN = {2413-1830},
ResearcherID-Numbers = {Epifanova, Ekaterina A./AAQ-6567-2020
   Epifanova, Ekaterina/AAR-8143-2021
   Strokova, Ludmila/E-7332-2014},
ORCID-Numbers = {Epifanova, Ekaterina A./0000-0003-1564-4207
   Strokova, Ludmila/0000-0001-9302-0630},
Unique-ID = {WOS:000435942900003},
}

@inproceedings{ WOS:000450117400104,
Author = {Giannelli, Carlo and Bellavista, Paolo and Scotece, Domenico},
Book-Group-Author = {IEEE},
Title = {Software Defined Networking for Quality-aware Management of Multi-hop
   Spontaneous Networks},
Booktitle = {2018 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND
   COMMUNICATIONS (ICNC)},
Series = {International Conference on Computer Networking and Communications},
Year = {2018},
Pages = {561-566},
Note = {International Conference on Computing, Networking and Communications
   (ICNC), Maui, HI, MAR 05-08, 2018},
Abstract = {The Software Defined Networking (SDN) approach has recently demonstrated
   its effectiveness in simplifying the dynamic management of networking
   capabilities of infrastructure environments such as datacenters, e.g.,
   by greatly enhancing the flexibility of dispatching features provided by
   industrial-grade switches. Inspired by the previous and more traditional
   scenario, we propose SDN adoption in infrastructureless distributed
   multi-hop spontaneous networks based on the impromptu collaboration of
   fixed/mobile devices, with the goal of significantly improving the
   Quality of Service perceived by final users. To this purpose, the paper
   outlines our primary guidelines and reference architecture to support
   quality-aware packet dispatching. In particular, we present how
   collaborative nodes can exploit the SDN approach to appropriately manage
   the quality of different traffic flows, by avoiding undesired
   interference and by taking into consideration network
   capabilities/conditions and application-level requirements.},
ISSN = {2325-2626},
ISBN = {978-1-5386-3652-7},
ResearcherID-Numbers = {Giannelli, Carlo/H-3822-2012
   Bellavista, Paolo/H-7256-2014
   },
ORCID-Numbers = {Giannelli, Carlo/0000-0002-2394-1191
   Bellavista, Paolo/0000-0003-0992-7948
   Scotece, Domenico/0000-0003-3824-197X},
Unique-ID = {WOS:000450117400104},
}

@incollection{ WOS:000442520600021,
Author = {Goni-Moreno, Angel and de Lorenzo, Victor},
Editor = {Braman, JC},
Title = {Bio-Algorithmic Workflows for Standardized Synthetic Biology Constructs},
Booktitle = {SYNTHETIC BIOLOGY: METHODS AND PROTOCOLS},
Series = {Methods in Molecular Biology},
Year = {2018},
Volume = {1772},
Pages = {363-372},
Abstract = {A synthetic biology workflow covers the roadmap from conceptualization
   of a genetic device to its construction and measurement. It is composed
   of databases that provide DNA parts/plasmids, wet-lab methods, software
   tools to design circuits, simulation packages, and tools to analyze
   circuit performance. The interdisciplinary nature of such a workflow
   requires that experimental results and their in-silico counterparts
   proceed alongside, with constant feedback between them. We present an
   end-to-end use case for engineering a simple synthetic device, where
   information standards maintain coherence throughout the workflow. These
   are the Standard European Vector Architecture (SEVA), the Synthetic
   Biology Open Language (SBOL), and the Systems Biology Markup Language
   (SBML).},
DOI = {10.1007/978-1-4939-7795-6\_20},
ISSN = {1064-3745},
EISSN = {1940-6029},
ISBN = {978-1-4939-7795-6; 978-1-4939-7794-9},
ORCID-Numbers = {Goni-Moreno, Angel/0000-0002-2097-2507},
Unique-ID = {WOS:000442520600021},
}

@inproceedings{ WOS:000449531900045,
Author = {Hasan, Kamrul and Shetty, Sachin and Hassanzadeh, Amin and Ben Salem,
   Malek and Chen, Jay},
Book-Group-Author = {IEEE},
Title = {Modeling Cost of Countermeasures in Software Defined Networking-enabled
   Energy Delivery Systems},
Booktitle = {2018 IEEE CONFERENCE ON COMMUNICATIONS AND NETWORK SECURITY (CNS)},
Series = {IEEE Conference on Communications and Network Security},
Year = {2018},
Note = {6th IEEE Conference on Communications and Network Security (CNS),
   Beijing, PEOPLES R CHINA, MAY 30-JUN 01, 2018},
Organization = {IEEE; Natl Sci Fdn; Chinese Acad Sci; Chinese Acad Sci, Inst Informat
   Engn},
Abstract = {Software defined networking (SDN) is a networking paradigm to provide
   automated network management at run time through network orchestration
   and virtualization. SDN is primarily used for quality of service (QoS)
   and automated response to network failures. In the context of Energy
   Delivery System (EDS), SDN can also enhance system resilience through
   recovery from failures and maintaining critical operations during cyber
   attacks. Researchers have proposed SDN based architectures for
   autonomous attack containment, which dynamically modifies access control
   rules based on configurable trust levels. One of the challenges with
   such architectures is the lack of a cost model to select the
   countermeasure which balances the tradeoff between security risk and
   network QoS. Prior to choosing a particular countermeasure which either
   quarantines the attack or mitigates the impact, it is also critical to
   assess the impact on the ability of the operator to conduct normal
   operations. In this paper, we present an approach to aid in selection of
   security countermeasures dynamically in an SDN enabled EDS and achieving
   tradeoff between providing security and QoS. We present the modeling of
   security cost based on end-toend packet delay and throughput. We propose
   a non-dominated sorting based multi-objective optimization framework
   which can be implemented within an SDN controller to address the joint
   problem of optimizing between security and QoS parameters by alleviating
   time complexity at O(MN2). The M is the number of objective functions
   and N is the number of population for each generation respectively. We
   present simulation results which illustrate how data availability and
   data integrity can be achieved while maintaining QoS constraints.},
ISSN = {2474-025X},
ISBN = {978-1-5386-4586-4},
ResearcherID-Numbers = {hasan, kamrul/HGV-0910-2022
   Shetty, Sachin/W-1101-2018},
ORCID-Numbers = {Shetty, Sachin/0000-0002-8789-0610},
Unique-ID = {WOS:000449531900045},
}

@article{ WOS:000428832800008,
Author = {Inoue, Takeru and Mano, Toru and Mizutani, Kimihiro and Minato,
   Shin-ichi and Akashi, Osamu},
Title = {Fast packet classification algorithm for network-wide forwarding
   behaviors},
Journal = {COMPUTER COMMUNICATIONS},
Year = {2018},
Volume = {116},
Pages = {101-117},
Month = {JAN},
Abstract = {Packet classification has been a key technology to quickly identify an
   action to be taken on a packet at a switch. Several advanced
   applications, most of which have been introduced recently with the
   advent of software-defined networking, commonly require the
   identification of a combination of switch actions, i.e., the
   network-wide forwarding behavior of a packet. Conventional
   classification methods, however, fail to well support network wide
   behaviors, since the search space is partitioned in convoluted manner
   due to the complexity posed by the combinations possible.
   This paper proposes a novel packet classification method that supports
   the fast determination of network wide forwarding behaviors. To avoid
   the inefficiencies of existing methods, which are revealed for the first
   time by our research, we base our method on a compressed data structure
   named the multi-valued decision diagram. On the solid foundation of
   decision diagrams, several algorithms are introduced with thorough
   theoretical analyses, and the construction process and the
   classification performance are highly optimized for the new
   classification problem. Experiments on real network datasets show that
   our method identifies the network-wide forwarding behaviors at the line
   basic rate, e.g., 10 Mpps, on a single CPU core with only tens of MB of
   memory.},
DOI = {10.1016/j.comcom.2017.11.011},
ISSN = {0140-3664},
EISSN = {1873-703X},
Unique-ID = {WOS:000428832800008},
}

@article{ WOS:000478832000014,
Author = {Ivanchenko, Grigory Mikhailovich and Pikul, Anatoliy Vladimirovich},
Title = {FINITE ELEMENTS CONVERGING TESTING ON THE BASYS OF THEORY OF ELASTICITY
   PROBLEMS WITH USING CURVILINEAR SOLID FINITE ELEMENT},
Journal = {OPIR MATERIALIV I TEORIA SPORUD-STRENGTH OF MATERIALS AND THEORY OF
   STRUCTURES},
Year = {2018},
Number = {100},
Pages = {172-180},
Abstract = {This paper cosiders the testing of the finite element method based on
   three-dimensional problems in the theory of elasticity using the
   curvilinear solid finite elements. These elements implemented in the
   scope of the thesis research {[}1, 5] and realized within Solver of the
   software package ``LIRA-SAPR{''} with the number 39. The test based on
   comparison between the obtained results and precise analytical solution
   and another finite elements models which exist on the software.
   The theory of thin plates and shells created based on the correspondence
   creation, which follows the Kirchhoff-Love hypothesis. This theory does
   not allow taking into account the difference of the stress function over
   the thickness of the plate. The thickness increasing of such elements
   leads to an increase of inaccuracy when calculating the structures with
   stress concentrators. We can also get somewhat better results using the
   elements based on the Mindlin-Reissner plate theory, which takes into
   account the deformation of the transverse shear.
   It is possible to evaluate more correctly the stress-strain state of
   projected structures using curvilinear solid finite elements in the
   simulation and calculation of non-thin plates and shells.
   Convergence of the finite element method is an important characteristic,
   since it determines the suitability of a finite element for constructing
   simulations. Since the approximation, as a rule, gives an approximate
   description of the deformation distribution within an element, hence the
   results of the calculation of the construction generally are also
   approximate. That is why the question about the accuracy, stability and
   convergence of the solution obtained by the method of finite elements
   remains important.
   As a test, it is considered the following tasks. The Lame problem of the
   stress-strain situation of the thick-walled cylinder under the external
   and internal pressure. The bend of the hinged and fixed along the
   contour thick square plates under the action of a transverse uniformly
   distributed load. The results of the calculation are compared with the
   theoretical solutions contained in the works of Lurie A. {[}4], Donnell
   L. {[}2], Lisitsyn B. {[}3] and others. To compare the accuracy, the
   problems also was modeled using the isoparametric finite elements from
   SP ``LIRA-SAPR{''} and the SP ``SCAD Office{''}.
   In the study of convergence of FEM while modeling the thick cylinder, it
   was defined the limits of applicability of the curvilinear finite
   element.},
EISSN = {2410-2547},
ResearcherID-Numbers = {Ivanchenko, Grigory/ABF-8932-2021},
Unique-ID = {WOS:000478832000014},
}

@article{ WOS:000454039500007,
Author = {Jones, Andrew and Carlier, Tim},
Title = {AVOIDING SHORT-TERM OVERHEAT FAILURES OF RECOVERY BOILER SUPERHEATER
   TUBES},
Journal = {J-FOR-JOURNAL OF SCIENCE \& TECHNOLOGY FOR FOREST PRODUCTS AND PROCESSES},
Year = {2018},
Volume = {7},
Number = {3, SI},
Pages = {44-49},
Note = {2nd International Chemical Recovery Conference (ICRC), Halifax, CANADA,
   MAY 24-26, 2017},
Abstract = {Recovery boilers are particularly vulnerable to short-term overheat
   superheater tube failures during start-up due to the potential for
   fouling (and suppression of heat transfer) of these surfaces and the
   need to restart these units quickly to minimize downtime. In addition, a
   superheater tube failure has a significant negative impact on unit
   safety because these failures can create a critical exposure due to
   secondary failure of screen and water-wall tubes.
   This paper will discuss a software package developed to reduce these
   risks by properly managing the start-up process. This management
   approach is based on the observation that tube clearing can be
   identified by a distinct temperature trend. Using these ``tube-clearing
   events{''} or TCEs along with a flue-gas temperature measurement device
   (i.e., an optical pyrometer) to limit the firing rate until the
   management system has identified that all tubes have cleared after every
   pause in recovery boiler steam generation (boiler trip or shutdown) is
   an effective way to minimize the risk of short-term superheater tube
   overheat.
   The management system is structured similarly to a burner management
   system (BMS), where permissives must be satisfied and authorization
   levels are established with provisions for interlock by-passes. Each
   start-up event is thoroughly documented for later review. The system can
   be run in this strict manner or can be configured as an advisory system
   only based on the preference of the end user.
   One critical observation made during development is that trips during
   start-up can present the most risk because there is a tendency to
   increase steam flows quickly after these events. Discipline must
   therefore be maintained regarding ramp-up after all pauses in steam
   generation.},
ISSN = {1927-6311},
EISSN = {1927-632X},
Unique-ID = {WOS:000454039500007},
}

@inproceedings{ WOS:000444710900044,
Author = {Juenger, Daniel and Hundt, Christian and Schmidt, Bertil},
Book-Group-Author = {IEEE},
Title = {WarpDrive: Massively Parallel Hashing on Multi-GPU Nodes},
Booktitle = {2018 32ND IEEE INTERNATIONAL PARALLEL AND DISTRIBUTED PROCESSING
   SYMPOSIUM (IPDPS)},
Series = {International Parallel and Distributed Processing Symposium IPDPS},
Year = {2018},
Pages = {441-450},
Note = {32nd IEEE International Parallel and Distributed Processing Symposium
   (IPDPS), Vancouver, CANADA, MAY 21-25, 2018},
Organization = {IEEE},
Abstract = {Hash maps are among the most versatile data structures in computer
   science because of their compact data layout and expected constant time
   complexity for insertion and querying. However, associated memory access
   patterns during the probing phase are highly irregular resulting in
   strongly memory-bound implementations. Massively parallel accelerators
   such as CUDA-enabled GPUs may overcome this limitation by virtue of
   their fast video memory featuring almost one TB/s bandwidth in
   comparison to main memory modules of state-of-the-art CPUs with less
   than 100 GB/s. Unfortunately, the size of hash maps supported by
   existing single-GPU hashing implementations is restricted by the limited
   amount of available video RAM. Hence, hash map construction and querying
   that scales across multiple GPUs is urgently needed in order to support
   structured storage of bigger datasets at high speeds.
   In this paper, we introduce WarpDrive - a scalable, distributed
   single-node multi-GPU implementation for the construction and querying
   of billions of key-value pairs. We propose a novel subwarp-based probing
   scheme featuring coalesced memory access over consecutive memory regions
   in order to mitigate the high latency of irregular access patterns. Our
   implementation achieves 1.4 billion insertions per second in single-GPU
   mode for a load factor of 0.95 thereby outperforming the GPU-cuckoo
   implementation of the CUDPP library by a factor of 2.8 on a P100.
   Furthermore, we present transparent scaling to multiple GPUs within the
   same node with up to 4.3 billion operations per second for high load
   factors on four P100 GPUs connected by NVLink technology. WarpDrive is
   free software and can be downloaded at
   https://github.com/sleeepyjack/warpdrive.},
DOI = {10.1109/IPDPS.2018.00054},
ISSN = {1530-2075},
ISBN = {978-1-5386-4368-6},
ORCID-Numbers = {Junger, Daniel/0000-0002-6899-9311},
Unique-ID = {WOS:000444710900044},
}

@inproceedings{ WOS:000446160800211,
Author = {Khalimov, Oleg and Shibaeva, Galina and Portnyagin, Denis and Ibe,
   Katerina},
Editor = {Askadskiy, A and Pustovgar, A and Matseevich, T and Adamtsevich, A},
Title = {The improvement of aseismic horizontal frame's thermal insulations},
Booktitle = {XXI INTERNATIONAL SCIENTIFIC CONFERENCE ON ADVANCED IN CIVIL ENGINEERING
   CONSTRUCTION - THE FORMATION OF LIVING ENVIRONMENT (FORM 2018)},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2018},
Volume = {365},
Note = {21st International Scientific Conference on Advanced in Civil
   Engineering Construction - The Formation of Living Environment (FORM),
   Moscow State Univ Civil Engn, Moscow, RUSSIA, APR 25-27, 2018},
Abstract = {Recommendations about elimination of the thermal insulation's defects
   are offered in this research. The recommendations are based on the
   thermal imaging inspection of residential buildings, which were built
   with aseismic demands. The designing construction solution of filler
   structures taking into account the increase of comfort in the room is
   presented. In the proposed work the typical designs structures of the
   site supporting floor slab on a brick wall (width 770 mm). Aseismic
   horizontal frame is abutting to the floor slab on the same level. The
   insulation layer of 140 mm width made with expanded polystyrene sites on
   the whole height of the wall. This layer is braked aseismic horizontal
   frame on the floor slab level. The aseismic horizontal frame is covered
   by the expanded polystyrene of 20 mm width. The thermal bridge in the
   direction of the facade is very typical for this designing construction
   solution. The thermal bridge is in the mid-height of the floor slab near
   the insulation layer. The purpose of the research is to find a solution
   which will minimize or elimination thermal bridge with using additional
   insulation layer sites in the different level. Repeatability of thermal
   imaging inspection of designing construction solution and theoretical
   calculations in the software package ElCut are confirmed.
   The presentation is illustrated with examples of temperature fields for
   the different residential buildings.},
DOI = {10.1088/1757-899X/365/4/042071},
Article-Number = {042071},
ISSN = {1757-8981},
ORCID-Numbers = {Portnyagin, Denis/0000-0002-1209-1849},
Unique-ID = {WOS:000446160800211},
}

@article{ WOS:000433069600002,
Author = {Kolosov, A. I. and Kuznetsova, G. A. and Gnezdilova, O. A.},
Title = {MANAGEMENT OF WORK OF EMERGENCY AND RECOVERY SERVICES OF A
   GAS-DISTRIBUTING ORGANIZATION},
Journal = {RUSSIAN JOURNAL OF BUILDING CONSTRUCTION AND ARCHITECTURE},
Year = {2018},
Number = {2},
Pages = {19-26},
Abstract = {Statement of the problem. When eliminating emergencies of gas supply
   systems, the most rational strategy for the actions of dispatching
   personnel is based on the management of the operation of emergency
   recovery services that allow one to monitor the restoration and to
   distribute limited material and technical resources considering changes
   in the parameters over time.
   Results. On the basis of the mass service theory, an algorithm was
   developed to determine the optimal structure and parameters of the
   emergency service of the gas distribution company. To predict the
   parameters of the emergency services, a software is developed in the
   environment of the MatLab-Simulink package, which allows flexible
   control over the reliability of the equipment depending on the changing
   conditions of its operation. The program has advanced graphical
   representation of the results.
   Conclusions. The use of the developed program allows one to increase the
   efficiency of the repair departments of a gas distribution organization
   and to maintain a high level of safety of operation of gas equipment.},
ISSN = {2542-0526},
Unique-ID = {WOS:000433069600002},
}

@inproceedings{ WOS:000471004000046,
Author = {Li, D. and Uy, B. and Aslani, F. and Hou, C.},
Editor = {Albero, V and Espinos, A and Ibanez, C and Lapuebla, A and Romero, ML},
Title = {Numerical analysis of concrete-filled spiral welded stainless steel
   tubes subjected to compression},
Booktitle = {PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON ADVANCES IN
   STEEL-CONCRETE COMPOSITE STRUCTURES (ASCCS 2018)},
Year = {2018},
Pages = {365-371},
Note = {12th International Conference on Advances in Steel-Concrete Composite
   Structures (ASCCS), Univ Politecnica Valencia, Valencia, SPAIN, JUN
   27-29, 2018},
Organization = {Assoc Steel \& Concrete Composite Struct},
Abstract = {Spiral welded stainless tubes are produced by helical welding of a
   continuous strip of stainless steel. Recently, concrete-filled spiral
   welded stainless steel tubes have found increasing application in the
   construction industry due to their ease of fabrication and aesthetic
   appeal. However, an in-depth understanding of the behaviour of this type
   of structure is still needed due to the lack of proper design guidance
   and insufficient experimental verification. In this paper, the
   mechanical performance of concrete-filled spiral welded stainless steel
   tubes will be numerically investigated with a commercial finite element
   software package, through which an experimental program can be designed
   properly. Specifically, the proposed finite element models take into
   account the effects of material and geometric nonlinearities. Moreover,
   the initial imperfections of stainless steel tubes and the form of
   helical welding will be appropriately included. Enhancement of the
   understanding of the analysis results can be achieved by extending
   results through a series of parametric studies based on the developed
   finite element model. Thus, the effects of various design parameters
   will be further evaluated by using the developed finite element model.
   Furthermore, for the purposes of wide application of such types of
   structure, the accuracy of the behaviour prediction in terms of ultimate
   strength based on current design codes will be studied. The authors
   herein compared the load capacity between the finite element analysis
   results and the existing codes of practice.},
DOI = {10.4995/ASCCS2018.2018.7200},
ISBN = {978-84-9048-601-6},
ResearcherID-Numbers = {Aslani, Farhad/AAE-5727-2020
   },
ORCID-Numbers = {uy, brian/0000-0002-4577-2915
   Aslani, Farhad/0000-0002-7640-711X},
Unique-ID = {WOS:000471004000046},
}

@inproceedings{ WOS:000568451100085,
Author = {Malakhova, Anna},
Editor = {Volkov, A and Pustovgar, A and Adamtsevich, A},
Title = {Estimation of cracking of reinforced concrete load-bearing construction
   structures at the stage of their technical inspection},
Booktitle = {VI INTERNATIONAL SCIENTIFIC CONFERENCE INTEGRATION, PARTNERSHIP AND
   INNOVATION IN CONSTRUCTION SCIENCE AND EDUCATION (IPICSE-2018)},
Series = {MATEC Web of Conferences},
Year = {2018},
Volume = {251},
Note = {6th International Scientific Conference on Integration, Partnership and
   Innovation in Construction Science and Education (IPICSE), Moscow,
   RUSSIA, NOV 14-16, 2018},
Abstract = {The article describes the process and causes of cracking of bearing
   reinforced concrete structures, revealed during the inspection of the
   technical condition of structural elements of buildings at the stage of
   their erection, at the stage of suspended construction and in the
   operational stage. The reasons for the appearance of cracks in
   reinforced concrete structures under force loads and climatic influences
   are analyzed. Possible reasons for the appearance of cracks in a
   monolithic reinforced concrete beam ceiling of a multi-storey building
   of a column structural system and in monolithic walls of an underground
   cylindrical reservoir, identified by the author of the article when
   inspecting the technical condition of these structures, are given. The
   relationship between the appearance of destructive cracks and the design
   errors of a monolithic reinforced concrete beam ceiling is shown. The
   complex nature of the causes of the appearance of cracks in the
   monolithic walls of the underground reservoir is revealed. The article
   describes the capabilities of the LIRA-CAD software package for
   determining the pattern and parameters of fracture-formation of
   reinforced concrete structures for subsequent comparison with the
   results of an inspection of the technical condition of structural
   elements of buildings and structures.},
DOI = {10.1051/matecconf/201825102040},
Article-Number = {02040},
ISSN = {2261-236X},
ResearcherID-Numbers = {Malakhova, Anna Nikolaevna/AAD-9986-2022},
ORCID-Numbers = {Malakhova, Anna Nikolaevna/0000-0003-0203-0193},
Unique-ID = {WOS:000568451100085},
}

@inproceedings{ WOS:000454316500044,
Author = {Nieoczym, Aleksander and Caban, Jacek and Marczuk, Andrzej and
   Brumercik, Frantisek},
Editor = {SzelagSikora, A},
Title = {Construction design of apple sorter},
Booktitle = {CONTEMPORARY RESEARCH TRENDS IN AGRICULTURAL ENGINEERING},
Series = {BIO Web of Conferences},
Year = {2018},
Volume = {10},
Note = {Conference on Contemporary Research Trends in Agricultural Engineering,
   Krakow, POLAND, SEP 25-27, 2017},
Abstract = {When assessing the quality of fruit and packaging process,
   fruit-producing farms owners decide to evaluate fruit by people or
   automated sorting lines. The purchase of an automated sorting line
   generates high costs for the company, but it brings benefits in the form
   of increased work efficiency, and the better organization of fruit
   packaging and storage processes. The use of that machinery and equipment
   is common in agricultural farms as well as in fruit and vegetable
   processing companies. Despite the widespread use of various types of
   fruit sorters, the analysis of the operation of the designed device and
   the study of its technological parameters is still a current research
   problem. During operation of the devices for sorting fruit there are
   many technical problems affecting technological processes and quality of
   fruits. In order to improve the efficiency of sorting fruit, this
   process should be quickly and accurately. The purpose of this paper is
   to present the automated apple sorter line construction design, and
   software for quality controlling fruits. Selected elements of the sorter
   structure including endurance calculation using the Finite Element
   Method (FEM) and fruit control system using image analysis were
   presented.},
DOI = {10.1051/bioconf/20181002025},
Article-Number = {02025},
ISSN = {2117-4458},
ResearcherID-Numbers = {Marczuk, Andrzej/AGM-5836-2022
   Brumerčík, František/AAE-3161-2021
   },
ORCID-Numbers = {Brumerčík, František/0000-0001-7475-3724
   Nieoczym, A;eksander/0000-0002-9725-8483
   Marczuk, Andrzej/0000-0002-2107-2068},
Unique-ID = {WOS:000454316500044},
}

@article{ WOS:000422893700027,
Author = {Paukstelis, Paul J.},
Title = {MolPrint3D: Enhanced 3D Printing of Ball-and-Stick Molecular, Models},
Journal = {JOURNAL OF CHEMICAL EDUCATION},
Year = {2018},
Volume = {95},
Number = {1},
Pages = {169-172},
Month = {JAN},
Abstract = {The increased availability of noncommercial 3D printers has provided
   instructors and students improved access to printing technology.
   However, printing complex ball-and-stick molecular structures faces
   distinct challenges, including the need for support structures that
   increase with molecular complexity. MolPrint3D is a software add-on for
   the Blender 3D modeling package that enhances the printability of
   ball-and-stick molecular models by allowing the user to selectively
   split molecules into fragments. MolPrint3D adds pins to the bond and
   holes in the atom at selected junction points to allow the fragments to
   be printed independently and assembled. This approach significantly
   minimizes the number of support structures needed and enables the
   construction of large macromolecular structures as ball-and-stick
   models. The MolPrint3D pipeline is described, and several examples
   demonstrate how improved printability can provide new tools for
   enhancing student interaction with 3D models.},
DOI = {10.1021/acs.jchemed.7b00549},
ISSN = {0021-9584},
EISSN = {1938-1328},
ORCID-Numbers = {Paukstelis, Paul/0000-0001-8536-4361},
Unique-ID = {WOS:000422893700027},
}

@inproceedings{ WOS:000446160800235,
Author = {Pavel, Churin and Anastasia, Fedosova},
Editor = {Askadskiy, A and Pustovgar, A and Matseevich, T and Adamtsevich, A},
Title = {Analysis of normative and scientific and technical documents in the
   field of testing bridge structures for wind loads},
Booktitle = {XXI INTERNATIONAL SCIENTIFIC CONFERENCE ON ADVANCED IN CIVIL ENGINEERING
   CONSTRUCTION - THE FORMATION OF LIVING ENVIRONMENT (FORM 2018)},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2018},
Volume = {365},
Note = {21st International Scientific Conference on Advanced in Civil
   Engineering Construction - The Formation of Living Environment (FORM),
   Moscow State Univ Civil Engn, Moscow, RUSSIA, APR 25-27, 2018},
Abstract = {In this paper, more than 40 domestic and foreign normative and
   scientific and technical documents in the field of calculation, testing
   and research of bridge loads on wind impacts was analyzed. It is
   established, that in the normative documents acting in the territory of
   the Russian Federation it is indicated that it is necessary to check the
   aerodynamic stability of the large-span bridge structures and the method
   of such testing is partially given in the industry standards. The
   requirements reflected in the standards do not fully exploit the
   achievements of modern architectural and construction aerodynamics, they
   lack methods of conducting the most common tests currently available on
   dynamically similar models of span structures in specialized stands,
   there are no methods for testing complete dynamically similar models of
   bridge structures giving the most complete and reliable information
   about the behavior of the object under investigation in the wind flow.
   Only static tests on geometrically similar models of span structures are
   being considered, allowing to estimate the possibility of occurrence of
   negative phenomena of aerodynamic instability, without any quantitative
   characteristics (maximum amplitudes of oscillations). Normative
   documents acting abroad contain much more complete information than
   domestic ones. Although there is no such thing as the experimental
   technique itself, however, the requirements for both the purpose of the
   tests and the tests themselves including wind tunnels are indicated, and
   these requirements are much higher and formulated more clearly than in
   the domestic ones. Despite this, the techniques themselves, both
   prototyping and testing are lightly or completely absent. It should be
   noted that at present, despite a large number of theoretical and
   experimental studies in the field of aerodynamics of bridges, the theme
   of the interaction of bridge structures with the wind flow is not
   exhausted. Currently continues to register incidents of aerodynamic
   instability of already constructed bridge as well as bridge under
   construction stage. The testing methods have constantly being improved,
   and the rapid development of computer technology and calculation
   software packages makes it possible to develop methods for solving the
   problems of aerodynamic stability of bridge structures and numerical
   simulation techniques.},
DOI = {10.1088/1757-899X/365/5/052009},
Article-Number = {052009},
ISSN = {1757-8981},
ResearcherID-Numbers = {Churin, Pavel/S-2955-2016},
ORCID-Numbers = {Churin, Pavel/0000-0002-2339-0289},
Unique-ID = {WOS:000446160800235},
}

@incollection{ WOS:000432818600006,
Author = {Pleshkova, Sn. and Bekiarski, Al.},
Editor = {Favorskaya, MN and Jain, LC},
Title = {Development of Fast Parallel Algorithms Based on Visual and Audio
   Information in Motion Control Systems of Mobile Robots},
Booktitle = {COMPUTER VISION IN CONTROL SYSTEMS-4: REAL LIFE APPLICATIONS},
Series = {Intelligent Systems Reference Library},
Year = {2018},
Volume = {136},
Pages = {105-138},
Abstract = {Decision making for movement is one of the essential activities in
   motion control systems of mobile robots. It is based on methods and
   algorithms of data processing obtained from the mobile robot sensors,
   usually video and audio sensors, like video cameras and microphone
   arrays. After image processing, information about the objects and
   persons including their current positions in area of mobile robot
   observation can be obtained. The aim of methods and algorithms is to
   achieve the appropriate precision and effectiveness of mobile robot's
   visual perception, as well as the detection and tracking of objects and
   persons applying the mobile robot motion path planning. The precision in
   special cases of visual speaking person's detection and tracking can be
   augmented adding the information of sound arrival in order to receive
   and execute the voice commands. There exist algorithms using only visual
   perception and attention or also the joined audio perception and
   attention. These algorithms are usually tested in the most cases as
   simulations and cannot provide a real time tracking objects and people.
   Therefore, the goal in this chapter is to develop and test the fast
   parallel algorithms for decision making in the motion control systems of
   mobile robots. The depth analysis of the existing methods and algorithms
   was conducted, which provided the main ways to increase the speed of an
   algorithm, such as the optimization, simplification of calculations,
   applying high level programming languages, special libraries for image
   and audio signal processing based on the hybrid hardware and software
   implementations, using processors like Digital Signal Processor (DSP)
   and Field-Programmable Gate Array (FPGA). The high speed proposed
   algorithms were implemented in the parallel computing multiprocessor
   hardware structure and software platform using the well known NVIDIA GPU
   processor and GUDA platform, respectively. The experimental results with
   different parallel structures confirm the real time execution of
   algorithms for the objects and speaking person's detection and tracking
   using the given mobile robot construction.},
DOI = {10.1007/978-3-319-67994-5\_5},
ISSN = {1868-4394},
ISBN = {978-3-319-67994-5; 978-3-319-67993-8},
Unique-ID = {WOS:000432818600006},
}

@inproceedings{ WOS:000516781500106,
Author = {Rocha Junior, Ronaldo Resende and Vieira, Marcos A. M. and Loureiro,
   Antonio A. F.},
Book-Group-Author = {IEEE},
Title = {Dynamic Link Aggregation in Software Defined Networking},
Booktitle = {2018 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS (ISCC)},
Series = {IEEE Symposium on Computers and Communications ISCC},
Year = {2018},
Pages = {620-625},
Note = {IEEE Symposium on Computers and Communications (IEEE ISCC), Natal,
   BRAZIL, JUN 25-28, 2018},
Organization = {IEEE},
Abstract = {The link aggregation technique not only provides robustness to computer
   networks, but can also be a solution to the link saturation problem.
   This technique combines several physical interfaces to create a virtual
   link, adding up their existing bandwidth. In addition to increasing the
   throughput for data transmission, such a technique provides a quick and
   transparent recovery if a particular link becomes unavailable.
   Considering that the use of Software-Defined Networking (SDN) in
   business environments increases every day, this research presents a way
   to create link aggregations in such environments. This brings a better
   availability of services, among other benefits. To do so, an
   architecture that allows automatic, scalable and self-adaptive link
   aggregations was defined and implemented. As a way of evaluating such
   implementation, three algorithms were created using different premises:
   Hash Table, Traffic Analysis and Virtual Round-Robin. All
   implementations were tested in virtual and real environments. In both,
   the Open vSwitch open platform was used for packet switching and the Ryu
   controller was chosen to control the switches.},
ISSN = {1530-1346},
ISBN = {978-1-5386-6950-1},
Unique-ID = {WOS:000516781500106},
}

@inproceedings{ WOS:000466546500035,
Author = {Sani, Mohammed Jawaluddeen and Rahman, Alias Abdul},
Editor = {Rahman, AA and Karim, H},
Title = {GIS AND BIM INTEGRATION AT DATA LEVEL: A REVIEW},
Booktitle = {INTERNATIONAL CONFERENCE ON GEOMATIC \& GEOSPATIAL TECHNOLOGY (GGT
   2018): GEOSPATIAL AND DISASTER RISK MANAGEMENT},
Series = {International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences},
Year = {2018},
Volume = {42-4},
Number = {W9},
Pages = {299-306},
Note = {International Conference on Geomatic and Geospatial Technology (GGT) -
   Geospatial and Disaster Risk Managemen, Kuala Lumpur, MALAYSIA, SEP
   03-05, 2018},
Organization = {Int Soc Photogrammetry \& Remote Sensing},
Abstract = {City Geography Markup Language (CityGML) and Industry Foundation Class
   (IFC) are the two most popular data exchange format for the integration
   of Geographic Information System (GIS) and Building Information
   Modelling (BIM) respectively and has been identified by many researchers
   as an auspicious means of data interoperability between the two domains
   but with challenges on the compatibility between them. The main issue is
   the data loss in the process of information transformation. The success
   of integrating these two domains (GIS and BIM objects) is a great
   achievement toward solving problems in Architecture, Engineering and
   Construction (AEC), Facility Management (FM), Disaster Management (DM)
   sectors. Nevertheless, as we all know GIS and BIM are different fields
   used by different professionals using different software packages, used
   for different purposes, it is definitely face with many challenges
   including data interoperability, mismatch and loss of semantic
   information are bound to occur during the process of integration. In
   order to comprehend the two domains and their data models of CityGML and
   IFC. This paper review existing models on GIS and BIM developed by
   different researchers, the complementarity and compatibility of GIS and
   BIM on the previous integration techniques were also reviewed and
   finally, the paper review the integration of GIS and BIM at the data
   level aimed at solving different problems surrounding it by considering
   the transformation of coordinates at geometric level from CityGML to
   IFC, in order to achieve flow of information between GIS and BIM.},
DOI = {10.5194/isprs-archives-XLII-4-W9-299-2018},
ISSN = {2194-9034},
ResearcherID-Numbers = {Rahman, Alias Abdul/AAM-9643-2021},
Unique-ID = {WOS:000466546500035},
}

@article{ WOS:000418569000006,
Author = {Sertse, Hamsasew M. and Goodsell, Johnathan and Ritchey, Andrew J. and
   Pipes, R. Byron and Yu, Wenbin},
Title = {Challenge problems for the benchmarking of micromechanics analysis:
   Level I initial results},
Journal = {JOURNAL OF COMPOSITE MATERIALS},
Year = {2018},
Volume = {52},
Number = {1},
Pages = {61-80},
Month = {JAN},
Abstract = {Because of composite materials' inherent heterogeneity, the field of
   micromechanics provides essential tools for understanding and analyzing
   composite materials and structures. Micromechanics serves two purposes:
   homogenization or prediction of effective properties and
   dehomogenization or recovery of local fields in the original
   heterogeneous microstructure. Many micromechanical tools have been
   developed and codified, including commercially available software
   packages that offer micromechanical analyses as stand-alone tools or as
   part of an analysis chain. With the increasing number of tools
   available, the practitioner must determine which tool(s) provides the
   most value for the problem at hand given budget, time, and resource
   constraints. To date, simple benchmarking examples have been developed
   in an attempt to address this challenge. The present paper presents the
   benchmark cases and results from the Micromechanical Simulation
   Challenge hosted by the Composites Design and Manufacturing HUB. The
   challenge is a series of comprehensive benchmarking exercises in the
   field of micromechanics against which such tools can be compared. The
   Level I challenge problems consist of six microstructure cases,
   including aligned, continuous fibers in a matrix, with and without an
   interphase; a cross-ply laminate; spherical inclusions; a plain-weave
   fabric; and a short-fiber microstructure with random fiber orientation.
   In the present phase of the simulation challenge, the material
   constitutive relations are restricted to linear thermoelastic. Partial
   results from DIGIMAT-MF, ESI VPS, MAC/GMC, finite volume direct
   averaging method, Altair MDS, SwiftComp, and 3D finite element analysis
   are reported. As the challenge is intended to be ongoing, the full
   results are hosted and updated online at www.cdmHUB.org.},
DOI = {10.1177/0021998317702437},
ISSN = {0021-9983},
EISSN = {1530-793X},
Unique-ID = {WOS:000418569000006},
}

@article{ WOS:000456212500003,
Author = {Shmeleva, T. R.},
Title = {VERIFICATION OF THE TRIANGULAR COMMUNICATION GRIDS PROTOCOLS BY INFINITE
   PETRI NETS},
Journal = {RADIO ELECTRONICS COMPUTER SCIENCE CONTROL},
Year = {2018},
Number = {4},
Pages = {31-41},
Abstract = {Context. Computing and communication grids are a powerful means of
   increasing the performance and quality of service of modern networks. In
   two-dimensional grids the basic cell forms are a triangle, a square, and
   a hexagon. Triangular grids are used in solving boundary value problems
   with triangular finite elements, in broadcasting systems and in
   television. The simplest and most efficient implementations of grids can
   possess hidden defects and vulnerabilities in terms of secure
   information exchange. Thus, the verification of grids is an urgent task.
   Objective. The goal of the paper is to construct the models of
   triangular communication grids in the form of infinite Petri nets and to
   investigate their properties for proving the protocols (verification)
   correctness.
   Method. Research methods are based on the basics of graph theory, linear
   algebra, the theoretical foundations of Petri nets, mathematical
   modeling and simulation.
   Results. A parametric description of the triangular communication grid
   on the plane, in a direct and a dual form, is constructed. The switching
   node implements full-duplex transmission and buffering of packets with a
   limited capacity of the internal buffer. Analytic expressions are
   obtained for estimating the number of model components. Solving infinite
   systems of linear equations in parametric form allowed us to prove the
   invariance of a model of arbitrary size. Invariance is one of the basic
   properties of the ideal protocol model which determines the safety of
   the network. The practical significance of the results obtained lies in
   the construction of safe grid schemes for further software and hardware
   implementation, which is officially confirmed by the inclusion of
   triangular grid models in the archive of Petri net models of the
   University Paris 6 Informatics Laboratory.
   Conclusions. For the first time, a mathematical model of triangular
   communication grids with a regular structure and an arbitrary size in
   the form of infinite Petri nets was constructed for verification of
   information transmit protocols in grids. The application of the
   technique for verification of triangular communication structures allows
   the further development of the infinite Petri nets theory for
   constructing and investigating models of arbitrary grids with a regular
   structure.},
DOI = {10.15588/1607-3274-2018-4-3},
ISSN = {1607-3274},
EISSN = {2313-688X},
Unique-ID = {WOS:000456212500003},
}

@inproceedings{ WOS:000458739600009,
Author = {Suciu, George and Dragu, Mihaela and Hussain, Ijaz and Iliescu,
   Ana-Maria and Orza, Oana and Mocanu, Cristian},
Editor = {Dobre, C and Melero, FJ and Ciobanu, RI and Palmieri, F},
Title = {3D Modeling Using Parrot Bebop 2 FPV},
Booktitle = {2018 IEEE 16TH INTERNATIONAL CONFERENCE ON EMBEDDED AND UBIQUITOUS
   COMPUTING (EUC 2018)},
Year = {2018},
Pages = {61-65},
Note = {16th IEEE International Conference on Embedded and Ubiquitous Computing
   (EUC), Univ Politehnica Bucharest, Bucharest, ROMANIA, OCT 29-31, 2018},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {It is expected that drones will play a major role in connecting the
   world in future. They will be delivering packages and merchandise,
   serving as mobile hotspots for broadband wireless access, will deal with
   surveillance purposes and security of smart cities. Drones, while can be
   used for the betterment of the community, can also be used by malevolent
   entities to gather physical and cyber-attacks, and threaten the society.
   However, we took a different approach using Parrot Bebop 2 by doing 3D
   modeling of our building. In this paper the main problems and the
   available solutions are addressed for the generation of 3D models from
   drone images. Close range photogrammetry has dealt for many years with
   manual or automatic image measurements for precise 3D modelling.
   Nowadays drones are also becoming a major source for scanning and
   snapping, but image-based modelling remains the most complete, portable,
   flexible and widely used approach. In this article we used 3D modeling
   as a process of establishing a mathematical representation of a
   3-dimensional building by using the software Pix4D or Pix4D cloud. 3D
   models are now widely used on industrial scale and real estate dealers,
   architecture, construction, dealing with hazardous situations and
   product development using 3D models for visualizing, simulating and
   depiction graphic designs.},
DOI = {10.1109/EUC.2018.00016},
ISBN = {978-1-5386-8296-8},
Unique-ID = {WOS:000458739600009},
}

@inproceedings{ WOS:000461174900094,
Author = {Ter-Martirosyan, A. Z. and Ter-Martirosyan, Z. G. and Sidorov, V. V.},
Editor = {Travush, VI and Fomin, VM},
Title = {Numerical simulation of the structures bases stress-strain state taking
   into account the time factor},
Booktitle = {VII INTERNATIONAL SYMPOSIUM ACTUAL PROBLEMS OF COMPUTATIONAL SIMULATION
   IN CIVIL ENGINEERING},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2018},
Volume = {456},
Note = {7th International Symposium on Actual Problems of Computational
   Simulation in Civil Engineering (APCSCE), Novosibirsk, RUSSIA, JUL
   01-08, 2018},
Organization = {Russian Acad Architecture \& Civil Engn Sci; Novosibirsk State Univ
   Architecture \& Civil Engn; Russian Univ Transport; Peoples Friendship
   Univ Russia; Moscow Inst Architecture; JSC Res Ctr Construct, Sci Res
   Ctr StaDyO; Fed Ctr Regulat, Standardizat \& Tech Conform Assessment
   Construct; Tomsk State Univ Architecture \& Civil Engn; Russian Acad
   Sci, Siberian Branch, Khristianovich Inst Theoret \& Appl Mech},
Abstract = {The complexity of recent numerical calculations is steadily growing
   because of the desire to obtain more reliable forecast results taking
   into account the complex processes of deformation of the ground base in
   complicated ground conditions and considering the use of modern
   construction technologies. The phenomenon of creep refers to such
   complex processes. Its occurrence can be taken into account using a
   creep-based model, for example Soft Soil Creep, which is a part of the
   PLAXIS software package. The article provides verification calculations
   that allow determining the degree of comparability of the samples
   behaviour in the calculation model and real soil base samples, methods
   for correcting the results of laboratory tests. An example of a
   numerical simulation of the construction of an underground structure is
   given, which shows the significant effect of creep on the deformed state
   of the substrate after a long period of its operation. Using the example
   of a single barrette, it was shown that taking creep into account under
   the lower end leads to a redistribution of the external load along the
   lateral surface, additional mobilization of frictional forces, which
   leads to a change in the operation of such an element as a whole.},
DOI = {10.1088/1757-899X/456/1/012094},
Article-Number = {012094},
ISSN = {1757-8981},
ResearcherID-Numbers = {Ter-Martirosyan, Armen/Q-8635-2017
   Sidorov, Vitalii V.V./C-2248-2018
   },
ORCID-Numbers = {Ter-Martirosyan, Armen/0000-0001-8787-826X
   Sidorov, Vitalii/0000-0003-2971-8852},
Unique-ID = {WOS:000461174900094},
}

@inproceedings{ WOS:000722199500094,
Author = {Traub, Kurt M. and Mai, Liem T. and Shah, Jay M.},
Book-Group-Author = {IEEE},
Title = {Instant Power Asset Information and Construction Status Tracking System
   using a Geographical Platform},
Booktitle = {2018 IEEE/PES TRANSMISSION AND DISTRIBUTION CONFERENCE AND EXPOSITION
   (T\&D)},
Series = {Transmission and Distribution Conference and Exposition},
Year = {2018},
Note = {IEEE/PES Transmission and Distribution Conference and Exposition (T and
   D), Denver, CO, APR 16-19, 2018},
Organization = {IEEE; IEEE Power \& Energy Soc},
Abstract = {The traditional way of issuing a transmission and distribution
   construction package is multiple copies of bulky binders and large plan
   and profile drawings in fiberboard tubes. Some of this information is
   also required to be transmitted to various agencies such as
   environmental, land acquisition and Construction personnel. Typical data
   include location identifier, pole height and class, foundation details,
   framing and wire stringing information. Ampirical has upgraded the
   transmittal of this design data to a more accurate and more efficient
   interface which utilizes currently available technologies. All design
   location information can be visualized in a geographical computer
   software platform such as Google Earth. The most up to date construction
   statuses are represented with icons and colors. Construction issues can
   be promptly identified by reviewing the status map and site photos that
   are attached to each structure location. All critical information could
   be quickly retrieved with or without an internet connection on a tablet
   at the office, construction site, and maintenance and/or storm damage
   locations.},
ISSN = {2160-8555},
EISSN = {2160-8563},
ISBN = {978-1-5386-5583-2},
Unique-ID = {WOS:000722199500094},
}

@article{ WOS:000462789700007,
Author = {Uvarov, B. M. and Nikitchuk, V, A.},
Title = {Radioelectronic Apparatus Design with Optimal Reliability Indicators},
Journal = {VISNYK NTUU KPI SERIIA-RADIOTEKHNIKA RADIOAPARATOBUDUVANNIA},
Year = {2018},
Number = {75},
Pages = {48-53},
Abstract = {Introduction. Considered the problem of maximizing the reliability of
   electronic structure elements of unit cells and entire electronic unit
   under the action of thermal destabilizing factors. Reliability of all
   device essentially depends from the thermal regime of electronic
   components, vibration and shock resistance. Optimization of the thermal
   regime of the cell can be achieved by an appropriate placement of the
   heat-producing elements, because the temperature of each of them is
   determined by convective, conductive and radiative connections with
   elements of construction and each other.
   Calculation of the cells average temperatures in the block. Described
   calculation of the average temperature of cells in the block. Averaged
   temperature of the cell is determined by the temperature of the coolant
   (in most cases, the air), the cells placement in the block and the
   temperatures of their elements.
   Temperature calculation of the electronic structure elements and the
   reliability calculations of cells and block. The calculation of the
   electronic structure elements temperatures and reliability of the cells
   and the block is considered. In each cell are usually set dozens of
   electronic structure elements, for each it is necessary to calculate the
   temperatures by which it is determined the reliability indicators.
   The optimization of the cells arrangement in the block. The optimization
   of the cells arrangement in the block is proposed. Heat generation in
   the cells in most designs is uneven, depending on the thermal power
   installed in them electronic structure elements, therefore, reliability
   indicators can be optimized, rationally placing the cells. For each
   accommodation option, the electronic structure elements temperature and
   the reliability of each cell are calculated. This is achieved through
   the specially created software package. Developed and described software
   modules for optimal placement of cells in the block to ensure that it
   has the minimum temperature of the electronic structure elements during
   operation. The design aim to reduce the temperature of heat-producing
   elements, and this can be achieved by appropriately placing the latter,
   removing them from each other to reduce interference and improve heat
   transfer. With a large number of cells in the block the rational
   placement it is difficult to achieve, since the number of non-repeatable
   placement of cells in a block is equal to the number of
   reconfigurations.
   Conclusions. The optimal cells arrangement in the block allows to obtain
   a design in which the maximum reliability is ensured. This is achieved
   by using a specially created software complex, which combines four
   software modules: BlockTermo2, InputData, Relia2015, OptimBlock},
DOI = {10.20535/RADAP.2018.75.48-53},
ISSN = {2310-0389},
ORCID-Numbers = {, Nikitchuk/0000-0002-4354-0547},
Unique-ID = {WOS:000462789700007},
}

@inproceedings{ WOS:000463037500091,
Author = {Wang Yi-Lin and Wang Jun},
Book-Group-Author = {IEEE},
Title = {onstruction of Interlibrary Loan Information System Based on SaaS Mode},
Booktitle = {2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN
   MEDICINE AND EDUCATION (ITME 2018)},
Year = {2018},
Pages = {432-436},
Note = {9th International Conference on Information Technology in Medicine and
   Education (ITME), Hangzhou, PEOPLES R CHINA, OCT 19-21, 2018},
Organization = {IEEE Comp Soc; China Jiliang Univ; Zhejiang Univ; Zhejiang Provincial
   Nat Sci Fdn; Fujian Univ Tradit Chinese Med; Univ Texas San Antonio;
   Swinburne Univ Technol; Iwate Prefectural Univ; Shandong Normal Univ;
   Xiamen Univ; Fuzhou Univ; Hunan Univ Humanities Sci \& Technol; Lanzhou
   Univ; Henan Univ Technol; Wuhan Univ Technol; E China Normal Univ;
   Birmingham City Univ; Univ Southern Queensland},
Abstract = {In order to increase sharing level and effect of books and information
   between libraries, construction of books-checkout information system in
   the interlibrary using the system architecture of SaaS (Software as a
   Service), books-checkout information system consists of four modules,
   including administrator management, user management, function management
   and literature management. The system structure and module design of
   information system software are also introduced in detail, and function
   analysis of each module are made, task content of each module is
   designed, the relation between entity class and attribute involved in
   information system are demonstrated, and system class diagram is listed,
   finally main module function of information system is implemented by
   Java Web technology and mysql database},
DOI = {10.1109/ITME.2018.00103},
ISBN = {978-1-5386-7743-8},
ResearcherID-Numbers = {Ajmi, Ghalia Al/AAB-6467-2019},
Unique-ID = {WOS:000463037500091},
}

@inproceedings{ WOS:000455329700132,
Author = {Xin, Jin},
Editor = {Zheng, Z},
Title = {Research and Design of Education and Teaching Resource Management System
   based on ASP.NET Technology},
Booktitle = {2018 3RD INTERNATIONAL CONFERENCE ON EDUCATION \& EDUCATION RESEARCH
   (EDUER 2018)},
Year = {2018},
Pages = {609-613},
Note = {3rd International Conference on Education \& Education Research (EDUER),
   Wuhan, PEOPLES R CHINA, NOV 08-09, 2018},
Abstract = {The informatization of career education is an important foundation and
   part of the reform of career education. The construction of professional
   education resource library is one of the important contents of
   professional education informatization construction. There is a lack of
   unified planning for the construction of teaching resource pool in
   higher vocational colleges in China. The system software construction
   investment is less, and the content is not system; Teacher education
   information technology and teaching strategy application level is not
   high, and there are slow development and other problems. This topic
   elaborates how to use the base. NET architecture, and SQL Server as the
   background database system, the development of teaching resource library
   platform design and implementation process. Based on the actual
   situation of Zaozhuang Vocational College of Science and Technology,
   this paper makes a comprehensive analysis on the demand of teaching
   resource management system, establishes a set of teaching resource
   management system for education teaching in our college, and improves
   the informatization level of education teaching. This paper analyzes the
   situation and existing problems of the construction of education
   teaching resource library in China. Research related technologies
   required for system development, include Ajax technologies. NET
   technology, and ADO. NET technology. The demand analysis of the system,
   the realization of the structural model and functional module are
   described in detail. The system is used for testing in the Department of
   Information Engineering of the college. Its functions are normal, and it
   basically meets the needs of normal teaching and course construction},
DOI = {10.25236/eduer.18.135},
ISBN = {978-1-912407-74-3},
Unique-ID = {WOS:000455329700132},
}

@inproceedings{ WOS:000515778200003,
Author = {Zhu, Jianping and Song, Wei and Zhu, Ziyuan and Ying, Jiameng and Li,
   Boya and Tu, Bibo and Shi, Gang and Hou, Rui and Meng, Dan},
Book-Group-Author = {ACM},
Title = {CPU Security Benchmark},
Booktitle = {PROCEEDINGS OF THE 1ST WORKSHOP ON SECURITY-ORIENTED DESIGNS OF COMPUTER
   ARCHITECTURES AND PROCESSORS (SECARCH'18)},
Year = {2018},
Pages = {8-14},
Note = {1st Workshop on Security-Oriented Designs of Computer Architectures and
   Processors (SecArch), Toronto, CANADA, OCT 15, 2018},
Organization = {Assoc Comp Machinery; ACM SIGSAC},
Abstract = {The current electronic-economy is booming, electronic-wallets, encrypted
   virtual-money, mobile payments, and other new generations of economic
   instruments are springing up. As the most important cornerstone, CPU is
   facing serious security challenges. And with the blowout of actual
   application requirements, the importance of CPU security testing is
   increasing. However, the actual security threats to computer systems are
   also becoming increasingly rampant (now attackers often use multiple
   different types of vulnerabilities to construct complex attack systems,
   not just a single attack chain). The traditional vulnerability detection
   model is not capable of comprehensive security assessment.
   We first proposed a comprehensive CPU Security Benchmark solution with
   high coverage for existing known vulnerabilities, including Undocumented
   Instructions detection, Control Flow Integrity test, Memory Errors
   detection, and Cache Side Channels detection, Out of Order and
   Speculative execution vulnerabilities (Meltdown and Spectre series)
   tests, and more. Our benchmark provides meaningful and constructive
   feedbacks for evading architecture/microarchitecture design flaws,
   system security (OS and libraries) software patches design, and user
   programming vulnerabilities tips.
   We hope that the work of this paper will promote the computer system
   security testing from the past scatter point and line mode (single
   specific vulnerability and attack chain testing) to coordinated and
   whole surface mode (multi-type vulnerabilities and attack network
   testing), thus creating a new research direction of the comprehensive
   and balanced CPU Security Benchmark. Our test suite will play an
   inspiring role in the comprehensive assessment of security in personal
   computer devices (PC/Mobile Phone) and large server clusters
   (Servers/Cloud), as well as the construction of more secure Block-Chain
   nodes (IOT), and many other practical applications.},
DOI = {10.1145/3267494.3267499},
ISBN = {978-1-4503-5991-7},
ResearcherID-Numbers = {meng, Danqing/GYV-0122-2022
   Song, Wei/AAF-9081-2020},
ORCID-Numbers = {Song, Wei/0000-0001-5649-1580},
Unique-ID = {WOS:000515778200003},
}

@article{ WOS:000418564200037,
Author = {Schiffthaler, Bastian and Bernhardsson, Carolina and Ingvarsson, Par K.
   and Street, Nathaniel R.},
Title = {BatchMap: A parallel implementation of the OneMap R package for fast
   computation of F-1 linkage maps in outcrossing species},
Journal = {PLOS ONE},
Year = {2017},
Volume = {12},
Number = {12},
Month = {DEC 20},
Abstract = {With the rapid advancement of high throughput sequencing, large numbers
   of genetic markers can be readily and cheaply acquired, but most current
   software packages for genetic map construction cannot handle such dense
   input. Modern computer architectures and server farms represent untapped
   resources that can be used to enable higher marker densities to be
   processed in tractable time. Here we present a pipeline using a modified
   version of OneMap that parallelizes over bottleneck functions and
   achieves substantial speedups for producing a high density linkage map
   (N = 20,000). Using simulated data we show that the outcome is as
   accurate as the traditional pipeline. We further demonstrate that there
   is a direct relationship between the number of markers used and the
   level of deviation between true and estimated order, which in turn
   impacts the final size of a genetic map.},
DOI = {10.1371/journal.pone.0189256},
Article-Number = {e0189256},
ISSN = {1932-6203},
ResearcherID-Numbers = {Ingvarsson, Pär/G-2748-2010
   Street, Nathaniel Robert/B-3920-2008
   },
ORCID-Numbers = {Street, Nathaniel Robert/0000-0001-6031-005X
   Ingvarsson, Par/0000-0001-9225-7521},
Unique-ID = {WOS:000418564200037},
}

@article{ WOS:000414030500002,
Author = {Al Azzawi, Wessam and Epaarachchi, J. A. and Islam, Mainul and Leng,
   Jinsong},
Title = {Implementation of a finite element analysis procedure for structural
   analysis of shape memory behaviour of fibre reinforced shape memory
   polymer composites},
Journal = {SMART MATERIALS AND STRUCTURES},
Year = {2017},
Volume = {26},
Number = {12},
Month = {DEC},
Abstract = {Shape memory polymers (SMPs) offer a unique ability to undergo a
   substantial shape deformation and subsequently recover the original
   shape when exposed to a particular external stimulus. Comparatively low
   mechanical properties being the major drawback for extended use of SMPs
   in engineering applications. However the inclusion of reinforcing fibres
   in to SMPs improves mechanical properties significantly while retaining
   intrinsic shape memory effects. The implementation of shape memory
   polymer composites (SMPCs) in any engineering application is a unique
   task which requires profound materials and design optimization. However
   currently available analytical tools have critical limitations to
   undertake accurate analysis/simulations of SMPC structures and slower
   derestrict transformation of breakthrough research outcomes to real-life
   applications. Many finite element (FE) models have been presented. But
   majority of them require a complicated user-subroutines to integrate
   with standard FE software packages. Furthermore, those subroutines are
   problem specific and difficult to use for a wider range of SMPC
   materials and related structures. This paper presents a FE simulation
   technique to model the thermomechanical behaviour of the SMPCs using
   commercial FE software ABAQUS. Proposed technique incorporates material
   time-dependent viscoelastic behaviour. The ability of the proposed
   technique to predict the shape fixity and shape recovery was evaluated
   by experimental data acquired by a bending of a SMPC cantilever beam.
   The excellent correlation between the experimental and FE simulation
   results has confirmed the robustness of the proposed technique.},
DOI = {10.1088/1361-665X/aa928e},
Article-Number = {125002},
ISSN = {0964-1726},
EISSN = {1361-665X},
ResearcherID-Numbers = {Leng, Jinsong/H-9954-2012
   Al-Azzawi, Wessam/AAX-7415-2021
   },
ORCID-Numbers = {Al-Azzawi, Wessam/0000-0002-2879-8849
   Islam, Md Mainul/0000-0003-3265-2348},
Unique-ID = {WOS:000414030500002},
}

@article{ WOS:000418395600020,
Author = {Bates, Maxwell and Lachoff, Joe and Meech, Duncan and Zulkower, Valentin
   and Moisy, Anais and Luo, Yisha and Tekotte, Hille and Scheitz, Cornelia
   Johanna Franziska and Khilari, Rupal and Mazzoldi, Florencio and
   Chandran, Deepak and Groban, Eli},
Title = {Genetic Constructor: An Online DNA Design Platform},
Journal = {ACS SYNTHETIC BIOLOGY},
Year = {2017},
Volume = {6},
Number = {12},
Pages = {2362-2365},
Month = {DEC},
Abstract = {Genetic Constructor is a cloud Computer Aided Design (CAD) application
   developed to support synthetic biologists from design intent through DNA
   fabrication and experiment iteration. The platform allows users to
   design, manage, and navigate complex DNA constructs and libraries, using
   a new visual language that focuses on functional parts abstracted from
   sequence. Features like combinatorial libraries and automated primer
   design allow the user to separate design from construction by focusing
   on functional intent, and design constraints aid iterative refinement of
   designs. A plugin architecture enables contributions from scientists and
   coders to leverage existing powerful software and connect to DNA
   foundries. The software is easily accessible and platform agnostic, free
   for academics, and available in an open-source community edition.
   Genetic Constructor seeks to democratize DNA design, manufacture, and
   access to tools and services from the synthetic biology community.},
DOI = {10.1021/acssynbio.7b00236},
ISSN = {2161-5063},
ResearcherID-Numbers = {Cai, Yizhi/A-3550-2010},
ORCID-Numbers = {Tekotte, Hille/0000-0003-2164-7412
   Cai, Yizhi/0000-0003-1663-2865},
Unique-ID = {WOS:000418395600020},
}

@article{ WOS:000415725100025,
Author = {Rufino, Joao and Alam, Muhammad and Almeida, Joao and Ferreira, Joaquim},
Title = {Software defined P2P architecture for reliable vehicular communications},
Journal = {PERVASIVE AND MOBILE COMPUTING},
Year = {2017},
Volume = {42},
Pages = {411-425},
Month = {DEC},
Abstract = {Vehicular networking is considered a promising and enabling technology
   that helps in the realization and diversification of vehicle related
   applications such as road traffic safety, emergency management, and
   infotainment. For applications such as the road traffic safety, the
   system has to respond in certain bounded time and ensure higher
   reliability. A major bottleneck of the existing architectures is the
   reliability and scalability that results in a considerable performance
   degradation. Therefore, in this paper, we proposed a software defined
   peer to peer (P2P) architecture for reliable vehicular communications
   solution that scales out overall network intelligence into the system
   components (Road side units, on-board units, Gateways, Control centre)
   and offer an unprecedented reliability, adaptability and scalability.
   The reliability of the communication is achieved by using redundant
   system components such as multiple switches and GWs links that are
   deployed to achieve lower communication latencies and higher packet
   success rates. Furthermore, higher reliability is achieved by the
   application of fast-failover and fast recovery techniques. The proposed
   architecture is implemented using Docker, and containerized ONOS and
   Cassandra. The obtained results show that the proposed architecture
   offer higher reliability by offering higher packet success rate (PSR)
   and smaller round trip time (RTT). (C) 2017 Elsevier B.V. All rights
   reserved.},
DOI = {10.1016/j.pmcj.2017.06.014},
ISSN = {1574-1192},
EISSN = {1873-1589},
ResearcherID-Numbers = {Almeida, João/AAQ-9039-2020
   Rufino, João/L-7295-2019
   alam, mohammed/GXH-3645-2022
   Ferreira, Joaquim/A-5376-2016},
ORCID-Numbers = {Almeida, João/0000-0001-6634-6213
   Rufino, João/0000-0001-7628-4423
   Ferreira, Joaquim/0000-0002-7471-5135},
Unique-ID = {WOS:000415725100025},
}

@article{ WOS:000413056200005,
Author = {Tahersima, Mohammad and Tikalsky, Paul},
Title = {Finite element modeling of hydration heat in a concrete slab-on-grade
   floor with limestone blended cement},
Journal = {CONSTRUCTION AND BUILDING MATERIALS},
Year = {2017},
Volume = {154},
Pages = {44-50},
Month = {NOV 15},
Abstract = {Delayed ettringite formation and thermal cracks are two main concerns of
   temperature rise in mass concrete structures. The mass concrete
   containing limestone blended cement and fly ash has different hydration
   heat that was not well predicted by commercially available software. The
   purpose of this study is to provide such data for designers or software
   developers. In this study, results of thermocouple measurement at the
   construction site are compared with a widely used commercial finite
   element model (FEM) and a widely used concrete design software package.
   Verification of the FEM with experimental data has been completed with
   cement ASTM C595 Type I limestone blended cement (IL) with a 25\%
   replacement by mass of ASTM C618 Type C fly ash (FC). In this study, the
   same mixture design of construction site was used in modeling of
   hydration heat of full scale building. The heat of hydration for cement
   Type I-II and 25\% of FC and cement Type IL are predicted to compare the
   results of hydration heat generated by different mix proportions. The
   FEM result has a much better compatibility with the construction site
   data with respect to the commercial concrete software results. The
   prediction accuracy of finite element results is about 15\% more for the
   maximum temperature rise and 30\% more for the peak time. Cement Type IL
   has greater hydration heat than cement Type I-II. Applying measured
   ambient temperature and calorimetry results are two main factors in
   precise prediction of hydration heat. (C) 2017 Elsevier Ltd. All rights
   reserved.},
DOI = {10.1016/j.conbuildmat.2017.07.176},
ISSN = {0950-0618},
EISSN = {1879-0526},
ResearcherID-Numbers = {Tikalsky, Paul/J-9353-2017},
ORCID-Numbers = {Tikalsky, Paul/0000-0001-9105-3578},
Unique-ID = {WOS:000413056200005},
}

@article{ WOS:000416018600006,
Author = {Chien, A. and Balaji, P. and Dun, N. and Fang, A. and Fujita, H. and
   Iskra, K. and Rubenstein, Z. and Zheng, Z. and Hammond, J. and Laguna,
   I. and Richards, D. and Dubey, A. and van Straalen, B. and Hoemmen, M.
   and Heroux, M. and Teranishi, K. and Siegel, A.},
Title = {Exploring versioned distributed arrays for resilience in scientific
   applications: global view resilience},
Journal = {INTERNATIONAL JOURNAL OF HIGH PERFORMANCE COMPUTING APPLICATIONS},
Year = {2017},
Volume = {31},
Number = {6},
Pages = {564-590},
Month = {NOV},
Abstract = {Exascale studies project reliability challenges for future HPC systems.
   We present the Global View Resilience (GVR) system, a library for
   portable resilience. GVR begins with a subset of the Global Arrays
   interface, and adds new capabilities to create versions, name versions,
   and compute on version data. Applications can focus versioning where and
   when it is most productive, and customize for each application structure
   independently. This control is portable, and its embedding in
   application source makes it natural to express and easy to maintain. The
   ability to name multiple versions and partially materialize them
   efficiently makes ambitious forward-recovery based on data slices across
   versions or data structures both easy to express and efficient. Using
   several large applications (OpenMC, preconditioned conjugate gradient
   (PCG) solver, ddcMD, and Chombo), we evaluate the programming effort to
   add resilience. The required changes are small (< 2\% lines of code
   (LOC)), localized and machine-independent, and perhaps most important,
   require no software architecture changes. We also measure the overhead
   of adding GVR versioning and show that overheads < 2\% are generally
   achieved. This overhead suggests that GVR can be implemented in
   large-scale codes and support portable error recovery with modest
   investment and runtime impact. Our results are drawn from both IBM BG/Q
   and Cray XC30 experiments, demonstrating portability. We also present
   two case studies of flexible error recovery, illustrating how GVR can be
   used for multi-version rollback recovery, and several different
   forward-recovery schemes. GVR's multi-version enables applications to
   survive latent errors (silent data corruption) with significant
   detection latency, and forward recovery can make that recovery extremely
   efficient. Our results suggest that GVR is scalable, portable, and
   efficient. GVR interfaces are flexible, supporting a variety of recovery
   schemes, and altogether GVR embodies a gentle-slope path to tolerate
   growing error rates in future extreme-scale systems.},
DOI = {10.1177/1094342016664796},
ISSN = {1094-3420},
EISSN = {1741-2846},
ResearcherID-Numbers = {Heroux, Michael/AAG-5894-2020
   Richards, David/AAA-7297-2019
   },
ORCID-Numbers = {Heroux, Michael/0000-0002-5893-0273
   Iskra, Kamil/0000-0001-7000-4195
   Dubey, Anshu/0000-0003-3299-7426},
Unique-ID = {WOS:000416018600006},
}

@article{ WOS:000417046500180,
Author = {Yuan, Ze and Wang, Weizhou and Peng, Jing and Liu, Fuchao and Yang,
   Jianhua},
Title = {Case Library Construction Technology of Energy Loss in Distribution
   Networks Considering Regional Differentiation Theory},
Journal = {ENERGIES},
Year = {2017},
Volume = {10},
Number = {11},
Month = {NOV},
Abstract = {The grid structures, load levels, and running states of distribution
   networks in different supply regions are known as the influencing
   factors of energy loss. In this paper, the case library of energy loss
   is constructed to differentiate the crucial factors of energy loss in
   the different supply regions. First of all, the characteristic state
   values are selected as the representation of the cases based on the
   analysis of energy loss under various voltage classes and in different
   types of regions. Then, the methods of Grey Relational Analysis and the
   K-Nearest Neighbor are utilized to implement the critical technologies
   of case library construction, including case representation, processing,
   analysis, and retrieval. Moreover, the analysis software of the case
   library is designed based on the case library construction technology.
   Some case studies show that there are many differences and similarities
   concerning the factors that influence the energy loss in different types
   of regions. In addition, the most relevant sample case can be retrieved
   from the case library. Compared with the traditional techniques,
   constructing a case library provides a new way to find out the
   characteristics of energy loss in different supply regions and
   constitutes differentiated loss-reducing programs.},
DOI = {10.3390/en10111861},
Article-Number = {1861},
ISSN = {1996-1073},
Unique-ID = {WOS:000417046500180},
}

@article{ WOS:000411777800032,
Author = {Liu Xiao-ying and Zeng Jie and Guo Xiao-hua and Gong Xiao-jing and Li
   Ning-xi and Li Tong-wei and Wang Ji-Gang},
Title = {The Integrated Monitoring Method of Optical Fiber Gas Pressure and
   Temperature Based on Reflection Spectrum Characteristic Identification},
Journal = {SPECTROSCOPY AND SPECTRAL ANALYSIS},
Year = {2017},
Volume = {37},
Number = {9},
Pages = {2838-2843},
Month = {SEP},
Abstract = {Aiming at aircraft airborne environment multi-parameter comprehensive
   testing requirements, by analyzing the theories and experimental
   results, a kind of fiber Bragg grating (FBG) gas pressure and
   temperature integrated monitoring method based on spectral reflectance
   characteristics identification is studied, and the dual parameter
   sensing mechanism as well as its theoretical model based on the
   diaphragm structure are also studied in this paper. OptiGrating software
   based on the coupled mode theory was used to simulate the reflection
   spectrum of the fiber Bragg grating sensor under different pressure and
   temperature conditions. Therefore, the characteristics of fiber Bragg
   grating sensor under different pressure and temperature conditions in
   simulate environment appeared. On this basis, with the aid of the flat
   diaphragm pressure sensitive structure enjoying a excellent elasticity
   and recovery performance, a diaphragm type double optical fiber gas
   pressure/temperature integrated monitoring system was constructed, and
   the package of the diaphragm type double fiber optic
   pressure/temperature sensing model was studied. Beyond that, the
   performance characteristics of the sensing model was also presented. A
   series of data analysis of the experiment showed that the strain sensing
   fiber Bragg grating reflection spectrum shifted to short wavelength
   direction under the condition of constant temperature with the
   increasing of the gas pressure, and the strain sensing fiber Bragg
   grating reflection spectrum sensitivity coefficient was about 0. 803 0
   nm center dot MPa-1. The reflection spectrum peak and the sidelobe level
   showed a good linear relationship with the pressure changing. When the
   air pressure was constant and temperature changed, fiber Bragg grating
   center wavelength sensitivity of temperature sensing fiber Bragg grating
   which was not affected by strain and only sensitive to temperature was
   about 9. 39 pm center dot degrees C-1. However, when the pressure and
   temperature cross changed, micro pressure can be monitored in real-time
   under the condition of variable temperature. Fiber Bragg grating sensing
   by the inhomogeneous strain effect has certain chirp reflection spectra,
   the sidelobe peak wavelength of reflection spectrum will shift because
   of the change of temperature and pressure, which needs measurements at
   any moment in accordance with the monitoring environment. It has to be
   noticed that the temperature and pressure both have a good linear
   relationship with the fiber Bragg grating reflection spectrum center
   wavelength, and the spectral reflectance under different air pressure
   corresponding to the same order number sidelobe peak amplitude is equal.
   The above research provides a useful help for online comprehensive test
   of multi physical parameters in aviation spacecraft system.},
DOI = {10.3964/j.issn.1000-0593(2017)09-2838-06},
ISSN = {1000-0593},
Unique-ID = {WOS:000411777800032},
}

@article{ WOS:000404707800012,
Author = {Abudeif, A. M. and Raef, A. E. and Moneim, A. A. Abdel and Mohammed, M.
   A. and Farrag, A. F.},
Title = {Dynamic geotechnical properties evaluation of a candidate nuclear power
   plant site (NPP): P- and S-waves seismic refraction technique, North
   Western Coast, Egypt},
Journal = {SOIL DYNAMICS AND EARTHQUAKE ENGINEERING},
Year = {2017},
Volume = {99},
Pages = {124-136},
Month = {AUG},
Abstract = {Determination of the dynamic geotechnical properties and V(s)30 of soil
   and rocks from seismic wave velocities serves as essential inputs for a
   foundation design cognizant of seismic site response and rock strength.
   This study evaluates a site which was suggested for a Nuclear Power
   Plant (NPP) in El-Dabaa area, north western coast of Egypt. On the near
   subsurface geology is made up of a thick succession of limestone
   overlain by a thin layer of soft soil. Assessment of geotechnical
   materials and V(s)30 of the near sub-surface lithological layers are
   required for design of the foundation of critical structures like
   turbo-generator and reactor buildings. Interpretation of ninety one
   shallow P-waves and S-waves seismic refraction profiles distributed
   within the study area in conjunction with data of 76 boreholes were
   undertaken to delineate the dynamic properties of shallow soil for
   construction NPP. The velocity of the P- and S-waves were acquired and
   interpreted using SeisImager Software Package, then the results were
   used to build a velocity-depth model to estimate the depth to the
   bedrock and the thicknesses of overburden layers. This model was
   verified using boreholes data dissected the seismic profiles to improve
   the final velocity depth model. The depth to bedrock was determined from
   both shallow seismic refraction profiles and boreholes. V(s)30, elastic
   moduli and dynamic geotechnical parameters were calculated and the site
   was classified as a National Earthquake Hazard Reduction Program (NEHRP)
   class ``B{''}. The values of seismic velocities, the engineering
   consolidations, and the strength parameters showed that the bedrock in
   the study area is characterized by more competent rock quality.},
DOI = {10.1016/j.soildyn.2017.05.006},
ISSN = {0267-7261},
EISSN = {1879-341X},
ResearcherID-Numbers = {Raef, Abdelmoneam/AAD-8617-2022
   Raef, Abdelmneam/AAH-3534-2021
   Abudeif, ِAbdelbaset Mohamed/M-1387-2017
   },
ORCID-Numbers = {Raef, Abdelmoneam/0000-0002-2629-9296
   Raef, Abdelmneam/0000-0002-2629-9296
   Abudeif, ِAbdelbaset Mohamed/0000-0002-7625-4512
   Farrag, Ahmed/0000-0002-1196-1718
   Mohammed, Mohammed/0000-0001-5703-0049},
Unique-ID = {WOS:000404707800012},
}

@article{ WOS:000406242800009,
Author = {Li, Menghan and Pei, Dan and Zhang, Xiaoping and Zhang, Beichuan and Xu,
   Ke},
Title = {Interest-suppression-based NDN live video broadcasting over wireless LAN},
Journal = {FRONTIERS OF COMPUTER SCIENCE},
Year = {2017},
Volume = {11},
Number = {4},
Pages = {675-687},
Month = {AUG},
Abstract = {Named data networking (NDN) is a new Internet architecture that replaces
   today's focus on where - addresses and hosts with what - the content
   that users and applications care about. One of NDN's prominent
   advantages is scalable and efficient content distribution due to its
   native support of caching and multicast in the network. However, at the
   last hop to wireless users, often the WiFi link, current NDN
   implementation still treats the communication as multiple unicast
   sessions, which will cause duplicate packets and waste of bandwidth when
   multiple users request for the same popular content. WiFi's built-in
   broadcast mechanism can alleviate this problem, but it suffers from
   packet loss since there is no MAC-layer acknowledgement as in unicast.
   In this paper, we develop a new NDN-based cross-layer approach called
   NLB for efficient and scalable live video streaming over wireless LAN.
   The core ideas are: using WiFi's broadcast channel to deliver content
   from the access point to the users, a leader-based mechanism to suppress
   duplicate requests from users, and receiver-driven rate control and loss
   recovery. The design is implemented and evaluated in a physical testbed
   comprised of one software AP and 20 Raspberry Pi-based WiFi clients.
   While NDN with multiple unicast sessions or plain broadcast can support
   no more than ten concurrent viewers of a 1Mbps streaming video, NDN plus
   NLB supports all 20 viewers, and Received December 29, 2015; accepted
   April 28, 2016 E-mail: zhxp@tsinghua.edu.cn can likely support much more
   when present.},
DOI = {10.1007/s11704-016-5563-x},
ISSN = {2095-2228},
EISSN = {2095-2236},
ORCID-Numbers = {Dan, Pei/0000-0002-5113-838X},
Unique-ID = {WOS:000406242800009},
}

@article{ WOS:000403326100001,
Author = {Hjorth Larsen, Ask and Mortensen, Jens Jorgen and Blomqvist, Jakob and
   Castelli, Ivano E. and Christensen, Rune and Dulak, Marcin and Friis,
   Jesper and Groves, Michael N. and Hammer, Bjork and Hargus, Cory and
   Hermes, Eric D. and Jennings, Paul C. and Jensen, Peter Bjerre and
   Kermode, James and Kitchin, John R. and Kolsbjerg, Esben Leonhard and
   Kubal, Joseph and Kaasbjerg, Kristen and Lysgaard, Steen and Maronsson,
   Jon Bergmann and Maxson, Tristan and Olsen, Thomas and Pastewka, Lars
   and Peterson, Andrew and Rostgaard, Carsten and Schiotz, Jakob and
   Schutt, Ole and Strange, Mikkel and Thygesen, Kristian S. and Vegge,
   Tejs and Vilhelmsen, Lasse and Walter, Michael and Zeng, Zhenhua and
   Jacobsen, Karsten W.},
Title = {The atomic simulation environment-a Python library for working with
   atoms},
Journal = {JOURNAL OF PHYSICS-CONDENSED MATTER},
Year = {2017},
Volume = {29},
Number = {27},
Month = {JUL 12},
Abstract = {The atomic simulation environment (ASE) is a software package written in
   the Python programming language with the aim of setting up, steering,
   and analyzing atomistic simulations. In ASE, tasks are fully scripted in
   Python. The powerful syntax of Python combined with the NumPy array
   library make it possible to perform very complex simulation tasks. For
   example, a sequence of calculations may be performed with the use of a
   simple `for-loop' construction. Calculations of energy, forces, stresses
   and other quantities are performed through interfaces to many external
   electronic structure codes or force fields using a uniform interface. On
   top of this calculator interface, ASE provides modules for performing
   many standard simulation tasks such as structure optimization, molecular
   dynamics, handling of constraints and performing nudged elastic band
   calculations.},
DOI = {10.1088/1361-648X/aa680e},
Article-Number = {273002},
ISSN = {0953-8984},
EISSN = {1361-648X},
ResearcherID-Numbers = {Groves, Michael N/H-6210-2014
   Zeng, Zhenhua/E-1795-2012
   Kitchin, John Robert/A-2363-2010
   Olsen, Thomas/L-1416-2014
   Hammer, Bjørk/C-3701-2013
   Larsen, Ask Hjorth/I-6888-2013
   Thygesen, Kristian S/B-1062-2011
   Jacobsen, Karsten Wedel/B-3602-2009
   Pastewka, Lars/ABC-7596-2021
   Kaasbjerg, Kristen/G-9202-2015
   Vegge, Tejs/A-9419-2011
   Walter, Michael/D-7984-2011
   Kermode, James R/O-6631-2014
   Schiøtz, Jakob/A-5692-2011
   Castelli, Ivano/N-1627-2015
   C. Jennings, Paul/L-6429-2013
   },
ORCID-Numbers = {Groves, Michael N/0000-0002-1369-4596
   Zeng, Zhenhua/0000-0002-3087-8581
   Kitchin, John Robert/0000-0003-2625-9232
   Olsen, Thomas/0000-0001-6256-9284
   Hammer, Bjørk/0000-0002-7849-6347
   Larsen, Ask Hjorth/0000-0001-5267-6852
   Thygesen, Kristian S/0000-0001-5197-214X
   Jacobsen, Karsten Wedel/0000-0002-1121-2979
   Pastewka, Lars/0000-0001-8351-7336
   Kaasbjerg, Kristen/0000-0003-3477-2028
   Vegge, Tejs/0000-0002-1484-0284
   Walter, Michael/0000-0001-6679-2491
   Kermode, James R/0000-0001-6755-6271
   Schiøtz, Jakob/0000-0002-0670-8013
   Castelli, Ivano/0000-0001-5880-5045
   Lysgaard, Steen/0000-0002-2032-8949
   Maxson, Tristan/0000-0002-7668-8986
   Christensen, Rune/0000-0002-7803-7896
   C. Jennings, Paul/0000-0003-3673-0147
   Hermes, Eric/0000-0001-6699-7948},
Unique-ID = {WOS:000403326100001},
}

@article{ WOS:000404688700005,
Author = {Berglund, Nils and Kuehn, Christian},
Title = {Model Spaces of Regularity Structures for Space-Fractional SPDEs},
Journal = {JOURNAL OF STATISTICAL PHYSICS},
Year = {2017},
Volume = {168},
Number = {2},
Pages = {331-368},
Month = {JUL},
Abstract = {We study model spaces, in the sense of Hairer, for stochastic partial
   differential equations involving the fractional Laplacian. We prove that
   the fractional Laplacian is a singular kernel suitable to apply the
   theory of regularity structures. Our main contribution is to study the
   dependence of the model space for a regularity structure on the
   three-parameter problem involving the spatial dimension, the polynomial
   order of the nonlinearity, and the exponent of the fractional Laplacian.
   The goal is to investigate the growth of the model space under parameter
   variation. In particular, we prove several results in the approaching
   subcriticality limit leading to universal growth exponents of the
   regularity structure. A key role is played by the viewpoint that model
   spaces can be identified with families of rooted trees. Our proofs are
   based upon a geometrical construction similar to Newton polygons for
   classical Taylor series and various combinatorial arguments. We also
   present several explicit examples listing all elements with negative
   homogeneity by implementing a new symbolic software package to work with
   regularity structures. We use this package to illustrate our analytical
   results and to obtain new conjectures regarding coarse-grained network
   measures for model spaces.},
DOI = {10.1007/s10955-017-1801-3},
ISSN = {0022-4715},
EISSN = {1572-9613},
ResearcherID-Numbers = {Kuehn, Christian/H-3950-2014
   },
ORCID-Numbers = {Kuehn, Christian/0000-0002-7063-6173
   Berglund, Nils/0000-0001-7044-7885},
Unique-ID = {WOS:000404688700005},
}

@article{ WOS:000407305000008,
Author = {Pakpahan, Andrew Fernando and Hwang, I-Shyan and Yu, Yu-Ming and Hsu,
   Wu-Hsiao and Liem, Andrew Tanny and Nikoukar, AliAkbar},
Title = {Novel elastic protection against DDF failures in an enhanced
   software-defined SIEPON},
Journal = {OPTICAL FIBER TECHNOLOGY},
Year = {2017},
Volume = {36},
Pages = {51-62},
Month = {JUL},
Abstract = {Ever-increasing bandwidth demands on passive optical networks (PONs) are
   pushing the utilization of every fiber strand to its limit. This is
   mandating comprehensive protection until the end of the distribution
   drop fiber (DDF). Hence, it is important to provide refined protection
   with an advanced fault-protection architecture and recovery mechanism
   that is able to cope with various DDF failures. We propose a novel
   elastic protection against DDF failures that incorporates a
   software-defined networking (SDN) capability and a bus protection line
   to enhance the resiliency of the existing Service Interoperability in
   Ethernet Passive Optical Networks (SIEPON) system. We propose the
   addition of an integrated SDN controller and flow tables to the optical
   line terminal and optical network units (ONUs) in order to deliver
   various DDF protection scenarios. The proposed architecture enables
   flexible assignment of backup ONU(s) in pre/post-fault conditions
   depending on the PON traffic load. A transient backup ONU and multiple
   backup ONUs can be deployed in the pre-fault and post-fault scenarios,
   respectively. Our extensively discussed simulation results show that our
   proposed architecture provides better overall throughput and drop
   probability compared to the architecture with a fixed DDF protection
   mechanism. It does so while still maintaining overall QoS performance in
   terms of packet delay, mean jitter, packet loss, and throughput under
   various fault conditions. (C) 2017 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.yofte.2017.02.007},
ISSN = {1068-5200},
EISSN = {1095-9912},
ResearcherID-Numbers = {Nikoukar, AliAkbar/AAP-6096-2020
   Liem, Andrew Tanny Tanny/A-3204-2016},
ORCID-Numbers = {Liem, Andrew Tanny Tanny/0000-0002-7167-573X},
Unique-ID = {WOS:000407305000008},
}

@article{ WOS:000405535600010,
Author = {Parrish, Robert M. and Burns, Lori A. and Smith, Daniel G. A. and
   Simmonett, Andrew C. and DePrince, III, A. Eugene and Hohenstein, Edward
   G. and Bozkaya, Ugur and Sokolov, Alexander Yu. and Di Remigio, Roberto
   and Richard, Ryan M. and Gonthier, Jerome F. and James, Andrew M. and
   McAlexander, Harley R. and Kumar, Ashutosh and Saitow, Masaaki and Wang,
   Xiao and Pritchard, Benjamin P. and Prakash, Verma and Schaefer, III,
   Henry F. and Patkowski, Konrad and King, Rollin A. and Valeev, Edward F.
   and Evangelista, Francesco A. and Turney, Justin M. and Crawford, T.
   Daniel and Sherrill, C. David},
Title = {PSI4 1.1: An Open-Source Electronic Structure Program Emphasizing
   Automation, Advanced Libraries, and Interoperability},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Year = {2017},
Volume = {13},
Number = {7},
Pages = {3185-3197},
Month = {JUL},
Abstract = {PSI4 is an ab initio electronic structure program providing methods such
   as Hartree Fock, density functional theory, configuration interaction,
   and coupled-cluster theory. The 1.1 release represents a major update
   meant to automate complex tasks, such as geometry optimization using
   complete basis-set extrapolation or focal-point methods. Conversion of
   the top-level code to a Python module means that PRI-can now be used in
   complex workflows alongside other Python tools. Several new features
   have been added with the aid of libraries providing easy access to
   techniques such as density fitting, Cholesky decomposition, and Laplace
   denominators. The build system has been completely rewritten to simplify
   interoperability with independent, reusable software components for
   quantum chemistry. Finally, a wide range of new theoretical methods and
   analyses have been added to the code base, including functional-group
   and open-shell symmetry adapted perturbation theory, density-fitted
   coupled cluster with frozen natural orbitals, orbital-optimized
   perturbation and coupled-cluster methods (e.g., OO-MP2 and OO-LCCD),
   density-fitted multiconfigurational self-consistent field, density
   cumulant functional theory, algebraic-diagrammatic construction excited
   states, improvements to the geometry optimizer, and the ``X2C{''}
   approach to relativistic corrections, among many other improvements.},
DOI = {10.1021/acs.jctc.7b00174},
ISSN = {1549-9618},
EISSN = {1549-9626},
ResearcherID-Numbers = {Turney, Justin M/G-5390-2014
   Valeyev, Eduard/AGI-7127-2022
   Di Remigio, Roberto/I-8811-2019
   Bozkaya, Ugur/A-4065-2016
   Kumar, Ashutosh/AAH-1711-2019
   Wang, Xiao/G-7591-2019
   Evangelista, Francesco/DUX-3610-2022
   Valeyev, Eduard F/A-5313-2009
   Crawford, Thomas/A-9271-2017
   Sokolov, Alexander/E-2869-2014},
ORCID-Numbers = {Turney, Justin M/0000-0003-3659-0711
   Valeyev, Eduard/0000-0001-9923-6256
   Di Remigio, Roberto/0000-0002-5452-9239
   Bozkaya, Ugur/0000-0002-5203-2210
   Kumar, Ashutosh/0000-0001-7589-6030
   Wang, Xiao/0000-0003-1402-7522
   Evangelista, Francesco/0000-0002-7917-6652
   Valeyev, Eduard F/0000-0001-9923-6256
   Crawford, Thomas/0000-0002-7961-7016
   Richard, Ryan/0000-0003-4235-5179
   DePrince, Eugene/0000-0003-1061-2521
   Patkowski, Konrad/0000-0002-4468-207X
   Burns, Lori/0000-0003-2852-5864
   Saitow, Masaaki/0000-0002-1977-7376
   Simmonett, Andrew/0000-0002-5921-9272
   Sokolov, Alexander/0000-0003-2637-4134},
Unique-ID = {WOS:000405535600010},
}

@article{ WOS:000405829300001,
Author = {Taghiyar, M. Jafar and Rosner, Jamie and Grewal, Diljot and Grande,
   Bruno M. and Aniba, Radhouane and Grewal, Jasleen and Boutros, Paul C.
   and Morin, Ryan D. and Bashashati, Ali and Shah, Sohrab P.},
Title = {Kronos: a workflow assembler for genome analytics and informatics},
Journal = {GIGASCIENCE},
Year = {2017},
Volume = {6},
Number = {7},
Month = {JUN 26},
Abstract = {Background: The field of next generation sequencing informatics has
   matured to a point where algorithmic advances in sequence alignment and
   individual feature detection methods have stabilized. Practical and
   robust implementation of complex analytical workflows (where such tools
   are structured into `best practices' for automated analysis of NGS
   datasets) still requires significant programming investment and
   expertise.
   Results: We present Kronos, a software platform for facilitating the
   development and execution of modular, auditable and distributable
   bioinformatics workflows. Kronos obviates the need for explicit coding
   of workflows by compiling a text configuration file into executable
   Python applications. Making analysis modules would still require
   programming The framework of each workflow includes a run manager to
   execute the encoded workflows locally (or on a cluster or cloud),
   parallelize tasks, and log all runtime events. Resulting workflows are
   highly modular and configurable by construction, facilitating flexible
   and extensible meta-applications which can be modified easily through
   configuration file editing. The workflows are fully encoded for ease of
   distribution and can be instantiated on external systems, a step towards
   reproducible research and comparative analyses. We introduce a framework
   for building Kronos components which function as shareable, modular
   nodes in Kronos workflows.
   Conclusion: The Kronos platform provides a standard framework for
   developers to implement custom tools, reuse existing tools, and
   contribute to the community at large. Kronos is shipped with both Docker
   and Amazon AWS machine images. It is free, open source and available
   through PyPI (Python Package Index) and https : //github.
   com/jtaghiyar/kronos.},
DOI = {10.1093/gigascience/gix042},
ISSN = {2047-217X},
ResearcherID-Numbers = {Morin, Ryan/AAE-7824-2019
   },
ORCID-Numbers = {Morin, Ryan/0000-0003-2932-7800
   Grande, Bruno/0000-0002-4621-1589
   Boutros, Paul/0000-0003-0553-7520},
Unique-ID = {WOS:000405829300001},
}

@article{ WOS:000401535600023,
Author = {Churchill, Berkeley and Sharma, Rahul and Bastien, J. F. and Aiken, Alex},
Title = {Sound Loop Superoptimization for Google Native Client},
Journal = {OPERATING SYSTEMS REVIEW},
Year = {2017},
Volume = {51},
Number = {2},
Pages = {313-326},
Month = {JUN},
Note = {22nd ACM International Conference on Architectural Support for
   Programming Languages and Operating Systems (ASPLOS), Xian, PEOPLES R
   CHINA, APR 08-12, 2017},
Organization = {Assoc Comp Machinery; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN; ACM SIGBED;
   IEEE Comp Soc; CAS, ICT, State Key Lab Comp Architecture; Xian Jiaotong
   Univ},
Abstract = {Software fault isolation (SFI) is an important technique for the
   construction of secure operating systems, web browsers, and other
   extensible software. We demonstrate that superoptimization can
   dramatically improve the performance of Google Native Client, a SFI
   system that ships inside the Google Chrome Browser. Key to our results
   are new techniques for superoptimization of loops: we propose a new
   architecture for superoptimization tools that incorporates both a fully
   sound verification technique to ensure correctness and a bounded
   verification technique to guide the search to optimized code. In our
   evaluation we optimize 13 libc string functions, formally verify the
   correctness of the optimizations and report a median and average speedup
   of 25\% over the libraries shipped by Google.},
DOI = {10.1145/3093315.3037754},
ISSN = {0163-5980},
EISSN = {1943-586X},
Unique-ID = {WOS:000401535600023},
}

@article{ WOS:000399851300007,
Author = {Jarema, D. and Bungartz, H. J. and Goerler, T. and Jenko, F. and Neckel,
   T. and Told, D.},
Title = {Block-structured grids in full velocity space for Eulerian gyrokinetic
   simulations},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2017},
Volume = {215},
Pages = {49-62},
Month = {JUN},
Abstract = {Global, i.e., full-torus, gyrokinetic simulations play an important role
   in exploring plasma microturbulence in magnetic fusion devices with
   strong radial variations. In the presence of steep temperature profiles,
   grid-based Eulerian approaches can become quite challenging as the
   correspondingly varying velocity space structures need to be
   accommodated and sufficiently resolved. A rigid velocity space grid then
   requires a very high number of discretization nodes resulting in
   enormous computational costs. To tackle this issue and reduce the
   computational demands, we introduce block-structured grids in the all
   velocity space dimensions. The construction of these grids is based on a
   general approach, making them suitable for various Eulerian
   implementations. In the current study, we explain the rationale behind
   the presented approach, detail the implementation, and provide
   simulation results obtained with the block-structured grids. The
   achieved reduction in the number of computational nodes depends on the
   temperature profile and simulation scenario provided. In the test cases
   at hand, about ten times fewer grid points are required for nonlinear
   simulations performed with block-structured grids in the plasma
   turbulence code GENE (http://genecode.org). With the speed-up found to
   scale almost exactly reciprocal to the number of grid points, the new
   implementation greatly reduces the computational costs and therefore
   opens new possibilities for applications of this software package. (C)
   2017 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.cpc.2017.02.005},
ISSN = {0010-4655},
EISSN = {1879-2944},
ResearcherID-Numbers = {Told, Daniel/AAV-4588-2020
   },
ORCID-Numbers = {Told, Daniel/0000-0001-9810-6724
   Gorler, Tobias/0000-0002-0851-6699
   Jarema, Denis/0000-0003-1800-3245
   Neckel, Tobias/0000-0002-3442-7171},
Unique-ID = {WOS:000399851300007},
}

@article{ WOS:000429602900004,
Author = {Bilal, Muhammad and Oyedele, Lukumon O. and Munir, Kamran and Ajayi,
   Saheed O. and Akinade, Olugbenga O. and Owolabi, Hakeem A. and Alaka,
   Hafiz A.},
Title = {The application of web of data technologies in building materials
   information modelling for construction waste analytics},
Journal = {SUSTAINABLE MATERIALS AND TECHNOLOGIES},
Year = {2017},
Volume = {11},
Pages = {28-37},
Month = {APR},
Abstract = {Predicting and designing out construction waste in real time is complex
   during building waste analysis (BWA) since it involves a large number of
   analyses for investigating multiple waste-efficient design strategies.
   These analyses require highly specific data of materials that are
   scattered across different data sources. A repository that facilitates
   applications in gaining seamless access to relatively large and
   distributed data sources of building materials is currently unavailable
   for conducting the BWA. Such a repository is the first step to
   developing a simulation tool for the BWA. Existing product data exchange
   ontologies and classification systems lack adequate modelling of
   building materials for the BWA. In this paper, we propose a highly
   resilient and data-agnostic building materials database. We use
   ontologies at the core of our approach to capture highly accurate and
   semantically conflicting data of building materials using the Resource
   Description Framework (RDF) and Web Ontology Language (OWL). Owing to
   the inherent capabilities of RDF, the architecture provides syntactical
   homogeneity while accessing the diverse and distributed data of building
   materials during the BWA. We use software packages such as Protege and
   Oracle RDF Graph database for implementing the proposed architecture.
   Our research provides technical details and insights for researchers and
   software engineers who are seeking to develop the semantic repositories
   of similar kind of simulation applications that can be used for building
   waste performance analysis. (c) 2017 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.susmat.2016.12.004},
ISSN = {2214-9937},
ResearcherID-Numbers = {Ajayi, Saheed/AAV-4209-2021
   Ajayi, Saheed/AHC-7157-2022
   },
ORCID-Numbers = {/0000-0001-6630-0717
   Akinade, Olugbenga/0000-0003-3950-3775},
Unique-ID = {WOS:000429602900004},
}

@article{ WOS:000408313700023,
Author = {Churchill, Berkeley and Sharma, Rahul and Bastien, J. F. and Aiken, Alex},
Title = {Sound Loop Superoptimization for Google Native Client},
Journal = {ACM SIGPLAN NOTICES},
Year = {2017},
Volume = {52},
Number = {4},
Pages = {313-326},
Month = {APR},
Note = {22nd ACM International Conference on Architectural Support for
   Programming Languages and Operating Systems (ASPLOS), Xian, PEOPLES R
   CHINA, APR 08-12, 2017},
Organization = {ACM; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN},
Abstract = {Software fault isolation (SFI) is an important technique for the
   construction of secure operating systems, web browsers, and other
   extensible software. We demonstrate that super-optimization can
   dramatically improve the performance of Google Native Client, a SFI
   system that ships inside the Google Chrome Browser. Key to our results
   are new techniques for superoptimization of loops: we propose a new
   architecture for superoptimization tools that incorporates both a fully
   sound verification technique to ensure correctness and a bounded
   verification technique to guide the search to optimized code. In our
   evaluation we optimize 13 libc string functions, formally verify the
   correctness of the optimizations and report a median and average speedup
   of 25\% over the libraries shipped by Google.},
DOI = {10.1145/3093336.3037754},
ISSN = {0362-1340},
EISSN = {1558-1160},
Unique-ID = {WOS:000408313700023},
}

@article{ WOS:000400822900261,
Author = {Lin, Zhaowen and Tao, Dan and Wang, Zhenji},
Title = {Dynamic Construction Scheme for Virtualization Security Service in
   Software-Defined Networks},
Journal = {SENSORS},
Year = {2017},
Volume = {17},
Number = {4},
Month = {APR},
Abstract = {For a Software Defined Network (SDN), security is an important factor
   affecting its large-scale deployment. The existing security solutions
   for SDN mainly focus on the controller itself, which has to handle all
   the security protection tasks by using the programmability of the
   network. This will undoubtedly involve a heavy burden for the
   controller. More devastatingly, once the controller itself is attacked,
   the entire network will be paralyzed. Motivated by this, this paper
   proposes a novel security protection architecture for SDN. We design a
   security service orchestration center in the control plane of SDN, and
   this center physically decouples from the SDN controller and constructs
   SDN security services. We adopt virtualization technology to construct a
   security meta-function library, and propose a dynamic security service
   composition construction algorithm based on web service composition
   technology. The rule-combining method is used to combine security
   meta-functions to construct security services which meet the
   requirements of users. Moreover, the RETE algorithm is introduced to
   improve the efficiency of the rule-combining method. We evaluate our
   solutions in a realistic scenario based on OpenStack. Substantial
   experimental results demonstrate the effectiveness of our solutions that
   contribute to achieve the effective security protection with a small
   burden of the SDN controller.},
DOI = {10.3390/s17040920},
Article-Number = {920},
EISSN = {1424-8220},
Unique-ID = {WOS:000400822900261},
}

@article{ WOS:000399094700020,
Author = {Yu, Yang and Liang, Mangui and Wang, Zhe},
Title = {A source-controlled data center network model},
Journal = {PLOS ONE},
Year = {2017},
Volume = {12},
Number = {3},
Month = {MAR 22},
Abstract = {The construction of data center network by applying SDN technology has
   become a hot research topic. The SDN architecture has innovatively
   separated the control plane from the data plane which makes the network
   more software-oriented and agile. Moreover, it provides virtual
   multi-tenancy, effective scheduling resources and centralized control
   strategies to meet the demand for cloud computing data center. However,
   the explosion of network information is facing severe challenges for SDN
   controller. The flow storage and lookup mechanisms based on TCAM device
   have led to the restriction of scalability, high cost and energy
   consumption. In view of this, a source-controlled data center network
   (SCDCN) model is proposed herein. The SCDCN model applies a new type of
   source routing address named the vector address (VA) as the
   packet-switching label. The VA completely defines the communication path
   and the data forwarding process can be finished solely relying on VA.
   There are four advantages in the SCDCN architecture. 1) The model adopts
   hierarchical multi-controllers and abstracts large-scale data center
   network into some small network domains that has solved the restriction
   for the processing ability of single controller and reduced the
   computational complexity. 2) Vector switches (VS) developed in the core
   network no longer apply TCAM for table storage and lookup that has
   significantly cut down the cost and complexity for switches. Meanwhile,
   the problem of scalability can be solved effectively. 3) The SCDCN model
   simplifies the establishment process for new flows and there is no need
   to download flow tables to VS. The amount of control signaling consumed
   when establishing new flows can be significantly decreased. 4) We design
   the VS on the NetFPGA platform. The statistical results show that the
   hardware resource consumption in a VS is about 27\% of that in an OFS.},
DOI = {10.1371/journal.pone.0173442},
Article-Number = {e0173442},
ISSN = {1932-6203},
Unique-ID = {WOS:000399094700020},
}

@article{ WOS:000394682400019,
Author = {Springate, David A. and Parisi, Rosa and Olier, Ivan and Reeves, David
   and Kontopantelis, Evangelos},
Title = {rEHR: An R package for manipulating and analysing Electronic Health
   Record data},
Journal = {PLOS ONE},
Year = {2017},
Volume = {12},
Number = {2},
Month = {FEB 23},
Abstract = {Research with structured Electronic Health Records (EHRs) is expanding
   as data becomes more accessible; analytic methods advance; and the
   scientific validity of such studies is increasingly accepted. However,
   data science methodology to enable the rapid searching/extraction,
   cleaning and analysis of these large, often complex, datasets is less
   well developed. In addition, commonly used software is inadequate,
   resulting in bottlenecks in research workflows and in obstacles to
   increased transparency and reproducibility of the research. Preparing a
   research-ready dataset from EHRs is a complex and time consuming task
   requiring substantial data science skills, even for simple designs. In
   addition, certain aspects of the workflow are computationally intensive,
   for example extraction of longitudinal data and matching controls to a
   large cohort, which may take days or even weeks to run using standard
   software. The rEHR package simplifies and accelerates the process of
   extracting ready-for-analysis datasets from EHR databases. It has a
   simple import function to a database backend that greatly accelerates
   data access times. A set of generic query functions allow users to
   extract data efficiently without needing detailed knowledge of SQL
   queries. Longitudinal data extractions can also be made in a single
   command, making use of parallel processing. The package also contains
   functions for cutting data by time-varying covariates, matching controls
   to cases, unit conversion and construction of clinical code lists. There
   are also functions to synthesise dummy EHR. The package has been tested
   with one for the largest primary care EHRs, the Clinical Practice
   Research Datalink (CPRD), but allows for a common interface to other
   EHRs. This simplified and accelerated work flow for EHR data extraction
   results in simpler, cleaner scripts that are more easily debugged,
   shared and reproduced.},
DOI = {10.1371/journal.pone.0171784},
Article-Number = {e0171784},
ISSN = {1932-6203},
ResearcherID-Numbers = {Kontopantelis, Evangelos/H-2966-2019
   Reeves, David/H-6203-2014},
ORCID-Numbers = {Kontopantelis, Evangelos/0000-0001-6450-5815
   Olier, Ivan/0000-0002-5679-7501
   Parisi, Rosa/0000-0002-0968-9153
   Reeves, David/0000-0001-6377-6859},
Unique-ID = {WOS:000394682400019},
}

@article{ WOS:000393930200014,
Author = {Tuysuz, Mehmet Fatih and Ankarali, Zekiye Kubra and Gozupek, Didem},
Title = {A survey on energy efficiency in software defined networks},
Journal = {COMPUTER NETWORKS},
Year = {2017},
Volume = {113},
Pages = {188-204},
Month = {FEB 11},
Abstract = {Wide deployment and dense usage of computer networks may cause excessive
   energy consumption due to the increase in probability of network
   congestion, frame collisions and packet dropping rates resulting from
   late-received frames. Besides, based on the dramatic increase in network
   complexity with wireless, mobile and split tunnel connections, weak
   visibility into network flows and high cost of some of public and
   private network services, current networks can also be implied as
   inefficient in terms of both performance and economy. Software-Defined
   Networking (SDN) is a novel networking architecture, which provides a
   directly programmable and (logically) centralized network control,
   separates network control from forwarding, and enables programmable
   network components. SDN can have a significant role in reducing the
   aforementioned excessive energy consumption caused by data centers,
   network components, and end hosts. In this paper,1 we examine the
   principles, benefits, and drawbacks of up-to-date SDN approaches that
   focus on energy efficiency. We also provide a brief comparison of
   possible energy gain ratios of existing approaches, discussion on open
   issues and a guideline for future research. (C) 2016 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.comnet.2016.12.012},
ISSN = {1389-1286},
EISSN = {1872-7069},
ResearcherID-Numbers = {Gözüpek, Didem/AAG-5946-2019
   Tuysuz, Mehmet Fatih/AAG-6097-2019},
ORCID-Numbers = {Gözüpek, Didem/0000-0001-8450-1897
   Tuysuz, Mehmet Fatih/0000-0002-8955-9710},
Unique-ID = {WOS:000393930200014},
}

@article{ WOS:000395889500014,
Author = {Mayoral, Arturo and Vilalta, Ricard and Munoz, Raul and Casellas, Ramon
   and Martinez, Ricardo and Moreolo, Michela Svaluto and Fabrega, Josep M.
   and Aguado, Alejandro and Yan, Shuangyi and Simeonidou, Dimitra and
   Gran, Jose M. and Lopez, Victor and Kaczmarek, Pavel and Szwedowski,
   Rafal and Szyrkowiec, Thomas and Autenrieth, Achim and Yoshikane,
   Norboru and Tsuritani, Takehiro and Morita, Itsuro and Shiraiwa, Masaki
   and Wada, Naoya and Tanaka, Nishihara T. and Takahara, Tomoo and
   Rasmussen, Jens C. and Yoshida, Yuki and Kitayama, Ken-Ichi},
Title = {Control Orchestration Protocol: Unified Transport API for Distributed
   Cloud and Network Orchestration},
Journal = {JOURNAL OF OPTICAL COMMUNICATIONS AND NETWORKING},
Year = {2017},
Volume = {9},
Number = {2, SI},
Pages = {A216-A222},
Month = {FEB},
Abstract = {In the context of the fifth generation of mobile technology (5G),
   multiple technologies will converge into a unified end-to-end system.
   For this purpose, software defined networking (SDN) is proposed, as the
   control paradigm will integrate all network segments and heterogeneous
   optical and wireless network technologies together with massive storage
   and computing infrastructures. The control orchestration protocol is
   presented as a unified transport application programming interface
   solution for joint cloud/network orchestration, allowing interworking of
   heterogeneous control planes to provide provisioning and recovery of
   quality of service (QoS)-aware end-to-end services. End-to-end QoS is
   guaranteed by provisioning and restoration schemes, which are proposed
   for optical circuit/packet switching restoration bymeans of
   signalmonitoring and adaptivemodulation and adaptive route control,
   respectively. The proposed solution is experimentally demonstrated in an
   international multi-partner test bed, which consists of a multi-domain
   transport network comprising optical circuit switching and optical
   packet switching domains controlled by SDN/OpenFlow and Generalized
   Multiprotocol Label Switching (GMPLS) control planes and a distributed
   cloud infrastructure. The results show the dynamic provisioning of IT
   and network resources and recovery capabilities of the architecture.},
DOI = {10.1364/JOCN.9.00A216},
ISSN = {1943-0620},
EISSN = {1943-0639},
ResearcherID-Numbers = {Vilalta, Ricard/Q-4438-2018
   Munoz, Raul/AAR-1565-2021
   Fabrega, Josep M./AAT-1391-2021
   Casellas, Ramon/AAQ-7278-2021
   Svaluto Moreolo, Michela/ABF-8540-2021
   Yan, Shuangyi/H-7233-2019
   Morita, Itsuro/AAC-8648-2020
   },
ORCID-Numbers = {Vilalta, Ricard/0000-0003-0391-9728
   Munoz, Raul/0000-0003-4651-4499
   Casellas, Ramon/0000-0002-2663-6571
   Yan, Shuangyi/0000-0002-5021-2840
   Morita, Itsuro/0000-0001-9919-245X
   Martinez, Ricardo/0000-0003-3097-5485
   Mayoral Lopez de Lerma, Arturo/0000-0002-5837-8685},
Unique-ID = {WOS:000395889500014},
}

@article{ WOS:000395889500020,
Author = {Yan, Shuangyi and Aguado, Alejandro and Ou, Yanni and Wang, Rui and
   Nejabati, Reza and Simeonidou, Dimitra},
Title = {Multilayer Network Analytics With SDN-Based Monitoring Framework},
Journal = {JOURNAL OF OPTICAL COMMUNICATIONS AND NETWORKING},
Year = {2017},
Volume = {9},
Number = {2, SI},
Pages = {A271-A279},
Month = {FEB},
Abstract = {Optical transport networks with an expanding variety and volume of
   network data services challenge network providers' ability to provide
   high-quality service assurance and network management. Software-defined
   networks (SDNs) decouple the data plane and control plane and enable
   network programmability in optical networks with a centralized network
   controller. The optical network then becomes more dynamic in network
   architecture and reconfigures frequently. Dynamic optical networks
   require all kinds of visibility into application data types, traffic
   flows, and end-to-end connections. Thus, we propose an SDN-based
   monitoring framework that expands network analytics to a converged
   packet and optical network. With a designated monitoring hub, monitoring
   information from multiple layers are collected and processed in a
   centralized server. Several monitoring technologies are provided as
   network services with the architecture-on-demand optical node
   architecture. The developed network applications on top of the SDN
   controller process all the collected monitoring information and enable
   multilayer network analytics based on the SDN-based monitoring
   framework. Several use cases demonstrated successfully that the
   developed multilayer network analytics provides powerful tools for
   network replanning and optimization. The multilayer network analytics
   enables quality of service recovery to avoid network disruption, optical
   power equalization at any combining device, and network debugging and
   restoration in optical networks.},
DOI = {10.1364/JOCN.9.00A271},
ISSN = {1943-0620},
EISSN = {1943-0639},
ResearcherID-Numbers = {Yan, Shuangyi/H-7233-2019
   },
ORCID-Numbers = {Yan, Shuangyi/0000-0002-5021-2840
   Nejabati, Reza/0000-0003-4664-9369},
Unique-ID = {WOS:000395889500020},
}

@inproceedings{ WOS:000455029500121,
Author = {Alhaqbani, Mohammed and Liu, Hang},
Editor = {Arabnia, HR and Deligiannidis, L and Tinetti, FG and Tran, QN and Yang, MQ},
Title = {Conceptual Mechanism Software Defined Network Topology in Multiprotocol
   Label Switching Network Domain},
Booktitle = {PROCEEDINGS 2017 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND
   COMPUTATIONAL INTELLIGENCE (CSCI)},
Year = {2017},
Pages = {680-684},
Note = {International Conference on Computational Science and Computational
   Intelligence (CSCI), Las Vegas, NV, DEC 14-16, 2017},
Organization = {American Council Sci \& Educ},
Abstract = {Multiprotocol Label Switching (MPLS) offers a data-transportation
   mechanism with a wide-range of alternatives towards the varying,
   complicated networking procedures and serves as a reliable broad-band
   technique. While it enhanced the performance of packet forwarding by the
   use of fast label-switching, however; among the issues that come with
   the inherited connection-oriented architecture of MPLS are reliability
   and fault-tolerance. Failure management is one of the fundamental
   instruments that allow network operators to provide communication
   services that are much more reliable than the individual network
   components (nodes and links). In this study, we present a
   multi-objective, fault tolerant model for MPLS network design problems
   utilizing a Software Defined Networks (SDN) paradigm by introducing a
   Bicriteria Direct Algorithm known as Maximum Flow under Minimum Cost
   algorithm model. This paradigm uses the OpenFlow control management
   plane to design a survivable model in MPLS that will reroute traffic
   along optimal paths in the case of link failures. This implementation
   maintains and ensures a continuous network functionality that aims to
   resolve the shortcomings of the typical inherited connection-oriented
   design of MPLS networks. This approach achieves faster recovery time
   compared to traditional MPLS networks, while attaining optimal Fast
   Re-Routing technique following link or node failure scenarios.},
DOI = {10.1109/CSCI.2017.117},
ISBN = {978-1-5386-2652-8},
Unique-ID = {WOS:000455029500121},
}

@inproceedings{ WOS:000407041100020,
Author = {Allenet, T. and Geoffray, F. and Bucci, D. and Guillerme, L. and Canto,
   F. and Bouchard, A. and Broquin, J-E.},
Editor = {GarciaBlanco, SM and Conti, GN},
Title = {Optofluidic sensor engineering towards plutonium concentration
   measurements},
Booktitle = {INTEGRATED OPTICS: DEVICES, MATERIALS, AND TECHNOLOGIES XXI},
Series = {Proceedings of SPIE},
Year = {2017},
Volume = {10106},
Note = {Conference on Integrated Optics - Devices, Materials, and Technologies
   XXI, San Francisco, CA, JAN 30-FEB 01, 2017},
Organization = {SPIE},
Abstract = {Research in nuclear safety and fuel reprocessing has led to a surging
   need for novel chemical analysis tools with reduced analyte and effluent
   volumes. Recent technological advances for the elaboration and packaging
   of glass optofluidic co-integrated sensors have opened up the way for
   said analysis in harsh environments. We discuss a sensor engineering
   approach for the construction of an integrated absorption spectrometer
   with an ion-exchange core.
   Pu(VI) oxidation state exhibits a major absorption peak at a wavelength
   of 831 nm with a molar absorption coefficient of 545 L.mol(-1). cm(-1).
   An evanescent waveguiding sensing structure that allows guided
   fluid/light interaction is investigated in order to provide absorption
   spectroscopy measurements. The work presented consists of optical
   simulations as well as experimental measurements. Waveguide engineering
   with respects to modal transmission, field/fluid interaction coefficient
   Gamma and device losses is presented. The simulations are carried out by
   computing ion-exchanged waveguide refractive index distribution and
   using it in mode solver software. Device optical characterization and
   bench tests are carried out to verify approach viability. First device
   measurements of a neodymium absorption peak in nuclear manipulation
   conditions are displayed.},
DOI = {10.1117/12.2252190},
Article-Number = {UNSP 101060U},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-0653-1; 978-1-5106-0654-8},
ResearcherID-Numbers = {Broquin, Jean-Emmanuel/AAO-6034-2021
   },
ORCID-Numbers = {Bucci, Davide/0000-0003-0666-8397
   broquin, jean-emmanuel/0000-0001-5314-8699},
Unique-ID = {WOS:000407041100020},
}

@inproceedings{ WOS:000468478800029,
Author = {Barn, Balbir and Barat, Souvik and Clark, Tony},
Editor = {Gorthi, RP and Sarkar, S and Medvidovic, N and Kulkarni, V and Kumar, A and Joshi, P and Inverardi, P and Sureka, A and Sharma, R},
Title = {Conducting Systematic Literature Reviews and Systematic Mapping Studies},
Booktitle = {PROCEEDINGS OF THE 10TH INNOVATIONS IN SOFTWARE ENGINEERING CONFERENCE},
Year = {2017},
Pages = {212-213},
Note = {10th Innovations in Software Engineering Conference (ISEC), LNMIIT,
   Jaipur, INDIA, FEB 05-07, 2017},
Abstract = {Context: An essential part of conducting software engineering (SE)
   research is the ability to identify extant research on tools,
   technologies, concepts and methods in order to evaluate and make
   rational and scientific decisions. The domain from which such knowledge
   is extracted is typically existing research literature found in
   journals, conference proceedings, books and gray literature. Empirical
   approaches that include various systematic review (SR) methodologies
   such as systematic literature review (SLR) and systematic mapping study
   (SMS) are found to be effective in this context. They adopt rigorous
   planning, follow repeatable and well-defined processes, and produce
   unbiased and evidence-based outcomes. Despite these significant
   benefits, the general trend on using these systematic review (SR)
   methodologies is not encouraging in SE research. The primary reasons
   emerging are twofold - a) SR methodologies are largely cited as
   time-consuming activities and b) lack of guidance to conduct systematic
   reviews. This tutorial discusses these concerns and describes an
   effective way of using SR methodologies for SE research.
   Objectives: Attendees will be introduced to the key concepts, methods
   and processes for conducting systematic literature reviews (SLR) and
   systematic mapping studies (SMS). The benefits, limitations, guidelines
   for using SR methodologies in an effective manner will discussed in the
   session. Attendees will be guided on the appropriate formulation of a
   research question and sub questions; the development of a review
   protocol such as inclusion criteria, exclusion criteria, quality
   criteria and classification structures; and execution of review protocol
   using digital libraries and syntheses of review data. A web based
   software tool 1, for supporting the systematic literature review process
   will be demonstrated and attendees will get the opportunity to use the
   tool to conduct the review to help in identification of relevant
   research and extraction and synthesis of data.
   Method: We will use a blend of information presentation, interactive
   hands-on session and knowledge sharing session. The presentation will
   introduce the key concepts, benefits, limitations and how to overcome
   the limitations; hands on session will illustrate a review process with
   a case study, and finally the knowledge sharing session will discuss the
   experiences, best practices and the lesson learnt.},
DOI = {10.1145/3021460.3021489},
ISBN = {978-1-4503-4856-0},
ORCID-Numbers = {clark, tony/0000-0003-3167-0739},
Unique-ID = {WOS:000468478800029},
}

@inproceedings{ WOS:000430359100010,
Author = {Boecher, Nils},
Editor = {Enescu, AA and Rausch, A},
Title = {Refurbishment of Automotive Electronic Components regarding Update
   Capability of Applications},
Booktitle = {NINTH INTERNATIONAL CONFERENCE ON ADAPTIVE AND SELF-ADAPTIVE SYSTEMS AND
   APPLICATIONS (ADAPTIVE 2017)},
Year = {2017},
Pages = {68-69},
Note = {9th International Conference on Adaptive and Self-Adaptive Systems and
   Applications (ADAPTIVE), Athens, GREECE, FEB 19-23, 2017},
Organization = {IARIA},
Abstract = {In the desktop multimedia area, modern update techniques are well known
   and firmly integrated. Fixed update management systems supply nearly
   automatically the latest software applications. In addition, the system
   based hardware drivers are also updated until the base system becomes
   obsolete and no longer supported. Additionally, functionality for data
   recovery, system and user settings are also given. In the automotive
   area, the application is based on platform package integrations and
   delivering states. Hereby, the application will mostly be frozen for the
   whole product life cycle after the end of line manufacturing process.
   For future Electrical/Electronic (E/E) architecture, with central
   functional integrations and domain centralized systems, there is a new
   challenge regarding the strategies over the product lifecycle. This
   includes update functionality in the field and after series production,
   for new functionalities (Car to Car/Environment (C2X) communication
   standards, automatic driving assistance, certificates) in consideration
   for latest security and safety requirements. Solutions must be developed
   in order to obtain a long life cycle regarding the obsolescence of the
   product.},
ISBN = {978-1-61208-532-6},
Unique-ID = {WOS:000430359100010},
}

@inproceedings{ WOS:000444105600027,
Author = {Bychkov, Igor V. and Oparin, Gennady A. and Bogdanova, Vera G. and
   Pashinin, Anton A. and Gorsky, Sergey A.},
Editor = {Malyshkin, V},
Title = {Automation Development Framework of Scalable Scientific Web Applications
   Based on Subject Domain Knowledge},
Booktitle = {PARALLEL COMPUTING TECHNOLOGIES (PACT 2017)},
Series = {Lecture Notes in Computer Science},
Year = {2017},
Volume = {10421},
Pages = {278-288},
Note = {14th International Conference on Parallel Computing Technologies (PaCT),
   Nizhny Novgorod, RUSSIA, SEP 04-08, 2017},
Organization = {Russian Acad Sci, Inst Computat Math \& Math Geophys; Lobachevsky State
   Univ Nizhny Novgorod; Novosibirsk State Univ; Novosibirsk State Tech
   Univ; Fed Agcy Sci Org; Minist Educ \& Sci Russian Federat; Russian Fdn
   Basic Res; Adv Micro Devices Inc; RSC Technologies; Intel Corp; Russian
   Acad Sci, Inst Computat Math \& Math Geophys Siberian Branch,
   Supercomputer Software Dept; Russian Acad Sci},
Abstract = {Currently high-performance computing technologies using computational
   capabilities for solving scientific, are actively improving. The purpose
   of our research is the development of toolkit for construction and
   execution of scientific service-oriented application in heterogeneous
   distributed computing environment (HDCE). These tools provide the access
   for subject domain experts to the high-capacity computing resource,
   using these resources without extensive knowledge of computing
   architecture and low-level software, and the parallel execution of the
   user application on the base of the service-oriented technology and
   multi-agent control. We describe an architecture and functional
   capabilities of automated toolkit for the service-oriented application
   creation based on applied programs package, and multi-agent control of
   this application parallel running in HDCE. We demonstrate an example of
   the creation of the web-application for parametric feedback synthesis of
   linear dynamic object by these tools. The offered technology allows
   simplifying service creation and provides new qualitative opportunities
   of controlling parallel high-performance computations.},
DOI = {10.1007/978-3-319-62932-2\_27},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-62932-2; 978-3-319-62931-5},
ResearcherID-Numbers = {Pashinin, Anton/AAR-5631-2020
   Gorsky, Sergey A/B-5701-2018
   Bogdanova, Vera/J-9285-2018
   Bychkov, Igor/AAC-9243-2022
   },
ORCID-Numbers = {Gorsky, Sergey A/0000-0003-0177-9741
   Vera, Bogdanova/0000-0002-5763-6849
   Bychkov, Igor/0000-0002-1765-0769
   Pashinin, Anton/0000-0002-1205-7595
   Kostromin, Roman/0000-0001-8406-8106},
Unique-ID = {WOS:000444105600027},
}

@inproceedings{ WOS:000425189700026,
Author = {Chan, Chia-Yen and Hwang, Po-Wen and Huang, Ting-Ming},
Editor = {Hatheway, AE and Stubbs, DM},
Title = {Development of an athermalized optomechanical system of large aperture
   remote sensing instruments},
Booktitle = {OPTOMECHANICAL ENGINEERING 2017},
Series = {Proceedings of SPIE},
Year = {2017},
Volume = {10371},
Note = {Conference on Optomechanical Engineering, San Diego, CA, AUG 09-10, 2017},
Organization = {SPIE},
Abstract = {An integrated optimum athermalization design and analysis system will be
   developed in the study. The distance between the primary and secondary
   mirrors for a remote sensing instrument (RSI) will be taken as the
   objective function to improve the influence of the environment
   temperature variation on the optical images. Under a developing RSI
   model, the athermalization design to the secondary mirror based on the
   established system integrating a computer-aided design software,
   materials library construction, finite element analysis and optimization
   program will be executed. The design variables of the barrel, supporting
   structure and shims will be carried under low temperature change
   requirement. The displacements of the secondary mirror with respect to
   primary mirror with the optimum athermalization design can be reduced to
   almost zero from -95.6 mu m for low temperature thermal boundary
   conditions.},
DOI = {10.1117/12.2273779},
Article-Number = {UNSP 103710U},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-1200-6; 978-1-5106-1199-3},
Unique-ID = {WOS:000425189700026},
}

@inproceedings{ WOS:000399481900038,
Author = {Chandra, Herry Pintardi and Nugraha, Paulus and Putra, Evan Sutanto},
Editor = {Tim, TC and Ueda, T and Mueller, HS},
Title = {Building information modeling in the architecture-engineering
   construction project in Surabaya},
Booktitle = {3RD INTERNATIONAL CONFERENCE ON SUSTAINABLE CIVIL ENGINEERING STRUCTURES
   AND CONSTRUCTION MATERIALS - SUSTAINABLE STRUCTURES FOR FUTURE
   GENERATIONS},
Series = {Procedia Engineering},
Year = {2017},
Volume = {171},
Pages = {348-353},
Note = {3rd International Conference on Sustainable Civil Engineering Structures
   and Construction Materials - Sustainable Structures for Future
   Generations (SCESCM), Bali, INDONESIA, SEP 05-07, 2016},
Organization = {Diponegoro Univ; Univ Sebelas Maret; Petra Christian Univ; Atma Jaya
   Yogyakarta Univ; Islam Univ Indonesia; Parahyangan Cathol Univ; Sultan
   Agung Islam Univ; Delft Univ Technol; Lehigh Univ Pennsylvania; Nihon
   Univ; Eindhoven Univ Technol},
Abstract = {In current practice, many digital models do not contain sufficient
   information from designers to contractors and operators. A great deal of
   literature has pointed to the importance of understanding the Building
   Information Modeling (BIM). BIM is a digital representation of the
   physical and functional characteristics of a building. In
   Architecture-Engineering-Construction, BIM is the development and use a
   computer software model to stimulate the construction and operation of a
   facility, to make decisions and to improve the process of delivering the
   facility. The aims of this paper are to explore the need of
   technological support to implement the site-linked of BIM, the benefits
   of BIM, and the challenges of BIM. The research was conducted through
   literature review of BIM. The data was collected from the questionnaire
   survey carried out to 26 valid responses within the main stakeholders of
   construction work in Surabaya. A questionnaire is prepared by
   incorporating the technological support, benefits, and challenges of
   BIM. The data was analyzed by using descriptive analysis including mean
   analysis with 5 Likert scale. The results of the research shows that the
   need of technological support to implement the BIM is parametric
   components (mean value of 4.46), the need of software package majority
   is prepared by contractor and construction management consultant, and
   the benefit of BIM is to reduce the construction cost (mean value of
   4.6). In addition, the main challenge of BIM is different brand with the
   mean value of 4.27, in which of incompatibility of different brand (mean
   value of 4.31. (C) 2017 The Authors. Published by Elsevier Ltd.},
DOI = {10.1016/j.proeng.2017.01.343},
ISSN = {1877-7058},
Unique-ID = {WOS:000399481900038},
}

@inproceedings{ WOS:000530090400023,
Author = {Churchill, Berkeley and Sharma, Rahul and Bastien, J. F. and Aiken, Alex},
Book-Group-Author = {ACM},
Title = {Sound Loop Superoptimization for Google Native Client},
Booktitle = {TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR
   PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII)},
Year = {2017},
Pages = {313-326},
Note = {22nd ACM International Conference on Architectural Support for
   Programming Languages and Operating Systems (ASPLOS), Xian, PEOPLES R
   CHINA, APR 08-12, 2017},
Organization = {Assoc Comp Machinery; ACM SIGARCH; ACM SIGOPS; ACM SIGPLAN; ACM SIGBED;
   IEEE Comp Soc; CAS, ICT, State Key Lab Comp Architecture; Xian Jiaotong
   Univ},
Abstract = {Software fault isolation (SFI) is an important technique for the
   construction of secure operating systems, web browsers, and other
   extensible software. We demonstrate that super-optimization can
   dramatically improve the performance of Google Native Client, a SFI
   system that ships inside the Google Chrome Browser. Key to our results
   are new techniques for superoptimization of loops: we propose a new
   architecture for superoptimization tools that incorporates both a fully
   sound verification technique to ensure correctness and a bounded
   verification technique to guide the search to optimized code. In our
   evaluation we optimize 13 libc string functions, formally verify the
   correctness of the optimizations and report a median and average speedup
   of 25\% over the libraries shipped by Google.},
ISBN = {978-1-4503-4465-4},
Unique-ID = {WOS:000530090400023},
}

@inproceedings{ WOS:000419177800024,
Author = {Correia, Ruben and Lobo, Paulo Silva},
Editor = {Iacoviello, F and Moreira, PMGP and Tavares, PJS},
Title = {Simplified Assessment of the Effects of Columns Shortening on the
   Response of Tall Concrete Buildings},
Booktitle = {2ND INTERNATIONAL CONFERENCE ON STRUCTURAL INTEGRITY, ICSI 2017},
Series = {Procedia Structural Integrity},
Year = {2017},
Volume = {5},
Pages = {179-186},
Note = {2nd International Conference on Structural Integrity (ICSI), Funchal,
   PORTUGAL, SEP 04-07, 2017},
Organization = {Portuguese Spanish \& Italian Grp Fracture},
Abstract = {The constructive process as well as the time-dependent effects must be
   considered in the assessment of the response of complex concrete
   structures. For tall buildings, the adequate prediction of vertical
   elements shortening is required to determine its effects on other
   structural and nonstructural elements, usually overestimated by linear
   elastic analysis. Thus, simple numerical methods which make it possible
   to consider the most relevant aspects of the structural behaviour may be
   useful in the early stages of a project. In the research presented
   herein a simplified method, which considers the viscoelasticity of
   concrete as well as the construction sequence, was used. Its adequacy
   was assessed by comparison of the results for a tall concrete building
   with those obtained with a commercial software which incorporates a
   nonlinear staged construction analysis package. The good correlation
   between the obtained results indicates that the simplified method used
   may be applied to help make appropriate design choices. (C) 2017 The
   Authors. Published by Elsevier B.V.},
DOI = {10.1016/j.prostr.2017.07.095},
ISSN = {2452-3216},
ORCID-Numbers = {Silva Lobo, Paulo/0000-0002-0512-2440},
Unique-ID = {WOS:000419177800024},
}

@inproceedings{ WOS:000417225000022,
Author = {Cosham, Andrew and Macdonald, Kenneth A. and Hadley, Isabel and Moore,
   Philippa},
Book-Group-Author = {ASME},
Title = {ECAs: LIFTING THE LID OF THE BLACK BOX},
Booktitle = {PROCEEDINGS OF THE ASME 36TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE
   AND ARCTIC ENGINEERING, 2017, VOL 5B},
Year = {2017},
Note = {36th ASME International Conference on Ocean, Offshore and Arctic
   Engineering, Trondheim, NORWAY, JUN 25-30, 2017},
Organization = {ASME, Ocean Offshore \& Arctic Engn Div},
Abstract = {An engineering critical assessment (ECA) is commonly conducted during
   the design of a pipeline in order to determine the tolerable sizes for
   flaws in the girth welds. An ECA is a method for assessing the
   acceptability of a flaw in a structure, i.e. to demonstrate
   fitness-for-service. API 579-1/ASME FFS-1 2016 and BS 7910:2013+A1:2015
   Incorporating Corrigenda Nos. 1 and 2 give guidance for conducting
   fitness-for-service assessments of cracks and crack-like flaws. Appendix
   A of DNV-OS-F101, 2013 provides detailed procedures for evaluation of
   the fracture limit state of girth welds, based on BS 7910:2005, but
   prescribes a number of amendments and modifications. API 579-1/ASME
   FFS-1 and BS 7910:2013 will give similar, but slightly different,
   results.
   An ECA can be something of a black box because of the use of software to
   conduct the calculations. Software can have the unfortunate side-effect
   of obscuring the understanding of the calculations. ECAs are also seen
   as a thing that is very complicated.
   In an attempt to lift the lid on the black box that is, or is perceived
   to be, the ECA, the assessment of circumferentially orientated,
   surface-breaking crack-like flaw (a planar flaw) in a girth weld is
   illustrated through a comparison of the relevant stress intensity factor
   and reference stress solutions in API 579-1/ASME FFS-1 and BS 7910:2013;
   and through illustrating the effect of the choice of the Level or Option
   of the assessment, and assumptions made in the assessment with respect
   to constraint, misalignment, residual stresses, etc. Also illustrated
   are the implications of defining the fracture toughness in terms of the
   crack tip opening displacement or J-integral, and of using a single
   value of fracture toughness, based on the initiation of stable tearing
   or the first attainment of a maximum force plateau (maximum load), or of
   using tearing resistance data.
   A comparison is also made with the notionally simplified
   fitness-for-service procedures given in Annex A of API Standard 1104,
   Annex K of CSA Z662-15, and the EPRG guidelines on the assessment of
   defects in transmission pipeline girth welds Revision 2014.
   The result is an outline of what might be considered to be good practice
   for when conducting an ECA during the design of a pipeline.},
Article-Number = {UNSP V05BT04A022},
ISBN = {978-0-7918-5770-0},
Unique-ID = {WOS:000417225000022},
}

@inproceedings{ WOS:000406094100016,
Author = {Gebert, Steffen and Muessig, Alexander and Lange, Stanislav and Zinner,
   Thomas and Gray, Nicholas and Phuoc Tran-Gia},
Editor = {Aguero, R and Zaki, Y and Wenning, BL and Forster, A and TimmGiel, A},
Title = {Processing Time Comparison of a Hardware-Based Firewall and Its
   Virtualized Counterpart},
Booktitle = {MOBILE NETWORKS AND MANAGEMENT (MONAMI 2016)},
Series = {Lecture Notes of the Institute for Computer Sciences Social Informatics
   and Telecommunications Engineering},
Year = {2017},
Volume = {191},
Pages = {220-228},
Note = {8th EAI International Conference on Mobile Networks and Management
   (MONAMI), New York Univ, Abu Dhabi, U ARAB EMIRATES, OCT 23-24, 2016},
Organization = {EAI},
Abstract = {The network functions virtualization (NFV) paradigm promises higher
   flexibility, vendor-independence, and higher cost-efficiency for network
   operators. Its key concept consists of virtualizing the functions of
   specialized hardware-based middleboxes like load balancers or firewalls
   and running them on commercial off-the-shelf (COTS) hardware. This work
   aims at investigating the performance implications that result from
   migrating from a middlebox-based hardware deployment to a NFV-based
   software solution. Such analyses pave the way towards deriving
   guidelines that help determining in which network environments NFV poses
   a viable alternative to today's middlebox-heavy architectures. To this
   end, a firewall is chosen as an exemplary network function and a
   performance comparison between a dedicated hardware device and a
   commercially distributed virtualized solution by the same vendor is
   drawn. This comparison focuses on the packet delay, while varying the
   load level that is applied to the network function under test. Based on
   traffic measurements of a university campus network, conclusions
   regarding possible fields of application are drawn.},
DOI = {10.1007/978-3-319-52712-3\_16},
ISSN = {1867-8211},
ISBN = {978-3-319-52712-3; 978-3-319-52711-6},
ResearcherID-Numbers = {Zinner, Thomas/AAA-6004-2019},
ORCID-Numbers = {Zinner, Thomas/0000-0002-4179-4105},
Unique-ID = {WOS:000406094100016},
}

@inproceedings{ WOS:000417412000156,
Author = {Gomes, Rahul and Straub, Jeremy and Jones, Andrew and Morgan, John and
   Tipparach, Santipab and Sletten, Aaron and Kim, Keon Woo and Loegering,
   David and Feikema, Nick and Dayananda, Karanam and Miryala, Goutham and
   Gass, Andrew and Setterstrom, Kevin and Mischel, Jacob and Shipman,
   Dylan and Nazzaro, Colleen},
Book-Group-Author = {IEEE},
Title = {An Interconnected Network of UAS as a System-of-Systems},
Booktitle = {2017 IEEE/AIAA 36TH DIGITAL AVIONICS SYSTEMS CONFERENCE (DASC)},
Series = {IEEE-AIAA Digital Avionics Systems Conference},
Year = {2017},
Note = {36th IEEE/AIAA Digital Avionics Systems Conference (DASC), St
   Petersburg, FL, SEP 17-21, 2017},
Organization = {IEEE; AIAA; IEEE Aerosp Elect Syst Soc; AIAA Digital Avion Tech Comm;
   Boeing; MITRE Corp; Mark III Syst; Aldec; Cray; Rockwell Collins;
   Embry-Riddle Aeronaut Univ, Dept Elect Comp Software \& Syst Engn},
Abstract = {Technological advancements and miniaturization have made it possible for
   Unmanned Aerial Systems (UAS) to perform a diverse range of tasks. UAS
   are being used in various applications ranging from remote sensing{[}1]
   to disaster response{[}2] to package delivery{[}3]. As drones rapidly
   fill the airspace, there are several threats that the system can
   encounter. These threats include mid-air collisions, loss of remote
   command connection, security breach of the drone's software system and
   critical damage to the hardware. To ensure the integrity of the drone in
   flight, an interconnected UAS architecture, having a fully functional
   system capable of responding to these situations, is required. This
   paper presents a system meeting these requirements.
   An interconnected UAS system of systems is proposed that includes
   systems for onboard GPS, obstacle avoidance central control and safety
   response. It focuses on the coordination level that the UAS systems will
   need to have amongst themselves. It proposes a control system to ensure
   effective monitoring of UAS. The UAS system architecture is comprised of
   eleven systems that are essential for a safe flight. In addition, the
   control system includes five different systems that are involved in the
   decision-making process. The importance and operation of these systems
   are discussed in detail.
   The proposed UAS software architecture is capable of performing
   autonomous self-control. It has decision making modules that can receive
   data from various sensors and integrate it to provide efficient route
   planning and data management. The software systems monitor the area
   surrounding the UAV to facilitate routing and decision making. The
   software architecture includes self-awareness to respond to situations
   such as adverse climatic conditions that impact the flight capabilities
   of the UAS. The proposed UAS system also includes hardware to monitor
   and update the list of obstacles in its path. This requires the UAS to
   be equipped with sensors for obstacle determination and a software
   system that accepts the inputs from these sensors, makes decisions and
   performs actions, promptly. An emergency response system is included to
   ensure that the drone will land safely in an appropriate location, if
   its systems are compromised. This response system is capable of deciding
   the severity of the situation and sends commands to the flight control
   system regarding the recovery steps that should be taken. The proposed
   system architecture incorporates cybersecurity in its design framework
   so that it is equipped to handle potential hackers that might try to
   gain access to onboard navigation controls and reroute the UAS for
   personal gain or another agenda. If the system senses a threat, it
   launches the emergency response system.
   The proposed UAS software architecture includes a maintenance and
   diagnostics system that coordinates and monitors drone activities. It
   performs functions ranging from monitoring the health of the hardware
   systems to uploading error reports to the central server. The system
   transmits error reports to the control unit which further processes the
   data to determine the source of the error and resolve the issue.
   In UAS systems, a computerized framework takes input from at least one
   sensor, and uses a pre-characterized set of guidelines to make
   decisions{[}4]. This paper aims to achieve an interconnected UAS system
   of systems that can address several issues and solutions related to
   flight autonomously.},
ISSN = {2155-7195},
ISBN = {978-1-5386-0365-9},
ResearcherID-Numbers = {Gomes, Rahul/Z-4475-2019
   Straub, Jeremy/O-7131-2016},
ORCID-Numbers = {Gomes, Rahul/0000-0002-5377-8196
   Straub, Jeremy/0000-0002-9821-2858},
Unique-ID = {WOS:000417412000156},
}

@inproceedings{ WOS:000449986100026,
Author = {Greff, Florian and Song, Ye-Qiong and Ciarletta, Laurent and Samama,
   Arnaud},
Book-Group-Author = {ACM},
Title = {Combining Source and Destination-Tag Routing to Handle Fault Tolerance
   in Software-Defined Real-Time Mesh Networks},
Booktitle = {PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON REAL-TIME NETWORKS
   AND SYSTEMS (RTNS 2017)},
Year = {2017},
Pages = {257-266},
Note = {25th International Conference on Real-Time Networks and Systems (RTNS),
   Grenoble, FRANCE, 2017},
Organization = {Verimag; CNRS; Univ Grenoble Alpes; Persyval Lab; Grenoble Alpes
   Metropole; Inria},
Abstract = {Software-Defined Real-Time Networking is an architecture for dynamic and
   incremental allocation of time-sensitive communication flows over
   embedded networks of various topologies and low-level properties. One of
   its major advantages is the ability to leverage the path redundancy
   provided by resilient (e.g. mesh) networks in order to recover from link
   or node failures through a flow reconfiguration process. However, it
   needs to be ensured that hard real-time packets will keep being
   delivered on time during this transient reconfiguration period.
   Anticipating every possible fault is very complex and can result in a
   waste of network resource. Our contribution combines optimized
   content-centric source routing in nominal mode with flexible and
   scalable destination-tag routing in transient recovery mode. We show the
   benefit of this approach in terms of flexibility and network resource
   utilization. We then detail our method for ensuring enforcement of
   real-time properties even during the transient reconfiguration period.
   Finally, we provide the necessary algorithms to extend the SDRN flow
   allocation and routing methods in order to implement this hybrid
   fault-tolerant extension.},
DOI = {10.1145/3139258.3139264},
ISBN = {978-1-4503-5286-4},
ResearcherID-Numbers = {Song, Ye-Qiong/AAP-6461-2020},
ORCID-Numbers = {Song, Ye-Qiong/0000-0002-3949-340X},
Unique-ID = {WOS:000449986100026},
}

@article{ WOS:000408465500004,
Author = {Hussen, Sultan and Melese, Ergoye},
Title = {Time-to-recovery from obstetric fistula and associated factors: The case
   of Harar Hamlin Fistula Center},
Journal = {ETHIOPIAN JOURNAL OF HEALTH DEVELOPMENT},
Year = {2017},
Volume = {31},
Number = {2},
Pages = {85-95},
Abstract = {Background: Obstetric fistula has caused a significant number of
   morbidity and mortality throughout the world especially in developing
   countries.
   Objective: The purpose of this study is to determine mean time to
   recovery from obstetric fistula and identify the potential risk factors
   associated with time to recovery of patients from obstetric fistula.
   Methods: An institutional-based retrospective cohort study was conducted
   among 433 patients that were selected by simple random sampling. Data
   were collected by using structured check list. Epi Data, Statistical
   Package for Social Science (SPSS) version 20 and R version 3.0.2
   software's were used for data entry and processing. Descriptive nature
   of data was examined using frequency tables and Kaplan-Meier curve.
   Furthermore, bi-variable and multivariable Cox proportional hazard
   regression analyses were used to identify predictors. The crude and
   adjusted hazard ratios together with their corresponding 95\% confidence
   intervals were computed and interpreted accordingly. To ensure the
   proportional hazards assumption is valid, the numerical and graphical
   methods of Goodness of fit test method that contains p-value and
   schoenfeld plot were used in this study.
   Result: The mean survival time of obstetric fistula patients to become
   recovered in this study is 18.71 days with standard deviation of 6.68
   days. The standard Cox proportional hazard analysis shows that being
   having complete bladder neck distraction (AHR=0.1324, CI: 0.0360,
   0.4867), partial urethral damage (AHR=0.6929, CI: 0.4812, 0.9976) and
   severe vaginal scaring (AHR=0.269, CI: 0.1399, 0.5174) have significant
   effect on mean time to recovery from obstetric fistula.
   Conclusion: In this study, a substantial proportion of obstetric fistula
   patients had recovered from the obstetric fistula and nearly one third
   of patients were censored observations. This study provides further
   evidence on the role of vaginal scarring, urethral and bladder neck
   involvement in predicting the time to recovery from obstetric fistula.},
ISSN = {1021-6790},
Unique-ID = {WOS:000408465500004},
}

@inproceedings{ WOS:000411758800014,
Author = {Kalina, Jacek},
Editor = {Dossena, V and Guardone, A and Astolfi, M},
Title = {Performance improvement of distributed combined cycle plants through
   modification of structure},
Booktitle = {4TH INTERNATIONAL SEMINAR ON ORC POWER SYSTEMS},
Series = {Energy Procedia},
Year = {2017},
Volume = {129},
Pages = {105-112},
Note = {4th International Seminar on ORC Power Systems (ORC), Politecnico Milano
   Bovisa Campus, MILANO, ITALY, SEP 13-15, 2017},
Abstract = {In this paper possible modifications of combined cycle power plants of
   electric power output below 50 MW are presented. The concept is based on
   modified configuration of a single pressure heat recovery steam
   generator and bigger number of system components. Alternative plant
   structures are examined theoretically using fundamental thermodynamic
   relations. Simulations were performed using the Engineering Equation
   Solver software package. Results show that there is a room for both
   efficiency and flexibility improvements. In the simulated cases the
   energy conversion efficiency of a combined cycle power plant increased
   from 49.9\% up to 52.7\%. In addition to this, design parameters such as
   gas turbine pressure ratio, combustion temperature, steam pressure are
   relatively low, in the range of values currently widely used in
   industry. The concept also assumes a bigger role of the ORC technology.
   Its share in the total power output increased from 5.5\% in the
   reference case up to 12.6\% in the configuration that achieves the
   highest value of energy efficiency. (C) 2017 The Authors. Published by
   Elsevier Ltd.},
DOI = {10.1016/j.egypro.2017.09.173},
ISSN = {1876-6102},
ResearcherID-Numbers = {Kalina, Jacek/O-4839-2018
   },
ORCID-Numbers = {Kalina, Jacek/0000-0001-5153-0974},
Unique-ID = {WOS:000411758800014},
}

@inproceedings{ WOS:000426660500039,
Author = {Kalmykov, Oleg and Gaponova, Ludmila and Reznik, Petro and Grebenchuk,
   Sergey},
Editor = {Vatulia, G and Plugin, A and Darenskyi, O},
Title = {Use of information technologies for energetic portrait construction of
   cylindrical reinforced concrete shells},
Booktitle = {6TH INTERNATIONAL SCIENTIFIC CONFERENCE RELIABILITY AND DURABILITY OF
   RAILWAY TRANSPORT ENGINEERING STRUCTURES AND BUILDINGS (TRANSBUD-2017)},
Series = {MATEC Web of Conferences},
Year = {2017},
Volume = {116},
Note = {6th International Scientific Conference on Reliability and Durability of
   Railway Transport Engineering Structures and Buildings (Transbud),
   Ukrainian State Univ Railway Transport, Kharkiv, UKRAINE, APR 19-21,
   2017},
Abstract = {The analysis of strain-stress state of new type of architectural and
   construction system `Monofant' was examined. The analysis of the
   advanced graphic, computing software packages was carried out. The
   possibility of joint applying these packages to the problem of
   sustainable conjunction study of a rational combination of the geometric
   parameters of the design-built system ``Monofant{''} was analyzed. From
   the constructive point of view, the search for the structural element
   shape that provides a minimum material consumption under desired
   conditions is of some interest. The approach based on the energy
   criterion of rationalization was adopted to solve this problem.
   Fundamentally, new opportunities in the field of building structures
   optimization are offered with introduction of visual programming
   complexes adopted for designers (Grasshopper, Dynamo). Applying the
   described approach to the problem of rationalizing of the constructive
   system ``Monofant{''} offers the opportunity of constructing,
   calculating, analyzing and rationalizing of construction that has
   complex external and internal geometry. An illustration of a possible
   approach is given in a specific numerical example.},
DOI = {10.1051/matecconf/201711602017},
Article-Number = {02017},
ISSN = {2261-236X},
ISBN = {978-2-7598-9022-4},
ResearcherID-Numbers = {Gaponova, Lyudmila/AAJ-2307-2021
   Reznik, Petro/U-5487-2019
   Reznik, Petro/E-9293-2019
   Kalmykov, Oleg/HCH-8660-2022
   },
ORCID-Numbers = {Reznik, Petro/0000-0003-3937-6833
   Kalmykov, Oleg/0000-0001-7294-4279},
Unique-ID = {WOS:000426660500039},
}

@inproceedings{ WOS:000426660500040,
Author = {Kalmykov, Oleg and Gaponova, Ludmila and Reznik, Petro and Grebenchuk,
   Sergey},
Editor = {Vatulia, G and Plugin, A and Darenskyi, O},
Title = {Study of fire-resistance of reinforced concrete slab of a new type},
Booktitle = {6TH INTERNATIONAL SCIENTIFIC CONFERENCE RELIABILITY AND DURABILITY OF
   RAILWAY TRANSPORT ENGINEERING STRUCTURES AND BUILDINGS (TRANSBUD-2017)},
Series = {MATEC Web of Conferences},
Year = {2017},
Volume = {116},
Note = {6th International Scientific Conference on Reliability and Durability of
   Railway Transport Engineering Structures and Buildings (Transbud),
   Ukrainian State Univ Railway Transport, Kharkiv, UKRAINE, APR 19-21,
   2017},
Abstract = {Reinforced concrete structures with complex inner geometry under the
   effect of high temperatures considering void former materials were
   examined. The analysis of strain-stress state of new type of
   architectural and construction system `Monofant' under the effect of
   high temperature heating in standard fire mode, considering the change
   of design pattern was carried out. Numerical study of concrete slab with
   given reinforcement and complex inner geometry was carried out with use
   of software packages based on finite element method. Temperature fields
   throughout the depth of cross section of the slab of new type of
   architectural and construction system `Monofant' upon heating in
   standard fire mode for time interval 0-240 min. were obtained. The
   carrying capacity of sections exposed to high temperatures was
   determined by deformation method. Offered the algorithm that considers
   the transformation of design patterns depending on temperature values
   and excessive pressure in thermal insulation cavities taking into
   account influence of deformation fields on temperature distribution.},
DOI = {10.1051/matecconf/201711602018},
Article-Number = {02018},
ISSN = {2261-236X},
ISBN = {978-2-7598-9022-4},
ResearcherID-Numbers = {Reznik, Petro/U-5487-2019
   Reznik, Petro/E-9293-2019
   Gaponova, Lyudmila/AAJ-2307-2021
   Kalmykov, Oleg/HCH-8660-2022
   },
ORCID-Numbers = {Reznik, Petro/0000-0003-3937-6833
   Kalmykov, Oleg/0000-0001-7294-4279},
Unique-ID = {WOS:000426660500040},
}

@inproceedings{ WOS:000427368200192,
Author = {Li, Qiong and Liang, Dong},
Editor = {Li, S and Dai, Y and Cheng, Y},
Title = {Design of High Voltage Power Supply Control System of Stationary Digital
   Breast Tomosynthesis Based on NI CompactRIO},
Booktitle = {2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL
   ENGINEERING (ICISCE)},
Year = {2017},
Pages = {947-951},
Note = {4th International Conference on Information Science and Control
   Engineering (ICISCE), Changsha, PEOPLES R CHINA, JUL 21-23, 2017},
Organization = {IEEE; IEEE Comp Soc; Univ Technol Sydney; Hunan Univ Humanities Sci \&
   Technol; Wayne State Univ; MINJIANG UNIV; BEIJING UNIV CIVIL ENGN \&
   ARCHITECTURE; UNIV AMOIENSIS; CHINA JILIANG UNIV; Hunan Univ; Xiamen
   Univ; Iwate Prefectural Univ},
Abstract = {In the Stationary Digital Breast Tomosynthesis(s-DBT) system, the role
   of the focusing electrode is to obtain clearer scanned images by
   focusing the electrons emitted by the carbon nanotubes. As one part of
   the control system of the s-DBT, the real-time control system of the
   focusing electrode high voltage power supply is designed. The control
   system using the NI CompactRlO device and LabVIEW as platform. The
   hardware unit mainly includes NI CompactRlO 9068, analog output module
   NI 9263 and analog acquisition module NI 9201. The software design
   includes the allocation of I/O ports, the choice of CompactRlO
   development architecture, the design of the power supply control's flow
   chart and the generation of dynamic link library. Through the
   construction of the test platform, the control system's feasibility and
   reliability were tested, analysis of the experimental data shows that
   the control system's relative error of the high voltage power supply's
   voltage control is less than 0.5\%.},
DOI = {10.1109/ICISCE.2017.200},
ISBN = {978-1-5386-3013-6},
Unique-ID = {WOS:000427368200192},
}

@inproceedings{ WOS:000424694700211,
Author = {Licht, Abigail S. and Shemelya, Corey S. and DeMeo, Dante F. and
   Carlson, Emily S. and Vandervelde, Thomas E.},
Book-Group-Author = {IEEE},
Title = {Optimization of GaSb Thermophotovoltaic Diodes with Metallic Photonic
   Crystal Front-Surface Filters},
Booktitle = {2017 IEEE 60TH INTERNATIONAL MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS
   (MWSCAS)},
Series = {Midwest Symposium on Circuits and Systems Conference Proceedings},
Year = {2017},
Pages = {843-846},
Note = {60th IEEE International Midwest Symposium on Circuits and Systems
   (MWSCAS), Tufts Univ, Medford Somerville Campus, Boston, MA, AUG 06-09,
   2017},
Organization = {IEEE; IEEE Circuits and Systems Society; DRAPER},
Abstract = {A promising technology for waste-heat recovery applications is
   thermophotovoltaics (TPVs), which use photovoltaic diodes to convert
   thermal energy into electricity. The most commonly used TPV diode
   material is gallium antimonide (GaSb). Recently, GaSb TPV diodes were
   fabricated with front-surface metallic photonic crystal (MPhC) filters
   to more optimally convert the incident spectrum. This method showed
   promising initial results, in part due to a shifting of the
   photogenerated carriers away from the front-surface and into the device.
   In this paper, we use the Atlas-Silvaco software package to optimize the
   TPV diode structure for MPhCs. We investigate the addition of an
   intrinsic region in the device to take advantage of the shifted
   photogeneration profile from the MPhCs. This design allows for a 10\%
   improvement in internal quantum at the peak MPhC transmission
   wavelength.},
ISSN = {1548-3746},
ISBN = {978-1-5090-6389-5},
ResearcherID-Numbers = {Shemelya, Corey/P-8721-2019
   },
ORCID-Numbers = {Shemelya, Corey/0000-0002-9844-4165
   Vandervelde, Thomas/0000-0003-3696-2765},
Unique-ID = {WOS:000424694700211},
}

@inproceedings{ WOS:000424702000201,
Author = {Lin, Puru Burce and Ko, Cheng-Ta and Ho, Wei-Tse and Kuo, Chi-Hai and
   Chen, Kuan-Wen and Chen, Yu-Hua and Tseng, Tzyy-Jang},
Book-Group-Author = {IEEE},
Title = {A Comprehensive Study on Stress and Warpage by Design, Simulation and
   Fabrication of RDL-First Panel Level Fan-Out Technology for Advanced
   Package},
Booktitle = {2017 IEEE 67TH ELECTRONIC COMPONENTS AND TECHNOLOGY CONFERENCE (ECTC
   2017)},
Series = {Electronic Components and Technology Conference},
Year = {2017},
Pages = {1413-1418},
Note = {IEEE 67th Electronic Components and Technology Conference (ECTC), Lake
   Buena Vista, FL, MAY 30-JUN 02, 2017},
Organization = {IEEE; IEEE Components, Packaging \& Mfg Technol Soc; IEEE Comp Soc},
Abstract = {Rapid development of semiconductor technology and multi-function demands
   of end products has driven IC foundry industry toward 7nm node process,
   and even next generation of 5nm. The I/O pitch of chip is reduced
   accordingly but the buildup layer of IC carrier is still too large to
   fit interconnects. In order to overcome the gap of I/O pitch between IC
   chip and carrier, the interposer technology has been considered as a
   solution to resolve the issue. However, the cost of silicon interposer
   is too high, and the glass interposer lacks the associated
   infrastructure and is difficult to be handled, which makes a technology
   drawback for market applications. Alternatively, fan-out wafer/panel
   level package technology is getting more attractions for advanced
   package recently because of its features of low profile, small form
   factor, and high bandwidth with fine line re-distribution layer (RDL)
   routability.
   There are lots of literatures addressing about the residual stress and
   warpage mostly on wafer level fan-out technology, especially for
   chip-first technology scheme. However, comprehensive study on the panel
   level fan-out is not mature yet. This paper investigates fundamental
   factors that impact the residual stress and warpage level of panel level
   fan-out package, such as metal layer counts, thickness of dielectric and
   metal layer, coefficient of thermal expansion (CTE) and Young's modulus
   of dielectric and molding compound, molding gap and molding process
   temperature, etc.
   In this study, a RDL-first (chip-last) fan-out panel level structure of
   three metal layers on releasing film molded with epoxy compound was
   established as a simulation model by means of finite element analysis
   software. The simulation results provide a guideline of design rules for
   fabricating multilayer RDL panel level fan-out package and making the
   minimum residual stress while chip assembly. Fabrication of three-layer
   dielectric panel level fan-out, where 370mmx470mm panel size is applied,
   is also demonstrated to compare with the simulation results.},
DOI = {10.1109/ECTC.2017.106},
ISSN = {0569-5503},
ISBN = {978-1-5090-6315-4},
Unique-ID = {WOS:000424702000201},
}

@inproceedings{ WOS:000426951100033,
Author = {Mack, Jane and Cabrol, Patrick},
Book-Group-Author = {IEEE},
Title = {EdgeLink (TM) mmW Mesh Transport Experiments in the Berlin 5G-Crosshaul
   Testbed},
Booktitle = {2017 IEEE LONG ISLAND SYSTEMS, APPLICATIONS AND TECHNOLOGY CONFERENCE
   (LISAT)},
Year = {2017},
Note = {IEEE Long Island Systems Applications and Technology Conference (LISAT),
   Farmingdale, NY, MAY 05-05, 2017},
Organization = {IEEE},
Abstract = {This paper presents the performance results for a series of outdoor
   experiments conducted on a fifth generation (5G) mmW Mesh Transport
   system to show multiplexing of high throughput fronthaul (FH) and
   backhaul (BH) traffic over a long-range mmW link with exceedingly low
   latency. The EdgeLink (TM) millimeter wave (mmW) system was deployed as
   part of the European 5G-Crosshaul H2020 5G-PPP project where an outdoor
   mesh network was setup in Berlin with link distances up to 185 meters.
   The inclusion of a self-organizing Mesh topology guarantees failure
   recovery within the system through redundant paths which are managed by
   a cloud based software defined network (SDN) controller.
   Key performance metrics (KPIs) such as throughput, jitter and latency
   results were collected for analysis and will be discussed in this paper.
   In addition to performance characterization, one of the main goals of
   the 5G-Crosshaul project was as a proof-of-concept exercise for using a
   mmW wireless Mesh network as a cost efficient fifth generation (5G)
   transport solution that can seamlessly support both packet based
   fronthaul and backhaul traffic on one unified network in place of the 2
   separate networks that is commonly used in today's infrastructure. The
   proposed solution involved moving away from the legacy Common Public
   Radio Interface (CPRI) communication between the baseband unit (BBU) and
   remote radio head (RRH) to a less demanding in terms of capacity and
   latency, packet data convergence protocol-radio resource controller
   (RLC-PDCP) functional split that is further enhanced with the deployment
   of a SDN based mmW transport as a Next Generation fronthaul interface
   and Cloud-RAN architecture.
   Based on our experiments we were able to conclude that a long-range
   wireless mmW Mesh transport network can effectively be used to support
   future 5G Cloud Ran (C-RAN) upper layer stack splits requirements.},
ISBN = {978-1-5386-3887-3},
Unique-ID = {WOS:000426951100033},
}

@inproceedings{ WOS:000427961400019,
Author = {Martinello, Magnos and Liberato, Alextian B. and Beldachi, Arash Farhadi
   and Kondepu, Koteswararao and Gomes, Roberta L. and Villaca, Rodolfo and
   Ribeiro, Moises R. N. and Yan, Yan and Hugues-Salas, Emilio and
   Simeonidou, Dimitra},
Book-Group-Author = {IEEE},
Title = {Programmable Residues Defined Networks for Edge Data Centres},
Booktitle = {2017 13TH INTERNATIONAL CONFERENCE ON NETWORK AND SERVICE MANAGEMENT
   (CNSM)},
Series = {International Conference on Network and Service Management},
Year = {2017},
Note = {13th AMC/IEEE/IFIP International Conference on Network and Service
   Management (CNSM), Tokyo, JAPAN, NOV 26-30, 2017},
Organization = {ACM; IEEE; IFIP},
Abstract = {Edge Data Centres (EDC) are often managed by a single administrative
   entity with logically centralized control. The architectural split of
   control and data planes and the new control plane abstractions have been
   touted as Software-Defined Networking (SDN), where the OpenFlow protocol
   is one common choice for the standardized programmatic interface to data
   plane devices. However, in the design of an SDN architecture, there is
   no clear distinction between functional network parts such as core and
   edge elements. It means that all switches require to support lookups
   over hundreds of bits with complex actions that have to be specified by
   multiple tables. In this paper, we propose a new programmable
   architecture for EDC networks, named Residues Defined Networks (RDN). In
   RDN, a controller defines a network policy (e.g. connectivity
   protection) setting flow entries at the edges. Based on these entries,
   the edge switches assign route-IDs to flows. A route is defined as the
   remainder of the division (Residue) between a route-ID and a set of
   switch-IDs within RDN core. In case of failures, emergency routes are
   compactly encoded as programmable residues forwarding paths written into
   the packets. RDN scalability is evaluated considering 2-tier Clos
   topologies which cover mostly EDC deployments supporting up to 2304
   servers. A RDN proof-of-concept prototype is implemented in Mininet for
   network emulation. Also, to increase the accuracy on latency measures,
   we implement RDN in NetFPGA that is validated in a testbed with 10Gbps
   Ethernet boards. RDN offers ultra-fast failure recovery
   (sub-milliseconds carrier grade), achieves low latency with RDN
   switching time per hop (approximate to 0.6 mu s) and no jitter within
   the RDN core.},
ISSN = {2165-9605},
ISBN = {978-3-9018-8298-2},
ResearcherID-Numbers = {Hugues-Salas, Emilio/AAZ-1830-2020
   Kondepu, Koteswararao/AAK-1066-2021
   Kondepu, Koteswararao/J-9184-2014
   Ribeiro, Moises R.N./O-7024-2017},
ORCID-Numbers = {Kondepu, Koteswararao/0000-0003-0184-1218
   Ribeiro, Moises R.N./0000-0002-9149-2391},
Unique-ID = {WOS:000427961400019},
}

@article{ WOS:000388777800012,
Author = {Morgan, Jacob A. and Brogan, Daniel J. and Nelson, Peter A.},
Title = {Application of Structure-from-Motion photogrammetry in laboratory flumes},
Journal = {GEOMORPHOLOGY},
Year = {2017},
Volume = {276},
Pages = {125-143},
Month = {JAN 1},
Abstract = {Structure-from-Motion (SfM) photogrammetry has become widely used for
   topographic data collection in field and laboratory studies. However,
   the relative performance of SfM against other methods of topographic
   measurement in a laboratory flume environment has not been
   systematically,evaluated, and there is a general lack of guidelines for
   SfM application in flume settings. As the use of SfM in laboratory flume
   settings becomes more widespread, it is increasingly critical to develop
   an understanding of how to acquire and process SfM data for a given
   flume size and sediment characteristics. In this study, we: (1) compare
   the resolution and accuracy of SfM topographic measurements to
   terrestrial laser scanning (TLS) measurements in laboratory flumes of
   varying physical dimensions containing sediments of varying grain sizes;
   (2) explore the effects of different image acquisition protocols and
   data processing methods on the resolution and accuracy of topographic
   data derived from SfM techniques; and (3) provide general guidance for
   image acquisition and processing for SfM applications in laboratory
   flumes. To investigate the effects of flume size, sediment size, and
   photo overlap on the density and accuracy of SfM data, we collected
   topographic data using both TLS and SfM in five flumes with widths
   ranging from 0.22 to 6.71 m, lengths ranging from 9.14 to 30.48 m, and
   median sediment sizes ranging from 0.2 to 31 mm. Acquisition time, image
   overlap, point density, elevation data, and computed roughness
   parameters were compared to evaluate the performance of SfM against TLS.
   We also collected images of a pan of gravel where we varied the distance
   and angle between the camera and sediment in order to explore how photo
   acquisition affects the ability to capture grain-scale microtopographic
   features in SfM-derived point clouds. A variety of image combinations
   and SfM software package settings were also investigated to determine
   optimal processing techniques. Results from this study suggest that SfM
   provides topographic data of similar accuracy to TLS, at higher
   resolution and lower cost. We found that about 100pixels per grain are
   required to resolve grain-scale topography. We suggest protocols for
   image acquisition and SfM software settings to achieve best results when
   using SfM in laboratory settings. In general, convergent imagery, taken
   from a higher angle, with at least several overlapping images for each
   desired point in the flume will result in an acceptable point cloud. (C)
   2016 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.geomorph.2016.10.021},
ISSN = {0169-555X},
EISSN = {1872-695X},
ResearcherID-Numbers = {Nelson, Peter/C-9538-2017
   Morgan, Jacob/A-6751-2015},
ORCID-Numbers = {Nelson, Peter/0000-0002-8679-3982
   Morgan, Jacob/0000-0001-9872-6472},
Unique-ID = {WOS:000388777800012},
}

@inproceedings{ WOS:000426764000431,
Author = {Ribeiro, Marcos F. O. and Vasconcelos, Joao A. and Teixeira, Douglas A.},
Book-Group-Author = {IEEE},
Title = {Optimization of Compact Overhead Lines of 138/230kV: Optimal Selection
   and Arrangement of Cables and Definition of the Best Transmission Line
   Tower Topology},
Booktitle = {2017 1ST IEEE INTERNATIONAL CONFERENCE ON ENVIRONMENT AND ELECTRICAL
   ENGINEERING AND 2017 17TH IEEE INDUSTRIAL AND COMMERCIAL POWER SYSTEMS
   EUROPE (EEEIC / I\&CPS EUROPE)},
Year = {2017},
Note = {17th IEEE International Conference on Environment and Electrical
   Engineering (IEEE EEEIC) / 1st IEEE Industrial and Commercial Power
   Systems Europe Conference (IEEE I and CPS Europe), Milan, ITALY, JUN
   06-09, 2017},
Organization = {IEEE; IEEE EMC Soc; IEEE Power \& Energy Soc; IEEE Ind Applicat Soc},
Abstract = {Due to the problem of the growing necessity of transmitting large
   electric power packages in high voltage within large urban centers, the
   construction of compact transmission lines with multiple circuits (LTC)
   becomes interesting. In this context, this paper presents a new
   methodology for LTC's optimization implemented on software, which
   includes since the selection of the topology of the structure until the
   choice of the conductor and overhead ground wire cables and its optimal
   positioning. This methodology includes up to four circuits of 138kV and
   230 kV in the same tower and makes use of evolutionary optimization
   techniques to many objectives and decision-making. The obtained results
   demonstrate the excellence of the proposed methodology.},
ISBN = {978-1-5386-3917-7},
ResearcherID-Numbers = {Vasconcelos, Joao A/C-4101-2013
   Vasconcelos, João A/A-6440-2009},
ORCID-Numbers = {Vasconcelos, Joao A/0000-0002-4200-0135
   },
Unique-ID = {WOS:000426764000431},
}

@inproceedings{ WOS:000414280700034,
Author = {Rozorinov, Heorhii and Chichikalo, Nina and Vlasjuk, Anna and Trapezon,
   Kirill and Arkhiiereieva, Elena},
Book-Group-Author = {IEEE},
Title = {Implementation of Opportunities of Labview Software Package in Case of
   Design of Electronic Frequency Synthesizer on the Basis of PAAF},
Booktitle = {2017 XIIITH INTERNATIONAL CONFERENCE ON PERSPECTIVE TECHNOLOGIES AND
   METHODS IN MEMS DESIGN (MEMSTECH)},
Year = {2017},
Pages = {140-142},
Note = {13th International Conference on Perspective Technologies and Methods in
   MEMS Design (MEMSTECH), Lyiv, UKRAINE, APR 20-23, 2017},
Organization = {Lviv Polytechn Natl Univ, CAD Dept; Warsaw Univ Technol, Inst
   Telecommunicat, CAD Dept, Inst Comp Sci \& Informat Technologies; AGH
   Univ Sci \& Technol; IEEE MTT ED AP CPMT SSC W Ukraine Chapter; IEEE
   Ukraine Sect; Minist Educ \& Sci Ukraine},
Abstract = {Identified features that need to be considered when developing virtual
   appliances in a LabVIEW environment. The simplest system of phaselocked
   loop with the indication of its disadvantages is given. Based on the
   software package Elanix SystemView complex LabVIEW built a model of
   frequency synthesizer based on PLL and for the given parameters and the
   division ratio of 5000 (the synthesized frequency is 2500 MHz), the
   system model was simulated on a time interval of 500 mu s and at
   3,075,782 sampling points. It was found that the spectrum of the signal
   in the region of the synthesized frequency as a whole approaches the
   spectrum of the harmonic signal, but it should be noted that parasitic
   harmonics are multiples of 50 kHz, and a low relative level of the
   useful signal is slightly more than 38 dBm.},
ISBN = {978-1-5386-4001-2},
ResearcherID-Numbers = {Rozorinov, Heorhii/AAH-5635-2020
   Trapezon, Kirill/J-6803-2017
   },
ORCID-Numbers = {Rozorinov, Heorhii/0000-0002-6095-7539
   Trapezon, Kirill/0000-0001-5873-9519
   Chichikalo, Nina/0000-0002-3619-5992},
Unique-ID = {WOS:000414280700034},
}

@inproceedings{ WOS:000418325100032,
Author = {Saha, Sujoy and Morrison, Corey and Sprintson, Alex},
Book-Group-Author = {IEEE},
Title = {StorageFlow: SDN-Enabled Efficient Data Regeneration for Distributed
   Storage Systems},
Booktitle = {2017 IEEE CONFERENCE ON COMPUTER COMMUNICATIONS WORKSHOPS (INFOCOM
   WKSHPS)},
Series = {IEEE Conference on Computer Communications Workshops},
Year = {2017},
Pages = {187-192},
Note = {IEEE Conference on Computer Communications (IEEE INFOCOM), Atlanta, GA,
   MAY 01-04, 2017},
Organization = {IEEE},
Abstract = {Distributed Storage Systems (DSS) have seen increasing levels of
   deployment in data centers and in cloud storage provider networks. They
   provide efficient and cost-effective ways to store large amounts of data
   and ensure resilience to failures by using mirroring and coding schemes
   at the block and file level. While mirroring techniques provide an
   efficient way to recover lost data, they require excessive amounts of
   storage space. Coding techniques provide equivalent resilience while
   reducing the amount of required storage space. However, the current
   recovery process for coded data is not efficient due to the need to
   transfer large amounts of data to regenerate the data lost as a result
   of a failure. This contributes to large delays and excessive amount of
   network traffic resulting in significant performance bottlenecks.
   In this paper, we propose a new architecture for efficient data
   regeneration in distribution storage systems. The key idea of our
   framework is to enable network switches to perform network coding
   operations, i.e., combine multiple packets received over incoming links
   and forward the resulting packets towards the destination. Our framework
   includes a transport-layer reverse multicast protocol that takes
   advantage of network coding to minimize the amount of transferred data.
   The new architecture is implemented using the principles of Software
   Defined Networking (SDN). To enable the switches to perform network
   coding operations, we extend the SDN packet processing pipeline using a
   principled abstract model that requires minimum changes to the existing
   SDN frameworks. Our testbed experiments show that the proposed
   architecture results in significant performance gains.},
ISSN = {2159-4228},
ISBN = {978-1-5386-2784-6},
ResearcherID-Numbers = {Sprintson, Alex/AAF-6088-2019},
Unique-ID = {WOS:000418325100032},
}

@inproceedings{ WOS:000579349600166,
Author = {Sinenko, Sergey},
Editor = {Slatineanu, L and Nagit, G and Dodun, O and Merticaru, V and Coteata, M and Ripanu, MI and Mihalache, AM and Boca, M and Ibanescu, R and Panait, CE and Oancea, G and Kyratsis, P},
Title = {The method of using current regulations and standards in designing
   management and technologies of construction},
Booktitle = {21ST INNOVATIVE MANUFACTURING ENGINEERING \& ENERGY INTERNATIONAL
   CONFERENCE - IMANE\&E 2017},
Series = {MATEC Web of Conferences},
Year = {2017},
Volume = {112},
Note = {21st Innovative Manufacturing Engineering and Energy International
   Conference (IManE and E), Iasi, ROMANIA, MAY 24-27, 2017},
Organization = {Gheorghe Asachi Tech Univ Iasi, Dept Machine Mfg Technol},
Abstract = {Economic efficiency of using funds in construction of buildings and
   structures begins from an effective, proper design, which is based on
   modern, cutting-edge, advanced equipment, streamlined organization and
   process solutions of construction operations. In the light of this it is
   considered application of ``Self-Organization and Technology of
   Building{''} multifunctional software package capable of solving various
   engineering tasks on designing management of construction operations in
   accordance with the applicable rules and regulations in view of using
   software for automatic generation of workflow. Implementing this
   software in the construction management processes may help to solve
   management tasks at the construction site.},
DOI = {10.1051/matecconf/201711209007},
Article-Number = {09007},
ISSN = {2261-236X},
ResearcherID-Numbers = {Sinenko, Sergey/AAF-6668-2021},
Unique-ID = {WOS:000579349600166},
}

@inproceedings{ WOS:000425868400061,
Author = {Smirnov, Vladimir U. and Kos, Oksana I.},
Book-Group-Author = {IEEE},
Title = {Program Module for Calculating the Optimal Interval of Preventive
   Substitutions},
Booktitle = {PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE QUALITY
   MANAGEMENT,TRANSPORT AND INFORMATION SECURITY, INFORMATION TECHNOLOGIES
   (IT\&QM\&IS)},
Year = {2017},
Pages = {281-282},
Note = {International Conference on Quality Management,Transport and Information
   Security, Information Technologies (IT and QM and IS), St Petersburg,
   RUSSIA, SEP 23-30, 2017},
Abstract = {Railway transport is one of the main transport modes of the Russian
   Federation, which is strategically important for the country. Artificial
   constructions on the railways are complex and responsible elements of
   the track economy. The probability of their failure-free operation must
   be high enough. For the adoption of optimal solutions for managing the
   technical state of man-made structures, it is necessary to use
   probabilistic operating models, build algorithms for calculating
   reliability and development indices using these algorithms of the
   software package, which makes it possible to automate the process of
   controlling the operation of artificial constructions on railways.},
ISBN = {978-1-5386-0703-9},
Unique-ID = {WOS:000425868400061},
}

@inproceedings{ WOS:000426801800016,
Author = {Sopher, Ariana and Shoop, Sally},
Editor = {Zufelt, JE},
Title = {Stress Analysis of the Phoenix Compacted Snow Runway to Support Wheeled
   Aircraft},
Booktitle = {CONGRESS ON TECHNICAL ADVANCEMENT 2017: COLD REGIONS ENGINEERING},
Year = {2017},
Pages = {161-172},
Note = {17th International Conference on Cold Regions Engineering at the 1st
   Congress on Technical Advancement, Duluth, MN, SEP 10-13, 2017},
Organization = {Amer Soc Civil Engineers; Amer Soc Civil Engineers, Comm Tech
   Advancement; Amer Soc Civil Engineers, Aerosp Engn Div; Amer Soc Civil
   Engineers, Cold Reg Engn Div; Amer Soc Civil Engineers, Comm Adaptat
   Changing Climate; Amer Soc Civil Engineers, Energy Div; Amer Soc Civil
   Engineers, Forens Engn Div; Amer Soc Civil Engineers, Infrastructure
   Resilience Div; Amer Soc Civil Engineers, Construct Inst; Amer Soc Civil
   Engineers, Duluth Sect; Amer Soc Civil Engineers, Util Engn \& Surveying
   Inst},
Abstract = {Landing wheeled aircraft on snow runways is uncommon and minimally
   documented. This report describes the modeling used to evaluate design
   and construction of the new compacted-snow Phoenix runway in Antarctica
   for the first wheeled C17 aircraft landing. Snow density from the target
   design and snow density of the as-built Phoenix runway structure were
   used to determine basic elastic parameters for use in Layered Elastic
   Analysis Formulation (LEAF). LEAF is part of a software package
   developed by the Federal Aviation Administration (FAA) that allows for
   forward calculation of runway stress, strain, displacement, and
   associated principal stress and strain based on design aircraft loading.
   Stress responses for the Phoenix runway were modeled for the C17, A319,
   and B757 aircraft. Two construction vehicles were also modeled for
   comparison. Results show the comparison of the stress profiles and
   effect of subgrade stiffness on stresses in the layers just above the
   subgrade. This is the first time the model was used for a snow runway
   and it provided valuable insight for design, construction and runway
   performance for the first landing of the C17 on compacted snow. The
   model is flexible enough for simulating additional aircraft that might
   use the runway in the future. The successful construction and use of the
   new compacted-snow Phoenix runway is significant and demonstrates the
   heaviest wheeled aircraft (C17) operating on a compacted snow runway,
   ever.},
ISBN = {978-0-7844-8101-1},
Unique-ID = {WOS:000426801800016},
}

@inproceedings{ WOS:000426426600058,
Author = {Stepanov, Maxim and Melnikov, Roman and Zazulya, Juriy and Ashihmin,
   Oleg},
Editor = {Murgul, V},
Title = {Generation of stress-strain state in combined strip pile foundation beds
   through pressing of soil},
Booktitle = {INTERNATIONAL SCIENCE CONFERENCE SPBWOSCE-2016 - SMART CITY},
Series = {MATEC Web of Conferences},
Year = {2017},
Volume = {106},
Note = {International Science Conference SPbWOSCE - SMART City, Peter Great
   Saint Petersburg Polytechn Univ, Inst Civil Engn, St Petersburg, RUSSIA,
   NOV 15-17, 2016},
Organization = {Univ Montenegro; Riga Techn Univ; Sci Tech \& Expert Inst Construct
   Expertise Ctr},
Abstract = {When erecting high-rise buildings, weak underlying soils cause a number
   of problems in design and construction. In order to ensure the required
   non-exceedance of the ultimate limit settlements, the combined strip
   pile foundation has been developed allowing the soil bed to be
   prestressed. This is achieved by injection of pressurized mortar
   (pressing). The paper analyzes the effect of soil pre-stressing followed
   by pressing of foundation with the cement mortar, as applied to existing
   structures using the Plaxis 3D software package in conditions of volume
   deformation and the Hardening Soil Model. Variable order of foundation
   pressing allows the required parameters of soil bed to be achieved in
   plan and depth, thus improving interaction with the foundation and
   superstructure.},
DOI = {10.1051/matecconf/201710602011},
Article-Number = {02011},
ISSN = {2261-236X},
ResearcherID-Numbers = {Stepanov, Maksim/ABI-3269-2020
   },
ORCID-Numbers = {Mel'nikov, Roman/0000-0002-8369-3206},
Unique-ID = {WOS:000426426600058},
}

@inproceedings{ WOS:000400539800026,
Author = {Strofylas, Giorgos A. and Mazanakis, Georgios I. and Sarakinos, Sotirios
   S. and Lygidakis, Georgios N. and Nikolos, Ioannis K.},
Book-Group-Author = {ASME},
Title = {ON THE USE OF IMPROVED RADIAL BASIS FUNCTIONS METHODS IN FLUID-STRUCTURE
   INTERACTION SIMULATIONS},
Booktitle = {PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS
   AND EXPOSITION, 2016, VOL. 1},
Year = {2017},
Note = {ASME International Mechanical Engineering Congress and Exposition
   (IMECE2016), Phoenix, AZ, NOV 11-17, 2016},
Organization = {Amer Soc Mech Engineers},
Abstract = {The development of an efficient partitioned FSI coupling scheme is
   reported in this paper, aimed to facilitate interaction between an
   open-source CSD software package and an in-house academic CFD code. The
   coupling procedure is based on Radial Basis Functions (RBFs)
   interpolation for both information transfer and mesh deformation,
   entailing no dependence on connectivities, and hence making it
   applicable to different type or even intersecting grids. However, the
   method calls for increased computational resources in its initial
   formulation; to alleviate this deficiency, appropriate acceleration
   techniques have been incorporated, namely the Partition of Unity (PoU)
   approach and a surface-point reduction scheme. The PoU approach was
   adopted in case of data transfer, localizing the interpolation process
   and therefore reducing the size of the coupling matrix. An alternative
   approach was applied to improve the efficiency of the mesh deformation
   procedure, based on the agglomeration of the flow/structure interface
   nodes used for the RBFs interpolation method. For the demonstration of
   the proposed scheme a static aeroelastic simulation of a real bridge
   model, during its construction phase, was performed. The extracted
   results exhibit its potential to encounter effectively such complicated
   test cases, in a computationally efficient way.},
Article-Number = {UNSP V001T03A026},
ISBN = {978-0-7918-5051-0},
Unique-ID = {WOS:000400539800026},
}

@inproceedings{ WOS:000463799400024,
Author = {Tanev, Georgi and Madsen, Jan},
Editor = {Charlot, B and Mita, Y and Nouet, P and Pellet, C and Pressecq, F and Schneider, P and Smith, S},
Title = {A Correct-by-Construction Design and Programming Approach for Open
   Paper-Based Digital Microfluidics},
Booktitle = {2017 SYMPOSIUM ON DESIGN, TEST, INTEGRATION AND PACKAGING OF MEMS/MOEMS
   (DTIP 2017)},
Year = {2017},
Note = {19th Symposium on Design, Test, Integration and Packaging of MEMS/MOEMS
   (DTIP), Bordeaux, FRANCE, MAY 29-JUN 01, 2017},
Organization = {Lirmm Fr; IMS Bordeaux Fr; Cnrs Fr; Cmp Imag Fr; IEEE Components,
   Packaging \& Mfg Technol Soc; Elemca Com; Cnes Fr; IEEE},
Abstract = {Advances in microfluidic research have allowed digital microfluidic
   (DMF) chips to be rapid prototyped using inexpensive materials and
   simple fabrication processes to the extend where the time spend on chip
   design can be significantly longer than the time required for
   fabrication. The growing need for application specific DMF chips
   challenges efficient handling of the increasing chip design and
   programming complexity. To address this, we propose a
   correct-by-construction modular chip design approach, which allows chips
   to be constructed from a component library and verified by simulation
   before fabrication. After fabrication, the chip is operated from a
   smartphone by remote instrumentation of a portable DMF chip control
   device. By combining structured design techniques with custom developed
   hardware and software tools, we present a full end-to-end solution for
   fast DMF chip design, simulation and operation.},
ISBN = {978-1-5386-2952-9},
Unique-ID = {WOS:000463799400024},
}

@inproceedings{ WOS:000568504400073,
Author = {Vacca, Giuseppina and Pili, Davide and Fiorino, Donatella Rita and
   Pintus, Valentina},
Editor = {Tucci, G and Bonora, V},
Title = {A WEBGIS FOR THE KNOWLEDGE AND CONSERVATION OF THE HISTORICAL WALL
   STRUCTURES OF THE 13TH-18TH CENTURIES},
Booktitle = {GEOMATICS \& RESTORATION - CONSERVATION OF CULTURAL HERITAGE IN THE
   DIGITAL ERA},
Series = {International Archives of the Photogrammetry, Remote Sensing and Spatial
   Information Sciences},
Year = {2017},
Volume = {42-5},
Number = {W1},
Pages = {551-556},
Note = {1st International Conference on Geomatics and Restoration - Conservation
   of Cultural Heritage in the Digital Era, Florence, ITALY, MAY 22-24,
   2017},
Organization = {Acad Assoc Topog \& Cartog; Acad Assoc Architectural Restorat; Italian
   Geog Mil Inst},
Abstract = {The presented work is part of the research project, titled ``Tecniche
   murarie tradizionali: conoscenza per la conservazione ed it
   miglioramento prestazionale{''} (Traditional building techniques: from
   knowledge to conservation and performance improvement), with the purpose
   of studying the building techniques of the 13th - 18th centuries in the
   Sardinia Region (Italy) for their knowledge, conservation, and
   promotion. The end purpose of the entire study is to improve the
   performance of the examined structures. In particular, the task of the
   authors within the research project was to build a WebGIS to manage the
   data collected during the examination and study phases. This
   infrastructure was entirely built using Open Source software.
   The work consisted of designing a database built in PostgreSQL and its
   spatial extension PostGIS, which allows to store and manage feature
   geometries and spatial data. The data input is performed via a form
   built in HTML and PHP. The HTML part is based on Bootstrap, an open
   tools library for websites and web applications. The implementation of
   this template used both PHP and Javascript code. The PHP code manages
   the reading and writing of data to the database, using embedded SQL
   queries.
   As of today, we surveyed and archived more than 300 buildings, belonging
   to three main macro categories: fortification architectures, religious
   architectures, residential architectures. The masonry samples
   investigated in relation to the construction techniques are more than
   150.
   The database is published on the Internet as a WebGIS built using the
   Leaflet Javascript open libraries, which allows creating map sites with
   background maps and navigation, input and query tools. This too uses an
   interaction of HTML, Javascript, PHP and SQL code.},
DOI = {10.5194/isprs-archives-XLII-5-W1-551-2017},
ISSN = {1682-1750},
EISSN = {2194-9034},
ResearcherID-Numbers = {Vacca, Giuseppina/Q-1152-2017
   FIORINO, DONATELLA RITA DRF/A-9086-2015},
ORCID-Numbers = {Vacca, Giuseppina/0000-0001-8161-7667
   },
Unique-ID = {WOS:000568504400073},
}

@inproceedings{ WOS:000426949000012,
Author = {Weber, Sam and Coblenz, Michael and Myers, Brad and Aldrich, Jonathan
   and Sunshine, Joshua},
Book-Group-Author = {IEEE},
Title = {Empirical Studies on the Security and Usability Impact of Immutability},
Booktitle = {2017 IEEE CYBERSECURITY DEVELOPMENT (SECDEV)},
Year = {2017},
Pages = {50-53},
Note = {Conference on IEEE Cybersecurity Development (SecDev), MA, SEP 24-26,
   2017},
Organization = {IEEE Comp Soc; IEEE},
Abstract = {Although it is well-known that API design has a large and long-term
   impact on security, the literature contains few substantial guidelines
   for practitioners on how to design APIs that improve security. Even
   fewer of those guidelines have been evaluated empirically.
   Security professionals have proposed that software engineers choose
   immutable APIs and architectures to enhance security. Unfortunately,
   prior empirical research argued that immutablity decreases API
   usability.
   This paper brings together the results from a number of previous papers
   that together aim to show that immutability, when carefully designed
   using usability as a first-class requirement, can have positive effects
   on both usability and security. We also make observations on study
   design in this field.},
DOI = {10.1109/SecDev.2017.21},
ISBN = {978-1-5386-3467-7},
ResearcherID-Numbers = {Myers, Brad/R-3816-2019
   Coblenz, Michael/Y-8910-2019},
ORCID-Numbers = {Coblenz, Michael/0000-0002-9369-4069},
Unique-ID = {WOS:000426949000012},
}

@article{ WOS:000391905200013,
Author = {Eldesoky, Ahmed R. and Yates, Esben S. and Nyeng, Tine B. and Thomsen,
   Mette S. and Nielsen, Hanne M. and Poortmans, Philip and Kirkove, Carine
   and Krause, Mechthild and Kamby, Claus and Mjaaland, Ingvil and Blix,
   Egil S. and Jensen, Ingelise and Berg, Martin and Lorenzen, Ebbe L. and
   Taheri-Kadkhoda, Zahra and Offersen, Birgitte V.},
Title = {Internal and external validation of an ESTRO delineation guideline -
   dependent automated segmentation tool for loco-regional radiation
   therapy of early breast cancer},
Journal = {RADIOTHERAPY AND ONCOLOGY},
Year = {2016},
Volume = {121},
Number = {3},
Pages = {424-430},
Month = {DEC},
Abstract = {Background and purpose: To internally and externally validate an atlas
   based automated segmentation (ABAS) in loco-regional radiation therapy
   of breast cancer.
   Materials and methods: Structures of 60 patients delineated according to
   the ESTRO consensus guideline were included in four categorized
   multi-atlas libraries using MIM Maestro (TM) software. These libraries
   were used for auto-segmentation in two different patient groups (50
   patients from the local institution and 40 patients from other
   institutions). Dice Similarity Coefficient, Average Hausdorff Distance,
   difference in volume and time were computed to compare ABAS before and
   after correction against a gold standard manual segmentation (MS).
   Results: ABAS reduced the time of MS before and after correction by 93\%
   and 32\%, respectively. ABAS showed high agreement for lung, heart,
   breast and humeral head, moderate agreement for chest wall and axillary
   nodal levels and poor agreement for interpectoral, internal mammary
   nodal regions and LADCA. Correcting ABAS significantly improved all the
   results. External validation of ABAS showed comparable results.
   Conclusions: ABAS is a clinically useful tool for segmenting structures
   in breast cancer loco-regional radiation therapy in a
   multi-institutional setting. However, manual correction of some
   structures is important before clinical use. The ABAS is now available
   for routine clinical use in Danish patients. (C) 2016 Elsevier Ireland
   Ltd. All rights reserved.},
DOI = {10.1016/j.radonc.2016.09.005},
ISSN = {0167-8140},
EISSN = {1879-0887},
ResearcherID-Numbers = {Poortmans, P.M.P./L-4581-2015
   },
ORCID-Numbers = {Krause, Mechthild/0000-0003-1776-9556
   Lorenzen, Ebbe Laugaard/0000-0003-1895-733X},
Unique-ID = {WOS:000391905200013},
}

@article{ WOS:000385595200008,
Author = {Ercan, Mehmet B. and Goodall, Jonathan L.},
Title = {Design and implementation of a general software library for using
   NSGA-II with SWAT for multi-objective model calibration},
Journal = {ENVIRONMENTAL MODELLING \& SOFTWARE},
Year = {2016},
Volume = {84},
Pages = {112-120},
Month = {OCT},
Abstract = {Calibrating watershed-scale hydrologic models remains a critical but
   challenging step in the modeling process. The Soil and Water Assessment
   Tool (SWAT) is one example of a widely used watershed-scale hydrologic
   model that requires calibration. The calibration algorithms currently
   available to SWAT modelers through freely available and open source
   software, however, are limited and do not include many multi-objective
   genetic algorithms (MOGAs). The Non-Dominated Sorting Genetic Algorithm
   II (NSGA-II) has been shown to be an effective and efficient MOGA
   calibration algorithm for a wide variety of applications including for
   SWAT model calibration. Therefore, the objective of this study was to
   create an open source software library for multi-objective calibration
   of SWAT models using NSGA-II. The design and implementation of the
   library are presented, followed by a demonstration of the library
   through a test case for the Upper Neuse Watershed in North Carolina, USA
   using six objective functions in the model calibration. (C) 2016
   Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.envsoft.2016.06.017},
ISSN = {1364-8152},
EISSN = {1873-6726},
ResearcherID-Numbers = {Ercan, Mehmet B/E-9141-2010
   Ercan, Mehmet B./ABG-8062-2020
   Ercan, Mehmet/G-9391-2017
   Goodall, Jonathan/B-3663-2009},
ORCID-Numbers = {Ercan, Mehmet B/0000-0002-6799-8990
   Goodall, Jonathan/0000-0002-1112-4522},
Unique-ID = {WOS:000385595200008},
}

@article{ WOS:000382340100018,
Author = {Javanainen, Matti and Martinez-Seara, Hector},
Title = {Efficient preparation and analysis of membrane and membrane protein
   systems},
Journal = {BIOCHIMICA ET BIOPHYSICA ACTA-BIOMEMBRANES},
Year = {2016},
Volume = {1858},
Number = {10, SI},
Pages = {2468-2482},
Month = {OCT},
Abstract = {Molecular dynamics (MD) simulations have become a highly important
   technique to consider lipid membrane systems, and quite often they
   provide considerable added value to laboratory experiments. Rapid
   development of both software and hardware has enabled the increase of
   time and size scales reachable by MD simulations to match those
   attainable by several accurate experimental techniques. However, until
   recently, the quality and maturity of software tools available for
   building membrane models for simulations as well as analyzing the
   results of these simulations have seriously lagged behind.
   Here, we discuss the recent developments of such tools from the
   end-users' point of view. In particular, we review the software that can
   be employed to build lipid bilayers and other related structures with or
   without embedded membrane proteins to be employed in MD simulations.
   Additionally, we provide a brief critical insight into force fields and
   MD packages commonly used for membrane and membrane protein simulations.
   Finally, we list analysis tools that can be used to study the properties
   of membrane and membrane protein systems. In all these points we comment
   on the respective compatibility of the covered tools.
   We also share our opinion on the current state of the available
   software. We briefly discuss the most commonly employed tools and
   platforms on which new software can be built. We conclude the review by
   providing a few ideas and guidelines on how the development of tools can
   be further boosted to catch up with the rapid pace at which the field of
   membrane simulation progresses. This includes improving the
   compatibility between software tools and promoting the openness of the
   codes on which these applications rely.
   This article is part of a Special Issue entitled: Biosimulations edited
   by Ilpo Vattulainen and Tomasz Rog. (C) 2016 Elsevier B.V. All rights
   reserved.},
DOI = {10.1016/j.bbamem.2016.02.036},
ISSN = {0005-2736},
EISSN = {1879-2642},
ResearcherID-Numbers = {Martinez-Seara, Hector/D-3734-2014
   Javanainen, Matti/GZA-9227-2022},
ORCID-Numbers = {Martinez-Seara, Hector/0000-0001-9716-1713
   Javanainen, Matti/0000-0003-4858-364X},
Unique-ID = {WOS:000382340100018},
}

@article{ WOS:000382113100028,
Author = {Lin, Liang and Zhou, Liang and Zhu, Yi and Hua, Yujie and Mao, Jun-Fa
   and Yin, Wen-Yan},
Title = {Improvement in Cavity and Model Designs of LDMOS Power Amplifier for
   Suppressing Metallic Shielding Cover Effects},
Journal = {IEEE TRANSACTIONS ON ELECTROMAGNETIC COMPATIBILITY},
Year = {2016},
Volume = {58},
Number = {5},
Pages = {1617-1628},
Month = {OCT},
Abstract = {Some key technical issues are addressed for improving electromagnetic
   compatibility (EMC) design of metallic shielding cavity for achieving
   better RF performance of LDMOSFET-based power amplifier (PA). With the
   help of an Automation Testing Bench, its input-output responses are at
   first measured and compared, where an interactive influence between the
   PA and PCB is studied in detail. The internal and outer matching
   structures of LDMOSFET on the PCB are simulated using the commercial
   full-wave software HFSS. Perturbation theory is used to analyze the
   shielding cover effects with different package sizes and cavity heights
   sensitivity considered. Some guidelines for selecting suitable inner
   shielding structure and cavity height are given, and an improved model
   for both LDMOSFET and PA module is proposed and analyzed. Furthermore,
   it is integrated with the lumped-element circuit model of die and
   cosimulated using the software ADS 2013, and good agreement between the
   simulated and measured results is obtained. The most sensitive feedback
   path of electromagnetic interference inside the shielding cavity is
   characterized and a metallic isolation wall is introduced between the
   input (gate) and output (drain) of the LDMOSFET die for suppressing
   inner coupling effects. This research should be useful for the
   development of compact and miniaturized PA modules with our desired EMC
   performance.},
ISSN = {0018-9375},
EISSN = {1558-187X},
Unique-ID = {WOS:000382113100028},
}

@article{ WOS:000383293600035,
Author = {Municchi, Federico and Goniva, Christoph and Radl, Stefan},
Title = {Highly efficient spatial data filtering in parallel using the opensource
   library CPPPO},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2016},
Volume = {207},
Pages = {400-414},
Month = {OCT},
Abstract = {CPPPO is a compilation of parallel data processing routines developed
   with the aim to create a library for ``scale bridging{''} (i.e.
   connecting different scales by mean of closure models) in a multi-scale
   approach. CPPPO features a number of parallel filtering algorithms
   designed for use with structured and unstructured Eulerian meshes, as
   well as Lagrangian data sets. In addition, data can be processed on the
   fly, allowing the collection of relevant statistics without saving
   individual snapshots of the simulation state. Our library is provided
   with an interface to the widely-used CFD solver OpenFOAM (R), and can be
   easily connected to any other software package via interface modules.
   Also, we introduce a novel, extremely efficient approach to parallel
   data filtering, and show that our algorithms scale super-linearly on
   multi-core clusters. Furthermore, we provide a guideline for choosing
   the optimal Eulerian cell selection algorithm depending on the number of
   CPU cores used. Finally, we demonstrate the accuracy and the parallel
   scalability of CPPPO in a showcase focusing on heat and mass transfer
   from a dense bed of particles.},
DOI = {10.1016/j.cpc.2016.05.026},
ISSN = {0010-4655},
EISSN = {1879-2944},
ResearcherID-Numbers = {Radl, Stefan/Q-4640-2017
   Municchi, Federico/AAD-2357-2020},
ORCID-Numbers = {Radl, Stefan/0000-0002-0738-0961
   Municchi, Federico/0000-0002-5105-6173},
Unique-ID = {WOS:000383293600035},
}

@article{ WOS:000393580700026,
Author = {Dagand, Pierre-Evariste and Tabareau, Nicolas and Tanter, Eric},
Title = {Partial Type Equivalences for Verified Dependent Interoperability},
Journal = {ACM SIGPLAN NOTICES},
Year = {2016},
Volume = {51},
Number = {9},
Pages = {298-310},
Month = {SEP},
Note = {21st ACM SIGPLAN International Conference on Functional Programming
   (ICFP), Nara, JAPAN, SEP 18-24, 2016},
Organization = {ACM SIGPLAN},
Abstract = {Full-spectrum dependent types promise to enable the development of
   correct-by-construction software. However, even certified software needs
   to interact with simply-typed or untyped programs, be it to perform
   system calls, or to use legacy libraries. Trading static guarantees for
   runtime checks, the dependent interoperability framework provides a
   mechanism by which simplytyped values can safely be coerced to dependent
   types and, conversely, dependently-typed programs can defensively be
   exported to a simply-typed application. In this paper, we give a
   semantic account of dependent interoperability. Our presentation relies
   on and is guided by a pervading notion of type equivalence, whose
   importance has been emphasized in recent work on homotopy type theory.
   Specifically, we develop the notion of partial type equivalences as a
   key foundation for dependent interoperability. Our framework is
   developed in Coq; it is thus constructive and verified in the strictest
   sense of the terms. Using our library, users can specify domain-specific
   partial equivalences between data structures. Our library then takes
   care of the (sometimes, heavy) lifting that leads to interoperable
   programs. It thus becomes possible, as we shall illustrate, to
   internalize and hand-tune the extraction of dependently-typed programs
   to interoperable OCaml programs within Coq itself.},
DOI = {10.1145/3022670.2951933},
ISSN = {0362-1340},
EISSN = {1558-1160},
ORCID-Numbers = {tabareau, nicolas/0000-0003-3366-2273},
Unique-ID = {WOS:000393580700026},
}

@article{ WOS:000382569000014,
Author = {Neale, Joanne and Brown, Caral},
Title = {``We are always in some form of contact': friendships among homeless
   drug and alcohol users living in hostels},
Journal = {HEALTH \& SOCIAL CARE IN THE COMMUNITY},
Year = {2016},
Volume = {24},
Number = {5},
Pages = {557-566},
Month = {SEP},
Abstract = {Homeless drug and alcohol users are one of the most marginalised groups
   in society. They frequently have complex needs and limited social
   support. In this paper, we explore the role of friendship in the lives
   of homeless drug and alcohol users living in hostels, using the concepts
   of social capital' and recovery capital' to frame the analyses. The
   study was undertaken in three hostels, each in a different English city,
   during 2013-2014. Audio recorded semi-structured interviews were
   conducted with 30 residents (9 females; 21 males) who self-reported
   drink and/or drug problems; follow-up interviews were completed 4-6weeks
   later with 22 participants (6 females; 16 males). Data were transcribed
   verbatim, coded using the software package MAXQDA, and analysed using
   Framework. Only 21 participants reported current friends at interview 1,
   and friendship networks were small and changeable. Despite this,
   participants desired friendships that were culturally normative. Eight
   categories of friend emerged from the data: family-like friends; using
   friends; homeless friends; childhood friends; online-only friends; drug
   treatment friends; work friends; and mutual interest friends. Routine
   and regular contact was highly valued, with family-like friends
   appearing to offer the most constant practical and emotional support.
   The use of information and communication technologies (ICTs) was central
   to many participants' friendships, keeping them connected to social
   support and recovery capital outside homelessness and substance-using
   worlds. We conclude that those working with homeless drug and alcohol
   users - and potentially other marginalised populations - could
   beneficially encourage their clients to identify and build upon their
   most positive and reliable relationships. Additionally, they might
   explore ways of promoting the use of ICTs to combat loneliness and
   isolation. Texting, emailing, online mutual aid meetings, chatrooms,
   Internet penpals, skyping and other social media all offer potentially
   valuable opportunities for building friendships that can bolster
   otherwise limited social and recovery capital.},
DOI = {10.1111/hsc.12215},
ISSN = {0966-0410},
EISSN = {1365-2524},
ORCID-Numbers = {Neale, Joanne/0000-0003-1502-5983},
Unique-ID = {WOS:000382569000014},
}

@article{ WOS:000384666800013,
Author = {Sheikhizadeh, Siavash and Schranz, M. Eric and Akdel, Mehmet and de
   Ridder, Dick and Smit, Sandra},
Title = {PanTools: representation, storage and exploration of pan-genomic data},
Journal = {BIOINFORMATICS},
Year = {2016},
Volume = {32},
Number = {17},
Pages = {487-493},
Month = {SEP 1},
Note = {15th European Conference on Computational Biology (ECCB), The Hague,
   NETHERLANDS, SEP 03-07, 2016},
Abstract = {Motivation: Next-generation sequencing technology is generating a wealth
   of highly similar genome sequences for many species, paving the way for
   a transition from single-genome to pan-genome analyses. Accordingly,
   genomics research is going to switch from reference-centric to
   pan-genomic approaches. We define the pan-genome as a comprehensive
   representation of multiple annotated genomes, facilitating analyses on
   the similarity and divergence of the constituent genomes at the
   nucleotide, gene and genome structure level. Current pan-genomic
   approaches do not thoroughly address scalability, functionality and
   usability.
   Results: We introduce a generalized De Bruijn graph as a pan-genome
   representation, as well as an online algorithm to construct it. This
   representation is stored in a Neo4j graph database, which makes our
   approach scalable to large eukaryotic genomes. Besides the construction
   algorithm, our software package, called PanTools, currently provides
   functionality for annotating pan-genomes, adding sequences, grouping
   genes, retrieving gene sequences or genomic regions, reconstructing
   genomes and comparing and querying pan-genomes. We demonstrate the
   performance of the tool using datasets of 62 E. coli genomes, 93 yeast
   genomes and 19 Arabidopsis thaliana genomes.},
DOI = {10.1093/bioinformatics/btw455},
ISSN = {1367-4803},
EISSN = {1460-2059},
ResearcherID-Numbers = {de Ridder, Dick/F-3169-2010
   Schranz, M. Eric/AAB-7601-2022
   Smit, Sandra/E-6787-2010
   },
ORCID-Numbers = {de Ridder, Dick/0000-0002-4944-4310
   Schranz, M. Eric/0000-0001-6777-6565
   Smit, Sandra/0000-0001-5239-5321},
Unique-ID = {WOS:000384666800013},
}

@article{ WOS:000383625900007,
Author = {Perea, Claudia and Fernando De la Hoz, Juan and Felipe Cruz, Daniel and
   David Lobaton, Juan and Izquierdo, Paulo and Camilo Quintero, Juan and
   Raatz, Bodo and Duitama, Jorge},
Title = {Bioinformatic analysis of genotype by sequencing (GBS) data with NGSEP},
Journal = {BMC GENOMICS},
Year = {2016},
Volume = {17},
Number = {5},
Month = {AUG 31},
Abstract = {Background: The recent development and availability of different
   genotype by sequencing (GBS) protocols provided a cost-effective
   approach to perform high-resolution genomic analysis of entire
   populations in different species. The central component of all these
   protocols is the digestion of the initial DNA with known restriction
   enzymes, to generate sequencing fragments at predictable and
   reproducible sites. This allows to genotype thousands of genetic markers
   on populations with hundreds of individuals. Because GBS protocols
   achieve parallel genotyping through high throughput sequencing (HTS),
   every GBS protocol must include a bioinformatics pipeline for analysis
   of HTS data. Our bioinformatics group recently developed the Next
   Generation Sequencing Eclipse Plugin (NGSEP) for accurate, efficient,
   and user-friendly analysis of HTS data.
   Results: Here we present the latest functionalities implemented in NGSEP
   in the context of the analysis of GBS data. We implemented a one step
   wizard to perform parallel read alignment, variants identification and
   genotyping from HTS reads sequenced from entire populations. We added
   different filters for variants, samples and genotype calls as well as
   calculation of summary statistics overall and per sample, and diversity
   statistics per site. NGSEP includes a module to translate genotype calls
   to some of the most widely used input formats for integration with
   several tools to perform downstream analyses such as population
   structure analysis, construction of genetic maps, genetic mapping of
   complex traits and phenotype prediction for genomic selection. We
   assessed the accuracy of NGSEP on two highly heterozygous F1 cassava
   populations and on an inbred common bean population, and we showed that
   NGSEP provides similar or better accuracy compared to other widely used
   software packages for variants detection such as GATK, Samtools and
   Tassel.
   Conclusions: NGSEP is a powerful, accurate and efficient bioinformatics
   software tool for analysis of HTS data, and also one of the best
   bioinformatic packages to facilitate the analysis and to maximize the
   genomic variability information that can be obtained from GBS
   experiments for population genomics.},
DOI = {10.1186/s12864-016-2827-7},
Article-Number = {498},
ISSN = {1471-2164},
ResearcherID-Numbers = {Castellanos, Jorge Alexander Duitama/I-6484-2019
   },
ORCID-Numbers = {Castellanos, Jorge Alexander Duitama/0000-0002-9105-6266
   De la Hoz, Juan/0000-0002-5584-7511},
Unique-ID = {WOS:000383625900007},
}

@article{ WOS:000389071800001,
Author = {Savitsky, Terrance D.},
Title = {Bayesian Nonparametric Mixture Estimation for Time-Indexed Functional
   Data in R},
Journal = {JOURNAL OF STATISTICAL SOFTWARE},
Year = {2016},
Volume = {72},
Number = {2},
Pages = {1-34},
Month = {AUG},
Abstract = {We present growfunctions for R that offers Bayesian nonparametric
   estimation models for analysis of dependent, noisy time series data
   indexed by a collection of domains. This data structure arises from
   combining periodically published government survey statistics, such as
   are reported in the Current Population Study (CPS). The CPS publishes
   monthly, by-state estimates of employment levels, where each state
   expresses a noisy time series. Published state-level estimates from the
   CPS are composed from household survey responses in a model-free manner
   and express high levels of volatility due to insufficient sample sizes.
   Existing software solutions borrow information over a modeled time-based
   dependence to extract a de-noised time series for each domain. These
   solutions, however, ignore the dependence among the domains that may be
   additionally leveraged to improve estimation efficiency. The
   growfunctions package offers two fully nonparametric mixture models that
   simultaneously estimate both a time and domain-indexed dependence
   structure for a collection of time series: (1) A Gaussian process (GP)
   construction, which is parameterized through the covariance matrix,
   estimates a latent function for each domain. The covariance parameters
   of the latent functions are indexed by domain under a Dirichlet process
   prior that permits estimation of the dependence among functions across
   the domains: (2) An intrinsic Gaussian Markov random field prior
   construction provides an alternative to the GP that expresses different
   computation and estimation properties. In addition to performing
   denoised estimation of latent functions from published domain estimates,
   growfunctions allows estimation of collections of functions for
   observation units (e. g., households), rather than aggregated domains,
   by accounting for an informative sampling design under which the
   probabilities for inclusion of observation units are related to the
   response variable. growfunctions includes plot functions that allow
   visual assessments of the fit performance and dependence structure of
   the estimated functions. Computational efficiency is achieved by
   performing the sampling for estimation functions using compiled C++.},
DOI = {10.18637/jss.v072.i02},
ISSN = {1548-7660},
Unique-ID = {WOS:000389071800001},
}

@article{ WOS:000381404300004,
Author = {Jiang, Lei and Feng, Jing and Shen, Ye and Xiong, Xinli},
Title = {Fast Recovery Routing Algorithm for Software Defined Network based
   Operationally Responsive Space Satellite Networks},
Journal = {KSII TRANSACTIONS ON INTERNET AND INFORMATION SYSTEMS},
Year = {2016},
Volume = {10},
Number = {7},
Pages = {2936-2951},
Month = {JUL 31},
Abstract = {An emerging satellite technology, Operationally Responsive Space (ORS)
   is expected to provide a fast and flexible solution for emergency
   response, such as target tracking, dense earth observation, communicate
   relaying and so on. To realize large distance transmission, we propose
   the use of available relay satellites as relay nodes. Accordingly, we
   apply software defined network (SDN) technology to ORS networks. We
   additionally propose a satellite network architecture refered to as the
   SDN-based ORS-Satellite (Sat) networking scheme (SDOS). To overcome the
   issures of node failures and dynamic topology changes of satellite
   networks, we combine centralized and distributed routing mechanisms and
   propose a fast recovery routing algorithm (FRA) for SDOS. In this
   routing method, we use centralized routing as the base mode. The
   distributed opportunistic routing starts when node failures or
   congestion occur. The performance of the proposed routing method was
   validated through extensive computer simulations. The results
   demonstrate that the method is effective in terms of resoving low
   end-to-end delay, jitter and packet drops.},
DOI = {10.3837/tiis.2016.07.004},
ISSN = {1976-7277},
Unique-ID = {WOS:000381404300004},
}

@article{ WOS:000382353400039,
Author = {Baros, Tatjana},
Title = {THE APPLICATION OF BIM TECHNOLOGY AND ITS RELIABILITY IN THE STATIC LOAD
   ANALYSIS},
Journal = {TEHNICKI VJESNIK-TECHNICAL GAZETTE},
Year = {2016},
Volume = {23},
Number = {4},
Pages = {1221-1226},
Month = {JUL-AUG},
Abstract = {BIM (Building Information Modelling), or modelling based on object data,
   is a process that involves the construction of virtual objects, i.e. the
   creation and management of digital representations of physical and
   functional characteristics of objects. The BIM system, therefore, is
   used to create a model using relevant information. BIM is applicable
   throughout the entire life cycle of a building, from the design phase,
   and is then also applicable in the phases of construction as well as
   management and maintenance. There are many software packages based on
   the BIM concept. Those are digital software packages used to create 3D
   object representations with layers of additional project information.
   Basically, the BIM system parameters are ``intelligent{''} objects that
   are not only visual displays, but also contain other information
   required in the design process and the preparation of project
   documentation. BIM software allows various kinds of analysis of objects.
   It has wide application in the theory of structures and static load
   analysis, because it allows a variety of analyses and 3D simulations as
   well as a range of calculations and the dimensioning of structural
   parts.},
DOI = {10.17559/TV-20141201232823},
ISSN = {1330-3651},
EISSN = {1848-6339},
Unique-ID = {WOS:000382353400039},
}

@article{ WOS:000374897200003,
Author = {Estebanez, Alvaro and Llanos, Diego R. and Gonzalez-Escribano, Arturo},
Title = {New Data Structures to Handle Speculative Parallelization at Runtime},
Journal = {INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING},
Year = {2016},
Volume = {44},
Number = {3, SI},
Pages = {407-426},
Month = {JUN},
Abstract = {Software-based, thread-level speculation (TLS) is a software technique
   that optimistically executes in parallel loops whose fully-parallel
   semantics can not be guaranteed at compile time. Modern TLS libraries
   allow to handle arbitrary data structures speculatively. This desired
   feature comes at the high cost of local store and/or remote recovery
   times: The easier the local store, the harder the remote recovery.
   Unfortunately, both times are on the critical path of any TLS system. In
   this paper we propose a solution that performs local store in constant
   time, while recover values in a time that is in the order of , being the
   number of threads. As we will see, this solution, together with some
   additional improvements, makes the difference between slowdowns and
   noticeable speedups in the speculative parallelization of non-synthetic,
   pointer-based applications on a real system. Our experimental results
   show a gain of 3.58 to 28 with respect to the baseline system, and a
   relative efficiency of up to, on average, 65 \% with respect to a TLS
   implementation specifically tailored to the benchmarks used.},
DOI = {10.1007/s10766-014-0347-0},
ISSN = {0885-7458},
EISSN = {1573-7640},
ResearcherID-Numbers = {Llanos, Diego R./L-8118-2014
   Gonzalez-Escribano, Arturo/K-9931-2014},
ORCID-Numbers = {Llanos, Diego R./0000-0001-6240-9109
   Gonzalez-Escribano, Arturo/0000-0003-1309-9321},
Unique-ID = {WOS:000374897200003},
}

@article{ WOS:000375630200003,
Author = {Hu, Zhen-Zhong and Zhang, Xiao-Yang and Wang, Heng-Wei and Kassem,
   Mohamad},
Title = {Improving interoperability between architectural and structural design
   models: An industry foundation classes-based approach with web-based
   tools},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2016},
Volume = {66},
Pages = {29-42},
Month = {JUN},
Abstract = {Medium and large construction projects typically involve multiple
   structural consultants who use a wide range of structural analysis
   applications. These applications and technologies have inadequate
   interoperability and there is still a dearth of investigations
   addressing interoperability issues in the structural engineering domain.
   This paper proposes a novel approach which combines an industry
   foundation classes (IFC)-based Unified Information Model with a number
   of algorithms to enhance the interoperability: (a) between architectural
   and structural models, and (b) among multiple structural analysis models
   (bidirectional conversion or round tripping). The proposed approach aims
   to achieve the conversion by overcoming the inconsistencies in data
   structures, representation logics and syntax used in different software
   applications.
   The approach was implemented in both Client Server (C/S) and Browser
   Server (B/S) environments to enable central and remote collaboration
   among geographically dispersed users. The platforms were tested in four
   large real-life projects. The testing involved four key scenarios: (a)
   the bidirectional conversion among four structural analysis tools; (b)
   the comparison of the conversion via the proposed approach with the
   conversion via direct links among the involved tools; (c) the direct
   export from an IFC-based architectural tool through the Application
   Program Interface (API), and (d) the conversion and visualization of
   structural analysis results. All these scenarios were successfully
   performed and tested in four significant case studies. In particular,
   the conversion among the four structural analysis applications (ETABS,
   SAP2000, ANSYS and MIDAS) was successfully tested for all possible
   conversion routes among the four applications in two of the case studies
   (i.e., Project A and Project B). The first four steps of natural mode
   shapes and their natural vibration periods were calculated and compared
   with the converted models. They were all achieved within a standard
   deviation of 0.1 s and 0.2 sin Project A and Project B, respectively,
   indicating an accurate conversion. (C) 2016 Elsevier B.V. All rights
   reserved.},
DOI = {10.1016/j.autcon.2016.02.001},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Hu, Zhen-Zhong/GQZ-0982-2022
   Jeong, Yongwook/N-7413-2016
   Hu, Zhen-Zhong/AAI-9454-2020
   },
ORCID-Numbers = {Hu, Zhen-Zhong/0000-0001-9653-0097
   Hu, Zhen-Zhong/0000-0001-9653-0097
   Wang, Hengwei/0000-0002-3696-0930
   Kassem, Mohamad/0000-0002-9837-3934},
Unique-ID = {WOS:000375630200003},
}

@article{ WOS:000379349700001,
Author = {Parton, Daniel L. and Grinaway, Patrick B. and Hanson, Sonya M. and
   Beauchamp, Kyle A. and Chodera, John D.},
Title = {Ensembler: Enabling High-Throughput Molecular Simulations at the
   Superfamily Scale},
Journal = {PLOS COMPUTATIONAL BIOLOGY},
Year = {2016},
Volume = {12},
Number = {6},
Month = {JUN},
Abstract = {The rapidly expanding body of available genomic and protein structural
   data provides a rich resource for understanding protein dynamics with
   biomolecular simulation. While computational infrastructure has grown
   rapidly, simulations on an omics scale are not yet widespread, primarily
   because software infrastructure to enable simulations at this scale has
   not kept pace. It should now be possible to study protein dynamics
   across entire (super) families, exploiting both available structural
   biology data and conformational similarities across homologous proteins.
   Here, we present a new tool for enabling highthroughput simulation in
   the genomics era. Ensembler takes any set of sequencesfrom a single
   sequence to an entire superfamily-and shepherds them through various
   stages of modeling and refinement to produce simulation-ready
   structures. This includes comparative modeling to all relevant PDB
   structures (which may span multiple conformational states of interest),
   reconstruction of missing loops, addition of missing atoms, culling of
   nearly identical structures, assignment of appropriate protonation
   states, solvation in explicit solvent, and refinement and filtering with
   molecular simulation to ensure stable simulation. The output of this
   pipeline is an ensemble of structures ready for subsequent molecular
   simulations using computer clusters, supercomputers, or distributed
   computing projects like Folding@home. Ensembler thus automates much of
   the time-consuming process of preparing protein models suitable for
   simulation, while allowing scalability up to entire superfamilies. A
   particular advantage of this approach can be found in the construction
   of kinetic models of conformational dynamics-such as Markov state models
   (MSMs)-which benefit from a diverse array of initial configurations that
   span the accessible conformational states to aid sampling. We
   demonstrate the power of this approach by constructing models for all
   catalytic domains in the human tyrosine kinase family, using all
   available kinase catalytic domain structures from any organism as
   structural templates. Ensembler is free and open source software
   licensed under the GNU General Public License (GPL) v2. It is compatible
   with Linux and OS X. The latest release can be installed via the conda
   package manager, and the latest source can be downloaded from
   https://github.com/choderalab/ensembler.},
DOI = {10.1371/journal.pcbi.1004728},
Article-Number = {e1004728},
EISSN = {1553-7358},
ORCID-Numbers = {Chodera, John/0000-0003-0542-119X},
Unique-ID = {WOS:000379349700001},
}

@article{ WOS:000377365800003,
Author = {Gamble, Katharine Brumbaugh and Lightsey, E. Glenn},
Title = {Decision Analysis Tool for Small Satellite Risk Management},
Journal = {JOURNAL OF SPACECRAFT AND ROCKETS},
Year = {2016},
Volume = {53},
Number = {3},
Pages = {420-432},
Month = {MAY},
Note = {53rd AIAA Aerospace Sciences Meeting / AIAA Atmospheric Flight Mechanics
   Conference / 17th AIAA Non-Deterministic Approaches Conference / AIAA
   Science and Technology Forum / AIAA Infotech at Aerospace Conference,
   Kissimmee, FL, JAN 05-09, 2015},
Organization = {AIAA},
Abstract = {Risk management plans improve the likelihood of mission success by
   identifying potential failures early and planning mitigation methods to
   circumvent any issues. However, in the aerospace industry to date, risk
   management plans have typically only been used for larger and more
   expensive satellites and have rarely been applied to lower-cost
   satellites such as CubeSats. Furthermore, existing risk management plans
   typically require experienced personnel and significant time to perform
   the analysis. The CubeSat Decision Advisor tool uses components of
   decision theory such as decision trees, multi-attribute utility theory,
   and utility elicitation methods to determine the expected utilities of
   proposed mitigation techniques to a mission risk. Based on the user's
   value preference system, assessment of success probabilities, and
   resources required for a given mitigation technique, the tool suggests
   the course of action that will normatively yield the most value for the
   COST, people, and time resources required. This research creates a risk
   management software tool never before available that is easily
   accessible and usable for low-cost small satellite missions due to its
   construction in Microsoft Excel. A demonstration analysis is conducted
   on a university CubeSat mission as a case study. The target audience for
   this tool is primarily university labs, which could not otherwise afford
   expensive software packages. However, the tool is broadly applicable to
   all organizations, including government, academic, and corporate, and is
   not constrained to university small satellite mission architectures.},
DOI = {10.2514/1.A33401},
ISSN = {0022-4650},
EISSN = {1533-6794},
Unique-ID = {WOS:000377365800003},
}

@article{ WOS:000375800200010,
Author = {Shin, Hanjun and Shi, Yi and Dai, Chao and Tjong, Harianto and Gong, Ke
   and Alber, Frank and Zhou, Xianghong Jasmine},
Title = {TopDom: an efficient and deterministic method for identifying
   topological domains in genomes},
Journal = {NUCLEIC ACIDS RESEARCH},
Year = {2016},
Volume = {44},
Number = {7},
Month = {APR 20},
Abstract = {Genome-wide proximity ligation assays allow the identification of
   chromatin contacts at unprecedented resolution. Several studies reveal
   that mammalian chromosomes are composed of topological domains (TDs) in
   sub-mega base resolution, which appear to be conserved across cell types
   and to some extent even between organisms. Identifying topological
   domains is now an important step toward understanding the structure and
   functions of spatial genome organization. However, current methods for
   TD identification demand extensive computational resources, require
   careful tuning and/or encounter inconsistencies in results. In this
   work, we propose an efficient and deterministic method, TopDom, to
   identify TDs, along with a set of statistical methods for evaluating
   their quality. TopDom is much more efficient than existing methods and
   depends on just one intuitive parameter, a window size, for which we
   provide easy-to-implement optimization guidelines. TopDom also
   identifies more and higher quality TDs than the popular directional
   index algorithm. The TDs identified by TopDom provide strong support for
   the cross-tissue TD conservation. Finally, our analysis reveals that the
   locations of housekeeping genes are closely associated with cross-tissue
   conserved TDs. The software package and source codes of TopDom are
   available at http://zhoulab.usc.edu/TopDom/.},
DOI = {10.1093/nar/gkv1505},
Article-Number = {e70},
ISSN = {0305-1048},
EISSN = {1362-4962},
Unique-ID = {WOS:000375800200010},
}

@article{ WOS:000373745300004,
Author = {Cassel, Sofia and Howar, Falk and Jonsson, Bengt and Steffen, Bernhard},
Title = {Active learning for extended finite state machines},
Journal = {FORMAL ASPECTS OF COMPUTING},
Year = {2016},
Volume = {28},
Number = {2},
Pages = {233-263},
Month = {APR},
Note = {12th International Conference on Software Engineering and Formal Methods
   (SEFM), Grenoble, FRANCE, SEP 01-05, 2014},
Organization = {Inria},
Abstract = {We present a black-box active learning algorithm for inferring extended
   finite state machines (EFSM)s by dynamic black-box analysis. EFSMs can
   be used to model both data flow and control behavior of software and
   hardware components. Different dialects of EFSMs are widely used in
   tools for model-based software development, verification, and testing.
   Our algorithm infers a class of EFSMs called register automata. Register
   automata have a finite control structure, extended with variables
   (registers), assignments, and guards. Our algorithm is parameterized on
   a particular theory, i.e., a set of operations and tests on the data
   domain that can be used in guards.
   Key to our learning technique is a novel learning model based on
   so-called tree queries. The learning algorithm uses tree queries to
   infer symbolic data constraints on parameters, e.g., sequence numbers,
   time stamps, identifiers, or even simple arithmetic. We describe
   sufficient conditions for the properties that the symbolic constraints
   provided by a tree query in general must have to be usable in our
   learning model. We also show that, under these conditions, our framework
   induces a generalization of the classical Nerode equivalence and
   canonical automata construction to the symbolic setting. We have
   evaluated our algorithm in a black-box scenario, where tree queries are
   realized through (black-box) testing. Our case studies include
   connection establishment in TCP and a priority queue from the Java Class
   Library.},
DOI = {10.1007/s00165-016-0355-5},
ISSN = {0934-5043},
EISSN = {1433-299X},
ORCID-Numbers = {Howar, Falk/0000-0002-9524-4459},
Unique-ID = {WOS:000373745300004},
}

@article{ WOS:000373502600004,
Author = {Gao Xianming and Wang Baosheng and Zhang Xiaozhe and Ma Shicong},
Title = {A High-Elasticity Router Architecture with Software Data Plane and Flow
   Switching Plane Separation},
Journal = {CHINA COMMUNICATIONS},
Year = {2016},
Volume = {13},
Number = {3},
Pages = {37-52},
Month = {MAR},
Abstract = {Routers have traditionally been architected as two elements: forwarding
   plane and control plane through ForCES or other protocols. Each
   forwarding plane aggregates a fixed amount of computing, memory, and
   network interface resources to forward packets. Unfortunately, the tight
   coupling of packet-processing tasks with network interfaces has severely
   restricted service innovation and hardware upgrade. In this context, we
   explore the insightful prospect of functional separation in forwarding
   plane to propose a next-generation router architecture, which, if
   realized, can provide promises both for various packet-processing tasks
   and for flexible deployment while solving concerns related to the above
   problems. Thus, we put forward an alternative construction in which
   functional resources within a forwarding plane are disaggregated. A
   forwarding plane is instead separated into two planes: software data
   plane (SDP) and flow switching plane (FSP), and each plane can be viewed
   as a collection of ``building blocks{''}. SDP is responsible for
   packet-processing tasks without its expansibility restricted with the
   amount and kinds of network interfaces. FSP is in charge of packet
   receiving/transmitting tasks and can incrementally add switching
   elements, such as general switches, or even specialized switches, to
   provide network interfaces for SDP. Besides, our proposed router
   architecture uses network fabrics to achieve the best connectivity among
   building blocks, which can support for network topology reconfiguration
   within one device. At last, we make an experiment on our platform in
   terms of bandwidth utilization rate, configuration delay, system
   throughput and execution time.},
ISSN = {1673-5447},
ResearcherID-Numbers = {Zhang, XZ/HJA-4189-2022
   Zhang, Xi/HJH-1211-2023},
Unique-ID = {WOS:000373502600004},
}

@article{ WOS:000372099800008,
Author = {Sapp, Wendi and Erck, Adam and Wang, Zenqiang and Kilin, Dmitri},
Title = {Electronic and spectral properties of ametal-organic super container
   molecule by single point DFT},
Journal = {MOLECULAR PHYSICS},
Year = {2016},
Volume = {114},
Number = {3-4, SI},
Pages = {394-399},
Month = {FEB 16},
Note = {Annual Sanibel Meeting, St Simons Island, GA, FEB 14-19, 2016},
Abstract = {Metal-organic super container (MOSC) molecules are ideal candidates for
   photocatalysis due to their construction with transition metal centres
   and tuneable cavity sizes that could house catalytic sites. The basic
   electronic structure for a model of extremely large size (more than 2000
   ions) is explored by single point calculation using unrestricted density
   functional theory, and Perdue-Burke-Ernzerhof functional in Vienna ab
   initio simulation package software. The information obtained through
   these calculations (such as density of states, absorbance spectra, and
   charge density) will allow for analysis of a MOSC's catalytic ability.
   Electronic characteristics of the nanostructures (MOSCs and their
   building blocks) in the ground and photoexcited electronic
   configurations are examined. We explore if the presence of transition
   metal ions with open shells in such close proximity to one another may
   result in high spin configurations and show any arrangement into
   ferromagnetic ordering. Spin-unrestricted computation was applied to
   evaluate how optical properties could be affected by d-d transitions. A
   scan of a spin-polarisation parameter allows one to resolve spin
   configuration and obtain a connection between theory and experiment.
   Analysis of Kohn-Sham orbitals of interest provides insight into charge
   transfer mechanisms, which were found to contribute to multiple
   low-energy charge transfer states to the electronic structure.},
DOI = {10.1080/00268976.2015.1076899},
ISSN = {0026-8976},
EISSN = {1362-3028},
ResearcherID-Numbers = {Kilin, Dmitri S/M-3668-2016},
ORCID-Numbers = {Kilin, Dmitri S/0000-0001-7847-5549},
Unique-ID = {WOS:000372099800008},
}

@article{ WOS:000366225800007,
Author = {All, Anissa and Castellar, Elena Patricia Nunez and Van Looy, Jan},
Title = {Assessing the effectiveness of digital game-based learning: Best
   practices},
Journal = {COMPUTERS \& EDUCATION},
Year = {2016},
Volume = {92-93},
Pages = {90-103},
Month = {JAN-FEB},
Abstract = {In recent years, research into the effectiveness of digital game-based
   learning (DGBL) has increased. However, a large heterogeneity in methods
   for assessing the effectiveness of DGBL exist, leading to questions
   regarding reliability and validity of certain methods. This has resulted
   in the need for a scientific basis to conduct this type of research,
   providing procedures, frameworks and methods that can be validated. The
   present study is part of a larger systematic process towards the
   development of a standardized procedure for conducting DGBL
   effectiveness studies. In a first phase, the variety in methods that are
   used for sampling, implementation of the interventions, measures and
   data analysis were mapped in a systematic literature review using
   Cochrane guidelines. The present paper reflects the second stage, where
   this variety in elements are presented to experts in psychology and
   pedagogy by means of semi-structured interviews, in order to define
   preferred methods for conducting DGBL effectiveness studies. The
   interview was structured according to five dimensions that were used in
   the literature review: 1) participants (e.g., characteristics of the
   sample involved) 2) intervention (e.g., contents, format, timings and
   treatment lengths, intervention(s) in control group(s)) 3) methods
   (sampling, assignment of participants to conditions, number of testing
   moments) 4) outcome measures (e.g., instruments used to measure a
   certain outcome) and 5) data-analysis. The interviews were transcribed
   and analyzed using qualitative software package nVivo. Our results show
   that areas for improvement involve the intervention dimension and the
   methods dimension. The proposed improvements relate to implementation of
   the interventions in both the experimental and control group,
   determining which elements are preferably omitted during the
   intervention (such as guidance by the instructor, extra elements that
   consist of substantive information) and which elements would be aloud
   (e.g., procedural help, training session). Also, variables on which
   similarity between experimental and control condition should be attained
   were determined (e.g., time exposed to intervention, instructor, day of
   the week). With regard to the methods dimension, proposed improvements
   relate to assignment of participants to conditions (e.g., variables to
   take into account when using blocked randomized design), general design
   (e.g. necessity of a pre-test and control group) test development (e.g.,
   develop and pilot parallel tests) and testing moments (e.g., follow up
   after minimum 2 weeks). In sum, the present paper provides best
   practices that cover all aspects of the study design and consist of game
   specific elements. While several suggestions have been previously made
   regarding research design of DGBL effectiveness studies, these often do
   not cover all aspects of the research design. Hence, the results of this
   study can be seen as a base for a more systematic approach, which can be
   validated in the future in order to develop a standardized procedure for
   assessing the effectiveness of DGBL that can be applied flexibly across
   different contexts. (C) 2015 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.compedu.2015.10.007},
ISSN = {0360-1315},
EISSN = {1873-782X},
ResearcherID-Numbers = {Van Looy, Jan/A-5354-2016},
ORCID-Numbers = {Van Looy, Jan/0000-0002-2498-7028},
Unique-ID = {WOS:000366225800007},
}

@inproceedings{ WOS:000388368100215,
Author = {Alogla, Kamal and Weekes, Laurence and Augusthus-Nelson, Levingshan},
Editor = {Gomes, JFS and Meguid, SA},
Title = {PROGRESSIVE COLLAPSE RESISTING MECHANISMS OF REINFORCED CONCRETE
   STRUCTURES},
Booktitle = {IRF2016: 5TH INTERNATIONAL CONFERENCE INTEGRITY-RELIABILITY-FAILURE},
Year = {2016},
Pages = {1401-1422},
Note = {5th International Conference on Integrity-Reliability-Failure (IRF),
   Univ Porto, Fac Engn, Porto, PORTUGAL, JUL 24-28, 2016},
Organization = {Univ Porto; Univ Toronto; Univ Toronto, Mech \& Aerosp Design Lab;
   Portuguese Assoc Experimental Mech; European Soc Experimental Mech; Amer
   Soc Experimental Mech; Japanese Soc Mech Engn; Int Measurement
   Confederat; Assoc Francaise Mecanique; European Assoc Dynam Mat; Inst
   Ciencia Inovacao Eng Mecanica Eng Ind; Lab Biomecanica Porto; Fundacao
   Ciencia Tecnologia; ABREU PCO},
Abstract = {Reinforced concrete (RC) buildings may be vulnerable to progressive
   collapse due to lack of sufficient continuous reinforcement. Progressive
   collapse is a situation when local failure is followed by collapse of
   adjoining members, which in turn causes global collapse, and can
   eventually result in injuries or loss of life. Design of structures
   against progressive collapse has not been an integral part of structural
   design. However, some guidelines such as General Service Administration
   (GSA) and Unified Facilities Criteria (UFC) guidelines have detailing
   requirements to reduce the likelihood of progressive collapse. It is
   difficult to predict the structural behaviour of building members during
   progressive collapse because the dynamic nature of the event and the
   limited experimental tests conducted to understand the nature of
   progressive collapse. Membrane action of beams and floors are important
   mechanisms of load redistribution and progressive collapse resistance in
   the event of failure of load-bearing elements. The behaviour of
   reinforced concrete beams under compressive and tensile membrane action
   is not yet fully understood.
   In order to investigate and quantify the structural resisting mechanisms
   of reinforced concrete structures against progressive collapse, two
   large scale specimens have been tested under quasi-static loading.
   Non-linear response is then converted into dynamic response
   (Pseudo-Static response) using the energy equilibrium approach proposed
   by Izzuddin et al. (Izzuddin et al., 2008)
   A finite element model was developed using the finite element software
   package ANSYS 11.0 in order to numerically simulate structural behaviour
   of RC beam-column sub-assemblages when load-carrying members are removed
   under the effect of abnormal loading. A macro-model based approach was
   used in the finite element analysis by using beam elements and a series
   of spring non-linear elements to capture the non-linear behaviour of
   structural members associated with the redistribution of loads after
   column removal. Numerical results were compared with those obtained from
   the experimental program. Test results showed that the RC
   sub-assemblages would experience three mechanism stages, flexural,
   compressive arch and catenary action stages to resist progressive
   collapse. Numerical results showed a good agreement with the
   experimental results.},
ISBN = {978-989-98832-5-3},
ResearcherID-Numbers = {Alogla, Kamal D.I./Y-1931-2018
   },
ORCID-Numbers = {Alogla, Kamal D.I./0000-0002-0853-5152
   Augusthus Nelson, Levingshan/0000-0003-3092-7635},
Unique-ID = {WOS:000388368100215},
}

@inproceedings{ WOS:000389510500023,
Author = {Alshara, Zakarea and Seriai, Abdelhak-Djamel and Tibermacine, Chouki and
   Bouziane, Hinde Lilia and Dony, Christophe and Shatnawi, Anas},
Editor = {Tekinerdogan, B and Zdun, U and Babar, A},
Title = {Materializing Architecture Recovered from Object-Oriented Source Code in
   Component-Based Languages},
Booktitle = {Software Architecture, ECSA 2016},
Series = {Lecture Notes in Computer Science},
Year = {2016},
Volume = {9839},
Pages = {309-325},
Note = {10th European Conference on Software Architecture (ECSA), Copenhagen,
   DENMARK, NOV 28-DEC 02, 2016},
Abstract = {In the literature of software engineering, many approaches have been
   proposed for the recovery of software architectures. These approaches
   propose to group classes into highly-cohesive and loosely-coupled
   clusters considered as architectural components. The recovered
   architecture plays mainly a documentation role, as high-level design
   views that enhance software understandability. In addition, architecture
   recovery can be considered as an intermediate step for migration to
   component-based platforms. This migration allows to fully benefit from
   all advantages brought by software component concept. For that, the
   recovered clusters should not be considered as simple packaging and
   deployment units. They should be treated as real components: true
   structural and behavior units that are instantiable from component
   descriptors and connected together to materialize the architecture of
   the software. In this paper, we propose an approach for revealing
   component descriptors, component instances and component-based
   architecture to materialize the recovered architecture of an
   object-oriented software in component-based languages. We applied our
   solution onto two well known component-based languages, OSGi and SOFA.},
DOI = {10.1007/978-3-319-48992-6\_23},
ISSN = {0302-9743},
ISBN = {978-3-319-48992-6; 978-3-319-48991-9},
ResearcherID-Numbers = {Shatnawi, Anas/S-6476-2019
   },
ORCID-Numbers = {Shatnawi, Anas/0000-0002-5561-4232
   Tibermacine, Chouki/0000-0002-2063-0291
   Seriai, Abdelhak-djamel/0000-0003-1961-1410
   AL SHARA, Zakarea/0000-0002-2727-6985},
Unique-ID = {WOS:000389510500023},
}

@inproceedings{ WOS:000386718500111,
Author = {Azmi, Nur Farhana and Ahmad, Faizah and Ali, Azlan Shah},
Editor = {Kamaruzzaman, SNB and Ali, ASB and Azmi, NFB and Chua, SJL},
Title = {Mechanisms for protecting the identity of small towns in Malaysia},
Booktitle = {4TH INTERNATIONAL BUILDING CONTROL CONFERENCE 2016 (IBCC 2016)},
Series = {MATEC Web of Conferences},
Year = {2016},
Volume = {66},
Note = {4th International Building Control Conference (IBCC), Kuala Lumpur,
   MALAYSIA, MAR 07-08, 2016},
Organization = {Fac Built Environm, Dept Bldg Surveying; Royal Inst Surveyors Malaysia;
   Univ Kebangsaan; Univ Technol MARA},
Abstract = {Built heritage constitute the very individuality of places hence to its
   identity. However, the built heritage in a small-scale town is nowadays
   subject to the triple threat of dilapidation, exhaustion and
   disappearance. Therefore, this paper attempts to examine the building
   regulations, guidelines and policies with regard to sustaining the
   unique features and identity of small towns through extensive review of
   literature. A semi structured interview was conducted amongst a
   purposive sample of nine stakeholders from different organizations
   involved in heritage matters in Malaysia; to investigate existing
   heritage protection measures impacting development of small towns. Using
   the qualitative software package NVivo 8.0, the study demonstrates that
   the towns are not only protected by legislative measures but also
   through community initiative activities and active collaboration between
   stakeholder groups. The paper concludes that a relatively strong policy
   and administrative framework are presently in place for protection and
   enhancement of built heritage despite some weaknesses in the existing
   mechanisms.},
DOI = {10.1051/matecconf/20166600113},
Article-Number = {00113},
ISSN = {2261-236X},
ResearcherID-Numbers = {Azmi, Nur Farhana/N-6706-2017
   Ali, Azlan Shah/B-5214-2010},
ORCID-Numbers = {Azmi, Nur Farhana/0000-0002-0286-6145
   Ali, Azlan Shah/0000-0003-1923-7473},
Unique-ID = {WOS:000386718500111},
}

@inproceedings{ WOS:000391207000017,
Author = {Balogh, Gergo and Gergely, Tamas and Beszedes, Arpad and Gyimothy, Tibor},
Book-Group-Author = {IEEE},
Title = {Are My Unit Tests in the Right Package?},
Booktitle = {2016 IEEE 16TH INTERNATIONAL WORKING CONFERENCE ON SOURCE CODE ANALYSIS
   AND MANIPULATION (SCAM)},
Series = {IEEE International Working Conference on Source Code Analysis and
   Manipulation},
Year = {2016},
Pages = {137-146},
Note = {16th IEEE International Working Conference on Source Code Analysis and
   Manipulation (SCAM), Raleigh, NC, OCT 02-03, 2016},
Organization = {IEEE; IEEE Comp Soc; IEEE Comp Soc Tech Council Software Engn; ABB},
Abstract = {The software development industry has adopted written and de facto
   standards for creating effective and maintainable unit tests.
   Unfortunately, like any other source code artifact, they are often
   written without conforming to these guidelines, or they may evolve into
   such a state. In this work, we address a specific type of issues related
   to unit tests. We seek to automatically uncover violations of two
   fundamental rules: 1) unit tests should exercise only the unit they were
   designed for, and 2) they should follow a clear packaging convention.
   Our approach is to use code coverage to investigate the dynamic
   behaviour of the tests with respect to the code elements of the program,
   and use this information to identify highly correlated groups of tests
   and code elements (using community detection algorithm). This grouping
   is then compared to the trivial grouping determined by package
   structure, and any discrepancies found are treated as ``bad smells.{''}
   We report on our related measurements on a set of large open source
   systems with notable unit test suites, and provide guidelines through
   examples for refactoring the problematic tests.},
DOI = {10.1109/SCAM.2016.10},
ISSN = {1942-5430},
ISBN = {978-1-5090-3849-7},
ResearcherID-Numbers = {Gergely, Tamás/ABA-7259-2021
   Balogh, Gergő/ABA-7560-2021
   Gyimothy, Tibor/M-3705-2018
   },
ORCID-Numbers = {Gergely, Tamás/0000-0001-7504-3580
   Balogh, Gergő/0000-0002-6781-5453
   Gyimothy, Tibor/0000-0002-2123-7387
   Beszedes, Arpad/0000-0002-5421-9302},
Unique-ID = {WOS:000391207000017},
}

@inproceedings{ WOS:000388804600043,
Author = {Buur, Hanne and Subramaniam, Annapurni and Gillies, Kim and Dumas,
   Christophe and Bhatia, Ravinder},
Editor = {Chiozzi, G and Guzman, JC},
Title = {TMT Approach to Observatory Software Development Process},
Booktitle = {SOFTWARE AND CYBERINFRASTRUCTURE FOR ASTRONOMY IV},
Series = {Proceedings of SPIE},
Year = {2016},
Volume = {9913},
Note = {Conference on Software and Cyberinfrastructure for Astronomy IV,
   Edinburgh, SCOTLAND, JUN 26-30, 2016},
Organization = {SPIE},
Abstract = {The purpose of the Observatory Software System (OSW) is to integrate all
   software and hardware components of the Thirty Meter Telescope (TMT) to
   enable observations and data capture; thus it is a complex software
   system that is defined by four principal software subsystems: Common
   Software (CSW), Executive Software (ESW), Data Management System (DMS)
   and Science Operations Support System (SOSS), all of which have
   interdependencies with the observatory control systems and data
   acquisition systems. Therefore, the software development process and
   plan must consider dependencies to other subsystems, manage
   architecture, interfaces and design, manage software scope and
   complexity, and standardize and optimize use of resources and tools.
   Additionally, the TMT Observatory Software will largely be developed in
   India through TMT's workshare relationship with the India TMT
   Coordination Centre (ITCC) and use of Indian software industry vendors,
   which adds complexity and challenges to the software development
   process, communication and coordination of activities and priorities as
   well as measuring performance and managing quality and risk. The
   software project management challenge for the TMT OSW is thus a
   multi-faceted technical, managerial, communications and interpersonal
   relations challenge. The approach TMT is using to manage this
   multi-faceted challenge is a combination of establishing an effective
   geographically distributed software team (Integrated Product Team) with
   strong project management and technical leadership provided by the TMT
   Project Office (PO) and the ITCC partner to manage plans, process,
   performance, risk and quality, and to facilitate effective
   communications; establishing an effective cross-functional software
   management team composed of stakeholders, OSW leadership and ITCC
   leadership to manage dependencies and software release plans, technical
   complexities and change to approved interfaces, architecture, design and
   tool set, and to facilitate effective communications; adopting an
   agile-based software development process across the observatory to
   enable frequent software releases to help mitigate subsystem
   interdependencies; defining concise scope and work packages for each of
   the OSW subsystems to facilitate effective outsourcing of software
   deliverables to the ITCC partner, and to enable performance monitoring
   and risk management. At this stage, the architecture and high-level
   design of the software system has been established and reviewed. During
   construction each subsystem will have a final design phase with reviews,
   followed by implementation and testing. The results of the TMT approach
   to the Observatory Software development process will only be preliminary
   at the time of the submittal of this paper, but it is anticipated that
   the early results will be a favorable indication of progress.},
DOI = {10.1117/12.2234102},
Article-Number = {991319},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-0205-2; 978-1-5106-0206-9},
ORCID-Numbers = {Subramaniam, Annapurni/0000-0003-4612-620X},
Unique-ID = {WOS:000388804600043},
}

@inproceedings{ WOS:000389763200022,
Author = {Chang, Po-Lun and Wang, Hen-Kung and Hsieh, Fei-Hu and Wu, Guo-Zhen},
Book-Group-Author = {DEStech Publicat Inc},
Title = {Nonlinear Phenomena of Varying Load Resistance Buck-Boost DC-DC
   Converting Solar Cell System with Perturbation and Observation Method
   for MPPT},
Booktitle = {2016 INTERNATIONAL CONFERENCE ON COMPUTATIONAL MODELING, SIMULATION AND
   APPLIED MATHEMATICS (CMSAM 2016)},
Year = {2016},
Pages = {115-119},
Note = {International Conference on Computational Modeling, Simulation and
   Applied Mathematics (CMSAM), Bangkok, THAILAND, JUL 24-25, 2016},
Abstract = {Inexhaustible solar power is booming for carbon reduction in recent
   years. The current efficiency of solar power generation is still low.
   Therefore, improvement of the power generation efficiency will be the
   research objective to scientists all over the world. The maximum power
   point tracking (MPPT) is an effective way to improve efficiency.
   Existing maximum power point tracking methods are incremental
   conductance method, perturbation and observation method, voltage
   feedback method and so on. Perturbation and observation method has the
   simple, easy to understand and the most common structure. In this paper,
   the nonlinear phenomena of buck-boost DC-DC converting solar cell system
   with perturbation and observation method for MPPT, considering converter
   operation in continuous conduction mode (CCM) with inductor current by
   varying load resistance is studied. The system is from periodic
   steady-state into period-doubling bifurcation to the chaos of nonlinear
   phenomena. Firstly the solar cell maximum power tracking system and
   converter working principle are explained. Secondly, the construction of
   solar maximum power tracking with perturbation and observation method
   and buck-boost DC-DC converter equivalent circuit model, then
   mathematical model is derived, and simulated by MATLAB / SIMULINK
   software package. Through the time-domain waveforms, phase plane and
   bifurcation diagrams, to explore the changing of load resistance system
   from a periodic steady-state into the chaos of nonlinear phenomena,
   chaos evolution mechanism of the system is analyzed.},
ISBN = {978-1-60595-385-4},
Unique-ID = {WOS:000389763200022},
}

@inproceedings{ WOS:000386910100036,
Author = {Chik, T. N. T. and Zakaria, M. F. and Remali, M. A. and Yusoff, N. A.},
Book-Group-Author = {IOP},
Title = {Vibration Response of Multi Storey Building Using Finite Element
   Modelling},
Booktitle = {SOFT SOIL ENGINEERING INTERNATIONAL CONFERENCE 2015 (SEIC2015)},
Series = {IOP Conference Series-Materials Science and Engineering},
Year = {2016},
Volume = {136},
Note = {Soft Soil Engineering International Conference (SEIC), Langkawi,
   MALAYSIA, OCT 27-29, 2015},
Organization = {Res Ctr Soft Soil Malaysia; Univ Katolik Parahyangan, Off Res Innovat
   Commercializat \& Consultancy, Fac Civil \& Environm Eng, Dept
   Infrastructure \& Geomat Eng},
Abstract = {Interaction between building, type of foundation and the geotechnical
   parameter of ground may trigger a significant effect on the building. In
   general, stiffer foundations resulted in higher natural frequencies of
   the building-soil system and higher input frequencies are often
   associated with other ground. Usually, vibrations transmitted to the
   buildings by ground borne are often noticeable and can be felt. It might
   affect the building and become worse if the vibration level is not
   controlled. UTHM building is prone to the ground borne vibration due to
   closed distance from the main road, and the construction activities
   adjacent to the buildings. This paper investigates the natural frequency
   and vibration mode of multi storey office building with the presence of
   foundation system and comparison between both systems. Finite element
   modelling (FEM) package software of LUSAS is used to perform the
   vibration analysis of the building. The building is modelled based on
   the original plan with the foundation system on the structure model. The
   FEM results indicated that the structure which modelled with rigid base
   have high natural frequency compare to the structure with foundation
   system. These maybe due to soil structure interaction and also the
   damping of the system which related to the amount of energy dissipated
   through the foundation soil. Thus, this paper suggested that modelling
   with soil is necessary to demonstrate the soil influence towards
   vibration response to the structure.},
DOI = {10.1088/1757-899X/136/1/012037},
Article-Number = {012037},
ISSN = {1757-8981},
ORCID-Numbers = {TUAN CHIK, TUAN NORHAYATI/0000-0003-2800-1139},
Unique-ID = {WOS:000386910100036},
}

@article{ WOS:000371921100005,
Author = {Cristelo, N. and Felix, C. and Lopes, M. L. and Dias, M.},
Title = {Monitoring and numerical modelling of an instrumented mechanically
   stabilised earth wall},
Journal = {GEOSYNTHETICS INTERNATIONAL},
Year = {2016},
Volume = {23},
Number = {1},
Pages = {48-61},
Abstract = {The use of mechanically stabilised earth (MSE) structures is becoming
   widespread, especially for transportation-related infrastructure. The
   present study is a contribution to a better understanding of MSE wall
   structural performance, through the analysis of measurements obtained
   during the construction of a MSE wall for the A4 Portuguese highway,
   between Amarante and Vila Real - and the development of a numerical
   model based on the finite-element method. The monitoring programme and
   equipment are described. The numerical model was developed using a
   commercially available finite-element software package together with the
   Mohr-Coulomb elastoplastic constitutive model for the soils. The
   numerical model was calibrated against measured data. Finally, a
   parametric analysis was performed to quantify some of the most
   significant parameters that influence the general behaviour of the wall.},
DOI = {10.1680/jgein.15.00032},
ISSN = {1072-6349},
EISSN = {1751-7613},
ResearcherID-Numbers = {Cristelo, Nuno Miguel/B-3636-2010
   lopes, maria l/E-7713-2013
   Lopes, Maria/GXF-7507-2022
   Félix, Carlos/ABC-1515-2021},
ORCID-Numbers = {Cristelo, Nuno Miguel/0000-0002-3600-1094
   lopes, maria l/0000-0002-2390-4825
   Félix, Carlos/0000-0002-2630-4972},
Unique-ID = {WOS:000371921100005},
}

@inproceedings{ WOS:000382936800019,
Author = {Dimova, Tatyana and Marinova, Mariya and Aprahamian, Bohos},
Book-Group-Author = {IEEE},
Title = {Assessment of the Influence of the Magnetic Filter Type on the Magnetic
   Field of a Separator Type MCR-5},
Booktitle = {2016 19TH INTERNATIONAL SYMPOSIUM ON ELECTRICAL APPARATUS AND
   TECHNOLOGIES (SIELA)},
Series = {International Symposium on Electrical Apparatus and Technologies},
Year = {2016},
Note = {19th International Symposium on Electrical Apparatus and Technologies
   (SIELA), Bourgas, BULGARIA, MAY 29-JUN 01, 2016},
Abstract = {The magnetic separation is a process of great practical relevance. The
   specifics of the separated material determine the type of the used
   separator. The aim is to achieve the maximum degree of purification of
   the processed product. The article examines a construction of real
   magnetic separator type MCR 5 with a specific problem defined by the
   practice. The analysis was conducted using a specially developed
   mathematical models based on the finite elements method. Models of the
   separator's structure with ferrite material concentrators and gaps of
   non-magnetic material were examined as well as those with different
   orientation of the magnetic induction vector of the individual permanent
   magnets. The stationary magnetic field of the system is analyzed as a
   flat parallel system, taking into account the nonlinear properties of
   the materials, which are separated by the separating apparatus. Also are
   examined the influence of the thickness and the material of the outer
   body (housing) of the separator. The distribution of the field is
   modeled numerically using the finite elements method and the software
   package FEMM 4.2. The results allow to optimize the structure of the
   magnetic separator and to achieve the highest level of purification.},
ISSN = {1314-6297},
ISBN = {978-1-4673-9522-9},
ResearcherID-Numbers = {Dimova, Tatyana/C-2978-2018
   Aprahamian, Bohos/I-7628-2012
   Marinova, Mariya/V-7862-2017},
ORCID-Numbers = {Dimova, Tatyana/0000-0001-8788-0220
   Aprahamian, Bohos/0000-0003-3595-1861
   Marinova, Mariya/0000-0001-9863-6434},
Unique-ID = {WOS:000382936800019},
}

@inproceedings{ WOS:000391523100031,
Author = {Edenhofer, Sarah and Raedler, Simon and Hoss, Manuel and von Mammen,
   Sebastian},
Editor = {Elnikety, S and Lewis, PR and Mullerschloer, C},
Title = {Self-organised Construction with Revit},
Booktitle = {2016 IEEE 1ST INTERNATIONAL WORKSHOPS ON FOUNDATIONS AND APPLICATIONS OF
   SELF{*} SYSTEMS (FAS{*}W)},
Year = {2016},
Pages = {160-161},
Note = {1st IEEE International Workshop on Foundations and Applications of
   Self-{*} Systems (FAS-W), Augsburg, GERMANY, SEP 12-16, 2016},
Organization = {IEEE; IEEE Comp Soc; ITG; VDE; Augsburg Univ},
Abstract = {Due to innovations in software, robotics and 3D printing, self-organised
   construction is within reach. It bears great potential for the automatic
   generation of a wide variety of designs, their integration into the
   built environment, their structural and automatised optimisation, as
   well as their dynamic adaptation over long periods of time. In this
   paper, motivated by the latest empirical findings on the construction
   methods of social insects, we present a software pipeline for the
   generation of architectural designs based on self-organisation. A
   probabilistic, grid-based multi-agent system that implements a flexible
   stigmergy-based behavioural construction model generates
   three-dimensional structures. Next, the generated structures could be
   evaluated in terms of their energy efficiency and these results be fed
   into an optimisation engine to improve the local behaviours of the
   construction agents. For visualisation and evaluation of the generated
   designs we utilise the API of the framework Revit 20161, a software for
   Building Information Modelling (BIM) provided by Autodesk.},
DOI = {10.1109/FAS-W.2016.44},
ISBN = {978-1-5090-3651-6},
ORCID-Numbers = {Radler, Simon/0000-0003-1491-7170},
Unique-ID = {WOS:000391523100031},
}

@article{ WOS:000459646200010,
Author = {Gawron, Piotr and Ostaszewski, Marek and Satagopam, Venkata and Gebel,
   Stephan and Mazein, Alexander and Kuzma, Michal and Zorzan, Simone and
   McGee, Fintan and Otjacques, Benoit and Balling, Rudi and Schneider,
   Reinhard},
Title = {MINERVA-a platform for visualization and curation of molecular
   interaction networks},
Journal = {NPJ SYSTEMS BIOLOGY AND APPLICATIONS},
Year = {2016},
Volume = {2},
Abstract = {Our growing knowledge about various molecular mechanisms is becoming
   increasingly more structured and accessible. Different repositories of
   molecular interactions and available literature enable construction of
   focused and high-quality molecular interaction networks. Novel tools for
   curation and exploration of such networks are needed, in order to foster
   the development of a systems biology environment. In particular,
   solutions for visualization, annotation and data cross-linking will
   facilitate usage of network-encoded knowledge in biomedical research. To
   this end we developed the MINERVA (Molecular Interaction NEtwoRks
   VisuAlization) platform, a standalone webservice supporting curation,
   annotation and visualization of molecular interaction networks in
   Systems Biology Graphical Notation (SBGN)-compliant format. MINERVA
   provides automated content annotation and verification for improved
   quality control. The end users can explore and interact with hosted
   networks, and provide direct feedback to content curators. MINERVA
   enables mapping drug targets or overlaying experimental data on the
   visualized networks. Extensive export functions enable downloading areas
   of the visualized networks as SBGN-compliant models for efficient reuse
   of hosted networks. The software is available under Affero GPL 3.0 as a
   Virtual Machine snapshot, Debian package and Docker instance at
   http://r3lab.uni.lu/web/minerva-website/. We believe that MINERVA is an
   important contribution to systems biology community, as its architecture
   enables set-up of locally or globally accessible SBGN-oriented
   repositories of molecular interaction networks. Its functionalities
   allow overlay of multiple information layers, facilitating exploration
   of content and interpretation of data. Moreover, annotation and
   verification workflows of MINERVA improve the efficiency of curation of
   networks, allowing life-science researchers to better engage in
   development and use of biomedical knowledge repositories.},
DOI = {10.1038/npjsba.2016.20},
Article-Number = {16020},
EISSN = {2056-7189},
ResearcherID-Numbers = {Mazein, Alexander/C-1369-2013
   Schneider, Reinhard/C-5441-2009
   },
ORCID-Numbers = {Mazein, Alexander/0000-0001-7137-4171
   Schneider, Reinhard/0000-0002-8278-1618
   Satagopam, Venkata/0000-0002-6532-5880
   Ostaszewski, Marek/0000-0003-1473-370X
   McGee, Fintan/0000-0001-7398-2664
   Balling, Rudi/0000-0003-2902-5650},
Unique-ID = {WOS:000459646200010},
}

@article{ WOS:000387347700019,
Author = {Ghysels, Pieter and Li, Xiaoye S. and Rouet, Francois-Henry and
   Williams, Samuel and Napov, Artem},
Title = {AN EFFICIENT MULTICORE IMPLEMENTATION OF A NOVEL HSS-STRUCTURED
   MULTIFRONTAL SOLVER USING RANDOMIZED SAMPLING},
Journal = {SIAM JOURNAL ON SCIENTIFIC COMPUTING},
Year = {2016},
Volume = {38},
Number = {5},
Pages = {S358-S384},
Abstract = {We present a sparse linear system solver that is based on a multifrontal
   variant of Gaussian elimination and exploits low-rank approximation of
   the resulting dense frontal matrices. We use hierarchically
   semiseparable (HSS) matrices, which have low-rank off-diagonal blocks,
   to approximate the frontal matrices. For HSS matrix construction, a
   randomized sampling algorithm is used together with interpolative
   decompositions. The combination of the randomized compression with a
   fast ULV HSS factorization leads to a solver with lower computational
   complexity than the standard multifrontal method for many applications,
   resulting in speedups up to sevenfold for problems in our test suite.
   The implementation targets many-core systems by using task parallelism
   with dynamic runtime scheduling. Numerical experiments show performance
   improvements over state-of-the-art sparse direct solvers. The
   implementation achieves high performance and good scalability on a range
   of modern shared memory parallel systems, including the Intel Xeon Phi
   (MIC). The code is part of a software package called STRUMPACK
   (STRUctured Matrices PACKage), which also has a distributed memory
   component for dense rank-structured matrices.},
DOI = {10.1137/15M1010117},
ISSN = {1064-8275},
EISSN = {1095-7197},
ORCID-Numbers = {Napov, Artem/0000-0003-0064-4079},
Unique-ID = {WOS:000387347700019},
}

@inproceedings{ WOS:000386635200001,
Author = {He, Lei and Zhang, Xiaoning and Cheng, Zijing and Jiang, Yajie},
Book-Group-Author = {IEEE},
Title = {Design and Implementation of SDN/IP Hybrid Space Information Network
   Prototype},
Booktitle = {2016 IEEE/CIC INTERNATIONAL CONFERENCE ON COMMUNICATIONS IN CHINA (ICCC
   WORKSHOPS)},
Series = {IEEE International Conference on Communications in China Workshops},
Year = {2016},
Note = {IEEE/CIC International Conference on Communication in China (ICCC),
   Chengdu, PEOPLES R CHINA, JUL 27-29, 2016},
Organization = {IEEE; CIC},
Abstract = {Software Defined Networking (SDN) promises to ease design, operation and
   management of communication networks. For years, however, most networks
   have been designed, deployed and managed as IP-based systems. It is
   necessary to introduce SDN in IP backbones to construct SDN/IP hybrid
   space information network.
   In this paper, we design and implement a new architecture of the SDN/IP
   hybrid space information network. It helps to achieve a collaborative
   management between the IP network and the SDN network through the model.
   So, in space information network, we can use traditional IP-based system
   in ground internet and use centralized SDN in satellite network. By
   combining Quagga and SDN controller, the entire flexible centralized SDN
   network is abstracted as a traditional distributed control IP router.
   Thus, the SDN controller can get routing tables from Quagga and forward
   routing protocol packets over the network. At the same time, other IP
   subnet can be aware of the existing SDN subnet, and its location. We
   designed our SDN/IP hybrid network prototype combining the POX as the
   controller and the Mininet for topology construction, all with Open Flow
   1.0 support. And results show that we achieve the interconnection
   between SDN and IP subnets efficiently.},
ISSN = {2377-8644},
Unique-ID = {WOS:000386635200001},
}

@article{ WOS:000391853100005,
Author = {Heinlein, Alexander and Klawonn, Axel and Rheinbach, Oliver},
Title = {A PARALLEL IMPLEMENTATION OF A TWO-LEVEL OVERLAPPING SCHWARZ METHOD WITH
   ENERGY-MINIMIZING COARSE SPACE BASED ON TRILINOS},
Journal = {SIAM JOURNAL ON SCIENTIFIC COMPUTING},
Year = {2016},
Volume = {38},
Number = {6},
Pages = {C713-C747},
Abstract = {We describe a new implementation of a two-level overlapping Schwarz
   preconditioner with energy-minimizing coarse space (GDSW: generalized
   Dryja-Smith-Widlund) and show numerical results for an additive and a
   hybrid additive-multiplicative version. Our parallel implementation
   makes use of the Trilinos software library and provides a framework for
   parallel two-level Schwarz methods. We show parallel scalability for
   two- and three-dimensional scalar second-order elliptic and linear
   elasticity problems for several thousands of cores. We also discuss
   techniques for the parallel construction of coarse spaces which are also
   of interest for other parallel preconditioners and discretization
   methods using energy minimizing coarse functions. We finally show an
   application in monolithic fluid-structure interaction, where significant
   improvements are achieved compared to a standard algebraic, one-level
   overlapping Schwarz method.},
DOI = {10.1137/16M1062843},
ISSN = {1064-8275},
EISSN = {1095-7197},
ResearcherID-Numbers = {Heinlein, Alexander/AAS-9712-2020
   Klawonn, Axel/P-8705-2014
   },
ORCID-Numbers = {Heinlein, Alexander/0000-0003-1578-8104
   Klawonn, Axel/0000-0003-4765-7387
   Rheinbach, Oliver/0000-0002-9310-8533},
Unique-ID = {WOS:000391853100005},
}

@article{ WOS:000370679800026,
Author = {Huang, Tao and Yan, Siyu and Yang, Fan and Pan, Tian and Liu, Jiang},
Title = {Building SDN-Based Agricultural Vehicular Sensor Networks Based on
   Extended Open vSwitch},
Journal = {SENSORS},
Year = {2016},
Volume = {16},
Number = {1},
Month = {JAN},
Abstract = {Software-defined vehicular sensor networks in agriculture, such as
   autonomous vehicle navigation based on wireless multi-sensor networks,
   can lead to more efficient precision agriculture. In SDN-based vehicle
   sensor networks, the data plane is simplified and becomes more efficient
   by introducing a centralized controller. However, in a wireless
   environment, the main controller node may leave the sensor network due
   to the dynamic topology change or the unstable wireless signal, leaving
   the rest of network devices without control, e.g., a sensor node as a
   switch may forward packets according to stale rules until the controller
   updates the flow table entries. To solve this problem, this paper
   proposes a novel SDN-based vehicular sensor networks architecture which
   can minimize the performance penalty of controller connection loss. We
   achieve this by designing a connection state detection and self-learning
   mechanism. We build prototypes based on extended Open vSwitch and Ryu.
   The experimental results show that the recovery time from controller
   connection loss is under 100 ms and it keeps rule updating in real time
   with a stable throughput. This architecture enhances the survivability
   and stability of SDN-based vehicular sensor networks in precision
   agriculture.},
DOI = {10.3390/s16010108},
Article-Number = {108},
ISSN = {1424-8220},
Unique-ID = {WOS:000370679800026},
}

@inproceedings{ WOS:000379703400006,
Author = {Idris, A. and Huynh, B. P. and Abdullah, Z.},
Book-Group-Author = {ASME},
Title = {The Simulation of Natural Ventilation of Buildings with Different
   Location of Windows/Openings},
Booktitle = {PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS
   AND EXPOSITION, 2015, VOL 6B},
Year = {2016},
Note = {ASME International Mechanical Engineering Congress and Exposition
   (IMECE2015), Houston, TX, NOV 13-19, 2015},
Organization = {ASME},
Abstract = {Ventilation is a process of changing air in an enclosed space. Air
   should continuously be withdrawn and replaced by fresh air from a clean
   external source to maintain internal good air quality, which may
   referred to air quality within and around the building structures. In
   natural ventilation the air flow is due through cracks in the building
   envelope or purposely installed openings. Its can save significant
   amount of fossil fuel based energy by reducing the needs for mechanical
   ventilation and air conditioning. Numerical predictions of air
   velocities and the flow patterns inside the building are determined. To
   achieve optimum efficiency of natural ventilation, the building design
   should start from the climatic conditions and orography of the
   construction to ensure the building permeability to the outside airflow
   to absorb heat from indoors to reduce temperatures. Effective
   ventilation in a building will affects the occupant health and
   productivity. In this work, computational simulation is performed on a
   real-sized box-room with dimensions 5 m x 5 m x 5 m. Single-sided
   ventilation is considered whereby openings are located only on the same
   wall. Two opening of the total area 4 m(2) are differently arranged,
   resulting in 16 configurations to be investigated. A logarithmic wind
   profile upwind of the building is employed. A commercial Computational
   Fluid Dynamics (CFD) software package CFD-ACE of ESI group is used. A
   Reynolds Average Navier Stokes (RANS) turbulence model \& LES turbulence
   model are used to predict the air's flow rate and air flow pattern. The
   governing equations for large eddy motion were obtained by filtering the
   Navier-Stokes and continuity equations. The computational domain was
   constructed had a height of 4H, width of 9H and length of 13H (H=5m),
   sufficiently large to avoid disturbance of air flow around the building.
   From the overall results, the lowest and the highest ventilation rates
   were obtained with windward opening and leeward opening respectively.
   The location and arrangement of opening affects ventilation and air flow
   pattern.},
Article-Number = {V06BT07A006},
ISBN = {978-0-7918-5744-1},
ResearcherID-Numbers = {IDRIS, ABUBAKAR MOHAMMED/J-2717-2017},
ORCID-Numbers = {IDRIS, ABUBAKAR MOHAMMED/0000-0001-5995-0295},
Unique-ID = {WOS:000379703400006},
}

@inproceedings{ WOS:000390033000001,
Author = {Ikau, Roseline and Joseph, Corina and Tawie, Rudy},
Editor = {Abbas, MY and Thani, SKSO},
Title = {Factors Influencing Waste Generation in the Construction Industry in
   Malaysia},
Booktitle = {AMER INTERNATIONAL CONFERENCE ON QUALITY OF LIFE: QUALITY OF LIFE IN THE
   BUILT \& NATURAL ENVIRONMENT 4 (AICQOL2016MEDAN)},
Series = {Procedia Social and Behavioral Sciences},
Year = {2016},
Volume = {234},
Pages = {11-18},
Note = {4th AMER International Conference on Quality of Life (AicQoL), Univ
   Sumatera Utara, Fac Engn, Dept Architecture, Medan, INDONESIA, FEB
   25-27, 2016},
Organization = {Assoc Malaysian Environm Behaviour Researchers},
Abstract = {The increasing amount of material wastes generated from construction
   activities is becoming a challenging issue to construction site
   operators. The Malaysian construction industry carries on to produce,
   benefiting the country's economy and providing necessary infrastructure.
   This paper aims to determine the current various factors causing
   construction waste generation in the Malaysian construction sector. The
   study was carried out through structured questionnaire focusing to
   contractors engaged in various types of construction projects in
   Malaysia. The list of contractors took from the CIDB directory. Data was
   analyzed with Statistical Software Package (SPSS). The results obtained
   to provided some insights for further work. (C) 2016 The Authors.
   Published by Elsevier Ltd.},
DOI = {10.1016/j.sbspro.2016.10.213},
ISSN = {1877-0428},
ResearcherID-Numbers = {Joseph, Corina/U-5761-2019
   Joseph, Corina/R-9521-2016
   },
ORCID-Numbers = {Joseph, Corina/0000-0001-8199-6791
   Tawie, Rudy/0000-0003-0320-954X},
Unique-ID = {WOS:000390033000001},
}

@inproceedings{ WOS:000388117501081,
Author = {Kumar, Sushil and Nag, Rajiv},
Editor = {Hoda, MN},
Title = {An Approach for Multimedia Software Size Estimation},
Booktitle = {PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON
   COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT},
Year = {2016},
Pages = {1321-1326},
Note = {3rd International Conference on Computing for Sustainable Global
   Development (INDIACom), New Delhi, INDIA, MAR 16-18, 2016},
Organization = {GGSIP Univ; Govt India, Minist Sci \& Technol, Dept Sci \& Technol;
   Council Sci \& Ind Res; All India Council Tech Educ; Inst Elect \&
   Telecommunicat Engineers, Delhi Ctr; Inst Engn \& Technol, Delhi Local
   Networks; Jagdishprasad Jhabarmal Tibrewala Univ; Bharati Vidyapeeths
   Inst Comp Applicat \& Management; ISTE, Delhi Sect},
Abstract = {Although, the Function Point Analysis has offered an idea to estimate
   the size of software in both planned and existing one {[}1], use of
   multimedia technology has provided a different direction for delivering
   instruction. The interested users in the era of Graphics User
   Interfaces, are being attracted, gets benefited and have new learning
   capabilities and this is reason the two-way multimedia training is a
   process, rather than being termed as a technology. Multimedia software
   Developer should use suitable methods for designing the package which
   will not only enhance its capabilities but will also be user friendly.
   Mixed media projects can he utilized to present data in combined form in
   a number of ways to energize routes hypermedia strategies with
   guideline. Great presentations can he made when they depend on
   psychological targets that create attention on the learning of themes at
   distinctive levels of appreciation.
   All media segments, for example, design, sound and video and so forth it
   adds to learning. While developing multimedia programs, developer should
   focus mainly on three point; firstly grabbing the user's attention;
   secondly it should ease the user to find and organize all necessary
   information and finally to integrate all information into the user's
   knowledge data bank.
   All elements like text, graphics icon and audio visual elements need to
   be utilized at the maximum to create visual appeal and organized in
   structured program, and thus for the purpose of systematic organization
   it is convenient to divide the screen into functional areas by the
   Developers.
   Class and sequence diagrams were utilized solely for the generation of
   FP counts {[}2]. A similar technique for creating automated counts using
   COSMIC FFP methods likewise depended on UML graphs {[}3]. But, neither
   of these strategies measures will be used for the estimation mixed media
   software the developers.},
ISBN = {978-9-3805-4419-9},
Unique-ID = {WOS:000388117501081},
}

@inproceedings{ WOS:000387827200003,
Author = {Kuramitsu, Kimio},
Editor = {Visser, E and MurphyHill, E and Lopes, C},
Title = {Nez: Practical Open Grammar Language},
Booktitle = {ONWARD!'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL SYMPOSIUM ON NEW
   IDEAS, NEW PARADIGMS, AND REFLECTIONS ON PROGRAMMING AND SOFTWARE},
Year = {2016},
Pages = {29-42},
Note = {ACM International Symposium on New Ideas, New Paradigms, and Reflections
   on Programming and Software (Onward), Amsterdam, NETHERLANDS, NOV 02-04,
   2016},
Organization = {Assoc Comp Machinery; ACM SIGPLAN},
Abstract = {Nez is a PEG(Parsing Expressing Grammar)-based open grammar language
   that allows us to describe complex syntax constructs without action
   code. Since open grammars are declarative and free from a host
   programming language of parsers, software engineering tools and other
   parser applications can reuse once-defined grammars across programming
   languages.
   A key challenge to achieve practical open grammars is the expressiveness
   of syntax constructs and the resulting parser performance, as the
   traditional action code approach has provided very pragmatic solutions
   to these two issues. In Nez, we extend the symbol-based state management
   to recognize context-sensitive language syntax, which often appears in
   major programming languages. In addition, the Abstract Syntax Tree
   constructor allows us to make flexible tree structures, including the
   left-associative pair of trees. Due to these extensions, we have
   demonstrated that Nez can parse many grammars of practical programming
   languages. Nez can generate various types of parsers since all
   Nez operations are independent of a specific parser language. To
   highlight this feature, we have implemented Nez with dynamic parsing,
   which allows users to integrate a Nez parser as a parser library that
   loads a grammar at runtime. To achieve its practical performance, Nez
   operators are assembled into low-level virtual machine instructions,
   including automated state modifications when backtracking, transactional
   controls of AST construction, and efficient memoization in packrat
   parsing. We demonstrate that Nez dynamic parsers achieve very
   competitive performance compared to existing efficient parser
   generators.},
DOI = {10.1145/2986012.2986019},
ISBN = {978-1-4503-4076-2},
Unique-ID = {WOS:000387827200003},
}

@inproceedings{ WOS:000391640800005,
Author = {Li, Liyun and Qiu, Zhongwang and Dong, Yingying and Du, Xiuli},
Editor = {Alpatov, S and Prentkovskis, O and Sterling, RL and Kaliampakos, D},
Title = {Risk caused by construction of the metro shaft adjacent to building and
   its control measure},
Booktitle = {15TH INTERNATIONAL SCIENTIFIC CONFERENCE UNDERGROUND URBANISATION AS A
   PREREQUISITE FOR SUSTAINABLE DEVELOPMENT},
Series = {Procedia Engineering},
Year = {2016},
Volume = {165},
Pages = {40-48},
Note = {15th World Conference of the Associated Research Centers for the Urban
   Underground Space - Underground Urbanization as a Prerequisite for
   Sustainable Development (ACUUS), St Petersburg, RUSSIA, SEP 12-15, 2016},
Abstract = {Before the construction of Metro station, many accessory structures need
   to be constructed firstly, including shaft, Cross channel, and metro
   station pilot tunnel, etc. Many Engineering cases show that the risks of
   the circumstances around the Metro station caused by the excavation of
   these accessory structures are the major component of the metro
   construction risk. In this paper, the risk caused by construction of the
   metro shaft adjacent to building was studied, which located at the
   crossings of ziyou Road and Renmin Street, Changchun, China. Firstly,
   the major risk was introduced and the construction mechanic behavior was
   researched by using MIDAS/GTS finite element software package. Then, a
   control method was proposed to eliminate this risk, which was verified
   by using finite element software. Finally, the proposed measure was also
   validated with the result of construction monitoring for this project.
   This works focus on studying the excavation mechanics behavior of metro
   shaft and its influence on the around environment, and these conclusions
   are useful for providing a basis for similar project construction in the
   future. (C) 2016 Published by Elsevier Ltd.},
DOI = {10.1016/j.proeng.2016.11.733},
ISSN = {1877-7058},
Unique-ID = {WOS:000391640800005},
}

@article{ WOS:000389708700075,
Author = {Liu, Minzhe and Li, Hefu and Yu, Weixing and Wang, Taisheng and Liu,
   Zhenyu and Desmulliez, Marc. P. Y.},
Title = {Influence of electrode types on the electrohydrodynamic instability
   patterning process: a comparative study},
Journal = {RSC Advances},
Year = {2016},
Volume = {6},
Number = {113},
Pages = {112300-112306},
Abstract = {This article studies the effect that different types of patterned
   electrodes have on the electrohydrodynamic instability patterning
   (EHDIP) process for the faithful replication of micro-and
   nanostructures. Two types of patterned electrodes are studied. One is
   fully conductive, i.e. both pattern and substrate are conductive. The
   other type has conductive microstructures fabricated on a dielectric
   substrate. By employing the COMSOL (TM) Multiphysics software package, a
   rigorous numerical simulation of the EHDIP process has been carried out
   for both types of electrodes. The simulation results show that both
   electrodes can realize a faithful replication of the micro-and
   nanostructures once the variable, Delta E/Delta x, reaches the critical
   value. Moreover, it is demonstrated that a fully conductive template is
   preferred if a small polymer film thickness is employed; a partially
   conductive electrode is preferred for larger film thickness. These
   results provide guidelines for the better control of the EHDIP process
   in order to realize the perfect pattern replication of structures for a
   variety of applications in MEMS or micro/nanofluidics.},
DOI = {10.1039/c6ra05596f},
ISSN = {2046-2069},
ResearcherID-Numbers = {Desmulliez, Marc/D-5841-2012
   Yu, Weixing/G-3658-2012
   Liu, Zhenyu/E-9551-2010},
ORCID-Numbers = {Desmulliez, Marc/0000-0002-2441-1598
   Yu, Weixing/0000-0002-3216-526X
   },
Unique-ID = {WOS:000389708700075},
}

@inproceedings{ WOS:000388374904004,
Author = {Lyke, James and Freden, Jerker and Ahlberg, Mikael and Bruhn, Fredrik
   and Preble, Jeff},
Book-Group-Author = {IEEE},
Title = {Architectural Framework and Toolflow Concepts for Rapidly Composable
   Wireless Spacecraft},
Booktitle = {2016 IEEE AEROSPACE CONFERENCE},
Series = {IEEE Aerospace Conference Proceedings},
Year = {2016},
Note = {IEEE Aerospace Conference, Big Sky, MT, MAR 05-12, 2016},
Organization = {IEEE},
Abstract = {This paper provides a glimpse of work being done in a nearly decade-long
   joint US/Sweden and spacecraft research collaboration exploring rapid
   spacecraft design based on modular components whose simplicity and
   composability motivate the exploitation of modern design automation
   approaches and concepts that have been popularized in consumer and
   industrial online commerce. Modular systems are engineered to minimize
   tight couplings between components, with an aim of permitting
   interchangeability of elements and free composability to form many
   different possible system designs. Physical wiring often contributes to
   the complexity, and reducing or eliminating it aids in the objectives of
   modularity. In this paper, we consider an approach to create modular
   spacecraft, with a particular emphasis on cube satellites having a 6U
   form factor, based on a composition of ``nearly wireless{''} elements.
   Since efficient power delivery remains problematic, our scheme permits
   the introduction of two terminals (analogous to residential wall
   outlets) for the sole purpose of power access. All other functions are
   delivered through a wireless network self organized based on a given
   collection of components necessary to form a particular spacecraft
   design. Panel structures can be prewired for power-only distribution,
   eliminating the need for custom wiring harnesses. In the proposed 6U
   Cubesat concept, a primary flat surface (similar to 200mm x 300mm)
   substrate is the basis of a ``dinner tray{''} convention. Modules are
   added to the pegboard-like ``dinner tray{''} by plugging them in
   topside, forming a single unified planar interface for electrical,
   mechanical, and thermal integration. Electrical power blocks energize
   the substrate, processor modules provide wireless connection access
   points, and all other modules extract power from the strategically
   distributed contact points throughout the substrate. Once powered, these
   modules are networked through a ``join and discovery{''} mechanism which
   provides a dynamically extensible application programming interface
   (API) expressed in the form of electronic data sheets. A sophisticated
   middleware layer (running on the same processing modules that provide
   the wireless ``hotspots{''}) matches applications using a
   ``brokerage{''} publish and subscribe mechanism. When the dependencies
   of each application is satisfied through the existence of suitable
   modules, the application is activated. The entire application suite is a
   hierarchy implemented as a direct acyclic graph (DAG) of these
   dependencies that when satisfied form a viable system (in this case a
   spacecraft) design. The implications of the method are profound in that
   it is possible to rapidly develop an virtually infinite variety of
   system designs given a sufficiently large collection of building block
   hardware and software applications. This paper describes a pushbutton
   toolflow (PBTF) motivated by concepts electronic design automation, only
   they are now extended to encompass satellite (or other system) designs.
   This pushbutton toolflow involves a configurator concept through which
   the user could negotiate (with relatively simple wizard like dialogs) a
   variety of viable spacecraft designs. The tool would access a
   multi-vendor ``electronic store{''}, where prebuilt components and
   applications are marshaled into DAGs corresponding to buildable systems.
   The approach is analogous to a shopping cart metaphor, in which system
   cost and delivery times can be tracked based on the collection of
   dependencies formed by the selection of particular components need to
   satisfy design constraints.
   Documentation, including bill of materials, data sheets, and a full work
   breakdown structure can be produced as a byproduct. The toolflow itself
   could be extended to encompass automatic program generation and
   three-dimensional printing approaches to permit automated, customizable
   software and hardware generation (respectively), to complement the
   catalog of available components. The toolflow has other profound
   impacts, such as the ability to publish ``recipes{''} (i.e., useful
   system representations) for use by other users, the ability to
   automatically coordinate communications (through a ``space dialtone{''}
   concept) between orbiting platforms in ground station networks, and
   techniques that arrange for the construction and launch of these
   configurator produced spacecraft design by cooperating third-party
   networks.},
ISSN = {1095-323X},
ISBN = {978-1-4673-7676-1},
Unique-ID = {WOS:000388374904004},
}

@inproceedings{ WOS:000385794000069,
Author = {Marrero, Juan},
Editor = {Angeli, GZ and Dierickx, P},
Title = {Optomechanical design software for segmented mirrors},
Booktitle = {MODELING, SYSTEMS ENGINEERING, AND PROJECT MANAGEMENT FOR ASTRONOMY VII},
Series = {Proceedings of SPIE},
Year = {2016},
Volume = {9911},
Number = {1},
Note = {Conference on Modeling, Systems Engineering, and Project Management for
   Astronomy VII, Edinburgh, SCOTLAND, JUN 26-28, 2016},
Organization = {SPIE},
Abstract = {The software package presented in this paper, still under development,
   was born to help analyzing the influence of the many parameters involved
   in the design of a large segmented mirror telescope. In summary, it is a
   set of tools which were added to a common framework as they were needed.
   Great emphasis has been made on the graphical presentation, as
   scientific visualization nowadays cannot be conceived without the use of
   a helpful 3d environment, showing the analyzed system as close to
   reality as possible. Use of third party software packages is limited to
   ANSYS, which should be available in the system only if the FEM results
   are needed. Among the various functionalities of the software, the next
   ones are worth mentioning here: automatic 3d model construction of a
   segmented mirror from a set of parameters, geometric ray tracing,
   automatic 3d model construction of a telescope structure around the
   defined mirrors from a set of parameters, segmented mirror human access
   assessment, analysis of integration tolerances, assessment of segments
   collision, structural deformation under gravity and thermal variation,
   mirror support system analysis including warping harness mechanisms,
   etc.},
DOI = {10.1117/12.2232950},
Article-Number = {99112A},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-0201-4; 978-1-5106-0202-1},
Unique-ID = {WOS:000385794000069},
}

@inproceedings{ WOS:000392339700018,
Author = {Mkrtychev, Oleg and Dzhinchvelashvili, Guram and Busalova, Marina},
Editor = {Andreev, V},
Title = {Normative approaches to structural design calculations in a non-linear
   framework},
Booktitle = {5TH INTERNATIONAL SCIENTIFIC CONFERENCE INTEGRATION, PARTNERSHIP AND
   INNOVATION IN CONSTRUCTION SCIENCE AND EDUCATION},
Series = {MATEC Web of Conferences},
Year = {2016},
Volume = {86},
Note = {5th International Scientific Conference on Integration, Partnership and
   Innovation in Construction Science and Education (IPICSE), Moscow,
   RUSSIA, OCT 16-17, 2016},
Abstract = {This article considers the issue of taking non-linearity into account
   when performing design calculations for buildings and structures exposed
   to intense seismic impact. The existing rule codex 14.13330.2014 SNIP
   11-7-81{*} ``Construction in seismic areas{''} regulates the possible
   behavior beyond the elastic range with by introducing a damage tolerance
   factor. However, extensive research conducted by the authors of this
   article and other experts in the seismic resistance theory reveal that
   the values of this factor for buildings of various structural layouts
   must be corrected. The paper investigates the behavior of multi-element
   systems exposed to seismic impact, the structural layouts of them being
   as follows: wall supported slab systems, reinforced concrete rigid frame
   buildings and buildings with a metal space frame. Calculations were made
   which describe the behavior of the buildings exposed to an intense
   earthquake modeled in the form of accelerograms standardized as 8 points
   on the MSK 64 scale. The calculations were performed using the LS-DYNA
   software package which makes use of the direct dynamic method and
   provides means for making all the calculations in the time domain and
   for taking the physical, geometric and structural non-linearity into
   account. The results obtained indicate that wall-supported slab
   buildings and rigid frame RC buildings collapse due to emerging and
   developing plastic deformations, while metal frame buildings lose their
   stability before their bearing capability is exhausted. The research
   carried out by the authors enabled them to pinpoint the reason for the
   discrepancy between the obtained values of the damage tolerance factor
   and the values stipulated by the regulation, as well as to draw a
   conclusion that these values should be refined.},
DOI = {10.1051/matecconf/20168601018},
Article-Number = {01018},
ISSN = {2261-236X},
ResearcherID-Numbers = {Dudareva, Marina/P-2134-2018
   Mkrtychev, Oleg/Q-2370-2017},
ORCID-Numbers = {Dudareva, Marina/0000-0002-8352-0962
   },
Unique-ID = {WOS:000392339700018},
}

@inproceedings{ WOS:000389517400007,
Author = {Moreno-Delgado, Antonio and Duran, Francisco and Meseguer, Jose},
Editor = {Lucanu, D},
Title = {Towards Generic Monitors for Object-Oriented Real-Time Maude
   Specifications},
Booktitle = {REWRITING LOGIC AND ITS APPLICATIONS, WRLA 2016},
Series = {Lecture Notes in Computer Science},
Year = {2016},
Volume = {9942},
Pages = {118-133},
Note = {11th International Workshop on Rewriting Logic and Its Applications
   (WRLA), Eindhoven, NETHERLANDS, APR 02-03, 2016},
Abstract = {Non-Functional Properties (NFPs) are crucial in the design of software.
   Specification of systems is used in the very first phases of the
   software development process for the stakeholders to make decisions on
   which architecture or platform to use. These specifications may be
   analyzed using different formalisms and techniques, simulation being one
   of them. During a simulation, the relevant data involved in the analysis
   of the NFPs of interest can be measured using monitors. In this work, we
   show how monitors can be parametrically specified so that the
   instrumentation of specifications to be monitored can be automatically
   performed. We prove that the original specification and the
   automatically obtained specification with monitors are bisimilar by
   construction. This means that the changes made on the original system by
   adding monitors do not affect its behavior. This approach allows us to
   have a library of possible monitors that can be safely added to analyze
   different properties, possibly on different objects of our systems, at
   will.},
DOI = {10.1007/978-3-319-44802-2\_7},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-44802-2; 978-3-319-44801-5},
Unique-ID = {WOS:000389517400007},
}

@article{ WOS:000369462500007,
Author = {Nomura, Yuji and Tamura, Masahisa and Ozawa, Toshihiro},
Title = {Software Technology Offers Real-time Analysis of Super-fast
   Telecommunications at 200 Gbps},
Journal = {FUJITSU SCIENTIFIC \& TECHNICAL JOURNAL},
Year = {2016},
Volume = {52},
Number = {1},
Pages = {41-45},
Month = {JAN},
Abstract = {Smartphones and tablets have become widely and readily available, and
   the expanding scope of applications for data center use has stimulated
   the advancement of cloud services. Thus, it is becoming crucial to
   improve the quality of IT-network-based services. Meanwhile,
   ever-increasing volume of data traffic and further sophistication of
   system structures are leading to service disruptions such as slow data
   propagation speeds and difficulties in establishing network connections.
   For early detection and swift recovery of such service disruptions, it
   is essential to analyze and distinguish between their causes to know
   whether they are due to network (connection) quality or lie in
   application programs. This requires detailed, real-time behavioral
   analysis of each communication packet. For complicated cases, evidence
   of the errors may need to be accumulated for later in-depth analyses. In
   order to meet these requirements, Fujitsu Laboratories has developed
   technology that facilitates real-time analyses of communication packets
   on super-fast communication networks, as well as high-speed data
   accumulation and retrieval. The technology has been realized in the form
   of software that requires no expensive, dedicated hardware, and it is
   installed on a versatile system. This paper describes the software
   technology that enables analysis, accumulation and retrieval of data
   from high-speed communications, and it explains the system structure.},
ISSN = {0016-2523},
Unique-ID = {WOS:000369462500007},
}

@inproceedings{ WOS:000387566500201,
Author = {Papan, Daniel and Valaskova, Veronika},
Editor = {Drusa, M and Yilmaz, I and Marschalko, M and Coisson, E and Segalini, A},
Title = {Dynamic Response Analysis of the Ondrej Nepela Stadium Grandstands},
Booktitle = {WORLD MULTIDISCIPLINARY CIVIL ENGINEERING-ARCHITECTURE-URBAN PLANNING
   SYMPOSIUM 2016, WMCAUS 2016},
Series = {Procedia Engineering},
Year = {2016},
Volume = {161},
Pages = {1308-1315},
Note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning
   Symposium (WMCAUS), Prague, CZECH REPUBLIC, JUN 13-17, 2016},
Abstract = {Behaviour of the spectators in the grandstands during a sports match may
   generate excessive vibration that can cause problems with the
   serviceability limit state. Construction of modern stadium due to
   architectural design and improving the material properties of the
   elements are made more efficient. This type of structural design brings
   many benefits. On the other hand, the size of its vibration, caused
   mainly by coordinated motion spectators, became of great values, which
   may exceed the limit values given in the respective standards. In this
   article we are dealing with modal analysis of structures Ondrej Nepela
   stadium in Bratislava and finding natural frequencies adjacent
   grandstands. Global support system and his description are based on the
   original design project documents from 1988 and subsequent project
   documentation from 2009.
   Complete reconstruction of the stadium in shape as it looks today, was
   in 2009. Ground plan dimensions of the hall are 86.0 x 102.8 meters and
   maximum theoretical amount at mid-range is 23.3 meters. The mods of
   natural vibration and natural frequencies were calculated by the
   computational model in computer program Scia Engineer with using the
   Lanczos mathematical method. Software package Scia Engineer uses the
   principle of finite element method (FEM) - numerical method for solving
   boundary value of continuum mechanics. FEM solves a system of
   differential equations transformed to a system of linear algebraic
   equations. (C) 2016 The Authors. Published by Elsevier Ltd.},
DOI = {10.1016/j.proeng.2016.08.594},
ISSN = {1877-7058},
ResearcherID-Numbers = {Papán, Daniel/AAB-7695-2020
   Valaskova, Veronika/ABH-5614-2020
   },
ORCID-Numbers = {Papan, Daniel/0000-0003-0416-1162},
Unique-ID = {WOS:000387566500201},
}

@inproceedings{ WOS:000392339700045,
Author = {Poddaeva, Olga and Churin, Pavel and Dunichkin, Ilya},
Editor = {Andreev, V},
Title = {Experimental study of wind loads on unique buildings and structures in
   Russia},
Booktitle = {5TH INTERNATIONAL SCIENTIFIC CONFERENCE INTEGRATION, PARTNERSHIP AND
   INNOVATION IN CONSTRUCTION SCIENCE AND EDUCATION},
Series = {MATEC Web of Conferences},
Year = {2016},
Volume = {86},
Note = {5th International Scientific Conference on Integration, Partnership and
   Innovation in Construction Science and Education (IPICSE), Moscow,
   RUSSIA, OCT 16-17, 2016},
Abstract = {Design and construction of unique buildings and structures (sports
   arenas, airport complexes, business centres, etc.) from an engineering
   point of view is a very difficult task as in most cases these facilities
   have an original architectural form. Therefore, consideration of wind
   loads is an important part of the design. The paper presents the
   definition of wind load for two complex of airport. Researches was
   applied the combined calculation an experimental method. During the
   experimental study a wind tunnel architectural and construction type NRU
   MSUCE was used. Numerical simulations were performed using the software
   package ANSYS. The result of research on each object are integral
   aerodynamic loads on the object (coefficients Cx, Cy, Cmz) and picture
   of the distribution of aerodynamic pressure coefficient Cp obtained in
   the numerical simulation. In conclusion, we discuss the possible
   formation of deposits of snow and recommendations to eliminate them from
   the roof of researched objects.},
DOI = {10.1051/matecconf/20168602012},
Article-Number = {02012},
ISSN = {2261-236X},
ResearcherID-Numbers = {Churin, Pavel/S-2955-2016
   Dunichkin, Ilya Vladimirovich/I-9455-2014
   Poddaeva, Olga I./H-1030-2019},
ORCID-Numbers = {Churin, Pavel/0000-0002-2339-0289
   Dunichkin, Ilya Vladimirovich/0000-0001-9372-0741
   },
Unique-ID = {WOS:000392339700045},
}

@inproceedings{ WOS:000383737200071,
Author = {Ra, Ho-Kyeong and Yoon, Hee Jung and Salekin, Asif and Lee, Jin-Hee and
   Stankovic, John A. and Son, Sang Hyuk},
Book-Group-Author = {ACM},
Title = {Poster: Software Architecture for Efficiently Designing Cloud
   Applications using Node.js},
Booktitle = {MOBISYS'16: COMPANION COMPANION PUBLICATION OF THE 14TH ANNUAL
   INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES},
Year = {2016},
Pages = {72},
Note = {14th Annual International Conference on Mobile Systems, Applications,
   and Services (MobiSys), Singapore, SINGAPORE, JUN 25-30, 2016},
Organization = {ACM SIGMOBILE; ACM SIGOPS},
Abstract = {We propose a practical solution for cloud application development using
   Node.js and Express library by presenting: (1) a software architecture
   which utilizes two standard inheritance pattern techniques, the top-down
   and divide and conquer approaches, to effectively organize the structure
   of the application for improved maintainability and extensibility in the
   long-run, and (2) an easy-to-follow guideline that instructs the
   implementation procedures for developing Node.js cloud applications.},
DOI = {10.1145/2938559.2948790},
ISBN = {978-1-4503-4416-6},
Unique-ID = {WOS:000383737200071},
}

@article{ WOS:000384678200002,
Author = {Ren, Y. Y. and Zhou, L. C. and Yang, L. and Liu, P. Y. and Zhao, B. W.
   and Liu, H. X.},
Title = {Predicting the aquatic toxicity mode of action using logistic regression
   and linear discriminant analysis},
Journal = {SAR AND QSAR IN ENVIRONMENTAL RESEARCH},
Year = {2016},
Volume = {27},
Number = {9},
Pages = {721-746},
Abstract = {The paper highlights the use of the logistic regression (LR) method in
   the construction of acceptable statistically significant, robust and
   predictive models for the classification of chemicals according to their
   aquatic toxic modes of action. Essentials accounting for a reliable
   model were all considered carefully. The model predictors were selected
   by stepwise forward discriminant analysis (LDA) from a combined pool of
   experimental data and chemical structure-based descriptors calculated by
   the CODESSA and DRAGON software packages. Model predictive ability was
   validated both internally and externally. The applicability domain was
   checked by the leverage approach to verify prediction reliability. The
   obtained models are simple and easy to interpret. In general, LR
   performs much better than LDA and seems to be more attractive for the
   prediction of the more toxic compounds, i.e. compounds that exhibit
   excess toxicity versus non-polar narcotic compounds and more reactive
   compounds versus less reactive compounds. In addition, model fit and
   regression diagnostics was done through the influence plot which
   reflects the hat-values, studentized residuals, and Cook's distance
   statistics of each sample. Overdispersion was also checked for the LR
   model. The relationships between the descriptors and the aquatic toxic
   behaviour of compounds are also discussed.},
DOI = {10.1080/1062936X.2016.1229691},
ISSN = {1062-936X},
EISSN = {1029-046X},
ORCID-Numbers = {Liu, Huanxiang/0000-0002-9284-3667},
Unique-ID = {WOS:000384678200002},
}

@inproceedings{ WOS:000385794000064,
Author = {Reuter, Michael A. and Cook, Kem H. and Delgado, Francisco and Petry,
   Catherine E. and Ridgway, Stephen T.},
Editor = {Angeli, GZ and Dierickx, P},
Title = {Simulating the LSST OCS for Conducting Survey Simulations Using the LSST
   Scheduler},
Booktitle = {MODELING, SYSTEMS ENGINEERING, AND PROJECT MANAGEMENT FOR ASTRONOMY VII},
Series = {Proceedings of SPIE},
Year = {2016},
Volume = {9911},
Number = {1},
Note = {Conference on Modeling, Systems Engineering, and Project Management for
   Astronomy VII, Edinburgh, SCOTLAND, JUN 26-28, 2016},
Organization = {SPIE},
Abstract = {The Operations Simulator was used to prototype the Large Synoptic Survey
   Telescope (LSST) Scheduler. Currently, the Scheduler is being developed
   separately to interface with the LSST Observatory Control System (OCS).
   A new Simulator is under concurrent development to adjust to this new
   architecture. This requires a package simulating enough of the OCS to
   allow execution of realistic schedules. This new package is called the
   Simulated OCS (SOCS). In this paper we detail the SOCS construction
   plan, package structure, LSST communication middleware platform use,
   provide some interesting use cases that the separated architecture
   allows and the software engineering practices used in development.},
DOI = {10.1117/12.2232680},
Article-Number = {991125},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-0201-4; 978-1-5106-0202-1},
Unique-ID = {WOS:000385794000064},
}

@article{ WOS:000381504200018,
Author = {Schiurunack, Manuel and Nguyen, Susan and Mercorelli, Paolo},
Title = {Anatomy of Haar Wavelet Filter and Its Implementation for Signal
   Processing},
Journal = {IFAC PAPERSONLINE},
Year = {2016},
Volume = {49},
Number = {6},
Pages = {99-104},
Note = {11th IFAC Symposium on Advances in Control Education (ACE), Bratislava,
   SLOVAKIA, JUN 01-03, 2016},
Organization = {Int Federat Automat Control, Tech Comm 9 4 Control Educ; Int Federat
   Automat Control, Tech Comm 2 1 Control Design; Int Federat Automat
   Control, Tech Comm 2 2 Linear Control Syst; Int Federat Automat Control,
   Tech Comm 4 1 Components \& Technologies Control; Int Federat Automat
   Control, Tech Comm 7 1 Automot Control; Int Federat Automat Control,
   Tech Comm 7 5 Intelligent Autonomous Vehicles},
Abstract = {This paper gives an insight to the workings of discrete transformation
   (DWT) in context of education, with the objective to integrate teaching
   and research by prot toting signal processing and control as a field
   that embraces science, technology, engineering and mathematics (STEM).
   In more detail, this contribution showcases a possible lecture structure
   of the basic principle of orthogonal wavelets in general, and the
   discrete wavelet decomposition method. The architecture of the presented
   software structure are described step-by-step, to provide an elementary
   guideline for a possible implementation into an embedded system. Herein,
   the focus is set on the Haar wavelet specifically, this as an
   illustrative example, the code for the use of it is presented. With the
   wavelet packet transform as a method of discrete wavelet transform, the
   algorithm is able to decompose and reconstruct an input signal with
   reduction of noise. The noise of a sequence can be located, so that the
   wavelet basis can be rearranged. In particular, this allows for the
   elimination of any incoherent parts that make up the unavoidable
   measuring noise of the acquired signal, which was tested in GNU Octave
   and MATLAB (R). (C) 2016, IFAC (International Federation of Automatic
   Control) Hosting by Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.ifacol.2016.07.160},
ISSN = {2405-8963},
Unique-ID = {WOS:000381504200018},
}

@inproceedings{ WOS:000400688200066,
Author = {Staszewski, Pawel and Woldan, Piotr and Korytkowski, Marcin and Scherer,
   Rafal and Wang, Lipo},
Editor = {Rutkowski, L and Korytkowski, M and Scherer, R and Tadeusiewicz, R and Zadeh, LA and Zurada, JM},
Title = {Query-by-Example Image Retrieval in Microsoft SQL Server},
Booktitle = {ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING, (ICAISC 2016), PT II},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2016},
Volume = {9693},
Pages = {746-754},
Note = {15th International Conference on Artificial Intelligence and Soft
   Computing (ICAISC), Zakopane, POLAND, JUN 12-16, 2016},
Organization = {Polish Neural Network Soc; Univ Social Sci; Czestochowa Univ Technol,
   Inst Computat Intelligence},
Abstract = {In this paper we present a system intended for content-based image
   retrieval tightly integrated with a relational database management
   system. Users can send query images over the appropriate web service
   channel or construct database queries locally. The presented framework
   analyses the query image based on descriptors which are generated by the
   bag-of-features algorithm and local interest points. The system returns
   the sequence of similar images with a similarity level to the query
   image. The software was implemented in. NET technology and Microsoft SQL
   Server 2012. The modular construction allows to customize the system
   functionality to client needs but it is especially dedicated to business
   applications. Important advantage of the presented approach is the
   support by SOA (Service-Oriented Architecture), which allows to use the
   system in a remote way. It is possible to build software which uses
   functions of the presented system by communicating over the web service
   API with the WCF technology.},
DOI = {10.1007/978-3-319-39384-1\_66},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-319-39384-1},
ResearcherID-Numbers = {Wang, Lipo/A-5154-2011
   Scherer, Rafal/F-6745-2012
   },
ORCID-Numbers = {Scherer, Rafal/0000-0001-9592-262X
   Korytkowski, Marcin/0000-0002-6002-2733},
Unique-ID = {WOS:000400688200066},
}

@inproceedings{ WOS:000387566500001,
Author = {Ugliotti, Francesca Maria and Dellosta, Maurizio and Osello, Anna},
Editor = {Drusa, M and Yilmaz, I and Marschalko, M and Coisson, E and Segalini, A},
Title = {BIM-Based Energy Analysis Using Edilclima EC770 Plug-In, Case Study
   Archimede Library EEB Project},
Booktitle = {WORLD MULTIDISCIPLINARY CIVIL ENGINEERING-ARCHITECTURE-URBAN PLANNING
   SYMPOSIUM 2016, WMCAUS 2016},
Series = {Procedia Engineering},
Year = {2016},
Volume = {161},
Pages = {3-8},
Note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning
   Symposium (WMCAUS), Prague, CZECH REPUBLIC, JUN 13-17, 2016},
Abstract = {In the recent years, energy efficiency issues as well as Building
   Information Modelling (BIM) have been the greatest and most challenging
   paradigms for the Architecture Engineering and Construction (AECO)
   industry in the context of Smart Cities. Digital models are used to
   analyse the existing building stock to promote a better management and
   retrofitting actions. Data modelling is the first step to pursue an
   integrated approach for the building lifecycle allowing simulations and
   analysis for different purposes through the interoperability process.
   This study aims to investigate the potential and the limitations of a
   BIM-based energy analysis by means of an Italian commonly used software
   for energy diagnosis and certificates, according to the quasi-steady
   method specified in the UNI/TS 11300-1: 2008. The case study is a
   library with municipal offices in Settimo Torinese (Italy), which is the
   demonstrator of the ongoing Zero Energy Buildings in Smart Urban
   Districts (EEB) national cluster, which has the main scope to create a
   data management infrastructure able to integrate heterogeneous networks.
   The energy rate is evaluated from a simplified Revit architectural
   model, where the most significant components of the building in terms of
   energy are defined with a proper Level of Detail/Development (LOD) to
   easily set the energy model through the EC770 plug-in. In this way, the
   acquisition of geometrical data is allowed by the interoperability among
   software, speeding up the redundant preliminary phases of the simulation
   concerning the description of the building envelope. (C) 2016 The
   Authors. Published by Elsevier Ltd.},
DOI = {10.1016/j.proeng.2016.08.489},
ISSN = {1877-7058},
ORCID-Numbers = {Ugliotti, Francesca Maria/0000-0001-5370-339X},
Unique-ID = {WOS:000387566500001},
}

@inproceedings{ WOS:000392734700262,
Author = {Wang, Xianjie and Sun, Yong and Xiang, Mengjie},
Editor = {Fang, Y and Xin, Y},
Title = {Building structure topology optimization},
Booktitle = {PROCEEDINGS OF THE 2016 4TH INTERNATIONAL CONFERENCE ON MACHINERY,
   MATERIALS AND INFORMATION TECHNOLOGY APPLICATIONS},
Series = {ACSR-Advances in Comptuer Science Research},
Year = {2016},
Volume = {71},
Pages = {1426-1429},
Note = {4th International Conference on Machinery, Materials and Information
   Technology Applications (ICMMITA), Xian, PEOPLES R CHINA, DEC 10-11,
   2016},
Abstract = {In this paper, a new estimate criterion is introduced to make the
   optimization process reasonable and credible, and it has a good control
   effect for the convergence of solution. An improved optimization program
   is exploiting for the directed building structure by means of a
   distinction design between the active and non-active elements. With the
   help of this plug-in program, the data can be exchanged between the FEA
   software of ABAQUS and the post-processor package in MATLAB. Several
   examples have shown the computer program can be used for directed
   structure topology optimization and portion construction design. This
   method is presented here for a wide used in structure design.},
ISSN = {2352-538X},
ISBN = {978-94-6252-285-5},
Unique-ID = {WOS:000392734700262},
}

@inproceedings{ WOS:000401916300019,
Author = {Yu, C. Y. and Ma, J. and Ren, Q. and Zhao, Y. Y.},
Book-Group-Author = {IEEE},
Title = {Design and Implementation of Software Case Library Supporting Software
   Capability Training},
Booktitle = {FIFTH INTERNATIONAL CONFERENCE ON EDUCATIONAL INNOVATION THROUGH
   TECHNOLOGY (EITT 2016)},
Year = {2016},
Pages = {96-101},
Note = {5th International Conference on Educational Innovation through
   Technology (EITT), Tainan, TAIWAN, SEP 22-24, 2016},
Organization = {Soc Int Chinese Educ Technol; IEEE CIS Tainan Chapter; Natl Univ Tainan;
   Takming Univ Sci \& Technol},
Abstract = {Software talents mainly come from university graduates, but the
   graduates could not meet the need of employers. Practical ability of
   software talents cultivating has aroused wide concern of the researches
   and teachers. In this paper, we consider that the software case is the
   most important factor and proposed SCL (Software Case Library) could
   provide real and attractive software case in the teaching and students'
   practical projects. The SCL consists of source and different documents
   and could adapt different kinds of practical training. We developed a
   SCL and applied it in Web Programming with 102 students. We have
   collected the data of the course achievement by questionnaire,
   face-to-face interview, big project, students' grading for the course.
   The result indicated that SCL promoted the satisfaction and practical
   abilities of students.},
DOI = {10.1109/EITT.2016.26},
ISBN = {978-1-5090-6138-9},
Unique-ID = {WOS:000401916300019},
}

@inproceedings{ WOS:000398964800206,
Author = {Yu, Liang},
Editor = {Yu, J and Liu, Q},
Title = {Interactive Research on WeChat Public Platform and Mobile Library
   Application in Colleges and Universities},
Booktitle = {PROCEEDINGS OF THE 2016 4TH INTERNATIONAL CONFERENCE ON ELECTRICAL \&
   ELECTRONICS ENGINEERING AND COMPUTER SCIENCE (ICEEECS 2016)},
Series = {ACSR-Advances in Comptuer Science Research},
Year = {2016},
Volume = {50},
Pages = {1073-1076},
Note = {4th International Conference on Electrical and Electronics Engineering
   and Computer Science (ICEEECS), Jinan, PEOPLES R CHINA, OCT 15-16, 2016},
Organization = {Int Informat \& Engn Assoc; Univ Sydney Technol, Indian Inst Technol;
   Univ Polytechn Bucharest; Univ Teknologi MARA},
Abstract = {With the continuous development of modern information technology,
   college reading platform model is also in constant change. From the
   perspective of the existing application software, system application
   measures and application survey structure of the whole service status
   quo and problems both need to carry on the comprehensive education
   development implementation according to the existing WeChat platform,
   and in accordance with relevant standards, achieve the application
   analytical analysis on basis of phase structure and to improve discuss
   effect.
   In face of modern education development form, different structure mode
   mainly focuses on teaching form content applications, and through the
   corresponding infrastructure education promotion, promotes good
   construction for mobile network platform in order to improve the
   application analysis on the whole interactive channels and realize the
   rationalization of whole education structure. A brief analysis and
   discussion of modern university WeChat public platform and the
   interactive application of mobile library is made below.},
ISSN = {2352-538X},
ISBN = {978-94-6252-265-7},
Unique-ID = {WOS:000398964800206},
}

@inproceedings{ WOS:000391229900019,
Author = {Zhang, Chuanhao and Bu, Youjun and Zhao, Zheng},
Editor = {Liao, Y and Tan, X},
Title = {SDN-based Path Hopping Communication Against Eavesdropping Attack},
Booktitle = {OPTICAL COMMUNICATION AND OPTICAL FIBER SENSORS AND OPTICAL MEMORIES FOR
   BIG DATA STORAGE},
Series = {Proceedings of SPIE},
Year = {2016},
Volume = {10158},
Note = {International Symposium on Optical Communication and Optical Fiber
   Sensors / International Symposium on Optical Memories for Big Data
   Storage, Beijing, PEOPLES R CHINA, MAY 09-11, 2016},
Organization = {Chinese Soc Opt Engn; Chinese Soc Astronaut, Photoelectron Technol Comm;
   China High Tech Industrializat Assoc, Photoelectron Industrializat Comm;
   China High Tech Industrializat Assoc, Dept Cooperat \& Coordinat Ind,
   Acad \& Res; SPIE},
Abstract = {Network eavesdropping is one of the most popular means used by cyber
   attackers, which has been a severe threat to network communication
   security. Adversaries could capture and analyze network communication
   data from network nodes or links, monitor network status and steal
   sensitive data such as username and password etc. Traditional network
   usually uses static network configuration, and existing defense methods,
   including firewall, IDS, IPS etc., cannot prevent eavesdropping, which
   has no distinguishing characteristic. Network eavesdropping become
   silent during most of the time of the attacking process, which is why it
   is difficult to discover and to defend. But A successful eavesdropping
   attack also has its' precondition, which is the target path should be
   relatively stable and has enough time of duration. So, In order to
   resolve this problem, it has to work on the network architecture. In
   this paper, a path hopping communication(PHC) mechanism based on
   Software Define Network(SDN) was proposed to solve this problem. In PHC,
   Ends in communication packets as well as the routing paths were changed
   dynamically. Therefore, the traffic would be distributed to multiple
   flows and transmitted along different paths. so that Network
   eavesdropping attack could be prevented effectively. It was concluded
   that PHC was able to increase the overhead of Network eavesdropping, as
   well as the difficulty of communication data recovery.},
DOI = {10.1117/12.2246538},
Article-Number = {101580J},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-1-5106-0774-3; 978-1-5106-0775-0},
ResearcherID-Numbers = {zhang, chuanhao/HMV-1192-2023},
Unique-ID = {WOS:000391229900019},
}

@article{ WOS:000367318100001,
Author = {Liu, Siyang and Huang, Shujia and Rao, Junhua and Ye, Weijian and Krogh,
   Anders and Wang, Jun and Genome Denmark Consortium},
Title = {Discovery, genotyping and characterization of structural variation and
   novel sequence at single nucleotide resolution from de novo genome
   assemblies on a population scale},
Journal = {GIGASCIENCE},
Year = {2015},
Volume = {4},
Month = {DEC 24},
Abstract = {Background: Comprehensive recognition of genomic variation in one
   individual is important for understanding disease and developing
   personalized medication and treatment. Many tools based on DNA
   re-sequencing exist for identification of single nucleotide
   polymorphisms, small insertions and deletions (indels) as well as large
   deletions. However, these approaches consistently display a substantial
   bias against the recovery of complex structural variants and novel
   sequence in individual genomes and do not provide interpretation
   information such as the annotation of ancestral state and formation
   mechanism.
   Findings: We present a novel approach implemented in a single software
   package, AsmVar, to discover, genotype and characterize different forms
   of structural variation and novel sequence from population-scale de novo
   genome assemblies up to nucleotide resolution. Application of AsmVar to
   several human de novo genome assemblies captures a wide spectrum of
   structural variants and novel sequences present in the human population
   in high sensitivity and specificity.
   Conclusions: Our method provides a direct solution for investigating
   structural variants and novel sequences from de novo genome assemblies,
   facilitating the construction of population-scale pan-genomes. Our study
   also highlights the usefulness of the de novo assembly strategy for
   definition of genome structure.},
DOI = {10.1186/s13742-015-0103-4},
Article-Number = {64},
ISSN = {2047-217X},
ResearcherID-Numbers = {Schierup, Mikkel H/F-1675-2010
   Rasmussen, Simon/G-6258-2016
   Huang, Shujia/GXG-8583-2022
   Izarzugaza, Jose/N-4503-2014
   Chang, Yuqi/E-9891-2017
   Grove, Jakob/K-8112-2015
   Liu, Siyang/M-6127-2014
   Krogh, Anders/M-1541-2014
   Lund, Ole/F-4437-2014
   },
ORCID-Numbers = {Schierup, Mikkel H/0000-0002-5028-1790
   Rasmussen, Simon/0000-0001-6323-9041
   Huang, Shujia/0000-0003-2100-2534
   Izarzugaza, Jose/0000-0003-1754-5122
   Villesen, Palle/0000-0001-7589-8781
   Chang, Yuqi/0000-0001-7487-370X
   Borglum, Anders/0000-0001-8627-7219
   Belling, Kirstine/0000-0001-5664-1948
   Grove, Jakob/0000-0003-2284-5744
   Liu, Siyang/0000-0001-6780-9419
   Krogh, Anders/0000-0002-5147-6282
   Lund, Ole/0000-0003-1108-0491
   Demontis, Ditte/0000-0001-9124-2766},
Unique-ID = {WOS:000367318100001},
}

@article{ WOS:000366386600008,
Author = {Strauss, Alfred and Eschbacher, Thomas and Kendicky, Peter and Benko,
   Vladimir},
Title = {Load capacity of slender compression members Part 1: Component tests and
   round robin test of the nonlinear numerical prediction of system
   behavior},
Journal = {BETON- UND STAHLBETONBAU},
Year = {2015},
Volume = {110},
Number = {12},
Pages = {845-856},
Month = {DEC},
Abstract = {Non-linear calculation methods allow a realistic description of the
   physically caused complex load-deformation behavior of reinforced
   concrete structures and suggest in comparison of proven approximate
   methods a more accurate design in both ultimate and serviceability limit
   states. Especially the geometrically and materially nonlinear analysis
   of slender compression members in the ultimate limit state is, however,
   still controversial because of the known inconsistencies in the design
   concept, which is why, despite the in EN 1992-1-1 explicitly granted
   ability to use non-linear methods, further research is needed. The
   following paper compares an a priori carried out round robin test of
   numerical simulations to predict the load capacity of a slender single
   column with an a posteriori conducted test series. The results show that
   the numerical calculations overestimate the load capacity in some cases
   significantly but, however, the widely accepted and normatively governed
   nominal curvature method yields in these cases to too conservative
   results. It will be necessary to develop universal and consistent safety
   formats for non-linear analysis and provide best practice guidelines to
   ensure the safe use of the now widely available software packages.},
DOI = {10.1002/best.201500040},
ISSN = {0005-9900},
EISSN = {1437-1006},
Unique-ID = {WOS:000366386600008},
}

@article{ WOS:000365766200001,
Author = {Donges, Jonathan F. and Heitzig, Jobst and Beronov, Boyan and
   Wiedermann, Marc and Runge, Jakob and Feng, Qing Yi and Tupikina, Liubov
   and Stolbova, Veronika and Donner, Reik V. and Marwan, Norbert and
   Dijkstra, Henk A. and Kurths, Juergen},
Title = {Unified functional network and nonlinear time series analysis for
   complex systems science: The pyunicorn package},
Journal = {CHAOS},
Year = {2015},
Volume = {25},
Number = {11},
Month = {NOV},
Abstract = {We introduce the pyunicorn (Pythonic unified complex network and
   recurrence analysis toolbox) open source software package for applying
   and combining modern methods of data analysis and modeling from complex
   network theory and nonlinear time series analysis. pyunicorn is a fully
   object-oriented and easily parallelizable package written in the
   language Python. It allows for the construction of functional networks
   such as climate networks in climatology or functional brain networks in
   neuroscience representing the structure of statistical
   interrelationships in large data sets of time series and, subsequently,
   investigating this structure using advanced methods of complex network
   theory such as measures and models for spatial networks, networks of
   interacting networks, node-weighted statistics, or network surrogates.
   Additionally, pyunicorn provides insights into the nonlinear dynamics of
   complex systems as recorded in uni-and multivariate time series from a
   non-traditional perspective by means of recurrence quantification
   analysis, recurrence networks, visibility graphs, and construction of
   surrogate time series. The range of possible applications of the library
   is outlined, drawing on several examples mainly from the field of
   climatology. (C) 2015 AIP Publishing LLC.},
DOI = {10.1063/1.4934554},
Article-Number = {113101},
ISSN = {1054-1500},
EISSN = {1089-7682},
ResearcherID-Numbers = {Donner, Reik/F-4400-2017
   Beronov, Boyan/AAC-1242-2020
   Stolbova, Veronika/M-1604-2017
   Donges, Jonathan F./HCI-2311-2022
   Dijkstra, Henk A./H-2559-2016
   Marwan, Norbert/D-9576-2011
   Heitzig, Jobst/E-8271-2011
   },
ORCID-Numbers = {Donner, Reik/0000-0001-7023-6375
   Beronov, Boyan/0000-0002-0900-752X
   Stolbova, Veronika/0000-0002-5574-3827
   Marwan, Norbert/0000-0003-1437-7039
   Runge, Jakob/0000-0002-0629-1772
   Heitzig, Jobst/0000-0002-0442-8077
   Donges, Jonathan/0000-0001-5233-7703},
Unique-ID = {WOS:000365766200001},
}

@article{ WOS:000363495400066,
Author = {Ferragina, A. and de los Campos, G. and Vazquez, A. I. and Cecchinato,
   A. and Bittante, G.},
Title = {Bayesian regression models outperform partial least squares methods for
   predicting milk components and technological properties using infrared
   spectral data},
Journal = {JOURNAL OF DAIRY SCIENCE},
Year = {2015},
Volume = {98},
Number = {11},
Pages = {8133-8151},
Month = {NOV},
Abstract = {The aim of this study was to assess the performance of Bayesian models
   commonly used for genomic selection to predict
   ``difficult-to-predict{''} dairy traits, such as milk fatty acid (FA)
   expressed as percentage of total fatty acids, and technological
   properties, such as fresh cheese yield and protein recovery, using
   Fouriertransform infrared (FTIR) spectral data. Our main hypothesis was
   that Bayesian models that can estimate shrinkage and perform variable
   selection may improve our ability to predict FA traits and technological
   traits above and beyond what can be achieved using the current
   calibration models (e.g., partial least squares, PLS). To this end, we
   assessed a series of Bayesian methods and compared their prediction
   performance with that of PLS. The comparison between models was done
   using the same sets of data (i.e., same samples, same variability, same
   spectral treatment) for each trait. Data consisted of 1,264 individual
   milk samples collected from Brown Swiss cows for which gas
   chromatographic FA composition, milk coagulation properties, and
   cheese-yield traits were available. For each sample, 2 spectra in the
   infrared region from 5,011 to 925 cm(-1) were available and averaged
   before data analysis. Three Bayesian models: Bayesian ridge regression
   (Bayes RR), Bayes A, and Bayes B, and 2 reference models: PLS and
   modified PLS (MPLS) procedures, were used to calibrate equations for
   each of the traits. The Bayesian models used were implemented in the R
   package BGLR (http://cran.r-project.org/web/packages/ BGLR/index.html),
   whereas the PLS and MPLS were those implemented in the WinISI II
   software (Infrasoft International LLC, State College, PA). Prediction
   accuracy was estimated for each trait and model using 25 replicates of a
   training-testing validation procedure. Compared with PLS, which is
   currently the most widely used calibration method, MPLS and the 3
   Bayesian methods showed significantly greater prediction accuracy.
   Accuracy increased in moving from calibration to external validation
   methods, and in moving from PLS and MPLS to Bayesian methods,
   particularly Bayes A and Bayes B. The maximum R-2 value of validation
   was obtained with Bayes B and Bayes A. For the FA, C10:0 (\% of each FA
   on total FA basis) had the highest R-2 (0.75, achieved with Bayes A and
   Bayes B), and among the technological traits, fresh cheese yield R-2 of
   0.82 (achieved with Bayes B). These 2 methods have proven to be useful
   instruments in shrinking and selecting very informative wavelengths and
   inferring the structure and functions of the analyzed traits. We
   conclude that Bayesian models are powerful tools for deriving
   calibration equations, and, importantly, these equations can be easily
   developed using existing open-source software. As part of our study, we
   provide scripts based on the open source R software BGLR, which can be
   used to train customized prediction equations for other traits or
   populations.},
DOI = {10.3168/jds.2014-9143},
ISSN = {0022-0302},
EISSN = {1525-3198},
ResearcherID-Numbers = {Bittante, Giovanni/M-4521-2019
   Bittante, Giovanni/B-2332-2010
   Ferragina, Alessandro/GSI-5522-2022},
ORCID-Numbers = {Bittante, Giovanni/0000-0001-7137-7049
   Bittante, Giovanni/0000-0001-7137-7049
   Ferragina, Alessandro/0000-0001-9533-1221},
Unique-ID = {WOS:000363495400066},
}

@article{ WOS:000364614000044,
Author = {Scherer, Martin K. and Trendelkamp-Schroer, Benjamin and Paul, Fabian
   and Perez-Hernandez, Guillermo and Hoffmann, Moritz and Plattner, Nuria
   and Wehmeyer, Christoph and Prinz, Jan-Hendrik and Noe, Frank},
Title = {PyEMMA 2: A Software Package for Estimation, Validation, and Analysis of
   Markov Models},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Year = {2015},
Volume = {11},
Number = {11},
Pages = {5525-5542},
Month = {NOV},
Abstract = {Markov (state) models (MSMs) and related models of molecular kinetics
   have recently received a surge of interest as they can systematically
   reconcile simulation data from either a few long or many short
   simulations and allow us to analyze the essential metastable structures,
   thermodynamics, and kinetics of the molecular system under
   investigation. However, the estimation, validation, and analysis of such
   models is far from trivial and involves sophisticated and often
   numerically sensitive methods. In this work we present the open-source
   Python package PyEMMA (http://pyemma.org) that provides accurate and
   efficient algorithms for kinetic model construction. PyEMMA can read all
   common molecular dynamics data formats, helps in the selection of input
   features, provides easy access to dimension reduction algorithms such as
   principal component analysis (PCA) and time-lagged independent component
   analysis (TICA) and clustering algorithms such as k-means, and contains
   estimators for MSMs, hidden Markov models, and several other models.
   Systematic model validation and error calculation methods are provided.
   PyEMMA offers a wealth of analysis functions such that the user can
   conveniently compute molecular observables of interest. We have derived
   a systematic and accurate way to coarse-grain MSMs to few states and to
   illustrate the structures of the metastable states of the system.
   Plotting functions to produce a manuscript-ready presentation of the
   results are available. In this work, we demonstrate the features of the
   software and show new methodological concepts and results produced by
   PyEMMA.},
DOI = {10.1021/acs.jctc.5b00743},
ISSN = {1549-9618},
EISSN = {1549-9626},
ResearcherID-Numbers = {Wehmeyer, Christoph/A-3410-2011
   Noe, Frank/Y-2766-2019
   },
ORCID-Numbers = {Wehmeyer, Christoph/0000-0002-9526-0328
   Scherer, Martin K./0000-0002-7983-4387
   Plattner, Nuria/0000-0003-4115-6741
   Perez-Hernandez, Guillermo/0000-0002-9287-8704},
Unique-ID = {WOS:000364614000044},
}

@article{ WOS:000363876000010,
Author = {Zemroch, Peter J.},
Title = {The Computerized Generation of Fractional-replicate Designs Using Galois
   Fields and Hadamard Matrices},
Journal = {QUALITY AND RELIABILITY ENGINEERING INTERNATIONAL},
Year = {2015},
Volume = {31},
Number = {7, SI},
Pages = {1197-1207},
Month = {NOV},
Note = {14th Annual Conference of the
   European-Network-for-Business-and-Industrial-Statistics (ENBIS),
   Johannes Kepler Univ, Linz, AUSTRALIA, SEP 21-25, 2014},
Organization = {European Network Business \& Ind Stat},
Abstract = {Many statistical programs can now generate fractional-replicate designs,
   but most depend on built-in libraries of experimental plans. Relatively
   few have design generation algorithms that would give the user true
   flexibility in the numbers of tests, factors, and factor levels.
   Powerful search algorithms are now available for generating a wide range
   of blocked and fractional-replicate designs using design keys, but these
   are mostly regular designs with p(q) units and factors with p, p(2), ...
   levels, where p is prime. Many useful classes of irregular design also
   exist with numbers of units and factor levels which are not powers of p,
   and the construction rules for most of these use Galois fields and
   Hadamard matrices in some way. However, the underlying mathematics can
   be difficult. The aim of this paper is to collate algorithms to expedite
   the implementation of these structures in software and to present these
   in an accessible form to a wider community. The use of Galois fields and
   Hadamard matrices in generating important design classes, such as the
   fractional 2(k) Plackett-Burman and s(k) Addelman-Kempthorne designs, is
   then detailed. Combining these algorithms with design key searches gives
   an arsenal of methods that can generate almost all existent balanced
   fractional-replicate designs of sizes likely to be used in real-world
   experimentation. These methods have all been implemented in the
   KEYFINDER program, which is available from the author, free of charge.
   Copyright (c) 2015 John Wiley \& Sons, Ltd.},
DOI = {10.1002/qre.1846},
ISSN = {0748-8017},
EISSN = {1099-1638},
Unique-ID = {WOS:000363876000010},
}

@article{ WOS:000361466700005,
Author = {Li, Xiao-Hua and Feng, Li and Yang, Zhen-Hua and Liao, Yun-Hua},
Title = {Effect of active vitamin D on cardiovascular outcomes in predialysis
   chronic kidney diseases: A systematic review and meta-analysis},
Journal = {NEPHROLOGY},
Year = {2015},
Volume = {20},
Number = {10},
Pages = {706-714},
Month = {OCT},
Abstract = {AimVitamin D deficient patients present an increased risk of
   cardiovascular disease. We conducted this systematic review and
   meta-analysis to evaluate the effect of active vitamin D analogue on
   cardiovascular outcomes in predialysis chronic kidney disease.
   MethodsPubmed, Embase, the Cochrane Library, CNKI, and article reference
   lists were searched for randomized controlled trials (RCTs) that
   compared active vitamin D analogues with placebo or no treatment for
   patients with predialysis chronic kidney disease. A meta-analysis was
   conducted using the standard methods consistent with the Preferred
   Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
   guidelines. Reviewer Manager Software, ver. 5.2, was used.
   ResultsSeven RCTs (five studies with paricalcitol and two studies with
   calcitriol, 731 patients) were included. Compared with control groups,
   active vitamin D reduced the incidence of cardiovascular events (RR,
   0.27; 95\% CI, 0.13-0.59), induced an increase in those with proteinuria
   reduction (RR, 1.9; 95\% CI, 1.34-2.71), but did not alter left
   ventricular mass index and systolic function (MD, 0.42g/m(2.7); 95\% CI,
   -0.23-1.07g/m(2.7), P=0.21 for left ventricular mass index and MD,
   -0.33; 95\% CI, -0.74-0.07, P=0.1 for left ventricular ejection
   fraction). Neither systolic blood pressure nor diastolic blood pressure
   was reduced by active vitamin D (MD, 0.3mmHg; 95\% CI, -4.95-5.56mmHg;
   MD, -0.24mmHg; 95\% CI: -6.21-5.72mmHg, respectively). Increased
   probability of hypercalcaemia after paricalcitol therapy was found (RR,
   7.85; 95\% CI, 2.92-21.10).
   ConclusionActive vitamin D reduced the incidence of cardiovascular
   events and induced a reduction in proteinuria, but its long-term effect
   on cardiac structure and function needed further confirmation. Increased
   probability of hypercalcaemia after paricalcitol therapy was found.
   Summary at a Glance This manuscript analyzes and reviews the ability of
   acitve Vitamin D treatment to reduce incidence of cardiovascular events
   and reduce proteinuria in pre-dialysis CKD patients. However, its
   long-term effect on cardiac structure and function needs further
   confirmation.},
DOI = {10.1111/nep.12505},
ISSN = {1320-5358},
EISSN = {1440-1797},
Unique-ID = {WOS:000361466700005},
}

@article{ WOS:000384888600002,
Author = {Kensek, Karen},
Title = {VISUAL PROGRAMING FOR BUILDING INFORMATION MODELING: ENERGY AND SHADING
   ANALYSIS CASE STUDIES},
Journal = {JOURNAL OF GREEN BUILDING},
Year = {2015},
Volume = {10},
Number = {4},
Pages = {28+},
Month = {FAL},
Abstract = {Although visual programming is being broadly implemented in other
   disciplines, it has only relatively recently become an important
   supplement to three-dimensional modeling programs in the architecture,
   engineering, and construction industry. Currently, Grasshopper in
   conjunction with Rhino is a leading example of a visual programming
   environment that is strongly supported by a user community that is
   developing additional functionality, but Grasshopper does not yet work
   directly with building information modeling (BIM) software. Dynamo is
   relatively new, but shows considerable promise in becoming a
   constructive tool to complement BIM, 3D modeling, and analysis programs
   because it includes parametric geometries and works with Revit, a
   leading BIM software program. Three case studies are described:
   extensibility of Dynamo through the use of a building energy simulation
   package, controlling a virtual model's response through light level
   sensors, and interactively updating shading components for a building
   facade based on solar angles. They demonstrate that one can work
   directly within building information models (BIM) using a visual
   programming language through updating component parameters. These case
   studies demonstrate the feasibility of a workflow for sustainable design
   simulations that is different than that more commonly used - having a
   separation between design and analysis models and using a neutral file
   format exchange such as IFC or gbXML to transfer data. As visual
   programming languages are still a bit uncommon in the building industry,
   a short background is provided to place them within the tool set of
   other customizable tools that designers have been developing.},
DOI = {10.3992/jgb.10.4.28},
ISSN = {1552-6100},
EISSN = {1943-4618},
Unique-ID = {WOS:000384888600002},
}

@article{ WOS:000356236800008,
Author = {Orazalin, Zhandos Y. and Whittle, Andrew J. and Olsen, Matthew B.},
Title = {NThree-Dimensional Analyses of Excavation Support System for the Stata
   Center Basement on the MIT Campus},
Journal = {JOURNAL OF GEOTECHNICAL AND GEOENVIRONMENTAL ENGINEERING},
Year = {2015},
Volume = {141},
Number = {7},
Month = {JUL},
Abstract = {The basement of the Stata Center building on the MIT campus required
   12.8 m deep excavations covering a large open-plan site and underlain by
   more than 25 m of lightly overconsolidated Boston Blue Clay. The
   excavations were supported by a floating, perimeter diaphragm and braced
   with a system of internal corner struts, rakers, and tieback anchors.
   The project involved a complex sequence of berms, access ramps, and
   phased construction of the concrete mat foundation. One of the key goals
   of the design was to limit ground movements in order to prevent damage
   to adjacent structures, including the Alumni Pool building, which was
   founded on shallow caissons and located less than 1.5 m from the south
   wall. Lateral wall movements and building settlements were closely
   monitored throughout construction, while photos from a network of
   webcams located around the open-plan site provide a detailed time
   history of the construction processes. This paper describes the
   development of a comprehensive three-dimensional (3D) finite-element
   (FE) model for the Stata Center basement excavation, which has been
   enabled by recent advances made available in a FE software package,
   including efficient multicore iterative solving capabilities, importing
   of geometric data from computer-aided design files, and use of embedded
   pile elements to represent tieback anchors. The analyses highlight the
   effects of the 3D excavation and support geometry on wall and ground
   movements. The base case results using a simple elasto-plastic
   Mohr-Coulomb soil model with undrained conditions in the clay are
   generally in very good agreement with measured performance. The effects
   of refined constitutive modeling and partial drainage within the clay
   have a secondary role in numerical predictions for this project. (C)
   2015 American Society of Civil Engineers.},
DOI = {10.1061/(ASCE)GT.1943-5606.0001326},
Article-Number = {05015001},
ISSN = {1090-0241},
EISSN = {1943-5606},
ResearcherID-Numbers = {Orazalin, Zhandos/L-3103-2018
   Whittle, Andrew/AAE-1377-2022
   },
ORCID-Numbers = {Orazalin, Zhandos/0000-0001-6348-1006
   /0000-0001-5358-4140},
Unique-ID = {WOS:000356236800008},
}

@article{ WOS:000369643200009,
Author = {Rossiter, Stuart},
Title = {Simulation Design: Trans-Paradigm Best-Practice from Software
   Engineering},
Journal = {JASSS-THE JOURNAL OF ARTIFICIAL SOCIETIES AND SOCIAL SIMULATION},
Year = {2015},
Volume = {18},
Number = {3},
Month = {JUN 30},
Abstract = {There are growing initiatives to apply software engineering (SE)
   best-practice to computational science, which includes simulation. One
   area where the simulation literature appears to be particularly light is
   in the overall structural design of simulations, and what architectures
   and features are valuable for what reasons. (Part of the problem is that
   parts of this knowledge are abstracted away in simulation toolkits which
   are often not easily comparable, and have different conceptual aims.) To
   address this, I outline three key software properties which embody SE
   best-practices, and then define an `idealised' software architecture for
   simulation-what SE would call a reference architecture-which strongly
   exhibits them. I show that this is universal to all simulations (largely
   because modelling-paradigm-specific detail is encapsulated into a
   `single black box' layer of functionality) but that simulation toolkits
   tend to differ in how they map to them; this relates to the aims of the
   toolkits, which I provide a useful categorisation of. I show that,
   interestingly, there are several core features of this architecture that
   are not fully represented in any simulation toolkit that I am aware of.
   I present a library-JSIT-which provides some proof-of-concept
   implementations of them for Java-based toolkits. This library, and other
   ideas in the reference architecture, are put into practice on a
   published, multi-paradigm model of health and social care which uses the
   AnyLogic toolkit. I conclude with some thoughts on why this area
   receives so little focus, how to take it forwards, and some of the
   related cultural issues.},
DOI = {10.18564/jasss.2842},
Article-Number = {9},
ISSN = {1460-7425},
Unique-ID = {WOS:000369643200009},
}

@article{ WOS:000356924700016,
Author = {Park, Min Kyung and Park, Jin Young and Nicolas, Genevieve and Paik, Hee
   Young and Kim, Jeongseon and Slimani, Nadia},
Title = {Adapting a standardised international 24 h dietary recall methodology
   (GloboDiet software) for research and dietary surveillance in Korea},
Journal = {BRITISH JOURNAL OF NUTRITION},
Year = {2015},
Volume = {113},
Number = {11},
Pages = {1810-1818},
Month = {JUN 14},
Abstract = {During the past decades, a rapid nutritional transition has been
   observed along with economic growth in the Republic of Korea. Since this
   dramatic change in diet has been frequently associated with cancer and
   other non-communicable diseases, dietary monitoring is essential to
   understand the association. Benefiting from pre-existing standardised
   dietary methodologies, the present study aimed to evaluate the
   feasibility and describe the development of a Korean version of the
   international computerised 24 h dietary recall method (GloboDiet
   software) and its complementary tools, developed at the International
   Agency for Research on Cancer (IARC), WHO. Following established
   international Standard Operating Procedures and guidelines, about
   seventy common and country-specific databases on foods, recipes, dietary
   supplements, quantification methods and coefficients were customised and
   translated. The main results of the present study highlight the specific
   adaptations made to adapt the GloboDiet software for research and
   dietary surveillance in Korea. New (sub-) subgroups were added into the
   existing common food classification, and new descriptors were added to
   the facets to classify and describe specific Korean foods.
   Quantification methods were critically evaluated and adapted considering
   the foods and food packages available in the Korean market. Furthermore,
   a picture book of foods/dishes was prepared including new pictures and
   food portion sizes relevant to Korean diet. The development of the
   Korean version of GloboDiet demonstrated that it was possible to adapt
   the IARC-WHO international dietary tool to an Asian context without
   compromising its concept of standardisation and software structure. It,
   thus, confirms that this international dietary methodology, used so far
   only in Europe, is flexible and robust enough to be customised for other
   regions worldwide.},
DOI = {10.1017/S0007114515000987},
ISSN = {0007-1145},
EISSN = {1475-2662},
ORCID-Numbers = {Park, Jin Young/0000-0003-2491-5099},
Unique-ID = {WOS:000356924700016},
}

@article{ WOS:000354724300052,
Author = {Fu, Songzhe and Liu, Ying and Li, Xian and Tu, Junling and Lan, Ruiting
   and Tian, Huiqin},
Title = {A preliminary stochastic model for managing microorganisms in a
   recirculating aquaculture system},
Journal = {ANNALS OF MICROBIOLOGY},
Year = {2015},
Volume = {65},
Number = {2},
Pages = {1119-1129},
Month = {JUN},
Abstract = {Predicting the growth of key microorganisms is essential to improve the
   efficiency of wastewater treatment of recirculating aquaculture systems
   (RAS). We have developed a stochastic model to assess quantitatively the
   microbial populations in RAS. This stochastic model encompassed the
   growth model into the Monte Carlo simulation and was constructed with
   risk analysis software. A modified logistic model combined with the
   saturation growth-rate model was successfully developed to regress the
   growth curves of six microorganisms. Monte Carlo simulation was employed
   to model the effects of chemical oxygen demand (COD) on the maximum
   specific growth rate. Probabilistic distributions and predictions under
   the different COD ranges were generated for each simulated scenario. The
   coefficient of determination (R (2)) and bias factor (Bf) were used to
   assess the performance of an established model. Logistic model produced
   a good fit to the growth curve of Flavobacterium sp. (R (2) = 0.9511),
   Acinetobacter baumannii (R (2) = 0.9970), Sphingomonas paucimobilis (R
   (2) = 0.9086), Vibrio natriegens (R (2) = 0.9993), Lutimonas sp. (R (2)
   = 0.9872) and Bacillus pumilus (R (2) = 0.9816). Bacterial population
   structure was determined by the construction of 16S rRNA gene libraries.
   A regular variation trend was observed for the dominant groups during
   the entire process, with a decrease of
   Cytophaga-Flavobacterium-Bacteroidetes from 37.6 to 18.7 \% and an
   increase in Gammaproteobacteria from 8.5 to 30.6 \%. The predicted model
   agreed well with observed values except for Flavobacterium sp., and the
   results can be applied to predict key microorganisms in actual
   environments. The results of this study provide a method to monitor the
   dynamics of key microorganisms, which can also help to evaluate the
   impacts of microorganisms on the operations of RAS.},
DOI = {10.1007/s13213-014-0958-0},
ISSN = {1590-4261},
EISSN = {1869-2044},
ResearcherID-Numbers = {Fu, Songzhe/AGI-2630-2022
   },
ORCID-Numbers = {Fu, Songzhe/0000-0003-1754-199X
   Lan, Ruiting/0000-0001-9834-5258},
Unique-ID = {WOS:000354724300052},
}

@article{ WOS:000353032900003,
Author = {Ristov, Strahil and Korencic, Damir},
Title = {Fast construction of space-optimized recursive automaton},
Journal = {SOFTWARE-PRACTICE \& EXPERIENCE},
Year = {2015},
Volume = {45},
Number = {6},
Pages = {783-799},
Month = {JUN},
Abstract = {Finite-state automata are important components in information retrieval
   and natural language processing software. A recursive automaton is the
   most compact representation of the acyclic deterministic finite-state
   automata. It is based on merging not only the equivalent states but also
   the identical substructures in an automaton. The LZ trie variant is the
   state-of-the-art in automata compression regarding space, but the time
   needed for its construction was, until now, quadratic, which has made it
   impractical for large inputs. In this paper, we present the first
   algorithm for LZ trie construction that runs in effectively linear time
   thereby making it an attractive choice for finite-state automata
   implementation. We achieve this goal by adding a new functionality to
   the enhanced suffix array data structure. We present two variants of the
   construction procedure - an optimal method regarding the final size and
   a method that sacrifices some compression for low intermediate memory
   usage. We have made the implementation of our algorithms available in an
   open source software package LzLex.Copyright (c) 2014 John Wiley \&
   Sons, Ltd.},
DOI = {10.1002/spe.2261},
ISSN = {0038-0644},
EISSN = {1097-024X},
Unique-ID = {WOS:000353032900003},
}

@article{ WOS:000353176500003,
Author = {Du, Likai and Lan, Zhenggang},
Title = {An On-the-Fly Surface-Hopping Program JADE for Nonadiabatic Molecular
   Dynamics of Polyatomic Systems: Implementation and Applications},
Journal = {JOURNAL OF CHEMICAL THEORY AND COMPUTATION},
Year = {2015},
Volume = {11},
Number = {4},
Pages = {1360-1374},
Month = {APR},
Abstract = {Nonadiabatic dynamics simulations have rapidly become an indispensable
   tool for understanding ultrafast photochemical processes in complex
   systems. Here, we present our recently developed on-the-fly nonadiabatic
   dynamics package, JADE, which allows researchers to perform nonadiabatic
   excited-state dynamics simulations of polyatomic systems at an
   all-atomic level. The nonadiabatic dynamics is based on Tullys
   surface-hopping approach. Currently, several electronic structure
   methods (CIS, TDHF, TDDFT(RPA/TDA), and ADC(2)) are supported,
   especially TDDFT, aiming at performing nonadiabatic dynamics on medium-
   to large-sized molecules. The JADE package has been interfaced with
   several quantum chemistry codes, including Turbomole, Gaussian, and
   Gamess (US). To consider environmental effects, the Langevin dynamics
   was introduced as an easy-to-use scheme into the standard
   surface-hopping dynamics. The JADE package is mainly written in Fortran
   for greater numerical performance and Python for flexible interface
   construction, with the intent of providing open-source, easy-to-use,
   well-modularized, and intuitive software in the field of simulations of
   photochemical and photophysical processes. To illustrate the possible
   applications of the JADE package, we present a few applications of
   excited-state dynamics for various polyatomic systems, such as the
   methaniminium cation, fullerene (C-20), p-dimethylaminobenzonitrile
   (DMABN) and its primary amino derivative aminobenzonitrile (ABN), and
   10-hydroxybenzo{[}h]quinoline (10-HBQ).},
DOI = {10.1021/ct501106d},
ISSN = {1549-9618},
EISSN = {1549-9626},
ResearcherID-Numbers = {Du, Likai/AGE-4187-2022
   Lan, Zhenggang/H-3676-2012},
ORCID-Numbers = {Du, Likai/0000-0002-3198-276X
   Lan, Zhenggang/0000-0002-8509-0388},
Unique-ID = {WOS:000353176500003},
}

@article{ WOS:000352323100001,
Author = {Zalaki, Sajjad Mohammadpour and Fathian, Hosein and Zalaghi, Ebrahim and
   Hormozi, Farhad Kalantar},
Title = {Investigation of hydraulic parameters and cavitation in Kheir Abad flood
   release structure},
Journal = {CANADIAN JOURNAL OF CIVIL ENGINEERING},
Year = {2015},
Volume = {42},
Number = {4},
Pages = {213-221},
Month = {APR},
Abstract = {The simulation of flow behaviors before the construction of spillways
   using physical model and simulating packages is a crucial task. This
   paper investigates the flow behaviors in Kheir Abad flood release
   structure on Ab Shirin River, Iran by using Flow3D software and the
   results were compared with those of the physical model. First, by
   setting up a virtual model of spillway, the software performance was
   verified and calibrated. Then, by entering different flow discharge
   values, the two-dimensional flow hydraulics through the spillway was
   simulated by the software and flow depths, velocities and pressure were
   obtained for different flow discharges along the spillway. The
   cavitation was calculated based on the hydraulic parameters. The results
   show that Flow3D software is capable of simulating two-dimensional
   hydraulic flow over spillways. The results also indicate that the Kheir
   Abad flood release structure performs well during flood events
   considering cavitation.},
DOI = {10.1139/cjce-2014-0409},
ISSN = {0315-1468},
EISSN = {1208-6029},
ResearcherID-Numbers = {Fathian, Hossein/AAN-3528-2021},
ORCID-Numbers = {Fathian, Hossein/0000-0002-0555-4454},
Unique-ID = {WOS:000352323100001},
}

@article{ WOS:000348756700001,
Author = {Hadjidoukas, P. E. and Angelikopoulos, P. and Papadimitriou, C. and
   Koumoutsakos, P.},
Title = {Pi 4U: A high performance computing framework for Bayesian uncertainty
   quantification of complex models},
Journal = {JOURNAL OF COMPUTATIONAL PHYSICS},
Year = {2015},
Volume = {284},
Pages = {1-21},
Month = {MAR 1},
Abstract = {We present Pi 4U,(1) an extensible framework, for non-intrusive Bayesian
   Uncertainty Quantification and Propagation (UQ+P) of complex and
   computationally demanding physical models, that can exploit massively
   parallel computer architectures. The framework incorporates Laplace
   asymptotic approximations as well as stochastic algorithms, along with
   distributed numerical differentiation and task-based parallelism for
   heterogeneous clusters. Sampling is based on the Transitional Markov
   Chain Monte Carlo (TMCMC) algorithm and its variants. The optimization
   tasks associated with the asymptotic approximations are treated via the
   Covariance Matrix Adaptation Evolution Strategy (CMA-ES). A modified
   subset simulation method is used for posterior reliability measurements
   of rare events. The framework accommodates scheduling of multiple
   physical model evaluations based on an adaptive load balancing library
   and shows excellent scalability. In addition to the software framework,
   we also provide guidelines as to the applicability and efficiency of
   Bayesian tools when applied to computationally demanding physical
   models. Theoretical and computational developments are demonstrated with
   applications drawn from molecular dynamics, structural dynamics and
   granular flow. (C) 2014 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.jcp.2014.12.006},
ISSN = {0021-9991},
EISSN = {1090-2716},
ResearcherID-Numbers = {Papadimitriou, Costas/C-9600-2018
   Koumoutsakos, Petros/A-2846-2008
   Koumoutsakos, Petros/P-4535-2019
   },
ORCID-Numbers = {Koumoutsakos, Petros/0000-0001-8337-2122
   Koumoutsakos, Petros/0000-0001-8337-2122
   Papadimitriou, Costas/0000-0002-9792-0481
   Angelikopoulos, Panagiotis/0000-0001-5770-5691},
Unique-ID = {WOS:000348756700001},
}

@article{ WOS:000349671800015,
Author = {Suh, Kangho and Gabriel, Susan and Adams, Michelle A. and Arcona, Steve},
Title = {Health economics and outcomes research fellowship practices reviewed},
Journal = {RESEARCH IN SOCIAL \& ADMINISTRATIVE PHARMACY},
Year = {2015},
Volume = {11},
Number = {2},
Pages = {280-287},
Month = {MAR-APR},
Abstract = {The guidelines for health economics and outcomes research (HEOR)
   fellowship training programs devised by the American College of Clinical
   Pharmacy (ACCP) and the International Society of Pharmacoeconomics and
   Outcomes Research (ISPOR) suggest that continuous improvements are made
   to ensure that postgraduate training through didactic and professional
   experiences prepare fellows for HEOR research careers. The HEOR
   Fellowship Program at Novartis Pharmaceuticals Corporation was
   standardized to enhance the fellows' HEOR research understanding and
   align professional skill sets with the ACCP-ISPOR Fellowship Program
   Guidelines. Based on feedback from an internal task force comprised of
   HEOR employees and current and former fellows, the HEOR Fellowship
   Program was normatively and qualitatively assessed to evaluate the
   current curricular program. Fellowship program activities were
   instituted to ensure that the suggested minimum level requirements
   established by the guidelines were being met. Research opportunities
   enabling fellows to work hand-in-hand with other fellows and HEOR
   professionals were emphasized. Curricular enhancements in research
   methodology and professional training and development, and materials for
   a structured journal club focusing on specific methodological and HEOR
   research topics were developed. A seminar series (e. g., creating SMART
   Goals, StrengthsFinder 2.0) and professional courses (e. g., ISPOR short
   courses, statistics.com) were included to enhance the fellows' short-and
   long-term professional experience. Additional program attributes include
   an online reference library developed to enrich the current research
   facilities and a Statistical Analysis Software training program.
   Continuously assessing and updating HEOR fellowship programs keeps
   programs up-to-date in the latest HEOR concepts and approaches used to
   evaluate health care, both professionally and educationally. (C) 2015
   Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.sapharm.2014.07.006},
ISSN = {1551-7411},
EISSN = {1934-8150},
ORCID-Numbers = {Suh, Kangho/0000-0002-7906-9398},
Unique-ID = {WOS:000349671800015},
}

@article{ WOS:000347740500006,
Author = {Marek, Lukas and Zheng, Yudi and Ansaloni, Danilo and Bulej, Lubomir and
   Sarimbekov, Aibek and Binder, Walter and Tuma, Petr},
Title = {Introduction to dynamic program analysis with DiSL},
Journal = {SCIENCE OF COMPUTER PROGRAMMING},
Year = {2015},
Volume = {98},
Number = {1},
Pages = {100-115},
Month = {FEB 1},
Abstract = {Dynamic program analysis (DPA) tools assist in many software engineering
   and development tasks, such as profiling, program comprehension, and
   performance model construction and calibration. On the Java platform,
   many DPA tools are implemented either using aspect-oriented programming
   (AOP), or rely on bytecode instrumentation to modify the base program
   code. The pointcut/advice model found in AOP enables rapid tool
   development, but does not allow expressing certain instrumentations due
   to limitations of mainstream AOP languages-developers thus use bytecode
   manipulation to gain more expressiveness and performance. However, while
   the existing bytecode manipulation libraries handle some low-level
   details, they still make tool development tedious and error-prone.
   Targeting this issue, we provide the first complete presentation of
   DiSL, an open-source instrumentation framework that reconciles the
   conciseness of the AOP pointcut/advice model and the expressiveness and
   performance achievable with bytecode manipulation libraries.
   Specifically, we extend our previous work to provide an overview of the
   DiSL architecture, advanced features, and the programming model. We also
   include case studies illustrating successful deployment of DiSL-based
   DPA tools. (C) 2014 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.scico.2014.01.003},
ISSN = {0167-6423},
EISSN = {1872-7964},
ResearcherID-Numbers = {Marek, Lukas/AAF-3206-2019
   Bulej, Lubomír/G-3045-2014
   Tuma, Petr/A-3094-2009},
ORCID-Numbers = {Marek, Lukas/0000-0001-5473-8930
   Bulej, Lubomír/0000-0002-4573-6084
   Tuma, Petr/0000-0002-7035-2322},
Unique-ID = {WOS:000347740500006},
}

@article{ WOS:000348981900011,
Author = {Parida, Bikram K. and Panda, Prasanna K. and Misra, Namrata and Mishra,
   Barada K.},
Title = {MaxMod: a hidden Markov model based novel interface to MODELLER for
   improved prediction of protein 3D models},
Journal = {JOURNAL OF MOLECULAR MODELING},
Year = {2015},
Volume = {21},
Number = {2},
Month = {FEB},
Abstract = {Modeling the three-dimensional (3D) structures of proteins assumes great
   significance because of its manifold applications in biomolecular
   research. Toward this goal, we present MaxMod, a graphical user
   interface (GUI) of the MODELLER program that combines profile hidden
   Markov model (profile HMM) method with Clustal Omega programto
   significantly improve the selection of homologous templates and
   target-template alignment for construction of accurate 3D protein
   models. MaxMod distinguishes itself from other existing GUIs of MODELLER
   software by implementing effortless modeling of proteins using templates
   that bear modified residues. Additionally, it provides various features
   such as loop optimization, express modeling (a feature where protein
   model can be generated directly from its sequence, without any further
   user intervention) and automatic update of PDB database, thus enhancing
   the user-friendly control of computational tasks. We find that HMM-based
   MaxMod performs better than other modeling packages in terms of
   execution time and model quality. MaxMod is freely available as a
   downloadable standalone tool for academic and non-commercial purpose at
   http://www.immt.res.in/maxmod/.},
DOI = {10.1007/s00894-014-2563-3},
Article-Number = {30},
ISSN = {1610-2940},
EISSN = {0948-5023},
ORCID-Numbers = {Misra, Namrata/0000-0003-1813-6478},
Unique-ID = {WOS:000348981900011},
}

@article{ WOS:000350802200001,
Author = {Tipaldi, Massimo and Bruenjes, Bernhard},
Title = {Survey on Fault Detection, Isolation, and Recovery Strategies in the
   Space Domain},
Journal = {JOURNAL OF AEROSPACE INFORMATION SYSTEMS},
Year = {2015},
Volume = {12},
Number = {2},
Pages = {235-256},
Month = {FEB},
Abstract = {Massimo Tipaldi is the Space System Software Project Manager at CGSS. p.
   A. (OHB System AG), and he is currently working in the MeteoSat Third
   Generation Satellite Project at OHB System AG. He worked more than two
   years as the onboard Software (SW) Procurement Manager in the Galileo
   full operational capability satellite (GalileoSatFOC) program and was
   responsible for the GalileoSat FOC packet utilization standard tailoring
   process. He has almost 15 years of experience in the managerial and
   technical coordination of ESA/ASI/Centre National d'Etudes Spatiales SW
   projects (satellite systems, experimental equipment for the
   International Space Station, and ground segments). Mr. Tipaldi has a
   M.S. degree in computer science engineering, with specialization in
   automation and control systems from the University of Sannio, where he
   is currently attending a Ph. D. program focusing on fault detection,
   isolation, and recovery; and agent-based autonomy in space. His research
   interests also include time and space partitioning, onboard SW reference
   architecture, electrical power control systems, and advanced system
   control techniques; and he is the author of more than 10 papers
   published on international conferences.
   Bernhard Bruenjes is the Head of the Software Department of OHB System
   AG. He has occupied this position for the past 18 years. In that time,
   he has conducted and overseen software development activities for a
   variety of projects, mainly in the area of satellite systems or
   experimental equipment for the International Space Station. The
   categories of software products developed in this context are embedded
   and flight software, software for test facilities (electrical ground
   support equipment, special check out equipment), satellite reference
   databases, satellite simulators, software verification facility (SVF)
   applications, and ground control software. Mr. Bruenjes obtained his
   Dipl.-Ing. from the Hochschule Bremen in 1983 after studying electrical
   engineering and information technology. Since then, he has worked as an
   electronics and software developer for several companies in the fields
   of communication systems, computer technologies, and automotive systems.
   He joined OHB System AG in 1991, and he has since worked on software
   engineering disciplines as concept and architecture definitions, team
   organization and leadership, as well as the software development tool
   chain and related methodology implementation. He has special interests
   in standardized software architectures and building blocks, fault
   detection, isolation and recovery, future onboard computer
   architectures, and innovative data and control bus systems for space
   applications.},
DOI = {10.2514/1.I010307},
ISSN = {1940-3151},
EISSN = {2327-3097},
ResearcherID-Numbers = {Tipaldi, Massimo/AAO-9190-2021
   },
ORCID-Numbers = {Tipaldi, Massimo/0000-0003-4003-1613},
Unique-ID = {WOS:000350802200001},
}

@article{ WOS:000351255400021,
Author = {Xu, Jianchun and Guo, Chaohua and Wei, Mingzhen and Jiang, Ruizhong},
Title = {Impact of parameters' time variation on waterflooding reservoir
   performance},
Journal = {JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING},
Year = {2015},
Volume = {126},
Pages = {181-189},
Month = {FEB},
Abstract = {Currently, most oil reservoirs are in high water cut stage due to long
   term waterflooding, especially for those water-drive naturally fractured
   formations. Long term waterflooding changes the microscopic pore
   structure, rock skeleton, and clay minerals of these reservoirs through
   complex physical and chemical reactions during the process, and causes
   reservoir and fluid Parameters changing over time. Our experimental data
   show that rock permeability, wettability, and oil viscosity are
   time-variant parameters during waterflooding, and they are closely
   related to the continuous water flow through the porous media, which is
   important to characterize water-drive reservoirs and understand fluid
   flow in them.
   In this paper, experimental statistic data from cores of Shengli
   oilfield, China were utilized. Firstly, the relationship between
   permeability/wettability/oil viscosity and the ratio of flow-through
   water volume to pore volume were presented. Secondly, the ratio of grid
   flow-through water volume to pore volume, denoted as R-wpv, was used to
   describe the parameter time-variation mechanism. Finally, a numerical
   simulation software package was developed by introducing R-wpv into the
   traditional black oil model to characterize the time-variation mechanism
   in water-drive reservoirs. The new simulator was verified through
   comparing with commercial reservoir simulator ECLIPSE and experimental
   data. Simulation results show that the time-variation of permeability
   and oil viscosity decreases the ultimate recovery compared to the cases
   without considering time-variation effects; and wettability
   time-variation makes the reservoir rock more water-wet, which leads to
   higher ultimate recovery. This research found that both time-variation
   effects affect the ultimate oil recovery, but in different ways; and
   time-variation impact becomes increasingly significant as the
   waterflooding process becomes longer. Both the experimental results and
   numerical simulations significantly improve the interpretation of the
   parameters' time variation mechanisms and effect on waterflooding. (C)
   2014 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.petrol.2014.11.032},
ISSN = {0920-4105},
EISSN = {1873-4715},
ResearcherID-Numbers = {Xu, Jianchun/AAU-8601-2020
   },
ORCID-Numbers = {Xu, Jianchun/0000-0002-1213-0756
   Xu, Jianchun/0000-0001-5632-095X},
Unique-ID = {WOS:000351255400021},
}

@article{ WOS:000355649800005,
Author = {Aibinu, Ajibade A. and Dassanayake, Dharma and Chan, Toong-Khuan and
   Thangaraj, Ram},
Title = {Cost estimation for electric light and power elements during building
   design A neural network approach},
Journal = {ENGINEERING CONSTRUCTION AND ARCHITECTURAL MANAGEMENT},
Year = {2015},
Volume = {22},
Number = {2},
Pages = {190-213},
Abstract = {Purpose - The study reported in this paper proposed the use of
   artificial neural networks (ANN) as viable alternative to regression for
   predicting the cost of building services elements at the early stage of
   design. The purpose of this paper is to develop, test and validate ANN
   models for predicting the costs of electrical services components.
   Design/Methodology/Approach - The research is based on data mining of
   over 200 building projects in the office of a medium size electrical
   contractor. Of the over 200 projects examined, 71 usable data were found
   and used for the ANN modeling. Regression models were also explored
   using IBM Statistical Package for Social Sciences Statistics Software
   21, for the purpose of comparison with the ANN models.
   Findings - The findings show that the cost forecasting models based on
   ANN algorithm are more viable alternative to regression models for
   predicting the costs of light wiring, power wiring and cable pathways.
   The ANN prediction errors achieved are 6.4, 4.5 and 4.5 per cent for the
   three models developed whereas the regression models were insignificant.
   They did not fit any of the known regression distributions.
   Practical implications - The validated ANN models were converted to a
   desktop application (user interface) package - ``Intelligent
   Estimator.{''} The application is important because it can be used by
   construction professionals to reliably and quickly forecast the costs of
   power wiring, light wiring and cable pathways using building variables
   that are readily available or measurable during design stage, i.e. fully
   enclosed covered area, unenclosed covered area, internal perimeter
   length and number of floors.
   Originality/value - Previous studies have concluded that the methods of
   estimating the budget for building structure and fabric work are
   inappropriate for use with mechanical and electrical services. Thus,
   this study is unique because it applied the ANN modeling technique, for
   the first time, to cost modeling of electrical services components for
   building using real world data. The analysis shows that ANN is a better
   alternative to regression models for predicting cost of services
   elements because the relationship between cost and the cost drivers are
   non-linear and distribution types are unknown.},
DOI = {10.1108/ECAM-01-2014-0010},
ISSN = {0969-9988},
EISSN = {1365-232X},
ResearcherID-Numbers = {Ajibade, Aibinu/D-2635-2017
   Chan, Toong Khuan/G-8611-2015},
ORCID-Numbers = {Ajibade, Aibinu/0000-0001-6896-1682
   Chan, Toong Khuan/0000-0002-1985-6592},
Unique-ID = {WOS:000355649800005},
}

@article{ WOS:000350590600005,
Author = {Aziz, M. S. Abdul and Abdullah, M. Z. and Khor, C. Y.},
Title = {Thermal fluid-structure interaction of PCB configurations during the
   wave soldering process},
Journal = {SOLDERING \& SURFACE MOUNT TECHNOLOGY},
Year = {2015},
Volume = {27},
Number = {1},
Pages = {31-44},
Abstract = {Purpose - This paper aims to investigate the thermal fluid-structure
   interactions (FSIs) of printed circuit boards (PCBs) at different
   component configurations during the wave soldering process and
   experimental validation.
   Design/methodology/approach - The thermally induced displacement and
   stress on the PCB and its components are the foci of this study. Finite
   volume solver FLUENT and finite element solver ABAQUS, coupled with a
   mesh-based parallel code coupling interface, were utilized to perform
   the analysis. A sound card PCB (138 x 85 x 1.5 mm(3)), consisting of a
   transistor, diode, capacitor, connector and integrated circuit package,
   was built and meshed by using computational fluid dynamics
   pre-processing software. The volume of fluid technique with the
   second-order upwind scheme was applied to track the molten solder. C
   language was utilized to write the user-defined functions of the thermal
   profile. The structural solver analyzed the temperature distribution,
   displacement and stress of the PCB and its components. The predicted
   temperature was validated by the experimental results.
   Findings - Different PCB component configurations resulted in different
   temperature distributions, thermally induced stresses and displacements
   to the PCB and its components. Results show that PCB component
   configurations significantly influence the PCB and yield unfavorable
   deformation and stress.
   Practical implications - This study provides PCB designers with a
   profound understanding of the thermal FSI phenomenon of the process
   control during wave soldering in the microelectronics industry.
   Originality/value - This study provides useful guidelines and references
   by extending the understanding on the thermal FSI behavior of molten
   solder for PCBs. This study also explores the behaviors and influences
   of PCB components at different configurations during the wave soldering
   process.},
DOI = {10.1108/SSMT-07-2014-0013},
ISSN = {0954-0911},
EISSN = {1758-6836},
ResearcherID-Numbers = {Aziz, Mohd Sharizal Abdul/AAC-5640-2020
   Abdullah, Mohd Z/F-6443-2010
   Khor, C.Y./B-1541-2014},
ORCID-Numbers = {Aziz, Mohd Sharizal Abdul/0000-0002-3271-9242
   Abdullah, Mohd Z/0000-0002-5353-6162
   Khor, C.Y./0000-0002-4831-7416},
Unique-ID = {WOS:000350590600005},
}

@inproceedings{ WOS:000382966700010,
Author = {Cai, Ran and Gao, Hong and Liu, Dunxun and Guo, Hongbo},
Book-Group-Author = {IEEE},
Title = {A lightning electromagn-etic environment evaluation software package
   based on FDTD - the design and implementation},
Booktitle = {2015 INTERNATIONAL SYMPOSIUM ON LIGHTNING PROTECTION (XIII SIPDA)},
Year = {2015},
Pages = {65-68},
Note = {International Symposium on Lightning Protection (XIII SIPDA), Balneario
   Camboriu, BRAZIL, SEP 28-OCT 02, 2015},
Abstract = {The major cause of lightning disaster is the Lightning Electromagnetic
   Pulse (LEMP). Disaster risk assessment services have been widely used in
   lightning protection centers of many cities in China for years. At
   present, electromagnetic environment assessment services are being used
   more and more at many lightning protection centers in China, with
   various methods. Most of them adopt an expensive commercial software
   package which needs 3D modeling of complex structure and high
   computational costs. This article describes a useful assessment tool
   based on finite-difference time-domain (FDTD) method focusing on an
   electromagnetic environment of buildings featured with a simple setup
   procedure. This software package has been applied in daily evaluation
   services, filling gaps of the current commercial software, leading to a
   great potential in this area.},
ISBN = {978-1-4799-8754-2},
Unique-ID = {WOS:000382966700010},
}

@inproceedings{ WOS:000373939100003,
Author = {Chien, A. and Balaji, P. and Beckman, P. and Dun, N. and Fang, A. and
   Fujita, H. and Iskra, K. and Rubenstein, Z. and Zheng, Z. and Schreiber,
   R. and Hammond, J. and Dinan, J. and Laguna, I. and Richards, D. and
   Dubey, A. and van Straalen, B. and Hoemmen, M. and Heroux, M. and
   Teranishi, K. and Siegel, A.},
Editor = {Koziel, S and Leifsson, L and Lees, M and Krzhizhanovskaya, VV and Dongarra, J and Sloot, PMA},
Title = {Versioned Distributed Arrays for Resilience in Scientific Applications:
   Global View Resilience},
Booktitle = {INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE, ICCS 2015
   COMPUTATIONAL SCIENCE AT THE GATES OF NATURE},
Series = {Procedia Computer Science},
Year = {2015},
Volume = {51},
Pages = {29-38},
Note = {15th Annual International Conference on Computational Science (ICCS),
   Reykjavik Univ, Reykjavik, ICELAND, JUN 01-03, 2015},
Organization = {Elsevier; Univ Amsterdam; NTU Singapore; Univ Tennessee},
Abstract = {Exascale studies project reliability challenges for future
   high-performance computing (HPC) systems. We propose the Global View
   Resilience (GVR) system, a library that enables applications to add
   resilience in a portable, application-controlled fashion using versioned
   distributed arrays. We describe GVR's interfaces to distributed arrays,
   versioning, and cross-layer error recovery. Using several large
   applications (OpenMC, the preconditioned conjugate gradient solver PCG,
   ddcMD, and Chombo), we evaluate the programmer effort to add resilience.
   The required changes are small (<2\% LOC), localized, and
   machine-independent, requiring no software architecture changes. We also
   measure the overhead of adding GVR versioning and show that generally
   overheads <2\% are achieved. We conclude that GVR's interfaces and
   implementation are flexible and portable and create a gentle-slope path
   to tolerate growing error rates in future systems.},
DOI = {10.1016/j.procs.2015.05.187},
ISSN = {1877-0509},
ResearcherID-Numbers = {Richards, David/AAA-7297-2019
   Schreiber, Robert/Q-7550-2019
   Heroux, Michael/AAG-5894-2020
   },
ORCID-Numbers = {Heroux, Michael/0000-0002-5893-0273
   Iskra, Kamil/0000-0001-7000-4195},
Unique-ID = {WOS:000373939100003},
}

@inproceedings{ WOS:000412164600022,
Author = {Cunha, Joao Bruno and Ayres Neto, Arthur},
Book-Group-Author = {IEEE},
Title = {MAPIRA - A Support decision tool for marine pipeline risk assessment},
Booktitle = {2015 IEEE/OES ACOUSTICS IN UNDERWATER GEOSCIENCES SYMPOSIUM},
Year = {2015},
Note = {IEEE/OES Acoustics in Underwater Geosciences Symposium (RIO Acoustics),
   Rio de Janeiro, BRAZIL, JUL 29-31, 2015},
Organization = {IEEE OES},
Abstract = {The lack of a supporting decision tool in submarine pipeline launching
   activity inspired this work, which intends to provide a quantitative
   measure about the risks involved in this kind of job. This tool uses
   Graphs, Fuzzy logic, AHP (Analytic Hierarchy Process) and the help of
   many experienced professionals to achieve this goal. The Theory of
   Graphs (Euler, 1736) studies the relationship between objects in a
   specific mathematical set and has a fundamental unity which is composed
   by a structure of nodes that interconnect the objects through their
   edges, the so called Graph. Nowadays, the graphs are used to model real
   spatial distributions in a computational environment in order to solve
   routing problems, network flows and flowing streams of any kind. The
   Fuzzy Logic is able to deal with uncertainty better than Crispy logic,
   bringing more confidence in supporting decisions regarding subjective
   issues. Fuzzy Inference Systems are supporting decision making in many
   areas. This project uses graphs abstraction and Fuzzy Inference System
   in Submarine Engineering scope to aid in the marine pipelines' route
   selecting problem. The area where the pipe will be placed is surveyed
   with acoustic geophysical methods as bathymetry, side-scan sonar and
   sub-bottom profilers. These resultant maps are then interpreted using
   the risk classification developed in this work and the gridded area is
   related to a graph, where each graph node represents a grid node, and a
   graph edge represents the path between two adjacent nodes with a
   specific weight. The object of interest of this project is the method
   responsible for the weight input (risks magnitudes) into graph's edges
   based on a Fuzzy Inference System. The Fuzzy Inference System uses AHP
   and a compilation of many specialists' opinions. These opinions were
   collected from pipeline engineers with many years of experience. They
   went through an online questionnaire about several factors that
   represent risks to pipelines such as tectonic zones, faulting,
   subsurface gases, different types of ocean floor, and marine
   physiographic features, presence of garbage, areas susceptible to
   freezing and domains taken by other submarine facilities. Once each
   graph's edge has the weight given by the risk magnitude the safer path
   can be calculate by the shortest path algorithm. This work is based on a
   methodology which was applied in many Civil engineering construction
   projects. This new methodology coded in Java (R) programing language
   resulted in a Software called MAPIRA (Marine Pipeline Risk Assessment),
   a support decision tool that intends to aid marine pipeline launching
   professionals. It uses GeoTools (R) libraries (Since 2006 in OSGeo
   Project) and brings a way to solve the safer path problem to marine
   pipelines installation and providing a quantitative measure about the
   risks marine pipelines could face.},
ISBN = {978-1-4673-7019-6},
ResearcherID-Numbers = {Neto, Arthur Ayres/AAL-2963-2021},
ORCID-Numbers = {Neto, Arthur Ayres/0000-0002-2982-245X},
Unique-ID = {WOS:000412164600022},
}

@article{ WOS:000369703300012,
Author = {Dogan, Serkan and Asic, Adna and Buljubasic, Sanida and Besic, Larisa
   and Avdic, Monia and Feric, Elma and Hukic, Mirsada and Turan, Yusuf and
   Marjanovic, Damir},
Title = {OVERVIEW OF EUROPEAN POPULATION CLUSTERING BASED ON 23 Y-STR LOCI},
Journal = {GENETIKA-BELGRADE},
Year = {2015},
Volume = {47},
Number = {3},
Pages = {901-908},
Abstract = {Short tandem repeats (STRs) located on the Y-chromosome are a useful
   tool for various scientific fields, such as forensic investigation, but
   also for the investigation of population structure and molecular
   history. In this study, population data based on 23 Y-STR loci (DYS19,
   DYS389I, DYS389II, DYS390, DYS391, DYS392, DYS393, DYS385a/b, DYS437,
   DYS438, DYS439, DYS448, DYS456, DYS458, DYS635, GATAH4, DYS481, DYS533,
   DYS549, DYS570, DYS576, and DYS643) from 23 European human populations
   were compared. All haplotype data for this research were gathered from
   previously published articles. Arlequin v3.5.1.2, POPTREE2, and MEGA 5.1
   software packages were used for the calculation of allelic frequencies
   and genetic distance, and the construction of the European, as well as
   worldwide phylogenetic trees. Obtained results indicate a formation of
   several distinct sub-clusters within European population cluster.
   Observed sub-clusters were mostly recognized within geographically
   closer populations, meaning that neighboring populations were a part of
   the same subcluster in most of the cases. Compared with the previously
   published results obtained using autosomal STR markers, a significant
   level of concordance was detected. However, it seems that Y-STRs
   analyzed in this study are more informative since they enabled regional
   clustering in addition to continental clustering. Also, the use of a
   larger number of loci yielded clustering that is more specific than what
   has been calculated to date. Finally, it can be concluded that this
   study has shown that the application of a larger number of loci enables
   the more detailed insight into the relationships between European
   populations, compared to what has been published before.},
DOI = {10.2298/GENSR1503901D},
ISSN = {0534-0012},
EISSN = {1820-6069},
ResearcherID-Numbers = {Asic, Adna/D-2366-2018
   Dogan, Serkan/I-1162-2016
   },
ORCID-Numbers = {Asic, Adna/0000-0001-6197-9572
   Dogan, Serkan/0000-0001-6932-7165
   Marjanovic, Damir/0000-0002-7472-4525
   Besic, Larisa/0000-0003-0121-2745},
Unique-ID = {WOS:000369703300012},
}

@inproceedings{ WOS:000380398500030,
Author = {Gao Xianming and Wang Baosheng and Zhang Xiaozhe and Wang Xu'an},
Editor = {Xhafa, F and Barolli, L and Messina, F and Ogiela, MR},
Title = {Software Data Plane and Flow Switching Plane Separation in
   Next-Generation Router Architecture},
Booktitle = {2015 10TH INTERNATIONAL CONFERENCE ON P2P, PARALLEL, GRID, CLOUD AND
   INTERNET COMPUTING (3PGCIC)},
Year = {2015},
Pages = {194-199},
Note = {10th International Conference on P2P, Parallel, Grid, Cloud and Internet
   Computing 3PGCIC, Krakow, POLAND, NOV 04-06, 2015},
Abstract = {Routers have traditionally been architected as two elements: forwarding
   plane and control plane through ForCES or other protocols. Each
   forwarding plane aggregates a fixed amount of computing, memory, and
   network interface resources to forward packets. Unfortunately, the tight
   coupling of packet-processing tasks with network interfaces has severely
   restricted service innovation and hardware upgrade. In this context, we
   explore the insightful prospect of functional separation in forwarding
   plane to propose a next-generation router architecture, which, if
   realized, can provide promises both for various packet-processing tasks
   and for flexible deployment while solving concerns related to the above
   problems.
   Thus, we put forward an alternative construction in which functional
   resources within a forwarding plane are disaggregated. A forwarding
   plane is instead separated into two planes: software data plane (SDP)
   and flow switching plane (FSP). SDP is responsible for packet-processing
   tasks without its expansibility restricted with the amount and kinds of
   network interfaces. FSP is in charge of packet receiving/transmitting
   tasks and can incrementally add switching elements, such as general
   switches, or even specialized switches, to provide network interfaces
   for SDP. At last, we make an experiment on our platform in terms of
   bandwidth utilization rate, configuration delay, and the processing time
   of a simple router. Our experimental results show that the separation of
   SDP and FSP brings greater modularity to router architecture, allowing
   operators to optimize their deployments.},
DOI = {10.1109/3PGCIC.2015.13},
ISBN = {978-1-4673-9473-4},
ResearcherID-Numbers = {Zhang, XZ/HJA-4189-2022
   Wang, Xu An/AAC-1494-2019
   wang, xuan/GXF-3679-2022
   },
ORCID-Numbers = {Wang, Xu An/0000-0003-2070-4913},
Unique-ID = {WOS:000380398500030},
}

@inproceedings{ WOS:000372316000039,
Author = {Garcia, Manuel Jimenez and Retsin, Gilles},
Editor = {Martens, B and Wurzer, G and Grasl, T and Lorenz, WE and Schaffranek, R},
Title = {Design Methods for Large Scale Printing},
Booktitle = {ECAADE 2015: REAL TIME - EXTENDING THE REACH OF COMPUTATION, VOL 2},
Year = {2015},
Pages = {331-339},
Note = {33rd International Conference on Education and Research in Computer
   Aided Architectural Design in Europe (eCAADe), TU Wien, Vienna, AUSTRIA,
   SEP 16-18, 2015},
Organization = {Autodesk; Bentley; TU Wien, Fac Architecture \& Reg Planning; ArchIng
   Akademie},
Abstract = {With an exponential increase in the possibilities of computation and
   computer-controlled fabrication, high density information is becoming a
   reality in digital design and architecture. However, construction
   methods and industrial fabrication processes have not yet been reshaped
   to accommodate the recent changes in those disciplines. Although it is
   possible to build up complex simulations with millions of particles, the
   simulation is often disconnected from the actual fabrication process.
   Our research proposes a bridge between both stages, where one drives the
   other, producing a smooth transition from design to production. The
   research showcased in this paper investigates tectonic systems
   associated with large scale 3D printing and additive manufacturing
   methods, inheriting both material properties and fabrication constraints
   at all stages from design to production. Computational models and custom
   design software packages are designed and developed as strategies to
   organise material in space in response to specific structural and
   logistical input. Filamentrics, the first of two projects described,
   intends to develop free-form space frames with robotic plastic
   extrusion. Through the use of custom made extruders a vast range of
   prototypes were developed, evolving the design process towards the
   fabrication of precise structures that can be materialised using
   additive manufacturing without the use of a layered printing method.
   Instead, material limitations were studied and embedded in custom
   algorithms that allow depositing material in the air for internal
   connectivity. While Filamentrics is reshaping the way we could design
   and build light-weight structures, the second project Microstrata aims
   to establish new construction methods for compression based materials. A
   layering 3D printing method combines both the deposition of the binder
   and the distribution of an interconnected network of capillaries. These
   capillaries are organised following structural principles, configuring a
   series of channels which are left empty within the mass. In a second
   stage aluminium is cast in this hollow space to build a continuous
   tension reinforcement.},
ISBN = {978-94-91207-09-9},
Unique-ID = {WOS:000372316000039},
}

@inproceedings{ WOS:000398586305132,
Author = {Hallo, M. and Lujan-Mora, S. and Trujillo, J.},
Editor = {Chova, LG and Martinez, AL and Torres, IC},
Title = {AN APPROACH TO PUBLISH STATISTICS FROM OPEN-ACCESS JOURNALS USING LINKED
   DATA TECHNOLOGIES},
Booktitle = {INTED2015: 9TH INTERNATIONAL TECHNOLOGY, EDUCATION AND DEVELOPMENT
   CONFERENCE},
Series = {INTED Proceedings},
Year = {2015},
Pages = {5940-5948},
Note = {9th International Technology, Education and Development Conference
   (INTED), Madrid, SPAIN, MAR 02-04, 2015},
Abstract = {Semantic web encourages digital libraries that include open access
   journals, to collect, link and share their data across the web in order
   to ease its processing by machines and humans to get better queries and
   results. Linked Data technologies enable connecting structured data
   across the web using the principles and recommendations set out by Tim
   Berners-Lee in 2006.
   Several universities develop knowledge, through scholarship and
   research, under open access policies and use several ways to disseminate
   information. Open access journals collect, preserve and publish
   scientific information in digital form using a peer review process. The
   evaluation of the usage of this kind of publications needs to be
   expressed in statistics and linked to external resources to give better
   information about the resources and their relationships. The statistics
   expressed in a data mart facilitate queries about the history of
   journals usage by several criteria. This data linked to another datasets
   gives more information such as: the topics in the research, the origin
   of the authors, the relation to the national plans, and the relations
   with the study curriculums.
   This paper reports a process to publish an open access journal data mart
   on the web using Linked Data technologies in such a way that it can be
   linked to related datasets. Furthermore, methodological guidelines are
   presented with related activities. The proposed process was applied
   extracting data from a university open journal system data mart and
   publishing it in a SPARQL endpoint using the open source edition of the
   software OpenLink Virtuoso. In this process the use of open standards
   facilitates the creation, development and exploitation of knowledge. The
   RDF data cube vocabulary has been used as a model to publish the
   multidimensional data on the web. The visualization was made using
   CubeViz a faceted browser filtering observations to be presented
   interactively in charts. The proposed process help to publish
   statistical datasets in an easy way.},
ISSN = {2340-1079},
ISBN = {978-84-606-5763-7},
ResearcherID-Numbers = {Luján-Mora, Sergio/D-9207-2013
   Trujillo, Juan/L-7079-2014
   },
ORCID-Numbers = {Luján-Mora, Sergio/0000-0001-5000-864X
   Trujillo, Juan/0000-0003-0139-6724
   Hallo, Maria/0000-0002-6718-0603},
Unique-ID = {WOS:000398586305132},
}

@inproceedings{ WOS:000380406200060,
Author = {Hsieh, Fu-Shiung},
Book-Group-Author = {IEEE},
Title = {Location-aware Workflow Scheduling in Supply Chains based on Multi-agent
   Systems},
Booktitle = {2015 CONFERENCE ON TECHNOLOGIES AND APPLICATIONS OF ARTIFICIAL
   INTELLIGENCE (TAAI)},
Year = {2015},
Pages = {441-448},
Note = {Conference on Technologies and Applications of Artificial Intelligence
   (TAAI), Tainan, TAIWAN, NOV 20-22, 2015},
Organization = {IEEE; NUTN; KUAS; INC Most; IEEE Computat Inteliigence Soc; TCGA;
   Sigmobile Taiwan; IEEE CIS Tainan Chapter; KIIS},
Abstract = {In construction industry, companies form a supply chain to respond to
   business opportunities. The complex workflows, dependency between
   partners and their location pose a big challenge in construction project
   management. How to schedule activities to meet the construction project
   requirements under resource constraints is an important issue. To create
   a feasible schedule for a construction project, companies in a typical
   construction supply chain need to negotiate with each other. Development
   of an effective software system to support negotiation and collaboration
   between the partners in a construction supply chain is urgent. Execution
   of workflows in a construction project usually depends on location.
   Although workflow management problems have been extensively studied for
   decades, location information of workflows is rarely taken into account
   in existing literature. In this paper, we will study the development of
   a location-aware workflow scheduling system for construction supply
   chains. We will propose a flexible scheduling system to optimize the
   construction project schedule based on collaboration of
   entities/partners in a construction supply chain. We propose a
   methodology that includes modeling of location-aware workflows in
   construction projects based on formal workflow models and develop a
   technique to transform workflow models to formulate and solve a project
   scheduling problem. We propose architecture to implement a
   location-aware multi-agent scheduling system based on JADE and Google
   API. The proposed methodology is verified by an example.},
ISBN = {978-1-4673-9606-6},
ResearcherID-Numbers = {HSIEH, FU-SHIUNG/AAH-6245-2020},
Unique-ID = {WOS:000380406200060},
}

@article{ WOS:000364457000042,
Author = {Isaac, Tobin and Burstedde, Carsten and Wilcox, Lucas C. and Ghattas,
   Omar},
Title = {RECURSIVE ALGORITHMS FOR DISTRIBUTED FORESTS OF OCTREES},
Journal = {SIAM JOURNAL ON SCIENTIFIC COMPUTING},
Year = {2015},
Volume = {37},
Number = {5},
Pages = {C497-C531},
Abstract = {The forest-of-octrees approach to parallel adaptive mesh refinement and
   coarsening has recently been demonstrated in the context of a number of
   large-scale PDE-based applications. Efficient reference software has
   been made freely available to the public both in the form of the
   standalone p4est library and more indirectly by the general-purpose
   finite element library deal. II, which has been equipped with a p4est
   backend. Although linear octrees, which store only leaf octants, have an
   underlying tree structure by definition, it is not fully exploited in
   previously published mesh-related algorithms. This is because tree
   branches are not explicitly stored and because the topological
   relationships in meshes, such as the adjacency between cells, introduce
   dependencies that do not respect the octree hierarchy. In this work, we
   combine hierarchical and topological relationships between octants to
   design efficient recursive algorithms that operate on distributed
   forests of octrees. We present three important algorithms with recursive
   implementations. The first is a parallel search for leaves matching any
   of a set of multiple search criteria, such as leaves that contain points
   or intersect polytopes. The second is a ghost layer construction
   algorithm that handles arbitrarily refined octrees that are not covered
   by previous algorithms, which require a 2: 1 condition between
   neighboring leaves. The third is a universal mesh topology iterator.
   This iterator visits every cell in a partition, as well as every
   interface (face, edge, and corner) between these cells. The iterator
   calculates the local topological information for every interface that it
   visits, taking into account the nonconforming interfaces that increase
   the complexity of describing the local topology. To demonstrate the
   utility of the topology iterator, we use it to compute the numbering and
   encoding of higher-order C-0 nodal basis functions used for finite
   elements. We analyze the complexity of the new recursive algorithms
   theoretically and assess their performance, both in terms of
   single-processor efficiency and in terms of parallel scalability,
   demonstrating good weak and strong scaling up to 458,000 cores of the
   JUQUEEN supercomputer.},
DOI = {10.1137/140970963},
ISSN = {1064-8275},
EISSN = {1095-7197},
ORCID-Numbers = {Isaac, Tobin/0000-0002-2628-3585},
Unique-ID = {WOS:000364457000042},
}

@inproceedings{ WOS:000374706100237,
Author = {Isupov, Fedor Yu and Ganin, V, Sergey and Kodzhaspirov, George E. and
   Tsemenko, Valeriy and Michailov, Vesselin G.},
Book-Group-Author = {TANGER Ltd},
Title = {NUMERICAL SIMULATION OF SHEET PROFILING PROCESS FROM ALUMINUM ALLOY},
Booktitle = {METAL 2015: 24TH INTERNATIONAL CONFERENCE ON METALLURGY AND MATERIALS},
Year = {2015},
Pages = {1463-1468},
Note = {24th International Conference on Metallurgy and Materials, Brno, CZECH
   REPUBLIC, JUN 03-05, 2015},
Organization = {Tanger Ltd; Tech Univ Ostrava; Czech Soc New Mat \& Technologies; ASM
   Int; Engn Acad Czech Republ; Mat Res Soc Serbia; Norwegian Co Mat \&
   Technol; French Soc Met \& Mat; Italian Assoc Met; Austrian Soc Met \&
   Mat; Portuguese Soc Mat},
Abstract = {Structured sheet metals are becoming more and more popular for using in
   lightweight constructions. In the present research three different
   schemes to produce structured sheet from aluminum AW6061-T6, namely
   stretch forming (two type of die) and stamping using elastomer, were
   investigated. Evaluation criteria for application of these schemes were
   proposed. Numerical models for each technological scheme with some
   assumptions were developed in the software package DEFORM (TM). The
   dimensions of sheet were limited only by the thickness (1 mm) and for
   numerical simulation a little part of the sheet was used. Influence of
   FE-mesh (type and number of elements) on calculation accuracy was
   estimated. Also such characteristics as power parameters of process,
   fields of internal stress and geometry of resulting profile were
   estimated.},
ISBN = {978-80-87294-62-8},
ResearcherID-Numbers = {Isupov, Fedor Yurevich/F-8639-2017
   Kodzhaspirov, Georgii E/T-5749-2017
   Ganin, Sergei V./A-5819-2014
   Michailov, Vesselin G/X-7003-2018
   },
ORCID-Numbers = {Michailov, Vesselin G/0000-0002-7886-3637
   Ganin, Sergei/0000-0002-2307-9319},
Unique-ID = {WOS:000374706100237},
}

@inproceedings{ WOS:000380479800047,
Author = {Kiefer, Marc Aurel and Molitorisz, Korbinian and Bieler, Jochen and
   Tichy, Walter F.},
Book-Group-Author = {IEEE},
Title = {Parallelizing a Real-time Audio Application - A Case Study in
   Multithreaded Software Engineering},
Booktitle = {2015 IEEE 29TH INTERNATIONAL PARALLEL AND DISTRIBUTED PROCESSING
   SYMPOSIUM WORKSHOPS},
Year = {2015},
Pages = {405-414},
Note = {29th IEEE International Parallel and Distributed Processing Symposium
   (IPDPS), Hyderabad, INDIA, MAY 25-29, 2015},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {Multicore hardware is ubiquitous, but billions of lines of code in
   performance-critical commodity software are still sequential. Although
   parallel libraries, design patterns, and best practice guidelines are
   available, thinking parallel is still a big challenge for many software
   engineers.
   In this paper we present a case study on parallelizing commodity
   software using a commercial real-time audio application with over
   700,000 lines of code. In contrast to best practice guidelines, our goal
   is to investigate what parallelization strategy can effectively be used
   in data stream-intensive applications. Performing an in-depth analysis
   of the software architecture and its run-time performance, we locate
   parallelization potential and propose three different parallelization
   strategies. We evaluate them with respect to their parallel performance
   impact.
   Regarding the application's intrinsic real-time requirement and a very
   short audio cycle turnaround time, a busy-waiting strategy offers the
   best audio performance of 327 mu s per cycle on an eight-core machine.
   With an efficiency of 99\% this is close to the optimal schedule.},
DOI = {10.1109/IPDPSW.2015.32},
ISBN = {978-1-4673-7684-6},
Unique-ID = {WOS:000380479800047},
}

@inproceedings{ WOS:000380453900075,
Author = {Kolek, L. and Ibrahim, M. Yousef and Gunawan, I. and Laribi, M. A. and
   Zegloul, S.},
Book-Group-Author = {IEEE},
Title = {Evaluation of Control System Reliability using combined Dynamic Fault
   Trees and Markov Models},
Booktitle = {PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS
   (INDIN)},
Series = {IEEE International Conference on Industrial Informatics INDIN},
Year = {2015},
Pages = {536-543},
Note = {13th IEEE International Conference on Industrial Informatics (INDIN),
   Cambridge, UNITED KINGDOM, JUL 22-24, 2015},
Organization = {IEEE; IEEE Ind Elect Soc (IES); Anglia Ruskin Univ; Inst of Engn Techn},
Abstract = {In this paper, dynamic simulation methods for reliability evaluation of
   common industry-based control system architectures are investigated.
   Control system design often employs complex reliability structures in
   the forms of several levels of software and hardware redundancies, hot
   and cold standby systems. This is required in order to achieve certain
   plant availability and safety functions. Control system maintenance
   requires expert knowledge due to the complexity of troubleshooting steps
   involved with a hardware or software failures of a large system. Hence,
   it is crucial to understand the effect of recovery time on reliability
   and on overall availability in a critical control system. Dynamic Fault
   Tree Analysis (DFTA), Markov Chains and Reliability Block Diagrams (RBD)
   are presented and a block library is introduced for addressing the
   aforementioned modelling problems. In order to be able to evaluate
   dynamic fault trees and Markov Chains, Monte Carlo simulation has been
   used.},
ISSN = {1935-4576},
ISBN = {978-1-4799-6649-3},
ResearcherID-Numbers = {Laribi, Med amine/AAA-7425-2019},
ORCID-Numbers = {Laribi, Med amine/0000-0003-0797-7669},
Unique-ID = {WOS:000380453900075},
}

@inproceedings{ WOS:000373517300063,
Author = {Lall, Pradeep and Sakalaukus, Peter and Davis, Lynn},
Book-Group-Author = {ASME},
Title = {AN INVESTIGATION OF CATASTROPHIC FAILURE IN SOLID-STATE LAMPS EXPOSED TO
   HARSH ENVIRONMENT OPERATIONAL CONDITIONS},
Booktitle = {INTERNATIONAL TECHNICAL CONFERENCE AND EXHIBITION ON PACKAGING AND
   INTEGRATION OF ELECTRONIC AND PHOTONIC MICROSYSTEMS, 2015, VOL 2},
Year = {2015},
Note = {ASME International Technical Conference and Exhibition on Packaging and
   Integration of Electronic and Photonic Microsystems (InterPACK), San
   Francisco, CA, JUL 06-09, 2015},
Organization = {ASME},
Abstract = {Today's lighting technology is steadily becoming more energy efficient
   and less toxic to the environment since the passing of the Energy
   Independence and Security Act of 2007 (EISA) {[}1]. EISA has mandated a
   higher energy efficiency standard for lighting products and the phase
   out of the common incandescent lamp. This has led lighting manufacturers
   to pursue solid-state lighting (SSL) technologies for consumer lighting
   applications. However, two major roadblocks are hindering the transition
   process to SSL lamps: cost and quality. In order to cut cost,
   manufactures are moving towards cheaper packaging materials and a
   variety of package architecture construction techniques which may
   potentially erode the quality of the lamp and reduce its survivability
   in everyday applications. Typically, SSL lamps are given product
   lifetimes of over twenty years based off of the IES TM-21-11 lighting
   standard which does not include moisture effects or large operational
   temperatures {[}2]. A group of recently released off-the-shelf lamps
   have undergone a steady-state temperature humidity bias life test of 85
   degrees C/85\%RH (85/85) to investigate the reliability in harsh
   environment applications.
   The lack of accelerated test methods for lamps to assess reliability
   prior to introduction into the marketplace does not exist in literature.
   There is a need for SSL physics based models for the assessment and
   prediction of a lamp's lifetime which is being spearheaded by the DOE
   {[}3]. In order to be fully accepted in the marketplace, SSL lamps must
   be able to perform similarly to incandescent lamps in these
   environments, as well as live up to the lifetime claims of
   manufacturers.
   A lamp's package architecture must be designed with performance factors
   in mind, as well as address some of the known and published package
   related failure mechanisms, such as carbonization of the encapsulant
   material, delamination, encapsulant yellowing, lens cracking, and
   phosphor thermal quenching {[}4]. Each failure mechanism produces the
   similar failure mode of lumen degradation predominately due to two
   contributing factors: high junction temperature and moisture ingress.
   The current state-of-the-art has focused on individual areas of the
   lamp, such as the LED chip, substrate material, electrical driver design
   and thermal management techniques. {[}5] -{[}16] Looking at the lamp as
   a whole is a novel approach and has not been seen before in literature.
   This work followed the JEDEC standard JESD22-A101C of 85/85 with a one
   hour interval of applied voltage followed by a one hour interval of no
   applied voltage {[}17]. This test was performed continuously for each
   SSL lamp until it became nonoperational, i.e. did not turn on.
   Periodically, photometric measurements were taken following the IES
   LM-79-08 standard at room temperature using an integrating sphere, a
   spectrometer, and lighting software. The overall health of the SSL lamps
   was assed using the relative luminous flux (RLF), correlated color
   temperature (CCT) and the color difference (Delta u'v') using the
   Euclidean distance of the CIE 1976 color space coordinates. Finally, a
   Weibull analysis was completed to compare the characteristic lifetime of
   the SSL lamp to the actual rated lifetime. An important result from this
   work shows that the rated lifetime does not come close to the actual
   lifetime when the SSL lamps are used in a harsh humid environment which
   is fairly common in outdoor applications across the U.S. Also, the
   photometric results are presented for the entire lifetime of each SSL
   lamp under test.},
Article-Number = {V002T02A045},
ISBN = {978-0-7918-5689-5},
ResearcherID-Numbers = {Davis, Lynn/AAI-2452-2020
   },
ORCID-Numbers = {Davis, Lynn/0000-0001-6023-6558
   Sakalaukus, Peter/0000-0003-3691-4210},
Unique-ID = {WOS:000373517300063},
}

@inproceedings{ WOS:000485516500058,
Author = {Li, Zhenjiang and Xie, Yaxiong and Li, Mo and Jamieson, Kyle},
Book-Group-Author = {Assoc Comp Machinery},
Title = {Recitation: Rehearsing Wireless Packet Reception in Software},
Booktitle = {MOBICOM `15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON
   MOBILE COMPUTING AND NETWORKING},
Year = {2015},
Pages = {291-303},
Note = {21st Annual International Conference on Mobile Computing and Networking
   (MobiCom), Paris, FRANCE, SEP 07-11, 2015},
Organization = {Assoc Comp Machinery; Assoc Comp Machinery SIGMOBILE; Atos Worldline;
   NSF; CISCO; Google; HP; Alcatel Lucent; Huawei; Inria; LIP6; Microsoft;
   NEC Labs; Orange; Technicolor; UPMC},
Abstract = {This paper presents Recitation, the first software system that uses
   lightweight channel state information (CSI) to accurately predict
   error-prone bit positions in a packet so that applications atop the
   wireless physical layer may take the best action during subsequent
   transmissions. Our key insight is that although Wi-Fi wifeless physical
   layer operations are complex, they are deterministic. This enables us to
   rehearse physical-layer operations on packet bits before they are
   transmitted. Based on this rehearsal, we calculate a hidden parameter in
   the decoding process, called error event probability (EVP). EVP captures
   fine-grained information about the receiver's convolutional or LDPC
   decoder, allowing Recitation to derive precise information about the
   likely fate of every hit in subsequent packets, without any wireless
   channel training. Recitation is the first system of its kind that is
   both software-implementable and compatible with the existing 802.11
   architecture for both SISO and MIMO settings. We experiment with
   commodity Atheros 9580 WiFi NICs to demonstrate Recitation's utility
   with three representative applications in static, mobile, and
   interference-dominated scenarios. We show that Recitation achieves
   33.8\% and 16\% average throughput gains for bit-rate adaptation and
   partial packet recovery, respectively, and 6 dB PSNR quality improvement
   for unequal error protection-based video.},
DOI = {10.1145/2789168.2790126},
ISBN = {978-1-4503-3619-2},
ORCID-Numbers = {Li, Mo/0000-0002-6047-9709
   Jamieson, Kyle/0000-0002-7940-2867
   Xie, Yaxiong/0000-0003-4258-6655},
Unique-ID = {WOS:000485516500058},
}

@inproceedings{ WOS:000380453900207,
Author = {Lindgren, Per and Lindner, Marcus and Lindner, Andreas and Pereira,
   David and Pinho, Luis Miguel},
Book-Group-Author = {IEEE},
Title = {Well-formed Control Flow for Critical Sections in RTFM-core},
Booktitle = {PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS
   (INDIN)},
Series = {IEEE International Conference on Industrial Informatics INDIN},
Year = {2015},
Pages = {1438-1445},
Note = {13th IEEE International Conference on Industrial Informatics (INDIN),
   Cambridge, UNITED KINGDOM, JUL 22-24, 2015},
Organization = {IEEE; IEEE Ind Elect Soc (IES); Anglia Ruskin Univ; Inst of Engn Techn},
Abstract = {The mainstream of embedded software development as of today is dominated
   by C programming. To aid the development, hardware abstractions,
   libraries, kernels and lightweight operating systems are commonplace.
   Such kernels and operating systems typically impose a thread based
   abstraction to concurrency. However, in general thread based programming
   is hard, plagued by race conditions and dead-locks.
   For this paper we take an alternative outset in terms of a language
   abstraction, RTFM-core, where the system is modelled directly in terms
   of tasks and resources. In compliance to the Stack Resource Policy (SRP)
   model, the language enforces (well-formed) LIFO nesting of claimed
   resources, thus SRP based analysis and scheduling can be readily
   applied. For the execution onto bare-metal single core architectures,
   the rtfm-core compiler performs SRP analysis on the model and render an
   executable that is deadlock free and (through RTFM-kernel primitives)
   exploits the underlying interrupt hardware for efficient scheduling. The
   RTFM-core language embeds C-code and links to C-object files and
   libraries, and is thus applicable to the mainstream of embedded
   development.
   However, while the language enforces well-formed resource management,
   control flow in the embedded C-code may violate the LIFO nesting
   requirement.
   In this paper we address this issue by lifting a subset of C into the
   RTFM-core language allowing arbitrary control flow at the model level.
   In this way well-formed LIFO nesting can be enforced, and models ensured
   to be correct by construction. We demonstrate the feasibility by means
   of a prototype implementation in the rtfm-core compiler. Additionally,
   we develop a set of running examples and show in detail how control flow
   is handled at compile time and during run-time execution.},
ISSN = {1935-4576},
ISBN = {978-1-4799-6649-3},
ResearcherID-Numbers = {Lindner, Andreas/E-3022-2017
   Pereira, David/M-4220-2016
   Pereira, David/X-6401-2019
   Pinho, Luis Miguel/M-3416-2013},
ORCID-Numbers = {Lindner, Andreas/0000-0001-5311-1781
   Pereira, David/0000-0002-7561-6649
   Pereira, David/0000-0002-7561-6649
   Pinho, Luis Miguel/0000-0001-6888-1340},
Unique-ID = {WOS:000380453900207},
}

@inproceedings{ WOS:000371885401061,
Author = {Loianno, Giuseppe and Mulgaonkar, Yash and Brunner, Chris and Ahuja,
   Dheeraj and Ramanandan, Arvind and Chari, Murali and Diaz, Serafin and
   Kumar, Vijay},
Book-Group-Author = {IEEE},
Title = {Smartphones Power Flying Robots},
Booktitle = {2015 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)},
Series = {IEEE International Conference on Intelligent Robots and Systems},
Year = {2015},
Pages = {1256-1263},
Note = {IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS), Hamburg, GERMANY, SEP 28-OCT 02, 2015},
Organization = {IEEE; RSJ; Univ Hamburg; DFG; RA; New Technol Fdn; SICE; KUKA; DJI;
   Rethink Robot; BOSCH; Chinese Acad Sci, SIAT; Boozhong; Adept;
   Automatica; HIT; Ascending Technol; OPTOFORCE; DST Robot; BA Syst;
   Rainbow Robot; SIA; CLEARPATH Robot; Swiss Natl Ctr Competence Res
   Robot; SINEVA; Dyson; SICK; Robocept; Force Dimension; Open Unit Robot;
   Luoyang Natl Univ Sci Park; Fuzhou Univ; Synapticon; Google; Technishce
   Univ Munchen; iRobot; Echord++; Khalifa Univ; Pan Robot; FESTO; Kinova
   Robot; SCHUNK; ies},
Abstract = {Consumer grade technology seen in cameras and phones has led to the
   price/performance ratio of sensors and processors falling dramatically
   over the last decade. In particular, most devices are packaged with a
   camera, a gyroscope, and an accelerometer, important sensors for aerial
   robotics. The low mass and small form factor make them particularly well
   suited for autonomous flight with small flying robots, especially in
   GPS-denied environments. In this work, we present the first fully
   autonomous smartphone-based quadrotor. All the computation, sensing and
   control runs on an off-the-shelf smartphone, with all the software
   functionality in a smartphone app. We show how quadrotors can be
   stabilized and controlled to achieve autonomous flight in indoor
   buildings with application to smart homes, search and rescue,
   construction and architecture. The work allows any consumer with a
   smartphone to autonomously drive a quadrotor robot platform, even
   without GPS, by downloading an app, and concurrently build 3-D maps.},
ISSN = {2153-0858},
ISBN = {978-1-4799-9994-1},
ResearcherID-Numbers = {Kumar, Vijay/AAM-8545-2020
   Loianno, Giuseppe/E-7473-2017
   },
ORCID-Numbers = {Loianno, Giuseppe/0000-0002-3263-5401
   Kumar, Vijay/0000-0002-3902-9391},
Unique-ID = {WOS:000371885401061},
}

@inproceedings{ WOS:000380490000043,
Author = {Mondrup, T. F. and Treldal, N. and Karlshoj, J. and Vestergaard, F.},
Editor = {Mahdavi, A and Martens, B and Scherer, R},
Title = {Introducing a new framework for using generic Information Delivery
   Manuals},
Booktitle = {EWORK AND EBUSINESS IN ARCHITECTURE, ENGINEERING AND CONSTRUCTION 2014},
Year = {2015},
Pages = {295-301},
Note = {PROCEEDINGS OF THE 10TH EUROPEAN CONFERENCE ON PRODUCT AND PROCESS
   MODELLING (ECPPM 2014), Department of Building Physics and Building
   Ecology of the Vienna Universi, Vienna, AUSTRIA, SEP 17-19, 2014},
Abstract = {Information flow management plays a significant role in ensuring the
   reliable exchange of Building Information Modeling (BIM) data between
   project team members in the Architecture, Engineering, Construction, and
   Facility Management (AEC/FM) industry. The buildingSMART standard
   approach to resolving this issue is based upon the Information Delivery
   Manual (IDM), which provides a collaborative methodology for specifying
   AEC/FM process flows and their information contents. The IDMs in current
   use indicate that focus has mainly been on formalizing more general
   parts of the building design process, where multiple project team
   members perform a wide range of design tasks. Because IDMs typically
   describe such complex processes, they are difficult to manage and
   complicated to implement in real-world AEC/FM projects. In this study,
   we address these challenges by proposing a Work Breakdown Structure
   (WBS) methodology, breaking down the IDMs into smaller IDM Packages. We
   introduce a modular IDM Framework aimed at defining and organizing
   generic IDM Packages for all main use cases of the AEC/FM project life
   cycle. In this methodology, an IDM Project Plan can be created by
   selecting the specific IDM Packages required for a specific AEC/FM
   project. Ultimately, we believe that the IDM Framework will help improve
   information flow management and the reusability of IDM Packages amongst
   unique AEC/FM projects. In addition, we believe that the IDM Framework
   will support the potential harmonization of the development of new IDMs,
   as the specific context of each IDM Package, and the relationship to
   other IDM Packages, becomes clearer. Such harmonization is also
   necessary, if improved interoperability between AEC/FM software tools is
   the goal.},
ISBN = {978-1-315-73695-2; 978-1-138-02710-7},
ResearcherID-Numbers = {Karlshoj, Jan/H-9532-2017},
ORCID-Numbers = {Karlshoj, Jan/0000-0001-5735-3032},
Unique-ID = {WOS:000380490000043},
}

@inproceedings{ WOS:000380560700023,
Author = {Padma, V. and Yogesh, P.},
Book-Group-Author = {IEEE},
Title = {Proactive Failure Recovery in OpenFlow Based Software Defined Networks},
Booktitle = {2015 3RD INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION
   AND NETWORKING (ICSCN)},
Year = {2015},
Note = {3rd International Conference on Signal Processing Communication and
   Networking, Chennai, INDIA, MAR 26-28, 2015},
Abstract = {Software Defined Networking (SDN) is a network architecture that
   decouples the control and data planes. SDN enables network control to
   become directly programmable and the underlying infrastructure to be
   abstracted from the network services. The foundation for open standards
   based software defined networking is the OpenFlow protocol. The OpenFlow
   architecture which is originally designed for Local Area Networks (
   LANs), doesn't include effective mechanisms for fast resiliency. But
   metro, carrier grade Ethernet networks and industrial area networks have
   to guarantee fast resiliency upon network failure. This paper
   experiments the link protection scheme that aims to enhance the OpenFlow
   architecture by adding fast recovery mechanisms in the switch and the
   controller. This is achieved by enabling the controller to add backup
   paths proactively along with the working paths and enabling the switches
   to perform the recovery actions locally. As this avoids controller
   intervention during recovery, the recovery time solely depends upon the
   failure detection time of the switch. As this will be less compared to
   the switch-controller round trip time, this gives better results. The
   performance of the system is evaluated by finding the packet loss and
   switch over time and comparing it with the current OpenFlow
   implementations. The system performs reasonably better than the existing
   systems in terms of switch over time. However the number of backup path
   entries increase relatively.},
ISBN = {978-1-4673-6823-0},
Unique-ID = {WOS:000380560700023},
}

@inproceedings{ WOS:000375565100090,
Author = {Ritchey, A. J. and Goodsell, J. and Sertse, H. M. and Yu, W. and Pipes,
   R. B.},
Editor = {Xiao, X and Loos, A and Liu, D},
Title = {Challenge Problems for the Benchmarking of Micromechanics Analysis},
Booktitle = {PROCEEDINGS OF THE AMERICAN SOCIETY FOR COMPOSITES: THIRTIETH TECHNICAL
   CONFERENCE},
Year = {2015},
Pages = {1182-1201},
Note = {30th Technical Conference of the American-Society-for-Composites, East
   Lansing, MI, SEP 28-30, 2015},
Organization = {Amer Soc Composites},
Abstract = {Because of the inherent heterogeneity in composites, the field of
   micromechanics provides essential tools for the understanding and
   analyzing composite materials and structures. Micromechanics can serve
   two purposes: homogenization or prediction of effective properties and
   dehomogenization or recovery of the local fields in the original
   heterogeneous material. Many micromechanical tools have been developed
   and codified, including many commercially-available software packages
   that offer micromechanical analyses as either a stand-alone tool or as
   part of a chain of analyses. However, with the increasing number of
   tools available, the practitioner must determine which tool(s) provides
   the most value for the problem at hand given budget, time and resource
   constraints. To date, simple benchmarking examples have been reported in
   the literature. The present paper suggests a series of comprehensive
   benchmarking exercises in the field of micromechanics against which such
   tools can be compared. The microstructures include aligned, continuous
   fibers in a matrix, with and without an interphase; a 0/90 laminate;
   spherical inclusions; a plain-weave fabric; and a ``random{''}
   short-fiber microstructure. In each case, the material constitutive
   relations are restricted to linear-elastic. Results from DIGMAT-MF,
   MAC/GMC, FVDAM, Altair MDS, SwiftComp, and 3D finite element analysis
   are reported.},
ISBN = {978-1-60595-225-3},
ResearcherID-Numbers = {Yu, Wenbin/B-1916-2009},
Unique-ID = {WOS:000375565100090},
}

@article{ WOS:000362994500005,
Author = {Rong, Ye and Vernaleken, Ingo and Winz, Oliver H. and Goedicke, Andreas
   and Mottaghy, Felix M. and Kops, Elena Rota},
Title = {Simulation-based partial volume correction for dopaminergic PET imaging:
   Impact of segmentation accuracy},
Journal = {ZEITSCHRIFT FUR MEDIZINISCHE PHYSIK},
Year = {2015},
Volume = {25},
Number = {3},
Pages = {230-242},
Abstract = {Aim: Partial volume correction (PVC) is an essential step for
   quantitative positron emission tomography (PET). In the present study,
   PVELab, a freely available software, is evaluated for PVC in F-18-FDOPA
   brain-PET with a special focus on the accuracy degradation introduced by
   various MR-based segmentation approaches.
   Methods: Four PVC algorithms (M-PVC; MG-PVC; mMG-PVC; and R-PVC) were
   analyzed on simulated F-18-FDOPA brain-PET images. MR image segmentation
   was carried out using FSL (FMRIB Software Library) and SPM (Statistical
   Parametric Mapping) packages, including additional adaptation for
   subcortical regions (SPML). Different PVC and segmentation combinations
   were compared with respect to deviations in regional activity values and
   time-activity curves (TACs) of the occipital cortex (OCC), caudate
   nucleus (CN), and putamen (PUT). Additionally, the PVC impact on the
   determination of the influx constant (K-i) was assessed.
   Results: Main differences between tissue-maps returned by three
   segmentation algorithms were found in the subcortical region, especially
   at PUT Average misclassification errors in combination with volume
   reduction was found to be lowest for SPML, (PUT < 30\%) and highest for
   FSL (PUT > 70\%). Accurate recovery of activity data at OCC is achieved
   by M-PVC (apparent recovery coefficient varies between 0.99 and 1.10).
   The other three evaluated PVC algorithms have demonstrated to be more
   suitable for subcortical regions with MG-PVC and mMG-PVC being less
   prone to the largest tissue misclassification error simulated in this
   study. Except for M-PVC, quantification accuracy of K-i for CN and PUT
   was clearly improved by PVC.
   Conclusions: The regional activity value of PUT was appreciably
   overcorrected by most of the PVC approaches employing FSL or SPM
   segmentation, revealing the importance of accurate MR image segmentation
   for the presented PVC framework. The selection of a PVC approach should
   be adapted to the anatomical structure of interest. Caution is
   recommended in subsequent interpretation of Ki values. The possible
   different change of activity concentrations due to PVC in both target
   and reference regions tends to alter the corresponding TACs, introducing
   bias to Ki determination. The accuracy of quantitative analysis was
   improved by PVC but at the expense of precision reduction, indicating
   the potential impropriety of applying the presented framework for group
   comparison studies.},
DOI = {10.1016/j.zemedi.2014.08.001},
ISSN = {0939-3889},
ResearcherID-Numbers = {Mottaghy, Felix/AAU-2673-2020},
ORCID-Numbers = {Mottaghy, Felix/0000-0002-7212-6521},
Unique-ID = {WOS:000362994500005},
}

@article{ WOS:000369106200005,
Author = {Searle, Samantha and Wolski, Malcolm and Simons, Natasha and Richardson,
   Joanna},
Title = {Librarians as partners in research data service development at Griffith
   University},
Journal = {PROGRAM-ELECTRONIC LIBRARY AND INFORMATION SYSTEMS},
Year = {2015},
Volume = {49},
Number = {4, SI},
Pages = {440-460},
Abstract = {Purpose - The purpose of this paper is to describe the evolution to date
   and future directions in research data policy, infrastructure, skills
   development and advisory services in an Australian university, with a
   focus on the role of librarians.
   Design/methodology/approach - The authors have been involved in the
   development of research data services at Griffith, and the case study
   presents observations and reflections arising from their first-hand
   experiences.
   Findings - Griffith University's organisational structure and
   ``whole-of-enterprise{''} approach has facilitated service development
   to support research data. Fostering strong national partnerships has
   also accelerated development of institutional capability. Policies and
   strategies are supported by pragmatic best practice guidelines aimed
   directly at researchers. Iterative software development and a commitment
   to well-supported enterprise infrastructure enable the provision of a
   range of data management solutions. Training programs, repository
   support and data planning services are still relatively immature.
   Griffith recognises that information services staff (including
   librarians) will need more opportunities to develop knowledge and skills
   to support these services as they evolve.
   Originality/value - This case study provides examples of library-led and
   library-supported activities that could be used for comparative purposes
   by other libraries. At the same time, it provides a critical perspective
   by contrasting areas of good practice within the University with those
   of less satisfactory progress. While other institutions may have
   different constraints or opportunities, some of the major concepts
   within this paper may prove useful to advance the development of
   research data capability and capacity across the library profession.},
DOI = {10.1108/PROG-02-2015-0013},
ISSN = {0033-0337},
EISSN = {1758-7301},
ResearcherID-Numbers = {Wolski, Malcolm/L-8120-2018
   Richardson, Joanna/A-7226-2008
   Wolski, Malcolm/O-3566-2019
   },
ORCID-Numbers = {Wolski, Malcolm/0000-0002-0196-7016
   Richardson, Joanna/0000-0002-1871-6707
   Wolski, Malcolm/0000-0002-0196-7016
   Searle, Samantha/0000-0002-0619-5756},
Unique-ID = {WOS:000369106200005},
}

@inproceedings{ WOS:000358038600074,
Author = {Silva, Edson M. A. and Bernardes, Luciano A. S. and Ollitrault, Patrick
   and Bonatto, Diego and Micheli, Fabienne},
Editor = {SabaterMunoz, B and Moreno, P and Pena, L and Navarro, L},
Title = {Data Mining and Systems Biology for Identifying Key Genes Involved in
   Citrus Quality},
Booktitle = {XII INTERNATIONAL CITRUS CONGRESS - INTERNATIONAL SOCIETY OF
   CITRICULTURE},
Series = {Acta Horticulturae},
Year = {2015},
Volume = {1065},
Pages = {591-598},
Note = {12th International Citrus Congress -
   International-Society-of-Citriculture, Valencia, SPAIN, NOV 18-23, 2012},
Organization = {Int Soc Citriculture; Int Soc Horticultural Sci},
Abstract = {Quality in citrus is mainly characterized by fruit and juice color,
   fruit and skin size, juice percent, total soluble solids, titrable
   acidity, and carotenoid/flavonoid contents. Moreover, studies of
   biosynthetic pathway of the metabolites/proteins involved in quality at
   transcriptional and translational levels may provide relevant
   information for subsequent functional studies and quality improvement.
   Data mining of ESTs from HarvEST database allowed the selection of 17
   cDNA libraries from albedo, flavedo, peel, pulp and juice sac of
   different orange, mandarin, clementine and grapefruit varieties. In
   order to select key genes involved in quality we used systems biology
   which offers mathematical tools that include the analysis of the
   structure, clustering and centralities of the network. In order to have
   information regarding physical protein-protein interactions (PPPI) from
   citrus sequences, orthologous sequences of A. thaliana were used
   (BLASTX; reciprocal BLASTP). Literature data mining was performed, and
   PPPI network design was obtained using the Cytoscape software. The
   interactome networks thus obtained were analyzed with MCODE. Gene
   ontology clustering analysis was performed using BiNGO. Specific
   algorithms were applied to identify modules and central nodes within the
   citrus libraries associated network. The obtained results will be used
   as a guideline to select specific genes/proteins from citrus for further
   functional studies as gene expression or plant transformation.},
ISSN = {0567-7572},
EISSN = {2406-6168},
ISBN = {978-94-62610-53-8},
ResearcherID-Numbers = {Micheli, Fabienne/AAL-9895-2021
   Bonatto, Diego/F-5893-2010
   Ollitrault, Patrick/HLQ-0557-2023
   Bernardes, Luciano/AAP-8359-2020
   },
ORCID-Numbers = {Bonatto, Diego/0000-0001-8679-2448
   Bernardes, Luciano/0000-0002-0983-500X
   Ollitrault, Patrick/0000-0002-9456-5517
   Micheli, Fabienne/0000-0002-9031-362X},
Unique-ID = {WOS:000358038600074},
}

@inproceedings{ WOS:000380402000011,
Author = {Silva Filho, Roberto S. and Tewari, Anuj},
Editor = {Altintas, O and Zhang, J},
Title = {Distributed Architecture for Mobile Contextual Integrated Field Work
   Applications},
Booktitle = {2015 IEEE THIRD INTERNATIONAL CONFERENCE ON MOBILE SERVICES MS 2015},
Series = {IEEE International Conference on Mobile Services},
Year = {2015},
Pages = {81-88},
Note = {IEEE 3rd International Conference on Mobile Services MS, New York, NY,
   JUN 27-JUL 02, 2015},
Organization = {IEEE; IEEE Comp Soc},
Abstract = {The current generation of corporate software tools \& applications were
   not designed to support the unique needs of industrial field service
   work. Business software applications such as project management and time
   keeping, for example, are typically designed for traditional desktop
   computing office environments. As such, they assume low user mobility,
   high network availability and WIMP (Windows, Icons, Menus, and Pointer)
   user interfaces. These are also agnostic to physical environment context
   and are loosely integrated with one another, often requiring users to
   maintain duplicated information records. As a result, Field Services
   Personnel as Engineers, Superintendents and Craftsmen end up spending
   significant amount of their work time dealing with the consequences of
   these inefficiencies. In this paper, we describe a distributed
   architecture for mobile, contextual and integrated fieldwork software
   applications (or MCI) designed for mobile and wearable computing
   platforms. This software architecture defines a contextual and
   mobility-aware client side API, a flexible integration middleware, and
   instrumented backend services. We show how MCI can enable the
   construction of portable, mobile, context-aware and integrated software
   applications discussing its use in the implementation of SmartOutage, a
   mobile app used for automating common tasks in Field Engineering work.},
ISSN = {2329-6429},
ISBN = {978-1-4673-7284-8},
Unique-ID = {WOS:000380402000011},
}

@inproceedings{ WOS:000371705200210,
Author = {Ureel, II, Leo C. and Wallace, Charles},
Book-Group-Author = {IEEE},
Title = {WebTA: Automated Iterative Critique of Student Programming Assignments},
Booktitle = {FRONTIERS IN EDUCATION CONFERENCE (FIE), 2015},
Series = {Frontiers in Education Conference},
Year = {2015},
Pages = {1227-1235},
Note = {45th Annual Frontiers in Education Conference (FIE), El Paso, TX, OCT
   21-24, 2015},
Organization = {IEEE Educ Soc; IEEE Comp Soc; ASEE Educ Res \& Methods Div; New Mexico
   State Univ; Univ Texas El Paso; Hewlett Packard; VentureWell; Markkula
   Ctr Appl Eth; IEEE},
Abstract = {We introduce an interactive tool called WebTA that facilitates learning
   through automatic critique of student source code. Our tool provides
   immediate feedback to students and gives them experience with
   test-driven development. Students receive the benefits of cognitive
   apprenticeship through the feedback they receive in the tool. This
   facilitates tight, productive cycles of inquiry, critique and learning.
   WebTA compiles each student submission and executes it over a series of
   shakedown tests. Immediate feedback is given concerning errors and
   warnings, coupled with suggestions for debugging. The tool performs a
   textual analysis of the students source code and critiques programming
   style based on standard programming guidelines. To encourage inquiry
   through test-driven development, edge-case coverage, and API compliance,
   students develop and submit their own tests to be evaluated by the
   software.
   We report on use of WebTA in one first-year programming course and one
   second-year data structures course. Lab and assignment scores have
   improved with WebTA, and student comments attest to the effectiveness of
   the tool. Preliminary results indicate students receive higher grades
   with WebTA. One area with mixed results is WebTAs analysis of student
   developed JUnit tests; this feature improved API compliance but reduced
   edge-case testing. With these successful initial results, we offer
   suggestions for future development.},
ISSN = {0190-5848},
ISBN = {978-1-4799-8454-1},
Unique-ID = {WOS:000371705200210},
}

@inproceedings{ WOS:000378413900034,
Author = {Yen, Freedman and Hung, Leo and Kao, Nicholas and Jiang, Don Son},
Book-Group-Author = {IEEE},
Title = {Mold-flow Study for Different Bump Height, Bump Pitch and Die Size in
   FCCSP with Molded Underfill Technology},
Booktitle = {2015 IEEE 17TH ELECTRONICS PACKAGING AND TECHNOLOGY CONFERENCE (EPTC)},
Year = {2015},
Note = {17th IEEE Electronics Packaging and Technology Conference (EPTC),
   Singapore, SINGAPORE, DEC 02-04, 2015},
Organization = {IEEE},
Abstract = {Currently electrical products such as smart phone and tablet, light
   weight and thin feature of them are definitely required. To achieve
   several requirements, size of IC package assembled in limiting space
   become more crucial and development orientated. Microelectronics
   products of FCCSP (Flip Chip Chip Scale Package) with more increasing
   challenges are countered to assure molding capability with rapidly
   advancing encapsulation process in flip chip technology such as
   decreasing bump height and bump pitch, especially when Molded Underfill
   (MUF) is used and in particular during transfer molding process.
   Moldflow simulation is valid method to predict and resolve in air void
   entrapment severely occurred under the die (air void concentrated within
   bump region). Optimum solutions would be found out for air void risk
   free of MUF FCCSP with different bump structure or die size design,
   which can reduce development cycle time before mass production.
   In this paper, two molding flow factors for void prediction will be
   performed by moldflow simulation software. One of the two is about
   characteristics of bump structure including bump height and bump pitch.
   By varying different bump dimensions, melt-front position of molding
   frontier is affected in relevantly significant. In addition, the other
   factor is package with different die size design. Die size also presents
   obvious influence on melt-front pattern while molding compound flow
   through region under die.
   From this study, some results can be concluded for getting improvement
   on mold flow performance of MUF FCCSP package during molding process.
   The first of all is MUF FCCSP with the 58um bump height and 100um bump
   pitch will perform low air void risk since there is more space under die
   letting compound flow though more easily. Based on the same concept
   mention previously, the second one is smaller die arrangement that also
   can be benefit to compound flowing and get better pattern of melt-front.
   Finally, the simulation results provide a prediction guideline that MUF
   FCCSP with suitable bump height/pitch and die size structure condition
   to prevent void issue occur under die region during molding process},
ISBN = {978-1-4244-5100-5},
Unique-ID = {WOS:000378413900034},
}

@article{ WOS:000347755200004,
Author = {Doroez, Yarkin and Ozturb, Erdinc and Sunar, Berk},
Title = {A million-bit multiplier architecture for fully homomorphic encryption},
Journal = {MICROPROCESSORS AND MICROSYSTEMS},
Year = {2014},
Volume = {38},
Number = {8, A},
Pages = {766-775},
Month = {NOV},
Abstract = {In this work we present a full and complete evaluation of a very large
   multiplication scheme in custom hardware. We designed a novel
   architecture to realize a million-bit multiplication scheme based on the
   Schonhage-Strassen Algorithm. We constructed our scheme using Number
   Theoretical Transform (NTT). The construction makes use of an innovative
   cache architecture along with processing elements customized to match
   the computation and access patterns of the NTT-based recursive
   multiplication algorithm. We realized our architecture with Verilog and
   using a 90 nm TSMC library, we could get a maximum clock frequency of
   666 MHz. With this frequency, our architecture is able to compute the
   product of two million-bit integers in 7.74 ms. Our data shows that the
   performance of our design matches that of previously reported software
   implementations on a high-end 3 GHz Intel Xeon processor, while
   requiring only a tiny fraction of the area.(1) (C) 2014 Elsevier B.V.
   All rights reserved.},
DOI = {10.1016/j.micpro.2014.06.003},
ISSN = {0141-9331},
EISSN = {1872-9436},
Unique-ID = {WOS:000347755200004},
}

@article{ WOS:000344547900054,
Author = {Fang, Hai},
Title = {dcGOR: An R Package for Analysing Ontologies and Protein Domain
   Annotations},
Journal = {PLOS COMPUTATIONAL BIOLOGY},
Year = {2014},
Volume = {10},
Number = {10},
Month = {OCT},
Abstract = {I introduce an open-source R package `dcGOR' to provide the
   bioinformatics community with the ease to analyse ontologies and protein
   domain annotations, particularly those in the dcGO database. The dcGO is
   a comprehensive resource for protein domain annotations using a panel of
   ontologies including Gene Ontology. Although increasing in popularity,
   this database needs statistical and graphical support to meet its full
   potential. Moreover, there are no bioinformatics tools specifically
   designed for domain ontology analysis. As an add-on package built in the
   R software environment, dcGOR offers a basic infrastructure with great
   flexibility and functionality. It implements new data structure to
   represent domains, ontologies, annotations, and all analytical outputs
   as well. For each ontology, it provides various mining facilities,
   including: (i) domain-based enrichment analysis and visualisation; (ii)
   construction of a domain (semantic similarity) network according to
   ontology annotations; and (iii) significance analysis for estimating a
   contact (statistical significance) network. To reduce runtime, most
   analyses support high-performance parallel computing. Taking as inputs a
   list of protein domains of interest, the package is able to easily carry
   out in-depth analyses in terms of functional, phenotypic and diseased
   relevance, and network-level understanding. More importantly, dcGOR is
   designed to allow users to import and analyse their own ontologies and
   annotations on domains (taken from SCOP, Pfam and InterPro) and RNAs
   (from Rfam) as well. The package is freely available at CRAN for easy
   installation, and also at GitHub for version control. The dedicated
   website with reproducible demos can be found at http://supfam.org/dcGOR.},
DOI = {10.1371/journal.pcbi.1003929},
Article-Number = {e1003929},
ISSN = {1553-734X},
EISSN = {1553-7358},
ORCID-Numbers = {Fang, Hai/0000-0003-3961-8572},
Unique-ID = {WOS:000344547900054},
}

@article{ WOS:000341434000005,
Author = {Han, Kyung (Chris) T. and Paek, Insu},
Title = {A Review of Commercial Software Packages for Multidimensional IRT
   Modeling},
Journal = {APPLIED PSYCHOLOGICAL MEASUREMENT},
Year = {2014},
Volume = {38},
Number = {6},
Pages = {486-498},
Month = {SEP},
Abstract = {In this study, the authors evaluate several commercially available
   multidimensional item response theory (MIRT) software packages,
   including IRTPRO 2.1, Mplus 7.1, FlexMIRT, and EQSIRT, as well as their
   built-in estimation algorithms, and compare them for their performance
   in MIRT model estimation. The study examines the performance of model
   parameter recovery via a series of simulations based on four approaches
   for latent structuring-within-item MIRT, between-item MIRT, a mixture of
   within-and between-item MIRT and a bifactor model. The simulation
   studies focused on realistic conditions and models that researchers and
   practitioners are likely to encounter in practice. The results showed
   that the studied software packages recovered the item parameters
   reasonably well but differed greatly in terms of the types of data and
   models they could handle and also the run time required for estimation
   completion.},
DOI = {10.1177/0146621614536770},
ISSN = {0146-6216},
EISSN = {1552-3497},
ORCID-Numbers = {Han, Kyung Chris T./0000-0002-6534-0183},
Unique-ID = {WOS:000341434000005},
}

@article{ WOS:000340390700010,
Author = {Lovell, Andrew and Bailey, Jan and Kingdon, Anne and Gentile, Domenica},
Title = {Working with people with learning disabilities in varying degrees of
   security: nurses' perceptions of competencies},
Journal = {JOURNAL OF ADVANCED NURSING},
Year = {2014},
Volume = {70},
Number = {9},
Pages = {2041-2050},
Month = {SEP},
Abstract = {Aim. To identify and discuss the competencies required by learning
   disability nurses to work effectively with people with an offending
   background in low, medium, high secure and community settings.
   Background. Research into the competencies required by nurses working
   with individuals with an offending background, particularly those with a
   learning disability, is limited. There is some uncertainty as to whether
   there should be differentiation according to specific setting.
   Design. A qualitative study addressing the perceptions of nurses on the
   knowledge, skills and competencies required to effectively work with
   people with learning disabilities and an offending background in
   different settings.
   Methods. Seven focus groups were conducted across the four settings to
   inform the construction of the semi-structured interview schedule.
   Thirty-nine interviews were subsequently undertaken with nurses across
   settings to develop a fuller understanding of the competencies and
   ascertain if these were influenced by the specific setting where the
   nurses worked. Data were collected over 1-year in 2010 and analysed
   using a structured thematic analysis supported by the software package
   MAXqda.
   Findings. The thematic analysis produced four over-arching competencies:
   knowledge assimilation and application; team working; communication
   skills; and decision-making. A further competency around personal
   attributes constitutes the basis of a future paper.
   Conclusion. The first three competencies combine well to inform the work
   of nurses and appear transferable across settings, but the fourth
   appears more complicated, specifically in terms of the role of risk in
   supporting or detracting from decision-making capacity.},
DOI = {10.1111/jan.12362},
ISSN = {0309-2402},
EISSN = {1365-2648},
Unique-ID = {WOS:000340390700010},
}

@article{ WOS:000344638600004,
Author = {Jin, Jiashun and Zhang, Cun-Hui and Zhang, Qi},
Title = {Optimality of Graph let Screening in High Dimensional Variable Selection},
Journal = {JOURNAL OF MACHINE LEARNING RESEARCH},
Year = {2014},
Volume = {15},
Pages = {2723-2772},
Month = {AUG},
Abstract = {Consider a linear model Y = X beta+sigma z, where X has n rows and p
   columns and z similar to N (0, I-n). We assume both p and n are large,
   including the case of p >> n. The unknown signal vector beta is assumed
   to be sparse in the sense that only a small fraction of its components
   is nonzero. The goal is to identify such nonzero coordinates (i.e.,
   variable selection).
   We are primarily interested in the regime where signals are both rare
   and weak so that successful variable selection is challenging but is
   still possible. We assume the Gram matrix G = X' X is sparse in the
   sense that each row has relatively few large entries (diagonals of G are
   normalized to 1). The sparsity of G naturally induces the sparsity of
   the so-called Graph of Strong Dependence (GOSD). The key insight is that
   there is an interesting interplay between the signal sparsity and graph
   sparsity: in a broad context, the signals decompose into many small-size
   components of GOSD that are disconnected to each other.
   We propose Graphlet Screening for variable selection. This is a two-step
   Screen and Clean procedure, where in the first step, we screen subgraphs
   of GOSD with sequential chi(2)-tests, and in the second step, we clean
   with penalized MLE. The main methodological innovation is to use GOSD to
   guide both the screening and cleaning processes.
   For any variable selection procedure beta, we measure its performance by
   the Hamming distance between the sign vectors of beta and beta, and
   assess the optimality by the minimax Hamming distance. Compared with
   more stringent criteria such as exact support recovery or oracle
   property, which demand strong signals, the Hamming distance criterion is
   more appropriate for weak signals since it naturally allows a small
   fraction of errors.
   We show that in a broad class of situations, Graphlet Screening achieves
   the optimal rate of convergence in terms of the Hamming distance. Unlike
   Graphlet Screening, well-known procedures such as the
   L-0/L-1-penalization methods do not utilize local graphic structure for
   variable selection, so they generally do not achieve the optimal rate of
   convergence, even in very simple settings and even if the tuning
   parameters are ideally set.
   The the presented algorithm is implemented as R-CRAN package Screen
   Clean and in matlab (available at http : //www.stat.cmu.edu/similar to
   jiashun/Research/software/GS-matlab/).},
ISSN = {1532-4435},
ORCID-Numbers = {Zhang, Qi/0000-0001-6197-0973},
Unique-ID = {WOS:000344638600004},
}

@article{ WOS:000339642700006,
Author = {Jee, Jun-Goo},
Title = {Systematic Assessment of the Effects of an All-Atom Force Field and the
   Implicit Solvent Model on the Refinement of NMR Structures with Subsets
   of Distance Restraints},
Journal = {BULLETIN OF THE KOREAN CHEMICAL SOCIETY},
Year = {2014},
Volume = {35},
Number = {7},
Pages = {1944-1950},
Month = {JUL 20},
Abstract = {Employment of a time consuming, sophisticated calculation using the
   all-atom force field and generalized-Born implicit solvent model (GBIS)
   for refinement of NMR structures has become practical through advances
   in computational methods and capacities. GBIS refinement improves the
   qualities of the. resulting NMR structures with reduced computational
   times. However, the contribution of GBIS to NMR structures has not been
   sufficiently studied in a quantitative way. In this paper, we report the
   effects of GBIS on the refined NMR structures of ubiquitin (UBQ) and GB1
   with subsets of distance restraints derived from experimental data.
   Random omission prepared a series of distance restraints 0.05, 0.1, 0.3,
   0.5, and 0.7 times smaller. For each number; we produced five different
   restraints for statistical analysis. We then recalculated the NMR
   structures using CYANA software, followed by GBIS refinements using the
   AMBER package. GBIS improved both the precision and accuracy of all the
   structures, but to varied levels. The degrees of improvement were
   significant When the input restraints were insufficient. In particular,
   GBIS enabled GB1 to form an accurate structure even with distance
   restraints of 5\%, revealing that die root-mean-square deviation was
   less than 1 angstrom from the X-ray backbone structure. We also showed
   that the efficiency of searching the conformational space was more
   important for finding accurate structures with the calculation of UBQ
   with 5\% distance restraints than the number of conformations generated.
   Our data will provide a meaningful guideline to judge and compare the
   structural improvements by GBIS.},
DOI = {10.5012/bkcs.2014.35.7.1944},
ISSN = {1229-5949},
Unique-ID = {WOS:000339642700006},
}

@article{ WOS:000340581300007,
Author = {Lee-Rausch, E. M. and Hammond, D. P. and Nielsen, E. J. and Pirzadeh, S.
   Z. and Rumsey, C. L.},
Title = {Application of the FUN3D Solver to the 4th AIAA Drag Prediction Workshop},
Journal = {JOURNAL OF AIRCRAFT},
Year = {2014},
Volume = {51},
Number = {4, SI},
Pages = {1149-1160},
Month = {JUL-AUG},
Abstract = {FUN3D Navier-Stokes solutions were computed for the 4th AIAA Drag
   Prediction Workshop grid-convergence study, downwash study, and
   Reynolds-number study on a set of node-based mixed-element grids. All of
   the baseline tetrahedral grids were generated with the VGRID
   (developmental) advancing-layer and advancing-front grid-generation
   software package following the gridding guidelines developed for the
   workshop. With maximum grid sizes exceeding 100 million nodes, the
   grid-convergence study was particularly challenging for the node-based
   unstructured grid generators and flow solvers. At the time of the
   workshop, the super-fine grid with 105 million nodes and 600 million
   tetrahedral elements was the largest grid known to have been generated
   using VGRID. FUN3D Version 11.0 has a completely new pre- and
   postprocessing paradigm that has been incorporated directly into the
   solver and functions entirely in a parallel, distributed-memory
   environment. This feature allowed for practical preprocessing and
   solution times on the largest unstructured-grid size requested for the
   workshop. For the constant-lift grid-convergence case, the convergence
   of total drag is approximately second-order on the finest three grids.
   The variation in total drag between the finest two grids is only two
   counts. At the finest grid levels, only small variations in wing and
   tail pressure distributions are seen with grid refinement. Similarly, a
   small wing side-of-body separation also shows little variation at the
   finest grid levels. Overall, the FUN3D results compare well with the
   structured-grid code CFL3D. For the grid-convergence case, the FUN3D
   total and component forces/moments are within one standard deviation of
   the workshop core solution medians and are very close to the median
   values especially at the finest grid levels. The FUN3D downwash study
   and Reynolds-number study results also compare well with the range of
   results shown in the workshop presentations.},
DOI = {10.2514/1.C032558},
ISSN = {0021-8669},
EISSN = {1533-3868},
Unique-ID = {WOS:000340581300007},
}

@article{ WOS:000209673100007,
Author = {Alnusair, Awny and Zhao, Tian and Yan, Gongjun},
Title = {Rule-based detection of design patterns in program code},
Journal = {INTERNATIONAL JOURNAL ON SOFTWARE TOOLS FOR TECHNOLOGY TRANSFER},
Year = {2014},
Volume = {16},
Number = {3},
Pages = {315-334},
Month = {JUN},
Abstract = {The process of understanding and reusing software is often
   time-consuming, especially in legacy code and open-source libraries.
   While some core code of open-source libraries may be well-documented, it
   is frequently the case that open-source libraries lack informative API
   documentation and reliable design information. As a result, the source
   code itself is often the sole reliable source of information for program
   understanding activities. In this article, we propose a
   reverse-engineering approach that can provide assistance during the
   process of understanding software through the automatic recovery of
   hidden design patterns in software libraries. Specifically, we use
   ontology formalism to represent the conceptual knowledge of the source
   code and semantic rules to capture the structures and behaviors of the
   design patterns in the libraries. Several software libraries were
   examined with this approach and the evaluation results show that
   effective and flexible detection of design patterns can be achieved
   without using hard-coded heuristics.},
DOI = {10.1007/s10009-013-0292-z},
ISSN = {1433-2779},
EISSN = {1433-2787},
ResearcherID-Numbers = {Alnusair, Awny/AAY-5066-2020
   Fritola, Renato/AAU-4721-2021},
ORCID-Numbers = {Alnusair, Awny/0000-0001-9513-3022
   },
Unique-ID = {WOS:000209673100007},
}

@article{ WOS:000335959500030,
Author = {Gundogdu, Tayfun and Komurgoz, Guven},
Title = {Development of large salient-pole synchronous machines by using
   fractional-slot concentrated windings},
Journal = {JOURNAL OF VIBROENGINEERING},
Year = {2014},
Volume = {16},
Number = {3},
Pages = {1405-1415},
Month = {MAY},
Abstract = {This paper presents a detailed analysis and comparison of large
   salient-pole synchronous generators (SPSG) with conventional slot
   distributed and fractional slot concentrated winding techniques for
   power generation applications. The fractional slot concentrated winding
   technique (FSCW) makes it possible to increase the machine inductance
   and permeability of the poles in order to achieve lighter, cheaper and
   high-efficiency SPSGs with a simpler structure without reducing the
   output power. The SPSGs are modeled and analyzed by using ANSYS MAXWELL
   packet program which is a Finite Element Method (FEM) based
   electromagnetic field simulation software. Detailed comparisons of the
   SPSGs' performance characteristics which include important issues such
   as wave form of the induced voltage, weight and cost of the machines,
   machine losses and saturation effect are presented. Guidelines are
   developed to help electrical machine designers faced with reducing the
   saturation of the SPSGs' poles.},
ISSN = {1392-8716},
ResearcherID-Numbers = {Komurgoz, Guven/ABB-4655-2020
   Gundogdu, Tayfun/H-2377-2012},
ORCID-Numbers = {Komurgoz, Guven/0000-0001-9301-4548
   Gundogdu, Tayfun/0000-0002-7150-1860},
Unique-ID = {WOS:000335959500030},
}

@article{ WOS:000338332800001,
Author = {Protosenya, A. G. and Karasev, M. A.},
Title = {Development of Numerical Model for Deformation Prediction of Soil Mass
   During Construction of a Moderately Deep Structure in the Abaqus
   Software Package},
Journal = {SOIL MECHANICS AND FOUNDATION ENGINEERING},
Year = {2014},
Volume = {51},
Number = {2},
Pages = {53-59},
Month = {MAY},
Abstract = {Questions concerning numerical modeling of the construction of a deep
   pit are examined, and parameters of the numerical model of a moderately
   deeply embedded structure are substantiated using a model of a
   ``hardening{''} soil in the Abaqus software package. Results of field
   investigations and those obtained during the numerical modeling are
   compared and indicate rather good agreement from both the qualitative
   and quantitative standpoints.},
DOI = {10.1007/s11204-014-9254-z},
ISSN = {0038-0741},
EISSN = {1573-9279},
ResearcherID-Numbers = {Karasev, Maksim/AAE-1765-2019
   Protosenya, Anatolii G./A-9198-2014},
ORCID-Numbers = {Karasev, Maksim/0000-0001-8939-0807
   Protosenya, Anatolii G./0000-0001-7829-6743},
Unique-ID = {WOS:000338332800001},
}

@article{ WOS:000331665400004,
Author = {Simanaviciene, Ruta and Liaudanskiene, Rita and Ustinovichius, Leonas},
Title = {Assessing reliability of design, construction, and safety related
   decisions},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2014},
Volume = {39},
Pages = {47-58},
Month = {APR},
Abstract = {Currently there is no approach which would help to comprehensively
   ensure occupational safety. Many scientists perform researches and
   calculations, create new methods related to safety and health, but most
   of them analyze separate aspects of safety in the field of construction.
   The authors of this paper present a new complex view on ensuring
   occupational safety and health during construction. The selection of
   safety solutions is performed based on complex evaluation of structure,
   technology and safety. In their previous works, the authors offered a
   new method for multiple attribute decision synthesis, SyMAD-3, which
   helps to choose an effective construction project alternative from
   multiple alternatives by assessing various construction, technological
   and occupational safety solutions, based on a set of quantitative
   attributes. However, the integration of these solutions may cause
   doubts, since decision making in construction is always associated with
   uncertainty. The investment projects in construction are characterized
   by the large accuracy variation (from 15 to 50\%) of some attribute
   values. Although the SyMAD-3 method is mathematically grounded, it does
   not answer the question if the error of attribute values impacts the
   final decision and if this decision can be reliably assessed.
   In the present paper, the authors supplement the SyMAD-3 method with
   decision sensitivity analysis (SyMAD-3 with SA) to improve the
   reliability of the SyMAD-3 method and assess the reliability of the
   obtained derision. The SyMAD-3 with SA method allows us to choose an
   effective alternative of a construction project by assessing three
   stages of construction, based on a set of attributes given the error of
   their values, and determine the reliability of the final decision. The
   proposed method is implemented in a software package created by the
   authors with the aim of analyzing decisions and performing experimental
   calculations in the field of construction. (C) 2013 Elsevier B.V. All
   rights reserved.},
DOI = {10.1016/j.autcon.2013.11.008},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Ustinovičius, Leonas/AAH-8369-2020
   },
ORCID-Numbers = {Ustinovicius, Leonas/0000-0002-0027-5501
   Simanaviciene, Ruta/0000-0002-1614-9470},
Unique-ID = {WOS:000331665400004},
}

@article{ WOS:000334997700007,
Author = {Ganesan, Dharmalingam and Lindvall, Mikael},
Title = {ADAM: External Dependency-Driven Architecture Discovery and Analysis of
   Quality Attributes},
Journal = {ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY},
Year = {2014},
Volume = {23},
Number = {2},
Month = {MAR},
Abstract = {This article introduces the Architecture Discovery and Analysis Method
   (ADAM). ADAM supports the discovery of module and runtime views as well
   as the analysis of quality attributes, such as testability, performance,
   and maintainability, of software systems. The premise of ADAM is that
   the implementation constructs, architecture constructs, concerns, and
   quality attributes are all influenced by the external entities (e.g.,
   libraries, frameworks, COTS software) used by the system under analysis.
   The analysis uses such external dependencies to identify, classify, and
   review a minimal set of key source-code files supported by a knowledge
   base of the external entities. Given the benefits of analyzing external
   dependencies as a way to discover architectures and potential risks, it
   is demonstrated that dependencies to external entities are useful not
   only for architecture discovery but also for analysis of quality
   attributes. ADAM is evaluated using the NASA's Space Network Access
   System (SNAS). The results show that this method offers systematic
   guidelines for discovering the architecture and locating potential risks
   (e.g., low testability and decreased performance) that are hidden deep
   inside the system implementation. Some generally applicable lessons for
   developers and analysts, as well as threats to validity are also
   discussed.},
DOI = {10.1145/2529998},
Article-Number = {17},
ISSN = {1049-331X},
EISSN = {1557-7392},
Unique-ID = {WOS:000334997700007},
}

@article{ WOS:000331275900006,
Author = {Karus, S.},
Title = {XML development with plug-ins as a service},
Journal = {SOFTWARE-PRACTICE \& EXPERIENCE},
Year = {2014},
Volume = {44},
Number = {3, SI},
Pages = {335-352},
Month = {MAR},
Abstract = {Extensible Markup Language (XML) has quickly become a mainstream
   language in software development. Not only is it used for message and
   document interchange, it is also used to define application logic and
   interfaces. However, modern general purpose integrated development
   environments have rather limited support for XML development. The wide
   variety of XML-based languages makes it a challenge to build tools for
   comprehensive support of XML development. In this paper, we present
   extensions to a library exposed as an add-in for Microsoft Visual Studio
   and a command line tool to improve the development experience by
   providing access to subscribable service-based pluggable helper tools.
   The architecture of the solution allows easier publishing and deployment
   of the tools, minimising the exposure of development data to third
   parties and providing service providers with much needed tool usage and
   development process data. The tools offer developers new means to check
   their files against good and bad practices, automatically fix errors in
   XML or improve the files' conformance with development guidelines,
   current state of project with other projects, or run service queries to
   estimate their projects future progress. Copyright (c) 2013 John Wiley
   \& Sons, Ltd.},
DOI = {10.1002/spe.2246},
ISSN = {0038-0644},
EISSN = {1097-024X},
Unique-ID = {WOS:000331275900006},
}

@article{ WOS:000332015400001,
Author = {Boshkovikj, Veselin and Fluke, Christopher J. and Crawford, Russell J.
   and Ivanova, Elena P.},
Title = {Three-dimensional visualization of nanostructured surfaces and bacterial
   attachment using Autodesk (R) Maya (R)},
Journal = {SCIENTIFIC REPORTS},
Year = {2014},
Volume = {4},
Month = {FEB 28},
Abstract = {There has been a growing interest in understanding the ways in which
   bacteria interact with nano-structured surfaces. As a result, there is a
   need for innovative approaches to enable researchers to visualize the
   biological processes taking place, despite the fact that it is not
   possible to directly observe these processes. We present a novel
   approach for the three-dimensional visualization of bacterial
   interactions with nano-structured surfaces using the software package
   Autodesk Maya. Our approach comprises a semi-automated stage, where
   actual surface topographic parameters, obtained using an atomic force
   microscope, are imported into Maya via a custom Python script, followed
   by a `creative stage', where the bacterial cells and their interactions
   with the surfaces are visualized using available experimental data. The
   `Dynamics' and `nDynamics' capabilities of the Maya software allowed the
   construction and visualization of plausible interaction scenarios. This
   capability provides a practical aid to knowledge discovery, assists in
   the dissemination of research results, and provides an opportunity for
   an improved public understanding. We validated our approach by
   graphically depicting the interactions between the two bacteria being
   used for modeling purposes, Staphylococcus aureus and Pseudomonas
   aeruginosa, with different titanium substrate surfaces that are
   routinely used in the production of biomedical devices.},
DOI = {10.1038/srep04228},
Article-Number = {4228},
ISSN = {2045-2322},
ResearcherID-Numbers = {Ivanova, Elena P/B-1255-2019
   },
ORCID-Numbers = {Ivanova, Elena P/0000-0002-5509-8071
   Crawford, Russell/0000-0003-1054-5285
   Fluke, Christopher/0000-0003-0961-2321},
Unique-ID = {WOS:000332015400001},
}

@article{ WOS:000329885500006,
Author = {Ahn, Ki-Uhn and Kim, Young-Jin and Park, Cheol-Soo and Kim, Inhan and
   Lee, Keonho},
Title = {BIM interface for full vs. semi-automated building energy simulation},
Journal = {ENERGY AND BUILDINGS},
Year = {2014},
Volume = {68},
Number = {B, SI},
Pages = {671-678},
Month = {JAN},
Note = {2nd International Conference on Building Energy and Environment (COBEE),
   Univ Colorado, Boulder, CO, AUG 01-04, 2012},
Abstract = {BIM (building information model) enables information sharing and reuse
   for interoperability between prevalent software tools in the AEC
   (Architecture, Engineering, and Construction) industry. Although a BIM
   based energy simulation tool can reduce costs and time required for
   building energy simulation work, no practical interface between CAD
   tools and dynamic energy analysis tools has been developed so far. With
   this in mind, this study suggests two approaches (Full automated
   interface (FAI) and semi automated interface (SAI)) enabling information
   transition from CAD tools (e.g., IFC) to EnergyPlus input file, IDF.
   FAI, if ideally developed, can convert IFC to IDF based on the use of
   pre-defined defaults without requiring human intervention. In contrast,
   SAI converts geometry information drawn from IFC to IDF and then require
   human data entry for uncertain simulation inputs. For this study, a
   library building was chosen and space boundary generated from ArchiCAD
   13 was employed for geometry mapping. The Morris method, one of
   sensitivity analysis methods, was used for identifying significant
   inputs. In FAI and SAI, dominant inputs, out of the Morris method, were
   identified for Monte Carlo simulation to quantify probabilistic
   simulation outputs. In the paper, FAI and SAI simulation results are
   cross-compared, and pros and cons of FAI and SAI are discussed. (C) 2013
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.enbuild.2013.08.063},
ISSN = {0378-7788},
EISSN = {1872-6178},
ResearcherID-Numbers = {Kim, Young-jin/GSD-3168-2022},
Unique-ID = {WOS:000329885500006},
}

@inproceedings{ WOS:000347410900014,
Author = {Bianco-Riccioz, M. and Bianco, P. and De Cesare, G.},
Editor = {Schleiss, AJ and Speerli, J and Pfammatter, R},
Title = {Design of a bed load and driftwood filtering dam, analysis of the
   phenomena and hydraulic design},
Booktitle = {SWISS COMPETENCES IN RIVER ENGINEERING AND RESTORATION},
Year = {2014},
Pages = {129-137},
Note = {7th International Conference on Fluvial Hydraulics (River Flow), Ecole
   Polytechnique Federale Lausanne, Lausanne, SWITZERLAND, SEP 03-05, 2014},
Organization = {Swiss Fed Off Environm; BG Consulting Engineers; Hydro Exploitat SA; E
   dric ch; IM \& IUB Engn; Basler \& Hofmann; AquaVis Engn; Met Flow SA;
   Int Assoc Hydro Environm Engn \& Res, Comm Fluvial Hydraul; Stucky;
   Groupe E; Patscheider Partner; HydroCosmos SA; Kissling Zbinden AG; Ribi
   SA; Poyry; Swiss Assoc Water Management; Ecole Polytechnique Federale
   Lausanne, Lab Hydraul Construct},
Abstract = {Flood protection often calls on to the realization of retention works
   for bed load as well as wood and debris flow. Certain relatively recent
   arrangements did not perform according to their intended function, what
   shows the complexity of the design and the implementation of such works.
   Adaptations were necessary to reach the security objectives.
   The design of a retention dam for solid materials and floating driftwood
   requires the consideration of numerous hydraulic and material transport
   processes. The analyses and design validation can be made with two
   approaches: physical modelling by the construction of a reduced scale
   model and the test realization or numerical simulation, by means of
   software packages such as GESMAT (1D) or TOPOFLOW (2D). The present work
   consists in implementing both approaches, in estimating and in comparing
   the answers which could be given for a bed load and debris flow
   filtering dam on a river with a slope of the order of 10\%.
   Thanks to water level gauges and visual observations during tests on the
   physical model, the progression of the obstructions by driftwood and bed
   load is well understood, and the effectiveness of these obstructions
   proven. The tested work plays at first a role of filtering and retention
   and secondly a role of side overflow towards a zone with low damage
   potential, when the capacity of the in-stream retention space is
   reached.
   The performed numerical simulations, essentially in 1D, reproduce well
   the phenomena of bed load aggradation. Moreover, the potential
   obstruction by floating wood is considered and influences the behavior
   of the structure.
   By putting in parallel physical and numerical models, it was possible
   thanks to the results from the physical scale model to refine the
   numerical simulation tools taking into consideration additional
   components and behavior-type rules. These further established rules can
   now be used for other cases where physical modelling is not foreseen.},
ISBN = {978-1-4987-0443-4; 978-1-138-02676-6},
Unique-ID = {WOS:000347410900014},
}

@incollection{ WOS:000339068400010,
Author = {Cela, Arben and Ben Gaid, Mongi and Li, Xu-Guang and Niculescu,
   Silviu-Iulian},
Book-Author = {Cela, A
   BenGaid, M
   Li, XG
   Niculescu, SI},
Title = {Insight in Delay System Modeling of DCESs},
Booktitle = {OPTIMAL DESIGN OF DISTRIBUTED CONTROL AND EMBEDDED SYSTEMS},
Series = {Communications and Control Engineering},
Year = {2014},
Pages = {173-184},
Abstract = {In the Part II of this document, the object of our study is the DCES
   analysis and design with a special emphasis on the scheduling of control
   signals and their real time update depending on the system state. This
   simply means that the systems resources are allocated with respect to
   the system's performance enhancement and robustness. Such a state
   dependent scheduling may be classified in the set of event driven
   scheduling algorithms which have attracted recently the attention of
   many researchers in control domain.
   Another characteristic of the class of applications treated there, which
   is more related to the technology of communication, concerns the
   communication and calculation model. The network was supposed to posses
   real-time characteristics and the transfer time of messages packets from
   a node processor to another node processor of the Hardware/Software
   application architecture was considered as being negligible. The
   calculation as well as the communication models were supposed to be
   synchronous. Thus, such an assumption simply means that the delay
   induced model is uniquely given by the static scheduling hyperperiod.
   Meanwhile, for the class of this type of applications, in our opinion,
   it is impossible to neglect the delay induced by scheduling of messages
   on the network as well as of control and other tasks (actuation,
   sampling, ...) running on the node processors.
   The object of our study in Chap. 10 will be the influence of the induced
   delay on the stability and on the performance of some special class of
   DCESs corresponding to those studied and discussed in Chap. 7. The
   stability analysis will allow us defining appropriate stability domain
   for task period variation and for DCES-induced delay. Such a study, done
   generally off-line, will help us to better understand their influence on
   the system's stability and performance as well as to propose new control
   switching algorithms in order to handle network or processor over-load
   state and control messages packets dropping. As it will be seen in the
   Chaps. 12 and 13, the knowledge of the induced delay influence will also
   allow slowing down the system's performance deterioration by switching
   from a given control law to the zero-control one.
   In our opinion, it is important to operate this analysis for the case
   where the induced delay is inferior to a sampling period as well when it
   is composed by a sequence of sampling period. The objective is double.
   First, it concerns the choice of sampling periods. We will see that we
   can increase it without degrading the system's performances. This means
   that, for the same level of service or performances, we use less
   communication and calculation resources. During this analysis, we
   observe that the generally accepted intuition consisting in the fact
   that more communication and calculation resources allow to better
   control a given system is not always verified. Second, it is helpful for
   proposing new control and/or recovery strategies in the case of messages
   dropouts and control task preemption. Based on this analysis, in Chap.
   11, we also propose scheduling algorithms able to handle resources
   and/or performance optimization of DCESs.
   Our main objective is to be as realistic as possible with respect to the
   representation of communication and calculation model in the presence of
   induced delays. We will see that this representation is sufficiently
   informative to address stability analysis as well as the performance
   optimization of DCESs.
   In the sequel, we briefly introduce the existing delay models of DCESs,
   the corresponding technical background, different related problems, as
   well as appropriate methodologies and approaches adopted in handling
   them. We absolutely do not have the pretentiousness to cover the whole
   field of DCESs or to provide all the solutions proposed so far in the
   literature. Meanwhile, in Sect. 9.6, we give a very brief presentation
   of some of them which seems to be in close relation or complementary the
   ones proposed in the book.},
DOI = {10.1007/978-3-319-02729-6\_9},
ISSN = {0178-5354},
ISBN = {978-3-319-02729-6; 978-3-319-02728-9},
ResearcherID-Numbers = {Niculescu, Silviu/AAA-7156-2020},
Unique-ID = {WOS:000339068400010},
}

@inproceedings{ WOS:000340830900018,
Author = {Chen, Liping},
Editor = {Zheng, R},
Title = {Desktop Virtualization of Private Cloud for University Library System},
Booktitle = {PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON E-EDUCATION,
   E-BUSINESS AND INFORMATION MANAGEMENT},
Series = {Advances in Intelligent Systems Research},
Year = {2014},
Volume = {91},
Pages = {62-64},
Note = {International Conference on e-Education, e-Business and Information
   Management (ICEEIM), Shanghai, PEOPLES R CHINA, APR 17-18, 2014},
Abstract = {Focusing on the design and construction of desktop virtualization of
   university library system, this article discusses the problems in
   current information construction of university libraries and makes an
   introduction to cloud computing and the basic concept and systematic
   structure of desktop virtualization. Based on the VMware VSphere5
   software, overall virtual desktop of university library has been
   realized, which solves the issues lied in which utility private cloud
   technology in university library system.},
ISSN = {1951-6851},
ISBN = {978-94-6252-007-3},
Unique-ID = {WOS:000340830900018},
}

@inproceedings{ WOS:000341306400305,
Author = {Dan, D. H. and Zhao, Y. M. and Chen, Z. H.},
Editor = {Chen, A and Frangopol, DM and Ruan, X},
Title = {Research on intelligent computing web platform of BHM based on Java EE
   and scientific computing library},
Booktitle = {BRIDGE MAINTENANCE, SAFETY, MANAGEMENT AND LIFE EXTENSION},
Year = {2014},
Pages = {2195-2201},
Note = {7th International Conference on Bridge Maintenance, Safety and
   Management (IABMAS), Shanghai, PEOPLES R CHINA, JUL 07-11, 2014},
Organization = {Int Assoc Bridge Maintenance \& Safety; Tongji Univ; China Highway \&
   Transportat Soc, Bridge \& Struct Engn Branch; China Assoc Highway
   Prospect \& Design; Shanghai Highway \& Transportat Soc; Fed Highway
   Adm, U S Dept Transportat; Natl Acad, Transportat Res Board; Int Assoc
   Life Cycle Civil Engn; Italian Assoc Reinforced \& Prestressed Concrete;
   Assoc Offshore \& Marine Engn; Lehigh Univ, Ctr Adv Technol Large
   Structural Syst; Changan Univ; Chongqing Jiaotong Univ; Fuzhou Univ;
   Harbin Inst Technol; Politecnico Milano; Lehigh Univ, P C Rossin Coll
   Engn \& Appl Sci; Fondazione Promozione Acciaio; Lab Univ Network Seism
   Engn; Fujian Prov Univ, Sustainable \& Innovat Bridge Engn Res Ctr;
   Bolina Ingn Ltd; Jiangsu Transportat Res Inst Co Ltd; CCCC Highway
   Consultants Co Ltd; Tongji Univ, Architectural Design \& Res Inst;
   Tianjin Urban Construct Design Inst, Tianjin Urban Construct Grp;
   Liuzhou OVM Machinery Co Ltd; Jiangsu Zhoushe Engn Consultancy Grp Co
   Ltd; Bentley Syst Inc; Jiangsu Fasten Steel Cable Co; Anhui Transportat
   Investment Grp Co; Anhui Transport Consulting \& Design Inst Co LTD;
   Shanghai Urban Construct Design \& Res Inst; Haiyu Technol Co Ltd;
   Shanghai Bohong Energy Saving Technol Co Ltd; GERB Vibrat Control Co
   Ltd; Zhejiang Sci Res Inst Transport},
Abstract = {In recent years, with the rapid development of bridge construction in
   China, the Bridge Health Monitoring (BHM) System has been extensively
   studied and widely used. The BHM System can provide foundation and
   guidance for maintenance and management of bridges. Most existed BHM
   Systems features remarkably in good hardware with poor software, or good
   measurement with bad computation. That means, the hardware system of
   present BHM System is well designed but the software system is not. Due
   to lack of support of the scientific computing, the existed BHM software
   can only perform the functions of data management, data display and
   simple data process, but can't satisfy the requirements of deeper data
   analysis and interpretation, which has become the bottleneck restricting
   the development of BHM System. Meanwhile, the technological gap between
   scientific computing researchers in the field of BHM research and
   software engineers has also become the throat of this bottleneck
   problem. This paper mainly focuses on the research of software
   technology of the BHM System. A component based software implementation
   technology is developed with the MATLAB environment. The general
   frameworks of the BHM System are constructed and an overall Java class
   library is built to realize this framework, in which, each Java class
   comes from the component mentioned above. Combined with Java EE standard
   platform specification, a web driven scientific computing software
   solution for BHM System are presented. As an example, an integrated
   early-warning component of BHM System of cable-stayed bridge are
   realized and docked into above software architecture, which is developed
   by mapping grouped cable forces of cable-stayed bridge into some well
   designed indices. This study is expected to build an intelligent
   computing web platform of BHM based on Java EE and Scientific Computing
   library in MATLAB language.},
ISBN = {978-1-315-76069-8; 978-1-138-00103-9},
Unique-ID = {WOS:000341306400305},
}

@inproceedings{ WOS:000371484600049,
Author = {Del Grosso, Angelo Mario and Nahli, Ouafae},
Editor = {ElMohajir, M and AlAchhab, M and Chahhou, M},
Title = {Towards a flexible open-source software library for multi-layered
   scholarly textual studies An Arabic case study dealing with
   semi-automatic language processing},
Booktitle = {2014 THIRD IEEE INTERNATIONAL COLLOQUIUM IN INFORMATION SCIENCE AND
   TECHNOLOGY (CIST'14)},
Series = {Colloquium in Information Science and Technology},
Year = {2014},
Pages = {285-290},
Note = {3rd IEEE International Colloquium on Information Science and Technology
   (CIST), Tetouan, MOROCCO, OCT 20-22, 2014},
Organization = {IEEE; IEEE Comp Soc; IEEE Commun Soc; IEEE Morocco Sect; IEEE Morocco
   Comp \& Commun Joint Chapter; UAE IEEE Student Branch; Univ Abdelmalek
   Essaadi; ENSA Tetouan},
Abstract = {This paper presents both the general model and a case study of the
   Computational and Collaborative Philology Library (CoPhiLib), an ongoing
   initiative underway at the Institute for Computational Linguistics (ILC)
   of the National Research Council (CNR), Pisa, Italy. The library,
   designed and organized as a reusable, abstract and open-source software
   component, aims at solving the needs of multi-lingual and cross-lingual
   analysis by exposing common Application Programming Interfaces (APIs).
   The core modules, coded by the Java programming language, constitute the
   groundwork of a Web platform designed to deal with textual scholarly
   needs. The Web application, implemented according to the Java Enterprise
   specifications, focuses on multi-layered analysis for the study of
   literary documents and related multimedia sources. This ambitious
   challenge seeks to obtain the management of textual resources, on the
   one hand by abstracting from current language, on the other hand by
   decoupling from the specific requirements of single projects. This goal
   is achieved thanks to methodologies declared by the ``agile process{''},
   and by putting into effect suitable use case modeling, design patterns,
   and component-based architectures. The reusability and flexibility of
   the system have been tested on an Arabic case study: the system allows
   users to choose the morphological engine (such as AraMorph or
   Al-Khalil), along with linguistic granularity (i.e. with or without
   declension). Finally, the application enables the construction of
   annotated resources for further statistical engines (training set).},
ISSN = {2327-185X},
ISBN = {978-1-4799-5979-2},
ResearcherID-Numbers = {DEL GROSSO, ANGELO MARIO/P-7993-2018},
ORCID-Numbers = {DEL GROSSO, ANGELO MARIO/0000-0002-4867-6304},
Unique-ID = {WOS:000371484600049},
}

@article{ WOS:000342130500001,
Author = {Demoz, Zaid and Legesse, Befikadu and Teklay, Gebrehiwot and Demeke,
   Birhanu and Eyob, Tewodros and Shewamene, Zewdneh and Abera, Mubarek},
Title = {Medication adherence and its determinants among psychiatric patients in
   an Ethiopian referral hospital},
Journal = {PATIENT PREFERENCE AND ADHERENCE},
Year = {2014},
Volume = {8},
Pages = {1329-1335},
Abstract = {Background: The degree to which an individual follows medical advice is
   a major concern in every medical specialty. Non-adherence to psychiatric
   treatment regimens has a pro-found impact on the disease course,
   relapse, future recovery, cost of health care, and the outcome for the
   patient. The aim of this study was to assess medication adherence and
   its correlates among psychiatric patients at Ayder Referral Hospital,
   Northern Ethiopia.
   Methods: A cross-sectional study was conducted from June to September
   2013 at Ayder Referral Hospital, where 423 patients were selected by a
   systematic random sampling technique from all patients attending the
   psychiatric clinic at the hospital. Data were collected by trained data
   collectors through interview of the patients using a structured
   questionnaire. The collected data were entered into Epi Info version 7
   and analyzed by Statistical Package for the Social Sciences version 16
   software. Logistic regression was used to assess independent predictors
   of adherence.
   Results: A total of 387 patients completed the interview. Two hundred
   and sixteen (55.8\%) and 113 (29.2\%) were patients with a diagnosis of
   schizophrenia and mood disorder, respectively, while 35 (9\%) and 23
   (5.9\%) had a diagnosis of drug addiction and autistic disorder. Two
   hundred and seven (71.6\%) patients were found to be adherent to their
   medication. When adherence rates were observed according to type of
   disorder, 60 (53.1\%), 24 (68.6\%), 149 (69\%), and 18 (78.3\%) of
   patients with mood disorder, drug addiction, schizophrenia, and autism,
   respectively, were adherent to their medications. Female gender
   (adjusted odds ratio {[}AOR] 2.34; 95\% confidence interval {[}CI]
   1.45-3.74), tertiary education (AOR 2.69; 95\% CI 1.46-4.85), living
   with family (AOR 2.57; 95\% CI 1.66-4.58), and shorter treatment
   duration (AOR 1.82; 95\% CI 1.21-2.84) were among the variables
   associated with better adherence.
   Conclusion: Suboptimal adherence was observed among psychiatric patients
   in this study. Health professionals in the psychiatric clinic and
   pharmacists need to focus on and counsel patients about adherence and
   its implications for their clinical outcome.},
DOI = {10.2147/PPA.S69702},
ISSN = {1177-889X},
ResearcherID-Numbers = {Wubishet, Befikadu/AAI-6038-2020
   },
ORCID-Numbers = {Wubishet, Befikadu/0000-0001-9896-8893
   Sabe, Zewdneh Shewamene/0000-0003-1490-9672},
Unique-ID = {WOS:000342130500001},
}

@inproceedings{ WOS:000356039101062,
Author = {Deshmukh, Meenakshi and Schwarz, Rene and Braukhane, Andy and Lopez,
   Rosa Paris and Gerndt, Andreas},
Book-Group-Author = {IEEE},
Title = {Model Linking to Improve Visibility and Reusability of Models during
   Space System Development},
Booktitle = {2014 IEEE AEROSPACE CONFERENCE},
Year = {2014},
Note = {IEEE Aerospace Conference, Big Sky, MT, MAR 01-08, 2014},
Organization = {IEEE},
Abstract = {The development of space systems involves complex interdisciplinary
   systems engineering. To manage such complexity, simulation and
   calculation models are becoming an integral part of it, where different
   domain-specific models (power, thermal, structure, propulsion,
   communication, etc.) are developed using different tools. Every domain
   model contains valuable knowledge of a respective discipline. However,
   creating such models takes an ample amount of time and efforts.
   Therefore, a common management for these models is needed to preserve
   the knowledge and to reuse them in future space missions. The project
   Simulation Model Library (SimMoLib) at the German Aerospace Center (DLR)
   develops guidelines and best practices regarding model development,
   model documentation, validation and verification, as well as model
   reviews to establish a collection of reusable models. To efficiently
   catalog the models, an innovative software system is created to support
   collaborative development, submission, archiving, review, search, and
   utilization of models. In SimMoLib, a model linking concept has been
   developed and implemented to enhance the model search and their probable
   reuse. Along with regular keyword-based search, a direct and an indirect
   linking between the models in the library has been implemented.
   Therefore, the model linking increases the visibility and consequently
   promotes the reuse of single and interdependent models within the
   library. The paper further describes different types of model
   relationships, categories, hierarchical levels of model development,
   implementation and presentation of model linking in detail.},
ISBN = {978-1-4799-1622-1},
ResearcherID-Numbers = {Gerndt, Andreas/AAO-2644-2021
   Schwarz, Rene/D-4186-2015},
ORCID-Numbers = {Gerndt, Andreas/0000-0002-0409-8573
   Schwarz, Rene/0000-0002-8255-9451},
Unique-ID = {WOS:000356039101062},
}

@inproceedings{ WOS:000343792400030,
Author = {Faur, Nicolae and Galatanu, Sergiu-Valentin and Hluscu, Mihai},
Editor = {Marsavina, L},
Title = {Study of multiple holes influence on theoretical stress concentration
   coefficient in case of cylindrical vessels},
Booktitle = {PROCEEDINGS OF THE 14TH SYMPOSIUM ON EXPERIMENTAL STRESS ANALYSIS AND
   MATERIALS TESTING},
Series = {Key Engineering Materials},
Year = {2014},
Volume = {601},
Pages = {129-132},
Note = {14th Symposium on Experimental Stress Analysis and Materials Testing,
   POLITEHNICA Univ Timisoara, Timisoara, ROMANIA, MAY 23-25, 2013},
Organization = {AQUATIM SA; ASOCIATIA ROMANA DE TENSOMETR; CONTINENTAL AUTOMOT ROMANIA
   SRL; FUNDATIA POLITEHNICA TIMISOARA; INICAD SRL; KATHREIN SA; MICRONIX
   PLUS SRL; PRIMARIA MUNICIPIULUI TIMISOARA; REGIA AUTONOMA DE TRANSPORT
   TIMISOARA; SELFMED CLINIQUE SRL; SPECTROMAS SRL; STANDARD SERV 2000 SRL;
   UNIVERSITATEA POLITEHNICA TIMISOARA; ZOPPAS INDUSTRIES ROMANIA SRL},
Abstract = {In the construction of pressure vessels, especially for steam boilers
   used in power plants, cylindrical vessels are commonly used. For this
   important class of mechanical structures, through holes also frequently
   cause stress concentration which must be taken into account in the
   design and estimation of their lifetime. In the scientific literature
   which addresses stress concentration effects for such structures are
   studied in cases of which the presence of through holes is singular.
   This paper studies the effect of stress concentration in case of two or
   more holes placed at a small distance between them, according to
   construction requirements. In these cases the simplifying assumption of
   Saint Venant according to which at sufficient distance from the analyzed
   area the state of stress is not influenced by how the load is applied
   cannot be accepted. In this paper we study the stress concentration
   coefficient with numerical methods using the finite element method and
   more precisely, ABAQUS software package, version 6.9. Different
   constructive cases of multiple holes are studied: two and three
   successive holes with the same diameter and different diameters placed
   on the generating line with variable distance, for which the ratio
   between the diameter and the distance between the holes varies.
   Numerical model validation was done by comparing the results obtained
   for specific computational models in case of which results are known and
   presented in the literature {[}1,2]. Based on the results, variation
   curves of the theoretical stress concentration coefficient were drawn
   for all cases listed above.},
DOI = {10.4028/www.scientific.net/KEM.601.129},
ISSN = {1013-9826},
ResearcherID-Numbers = {Galatanu, Sergiu-valentin/ADO-6571-2022
   Hluscu, Mihai/AAX-1944-2020
   Galatanu, Sergiu Valentin/CAH-3214-2022},
ORCID-Numbers = {Galatanu, Sergiu Valentin/0000-0002-7629-8662},
Unique-ID = {WOS:000343792400030},
}

@inproceedings{ WOS:000351506700018,
Author = {Folino, Antonietta and Guaglianoneand, Maria Teresa and Aracri, Giovanna},
Editor = {Rooney, J and Murthy, V},
Title = {Knowledge Management Techniques for Improving Lifelong Learning and
   Professional Competences Description Frameworks},
Booktitle = {PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON INTELLECTUAL
   CAPITAL, KNOWLEDGE MANAGEMENT AND ORGANISATIONAL LEARNING (ICICKM 2014)},
Series = {Proceedings of the International Conference on Intellectual Capital
   Knowledge Management \& Organizational Learning},
Year = {2014},
Pages = {157-165},
Note = {11th International Conference on Intellectual Capital, Knowledge
   Management and Organisational Learning (ICICKM), Univ Sydney Business
   Sch, Sydney, AUSTRALIA, NOV 06-07, 2014},
Abstract = {The intent of this paper is to present a knowledge acquisition and
   organization methodological approach adopted in a European project
   regarding the building construction domain and the obtained results. The
   activity of knowledge formalization is carried out taking inspiration
   from the Common KADS approach, especially its prescriptions about
   knowledge acquisition and elicitation, while the modeling process is
   supported and facilitated by tools made available by PCPACK5, a software
   package for knowledge organization and management. Such a procedure of
   knowledge acquisition and organization - carried out together with some
   reference domain experts - is aimed to capture the interiorized know-how
   and the strategic skills gained through years of experience, to make
   them explicit, to formalize them in a knowledge base and to make them
   reusable. In our case, the main objective of knowledge extraction is not
   only capitalization and formalization of tacit knowledge related to
   processes and activities, but also the description and the subsequent
   recognition of professional competences and skills. The idea is to
   support development, valorization and sharing of expertise in a
   community of practice, in order to make tacit knowledge and competences
   explicit, attestable and easily reusable, for practice improvement and
   organizational learning enhancement. To this aim, we experimented with a
   procedure for knowledge acquisition and organization that aimed at
   highlighting, validating and recognizing innovative competences,
   acquired through informal and non-formal learning paths. It is focused
   on ``green{''} professional profiles, whose activities have effects on
   supporting the reduction of energy consumption and the rational use of
   energy. Their role is completely recognized and consolidated in the
   domain while the green component of their activity is not always known
   and attested. These profiles have been selected, subsequent to a survey
   among the main stakeholders involved in the construction sector, from a
   list provided by qualification frameworks, which describe them by
   structuring them according to competence, knowledge and skill, in
   compliance with the European Qualifications Framework (EQF). The final
   aim is also to extend and improve qualification frameworks and to
   delineate the route for designing training paths of lifelong learning,
   which also take into account the transfer of informal and non-formal
   skills.},
ISSN = {2048-9803},
ISBN = {978-1-910309-73-5},
ResearcherID-Numbers = {GUAGLIANONE, MARIA TERESA/C-1476-2015
   ARACRI, GIOVANNA/N-8020-2015},
ORCID-Numbers = {GUAGLIANONE, MARIA TERESA/0000-0002-4070-5828
   ARACRI, GIOVANNA/0000-0002-8716-3798},
Unique-ID = {WOS:000351506700018},
}

@inproceedings{ WOS:000392944400031,
Author = {Inoue, Takeru and Mano, Toru and Mizutani, Kimihiro and Minato,
   Shin-ichi and Akashi, Osamu},
Book-Group-Author = {IEEE},
Title = {Rethinking Packet Classification for Global Network View of
   Software-Defined Networking},
Booktitle = {2014 IEEE 22ND INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS (ICNP)},
Series = {IEEE International Conference on Network Protocols Proceedings},
Year = {2014},
Pages = {296-307},
Note = {22nd IEEE International Conference on Network Protocols (ICNP), Raleigh,
   NC, OCT 21-24, 2014},
Organization = {IEEE; IEEE Comp Soc; Univ N Carolina; N Carolina State Univ},
Abstract = {In software-defined networking, applications are allowed to access a
   global view of the network so as to provide sophisticated
   functionalities, such as quality-oriented service delivery, automatic
   fault localization, and network verification. All of these
   functionalities commonly rely on a well-studied technology, packet
   classification. Unlike the conventional classification problem to search
   for the action taken at a single switch, the global network view
   requires to identify the network-wide behavior of the packet, which is
   defined as a combination of switch actions. Conventional classification
   methods, however, fail to well support network-wide behaviors, since the
   search space is complicatedly partitioned due to the combinations.
   This paper proposes a novel packet classification method that
   efficiently supports network-wide packet behaviors. Our method utilizes
   a compressed data structure named the multi-valued decision diagram,
   allowing it to manipulate the complex search space with several
   algorithms. Through detailed analysis, we optimize the classification
   performance as well as the construction of decision diagrams.
   Experiments with real network datasets show that our method identifies
   the packet behavior at 20.1 Mpps on a single CPU core with only 8.4 MB
   memory; by contrast, conventional methods failed to work even with 16 GB
   memory. We believe that our method is essential for realizing advanced
   applications that can fully leverage the potential of software-defined
   networking.},
DOI = {10.1109/ICNP.2014.52},
ISSN = {1092-1648},
ISBN = {978-1-4799-6204-4},
Unique-ID = {WOS:000392944400031},
}

@inproceedings{ WOS:000356039102046,
Author = {Jaekel, Steffen and Stelzer, Martin and Herpel, Hans-Juergen},
Book-Group-Author = {IEEE},
Title = {Robust and Modular On-Board Architecture for Future Robotic Spacecraft},
Booktitle = {2014 IEEE AEROSPACE CONFERENCE},
Year = {2014},
Note = {IEEE Aerospace Conference, Big Sky, MT, MAR 01-08, 2014},
Organization = {IEEE},
Abstract = {This paper presents a novel approach for future robotic spacecraft by
   utilizing a modular and robust software architecture based on the time
   and space partitioning (TSP) concept. Classic satellites are
   characterized by a strict separation between platform and payload
   subsystems, both in hardware resources as well as in control software.
   Novel space-robotic applications such as on-orbit servicing (OOS)
   feature dexterous robotic devices attached onto the satellite that
   impose a direct physical feedback on their floating base. Through the
   high degree of interdependencies, the whole satellite turns into a space
   robot. Hence, the robot becomes an integral part of the spacecraft
   itself and needs to be integrated into the existing control and
   operations approach. The developed embedded on-board framework
   represents a modular and robust control and communication environment
   that allows both classic satellite as well as real-time and autonomous
   robotic operations. The framework features an integral fault detection,
   isolation and recovery (FDIR) concept in order to prevent overall system
   shutdown upon single-point failure. Single software components reside in
   separate logical modules, i.e. partitions, in order to avoid resource
   violations. Upon critical failure, partitions can be restarted without
   detracting the rest of the system. By applying explicit time scheduling
   of partitions, system resources can be optimally distributed and
   deterministic behavior be achieved. Core system functionality has been
   implemented by ECSS-tested components that are configurable and thus
   re-usable over multiple missions. As demonstrator, a realistic on-orbit
   servicing simulation was set up that comprises autonomous target
   satellite capture and fault management. The presented architecture
   follows an integrated approach that is required for safely operating
   future robotic spacecraft. Through reusability of software components,
   fewer resources for the implementation and verification process are
   required as only additional, mission-specific components need to be
   taken care of. Application developers can use the core functionality and
   communication API and concentrate on their own task at hand.},
ISBN = {978-1-4799-1622-1},
Unique-ID = {WOS:000356039102046},
}

@inproceedings{ WOS:000371585600038,
Author = {Kostov, Vladimir and Dikov, Dimitar},
Book-Group-Author = {SGEM},
Title = {DETERMINATION OF THE DEPENDENCES G = f (gamma) AND D = f (gamma) ON A
   TAILING IN A TRIAXIAL DYNAMICS SHEAR APPARATUS},
Booktitle = {GEOCONFERENCE ON SCIENCE AND TECHNOLOGIES IN GEOLOGY, EXPLORATION AND
   MINING, SGEM 2014, VOL II},
Series = {International Multidisciplinary Scientific GeoConference-SGEM},
Year = {2014},
Pages = {291-298},
Note = {14th International Multidisciplinary Scientific Geoconference (SGEM),
   Albena, BULGARIA, JUN 17-26, 2014},
Organization = {Bulgarian Acad Sci; Acad Sci Czech Repub; Latvian Acad Sci; Polish Acad
   Sci; Russian Acad Sci; Serbian Acad Sci \& Arts; Slovak Acad Sci; Natl
   Acad Sci Ukraine; Inst Water Problem \& Hydropower NAS KR; Natl Acad Sci
   Armenia; Sci Council Japan; World Acad Sci; European Acad Sci Arts \&
   Letters; Acad Sci Moldova; Montenegrin Acad Sci \& Arts; Croatian Acad
   Sci \& Arts; Georgian Natl Acad Sci; Acad Fine Arts \& Design
   Bratislava; Turkish Acad Sci; Bulgarian Ind Assoc; Bulgarian Minist
   Environ \& Water},
Abstract = {Main task in the design and construction of a tailing pond is to ensure
   that the tailings dam can withstand seismic actions, in an area with
   high seismic activity, such as Bulgaria.
   Modern FEM analysis software packages, that can evaluate the seismic
   stability of such type of structures require as an input parameter the
   dependencies G = f (gamma) and D = f (gamma) in the range from 10(-4)\%
   to10(1)\%. One method to derive with these dependencies in a laboratory
   conditions is the dynamic triaxial shear.
   In this article we present, the results from the tests carried out on
   undisturbed tailings samples taken from different depths (20 to 90m) of
   ``Lyulyakovitza{''} tailing.
   The obtained results were evaluated and statistically analyzed in order
   to derive with the dependencies for G = f (gamma) and D = f (gamma).},
ISSN = {1314-2704},
ISBN = {978-619-7105-08-7},
Unique-ID = {WOS:000371585600038},
}

@article{ WOS:000335543800021,
Author = {Morgenroth, J. and Gomez, C.},
Title = {Assessment of tree structure using a 3D image analysis technique-A proof
   of concept},
Journal = {URBAN FORESTRY \& URBAN GREENING},
Year = {2014},
Volume = {13},
Number = {1},
Pages = {198-203},
Abstract = {Efforts to improve the efficiency and efficacy of tree structure and
   crown architecture measurement are necessary to reduce error associated
   with indirect estimation of volume, which affects biophysical and
   ecosystem modelling, as well as resource assessment. In this short
   communication, we test the potential for a commercial SfM-MVS (Structure
   from Motion coupled with Multiple-View Stereophotogrammetry) software
   package, Photoscan-Professional, to accurately determine tree height,
   stem diameter, and eventually volume. SfM is a technique in computer
   vision, which calculates the 3D position of objects in a scene from a
   series of photographs. It uses a technique assuming that an object in a
   3D scene is located on a vector between the image of the object in the
   camera and the object itself. The technique allows the construction of a
   3D pointcloud, such as the one produced by laser technologies -
   terrestrial or airborne LiDAR. SfM requires no camera calibration or
   control points to construct an initial model. Moreover, it is a low-cost
   alternative to laser technologies. The second part of our method - MVS -
   is a visualization technique that reconstructs a 3D textured mesh of the
   scene from the SfM-derived pointcloud and RGB photographs. As a proof of
   concept, our methods were limited to two scenarios: (1) a single potted
   tree in a lab environment, where exact measurements could be made, and
   (2) two trees of different species and size in natural environments to
   test feasibility outside the laboratory. Precise measurements of tree
   height and stem diameter were compared with estimates obtained from the
   3D model created using SfM-MVS. The results indicate that the SfM method
   is a promising - and inexpensive - alternative to terrestrial LiDAR and
   3D scanners. Tree height estimates had error of 2.59\%, while stem
   diameter estimates had error of 3.7\%. The MVS algorithm used in this
   study was developed for plan surfaces such as topography or `compact'
   objects and does not provide a representative 3D mesh for slender trees,
   although it works well for large stems. The authors link this disparity
   to the complex branching structure of trees.
   Future work requires (1) the development of effective automated volume
   reconstruction software specific to trees, and (2) the validation of the
   wide-scale applicability of the technology, which must include trees of
   various size, species, and growth forms. Moreover, future testing must
   include more complex environments, such as heterogeneous urban sites or
   closed-canopy forest sites where the proximity of other features may
   limit the utility of the new technology. (C) 2013 Elsevier GmbH. All
   rights reserved.},
DOI = {10.1016/j.ufug.2013.10.005},
ISSN = {1618-8667},
EISSN = {1610-8167},
ResearcherID-Numbers = {Gomez, Christopher/Q-8402-2019
   },
ORCID-Numbers = {Gomez, Christopher/0000-0002-1738-2434
   morgenroth, justin/0000-0002-2747-7349},
Unique-ID = {WOS:000335543800021},
}

@inproceedings{ WOS:000334025000007,
Author = {Pan, Xunyu and Free, Kevin M.},
Editor = {Creutzburg, R and Akopian, D},
Title = {Interactive Real-time Media Streaming with Reliable Communication},
Booktitle = {MOBILE DEVICES AND MULTIMEDIA: ENABLING TECHNOLOGIES, ALGORITHMS, AND
   APPLICATIONS 2014},
Series = {Proceedings of SPIE},
Year = {2014},
Volume = {9030},
Note = {Conference on Mobile Devices and Multimedia - Enabling Technologies,
   Algorithms, and Applications, San Francisco, CA, FEB 03-05, 2014},
Organization = {Soc Imaging Sci \& Technol; SPIE},
Abstract = {Streaming media is a recent technique for delivering multimedia
   information from a source provider to an end-user over the Internet. The
   major advantage of this technique is that the media player can start
   playing a multimedia file even before the entire file is transmitted.
   Most streaming media applications are currently implemented based on the
   client-server architecture, where a server system hosts the media file
   and a client system connects to this server system to download the file.
   Although the client-server architecture is successful in many
   situations, it may not be ideal to rely on such a system to provide the
   streaming service as users may be required to register an account using
   personal information in order to use the service. This is troublesome if
   a user wishes to watch a movie simultaneously while interacting with a
   friend in another part of the world over the Internet. In this paper, we
   describe a new real-time media streaming application implemented on a
   peer-to-peer (P2P) architecture in order to overcome these challenges
   within a mobile environment. When using the peer-to-peer architecture,
   streaming media is shared directly between end-users, called peers, with
   minimal or no reliance on a dedicated server. Based on the proposed
   software rho epsilon nu mu alpha (pronounced {[}revma]), named for the
   Greek word meaning stream, we can host a media file on any computer and
   directly stream it to a connected partner. To accomplish this, rho
   epsilon nu mu alpha utilizes the Microsoft.NET Framework and Windows
   Presentation Framework, which are widely available on various types of
   windows-compatible personal computers and mobile devices. With specially
   designed multi-threaded algorithms, the application can stream HD video
   at speeds upwards of 20 Mbps using the User Datagram Protocol (UDP).
   Streaming and playback are handled using synchronized threads that
   communicate with one another once a connection is established.
   Alteration of playback, such as pausing playback or tracking to a
   different spot in the media file, will be reflected in all media
   streams. These techniques are designed to allow users at different
   locations to simultaneously view a full length HD video and
   interactively control the media streaming session. To create a
   sustainable media stream with high quality, our system supports UDP
   packet loss recovery at high transmission speed using custom
   File-Buffers. Traditional real-time streaming protocols such as
   Real-time Transport Protocol/RTP Control Protocol (RTP/RTCP) provide no
   such error recovery mechanism. Finally, the system also features an
   Instant Messenger that allows users to perform social interactions with
   one another while they enjoy a media file. The ultimate goal of the
   application is to offer users a hassle free way to watch a media file
   over long distances without having to upload any personal information
   into a third party database. Moreover, the users can communicate with
   each other and stream media directly from one mobile device to another
   while maintaining an independence from traditional sign up required by
   most streaming services.},
DOI = {10.1117/12.2042388},
Article-Number = {903008},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-0-8194-9947-9},
ResearcherID-Numbers = {Pan, Xunyu/G-9194-2014},
Unique-ID = {WOS:000334025000007},
}

@inproceedings{ WOS:000363497500023,
Author = {Perez, Gabriel Vazquez and Pistidda, Alessio and Smienk, Henk},
Book-Group-Author = {ASME},
Title = {MOONPOOL EFFECT ASSESSMENT DURING STRUCTURE INSTALLATION},
Booktitle = {33RD INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING,
   2014, VOL 6A: PIPELINE AND RISER TECHNOLOGY},
Year = {2014},
Note = {33rd ASME International Conference on Ocean, Offshore and Arctic
   Engineering, San Francisco, CA, JUN 08-13, 2014},
Organization = {ASME, Ocean, Offshore \& Arct Engn Div},
Abstract = {Heerema Marine Contractors (HMC) is entering a new era of pipe laying
   with the new Deep water Construction Vessel (DCV) Aegir, which is
   designed to be able to reel/J-lay pipelines for a wide range of pipe
   dimension and water depth combinations. The DCV Aegir has a moonpool
   close to the center of the vessel in which the Hang Off Module (HOM) for
   pipeline installation is positioned. The HOM is utilized to hold the
   pipeline and its structures during installation {[}1]. Pipelines and its
   structures will be lowered through the moonpool. When a pipeline
   structure is lowered through the moonpool the HOM is refracted, thus
   clearing the moonpool to allow the free pass of the structure.
   The behavior of the water in the moonpool will induce loads on the
   structures when these are being lowered. These loads will induce motions
   and bending moments on the attached pipeline. The effect of the moonpool
   on the structure has to be known in order to assess the integrity of the
   pipe (fatigue and local buckling limit state) and the optimal procedure
   for structure installation.
   The assessment of the moonpool effect on the structure during the
   lowering through the splash zone is complex because it is governed on
   one side by the motions of the vessel and on the other side by the
   motion of the water inside the moonpool.
   This paper will focus on the moonpool effect assessment in a Pipe Line
   End Termination (PLET) installation analysis. Two different software
   packages are utilized; Flexcom of MCSKenny and STAR-CCM+ of CD-Adapco.
   Flexcom is used to reproduce the vessel motions which affect the PLET
   motion behavior. Pipe properties (OD, WT and length) and PLET together
   with pipelay equipment; such as Upper Tensioner (UT), HOM, Pipe Wire
   Centralizer (PWC) and winch, are modelled.
   To assess the hydrodynamic loads on the PLET inside the moonpool a
   Computational Fluid Dynamics (CFD) approach is required. STAR-CCM+ is a
   CFD program which uses Finite Volume Method (FVM) to solve the flow
   field motion equations. With this software it is possible to accurately
   model the geometry of the PLET and simulate the interaction between
   water in the moonpool and the PLET.
   The paper will present an overview of the models and the analyses
   performed, together with the methodology used to combine the outcome of
   both software packages.},
Article-Number = {V06AT04A023},
ISBN = {978-0-7918-4546-2},
Unique-ID = {WOS:000363497500023},
}

@inproceedings{ WOS:000345075200034,
Author = {Pineda Osorio, Mateo},
Editor = {Southern, SO and Mentzer, MA and RodriguezChavez, I and Wotring, VE},
Title = {Using quantum filters to process images of diffuse axonal injury},
Booktitle = {SENSING TECHNOLOGIES FOR GLOBAL HEALTH, MILITARY MEDICINE, AND
   ENVIRONMENTAL MONITORING IV},
Series = {Proceedings of SPIE},
Year = {2014},
Volume = {9112},
Note = {Conference on Sensing Technologies for Global Health, Military Medicine,
   and Environmental Monitoring IV, Baltimore, MD, MAY 05-07, 2014},
Organization = {SPIE},
Abstract = {Some images corresponding to a diffuse axonal injury (DAI) are processed
   using several quantum filters such as Hermite Weibull and Morse. Diffuse
   axonal injury is a particular, common and severe case of traumatic brain
   injury (TBI). DAI involves global damage on microscopic scale of brain
   tissue and causes serious neurologic abnormalities. New imaging
   techniques provide excellent images showing cellular damages related to
   DAI. Said images can be processed with quantum filters, which accomplish
   high resolutions of dendritic and axonal structures both in normal and
   pathological state. Using the Laplacian operators from the new quantum
   filters, excellent edge detectors for neurofiber resolution are
   obtained. Image quantum processing of DAI images is made using computer
   algebra, specifically Maple. Quantum filter plugins construction is
   proposed as a future research line, which can incorporated to the ImageJ
   software package, making its use simpler for medical personnel.},
DOI = {10.1117/12.2049789},
Article-Number = {91121Q},
ISSN = {0277-786X},
ISBN = {978-1-62841-049-5},
Unique-ID = {WOS:000345075200034},
}

@inproceedings{ WOS:000329970600048,
Author = {Rinaldin, Giovanni and Fragiacomo, Massimo},
Editor = {Aicher, S and Reinhardt, HW and Garrecht, H},
Title = {A Component Model for Cyclic Behaviour of Wooden Structures},
Booktitle = {MATERIALS AND JOINTS IN TIMBER STRUCTURES: RECENT DEVELOPMENTS OF
   TECHNOLOGY},
Series = {RILEM Bookseries},
Year = {2014},
Volume = {9},
Pages = {519-530},
Note = {RILEM International Symposium on Materials and Joints in Timber
   Structures, Stuttgart, GERMANY, OCT 08-10, 2013},
Organization = {RILEM; AkzoNobel; BASF; Dresselhaus; Dynea; ENGEL; HECO Schrauben; HESS;
   Kielsteg; Oest; Purbond; Reisser Schraubentechnik; Schworer Haus KG; SFS
   Intec; SPAX; STEICO; Stephan; BS Holz; BSP Holz; Weinig; WEVO; Wurth;
   Lubbert Warenhandel; Assy Das Original},
Abstract = {Wooden structures have become widespread in several regions of the
   world, including earthquake-prone areas. An estimation of their
   dissipative capacity is of fundamental importance for an accurate design
   for seismic actions. To reproduce the structural behaviour under
   earthquake excitation, a numerical model was developed and validated on
   experimental results. The cyclic behaviour of connections for
   cross-laminated (X-lam) buildings, light-frame construction and
   moment-resisting frames were schematised by a piecewise linear
   hysteretic relationship taking into account strength/stiffness
   degradation, pinching and friction between elements. This relationship
   was assigned to a non-linear elasto-plastic spring implemented in Abaqus
   software package. The spring has to be calibrated on experimental tests
   performed on single connections, which are the only components
   dissipating energy in the structure. All the other timber components
   (X-lam panels, timber beams, sheathing panels, etc.) are regarded as
   linear elastic. In this work, several examples of cyclic analyses of
   wooden structures are presented, including X-lam buildings, light-frame
   construction, and moment-resisting frames. Numerical predictions are
   compared with experimental results demonstrating the effectiveness of
   the model.},
DOI = {10.1007/978-94-007-7811-5\_48},
ISSN = {2211-0844},
ISBN = {978-94-007-7810-8},
ResearcherID-Numbers = {Rinaldin, Giovanni/J-5335-2019
   Rinaldin, Giovanni/H-5867-2016
   Fragiacomo, Massimo/AAB-4005-2020},
ORCID-Numbers = {Rinaldin, Giovanni/0000-0003-0948-3611
   Rinaldin, Giovanni/0000-0003-0948-3611
   Fragiacomo, Massimo/0000-0002-9178-7501},
Unique-ID = {WOS:000329970600048},
}

@inproceedings{ WOS:000361021000009,
Author = {Rughetti, Diego and Di Sanzo, Pierangelo and Ciciani, Bruno and Quaglia,
   Francesco},
Editor = {Balaji, P and Foster, I and Sun, XH and Cameron, KW and Nikolopoulos, DS},
Title = {Analytical/ML Mixed Approach for Concurrency Regulation in Software
   Transactional Memory},
Booktitle = {2014 14TH IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER, CLOUD AND GRID
   COMPUTING (CCGRID)},
Series = {IEEE-ACM International Symposium on Cluster Cloud and Grid Computing},
Year = {2014},
Pages = {81-91},
Note = {14th IEEE/ACM International Symposium on Cluster, Cloud and Grid
   Computing (CCGrid), Chicago, IL, MAY 26-29, 2014},
Organization = {IEEE; Assoc Comp Machinery; IEEE Comp Soc; U S Natl Sci Fdn; IEEE Tech
   Comm Scalable Comp},
Abstract = {In this article we exploit a combination of analytical and Machine
   Learning (ML) techniques in order to build a performance model allowing
   to dynamically tune the level of concurrency of applications based on
   Software Transactional Memory (STM). Our mixed approach has the
   advantage of reducing the training time of pure machine learning
   methods, and avoiding approximation errors typically affecting pure
   analytical approaches. Hence it allows very fast construction of highly
   reliable performance models, which can be promptly and effectively
   exploited for optimizing actual application runs. We also present a real
   implementation of a concurrency regulation architecture, based on the
   mixed modeling approach, which has been integrated with the open source
   TinySTM package, together with experimental data related to runs of
   applications taken from the STAMP benchmark suite demonstrating the
   effectiveness of our proposal.},
DOI = {10.1109/CCGrid.2014.118},
ISSN = {2376-4414},
ISBN = {978-1-4799-2784-5},
ResearcherID-Numbers = {Di Sanzo, Pierangelo/HII-2776-2022
   Di Sanzo, Pierangelo/W-6340-2019
   Di Sanzo, Pierangelo/E-3776-2018},
ORCID-Numbers = {Di Sanzo, Pierangelo/0000-0001-6136-6303
   Di Sanzo, Pierangelo/0000-0001-6136-6303
   Di Sanzo, Pierangelo/0000-0001-6136-6303},
Unique-ID = {WOS:000361021000009},
}

@inproceedings{ WOS:000411853300046,
Author = {Segura, Angel Mora and Cuadrado, Jesus Sanchez and de Lara, Juan},
Editor = {Grossmann, G and Halle, S and Karastoyanova, D and Reichert, M and StefanieRinderleMa},
Title = {ODaaS: Towards the model-driven engineering of open data applications as
   data services},
Booktitle = {2014 IEEE 18TH INTERNATIONAL ENTERPRISE DISTRIBUTED OBJECT COMPUTING
   CONFERENCE WORKSHOPS AND DEMONSTRATIONS (EDOCW)},
Series = {IEEE International Enterprise Distributed Object Computing Conference
   Workshops-EDOCW},
Year = {2014},
Pages = {335-339},
Note = {18th IEEE International Enterprise Distributed Object Computing
   Conference (EDOC), Ulm Univ, Ulm, GERMANY, SEP 01-05, 2014},
Organization = {IEEE; IEEE Commun Soc; IEEE Comp Soc},
Abstract = {The Data-as-a-Service (DaaS, or Data Services) paradigm enables an
   on-demand, service-based access to data, relying on similar principles
   to Software-as-a-Service (SaaS). DaaS permits centralized data quality
   management, a uniform view and access to heterogeneous data, and enables
   exposing a richer, domain-specific data model to users.
   Within this context, we are witnessing a trend in institutions to make
   information public as open data. However, such information is normally
   released ``as-is{''}, in heterogeneous formats, requiring costly, ad-hoc
   pre-processing steps for cleansing and analysis of its underlying
   structure.
   This paper proposes an adaptation of the DaaS paradigm for the
   construction of open data applications. For this purpose, we introduce
   an architecture based on Model-Driven Engineering (MDE), consisting of
   (i) multi-level modelling for the description of domains, based on
   generic meta-models, (ii) a library of injectors to bring data on demand
   from heterogeneous sources into the MDE technical space, and (iii) a
   REST-based infrastructure to access the data services. This work
   presents the architecture of such framework and the first steps in its
   realization.},
DOI = {10.1109/EDOCW.2014.55},
ISSN = {2325-6583},
ISBN = {978-1-4799-5467-4},
ResearcherID-Numbers = {Mora-Segura, Ángel/F-3864-2019
   Mora-Segura, Ángel/L-8717-2019
   Sanchez Cuadrado, Jesus/L-2365-2013},
ORCID-Numbers = {Mora-Segura, Ángel/0000-0003-4318-499X
   Mora-Segura, Ángel/0000-0003-4318-499X
   Sanchez Cuadrado, Jesus/0000-0001-9755-5616},
Unique-ID = {WOS:000411853300046},
}

@inproceedings{ WOS:000380569400028,
Author = {Wang, Feng and Wang, Heyu and Lei, Baohua and Ma, Wenting},
Book-Group-Author = {IEEE},
Title = {A Research on Carrier-grade SDN Controller},
Booktitle = {2014 INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA (CCBD)},
Series = {International Conference on Cloud Computing and Big Data-CCBD},
Year = {2014},
Pages = {168-174},
Note = {International Conference on Cloud Computing and Big Data (CCBD), Wuhan,
   PEOPLES R CHINA, NOV 12-14, 2014},
Abstract = {Software Defined Networking (SDN) is a new programmable network
   construction technology that enables centrally management and control,
   which is considered to be the future evolution trend of networks. A
   modularized carrier-grade SDN controller according to the
   characteristics of carrier-grade networks is designed and proposed,
   resolving the problem of controlling large-scale networks of carrier.
   The modularized architecture offers the system flexibility, scalability
   and stability. Functional logic of modules and core modules, such as
   link discovery module and topology module, are designed to meet the
   carrier's need. Static memory allocation, multi-threads technique and
   stick-package processing are used to improve the performance of
   controller, which is C programming language based. Processing logic of
   the communication mechanism of the controller is introduced, proving
   that the controller conforms to the OpenFlow specification and has a
   good interaction with OpenFlow-based switches. A controller cluster
   management system is used to interact with controllers through the
   east-west interface in order to manage large-scale networks.
   Furthermore, the effectiveness and high performance of the work in this
   paper has been verified by the testing using Cbench testing program.
   Moreover, the SDN controller we proposed has been running in China
   Telecom's Cloud Computing Key Laboratory, which showed the good results
   is achieved.},
DOI = {10.1109/CCBD.2014.41},
ISSN = {2378-3680},
ISBN = {978-1-4799-6621-9},
Unique-ID = {WOS:000380569400028},
}

@inproceedings{ WOS:000380491200052,
Author = {Yen, Freedman and Hung, Leo and Kao, Nicholas and Jiang, Don Son},
Book-Group-Author = {IEEE},
Title = {MoldFlow Simulation Study on Void Risk Prediction for FCCSP with Molded
   Underfill Technology},
Booktitle = {2014 IEEE 16TH ELECTRONICS PACKAGING TECHNOLOGY CONFERENCE (EPTC)},
Year = {2014},
Pages = {817-821},
Note = {IEEE 16th Electronics Packaging Technology Conference (EPTC), Marina Bay
   Sands, SINGAPORE, DEC 03-05, 2014},
Organization = {IEEE; CPMT},
Abstract = {The microelectronics products of Flip Chip-Chip Scale Package (FCCSP)
   with more increasing challenges are faced to assure molding capability
   with rapid advances in flip chip technology such as decreasing stand-off
   height and bump pitch, especially when Molded Underfill (MUF) is used
   during transfer molding process. There is one important challenge that
   faced severe air void entrapment under the die (air void concentrate
   among bumps region). Generally, the experiments involving a lot of DOE
   matrixes which spend a lot of time and materials (dummy die, substrate,
   mold compound... etc.) to solve this air void issue. As above reasons,
   the moldflow simulation can be used to apply molding parameters to find
   out optimum solutions for air void risk free of MUF FCCSP with different
   bump structure or substrate structure design, which can reduce
   development cycle time before mass production.
   In this paper, 3D moldflow simulation software which can apply transfer
   molding process parameters is used. There are two molding flow factors
   will be presented in this paper. One is MUF FCCSP with different
   stand-off height construction (control different bump height dimension)
   which performs significant difference molding melt-front position. And
   another is substrate solder mask with different pattern design (solder
   mask w/all open or finger like pattern design) which molding compound
   through over on solder mask pattern (solder mask with open region as
   10um depth structure) and performs different melt-front pattern..
   From this study, we can conclude some results for improvement molding
   performance of MUF FCCSP during transfer molding process. The MUF FCCSP
   with the 50um stand-off height structure performs low air void risk due
   to mold compound could easily flow under die region with more flow
   space. In addition, mold compound also performs well melt-front flow
   that the substrate solder mask with all open structure design can get
   more 10um flow space under die region. Finally, the simulation results
   are aligned with experiments and it can be used to predict void risk.},
ISBN = {978-1-4799-6994-4},
Unique-ID = {WOS:000380491200052},
}

@inproceedings{ WOS:000338596900056,
Author = {Zha, Xiaoxiong and Zuo, Yang and Chen, Shiyun},
Editor = {Zhang, H and Jin, D and Zhao, XJ},
Title = {Study on the Optimal Design of the Memebers of Contaniner Building with
   Building Materials I: Side Board Optimization},
Booktitle = {ADVANCED RESEARCH ON CIVIL ENGINEERING, MATERIALS ENGINEERING AND
   APPLIED TECHNOLOGY},
Series = {Advanced Materials Research},
Year = {2014},
Volume = {859},
Pages = {270-273},
Note = {2nd International Conference on Civil Engineering and Material
   Engineering (CEME 2013), Wuhan, PEOPLES R CHINA, DEC 21-22, 2013},
Organization = {Int Sci \& Educ Res Assoc; Beijing Gireida Educ Res Ctr; VIP Informat
   Conf Ctr},
Abstract = {Container, as a light steel structure, being increasingly used in
   building construction, containers used in construction has many
   advantages and applications. However, the current study mostly from the
   view of the architecture, as for the mechanical properties of the
   container building has not mentioned, that brings obstacles of the
   application and development of the container building. Based on the
   software package of HyperWorks and optimization design theory, the
   cross-sectional size of container building is taken as design variables,
   and then selected objective function and constraint functions. Finally,
   calculated by software, get the optimal cross-sectional dimension.},
DOI = {10.4028/www.scientific.net/AMR.859.270},
ISSN = {1022-6680},
ISBN = {978-3-03785-979-7},
Unique-ID = {WOS:000338596900056},
}

@inproceedings{ WOS:000366666801145,
Author = {Zhao, Miao and Jia, Bin and Wu, Mingquan and Yu, Heather and Xu, Yang},
Editor = {Jamalipour, A and Deng, DJ},
Title = {Software Defined Network-Enabled Multicast for Multi-Party Video
   Conferencing Systems},
Booktitle = {2014 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC)},
Series = {IEEE International Conference on Communications},
Year = {2014},
Pages = {1729-1735},
Note = {IEEE International Conference on Communications (ICC), Sydney,
   AUSTRALIA, JUN 10-14, 2014},
Organization = {IEEE; IEEE Commun Soc; IEEE, New S Wales Sect},
Abstract = {Providing well-satisfactory multi-party video conferencing service is
   remarkably challenging, which demands not only high bandwidth but also
   low latency. Current commercial implementations typically use one or
   multiple multipoint control units (MCUs) as the central points for
   distributing video bit-streams to all participants in the conferencing
   session. Such MCU-based solution has limited control on quality of
   service (QoS), which may cause large delay, single-point malfunction and
   communication bottleneck for entire system. Recent years have witnessed
   the emergence of a new paradigm in networking, software defined
   networking (SDN), which advocates separating data plane and control
   plane, making network switches in the data plane simple packet
   forwarding devices and leaving a logically centralized controller to
   manipulate network behaviors. SDN provides the flexibility of changing
   underlying infrastructure and makes it possible to radically provide
   service/flow-aware QoS guarantee. In this paper, we propose a novel
   architecture for multi-party video conferencing by utilizing SDN-enabled
   multicasting, where SDN controller helps media controller to buildup
   multicast trees for video flows originated at video parties. To realize
   the architecture, we correspondingly propose a novel multicast
   construction and packing method, in which multiple source-based
   multicast trees are constructed and integrated to maximize system-wide
   utility while guaranteeing an end-to-end delay bound. Extensive
   simulations demonstrate that it could provide better video delivery
   compared to the conventional MCU-based solution in terms of video rate
   and delay in both sparsely and densely distributed networks.},
ISSN = {1550-3607},
ISBN = {978-1-4799-2003-7},
ResearcherID-Numbers = {xu, yang/HOC-0456-2023
   },
ORCID-Numbers = {Xu, Yang/0000-0002-0958-8547
   Zhao, Miao/0000-0002-4324-1467},
Unique-ID = {WOS:000366666801145},
}

@article{ WOS:000324229600006,
Author = {Davies, Julius and German, Daniel M. and Godfrey, Michael W. and Hindle,
   Abram},
Title = {Software Bertillonage Determining the provenance of software development
   artifacts},
Journal = {EMPIRICAL SOFTWARE ENGINEERING},
Year = {2013},
Volume = {18},
Number = {6},
Pages = {1195-1237},
Month = {DEC},
Abstract = {Deployed software systems are typically composed of many pieces, not all
   of which may have been created by the main development team. Often, the
   provenance of included components-such as external libraries or cloned
   source code-is not clearly stated, and this uncertainty can introduce
   technical and ethical concerns that make it difficult for system owners
   and other stakeholders to manage their software assets. In this work, we
   motivate the need for the recovery of the provenance of software
   entities by a broad set of techniques that could include signature
   matching, source code fact extraction, software clone detection, call
   flow graph matching, string matching, historical analyses, and other
   techniques. We liken our provenance goals to that of Bertillonage, a
   simple and approximate forensic analysis technique based on bio-metrics
   that was developed in 19th century France before the advent of
   fingerprints. As an example, we have developed a fast, simple, and
   approximate technique called anchored signature matching for identifying
   the source origin of binary libraries within a given Java application.
   This technique involves a type of structured signature matching
   performed against a database of candidates drawn from the Maven2
   repository, a 275 GB collection of open source Java libraries. To show
   the approach is both valid and effective, we conducted an empirical
   study on 945 jars from the Debian GNU/Linux distribution, as well as an
   industrial case study on 81 jars from an e-commerce application.},
DOI = {10.1007/s10664-012-9199-7},
ISSN = {1382-3256},
EISSN = {1573-7616},
Unique-ID = {WOS:000324229600006},
}

@article{ WOS:000327409300009,
Author = {Furber, Steve B. and Lester, David R. and Plana, Luis A. and Garside,
   Jim D. and Painkras, Eustace and Temple, Steve and Brown, Andrew D.},
Title = {Overview of the SpiNNaker System Architecture},
Journal = {IEEE TRANSACTIONS ON COMPUTERS},
Year = {2013},
Volume = {62},
Number = {12},
Pages = {2454-2467},
Month = {DEC},
Abstract = {SpiNNaker (a contraction of Spiking Neural Network Architecture) is a
   million-core computing engine whose flagship goal is to be able to
   simulate the behavior of aggregates of up to a billion neurons in real
   time. It consists of an array of ARM9 cores, communicating via packets
   carried by a custom interconnect fabric. The packets are small (40 or 72
   bits), and their transmission is brokered entirely by hardware, giving
   the overall engine an extremely high bisection bandwidth of over 5
   billion packets/s. Three of the principal axioms of parallel machine
   design (memory coherence, synchronicity, and determinism) have been
   discarded in the design without, surprisingly, compromising the ability
   to perform meaningful computations. A further attribute of the system is
   the acknowledgment, from the initial design stages, that the sheer size
   of the implementation will make component failures an inevitable aspect
   of day-to-day operation, and fault detection and recovery mechanisms
   have been built into the system at many levels of abstraction. This
   paper describes the architecture of the machine and outlines the
   underlying design philosophy; software and applications are to be
   described in detail elsewhere, and only introduced in passing here as
   necessary to illuminate the description.},
DOI = {10.1109/TC.2012.142},
ISSN = {0018-9340},
EISSN = {1557-9956},
ORCID-Numbers = {Plana, Luis A./0000-0002-6113-3929
   Furber, Stephen/0000-0002-6524-3367
   Lester, David/0000-0002-7267-291X
   Garside, James/0000-0001-8812-4742},
Unique-ID = {WOS:000327409300009},
}

@article{ WOS:000328234000035,
Author = {Hoque, A. S. M. and Halder, P. K. and Parvez, M. S. and Szecsi, T.},
Title = {Integrated manufacturing features and Design-for-manufacture guidelines
   for reducing product cost under CAD/CAM environment},
Journal = {COMPUTERS \& INDUSTRIAL ENGINEERING},
Year = {2013},
Volume = {66},
Number = {4},
Pages = {988-1003},
Month = {DEC},
Abstract = {The main contribution of the work is to develop an intelligent system
   for manufacturing features in the area of CAD/CAM. It brings the design
   and manufacturing phase together in design stage and provides an
   intelligent interface between design and manufacturing data by
   developing a library of features. The library is called manufacturing
   feature library which is linked with commercial CAD/CAM software package
   named Creo Elements/Pro by toolkit. Inside the library, manufacturing
   features are organised hierarchically. A systematic database system also
   have been developed and analysed for each feature consists of
   parameterised geometry, manufacturing information (including machine
   tool, cutting tools, cutting conditions, cutting fluids and recommended
   tolerances and surface finishing values, etc.), design limitations,
   functionality guidelines, and Design-for-manufacture guidelines. The
   approach has been applied in two case studies in which a rotational part
   (shaft) and a non-rotational part are designed through manufacturing
   features. Therefore, from manufacturing feature library a design can
   compose entirely in a bottom-up manner using manufacturable entities in
   the same way as they would be produced during the manufacturing phase.
   Upon insertion of a feature, the system ensures that no functionality or
   manufacturing guidelines are violated. The designers are warned if they
   attempt to include features that violate Design-for-manufacture and
   Design functionality guidelines. If a feature is modified, the system
   validates the feature by making sure that it remains consistent with its
   original functionality and Design-for-manufacture guidelines are
   re-applied. The system will be helped the process planner/manufacturing
   engineer by automatically creating work-piece data structure. (C) 2013
   Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.cie.2013.08.016},
ISSN = {0360-8352},
EISSN = {1879-0550},
ResearcherID-Numbers = {Halder, Pobitra/F-8758-2016
   Halder, Pobitra/AAH-4001-2020},
ORCID-Numbers = {Halder, Pobitra/0000-0002-7638-8836
   Halder, Pobitra/0000-0002-7638-8836},
Unique-ID = {WOS:000328234000035},
}

@article{ WOS:000326258100001,
Author = {Kitchenham, Barbara and Brereton, Pearl},
Title = {A systematic review of systematic review process research in software
   engineering},
Journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
Year = {2013},
Volume = {55},
Number = {12},
Pages = {2049-2075},
Month = {DEC},
Abstract = {Context: Many researchers adopting systematic reviews (SRs) have also
   published papers discussing problems with the SR methodology and
   suggestions for improving it. Since guidelines for SRs in software
   engineering (SE) were last updated in 2007, we believe it is time to
   investigate whether the guidelines need to be amended in the light of
   recent research.
   Objective: To identify, evaluate and synthesize research published by
   software engineering researchers concerning their experiences of
   performing SRs and their proposals for improving the SR process.
   Method: We undertook a systematic review of papers reporting experiences
   of undertaking SRs and/or discussing techniques that could be used to
   improve the SR process. Studies were classified with respect to the
   stage in the SR process they addressed, whether they related to
   education or problems faced by novices and whether they proposed the use
   of textual analysis tools.
   Results: We identified 68 papers reporting 63 unique studies published
   in SE conferences and journals between 2005 and mid-2012. The most
   common criticisms of SRs were that they take a long time, that SE
   digital libraries are not appropriate for broad literature searches and
   that assessing the quality of empirical studies of different types is
   difficult.
   Conclusion: We recommend removing advice to use structured questions to
   construct search strings and including advice to use a quasi-gold
   standard based on a limited manual search to assist the construction of
   search stings and evaluation of the search process. Textual analysis
   tools are likely to be useful for inclusion/exclusion decisions and
   search string construction but require more stringent evaluation. SE
   researchers would benefit from tools to manage the SR process but
   existing tools need independent validation. Quality assessment of
   studies using a variety of empirical methods remains a major problem.
   (C) 2013 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.infsof.2013.07.010},
ISSN = {0950-5849},
EISSN = {1873-6025},
ResearcherID-Numbers = {Kitchenham, Barbara/AAL-4311-2020},
ORCID-Numbers = {Kitchenham, Barbara/0000-0002-6134-8460},
Unique-ID = {WOS:000326258100001},
}

@article{ WOS:000330219800032,
Author = {Novikova, Irina V. and Hennelly, Scott P. and Sanbonmatsu, Karissa Y.},
Title = {Tackling Structures of Long Noncoding RNAs},
Journal = {INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES},
Year = {2013},
Volume = {14},
Number = {12},
Pages = {23672-23684},
Month = {DEC},
Abstract = {RNAs are important catalytic machines and regulators at every level of
   gene expression. A new class of RNAs has emerged called long non-coding
   RNAs, providing new insights into evolution, development and disease.
   Long non-coding RNAs (lncRNAs) predominantly found in higher eukaryotes,
   have been implicated in the regulation of transcription factors,
   chromatin-remodeling, hormone receptors and many other processes. The
   structural versatility of RNA allows it to perform various functions,
   ranging from precise protein recognition to catalysis and metabolite
   sensing. While major housekeeping RNA molecules have long been the focus
   of structural studies, lncRNAs remain the least characterized class,
   both structurally and functionally. Here, we review common methodologies
   used to tackle RNA structure, emphasizing their potential application to
   lncRNAs. When considering the complexity of lncRNAs and lack of
   knowledge of their structure, chemical probing appears to be an
   indispensable tool, with few restrictions in terms of size, quantity and
   heterogeneity of the RNA molecule. Probing is not constrained to in
   vitro analysis and can be adapted to high-throughput sequencing
   platforms. Significant efforts have been applied to develop new in vivo
   chemical probing reagents, new library construction protocols for
   sequencing platforms and improved RNA prediction software based on the
   experimental evidence.},
DOI = {10.3390/ijms141223672},
ISSN = {1422-0067},
Unique-ID = {WOS:000330219800032},
}

@article{ WOS:000327068800007,
Author = {Osinowo, Olakunle Olawale and Olayinka, Abel Idowu},
Title = {Aeromagnetic mapping of basement topography around the Ijebu-Ode
   geological transition zone, Southwestern Nigeria},
Journal = {ACTA GEODAETICA ET GEOPHYSICA},
Year = {2013},
Volume = {48},
Number = {4},
Pages = {451-470},
Month = {DEC},
Abstract = {Ijebu-Ode and its environs in southwestern Nigeria is located on a
   geological transition zone from the Precambrian migmatite gneiss rocks
   and Cretaceous sedimentary rock units of Abeokuta Group. Unique
   geological structures, complex coexistence of different rock types and
   poorly defined basal/lateral contacts between the basement and
   sedimentary rocks typify tectonic events that accompany crustal thinning
   and downwarping at the basement/sedimentary-basin edge. This posed
   serious geological decision challenges especially since it is difficult
   to characterize the area either as sedimentary or crystalline.
   Aeromagnetic data were collected, the data were filtered, inverted and
   enhanced using appropriate software packages and subsequently employed
   to generate model of the subsurface basement topography.
   The magnetic intensity distribution in the field ranged from -120 to +80
   nT and was found to depend on the size, depth of burial and the
   thickness of low susceptibility superficial material overlying the
   magnetite rich crystalline rocks. High magnetic intensity characterized
   the basement rocks while lower values distinguished the metasediments
   rocks and sedimentary terrain. The average radial power analysis
   delineated shallow and relatively deep sources as the two distinct types
   of magnetic anomalies. Euler deconvolution results referenced to the
   Minna, Nigeria datum indicate depth range from -80 to +30 m for sources
   located below and above the mean ground level, respectively. Euler
   result also revealed that the depth to magnetic sources is heterogeneous
   and highly erratic around the transition zone, ranging between -25 and
   +12 m.
   This study identified rugged and undulating basement topography around
   the contact zone; it also delineated the lateral and basal contacts
   between the two coexisting rock types. Aeromagnetic mapping of basement
   sedimentary transition zone around Ijebu-Ode has proved very efficient
   in providing relevant information about the nature of the basement
   topography which provided vital information requisite for drilling and
   some engineering constructions decisions.},
DOI = {10.1007/s40328-013-0032-6},
ISSN = {2213-5812},
EISSN = {2213-5820},
ResearcherID-Numbers = {Osinowo, Olawale/AAX-7395-2021},
ORCID-Numbers = {Osinowo, Olawale/0000-0002-0436-3461},
Unique-ID = {WOS:000327068800007},
}

@article{ WOS:000325387500009,
Author = {Chong, Chun Yong and Lee, Sai Peck and Ling, Teck Chaw},
Title = {Efficient software clustering technique using an adaptive and preventive
   dendrogram cutting approach},
Journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
Year = {2013},
Volume = {55},
Number = {11},
Pages = {1994-2012},
Month = {NOV},
Abstract = {Context: Software clustering is a key technique that is used in reverse
   engineering to recover a high-level abstraction of the software in the
   case of limited resources. Very limited research has explicitly
   discussed the problem of finding the optimum set of clusters in the
   design and how to penalize for the formation of singleton clusters
   during clustering.
   Objective: This paper attempts to enhance the existing agglomerative
   clustering algorithms by introducing a complementary mechanism. To solve
   the architecture recovery problem, the proposed approach focuses on
   minimizing redundant effort and penalizing for the formation of
   singleton clusters during clustering while maintaining the integrity of
   the results.
   Method: An automated solution for cutting a dendrogram that is based on
   least-squares regression is presented in order to find the best cut
   level. A dendrogram is a tree diagram that shows the taxonomic
   relationships of clusters of software entities. Moreover, a factor to
   penalize clusters that will form singletons is introduced in this paper.
   Simulations were performed on two open-source projects. The proposed
   approach was compared against the exhaustive and highest gap dendrogram
   cutting methods, as well as two well-known cluster validity indices,
   namely, Dunn's index and the Davies-Bouldin index.
   Results: When comparing our clustering results against the original
   package diagram, our approach achieved an average accuracy rate of
   90.07\% from two simulations after the utility classes were removed. The
   utility classes in the source code affect the accuracy of the software
   clustering, owing to its omnipresent behavior. The proposed approach
   also successfully penalized the formation of singleton clusters during
   clustering.
   Conclusion: The evaluation indicates that the proposed approach can
   enhance the quality of the clustering results by guiding software
   maintainers through the cutting point selection process. The proposed
   approach can be used as a complementary mechanism to improve the
   effectiveness of existing clustering algorithms. (C) 2013 Elsevier B.V.
   All rights reserved.},
DOI = {10.1016/j.infsof.2013.07.002},
ISSN = {0950-5849},
EISSN = {1873-6025},
ResearcherID-Numbers = {Lee, Sai Peck/B-8841-2010
   LING, TECK CHAW/B-8873-2010
   Chong, Chun Yong/G-5067-2015},
ORCID-Numbers = {Lee, Sai Peck/0000-0002-4551-430X
   LING, TECK CHAW/0000-0002-1225-7941
   Chong, Chun Yong/0000-0003-1164-0049},
Unique-ID = {WOS:000325387500009},
}

@article{ WOS:000324018400008,
Author = {Epifanovsky, Evgeny and Wormit, Michael and Kus, Tomasz and Landau, Arie
   and Zuev, Dmitry and Khistyaev, Kirill and Manohar, Prashant and
   Kaliman, Ilya and Dreuw, Andreas and Krylov, Anna I.},
Title = {New implementation of high-level correlated methods using a general
   block tensor library for high-performance electronic structure
   calculations},
Journal = {JOURNAL OF COMPUTATIONAL CHEMISTRY},
Year = {2013},
Volume = {34},
Number = {26},
Pages = {2293-2309},
Month = {OCT 5},
Abstract = {This article presents an open-source object-oriented C++ library of
   classes and routines to perform tensor algebra. The primary purpose of
   the library is to enable post-Hartree-Fock electronic structure methods;
   however, the code is general enough to be applicable in other areas of
   physical and computational sciences. The library supports tensors of
   arbitrary order (dimensionality), size, and symmetry. Implemented data
   structures and algorithms operate on large tensors by splitting them
   into smaller blocks, storing them both in core memory and in files on
   disk, and applying divide-and-conquer-type parallel algorithms to
   perform tensor algebra. The library offers a set of general tensor
   symmetry algorithms and a full implementation of tensor symmetries
   typically found in electronic structure theory: permutational, spin, and
   molecular point group symmetry. The Q-Chem electronic structure software
   uses this library to drive coupled-cluster, equation-of-motion, and
   algebraic-diagrammatic construction methods. (c) 2013 Wiley Periodicals,
   Inc.},
DOI = {10.1002/jcc.23377},
ISSN = {0192-8651},
EISSN = {1096-987X},
ResearcherID-Numbers = {Manohar, Prashant/ABB-2566-2021
   },
ORCID-Numbers = {Krylov, Anna/0000-0001-6788-5016},
Unique-ID = {WOS:000324018400008},
}

@article{ WOS:000323298000005,
Author = {Daneshgar, Farhad and Low, Graham C. and Worasinchai, Lugkana},
Title = {An investigation of `build vs. buy' decision for software acquisition by
   small to medium enterprises},
Journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
Year = {2013},
Volume = {55},
Number = {10},
Pages = {1741-1750},
Month = {OCT},
Abstract = {Context: The prevalence of computing and communication technologies,
   combined with the availability of sophisticated and highly specialised
   software packages from software vendors has made package acquisition a
   viable option for many organisations. While some research has addressed
   the factors that influence the selection of the software acquisition
   method in large organisations, little is known about the factors
   affecting SMEs.
   Objective: To provide an understanding of factors that affect the
   decision process of software acquisition for SMEs. It is expected that
   results from this study: (i) will assist the SME decision process for
   software acquisition, and (ii) will assist policy makers in terms of
   developing appropriate guidelines for SME software acquisition.
   Method: A positivist research perspective has been adopted involving
   semi-structured interviews in eight SMEs in Thailand with the
   interviewees assigning to each of the potential factors.
   Results: The study found that the following factors affect both SMEs and
   large organisations: requirements fit, cost, scale and complexity,
   commoditization/flexibility, time, in-house experts, support structure,
   and operational factors. Factors mainly applying to large organisations
   were strategic role of the software, intellectual property concerns, and
   risk, Factors particularly relevant to SMEs (ubiquitous systems,
   availability of free download, and customizable to specific
   government/tax regulations).
   Conclusion: The results suggest that: (i) when deciding on their
   software acquisition method, SMEs are generally less likely to pursue a
   long-term vision compared with larger organisations, possibly because
   SMEs mainly serve their local markets; and (ii) contrary to the large
   organisations, the role that the IT plays in SMEs may not be as vital to
   the SMEs' core business processes, to their supply chains, and/or to the
   management of their customer relationship. Furthermore, neither the
   level of technological intensity nor size of the SME appears to affect
   the ranks given by the interviewees for the various factors. (C) 2013
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.infsof.2013.03.009},
ISSN = {0950-5849},
Unique-ID = {WOS:000323298000005},
}

@article{ WOS:000318059400011,
Author = {Delosevic, M. and Tepic, J. and Doroslovacki, R. and Gajic, V. and
   Tanackov, I.},
Title = {MULTIDISCIPLINARY ANALYSIS OF STEEL PLATE OF VARIABLE THICKNESS IN VIEW
   OF OPTIMAL DESIGN},
Journal = {METALURGIJA},
Year = {2013},
Volume = {52},
Number = {4},
Pages = {477-480},
Month = {OCT-DEC},
Abstract = {The paper shows the indentification of deformational-stress state of
   partially loaded plates composed of two elements of different
   thicknesses. We analyzed metallurgical processes that characterize the
   technology of welding steel structures. The mathematical interpretation
   of the local stress state, through the developed software, enables an
   optimal design of geometric parameters of plate or supporting elements
   of construction according to the stress criteria. Comparative analysis
   of deflection and equivalent stress obtained by the analytical method
   and Finite elements method (FEM), using ANSYS 12 software package, the
   high agreement of results in terms of values and distribution trends is
   noticed. The application of the results of this paper is particularly
   important for the optimal design of steel girders.},
ISSN = {0543-5846},
EISSN = {1334-2576},
ResearcherID-Numbers = {Tanackov, Ilija/AAH-8043-2020},
Unique-ID = {WOS:000318059400011},
}

@article{ WOS:000323283100009,
Author = {Aydee Sanchez-Bojorge, Nora and Maria Rodriguez-Valdez, Luz and
   Flores-Holguin, Norma},
Title = {DFT calculation of the electronic properties of
   fluorene-1,3,4-thiadiazole oligomers},
Journal = {JOURNAL OF MOLECULAR MODELING},
Year = {2013},
Volume = {19},
Number = {9},
Pages = {3537-3542},
Month = {SEP},
Abstract = {Thiadiazole derivatives have been widely employed in the areas of
   pharmaceutical, agricultural, industrial, and polymer chemistry. The
   electronic and molecular structures of thiadiazoles are of interest
   because they have an equal number of valence electrons and similar
   molecular structures to thiophenes, which are currently used in the
   construction of organic solar cells due to their relatively high hole
   mobilities and good light-harvesting properties. For this reason, the
   electronic properties of fluorene-1,3,4-thiadiazole oligomers warrant
   investigation. In the present work, the structure of
   fluorene-1,3,4-thiadiazole with one thiadiazole unit in the structure
   was analyzed. This molecule was then expanded until there were 10
   thiadiazole units in the structure. The band gap, HOMO and LUMO
   distributions, and absorption spectrum were analyzed for each molecule.
   All calculations were performed by applying the B3LYP/6-31G(d) chemical
   model in the Gaussian 03W and GaussView software packages. The
   electronic properties were observed to significantly enhance as the
   number of monomeric units increased, which also caused the gap energy to
   decrease from 3.51 eV in the oligomer with just one thiadiazole ring to
   2.33 eV in the oligomer with 10 units. The HOMO and LUMO regions were
   well defined and separated for oligomers with at least 5 monomer units
   of thiadiazole.},
DOI = {10.1007/s00894-013-1878-9},
ISSN = {1610-2940},
EISSN = {0948-5023},
ResearcherID-Numbers = {Glossman-Mitnik, Daniel/I-5742-2013
   Sánchez-Bojorge, Nora-Aydeé/AFF-7917-2022
   RODRIGUEZ, LUZ/AAR-3531-2021
   },
ORCID-Numbers = {Glossman-Mitnik, Daniel/0000-0002-9583-4256
   RODRIGUEZ-VALDEZ, LUZ MARIA/0000-0002-4954-2631
   Flores-Holguin, Norma/0000-0002-4836-233X
   Sanchez-Bojorge, Nora-Aydee/0000-0002-0034-1214},
Unique-ID = {WOS:000323283100009},
}

@article{ WOS:000323500300012,
Author = {Chelidze, T. and Matcharashvili, T. and Abashidze, V.},
Title = {Application of new nonlinear elasticity and nonlinear dynamics tools in
   real time monitoring of large engineering constructions (case of high
   arc Enguri dam)},
Journal = {DISASTER ADVANCES},
Year = {2013},
Volume = {6},
Number = {9},
Pages = {84-89},
Month = {SEP},
Abstract = {Large engineering constructions (dams, bridges, high towers etc.) are
   complex structures with nonlinear dynamic behavior. Engineers often are
   forced to assess their safety based on the available incomplete data
   which is extremely difficult. This important problem can be solved with
   the modern theory of complex systems. It is possible to derive
   characteristics of the whole unknown dynamics of a structure using few
   data sets of certain carefully selected representative parameter(s).
   We created a cost-effective Monitoring Telemetric System for Dam
   Diagnostics (DAMWATCH) which consists of sensors (tiltmeters), terminal
   and central controllers connected by the GSM/GPRS Modem to the
   diagnostic center. The tilt data recorded for varying reservoir level
   are compared with static design model of dam deformations computed by a
   finite element method (FEM) for the dam-reservoir-foundation system.
   Besides, recently developed linear/nonlinear data analysis and
   prediction schemes may help to quantify fine dynamical features of the
   dam behavior using monitoring time series. The software package DAMTOOL
   has been developed for this purpose. The differences between measured
   and theoretically predicted response parameters of the dam may signal
   abnormal behavior of the object. The data obtained already by testing of
   the DAMWATCH/DAMTOOL system during operation of the high Enguri arc dam
   and its reservoir (Georgia) show interesting long-term and short-term
   patterns of tilts in the dam body which can be used for diagnostics.
   Of course main principles of both the real-time monitoring system and
   analysis approach can be applied to any large engineering construction.
   Thus, in following we will use the word ``dam{''} as a collective term
   for large engineering constructions.
   The 271 m high Enguri arch dam, still the highest (in its class) dam in
   the world, was built in the canyon of Enguri river (West Georgia) in the
   1970s. The high seismic and geodynamical activities together with a high
   population density of the adjoining region made the Enguri dam a
   potential source of a major technological catastrophe in Georgia. That
   is why in 1996 the European Centre ``Geodynamical Hazards of High
   Dams{''} with the Enguri Dam International Test Area (EDITA) was
   organized in Georgia by the Council of Europe. From the very beginning
   in EDITA the unique geotechnical, geodynamical and geophysical
   monitoring system was functioning(1). The database of observations
   (tilts, strains etc) contains information accumulated during more than
   30 years.},
ISSN = {0974-262X},
EISSN = {2278-4543},
Unique-ID = {WOS:000323500300012},
}

@article{ WOS:000323850000001,
Author = {Hofstede, Stefanie N. and Marang-van de Mheen, Perla J. and Wentink,
   Manon M. and Stiggelbout, Anne M. and Vleggeert-Lankamp, Carmen L. A.
   and Vlieland, Thea P. M. Vliet and van Bodegom-Vos, Leti and DISC Study
   Grp},
Title = {Barriers and facilitators to implement shared decision making in
   multidisciplinary sciatica care: a qualitative study},
Journal = {IMPLEMENTATION SCIENCE},
Year = {2013},
Volume = {8},
Month = {AUG 23},
Abstract = {Background: The Dutch multidisciplinary sciatica guideline recommends
   that the team of professionals involved in sciatica care and the patient
   together decide on surgical or prolonged conservative treatment (shared
   decision making {[}SDM]). Despite this recommendation, SDM is not yet
   integrated in sciatica care. Existing literature concerning barriers and
   facilitators to SDM implementation mainly focuses on one discipline
   only, whereas multidisciplinary care may involve other barriers and
   facilitators, or make these more complex for both professionals and
   patients. Therefore, this qualitative study aims to identify barriers
   and facilitators perceived by patients and professionals for SDM
   implementation in multidisciplinary sciatica care.
   Methods: We conducted 40 semi-structured interviews with professionals
   involved in sciatica care (general practitioners, physical therapists,
   neurologists, neurosurgeons, and orthopedic surgeons) and three focus
   groups among patients (six to eight per group). The interviews and focus
   groups were audiotaped and transcribed in full. Reported barriers and
   facilitators were classified according to the framework of Grol and
   Wensing. The software package Atlas.ti 7.0 was used for analysis.
   Results: Professionals reported 53 barriers and 5 facilitators, and
   patients 35 barriers and 18 facilitators for SDM in sciatica care.
   Professionals perceived most barriers at the level of the organizational
   context, and facilitators at the level of the individual professional.
   Patients reported most barriers and facilitators at the level of the
   individual professional. Several barriers and facilitators correspond
   with barriers and facilitators found in the literature (e. g., lack of
   time, motivation) but also new barriers and facilitators were
   identified. Many of these new barriers mentioned by both professionals
   and patients were related to the multidisciplinary setting, such as lack
   of visibility, lack of trust in expertise of other disciplines, and lack
   of communication between disciplines.
   Conclusions: This study identified barriers and facilitators for SDM in
   the multidisciplinary sciatica setting, by both professionals and
   patients. It is clear that more barriers than facilitators are perceived
   for implementation of SDM in sciatica care. Newly identified barriers
   and facilitators are related to the multidisciplinary care setting.
   Therefore, an effective implementation strategy of SDM in a
   multidisciplinary setting such as in sciatica care should focus on these
   barriers and facilitators.},
DOI = {10.1186/1748-5908-8-95},
Article-Number = {95},
ISSN = {1748-5908},
ResearcherID-Numbers = {van Bodegom-Vos, Leti/J-8087-2015
   assendelft, willem/AAW-1854-2021
   Vlieland, Thea Vliet/AAJ-8119-2020
   Stiggelbout, Anne M/D-2293-2018
   Assendelft, W.J.J./H-8008-2014
   de Mheen, Perla J. Marang-van/I-2783-2015
   Vleggeert-Lankamp, Carmen/AGX-9660-2022
   },
ORCID-Numbers = {van Bodegom-Vos, Leti/0000-0002-8486-6404
   assendelft, willem/0000-0002-2966-3778
   Vlieland, Thea Vliet/0000-0001-6322-3859
   Stiggelbout, Anne M/0000-0002-6293-4509
   Assendelft, W.J.J./0000-0002-2966-3778
   de Mheen, Perla J. Marang-van/0000-0003-1439-0989
   Vleggeert-Lankamp, Carmen/0000-0001-9597-7225
   Hofstede, Stefanie/0000-0002-7181-9959
   van den Hout, Wilbert B./0000-0002-6425-0135},
Unique-ID = {WOS:000323850000001},
}

@article{ WOS:000321889500007,
Author = {Fernandes, P. and O'Kelly, M. E. J. and Papadopoulos, C. T. and Sales,
   A.},
Title = {Analysis of exponential reliable production lines using Kronecker
   descriptors},
Journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
Year = {2013},
Volume = {51},
Number = {14},
Pages = {4240-4257},
Month = {JUL 1},
Abstract = {This paper presents a solution procedure for reliable production lines
   with service times distributed according to an exponential distribution,
   based on a Markovian formulation with a Kronecker structured
   representation (sum of tensor products). Specifically, structured
   Markovian formalisms are used to reduce the impact of the well-known
   state explosion problem associated with other methods of solution. Such
   formalisms combined with the Kronecker representation deliver memory
   efficiency in storing very large models, i.e. models with more than
   states. The exact steady-state solutions of these models may be obtained
   using efficient existing software packages. The proposed solution
   procedure is illustrated with two detailed examples, and generalised
   with a model construction algorithm. The computed throughput for several
   examples of production lines with perfectly reliable machines, as well
   as the computational costs in terms of CPU time to solve them with
   PEPS2007 and GTAexpress software packages, are also presented. In effect
   the paper demonstrates the power of the use of the Kronecker descriptor
   analysis applied to the derivation of the exact solution of the
   particular class of production lines considered. The Kronecker
   descriptor methodology is well-known to analysts concerned with computer
   and communication systems.},
DOI = {10.1080/00207543.2012.754550},
ISSN = {0020-7543},
EISSN = {1366-588X},
ResearcherID-Numbers = {Papadopoulos, Chrissoleon/ABD-5274-2021
   Fernandes, Paulo/J-6935-2013
   Fernandes, Paulo/AAS-4213-2021},
ORCID-Numbers = {Papadopoulos, Chrissoleon/0000-0002-6721-5175
   Fernandes, Paulo/0000-0003-3959-0434
   Fernandes, Paulo/0000-0003-3959-0434},
Unique-ID = {WOS:000321889500007},
}

@article{ WOS:000321325700030,
Author = {Sarrion-Perdigones, Alejandro and Vazquez-Vilar, Marta and Palaci, Jorge
   and Castelijns, Bas and Forment, Javier and Ziarsolo, Peio and Blanca,
   Jose and Granell, Antonio and Orzaez, Diego},
Title = {GoldenBraid 2.0: A Comprehensive DNA Assembly Framework for Plant
   Synthetic Biology},
Journal = {PLANT PHYSIOLOGY},
Year = {2013},
Volume = {162},
Number = {3},
Pages = {1618-1631},
Month = {JUL},
Abstract = {Plant synthetic biology aims to apply engineering principles to plant
   genetic design. One strategic requirement of plant synthetic biology is
   the adoption of common standardized technologies that facilitate the
   construction of increasingly complex multigene structures at the DNA
   level while enabling the exchange of genetic building blocks among plant
   bioengineers. Here, we describe GoldenBraid 2.0 (GB2.0), a comprehensive
   technological framework that aims to foster the exchange of standard DNA
   parts for plant synthetic biology. GB2.0 relies on the use of type IIS
   restriction enzymes for DNA assembly and proposes a modular cloning
   schema with positional notation that resembles the grammar of natural
   languages. Apart from providing an optimized cloning strategy that
   generates fully exchangeable genetic elements for multigene engineering,
   the GB2.0 toolkit offers an ever-growing open collection of DNA parts,
   including a group of functionally tested, premade genetic modules to
   build frequently used modules like constitutive and inducible expression
   cassettes, endogenous gene silencing and protein-protein interaction
   tools, etc. Use of the GB2.0 framework is facilitated by a number of Web
   resources that include a publicly available database, tutorials, and a
   software package that provides in silico simulations and laboratory
   protocols for GB2.0 part domestication and multigene engineering. In
   short, GB2.0 provides a framework to exchange both information and
   physical DNA elements among bioengineers to help implement plant
   synthetic biology projects.},
DOI = {10.1104/pp.113.217661},
ISSN = {0032-0889},
EISSN = {1532-2548},
ResearcherID-Numbers = {Vilar, Marta Vázquez/AAS-7127-2020
   ORZAEZ, DIEGO/X-2915-2019
   Sarrion-Perdigones, Alejandro/H-6750-2019
   Orzaez, Diego/H-3457-2012
   Granell, Antonio/G-3664-2010
   Ziarsolo, Pello/H-6699-2015
   Blanca, Jose M/H-6695-2015
   },
ORCID-Numbers = {ORZAEZ, DIEGO/0000-0003-1662-5403
   Sarrion-Perdigones, Alejandro/0000-0002-4584-4827
   Orzaez, Diego/0000-0003-1662-5403
   Ziarsolo, Pello/0000-0002-9660-1286
   Blanca, Jose M/0000-0002-5884-8624
   Palaci, Jorge/0000-0002-7676-7676
   Vazquez-Vilar, Marta/0000-0001-7175-7818},
Unique-ID = {WOS:000321325700030},
}

@article{ WOS:000321042700001,
Author = {Konwar, Kishori M. and Hanson, Niels W. and Page, Antoine P. and Hallam,
   Steven J.},
Title = {MetaPathways: a modular pipeline for constructing pathway/genome
   databases from environmental sequence information},
Journal = {BMC BIOINFORMATICS},
Year = {2013},
Volume = {14},
Month = {JUN 21},
Abstract = {Background: A central challenge to understanding the ecological and
   biogeochemical roles of microorganisms in natural and human engineered
   ecosystems is the reconstruction of metabolic interaction networks from
   environmental sequence information. The dominant paradigm in metabolic
   reconstruction is to assign functional annotations using BLAST.
   Functional annotations are then projected onto symbolic representations
   of metabolism in the form of KEGG pathways or SEED subsystems.
   Results: Here we present MetaPathways, an open source pipeline for
   pathway inference that uses the PathoLogic algorithm to map functional
   annotations onto the MetaCyc collection of reactions and pathways, and
   construct environmental Pathway/Genome Databases (ePGDBs) compatible
   with the editing and navigation features of Pathway Tools. The pipeline
   accepts assembled or unassembled nucleotide sequences, performs quality
   assessment and control, predicts and annotates noncoding genes and open
   reading frames, and produces inputs to PathoLogic. In addition to
   constructing ePGDBs, MetaPathways uses MLTreeMap to build phylogenetic
   trees for selected taxonomic anchor and functional gene markers,
   converts General Feature Format (GFF) files into concatenated GenBank
   files for ePGDB construction based on third-party annotations, and
   generates useful file formats including Sequin files for direct GenBank
   submission and gene feature tables summarizing annotations, MLTreeMap
   trees, and ePGDB pathway coverage summaries for statistical comparisons.
   Conclusions: MetaPathways provides users with a modular annotation and
   analysis pipeline for predicting metabolic interaction networks from
   environmental sequence information using an alternative to KEGG pathways
   and SEED subsystems mapping. It is extensible to genomic and
   transcriptomic datasets from a wide range of sequencing platforms, and
   generates useful data products for microbial community structure and
   function analysis. The MetaPathways software package, installation
   instructions, and example data can be obtained from
   http://hallam.microbiology.ubc.ca/MetaPathways.},
DOI = {10.1186/1471-2105-14-202},
Article-Number = {202},
ISSN = {1471-2105},
ORCID-Numbers = {Hanson, Niels William/0000-0003-3081-0736},
Unique-ID = {WOS:000321042700001},
}

@article{ WOS:000315933100014,
Author = {Riveiro, B. and Solla, M. and de Arteaga, I. and Arias, P. and Morer, P.},
Title = {A novel approach to evaluate masonry arch stability on the basis of
   limit analysis theory and non-destructive geometric characterization},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2013},
Volume = {31},
Pages = {140-148},
Month = {MAY},
Abstract = {Knowledge of the functional and conservation state of a structure is a
   fundamental aspect in order to achieve its maintenance and preservation.
   Having adequate techniques for reaching this purpose is one of the most
   important aspects for professionals working about built-up structures.
   Geometry usually plays an important role in the diagnosis of these
   structures, and for masonry arches particularly. The most common
   software packages focused on the stability analysis of masonry arches
   use rigid blocks assuming masonry as plastic material into the context
   of Limit Analysis Theory.
   This paper presents the first results of a novel methodology for the
   analysis of arch bridge stability based on the construction of integral
   3D models of entire vaults. This geometric reconstruction is achieved
   thanks to the employment of non-destructive techniques such as
   photogrammetry and Ground Penetrating Radar. Then, stability of vaults
   is evaluated through a tool specifically developed using Matlab
   software. (C) 2012 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.autcon.2012.11.035},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {, Arias/AAL-9383-2020
   de Arteaga Jordá, Ignacio/P-8701-2015
   Arias, Pedro/I-4949-2015
   Solla, Mercedes/S-8020-2019
   morer-camo, paz/GVS-7058-2022
   morer-camo, paz/AAU-7425-2021
   Riveiro, Belen/L-6215-2014},
ORCID-Numbers = {, Arias/0000-0002-3547-8907
   de Arteaga Jordá, Ignacio/0000-0003-4315-6008
   Arias, Pedro/0000-0002-3547-8907
   Solla, Mercedes/0000-0003-1042-4985
   morer-camo, paz/0000-0002-8430-9501
   Riveiro, Belen/0000-0002-1497-4370},
Unique-ID = {WOS:000315933100014},
}

@article{ WOS:000318109300004,
Author = {Sievers, Fabian and Dineen, David and Wilm, Andreas and Higgins, Desmond
   G.},
Title = {Making automated multiple alignments of very large numbers of protein
   sequences},
Journal = {BIOINFORMATICS},
Year = {2013},
Volume = {29},
Number = {8},
Pages = {989-995},
Month = {APR 15},
Abstract = {Motivation: Recent developments in sequence alignment software have made
   possible multiple sequence alignments (MSAs) of 4100 000 sequences in
   reasonable times. At present, there are no systematic analyses
   concerning the scalability of the alignment quality as the number of
   aligned sequences is increased.
   Results: We benchmarked a wide range of widely used MSA packages using a
   selection of protein families with some known structures and found that
   the accuracy of such alignments decreases markedly as the number of
   sequences grows. This is more or less true of all packages and protein
   families. The phenomenon is mostly due to the accumulation of alignment
   errors, rather than problems in guide-tree construction. This is partly
   alleviated by using iterative refinement or selectively adding
   sequences. The average accuracy of progressive methods by comparison
   with structure-based benchmarks can be improved by incorporating
   information derived from high-quality structural alignments of sequences
   with solved structures. This suggests that the availability of high
   quality curated alignments will have to complement algorithmic and/or
   software developments in the long-term.},
DOI = {10.1093/bioinformatics/btt093},
ISSN = {1367-4803},
EISSN = {1460-2059},
ResearcherID-Numbers = {Sievers, Fabian/AAG-2021-2020
   },
ORCID-Numbers = {Sievers, Fabian/0000-0002-7885-6867
   Wilm, Andreas/0000-0002-7344-3764
   Higgins, Des/0000-0002-3952-3285},
Unique-ID = {WOS:000318109300004},
}

@article{ WOS:000316978800007,
Author = {Katsanos, Evangelos I. and Sextos, Anastasios G.},
Title = {ISSARS: An integrated software environment for structure-specific
   earthquake ground motion selection},
Journal = {ADVANCES IN ENGINEERING SOFTWARE},
Year = {2013},
Volume = {58},
Pages = {70-85},
Month = {APR},
Abstract = {Current practice enables the design and assessment of structures in
   earthquake prone areas by performing time history analysis with the use
   of appropriately selected strong ground motions. This study presents a
   Matlab-based software environment, which is integrated with a finite
   element analysis package, and aims to improve the efficiency of
   earthquake ground motion selection by accounting for the variability of
   critical structural response quantities. This additional selection
   criterion, which is tailored to the specific structure studied, leads to
   more reliable estimates of the mean structural response quantities used
   in design, while fulfils the criteria already prescribed by the European
   and US seismic codes and guidelines. To demonstrate the applicability of
   the software environment developed, an existing irregular, multi-storey,
   reinforced concrete building is studied for a wide range of seismic
   scenarios. The results highlight the applicability of the software
   developed and the benefits of applying a structure-specific criterion in
   the process of selecting suites of earthquake motions for the seismic
   design and assessment. (C) 2013 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.advengsoft.2013.01.003},
ISSN = {0965-9978},
EISSN = {1873-5339},
ResearcherID-Numbers = {Sextos, Anastasios/AAS-7823-2021
   },
ORCID-Numbers = {Sextos, Anastasios/0000-0002-2616-9395
   Katsanos, Evangelos/0000-0003-2574-5327},
Unique-ID = {WOS:000316978800007},
}

@article{ WOS:000317989000004,
Author = {Lindsey, Wesley T. and Olin, Bernie R.},
Title = {PubMed Searches: Overview and Strategies for Clinicians},
Journal = {NUTRITION IN CLINICAL PRACTICE},
Year = {2013},
Volume = {28},
Number = {2},
Pages = {165-176},
Month = {APR},
Abstract = {PubMed is a biomedical and life sciences database maintained by a
   division of the National Library of Medicine known as the National
   Center for Biotechnology Information (NCBI). It is a large resource with
   more than 5600 journals indexed and greater than 22 million total
   citations. Searches conducted in PubMed provide references that are more
   specific for the intended topic compared with other popular search
   engines. Effective PubMed searches allow the clinician to remain current
   on the latest clinical trials, systematic reviews, and practice
   guidelines. PubMed continues to evolve by allowing users to create a
   customized experience through the My NCBI portal, new arrangements and
   options in search filters, and supporting scholarly projects through
   exportation of citations to reference managing software. Prepackaged
   search options available in the Clinical Queries feature also allow
   users to efficiently search for clinical literature. PubMed also
   provides information regarding the source journals themselves through
   the Journals in NCBI Databases link. This article provides an overview
   of the PubMed database's structure and features as well as strategies
   for conducting an effective search. (Nutr Clin Pract. 2013; 28: 165-176)},
DOI = {10.1177/0884533613475821},
ISSN = {0884-5336},
Unique-ID = {WOS:000317989000004},
}

@article{ WOS:000315667500006,
Author = {Gallo, I. B. and Zanatta, A. R.},
Title = {A simple-versatile approach to achieve all-Si-based optical
   micro-cavities},
Journal = {JOURNAL OF APPLIED PHYSICS},
Year = {2013},
Volume = {113},
Number = {8},
Month = {FEB 28},
Abstract = {At present, solid thin films are recognized by their well established
   and mature processing technology that is able to produce components
   which, depending on their main characteristics, can perform either
   passive or active functions. Additionally, Si-based materials in the
   form of thin films perfectly match the concept of miniaturized and
   low-consumption devices-as required in various modern technological
   applications. Part of these aspects was considered in the present work
   that was concerned with the study of optical micro-cavities entirely
   based on silicon and silicon nitride thin films. The structures were
   prepared by the sputtering deposition method which, due to the adopted
   conditions (atmosphere and deposition rate) and arrangement of layers,
   provided cavities operating either in the visible (at similar to 670 nm)
   or in the near-infrared (at similar to 1560 nm) wavelength ranges. The
   main differential of the work relies on the construction of optical
   microcavities with a reduced number of periods whose main properties can
   be changed by thermal annealing treatments. The work also discusses the
   angle-dependent behavior of the optical transmission profiles as well as
   the use of the COMSOL software package to simulate the microcavities.
   (C) 2013 American Institute of Physics.
   {[}http://dx.doi.org/10.1063/1.4793592]},
DOI = {10.1063/1.4793592},
Article-Number = {083106},
ISSN = {0021-8979},
EISSN = {1089-7550},
ResearcherID-Numbers = {Gallo, Ivan/E-8407-2014
   Zanatta, Antonio R/C-1878-2012},
ORCID-Numbers = {Zanatta, Antonio R/0000-0002-5217-7524},
Unique-ID = {WOS:000315667500006},
}

@article{ WOS:000314881600021,
Author = {Dlugosch, Katrina M. and Lai, Zhao and Bonin, Aurelie and Hierro, Jose
   and Rieseberg, Loren H.},
Title = {Allele Identification for Transcriptome-Based Population Genomics in the
   Invasive Plant Centaurea solstitialis},
Journal = {G3-GENES GENOMES GENETICS},
Year = {2013},
Volume = {3},
Number = {2},
Pages = {359-367},
Month = {FEB 1},
Abstract = {Transcriptome sequences are becoming more broadly available for multiple
   individuals of the same species, providing opportunities to derive
   population genomic information from these datasets. Using the 454 Life
   Science Genome Sequencer FLX and FLX-Titanium next-generation platforms,
   we generated 112430 Mbp of sequence for normalized cDNA for 40 wild
   genotypes of the invasive plant Centaurea solstitialis, yellow
   starthistle, from across its worldwide distribution. We examined the
   impact of sequencing effort on transcriptome recovery and overlap among
   individuals. To do this, we developed two novel publicly available
   software pipelines: SnoWhite for read cleaning before assembly, and
   AllelePipe for clustering of loci and allele identification in assembled
   datasets with or without a reference genome. AllelePipe is designed
   specifically for cases in which read depth information is not
   appropriate or available to assist with disentangling closely related
   paralogs from allelic variation, as in transcriptome or previously
   assembled libraries. We find that modest applications of sequencing
   effort recover most of the novel sequences present in the transcriptome
   of this species, including single-copy loci and a representative
   distribution of functional groups. In contrast, the coverage of variable
   sites, observation of heterozygosity, and overlap among different
   libraries are all highly dependent on sequencing effort. Nevertheless,
   the information gained from overlapping regions was informative
   regarding coarse population structure and variation across our small
   number of population samples, providing the first genetic evidence in
   support of hypothesized invasion scenarios.},
DOI = {10.1534/g3.112.003871},
ISSN = {2160-1836},
ResearcherID-Numbers = {Rieseberg, Loren H/B-3591-2013
   },
ORCID-Numbers = {Rieseberg, Loren H/0000-0002-2712-2417
   Dlugosch, Katrina/0000-0002-7302-6637
   Bonin, Aurelie/0000-0001-7800-8609},
Unique-ID = {WOS:000314881600021},
}

@article{ WOS:000313523200005,
Author = {Malek, Abdel Salam and Drean, Jean-Yves and Bigue, Laurent and Osselin,
   Jean-Francois},
Title = {Optimization of automated online fabric inspection by fast Fourier
   transform (FFT) and cross-correlation},
Journal = {TEXTILE RESEARCH JOURNAL},
Year = {2013},
Volume = {83},
Number = {3},
Pages = {256-268},
Month = {FEB},
Abstract = {Fabric inspection has an importance to prevent the risk of delivering
   inferior quality product. Until recently, the process was still
   undertaken offline and manually by humans, which has many drawbacks. The
   continuous development in computer technology introduces the automated
   fabric inspection as an effective alternative. In our work, Fast Fourier
   Transform and Cross-correlation techniques, i.e. linear operations, are
   first implemented to examine the structure regularity features of the
   fabric image in the spatial domain. To improve the efficiency of the
   technique and overcome the problem of detection errors, further
   thresholding operation is implemented using a level selection filter.
   Through this filter, the technique is able to detect only the actual or
   real defects and highlight its exact dimensions. A software package such
   as Matlab or Scilab is used for this procedure. It is implemented
   firstly on a simulated plain fabric to determine the most important
   parameters during the process of defect detection and then to optimize
   each of them even considering noise. To verify the success of the
   technique, it is implemented on real plain fabric samples with different
   colors containing various defects. Several results of the proposed
   technique for the simulated and real plain fabric structures with the
   most common defects are presented. Finally, a vision-based fabric
   inspection prototype that could be accomplished on-loom to inspect the
   fabric under construction with 100\% coverage is proposed.},
DOI = {10.1177/0040517512458340},
ISSN = {0040-5175},
ResearcherID-Numbers = {MALEK, Abdelsalam/AAC-3545-2020
   Bigué, Laurent/A-3021-2011
   },
ORCID-Numbers = {Bigué, Laurent/0000-0002-1306-7285
   Osselin, Jean-Francois/0009-0008-1674-9004},
Unique-ID = {WOS:000313523200005},
}

@article{ WOS:000315548700010,
Author = {Murphy, Maurice and McGovern, Eugene and Pavia, Sara},
Title = {Historic Building Information Modelling - Adding intelligence to laser
   and image based surveys of European classical architecture},
Journal = {ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING},
Year = {2013},
Volume = {76},
Number = {SI},
Pages = {89-102},
Month = {FEB},
Abstract = {Historic Building Information Modelling (HBIM) is a novel prototype
   library of parametric objects, based on historic architectural data and
   a system of cross platform programmes for mapping parametric objects
   onto point cloud and image survey data. The HBIM process begins with
   remote collection of survey data using a terrestrial laser scanner
   combined with digital photo modelling. The next stage involves the
   design and construction of a parametric library of objects, which are
   based on the manuscripts ranging from Vitruvius to 18th century
   architectural pattern books. In building parametric objects, the problem
   of file format and exchange of data has been overcome within the BIM
   ArchiCAD software platform by using geometric descriptive language
   (GDL). The plotting of parametric objects onto the laser scan surveys as
   building components to create or form the entire building is the final
   stage in the reverse engineering process. The final HBIM product is the
   creation of full 3D models including detail behind the object's surface
   concerning its methods of construction and material make-up. The
   resultant HBIM can automatically create cut sections, details and
   schedules in addition to the orthographic projections and 3D models
   (wire frame or textured) for both the analysis and conservation of
   historic objects, structures and environments. (C) 2012 International
   Society for Photogrammetry and Remote Sensing, Inc. (ISPRS) Published by
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.isprsjprs.2012.11.006},
ISSN = {0924-2716},
EISSN = {1872-8235},
ResearcherID-Numbers = {MA, Lei/I-4597-2014
   },
ORCID-Numbers = {Pavia, Sara/0000-0003-4506-8386},
Unique-ID = {WOS:000315548700010},
}

@inproceedings{ WOS:000320476800145,
Author = {Abu Eusuf, Muhammad and Rashid, Khairuddin A. and Noor, Wira Mohd and Al
   Hasan, Abdullah},
Editor = {Liu, H and Yang, Y and Shen, S and Zhong, Z and Zheng, L and Feng, P},
Title = {Shear Wall Construction in Buildings: A Conceptual Framework on the
   Aspect of Analysis and Design},
Booktitle = {MATERIALS, MECHANICAL ENGINEERING AND MANUFACTURE, PTS 1-3},
Series = {Applied Mechanics and Materials},
Year = {2013},
Volume = {268-270},
Pages = {706+},
Note = {2nd International Conference on Applied Mechanics, Materials and
   Manufacturing (ICAMMM 2012), Changsha, PEOPLES R CHINA, NOV 17-18, 2012},
Organization = {Hunan Prov Text Engn Ctr},
Abstract = {This study describes the analysis and design process of shear wall
   construction, which is applied in various types of building
   construction. Shear walls resist lateral forces viz, earthquake force
   and wind force for high-rise structure and gravity load for all type of
   structure. Besides, Buildings with cast-in-situ reinforced concrete
   shear walls are widely used in earthquake-prone area and regions in the
   world. Research methods were confined to library research and employed
   software for analysis. The analytical accuracy of complex shear wall
   system have always been of concern to the civil and structural
   Engineering system. The software of this system is performed on the
   platform of modelling and then, the system models are usually idealized
   as line elements instead of continuum elements. Single walls are
   modelled as cantilevers and walls with openings are modelled as pier/
   spandrel systems. In order to find the stiffness, the simple systems
   models can provide reasonable results. It has always been accepted that
   a scale based model in the FEM is exact and justifiable.},
DOI = {10.4028/www.scientific.net/AMM.268-270.706},
ISSN = {1660-9336},
ISBN = {978-3-03785-579-9},
Unique-ID = {WOS:000320476800145},
}

@article{ WOS:000326315200005,
Author = {Amollo, Beatrice Adera},
Title = {Feasibility of adaptation of open source ILS for libraries in Kenya: a
   practical evaluation},
Journal = {ELECTRONIC LIBRARY},
Year = {2013},
Volume = {31},
Number = {5},
Pages = {608-634},
Abstract = {Purpose-Despite its fast growth and penetration in all sectors, it has
   been noted that open source software (OSS) is yet to find its optimal
   place in libraries, particularly libraries in the developing countries.
   Lack of documented information on the experiences and use of open source
   integrated library system (ILS) is a major drawback, and so the need for
   this study. The proposed study aims to help to investigate and test
   usability and cost effectiveness of a typical OSS for ILS. It will
   involve deploying the software from installation, configuration to
   creating customized user interfaces and structures that are specific to
   the requirements of the library's parent organization. The cost and
   performance of the OSS will then be compared with that of a typical
   commercial based software with the same functionalities.
   Design/methodology/approach-A preliminary study has been conducted to
   collect data from libraries in the country through distribution of
   questionnaires to provide data for accurate analysis that will form the
   basis for recommendations. The target group includes library and IT
   personnel in the various institutions and the end-users within sample
   group. A case study is proposed to help establish OSS effectiveness in
   libraries. To test a typical OSS, parameters are to be drawn from two
   models open source maturity model and business readiness rating.
   Findings-A casual observation of the Kenyan situation reveals that the
   majority of academic, public and research libraries depend on
   commercial, free or locally developed systems. This scenario may be
   attributed to lack of knowledge (or interest) in OSS alternatives and
   lack of sufficient technical expertise to support them. While there are
   quite a number of libraries and librarians worldwide that have shown a
   great interest in OSS, few library administrators have actually
   implemented OSS. Could this be due to fear of taking on the risks that
   may come with reliance on open source library automation systems? Is the
   low uptake due to lack of sufficient technical expertise in the
   libraries? The research outcomes will help formulate a model and
   guidelines to be used by systems librarians considering the use of OSS
   for library processes. Factors to be considered when deciding on OSS
   will be outlined.
   Research limitations/implications-This paper is of importance to library
   personnel in Kenya as it establishes the effectiveness of OSS, with the
   aim of empowering the library staff who have for a long time relied on
   their IT departments and vendors for systems installation and
   implementation.
   Originality/value-The study will result in a comprehensive evaluation of
   the economic and functional advantages of OSS as an alternative for the
   library in Kenya. Librarians involved in selection of software for their
   libraries will find this helpful when deciding on the type of software
   to select for their libraries. It will help to enlighten library
   professional about the value of OSS and how they can participate in the
   development of their own systems, instead of always relying on vendors.},
DOI = {10.1108/EL-12-2011-0171},
ISSN = {0264-0473},
EISSN = {1758-616X},
Unique-ID = {WOS:000326315200005},
}

@inproceedings{ WOS:000356716900284,
Author = {Bradatsch, Christian and Kluge, Florian and Ungerer, Theo},
Book-Group-Author = {IEEE},
Title = {A Cross-Domain System Architecture for Embedded Hard Real-Time Many-Core
   Systems},
Booktitle = {2013 IEEE 15TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING
   AND COMMUNICATIONS \& 2013 IEEE INTERNATIONAL CONFERENCE ON EMBEDDED AND
   UBIQUITOUS COMPUTING (HPCC\_EUC)},
Year = {2013},
Pages = {2034-2041},
Note = {15th IEEE International Conference on High Performance Computing and
   Communications (HPCC) /11th IEEE/IFIP International Conference on
   Embedded and Ubiquitous Computing (EUC), Zhangjiajie, PEOPLES R CHINA,
   NOV 13-15, 2013},
Organization = {IEEE; IEEE Comp Soc; Cent S Univ; Natl Univ Defense Technol; Hunan Univ;
   Jishou Univ; IFIP},
Abstract = {The EC project parMERASA investigates techniques for the parallelization
   of industrial real-time applications from automotive, avionic, and
   construction machinery domains. The aim is to execute such applications
   on many-core processors with up to 64 cores. The system software plays a
   key role in the deployment of applications. However, requirements of
   application domains differ widely, and thus no general solution can be
   implemented. In this paper we present the cross-domain system
   architecture utilized in the parMERASA project to provide Runtime
   Environments (RTEs) for the three different domains. The approach eases
   the implementation of domain-specific RTEs through a generic kernel
   library that provides basic hardware abstractions and timing-analyzable
   synchronization mechanisms.},
DOI = {10.1109/HPCC.and.EUC.2013.293},
ISBN = {978-0-7695-5088-6},
Unique-ID = {WOS:000356716900284},
}

@inproceedings{ WOS:000357105902030,
Author = {Car, Nicholas J. and Hartcher, Michael G. and Stenson, Matthew P.},
Editor = {Piantadosi, J and Anderssen, RS and Boland, J},
Title = {Driving Data Management cultural change via automated provenance
   management systems},
Booktitle = {20TH INTERNATIONAL CONGRESS ON MODELLING AND SIMULATION (MODSIM2013)},
Year = {2013},
Pages = {2173-2179},
Note = {20th International Congress on Modelling and Simulation (MODSIM),
   Adelaide, AUSTRALIA, DEC 01-06, 2013},
Organization = {CSIRO; Univ S Australia, Ctr Ind \& Appl Math; Australian Govt, Bur
   Meteorol; GOYDER Inst; Govt S Australia; Australian Math Soc; Australian
   Math Sci Inst; Simulat Australia; Australian \& New Zealand Ind \& Appl
   Math},
Abstract = {Large multi-disciplinary scientific projects that inform government
   policy and have a high public profile are often exposed to high levels
   of scrutiny. Such projects rely on a range of input datasets and
   modelling software packages and generate high volumes of output data,
   which are presented as summarised results in published reports.
   Defending the scientific integrity of project reporting requires that
   all project results have demonstrable integrity with clear evidence of
   the workflows and processes used to generate them, i.e. they must
   implement structured data management including provenance capture and
   storage.
   Provenance data capture forms part of effective data management. The
   reporting of data provenance needs to occur in all workflows within a
   project and crucially needs support from project management, and
   adoption by project staff so that provenance chains are unbroken at
   every step, thus providing demonstrable integrity. Even when project
   funds and milestones are allocated to provenance tasks, such as ensuring
   staff store project datasets in managed locations and generate
   standardised dataset metadata records, data provenance capture has often
   been poor. This indicates that the barrier to the adoption of useful
   data provenance tasks is still significant. The development and
   application of automated systems, which capture and report provenance
   without additional user effort, are therefore of critical importance in
   helping to lower this barrier thus easing cultural change in data
   management.
   Even if a project or organisation has motivation, has made the case,
   established a vision, and developed plans to implement provenance
   management, buy-in from all project staff is still required for success.
   This is because provenance chains containing information about data
   lifecycles need to be unbroken for all results, thus requiring
   involvement from all project staff. Some, perhaps the majority, of
   project processes cannot be automated, thus they will require
   significant manual effort in order to be included in provenance
   management.
   This paper outlines previous best-practice regarding CSIRO's data
   management approach as demonstrated by the Murray Darling Basin
   Sustainable Yields project, and reflects on their shortcomings, such as
   the lack of adequate provenance capture, with improvements suggested. It
   then describes several automated provenance management tools that employ
   semantic web technologies and preserve the identity of provenance
   reports and datasets; which may be used to help with bottom-up practice
   adoption. The automated provenance management tools can provide
   well-defined, automated processes, which may help to lower the barriers
   preventing cultural change for data management at the project and
   organisational level.
   It is hoped that the improved data management practices and the
   automated tools discussed here can inform current and new high-profile
   projects, such as the Bioregional Assessments program, to attain a
   higher quality of demonstrable data integrity through more robust
   provenance management.},
ISBN = {978-0-9872143-3-1},
ResearcherID-Numbers = {Stenson, Matthew P/H-1546-2011},
ORCID-Numbers = {Stenson, Matthew P/0000-0002-3162-4823},
Unique-ID = {WOS:000357105902030},
}

@inproceedings{ WOS:000323397400122,
Author = {Chen, Dong},
Editor = {Yang, W},
Title = {Design of Open-architecture Gantry Milling NC System Based On Motion
   Control Card},
Booktitle = {ADVANCED POLYMER PROCESSING III},
Series = {Key Engineering Materials},
Year = {2013},
Volume = {561},
Pages = {663-666},
Note = {Advanced Polymer Processing International Forum (APPF 2012), Qingdao
   Univ Sci \& Technol, Qingdao, PEOPLES R CHINA, SEP 28-30, 2012},
Organization = {Natl Nat Sci Fdn China; Qingdao Municipal Govt; Qingdao Univ Sci \&
   Technol; Shandong Prov Key Lab Polymer Mat; Adv Mfg Technol},
Abstract = {The open-architecture control system is designed to realize the accurate
   motion control of gantry milling machine in this article. The hardware
   construction of control system adopts ``industry PC + Clipper motion
   control card{''} architecture to achieve the opening and modularization.
   The Full coordination method is selected by assigning two motors of the
   gantry system to the same axis in the same coordinate system. The
   function modules of control software are demonstrated. It is programmed
   based on Visual c++ and Pcomm32 pro DLL library. The design of the
   gantry milling control system can realize the opening of architecture,
   and more accurate, smooth motion control of 4 axes.},
DOI = {10.4028/www.scientific.net/KEM.561.663},
ISSN = {1013-9826},
Unique-ID = {WOS:000323397400122},
}

@article{ WOS:000313911800024,
Author = {Chrysos, Grigorios and Dagritzikos, Panagiotis and Papaefstathiou,
   Ioannis and Dollas, Apostolos},
Title = {HC-CART: A Parallel System Implementation of Data Mining Classification
   and Regression Tree (CART) Algorithm on a Multi-FPGA System},
Journal = {ACM TRANSACTIONS ON ARCHITECTURE AND CODE OPTIMIZATION},
Year = {2013},
Volume = {9},
Number = {4},
Month = {JAN},
Abstract = {Data mining is a new field of computer science with a wide range of
   applications. Its goal is to extract knowledge from massive datasets in
   a human-understandable structure, for example, the decision trees. In
   this article we present an innovative, high-performance, system-level
   architecture for the Classification And Regression Tree (CART)
   algorithm, one of the most important and widely used algorithms in the
   data mining area. Our proposed architecture exploits parallelism at the
   decision variable level, and was fully implemented and evaluated on a
   modern high-performance reconfigurable platform, the Convey HC-1 server,
   that features four FPGAs and a multicore processor. Our FPGA-based
   implementation was integrated with the widely used ``rpart{''} software
   library of the R project in order to provide the first fully functional
   reconfigurable system that can handle real-world large databases. The
   proposed system, named HC-CART system, achieves a performance speedup of
   up to two orders of magnitude compared to well-known single-threaded
   data mining software platforms, such as WEKA and the R platform. It also
   outperforms similar hardware systems which implement parts of the
   complete application by an order of magnitude. Finally, we show that the
   HC-CART system offers higher performance speedup than some other
   proposed parallel software implementations of decision tree construction
   algorithms.},
DOI = {10.1145/2400682.2400706},
Article-Number = {47},
ISSN = {1544-3566},
EISSN = {1544-3973},
ResearcherID-Numbers = {Dollas, Apostolos/AAN-2886-2021
   },
ORCID-Numbers = {Dollas, Apostolos/0000-0003-0060-6240
   Papaefstathiou, Ioannis/0000-0001-6386-5616},
Unique-ID = {WOS:000313911800024},
}

@inproceedings{ WOS:000337235200130,
Author = {Doroez, Yarkin and Ozturk, Erdinc and Sunar, Berk},
Editor = {Matos, JS and Leporati, F},
Title = {Evaluating the Hardware Performance of a Million-bit Multiplier},
Booktitle = {16TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN (DSD 2013)},
Year = {2013},
Pages = {955-962},
Note = {16th Euromicro Conference on Digital System Design (DSD), Santander,
   SPAIN, SEP 04-06, 2013},
Abstract = {In this work we present the first full and complete evaluation of a very
   large multiplication scheme in custom hardware. We designed a novel
   architecture to realize a million-bit multiplication architecture based
   on the Schonhage-Strassen Algorithm and the Number Theoretical Transform
   (NTT). The construction makes use of an innovative cache architecture
   along with processing elements customized to match the computation and
   access patterns of the FFT-based recursive multiplication algorithm.
   When synthesized using a 90nm TSMC library operating at a frequency of
   666 MHz, our architecture is able to compute the product of integers in
   excess of a million bits in 7.74 milliseconds. Estimates show that the
   performance of our design matches that of previously reported software
   implementations on a high-end 3 Ghz Intel Xeon processor, while
   requiring only a tiny fraction of the area.},
DOI = {10.1109/DSD.2013.108},
ISBN = {978-0-7695-5074-9},
ResearcherID-Numbers = {Öztürk, Erdinç/AAY-5700-2020},
Unique-ID = {WOS:000337235200130},
}

@inproceedings{ WOS:000336185300066,
Author = {Ferreira, Carina Fonseca and D'Ayala, Dina and Fernandez Cabo, Jose. L.
   and Diez, Rafael},
Editor = {Piazza, M and Riggio, M},
Title = {Numerical Modelling of Historic Vaulted Timber Structures},
Booktitle = {STRUCTURAL HEALTH ASSESSMENT OF TIMBER STRUCTURES},
Series = {Advanced Materials Research},
Year = {2013},
Volume = {778},
Pages = {517-525},
Note = {2nd International Conference on Structural Health Assessment of Timber
   Structures (SHATIS), Trento, ITALY, SEP 04-06, 2013},
Organization = {Rubner Holzbau; Rothoblaas; Bovair; Trentino Network; Univ Trento, Dept
   Civil Engn \& Mech Engn; CNR IVALSA, Trees \& Timber Inst; European Corp
   Sci \& Technol},
Abstract = {Historic timber structures forming vaulted roofs of public and
   ecclesiastical buildings are present worldwide. The structural response
   of these constructions is usually governed by the structural performance
   of the joints, the interaction between the timber structure and the
   masonry parts, and the current condition of both joints and timber
   members. At present, numerical approaches, such as finite element
   method-based approaches are well-established tools for investigating the
   global response of complex historic structures. Using a FE-based
   software package, the authors developed a numerical model of a portion
   of an existing historic vaulted timber structure, which is part of the
   roof of the Cathedral of Ica in Peru, considering the in-plane
   semi-rigid response of the planked arches in the elastic range. For this
   purpose, the rotational and shear stiffness of the joints and the
   properties of the materials, which are assumed in good conditions, are
   calibrated by comparing the numerical outputs with experimental results
   available in literature. The aim of the work presented here is to
   compare the response of the same vault assuming either continuous
   (planks continuously connected) or discontinuous arches (modelling of
   the semi-rigid response of the joints which connect the planks
   together).},
DOI = {10.4028/www.scientific.net/AMR.778.517},
ISSN = {1022-6680},
ISBN = {978-3-03785-812-7},
ResearcherID-Numbers = {D'Ayala, Dina/K-9626-2019
   MODESTO-RAFAEL, DÍEZ-BARRA/F-2171-2015},
ORCID-Numbers = {D'Ayala, Dina/0000-0003-3864-2052
   MODESTO-RAFAEL, DÍEZ-BARRA/0000-0002-9206-4866},
Unique-ID = {WOS:000336185300066},
}

@inproceedings{ WOS:000343761501026,
Author = {Gabl, Roman and Achleitner, Stefan and Neuner, Johann and Aufleger,
   Markus},
Editor = {Zhaoyin, W and Lee, JHW and Jizhang, G and Shuyou, C},
Title = {3D-numerical Refinements to Simulate High-Head Power Plants},
Booktitle = {PROCEEDINGS OF THE 35TH IAHR WORLD CONGRESS, VOLS I AND II},
Year = {2013},
Pages = {1009-1016},
Note = {35th World Congress of the
   International-Association-for-Hydro-Environment-Engineering-and-Research
   (IAHR), Chengdu, PEOPLES R CHINA, SEP 08-13, 2013},
Organization = {Int Assoc Hydro Environm Engn \& Res},
Abstract = {The presented work deals with the coupling of 1D- and 3D-numerical
   approaches to simulate water hammer effects and surge tanks of high-head
   power plants. 3D-numerical simulations are used for an optimization
   process for nonstandard parts as well as to create more realistic local
   model parameters. Thus, the global 1D-numerical model can be refined and
   this leads to an overall improvement of the performance of the design.
   Three different examples are shown which illustrate the need for and the
   advantages of additional supporting 3D-numerical investigations in this
   field. As long as standard parts are used, the hydraulic specifications
   - especially the local head loss coefficient - are well known. Still,
   most hydro power plants require a unique design either due to specific
   local conditions or optimization needs. Hence, for some parts of a
   high-head power plant, a local investigation using 3D-numerical
   simulation is obligatory in order to increase the accuracy of the
   1D-numerical simulation. The presented examples are all part of an
   actual modernization project of an existing high-head power plant
   operated by the TIWAG-Tiroler Wasserkraft AG, where a new penstock and
   surge tank had to be designed. For the 3D-numerical modeling, the two
   software packages (a) ANSYS-CFX (pipe flow) and (b) FLOW-3D (free
   surface) are used. The work concentrates on the quantification and
   optimization of hydraulic structures with a focus on the local head loss
   coefficient. As a first example, the simulation of the connection
   between the existing head race tunnel and the newly built structure
   (penstock and surge tank) is presented. For this case, in each flow
   direction the head loss should be minimized. Going up into the surge
   tank, the local head loss coefficient of the asymmetric orifice is an
   important design parameter for the mass oscillation in the surge tank.
   To minimize the construction costs of the chambers, the asymmetric
   orifice is optimized with the help of 3D-numerics. In this case the
   modeling results could be validated with a physical lab scale test
   (scale 1: 25). In addition, the filling process of the upper chamber of
   the surge tank is investigated. Based on the 1D-numerical simulation,
   the inflowing discharge is calculated with a peak flow of up to 140
   m(3)/s. The existing chamber has been redesigned and extended so that a
   circular flow through the complete upper chamber is enabled. The main
   goal is to analyze the travelling of the surge front and to guarantee
   that no water flows out of the chamber. In addition, the free surface
   flow should be the main flow type in as many sections as possible over
   the complete filling time. Therefore, different built-in components are
   investigated.},
ISBN = {978-7-302-33544-3},
ResearcherID-Numbers = {Achleitner, Stefan/AAC-2224-2019
   Gabl, Roman/I-3390-2017},
ORCID-Numbers = {Achleitner, Stefan/0000-0003-3339-992X
   Gabl, Roman/0000-0001-9701-879X},
Unique-ID = {WOS:000343761501026},
}

@inproceedings{ WOS:000369728800030,
Author = {Gorringe, Chris and Brown, Malcolm},
Book-Group-Author = {IEEE},
Title = {Recommendations and Best Practices for creating reusable Test Signal
   Framework definitions},
Booktitle = {2013 IEEE AUTOTESTCON},
Series = {IEEE Autotestcon},
Year = {2013},
Note = {48th Annual AUTOTESTCON Conference, Schaumburg, IL, SEP 16-19, 2013},
Organization = {IEEE; IEEE Aerosp \& Elect Syst Soc; IEEE Instrumentat \& Measurement
   Soc},
Abstract = {Critical to the successful implementation of Open System Architecture
   (OSA) test software, in accordance with the requirements of IEEE
   1641{[}1]{[}2], is the implementation Signal Libraries using the Test
   Signal Frameworks (TSFs) that provide the test program set (TPS)
   Developer with the suite of defined signals to be used for the test
   software. This paper aims to provide insight for the TPS developers and
   guidance on the best practices to be followed when creating and reusing
   Signal Libraries. It also considers issues to be kept in mind when
   bringing together a collection of existing Signal Libraries to be used
   for testing, and references examples that are available for future
   developers.
   The paper considers research into the effects of different methods that
   could be used for matching the TSFs used in describing UUT test
   requirements and instrument capabilities. This includes evaluation of
   their relative merits using a variety of ``use cases{''} and example
   TSFs. Typical User questions are also considered along with their
   answers.
   The example TSFs referenced within the paper represent a collection of
   Signal Libraries available used in previous demonstrations and brought
   up to the IEEE Std. 1641-2010 standard. These include Signal Libraries
   provided as part of 1641 demonstrations and previous reports and form
   the basis of an OSA Signal Library.
   The technical analysis considers both symbolic signal expression and
   signal simulation, to address issues such as signal equivalence,
   quantization, phase jitter and overall performance. The purpose of the
   paper being to produce material for inclusion into recommendations and
   guidelines that will ensure best practice can be followed when
   generating Signal libraries for a wide range of signal types. This will
   ensure that Signal libraries are fit for purpose, and achieve the
   desired level of quality necessary for future matching UUT test
   requirements against the instrument capabilities. In addition, this will
   ensure that the UUT requirements are NOT over specified against the
   instrument capability used to initially implement the test solution.
   To ensure best practice across OSA RTS users, formal guidelines are
   required for the process of creating and defining IEEE 1641 TSFs in
   order to minimize differences between signal libraries and any effects
   that become apparent when implemented on different ATEs, using different
   instrumentation. The paper considers guidelines which could form the
   basis of a recommended practice to ensure standardization across the
   1641 user base for use on Military or Commercial Automatic Test Systems
   (ATS).},
ISSN = {1088-7725},
ISBN = {978-1-4673-5681-7; 978-1-4673-5683-1},
Unique-ID = {WOS:000369728800030},
}

@article{ WOS:000327284000008,
Author = {Huang, Luoyi and Yao, Jiao and Wu, Wei and Yang, Xiaoguang},
Title = {FEASIBILITY ANALYSIS OF VEHICLE-TO-VEHICLE COMMUNICATION ON SUBURBAN
   ROAD},
Journal = {PROMET-TRAFFIC \& TRANSPORTATION},
Year = {2013},
Volume = {25},
Number = {5},
Pages = {483-493},
Abstract = {With the evolution of advanced wireless communication technologies,
   tremendous efforts have been invested in vehicular networking,
   particularly the construction of a vehicle-to-vehicle communication
   system that supports high speed and mobility. In vehicle-to-vehicle
   communication environment, vehicles constantly exchange information
   using wireless technology.
   This paper aims to propose a vehicle-to-vehicle communication system and
   validate the feasibility of the system on a suburban road in China. Two
   vehicles were used equipped with IEEE 802.11p based DSRC (Dedicated
   Short Range Communications) device to construct a vehicle-to-vehicle
   communication platform. The system architecture consisting of hardware
   and software was described in details. Then, communication
   characteristics such as RSSI (Received Signal Strength Indicator),
   latency and PLR (packet loss rate) were analyzed. Additionally,
   GPS-related information (such as ground speed and location) was obtained
   through field test on a suburban road in Shanghai and Taicang City. The
   test results demonstrate satisfactory performance of the proposed
   system.},
DOI = {10.7307/ptt.v25i5.446},
ISSN = {0353-5320},
EISSN = {1848-4069},
ResearcherID-Numbers = {yang, xiao/HJI-7815-2023},
Unique-ID = {WOS:000327284000008},
}

@inproceedings{ WOS:000326838900068,
Author = {Kamal, C. and Ramkumar, S. and Hariharan, N.},
Book-Group-Author = {IEEE},
Title = {Experimental verification and implementation of a isolated ZVT boost
   converter for high step-up electric traction},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON POWER, ENERGY AND CONTROL (ICPEC)},
Year = {2013},
Pages = {346-349},
Note = {International Conference on Power, Energy and Control (ICPEC), Dindigul,
   INDIA, FEB 06-08, 2013},
Organization = {IEEE; PSNA Coll Engn \& Technol, Dept Elect \& Elect Engn; IEEE Power \&
   Energy Soc; Electron Devices Soc; IET; SIEMENS; Natl Instruments; IEEE
   Madras Sect},
Abstract = {Lofty power and elevated step-up isolated dc-dc converters have been
   widely employed in the emerald power systems and especially for electric
   traction drives. In electronics engineering, a DC to DC converter is a
   circuit, which converts a source of direct current from one voltage to
   another. It is a class of power converter. In many DC-DC applications,
   output isolation may need to be implemented depending on the type of
   application to meet safety standards to provide impedance matching. This
   galvanic isolation is required to attain a flexible system
   reconfiguration. Active lamp boost converter with coupled-inductors is
   proposed for high step-up electric traction applications. The
   primary-parallel-secondary-series structure is employed in this project
   to knob the huge input current, maintain the high output voltage and
   enlarge the voltage gain. Purpose of the coupled-inductors is to reduce
   the voltage gain addition and lesser concern in switching. Clamp circuit
   is used to re circulate the energy. The voltage strain in rectifier is
   minimized and reverse-recovery problem is eliminated. The hardware unit
   utilizes embedded technology using PIC microcontroller16F84A to give
   gate pulses to the MOSFET converter switches. In this proposed converter
   only three MOSFET's are used which reduces the number of devices and
   makes circuit simple in construction. Efficiency, size, and cost are the
   primary advantages of the proposed isolated ZVT boost converter when
   compared to other existing converters. Isolated ZVT boost converter
   employing series parallel arrangement will have an efficiency of about
   88-94\%, whereas existing converters are usually 80 to 85\% efficient.
   In the present work, a novel ZVT boost converter for electric traction
   drive application is developed has been designed in MATLAB/SIMULINK
   software packages. The simulation result for a 40-V-to600- V converter
   is simulated in open circuit and closed loop system. Simulated
   disturbance is applied at a time period of 0.7 micro second and noted
   that the output voltage is maintained constant always. The hardware unit
   and experimental results of a 15V to 85-V ZVT converter is implemented
   and the results are verified.},
ISBN = {978-1-4673-6030-2; 978-1-4673-6027-2},
Unique-ID = {WOS:000326838900068},
}

@inproceedings{ WOS:000333801000013,
Author = {Kang, Byeong-Chul and Kang, Kyu-young and Lee, Sung-chan and Lee, Giyong
   and Lee, Kyoungpyo},
Book-Group-Author = {IEEE},
Title = {A Simple and Robust Network Protocol for Remote Controlled Robot under
   Punctuated Network Environment},
Booktitle = {2013 10TH INTERNATIONAL CONFERENCE ON UBIQUITOUS ROBOTS AND AMBIENT
   INTELLIGENCE (URAI)},
Series = {International Conference on Ubiquitous Robots and Ambient Intelligence},
Year = {2013},
Pages = {45-50},
Note = {10th International Conference on Ubiquitous Robots and Ambient
   Intelligence (URAI), Jeju, SOUTH KOREA, OCT 30-NOV 02, 2013},
Organization = {Korea Robot Soc; Sungkyunkwan Univ; IEEE; IEEE Robot \& Automat Soc;
   Robot Soc Japan; Robot Soc Taiwan; Chinese Inst Measurement Technol Mech
   Engn; Korea Tourism Org; KOFST; Jeju Convent \& Visitors Bur},
Abstract = {Software architecture with RESTful API is proposed for ILI robot system
   under unstable networking environment in order to increase reliability,
   scalability, independency, and generality, yet simple to implement.
   Evaluation of the architecture showed that the system could be flexibly
   adapted to emergency cases, which is performed for diverse scenarios
   including sudden shutdown of component, hot-swap of component, network
   failure, etc. Consequently, the proposed software architecture is
   conducted to reduce complexity of implementation and maintenance, to
   increase scalability, and to allow easy rescue and recovery scheme.},
ISSN = {2325-033X},
ISBN = {978-1-4799-1197-4; 978-1-4799-1195-0},
Unique-ID = {WOS:000333801000013},
}

@inproceedings{ WOS:000357105901121,
Author = {Khaiter, P. A. and Erechtchoukova, M. G.},
Editor = {Piantadosi, J and Anderssen, RS and Boland, J},
Title = {Ecosystem services in environmental sustainability: a formalized
   approach using UML},
Booktitle = {20TH INTERNATIONAL CONGRESS ON MODELLING AND SIMULATION (MODSIM2013)},
Year = {2013},
Pages = {1805-1811},
Note = {20th International Congress on Modelling and Simulation (MODSIM),
   Adelaide, AUSTRALIA, DEC 01-06, 2013},
Organization = {CSIRO; Univ S Australia, Ctr Ind \& Appl Math; Australian Govt, Bur
   Meteorol; GOYDER Inst; Govt S Australia; Australian Math Soc; Australian
   Math Sci Inst; Simulat Australia; Australian \& New Zealand Ind \& Appl
   Math},
Abstract = {Ecological systems generate diverse services that are vital for human
   well-being and socioeconomic development. Ecosystem services are a key
   notion for the idea of sustainable environmental development. A
   practical utilization of this concept of sustainability requires a
   mechanism whereby all the goods and services generated by ecosystems are
   adequately quantified, valuated and incorporated in the decision-making
   process. Each of these tasks is substantially non-trivial. To deal with
   the named complexity, an adequate theoretical framework and a
   sophisticated information system have to be in place for the use by
   stakeholders of environmental sustainability.
   A theoretical framework for environmental sustainable management and its
   further extension on the basis of a meta-modeling approach have been
   suggested in our prior publications. As the next step, we need to build
   an information system implementing the main elements of the framework in
   corresponding software components.
   In this paper, we are suggesting a formalized approach to the entire
   process of sustainable environmental management on the basis of the
   Unified Modeling Language (UML). UML is a standardized graphical
   language being used in object-oriented software engineering for
   specifying, documenting and visual modeling of an IT project's artifacts
   or components within a wide range of applications. With the three groups
   of graphical models (i.e., functional, object and dynamic), UML is aimed
   to provide a standard notation and describe different aspects of a
   software project. We demonstrate the ways, in which UML can be applied
   in the information systems development for the needs of environmental
   management and protection. The constituting software blocks of an
   environmental information system implementing the framework are
   presented by the following UML graphical models:
   overall system architecture is depicted as a UML package diagram;
   contents of the Ecosystem package are shown as a UML use-case diagram;
   activities of the Monitoring package are shown in the notation of a UML
   use-case diagram
   the Modeling package is presented as a UML component diagram;
   internal steps within the Modeling natural dynamics module are shown as
   a UML use-case diagram;
   logic of the Modeling anthropogenic dynamics module is demonstrated as a
   UML use-case diagram;
   internal steps of the Valuation package are shown as a UML use-case
   diagram; and
   flow of operations within the Management package is presented as a UML
   activity diagram.
   For many years, UML has been successfully utilized in the systems
   development life cycles of business-type IT projects. It is supported by
   multiple commercial and free CASE-tools, including those featuring
   reverse-and round-trip engineering.
   At the same time, despite obvious advantages, applications of UML in the
   design of environmental information systems remain rather limited. While
   UML has not been widely adopted and used in environmental modeling and
   software design, it is reasonable to expect growing interest towards UML
   in the field as a tool to combine the power of visual and simulation
   modeling and to facilitate automated construction and synthesis of
   environmental information systems.
   The paper is also intended to inspire and shape the discussion within
   the session on ``Models, Methods, Techniques and Tools in Quantifying
   Ecosystem Services and Environmental Sustainability{''}.},
ISBN = {978-0-9872143-3-1},
Unique-ID = {WOS:000357105901121},
}

@inproceedings{ WOS:000335232400104,
Author = {Kim, Do Young and Lee, Mi Suk and Choi, Seung Han and Koo, Ki-Jong and
   Hwang, Inki and Kim, Yeong Jin},
Book-Group-Author = {IEEE},
Title = {An Immersive Telepresence Platform Based on Distributed Architecture},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON ICT CONVERGENCE (ICTC 2013): FUTURE
   CREATIVE CONVERGENCE TECHNOLOGIES FOR NEW ICT ECOSYSTEMS},
Year = {2013},
Pages = {466-468},
Note = {International Conference on ICT Convergence (ICTC) - Future Creative
   Convergence Technologies for New ICT Ecosystems, Minist Sci, ICT \&
   Future Planning, SOUTH KOREA, OCT 14-16, 2013},
Organization = {IEEE Commun Soc; Korean Inst Commun \& Informat Sci; Elect \&
   Telecommunicat Res Inst; Korea Commun Agcy; Samsung Elect; Multi Screen
   Serv Forum Korea; KT; SK Telecom; LG U+; Ericsson LG; LG Elect},
Abstract = {A novel telepresence platform is proposed in this paper to provide
   immersive video conferencing based on distributed architecture and
   enhanced QoS/QoE technology. MCU(Multi-point Control Unit)-based
   telepresence platform has been widely used as a popular architecture due
   to its easy implementation of multiple video-audio mixing and conference
   controls from a central point. MCU-based platform usually inputs all
   participant's media and outputs mixed media for conference participants
   one by one through network. Proposed platform has a concept of `main
   speaker' by decision algorithm during conference and determines a main
   speaker who sends a high-resolution media to simple nodes. Simple nodes
   manage routing tables, and they copy the media packets and forward them
   to conference participants according to the tables. For the immersive
   service, a robust packet loss recovery algorithm newly developed covers
   upto 10\% bursty loss to prevent interruption of conference often
   observed in commercial Internet. Proposed platform with distributed
   media control protocol with enhanced QoS/QoE technology based on
   distributed architecture has been designed and implemented by software.
   It shows meaningful reduction of traffic over network with the shorter
   end-to-end media delay than observed in MCU-based platform under same
   conditions.},
ISBN = {978-1-4799-0698-7},
Unique-ID = {WOS:000335232400104},
}

@inproceedings{ WOS:000371647300138,
Author = {Kusy, Vojtech},
Editor = {Hajek, P and Tywoniak, J and Lupisek, A and Sojkova, K},
Title = {AN ONTOLOGY DRIVEN BIM COMPONENTS REPOSITORY: A NEW WAY TO SHARE BIM
   COMPONENTS},
Booktitle = {SUSTAINABLE BUILDING AND REFURBISHMENT FOR NEXT GENERATIONS},
Year = {2013},
Pages = {559-562},
Note = {Conference on Central Europe towards Sustainable Building (CESB13),
   Prague, CZECH REPUBLIC, JUN 26-28, 2013},
Organization = {Czech Tech Univ, Fac Civil Engn; iiSBE; UNEP; CIB; FIDIC},
Abstract = {In the recent years Building Information Modeling (BIM) has been finally
   widely recognized as the next evolutionary step in the architecture,
   engineering and construction (AEC) industry and also as a step towards
   the sustainable construction. However for the effective work with the
   BIM design tools an extensive library of atomic building blocks BIM
   components is needed. Due to the limitation of built-in libraries
   provided by the software vendors, several BIM component portals emerged
   in the recent years enabling vendors and manufacturers to share the
   components with the designers. This paper addresses this issue as well
   and proposes a solution for a better ontology-driven knowledge
   management, categorization, conversion, enrichment and querying of these
   components.},
ISBN = {978-80-247-5015-6},
Unique-ID = {WOS:000371647300138},
}

@article{ WOS:000209652700018,
Author = {Pickles, Austin J. and Kilgore, Ian M. and Steer, Michael B.},
Title = {Automated Creation of Complex Three-Dimensional Composite Mixtures for
   Use in Electromagnetic Simulation},
Journal = {IEEE ACCESS},
Year = {2013},
Volume = {1},
Pages = {248-251},
Abstract = {The manual creation of complex 3D structures for use in engineering
   analysis is a major obstacle to analyzing physically realistic
   structures. A bias is invariably imposed when a mixture is manually
   composed, and the structure is rarely representative of the process by
   which composites are fabricated. Properties such as packing density and
   anisotropies that seem to easily occur in nature are very difficult to
   obtain with manual arrangements. This paper addresses the creation of
   complex 3D mixtures, comprising crystals embedded in a matrix, for
   subsequent electromagnetic (EM) analysis. The physically realistic
   arrangement of the crystals is facilitated by the use of physics engine
   software, specifically the Bullet physics library, which renders the
   realistic effects in advanced computer games. A composite mixture of
   crystals is created by pouring a series of random crystals into a box
   with the crystals bouncing against each other and aligning just as they
   do in the real world. Higher packing densities are obtained than can be
   reasonably obtained with manual construction. The arrangement of the
   obtained crystals reflects the real world alignment of asymmetric
   crystals. A composite is created here and used with EM simulation
   software to investigate energy localization in materials.},
DOI = {10.1109/ACCESS.2013.2262014},
ISSN = {2169-3536},
Unique-ID = {WOS:000209652700018},
}

@inproceedings{ WOS:000368423200082,
Author = {Pisu, C. and Casu, P.},
Editor = {Grussenmeyer, P},
Title = {CLOUD GIS AND 3D MODELLING TO ENHANCE SARDINIAN LATE GOTHIC
   ARCHITECTURAL HERITAGE},
Booktitle = {XXIV INTERNATIONAL CIPA SYMPOSIUM},
Series = {International Archives of the Photogrammetry Remote Sensing and Spatial
   Information Sciences},
Year = {2013},
Volume = {40-5-W2},
Pages = {489-494},
Note = {24th International CIPA Symposium, Strasbourg, FRANCE, SEP 02-06, 2013},
Abstract = {This work proposes the documentation, virtual reconstruction and
   spreading of architectural heritage through the use of software packages
   that operate in cloud computing. Cloud computing makes available a
   variety of applications and tools which can be effective both for the
   preparation and for the publication of different kinds of data. We
   tested the versatility and ease of use of such documentation tools in
   order to study a particular architectural phenomenon. The ultimate aim
   is to develop a multi-scale and multi-layer information system, oriented
   to the divulgation of Sardinian late gothic architecture. We tested the
   applications on portals of late Gothic architecture in Sardinia.
   The actions of conservation, protection and enhancement of cultural
   heritage are all founded on the social function that can be reached only
   through the widest possible fruition by the community. The applications
   of digital technologies on cultural heritage can contribute to the
   construction of effective communication models that, relying on sensory
   and emotional involvement of the viewer, can attract a wider audience to
   cultural content.},
ISSN = {2194-9034},
Unique-ID = {WOS:000368423200082},
}

@inproceedings{ WOS:000349244200066,
Author = {Plumbridge, Gary and Audsley, Neil C.},
Editor = {Cumplido, R and DeLaTorre, E and Wirthlin, M},
Title = {Programming FPGA Based NoCs with Java},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS
   (RECONFIG)},
Series = {Proceedings International Conference on Reconfigurable Computing and
   FPGAs},
Year = {2013},
Note = {International Conference on Reconfigurable Computing and FPGAs
   (ReConFig), Cancun, MEXICO, DEC 09-11, 2013},
Organization = {INAOE; Univ Politecnica Madrid; BYU; IEEE; IEEE Circuits \& Syst Soc;
   XILINX; Intel; Natl Instruments; Pico Comp},
Abstract = {FPGAs enable NoC architecture experimentation, although to be effective
   they need to be supported by tools and frameworks for construction of
   the NoC and effective software programming of the NoC. In this paper, we
   focus upon effective programming of the NoC using Java, complementing
   previous work which proposes the Blueshell framework for NoC generation
   for FPGAs. The approach taken is called Network-Chi, providing a number
   of key extensions to the Chi Java compiler. This includes provision of a
   networking API within Java giving a mesh based abstraction for network
   communication, allowing the programmer to send Java objects to other
   nodes without consideration for the underlying hardware topology or
   protocols; and a region-based memory management API that enables the
   definition of transient allocation contexts that discard all objects
   allocated within them when they reach the end of execution. Results show
   the approach taken to be efficient and effective.},
ISSN = {2325-6532},
ISBN = {978-1-4799-2078-5},
ORCID-Numbers = {Audsley, Neil/0000-0003-3739-6590},
Unique-ID = {WOS:000349244200066},
}

@article{ WOS:000325814600004,
Author = {Rababeh, Shaher and Al Qablan, Husam and El-Mashaleh, Mohammad},
Title = {Utilization of tie-beams for strengthening stone masonry arches in
   Nabataean construction},
Journal = {JOURNAL OF ARCHITECTURAL CONSERVATION},
Year = {2013},
Volume = {19},
Number = {2},
Pages = {118-130},
Abstract = {This paper attempts to demonstrate the use of timber ties in the
   construction of stone masonry arches in Jordan from the late first
   century BC onwards. It investigates the techniques used and relates them
   to present-day structural design. In Nabataean architecture, tie-beams
   were used for strengthening of masonry structures and, in particular, to
   strengthen masonry arches and vaults against their most critical failure
   mechanism. The tie-beams introduce tension resistance. This fact has two
   important consequences: the capacity of the arch itself is increased,
   and the lateral thrust transmitted to the piers is reduced, thereby
   increasing the strength of the arch-pier system. Collapse of an arch
   typically occurs when no tie-rods or tie-beams are used and the piers
   are unable to bear the thrust of the arch. While the first effect has
   been stressed by the existing research, less attention has been paid to
   the second effect, which nevertheless is very important for practical
   applications. In this paper, the main methods are critical analysis of
   archaeological information enriched by structural evaluation of the
   strength of such systems. From such an approach, reliable hypotheses can
   be made concerning the design and construction process. The effect of
   bonding a tie-beam to the intrados of a semi-circular arch on the
   lateral thrust is evaluated analytically using the non-linear finite
   element package ABAQUS software (HKS, version 6.5). The results are
   analysed based on the determination of the thrust line. This study
   concludes some recommendations for the conservation of historical
   buildings in seismic regions, based on a thorough understanding of their
   structure, construction features and materials, which may help to
   prevent earthquake-induced damage to buildings taking into account
   traditional and modern materials and techniques.},
DOI = {10.1080/13556207.2013.819656},
ISSN = {1355-6207},
EISSN = {2326-6384},
ResearcherID-Numbers = {El-Mashaleh, Mohammad S./AAE-9029-2020
   },
ORCID-Numbers = {Al Qablan, Husam/0000-0001-5610-4173
   Rababeh, Shaher/0000-0001-5596-6273
   El-Mashaleh, Mohammad/0000-0002-6279-9335},
Unique-ID = {WOS:000325814600004},
}

@inproceedings{ WOS:000350208600220,
Author = {Rey, J. Rey},
Editor = {Cruz, PJS},
Title = {The disappearance of the structural analysis barrier: the Sydney Opera
   House from a contemporary perspective},
Booktitle = {STRUCTURES AND ARCHITECTURE: CONCEPTS: APPLICATIONS AND CHALLENGES},
Year = {2013},
Pages = {1776-1783},
Note = {2nd International Conference on Structures and Architecture, Guimaraes,
   PORTUGAL, JUL 24-26, 2013},
Organization = {European Convent Construct Steelwork; Int Assoc Bridge Maintenance \&
   Safety; Int Assoc Bridge \& Struct Engn; Int Assoc Shell \& Spatial
   Struct; Architects Council Europe; American Inst Architects; American
   Soc Civil Engineers; European Federat Precast Concrete; Building Technol
   Educators Soc; European Assoc Architectural Educ; European Federat Engn
   Consultancy Assoc; European Federat Stuct Glass Lab; Int Sci Comm Anal
   \& Restorat Struct Architectural Heritage; American Soc Civil Engineers,
   Struct Engn Inst; TensiNet; Inst Struct Engineers;
   StructuralEngineer.info Website; Associacao Nacl Industriais
   Prefabricacao Betao; Associacao Portuguesa Projetistas Consultores;
   Associacao Portuguesa Seguranca Conservacao Pontes; Associacao
   Portuguesa Construcao Metalica Mista; Ordem Arquitetos Seccao Reg Norte;
   Ordem Engenheiros; Sociedad Espanola Historia Construcc},
Abstract = {Over the last decades, the new technical means and methods, and in
   particular computing technology, has became more and more popular and
   efficient and their capabilities evolved exponentially in due time. This
   development has led to the present situation, where structural analysis
   has stopped being a hindrance to the development of projects with
   geometrical and construction complexity.
   The Sydney Opera House, designed by Danish architect Jorn Utzon, it is
   one of the most representative examples of the architecture developed
   prior to the disappearance of the structural analysis barrier in
   architectural design that tried to cross this imaginary border. When
   design work for the competition started in 1956, personal computers did
   not exist, nor did the software packages for graphic representation and
   structural analysis. Thus, to enable their definition and analysis, the
   proposed free-form surfaces have been forced to adapt to a well
   understood geometry as well as the shells have been replaced by
   fan-shaped concrete ribs, dramatically altering Utzon's initial vision.
   This paper explores and describes the possibilities of analysis and
   design of the structure of Utzon's original concept employing the
   scientific, technological and industrial tools available nowadays.},
ISBN = {978-0-203-79856-0; 978-0-415-66195-9},
Unique-ID = {WOS:000350208600220},
}

@inproceedings{ WOS:000332043900081,
Author = {SalarKaleji, Fatemeh and Dayyani, Aboulfazl},
Editor = {Ilarslan, M and Ince, F and Kaynak, O and Basturk, S},
Title = {A survey on Fault Detection, Isolation and Recovery (FDIR) Module in
   Satellite Onboard Software},
Booktitle = {PROCEEDINGS OF 6TH INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN SPACE
   TECHNOLOGIES (RAST 2013)},
Year = {2013},
Pages = {545-548},
Note = {6th International Conference on Recent Advances in Space Technologies
   (RAST), Istanbul, TURKEY, JUN 12-14, 2013},
Organization = {IEEE; Amer Inst Aeronaut \& Astronaut; Int Union Radio Sci; IEEE Aerosp
   \& Elect Syst Soc; IEEE Geoscience \& Remote Sensing Soc; European Assoc
   Remote Sensing Lab; Turkish AF Acad; Istanbul Tech Univ; Bogazici Univ;
   Middle E Tech Univ; ASELSAN; TUBITAK UZAY; TAI; Space \& Def
   Technologies; Asia Pacific Space Cooperat Org; Mitsubishi Elect; Ingn
   Dei Sistemi; Simmeca; Def \& Aerosp; MSI; Int Soc Photogrammetry \&
   Remote Sensing; Turkish AF Acad, Aeronaut \& Space Technologies Inst},
Abstract = {The complexity of the avionic systems in satellites is rising as space
   missions become increasingly more sophisticated. This complexity
   emphasizes the need for more dependable systems with minimal anomalies.
   As satellite manufactures seek to convert many hardware implemented
   functionalities into software, the On-Board Software (OBSW) is becoming
   a major component in every satellite. Noticeably, more tasks for Fault
   Detection, Isolation and Recovery (FDIR) are being implemented in
   software, where the need comes for a well-defined software architecture
   that supports a cost-effective implementation of the FDIR functions.
   FDIR was already explained as key functionality of the OBSW. Obviously
   not all failures are subject to onboard identification and not all
   failures are subject to onboard recovery. The FDIR concept to be worked
   out for the spacecraft during the engineering phase follows some basic
   requirements and principles, implements a certain failure
   hierarchy-specifying furthermore on which level the failure is to be
   fixed-and finally it implements a consistent approach for the
   functionality transferring the spacecraft to Safe Mode and how to
   recover from there.
   Since a FDIR concept usually follows a hierarchical approach, in this
   paper we will indicate a FDIR and safeguarding hierarchy example in the
   paper. In this structure we will indicate the levels of failures which
   handled by unit internal, subsystem software, satellite system software,
   onboard computer hardware reconfiguration unit and ground. Also we will
   explain the FDIR hierarchy in safe mode implementation in a bit more
   detail.
   In this paper we will consider FDIR technologies in the On-board
   software in a satellite. Today, there are several proposed methodologies
   and frameworks which try to solve this problem. We will analyze the
   functionalities in FDIR Module implemented in an OBSW Framework. Also we
   have a survey on the FDIR hierarchies and their relationship to the
   Packet Utilization Standard (PUS) Services.},
ISBN = {978-1-4673-6396-9; 978-1-4673-6395-2},
Unique-ID = {WOS:000332043900081},
}

@inproceedings{ WOS:000360515000026,
Author = {Smienk, Henk and Karjadi, Erwan and Vazquez, Gabriel and Doherty, Peter
   and Dooley, Patrick},
Book-Group-Author = {ASME},
Title = {DCV AEGIR PIPELAY INSTALLATION ANALYSES AND CAPABILITIES},
Booktitle = {PROCEEDINGS OF THE ASME 32ND INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE
   AND ARCTIC ENGINEERING - 2013, VOL 4A},
Year = {2013},
Note = {32nd ASME International Conference on Ocean, Offshore and Arctic
   Engineering, Nantes, FRANCE, JUN 09-14, 2013},
Organization = {ASME, Ocean Offshore \& Arctic Engn Div},
Abstract = {Heerema Marine Contractors (HMC) is entering a new era of pipe laying
   with the new Deep water Construction Vessel (DCV) Aegir, designed to be
   able to reel/J-lay pipelines for a broad range of pipe dimension and
   water depth combinations. On the one side this is governed by
   equipment/vessel limitations (moonpool size, high top tension capacity,
   stinger component capabilities) and on the other side limited by
   pipeline design code (e.g. DNV, API) acceptance criteria for reel-lay
   and J-lay installation.
   This paper outlines the pipelay capabilities of DCV Aegir and details
   the J-lay (with quad joints) and reel-lay installation analyses
   performed to aid in the design of the vessel pipelay equipment. DCV
   Aegir has two modes for J-lay installation, which are light J-lay with
   friction clamps and heavy J-lay with collar clamps in combination with
   collars in the pipeline. DCV Aegir reel-lay installation from the
   pipeline in the tensioners down to the seabed will also be explained.
   Light J-lay, heavy J-lay and reel-lay have maximum top tension
   capacities (related to the equipment) of 600 mT, 2000 mT and 800 mT,
   respectively. The top tension capacity also depends on pipe OD, coating
   type and thickness. J-lay and reel-lay installation analyses are
   performed with the non-linear finite element software package Flexcom
   from MCS Kenny to determine installation capabilities with respect to
   pipe OD, wall thickness and water depth combinations. Together with that
   the pipelay equipment design is validated by pipeline installation
   analyses.
   Shallow and deep water normal pipeline installation for all three
   pipelay options will be discussed. DCV Aegir pipelay equipment includes
   a retractable hang off module/stinger for deployment of pipelines. The
   usage and benefits of the hang off module will be documented. For the
   J-lay installation modes the procedure for lowering a quad joint is
   analysed in order to optimise equipment usage. DCV Aegir possesses a
   high capacity abandonment and recovery system (up to 2000mT).
   Abandonment and recovery analyses description and design review aspects
   will be discussed Finally, the pipeline in-line structure installation
   analyses, together with design review considerations will be documented},
Article-Number = {V04AT04A026},
ISBN = {978-0-7918-5536-2},
Unique-ID = {WOS:000360515000026},
}

@inproceedings{ WOS:000333754700046,
Author = {Sung, Yingrong Coral and Sung, Jui-Feng},
Book-Group-Author = {IEEE},
Title = {Short Message Service for Internet-Mobile Platform},
Booktitle = {2013 15TH ASIA-PACIFIC NETWORK OPERATIONS AND MANAGEMENT SYMPOSIUM
   (APNOMS)},
Series = {Asia-Pacific Network Operations and Management Symposium-APNOMS},
Year = {2013},
Note = {15th Asia-Pacific Network Operations and Management Symposium (APNOMS),
   Hiroshima, JAPAN, SEP 25-27, 2013},
Organization = {IEICE Tech Comm Informat Commun Management; Korea Informat \& Commun Soc
   Comm Korean Network Operat \& Management; IEEE Commun Soc; IEEE},
Abstract = {In 2003, the Parlay group proposed a concept and technique of the web
   service and proposed the other suite of Open Services Architecture (OSA)
   specifications, Parlay X. The Parlay X Application Programming Interface
   (API) defines a set of easy-to-use API, which utilizes the web service
   for application developer to access telecommunications functions more
   easily. In this paper, we use Parlay X-based Internet-Mobile platform
   called IBM WebSphere software for Telecom (WsT). To accommodate the
   telecom services (i.e., Short Message Service (SMS), Multimedia
   Messaging Service (MMS)) on IBM WsT, a set of API named OpenAPI is
   implemented. The paper studies the SMS behaviors and investigates the
   OpenAPI SMS performance. To improve the OpenAPI SMS delivery delay, a
   timeout timer is implemented. Our research also predicts SMS delivery
   failure and provides guidelines to select appropriate timeout timer
   values for OpenAPI SMS.},
ISSN = {2576-8565},
EISSN = {2576-8557},
Unique-ID = {WOS:000333754700046},
}

@inproceedings{ WOS:000325977200040,
Author = {Tidman, James and Weston, Tyler and Hewitt, Donna and Herman, Matthew A.
   and McMackin, Lenore},
Editor = {Tescher, AG},
Title = {Compact opto-electronic engine for high-speed compressive sensing},
Booktitle = {APPLICATIONS OF DIGITAL IMAGE PROCESSING XXXVI},
Series = {Proceedings of SPIE},
Year = {2013},
Volume = {8856},
Note = {Conference on Applications of Digital Image Processing XXXVI, San Diego,
   CA, AUG 26-29, 2013},
Organization = {SPIE},
Abstract = {The measurement efficiency of Compressive Sensing (CS) enables the
   computational construction of images from far fewer measurements than
   what is usually considered necessary by the Nyquist-Shannon sampling
   theorem. There is now a vast literature around CS mathematics and
   applications since the development of its theoretical principles about a
   decade ago. Applications include quantum information to optical
   microscopy to seismic and hyper-spectral imaging. In the application of
   shortwave infrared imaging, InView has developed cameras based on the CS
   single-pixel camera architecture. This architecture is comprised of an
   objective lens to image the scene onto a Texas Instruments DLP (R)
   Micromirror Device (DMD), which by using its individually controllable
   mirrors, modulates the image with a selected basis set. The intensity of
   the modulated image is then recorded by a single detector.
   While the design of a CS camera is straightforward conceptually, its
   commercial implementation requires significant development effort in
   optics, electronics, hardware and software, particularly if high
   efficiency and high-speed operation are required. In this paper, we
   describe the development of a high-speed CS engine as implemented in a
   lab-ready workstation. In this engine, configurable measurement patterns
   are loaded into the DMD at speeds up to 31.5 kHz. The engine supports
   custom reconstruction algorithms that can be quickly implemented. Our
   work includes optical path design, Field programmable Gate Arrays for
   DMD pattern generation, and circuit boards for front end data
   acquisition, ADC and system control, all packaged in a compact
   workstation.},
DOI = {10.1117/12.2024148},
Article-Number = {885616},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-0-8194-9706-2},
Unique-ID = {WOS:000325977200040},
}

@article{ WOS:000325401800001,
Author = {Wheelock, Asa M. and Wheelock, Craig E.},
Title = {Trials and tribulations of `omics data analysis: assessing quality of
   SIMCA-based multivariate models using examples from pulmonary medicine},
Journal = {MOLECULAR BIOSYSTEMS},
Year = {2013},
Volume = {9},
Number = {11},
Pages = {2589-2596},
Abstract = {Respiratory diseases are multifactorial heterogeneous diseases that have
   proved recalcitrant to understanding using focused molecular techniques.
   This trend has led to the rise of `omics approaches (e.g.,
   transcriptomics, proteomics) and subsequent acquisition of large-scale
   datasets consisting of multiple variables. In `omics technology-based
   investigations, discrepancies between the number of variables analyzed
   (e.g., mRNA, proteins, metabolites) and the number of study subjects
   constitutes a major statistical challenge. The application of
   traditional univariate statistical methods (e.g., t-test) to these
   ``short-and-wide'' datasets may result in high numbers of false
   positives, while the predominant approach of p-value correction to
   account for these high false positive rates (e.g., FDR, Bonferroni) are
   associated with significant losses in statistical power. In other words,
   the benefit in decreased false positives must be counterbalanced with a
   concomitant loss in true positives. As an alternative, multivariate
   statistical analysis (MVA) is increasingly being employed to cope with
   `omics-based data structures. When properly applied, MVA approaches can
   be powerful tools for integration and interpretation of complex
   `omics-based datasets towards the goal of identifying biomarkers and/or
   subphenotypes. However, MVA methods are also prone to
   over-interpretation and misuse. A common software used in biomedical
   research to perform MVA-based analyses is the SIMCA package, which
   includes multiple MVA methods. In this opinion piece, we propose
   guidelines for minimum reporting standards for a SIMCA-based workflow,
   in terms of data preprocessing (e.g., normalization, scaling) and model
   statistics (number of components, R-2, Q(2), and CV-ANOVA p-value).
   Examples of these applications in recent COPD and asthma studies are
   provided. It is expected that readers will gain an increased
   understanding of the power and utility of MVA methods for applications
   in biomedical research.},
DOI = {10.1039/c3mb70194h},
ISSN = {1742-206X},
EISSN = {1742-2051},
ResearcherID-Numbers = {Wheelock, Åsa/B-7904-2017},
ORCID-Numbers = {Wheelock, Åsa/0000-0002-8013-2745},
Unique-ID = {WOS:000325401800001},
}

@inproceedings{ WOS:000321051200057,
Author = {Yin, Zhaoming and Tang, Jijun and Schaeffer, Stephen W. and Bader, David
   A.},
Editor = {Alexandrov, V and Lees, M and Krzhizhanovskaya, V and Dongarra, J and Sloot, PMA},
Title = {Streaming Breakpoint Graph Analytics for Accelerating and Parallelizing
   the Computation of DCJ Median of Three Genomes},
Booktitle = {2013 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE},
Series = {Procedia Computer Science},
Year = {2013},
Volume = {18},
Pages = {561-570},
Note = {13th Annual International Conference on Computational Science (ICCS),
   Barcelona, SPAIN, JUN 05-07, 2013},
Organization = {Univ Amsterdam; Univ Tennessee; Nanyang Technol Univ; Barcelona Super
   Comp Ctr},
Abstract = {The problem of finding the median of three genomes is the key process in
   building the most parsimonious phylogenetic trees from genome
   rearrangement data. The median problem using Double-Cut-and-Join (DCJ)
   distance is NP-hard and the best exact algorithm is based on a
   branch-and-bound best-first search strategy to explore sub-graph
   patterns in Multiple BreakPoint Graph (MBG). In this paper, by taking
   advantage of the ``streaming{''} property of MBG, we introduce the
   ``footprint-based{''} data structure to reduce the space requirement of
   a single search nodes from O(v(2)) to O(v); minimize the redundant
   computation in counting cycles/paths to update bounds, which leads to
   dramatically decrease of workload of a single search node. Additional
   heuristic of branching strategy is introduced to help reducing the
   searching space. Last but not least, the introduction of a multi-thread
   shared memory parallel algorithm with two load balancing strategies
   bring in additional benefit by distributing search work efficiently
   among different processors. We conduct extensive experiments on
   simulated datasets and our results show significant improvement on all
   datasets. And we test our DCJ median algorithm with GASTS, a state of
   the art software phylogenetic tree construction package. On the real
   high resolution Drosophila data set, our exact algorithm run as fast as
   the heuristic algorithm and help construct a better phylogenetic tree.},
DOI = {10.1016/j.procs.2013.05.220},
ISSN = {1877-0509},
ResearcherID-Numbers = {Tang, Jijun/ABH-6528-2020
   Bader, David/AAI-4029-2021
   Schaeffer, Stephen/AAA-2303-2021},
ORCID-Numbers = {Bader, David/0000-0002-7380-5876
   Schaeffer, Stephen/0000-0003-2070-5342},
Unique-ID = {WOS:000321051200057},
}

@inproceedings{ WOS:000345322600004,
Author = {Zakharova, Tetiana and Moskalenko, Valentyna},
Editor = {Mayr, HC and Kop, C and Liddle, S and Ginige, A},
Title = {Information Technology for the Decision-Making Process in an Investment
   Company},
Booktitle = {INFORMATION SYSTEMS: METHODS, MODELS, AND APPLICATIONS, UNISCON 2012},
Series = {Lecture Notes in Business Information Processing},
Year = {2013},
Volume = {137},
Pages = {37-48},
Note = {4th International United Information Systems Conference (UNISCON),
   Crimean State Humanitarian Univ, Yalta, UKRAINE, JUN 01-03, 2012},
Organization = {BOC Grp; Alpen Adria Univ, Dept Appl Informat},
Abstract = {This paper covers one of the directions of the Investment Company
   functioning, namely investments in business projects of the national
   economy. The information technology in order to justify and make
   investment decisions has been developed. This technology is the complex
   of models and techniques of analysis of the investment market
   conditions, formation of the investment policy, evaluation of
   effectiveness and risks of investment directions and projects,
   construction of investment portfolio. The implementation of the Decision
   Support System for the Investment Company is considered. Main functions
   and requirements of the system are presented. The software solution
   structure based on service-oriented architecture (SOA) is considered.
   The description of packages, interfaces and their interaction is
   described. The developed application is a complete solution for
   operating with the internal and external data of the Investment Company,
   obtaining the results of assessment and report generation.},
ISSN = {1865-1348},
ISBN = {978-3-642-38370-0; 978-3-642-38369-4},
ResearcherID-Numbers = {Moskalenko, Valentyna/R-9960-2018},
ORCID-Numbers = {Moskalenko, Valentyna/0000-0002-9994-5404},
Unique-ID = {WOS:000345322600004},
}

@article{ WOS:000312987200016,
Author = {Numnark, Somrak and Mhuantong, Wuttichai and Ingsriswang, Supawadee and
   Wichadakul, Duangdao},
Title = {C-mii: a tool for plant miRNA and target identification},
Journal = {BMC GENOMICS},
Year = {2012},
Volume = {13},
Number = {7},
Month = {DEC 13},
Note = {Asia Pacific Bioinformatics Network (APBioNet) 11th International
   Conference on Bioinformatics (InCoB), Thai Govt Natl Ctr Genet Engn \&
   Biotechnol (BIOTEC), Bangkok, THAILAND, OCT 03-05, 2012},
Organization = {Asia Pacific Bioinformat Network (APBioNet); King Mongkuts Univ Technol
   Thonburi (KMUTT)},
Abstract = {Background: MicroRNAs (miRNAs) have been known to play an important role
   in several biological processes in both animals and plants. Although
   several tools for miRNA and target identification are available, the
   number of tools tailored towards plants is limited, and those that are
   available have specific functionality, lack graphical user interfaces,
   and restrict the number of input sequences. Large-scale computational
   identifications of miRNAs and/or targets of several plants have been
   also reported. Their methods, however, are only described as flow
   diagrams, which require programming skills and the understanding of
   input and output of the connected programs to reproduce.
   Results: To overcome these limitations and programming complexities, we
   proposed C-mii as a ready-made software package for both plant miRNA and
   target identification. C-mii was designed and implemented based on
   established computational steps and criteria derived from previous
   literature with the following distinguishing features. First, software
   is easy to install with all-in-one programs and packaged databases.
   Second, it comes with graphical user interfaces (GUIs) for ease of use.
   Users can identify plant miRNAs and targets via step-by-step execution,
   explore the detailed results from each step, filter the results
   according to proposed constraints in plant miRNA and target biogenesis,
   and export sequences and structures of interest. Third, it supplies
   bird's eye views of the identification results with infographics and
   grouping information. Fourth, in terms of functionality, it extends the
   standard computational steps of miRNA target identification with
   miRNA-target folding and GO annotation. Fifth, it provides helper
   functions for the update of pre-installed databases and automatic
   recovery. Finally, it supports multi-project and multi-thread
   management.
   Conclusions: C-mii constitutes the first complete software package with
   graphical user interfaces enabling computational identification of both
   plant miRNA genes and miRNA targets. With the provided functionalities,
   it can help accelerate the study of plant miRNAs and targets, especially
   for small and medium plant molecular labs without bioinformaticians.
   C-mii is freely available at http://www.biotec.or.th/isl/c-mii for both
   Windows and Ubuntu Linux platforms.},
DOI = {10.1186/1471-2164-13-S7-S16},
Article-Number = {S16},
ISSN = {1471-2164},
ORCID-Numbers = {Wichadakul, Duangdao/0000-0002-8802-2524},
Unique-ID = {WOS:000312987200016},
}

@article{ WOS:000314138900072,
Author = {Kuehner, Georg and Bluhm, Torsten and Heimann, Peter and Hennig,
   Christine and Kroiss, Hugo and Krom, Jon and Laqua, Heike and Lewerentz,
   Marc and Maier, Josef and Schacht, Joerg and Spring, Anett and Werner,
   Andreas and Zilker, Manfred},
Title = {Progress on standardization and automation in software development on
   W7X},
Journal = {FUSION ENGINEERING AND DESIGN},
Year = {2012},
Volume = {87},
Number = {12, SI},
Pages = {2232-2237},
Month = {DEC},
Note = {8th IAEA Technical Meeting on Control, Data Acquisition, and Remote
   Participation for Fusion Research, San Francisco, CA, JUN 20-24, 2011},
Abstract = {For a complex experiment like W7X being subject to changes all along its
   projected lifetime the advantages of a formalized software development
   method have already been stated {[}1]. Quality standards like
   ISO/IEC-12207 provide a guideline for structuring of development work
   and improving process and product quality. A considerable number of
   tools has emerged supporting and automating parts of development work.
   On W7X progress has been made during the last years in exploiting the
   benefit of automation and management during software development:
   - Continuous build, integration and automated test of software
   artefacts.
   Syntax checks and code quality metrics.
   Documentation generation.
   Feedback for developers by temporal statistics.
   - Versioned repository for build products (libraries, executables).
   - Separate snapshot and release repositories and automatic deployment.
   - Semi-automatic provisioning of applications.
   - Feedback from testers and feature requests by ticket system.
   This toolset is working efficiently and allows the team to concentrate
   on development. The activity there is presently focused on increasing
   the quality of the existing software to become a dependable product.
   Testing of single functions and qualities must be simplified. So a
   restructuring is underway which relies more on small, individually
   testable components with standardized interfaces providing the
   capability to construct arbitrary function aggregates for dedicated
   tests of quality attributes as availability, reliability, performance.
   A further activity is on improving the development cycle. The use of
   release cycles has already provided favourable concentration of work and
   predictability of delivery times. However, the demand has risen, to
   react quickly on priority changes from W7X-project management. So a more
   agile development cycle is being prepared relying on smaller working
   packages, shorter release cycles and an associated release plan giving
   the software development responsible the possibility to react on a
   shorter time scale. (c) 2012 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.fusengdes.2012.06.003},
ISSN = {0920-3796},
Unique-ID = {WOS:000314138900072},
}

@article{ WOS:000313429600009,
Author = {Mortier, Jeremie and Rakers, Christin and Frederick, Raphael and Wolber,
   Gerhard},
Title = {Computational Tools for In Silico Fragment-Based Drug Design},
Journal = {CURRENT TOPICS IN MEDICINAL CHEMISTRY},
Year = {2012},
Volume = {12},
Number = {17},
Pages = {1935-1943},
Month = {SEP},
Abstract = {Fragment-based strategy in drug design involves the initial discovery of
   low-molecular mass molecules. Owing to their small-size, fragments are
   molecular tools to probe specific sub-pockets within a protein active
   site. Once their interaction within the enzyme cavity is clearly
   understood and experimentally validated, they represent a unique
   opportunity to design potent and efficient larger compounds.
   Computer-aided methods can essentially support the identification of
   suitable fragments. In this review, available tools for computational
   drug design are discussed in the frame of fragment-based approaches. We
   analyze and review (i) available commercial fragment libraries with
   respect to their properties and size, (ii) computational methods for the
   construction of such a library, (iii) the different strategies and
   software packages for the selection of the fragments with predicted
   affinity to a given target, and (iv) tools for the in silico linkage of
   fragments into an actual high-affinity lead structure candidate.},
DOI = {10.2174/156802612804547371},
ISSN = {1568-0266},
EISSN = {1873-5294},
ResearcherID-Numbers = {Wolber, Gerhard/ABD-4955-2020
   Mortier, Jérémie/J-7134-2012
   Wolber, Gerhard/C-7133-2008
   },
ORCID-Numbers = {Wolber, Gerhard/0000-0002-5344-0048
   Mortier, Jérémie/0000-0001-8707-3867
   Wolber, Gerhard/0000-0002-5344-0048
   Frederick, Raphael/0000-0001-8119-1272
   Rakers, Christin/0000-0002-5668-6844},
Unique-ID = {WOS:000313429600009},
}

@article{ WOS:000308337800012,
Author = {Tessema, Alemayehu Tadesse and Ayane, Amene Tesfaye and Wabe, Nasir
   Tajure},
Title = {Indicator-Based Assessment on Antimalarial Drug Availability and
   Utilization Among Selected Public Health Facilities in Southwest
   Ethiopia},
Journal = {DRUG INFORMATION JOURNAL},
Year = {2012},
Volume = {46},
Number = {5},
Pages = {587-592},
Month = {SEP},
Abstract = {Malaria is one of the most important causes of morbidity and mortality
   in tropical and subtropical countries. The availability, appropriate
   management, and rational use of medicines are critical to the successful
   implementation of the malaria control programs. The aim was to assess
   the availability, use, and utilization of medicines used for preventing
   and treating malaria in public health facilities in Jimma zone in
   Southwest Ethiopia. An indicator-based descriptive cross-sectional study
   was conducted from March 25 to April 30, 2011 at selected public health
   facilities, providing antimalaria drugs to treat and prevent malaria in
   Jimma zone. Three data collection techniques were used: document
   reviews, structured interviews, and physical inventory checks using the
   World Health Organization Checklist. All data collected were then
   analyzed using the Statistical Package for the Social Sciences (SPSS),
   version 16.0 software. On average, 88\% of the core medicines monitored
   was available in the public health facilities and 90\% of the medicines
   prescribed by the prescriber were dispensed to the patients. On average,
   stock out period was 38\%. The highest and lowest prescribed antimalaria
   drugs observed are Artemether/lumefantrine 120 mg/20 mg tablet and
   chloroquine syrup, which were 45\% and 5.5\%, respectively. The
   adherence to Standard Treatment Guidelines of Ethiopia (STG) was 85\%.
   There was poor inventory control system and long stock out period, and
   the majority of prescribers are adhering to national STGs. Implementing
   good inventory control system, training on drug supply management, and
   continuous supervision of the public health facilities by zonal health
   bureau is recommended.},
DOI = {10.1177/0092861512452122},
ISSN = {0092-8615},
ResearcherID-Numbers = {Ayane, Amene Tesfaye/S-1456-2019
   Wabe, Nasir/W-9089-2019},
ORCID-Numbers = {Ayane, Amene Tesfaye/0000-0003-0482-8895
   Wabe, Nasir/0000-0002-9740-6319},
Unique-ID = {WOS:000308337800012},
}

@article{ WOS:000305958000013,
Author = {Hills, Adrian and Chen, Zhong-Hua and Amtmann, Anna and Blatt, Michael
   R. and Lew, Virgilio L.},
Title = {OnGuard, a Computational Platform for Quantitative Kinetic Modeling of
   Guard Cell Physiology},
Journal = {PLANT PHYSIOLOGY},
Year = {2012},
Volume = {159},
Number = {3},
Pages = {1026-1042},
Month = {JUL},
Abstract = {Stomatal guard cells play a key role in gas exchange for photosynthesis
   while minimizing transpirational water loss from plants by opening and
   closing the stomatal pore. Foliar gas exchange has long been
   incorporated into mathematical models, several of which are robust
   enough to recapitulate transpirational characteristics at the
   whole-plant and community levels. Few models of stomata have been
   developed from the bottom up, however, and none are sufficiently
   generalized to be widely applicable in predicting stomatal behavior at a
   cellular level. We describe here the construction of computational
   models for the guard cell, building on the wealth of biophysical and
   kinetic knowledge available for guard cell transport, signaling, and
   homeostasis. The OnGuard software was constructed with the HoTSig
   library to incorporate explicitly all of the fundamental properties for
   transporters at the plasma membrane and tonoplast, the salient features
   of osmolite metabolism, and the major controls of cytosolic-free Ca2+
   concentration and pH. The library engenders a structured approach to
   tier and interrelate computational elements, and the OnGuard software
   allows ready access to parameters and equations `on the fly' while
   enabling the network of components within each model to interact
   computationally. We show that an OnGuard model readily achieves
   stability in a set of physiologically sensible baseline or Reference
   States; we also show the robustness of these Reference States in
   adjusting to changes in environmental parameters and the activities of
   major groups of transporters both at the tonoplast and plasma membrane.
   The following article addresses the predictive power of the OnGuard
   model to generate unexpected and counterintuitive outputs.},
DOI = {10.1104/pp.112.197244},
ISSN = {0032-0889},
ResearcherID-Numbers = {Chen, Zhong-Hua/B-8927-2008
   },
ORCID-Numbers = {Chen, Zhong-Hua/0000-0002-7531-320X
   Blatt, Michael R/0000-0003-1361-4645
   Amtmann, Anna/0000-0001-8533-121X},
Unique-ID = {WOS:000305958000013},
}

@article{ WOS:000306198700026,
Author = {Takeuchi, Hironori and Nakamura, Taiga and Yamaguchi, Takahira},
Title = {Predicate Argument Structure Analysis for Use Case Description Modeling},
Journal = {IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS},
Year = {2012},
Volume = {E95D},
Number = {7},
Pages = {1959-1968},
Month = {JUL},
Abstract = {In a large software system development project, many documents are
   prepared and updated frequently. In such a situation, support is needed
   for looking through these documents easily to identify inconsistencies
   and to maintain traceability. In this research, we focus on the
   requirements documents such as use cases and consider how to create
   models from the use case descriptions in unformatted text. In the model
   construction, we propose a few semantic constraints based on the
   features of the use cases and use them for a predicate argument
   structure analysis to assign semantic labels to actors and actions. With
   this approach, we show that we can assign semantic labels without
   enhancing any existing general lexical resources such as case frame
   dictionaries and design a less language-dependent model construction
   architecture. By using the constructed model, we consider a system for
   quality analysis of the use cases and automated test case generation to
   keep the traceability between document sets. We evaluated the reuse of
   the existing use cases and generated test case steps automatically with
   the proposed prototype system from real-world use cases in the
   development of a system using a packaged application. Based on the
   evaluation, we show how to construct models with high precision from
   English and Japanese use case data. Also, we could generate good test
   cases for about 90\% of the real use cases through the manual
   improvement of the descriptions based on the feedback from the quality
   analysis system.},
DOI = {10.1587/transinf.E95.D.1959},
ISSN = {1745-1361},
Unique-ID = {WOS:000306198700026},
}

@article{ WOS:000303094300003,
Author = {Tych, W. and Young, P. C.},
Title = {A Matlab software framework for dynamic model emulation},
Journal = {ENVIRONMENTAL MODELLING \& SOFTWARE},
Year = {2012},
Volume = {34},
Number = {SI},
Pages = {19-29},
Month = {JUN},
Abstract = {The paper describes a software framework for implementing the main
   stages of the Data Based Mechanistic (DBM) modelling approach to the
   reduced order emulation (meta-modelling) of large dynamic system
   computer models, within the Matlab software environment. The framework
   exploits routines in the CAPTAIN Toolbox to identify and estimate
   transfer function models that reflect the dominant modes of the dynamic
   behaviour in the large model. This allows for the `nominal emulation'
   and validation of the large model for a single, specified set of
   parameters: as well as `stand-alone, full emulation' based on the
   construction and validation of hyper-dimensional maps between a
   user-specified range of large model parameters and the parameters of the
   associated, low order transfer function models. The software framework
   uses the multivariable structure constructs available within Matlab (TM)
   to form a small library of routines that will become part of the Captain
   Toolbox. The library is formed around special data structures that
   facilitate multivariable operations and visualisations which both
   enhance the efficiency of the emulation modelling analysis and the
   modeller's interaction with the process of emulation. The nature of the
   analysis is illustrated by a topical example concerned with the
   emulation of the OTIS computer simulation model for the transport and
   dispersion of solutes in a river system. (C) 2011 Elsevier Ltd. All
   rights reserved.},
DOI = {10.1016/j.envsoft.2011.08.008},
ISSN = {1364-8152},
EISSN = {1873-6726},
ResearcherID-Numbers = {Tych, Wlodek/K-6510-2012},
ORCID-Numbers = {Tych, Wlodek/0000-0003-1655-844X},
Unique-ID = {WOS:000303094300003},
}

@article{ WOS:000307756400001,
Author = {Zhou, Sun and Ji, Guoli and Liu, Xiaolin and Li, Pei and Moler, James
   and Karro, John E. and Liang, Chun},
Title = {Pattern analysis approach reveals restriction enzyme cutting
   abnormalities and other cDNA library construction artifacts using raw
   EST data},
Journal = {BMC BIOTECHNOLOGY},
Year = {2012},
Volume = {12},
Month = {MAY 3},
Abstract = {Background: Expressed Sequence Tag (EST) sequences are widely used in
   applications such as genome annotation, gene discovery and gene
   expression studies. However, some of GenBank dbEST sequences have proven
   to be ``unclean{''}. Identification of cDNA termini/ends and their
   structures in raw ESTs not only facilitates data quality control and
   accurate delineation of transcription ends, but also furthers our
   understanding of the potential sources of data abnormalities/errors
   present in the wet-lab procedures for cDNA library construction.
   Results: After analyzing a total of 309,976 raw Pinus taeda ESTs, we
   uncovered many distinct variations of cDNA termini, some of which prove
   to be good indicators of wet-lab artifacts, and characterized each raw
   EST by its cDNA terminus structure patterns. In contrast to the expected
   patterns, many ESTs displayed complex and/or abnormal patterns that
   represent potential wet-lab errors such as: a failure of one or both of
   the restriction enzymes to cut the plasmid vector; a failure of the
   restriction enzymes to cut the vector at the correct positions; the
   insertion of two cDNA inserts into a single vector; the insertion of
   multiple and/or concatenated adapters/linkers; the presence of 3'-end
   terminal structures in designated 5'-end sequences or vice versa; and so
   on. With a close examination of these artifacts, many problematic ESTs
   that have been deposited into public databases by conventional
   bioinformatics pipelines or tools could be cleaned or filtered by our
   methodology. We developed a software tool for Abnormality Filtering and
   Sequence Trimming for ESTs (AFST, http://code.google.com/p/afst/) using
   a pattern analysis approach. To compare AFST with other pipelines that
   submitted ESTs into dbEST, we reprocessed 230,783 Pinus taeda and 38,709
   Arachis hypogaea GenBank ESTs. We found 7.4\% of Pinus taeda and 29.2\%
   of Arachis hypogaea GenBank ESTs are ``unclean{''} or abnormal, all of
   which could be cleaned or filtered by AFST.
   Conclusions: cDNA terminal pattern analysis, as implemented in the AFST
   software tool, can be utilized to reveal wet-lab errors such as
   restriction enzyme cutting abnormities and chimeric EST sequences,
   detect various data abnormalities embedded in existing Sanger EST
   datasets, improve the accuracy of identifying and extracting bona fide
   cDNA inserts from raw ESTs, and therefore greatly benefit downstream
   EST-based applications.},
DOI = {10.1186/1472-6750-12-16},
Article-Number = {16},
ISSN = {1472-6750},
ResearcherID-Numbers = {Ji, GL/G-3355-2010
   },
ORCID-Numbers = {Zhou, Sun/0000-0003-0788-0955},
Unique-ID = {WOS:000307756400001},
}

@article{ WOS:000303938900001,
Author = {Ye, Chengxi and Ma, Zhanshan Sam and Cannon, Charles H. and Pop, Mihai
   and Yu, Douglas W.},
Title = {Exploiting sparseness in de novo genome assembly},
Journal = {BMC BIOINFORMATICS},
Year = {2012},
Volume = {13},
Number = {6},
Month = {APR 19},
Note = {2nd Annual RECOMB Satellite Workshop on Massively Parallel Sequencing
   (RECOMB-seq), Barcelona, SPAIN, APR 19-20, 2012},
Abstract = {Background: The very large memory requirements for the construction of
   assembly graphs for de novo genome assembly limit current algorithms to
   super-computing environments.
   Methods: In this paper, we demonstrate that constructing a sparse
   assembly graph which stores only a small fraction of the observed k-mers
   as nodes and the links between these nodes allows the de novo assembly
   of even moderately-sized genomes (similar to 500 M) on a typical laptop
   computer.
   Results: We implement this sparse graph concept in a proof-of-principle
   software package, SparseAssembler, utilizing a new sparse k-mer graph
   structure evolved from the de Bruijn graph. We test our SparseAssembler
   with both simulated and real data, achieving similar to 90\% memory
   savings and retaining high assembly accuracy, without sacrificing speed
   in comparison to existing de novo assemblers.},
DOI = {10.1186/1471-2105-13-S6-S1},
Article-Number = {S1},
ISSN = {1471-2105},
ResearcherID-Numbers = {Pop, Mihai/A-7987-2013
   Cannon, Charles H/D-7186-2012
   Ma, Zhanshan (Sam)/AAZ-3896-2020
   Yu, Douglas W./D-2536-2009
   Cannon, Charles/AAE-7715-2019},
ORCID-Numbers = {Pop, Mihai/0000-0001-9617-5304
   Cannon, Charles H/0000-0003-1901-0420
   Yu, Douglas W./0000-0001-8551-5609
   Cannon, Charles/0000-0003-1901-0420},
Unique-ID = {WOS:000303938900001},
}

@article{ WOS:000303938200002,
Author = {Pirola, Yuri and Rizzi, Raffaella and Picardi, Ernesto and Pesole,
   Graziano and Della Vedova, Gianluca and Bonizzoni, Paola},
Title = {PIntron: a fast method for detecting the gene structure due to
   alternative splicing via maximal pairings of a pattern and a text},
Journal = {BMC BIOINFORMATICS},
Year = {2012},
Volume = {13},
Number = {5},
Month = {APR 12},
Note = {1st IEEE International Conference on Computational Advances in Bio and
   Medical Sciences (ICCABS), Orlando, FL, FEB 03-05, 2011},
Organization = {IEEE},
Abstract = {Background: A challenging issue in designing computational methods for
   predicting the gene structure into exons and introns from a cluster of
   transcript (EST, mRNA) sequences, is guaranteeing accuracy as well as
   efficiency in time and space, when large clusters of more than 20,000
   ESTs and genes longer than 1 Mb are processed. Traditionally, the
   problem has been faced by combining different tools, not specifically
   designed for this task.
   Results: We propose a fast method based on ad hoc procedures for solving
   the problem. Our method combines two ideas: a novel algorithm of proved
   small time complexity for computing spliced alignments of a transcript
   against a genome, and an efficient algorithm that exploits the inherent
   redundancy of information in a cluster of transcripts to select, among
   all possible factorizations of EST sequences, those allowing to infer
   splice site junctions that are largely confirmed by the input data. The
   EST alignment procedure is based on the construction of maximal
   embeddings, that are sequences obtained from paths of a graph structure,
   called embedding graph, whose vertices are the maximal pairings of a
   genomic sequence T and an EST P. The procedure runs in time linear in
   the length of P and T and in the size of the output. The method was
   implemented into the PIntron package. PIntron requires as input a
   genomic sequence or region and a set of EST and/or mRNA sequences.
   Besides the prediction of the full-length transcript isoforms
   potentially expressed by the gene, the PIntron package includes a module
   for the CDS annotation of the predicted transcripts.
   Conclusions: PIntron, the software tool implementing our methodology, is
   available at http://www.algolab.eu/PIntron under GNU AGPL. PIntron has
   been shown to outperform state-of-the-art methods, and to quickly
   process some critical genes. At the same time, PIntron exhibits high
   accuracy (sensitivity and specificity) when benchmarked with ENCODE
   annotations.},
DOI = {10.1186/1471-2105-13-S5-S2},
Article-Number = {S2},
ISSN = {1471-2105},
ResearcherID-Numbers = {Pesole, Graziano/K-5283-2019
   Della Vedova, Gianluca/AAW-1572-2021
   Pesole, Graziano/C-1408-2009
   Picardi, Ernesto/A-5863-2015
   Pirola, Yuri/AAD-7938-2020
   Pesole, Graziano/E-9051-2014
   },
ORCID-Numbers = {Della Vedova, Gianluca/0000-0001-5584-3089
   Pesole, Graziano/0000-0003-3663-0859
   Picardi, Ernesto/0000-0002-6549-0114
   Pirola, Yuri/0000-0002-8479-7592
   Pesole, Graziano/0000-0003-3663-0859
   BONIZZONI, PAOLA/0000-0001-7289-4988},
Unique-ID = {WOS:000303938200002},
}

@article{ WOS:000303230900003,
Author = {Barriuso, Jorge and Mellado, Rafael P.},
Title = {Glyphosate affects the rhizobacterial communities in glyphosate-tolerant
   cotton},
Journal = {APPLIED SOIL ECOLOGY},
Year = {2012},
Volume = {55},
Pages = {20-26},
Month = {APR},
Abstract = {The use of herbicides to kill undesirable weeds is an important element
   of agricultural management that can greatly alter soil characteristics.
   Moreover, the composition of rhizobacterial communities varies according
   to the soil texture. The effect of glyphosate, a post-emergence applied
   herbicide, on the rhizobacterial communities of genetically modified
   GHB614, a glyphosate-tolerant cotton, was evaluated in two different
   agricultural fields, one with clayey soil and the other with clayey-loam
   soil texture. The potential effect was monitored at two different
   sampling times (7 days after glyphosate application and just before crop
   harvesting) by high throughput DNA pyrosequencing of rhizobacterial DNA
   coding for the 16SrRNA hypervariable V6 region. The taxonomic analysis
   indicated that Proteobacteria, Acidobacteria and Actinobacteria were the
   more abundant taxa in both fields, although the UniFrac phylogenetic
   analysis differentiated one field from the other. To analyse bacterial
   diversity. MUSCLE alignment, DNADIST distance calculation and Mothur
   clustering were compared with the ESPRIT software package and both
   approaches gave consistent results. Thus, rhizobacterial diversity was
   apparently higher in the clayey soil than in the clayey-loam, judging
   from the OTUs and diversity index estimates. The glyphosate treatment,
   in general, does not seem to greatly affect the structure of bacterial
   communities in the cotton rhizosphere. However, the degree of recovery
   of the soil bacterial communities throughout plant growth was apparently
   less effective in the clayey-loam field than in the clayey one, strongly
   suggesting that recovery does indeed greatly depend on the soil textures
   and their associated bacterial community diversity. (C) 2011 Elsevier
   B.V. All rights reserved.},
DOI = {10.1016/j.apsoil.2011.12.010},
ISSN = {0929-1393},
EISSN = {1873-0272},
ResearcherID-Numbers = {Barriuso, Jorge/AAF-6118-2020
   Mellado, Rafael P/D-2680-2009
   },
ORCID-Numbers = {Barriuso, Jorge/0000-0003-0916-6560},
Unique-ID = {WOS:000303230900003},
}

@article{ WOS:000301028700020,
Author = {Casperson, R. J.},
Title = {IBAR: Interacting boson model calculations for large system sizes},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2012},
Volume = {183},
Number = {4},
Pages = {1029-1035},
Month = {APR},
Abstract = {Scaling the system size of the interacting boson model-1 (IBM-1) into
   the realm of hundreds of bosons has many interesting applications in the
   field of nuclear structure, most notably quantum phase transitions in
   nuclei. We introduce IBAR, a new software package for calculating the
   eigenvalues and eigenvectors of the IBM-1 Hamiltonian, for large numbers
   of bosons. Energies and wavefunctions of the nuclear states, as well as
   transition strengths between them, are calculated using these values.
   Numerical errors in the recursive calculation of reduced matrix elements
   of the d-boson creation operator are reduced by using an arbitrary
   precision mathematical library. This software has been tested for up to
   1000 bosons using comparisons to analytic expressions. Comparisons have
   also been made to the code PHINT for smaller system sizes.
   Program summary
   Program title: IBAR
   Catalogue identifier: AELI\_v1\_0
   Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AELI\_v1\_0.html
   Program obtainable from: CPC Program Library, Queen's University,
   Belfast, N. Ireland
   Licensing provisions: GNU General Public License version 3
   No. of lines in distributed program, including test data, etc.: 28 734
   No. of bytes in distributed program, including test data, etc.: 4104 467
   Distribution format: tar.gz
   Programming language: C++
   Computer: Any computer system with a C++ compiler
   Operating system: Tested under Linux
   RAM: 150 MB for 1000 boson calculations with angular momenta of up to L
   = 4
   Classification: 17.18, 17.20
   External routines: ARPACK (http://www.caam.rice.edu/software/ARPACK/)
   Nature of problem: Construction and diagonalization of large Hamiltonian
   matrices, using reduced matrix elements of the d-boson creation
   operator.
   Solution method: Reduced matrix elements of the d-boson creation
   operator have been stored in data files at machine precision, after
   being recursively calculated with higher than machine precision. The
   Hamiltonian matrix is calculated and diagonalized, and the requested
   transition strengths are calculated using the eigenvectors.
   Restrictions: The 1000 boson coefficients for L = 0 and L = 20 have been
   included in the IBAR distribution and the 7.3 GB of data that make up
   the remaining coefficients for L = 21 to L = 2000 are available upon
   request.
   Running time: If the provided example is changed to include 100 bosons,
   the calculation requires about 1 second of run time. For 1000 bosons,
   the calculation requires about 9 minutes of run time. (C) 2012 Elsevier
   B.V. All rights reserved.},
DOI = {10.1016/j.cpc.2011.12.024},
ISSN = {0010-4655},
Unique-ID = {WOS:000301028700020},
}

@article{ WOS:000301897400010,
Author = {Gann, David and Salter, Ammon and Dodgson, Mark and Phillips, Nelson},
Title = {Inside the World of the Project Baron},
Journal = {MIT SLOAN MANAGEMENT REVIEW},
Year = {2012},
Volume = {53},
Number = {3},
Pages = {63+},
Month = {SPR},
Abstract = {In industrial sectors such as consulting, advertising, filmmaking,
   software, architecture, engineering and construction - where activities
   tend to be organized through the delivery of projects aimed at meeting
   the highly differentiated and customized needs of clients - most
   individual businesses, by definition, are ``project-based firms.{''}
   They depend on executing discrete task-oriented packages for clients,
   often through temporary coalitions with other project-based
   organizations, and on routinely combining knowledge and skills in new
   ways.
   The authors propose the concept of ``baronies{''} to describe the
   organizational units that direct the projects within project-based
   firms. This metaphor is appropriate because it describes entities that
   often are led by powerful individuals ({''}barons{''}) who exhibit
   competitive, protective and entrepreneurial behaviors. Drawing on over
   200 interviews with project managers and company leaders conducted in 40
   project-based firms in eight countries, this article highlights the
   roles that barons play in three basic types of project-based firms:
   dominions, tight federations and loose federations.
   In dominions, all baronies must respond to strict procedures established
   by the center; while the management of projects is decentralized, the
   power of project managers is limited. In tight federations, by contrast,
   barons have considerable autonomy with respect to project creation and
   execution, even though well-developed central departments place some
   limits on barons' decision-making power. Loose federations are
   characterized by strong barons, weak central control and low levels of
   central services; barons independently bid for and manage their own
   projects and resources.
   The authors examine the management implications of baronies in these
   three basic types of governance, highlighting the trade-offs made by
   firms in three areas of management behavior: synthesizing (transferring
   knowledge and experience across projects); spawning (the creation of new
   entrepreneurial endeavors); and squirreling (a baron's diversion of
   resources, frequently concealed, for discretionary use).},
ISSN = {1532-9194},
ResearcherID-Numbers = {Dodgson, Mark J/A-5953-2008
   Salter, Ammon/A-9217-2010
   },
ORCID-Numbers = {Dodgson, Mark J/0000-0001-8532-946X
   Salter, Ammon/0000-0003-2065-1268
   Phillips, Nelson/0000-0001-6863-2758
   Gann, David/0000-0002-8245-3948},
Unique-ID = {WOS:000301897400010},
}

@article{ WOS:000298984600022,
Author = {Morris, Roisin and Scott, Philomena Anne and Cocoman, Angela and
   Chambers, Mary and Guise, Veslemoy and Valimaki, Maritta and Clinton,
   Gerard},
Title = {Is the Community Attitudes towards the Mentally Ill scale valid for use
   in the investigation of European nurses' attitudes towards the mentally
   ill? A confirmatory factor analytic approach},
Journal = {JOURNAL OF ADVANCED NURSING},
Year = {2012},
Volume = {68},
Number = {2},
Pages = {460-470},
Month = {FEB},
Abstract = {Aim. This study aimed to assess the construct validity of the Community
   Attitudes towards the Mentally Ill scale in the investigation of
   European nurses attitudes towards mental illness and mental health
   patients. Background. The harbouring of negative attitudes by nurses
   towards any patient can have implications for recovery. To gather robust
   evidence upon which to base information and education aimed at fostering
   acceptance, support and general positivity towards people with mental
   health illness, a valid and reliable system of data collection is
   required. Method. A confirmatory factor analysis of both the original
   Community Attitudes towards the Mentally Ill scale and two modified
   versions of the scale were carried out during May - June 2007 using a
   data set representing the responses of 858 European nurses to the scale.
   Data were subjected to three different confirmatory factor analyses
   using Maximum Likelihood estimation in the software package, Analysis of
   Moment Structures 7. A number of absolute, relative and incremental fit
   statistics were used to assess the fit of the original Community
   Attitudes towards the Mentally Ill scale and two modified versions to
   the European nursing data. Findings. A modification of the scale was
   found to be most suitable for use in the investigation of European
   nurses attitudes towards mental illness and people with mental illness.
   Conclusion. Further research is recommended to develop a valid and
   reliable research tool to specifically measure the attitudes of nurses
   working across different mental healthcare facilities towards this
   vulnerable patient group.},
DOI = {10.1111/j.1365-2648.2011.05739.x},
ISSN = {0309-2402},
EISSN = {1365-2648},
ResearcherID-Numbers = {Välimäki, Maritta A/E-7092-2017
   Scott, P Anne/N-3400-2019
   Cocoman, Angela/V-9506-2019},
ORCID-Numbers = {Välimäki, Maritta A/0000-0001-7234-2454
   Scott, P Anne/0000-0002-5790-8066
   Cocoman, Angela/0000-0002-5933-7682},
Unique-ID = {WOS:000298984600022},
}

@inproceedings{ WOS:000312226800012,
Author = {Arino, Javier and Murga, Gaizka and Campo, Ramon and Eletxigerra, Inigo
   and Ampuero, Pedro},
Editor = {Angeli, GZ and Dierickx, P},
Title = {Building Information Models for Astronomy Projects},
Booktitle = {MODELING, SYSTEMS ENGINEERING, AND PROJECT MANAGEMENT FOR ASTRONOMY V},
Series = {Proceedings of SPIE},
Year = {2012},
Volume = {8449},
Note = {Conference on Modeling, Systems Engineering, and Project Management for
   Astronomy V, Amsterdam, NETHERLANDS, JUL 01-03, 2012},
Organization = {SPIE},
Abstract = {A Building Information Model is a digital representation of physical and
   functional characteristics of a building. BIMs represent the geometrical
   characteristics of the Building, but also properties like bills of
   quantities, definition of COTS components, status of material in the
   different stages of the project, project economic data, etc.
   The BIM methodology, which is well established in the Architecture
   Engineering and Construction (AEC) domain for conventional buildings,
   has been brought one step forward in its application for
   Astronomical/Scientific facilities. In these facilities steel/concrete
   structures have high dynamic and seismic requirements, M\&E
   installations are complex and there is a large amount of special
   equipment and mechanisms involved as a fundamental part of the facility.
   The detail design definition is typically implemented by different
   design teams in specialized design software packages. In order to allow
   the coordinated work of different engineering teams, the overall model,
   and its associated engineering database, is progressively integrated
   using a coordination and roaming software which can be used before
   starting construction phase for checking interferences, planning the
   construction sequence, studying maintenance operation, reporting to the
   project office, etc.
   This integrated design \& construction approach will allow to
   efficiently plan construction sequence (4D). This is a powerful tool to
   study and analyze in detail alternative construction sequences and
   ideally coordinate the work of different construction teams.
   In addition engineering, construction and operational database can be
   linked to the virtual model (6D), what gives to the end users a
   invaluable tool for the lifecycle management, as all the facility
   information can be easily accessed, added or replaced.
   This paper presents the BIM methodology as implemented by IDOM with the
   E-ELT and ATST Enclosures as application examples.},
DOI = {10.1117/12.926180},
Article-Number = {84490D},
ISSN = {0277-786X},
ISBN = {978-0-8194-9150-3},
Unique-ID = {WOS:000312226800012},
}

@incollection{ WOS:000303026000003,
Author = {Assif, S. and Agouzoul, M. and El Hami, A. and Bendaou, O. and Gbati, Y.},
Editor = {Karama, M},
Title = {Numerical model to simulate the drop test of printed circuit board (PCB)},
Booktitle = {INNOVATING PROCESSES},
Series = {Advanced Materials Research},
Year = {2012},
Volume = {423},
Pages = {26-30},
Abstract = {Increasing demand for smaller consumer electronic devices with
   multi-function capabilities has driven the packaging architectures
   trends for the finer-pitch interconnects, thus increasing chances of
   their failures.
   A simulation of the Board Level Drop-Test according to JEDEC (Joint
   Electron Device Council) is performed to evaluate the solder joint
   reliability under drop impact test. After good insights to the physics
   of the problem, the results of the numerical analysis on a simple
   Euler-Bernoulli beam were validated against analytical analysis. Since
   the simulation has to be performed on ANSYS Mechanical which is an
   implicit software, two methods were proposed, the acceleration-input and
   the displacement-input. The results are the same for both methods.
   Therefore, the simulation is carried on the real standard model
   construction of the board package level2. Then a new improved model is
   proposed to satisfy shape regular element and accuracy. All the models
   are validated to show excellent first level correlation on the dynamic
   responses of Printed Circuit Board, and second level correlation on
   solder joint stress. Then a static model useful for quick design
   analysis and optimization's works is proposed and validated. Finally,
   plasticity behavior is introduced on the solder ball and a non-linear
   analysis is performed.},
DOI = {10.4028/www.scientific.net/AMR.423.26},
ISSN = {1022-6680},
ISBN = {978-3-03785-329-0},
ResearcherID-Numbers = {EL HAMI, Abdelkhalak/HHZ-6269-2022
   },
ORCID-Numbers = {EL HAMI, Abdelkhalak/0000-0001-8080-7952},
Unique-ID = {WOS:000303026000003},
}

@article{ WOS:000300071300011,
Author = {Bosche, Frederic},
Title = {Plane-based registration of construction laser scans with 3D/4D building
   models},
Journal = {ADVANCED ENGINEERING INFORMATICS},
Year = {2012},
Volume = {26},
Number = {1},
Pages = {90-102},
Month = {JAN},
Abstract = {With the development of building information modelling (BIM) and
   terrestrial laser scanning (TLS) in the architecture, engineering,
   construction and facility management (AEC/FM) industry, the registration
   of site laser scans and project 3D (BIM) models in a common coordinate
   system is becoming critical to effective project control. The
   co-registration of 3D datasets is normally performed in two steps:
   coarse registration followed by fine registration. Focusing on the
   coarse registration, model-scan registration has been well investigated
   in the past, but it is shown in this article that the context of the
   AEC/FM industry presents specific (1) constraints that make
   fully-automated registration very complex and often ill-posed, and (2)
   advantages that can be leveraged to develop simpler yet effective
   registration methods.
   This paper thus presents a novel semi-automated plane-based registration
   system for coarse registration of laser scanned 3D point clouds with
   project 3D models in the context of the AEC/FM industry. The system is
   based on the extraction of planes from the laser scanned point cloud and
   project 3D/4D model. Planes are automatically extracted from the 3D/4D
   model. For the point cloud data, two methods are investigated. The first
   one is fully automated, and the second is a semi-automated but effective
   one-click RANSAC-supported extraction method. In both cases, planes are
   then manually but intuitively matched by the user. Experiments, which
   compare the proposed system to software packages commonly used in the
   AEC/FM industry, demonstrate that at least as good registration quality
   can be achieved by the proposed system, in a simpler and faster way. It
   is concluded that, in the AEC/FM context, the proposed plane-based
   registration system is a compelling alternative to standard point-based
   registration techniques. (C) 2011 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.aei.2011.08.009},
ISSN = {1474-0346},
EISSN = {1873-5320},
ResearcherID-Numbers = {Bosché, Frédéric/AAJ-9117-2020
   },
ORCID-Numbers = {Bosche, Frederic/0000-0002-4064-8982},
Unique-ID = {WOS:000300071300011},
}

@article{ WOS:000304867400005,
Author = {de Souza, Rafael Alves and Ferrari, Vladimir Jose},
Title = {Automatic design of the flexural strengthening of reinforced concrete
   beams using fiber reinforced polymers (FRP)},
Journal = {ACTA SCIENTIARUM-TECHNOLOGY},
Year = {2012},
Volume = {34},
Number = {2},
Pages = {157-165},
Abstract = {Changing the functions of a building, the presence of some design or
   construction errors, the incidence of seismic actions and even the
   updating of design codes may demand the strengthening of certain
   structures. In the specific case of reinforced concrete structures it is
   desirable the application of a technique of strengthening which is fast,
   economic and efficient, in order to provide advantages when an
   intervention is necessary. The technique of strengthening chosen must
   provide less disorder as possible as well as the guaranty of safety.
   Taking into account this scenery, fiber reinforced polymers have been
   working as a very attractive alternative for rehabilitating in-service
   structures. In that way, the present study aims at presenting the main
   properties of this new material as well as the design routines for
   flexural strengthening of reinforced concrete beams. Finally, a
   package-software developed into the MATLAB platform is presented,
   intending to generate a simple tool for the automatic design using fiber
   reinforced polymers.},
DOI = {10.4025/actascitechnol.v34i2.8318},
ISSN = {1806-2563},
ResearcherID-Numbers = {Jose Ferrari, Vladimir/HDO-0733-2022
   de Souza, Rafael Alves/I-4363-2018
   Technology, Acta Scientiarum./AAB-4509-2020},
Unique-ID = {WOS:000304867400005},
}

@inproceedings{ WOS:000327308700327,
Author = {Dobritsky, A. U. and Ilyin, N. A. and Nikonorova, T. V. and Sherstyuk,
   N. E. and Mishina, E. D.},
Book-Group-Author = {Electromagnet Acad},
Title = {Optical Properties of Fotonnokristallicheskih Structures Based on
   Single-crystal GaAs},
Booktitle = {PIERS 2012 MOSCOW: PROGRESS IN ELECTROMAGNETICS RESEARCH SYMPOSIUM},
Series = {Progress in Electromagnetics Research Symposium},
Year = {2012},
Pages = {1479-1483},
Note = {Progress In Electromagnetics Research Symposium (PIERS), Moscow, RUSSIA,
   AUG 19-23, 2012},
Organization = {Moscow State Inst Radio Engn, Elect \& Automat; Russian New Univ; NVK
   VIST; Russian Fdn Basic Res; Swedish Inst; Russian Acad Sci; Zhejiang
   Univ, Electromagnet Acad; Electromagnet Acad},
Abstract = {Photonic crystals (PhC) offer wide opportunities for creation of
   small-type components, which allow to downsize the existing integrated
   circuits. The operation of these devises is based on the presence of
   photonic bandgap. Size and period of photonic crystal determine
   effective range of wavelength for the concrete functional element. The
   creation of PhC structures on the basis of functional materials with the
   characteristics being switched by external electric/magnetic field allow
   to manage the propagation parameters of electromagnetic field inside
   them.
   The majority of research in the sphere of PhC waveguides is based on the
   modeling of their characteristics. Herewith 2 models are used: model of
   dielectric channels in air and model of ordered group of holes in
   functional material. The advantage of the first model waveguides lies in
   the fact that they are single-mode. The literature also offers variants
   of calculation of deflecting and bisected waveguides. But such a
   configuration of the waveguide does not provide the acceptable level of
   losses for the creation of actually functioning prototypes, working in
   the optical range. PhC structures, based on the second model are much
   more diverse and allow to create vaweguides of more complicated
   construction, for example, optical T- and Y-couplers, interferometers
   and so on.
   This work presents the result of the systematical research using the
   method of numerical simulation of transmission spectra for PhC
   waveguides with different type of ordering and their characterization by
   scanning near-field optical microscopy (SNOM).
   The modeling of the characteristics of 2D PhC structures was carried out
   with the help of software package CST Studio Suite. The following
   parameters were used for modeling: wavelength of 900 2000 nm, ratio of
   period of the structure to wavelength 0.24 <= a/lambda <= 0.42; ratio of
   hole diameter to period 0.5 <= d/a <= 0.9. Photonic crystals based on
   silicon and gallium arsenide with quadratic and hexagonal ordering were
   investigated.},
ISSN = {1559-9450},
ISBN = {978-1-934142-22-6},
ResearcherID-Numbers = {Sherstyuk, Natalia E./A-3460-2014
   Mishina, Elena D/D-6402-2014
   Ilyin, Nikita/F-4240-2017},
ORCID-Numbers = {Sherstyuk, Natalia E./0000-0002-7068-4028
   Mishina, Elena D/0000-0003-0387-5016
   },
Unique-ID = {WOS:000327308700327},
}

@incollection{ WOS:000299709500006,
Author = {Fulle, Simone and Gohlke, Holger},
Editor = {Baron, R},
Title = {Flexibility Analysis of Biomacromolecules with Application to
   Computer-Aided Drug Design},
Booktitle = {COMPUTATIONAL DRUG DISCOVERY AND DESIGN},
Series = {Methods in Molecular Biology},
Year = {2012},
Volume = {819},
Pages = {75-91},
Abstract = {Flexibility characteristics of biomacromolecules can be efficiently
   determined down to the atomic level by a graph-theoretical technique as
   implemented in the FIRST (Floppy Inclusion and Rigid Substructure
   Topology) and ProFlex software packages. The method has been
   successfully applied to a series of protein and nucleic acid structures.
   Here, we describe practical guidelines for setting up and performing a
   flexibility analysis, discuss current bottlenecks of the approach, and
   provide sample applications as to how this technique can support
   computer-aided drug design approaches.},
DOI = {10.1007/978-1-61779-465-0\_6},
ISSN = {1064-3745},
EISSN = {1940-6029},
ISBN = {978-1-61779-464-3},
Unique-ID = {WOS:000299709500006},
}

@inproceedings{ WOS:000309406703091,
Author = {Jing, Gangyuan and Finucane, Cameron and Raman, Vasumathi and
   Kress-Gazit, Hadas},
Book-Group-Author = {IEEE},
Title = {Correct High-level Robot Control from Structured English},
Booktitle = {2012 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
Series = {IEEE International Conference on Robotics and Automation ICRA},
Year = {2012},
Pages = {3543-3544},
Note = {IEEE International Conference on Robotics and Automation (ICRA), St
   Paul, MN, MAY 14-18, 2012},
Organization = {IEEE},
Abstract = {The Linear Temporal Logic MissiOn Planning (LTLMoP) toolkit is a
   software package designed to generate a controller that guarantees a
   robot satisfies a task specification written by the user in structured
   English. The controller can be implemented on either a simulated or
   physical robot. This video illustrates the use of LTLMoP to generate a
   correct-by-construction robot controller. Here, an Aldebaran Nao
   humanoid robot carries out tasks as a worker in a simplified grocery
   store scenario.},
ISSN = {1050-4729},
EISSN = {2577-087X},
ISBN = {978-1-4673-1405-3},
ResearcherID-Numbers = {Kress-Gazit, Hadas/A-4246-2018},
ORCID-Numbers = {Kress-Gazit, Hadas/0000-0002-7754-1011},
Unique-ID = {WOS:000309406703091},
}

@article{ WOS:000312687400006,
Author = {Kersten, Thomas P. and Lindstaedt, Maren},
Title = {Automatic 3D Object Reconstruction from Multiple Images for
   Architectural, Cultural Heritage and Archaeological Applications Using
   Open-Source Software and Web Services},
Journal = {PHOTOGRAMMETRIE FERNERKUNDUNG GEOINFORMATION},
Year = {2012},
Number = {6},
Pages = {727-740},
Abstract = {Constant improvements in the performance of internet and computer
   technologies combined with rapid advancements in computer vision
   algorithms now make it possible to efficiently and flexibly reconstruct
   the 3D geometry of objects. Objects of different sizes can be modelled
   using image sequences from commercial digital cameras that are processed
   by web services and freely available software packages, forming low-cost
   systems for numerous applications (restoration, historical care of
   monuments, visualization, analysis of the state of construction and the
   damage, etc.). In this contribution various cultural objects (historical
   buildings, statues/figures, archaeological finds, etc.) have been
   reconstructed in order to investigate the potential of this technology
   which enables the automatic generation of 3D point clouds or surface
   models (as 3D polygons) with photo-realistic texture from image data.
   These so-called low-cost systems represent an efficient alternative to
   expensive terrestrial laser scanning systems for the as-built
   documentation of 3D objects in architecture, cultural heritage and
   archaeology. The accuracy of the automatically generated 3D models is
   assessed by comparison with results from terrestrial laser scanning.},
DOI = {10.1127/1432-8364/2012/0152},
ISSN = {1432-8364},
EISSN = {2363-7145},
ORCID-Numbers = {Kersten, Thomas P./0000-0001-8910-2887},
Unique-ID = {WOS:000312687400006},
}

@article{ WOS:000307862100002,
Author = {Lau, Chun-Sean and Abdullah, M. Z. and Ani, F. Che},
Title = {Three-dimensional thermal investigations at board level in a reflow oven
   using thermal-coupling method},
Journal = {SOLDERING \& SURFACE MOUNT TECHNOLOGY},
Year = {2012},
Volume = {24},
Number = {3},
Pages = {167-182},
Abstract = {Purpose - The purpose of this paper is to develop thermal modelling to
   investigate the thermal response of sample boards (at board level)
   during the preheating stage of the reflow process and to validate with
   experimental measurements.
   Design/methodology/approach - A thermal-coupling method that adopted the
   Multi-physics Code Coupling Interface (MpCCI) was utilized. A
   forced-convection reflow oven was modelled using computational fluid
   dynamic software (FLUENT 6.3.26), whereas structural heating at the
   board level was conducted using finite-element method software (ABAQUS
   6.9).
   Findings - The simulation showed a complex flow pattern having
   characteristics of a free-jet region, stagnation-flow region, wall
   jet-region, recirculation region and vortices. A sharp maximum
   heat-transfer coefficient was detected in the stagnation region of the
   jet, resulting in a spatial variation of local heat transfer on a
   thermal profile board (TPB). This coefficient affected the temperature
   distribution in the TPB with different specific heat capacitances and
   thermal conductivity of the structure. The simulation results were in
   good agreement with the experimental data and analytical model. The cold
   region and temperature uniformity (Delta T) increased with increasing
   complexity of the TPB. The cold region can occur in two possible
   locations in the TPB. Both occurrences can be related to the flow field
   of the reflow oven. Delta T of the TPB decreased when the conveyor speed
   (v) was reduced. A suitable conveyor speed (1.0 cm/s) was determined to
   maintain Delta T below 10 degrees C, which prevented the thermally
   critical package from overheating.
   Practical implications - The paper provies a methodology for designing a
   thermal profile for reflow soldering production.
   Originality/value - The findings provide fundamental guidelines to the
   thermal-coupling method at the board and package levels, very useful for
   accurate control of Delta T at the board and package levels, one of the
   major requirements in achieving a high degree of reliability for
   electronic assemblies.},
DOI = {10.1108/09540911211240038},
ISSN = {0954-0911},
EISSN = {1758-6836},
ResearcherID-Numbers = {Abdullah, Mohd Z/F-6443-2010
   },
ORCID-Numbers = {Abdullah, Mohd Z/0000-0002-5353-6162
   Che Ani, Fakhrozi/0000-0001-7023-5591},
Unique-ID = {WOS:000307862100002},
}

@inproceedings{ WOS:000351498300030,
Author = {Mcgee, Wes and Pigram, David and Kaczynski, Maciej P.},
Editor = {Fischer, T and DeBiswas, K and Ham, JJ and Naka, R and Huang, W},
Title = {ROBOTIC RETICULATIONS A method for the integration of multi-axis
   fabrication processes with algorithmic form-finding techniques},
Booktitle = {PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON COMPUTER-AIDED
   ARCHITECTURAL DESIGN RESEARCH IN ASIA (CAADRIA 2012): BEYOND CODES AND
   PIXELS},
Year = {2012},
Pages = {295-304},
Note = {17th International Conference on Computer-Aided Architectural Design
   Research in Asia (CAADRIA), Hindustan Univ, Sch Architecture, Chennai,
   INDIA, APR 25-28, 2012},
Organization = {CADD Ctr India Private Ltd; Ramky Wavoo Devvelopers Private Ltd;
   Nemetschek Vectorworks Inc; Ind Datacomm Engineers},
Abstract = {This paper addresses the design and fabrication of non-uniform
   structural shell systems. Structural shells, particularly gridshells,
   have a long history but due to their complexity and the accompanying
   high cost of construction, their application has been limited. The
   research proposes a method for integrating the design and fabrication
   processes such that complex double curved reticulated frames can be
   constructed efficiently, from prefabricated components, requiring
   significantly less formwork than is typical. A significant aspect of the
   method has been the development of software tools that allow for both
   algorithmic formfinding and the direct control of robotic fabrication
   equipment from within the same modelling package. A recent case-study is
   examined where the methodology has been applied to construct a
   reticulated shell structure in the form of a partial vault. Components
   were prefabricated using 6-axis robotic fabrication equipment.
   Individual parts are designed such that the assembly of components
   guides the form of the vault, requiring no centring to create the
   desired shape. Algorithmically generated machine instructions controlled
   a sequence of three tool changes for each part, using a single modular
   fixture, greatly increasing accuracy. The complete integration of
   computational design techniques and fabrication methodologies now
   enables the economical deployment of non-uniform structurally optimised
   reticulated frames.},
ISBN = {978-988-19026-3-4},
ORCID-Numbers = {Pigram, Dave/0000-0002-5875-2366},
Unique-ID = {WOS:000351498300030},
}

@inproceedings{ WOS:000313536600017,
Author = {Nedovodeev, Konstantin},
Editor = {Ovchinnikov, A},
Title = {Adaptive Libraries for Multicore Architectures with Explicitly-Managed
   Memory Hierarchies},
Booktitle = {PROCEEDINGS OF THE 11TH CONFERENCE OF OPEN INNOVATIONS ASSOCIATION FRUCT},
Year = {2012},
Pages = {126-135},
Note = {11th Conference of Open-Innovations-Association FRUCT, St Petersburg
   State Univ Aerosp Instrumentat, St Petersburg, RUSSIA, APR 23-27, 2012},
Organization = {St Petersburg State Electrotechn Univ; FRUCT Oy; NOKIA Univ Cooperat
   Program; IEEE Russia NW Sect; ENPI Karelia; NOKIA Qt Dev Framework
   Program; EMC2 Univ Cooperat Program},
Abstract = {Programming of commodity multicore processors is a challenging task and
   it becomes even harder when the processor has an explicitly-managed
   memory hierarchy (EMMA). Software libraries in the field of matrix
   algebra try to keep pace with this challenge by using the dataflow model
   of computation and constructing tiled algorithms. A new approach to
   high-performance software library construction is proposed, which moves
   scheduling decisions to compile-time and is portable between different
   EMMA platforms. Performance and scalability analyses both demonstrate
   promising results. Experiments demonstrate near linear speedup on a
   synthetic multicore architecture, incorporating up to 16 working
   computational cores. Performance of a generated code is competitive with
   vendor BLAS implementations for the Cell processor.},
ISBN = {978-5-8088-0707-5},
ResearcherID-Numbers = {Nedovodeev, Konstantin/AAO-8610-2020},
Unique-ID = {WOS:000313536600017},
}

@inproceedings{ WOS:000321076701043,
Author = {Pini, M. and Spinelli, A. and Dossena, V. and Gaetani, P. and Casella,
   F.},
Book-Group-Author = {ASME},
Title = {DYNAMIC SIMULATION OF A TEST RIG FOR ORGANIC VAPOURS},
Booktitle = {PROCEEDINGS OF THE ASME 5TH INTERNATIONAL CONFERENCE ON ENERGY
   SUSTAINABILITY 2011, PTS A-C},
Year = {2012},
Pages = {1975-1986},
Note = {ASME 5th International Conference on Energy Sustainability, Washington,
   DC, AUG 07-10, 2011},
Organization = {ASME, Adv Energy Syst Div; ASME, Solar Energy Div},
Abstract = {A blow-down facility for experimental analysis of real gases is under
   construction at Politecnico di Milano (Italy), in collaboration with
   Turboden s.r.l. and in the frame of the research project named Solar.
   Experiments are meant to characterize flow fields representative of
   expansions taking place in Organic Rankine Cycle (ORC) turbine passages.
   Indeed, ORC power plants represent a viable technology to exploit clean
   energy sources, but ORC turbines design tools still require accurate
   experimental data for validation. A significant improvement of turbine
   efficiency is expected from detailed investigations on vapour streams;
   in fact, ORC turbines design tools still require accurate experimental
   data for validation.
   The facility is equipped with a straight axis supersonic nozzle as a
   test section and a batch-closed loop plant has been designed in order to
   reduce investment and operational costs. Due to the batch operation, the
   evaluation of the time evolution of main processes involved in the cycle
   is of great importance. To this purpose a dynamic simulation of the test
   rig has been carried out using a dynamic simulator based on an
   object-oriented modeling language, Modelica, allowing an easy
   development of component models structured with a hierarchical approach.
   Models include control loop devices, strongly influencing processes
   duration.
   This paper presents how the test rig has been modelled, with particular
   emphasis on the models framework and on simulation procedure; the
   calculation results are finally discussed.
   With a lumped parameter approach, a first scheme of the facility has
   been built by modelling each of the three main plant section (heating,
   test, condensation) using components included in a self-made library.
   Several models, not embedded in the Modelica standard libraries, have
   been created using Modelica code; among them the most important has been
   the supersonic nozzle.
   In order to better describe the facility behaviour and the thermal
   losses, a plant calculation refinement has been carried out by the
   development of finite volume based one-dimensional models of ducts and
   reservoirs, either in radial or axial direction; in particular, a novel
   distributed-parameters model has been built for the heating section.
   All simulations have been performed using Siloxane MDM and
   Hydrofluorocarbon R245fa as reference fluids and FluidProp (R) to
   calculate thermodynamic properties.
   A quasi 1-D steady nozzle flow calculation has also been carried out by
   implementing FluidProp routines in a dedicated Fortran software. Since
   the unsteady nozzle expansion is well approximated by a sequence of
   steady states, the computation provides all thermodynamic properties and
   velocity along the nozzle axis as a function of time.
   Simulation results have given a fundamental support to both plant and
   experiments design.},
ISBN = {978-0-7918-5468-6},
ResearcherID-Numbers = {Casella, Francesco/C-5434-2012
   Pini, Matteo/O-8826-2017},
ORCID-Numbers = {CASELLA, FRANCESCO/0000-0002-4509-0711
   dossena, vincenzo/0000-0002-7192-0002
   Spinelli, Andrea/0000-0002-8715-6758
   Pini, Matteo/0000-0003-4538-3183},
Unique-ID = {WOS:000321076701043},
}

@inproceedings{ WOS:000321224300035,
Author = {Roca, Pere and Cervera, Miguel and Pela, Luca and Clemente, Roberto and
   Chiumenti, Michele},
Editor = {Jasienko, J},
Title = {STRUCTURAL ASSESSMENT OF MALLORCA CATHEDRAL},
Booktitle = {STRUCTURAL ANALYSIS OF HISTORICAL CONSTRUCTIONS, VOLS 1-3},
Year = {2012},
Pages = {359-367},
Note = {8th International Conference on Structural Analysis of Historical
   Constructions, SAHC 2012, Centennial Hall, Wroclaw, POLAND, OCT 15-17,
   2012},
Organization = {MAPEI, Profesjonalna Chemia Budowlana; KEIM; PLAZA CTR; quick mix; TITAN
   POLSKA; ULMA; Ruredil},
Abstract = {This paper presents a thorough structural assessment of Mallorca
   Cathedral, one of the largest Gothic constructions in Europe. Historical
   investigation has provided a preliminary knowledge about the structure.
   The construction condition and the materials properties have been
   assessed by structural inspection, monitoring and experimental testing.
   The building process and the long-term time-dependent phenomena have
   been identified as possible causes of the actual state of deformation in
   the naves and pillars. A special FE package has been devised with the
   aim of simulating numerically the structural response to critical
   conditions. Suitable constitutive laws have been considered to describe
   the inelastic behaviour of the material, including the representation of
   mechanical damage and time-dependent deformation due to creep. Advanced
   algorithms have been developed to improve the potential of FE software.
   Sequential-evolutionary analyses have been carried out to simulate the
   construction process and to understand the stability conditions during
   intermediate building stages. The response of the cathedral bay
   structure to earthquake equivalent horizontal forces has been also
   assessed. An accurate representation of cracks distribution has been
   achieved and thus the most critical structural members have been
   detected. The study presents a structural assessment procedure that
   could be applied also to similar historical masonry constructions.},
ISBN = {978-83-7125-216-7},
Unique-ID = {WOS:000321224300035},
}

@inproceedings{ WOS:000316156900036,
Author = {Song, Gao and GaoYan},
Editor = {Duserick, FG},
Title = {Electronic Commerce Teaching Resources Platform Construction Solution
   Study},
Booktitle = {ELEVENTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS},
Year = {2012},
Pages = {274-279},
Note = {11th Wuhan International Conference on E-Business, Wuhan, PEOPLES R
   CHINA, MAY 26-27, 2012},
Organization = {China Univ Geosciences, Ctr Int Cooperat E Business; China Univ
   Geosciences, Sch Econom \& Management; Alfred Univ, Coll Business; Int
   Business Interface Inc; Assoc Informat Syst; China Assoc Informat Syst;
   China Informat Econom Soc; Baden Wuerttemberg Cooperat State Univ; Royal
   Melbourne Inst Technol Univ; Ankara Univ; Univ N Dakota, Coll Business
   \& Publ Adm; NJ Inst Technol, Sch Management; Univ Appl Sci, FHS St
   Gallen; Singapore Management Univ, Sch Informat Syst; Huazhong Univ Sci
   \& Technol, Coll Management; Wuhan Univ, Business Sch; Wuhan Univ, Coll
   Informat Management; Zhongnan Univ Econom Law, Xinhua Coll Finance
   Insurance; Zhongnan Univ Econom Law, Sch Business Adm; Huazhong Normal
   Univ, Informat Management Dept; Wuhan Univ, Grad Sch Syst Engn; China E
   Commerce Assoc Wuhan Representat Off; Tsinghua Univ, Sch Econom
   Management, China Journal Informat Syst; Huazhong Univ Technol \& Sci,
   Chinese Journal Management; Wuhan Univ Technol, Coll Management; China
   Journal Informat Syst; Chinese Journal Management; Elect Govt Int
   Journal; Elect Markets Int Journal Networked business; Int Journal
   Business Proc Integrat \& Management; Int Journal Informat Syst \&
   Change Management; Int Journal Informat Technol \& Management t; Int
   Journal Mobile Learn \& Org; Int Journal Network \& Virtual Org t; Int
   Journal Revenue Management; Int Journal Serv Technol \& Management;
   Journal Theoret \& Appl Elect Commerce Res; Int Journal Biomed Engn \&
   Technol},
Abstract = {The paper proposes the solution to teaching resource application
   platform that applies to electronic commerce teaching. Electronic
   commerce teaching resource platform system includes hardware system
   structure and software system structure. It is an open approach based on
   B/S mode that provides each faculty with local and remote teaching
   resources via Intranet/Internet and booming cloud technology. It
   establishes inner- and inter-scholastic supporting teaching resource
   library to meet the demands in teaching practices, and includes
   functions such as theory teaching, practice teaching, skill practice,
   entrepreneurial internship, teaching resource service, laboratory
   management, teaching management, teacher-student interaction and etc.
   The paper combines the comprehensive teaching resource platform
   construction practices of modem business and logistics in Beijing Youth
   Politics College, and puts forward opinions on and solutions to
   construction scheme of electronic commerce laboratory and teaching
   resource system in colleges and universities, including construction
   purpose, system structure, network laboratory composition, teaching
   function, function of teaching resource library contents and other
   aspects. Besides, it proposes the basic solutions to experimental
   teaching platform, teaching management platform, practice and
   entrepreneurial platform and teaching resource library construction.},
ISBN = {978-0-9800510-5-6},
Unique-ID = {WOS:000316156900036},
}

@inproceedings{ WOS:000321950300056,
Author = {Su, Yen-Fu and Yang, Yu-Hsiang and Yang, Wen-Kun and Chiang, Kuo-Ning},
Book-Group-Author = {IEEE},
Title = {A Thermal Performance Assessment of Panel Type Packaging (PTP)
   Technology for High Efficiency LED},
Booktitle = {14TH INTERNATIONAL CONFERENCE ON ELECTRONIC MATERIALS AND PACKAGING
   (EMAP 2012)},
Series = {International Conference on Electronic Materials and Packaging},
Year = {2012},
Note = {14th International Conference on Electronic Materials and Packaging
   (EMAP), Hong Kong, PEOPLES R CHINA, DEC 13-16, 2012},
Organization = {IEEE; IEEE Components, Packaging \& Mfg Technol Soc (CPMT), Hong Kong
   Chapter; Ctr Adv Microsystems Packaging (CAMP); HKUST LED FPD Technol
   R\&D Ctr; Ctr Engn Mat \& Reliabil (CEMAR); Shanghai Sinyang Elect
   Chemicals Co, Ltd; Hong Kong Univ Sci \& Technol, Sch Engn; Hong Kong
   Univ Sci \& Technol, Inst Integrated Micro Syst (I2MS); Hong Kong Univ
   Sci \& Technol; Chinese Univ Hong Kong (CUHK); ASM Pacific Technol Ltd;
   Nano \& Adv Mat Inst Ltd (NAMI)},
Abstract = {In response to the effect of global warming, increasing number of
   industries have focused their attention on green technology products
   such as the light-emitting diode (LED), currently has been widely
   applied in many products because of its low pollution potential, low
   power consumption, and long life characteristics. Panel type packaging
   (PTP) technology, applicable in a wafer level packaging process, is one
   of the solutions for LED packaging structure. However, LED with low
   electro-optical conversion efficiency converts a high-percentage of the
   input power into redundant heat; thus, junction temperature increases.
   In this research, the finite element (FE) model of the PTP technology
   was developed by commercial software ANSYS (R) for high-power LED
   mounted on metal-core printed circuit board (MCPCB), composed of copper
   foil, dielectric layer, and aluminum base plate. The forward-voltage
   method for characterization of diodes was also employed to measure the
   junction temperature of PTP for LED packaging, validated with the FE
   results. Next, the effects of MCPCB dielectric material, MCPCB size,
   filler material, and black bismaleimide triazine (BT) substrate material
   were analyzed. In addition, the multi-chip LED module was also
   investigated. By adopting the design guideline determined by the FE
   analysis, the thermal performance of the PTP technology for LED can be
   improved further, enhancing its suitability for high-power LED
   application.},
ISSN = {2151-7231},
ISBN = {978-1-4673-4944-4; 978-1-4673-4945-1},
Unique-ID = {WOS:000321950300056},
}

@article{ WOS:000300436900012,
Author = {Tsiafis, I. and Bouzakis, K-D. and Tsouknidas, A. and Michailidis, N.},
Title = {Effects of Partial Bonding and Adhesive Thickness on the Load-Carrying
   Capacity of Tension Specimens Bonded with Epoxy Adhesive Investigated by
   Microindentation, Tensile Testing, and FEM Simulation},
Journal = {JOURNAL OF AEROSPACE ENGINEERING},
Year = {2012},
Volume = {25},
Number = {1},
Pages = {103-107},
Month = {JAN},
Abstract = {The use of epoxy adhesives in metal structures for adhesive reasons is a
   continuously growing market, because of the increasing demand of
   lightweight constructions, which renders in certain cases conventional
   mechanical bonding unpractical. Furthermore, epoxy adhesives possess
   enhanced mechanical, chemical, and physical properties, i.e., increased
   shear and compression strength and resistance in solvents, whereas
   maintaining these properties at higher temperatures than traditional
   mechanical bonding methods. Next generation aircraft engines and
   pipelines are common application areas of these adhesives. In order,
   however, to ensure the safe use of epoxy adhesives in such structures,
   computational analyses must be conducted to simulate eventual failure
   mechanisms. These simulations require the exact determination of the
   epoxy adhesion strength properties with respect to several parameters. A
   systematic investigation of these properties will be presented, on the
   basis of tensile tests of standard aluminium tension specimens bonded
   together with epoxy adhesives, whereas their failure mechanisms will be
   simulated by a FEM-supported model to evaluate the extracted
   characteristics. The specimens were cemented by means of a developed
   experimental device under constant temperature and humidity conditions.
   The deformation as well as the developed stress distribution of the
   specimens during the tensile experiments were simulated by means of a
   FEM-supported software package, considering the stress-strain curves of
   the adhesive, determined by a FEM-based evaluation of microindentation
   results. DOI: 10.1061/(ASCE)AS.19435525.0000092. (C) 2012 American
   Society of Civil Engineers.},
DOI = {10.1061/(ASCE)AS.1943-5525.0000092},
ISSN = {0893-1321},
EISSN = {1943-5525},
ORCID-Numbers = {Bouzakis, Konstantinos/0000-0002-5923-6945
   Michailidis, Nikolaos/0000-0002-4291-5327
   Ioannis, Tsiafis/0000-0002-3746-607X},
Unique-ID = {WOS:000300436900012},
}

@inproceedings{ WOS:000321160300021,
Author = {Yang, Ling and Zhong, Jingjun and Han, Ji-ang},
Book-Group-Author = {ASME},
Title = {NUMERICAL RESEARCH OF THE RAM-ROTOR WITH DIFFERENT GEOMETRIC PARAMETERS},
Booktitle = {PROCEEDINGS OF THE ASME TURBO EXPO 2011, VOL 7, PTS A-C},
Year = {2012},
Pages = {251-262},
Note = {ASME Turbo Expo 2011, Vancouver, CANADA, JUN 06-10, 2011},
Organization = {ASME; Int Gas Turbine Inst},
Abstract = {With the design methods of typical supersonic aircraft intakes and the
   advantages of shock wave compression, ram-rotors have become a new
   attractive compression system. Lots of research work has been carried
   out on rampressors, but the influence of the geometric parameters on the
   shock wave structure and compression performance of the ram-rotor has
   not been studied systematically. Therefore, a thorough study on
   ram-rotor with different geometric parameters is required.
   In this paper, a steady three-dimensional Navier-Stokes equation adopted
   in the ``Fluent{''} software package is carried out on a large parallel
   computer. Six factors which may influence the ram-rotor performance are
   investigated numerically. These geometric parameters are strake section
   shape, throat length-height ratio, strake stagger angle, compression
   ramp angle, subsonic divergent angle and throat contraction ratio. The
   study is composed of two parts. The aim of the first part is to
   understand the influence of the geometric parameters listed above on the
   shock wave structure and compression performance of the ram-rotor by
   comparison and analysis of the relative Mach number and static pressure
   in the flow-path. The aim of the second part is to obtain the optimal
   geometric structure of the ram-rotor by comparison and analysis of the
   structure of the flow fields, the compression performance and the
   ram-rotor properties.
   First of all, the numerical method is validated by comparing the
   numerical results of the flow field of a supersonic intake with
   experimental results in this paper. Secondly, the flow field structures
   in the ram-rotor, especially the number and position of shock waves and
   the separation zone, are studied. Thirdly, the influence of the
   geometric parameters on the rotor performance is studied. Some parameter
   distributions, such as the flow angle, adiabatic efficiency, total
   pressure ratio, total pressure recovery coefficient, are compared and
   analyzed. The rules of the ram-rotor performance variation with
   different geometric parameters are also presented. Finally, some advice
   for improving the overall performance of the ram-rotor is given
   according to the flow field analysis.},
ISBN = {978-0-7918-5467-9},
Unique-ID = {WOS:000321160300021},
}

@inproceedings{ WOS:000309016402054,
Author = {Zheng, Haichun},
Editor = {Fan, W},
Title = {Applying SKILL Technology to Aid Cadence Allegro Part Management},
Booktitle = {MANUFACTURING SCIENCE AND TECHNOLOGY, PTS 1-8},
Series = {Advanced Materials Research},
Year = {2012},
Volume = {383-390},
Pages = {4180-4183},
Note = {International Conference on Manufacturing Science and Technology (ICMST
   2011), Singapore, SINGAPORE, SEP 16-18, 2011},
Organization = {Singapore Inst Elect},
Abstract = {Library construction is a fundamental work in Cadence Allegro's
   schematic and PCB (printed circuit board) constructing process. Existing
   part libraries are normally organized in directory structure. With the
   increment of the amount of library files, part management becomes slow
   and error-prone which is carried out on manual basis. A new method is
   given by designing part management software through applying the SKILL
   technology to aid Cadence Allegro part management. Application results
   show that the new method is effective in the library construction.},
DOI = {10.4028/www.scientific.net/AMR.383-390.4180},
ISSN = {1022-6680},
ISBN = {978-3-03785-295-8},
Unique-ID = {WOS:000309016402054},
}

@article{ WOS:000298923600020,
Author = {Wu, Houbo and Guo, Yatao and Wang, Guanghua and Dai, Shikun and Li,
   Xiang},
Title = {Composition of bacterial communities in deep-sea sediments from the
   South China Sea, the Andaman Sea and the Indian Ocean},
Journal = {AFRICAN JOURNAL OF MICROBIOLOGY RESEARCH},
Year = {2011},
Volume = {5},
Number = {29},
Pages = {5273-5283},
Month = {DEC 9},
Abstract = {The bacterial diversity of deep-sea sediments collected from the South
   China Sea (B111), the Andaman Sea (ADM), and the Indian Ocean (P10) was
   investigated through the construction of 16S recombinant
   deoxyribonucleic acid (DNA) clone libraries for sequencing. The
   structure of the bacterial community and their phylogenetic
   relationships were analyzed using the Mothur software. The nutrient and
   heavy metal content of the sediments were also analyzed. In total, 263
   valid sequences were obtained from 300 sequenced clones. There were 151
   operational taxonomic units (OTUs) that belong to Actinobacteria,
   Proteobacteria, Planctomycetes, Bacteroidetes, Gemmatimonadetes,
   Nitrospira, Chloroflexi, Acidobacteria, WS3, BRC1, OD1, and numerous
   unclassified bacteria. The Chao1 and Shannon indices showed that B111
   had the highest diversity, but P10 had the richest OTUs. The P < 0.05
   value of the parsimony index, LIBSHUFF, and weighted UniFrac analyses
   indicated important distinctions in the structure and diversity of the
   bacterial communities. These differences are related to the nutrient and
   heavy metal content of the sediments. The present study clearly
   demonstrates the wide bacterial diversity in deep-sea sediments from the
   South China Sea, the ADM, and the P10, including a large number of
   unknown bacteria. Due to the different geographical locations of the
   sediments, the structures of their bacterial communities are also
   remarkably distinct.},
DOI = {10.5897/AJMR11.302},
ISSN = {1996-0808},
ResearcherID-Numbers = {Guan, Xiaokang/A-6675-2012
   Ma, Wentao/A-8800-2010},
Unique-ID = {WOS:000298923600020},
}

@article{ WOS:000298632200011,
Author = {Kim, June and Ahn, Chiyoung and Choi, Seungwon and Glossner, John},
Title = {Implementation of smart antenna API and transceiver API in software
   communication architecture for a wireless innovation forum standard},
Journal = {ANALOG INTEGRATED CIRCUITS AND SIGNAL PROCESSING},
Year = {2011},
Volume = {69},
Number = {2-3, SI},
Pages = {219-226},
Month = {DEC},
Abstract = {This study presents an implementation of the standard smart antenna (SA)
   application programming interface (API) and Transceiver API developed by
   the wireless innovation forum's (WINNF) smart antenna working group
   (SAWG). The API is implemented using the open-source SCA
   implementation-embedded (OSSIE) developed at Virginia Tech. Our
   implementation verified that the SA API can be utilized in software
   communication architecture (SCA)-based software defined radio (SDR)
   systems. We also verified that the Transceiver API can be realized with
   a real radio frequency (RF) transceiver module such as universal
   software radio peripheral2 (USRP2). The SA API enables various functions
   of multi-antenna systems such as beamforming and multiple input multiple
   output (MIMO) of spatial multiplexing. These are core technologies
   prevalent in 4G mobile communication systems. In order to support
   multi-antenna structures, the Transceiver API has first been extended
   for multichannel use. The paper details how the API is extended using
   OSSIE and the current status of the API as a standard within the
   Wireless Innovation Forum.},
DOI = {10.1007/s10470-011-9759-6},
ISSN = {0925-1030},
EISSN = {1573-1979},
Unique-ID = {WOS:000298632200011},
}

@article{ WOS:000296504300010,
Author = {Yi, Ting-Hua and Li, Hong-Nan and Gu, Ming},
Title = {Optimal sensor placement for structural health monitoring based on
   multiple optimization strategies},
Journal = {STRUCTURAL DESIGN OF TALL AND SPECIAL BUILDINGS},
Year = {2011},
Volume = {20},
Number = {7},
Pages = {881-900},
Month = {NOV},
Abstract = {Careful selection and placement of sensors are the critical issue in the
   construction and implementation of an effective structural health
   monitoring system. A hybrid method termed the optimal sensor placement
   strategy (OSPS) based on multiple optimization methods is proposed in
   this paper. The initial sensor placement is firstly obtained by the QR
   factorization. Then, using the minimization of the off-diagonal elements
   in the modal assurance criterion matrix as a measure of the utility of a
   sensor configuration, the quantity of the sensors is determined by the
   forward and backward sequential sensor placement algorithm together.
   Finally, the locations of the sensor are determined by the
   dual-structure coding-based generalized genetic algorithm (GGA). Taking
   the scientific calculation software MATLAB (MathWorks, Natick, MA, USA)
   as a platform, an OSPS toolbox, which is working as a black box, is
   developed based on the command-line compiling and graphical user
   interface-aided graphical interface design. The characteristic and
   operation method of the toolbox are introduced in detail, and the scheme
   selection of the OSP is carried out on the world's tallest TV tower
   (Guangzhou New TV Tower) based on the developed toolbox. The results
   indicate that the proposed method is effective and the software package
   has a friendly interface, plenty of functions, good expansibility and is
   easy to operate, which can be easily applied in practical engineering.
   Copyright (C) 2011 John Wiley \& Sons, Ltd.},
DOI = {10.1002/tal.712},
ISSN = {1541-7794},
EISSN = {1541-7808},
ORCID-Numbers = {Yi, Ting-Hua/0000-0002-8952-2846},
Unique-ID = {WOS:000296504300010},
}

@article{ WOS:000297110700001,
Author = {Ramirez, Sergio and Karlsson, Johan and Trelles, Oswaldo},
Title = {MAPI: towards the integrated exploitation of bioinformatics Web Services},
Journal = {BMC BIOINFORMATICS},
Year = {2011},
Volume = {12},
Month = {OCT 27},
Abstract = {Background: Bioinformatics is commonly featured as a well assorted list
   of available web resources. Although diversity of services is positive
   in general, the proliferation of tools, their dispersion and
   heterogeneity complicate the integrated exploitation of such data
   processing capacity.
   Results: To facilitate the construction of software clients and make
   integrated use of this variety of tools, we present a modular
   programmatic application interface ( MAPI) that provides the necessary
   functionality for uniform representation of Web Services metadata
   descriptors including their management and invocation protocols of the
   services which they represent. This document describes the main
   functionality of the framework and how it can be used to facilitate the
   deployment of new software under a unified structure of bioinformatics
   Web Services. A notable feature of MAPI is the modular organization of
   the functionality into different modules associated with specific tasks.
   This means that only the modules needed for the client have to be
   installed, and that the module functionality can be extended without the
   need for re-writing the software client.
   Conclusions: The potential utility and versatility of the software
   library has been demonstrated by the implementation of several currently
   available clients that cover different aspects of integrated data
   processing, ranging from service discovery to service invocation with
   advanced features such as workflows composition and asynchronous
   services calls to multiple types of Web Services including those
   registered in repositories (e.g. GRID-based, SOAP, BioMOBY,
   R-bioconductor, and others).},
DOI = {10.1186/1471-2105-12-419},
Article-Number = {419},
ISSN = {1471-2105},
ORCID-Numbers = {Trelles Salazar, Oswaldo/0000-0003-1554-8987},
Unique-ID = {WOS:000297110700001},
}

@article{ WOS:000295680600004,
Author = {Eissler, Tilo and Hodges, Christopher P. and Meier, Harald},
Title = {PTPan-overcoming memory limitations in oligonucleotide string matching
   for primer/probe design},
Journal = {BIOINFORMATICS},
Year = {2011},
Volume = {27},
Number = {20},
Pages = {2797-2805},
Month = {OCT 15},
Abstract = {Motivation: Nucleic acid diagnostics has high demands for non-heuristic
   exact and approximate oligonucleotide string matching concerning in
   silico primer/probe design in huge nucleic acid sequence collections.
   Unfortunately, public sequence repositories grow much faster than
   computer hardware performance and main memory capacity do. This growth
   imposes severe problems on existing oligonucleotide primer/probe design
   applications necessitating new approaches based on space-efficient
   indexing structures.
   Results: We developed PTPan (spoken Peter Pan, `PT' is for Position
   Tree, the earlier name of suffix trees), a space-efficient indexing
   structure for approximate oligonucleotide string matching in nucleic
   acid sequence data. Based on suffix trees, it combines partitioning,
   truncation and a new suffix tree stream compression to deal with large
   amounts of aligned and unaligned data. PTPan operates efficiently in
   main memory and on secondary storage, balancing between memory
   consumption and runtime during construction and application. Based on
   PTPan, applications supporting similarity search and primer/probe design
   have been implemented, namely FindFamily, ProbeMatch and ProbeDesign.
   All three use a weighted Levenshtein distance metric for approximative
   queries to find and rate matches with indels as well as substitutions.
   We integrated PTPan in the worldwide used software package ARB to
   demonstrate usability and performance. Comparing PTPan and the original
   ARB index for the very large ssu-rRNA database SILVA, we recognized a
   shorter construction time, extended functionality and dramatically
   reduced memory requirements at the price of expanded, but very
   reasonable query times.
   PTPan enables indexing of huge nucleic acid sequence collections at
   reasonable application response times. Not being limited by main memory,
   PTPan constitutes a major advancement regarding rapid oligonucleotide
   string matching in primer/probe design now and in the future facing the
   enormous growth of molecular sequence data.},
DOI = {10.1093/bioinformatics/btr483},
ISSN = {1367-4803},
EISSN = {1460-2059},
Unique-ID = {WOS:000295680600004},
}

@article{ WOS:000291486100005,
Author = {Chatzigeorgiou, Alexander and Stiakakis, Emmanouil},
Title = {Benchmarking library and application software with Data Envelopment
   Analysis},
Journal = {SOFTWARE QUALITY JOURNAL},
Year = {2011},
Volume = {19},
Number = {3},
Pages = {553-578},
Month = {SEP},
Abstract = {Library software is generally believed to be well-structured and follows
   certain design guidelines due to the need of continuous evolution and
   stability of the respective APIs. We perform an empirical study to
   investigate whether the design of open-source library software is
   actually superior to that of application software. By analyzing certain
   design principles and heuristics that are considered important for API
   design, we extract a set of software metrics that are expected to
   reflect the improved nature of libraries. An initial comparison by
   conventional statistical analysis confirms the overall belief that
   products of different software size scale should not be compared by
   simply examining metric values in isolation. In this paper, we propose
   the use of Data Envelopment Analysis (DEA), borrowed from production
   economics, as a means of measuring and benchmarking the quality of
   different object-oriented software designs captured by software metrics
   and apply this approach to the comparison of library and application
   software. The advantages offered by DEA and the differences between the
   application of DEA in an economic and a technological context are
   discussed. Results of the approach are presented for 44 open-source
   projects, equally divided between libraries and applications.},
DOI = {10.1007/s11219-010-9113-8},
ISSN = {0963-9314},
EISSN = {1573-1367},
ResearcherID-Numbers = {Chatzigeorgiou, Alexandros/AAL-6077-2021},
Unique-ID = {WOS:000291486100005},
}

@article{ WOS:000293116200016,
Author = {Wu, Minger and Wu, Yuanyuan and Kim, Jae-Yeol},
Title = {ETFE foil spring cushion structure and its analytical method},
Journal = {THIN-WALLED STRUCTURES},
Year = {2011},
Volume = {49},
Number = {9},
Pages = {1184-1190},
Month = {SEP},
Abstract = {The traditional Ethylene Tetra Fluoro Ethylene (ETFE) foil cushion is in
   the form of air cushion, whose structural stiffness is offered by the
   inner air pressure. Because the air supply and control systems are
   needed, air cushion structures cost extra energy and need much
   maintenance. In order to overcome the shortcomings of air cushion, the
   ETFE foil spring cushion that uses a spring to take the place of air
   pressure is developed in this paper. At first, the analytical method for
   shape finding analysis and stress analysis of the spring cushion is
   described in the paper. A numerical example is shown by means of the
   software package ANSYS and the suggested method so as to verify the
   validity of the method. Then, a model experiment on the ETFE foil spring
   cushion is carried out. In the loading test, the compression of spring
   is recorded and the experimental results are compared with the
   analytical results. At the end of the paper, an experimental hall using
   both ETFE foil air cushion units and spring cushion units as its roof
   structure is introduced. Through the construction and daily use of the
   experimental hall, the spring cushion system shows advantages such as
   easier construction, needing no air supply and control equipments, no
   running energy and little maintenance compared with the air cushion
   system. (C) 2011 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.tws.2011.05.005},
ISSN = {0263-8231},
Unique-ID = {WOS:000293116200016},
}

@article{ WOS:000292437800016,
Author = {Wang, Lai and Lee, Frank X.},
Title = {MathQCDSR: A Mathematica package for QCD sum rules calculations},
Journal = {COMPUTER PHYSICS COMMUNICATIONS},
Year = {2011},
Volume = {182},
Number = {8},
Pages = {1721-1731},
Month = {AUG},
Abstract = {We present a software package written in Mathematica for standard QCD
   suns rules calculations. Two examples are given to demonstrate how to
   use the package. One is for the mass spectrum of octet baryons from
   two-point correlation functions: the other for the magnetic moments of
   octet baryons in the external-field method. The free package FeynCalc is
   used to handle the gamma-matrix algebra. In addition to two notebooks
   for the construction of the QCD sum rules, two corresponding notebooks
   are provided for a Monte Carlo-based numerical analysis, complete with
   in-line graphical display of sum rule matching, error distributions, and
   scatter plots for correlations.
   Program summary
   Program title: MathQCDSR Catalogue identifier: AEJA\_v1\_0
   Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEJA\_v1\_0.html
   Program obtainable from: CPC Program Library, Queen's University,
   Belfast, N. Ireland
   Licensing provisions: Standard CPC licence,
   http://cpc.cs.qub.ac.uk/licence/licence.html
   No. of lines in distributed program, including test data, etc.: 93 897
   No. of bytes in distributed program, including test data, etc.: 631 481
   Distribution format: tar.gz
   Programming language: Mathematica
   Computer: PCs and Workstations
   Operating system: Any OS that supports Mathematica. The package has been
   tested under Windows XP, Macintosh OS X, and Linux
   Classification: 11.5
   External routines: FeynCalc (http://www.feyncalc.org/). It is a freely
   available Mathematica package for high-energy physics calculations. Here
   it is used primarily to handle gamma-matrix algebra.
   Nature of problem: The QCD sum rule method is a nonperturbative approach
   to solving quantum chromodynamics (QCD), the fundamental theory of the
   strong force. The approach establishes a direct link between hadron
   phenomenology and the QCD vacuum structure via a few QCD parameters
   called vacuum condensates and susceptibilities. It has been widely
   applied in nuclear and particle physics to gain insight into various
   aspects of strong-interaction physics.
   Solution method: First, QCD sum rules are constructed by evaluating
   correlation functions from two perspectives. On the quark level, it
   leads to a function of QCD parameters and the Borel mass parameter M. On
   the hadronic level, it leads to a function of phenomenological
   parameters and the same M. By numerically matching the two sides over a
   range in M. the phenomenological parameters can be extracted. The
   construction involves a large amount of gamma-matrix algebra. Fourier
   transform, and Borel transform. The matching usually involves searching
   for minimum chi(2). We employ a Monte Carlo-based procedure to perform
   the analysis which allows for realistic error estimates.
   Restrictions: The package deals with only standard (SVZ) QCD sum rule
   calculations. It can be easily adapted to handle other variants of the
   method (like finite-energy sum rules). Due to the use of FeynCalc, two
   of the notebooks (qcdsr2pt-construction.nb and qcdsr3pt-construction.nb)
   only run on version 6.0 of Mathematica. The other two can run on any
   version.
   qcdsr2pt-construction.nb - This notebook constructs the QCD sum rules
   for octet baryon masses and outputs them, one particle at a time, to
   disk in plain text for analysis. For reference, we include all the
   output files (named Mass-{*}.txt) as part of the package, totaling 8
   files in about 20 lines. The user should generate the outputs on their
   own computer and check against the supplied ones.
   qcdsr2pt-analysis.nb - This notebook reads and analyzes the QCD sum
   rules produced by qcdsr2pt-construction.nb. The user can save the
   graphics in the analysis to disk in a variety of formats.
   qcdsr3pt-construction.nb - This notebook constructs the QCD sum rules
   for the octet baryon magnetic moments and outputs them, one particle at
   a time, to disk in plain text for analysis. Again, for reference, we
   include all the output files (named Mag-{*}.txt), totaling 24 files in
   about 400 lines. The user should generate the outputs on their own
   computer and check against the supplied ones to make sure the program is
   running properly.
   qcdsr3pt-analysis.nb - This notebook reads and analyzes the QCD sum
   rules produced by qcdsr3pt-construction.nb. The user can save the
   graphics in the analysis to disk in a variety of formats.
   Each notebook can be run separately, apart from the simple interface
   between the construction and analysis programs via plain text files
   written to disk.
   Running time: For mass calculations, qcdsr2ptconstruction.nb and
   qcdsr2pt-analysis.nb take about a minute each to run on a laptop. For
   magnetic moment calculations, qcdsr3pt-construction.nb can take up to 10
   minutes for a given particle, and qcdsr3pt-analysis.nb typically a few
   minutes, depending on the number of Monte Carlo samples. (C) 2011
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.cpc.2011.04.017},
ISSN = {0010-4655},
ORCID-Numbers = {Lee, Frank/0000-0001-8169-3440},
Unique-ID = {WOS:000292437800016},
}

@article{ WOS:000293450400006,
Author = {Gipson, Bryant R. and Masiel, Daniel J. and Browning, Nigel D. and
   Spence, John and Mitsuoka, Kaoru and Stahlberg, Henning},
Title = {Automatic recovery of missing amplitudes and phases in tilt-limited
   electron crystallography of two-dimensional crystals},
Journal = {PHYSICAL REVIEW E},
Year = {2011},
Volume = {84},
Number = {1, 1},
Month = {JUL 22},
Abstract = {Electron crystallography of 2D protein crystals provides a powerful tool
   for the determination of membrane protein structure. In this method,
   data is acquired in the Fourier domain as randomly sampled, uncoupled,
   amplitudes and phases. Due to physical constraints on specimen tilting,
   those Fourier data show a vast un-sampled ``missing cone{''} of
   information, producing resolution loss in the direction perpendicular to
   the membrane plane. Based on the flexible language of projection onto
   sets, we provide a full solution for these problems with a projective
   constraint optimization algorithm that, for sufficiently oversampled
   data, produces complete recovery of unmeasured data in the missing cone.
   We apply this method to an experimental data set of Bacteriorhodopsin
   and show that, in addition to producing superior results compared to
   traditional reconstruction methods, full, reproducible, recovery of the
   missing cone from noisy data is possible. Finally, we present an
   automatic implementation of the refinement routine as open source,
   freely distributed, software that will be included in our 2dx software
   package.},
DOI = {10.1103/PhysRevE.84.011916},
Article-Number = {011916},
ISSN = {1539-3755},
EISSN = {1550-2376},
ResearcherID-Numbers = {Stahlberg, Henning/H-1868-2011
   },
ORCID-Numbers = {Stahlberg, Henning/0000-0002-1185-4592
   Browning, Nigel/0000-0003-0491-251X},
Unique-ID = {WOS:000293450400006},
}

@article{ WOS:000292520200011,
Author = {Sun, Guojun and Chen, Zhihua and Liu, Zhansheng},
Title = {Analytical and Experimental Investigation of Thermal Expansion Mechanism
   of Steel Cables},
Journal = {JOURNAL OF MATERIALS IN CIVIL ENGINEERING},
Year = {2011},
Volume = {23},
Number = {7},
Pages = {1017-1027},
Month = {JUL},
Abstract = {Different countries adopt different values of thermal expansion
   coefficients for steel cables in their design specifications, which
   results in the inconsistency for the design of prestressed structures.
   Therefore, a systematic study was conducted analytically and
   experimentally on thermal expansion mechanism of steel cables in this
   paper. First, the thermal expansion mechanism of steel cables was
   proposed, and the evaluation approach on the linear thermal expansion
   coefficient of steel cables was developed. The study identified the
   parameters that significantly affected the thermal expansion
   coefficient. After that, six types of steel-wire rope cables were tested
   by a thermal expansion coefficient measuring instrument. The identified
   parameters of the thermal expansion coefficient for these steel-wire
   rope cables were examined to show the effectiveness of the proposed
   approach. The analytical analysis further showed that the linear thermal
   expansion coefficients of these steel cables decreased with the lay
   pitch of the steel wires increasing, but the coefficients increased with
   the diameter of the steel wires increasing. Because the explicit
   expression of the analytical formula for the thermal expansion
   coefficients was very complicated, an advanced nonlinear finite-element
   model (FEM) was established to evaluate the thermal expansion
   coefficients of steel cables under idealized conditions by using the
   ANSYS software package. The analysis showed that the test result, the
   analytical result, and the numerical result were consistent, which
   exhibited that the FEM was effective to simulate the expansion of steel
   cables. In addition, 14 types of steel cables were tested. The tested
   steel cables included steel-wire rope cable, steel strand cable, and
   steel tendon cable. The thermal expansion coefficients were as follows:
   the steel-wire rope cable was 1.92 x 10(-5)/degrees C; the steel strand
   cable was 1.38 x 10(-5)/degrees C; the steel tendon cable was 1.87 x
   10(-5)/degrees C; and the steel rod was 1.19 x 10(-5)/degrees C. The
   analytical and experimental results can provide a guideline for the
   relevant study on prestressed structures. DOI:
   10.1061/(ASCE)MT.1943-5533.0000271. (C) 2011 American Society of Civil
   Engineers.},
DOI = {10.1061/(ASCE)MT.1943-5533.0000271},
ISSN = {0899-1561},
Unique-ID = {WOS:000292520200011},
}

@article{ WOS:000293569400002,
Author = {Li, Jiang and Gong, Binsheng and Chen, Xi and Liu, Tao and Wu, Chao and
   Zhang, Fan and Li, Chunquan and Li, Xiang and Rao, Shaoqi and Li, Xia},
Title = {DOSim: An R package for similarity between diseases based on Disease
   Ontology},
Journal = {BMC BIOINFORMATICS},
Year = {2011},
Volume = {12},
Month = {JUN 29},
Abstract = {Background: The construction of the Disease Ontology (DO) has helped
   promote the investigation of diseases and disease risk factors. DO
   enables researchers to analyse disease similarity by adopting semantic
   similarity measures, and has expanded our understanding of the
   relationships between different diseases and to classify them.
   Simultaneously, similarities between genes can also be analysed by their
   associations with similar diseases. As a result, disease heterogeneity
   is better understood and insights into the molecular pathogenesis of
   similar diseases have been gained. However, bioinformatics tools that
   provide easy and straight forward ways to use DO to study disease and
   gene similarity simultaneously are required.
   Results: We have developed an R-based software package (DOSim) to
   compute the similarity between diseases and to measure the similarity
   between human genes in terms of diseases. DOSim incorporates a DO-based
   enrichment analysis function that can be used to explore the disease
   feature of an independent gene set. A multilayered enrichment analysis
   (GO and KEGG annotation) annotation function that helps users explore
   the biological meaning implied in a newly detected gene module is also
   part of the DOSim package. We used the disease similarity application to
   demonstrate the relationship between 128 different DO cancer terms. The
   hierarchical clustering of these 128 different cancers showed modular
   characteristics. In another case study, we used the gene similarity
   application on 361 obesity-related genes. The results revealed the
   complex pathogenesis of obesity. In addition, the gene module detection
   and gene module multilayered annotation functions in DOSim when applied
   on these 361 obesity-related genes helped extend our understanding of
   the complex pathogenesis of obesity risk phenotypes and the
   heterogeneity of obesity-related diseases.
   Conclusions: DOSim can be used to detect disease-driven gene modules,
   and to annotate the modules for functions and pathways. The DOSim
   package can also be used to visualise DO structure. DOSim can reflect
   the modular characteristic of disease related genes and promote our
   understanding of the complex pathogenesis of diseases. DOSim is
   available on the Comprehensive R Archive Network (CRAN) or
   http://bioinfo.hrbmu.edu.cn/dosim.},
DOI = {10.1186/1471-2105-12-266},
Article-Number = {266},
ISSN = {1471-2105},
ResearcherID-Numbers = {Gong, Binsheng/E-2306-2015
   Li, Chunquan/AAI-1072-2020
   li, xia/HHZ-7468-2022
   Li, t/E-8304-2012
   },
ORCID-Numbers = {Gong, Binsheng/0000-0002-8724-5435
   Li, Chunquan/0000-0002-4700-5496
   Rao, Shaoqi/0000-0001-7809-3700},
Unique-ID = {WOS:000293569400002},
}

@article{ WOS:000291657700003,
Author = {Stoma, Szymon and Froehlich, Martina and Gerber, Susanne and Klipp, Edda},
Title = {STSE: Spatio-Temporal Simulation Environment Dedicated to Biology},
Journal = {BMC BIOINFORMATICS},
Year = {2011},
Volume = {12},
Month = {APR 28},
Abstract = {Background: Recently, the availability of high-resolution microscopy
   together with the advancements in the development of biomarkers as
   reporters of biomolecular interactions increased the importance of
   imaging methods in molecular cell biology. These techniques enable the
   investigation of cellular characteristics like volume, size and geometry
   as well as volume and geometry of intracellular compartments, and the
   amount of existing proteins in a spatially resolved manner. Such
   detailed investigations opened up many new areas of research in the
   study of spatial, complex and dynamic cellular systems. One of the
   crucial challenges for the study of such systems is the design of a well
   stuctured and optimized workflow to provide a systematic and efficient
   hypothesis verification. Computer Science can efficiently address this
   task by providing software that facilitates handling, analysis, and
   evaluation of biological data to the benefit of experimenters and
   modelers.
   Results: The Spatio-Temporal Simulation Environment (STSE) is a set of
   open-source tools provided to conduct spatio-temporal simulations in
   discrete structures based on microscopy images. The framework contains
   modules to digitize, represent, analyze, and mathematically model
   spatial distributions of biochemical species. Graphical user interface
   (GUI) tools provided with the software enable meshing of the simulation
   space based on the Voronoi concept. In addition, it supports to
   automatically acquire spatial information to the mesh from the images
   based on pixel luminosity (e. g. corresponding to molecular levels from
   microscopy images). STSE is freely available either as a stand-alone
   version or included in the linux live distribution Systems Biology
   Operational Software (SB.OS) and can be downloaded from
   http://www.stse-software.org/. The Python source code as well as a
   comprehensive user manual and video tutorials are also offered to the
   research community. We discuss main concepts of the STSE design and
   workflow. We demonstrate it's usefulness using the example of a
   signaling cascade leading to formation of a morphological gradient of
   Fus3 within the cytoplasm of the mating yeast cell Saccharomyces
   cerevisiae.
   Conclusions: STSE is an efficient and powerful novel platform, designed
   for computational handling and evaluation of microscopic images. It
   allows for an uninterrupted workflow including digitization,
   representation, analysis, and mathematical modeling. By providing the
   means to relate the simulation to the image data it allows for
   systematic, image driven model validation or rejection. STSE can be
   scripted and extended using the Python language. STSE should be
   considered rather as an API together with workflow guidelines and a
   collection of GUI tools than a stand alone application. The priority of
   the project is to provide an easy and intuitive way of extending and
   customizing software using the Python language.},
DOI = {10.1186/1471-2105-12-126},
Article-Number = {126},
ISSN = {1471-2105},
ResearcherID-Numbers = {Gerber, Susanne/I-6352-2018},
ORCID-Numbers = {Gerber, Susanne/0000-0001-9513-0729},
Unique-ID = {WOS:000291657700003},
}

@article{ WOS:000290369000023,
Author = {Cavlovic, Juro and Teslak, Krunoslav and Jazbec, Anamarija and Vedris,
   Mislav},
Title = {Impact of Stand, Site and Structural Characteristics on Stand
   Regeneration Planning in Pedunculate Oak Forests},
Journal = {CROATIAN JOURNAL OF FOREST ENGINEERING},
Year = {2011},
Volume = {32},
Number = {1},
Pages = {271-285},
Month = {APR},
Abstract = {Starting from the fact that stand structure is significantly disturbed
   and from the assumption that all stands of pedunculate oak at forest
   level older than 100 years are potentially considered for stand
   regeneration, it is obvious that forest planning and management have to
   define their priorities on the basis of which individual, potentially
   mature and mature stands, will be regenerated. The general objective of
   this paper is to research the structure and mutual impacts of stand,
   site and economic factors on the elements of stand structure in a
   greater area of pedunculate oak forests consisting of old or potentially
   mature pedunculate oak stands with a disturbed structure. Based on
   obtained structural relations, the goal was to determine by modeling the
   easily obtainable elements of the variable (model), which will represent
   the objective criterion for stand ranking according to the regeneration
   priority and structural and economic requirements.
   The subject of the research are pedunculate oak forests in the Central
   Posavina region, where 37 stands, with stocking of less than 0.8, were
   randomly selected and divided into 3 age groups (101-120 years, 121-135
   years, > 135 years) in 16 management units.
   On a total of 146 established plots of 25 m radius, estimates and
   measurements were carried out of variables at the level of individual
   model trees (6 trees of the main species upper storey layer nearest to
   the plot centre) and of stand and site variables (2 breast-height
   diameters, crown diameter, tree height, base height of the crown and
   height of the widest part of the crown, varietal trunk structure, crown
   defoliation, increment core, breast-height diameter of all trees higher
   than 1.3 m, seedlings, potential intermediate cutting, died trees,
   diameters of stump of cut trees, shrub layer, microrelief, canopy,
   health status, litter, understorey vegetation). The estimated and
   measured data were processed and appropriate variables for data analysis
   derived. The complex potential revenue (rent) difference was defined for
   stand regeneration (Equation 3) as a dependent variable and objective
   criterion for stand ranking according to regeneration priority. By means
   of partial linear analyses, multivariate analysis and modeling
   (generalized linear modeling), mutual and complex impacts of stand, site
   and economic factors on elements of stand structure and potential
   revenue (rent) difference were investigated. Statistical analyses were
   performed by application of the SAS software package, whereas
   descriptive statistics, correlation analyses, integration of functions
   and development of graphical presentations were carried out by means of
   the STATISTICA 8.2 software package.
   The research included different management, stand and structural
   characteristics (Table 1) and determined a statistically significant
   mutual impact of individual variables of stand, site and economic
   factors on certain elements of stand structure and potential revenue
   (rent) difference (Table 2). Multivariate analysis revealed models of
   complex impacts of stand, site and economic factors on standing volume
   of pedunculate oak and revenue (rent) difference (Table 3). According to
   the obtained data, all three models are statistically significant for
   the estimated standing volume of pedunculate oak and also for the
   estimated revenue (rent) difference. The stand factors in total account
   for 35\% variability of standing volume of pedunculate oak and for 22\%
   variability of revenue (rent) difference. The site factors account for
   39\% variability of standing volume of pedunculate oak and for 34\%
   variability of revenue (rent) difference, whereas economic factors
   account for even 45\% variability of standing volume of pedunculate oak
   and for 43\% variability of revenue (rent) difference. The results of
   the Tukey post hoc (HSD) test for statistically significant variables of
   multivariate analysis of the revenue (rent) difference and the standing
   volume of pedunculate oak (Table 4) indicate that there exists a
   relation between the standing volume of pedunculate oak as a complex
   structure element and the potential revenue (rent) difference. The
   results proved the existence of a direct impact of individual elements
   of main tree species as well as a complex impact of structure elements
   on the variable of potential revenue (rent) difference (Fig. 1 and Fig.
   2).
   The standing volume of pedunculate oak as a complex structure element is
   a suitable dependent variable, along with the revenue (rent) difference,
   for comparative analysis of impacts of statistically significant stand,
   site and economic factors on stand regeneration priority. Due to a
   significantly smaller standing volume of pedunculate oak as a
   consequence of its disturbed structure, there is a higher regeneration
   priority for the youngest stand group (100 to 119 years) in comparison
   with the older stands. Accordingly, in forests with disturbed structure
   age cannot be an objective criterion for planning stand regeneration.
   Stands with disturbed canopy of understorey layer with the smallest and
   lowest value of standing volume expectedly have the highest regeneration
   priority, i.e. their maintenance would lead to both economic losses and
   disturbed site conditions. In view of site preservation, the problem is
   particularly marked in case of pure stands, where pedunculate oak trees
   do not cover the soil in a sufficient measure, and have a worsening
   tendency due to weakening and dying of trees. According to the obtained
   results, which indicate that there is a relation between site and
   standing volume of pedunculate oak and revenue (rent) difference, stands
   with better site have lower regeneration priority. This is related to
   the fact that stand structure on a higher quality site is characterized
   by a lower number of pedunculate oak trees of larger dimensions and
   potentially large value increase, which is also characteristic of forest
   communities of pedunculate oak and common hornbeam. On average, a
   smaller standing volume of pedunculate oak in a floodplain pedunculate
   oak forest and a higher regeneration priority indicate disturbed
   structure conditions. Due to hydro-ameliorative works, this community
   has suffered even greater site changes and is more in danger of dieback,
   so regeneration will be more difficult and expectably less successful in
   case of stands with disturbed site and structure conditions, both at
   early and delayed regeneration. Regarding the proportional relation
   between the quantity of prescribed cut and the standing volume of
   pedunculate oak, stands with better structure and higher prescribed
   intermediate cut are expected to have a lower regeneration priority. A
   prescribed cut below 15 m(3)/ha (50\% sampled plots) indicates a
   significant share of preserved stands left to natural development, with
   necessary felling of only weak and dying trees by the beginning of
   regeneration or recovery. The stand structure characteristics according
   to their belonging to individual management units are more a consequence
   of spatial differences in site and structure conditions than a
   management characteristic. Thus we can separate 3 management units in
   the centre of the Lonjsko polje floodplain (Zutica, Kutina flood plain
   forests and Brezovica), where dieback of pedunculate oak was the highest
   and which have a more marked stand regeneration priority in comparison
   to the management units with a more preserved structure of old stands
   (Trstika, Josip Kozarac). The impact of individual structure elements of
   pedunculate oak, unlike the rent, has a high correlation, with the
   exception of numbers of pedunculate oak trees. This can be explained by
   the fact that the same number of pedunculate oak trees can represent
   very different structural relations in terms of average dimensions of
   pedunculate oak trees, basal area and standing volume of pedunculate oak
   as well as different rate of other tree species in the structure.
   A particular significnce with regards to the application of the results
   can be seen in the fact that easily obtainable structural elements, such
   as pedunculate oak basal area, number of pedunculate oak trees and
   diameter increment of pedunculate oak, proved to be very good
   indicators, which can explain nearly 70\% variability of the complex
   impact on the revenue (rent) difference.
   The obtained research results prove the existence of a direct relation
   between stand structure and regeneration priority. There is, therefore,
   a practical possibility to apply the established complex variable of
   revenue (rent) difference for ranking of stands or stand parts (stand
   dividing) according to regeneration priority, which can be determined by
   means of easily obtainable elements of structure (tree numbers, basal
   area and diameter increment of pedunculate oak). An extension of the
   research into site-structure relations to include a greater, or even the
   entire area of pedunculate oak forests in Croatia as well as spatial and
   multiobjective forest management criteria (spatial distribution and area
   of stands for regeneration, habitat requirements) for stand regeneration
   planning would be a welcome continuation of this work.},
ISSN = {1845-5719},
ResearcherID-Numbers = {Jazbec, Anamarija/U-9521-2017
   Teslak, Krunoslav/I-5429-2017
   Vedriš, Mislav/I-5722-2017},
Unique-ID = {WOS:000290369000023},
}

@article{ WOS:000288658800009,
Author = {Wissenbach, Dirk K. and Meyer, Markus R. and Remane, Daniela and Weber,
   Armin A. and Maurer, Hans H.},
Title = {Development of the first metabolite-based LC-MS (n) urine drug screening
   procedure-exemplified for antidepressants},
Journal = {ANALYTICAL AND BIOANALYTICAL CHEMISTRY},
Year = {2011},
Volume = {400},
Number = {1},
Pages = {79-88},
Month = {APR},
Abstract = {In contrast to GC-MS libraries, currently available LC-MS libraries for
   toxicological detection contain besides parent drugs only some main
   metabolites limiting their applicability for urine screening. Therefore,
   a metabolite-based LC-MS (n) screening procedure was developed and
   exemplified for antidepressants. The library was built up with MS2 and
   MS3 wideband spectra using an LXQ linear ion trap with electrospray
   ionization in the positive mode and full-scan information-dependent
   acquisition. Pure substance spectra were recorded in methanolic solution
   and metabolite spectra in urine from rats after administration of the
   corresponding drugs. After identification, the metabolite spectra were
   added to the library. Various drugs and metabolites could be
   sufficiently separated. Recovery, process efficiency, matrix effects,
   and limits of detection for selected drugs were determined using protein
   precipitation. Automatic data evaluation was performed using ToxID and
   SmileMS software. The library consists of over 700 parent compounds
   including 45 antidepressants, over 1,600 metabolites, and artifacts.
   Protein precipitation led to sufficient results for sample preparation.
   ToxID and SmileMS were both suitable for target screening with some pros
   and cons. In our study, only SmileMS was suitable for untargeted
   screening being not limited to precursor selection. The LC-MS (n) method
   was suitable for urine screening as exemplified for antidepressants. It
   also allowed detecting unknown compounds based on known fragment
   structures. As ion suppression can never be excluded, it is advantageous
   to have several targets per drug. Furthermore, the detection of
   metabolites confirms the body passage. The presented LC-MS (n) method
   complements established GC-MS or LC-MS procedures in the authors' lab.},
DOI = {10.1007/s00216-010-4398-9},
ISSN = {1618-2642},
EISSN = {1618-2650},
ResearcherID-Numbers = {Meyer, Markus R/B-9293-2009
   Wissenbach, Dirk/B-1230-2012
   },
ORCID-Numbers = {Maurer, Hans/0000-0003-4579-4660},
Unique-ID = {WOS:000288658800009},
}

@article{ WOS:000288728300001,
Author = {Baeta, Alexandra and Niquil, Nathalie and Marques, Joao C. and Patricio,
   Joana},
Title = {Modelling the effects of eutrophication, mitigation measures and an
   extreme flood event on estuarine benthic food webs},
Journal = {ECOLOGICAL MODELLING},
Year = {2011},
Volume = {222},
Number = {6},
Pages = {1209-1221},
Month = {MAR 24},
Abstract = {Human-mediated and natural disturbances such as nutrient enrichment,
   habitat modification, and flood events often result in significant
   shifts in species composition and abundance that translate into changes
   in the food web structure. Six mass-balanced models were developed using
   the ``Ecopath with Ecosim{''} software package to assess changes in
   benthic food web properties in the Mondego estuarine ecosystem
   (Portugal). Field, laboratory and literature information were used to
   construct the models. The main study objective was to assess at 2 sites
   (a Zostera meadow and a bare sediment area) the effects of: (1) a period
   of anthropogenic enrichment, which led to excessive production of
   organic matter in the form of algal blooms (1993/1994); (2) the
   implementation of mitigation measures, following a long period of
   eutrophication (1999/2000); and (3) a centenary flood (winter
   2000/2001). Different numbers of compartments were identified at each
   site and in each time period. In general, the Zostera site, due to its
   complex community, showed a higher number of compartments and a higher
   level of system activity (i.e. sum of consumptions, respiration, flow to
   detritus, production, total system throughput, net primary production
   and system omnivory index). The differences at the two sites in the
   three time periods in the breakdown of throughput were mainly due to
   differences in the biomass of the primary producers (higher primary
   production at the Zostera site). Consumption, respiration and flow to
   detritus were dominated by the grazers Hydrobia ulvae and Scrobicularia
   piano at the Zostera and bare sediment sites respectively. At both
   sites, after recovery measures were implemented there was an increase in
   S. piano and Hediste diversicolor biomass, consumption, respiration and
   flows to detritus, and a decrease in H. ulvae biomass and associated
   flows, which increased again after the flood event. The mass-balanced
   models showed that the trophic structure of the benthic communities in
   Mondego estuary was affected differently by each disturbance event.
   Interestingly, in our study a high system throughput seems to be
   associated with higher stress levels, which contradicts the idea that
   higher system activity is always a sign of healthier conditions. (C)
   2010 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.ecolmodel.2010.12.010},
ISSN = {0304-3800},
EISSN = {1872-7026},
ResearcherID-Numbers = {Patricio, Joana/O-5030-2014
   Niquil, Nathalie/N-2561-2013
   Niquil, Nathalie/C-8233-2009
   MARQUES, JOAO CARLOS/L-9478-2014
   Baeta, Alexandra/Q-6704-2018},
ORCID-Numbers = {Patricio, Joana/0000-0003-1832-0996
   MARQUES, JOAO CARLOS/0000-0001-8865-8189
   Niquil, Nathalie/0000-0002-0772-754X
   Baeta, Alexandra/0000-0001-9898-9096},
Unique-ID = {WOS:000288728300001},
}

@article{ WOS:000287519000004,
Author = {Delene, David J.},
Title = {Airborne data processing and analysis software package},
Journal = {EARTH SCIENCE INFORMATICS},
Year = {2011},
Volume = {4},
Number = {1},
Pages = {29-44},
Month = {MAR},
Abstract = {The practice of conducting quality control and quality assurance in the
   construction of data sets is often an overlooked and underestimated task
   of many research projects in the Earth Sciences. The development of
   software to effectively process and quickly analyze measurements is a
   critical aspect of a research project. An evolutionary approach has been
   used at the University of North Dakota to develop and implement software
   to process and analyze airborne measurements. Development over the past
   eight years has resulted in a collection of software named the Airborne
   Data Processing and Analysis (ADPAA) package which has been published as
   an open source project on Source Forge. The ADPAA package is intended to
   fully automate data processing while incorporating the concept of
   missing value codes and levels of data processing. At each data level,
   ADPAA utilizes a standard ASCII file format to store measurements from
   individual instruments into separate files. After all data levels have
   been processed, a summary file containing parameters of scientific
   interest for the field project is created for each aircraft flight. All
   project information is organized into a standard directory structure.
   ADPAA contains several tools that facilitate quality control procedures
   conducted on instruments during field projects and laboratory testing.
   Each quality control procedure is designed to ensure proper instrument
   performance and hence the validity of the instrument's measurement. Data
   processing by ADPAA allows edit files to be created that are
   automatically used to insert missing value codes into a time period that
   had instrument problems. The creation of edit files is typically done
   after the completion of a field project when scientists are performing
   quality assurance of the data set. Since data processing is automatic,
   preliminary data can be created and analyzed within hours of an aircraft
   flight and a complete field project data set can be reprocessed many
   times during the quality assurance process. Once a final data set has
   been created, ADPAA provides several tools for visualization and
   analysis. In addition to aircraft data, ADPAA can be used on any data
   set that is based on time series measurements. The concepts illustrated
   by ADPAA and components of ADPAA, such as the Cplot visualization tool,
   are applicable to areas of Earth Science that work with time series
   measurements.},
DOI = {10.1007/s12145-010-0061-4},
ISSN = {1865-0473},
Unique-ID = {WOS:000287519000004},
}

@article{ WOS:000288644400009,
Author = {Lagorce, David and Villoutreix, Bruno O. and Miteva, Maria A.},
Title = {Three-dimensional structure generators of drug-like compounds: DG-AMMOS,
   an open-source package},
Journal = {EXPERT OPINION ON DRUG DISCOVERY},
Year = {2011},
Volume = {6},
Number = {3},
Pages = {339-351},
Month = {MAR},
Abstract = {Introduction: Drug discovery is a time consuming and costly process.
   Thus, a trend towards the use of in silico approaches such as structure-
   and ligand-based virtual screening methods to speed up the process has
   gained significant momentum in recent years. Most of these in silico
   applications require a good quality 3D structure of the small drug-like
   molecules as input.
   Areas covered: This article reviews the algorithm and validation of the
   open-source software DG-AMMOS, a tool that generates the 3D conformation
   of small molecules using distance geometry construction and molecular
   mechanics optimization comparing its performance with some related free
   and commercial packages.
   Expert opinion: The number of chemo/bioinformatics free and/or
   open-source tools assisting drug discovery projects is increasing, and
   many successful contributions making use of these computer programs have
   been reported. DG-AMMOS is an efficient 3D structure generator engine
   that provides fast and reliable generation of 3D structures and
   contributes to the preparation of a compound collection. DG-AMMOS can
   still be improved and an increased speed and user-friendly interface in
   addition to the implementation of workflow engines will increase its
   effectiveness.},
DOI = {10.1517/17460441.2011.554393},
ISSN = {1746-0441},
EISSN = {1746-045X},
ResearcherID-Numbers = {Miteva, Maria M/N-2419-2018
   Villoutreix, Bruno/I-4565-2015},
ORCID-Numbers = {Miteva, Maria M/0000-0001-6895-1214
   Villoutreix, Bruno/0000-0002-6456-7730},
Unique-ID = {WOS:000288644400009},
}

@article{ WOS:000287085500025,
Author = {Saranli, Uluc and Avci, Akin and Oeztuerk, M. Cihan},
Title = {A Modular Real-Time Fieldbus Architecture for Mobile Robotic Platforms},
Journal = {IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT},
Year = {2011},
Volume = {60},
Number = {3},
Pages = {916-927},
Month = {MAR},
Abstract = {The design and construction of complex and reconfigurable embedded
   systems such as small autonomous mobile robots is a challenging task
   that involves the selection, interfacing, and programming of a large
   number of sensors and actuators. Facilitating this tedious process
   requires modularity and extensibility both in hardware and software
   components. In this paper, we introduce the universal robot bus (URB), a
   real-time fieldbus architecture that facilitates rapid integration of
   heterogeneous sensor and actuator nodes to a central processing unit
   (CPU) while providing a software abstraction that eliminates
   complications arising from the lack of hardware homogeneity. Motivated
   by our primary application area of mobile robotics, URB is designed to
   be very lightweight and efficient, with real-time support for
   Recommended Standard (RS) 232 or universal serial bus connections to a
   central computer and inter-integrated circuit ((IC)-C-2), controller
   area network, or RS485 bus connections to embedded nodes. It supports
   automatic synchronization of data acquisition across multiple nodes,
   provides high data bandwidth at low deterministic latencies, and
   includes flexible libraries for modular software development both for
   local nodes and the CPU. This paper describes the design of the URB
   architecture, provides a careful experimental characterization of its
   performance, and demonstrates its utility in the context of its
   deployment in a legged robot platform.},
DOI = {10.1109/TIM.2010.2078351},
ISSN = {0018-9456},
EISSN = {1557-9662},
ResearcherID-Numbers = {Saranli, Uluc/A-8383-2018
   Saranlı, Uluç/ABA-5059-2020},
Unique-ID = {WOS:000287085500025},
}

@inproceedings{ WOS:000314667200011,
Author = {Bahtui, Ali and Tkach, Yuri},
Book-Group-Author = {ASME},
Title = {COMPUTATIONAL FRACTURE MECHANICS ASSESSMENT OF 3-D CRACK-LIKE DEFECTS IN
   COMPLEX GEOMETRIES},
Booktitle = {OMAE2011: PROCEEDINGS OF THE ASME 30TH INTERNATIONAL CONFERENCE ON
   OCEAN, OFFSHORE AND ARCTIC ENGINEERING, VOL 3: MATERIALS TECHNOLOGY},
Year = {2011},
Pages = {123-129},
Note = {30th International Conference on Ocean, Offshore and Arctic Engineering,
   Rotterdam, NETHERLANDS, JUN 19-24, 2011},
Organization = {ASME, Ocean Offshore \& Arctic Engn Div},
Abstract = {The application of a fracture mechanics based methodology for integrity
   assessment of complex structural components containing flaws often
   requires computational simulation where the flaws have to be modeled
   explicitly due to the absence of the appropriate analytical solutions.
   Moreover, for large flaws, re-distribution of loads may occur and
   therefore for an accurate assessment the flaw needs to be embedded
   within the structural model.
   A critical issue that must be addressed in a three-dimensional finite
   element simulation of a structural component containing a crack-like
   defect is that of mesh generation. In this paper, an efficient
   methodology has been developed based on the use of general purpose
   finite element packages and other commercial software to automatically
   generate the meshes. The model reflects the full complexity of a 3-D
   structure ensuring at the same time the required mesh resolution around
   the crack front for the determination of parameters used in fracture
   mechanics assessment.
   The approach is demonstrated through computational simulations of a
   tubular K-joint containing a crack-like defect with elastic and
   elastic-plastic material properties. From these analyses, the parameters
   required for construction of a geometry-dependent and material-specific
   failure assessment diagram (FAD) are derived. Results of the fully
   computational fracture mechanics assessment are compared with those
   generated in accordance with Annexes B and P of BS 7910:2005. It is
   demonstrated that the level of conservatism in fracture integrity
   assessment of tubular joints can be reduced if the crack driving force
   is determined directly from the computational analysis of the cracked
   geometry. A discussion on the application of different limit load and
   reference stress solutions is presented and the importance of the
   appropriate selection of the limit load solution is highlighted.},
ISBN = {978-0-7918-4435-9},
Unique-ID = {WOS:000314667200011},
}

@inproceedings{ WOS:000324383402133,
Author = {Blumenthal, Sebastian and Prassler, Erwin and Fischer, Jan and Nowak,
   Walter},
Book-Group-Author = {IEEE},
Title = {Towards Identification of Best Practice Algorithms in 3D Perception and
   Modeling},
Booktitle = {2011 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA)},
Series = {IEEE International Conference on Robotics and Automation ICRA},
Year = {2011},
Note = {IEEE International Conference on Robotics and Automation (ICRA),
   Shanghai, PEOPLES R CHINA, MAY 09-13, 2011},
Organization = {IEEE; Robot \& Automat Soc; Minist Educ China; Minist Sci \& Technol
   China; Natl Nat Sci Fdn China; Sci \& Technol Commiss Shanghai
   Municipal; Shanghai Jiao Tong Univ, State Key Lab Mech Syst \& Vibrat;
   Huazhong Univ Sci \& Technol, State Key Lab Digital Mfg Equipment \&
   Technol; Harbin Inst Technol, State Key Lab Robot \& Syst; Zhejiang
   Univ, Inst Cyber-Syst \& Control; Chinese Acad Sci, Shenyang Inst
   Automat; Beihang Univ, Robotics Inst; Beijing Res Inst Automat Machinery
   Ind; Tianjin Univ, Sch Mech Engn; ABB; YASKAWA Elect; KUKA; Willow
   Garage; Googol Tech.; Adept Mobile Robots; Harbin Boshi Automat; Natl
   Instruments; Beijing Universal Pioneering Technol; Real-Time Control \&
   Instrumentat Lab, GE Global Res; ALDEBARAN Robot; Int Federat Robot
   (IFR); Shanghai Jiao Tong Univ},
Abstract = {Robots need a representation of their environment to reason about and to
   interact with it. Different 3D perception and modeling approaches exist
   to create such a representation, but they are not yet easily comparable.
   This work tries to identify best practice algorithms in the domain of 3D
   perception and modeling with a focus on environment reconstruction for
   robotic applications. The goal is to have a collection of refactored
   algorithms that are easily measurable and comparable. The realization
   follows a methodology consisting of five steps. After a survey of
   relevant algorithms and libraries, common representations for the core
   data-types Cartesian point, Cartesian point cloud and triangle mesh are
   identified for use in harmonized interfaces. Atomic algorithms are
   encapsulated into four software components: the Octree component, the
   Iterative Closest Point component, the k-Nearest Neighbors search
   component and the Delaunay triangulation component. A sample experiment
   demonstrates how the component structure can be used to deduce best
   practice.},
ISSN = {1050-4729},
EISSN = {2577-087X},
ISBN = {978-1-61284-385-8},
ResearcherID-Numbers = {Nowak, Wolfgang/C-6487-2011},
ORCID-Numbers = {Nowak, Wolfgang/0000-0003-2583-8865},
Unique-ID = {WOS:000324383402133},
}

@inproceedings{ WOS:000378520700036,
Author = {Chen, Don and Gehrig, G. Bruce},
Book-Group-Author = {ASEE},
Title = {Implementing Building Information Modeling in Construction Engineering
   Curricula},
Booktitle = {2011 ASEE ANNUAL CONFERENCE \& EXPOSITION},
Series = {ASEE Annual Conference \& Exposition},
Year = {2011},
Note = {ASEE Annual Conference and Exposition, Vancouver, CANADA, JUN 26-29,
   2011},
Organization = {ASEE},
Abstract = {Building Information Modeling (BIM) has been profoundly transforming the
   architecture, engineering, and construction (AEC) industry and likewise
   is having a similar impact on construction engineering (ConE) education.
   Without understanding the limitations of existing BIM software packages,
   the acceptance and implementation of BIM in ConE education will not take
   place. This paper presents a study that was conducted to compare the
   functions of Autodesk Revit products and the Vico Virtual Construction
   Software Suite, the two most popular BIM solutions, and to determine the
   feasible BIM-based work flow for fulfilling the Body of Knowledge (BOK)
   requirements in ConE, such as cost estimating, construction scheduling
   and control, project administration, and contract documents. This paper
   describes the procedures for identifying the strengths and weaknesses of
   each software package, and proposes a protocol of implementing BIM in
   ConE curricula. Finally, conclusions are made and recommendations for
   avenues of future research are made.},
ISSN = {2153-5965},
Unique-ID = {WOS:000378520700036},
}

@inproceedings{ WOS:000290932300003,
Author = {Cheng, Chung-Kuan},
Book-Group-Author = {ACM/SIGDA
   CAS},
Title = {Placement and Beyond in Honor of Ernest S. Kuh},
Booktitle = {ISPD 11: PROCEEDINGS OF THE 2011 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON
   PHYSICAL DESIGN},
Year = {2011},
Pages = {5-7},
Note = {International Symposium on Physical Design, Santa Barbara, CA, MAR
   27-30, 2011},
Organization = {ACM SIGDA; CAS},
Abstract = {Professor Kuh is a pioneer and giant in physical layout. In this talk,
   we will describe his influence in placement. His pioneering work from
   interval graph for one dimensional gate assignment, BBL (Building-Block
   Layout System for Custom Chip IC Design) {[}2, 3, 6, 8], BEAR {[}7]
   layout system, BAGEL (Gate Array Layout) {[}17], RAMP (Resistive Analog
   Module Placement) {[}5], PROUD (Sea of Gates Placement) {[}28] to
   congestion, timing, and low power driven placement, Prof. Kuh always
   starts with innovative theoretical construction, software system
   building, and applications with impact on productivity.
   Physical layout is an indispensable software system for VLSI Design with
   millions of modules. Placement is the key component of about 500 million
   dollars market for physical synthesis. In the layout design flow, the
   placement is the core of the system integrating with other synthesis and
   analysis tools.
   For building block layout, Kuh's group tackled the nonslicing
   architecture. They devised the tile plane to represent the topology of
   the floorplan and the bottleneck of the routing. A routing order is
   derived to guarantee 100 percent routing completion {[}12]. The
   methodology and algorithms of building block placement {[}2] were
   adopted by companies such as Digital Equipment Corporation and ECAD,
   which was later renamed as Cadence.
   For standard cell placement, RAMP placement is devised using the analogy
   of a resistive network {[}12]. The minimization of the circuit power
   corresponds to the quadratic wire length reduction. The approach
   provides a convergent solution in the era of interconnect dominance. As
   the technology scales, interconnect becomes dominating the system
   performance in terms on delay and power consumption. Performance driven
   logic synthesis, signal interconnect with repeater insertion, power
   ground and clock distribution strongly rely on the physical layout
   information. On the other hand the placement requires the synthesis
   result to perform the task. This mutual dependence has caused serious
   design convergence issues in the 1990s.
   The work was performed at a time when simulated annealing method became
   a fashion in the EDA field. In 1983, Kirpatrick et al. {[}15] adopted
   the annealing method for placement and achieved excellent results. The
   annealing method takes the analogy of thermal annealing to perturb the
   partial solution with a probability according to the simulated
   temperature of the process. The strategy derives excellent results on
   some difficult problems at the expense of many random trials of the
   perturbations. Since most of EDA problem are known to be NP complete,
   annealing approach was considered as the right tool to solve the
   problem. Significant resources and efforts have thus been focused on
   annealing method in placement and other EDA subjects.
   The RAMP placement 5 method falls into the category of the analytical
   method. In 1979, Quinn and Breuer {[}23] introduced a force model to
   determine the state of equilibrium. They applied Hook's law to attract
   the modules connected by signal nets and repulsive forces to separate
   the modules with no connection. The repulsive forces cause a large set
   of nonlinear equations in the formulation, which complicates the
   calculation. An improvement has been proposed by Antreich, Johannes, and
   Kirsh {[}1] using the same force-directed method but with a more
   systematic formulation of equations. Kuh's approach removes the
   repulsive forces to simplify the nonlinear programming problem. His
   group demonstrated the feasibility of the global optimization approach
   via fast sparse matrix solvers {[}27]. The RAMP placement package was
   first installed at Hughes Aircraft Company to automate the placement in
   an in-house EDA project in 1983. The system had been used by the gate
   array design group since then. In 1988, in Kuh's group, Ren-Song Tsay
   developed PROUD to replace the matrix solver with successive over
   relaxation method. In 1992, at Cadence, CAD engineer, Louis Chao,
   implemented the algorithm and coined the placement tool as Qplacer to
   emphasize the usage of the quadratic cost function in the formulation.
   The Qplacer is the main placement tool of the ASIC design community for
   a long time. In the same year (1992), Ren-Song Tsay implemented the
   quadratic placement package at a start-up company which was later
   renamed as Avanti and acquired by Synopsys. The Qplacer became the main
   placement tool of the ASIC design community. Up to now, all major EDA
   companies, i.e. Cadence, Magma, Mentor Graphics, and Synopsys adopted
   the quadratic placement strategy and its extension by the group of
   Johannes. The placement is integrated with timing analysis and
   interconnect synthesis to provide convergent solutions for the
   designers.
   For the usage of the quadratic placement, it took 104 seconds at IMIPs
   machine to place 136 modules as the tool was installed at Hughes in
   1983. In 1991, Kleinhans et al. {[}16] reported to take 2500 seconds at
   15MIPs machine to place 6417 modules. In 1998, Eisenmann and Johannes
   {[}9] took 2031 seconds at Alphastation 250 4/266 (266MHz) to place 25K
   modules. As the quadratic placement method become popular, more features
   are integrated into the software package. In current designs, a state of
   art package can handle 6-7M components with 30-40 transistors. The limit
   is set by the memory capacity used for the analysis routine. Kuh's group
   has hosted a thriving set of Ph.D. students and visiting scholars to
   study placement. Margaret Sadowska has been a mentor to many in the
   group. The building block layout was contributed by Nang-Ping Chen,
   Chi-Ping Hsu, Chao-Chiang Chen, Wayne Dai, Bernhard Eschermann, Massoud
   Pedram, Yasushi Ogawa, and Margaret Sadowska. The channel ordering
   scheme for the layout {[}6] was published by Wayne Dai and Tetsuo Asano.
   The gate array layout {[}17] was constructed by Margaret Sadowska,
   Jeong-Tyng Li and C.K. Cheng. The standard placement has been studied by
   C.K. Cheng, Ren-Song Tsay; the low power placement by Massoud Pedram;
   the timing driven placement {[}9, 20] by Shen Lin, Srinivasan Arvind,
   Michael Jaskson, Henrik Esbensen, and Margaret Sadowska; the IO
   assignment {[}22] by Massoud Pedram, Narasimha Bhat, Kamal Chaudhary,
   Deborah Wang, and Margaret Sadowska. Dong-Min Xu developed gate matrix
   layout {[}29, 30]; Minshine Shih studied partitioning {[}29, 30];
   Pinhong Chen worked on floorplan sizing {[}4]; Hiroshi Murata extended a
   floorplan representation, sequence pair, to handle a mixture of soft and
   hard modules {[}18].
   As a second wave, renowned visiting scholars went back and continued to
   contribute to placement. Prof. Hidoshi Onodera used branch and bound
   approach for building block placement at Kyoto University in 1991
   {[}21]. Prof. Xianlong Hong devised an efficient floorplan
   representation, corner block list at Tsinghua University in 2000 {[}10].
   Prof. John Lillis devised a new placement tool, Mongrel using hybrid
   techniques for standard cell placement at University of Illinois,
   Chicago in 2000 {[}11]. Prof. Andrew B. Kahng's group at UC San Diego
   developed an APlacer which won ACM International Symposium on Physical
   Design placement contest in 2005 {[}13, 14]. At Kuh's group, we had the
   privilege to meet many leaders in placement, e.g. Satoshi Goto, Tatsuo
   Ohtsuki, Brian Preas, David Liu, T.C. Hu, Jerry Lee, Scott Kirkpatrick,
   Melvin A. Breuer, Ulrich Lauther, and Kurt Antreich.
   The placement with analogy of resistive network was inspired by Kuh's
   book entitled ``Basic Circuit Theory{''}. Kuh's group continues to
   investigate circuit analysis and synthesis. In 1991 {[}31], Shen Lin and
   Margaret Sadowska constructed an efficient timing simulator using a Step
   Wise Equivalent Conductance (SWEC) for nonlinear devices. In 1999
   {[}32], Janet Wang and Qingjian Yu simplified transmission line analysis
   using model order reduction. In recent years (2005-2009), Kuh has
   advised analysis and synthesis of transmission lines using passive
   equalizer {[}33-38] and circuit simulation techniques {[}39-41].
   VLSI placement is a challenge task with the increasing complexity of the
   circuit. The size of the modular placement cases scales with the advance
   of VLSI technologies. In 1961, Steinberg introduced a case of 34
   modules. In 1972, Stevens in his thesis posted the ILLIAC IV boards with
   67-151 modules {[}26]. In 1983, the test case from Hughes ranges from
   300 to 500 modules. By 1991, the largest test case released by MCNC
   Center for Microelectronics is 15K modules. Moreover, subjects in
   placement such as mixed module placement, placement of heterogeneous
   circuits, placement integrating with behavior synthesis, 3-dimensional
   placement, and parallel placement are related to geometry handling,
   circuit performance, and advancement of the technologies and call for
   further study in the field.},
ISBN = {978-1-4503-0711-6},
Unique-ID = {WOS:000290932300003},
}

@inproceedings{ WOS:000392144900019,
Author = {Falleri, Jean-Remy and Denier, Simon and Laval, Jannik and Vismara,
   Philippe and Ducasse, Stephane},
Editor = {Bishop, J and Vallecillo, A},
Title = {Efficient Retrieval and Ranking of Undesired Package Cycles in Large
   Software Systems},
Booktitle = {OBJECTS, MODELS, COMPONENTS, PATTERNS, TOOLS 2011},
Series = {Lecture Notes in Computer Science},
Year = {2011},
Volume = {6705},
Pages = {260-275},
Note = {49th International Conference on Objects, Models, Components, Patterns
   (TOOLS), Swiss Fed Inst Technol, Zurich, SWITZERLAND, JUN 28-30, 2011},
Organization = {Chair Software Engn},
Abstract = {Many design guidelines state that a software system architecture should
   avoid cycles between its packages. Yet such cycles appear again and
   again in many programs. We believe that the existing approaches for
   cycle detection are too coarse to assist the developers to remove cycles
   from their programs. In this paper, we describe an efficient algorithm
   that performs a fine-grained analysis of the cycles among the packages
   of an application. In addition, we define a metric to rank cycles by
   their level of undesirability, prioritizing the cycles that seems the
   more undesired by the developers. Our approach is validated on two large
   and mature software systems in Java and Smalltalk.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-21952-8},
ORCID-Numbers = {Ducasse, Stephane/0000-0001-6070-6599
   Laval, Jannik/0000-0002-7155-5762
   Falleri, Jean-Remy/0000-0002-8284-7218},
Unique-ID = {WOS:000392144900019},
}

@inproceedings{ WOS:000298861500002,
Author = {Farkas, David K. and Larson, Jerrod and Naranjo, Steven J.},
Book-Group-Author = {IEEE},
Title = {LabelPatterns.Org: A Comprehensive Pattern Library for Consumer-Decision
   Labels},
Booktitle = {2011 IEEE INTERNATIONAL PROFESSIONAL COMMUNICATION CONFERENCE (IPCC)},
Year = {2011},
Note = {IEEE International Professional Communication Conference (IPCC),
   Cincinnati, OH, OCT 17-19, 2011},
Organization = {IEEE; IEEE Profess Commun Soc},
Abstract = {Consumer-decision labels are relatively small panels of information,
   placed where consumers make decisions, that help those consumers make
   informed choices and, at times, motivate desired behaviors. They provide
   information about environmental impact/sustainability, nutrition,
   health, safety, the quality and suitability of consumer goods, and other
   domains. Design patterns are expanded guidelines that follow a
   problem-solution structure, provide more context than standard
   guidelines, and are supported when possible by citations to relevant
   research and professional literature. Pattern libraries are sets of
   coordinated patterns that strive to comprehensively support the design
   process in a particular domain. Pattern libraries have proven successful
   and are now used in such domains as urban planning, object-oriented
   programming, software user interface design, and web design.
   LabelPatterns.org is a newly launched website currently hosting over 75
   design patterns that support the design of consumer-decision labels. It
   also offers other kinds of information about these labels and related
   messaging. The patterns and the website were begun as student projects
   in the Department of Human Centered Design \& Engineering at the
   University of Washington, USA. It is now being managed and expanded by a
   volunteer project team.},
ISBN = {978-1-61284-779-5},
Unique-ID = {WOS:000298861500002},
}

@article{ WOS:000287248800005,
Author = {Hsu, Yi-Cheng},
Title = {An Optimum Design and Fabrication of Lens on Luminous Uniformity and
   Light Extraction of High-Power Light-Emitting Diode},
Journal = {OPTICAL REVIEW},
Year = {2011},
Volume = {18},
Number = {1},
Pages = {27-33},
Month = {JAN},
Abstract = {The effect of lens inner structure on radiation pattern and light
   extraction efficiency of light-emitting diode (LED) and the application
   to artificial light in compact greenhouse is demonstrated. A commercial
   software package of Trace Pro and one-factor at-a-time (OFAT) method are
   used to simulate the lens with different inner structure. The optimum
   parameters of lens inner structure design for the maximum light
   extraction efficiency and the best uniform luminous are described by the
   corner radius of curvature, lens width, and lens height, respectively.
   For a real single LED module, base on the optimum parameters of lens
   inner structure, the corresponding best luminous uniformity is 62\% and
   corresponding output extraction is 14.11 lm. The maximum uniformity of
   illumination for LED matrix assembled by LED modules with optimum lens
   is 88\% and corresponding light extraction 1141 lm. In comparison with a
   commercial artificial light of LED matrix used in agriculture, the
   high-power LED module with proposed lens inner structure exhibit good
   improvement in uniformity of illumination and light extraction. This
   study may provide a practical guideline for design and fabrication of a
   high-performance lens used in various compact agricultural applications.
   (C) 2011 The Japan Society of Applied Physics},
DOI = {10.1007/s10043-011-0024-0},
ISSN = {1340-6000},
EISSN = {1349-9432},
Unique-ID = {WOS:000287248800005},
}

@article{ WOS:000299937500003,
Author = {Huang, Fang and Liu, Dingsheng and Li, Xiaowen and Wang, Lizhe and Xu,
   Wenbo},
Title = {Preliminary study of a cluster-based open-source parallel GIS based on
   the GRASS GIS},
Journal = {INTERNATIONAL JOURNAL OF DIGITAL EARTH},
Year = {2011},
Volume = {4},
Number = {5},
Pages = {402-420},
Abstract = {In response to the problem of how to give geographic information system
   (GIS) high-performance capabilities for certain specific GIS
   applications, a new GIS research direction, parallel GIS processing, has
   emerged. However, traditional research has focused mostly on
   implementing typical GIS parallel algorithms, with little discussion of
   how to parallelize an entire GIS package on clusters based on theory.
   Therefore, the authors have chosen the geographic resources analysis
   support system (GRASS) GIS as the object of their research and have put
   forward the concept of a cluster-based open-source parallel GIS
   (cluster-based OP-GIS) as a tool to support Digital Earth construction.
   The related theory includes not only the parallel computing mode,
   architecture, and software framework of such a system, but also various
   parallelization patterns. From experiments on the prototype system, it
   can be concluded that the parallel system has better efficiency and
   performance than the conventional system on certain selected modules.},
DOI = {10.1080/17538947.2010.543954},
ISSN = {1753-8947},
EISSN = {1753-8955},
ResearcherID-Numbers = {Wang, Lizhe/L-7453-2014
   Huang, Fang/AAG-3688-2019},
ORCID-Numbers = {Wang, Lizhe/0000-0003-2766-0845
   },
Unique-ID = {WOS:000299937500003},
}

@inproceedings{ WOS:000395775400065,
Author = {Kapitza, P. J.},
Editor = {Clincy, VA},
Title = {An Implementation of Heegaard Diagrams},
Booktitle = {PROCEEDINGS OF THE 49TH ANNUAL ASSOCIATION FOR COMPUTING MACHINERY
   SOUTHEAST CONFERENCE (ACMSE `11)},
Year = {2011},
Pages = {302-303},
Note = {49th Annual Association-for-Computing-Machinery (ACM) Southeast
   Conference (ACMSE), Kennesaw, GA, MAR 24-26, 2011},
Organization = {Assoc Comp Machinery},
Abstract = {A software platform for the construction and study of three dimensional
   manifold structures and more generally, of two dimensional cell
   complexes is presented. Interactive operations allow the construction of
   3-manifold spaces in planar form. Manifold invariants, based upon the
   diagrams and their associated presentations are constructed directly.
   The package incorporates the operations defined by J. Singer. This
   package enables a user to graphically define and apply 3-manifold
   operations in the context of Heegaard Diagrams.},
ISBN = {978-1-4503-0686-7},
Unique-ID = {WOS:000395775400065},
}

@inproceedings{ WOS:000299539700063,
Author = {Kwon, Jagun and Hailes, Stephen},
Book-Group-Author = {IEEE},
Title = {A Lightweight, Component-based Approach to Engineering Reconfigurable
   Embedded Real-Time Control Software},
Booktitle = {2011 NINTH IEEE INTERNATIONAL SYMPOSIUM ON PARALLEL AND DISTRIBUTED
   PROCESSING WITH APPLICATIONS WORKSHOPS (ISPAW)},
Year = {2011},
Pages = {361-366},
Note = {9th IEEE International Symposium on Parallel and Distributed Processing
   with Applications Workshops (ISPAW)/ICASE/SGH/GSDP, Busan, SOUTH KOREA,
   MAY 26-28, 2011},
Organization = {IEEE; IEEE Comp Soc; IEEE Tech Comm Scalable Comp (TCSC); FTRA; KITCS;
   Inst Informat Convergence Technol (IICT)},
Abstract = {The cost of poor or repeat engineering in complex control systems is
   extremely high, and flexibility in software design and implementation is
   one of the key factors in staying competitive in the market. Complexity
   can be managed most effectively if the underlying software systems
   support structured, standardised, high-level abstraction layers that
   encapsulate unnecessary details behind well-defined interfaces.
   Moreover, since the costs of software maintenance are often as high as
   that of initial development, the ease with which it is possible flexibly
   to reconfigure, re-engineer, and replace software components in
   operational systems is also critical. In this paper, we present a
   lightweight, component-based approach to engineering embedded real-time
   control software, which is realized in the form of a middleware system
   named MIREA. The middleware supports dynamic reconfiguration of
   components written in C/C++, and addresses variability management in
   relation to non-functional properties, such as quality-of-service (QoS)
   and real-time scheduling. Users are allowed to componentize existing
   libraries easily, such as the standard NIST 4D/Real-time Control Systems
   (RCS) library, which has been successfully used in many U.S
   government-driven intelligent control projects, and to reuse them as
   dynamically reconfigurable components. A realistic illustration is
   provided showing how control systems are structured and reconfigured
   using our approach. In fact, we discuss our approach to control using a
   fusion of NIST RCS as a means of architecting a real time control system
   and MIREA as a means of realising that architecture. Our progress to
   date suggests that MIREA is indeed well suited as a middleware
   facilitating the construction of efficient, lightweight, and scalable
   real-time embedded control systems.},
DOI = {10.1109/ISPAW.2011.69},
ISBN = {978-0-7695-4429-8},
Unique-ID = {WOS:000299539700063},
}

@inproceedings{ WOS:000306764900006,
Author = {Li, Shunxin and Shi, Leijun},
Editor = {Du, ZY and Liu, B},
Title = {Ontology Mapping Based on Domain Framework},
Booktitle = {MECHATRONIC SYSTEMS AND AUTOMATION SYSTEMS},
Series = {Applied Mechanics and Materials},
Year = {2011},
Volume = {65},
Pages = {21-24},
Note = {International Conference on Mechatronic Systems and Automation Systems
   (MSAS 2011), Xian, PEOPLES R CHINA, JUL 23-24, 2011},
Abstract = {For service-oriented software engineering, the basic problem faced is
   the sharing and interaction of information between software, and in
   essence, is to make the software to communicate from the semantic level.
   Ontology is as a carrier of information and expression methods and is
   used to describe software entities. We may want to take into account the
   reuse of ontology modules, meanwhile different ontology libraries may
   face ontology heterogeneity and other problems, and all the above
   problems can be resolved through ontology mapping. Based on the process
   of ontology construction, the ways of presenting information by ontology
   and the reference to the practical process of ontology mapping, we put
   forward a framework of ontology mapping and ontology mapping algorithm
   which analyze from the ontology concept, instance, structure, hierarchy,
   and while take advantage of Word Net thesaurus and manual adjustments
   and other means to improve the accuracy of mapping discovery and mapping
   process.},
DOI = {10.4028/www.scientific.net/AMM.65.21},
ISSN = {1660-9336},
ISBN = {978-3-03785-184-5},
Unique-ID = {WOS:000306764900006},
}

@inproceedings{ WOS:000300520800005,
Author = {Muller, Dan},
Editor = {Jain, S and Creasey, R and Himmelspach, J},
Title = {AUTOMOD (TM) - PROVIDING SIMULATION SOLUTIONS FOR OVER 25 YEARS},
Booktitle = {PROCEEDINGS OF THE 2011 WINTER SIMULATION CONFERENCE (WSC)},
Series = {Winter Simulation Conference Proceedings},
Year = {2011},
Pages = {39-51},
Note = {Winter Simulation Conference (WSC)/Conference on Modeling and Analysis
   for Semiconductor Manufacturing (MASM), Phoenix, AZ, DEC 11-14, 2011},
Organization = {Amer Stat Assoc (ASA); Assoc Comp Machinery-Special Interest Grp Simulat
   (ACM/SIGSIM); Inst Elect \& Electron Engineers-Syst, Man, \& Cybernet
   Soc (IEEE/SMC); Inst Ind Engineers (IIE); Inst Operat Res \& Management
   Sci-Simulat Soc (INFORMS-SIM); Natl Inst Stand \& Technol (NIST); Soc
   Modeling \& Simulat Int (SCS)},
Abstract = {Decision making in industry continues to become more complicated.
   Customers are more demanding, competition is more fierce, and costs for
   labor and raw materials continue to rise. Managers need state-of-the-art
   tools to help in planning, design, and operations of their facilities.
   Simulation provides a virtual factory where ideas can be tested and
   performance improved. The AutoMod product suite from Applied Materials
   has been used on thousands of projects to help engineers and managers
   make the best decisions possible. AutoMod supports hierarchical model
   construction. This architecture allows users to reuse model components
   in other models, decreasing the time required to build a model. In
   addition, recent enhancements to AutoMod's material handling template
   systems have in-creased modeling accuracy and ease-of-use. These latest
   advances have helped make AutoMod one of the most widely used simulation
   software packages.},
ISSN = {0891-7736},
ISBN = {978-1-4577-2109-0},
Unique-ID = {WOS:000300520800005},
}

@inproceedings{ WOS:000292115400014,
Author = {Pawletko, Joseph G. and Caplan, Priscilla},
Book-Group-Author = {IS \& T},
Title = {Towards Interoperable Preservation Repositories: Repository Exchange
   Package Use Cases and Best Practices},
Booktitle = {ARCHIVING 2011: PRESERVATION STRATEGIES AND IMAGING TECHNOLOGIES FOR
   CULTURAL HERITAGE INSTITUTIONS AND MEMORY ORGANIZATIONS},
Year = {2011},
Pages = {51+},
Note = {8th IS \& T and Archiving Conference, Salt Lake City, UT, MAY 16-19,
   2011},
Organization = {Family Search; Image Engn; Lifewood; MAM A; Oracle; Tessella},
Abstract = {Towards Interoperable Preservation Repositories (TIPR) is a project
   funded by the Institute of Museum and Library Services to create and
   test a packaging format for inter-repository Archival Information
   Package (AIP) exchange. The result of this work is the Repository
   eXchange Package (RXP), a lightweight packaging format that uses the
   Metadata Encoding and Transmission Standard (METS) schema to encode the
   RXP structure and the Preservation Metadata: Implementation Strategies
   (PREMIS) schema to encode digital provenance. The RXP can accommodate
   heterogeneous AIP structures and can be easily generated and ingested by
   heterogeneous preservation environments while maintaining an unbroken
   chain of digital provenance. These attributes make the RXP useful in a
   variety of preservation contexts: succession, disaster recovery,
   software migration, diversification, and specialized content processing.
   The TIPR project and the RXP format have been described in papers
   delivered at iPRES 2009 and 2010, DLF Spring 2009, and the NIST US
   Digital Preservation Interoperability Framework Workshop. This paper
   focuses on how the RXP can be used in each of the different contexts
   listed above, describes potential issues for each scenario, and
   recommends best practices for creating, interpreting and ingesting RXP
   packages. It also explores limitations of the RXP and modifications that
   have been suggested by other parties. Finally, it outlines decisions
   that transfer partners must negotiate in order to support successful
   exchange of content between repositories.},
ISBN = {978-0-89208-294-0},
Unique-ID = {WOS:000292115400014},
}

@inproceedings{ WOS:000400796300050,
Author = {Razazi, Mohammadreza and Hejazinia, Meisam},
Editor = {Ting, Z},
Title = {A New Approach for Implementing RMI in Mobile Phone Platforms},
Booktitle = {COMPUTING, COMMUNICATION, AND CONTROL},
Series = {International Proceedings of Computer Science and Information Technology},
Year = {2011},
Volume = {1},
Pages = {275-281},
Note = {International Symposium on Computing, Communication, and Control (ISCCC
   2009), Singapore, SINGAPORE, OCT 09-11, 2009},
Organization = {Int Assoc Comp Sci \& Informat Technol; Singapore Inst Elect},
Abstract = {This paper discusses challenges of implementing RMI over mobile phone
   platforms. Improving Distributed Computing capabilities in mobile phones
   has three emerging challenges: first, mobile devices have a limited
   processing power and storage, second, mobile devices have a limited
   electrical power, and third, there is a limited networking capability.
   Java RMI supports wireless environments, such as Bluetooth, General
   Packet Radio Service (GPRS), and Wirelesses LAN (WLAN). It provides a
   software infrastructure for Desktop and Server environments, but
   unfortunately there is a lack of support over limited devices such as
   mobile phones. Today, mobile phones have the same capability of Desktop
   computers in 90s. At that time, computers were used in distributed
   systems and the main challenge was optimization. But when Desktop's
   processing power and memory increased, the focus changed. Today, mobile
   phone platforms provide an opportunity for improving distributed
   processing, but due to their. constraints, most of the previous
   solutions should change according to the general guidelines for
   optimization. This paper describes different network infrastructures for
   implementing RMI, and provides a guideline for suitability of the
   network medium, the underlying software layer of RMI. Moreover, the
   concept of Roaming layer is discussed and finally, an architecture style
   is proposed to facilitate Distributed Processing over mobile phone
   platforms.},
ISSN = {2010-460X},
ISBN = {978-981-08-8633-2},
ResearcherID-Numbers = {Razzazi, Mohammadreza/ABD-9473-2021},
ORCID-Numbers = {Razzazi, Mohammadreza/0000-0002-3936-6355},
Unique-ID = {WOS:000400796300050},
}

@inproceedings{ WOS:000302967700041,
Author = {Ren, Yongchang and Xing, Tao and Zhu, Ping},
Editor = {Hou, ZX},
Title = {An Attributes Reduction Algorithms of Expert System Knowledge
   Acquisition},
Booktitle = {MEASURING TECHNOLOGY AND MECHATRONICS AUTOMATION, PTS 1 AND 2},
Series = {Applied Mechanics and Materials},
Year = {2011},
Volume = {48-49},
Number = {1 \& 2},
Pages = {187+},
Note = {3rd International Conference on Measuring Technology and Mechatronics
   Automation (ICMTMA 2011), Shanghai, PEOPLES R CHINA, JAN 06-07, 2011},
Organization = {IEEE Instrumentat \& Measurement Soc; Shanghai Univ Engn Sci; City Univ
   Hong Kong; Changsha Univ Sci \& Technol; Hunan Univ Sci \& Technol},
Abstract = {Knowledge acquisition is the bottleneck of construction expert system,
   to provide an accurate inference of knowledge is the key decision-making
   plan. This article use the rough sets theory, through the rough sets
   reduction eliminate redundant condition attribute, to achieve the
   streamlining of the knowledge library. First study the knowledge
   acquisition, in exposition knowledge hierarchical structure foundation,
   has given the conceptualization, formal, the knowledge library
   refinement and so on three knowledge acquisition; and then study
   attributes reduction algorithms, in the research sets difference and the
   attribute importance, the reduction algorithms inferential reasoning
   process's foundation, has given the attribute reduction algorithms six
   steps. Finally, according to the attributes reduction algorithms and the
   steps, to estimate the expert system to the function analytic method
   construction software cost, the composition technology complexity factor
   of 14 factors reduction. The results showed that the use of rough sets
   theory to reduce the attributes, can simplify the structure of complex
   systems, and can effectively maintain the knowledge library structure
   and performance.},
DOI = {10.4028/www.scientific.net/AMM.48-49.187},
ISSN = {1660-9336},
ISBN = {978-3-03785-019-0},
Unique-ID = {WOS:000302967700041},
}

@inproceedings{ WOS:000292115400003,
Author = {Smorul, Michael and JaJa, Joseph},
Book-Group-Author = {IS \& T},
Title = {Implementation of a high performance architecture for managing and
   storing web-harvested collections},
Booktitle = {ARCHIVING 2011: PRESERVATION STRATEGIES AND IMAGING TECHNOLOGIES FOR
   CULTURAL HERITAGE INSTITUTIONS AND MEMORY ORGANIZATIONS},
Year = {2011},
Pages = {3+},
Note = {8th IS \& T and Archiving Conference, Salt Lake City, UT, MAY 16-19,
   2011},
Organization = {Family Search; Image Engn; Lifewood; MAM A; Oracle; Tessella},
Abstract = {As institutions continue to grow their collections of web-harvested
   content, there is an ever increasing need for tools that organize, index
   and share this data. Even a modest web crawl consisting of a few web
   sites may generate millions of harvested documents. Repeating these
   crawls over time greatly expands the complexity of stored data
   Identifiling the scope of a crawl, the location of a page within a crawl
   and the differences over time behveen crawls becomes a challenging task
   In this paper we will describe a software architecture in use at the
   University of Maryland designed to support research on quickly
   extracting information about the crawls, including statistical
   information, and on indexing web content. While designed to support
   research, many of the challenges addressed in this software exist at any
   site which has to manage large sets of time-spanning data.
   Our architecture consists of two components. The first is a database
   application for organizing WARC-based web data called a WarcManager. The
   WarcManager was designed to track URL location and to allow easy
   extraction of crawl statistics across collections of warc-stored data It
   provides both a REST-based API to harvested data as well as a portal for
   viewing statistics across the collection. The second component is a high
   performance, http based, storage service called the Simple
   Web-Accessible Preservation(SWAP) system. The SWAP system is
   distributed, novel file placement and retrieval service. It has been
   designed to be minimally intrusive and to allow complete data recovery
   even in the absence of any SWAP software.
   These two components have been used to successfully support research
   into high performance indexing of web-based content. We will describe
   the implementation and performance characteristics of each component as
   well as possible real-world uses for the system.},
ISBN = {978-0-89208-294-0},
Unique-ID = {WOS:000292115400003},
}

@inproceedings{ WOS:000326447703028,
Author = {Souto-Iglesias, A. and Morillo-Balsera, M. and Guadalupe-Garcia, R. and
   Toman, M. and Gonzalez Gutierrez, L.},
Editor = {Chova, LG and Torres, IC and Martinez, AL},
Title = {INTEGRATED LEARNING OF COMPUTER APPLICATIONS FOR PRODUCTION ENGINEERING},
Booktitle = {INTED2011: 5TH INTERNATIONAL TECHNOLOGY, EDUCATION AND DEVELOPMENT
   CONFERENCE},
Year = {2011},
Pages = {3155-3163},
Note = {5th International Technology, Education and Development Conference
   (INTED), Valencia, SPAIN, MAR 07-09, 2011},
Abstract = {A high productivity rate in Engineering is related to an efficient
   management of the flow of the large quantities of information and
   associated decision making activities that are consubstantial to the
   Engineering processes both in design and production contexts. Dealing
   with such problems from an integrated point of view and mimicking real
   scenarios is not given much attention in Engineering degrees. In the
   context of Engineering Education, there are a number of courses designed
   for developing specific competencies, as required by the academic
   curricula, but not that many in which integration competencies are the
   main target. In this paper, a course devoted to that aim is discussed.
   The course is taught in a Marine Engineering degree but the philosophy
   could be used in any Engineering field. All the lessons are given in a
   computer room in which every student can use each all the treated
   software applications. The first part of the course is dedicated to
   Project Management: the students acquire skills in defining, using
   Ms-PROJECT, the work breakdown structure (WBS), and the organization
   breakdown structure (OBS) in Engineering projects, through a series of
   examples of increasing complexity, ending up with the case of vessel
   construction. The second part of the course is dedicated to the use of a
   database manager, Ms-ACCESS, for managing production related
   information. A series of increasing complexity examples is treated
   ending up with the management of the pipe database of a real vessel.
   This database consists of a few thousand of pipes, for which a
   production timing frame is defined, which connects this part of the
   course with the first one. Finally, the third part of the course is
   devoted to the work with FORAN, an Engineering Production package of
   widespread use in the shipbuilding industry. With this package, the
   frames and plates where all the outfitting will be carried out are
   defined through cooperative work by the studens, working simultaneously
   in the same 3D model. In the paper, specific details about the learning
   process are given. Surveys have been posed to the students in order to
   get feed-back from their experience as well as to assess their
   satisfaction with the learning process. Results from these surveys are
   discussed in the paper.},
ISBN = {978-84-614-7423-3},
ResearcherID-Numbers = {morillo, Carmen/AAS-5584-2020
   DEL CARMEN MORILLO BALSERA, M/AAX-9543-2020
   Souto-Iglesias, Antonio/A-1294-2013
   GUTIERREZ, LEO MIGUEL M GONZALEZ/R-4350-2018},
ORCID-Numbers = {DEL CARMEN MORILLO BALSERA, M/0000-0002-0788-8394
   Souto-Iglesias, Antonio/0000-0001-7207-5341
   GUTIERREZ, LEO MIGUEL M GONZALEZ/0000-0002-1629-0001},
Unique-ID = {WOS:000326447703028},
}

@inproceedings{ WOS:000298000900015,
Author = {Staessens, Dimitri and Sharma, Sachin and Colle, Didier and Pickavet,
   Mario and Demeester, Piet},
Book-Group-Author = {IEEE},
Title = {Software Defined Networking: Meeting Carrier Grade Requirements},
Booktitle = {2011 18TH IEEE WORKSHOP ON LOCAL AND METROPOLITAN AREA NETWORKS (LANMAN)},
Series = {IEEE Workshop on Local and Metropolitan Area Networks},
Year = {2011},
Note = {18th IEEE Workshop on Local and Metropolitan Area Networks (LANMAN),
   Chapel Hill, NC, OCT 13-14, 2011},
Organization = {IEEE; AT\&T; HP Labs; IEEE Commun Soc; Univ N Carolina, Comp Sci Dept},
Abstract = {Software Defined Networking is a networking paradigm which allows
   network operators to manage networking elements using software running
   on an external server. This is accomplished by a split in the
   architecture between the forwarding element and the control element. Two
   technologies which allow this split for packet networks are ForCES and
   Openflow. We present energy efficiency and resilience aspects of carrier
   grade networks which can be met by Openflow. We implement flow
   restoration and run extensive experiments in an emulated carrier grade
   network. We show that Openflow can restore traffic quite fast, but its
   dependency on a centralized controller means that it will be hard to
   achieve 50 ms restoration in large networks serving many flows. In order
   to achieve 50 ms recovery, protection will be required in carrier grade
   networks.},
ISSN = {1944-0367},
ISBN = {978-1-4577-1265-4},
ResearcherID-Numbers = {Demeester, Piet/N-6619-2013
   Pickavet, Mario J/E-9530-2019
   },
ORCID-Numbers = {Demeester, Piet/0000-0003-2810-3899
   Pickavet, Mario J/0000-0001-5817-7886
   Sharma, Sachin/0000-0002-8358-2258
   Colle, Didier/0000-0002-1428-0301},
Unique-ID = {WOS:000298000900015},
}

@inproceedings{ WOS:000298911600026,
Author = {Vassigh, S.},
Editor = {Villacampa, Y and Brebbia, CA},
Title = {Sustainable carbon neutral building design: new simulation tools for
   teaching green buildings},
Booktitle = {ECOSYSTEMS AND SUSTAINABLE DEVELOPMENT VIII},
Series = {Search Institute Series on Developmentally Attentive Community and
   Society},
Year = {2011},
Pages = {277-284},
Note = {8th International Conference on Ecosystems and Sustainable Development
   (ECOSUD 2011), Univ Alicante, Alicante, SPAIN, APR 13-15, 2011},
Organization = {Wessex Inst Technol, UK; Univ Siena, Italy; Int Journal Design \& Nat \&
   Ecodynam; Minist Ciencia Innovac; Caja Mediterranea},
Abstract = {The Building Industry is one of the major consumers of energy. In the
   United States buildings use 48\% of the total energy consumed,
   significantly impacting national energy demand and contributing to
   global warming. The vast majority of architectural practice in the U.S.
   leads to construction of buildings with little concern to sustainability
   contributing to environmental degradation. However, as the environmental
   impact of buildings become increasingly recognized, the role of
   architects and the initial decision making process which determines the
   resource use and life-cycle of materials, systems, and construction
   processes, becomes more critical.
   In order to advance sustainable development and make green building
   construction the standard rather than the anomaly, the education of the
   architects must be reconsidered. The traditional American architectural
   curriculum that is based on a schism between ``design{''} and
   ``technology{''} is inherently in conflict with the principal of
   sustainability.
   Recent research in building design indicates that the most resource
   efficient, best performing, and environmentally sustainable buildings
   are designed utilizing ``integrated practice,{''} in which the various
   disciplines involved in building design work together at the conception
   of the project to improve overall building performance, reduce GHG's,
   and lower the costs for operation and maintenance of buildings.
   Although large-scale reform of architectural curricula is a complex,
   ongoing, and difficult debate, producing teaching tools that can
   simulate integrated design can impact and promote a better understanding
   of sustainable practice in architecture.
   The proposed paper will present the progress of a multi-disciplinary
   team of faculty who are collectively working on the completion,
   implementation and evaluation of a simulation software package in an
   interactive game format. The project teaches the concepts of
   ``integrated design{''} through immersing students in a virtual world
   that imitates the complexity of the real world of decision-making and
   material choices in design. The project accomplishes this by harnessing
   the capabilities of simulation and dynamic modelling programs as well as
   powerful game engines while creating compelling and rewarding reasons
   for student's engagement in the learning process. The project is funded
   by the US Department of Education for the period of 2007-2011.},
DOI = {10.2495/ECO110251},
ISBN = {978-1-84564-510-6},
Unique-ID = {WOS:000298911600026},
}

@inproceedings{ WOS:000392353500013,
Author = {Veryzhenko, Iryna and Mathieu, Philippe and Brandouy, Olivier},
Editor = {Filipe, J and Fred, A},
Title = {KEY POINTS FOR REALISTIC AGENT-BASED FINANCIAL MARKET SIMULATIONS},
Booktitle = {ICAART 2011: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON AGENTS
   AND ARTIFICIAL INTELLIGENCE, VOL 2},
Year = {2011},
Pages = {74-83},
Note = {3rd International Conference on Agents and Artificial Intelligence,
   Rome, ITALY, JAN 28-30, 2011},
Organization = {Inst Syst \& Technol Informat Control \& Commun},
Abstract = {The purpose of this paper is to define software engineering abstractions
   that provide a generic framework for stock market simulations. We
   demonstrate a series of key points and principles that has governed the
   development of an Agent-Based financial market in the form of an API.
   The simulator architecture is presented. During artificial market
   construction we have faced the whole variety of agent-based modeling
   issues and solved them : local interaction, distributed knowledge and
   resources, heterogeneous environments, agents autonomy, artificial
   intelligence, speech acts, discrete scheduling and simulation. Our study
   demonstrates that the choices made for agent-based modeling in this
   context deeply impact the resulting market dynamics and proposes a
   series of advances regarding the main limits the existing platforms
   actually meet.},
ISBN = {978-989-8425-41-6},
ORCID-Numbers = {MATHIEU, Philippe/0000-0003-2786-1209},
Unique-ID = {WOS:000392353500013},
}

@inproceedings{ WOS:000398852700101,
Author = {Wang Qian and Zhang Lizhong and Liang Guoling and Zhang Yongbo and Wang
   Wei and Zhou Xiaoyuan and Huo Zhibin and Cai Zizhao and Zhang ChunYing
   and Shi Lei and Lu Yan},
Editor = {Deng, W},
Title = {Design and Implementation of Field Hand-map Corrector System Based on
   Secondary Development Library of MAPGIS6.7 Software},
Booktitle = {2011 AASRI CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INDUSTRY
   APPLICATION (AASRI-AIIA 2011), VOL 3},
Year = {2011},
Pages = {425-427},
Note = {AASRI Conference on Artificial Intelligence and Industry Application
   (AASRI-AIIA 2011), Male, MALDIVES, MAY 23-24, 2011},
Organization = {Amer Appl Sci Res Inst},
Abstract = {This paper describes the design and implementation methods of the field
   hand map projection correction system.Using this system, the field
   hand-map can be corrected from Beijing 54 (or Xi'an 80) ellipsoid
   parameters, Gauss - Kruger (or Lambert) projection coordinate system to
   the WGS84 ellipsoid, Mercator projection coordinates. The field hand-map
   which has be corrected,meet the precision of the field data collection
   system.
   Field hand-map corrector system not only saves the cost of the
   investigation but also improves the efficiency of field investigations,
   It play an important role in for the promotion and application of field
   data collection system.},
ISBN = {978-1-937728-02-1},
Unique-ID = {WOS:000398852700101},
}

@inproceedings{ WOS:000398331100061,
Author = {Wang Qian and Zhang Lizhong and Liang Guoling and Zhang Yongbo and Wang
   Wei and Zhou Xiaoyuan and Huo Zhibin and Cai Zizhao and Zhang ChunYing
   and Shi Lei and Lu Yan},
Editor = {Yun, Z},
Title = {Design and Implementation of Field Hand-map Corrector System Based on
   Secondary Development Library of MAPGIS6.7 Software},
Booktitle = {2011 INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND NEURAL COMPUTING
   (FSNC 2011), VOL VI},
Year = {2011},
Pages = {259-261},
Note = {International Conference on Fuzzy Systems and Neural Computing (FSNC
   2011), Hong Kong, PEOPLES R CHINA, FEB 20-21, 2011},
Organization = {Hong Kong Educ Soc; Huazhong Univ Sci \& Technol; IEEE Beijing Nanchang
   Sect IAS Chapter},
Abstract = {This paper describes the design and implementation methods of the field
   hand map projection correction system.Using this system, the field
   hand-map can be corrected from Beijing 54 (or Xi'an 80) ellipsoid
   parameters, Gauss - Kruger (or Lambert) projection coordinate system to
   the WGS84 ellipsoid, Mercator projection coordinates. The field hand-map
   which has be corrected,meet the precision of the field data collection
   system.
   Field hand-map corrector system not only saves the cost of the
   investigation but also improves the efficiency of field investigations,
   It play an important role in for the promotion and application of field
   data collection system.},
ISBN = {978-1-4244-9216-9},
Unique-ID = {WOS:000398331100061},
}

@inproceedings{ WOS:000290199900251,
Author = {Wang, Zhongquan and Luo, Xiaoqun and Hu, Jiangmin and Yang, Zonglin},
Editor = {Zhang, LC and Zhang, CL and Shi, TL},
Title = {Integrated CAD/CAM Software for Steel Tubular Truss Structures},
Booktitle = {MANUFACTURING ENGINEERING AND AUTOMATION I, PTS 1-3},
Series = {Advanced Materials Research},
Year = {2011},
Volume = {139-141},
Number = {1-3},
Pages = {1117+},
Note = {International Conference on Manufacturing Engineering and Automation,
   Guangzhou, PEOPLES R CHINA, DEC 07-09, 2010},
Organization = {Guangzhou Univ; Univ New S Wales; Huazhong Univ Sci \& Technol; Xian
   Jiaotong Univ},
Abstract = {Using ObjectARX, an integrated CAD/CAM software for steel tubular truss
   structures has been developed based on AutoCAD platform and Visual
   studio.Net environment. The design ideas and the application effects are
   introduced in this paper. Because of different data requirements in
   different design stages of steel tubular structures, i.e., the whole
   structure design and the detail design, a wireframe model and a solid
   model were adopted in the software, respectively. Joint lofting is the
   key point in manufacturing of complex tubular trusses. To solve this, 3D
   solid model was constructed by self-defined solid objects, which were
   designed with inheritance of class AcDb3dSolid in ObjectARX, and Boolean
   operation was used to realize 3D end lofting of branch pipes. Based on
   the lofted solid model, the G-codes can be generated automatically. At
   last, an actual tubular truss structure was designed and lofted by using
   the software. The proposed software package realizes the integration of
   CAD/CAM and it can greatly reduce the cost and time of design and
   fabrication, improve the quality of construction.},
DOI = {10.4028/www.scientific.net/AMR.139-141.1117},
ISSN = {1022-6680},
ISBN = {978-0-87849-226-8},
Unique-ID = {WOS:000290199900251},
}

@inproceedings{ WOS:000298656802188,
Author = {Yang, Yanlan and Ye, Hua and Fei, Shumin and Yang, Yanlan and Ye, Hua
   and Fei, Shumin},
Book-Group-Author = {IEEE},
Title = {Design of Communication Interface for M2M-based Positioning and
   Monitoring System},
Booktitle = {2011 INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND CONTROL
   (ICECC)},
Year = {2011},
Pages = {2624-2627},
Note = {IEEE International Conference on Electronics, Communications and Control
   (ICECC), Ningbo, PEOPLES R CHINA, SEP 09-11, 2011},
Organization = {IEEE; Ningbo Univ; Key Lab Sci \& Tech Natl Def (KLSTND)},
Abstract = {The idea of M2M platform proposes a completely new system architecture
   for positioning and monitoring applications with wider coverage and
   higher communication efficiency. With the comparison of the original
   communication mode to the M2M-based one, the communication interface is
   changed in the new communication solution for the application side
   accessing to the platform, which is finally designed into two parts
   according to the interface specification for M2M platform. The one is
   the local web service which provides methods for the platform to
   transmit data from the terminal side, and the other is the communication
   service software developed based on the SDK package from the M2M
   platform. This interface has been implemented into an actual application
   for the construction machinery monitoring, and the communication work
   between the terminals and the control center through M2M platform
   performs efficiently and reliably.},
ISBN = {978-1-4577-0321-8},
Unique-ID = {WOS:000298656802188},
}

@inproceedings{ WOS:000314666500020,
Author = {Yang, Zhiyong and DaSilva, Otto},
Book-Group-Author = {ASME},
Title = {A RATIONAL APPROACH TO AUTOMATED PRE AND POSTPROCESSING OF OFFSHORE
   STRUCTURE GLOBAL STRENGTH FINITE ELEMENT ANALYSIS},
Booktitle = {OMAE2011: PROCEEDINGS OF THE ASME 30TH INTERNATIONAL CONFERENCE ON
   OCEAN, OFFSHORE AND ARCTIC ENGINEERING, VOL 2: STRUCTURES, SAFETY AND
   RELIABILITY},
Year = {2011},
Pages = {185-195},
Note = {30th International Conference on Ocean, Offshore and Arctic Engineering,
   Rotterdam, NETHERLANDS, JUN 19-24, 2011},
Organization = {ASME, Ocean Offshore \& Arctic Engn Div},
Abstract = {Offshore structure global strength analysis based on finite element
   plate model is a requirement for today's classification societies and
   designers. Wave, wind, current loads have to be applied to the global
   strength model as a pre processing step to allow the analysis to take
   place. After the analysis, code checking must be performed to verify if
   the structure meets class or other requirements. Due to its complex
   nature, a large amount of engineering hours have to be spent for the pre
   and post processing. This is not only lengthy if performed manually or
   semi-automatically, but also mistake prone. General guidelines from
   classification societies exist, but general purpose commercial software
   is scarce and often still requires significant amount of engineering
   time to perform these tasks.
   This paper shows a rational approach to automate the pre and post
   processing of offshore structure global strength finite element
   analysis. Utilizing the FEMAP Application Program Interface (API), a
   complete automatic pre and post processing is implemented in one
   integrated program, Exmar Design Suite (EDS). The program will load the
   model from WAMIT generated wave pressure, apply internal pressure
   induced from motions to internal tanks, and also apply other
   environmental loads. After the finite element analysis, the program can
   execute strength code checking including yielding and buckling for the
   model. Both beam and stiffened plate panels can be identified using an
   automatic search algorithm, which is not a function available for
   general finite element software. The panels and beams are then checked
   against various common codes such as API/AISC/ABS/DNV. In addition,
   fatigue analysis can also be performed in either spectral or simplified
   approach.
   The benefits of automation are timesaving, accuracy and reliability. It
   also makes the check of whole model possible. Instead of relying more on
   ``screening{''} or experience based structure check, engineers will have
   more confidence in the results by going through the whole model.},
ISBN = {978-0-7918-4434-2},
Unique-ID = {WOS:000314666500020},
}

@inproceedings{ WOS:000290199900264,
Author = {Zhang, Shengfang and Ji, Changjun and Sha, Zhihua and Ma, Chenhao},
Editor = {Zhang, LC and Zhang, CL and Shi, TL},
Title = {Study on Key Technologies of Practical DNC System},
Booktitle = {MANUFACTURING ENGINEERING AND AUTOMATION I, PTS 1-3},
Series = {Advanced Materials Research},
Year = {2011},
Volume = {139-141},
Number = {1-3},
Pages = {1184-1187},
Note = {International Conference on Manufacturing Engineering and Automation,
   Guangzhou, PEOPLES R CHINA, DEC 07-09, 2010},
Organization = {Guangzhou Univ; Univ New S Wales; Huazhong Univ Sci \& Technol; Xian
   Jiaotong Univ},
Abstract = {Aim at the communication problems of computer and NC machine tool, a
   practical DNC system is developed in this paper. A three-layer control
   structure mode - which is composed by the unit layer, DNC computer
   workstation layer and NC system equipment layer - is presented. Based on
   its good real-time capability and high reliability, CAN field-bus is
   introduced into the DNC communication system construction, the network
   topology is constructed and related hardware is chosen properly. Using
   VC++6.0 as development tool, and assisted with special compiling tools
   LEX\&YACC, 3D standard graphic library OpenGL and database Microsoft
   Access, and based on the multi-task and multi-thread technologies, the
   software of the system is developed. The system has the characteristics
   of reliable operation, simple structure and high real-time capability,
   can improve the efficiency of NC machining process obviously, and has a
   certain application prospect in the middle and small scale manufacturing
   enterprises of our country.},
DOI = {10.4028/www.scientific.net/AMR.139-141.1184},
ISSN = {1022-6680},
ISBN = {978-0-87849-226-8},
Unique-ID = {WOS:000290199900264},
}

@article{ WOS:000284652300007,
Author = {Buvaneswari, Arumugam and Drabeck, Lawrence and Nithi, Nachi and Haner,
   Mark and Polakos, Paul and Sawkar, Chitra},
Title = {Self-Optimization of LTE Networks Utilizing Celnet Xplorer},
Journal = {BELL LABS TECHNICAL JOURNAL},
Year = {2010},
Volume = {15},
Number = {3},
Pages = {99-117},
Month = {DEC},
Abstract = {In order to meet demanding performance objectives in Long Term Evolution
   (LTE) networks, it is mandatory to implement highly efficient, autonomic
   self-optimization and configuration processes. Self-optimization
   processes have already been studied in second generation (2G) and third
   generation (3G) networks, typically with the objective of improving
   radio coverage and channel capacity. The 3rd Generation Partnership
   Project (3GPP) standard for LTE self-organization of networks (SON)
   provides guidelines on self-configuration of physical cell ID and
   neighbor relation function and self-optimization for mobility
   robustness, load balancing, and inter-cell interference reduction. While
   these are very important from an optimization perspective of local
   phenomenon (i.e., the eNodeB's interaction with its neighbors), it is
   also essential to architect control algorithms to optimize the network
   as a whole. In this paper, we propose a Celnet Xplorer-based SON
   architecture that allows detailed analysis of network performance
   combined with a SON control engine to optimize the LTE network. The
   network performance data is obtained in two stages. In the first stage,
   data is acquired through intelligent non-intrusive monitoring of the
   standard interfaces of the Evolved UMTS Terrestrial Radio Access Network
   (E-UTRAN) and Evolved Packet Core (EPC), coupled with reports from a
   software client running in the eNodeBs. In the second stage, powerful
   data analysis is performed on this data, which is then utilized as input
   for the SON engine. Use cases involving tracking area optimization,
   dynamic bearer profile reconfiguration, and tuning of network-wide
   coverage and capacity parameters are presented. (C) 2010 Alcatel-Lucent.},
DOI = {10.1002/bltj.20459},
ISSN = {1089-7089},
Unique-ID = {WOS:000284652300007},
}

@article{ WOS:000285886100002,
Author = {Frenkel, Zeev and Paux, Etienne and Mester, David and Feuillet,
   Catherine and Korol, Abraham},
Title = {LTC: a novel algorithm to improve the efficiency of contig assembly for
   physical mapping in complex genomes},
Journal = {BMC BIOINFORMATICS},
Year = {2010},
Volume = {11},
Month = {NOV 30},
Abstract = {Background: Physical maps are the substrate of genome sequencing and
   map-based cloning and their construction relies on the accurate assembly
   of BAC clones into large contigs that are then anchored to genetic maps
   with molecular markers. High Information Content Fingerprinting has
   become the method of choice for large and repetitive genomes such as
   those of maize, barley, and wheat. However, the high level of repeated
   DNA present in these genomes requires the application of very stringent
   criteria to ensure a reliable assembly with the FingerPrinted Contig
   (FPC) software, which often results in short contig lengths (of 3-5
   clones before merging) as well as an unreliable assembly in some
   difficult regions. Difficulties can originate from a non-linear
   topological structure of clone overlaps, low power of clone ordering
   algorithms, and the absence of tools to identify sources of gaps in
   Minimal Tiling Paths (MTPs).
   Results: To address these problems, we propose a novel approach that:
   (i) reduces the rate of false connections and Q-clones by using a new
   cutoff calculation method; (ii) obtains reliable clusters robust to the
   exclusion of single clone or clone overlap; (iii) explores the
   topological contig structure by considering contigs as networks of
   clones connected by significant overlaps; (iv) performs iterative clone
   clustering combined with ordering and order verification using
   re-sampling methods; and (v) uses global optimization methods for clone
   ordering and Band Map construction. The elements of this new analytical
   framework called Linear Topological Contig (LTC) were applied on
   datasets used previously for the construction of the physical map of
   wheat chromosome 3B with FPC. The performance of LTC vs. FPC was
   compared also on the simulated BAC libraries based on the known genome
   sequences for chromosome 1 of rice and chromosome 1 of maize.
   Conclusions: The results show that compared to other methods, LTC
   enables the construction of highly reliable and longer contigs (5-12
   clones before merging), the detection of ``weak{''} connections in
   contigs and their ``repair{''}, and the elongation of contigs obtained
   by other assembly methods.},
DOI = {10.1186/1471-2105-11-584},
Article-Number = {584},
ISSN = {1471-2105},
ResearcherID-Numbers = {Frenkel, Zeev/A-5589-2019
   },
ORCID-Numbers = {Frenkel, Zeev/0000-0003-0211-6966
   Paux, Etienne/0000-0002-3094-7129},
Unique-ID = {WOS:000285886100002},
}

@article{ WOS:000281541400003,
Author = {Rupp, Bernd and Appel, Klaus E. and Gundert-Remy, Ursula},
Title = {Chronic oral LOAEL prediction by using a commercially available
   computational QSAR tool},
Journal = {ARCHIVES OF TOXICOLOGY},
Year = {2010},
Volume = {84},
Number = {9},
Pages = {681-688},
Month = {SEP},
Abstract = {In the absence of toxicological data, as it is the case for, e.g.
   naturally occurring substances and chemicals underlying the new European
   chemicals legislation, distinct tools to derive quantitative
   toxicological data are of particular interest with regard to risk
   assessment of substances humans are repeatedly exposed. The software
   package TOPKAT 6.2 version 3.1 (Accelrys Inc., San Diego, USA) is a
   commercially available tool containing a (sub)chronic oral low observed
   adverse level (LOAEL) prediction model constructed by using structures
   and LOAELs of 393 chemicals contained in publicly accessible data banks.
   Applying this tool, we tested the prediction of (sub)chronic LOAELS for
   807 industrial chemicals (purity a parts per thousand yen 95\%) by
   comparing the predicted values with their experimental LOAELs derived
   from repeated dose animal experiments performed according to standard
   guidelines. For 460 chemicals, a prediction could not be performed
   because of exclusion criteria defined in the system. They had either a
   lower LD50 as the predicted LOAEL (n = 214) were outside the optimum
   prediction space which defines the domain of applicability (n = 175),
   were used in the training data set (n = 155), were not known to the
   system (n = 50) or fulfilled other criteria for data exclusion (n = 21).
   Of the remaining 347 substances, 34 to 62\% LOAELs were predicted within
   a range of 1/5 and fivefold of the experimental LOAEL (factor 5),
   whereas 84 and 99\% of the predicted LOAELs were within a range of 1/100
   and 100-fold indicating high uncertainty of the prediction. Hence, a
   refined prediction tool is highly warranted. However, the uncertainty of
   the prediction could be accounted for if an additional factor of 100 is
   applied in addition to standard default adjustment factor of 100 which
   would result in an adjustment factor of 10,000 to be able to use a
   predicted NOAEL for risk assessment..},
DOI = {10.1007/s00204-010-0532-x},
ISSN = {0340-5761},
ORCID-Numbers = {Rupp, Bernd/0000-0001-9447-9638},
Unique-ID = {WOS:000281541400003},
}

@article{ WOS:000281389500009,
Author = {Cuntz, Hermann and Forstner, Friedrich and Borst, Alexander and
   Haeusser, Michael},
Title = {One Rule to Grow Them All: A General Theory of Neuronal Branching and
   Its Practical Application},
Journal = {PLOS COMPUTATIONAL BIOLOGY},
Year = {2010},
Volume = {6},
Number = {8},
Month = {AUG},
Abstract = {Understanding the principles governing axonal and dendritic branching is
   essential for unravelling the functionality of single neurons and the
   way in which they connect. Nevertheless, no formalism has yet been
   described which can capture the general features of neuronal branching.
   Here we propose such a formalism, which is derived from the expression
   of dendritic arborizations as locally optimized graphs. Inspired by
   Ramon y Cajal's laws of conservation of cytoplasm and conduction time in
   neural circuitry, we show that this graphical representation can be used
   to optimize these variables. This approach allows us to generate
   synthetic branching geometries which replicate morphological features of
   any tested neuron. The essential structure of a neuronal tree is thereby
   captured by the density profile of its spanning field and by a single
   parameter, a balancing factor weighing the costs for material and
   conduction time. This balancing factor determines a neuron's
   electrotonic compartmentalization. Additions to this rule, when required
   in the construction process, can be directly attributed to developmental
   processes or a neuron's computational role within its neural circuit.
   The simulations presented here are implemented in an open-source
   software package, the ``TREES toolbox,{''} which provides a general set
   of tools for analyzing, manipulating, and generating dendritic
   structure, including a tool to create synthetic members of any
   particular cell group and an approach for a model-based supervised
   automatic morphological reconstruction from fluorescent image stacks.
   These approaches provide new insights into the constraints governing
   dendritic architectures. They also provide a novel framework for
   modelling and analyzing neuronal branching structures and for
   constructing realistic synthetic neural networks.},
DOI = {10.1371/journal.pcbi.1000877},
Article-Number = {e1000877},
EISSN = {1553-7358},
ResearcherID-Numbers = {Cuntz, Hermann/E-7057-2012
   Hausser, Michael/AAW-6827-2020},
ORCID-Numbers = {Cuntz, Hermann/0000-0001-5445-0507
   Hausser, Michael/0000-0002-2673-8957},
Unique-ID = {WOS:000281389500009},
}

@article{ WOS:000276495300006,
Author = {Brennan, Nicola and Corrigan, Oonagh and Allard, Jon and Archer, Julian
   and Barnes, Rebecca and Bleakley, Alan and Collett, Tracey and de Bere,
   Sam Regan},
Title = {The transition from medical student to junior doctor: today's
   experiences of Tomorrow's Doctors},
Journal = {MEDICAL EDUCATION},
Year = {2010},
Volume = {44},
Number = {5},
Pages = {449-458},
Month = {MAY},
Abstract = {Context
   Medical education in the UK has recently undergone radical reform.
   Tomorrow's Doctors has prescribed undergraduate curriculum change and
   the Foundation Programme has overhauled postgraduate education.
   Objectives
   This study explored the experiences of junior doctors during their first
   year of clinical practice. In particular, the study sought to gain an
   understanding of how junior doctors experienced the transition from the
   role of student to that of practising doctor and how well their medical
   school education had prepared them for this.
   Methods
   The study used qualitative methods comprising of semi-structured
   interviews and audio diary recordings with newly qualified doctors based
   at the Peninsula Foundation School in the UK. Purposive sampling was
   used and 31 of 186 newly qualified doctors self-selected from five
   hospital sites. All 31 participants were interviewed once and 17 were
   interviewed twice during the year. Ten of the participants also kept
   audio diaries. Interview and audio diary data were transcribed verbatim
   and thematically analysed with the aid of a qualitative data analysis
   software package.
   Results
   The findings show that, despite recent curriculum reforms, most
   participants still found the transition stressful. Dealing with their
   newly gained responsibility, managing uncertainty, working in
   multi-professional teams, experiencing the sudden death of patients and
   feeling unsupported were important themes. However, the stress of
   transition was reduced by the level of clinical experience gained in the
   undergraduate years.
   Conclusions
   Medical schools need to ensure that students are provided with early
   exposure to clinical environments which allow for continuing
   `meaningful' contact with patients and increasing opportunities to `act
   up' to the role of junior doctor, even as students. Patient safety
   guidelines present a major challenge to achieving this, although with
   adequate supervision the two aims are not mutually exclusive. Further
   support and supervision should be made available to junior doctors in
   situations where they are dealing with the death of a patient and on
   surgical placements.},
DOI = {10.1111/j.1365-2923.2009.03604.x},
ISSN = {0308-0110},
EISSN = {1365-2923},
ORCID-Numbers = {Archer, Julian/0000-0001-9983-9655
   Corrigan, Oonagh/0000-0002-7301-2200
   Collett, Tracey/0000-0002-8541-0417
   Brennan, Nicola/0000-0002-2165-0155},
Unique-ID = {WOS:000276495300006},
}

@article{ WOS:000276737100023,
Author = {Loriot, Sebastien and Cazals, Frederic and Bernauer, Julie},
Title = {ESBTL: efficient PDB parser and data structure for the structural and
   geometric analysis of biological macromolecules},
Journal = {BIOINFORMATICS},
Year = {2010},
Volume = {26},
Number = {8},
Pages = {1127-1128},
Month = {APR 15},
Abstract = {The ever increasing number of structural biological data calls for
   robust and efficient software for analysis. Easy Structural Biology
   Template Library (ESBTL) is a lightweight C++ library that allows the
   handling of PDB data and provides a data structure suitable for
   geometric constructions and analyses. The parser and data model provided
   by this ready-to-use include-only library allows adequate treatment of
   usually discarded information (insertion code, atom occupancy, etc.)
   while still being able to detect badly formatted files. The
   template-based structure allows rapid design of new computational
   structural biology applications and is fully compatible with the new
   remediated PDB archive format. It also allows the code to be easy-to-use
   while being versatile enough to allow advanced user developments.},
DOI = {10.1093/bioinformatics/btq083},
ISSN = {1367-4803},
ResearcherID-Numbers = {Bernauer, Julie/M-7518-2014},
ORCID-Numbers = {Bernauer, Julie/0000-0003-2345-995X},
Unique-ID = {WOS:000276737100023},
}

@article{ WOS:000277057400005,
Author = {Hogg, J. D. and Scott, J. A.},
Title = {A Fast and Robust Mixed-Precision Solver for the Solution of Sparse
   Symmetric Linear Systems},
Journal = {ACM TRANSACTIONS ON MATHEMATICAL SOFTWARE},
Year = {2010},
Volume = {37},
Number = {2},
Month = {APR},
Abstract = {On many current and emerging computing architectures, single-precision
   calculations are at least twice as fast as double-precision
   calculations. In addition, the use of single precision may reduce
   pressure on memory bandwidth. The penalty for using single precision for
   the solution of linear systems is a potential loss of accuracy in the
   computed solutions. For sparse linear systems, the use of mixed
   precision in which double-precision iterative methods are preconditioned
   by a single-precision factorization can enable the recovery of
   high-precision solutions more quickly and use less memory than a sparse
   direct solver run using double-precision arithmetic.
   In this article, we consider the use of single precision within direct
   solvers for sparse symmetric linear systems, exploiting both the
   reduction in memory requirements and the performance gains. We develop a
   practical algorithm to apply a mixed-precision approach and suggest
   parameters and techniques to minimize the number of solves required by
   the iterative recovery process. These experiments provide the basis for
   our new code HSL MA79-a fast, robust, mixed-precision sparse symmetric
   solver that is included in the mathematical software library HSL.
   Numerical results for a wide range of problems from practical
   applications are presented.},
DOI = {10.1145/1731022.1731027},
Article-Number = {17},
ISSN = {0098-3500},
EISSN = {1557-7295},
ResearcherID-Numbers = {Scott, Jennifer/AAS-1492-2020
   },
ORCID-Numbers = {Scott, Jennifer/0000-0003-2130-1091
   Hogg, Jonathan/0000-0001-6372-4880},
Unique-ID = {WOS:000277057400005},
}

@article{ WOS:000276122500017,
Author = {Frasson, Renato Prata de Moraes and Krajewski, Witold F.},
Title = {Three-dimensional digital model of a maize plant},
Journal = {AGRICULTURAL AND FOREST METEOROLOGY},
Year = {2010},
Volume = {150},
Number = {3},
Pages = {478-488},
Month = {MAR 15},
Abstract = {Mechanistic modeling of crop interactions with the atmosphere requires
   knowledge of the canopy architecture. For example, studies of light and
   rainfall interception and microwave radiative transfer modeling have
   motivated the development of a number of virtual canopies by
   agronomists, computer scientists, and hydrologists. While a number of
   canopy measuring techniques are already available, recent improvements
   in digital photography and the availability of affordable commercial
   photogrammetry packages have created an opportunity to develop highly
   detailed three-dimensional digital models of maize canopies. Here, we
   present a non-destructive one-man methodology to digitize plants that
   uses an unmodified consumer grade digital single-lens reflex (SLR)
   camera and commercially available photogrammetry software. This
   methodology allows tracking of individual plant development, which was
   not possible in earlier techniques. The construction of the digital
   plant model is divided into three parts: plant preparation, where
   several artificial targets are placed on the plant and their relative
   distances are measured; leaf digitizing in which the targets are marked
   and referenced by the software, thereby allowing the photographs to be
   oriented; and the creation of three-dimensional models of individual
   leaves and the final model buildup when, through cross-referencing, the
   complete digital plant model is arranged. We demonstrate the
   applicability of the presented methodology by digitizing the same plant
   at two different stages of development (6 and 10 leaves) and
   subsequently assessing the models' precision. In both cases, the
   objective is to define the edges and mid-rib of the leaves as well as
   their vertical and horizontal orientation; this allows the calculation
   of geometric descriptive parameters including plant area, leaf overlap,
   leaf area index, and gap fraction. The two presented models were built
   from 48 and 119 pictures, respectively, which correspond to 348 and 1553
   three-dimensional points. (C) 2010 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.agrformet.2010.01.003},
ISSN = {0168-1923},
ResearcherID-Numbers = {Frasson, Renato/F-5816-2010},
ORCID-Numbers = {Frasson, Renato/0000-0003-4299-1730},
Unique-ID = {WOS:000276122500017},
}

@article{ WOS:000274135200011,
Author = {Zhai, Zhiqiang (John) and Previtali, Jonathan M.},
Title = {Ancient vernacular architecture: characteristics categorization and
   energy performance evaluation},
Journal = {ENERGY AND BUILDINGS},
Year = {2010},
Volume = {42},
Number = {3},
Pages = {357-365},
Month = {MAR},
Abstract = {Building has significant impacts on the environment and natural
   resources. The emerging world energy and environment challenges demand a
   substantial revolution of building design philosophies. strategies,
   technologies, and construction methods. Vernacular architectures, built
   by people whose design decisions are influenced by traditions in their
   culture, have been gleaned through a long period of trial and error and
   the ingenuity of local builders who possess specific knowledge about
   their place on the planet. and thus are valuable in promoting
   climate-specific passive building technologies to modern buildings. This
   study introduced an approach to categorizing distinct vernacular regions
   and evaluating energy performance of ancient vernacular homes as well as
   identifying optimal constructions using vernacular building techniques.
   The research conducted an extensive Computer energy modeling for a
   number of representative ancient vernacular architectural
   characteristics observed for different climatic regions. The vernacular
   test subjects were compared against those established according to the
   International Energy Conservation Code and those generated by the
   optimization software. The simulation results of the energy models
   suggest that considering traditions seen in ancient vernacular
   architecture as an approach to improving building energy performance is
   a worthwhile endeavor and a scientific guidance can help enhance the
   performance. The study indicates that, although many vernacular dwells
   exist in the world, it is challenging (but desired) to package
   vernacular architecture traditions and quantitative design knowledge to
   modern building designers. This project is the first part of a much
   larger project that intends to create a knowledge base of vernacular
   building traditions that will include information about not only the
   energy performance of traditional building techniques, but also address
   areas of cost, material availability and cultural traditions. (C) 2009
   Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.enbuild.2009.10.002},
ISSN = {0378-7788},
ResearcherID-Numbers = {Zhai, John/W-4013-2019},
Unique-ID = {WOS:000274135200011},
}

@article{ WOS:000272561500010,
Author = {Liu, J. L.},
Title = {Preventing progressive collapse through strengthening beam-to-column
   connection, Part 2: Finite element analysis},
Journal = {JOURNAL OF CONSTRUCTIONAL STEEL RESEARCH},
Year = {2010},
Volume = {66},
Number = {2},
Pages = {238-247},
Month = {FEB},
Abstract = {This two-part paper presents part of the results of a study devoted to
   the investigation of retrofitting effect of simple steel construction.
   Through strengthening simple beam-to-column connection, progressive
   collapse can be prevented by catenary action. The evaluation of catenary
   action represents a complex analytical problem with a large tension
   affecting its structural behavior. With the advent of high speed
   computers and powerful calculation software package, the finite element
   method offers an ideal tool for tackling such a complex problem. This
   paper develops sophisticated one-, two- and three-dimensional models of
   catenary action, and simulates the post-attack behavior of the original
   and the strengthened structures by means of the ABAQUS finite element
   package. The global behavior of the one-dimensional beam element model
   is close to that corresponding to the two-dimensional solid or the
   three-dimensional shell models, particularly for the structures with
   strengthened joints. Comparison of results between this study and
   literature has been carried out for the purpose of validating the
   present finite element prediction model, Through the comparing
   computational results before and after strengthening, the advantages of
   proposed retrofitting scheme are demonstrated. (C) 2009 Elsevier Ltd.
   All rights reserved.},
DOI = {10.1016/j.jcsr.2009.09.005},
ISSN = {0143-974X},
EISSN = {1873-5983},
Unique-ID = {WOS:000272561500010},
}

@incollection{ WOS:000285166000005,
Author = {Amalio, Nuno and Kelsen, Pierre and Ma, Qin and Glodt, Christian},
Editor = {Katz, S and Mezini, M and Kienzle, J},
Title = {Using VCL as an Aspect-Oriented Approach to Requirements Modelling},
Booktitle = {TRANSACTIONS ON ASPECT-ORIENTED SOFTWARE DEVELOPMENT VII: A COMMON CASE
   STUDY FOR ASPECT-ORIENTED MODELING},
Series = {Lecture Notes in Computer Science},
Year = {2010},
Volume = {6210},
Pages = {151-199},
Abstract = {Software systems are becoming larger and more complex By tackling the
   modularisation of crosscutting concerns, aspect orientation draws
   attention to modularity as a means to address the problems of
   scalability, complexity and evolution in software systems development
   Aspect-oriented modelling (AOM) applies aspect-orientation to the
   construction of models Most existing AOM approaches are designed without
   a formal semantics, and use multi-view partial descriptions of behaviour
   This paper presents an AOM approach based on the Visual Contract
   Language (VCL) a visual language for abstract and precise modelling,
   designed with a formal semantics, and comprising a novel approach to
   visual behavioural modelling based on design by contract where
   behavioural descriptions are total By applying VCL to a large case study
   of a car-crash crisis management system, the paper demonstrates how
   modularity of VCL's constructs, at different levels of granularity, help
   to tackle complexity In particular, it shows how VCL's package construct
   and its associated composition mechanisms are key in supporting
   separation of concerns coarse-grained problem decomposition and
   aspect-orientation The case study's modelling solution has a clear and
   well-defined modular structure, the backbone of this structure is a
   collection of packages encapsulating local solutions to concerns},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-16085-1},
ResearcherID-Numbers = {Ma, Qin/AAI-3810-2021
   },
ORCID-Numbers = {Rodrigues Amalio, Nuno/0000-0003-4762-9968
   Ma, Qin/0000-0001-8520-8190
   Amalio, Nuno/0000-0001-8751-5039},
Unique-ID = {WOS:000285166000005},
}

@inproceedings{ WOS:000287997400127,
Author = {Constantinides, C. and Aristokleous, N. and Johnson, G. A. and
   Perperides, D.},
Book-Group-Author = {IEEE},
Title = {STATIC AND DYNAMIC CARDIAC MODELLING: INITIAL STRIDES AND RESULTS
   TOWARDS A QUANTITATIVELY ACCURATE MECHANICAL HEART MODEL},
Booktitle = {2010 7TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO
   TO MACRO},
Series = {IEEE International Symposium on Biomedical Imaging},
Year = {2010},
Pages = {496-499},
Note = {7th IEEE International Symposium on Biomedical Imaging: From Nano to
   Macro, Rotterdam, NETHERLANDS, APR 14-17, 2010},
Organization = {IEEE; Engn Med Biol Soc; Signal Processing Soc},
Abstract = {Magnetic Resonance Imaging (MRI) has exhibited significant potential for
   quantifying cardiac function and dysfunction in the mouse. Recent
   advances in high-resolution cardiac MR imaging techniques have
   contributed to the development of acquisition approaches that allow fast
   and accurate description of anatomic structures, and accurate surface
   and finite element (FE) mesh model constructions for study of global
   mechanical function in normal and transgenic mice. This study presents
   work in progress for construction of quantitatively accurate
   three-dimensional (3D) and 4D dynamic surface and FE models of murine
   left ventricular (LV) muscle in C57BL/6J (n=10) mice. Constructed models
   are subsequently imported into commercial software packages for the
   solution of the constitutive equations that characterize mechanical
   function, including computation of the stress and strain fields. They
   are further used with solid-free form fabrication processes to construct
   model-based material renditions of the human and mouse hearts.},
DOI = {10.1109/ISBI.2010.5490300},
ISSN = {1945-7928},
ISBN = {978-1-4244-4126-6},
ResearcherID-Numbers = {Aristokleous, Nicolas/O-3106-2019
   Aristokleous, Nicolas/F-6610-2015
   },
ORCID-Numbers = {Aristokleous, Nicolas/0000-0002-0206-7773
   Aristokleous, Nicolas/0000-0002-0206-7773
   Johnson, G.Allan/0000-0002-7606-5447},
Unique-ID = {WOS:000287997400127},
}

@inproceedings{ WOS:000287539100344,
Author = {DeCew, Judson and Baldwin, Kenneth and Celikkol, Barbaros and Chambers,
   Michael and Fredriksson, David W. and Irish, Jim and Langan, Rich and
   Rice, Glenn and Swift, M. Robinson and Tsukrov, Igor},
Book-Group-Author = {IEEE},
Title = {Assessment of a Submerged Grid Mooring in the Gulf of Maine},
Booktitle = {OCEANS 2010},
Series = {OCEANS-IEEE},
Year = {2010},
Note = {Washington State Conference and Trade Center (WSCTC), Seattle, WA, SEP
   20-23, 2010},
Organization = {IEEE; Marine Technol; OES},
Abstract = {The University of New Hampshire (UNH) developed and maintained an
   offshore aquaculture test site in the Western Gulf of Maine, south of
   the Isles of Shoals in approximately 50 m of water. This site was
   designed to have a permanent moored grid to which prototype fish cages
   or surface buoys could be attached for testing new designs and the
   viability of the structure in the exposed Gulf of Maine.
   In 1999, the first moorings deployed consisted of twin single bay grids
   each capable of each securing one fish cage. These systems were
   maintained until 2003. To expand the biomass capacity of the site, the
   single bay moorings were recovered and a new four bay submerged grid
   mooring was deployed within the same foot print of the previous twin
   systems. This unique system operated as a working platform to test
   various structures, including surface and submersible fish cages,
   feeding buoys and other supporting equipment. In addition, the expanded
   capability allowed aquaculture fish studies to be conducted along with
   engineering and new cage/feeder testing.
   The 4 bays of the mooring system were located 15 meters below the
   surface. These bays were supported by nine flotation elements. The
   system was secured to the seafloor on the sides with twelve catenary
   mooring legs, consisting of Polysteel(R) line, 27.5 m of 52 mm chain and
   a 1 ton embedment anchor, and in the center, with a single vertical line
   to a 2 ton weight. To size the mooring gear, the UNH software package
   Aqua-FE was employed. This program can apply waves and currents to
   oceanic structures, predicting system motions and mooring component
   tensions. The submerged grid was designed to withstand 9 meter, 8.8
   second waves with a 1 m/s collinear current, when securing four fish
   cages.
   During its seven year deployment, the site regularly experienced extreme
   weather events, most notably a storm with a 9 m significant wave height,
   10 second dominate period in April 2007. The maximum currents at the
   site were observed during internal solitary wave events when 0.75 m/s
   currents with 25 minute periods and 8 m duration were observed. The
   mooring was recovered in 2010 after 7 years of continuous deployment
   without problems. The dominate maintenance requirement of the mooring
   was the cleaning once a year of excessive mussel growth on the flotation
   elements and grid lines. No problems of anchor dragging or failure of
   mooring components were documented during the deployment. Upon recovery,
   critical mooring components were inspected and documented, focusing on
   items with wear or other areas of interest. The mooring proved to be a
   reliable, stable working platform for a variety of prototype ocean
   projects, highlighting the importance of a sound engineering approach
   taken in the design process.},
ISSN = {0197-7385},
ISBN = {978-1-4244-4333-8},
Unique-ID = {WOS:000287539100344},
}

@inproceedings{ WOS:000282623900088,
Author = {Gales, J. and Bisby, L. and Macdougall, C.},
Editor = {Kodur, V and Franssen, JM},
Title = {Fire Induced Transient Creep Causing Stress Relaxation and Tendon
   Rupture in Unbonded Post-Tensioned Structures: Experiments and Modeling},
Booktitle = {STRUCTURES IN FIRE: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE},
Year = {2010},
Pages = {727+},
Note = {6th International Conference on Structures in Fire (SiF 10), E Lansing,
   MI, JUN 02-04, 2010},
Organization = {Michigan State Univ, Coll Engn},
Abstract = {Unbonded post-tensioned (UPT) flat plate concrete structures are widely
   used in multi-storey construction. They have numerous benefits,
   including reductions in slab thickness and excellent deflection control
   over large spans; their inherent fire resistance is also widely
   considered a key benefit compared to competing floor systems. The fire
   resistant design of UPT structures is typically assured largely on the
   basis of results from unrealistic standard fire resistance tests
   performed prior to 1983. However, much remains unknown about the true
   behavior of continuous multiple bay UPT slabs in real fires. Relatively
   little data exist on the effects of localized exposure to elevated
   temperature on cold drawn prestressing steel under realistic sustained
   service stress levels, and the potential consequences of the resulting
   thermally induced stress relaxation and/or tendon rupture remain
   unexamined. To aid in the fire-safe design and post-fire evaluation of
   real, multiple-bay, continuous UPT structures, large-scale tests and
   computational modeling have been performed to assess high temperature
   stress relaxation in unbonded prestressing steel tendons subjected to
   localized heating, as would be the case in a real building fire. A
   series of novel high temperature experiments and computational modeling
   on locally-heated, stressed and restrained prestressing tendons with
   realistic configurations is presented to shed light on the transient
   response of the tendons at high temperature. Reasonable agreement is
   achieved between observed and predicted stress variation during heating,
   although minor refinement of the predictive model's creep parameters
   appears to be required. Both the experiments and the modeling show that
   inherent inclusion of high temperature creep, as assumed in essentially
   all available fire design guidance documents and software packages
   globally, is unable to represent the true response of UPT tendons in
   realistic UPT concrete slabs. This may lead to unconservative design
   against, and assessment of, fire damage to UPT slabs in both full
   floor-plate and localized fires.},
ISBN = {978-1-60595-027-3},
Unique-ID = {WOS:000282623900088},
}

@inproceedings{ WOS:000397473500022,
Author = {Geramitcioski, Tale and Vilos, Ilios and Mitrevski, Vangelce},
Book-Group-Author = {ADEKO},
Title = {MODELLING THE CONECTION BETWEEN END-PLATE AND STEEL STRUCTURE USING FEM},
Booktitle = {SIXTH INTERNATIONAL SYMPOSIUM ABOUT FORMING AND DESIGN IN MECHANICAL
   ENGINEERING},
Year = {2010},
Pages = {133-136},
Note = {6th International Symposium about Forming and Design in Mechanical
   Engineering (KOD 2010), Palic, SERBIA, SEP 29-30, 2010},
Organization = {Univ Novi Sad, Fac Tech Sci; Assoc Design Elements \& Construct},
Abstract = {In this paper is presented the numerical model for analyzing three cases
   of the end-plate connection of the steel structures fixed by high
   strength bolts with 10.9 class of strength. In the same dimension of the
   parts of the steel structure, the differences exist on the end-plate
   thickness. In the first case, the thickness of the end-plate is dp=12
   mm, in the second case dp=14 mm, and at the third case dp=16 mm.
   Numerical model of the construction based on FEM is functional using the
   ``SOFISTIK{''} software package for real modeling of contact problems in
   the end-plate connections of the steel structures.
   Also, the cooperation between numerical obtained M-4 diagram using FEM
   and the M-Delta diagram obtained from EUROCODE 3 rules are given as a
   part of the final conclusions for improving analysis of the straight
   strength condition and the limited loading of the construction.},
ISBN = {978-86-7892-278-7},
Unique-ID = {WOS:000397473500022},
}

@inproceedings{ WOS:000291982100095,
Author = {Green, Mark L. and Miller, Stephen D. and Vazhkudai, Sudharshan S. and
   Trater, James R.},
Book-Group-Author = {IOP},
Title = {Doing Your Science While You're in Orbit},
Booktitle = {INTERNATIONAL CONFERENCE ON NEUTRON SCATTERING 2009},
Series = {Journal of Physics Conference Series},
Year = {2010},
Volume = {251},
Note = {International Conference on Neutron Scattering 2009, Knoxville, TN, MAY
   03-07, 2009},
Abstract = {Large-scale neutron facilities such as the Spallation Neutron Source
   (SNS) located at Oak Ridge National Laboratory need easy-to-use access
   to Department of Energy Leadership Computing Facilities and experiment
   repository data. The Orbiter thick- and thin-client and its supporting
   Service Oriented Architecture (SOA) based services (available at
   https://orbiter.sns.gov) consist of standards-based components that are
   reusable and extensible for accessing high performance computing, data
   and computational grid infrastructure, and cluster-based resources
   easily from a user configurable interface. The primary Orbiter system
   goals consist of (1) developing infrastructure for the creation and
   automation of virtual instrumentation experiment optimization, (2)
   developing user interfaces for thin- and thick-client access, (3)
   provide a prototype incorporating major instrument simulation packages,
   and (4) facilitate neutron science community access and collaboration.
   The secure Orbiter SOA authentication and authorization is achieved
   through the developed Virtual File System (VFS) services, which use
   Role-Based Access Control (RBAC) for data repository file access, thin-
   and thick-client functionality and application access, and computational
   job workflow management. The VFS Relational Database Management System
   (RDMS) consists of approximately 45 database tables describing 498 user
   accounts with 495 groups over 432,000 directories with 904,077
   repository files. Over 59 million NeXus file metadata records are
   associated to the 12,800 unique NeXus file field/class names generated
   from the 52,824 repository NeXus files. Services that enable (a) summary
   dashboards of data repository status with Quality of Service (QoS)
   metrics, (b) data repository NeXus file field/class name full text
   search capabilities within a Google like interface, (c) fully functional
   RBAC browser for the read-only data repository and shared areas, (d)
   user/group defined and shared metadata for data repository files, (e)
   user, group, repository, and web 2.0 based global positioning with
   additional service capabilities are currently available. The SNS based
   Orbiter SOA integration progress with the Distributed Data Analysis for
   Neutron Scattering Experiments (DANSE) software development project is
   summarized with an emphasis on DANSE Central Services and the Virtual
   Neutron Facility (VNF). Additionally, the DANSE utilization of the
   Orbiter SOA authentication, authorization, and data transfer services
   best practice implementations are presented.},
DOI = {10.1088/1742-6596/251/1/012095},
Article-Number = {012095},
ISSN = {1742-6588},
ORCID-Numbers = {Vazhkudai, Sudharshan/0000-0002-8596-5033},
Unique-ID = {WOS:000291982100095},
}

@inproceedings{ WOS:000279394000016,
Author = {Hemel, Zef and Visser, Eelco},
Editor = {VanDenBrand, M and Gasevic, D and Gray, J},
Title = {PIL: A Platform Independent Language for Retargetable DSLs},
Booktitle = {SOFTWARE LANGUAGE ENGINEERING},
Series = {Lecture Notes in Computer Science},
Year = {2010},
Volume = {5969},
Pages = {224-243},
Note = {2nd International Conference on Software Language Engineering, Denver,
   CO, OCT 05-06, 2009},
Organization = {Software Engn Ctr; Univ Minnesota},
Abstract = {Intermediate languages are used in compiler construction to simplify
   retargeting compilers to multiple machine architectures. In the
   implementation of domain-specific languages (DSLs), compilers typically
   generate high-level source code, rather than low-level machine
   instructions. DSL compilers target a software platform, i.e. a
   programming language with a set of libraries, deployable on one or more
   operating systems. DSLs enable targeting multiple software platforms if
   its abstractions are platform independent. While transformations from
   DSL to each targeted platform are often conceptually very similar, there
   is little reuse between transformations due to syntactic and API
   differences of the target platforms, making supporting multiple
   platforms expensive. In this paper, we discuss the design and
   implementation of PIL, a Platform Independent Language, an intermediate
   language providing a layer of abstraction between DSL and target
   platform code, abstracting from syntactic and API differences between
   platforms, thereby removing the need for platform-specific
   transformations. We discuss the use of PIL in an implemementation of
   WebDSL, a DSL for building web applications.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-12106-7},
ORCID-Numbers = {Visser, Eelco/0000-0002-7384-3370},
Unique-ID = {WOS:000279394000016},
}

@incollection{ WOS:000282535500008,
Author = {Jain, Deepti and Lamour, Valerie},
Editor = {Fenyo, D},
Title = {Computational Tools in Protein Crystallography},
Booktitle = {COMPUTATIONAL BIOLOGY},
Series = {Methods in Molecular Biology},
Year = {2010},
Volume = {673},
Pages = {129-156},
Abstract = {Protein crystallography emerged in the early 1970s and is, to this day,
   one of the most powerful techniques for the analysis of enzyme
   mechanisms and macromolecular interactions at the atomic level. It is
   also an extremely powerful tool for drug design. This field has evolved
   together with developments in computer science and molecular biology,
   allowing faster three-dimensional structure determination of complex
   biological assemblies. In recent times, structural genomics initiatives
   have pushed the development of methods to further speed up this process.
   The algorithms initially defined in the last decade for structure
   determination are now more and more elaborate, but the computational
   tools have evolved toward simpler and more user-friendly packages and
   web interfaces. We present here a modest overview of the popular
   software packages that have been developed for solving protein
   structures, and give a few guidelines and examples for structure
   determination using the two most popular methods, molecular replacement
   and multiple anomalous dispersion.},
DOI = {10.1007/978-1-60761-842-3\_8},
ISSN = {1064-3745},
EISSN = {1940-6029},
ISBN = {978-1-60761-841-6},
ORCID-Numbers = {Lamour, Valerie/0000-0001-7793-4029
   Jain, Deepti/0000-0002-8631-7230},
Unique-ID = {WOS:000282535500008},
}

@inproceedings{ WOS:000285731000071,
Author = {Li Guang and Zhang Dawei},
Editor = {Lu, BL},
Title = {Research on Design Method of Packaging Machinery Based on KBE},
Booktitle = {PROCEEDINGS OF THE 17TH IAPRI WORLD CONFERENCE ON PACKAGING},
Year = {2010},
Pages = {294+},
Note = {17th IAPRI World Conference on Packaging, Tianjin, PEOPLES R CHINA, OCT
   12-15, 2010},
Abstract = {In the design process of packaging machinery, designers often need to
   learn from similar products' design methods that they have been used.
   How to apply knowledge such as principia, guidelines, standards and
   experiences to the CAX system, so that the system can construct the
   required digital geometric model by automated reasoning based on
   relevant knowledge after designers input parameters and application
   requirements to the system? This is precisely the problem that the
   Knowledge-Based Engineering (KBE) can solve. This paper introduces the
   meaning and key technologies of KBE. The digital design method of
   packaging machinery based on KBE is brought forward compared with the
   traditional design process. The structure of packaging machinery design
   system based on KBE is analyzed, and the user interface is shown. The
   system is developed using programming tools VB.NET and SQL Server
   database development tools taking the three-dimensional software
   SolidWorks as a platform. The system established repository of
   principles and design knowledge for packaging machinery using
   object-oriented technology and database technology, and realized the
   reuse of design knowledge using Rule-Based Reasoning and Case-Based
   Reasoning. The developmental efficiency and market responsive speed of
   the development for packaging machinery is improved notably.},
ISBN = {978-1-935068-36-5},
Unique-ID = {WOS:000285731000071},
}

@inproceedings{ WOS:000286453100020,
Author = {Marjane, Abdelaziz and Allailou, Boufeldja},
Editor = {Carlet, C and Pott, A},
Title = {Vectorial Conception of FCSR},
Booktitle = {SEQUENCES AND THEIR APPLICATIONS-SETA 2010},
Series = {Lecture Notes in Computer Science},
Year = {2010},
Volume = {6338},
Pages = {240+},
Note = {6th International Conference on Sequences and Their Applications,
   Telecom ParisTech, Paris, FRANCE, SEP 13-17, 2010},
Organization = {Digiteo; CNRS; LAGA},
Abstract = {In this paper, we investigate the structure of FCSR made by Goresky and
   Klapper. Using a vectorial construction of the objects and of the
   register, we extend the analysis of FCSRs. We call these registers
   vectorial FCSRs or VFCSRs. We obtain similar results to those of
   analysis of FCSRs and of d-FCSRs generating binary sequences or p-ary
   sequences. In fact, the AFSRs built over finite fields F-pn with n >= 2
   suffer from an very difficult and formal analysis. But if you analyze
   these registers with a vectorial structure, you can decompose the output
   sequence into a vector of binary sequences or p-ary sequences. This
   method allows us to obtain very easily the period, the behavior of
   memory with interval optimized, the maximal period, the existence of
   l-sequences and the calculations become explicit and easily
   implementable. At the end of this paper, we implement the quadratic case
   (F-22 case) and present the conclusions about pseudorandom properties of
   quadratic l-sequences which are tested by NIST STS package. In
   conclusion, VFCSRs are easy to implement in software and hardware and
   have excellent pseudorandomn property.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-15873-5},
Unique-ID = {WOS:000286453100020},
}

@article{ WOS:000282245700003,
Author = {Moon, Ka-Leung and Ngai, Eric W. T.},
Title = {R\&D framework for an intelligent fabric sample management system A
   design science approach},
Journal = {INTERNATIONAL JOURNAL OF OPERATIONS \& PRODUCTION MANAGEMENT},
Year = {2010},
Volume = {30},
Number = {7-8},
Pages = {721-743},
Abstract = {Purpose - The purpose of this paper is to examine the problems
   encountered by clothing merchandisers in managing fabric sample
   resources, and analyses their expectations of and requirements for a
   desirable management solution. It then aims to consider the
   architectural design of an intelligent fabric sample management system
   prototype, and the development of a construction and implementation plan
   for a fashion enterprise to install the system.
   Design/methodology/approach - A three-stage methodological approach is
   adopted. The first stage is a preliminary study using in-depth
   interviews with potential system users; the second involves the
   architectural design of a five-component system prototype; and the third
   develops a multi-phase construction and implementation plan using a case
   study.
   Findings - Four categories of problems in the management of fabric
   sample resources are identified, the system architecture is designed,
   and a construction and implementation plan is proposed. Managerial and
   theoretical implications are also discussed.
   Practical implications - The design process of the research and
   development (R\&D) framework enables fashion enterprises to obtain a
   deeper understanding of their operations in managing fabric samples and
   to make wiser decisions in resource allocation. By taking the specific
   needs of system users into consideration, system solution developers are
   able to design tailor-made software packages that are intelligent,
   user-friendly and user-oriented.
   Originality/value - Through a logical and systematic process of
   designing the R\&D framework, the paper identifies some ``technological
   rules{''} which links up the practice of solving an industrial problem
   with the formulation of substantive theories. This adds to the
   application of design science research.},
DOI = {10.1108/01443571011057317},
ISSN = {0144-3577},
EISSN = {1758-6593},
ResearcherID-Numbers = {Ngai, Eric/L-8152-2015
   Ngai, Eric/ABC-2167-2020
   },
ORCID-Numbers = {Ngai, Eric/0000-0002-7278-7434
   NGAI, WT Eric/0000-0001-6891-6750},
Unique-ID = {WOS:000282245700003},
}

@inproceedings{ WOS:000288686900011,
Author = {Trisovic, Natasa and Lazovic, Tatjana and Mitrovic, Caslav and
   Marinkovic, Aleksandar and Lazarevic, Mihailo and Sumarac, Dragoslav and
   Golubovic, Zoran},
Editor = {Martin, O and Zheng, X},
Title = {New procedure for dynamic structural reanalysis},
Booktitle = {LATEST TRENDS ON ENGINEERING MECHANICS, STRUCTURES, ENGINEERING GEOLOGY},
Series = {Mathematics and Computers in Science and Engineering},
Year = {2010},
Pages = {57+},
Note = {3rd WSEAS International Conference on Engineering Mechanics, Structures,
   Engineering Geology/International Conference on Geography and Geology,
   Corfu Island, GREECE, JUL 22-24, 2010},
Abstract = {The methods of structural dynamic modification, especially those with
   their roots in finite element models, have often been described as
   reanalysis. The present paper deals with the problem of improving of
   dynamic characteristics some structures. New dynamic modification
   procedure is given as using distribution of potential and kinetic energy
   in every finite element is used for analysis. The main goal of dynamic
   modification is to increase natural frequencies and to increase the
   difference between them. Some information should be prepared, before
   setting up the FE model. The first pack of information includes referent
   pieces of information about the structure: size, material, and boundary
   conditions. It should be noticed that dynamic response is given
   primarily through corresponding eigenfrequencies and main oscillation
   forms as characteristic (typical) variables. Changing them by changing
   the design parameters of a structure it is possible to achieve (can
   bring about) requested structural dynamic response. Sensitivity analysis
   is an important point within the dynamical modification process.
   Sensitivity analysis represents a collection of mathematical methods for
   reanalyzing constructions which is, within dynamical modification,
   related to sensitivity of eigenvalues and eigenvectors. Therefore, the
   application of sensitivity analysis is limited to construction of
   segments for which necessary mathematical relations can be determined.
   If this is not possible, sensitivity analysis is only partially
   applicable. Dynamical analysis of complex structures can easily be
   conducted via finite elements modeling. Therefore, while finite element
   analysis method is highly adequate for modeling complex structures, one
   of its major drawbacks lies in the usage of large number of degrees of
   freedom in calculating the exact eigenpairs. This number can amount to
   few tens of thousands, or even more. To reduce the calculation time it
   is possible to divide the complex structure into connected substructures
   and analyze each one separately. The dynamical behavior of each
   substructure is represented only by a reduced set of eigenpairs of
   interest, which contributes to significant problem simplification. A
   more general problem of structural dynamic analysis has three important
   aspects. Firstly, the observed physical structure is represented by
   initial finite element model. Modeling is based on numerous idealizing
   approximations within an exaggerated elaboration of details, which in
   essence does not significantly improve the accuracy of output data,
   especially having available powerful computers and appropriate software
   packages. Optimal alternative is to have the possibility of verifying
   outputted data that were measured on a prototype or real structure.
   Secondly, the dynamic characteristics of construction under reanalysis
   are analyzed. What is basically observed are eigenvalues and main forms
   of oscillations as characteristic variables that can invoke inadequate
   actual dynamic behavior. Thirdly, on the basis of analysis of actual
   dynamic behavior, modification steps are proposed after which a modified
   model is obtained. Having in mind that mechanical structures are most
   often very complex, the most convenient modification steps are not
   easily obtained. Figure 1 shows a simplified triangle of fine
   reanalysis. Choosing the structural parts most suitable for reanalysis
   requires the analysis of sensitivity for separate segments to changes in
   construction. Most importantly, the best result should be obtained with
   minimal changes.
   This most frequently involves the icrease in frequency and distance
   between two neighboring frequencies.},
ISSN = {1792-4308},
ISBN = {978-960-474-203-5},
ResearcherID-Numbers = {Lazarevic, Mihailo/L-2396-2018
   },
ORCID-Numbers = {Lazarevic, Mihailo/0000-0002-3326-6636
   Marinkovic, Aleksandar/0000-0003-1657-4431
   Lazovic, Tatjana/0000-0001-8919-3336
   Sumarac, Dragoslav/0000-0002-4045-5582},
Unique-ID = {WOS:000288686900011},
}

@article{ WOS:000273526300006,
Author = {van de Lindt, John W. and Pei, Shiling and Liu, Hongyan and Filiatrault,
   Andre},
Title = {Three-Dimensional Seismic Response of a Full-Scale Light-Frame Wood
   Building: Numerical Study},
Journal = {JOURNAL OF STRUCTURAL ENGINEERING-ASCE},
Year = {2010},
Volume = {136},
Number = {1},
Pages = {56-65},
Month = {JAN},
Abstract = {The experimental seismic responses of a full-scale two-story light-frame
   wood townhouse building, designed to modern U.S. engineered seismic
   design requirements, were compared against the predictions of a new
   software package entitled seismic analysis package for woodframe
   structures (SAPWood) developed recently within the NEESWood Project. The
   main objective of this paper was to verify the accuracy of the
   predictions from the SAPWood model, which incorporates shear
   deformations of shear walls as well as cumulative floor displacements
   caused by the out-of-plane rotations of the floor and ceiling
   diaphragms. A comparison was conducted on interstory drifts and shear
   wall deformations for various structural configurations (construction
   phases) of the test building and excitation levels. Good agreement was
   found between the numerical predictions and test results for the four
   different construction phases. The SAPWood model was shown to be a
   promising numerical tool for predicting the seismic response of
   light-frame wood structures.},
DOI = {10.1061/(ASCE)ST.1943-541X.0000086},
ISSN = {0733-9445},
ResearcherID-Numbers = {Pei, Shiling/S-6672-2017},
ORCID-Numbers = {Pei, Shiling/0000-0002-6458-3124},
Unique-ID = {WOS:000273526300006},
}

@inproceedings{ WOS:000285850200070,
Author = {Wolk, Stefan and Shea, Kristina},
Book-Group-Author = {ASME},
Title = {A COMPUTATIONAL PRODUCT MODEL FOR CONCEPTUAL DESIGN USING SYSML},
Booktitle = {ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND
   COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, PROCEEDINGS, VOL 2,
   PTS A AND B},
Year = {2010},
Pages = {635-645},
Note = {ASME International Design Engineering Technical Conferences/Computers
   and Information in Engineering Conference, San Diego, CA, AUG 30-SEP 02,
   2009},
Organization = {ASME, Design Engn Div; ASME, Computers \& Info Engn Div},
Abstract = {The importance of the concept development phase in product development
   is contradictory to the level and amount of current computer-based
   support for it, especially with regards to mechanical design Paper-based
   methods for conceptual design offer a far greater level of maturity and
   familiarity than current computational methods Engineers usually work
   with software designed to address only a single stage of the concept
   design phase, such as requirements management tools Integration with
   software covering other stages, e g functional modeling, is generally
   poor Using the requirements for concept models outlined in the VDI 2221
   guideline for systematic product development as a starting point, the
   authors propose an integrated product model constructed using the
   Systems Modeling Language (SysML) that moves beyond geometry to
   integrate all necessary aspects for conceptual design These include
   requirements, functions and function structures, working principles and
   their structures as well as physical effects In order to explore the
   applicability of SysML for mechanical design, a case study on the design
   of a passenger car's luggage compartment cover is presented The case
   study shows that many different SysML diagram types are suitable for
   formal modeling in mechanical concept design, though they were
   originally defined for software and control system development It is
   then proposed that the creation and use of libraries defining generic as
   well as more complicated templates raises efficiency in modeling The use
   of diagrams and their semantics for conceptual modeling make SysML a
   strong candidate for integrated product modeling of mechanical as well
   as mechatronic systems},
ISBN = {978-0-7918-4899-9},
Unique-ID = {WOS:000285850200070},
}

@article{ WOS:000273227800001,
Author = {Pellicer, E. and Pellicer, T. M. and Catala, J.},
Title = {An Integrated Control System for SMEs in the Construction Industry},
Journal = {REVISTA DE LA CONSTRUCCION},
Year = {2009},
Volume = {8},
Number = {2},
Pages = {4-17},
Month = {DEC},
Abstract = {Most construction companies are small and medium-sized enterprises
   (SMEs) that manage project-based and business-focused activities
   simultaneously. Commercial software packages are not fully developed to
   offer a unique solution that tackles both. To fill this gap, this paper
   describes an integrated management system specifically developed for
   SMEs in the construction industry, whether contractors or consulting
   engineering and architectural firms. Both, project and business
   functions are addressed and handled by the system as a whole. Particular
   attention is given to the procurement process because it is essential
   for the strategic planning of these companies, which must naturally
   engage new contracts to remain in business. The system is based on a
   client/server architecture that is arranged in three tiers: presentation
   (user interfaces and forms), application (queries), and data (relational
   tables). The system uses MS Access (TM) as a database management system.
   Users are categorized according to their functions within the hierarchy
   of the company, and therefore several different interfaces are designed
   for each personnel category. The system works in real-time, so every
   employee with a pre-established right of access can obtain information
   instantaneously. This is crucial for making immediate decisions when
   problems arise, allowing prompt actions from every manager in the
   hierarchy and resulting in noteworthy time savings. This computer-based
   application has been successfully implemented by four Spanish SMEs in
   the construction industry. The suitability and advantages of the system
   implementation are highlighted in this paper with specific data
   regarding its current operation.},
ISSN = {0718-915X},
ResearcherID-Numbers = {Pellicer, Eugenio/B-5593-2012},
ORCID-Numbers = {Pellicer, Eugenio/0000-0001-9100-0644},
Unique-ID = {WOS:000273227800001},
}

@article{ WOS:000272110800007,
Author = {Birk, Yitzhak and Fiksman, Evgeny},
Title = {Dynamic reconfiguration architectures for multi-context FPGAs},
Journal = {COMPUTERS \& ELECTRICAL ENGINEERING},
Year = {2009},
Volume = {35},
Number = {6, SI},
Pages = {878-903},
Month = {NOV},
Abstract = {Field-programmable gate arrays (FPGAs) are being integrated with
   processors on the same motherboard or even chip in order to achieve
   flexible high-performance computing, and this may become main stream in
   chip multi-core architectures. However, the expensive FPGA area is often
   used inefficiently, with much of the logic idle at any given time. This
   work, motivated by the Dynamic-Link Library (DLL) concept in software,
   explores the possibility of ``hardware DLLs{''} by finding ways for fast
   dynamic incremental reconfiguration of FPGAs. So doing would, among
   other things, enable same-function replication at any given time, with
   functions changing quickly over time, thereby enabling efficient
   exploitation of data parallelism at no additional hardware cost.
   We present two new multi-context FPGA architectures based on two
   different configuration storage architectures: local and centralized.
   Problems such as configuration storage and reconfiguration (time, power
   and space) overhead are considered. Weil known area and power models are
   used in evaluating various approaches and in order to provide guidelines
   for matching architectures to target applications. Lastly, we provide
   insights into resulting scheduling issues. Our findings provide the
   foundation and ``rules of the game{''} for subsequent development of
   reconfiguration schedulers and execution environments. (C) 2008 Elsevier
   Ltd. All rights reserved.},
DOI = {10.1016/j.compeleceng.2008.11.024},
ISSN = {0045-7906},
EISSN = {1879-0755},
ORCID-Numbers = {Birk, Yitzhak/0000-0001-7754-1970},
Unique-ID = {WOS:000272110800007},
}

@article{ WOS:000270619300010,
Author = {Ganesan, Dharmalingam and Keuler, Thorsten and Nishimura, Yutaro},
Title = {Architecture compliance checking at run-time},
Journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
Year = {2009},
Volume = {51},
Number = {11, SI},
Pages = {1586-1600},
Month = {NOV},
Abstract = {In this paper, we report on our experiences with architecture compliance
   checking - the process of checking whether the planned or specified
   software architecture is obeyed by the running system - of an
   OSGi-based, dynamically evolving application in the office domain. To
   that end, we first show how to dynamically instrument a running system
   in the context of OSGi in order to collect run-time traces. Second, we
   explain how to bridge the abstraction gap between run-time traces and
   software architectures, through the construction of hierarchical Colored
   Petri nets (CP-nets). In addition, we demonstrate how to design reusable
   hierarchical CP-nets. In an industry example. we were able to extract
   views that helped us to identify a number of architecturally relevant
   issues (e.g., architectural style violations, behavior violations) that
   would not have been detected otherwise, and could have caused serious
   problems like system malfunctioning or unauthorized access to sensitive
   data. Finally, we package valuable experiences and lessons learned from
   this endeavor. (C) 2009 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.infsof.2009.06.007},
ISSN = {0950-5849},
EISSN = {1873-6025},
Unique-ID = {WOS:000270619300010},
}

@article{ WOS:000268543800004,
Author = {Lam, Alexander and Boehm, Barry},
Title = {Experiences in developing and applying a software engineering technology
   testbed},
Journal = {EMPIRICAL SOFTWARE ENGINEERING},
Year = {2009},
Volume = {14},
Number = {5},
Pages = {579-601},
Month = {OCT},
Abstract = {A major problem in empirical software engineering is to determine or
   ensure comparability across multiple sources of empirical data. This
   paper summarizes experiences in developing and applying a software
   engineering technology testbed. The testbed was designed to ensure
   comparability of empirical data used to evaluate alternative software
   engineering technologies, and to accelerate the technology maturation
   and transition into project use. The requirements for such software
   engineering technology testbeds include not only the specifications and
   code, but also the package of instrumentation, scenario drivers, seeded
   defects, experimentation guidelines, and comparative effort and defect
   data needed to facilitate technology evaluation experiments. The
   requirements and architecture to build a particular software engineering
   technology testbed to help NASA evaluate its investments in software
   dependability research and technology have been developed and applied to
   evaluate a wide range of technologies. The technologies evaluated came
   from the fields of architecture, testing, state-model checking, and
   operational envelopes. This paper will present for the first time the
   requirements and architecture of the software engineering technology
   testbed. The results of the technology evaluations will be analyzed from
   a point of view of how researchers benefitted from using the SETT. The
   researchers just reported how their technology performed in their
   original findings. The testbed evaluation showed (1) that certain
   technologies were complementary and cost-effective to apply; (2) that
   the testbed was cost-effective to use by researchers within a
   well-specified domain of applicability; (3) that collaboration in
   testbed use by researchers and the practitioners resulted comparable
   empirical data and in actions to accelerate technology maturity and
   transition into project use, as shown in the AcmeStudio evaluation; and
   (4) that the software engineering technology testbed's requirements and
   architecture were suitable for evaluating technologies and accelerating
   their maturation and transition into project use.},
DOI = {10.1007/s10664-008-9096-2},
ISSN = {1382-3256},
EISSN = {1573-7616},
ResearcherID-Numbers = {Celar, Stipe/G-4728-2017},
ORCID-Numbers = {Celar, Stipe/0000-0003-4234-5819},
Unique-ID = {WOS:000268543800004},
}

@article{ WOS:000270016400010,
Author = {Douglas, Shawn M. and Marblestone, Adam H. and Teerapittayanon, Surat
   and Vazquez, Alejandro and Church, George M. and Shih, William M.},
Title = {Rapid prototyping of 3D DNA-origami shapes with caDNAno},
Journal = {NUCLEIC ACIDS RESEARCH},
Year = {2009},
Volume = {37},
Number = {15},
Pages = {5001-5006},
Month = {AUG},
Abstract = {DNA nanotechnology exploits the programmable specificity afforded by
   base-pairing to produce self-assembling macromolecular objects of custom
   shape. For building megadalton-scale DNA nanostructures, a long
   `scaffold' strand can be employed to template the assembly of hundreds
   of oligonucleotide `staple' strands into a planar antiparallel array of
   cross-linked helices. We recently adapted this `scaffolded DNA origami'
   method to producing 3D shapes formed as pleated layers of double helices
   constrained to a honeycomb lattice. However, completing the required
   design steps can be cumbersome and time-consuming. Here we present
   caDNAno, an open-source software package with a graphical user interface
   that aids in the design of DNA sequences for folding 3D
   honeycomb-pleated shapes A series of rectangular-block motifs were
   designed, assembled, and analyzed to identify a well-behaved motif that
   could serve as a building block for future studies. The use of caDNAno
   significantly reduces the effort required to design 3D DNA-origami
   structures. The software is available at http://cadnano.org/, along with
   example designs and video tutorials demonstrating their construction.
   The source code is released under the MIT license.},
DOI = {10.1093/nar/gkp436},
ISSN = {0305-1048},
EISSN = {1362-4962},
ResearcherID-Numbers = {Douglas, Shawn/GVU-2004-2022
   },
ORCID-Numbers = {Douglas, Shawn/0000-0001-5398-9041
   Marblestone, Adam/0000-0001-9833-9931
   church, george/0000-0001-6232-9969},
Unique-ID = {WOS:000270016400010},
}

@article{ WOS:000267455100005,
Author = {Bogatkov, D. and Babadagli, T.},
Title = {Characterization of Fracture Network System of the Midale Field},
Journal = {JOURNAL OF CANADIAN PETROLEUM TECHNOLOGY},
Year = {2009},
Volume = {48},
Number = {7},
Pages = {30-39},
Month = {JUL},
Note = {Canadian International Petroleum Conference, Calgary, CANADA, JUN 16-18,
   2009},
Organization = {Fekete; Sproule; Champion Technol; KADE Technol; June Warren-Nickles
   Energy Grp; DeGolyer MacNaughton; ajm Petroleum Consultants},
Abstract = {Enhanced oil recovery from challenging/complex fields requires extensive
   analysis of reservoir structure and good understanding of the effect of
   this structure on the dynamics of the process. Naturally fractured
   reservoirs are good examples of this kind and their fracture network
   characterization is still a big challenge.
   In this study, we analyzed the fracture network system of a portion of
   the Midale Field, a naturally fractured carbonate reservoir in the
   Williston Basin of southeastern Saskatchewan, Canada. Our study aims at
   an extensive characterization of fracture and fracture network
   properties and construction of a reliable fracture network model for
   further use in assessing the oil recovery by CO(2) injection and CO(2)
   sequestration potential.
   We integrated static data such as cores, logs and well tests to build 3D
   discrete fracture network models. Stochastic numerical approach was
   applied using a commercial software package.
   A fracture network constructed from static data was calibrated using
   well test data. Several parameters were evaluated in sensitivity studies
   to determine those characteristics of the network which have higher
   influence on the reservoir performance. Simulated well test response was
   checked against previously published well test data. This study allowed
   us to recognize uncertainties in critical parameters and propose some
   measures to manage those uncertainties.},
DOI = {10.2118/09-07-30},
ISSN = {0021-9487},
Unique-ID = {WOS:000267455100005},
}

@article{ WOS:000266249400007,
Author = {Wiese, D. N. and Folkner, W. M. and Nerem, R. S.},
Title = {Alternative mission architectures for a gravity recovery satellite
   mission},
Journal = {JOURNAL OF GEODESY},
Year = {2009},
Volume = {83},
Number = {6},
Pages = {569-581},
Month = {JUN},
Abstract = {Since its launch in 2002, the Gravity Recovery and Climate Experiment
   (GRACE) mission has been providing measurements of the time-varying
   Earth gravity field. The GRACE mission architecture includes two
   satellites in near-circular, near-polar orbits separated in the
   along-track direction by approximately 220 km (e.g. collinear). A
   microwave ranging instrument measures changes in the distance between
   the spacecraft, while accelerometers on each spacecraft are used to
   measure changes in distance due to non-gravitational forces. The fact
   that the satellites are in near-polar orbits coupled with the fact that
   the inter-satellite range measurements are directed in the along-track
   direction, contributes to longitudinal striping in the estimated gravity
   fields. This paper examines four candidate mission architectures for a
   future gravity recovery satellite mission to assess their potential in
   measuring the gravity field more accurately than GRACE. All satellites
   were assumed to have an improved measurement system, with an
   inter-satellite laser ranging instrument and a drag-free system for
   removal of non-gravitational accelerations. Four formations were
   studied: a two-satellite collinear pair similar to GRACE; a
   four-satellite architecture with two collinear pairs; a two-satellite
   cartwheel formation; and a four-satellite cartwheel formation. A
   cartwheel formation consists of satellites performing in-plane, relative
   elliptical motion about their geometric center, so that inter-satellite
   measurements are, at times, directed radially (e.g. parallel to the
   direction towards the center of the Earth) rather than along-track.
   Radial measurements, unlike along-track measurements, have equal
   sensitivity to mass distribution in all directions along the Earth's
   surface and can lead to higher spatial resolution in the derived gravity
   field. The ability of each architecture to recover the gravity field was
   evaluated using numerical simulations performed with JPL's GIPSY-OASIS
   software package. Thirty days of data were used to estimate gravity
   fields complete to degree and order 60. Evaluations were done for 250
   and 400 km nominal orbit altitudes. The sensitivity of the recovered
   gravity field to under-sampled effects was assessed using simulated
   errors in atmospheric/ocean dealiasing (AOD) models. Results showed the
   gravity field errors associated with the four-satellite cartwheel
   formation were approximately one order of magnitude lower than the
   collinear satellite pair when only measurement system errors were
   included. When short-period AOD model errors were introduced, the
   gravity field errors for each formation were approximately the same. The
   cartwheel formations eliminated most of the longitudinal striping seen
   in the gravity field errors. A covariance analysis showed the error
   spectrum of the cartwheel formations to be lower and more isotropic than
   that of the collinear formations.},
DOI = {10.1007/s00190-008-0274-1},
ISSN = {0949-7714},
EISSN = {1432-1394},
Unique-ID = {WOS:000266249400007},
}

@article{ WOS:000266503200003,
Author = {Yang, Ming and Bourbakis, Nikolaos G.},
Title = {An Efficient Packet Loss Recovery Methodology for Video Streaming Over
   IP Networks},
Journal = {IEEE TRANSACTIONS ON BROADCASTING},
Year = {2009},
Volume = {55},
Number = {2, 1},
Pages = {190-201},
Month = {JUN},
Abstract = {Large amounts of data and limited bandwidth are always at odds for
   digital video streaming over Internet Protocol (IP) networks. Packets
   could possibly be delayed or lost. Sender-based recovery techniques for
   packet loss generate redundant information that causes lower compression
   ratio and consumes additional bandwidth. In order to address this issue,
   a novel information hiding based recovery methodology has been
   developed. The basic idea is that the redundant information used for
   error recovery is embedded within the frames of the original video
   contents, by means of high bitrate information hiding techniques. In the
   receiver end, the hidden information is extracted for lost-packet
   recovery. Experimental results show that the damaged macroblocks can be
   recovered with a higher quality using the proposed methodology, as
   compared to spatial extrapolation used in H.264/AVC reference software.
   The main contributions of the proposed methodology are: 1) the delivery
   of redundant information does not increase bandwidth requirement
   significantly under the same media quality, bringing out advantages in
   practical applications; 2) the structures of encoder and decoder do not
   need to be changed in the case of intra-only coding mode.},
DOI = {10.1109/TBC.2009.2016491},
ISSN = {0018-9316},
EISSN = {1557-9611},
Unique-ID = {WOS:000266503200003},
}

@article{ WOS:000267197700001,
Author = {Suardin, Jaffee A. and McPhate, Jr., A. Jeff and Sipkema, Anthony and
   Childs, Matt and Mannan, M. Sam},
Title = {Fire and explosion assessment on oil and gas floating production storage
   offloading (FPSO): An effective screening and comparison tool},
Journal = {PROCESS SAFETY AND ENVIRONMENTAL PROTECTION},
Year = {2009},
Volume = {87},
Number = {3},
Pages = {147-160},
Month = {MAY},
Abstract = {Fires and explosions have been identified as major potential hazards for
   Oil and Gas Floating Production Storage Offloading (FPSO) installations
   and pose risk to personnel, assets, and the environment. Current fire
   and explosion assessment (FEA) tools require physical effect modeling
   software and follows standards from API, ISO, and engineering practices.
   However, the tools are not specific to any particular system such as an
   FPSO, and do not provide comprehensive guidance for safety engineers to
   perform FEA.
   This paper discusses the development of a screening and comparison tool
   for FEA on FPSOs and the incorporation of an expert system into the
   tool. The results are computerized using MS Excel/VBA to provide a
   structured and comprehensive assessment on each equipment and module
   handling natural gas, crude oil, methanol and diesel on FPSO topsides.
   This tool features built-in calculations for jet and pool fire size
   estimation for gas/liquid releases, and the ability to perform
   Quantitative Risk Analysis (QRA) to specify the personnel and equipment
   risk for varying leak sizes and process conditions. Control and recovery
   measures are incorporated as an expert system based on report findings,
   engineering practices, and relevant standards. Bowtie analysis is
   applied in the tool to define detailed control and recovery measures for
   the FPSO based on the incident scenarios. An explosion assessment is
   performed by incorporating physical effect modeling software results.
   Unique features provided in the tool include fire and radiation contour
   mapping on an FPSO layout to help determine personnel and equipment risk
   more accurately and fire pump sizing that can be used to verify the
   amount of water deluge system required to mitigate fires and explosions.
   in addition, flexibility of data input (process data, failure rate data,
   etc.) and user interfaces assist safety engineers to screen and compare
   process alternatives, check design quality, and evaluate design options
   at any design stage. (C) 2008 The Institution of Chemical Engineers.
   Published by Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.psep.2008.12.002},
ISSN = {0957-5820},
EISSN = {1744-3598},
Unique-ID = {WOS:000267197700001},
}

@article{ WOS:000267326700004,
Author = {Al-Qallaf, Charlene L.},
Title = {A bibliometric analysis of the Punica grantum L. literature},
Journal = {MALAYSIAN JOURNAL OF LIBRARY \& INFORMATION SCIENCE},
Year = {2009},
Volume = {14},
Number = {1},
Pages = {83-103},
Month = {APR},
Abstract = {The purpose of this study was to identify and analyze the intellectual
   structure of the Punica granatum L (pomegranate) literature and to
   determine trends and patterns. Specific areas addressed were growth of
   the literature, publication type, author productivity and patterns,
   subject focus, language dispersion, and characteristics of the journal
   literature. Thirty-one related databases and the online catalogs of two
   United States national libraries were searched to identify publications.
   The final data set consisted of 3,306 items. All publications were
   reviewed through 2006. Data were sorted and manipulated using the
   software package ProCite. For analysis of the data, bibliometric
   techniques were applied. The results show that the literature has grown
   consistently from 1970 onwards exploding to significant proportions
   beginning in 2000. Most of the publications are the result of author
   collaboration (71.82\%) and written in the English language (69.57\%).
   India and the United States are the leading contributors to the
   literature and educational institutions make-up more than fifty percent
   of the authors' affiliation. The literature is multi- and
   inter-disciplinary in nature. The major subject areas are plant
   diseases, growth (plants), botanical chemistry, pharmacognosy, and plant
   products. Journal articles (75.53\%) constitute the largest single type
   of publication. There are 1,045 unique journal titles containing 2,497
   publications. According to Bradford's Law a core of 38 journal titles
   form the nucleus of this literature. This study provides future
   direction for researchers, facilitates discussion within multiple
   disciplines, and assists information providers in formulating policy
   guidelines for the selection and acquisition of information resources.},
ISSN = {1394-6234},
Unique-ID = {WOS:000267326700004},
}

@article{ WOS:000272462600012,
Author = {Arquero, A. and Alvarez, M. and Martinez, E.},
Title = {Decision Management Making by AHP (Analytical Hierarchy Process) trought
   GIS data},
Journal = {IEEE LATIN AMERICA TRANSACTIONS},
Year = {2009},
Volume = {7},
Number = {1},
Pages = {101-106},
Month = {MAR},
Abstract = {In this work, we propose the use of the AHP (Analytical Hierarchy
   Process) as a mathematical tool to structure a multiple criteria problem
   like a visual pattern. The main objective pursued is to determine what
   is the optimal placing to build a urban construction to locate a
   university library. In this case, it has been applied for the Campus of
   Montegancedo of the Polytechnic University of Madrid (Spain), where the
   Faculty of Computer science is placed. The data come from a Geographical
   Information System (GIS). Three different profiles of standard users
   have been considered and they determine the management of the final
   decision to take, in order to carry out the study. The use of the
   commercial software Expert Choice facilitates efficiently this
   management.},
DOI = {10.1109/TLA.2009.5173471},
ISSN = {1548-0992},
ResearcherID-Numbers = {Martinez, Estibaliz/C-1338-2012
   Álvarez-Alonso, Marina/AEA-3444-2022},
ORCID-Numbers = {Martinez, Estibaliz/0000-0003-0296-6151
   },
Unique-ID = {WOS:000272462600012},
}

@article{ WOS:000272000600005,
Author = {Magli, Giulio and Schiavottiello, Nicola},
Title = {The Megalithic Building of S. Erasmo di Cesi: Architecture, Astronomy,
   and Landscape},
Journal = {NEXUS NETWORK JOURNAL},
Year = {2009},
Volume = {11},
Number = {1},
Pages = {51-61},
Month = {SPR},
Abstract = {One of the most enigmatic megalithic buildings of Italy is the structure
   which lies on the S. Erasmo hill near Cesi, in Umbria, a huge complex
   encompassing an area of around 8000 square meters and enclosed by
   refined cyclopean walls. Although its date is uncertain, suggested dares
   comprise the Iron Age and archaic period, down to the third century B.C.
   The building's function is also uncertain. Usually identified as a
   fortified structure, in fact there is a megalithic platform at the
   Southern end of the enclosure which could have served as foundation of a
   temple or palace and, from the top of Monte Torre Maggiore, a complex of
   temples dating from the fourth century B.C. overlooks the hill. Similar
   combinations of megalithic buildings resting half-way to temples placed
   on high peaks are known to exist. In order to clarify the function of
   this structure and its position in relation to the surrounding
   landscape, with particular attention to its visibility and to the
   directions of visibility from the complex, as well as to the possible
   astronomical alignments, we present a multi-disciplinary approach to the
   study of the S. Erasmo complex, which includes the mapping of the sky at
   the various possible epochs of construction, the creation of a digital
   model of the landscape in forms of digital maps using Geographic
   Information System technologies, and a 3D model using various 3D
   software packages.},
DOI = {10.1007/s00004-007-0084-4},
ISSN = {1590-5896},
EISSN = {1522-4600},
ORCID-Numbers = {Schiavottiello, Nicola/0000-0001-5013-6641},
Unique-ID = {WOS:000272000600005},
}

@article{ WOS:000262884700021,
Author = {Plets, Ruth M. K. and Dix, Justin K. and Adams, Jon R. and Bull,
   Jonathan M. and Henstock, Timothy J. and Gutowski, Martin and Best,
   Angus I.},
Title = {The use of a high-resolution 3D Chirp sub-bottom profiler for the
   reconstruction of the shallow water archaeological site of the Grace
   Dieu (1439), River Hamble, UK},
Journal = {JOURNAL OF ARCHAEOLOGICAL SCIENCE},
Year = {2009},
Volume = {36},
Number = {2},
Pages = {408-418},
Month = {FEB},
Abstract = {The remains of Henry Vs flagship, the Grace Dieu, currently lie buried
   within the inter-tidal sediments of the River Hamble (S. England).
   Previous archaeological investigations have been hindered by difficult
   excavation conditions resulting in a poor understanding of the
   dimensions, shape and degradation state of the hull's deeper structure.
   This study therefore aimed to image, characterize and reconstruct the
   buried remains of this vessel using a high-resolution 3D acoustic
   sub-bottom Chirp system with RTK-GPS positioning capability. The
   accurate navigation and high-resolution data that were acquired enabled
   the construction of a full 3D image of the site that not only identified
   the remains of the wooden hull, but also features buried within it. in
   addition, the degradation state of these buried wooden remains were
   investigated by calculating reflection coefficients while a hypothetical
   larger reconstruction of the Grace Dieu's hull was achieved, through the
   use of the ShipShape ship design software package.
   The results of this project demonstrate that (i) acoustic data can be
   used to successfully image buried wooden shipwrecks, (ii) artefacts are
   buried within the hull of the Grace Dieu, (iii) there is variation in
   the degradation state of the buried timbers, as calculated from the
   acoustic data, with the shell of the vessel being moderately well
   preserved, and (iv) the Grace Dieu was a very large ship for its time
   (possibly over 60 in long and 16 in wide).
   The outcomes of this research not only have considerable implications
   for the management and monitoring of submerged and buried archaeological
   sites but also for planning intrusive surveys, should they be required.
   (C) 2008 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.jas.2008.09.026},
ISSN = {0305-4403},
ResearcherID-Numbers = {Plets, Ruth/AAG-9717-2019
   Dix, Justin/X-9344-2019
   Dix, Justin/E-7289-2011
   Henstock, Timothy J/C-3583-2014
   },
ORCID-Numbers = {Plets, Ruth/0000-0002-9092-7533
   Dix, Justin/0000-0003-2905-5403
   Dix, Justin/0000-0003-2905-5403
   Henstock, Timothy J/0000-0002-2132-2514
   Bull, Jonathan/0000-0003-3373-5807},
Unique-ID = {WOS:000262884700021},
}

@article{ WOS:000413029100001,
Author = {Peirce, Jonathan W.},
Title = {Generating stimuli for neuroscience using PsychoPy},
Journal = {FRONTIERS IN NEUROINFORMATICS},
Year = {2009},
Volume = {2},
Month = {JAN 15},
Abstract = {PsychoPy is a software library written in Python, using OpenGL to
   generate very precise visual stimuli on standard personal computers. It
   is designed to allow the construction of as wide a variety of
   neuroscience experiments as possible, with the least effort. By writing
   scripts in standard Python syntax users can generate an enormous variety
   of visual and auditory stimuli and can interact with a wide range of
   external hardware (enabling its use in fMRI, EEG, MEG etc.). The
   structure of scripts is simple and intuitive. As a result, new
   experiments can be written very quickly, and trying to understand a
   previously written script is easy, even with minimal code comments.
   PsychoPy can also generate movies and image sequences to be used in
   demos or simulated neuroscience experiments. This paper describes the
   range of tools and stimuli that it provides and the environment in which
   experiments are conducted.},
DOI = {10.3389/neuro.11.010.2008},
Article-Number = {10},
ISSN = {1662-5196},
ResearcherID-Numbers = {Peirce, Jonathan Westley/AAA-8603-2020},
ORCID-Numbers = {Peirce, Jonathan Westley/0000-0002-9504-4342},
Unique-ID = {WOS:000413029100001},
}

@article{ WOS:000261873500005,
Author = {Yang, Feng-Rong and Lee, Cheng-Haw and Kung, Wen-Jui and Yeh, Hsin-Fu},
Title = {The impact of tunneling construction on the hydrogeological environment
   of ``Tseng-Wen Reservoir Transbasin Diversion Project{''} in Taiwan},
Journal = {ENGINEERING GEOLOGY},
Year = {2009},
Volume = {103},
Number = {1-2},
Pages = {39-58},
Month = {JAN 8},
Abstract = {Groundwater flow and the associated surface water flow are potential
   negative factors on underground tunnels. Early detection of
   environmental impacts on water resources is of significant importance to
   planning, design and construction of tunnel projects, as early detection
   can minimize accidents and project delays during construction. The
   groundwater modeling software package Groundwater Modeling System (GMS),
   which supports the groundwater numerical codes MODFLOW and FEMWATER, was
   utilized to determine the impact of tunneling excavation on the
   hydrogeological environment in a regional area around the tunnel and a
   local hot springs area, at the ``Tseng-Wen Reservoir Transbasin
   Diversion Project, in Taiwan. A hydrogeological conceptual model was
   first developed to simplify structures related to the site topography,
   geology and geological structure. The MODFLOW code was then applied to
   simulate groundwater flow pattern for the hydrogeological conceptual
   model in the tunnel area. The automated parameter estimation method was
   applied to calibrate groundwater level fluctuation and hydrogeological
   parameters in the region. Calibration of the model demonstrated that
   errors between simulated and monitored results are smaller than
   allowable errors. The study also observed that tunneling excavation
   caused groundwater to flow toward the tunnel. No obvious changes in the
   groundwater flow field due to tunnel construction were observed far away
   in the surrounding regions. Furthermore, the FEMWATER code for solving
   3-D groundwater flow problems, in which hydrogeological characteristics
   are integrated into a geographic information system (GIS), is applied to
   evaluate the impact of tunnel construction on an adjacent hot spring.
   Simulation results indicated that the groundwater drawdown rate is less
   than the groundwater recharge rate, and the change to the groundwater
   table after tunnel construction was insignificant for the hot spring
   area. Finally, the groundwater flow obtained via the GMS indicated that
   the hydrogeological conceptual model can estimate the possible quantity
   of tunnel inflow and the impact of tunnel construction on the regional
   and local groundwater resources regime of the transbasin diversion
   project. (C) 2008 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.enggeo.2008.07.012},
ISSN = {0013-7952},
EISSN = {1872-6917},
ResearcherID-Numbers = {Yeh, Hsin-Fu/A-5863-2016},
ORCID-Numbers = {Yeh, Hsin-Fu/0000-0001-6404-6356},
Unique-ID = {WOS:000261873500005},
}

@inproceedings{ WOS:000287783200023,
Author = {Black, W. Z.},
Book-Group-Author = {ASHRAE},
Title = {Pressurization of Floors to Improve Life Safety During a High-Rise Fire},
Booktitle = {ASHRAE TRANSACTIONS 2009, VOL 115, PT 2},
Series = {ASHRAE Transactions},
Year = {2009},
Volume = {115},
Number = {2},
Pages = {278-289},
Note = {ASHRAE Annual Conference, Louisville, KY, JUN 20-24, 2009},
Abstract = {Life safety can be dramatically improved during afire by prudent use of
   pressurization equipment to clear smoke from occupied spaces and escape
   routes within the structure. Air handling equipment can be utilized to
   manage and control the route that smoke takes during afire and thereby
   improve the air quality within the building and at the same time provide
   greater time for occupants to escape. The issue of life safety is
   particularly important in ultra high-rise structures that are being
   planned and are currently being built throughout the world.
   A smoke control software package is used to examine the interacting
   factors that affect smoke movement in high-rise structures with the
   overall objective of determining the practicality of using air-handling
   equipment to pressurize occupant spaces in order to keep smoke out and
   provide a safe area to survive the fire. Exterior and interior building
   construction and fire conditions are examined to determine conditions
   which provide a successful smoke management plan in a high-rise
   structure. A selective floor pressurization scheme is proposed that
   reduces the capacity and number fair handling units needed to pressurize
   the high-rise structure.},
ISSN = {0001-2505},
Unique-ID = {WOS:000287783200023},
}

@inproceedings{ WOS:000271113800004,
Author = {Cheung, Eric and Smith, Thomas M.},
Book-Group-Author = {IEEE},
Title = {Experience with Modularity in an Advanced Teleconferencing Service
   Deployment},
Booktitle = {2009 31ST INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, COMPANION
   VOLUME},
Series = {International Conference on Software Engineering},
Year = {2009},
Pages = {39-49},
Note = {31st International Conference on Software Engineering (ICSE 2009),
   Vancouver, CANADA, MAY 16-27, 2009},
Organization = {ACM; IEEE; IEEE Comp Soc; SIGSOFT},
Abstract = {In this paper, we describe our experience with the design of an advanced
   teleconferencing service under two different frameworks - an early
   implementation of the Distributed Feature Composition architecture, and
   the SIP Servlet API. The usual design goals of software modularity for
   encapsulation and reuse are pursued. Interestingly, two vend different
   designs resulted. This paper discusses the factors that influenced our
   design decisions. In particular, we examine the different
   characteristics of the two frameworks as well as the maturity of project
   requirements, and illustrate the ways in which these factors affect
   various mechanisms for achieving software modularity. We also aim to
   draw on this experience to propose a set of design guidelines for
   building modular, composable SIP Servlet applications for Voice over IP
   and converged services.},
DOI = {10.1109/ICSE-COMPANION.2009.5070962},
ISSN = {0270-5257},
Unique-ID = {WOS:000271113800004},
}

@inproceedings{ WOS:000266039900006,
Author = {Dechev, Damian and Pirkelbauer, Peter and Rouquette, Nicolas and
   Stroustrup, Bjarne},
Book-Group-Author = {IEEE COMPUTER SOC},
Title = {Semantically Enhanced Containers for Concurrent Real-Time Systems},
Booktitle = {16TH ANNUAL IEEE INTERNATIONAL CONFERENCE AND WORKSHOP ON THE
   ENGINEERING OF COMPUTER BASED SYSTEMS, PROCEEDINGS},
Year = {2009},
Pages = {48+},
Note = {16th IEEE International Conference and Workshop on the Engineering of
   Computer-Based Systems, San Francisco, CA, APR 14-16, 2009},
Organization = {IEEE, TCECBS},
Abstract = {Future space missions, such as Mars Science Laboralory, are built upon
   computing platforms providing a high degree of autonomy, and diverse
   functionality. The increased sophistication of robotic spacecraft has
   skyrockeled the complexity and cost of its software development and
   validation. The engineering of autonomous spacecraft software relies oil
   the availability and application of advanced methods and fools that
   deliver safe concurrent synchronization as well as enable the validation
   of domain-specific semantic invariants. The software design and
   certification methodologies applied at NASA do not reach the level of
   detail of providing guidelines for the development of reliable
   concurrent software. To achieve effective and safe concurrent
   interactions as well as guarantee critical domain-specific properties
   ill code, we introduce the notion of a Semantically Enhanced Container
   (SEC). A SEC is a data structure engineered to deliver the flexibility
   and usability of the popular ISO C++ Standard Template Library
   containers, while at the same time it is hand-crafted to guarantee
   domain-specific policies. We demonstrate the SEC proof-of-concept by
   presenting a shared nonblocking SEC vector To eliminate the hazards of
   the ABA problem (a fundamental problem ill lock-free programming), it e
   introduce all innovative library for querying C++ semantic information.
   Our SEC design aims at providing all effective model for shared data
   access within the JPL's Mission Data System. Our test results show that
   the SEC vector delivers significant performance gains (a factor of 3 or
   more) ill contrast to the application of nonblocking synchronization
   amended with the traditional ABA a voidance scheme.},
DOI = {10.1109/ECBS.2009.12},
ISBN = {978-0-7695-3602-6},
ResearcherID-Numbers = {Pirkelbauer, Peter/AAQ-2582-2020
   },
ORCID-Numbers = {Pirkelbauer, Peter/0000-0003-4185-5008
   Dechev, Damian/0000-0002-0569-3403},
Unique-ID = {WOS:000266039900006},
}

@article{ WOS:000265103800015,
Author = {Feng Guofu and Dong Xiaoshe and Wang Xuhao and Chu Ying and Zhang
   Xingjun},
Title = {An Efficient Software-Managed Cache Based on Cell Broadband Engine
   Architecture},
Journal = {INTERNATIONAL JOURNAL OF DISTRIBUTED SENSOR NETWORKS},
Year = {2009},
Volume = {5},
Number = {1},
Pages = {16},
Abstract = {While the CBEA (Cell Broadband Engine Architecture) offers substantial
   computational power, its explicit multilevel memory hierarchy poses
   significant challenges to traditional programming, especially in
   performance and programmability. Software-managed cache is a technique
   that attempts to address such issues. But there still remain some
   limitations in current software-managed cache technologies. First, a
   complex managing logic of the full implementation of the
   software-managed cache which is not suitable for SPU designed as a
   vector computational unit influences computation significantly. Second,
   the external managing code of the software-managed cache causes the
   amount of computing code explosion. Besides increasing the computing
   burden of the accelerator, the external code occupies precious room of
   local storage which is shared by computing code and data. Finally, its
   inconvenient user interface presents significant obstacles for it to be
   widely applied. Based on the locality of memory access, this paper
   proposes an efficient software-managed cache named ECellS cache. In the
   paper, several customized library interfaces were designed respectively
   to simplify the cache implementation and reduce cache code size. On the
   other hand, the code segment of the application which accesses data
   residing in main memory frequently is defined as a cache section.
   Several of these types of cache sections could be combined into a cache
   bind and be managed in parallel by using SIMD technology. By this type
   of coarse-grain managing method and additional simplifying the
   construction of software cache, ECellS cache reduces both the burden of
   SPU to manage the cache buffer and the size of the cache managing code.
   Finally, based on local address remapping technology, a more convenient
   programming interface which is similar to OpenMP directive is presented
   to facilitate programming. By this technology, requesting for data
   residing in the main memory will be automatically mapped to cache buffer
   in local storage of SPE, and there needs to be no extra modifying for
   the computing source code inside the cache section except for inserting
   the directive outside of it. Experimental results of this paper based on
   the Cell processor demonstrate that our proposed software-managed cache
   improves performance by 15-35\% over the CBE SDK software-managed cache
   in most test cases. With ECellS cache it could be more convenient for
   the user to develop and port applications based on CBE architecture.},
DOI = {10.1080/15501320802506034},
Article-Number = {PII 908295318},
ISSN = {1550-1477},
ResearcherID-Numbers = {Zhang, James/D-8515-2013},
Unique-ID = {WOS:000265103800015},
}

@inproceedings{ WOS:000275741600106,
Author = {Gao, Wenxue and Kugel, Andreas and Wurz, Andreas and Marcus, Guillermo
   and Manner, Reinhard},
Book-Group-Author = {IEEE},
Title = {Active Buffer for DAQ in CBM Experiment},
Booktitle = {2009 16TH IEEE-NPSS REAL TIME CONFERENCE},
Year = {2009},
Pages = {527-531},
Note = {16th IEEE/NPSS Real-Time Conference, Beijing, PEOPLES R CHINA, MAY
   10-15, 2009},
Organization = {IEEE; NPSS},
Abstract = {The detectors of the CBM experiment, tinder construction at GSI,
   Germany, will generate all enormous data flow of ITB/s. which is
   provided via approximately 6000 optical links to the DAQ system. For
   proper analysis, so called events have to be composed of data packets
   originating front different geometrical regions and with temporal
   coincidence. at a resolution of 1 nS. Rowing and aggregating of packets
   is performed in a complex custom interconnect structure, where an active
   buffer is the interface to the commodity DAQ farm. We present the
   prototype of an active buffer board (ABB) in PCIe formal. On each ABB.
   data front 2 optical links (each 2.5 or 5 Chit/s) are received by an
   FPGA and, the packets being sorted according to spatial and temporal
   information and stored into a local buffer. To enable simultaneous
   readout towards the host, an efficient dual-port memory emulation was
   implemented, which is non trivial due to the small packet sizes and the
   high bandwidth requirements. Our advanced memory arbitration mechanism
   matches full incoming, bandwidth and a concurrent DMA towards PCIe of
   600MB/s for a 4-lane PCIe interface. using an ordinary DDR2 memory
   module. In parallel to the data path the ABB implements a control flow
   interface, where requests can be sent from the host towards the
   detectors and in turn response messages are being received. The
   corresponding I/O mechanisms are implemented via DMA or PIO. At the
   software level, (tic DMA channels are controlled by a customized Linux
   device driver. Initial integration of the prototype ABB into the GSI
   soft-ware framework has been performed.},
DOI = {10.1109/RTC.2009.5321680},
ISBN = {978-1-4244-5796-0},
ORCID-Numbers = {Marcus, Guillermo/0000-0003-4270-4359},
Unique-ID = {WOS:000275741600106},
}

@inproceedings{ WOS:000291662900035,
Author = {Guo, Wei and Wang, Ying},
Editor = {Tian, XZ and Zhu, E},
Title = {An Incident Management Model for SaaS Application in the IT Organization},
Booktitle = {2009 INTERNATIONAL CONFERENCE ON RESEARCH CHALLENGES IN COMPUTER
   SCIENCE, ICRCCS 2009},
Year = {2009},
Pages = {137-140},
Note = {International Conference on Research Challenges in Computer Science,
   Shanghai, PEOPLES R CHINA, DEC 28-29, 2009},
Organization = {Int Comp Sci Soc; Intelligent Informat Technol Appl Res Assoc; Wuhan
   Univ Sci \& Technol; S China Normal Univ},
Abstract = {Software as a Service (SaaS) is a new style mode of software deployment
   whereby a provider licenses an application to customers for use as a
   service on demand, and it challenges the traditional software mode and
   makes major changes for IT organizations to deploy application.
   Incidents are recognized as service disruptions which can have a
   considerable impact on business capability of IT organizations, thus
   calling for the implementation of efficient incident management and
   service restoration processes. Information Technology Infrastructure
   Library (ITIL) as the best- practice of IT Service Management proposes
   incident management model. However the original model doesn't adapt for
   the organizations which change their IT department structures and
   operation processes owing to adopting SaaS application. This paper
   firstly describes the background of SaaS, analyzes the issues of
   existing incident management model based on ITIL, then propose an
   organization model to manage SaaS operation and maintenance and a
   notation approach for modeling process. Finally, this paper optimizes
   the incident process based on above approach.},
DOI = {10.1109/ICRCCS.2009.42},
ISBN = {978-0-7695-3927-0},
Unique-ID = {WOS:000291662900035},
}

@inproceedings{ WOS:000282068100013,
Author = {Khazaei, M. and Berangi, R.},
Book-Group-Author = {IEEE},
Title = {A Multi-Path Routing Protocol with Fault Tolerance in Mobile Ad hoc
   Networks},
Booktitle = {2009 14TH INTERNATIONAL COMPUTER CONFERENCE},
Year = {2009},
Pages = {77-82},
Note = {14th International Computer Conference, Tehran, IRAN, OCT 20-21, 2009},
Abstract = {In recent years many researches have focused on ad-hoc networks, mainly
   because of their independence to any specific structure. These networks
   suffers from frequent and rapid topology changes that cause many
   challenges in their routing. Most of the routing protocols try to find a
   path between source and destination nodes because any path will expire,
   offer a short period, the path reconstruction may cause the network
   inefficiency. The proposed protocol build two paths between source and
   destination and create backup paths during the route reply process,
   route maintenance process and local recovery process in order to improve
   the data transfer and the fault tolerance. The protocol performance is
   demonstrated by using the simulation results obtain from the global
   mobile simulation software(Glomosim). The experimental results show that
   this protocol can decrease the packet loss ratio rather than DSR and SMR
   and it is useful for the applications that need a high level of
   reliability.},
DOI = {10.1109/CSICC.2009.5349359},
ISBN = {978-1-4244-4261-4},
ResearcherID-Numbers = {Berangi, Reza/T-1065-2018},
Unique-ID = {WOS:000282068100013},
}

@inproceedings{ WOS:000271966900106,
Author = {Limsoonthrakul, Somphop and Dailey, Matthew N. and Srisupundit, Methee
   and Tongphu, Suwan and Parnichkun, Manukid},
Book-Group-Author = {IEEE},
Title = {A Modular System Architecture for Autonomous Robots Based on Blackboard
   and Publish-Subscribe Mechanisms},
Booktitle = {2008 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-4},
Year = {2009},
Pages = {633-638},
Note = {IEEE International Conference on Robotics and Biomimetics (ROBIO),
   Bangkok, THAILAND, FEB 22-25, 2009},
Organization = {IEEE Robot \& Automat Soc},
Abstract = {We present a system software architecture for mobile robots such as
   autonomous vehicles. The system achieves the goals of flexibility,
   maintainability, testability, and modifiability through a decoupled
   software architecture based on an asynchronous publish-subscribe
   mechanism and a blackboard object handling synchronized access to shared
   data. We report on two implementations using the proposed generic
   architecture and the POSIX real time API. The first implementation is
   for an autonomous vehicle using waypoint-based navigation, and the
   second implementation uses the same high-level modules but replaces the
   low-level hardware interfaces with a virtual reality simulation. Our
   experiments and an evaluation indicate that the architecture is suitable
   for a wide variety of control algorithms and supports the construction
   of testable, maintainable, and modifiable autonomous robot vehicles at
   low cost in terms of real-time performance.},
DOI = {10.1109/ROBIO.2009.4913075},
ISBN = {978-1-4244-2678-2},
Unique-ID = {WOS:000271966900106},
}

@inproceedings{ WOS:000280514700003,
Author = {Nystroem, Jan Henry},
Book-Group-Author = {acm},
Title = {Automatic Assessment of Failure Recovery in Erlang Applications},
Booktitle = {ERLANG'09: PROCEEDINGS OF THE 2009 ACM SIGPLAN ERLANG WORKSHOP},
Year = {2009},
Pages = {23-32},
Note = {8th ACM SIGPLAN Erlang Workshop, Edinburgh, SCOTLAND, SEP 05, 2009},
Organization = {ACM SIGPLAN},
Abstract = {Erlang is a concurrent functional language, especially tailored for
   distributed, highly concurrent and fault-tolerant software. An important
   part of Erlang is its support for failure recovery. A designer
   implements failure recovery by organising the processes of an Erlang
   application into tree structures, in which parent processes monitor
   failures of their children and are responsible for their restart.
   Libraries support the creation of such structures during system
   initialisation.
   We present a technique to automatically analyse that the process
   structure of an Erlang application is constructed in a way that
   guarantees recovery from process failures. First, we extract (part of)
   the process structure by static analysis of the initialisation code of
   the application. Thereafter, analysis of the process structure checks
   that it will recover from any process failure. We have implemented the
   technique in a tool, and applied it to several OTP library applications
   and to a subsystem of the AXD 301 ATM switch.},
ISBN = {978-1-60558-507-9},
Unique-ID = {WOS:000280514700003},
}

@inproceedings{ WOS:000265085500021,
Author = {Rahman, Mostafijur and Ishwar, Zahereel and Khalib, Abdul and Ahmad, R.
   B.},
Editor = {Zhou, JH and Zhou, XX},
Title = {Performance Evaluation of PNtMS: A Portable Network Traffic Monitoring
   System on Embedded Linux Platform},
Booktitle = {2009 INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND TECHNOLOGY,
   VOL I, PROCEEDINGS},
Year = {2009},
Pages = {108-113},
Note = {International Conference on Computer Engineering and Technology,
   Singapore, SINGAPORE, JAN 22-24, 2009},
Organization = {Int Assoc Comp Sci \& Informat Technol},
Abstract = {The principal role of embedded software is the transformation of data
   and the interaction with the physical world stimulus. The main concern
   in developing embedded software for network application is the lack of
   published best practice software architecture for optimizing performance
   by means of reducing protocol processing overhead, memory usage, and
   power consumption. This paper presents the implementation, operation and
   performance evaluation of the portable network traffic monitoring system
   (PNtMS) on an embedded Linux platform. The system has been designed to
   capture network packets information from the network and performs some
   statistical analysis, which includes providing data on the volume and
   types of traffic transferred within a LAN, traffic generated per node,
   number of traffic going through or comming from a system or application
   which is causing bottleneck, and the level of peak traffic. These data
   are then stored into the log files, and the traffic information can be
   shown through web browser or onboard LCD. Result shows that PNtMS is
   performing at par with an existing network protocol analyzer with
   minimal usage of RAM ( 578 KB), low end processor (133MHz) and
   storage(less than 1GB).},
DOI = {10.1109/ICCET.2009.37},
ISBN = {978-0-7695-3521-0},
ResearcherID-Numbers = {Rather, R A/J-3509-2019
   Rahman, Mostafijur/C-9032-2011
   Ahmad, Badlishah/A-4703-2019
   Ahmad, R Badlishah/G-5892-2015
   Ahmad, RB/U-3211-2019},
ORCID-Numbers = {Rahman, Mostafijur/0000-0001-6504-6331
   Ahmad, Badlishah/0000-0002-4862-2728
   Ahmad, RB/0000-0002-4862-2728},
Unique-ID = {WOS:000265085500021},
}

@inproceedings{ WOS:000274303300294,
Author = {Tripathi, Vikas and Mahesh, T. Sai Guru and Snivastava, Anurag},
Book-Group-Author = {IEEE},
Title = {Performance and Language Compatibility in Software Pattern Detection},
Booktitle = {2009 IEEE INTERNATIONAL ADVANCE COMPUTING CONFERENCE, VOLS 1-3},
Year = {2009},
Pages = {1638-1642},
Note = {IEEE International Advance Computing Conference, Patiala, INDIA, MAR
   06-07, 2009},
Organization = {IEEE},
Abstract = {Re documentation and design recovery are two important areas of reverse
   engineering. Detection of recurring organizations of classes and
   communicating objects, called Software Patterns, supports this process
   {[}1]. Many approaches to detect Software Patterns which have been
   published in the past years suffer from the problems of necessity of
   reference library, performance and language compatibility. This paper
   presents a model to solve those problems in software pattern detection.
   The proposed model solves the problem of necessity of reference library
   by detecting software patterns using Formal Concept Analysis (FCA). The
   proposed model solves the problem of performance by using the most
   efficient algorithm CMCG (Concept-Matrix based Concepts Generation) for
   the construction of concept lattice, which is the core data structure of
   FCA {[}3]. The proposed model solves the problem of language
   compatibility by using the language independent Meta model called MOOSE
   for taking the input information. The validity of this model was proved
   in theory and by experiment.},
ISBN = {978-1-4244-2927-1},
ResearcherID-Numbers = {TRIPATHI, Vikas/AAN-6361-2020},
ORCID-Numbers = {TRIPATHI, Vikas/0000-0002-2254-3044},
Unique-ID = {WOS:000274303300294},
}

@inproceedings{ WOS:000268998000057,
Author = {Varcholik, Paul and Laviola, Jr., Joseph J. and Nicholson, Denise},
Editor = {Jacko, JA},
Title = {TACTUS: A Hardware and Software Testbed for Research in Multi-Touch
   Interaction},
Booktitle = {HUMAN-COMPUTER INTERACTION, PT II: NOVEL INTERACTION METHODS AND
   TECHNIQUES},
Series = {Lecture Notes in Computer Science},
Year = {2009},
Volume = {5611},
Number = {II},
Pages = {523-532},
Note = {13th International Conference on Human-Computer Interaction, San Diego,
   CA, JUL 19-24, 2009},
Abstract = {This paper presents the TACTUS Multi-Touch Research Testbed, a hardware
   and software system for enabling research in multi-touch interaction. A
   detailed discussion is provided oil hardware construction. pitfalls,
   design Options, and software architecture to bridge the gaps in the
   existing literature and inform the researcher on the practical
   requirements of it multi-touch research testbed. This includes it
   comprehensive description of the vision-based image processing pipeline,
   developed for the TACTUS software library, which makes surface
   interactions available to multi-touch applications. Furthemore, the
   paper explores the higher-level functionality and Utility of the TACTUS
   software library and how researchers call leverage the system to
   investigate multi-touch interaction techniques.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-642-02576-1},
Unique-ID = {WOS:000268998000057},
}

@inproceedings{ WOS:000290045002096,
Author = {Wang, C-H and Wang, X.},
Editor = {Anderssen, RS and Braddock, RD and Newham, LTH},
Title = {Hazard of extreme wind gusts to buildings in Australia and its
   sensitivity to climate change},
Booktitle = {18TH WORLD IMACS CONGRESS AND MODSIM09 INTERNATIONAL CONGRESS ON
   MODELLING AND SIMULATION: INTERFACING MODELLING AND SIMULATION WITH
   MATHEMATICAL AND COMPUTATIONAL SCIENCES},
Year = {2009},
Pages = {2576-2582},
Note = {Combined IMACS World Congress/Modelling and Simulation
   Society-of-Australia-and-New-Zealand (MSSANZ)/18th Biennial Conference
   on Modelling and Simulation, Cairns, AUSTRALIA, JUL 13-17, 2009},
Organization = {IMACS; MSSANZ; CSIRO; Australian Math Sci Inst; Griffith Univ; eWater
   Cooperat Res Ctr; Dept Sustainabil \& Environm; HEMA Consulting;
   Hellenic European Res Comp Math \& Applicat; Int Council Ind Appl Math;
   Int Soc Grid Generat; Int Soc Photogrammetry \& Remote Sensing; Japan
   Soc Simulat Technol; Pacific Rim Math Assoc; Rutgers, State Univ New
   Jersey},
Abstract = {This paper presents a preliminary study on hazard modeling, estimation,
   and mapping of extreme wind gusts for consideration of buildings and
   infrastructure in Australia. Buildings and infrastructure provide
   essential support for the quality of life and are the founding blocks
   for social and economic development. Typically the design service life
   of buildings is around 50 years and that of infrastructure is 100 years
   or beyond; therefore, most existing building stock and infrastructure
   will be impacted by, and design/construction of new ones needs to
   consider the potential effect of, climate change.
   Achievement of a performance level is typically via the consideration of
   return period (or average recurrence interval) of the hazard under
   consideration. Depending on its in-service importance and functionality,
   a structure such as a low-rise residential construction may need to be
   designed to withstand a 500-year, and a school building a 1000-year,
   return period wind speed to ensure an acceptable structural safety
   level.
   To quantify the hazard of extreme gust, daily maximum gust wind speeds
   recorded up to 2007 at 545 anemometer stations maintained by the Bureau
   of Meteorology (BOM) were collected. Statistical and probabilistic
   approaches are used to model the gust wind speeds. Cyclonic gust speeds
   are modeled by the generalized Pareto distribution. Non-cyclonic wind
   speeds at sites within 150 km from the coast and affected by cyclones
   are modeled by the shifted exponential distribution; otherwise, they are
   modeled by the Weibull distribution of the largest value.
   The current provision for design of structures under wind action, AS/NZS
   1170.2:2002, was developed based on historical climate data during which
   no sufficient evidence indicated any trend in wind speeds due to climate
   change, as stated in its commentary: ``the Standard does not attempt to
   predict the effects of possible future climatic changes, as the evidence
   for changes in wind speeds is inconclusive.{''} While the efforts over
   the last decade for establishing trend changes in severe cyclonic wind
   intensity and frequency have been inconclusive, preliminary research
   results suggest that significant alteration in severe cyclonic wind
   intensity and frequency are possible within the lifetime of existing
   buildings and infrastructure. As such, sensitivity study conducted at
   this stage is conducive to gain insight about future gust speed range
   due to possible frequency and intensity changes of severe cyclonic wind
   events.
   Instead of attempting to give projection for wind gusts under a
   specified climate scenario as that given in the IPCC greenhouse gas
   emissions scenarios, this study examines the gust wind hazards under a
   range of frequency changes: from -50\% to + 100\%, and under a range of
   intensity changes: from -20\% to + 20\%. Wind gust hazard maps of
   Australia under current and likely future climate conditions subjected
   to frequency and intensity changes are produced by the software package
   ArcGIS 9.2 using kriging and exponential semivariogram for spatial
   interpolation.
   The hazard analysis of this preliminary work is based on the Australian
   BOM anemometer data only. For improving the accuracy in hazard
   estimation, the next step of this study will consider: (a)
   disaggregation of non-cyclonic gusts generated by different mechanisms;
   e. g. thunderstorms, tornadoes, and synoptic winds; (b) reassessment of
   mutual independence of the cyclonic gust data included in a superstation
   according to the homogeneity of physical geography and meteorology; and
   (c) merger of wind speeds on the best tracks estimated by the Dvorak
   technique after 1984 at landfall of tropical cyclones to nearby
   anemometer data.},
ISBN = {978-0-9758400-7-8},
ResearcherID-Numbers = {Wang, Chi-Hsiang/A-1961-2008
   Wang, Xiaoming/A-3804-2008
   Wang, Xiaoming/A-3804-2008},
ORCID-Numbers = {Wang, Chi-Hsiang/0000-0001-5486-7046
   Wang, Xiaoming/0000-0002-6648-0057
   Wang, Xiaoming/0000-0002-1088-8862},
Unique-ID = {WOS:000290045002096},
}

@article{ WOS:000262567300001,
Author = {Wang, Hongjun and Akinci, Burcu and Garrett, Jr., James H. and Nyberg,
   Eric and Reed, Kent A.},
Title = {Semi-automated model matching using version difference},
Journal = {ADVANCED ENGINEERING INFORMATICS},
Year = {2009},
Volume = {23},
Number = {1},
Pages = {1-11},
Month = {JAN},
Abstract = {Interoperability of software is a critical requirement in the
   architecture, engineering, and construction (AEC) industry, where a
   number of data exchange standards have been created to enable data
   exchange among different software packages. To be able to comply with
   existing data exchange standards, the software developers need to match
   their internal data schemas to the schema defined in a standard and vice
   versa. The process of matching two large scale data models is time
   consuming and cumbersome when performed manually, and becomes even more
   challenging when a Source and/or a target model is being updated
   frequently to meet the ever expanding real world requirements. While
   several prior studies discussed the need for approaches toward automated
   or semi-automated schema matching, an approach that builds on existing
   matches between two models has rarely been studied. In this paper, we
   present a semi-automated approach for model matching. This approach
   leverages a given set of existing matching between two models and
   upgrades those matching when a new version of a target model is
   released. The paper describes in detail a list of upgrade patterns
   generated and validated through a prototype by matching a
   domain-specific data model to several recent releases of the industry
   foundation classes. (C) 2008 Published by Elsevier Ltd.},
DOI = {10.1016/j.aei.2008.05.005},
ISSN = {1474-0346},
EISSN = {1873-5320},
ORCID-Numbers = {Garrett, James/0000-0002-8408-2407},
Unique-ID = {WOS:000262567300001},
}

@inproceedings{ WOS:000274805500026,
Author = {Wang, Yan and Gaspes, Veronica},
Book-Group-Author = {IEEE},
Title = {A Domain Specific Approach to Network Software Architecture Assuring
   Conformance Between Architecture and Code},
Booktitle = {ICDT: 2009 FOURTH INTERNATIONAL CONFERENCE ON DIGITAL TELECOMMUNICATIONS},
Year = {2009},
Pages = {127-132},
Note = {4th International Conference on Digital Telecommunications (ICDT 2009,
   Colmar, FRANCE, JUL 20-25, 2009},
Abstract = {Network software is typically organized according to a layered
   architecture that is well understood. However, writing correct and
   efficient code that conforms with the architecture still remains a
   problem. To overcome this problem we propose to use a domain specific
   language based approach. The architectural constraints are captured in a
   domain specific notation that can be used as a source for automatic
   program generation. Conformance with the architecture is thus assured by
   construction. Knowledge from the domain allows us to generate efficient
   code. In addition, this approach enforces reuse of both code and
   designs, one of the major concerns in software architecture. In this
   paper, we illustrate our approach with PADDLE, a tool that generates
   packet processing code from packet descriptions. To describe packets we
   use a domain specific language of dependent types that includes packet
   overlays. From the description we generate C libraries for packet
   processing that are easy to integrate with other parts of the code. We
   include an evaluation of our tool.},
ISBN = {978-1-4244-4505-9},
Unique-ID = {WOS:000274805500026},
}

@inproceedings{ WOS:000271565900017,
Author = {Zhou, Qinian and Li, Wei and Yang, Jihui},
Editor = {Wang, FL and Li, F and Miao, LF and Zhao, JM},
Title = {A Study of Construction and Share Digital Resources in an Higher
   Education District in China},
Booktitle = {SHORT PAPER PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON WEB-BASED
   LEARNING},
Year = {2009},
Pages = {73-75},
Note = {7th International Conference on Web Based Learning (ICWL 2008), Zhejiang
   Normal Univ, Jinhua, PEOPLES R CHINA, AUG 20-22, 2008},
Organization = {Hong Kong Web Soc},
Abstract = {This article includes the digital library experience in Xiasha Education
   District of Hangzhou city in China. The first part is the necessity to
   construct the digital library center in the changing trends of the
   higher education in China. It appears that the traditional library could
   not follow the trends of the higher education in China. The second part
   is the technology experience for building up the digital library. It
   introduces the theory structure and construction steps of digital
   library and resource. In this section, we introduce all levels of the
   structure theory. Meanwhile, we show the steps of constructing the
   digital library including organization of the committee, connection of
   Internet and the hardware of computers, standard and development of
   software and preservation and building up of digital resource.},
DOI = {10.1109/ICWL.2008.18},
ISBN = {978-0-7695-3518-0},
Unique-ID = {WOS:000271565900017},
}

@article{ WOS:000262999000003,
Author = {Nix, David A. and Courdy, Samir J. and Boucher, Kenneth M.},
Title = {Empirical methods for controlling false positives and estimating
   confidence in ChIP-Seq peaks},
Journal = {BMC BIOINFORMATICS},
Year = {2008},
Volume = {9},
Month = {DEC 5},
Abstract = {Background: High throughput signature sequencing holds many promises,
   one of which is the ready identification of in vivo transcription factor
   binding sites, histone modifications, changes in chromatin structure and
   patterns of DNA methylation across entire genomes. In these experiments,
   chromatin immunoprecipitation is used to enrich for particular DNA
   sequences of interest and signature sequencing is used to map the
   regions to the genome (ChIP-Seq). Elucidation of these sites of
   DNA-protein binding/modification are proving instrumental in
   reconstructing networks of gene regulation and chromatin remodelling
   that direct development, response to cellular perturbation, and
   neoplastic transformation.
   Results: Here we present a package of algorithms and software that makes
   use of control input data to reduce false positives and estimate
   confidence in ChIP-Seq peaks. Several different methods were compared
   using two simulated spike-in datasets. Use of control input data and a
   normalized difference score were found to more than double the recovery
   of ChIP-Seq peaks at a 5\% false discovery rate (FDR). Moreover, both a
   binomial p-value/q-value and an empirical FDR were found to predict the
   true FDR within 2 -3 fold and are more reliable estimators of confidence
   than a global Poisson p-value. These methods were then used to reanalyze
   Johnson et al.' s neuron-restrictive silencer factor (NRSF) ChIP-Seq
   data without relying on extensive qPCR validated NRSF sites and the
   presence of NRSF binding motifs for setting thresholds.
   Conclusion: The methods developed and tested here show considerable
   promise for reducing false positives and estimating confidence in
   ChIP-Seq data without any prior knowledge of the chIP target. They are
   part of a larger open source package freely available from
   http://useq.sourceforge.net/.},
DOI = {10.1186/1471-2105-9-523},
Article-Number = {523},
ISSN = {1471-2105},
ORCID-Numbers = {Boucher, Kenneth/0000-0003-2833-0127},
Unique-ID = {WOS:000262999000003},
}

@article{ WOS:000261119800015,
Author = {Brukman, Olga and Dolev, Shlomi and Kolodner, Elliot K.},
Title = {A self-stabilizing autonomic recoverer for eventual Byzantine software},
Journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
Year = {2008},
Volume = {81},
Number = {12, SI},
Pages = {2315-2327},
Month = {DEC},
Note = {Australian Software Engineering Conference, Melbourne, AUSTRALIA, APR
   10-13, 2007},
Organization = {Data Proc; Infosys Australia; Davies Collison Cave; Object Consulting;
   Sun Microsyst Lab; Swinburne Univ Technol; IT Today; Elsevier Australia},
Abstract = {We suggest modeling software package flaws (bugs) by assuming eventual
   Byzantine behavior of the package. We assume that if a program is
   started in a predefined initial state, it will exhibit legal behavior
   for a period of time but will eventually become Byzantine. We assume
   that this behavior pattern can be attributed to the fact that the
   manufacturer had performed sufficient package tests for limited time
   scenarios. Restarts are useful for recovering such systems. We suggest a
   general, yet practical, framework and paradigm for the monitoring and
   restarting of systems where the framework and paradigm are based on a
   theoretical foundation. An autonomic recoverer that monitors and
   initiates system recovery is proposed. It is designed to handle a task,
   given specific task requirements in the form of predicates and actions.
   A directed acyclic graph subsystem hierarchical structure is used by a
   consistency monitoring procedure for achieving a gracious recovery. The
   existence and correct functionality of the autonomic recovery is
   guaranteed by the use of a self-stabilizing kernel resident (anchor)
   process. The autonomic recoverer uses a new scheme for liveness
   assurance via on-line monitoring that complements known schemes for
   on-line safety assurance. (C) 2008 Elsevier Inc. All rights reserved.},
DOI = {10.1016/j.jss.2008.04.028},
ISSN = {0164-1212},
EISSN = {1873-1228},
ORCID-Numbers = {Dolev, Shlomi/0000-0001-5418-6670},
Unique-ID = {WOS:000261119800015},
}

@article{ WOS:000262151500005,
Author = {Gnoth, Steffen and Hansel, Frank and Haeupl, Peter and Fechner, Heiko},
Title = {Aero-hygro-thermal behaviour of building enslosure components with
   opened and enclosed air cavities},
Journal = {BAUPHYSIK},
Year = {2008},
Volume = {30},
Number = {6},
Pages = {380-388},
Month = {DEC},
Abstract = {Air cavities with different functions can be found in historical as well
   as in modem structural design. In the past, air cavities mainly have
   been made for practical reasons to improve the thermal insulation
   properties of wall and roof constructions. In modem architecture, hollow
   spaces frequently arise due to constructional or due to aesthetic
   reasons such as in glass curtain walls or in cladding of inside walls.
   On environmental grounds the obligations towards the energy efficient
   construction and operating of buildings are tightened increasingly. This
   leads, under compliance with today's room climatic requirements, to the
   focusing of activities on appropriate use of building materials and the
   expenses and energy optimized design of building envelopes. On the
   rehabilitation of historical structures often questions regarding the
   hygro-thermal evaluation of such air cavities and the therein occurring
   convection processes arise. So, a long term cross flow in open cavities
   with moisture-loaded air can cause damages on the service ability and
   bearing capacity of components.
   The consideration of cross flows in enclosed cavities of single
   components or in open constructions, such as wall or roof constructions,
   can be calculated at present only by use of special assumptions. For the
   solution of this complex problem the coupling of a CFD-solver with a
   hygro-thermal building simulation software has been carried out. Within
   the scope of a research project a CaFD (cavity fluid dynamics)
   simulation code has been developed which especially is adjusted to the
   requirements and boundary conditions of hygro-thermal component
   simulations. Therefore, the functional range concentrates on building
   physical problems definitions of structural designs. With the
   implementation of the CaFD-tool as an additional application for the
   software package DELPHIN 4.x, an independent operational CFD software
   module has been developed. By means of the SIMPLE-Algorithm it solves
   the Navier-Stokes-Equations additionally under consideration of the heat
   and mass transport in the cavity A data interface controls the
   interactive data interchange between DELPHIN and the CaFD-tool.
   The presentation of simulations of the aero-hygro-thermal behaviour of
   components with opened and enclosed air cavities completes the
   contribution. With selected test cases the coupled simulation is
   analysed and discussed more closely from the building physical point of
   view.},
DOI = {10.1002/bapi.200810049},
ISSN = {0171-5445},
Unique-ID = {WOS:000262151500005},
}

@article{ WOS:000267664200007,
Author = {Laakso, Antti and Dumitrescu, Mihail and Pietilae, Pasi and Suominen,
   Mikko and Pessa, Markus},
Title = {Optimization studies of single-transverse-mode 980 nm ridge-waveguide
   lasers},
Journal = {OPTICAL AND QUANTUM ELECTRONICS},
Year = {2008},
Volume = {40},
Number = {11-12},
Pages = {853-861},
Month = {SEP},
Note = {17th International Workshop on Optical Waveguide Theory and Numerical
   Modelling, Eindhoven, NETHERLANDS, JUN 13-14, 2008},
Abstract = {The paper presents a simple and efficient Mode-Solver-based method for
   determining the ridge profile that ensures stable single-transverse-mode
   operation in ridge waveguide lasers. A quantitative figure of merit,
   easily derived from the Mode Solver analysis of the structure, indicates
   the likelihood of single transverse modal behavior over the whole bias
   range. The transverse mode analysis performed with the in-house
   developed software has been compared with the results of the commercial
   simulation package LASTIP and with experiments. The simulation and
   experimental results have been used to derive processing guidelines for
   achieving a stable single-transverse-mode operation in ridge waveguide
   lasers.},
DOI = {10.1007/s11082-009-9280-7},
ISSN = {0306-8919},
Unique-ID = {WOS:000267664200007},
}

@article{ WOS:000256967900020,
Author = {Reviriego, P. and Maestro, J. A. and Ruano, O.},
Title = {Efficient protection techniques against SEUs for adaptive filters: An
   echo canceller case study},
Journal = {IEEE TRANSACTIONS ON NUCLEAR SCIENCE},
Year = {2008},
Volume = {55},
Number = {3, 3},
Pages = {1700-1707},
Month = {JUN},
Note = {15th International Workshop on Room-Temperature Semiconductor X- and
   Gamma-Ray Detectors/ 2006 IEEE Nuclear Science Symposium, San Diego, CA,
   OCT 29-NOV 04, 2006},
Organization = {IEEE},
Abstract = {In this paper, novel protection techniques against soft errors for
   adaptive filters are presented. The new techniques are based on the use
   of system knowledge in terms of both filter structure and functionality,
   as well as the application tolerance to soft errors. Adaptive filters by
   nature recover from soft errors on their coefficients, but in existing
   implementations the recovery time can exceed what is acceptable for many
   applications. The proposed techniques dramatically reduce the recovery
   time after a soft error with an acceptable increment on circuit
   complexity, as they rely on reusing existing logic. To illustrate these
   techniques, a case study is presented in which their effectiveness is
   evaluated using a software-based fault injection platform. Also, their
   complexity is estimated in terms of the number of equivalent gates
   generated for the synthesized circuit implementation using a commercial
   ASIC library.},
DOI = {10.1109/TNS.2008.924053},
ISSN = {0018-9499},
EISSN = {1558-1578},
ResearcherID-Numbers = {Reviriego, Pedro/ABE-4167-2020
   Ruano, Oscar/AAH-7625-2021
   Reviriego, Pedro/B-8353-2009
   Maestro, Juan Antonio/L-6091-2014
   },
ORCID-Numbers = {Reviriego, Pedro/0000-0003-2540-5234
   Reviriego, Pedro/0000-0001-6805-6519
   Maestro, Juan Antonio/0000-0001-7133-9026
   Ruano, Oscar/0000-0001-8275-1745},
Unique-ID = {WOS:000256967900020},
}

@article{ WOS:000256824900005,
Author = {Saha, Proshanta and El-Araby, Esam and Huang, Miaoqing and Taher,
   Mohamed and Lopez-Buedo, Sergio and El-Ghazawi, Tarek and Shu, Chang and
   Gaj, Kris and Michalski, Alan and Buell, Duncan},
Title = {Portable library development for reconfigurable computing systems: A
   case study},
Journal = {PARALLEL COMPUTING},
Year = {2008},
Volume = {34},
Number = {4-5},
Pages = {245-260},
Month = {MAY},
Note = {3rd Reconfigurable Systems Summer Institute, Urbana, IL, JUL 17-20, 2007},
Organization = {Natl Ctr Supercomp Applicat},
Abstract = {Portable libraries of highly-optimized hardware cores can significantly
   reduce the development time of reconfigurable computing applications.
   This paper presents the tradeoffs and challenges in the design of such
   libraries. A set of library development guidelines is provided, which
   has been validated with the RCLib case study. RCLib is a set of portable
   libraries with over 100 cores, targeting a wide range of applications.
   RCLib portability has been verified in three major High-Performance
   reconfigurable computing architectures: SRC6, Cray XD1 and SGI RC100.
   Compared to full-software implementations, applications using RCLib
   hardware acceleration cores show speedups ranging from one to four
   orders of magnitude. (C) 2008 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.parco.2008.03.005},
ISSN = {0167-8191},
EISSN = {1872-7336},
ResearcherID-Numbers = {Lopez-Buedo, Sergio/L-3250-2013
   },
ORCID-Numbers = {Lopez-Buedo, Sergio/0000-0002-0815-7921
   Taher, Mohamed/0000-0002-4808-4018
   El-Araby, Esam/0000-0002-4575-1049
   Buell, Duncan/0000-0002-4668-5848},
Unique-ID = {WOS:000256824900005},
}

@article{ WOS:000256778800082,
Author = {Lagin, L. J. and Bettenhausen, R. C. and Bowers, G. A. and Carey, R. W.
   and Edwards, O. D. and Estes, C. M. and Demaret, R. D. and Ferguson, S.
   W. and Fisher, J. M. and Ho, J. C. and Ludwigsen, A. P. and Mathisen, D.
   G. and Marshall, C. D. and Matone, J. T. and McGuigan, D. L. and
   Sanchez, R. J. and Stout, E. A. and Tekle, E. A. and Townsend, S. L. and
   Van Arsdall, P. J. and Wilson, E. F.},
Title = {Status of the National Ignition Facility Integrated Computer Control
   System (ICCS) on the path to ignition},
Journal = {FUSION ENGINEERING AND DESIGN},
Year = {2008},
Volume = {83},
Number = {2-3},
Pages = {530-534},
Month = {APR},
Note = {6th IAEA Technical Meeting on Control, Data Acquisition, and Remote
   Participation for Fusion Research, Inuyama, JAPAN, JUN 04-08, 2007},
Organization = {IAEA; Natl Inst Fus Sci},
Abstract = {The National Ignition Facility (NIF) at the Lawrence Livermore National
   Laboratory is a stadium-sized facility under construction that will
   contain a 192-beam, 1.8-MJ, 500-TW, ultraviolet laser system together
   with a 10-m diameter target chamber with room for multiple experimental
   diagnostics. NIF is the world's largest and most energetic laser
   experimental system, providing a scientific center to study inertial
   confinement fusion (ICF) and matter at extreme energy densities and
   pressures. NIF's laser beams are designed to compress fusion targets to
   conditions required for thermonuclear burn, liberating more energy than
   required to initiate the fusion reactions. NIF is comprised of 24
   independent bundles of eight beams each using laser hardware that is
   modularized into more than 6000 line replaceable units such as optical
   assemblies, laser amplifiers, and multi-function sensor packages
   containing 60,000 control and diagnostic points. NIF is operated by the
   large-scale Integrated Computer Control System (ICCS) in an architecture
   partitioned by bundle and distributed among over 800 front-end
   processors and 50 supervisory servers. NEF's automated control
   subsystems are built from a common object-oriented software framework
   based on CORBA distribution that deploys the software across the
   computer network and achieves interoperation between different languages
   and target architectures. A shot automation framework has been deployed
   during the past year to orchestrate and automate shots performed at the
   NIF using the ICCS. In December 2006, a full cluster of 48 beams of NIF
   was fired simultaneously, demonstrating that the independent bundle
   control system will scale to full scale of 192 beams. At present, 72
   beams have been commissioned and have demonstrated 1.4-MJ capability of
   infrared light. During the next 2 years, the control system will be
   expanded in preparation for project completion in 2009 to include
   automation of target area systems including final optics, target
   positioners and diagnostics. Additional capabilities to support fusion
   ignition shots in a National Ignition Campaign (NIC) beginning in 2010
   will include a cryogenic target system, target diagnostics, and
   integrated experimental shot data analysis with tools for data
   visualization and archiving. This talk discusses the current status of
   the control system implementation and discusses the plan to complete the
   control system on the path to ignition. (c) 2007 Elsevier B. V. All
   rights reserved.},
DOI = {10.1016/j.fusengdes.2007.09.014},
ISSN = {0920-3796},
Unique-ID = {WOS:000256778800082},
}

@article{ WOS:000255149300007,
Author = {Sandewall, Erik},
Title = {Extending the concept of publication: factbases and knowledgebases},
Journal = {LEARNED PUBLISHING},
Year = {2008},
Volume = {21},
Number = {2},
Pages = {123-131},
Month = {APR},
Abstract = {The concept of a `publication' no longer applies only to printed works;
   information technology has extended its application to several other
   types of works. This article describes a facility called the Common
   Knowledge Library that publishes modules of formally structured
   information representing facts and knowledge of various kinds.
   Publications of this new type have some characteristics in common with
   databases, and others in common with software modules; however, they
   also share some important characteristics with traditional publications.
   A framework for citation of previous work is important in order to
   provide an incentive for contributors of such modules. Peer review - the
   traditional method of quality assurance for scientific articles - must
   also be applied, although in a modified form, for fact and knowledge
   modules. The construction of the Common Knowledge Library is a
   cumulative process; new contributions are obtained by interpreting the
   contents Of existing knowledge sources on the Internet, and the existing
   contents of the Library are an important resource for that
   interpretation process.},
DOI = {10.1087/095315108X288893},
ISSN = {0953-1513},
EISSN = {1741-4857},
Unique-ID = {WOS:000255149300007},
}

@article{ WOS:000253310900007,
Author = {Magne, Pascal and Tan, Derek T.},
Title = {Incisor compliance following operative procedures: A rapid 3-D finite
   element analysis using micro-CT data},
Journal = {JOURNAL OF ADHESIVE DENTISTRY},
Year = {2008},
Volume = {10},
Number = {1},
Pages = {49-56},
Month = {FEB},
Abstract = {Purpose: New methods are available for the rapid generation of 3-D
   finite element models of dental structures and restorations. Validation
   of these methods are required. The aim of the present study is to
   utilize stereolithography and surface-driven automatic meshing to
   generate models of specific restorative conditions, and to examine these
   models under loading. The data generated are compared to existing
   experimental data in an attempt to validate the model.
   Materials and Methods: An intact maxillary central incisor was digitized
   with a micro-CT scanner. Surface contours of enamel and dentin were
   fitted following tooth segmentation based on pixel density using an
   interactive medical image control system. Stereolithography (STL) files
   of enamel and dentin surfaces were then remeshed to reduce mesh density
   and imported in a rapid prototyping software, where Boolean operations
   were used to assure the interfacial mesh congruence (dentinoenamel
   junction) and simulate different tooth preparations (endodontic access,
   veneer, proximal, and Class III preparations) and restorations (Class
   III composites). The different parts were then imported in a finite
   element software package to create 3D solid models. A 50-N point load
   perpendicular to the tooth's long axis and centered on the incisal edge
   was applied either on the buccal or palatal surface. The surface strain
   was obtained from selected nodes corresponding to the location of the
   strain gauges in the validation experiments.
   Results: The increase in crown flexure (compared to the unaltered tooth)
   ranged from near zero values (conservative endodontic access, removal of
   proximal enamel) to ca 10\% (aggressive endodontic access, conservative
   Class III preparations), 23\% and 34\% (moderate and aggressive Class
   III preparations, respectively), and 91\% (veneer preparation).
   Placement of Class III composite resin restorations resulted in 85\%
   recovery of the original crown stiffness. 3D FEA data correlated well
   with existing experimental data. In two situations, smaller FEA strains
   were recorded compared to the experimental strains, perhaps due to
   enamel cracking under the strain gauges. This artefact was not simulated
   by the FEA models.
   Conclusion: Experimental data validated the FEA models. The described
   method can generate detailed three-dimensional finite element models of
   a maxillary central incisor with different cavities and restorative
   materials. This method is rapid and can readily be used for other
   medical (and dental) applications.},
ISSN = {1461-5185},
EISSN = {1757-9988},
Unique-ID = {WOS:000253310900007},
}

@inproceedings{ WOS:000262405300027,
Author = {Ardimento, Pasquale and Baldassarre, Maria Teresa and Cimitile, Marta
   and Visaggio, Giuseppe},
Editor = {Filipe, J and Shishkov, B and Helfert, M and Maciaszek, LA},
Title = {Empirical Experimentation for Validating the Usability of Knowledge
   Packages in Transferring Innovations},
Booktitle = {SOFTWARE AND DATA TECHNOLOGIES},
Series = {Communications in Computer and Information Science},
Year = {2008},
Volume = {22},
Pages = {357+},
Note = {2nd International Conference on Software and Data Technologies,
   Barcelona, SPAIN, JUL 22-25, 2007},
Organization = {INSTICC; Workflow Management Coalit; Interdisciplinary Inst Collaborat
   \& Res Enterprise Syst \& Technol},
Abstract = {Transfer of research results following to technological innovation and
   to the experience collected in applying the innovation within an
   enterprise is a key success factor. A critical factor in transferring
   innovations to software processes concerns the knowledge transfer
   activity which requires the knowledge be explicit and understandable by
   stakeholders. As so many researchers have been studying alternative ways
   to conventional approaches i.e. books. papers. reports and other written
   communication means that favour knowledge acquisition on behalf of
   users. In this context, we propose the Knowledge Package (KP) structure
   as alternative. We have carried out an experiment which compared the
   usability of the proposed approach with conventional ones, along with
   the efficiency and the comprehensibility of the knowledge enclosed in a
   KP rather than in a set of Conventional Sources. The experiment has
   pointed Out that knowledge packages are more efficient than conventional
   ones, for knowledge transfer. The experiment has been described
   according to guidelines that allow for replications, In this way other
   researchers can confirm or refute the results and enforce their
   validity.},
ISSN = {1865-0929},
EISSN = {1865-0937},
ISBN = {978-3-540-88654-9},
ResearcherID-Numbers = {Ardimento, Pasquale/V-1406-2019
   CIMITILE, Marta/F-2083-2018},
ORCID-Numbers = {Ardimento, Pasquale/0000-0001-6134-2993
   VISAGGIO, GIUSEPPE/0000-0001-5258-5328
   baldassarre, maria teresa/0000-0001-8589-2850
   CIMITILE, Marta/0000-0003-2403-8313},
Unique-ID = {WOS:000262405300027},
}

@inproceedings{ WOS:000259299500013,
Author = {Caballe, Santi},
Editor = {Xhafa, F and Barolli, L},
Title = {Combining Generic Programming and service-oriented architectures for the
   effective and timely development of complex e-Learning systems},
Booktitle = {CISIS 2008: THE SECOND INTERNATIONAL CONFERENCE ON COMPLEX, INTELLIGENT
   AND SOFTWARE INTENSIVE SYSTEMS, PROCEEDINGS},
Year = {2008},
Pages = {94-100},
Note = {2nd International Conference on Complex, Intelligent and Software
   Intensive Systems, Polytechn Univ Catalonia, Catalonia, SPAIN, MAR
   04-07, 2008},
Organization = {Tech Univ Wien; Austrian Comp Soc; IEEE Comp Soc; Oesterreich Comp
   Gesell},
Abstract = {Over the last years, e-Learning needs have been evolving accordingly
   with more and more demanding pedagogical and technological requirements.
   On-line learning environments no longer depend on homogeneous groups,
   static content and resources, and single pedagogies, hut high
   customization and flexibility are a must in this context. As a result,
   current educational organizations' needs involve extending and moving to
   highly customized learning and teaching forms in timely fashion, each
   incorporating its own pedagogical approach, each targeting a specific
   learning goal, and each incorporating its specific resources. Moreover,
   organizations' demands include a cost-effective integration of legacy
   and separated learning systems, from different institutions, departments
   and courses, which are implemented in different languages, supported by
   heterogeneous platforms and distributed everywhere, to name some of
   them. Therefore, e-Learning applications need to be developed in a way
   that overcome these demanding requirements as well as provide
   educational organizations with fast, flexible and effective solutions
   for the enhancement and improvement of the learning performance and
   outcomes. To this end, in this paper, an innovative engineering software
   technique is introduced that combines the Generic Programming paradigm
   and Service-Oriented Architectures in the form of Web-services for the
   effective and timely construction of flexible, scalable, interoperable
   and robust applications as key aspects to address the current demanding
   and changing requirements in software development in general and
   specifically in the e-Learning domain. This results in a generic,
   reusable, extensible platform called Collaborative Learning Purpose
   Library for the systematic development of collaborative learning
   applications that help meet these demanding requirements.},
DOI = {10.1109/CISIS.2008.99},
ISBN = {978-0-7695-3109-0},
ResearcherID-Numbers = {Caballé, Santi/F-7255-2010
   Xhafa, Fatos/B-8869-2012
   Caballé, Santi/S-3526-2019},
ORCID-Numbers = {Caballé, Santi/0000-0002-2490-6830
   Xhafa, Fatos/0000-0001-6569-5497
   Caballé, Santi/0000-0002-2490-6830},
Unique-ID = {WOS:000259299500013},
}

@inproceedings{ WOS:000260417700003,
Author = {Calvo-Manzano, Jose A. and Cuevas, Gonzalo and Feliu, Tomas San and
   Serrano, Ariel},
Editor = {OConnor, RV and Baddoo, N and Smolander, K and Messnarz, R},
Title = {Process Asset Library to Support Software Process Improvement in Small
   Settings},
Booktitle = {SOFTWARE PROCESS IMPROVEMENT, PROCEEDINGS},
Series = {COMMUNICATIONS IN COMPUTER AND INFORMATION SCIENCE},
Year = {2008},
Volume = {16},
Pages = {25-35},
Note = {15th European Conference on Software Process Improvement, Dublin,
   IRELAND, SEP 03-05, 2008},
Abstract = {A main factor to the success of any organization process improvement
   effort is the Process Asset Library implementation that provides a
   central database accessible by anyone at the organization. This
   repository includes any process support materials to help process
   deployment. Those materials are composed of organization's standard
   software process, software process related documentation, descriptions
   of the software life cycles, guidelines, examples, templates, and any
   artefacts that the organization considers useful to help the process
   improvement. This paper describes the structure and contents of the
   Web-based Process Asset Library for Small businesses and small groups
   within large organizations. This library is structured using CMMI as
   reference model in order to implement those Process Areas described by
   this model.},
ISSN = {1865-0929},
ISBN = {978-3-540-85934-5},
ResearcherID-Numbers = {Calvo-Manzano, Jose A./K-9426-2013
   San Feliu, Tomas/B-4515-2011},
ORCID-Numbers = {Calvo-Manzano, Jose A./0000-0002-2864-2203
   San Feliu, Tomas/0000-0002-6104-7430},
Unique-ID = {WOS:000260417700003},
}

@inproceedings{ WOS:000258743700062,
Author = {Chen, Po-Han and Truc, Nguyen Thi Lan},
Editor = {Zavadskas, EK and Kaklauskas, A and Skibniewski, MJ},
Title = {Automatic 3D modeling development and application for hydraulic
   construction},
Booktitle = {25TH INTERNATIONAL SYMPOSIUM ON AUTOMATION AND ROBOTICS IN CONSTRUCTION
   - ISARC-2008},
Year = {2008},
Pages = {435-439},
Note = {25th International Symposium on Automation and Robotics in Construction,
   Vilnius, LITHUANIA, JUN 26-29, 2008},
Organization = {Int Assoc Automat \& Robit Construct; Vilnius Gediminas Tech Univ, Inst
   Internet \& Intelligent Technol; Int Council Res \& Innovat Bldg \&
   Construct; Lithuanian Acad Sci; Russian Acad Engn; Int Acad Engn},
Abstract = {Nowadays, the application of 3D models is increasing in almost every
   field, especially the AEC (Architecture, Engineering, and Construction)
   industry. Most 3D models are generated through human manipulation with
   the use of CAD software like 3D Max, AutoCAD or Maya. These software
   packages are very efficient in 3D modeling, but sometimes they are not
   easy to use and experience might be required. Moreover, traditional 3D
   modeling is time-consuming, as manual input is required for each
   component as well as for the whole scene. Therefore, it is difficult for
   a person to build 3D models without good 3D modeling skills and the
   knowledge of 3D modeling tools, although CAD software provides man),
   facilitating functions.
   In the hydraulic engineering domain, 3D models are used not only to
   illustrate the realistic view before construction, but also to measure
   the construction's volume in order to estimate cost based on
   cut-and-fill volume quantity, concrete volume quantity, etc. Therefore,
   3D modeling is crucial and greatly needed in planning hydraulic
   construction. Since 3D modeling for hydraulic facilities requires
   accuracy and details, it would need a lot or time, effort and prior
   experience to manually build the whole model from scratch.
   To meet this need, an approach of automatic 3D modeling is proposed in
   this paper. The main purpose is to help the user build a 3D model of
   hydraulic construction in less time and with less manual operation.
   Also, the proposed approach aims to provide easy-to-use features for
   those who do not have much experience in 3D modeling.
   To demonstrate the proposed approach, an application is developed in the
   AutoCAD environment to automatically generate a 3D sluice model, a model
   of artificial passageway for water fitted with a valve or gate to stop
   or regulate water flow.
   With this approach, users no longer have to manually build 3D models
   step by step. A 3D model could be automatically generated shortly after
   the input of data. As most steps are automatically done by the computer,
   the result would be of high accuracy. A complete 3D sluice model could
   be created from a sketch without much effort and time spent comparing to
   the traditional step-by-step manual input and operation.},
DOI = {10.3846/isarc.20080626.435},
ISBN = {978-9955-28-304-1},
Unique-ID = {WOS:000258743700062},
}

@article{ WOS:000253027300009,
Author = {Filippi, Stefano and Motyl, Barbara and Bandera, Camillo},
Title = {Analysis of existing methods for 3D modelling of femurs starting from
   two orthogonal images and development of a script for a commercial
   software package},
Journal = {COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE},
Year = {2008},
Volume = {89},
Number = {1},
Pages = {76-82},
Month = {JAN},
Abstract = {Background: At present the interest in medical field about the
   generation of three-dimensional digital models of anatomical structures
   increases due to the widespread diffusion of CAS - computer assisted
   surgery - systems. Most of them are based on CT computer tomography - or
   MR - magnetic resonance - data volumes but sometimes this information is
   not available; there are only few X-ray, ultrasound or fluoroscopic
   images.
   Methods: This paper describes the study and the development of a script
   for a commercial software package (ads Max) able to reconfigure the
   template model of a femur starting from two orthogonal images
   representing the specific patient's anatomy. Results: The script was
   used in several tests as summarized in this paper and the results appear
   to be interesting and acceptable, even for the medical experts that
   evaluated them.
   Conclusions: The script developed in this work allows the generation of
   the 3D model of a femur in a very simple way (the user interface has
   been developed obeying to the main usability guidelines) and using a
   widespread commercial package. The quality of the results can be
   compared to the quality of more expensive and specialized systems. (c)
   2007 Elsevier Ireland Ltd. All rights reserved.},
DOI = {10.1016/j.cmpb.2007.10.011},
ISSN = {0169-2607},
EISSN = {1872-7565},
ResearcherID-Numbers = {Motyl, Barbara/B-1193-2010
   Filippi, Stefano/ABB-6386-2020
   Filippi, Stefano/A-6118-2009},
ORCID-Numbers = {Motyl, Barbara/0000-0001-9632-9799
   Filippi, Stefano/0000-0001-6281-8690
   Filippi, Stefano/0000-0001-6281-8690},
Unique-ID = {WOS:000253027300009},
}

@inproceedings{ WOS:000259898200119,
Author = {Gameiro, C. P. and Cirne, J.},
Editor = {Marques, AT and Silva, AF and Baptista, APM and Sa, C and Alves, FJLA and Malheiros, LF and Vieira, M},
Title = {Dynamic Mechanical Behaviour of Innovative Structures Incorporating Cork},
Booktitle = {ADVANCED MATERIALS FORUM IV},
Series = {MATERIALS SCIENCE FORUM},
Year = {2008},
Volume = {587-588},
Pages = {599-603},
Note = {13th Conference of the Sociedade-Portuguesa-de-Materiais/4th
   International Materials Symposium, Oporto, PORTUGAL, APR 01-04, 2007},
Abstract = {Cork is a natural cellular material which has been used for centuries,
   in natural and agglomerate forms, mainly for applications related to the
   wine, the automotive and the construction industries. It is a very
   durable and ecological material, used for thermal, acoustic and
   vibrating insulation as well as packaging, among others. This paper
   highlights some of the aspects of a topic of great interest, not much
   explored yet, which consists of the study of the dynamic mechanical
   behaviour of innovative structures incorporating cork, dedicated to
   energy-absorption. Experimental and numerical tests, using the finite
   element method software LS-DYNA(TM), were performed in order to evaluate
   the effects of filling agglomerate cork inside thin-walled metallic
   tubes, with variable geometries and thicknesses, impacted uniaxially at
   quasi-static and high strain rates. Sonic relevant comparisons were
   carried out and the results obtained allowed concluding that cork might
   be a viable energy-absorbing material for application in some metallic
   structures subjected to impact loadings.},
DOI = {10.4028/www.scientific.net/MSF.587-588.599},
ISSN = {0255-5476},
Unique-ID = {WOS:000259898200119},
}

@inproceedings{ WOS:000260562701099,
Author = {Hou Shuping and Yang Qingxin and Yan Rongge and Chen Haiyan and Yang
   Wenrong},
Book-Group-Author = {IEEE},
Title = {Design and Implementation of Numerical Calculation Software Package for
   New High Performance Materials Applied to Electrical Engineering},
Booktitle = {2008 CHINESE CONTROL AND DECISION CONFERENCE, VOLS 1-11},
Year = {2008},
Pages = {1905-1908},
Note = {20th Chinese Control and Decision Conference, Yantai, PEOPLES R CHINA,
   JUL 02-04, 2008},
Abstract = {A software package, modeling for new high performance materials applied
   to electrical engineering, for one, two or three dimensional
   electrostatic and magnetostatic field analysis has been developed. This
   software package is designed based on the FEM method, element-free
   method, boundary element method and so on for the solution of 2D or 3D
   electromagnetic problems. The package allows us to solve many kinds of
   problems such as static and dynamic characteristics application
   characteristics, weak and strong coupling, etc. The package includes
   three parts: the pre-processing for generating mesh automatically and
   displaying it, computing 2D or3D electromagnetic field with the method
   of the vector potential and the two scalar potentials based on
   isoparametric elements, the post-processing. In this paper, application
   of electro-rheological technology on electro-hydraulic braking control
   system is taken as an example given to show how the program helps the
   designer to improve the design of the product. The results show the
   software package could bring advantages to the producers and designers.},
DOI = {10.1109/CCDC.2008.4597656},
ISBN = {978-1-4244-1733-9},
ResearcherID-Numbers = {Chen, Haiyan/HGB-6216-2022
   Yang, Heroine/GRY-2630-2022},
Unique-ID = {WOS:000260562701099},
}

@inproceedings{ WOS:000261094400097,
Author = {Kafadarova, Nadezhda and Andonova, Anna and Andreev, Svetozar and
   Arnaudov, Radosvet and Tzanova, Slavka},
Book-Group-Author = {IEEE},
Title = {Thermal Optimization of 3D Microcontacts using DOE and CFD Analysis},
Booktitle = {ESTC 2008: 2ND ELECTRONICS SYSTEM-INTEGRATION TECHNOLOGY CONFERENCE,
   VOLS 1 AND 2, PROCEEDINGS},
Year = {2008},
Pages = {535+},
Note = {2nd Electronics System-Integration Technology Conference, Greenwich,
   ENGLAND, SEP 01-04, 2008},
Organization = {Univ Greenwich; IEEE; CPMT; iMAPS},
Abstract = {The present article describes an approach for optimization of thermal
   performance of 3D micro-components from pin-ring type implemented in IC
   package to PCB assembly and micro mechanical actuators by common use of
   DOE and CFD simulations. The goal of the considered approach is to
   define a real concurrent process for design of reliable microelectronic
   systems for specific applications. A model, containing all of the
   requisite design factors such as sizes, material and form was created
   using the commercially available CFD software, FLOTHERM. The results of
   simulation of test structures are verified by thermovision measurements
   by infrared camera P640 of FLIR. Advantages and disadvantages of
   different studied constructions of micro-contacts are analyzed in
   respect of better parameters of the process of heat transfer.},
DOI = {10.1109/ESTC.2008.4684406},
ISBN = {978-1-4244-2813-7},
ResearcherID-Numbers = {Stoynova, Anna V/A-4888-2016
   , Andonova/AAD-8316-2021
   Kafadarova, Nadezhda/AAC-4219-2021
   },
ORCID-Numbers = {Stoynova, Anna V/0000-0003-0929-1169
   Kafadarova, Nadezhda/0000-0003-3161-7516},
Unique-ID = {WOS:000261094400097},
}

@inproceedings{ WOS:000270725700013,
Author = {Magnusson, Magnus S.},
Editor = {Westerink, JHDM and Ouwerkerk, M and Overbeek, TJM and Pasveer, WF and deRuyter, B},
Title = {DISCOVERY OF T-TEMPLATES AND THEIR REAL-TIME INTERPRETATION USING THEME},
Booktitle = {PROBING EXPERIENCE: FROM ASSESSMENT OF USER EMOTIONS AND BEHAVIOUR TO
   DEVELOPMENT OF PRODUCTS},
Series = {Philips Research Book Series},
Year = {2008},
Volume = {8},
Pages = {119-126},
Note = {Probing Experience Symposium, Eindhoven, NETHERLANDS, JUN 07-08, 2006},
Abstract = {The temporal structure of every-day human behavior and interactions is
   certainly a complex affaire rich in repeated patterns or translation
   symmetry. This paper concerns a view of the structure of real-time
   streams of behavior as repeated, temporal patterns of a particular kind
   called t-patterns. An instance of a pattern of this kind consists of a
   particular and possibly quite small set of primitives of behavioral
   significance ( verbal, nonverbal and/or environmental) occurring
   significantly more often than chance expectation in a particular order
   and/or concurrently with characteristic intervals between them. The
   analogies thus exist with speech and writing where only a few letters or
   phonemes are combined to create hundreds of thousands of different words
   and common word combinations. While remaining statistically significant,
   the time structure of t-patterns is also flexible and thus accommodates
   that of, for example, words, phrases, melodies and musical themes, which
   may be performed with considerable variation between repetitions in
   speed and internal intervals.
   T-patterns are thus hierarchical patterns of patterns etc. and as
   phrases, their interpretation and effects ( meaning, function) depend on
   the particular words involved, the temporal aspects of their production
   ( performance) and the general context in which they occur. Some
   t-patterns occur cyclically and this better known aspect is now also
   automatically detected by the software THEME, which has been specially
   developed for t-pattern detection. A typical characteristic that has
   caused much difficulty regarding the detection of behavioral
   ``sequences{''} is that routines, ceremonies and verbal ``t-frames{''} (
   such as if.. then.. else) is that other behavior may occur in various
   numbers and ways between the components of different instances of the
   same pattern. Profiling individuals, interactions and/or groups can be
   based on the existence of particular t-patterns and/or the absolute or
   relative frequencies of patterns.
   The THEME software also detects various other phenomena derived from the
   so called critical interval relationship and the t-pattern type such as
   t-bursts, t-cycles, t-markers, t-paths, t-associates, t-frames, and
   t-packets. This is the primary task of Theme, but a considerable part of
   the software helps with the analysis and use of the detected patterns,
   which is done both through visual and statistical means.
   The software can thus automatically analyze a large number of datasets
   in a single run and automatically build a data base of detected patterns
   that can then be consulted in various ways. Highly significant effects
   of independent ( experimental) variables on the frequency and complexity
   of detected t-patterns have often been found in studies where no
   significant effects were detected using the same initial data and
   standard statistical methods alone.
   Real-time use of THEME for the interpretation of ongoing behavior seems
   feasible given some further development. Theme thus already
   automatically creates t-pattern templates (t-templates) on the basis of
   a detected pattern base - which can be updated off-line from time to
   time. Template matching being much faster than the preceding pattern
   discovery and template construction, t-templates could be matched
   real-time against incoming data. Higher speed could be obtained through
   parallel processing. THEME is currently developed in Delphi 2005
   Professional and large parts have already been transferred to Linux
   using Kylix ( for use in Bioinformatics) partly in preparation for a
   parallel processing version.},
DOI = {10.1007/978-1-4020-6593-4\_11},
ISSN = {1571-5671},
ISBN = {978-1-4020-6593-4; 978-1-4020-6592-7},
ResearcherID-Numbers = {Magnusson, Magnus/AAX-7854-2021},
Unique-ID = {WOS:000270725700013},
}

@inproceedings{ WOS:000261332300001,
Author = {Marzullo, Fabio Perez and Porto, Rodrigo Novo and da Silva, Geraldo
   Zimbrao and de Souza, Jano Moreira and Blaschek, Jose Roberto},
Editor = {Lee, R and Kim, HK},
Title = {An MDA Approach for Database Profiling and Performance Assessment},
Booktitle = {COMPUTER AND INFORMATION SCIENCE},
Series = {Studies in Computational Intelligence},
Year = {2008},
Volume = {131},
Pages = {1+},
Note = {7th IEEE/ACIS International Conference on Computer and Information
   Science in Conjunction with 2nd IEEE/ACIS International Workshop on
   e-Activity, Portland, OR, MAY 14-16, 2008},
Organization = {IEEE Comp Soc; Int Assoc Comp \& Informat Sci},
Abstract = {This paper describes a Model Driven Architecture (MDA) approach for
   assessing database performance. The increase in the area of
   component-based development is reaching a point where performance issues
   are critical to successful system deployment. It is widely accepted that
   the model development approach is playing an important role on IT
   projects; therefore the profiling technique discussed here presents a
   way to assess performance and identify flaws while performing software
   construction activities. The approach is straight forward: using new
   defined MDA stereotypes and tagged values in conjunction with profiling
   libraries, the proposed MDA extension enables code generation to conduct
   a thorough set of performance analysis elements. This implementation
   uses the well-known MDA framework AndroMDA {[}21, the profiling
   libraries JAMon {[}11] and InfraRED {[}15], and creates a Profiling
   Cartridge, in order to generate the assessment code.},
ISSN = {1860-949X},
EISSN = {1860-9503},
ISBN = {978-3-540-79186-7},
ResearcherID-Numbers = {, Jano/AAG-9116-2019},
ORCID-Numbers = {, Jano/0000-0001-5080-1955},
Unique-ID = {WOS:000261332300001},
}

@inproceedings{ WOS:000261491700088,
Author = {Rodriguez-Ramos, J. M. and Magdaleno Castello, E. and Dominguez Conde,
   C. and Rodriguez Valido, M. and Marichal-Hernandez, J. G.},
Editor = {Hubin, N and Max, CE and Wizinowich, PL},
Title = {2D-FFT implementation on FPGA for wavefront phase recovery from the
   CAFADIS camera},
Booktitle = {ADAPTIVE OPTICS SYSTEMS, PTS 1-3},
Series = {Proceedings of SPIE},
Year = {2008},
Volume = {7015},
Number = {1-3},
Note = {Conference on Adaptive Optics Systems, Marseille, FRANCE, JUN 23-28,
   2008},
Organization = {SPIE; SPIE Europe},
Abstract = {The CAFADIS camera is a new sensor patented by Universidad de La Laguna
   (Canary Islands, Spain): international patent PCT/ES2007/000046 (WIPO
   publication number WO/2007/082975). It can measure the wavefront phase
   and the distance to the light source at the same time in a real time
   process. It uses specialized hardware: Graphical Processing Units (GPUs)
   and Field Programmable Gates Arrays (FPGAs). These two kinds of
   electronic hardware present an architecture capable of handling the
   sensor Output stream in a massively parallel approach. Of course, FPGAs
   are faster than GPUs, this is why it is worth it using FPGAs integer
   arithmetic instead of GPUs floating point arithmetic.
   GPUs must not be forgotten, as we have shown in previous papers, they
   are efficient enough to resolve several problems for AO in Extremely
   Large Telescopes (ELTs) in terms of time processing requirements; in
   addition, the GPUs show a widening gap in computing speed relative to
   CPUs. They are much more powerful in order to implement AO simulation
   than common software packages running on top of CPUs.
   Our paper shows an FPGA implementation of the wavefront phase recovery
   algorithm using the CAFADIS camera. This is done in two steps: the
   estimation of the telescope pupil gradients from the telescope focus
   image, and then the very novelty 2D-FFT over the FPGA. Time processing
   results are compared to our GPU implementation. In fact, what we are
   doing is a comparison between the two different arithmetic mentioned
   above, then we are helping to answer about the viability of the FPGAs
   for AO in the ELTs.},
DOI = {10.1117/12.789312},
Article-Number = {701539},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-0-8194-7225-0},
ResearcherID-Numbers = {Valido, Manuel Rodriguez/L-1840-2014},
ORCID-Numbers = {Valido, Manuel Rodriguez/0000-0003-0873-9857},
Unique-ID = {WOS:000261491700088},
}

@inproceedings{ WOS:000263190000198,
Author = {Russo, G. and De Lucia, Barbara},
Editor = {DePascale, S and Mugnozza, GS and Maggio, A and Schettini, E},
Title = {Environmental Evaluation by Means of LCA Regarding the Ornamental
   Nursery Production in Rose and Sowbread Greenhouse Cultivation},
Booktitle = {PROCEEDINGS OF THE INTERNATIONAL SYMPOSIUM ON HIGH TECHNOLOGY FOR
   GREENHOUSE SYSTEM MANAGEMENT, VOLS 1 AND 2},
Series = {Acta Horticulturae},
Year = {2008},
Number = {801},
Pages = {1597+},
Note = {International Symposium on High Technology for Greenhouse System
   Management (Greensys 2007), Naples, ITALY, OCT 04-06, 2007},
Abstract = {This research has been carried out funded by EU project ``Ecoflower
   Terlizzi{''} (LIFE04 ENV/IT/000480) with ISO standardization guidelines,
   with the participation of Italian ornamental growers and breeders, in
   order to gather information regarding the floricultural production
   process. The applications of LCA (life cycle assessment) methodology to
   greenhouse crops, point out the difficult to analyse an agricultural
   production with large use of energy, raw materials, structures and
   equipments, but also depending by the biology of vegetables, cultivation
   systems and the economic growers chooses. The evaluation of baby plants,
   in term of energy and materials consumptions, is the first step to
   realize an LCA analysis of whole productive cycle. At this aim, an LCI
   (life cycle inventory) was been realised near nursery farm in order to
   gathering production data of grafted baby plant of rose. The same search
   was carried out into a nursery farm that produces sowbread baby plants
   by F1 seedling. These two ornamental plants have been selected because
   representing the commercial typologies of cut flower and pot plant most
   growed in Terlizzi's area, an ornamental greenhouse district in Apulia -
   Southern Italy. The informations about the production of baby plant have
   been collected in a database and processed by specific LCA software.
   Further investigations have been done in order to analyze and evaluate
   the fertilisers and pesticides used in the nursery. The results of LCA
   analysis have been expressed by means of the environmental burdens
   produced by energy, water, packaging, fertilisers and pesticides
   consumptions, structures and equipments, used in the production of baby
   plant. The results of this research point out that the environmental
   burdens of the production of baby plants are negligible in the rose
   production and considerable in the pot sowbread production principally
   because of the contributions of polystyrene plateaux and
   Polyvinylchloride plastic pot.},
DOI = {10.17660/ActaHortic.2008.801.198},
ISSN = {0567-7572},
ISBN = {978-90-6605-621-3},
ResearcherID-Numbers = {De Lucia, B./G-8360-2012
   De Lucia, Barbara/ABC-4465-2020
   Russo, Giovanni/H-1050-2012},
ORCID-Numbers = {De Lucia, Barbara/0000-0001-7661-1818
   Russo, Giovanni/0000-0002-5803-4866},
Unique-ID = {WOS:000263190000198},
}

@inproceedings{ WOS:000259629900037,
Author = {Son, Hoang M.},
Editor = {Bock, HG and Kostina, E and Phu, HX and Rannacher, R},
Title = {Design Patterns for High-Performance Matrix Computations},
Booktitle = {MODELING, SIMULATION AND OPTIMIZATION OF COMPLEX PROCESSES},
Year = {2008},
Pages = {509-519},
Note = {3rd International Conference on High Performance Scientific Computing,
   Hanoi, VIETNAM, MAR 06-10, 2006},
Organization = {Interdisciplinary Ctr Sci Comp; Int PhD Prog; Univ Heidelberg, Complex
   Proc - Modeling, Simulat \& Optimizat; Gottlieb Daimler; Karl Benz Fdn;
   DFG Res Ctr Matheon; Berlin/Bradenburg Acad Sci \& Humanities; Abdus
   Salam Int Ctr Theoret Phys; Vietnamese Acad Sci \&Technol; Inst Math;
   Vietnam Natl Program Basic Sci; Ho Minh City Univ Technol},
Abstract = {This paper discusses fundamental issues of developing high-performance
   matrix computation software in an object-oriented language like C++ and
   presents key design patterns for solving these problems. These
   object-oriented design patterns are implemented in FMOL++ (Fundamental
   Mathematical Object Library) a C++ template library, which includes
   basic algebraic structures and operations as well as common model
   classes needed by control system analysis and design. Through over a
   decade of evolutions, these patterns have proved to provide
   best-practice solutions to most common problems in the context of
   high-performance matrix computation. Benchmarks are made for performance
   comparisons between FMOL++ and alternative approaches.},
DOI = {10.1007/978-3-540-79409-7\_37},
ISBN = {978-3-540-79408-0},
Unique-ID = {WOS:000259629900037},
}

@inproceedings{ WOS:000253656300045,
Author = {Tang, Liang and Liu, Hui and Zhu, Yan and Cao, Weixing},
Editor = {Li, DL},
Title = {Development of a model-based digital and visual wheat growth system},
Booktitle = {COMPUTER AND COMPUTING TECHNOLOGIES IN AGRICULTURE, VOL 2},
Series = {INTERNATIONAL FEDERATION FOR INFORMATION PROCESSING},
Year = {2008},
Volume = {259},
Pages = {1113-1120},
Note = {1st International Conference on Computer and Computing Technologies in
   Agriculture (CCTA 2007), Wuyishan, PEOPLES R CHINA, AUG 18-20, 2007},
Organization = {China Agr Univ; Chinese Soc Agr Engn; Beijing Soc Informat Technol Agr;
   IFIP TC 12},
Abstract = {Driven by soil, variety, weather and management databases and
   integrating process-based growth simulation model, morphological model
   and visualization model, a model-based digital and visual wheat growth
   system (MDVWGS) was developed using component-based software and
   visualization techniques. The system was programmed by the Net framework
   with the language of C\# and CsGL Library was used for realizing 2D and
   3D graphics application and visualization. The implemented system could
   be used for predicting growth processes and visualizing morphological
   architecture of wheat plant under various environments, genotypes and
   management strategies, and has the functions as data management, dynamic
   simulation, strategy evaluation, real-time prediction, temporal and
   spatial analysis,visualization output, expert consultation and system
   help. The MDVWGS should be useful for construction and application of
   digital farming system and provide a precise and scientific tool for
   cultivar design, cultural regulation and productivity evaluation under
   different growing conditions.},
ISSN = {1571-5736},
ISBN = {978-0-387-77252-3},
ResearcherID-Numbers = {Tang, Liang/L-2889-2019},
ORCID-Numbers = {Tang, Liang/0000-0002-3483-0462},
Unique-ID = {WOS:000253656300045},
}

@inproceedings{ WOS:000256900300051,
Author = {Wu, Harris and Cao, Lan},
Editor = {Shen, WM and Zheng, QH and Barthes, JP and Luo, JZ and Yong, JM and Duan, ZH and Tian, F},
Title = {Supporting case-based design for packaged software implementations},
Booktitle = {PROCEEDINGS OF THE 2008 12TH INTERNATIONAL CONFERENCE ON COMPUTER
   SUPPORTED COOPERATIVE WORK IN DESIGN, VOLS I AND II},
Series = {International Conference on Computer Supported Cooperative Work in
   Design},
Year = {2008},
Pages = {306-311},
Note = {12th International Conference on Computer Supported Cooperative Work in
   Design, Xian Jiaotong Univ, Xian, PEOPLES R CHINA, APR 16-18, 2008},
Organization = {IEEE China Council; Int Working Grp CSCW Design; Xidian Univ},
Abstract = {Design in packaged software implementation (PSI) is the process to solve
   business problems by customizing and integrating the off-the-shelf
   software package. PSI experts frequently practice case-based design
   (CBD) when facing a new problem situation: explore the past design
   cases, find a similar case, and reuse the design for that case in the
   new problem situation. The success of CBD depends on a continuous cycle
   of knowledge creation. This paper presents a theoretical framework of
   case-based design as an organizational knowledge creation process. Based
   on this framework, the research proposes an innovative tool to support
   CBD in packaged software implementations. The fundamental belief is that
   by utilizing the collective power of a large group of people, better
   designs can be achieved at lower costs with lower risks.},
ISBN = {978-1-4244-1650-9},
Unique-ID = {WOS:000256900300051},
}

@inproceedings{ WOS:000261330600087,
Author = {Young, Peter J. and Nielsen, Jon J. and Roberts, William H. and Wilson,
   Greg M.},
Editor = {Bridger, A and Radziwill, NM},
Title = {Achieving Design Reuse: A Case Study},
Booktitle = {ADVANCED SOFTWARE AND CONTROL FOR ASTRONOMY II, PTS 1 \& 2},
Series = {Proceedings of SPIE},
Year = {2008},
Volume = {7019},
Number = {1-2},
Note = {2nd Conference on Advanced Software and Control for Astronomy,
   Marseille, FRANCE, JUN 26-28, 2008},
Organization = {iAG; INAF; INTA; CESR; CNRS; Univ Tubingen; Ist Astrofis Spaziale Fis
   Cosm; IASF Roma; INAF; Dept Programas Espciales Cien Espac; DPECE INTA
   Madrid; E O Hulburt Ctr Space Res; HCA NRL; Max Planck Inst
   Extraterrestrial Phys; MPE; St John Coll; CNRS UPS OMP, Ctr Etude
   Spatiale Rayoonnements; Inst Astron Astrophys; Univ SAND I;
   Competitiveness Cluster, POPsud Pole Opt \& Photon; Opt Sud; SPIE},
Abstract = {The RSAA CICADA data acquisition and control software package uses an
   object-oriented approach to model astronomical instrumentation and a
   layered architecture for implementation. Emphasis has been placed on
   building reusable C++ class libraries and on the use of attribute/value
   tables for dynamic configuration. This paper details how the approach
   has been successfully used in the construction of the instrument control
   software for the Gemini NIFS and GSAOI instruments. The software is
   again being used for the new RSAA SkyMapper and WiFeS instruments.},
DOI = {10.1117/12.787698},
Article-Number = {70192M},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {978-0-8194-7229-8},
ORCID-Numbers = {Nielsen, Jon/0000-0003-4685-4231
   Young, Peter/0000-0002-2565-1964},
Unique-ID = {WOS:000261330600087},
}

@article{ WOS:000250548900006,
Author = {Sia, Siew Kien and Soh, Christina},
Title = {An assessment of package-organisation misalignment: institutional and
   ontological structures},
Journal = {EUROPEAN JOURNAL OF INFORMATION SYSTEMS},
Year = {2007},
Volume = {16},
Number = {5},
Pages = {568-583},
Month = {OCT},
Abstract = {Even with today's `best practice' software, commercial packages continue
   to pose significant alignment challenges for many organisations. This
   paper proposes a conceptual framework, based on institutional theory and
   systems ontology, to assess the misalignments between package
   functionality and organisational requirements. We suggest that these
   misalignments can arise from incompatibility in the externally imposed
   or voluntarily adopted structures embedded in the organisation and
   package, as well as differences in the way the meaning of organisational
   reality is ontologically represented in the deep or surface structure of
   packages. The synthesis of the institutional-ontological dimensions
   leads us to identify four types of misalignments with varying degrees of
   severity - imposed-deep, imposed-surface, voluntary-deep, and
   voluntary-surface - and to predict their likely resolution. We test the
   predictions using over 400 misalignments from package implementations at
   three different sites. The findings support the predictions: the
   majority of imposed-deep misalignments were resolved via package
   customisation. Imposed-surface and voluntary-deep misalignments were
   more often resolved via organisational adaptation and voluntary-surface
   misalignments were almost always resolved via organisational adaptation.
   The extent of project success also appeared to be influenced by the
   number of misalignments and the proportion of imposed-deep
   misalignments. We conclude by suggesting strategies that implementing
   organisations and package vendors may pursue.},
DOI = {10.1057/palgrave.ejis.3000700},
ISSN = {0960-085X},
EISSN = {1476-9344},
Unique-ID = {WOS:000250548900006},
}

@article{ WOS:000248815800004,
Author = {Wang, Hongjun and Akinci, Burcu and Garrett, Jr., James H.},
Title = {Formalism for detecting version differences in data models},
Journal = {JOURNAL OF COMPUTING IN CIVIL ENGINEERING},
Year = {2007},
Volume = {21},
Number = {5},
Pages = {321-330},
Month = {SEP-OCT},
Abstract = {In the architecture/engineering/construction (AEC) industry, a large
   number of data models (e.g., data exchange standards and task-specific
   data models) have been created and utilized to represent and exchange
   data in software packages. To meet the ever-expanding requirements for
   modeling real world information, the data models need to be updated
   frequently. Accordingly, those who need to implement these data models
   in their AEC-related software which often requires that they possess
   civil engineering domain knowledge, have to chan-e their existing
   implementations for compliance with these models to account for the
   latest update. Before adopting changes of such data models, those
   developers working at AEC-related software companies must precisely
   identify which parts of the data models have been modified in a new
   release. Given the growing scale and complexity of today's data models
   involved in the AEC domain, identification of differences in two
   versions of a data model is a time-consuming and error-prone process,
   when performed manually. A semiautomated approach that identifies the
   differences in two versions of a data model could enable a rapid update
   of existing implementations of the model in AEC-related software. Due to
   the likelihood of having some commonality between the two versions of a
   model, it is possible to automatically identify version differences
   accurately. In this paper, we present an approach for detecting the
   differences between two releases of the same data model accurately and
   efficiently. This approach incorporates taxonomy for describing possible
   differences between two versions of a data model and provides a way to
   classify these differences. A prototype is implemented and used to
   validate the approach with the recent releases of some real world data
   models. The approach developed in this paper can help AEC-related
   software developers adopt and implement data models in their software
   systems.},
DOI = {10.1061/(ASCE)0887-3801(2007)21:5(321)},
ISSN = {0887-3801},
EISSN = {1943-5487},
ORCID-Numbers = {Garrett, James/0000-0002-8408-2407},
Unique-ID = {WOS:000248815800004},
}

@article{ WOS:000248247800012,
Author = {Karpushkin, Eugene and Bogomolov, Andrey and Zhukov, Yury and Boruta,
   Michael},
Title = {New system for computer-aided infrared and Raman spectrum interpretation},
Journal = {CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS},
Year = {2007},
Volume = {88},
Number = {1},
Pages = {107-117},
Month = {AUG 15},
Abstract = {A new software tool for the interpretation of infrared and Raman spectra
   has been developed. It makes use of fragment libraries comprising
   representative, carefully selected and refined data on characteristic
   vibration frequencies. The information was collected from multiple
   sources including published correlation tables and reference spectral
   databases. fit an automatic mode, the system performs structure to
   spectrum verification to test the correspondence between a drawn
   chemical structure and an experimental spectrum. Computer-aided
   ``manual{''} interpretation (an expert mode) facilitates the assignment
   of structural elements to spectral features by means of highlighting the
   library information over the spectrum. The interpretation performance
   was significantly improved compared to other systems of this type due to
   some novel features. These are original fragment structures
   incorporating the concept of nucleus (vibrating group) and a new system
   of queries (fuzzy atoms and bonds), as well as the inter-fragment logic,
   which make fragment formulation extremely flexible. Important
   methodological aspects of fragment library construction were considered,
   and the main principles of fragment formulation on the basis of
   experimental spectra were formalized. Principal component analysis (PCA)
   was applied to distinguish clusters formed by spectral responses due to
   a specific structural environment and to refine characteristic frequency
   regions. (C) 2006 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.chemolab.2006.08.010},
ISSN = {0169-7439},
EISSN = {1873-3239},
ResearcherID-Numbers = {Karpushkin, Evgeny/A-6545-2013
   Karpushkin, Evgeny/AAH-5979-2019
   Bogomolov, Andrey/B-9400-2011},
ORCID-Numbers = {Karpushkin, Evgeny/0000-0001-5239-9787
   Bogomolov, Andrey/0000-0002-4832-638X},
Unique-ID = {WOS:000248247800012},
}

@article{ WOS:000247701400005,
Author = {van der Schaaf, Tom and Germans, Desmond and Bal, Henri E. and Koutek,
   Michal},
Title = {Lessons learned from building and calibrating the ICWall, a stereo tiled
   display},
Journal = {COMPUTER ANIMATION AND VIRTUAL WORLDS},
Year = {2007},
Volume = {18},
Number = {3},
Pages = {193-210},
Month = {JUL},
Note = {International Conference on Virtual Reality Continuum and Its
   Applications (VRCIA), Hong Kong, PEOPLES R CHINA, 2006},
Organization = {ACM SIGGRAPH; Eurgograph Assoc; Chinese Soc Image \& Graph; INI
   GraphicsNet},
Abstract = {Implementation of stereo tiled displays is a rather demanding task. In
   this article we want to share the lessons we have learned during the
   design and construction of the ICWall tiled display. This large display,
   used in a classroom setting, is a high-resolution stereo tiled display
   (2 x 8 tiles), built from low-cost commodity components. The overall
   image is produced by an array of projectors. When building such a
   system, a key challenge is to align the projector images. We describe
   our automated approach for alignment/calibration of the left- and
   right-eye stereo images. We provide measurements that show accuracy of
   this procedure. We explain and compare two calibration approaches: a
   single-pass and a two-pass rendering method to align the tiled images.
   We explain how to provide seamless image on the tiled display and which
   issues have to be solved. We also discuss the depth perception issues on
   the ICWall for the large audiences. Another important aspect, is the
   architecture of the software used for PC-cluster-based rendering. We
   describe Aura, the parallel scene graph API that is used for rendering
   on our tiled display. Copyright (c) 2007 John Wiley \& Sons, Ltd.},
DOI = {10.1002/cav.172},
ISSN = {1546-4261},
EISSN = {1546-427X},
ORCID-Numbers = {Bal, H.E./0000-0001-9827-4461},
Unique-ID = {WOS:000247701400005},
}

@article{ WOS:000245820700003,
Author = {Magne, Pascal},
Title = {Efficient 3D finite element analysis of dental restorative procedures
   using micro-CT data},
Journal = {DENTAL MATERIALS},
Year = {2007},
Volume = {23},
Number = {5},
Pages = {539-548},
Month = {MAY},
Abstract = {Objectives. This investigation describes a rapid method for the
   generation of finite element models of dental structures and
   restorations.
   Methods. An intact mandibular molar was digitized with a micro-CT
   scanner. Surface contours of enamel and dentin were fitted following
   tooth segmentation based on pixel density using an interactive medical
   image control system. Stereolithography (STL) files of enamel and dentin
   surfaces were then remeshed to reduce mesh density and imported in a
   rapid prototyping software, where Boolean operations were used to assure
   the interfacial mesh congruence (dentinoenamel junction) and simulate
   different cavity preparations (MO/MOD preparations, endodontic access)
   and restorations (feldspathic porcelain and composite resin inlays). The
   different tooth parts were then imported in a finite element software
   package to create 3D solid models. The potential use of the model was
   demonstrated using nonlinear contact analysis to simulate occlusal
   loading. Cuspal deformation was measured at different restorative steps
   and correlated with existing experimental data for model validation and
   optimization.
   Results. Five different models were validated by existing experimental
   data. Cuspal widening (between mesial cusps) at 100 N load ranged from
   0.4 mu m for the unrestored tooth, 9-12 mu m for MO, MOD cavities, to
   12-21 mu m for endodontic access cavities. Placement of an MOD adhesive
   restoration in porcelain resulted in 100\% cuspal stiffness recovery
   (0.4 mu m of cuspal widening at 100 N) while the composite resin inlay
   allowed for a partial recuperation of cusp stabilization (1.3 mu m of
   cuspal widening at 100 N).
   Significance. The described method can generate detailed and valid three
   dimensional finite element models of a molar tooth with different
   cavities and restorative materials. This method is rapid and can readily
   be used for other medical (and dental) applications. (c) 2006 Academy of
   Dental Materials. Published by Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.dental.2006.03.013},
ISSN = {0109-5641},
EISSN = {1879-0097},
ResearcherID-Numbers = {Self, Casey J/B-6871-2011},
Unique-ID = {WOS:000245820700003},
}

@article{ WOS:000245092100003,
Author = {Poletti, Francesco and Poggiali, Antonio and Bertozzi, Davide and
   Benini, Luca and Marchal, Pol and Loghi, Mirko and Poncino, Massimo},
Title = {Energy-efficient multiprocessor systems-on-chip for embedded computing:
   Exploring programming models and their architectural support},
Journal = {IEEE TRANSACTIONS ON COMPUTERS},
Year = {2007},
Volume = {56},
Number = {5},
Pages = {606-621},
Month = {MAY},
Abstract = {In today's multiprocessor SoCs (MPSoCs), parallel programming models are
   needed to fully exploit hardware capabilities and to achieve the 100
   Gops/W energy efficiency target required for Ambient Intelligence
   Applications. However, mapping abstract programming models onto tightly
   power-constrained hardware architectures imposes overheads which might
   seriously compromise performance and energy efficiency. The objective of
   this work is to perform a comparative analysis of message passing versus
   shared memory as programming models for single-chip multiprocessor
   platforms. Our analysis is carried out from a hardware-software
   viewpoint: We carefully tune hardware architectures and software
   libraries for each programming model. We analyze representative
   application kernels from the multimedia domain, and identify
   application-level parameters that heavily influence performance and
   energy efficiency. Then, we formulate guidelines for the selection of
   the most appropriate programming model and its architectural support.},
DOI = {10.1109/TC.2007.1040},
ISSN = {0018-9340},
EISSN = {1557-9956},
ORCID-Numbers = {BENINI, LUCA/0000-0001-8068-3806},
Unique-ID = {WOS:000245092100003},
}

@article{ WOS:000245136100017,
Author = {Chen, Wei and Gilson, Michael K.},
Title = {ConCept: De novo design of synthetic receptors for targeted ligands},
Journal = {JOURNAL OF CHEMICAL INFORMATION AND MODELING},
Year = {2007},
Volume = {47},
Number = {2},
Pages = {425-434},
Month = {MAR-APR},
Abstract = {Low-molecular-weight receptors that bind targeted guest molecules have a
   wide range of potential applications but are difficult to design. This
   paper describes an evolutionary method for computer-aided design of such
   receptors that works by linking together chemical components from a
   user-defined library around a stable conformation of the targeted
   ligand. The software can operate in three modes: de novo design, in
   which it builds a wide variety of receptors from small components;
   macrocycle design, in which it builds homopolymeric macrocycles around
   the ligand; and elaboration of an existing receptor structure. The top
   candidates generated by the automatic construction process are further
   studied with detailed affinity calculations whose validity is supported
   by prior studies of experimentally characterized host-guest systems. All
   three modes of operation are illustrated here through the design of
   novel adenine receptors.},
DOI = {10.1021/ci600233v},
ISSN = {1549-9596},
Unique-ID = {WOS:000245136100017},
}

@inproceedings{ WOS:000260249900266,
Author = {Balaji, V and Ramakrishna, P. V.},
Book-Group-Author = {IEEE},
Title = {DESIGN STUDIES ON THE IMPLEMENTATION OF ON BOARD CONTROL, SIGNAL
   ACQUISITION AND COMMUNICATION (OBCSAC) SYSTEM ON FPGA/ASIC PLATFORMS},
Booktitle = {ICIAS 2007: INTERNATIONAL CONFERENCE ON INTELLIGENT \& ADVANCED SYSTEMS,
   VOLS 1-3, PROCEEDINGS},
Year = {2007},
Pages = {1340-1344},
Note = {International Conference on Intelligent and Advanced Systems, Kuala
   Lumpur, MALAYSIA, NOV 25-28, 2007},
Organization = {Petronas; IET; Mitsubishi Corp; Techsource},
Abstract = {The On Board Control, Signal Acquisition and Communication (OBCSAC)
   subsystem is a critical component of all satellite systems. This paper
   presents the results of a feasibility study on realizing the design
   subsystem first on FPGA, and then on ASIC. Apart from giving the design
   details and results on a typical basic configuration, a number of
   extensions of this architecture have been studied and the results are
   presented. Specifically, starting from the realization of the basic
   configuration on a commercial antifuse FPGA, the studies carried out
   include (a) mapping the vital configuration onto an equivalent radiation
   hardened antifuse FPGA (b) mapping the same configuration onto a
   commercial FPGA after incorporating `Triple Modular Redundancy (TMR)
   manually at the RTL level, (c) altering the basic architecture
   significantly to conform to the CCSDS standard prescribed for the OBCSAC
   of any satellite and mapping the design onto to FPGAs as well as on
   ASICS first without TMR and then separately incorporating TMR for each
   case. The basic configuration realized in an FPGA had been extensively
   tested in hardware under various expected environmental conditions and
   the FPGA utilization details are provided. For the ASIC design, the
   details up to the layout/GDSII stage are presented. Further, in the case
   of the ASIC, the test patterns for the netlist were generated based on
   ATPG software and the details of fault coverage are provided. The FPGA
   designs were carried out with Actel SX 72A and AX 1000 device families
   and the ASIC designs were carried out using the AMIS 0.35u CMOS
   libraries. The results presented in this work will serve as a design
   guideline in terms of FPGA/ASIC area, power, speed, and fault coverage
   and fault tolerance to those working in the area of system design.},
ISBN = {978-1-4244-1355-3},
Unique-ID = {WOS:000260249900266},
}

@inproceedings{ WOS:000247868000001,
Author = {Ceri, Stefano},
Editor = {Cooper, R and Kennedy, J},
Title = {Design abstractions for innovative web applications},
Booktitle = {Data Management: Data, Data Everywhere, Proceedings},
Series = {LECTURE NOTES IN COMPUTER SCIENCE},
Year = {2007},
Volume = {4587},
Pages = {1-2},
Note = {24th British National Conference on Databases (BNCOD 24), Univ Glasgow,
   Glasgow, SCOTLAND, JUL 03-05, 2007},
Abstract = {Web Modelling Language (WebML) {[}1-2] was defined, about 8 years ago,
   as a conceptual model for data-intensive Web applications. Early
   deployment technologies were very unstable and immature; as a reaction,
   WebML was thought as a high level, implementation-independent conceptual
   model, and the associated design support environment, called WebRatio
   {[}7], has always been platform-independent, so as to adapt to frequent
   technological changes. WebML is based upon orthogonal separation of
   concerns: content, interface logics, and presentation logics are defined
   as separate components. The main innovation in WebML comes from the
   interface logics, that enables the computation of Web pages made up of
   logical components (units) interconnected by logical links (i.e., not
   only the units but also the links have a formal semantics); the
   computation is associated with powerful defaults so as to associate to
   simple diagrams all the required semantics for a full deployment,
   through code generators. While the Web has gone through waves of
   innovation, new application sectors have developed, and revolutionary
   concepts - such as enabling the interaction of software artefacts rather
   than only humans - are opening up. While the foundations of the WebML
   model and method are still the same, the pragmatics of its
   interpretation and use has dramatically changed through the last years
   {[}3-6]. A retrospective consideration of our work shows that we have
   addressed every new challenge by using a common approach, which indeed
   has become evident to us during the course of time, and now is well
   understood and consolidated. For every new research directions, we had
   to address four different kinds of extensions, respectively addressing
   the development process, the content model, the hypertext meta-model,
   and the tool framework.
   Extensions of the development process capture the new steps of the
   design that are needed to address the new functionalities, providing as
   well the methodological guidelines and best practices for helping
   designers.
   Extensions of the content model capture state information associated
   with providing the new functionalities, in the format of standard model,
   e.g. a collection of entities and relationship that is common to all
   applications; this standard model is intertwined with the application
   model, so as to enable a unified use of all available content.
   Extension of the hypertext meta-model capture the new abstractions that
   are required for addressing the new functionalities within the design of
   WebML specifications, through new kinds of units and links which
   constitute a functionality-specific ``library{''}, which adds to the
   ``previous{''} ones.
   Extensions of the tool framework introduce new tools in order to extend
   those modelling capability falling outside of standard WebRatio
   components (content, interface logics, presentation logics), or to
   empower users with new interfaces and wizards to express the semantics
   of new units and links in terms of existing ones, or to provide direct
   execution support for new units and links (e.g. invoking a web service).
   In this talk, I first illustrate the common approach to innovation, and
   then show such approach at work in two contexts. One of them, dealing
   with ``Service-Oriented Architectures{''} (SOA), has reached a mature
   state; the other one, ``Semantic Web Services{''} (SWS), is at its
   infancy, but promises to deliver very interesting results in the
   forthcoming years.},
ISSN = {0302-9743},
ISBN = {978-3-540-73389-8},
Unique-ID = {WOS:000247868000001},
}

@inproceedings{ WOS:000253442200028,
Author = {Chen, Hun-Chen and Yen, Jui-Cheng and Juan, Jui-Hsiang and Fan, Kuo-Tai
   and Wu, Shu-Meng},
Book-Group-Author = {IEEE},
Title = {A new cryptography system and its IP core design for multimedia
   application},
Booktitle = {2007 IEEE INTERNATIONAL SYMPOSIUM ON CONSUMER ELECTRONICS, VOLS 1 AND 2},
Series = {IEEE International Symposium on Consumer Electronics},
Year = {2007},
Pages = {154-160},
Note = {IEEE International Symposium on Consumer Electronics, Irving, TX, JUN
   20-23, 2007},
Organization = {IEEE},
Abstract = {In this paper, we have proposed a new cryptography system which combines
   both the position permutation and the value transformation encryption
   methods. Three good features involve in this system: (1) High security
   evaluated with the measure of fractal dimension, (2) The content of
   encrypted image is sensitive to the initial key, and (3) This system can
   easily defense against the exhaustive search attack. Besides, for the
   requirement of real-time in multimedia system, we also proposed the high
   performance reconfigurable architecture for this system as well as the
   IP core generator software. The proposed IP core generator can be
   parameterized by the parameters of system-type, packet size, throughput
   and security to create the proper IP core for the applications. All the
   architectures generated from the IP core generator have been verified;
   except for the coding guideline checking, there exist 100\% code
   coverage. According to the UMC 0.18 um cell library, we further verified
   all the configurations of architecture for speed, area and power
   consumption as well as delivering the essential scripts. The
   verifications of all the configurations, the throughput can be ranged
   between 1.59 and 2.25 Gbps 2 with the hardware cost of 0.54 and 3.92
   mm(2). Compared with the existing designs, the proposed design possesses
   performance enough for most of multimedia system applications.},
ISSN = {2158-3994},
ISBN = {978-1-4244-1109-2},
Unique-ID = {WOS:000253442200028},
}

@inproceedings{ WOS:000244782600003,
Author = {De Wolf, Tom and Holvoet, Tom},
Editor = {Brueckner, SA and Hassa, S and Jelasity, M and Yamins, D},
Title = {Design patterns for decentralised coordination in self-organising
   emergent systems},
Booktitle = {ENGINEERING SELF-ORGANISING SYSTEMS},
Series = {Lecture Notes in Artificial Intelligence},
Year = {2007},
Volume = {4335},
Pages = {28+},
Note = {4th International Workshop on Engineering Self-Organising Systems,
   Hakodate, JAPAN, MAY09, 2006},
Abstract = {There is little or no guidance to systematically design a
   self-organising emergent solution that achieves the desired macroscopic
   behaviour. This paper describes decentralised coordination mechanisms
   such as gradient fields as design patterns, similar to patterns used in
   mainstream software engineering. As a consequence, a structured
   consolidation of best practice in using each coordination mechanism
   becomes available to guide engineers in applying them, and to directly
   decide which mechanisms are promising to solve a certain problem. As
   such, self-organising emergent solutions can be engineered more
   systematically, which is illustrated in a packet delivery service
   application.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {978-3-540-69867-8},
ResearcherID-Numbers = {Holvoet, Tom/A-3649-2015},
ORCID-Numbers = {Holvoet, Tom/0000-0003-1304-3467},
Unique-ID = {WOS:000244782600003},
}

@article{ WOS:000252847100005,
Author = {Doerfel, H. and Andrasi, A. and Bailey, M. and Blanchardon, E. and
   Cruz-Suarez, R. and Berkovski, V. and Castellani, C. -M. and Hurtgen, C.
   and LeGuen, B. and Malatova, I. and Marsh, J. and Stather, J. and Zeger,
   J.},
Title = {General guidelines for the assessment of internal dose from monitoring
   data: Progress of the ideas project},
Journal = {RADIATION PROTECTION DOSIMETRY},
Year = {2007},
Volume = {125},
Number = {1-4, SI},
Pages = {19-22},
Note = {European Workshop on Individual Monitoring of Ionising Radiation
   (IM2005), Vienna, AUSTRIA, APR 11-15, 2005},
Organization = {Austrian Res Ctr; European Radiat Dosimetry Grp; THERMO Fischer; GmbH
   Austria \& Toshiba Europe Div Personal Comp},
Abstract = {In recent major international intercomparison exercises on intake and
   internal dose assessments from monitoring data, the results calculated
   by different participants varied significantly. Based on this experience
   the need for harmonisation of the procedures has been formulated within
   an EU 5th Framework Programme research project. The aim of the project,
   IDEAS, is to develop general guidelines for standardising assessments of
   intakes and internal doses. The IDEAS project started in October 2001
   and ended in June 2005. The project is closely related to some goals of
   the work of Committee 2 of the ICRP and since 2003 there has been close
   cooperation between the two groups. To ensure that the guidelines are
   applicable to a wide range of practical situations, the first step was
   to compile a database of well-documented cases of internal
   contamination. In parallel, an improved version of an existing software
   package was developed and distributed to the partners for further use. A
   large number of cases from the database was evaluated independently by
   the partners and the results reviewed. Based on these evaluations,
   guidelines were drafted and discussed with dosimetry professionals from
   around the world by means of a virtual workshop on the Internet early in
   2004. The guidelines have been revised and refined on the basis of the
   experiences and discussions in this virtual workshop. The general
   philosophy of the Guidelines is presented here, focusing on the
   principles of harmonisation, optimisation and proportionality. Finally,
   the proposed Levels of Task to structure the approach of internal dose
   evaluation are reported.},
DOI = {10.1091/rpd/ncl132},
ISSN = {0144-8420},
EISSN = {1742-3406},
Unique-ID = {WOS:000252847100005},
}

@inproceedings{ WOS:000249782700202,
Author = {Han Dan and Xu Xudong and Lv Guanfeng and Zhang Xinfang},
Editor = {Li, M},
Title = {The design and implementation of map projection and coordinate
   conversion software package},
Booktitle = {Advanced Computer Technology, New Education, Proceedings},
Year = {2007},
Pages = {943-946},
Note = {2nd International Conference on Computer Science and Education, Wuhan,
   PEOPLES R CHINA, JUL 25-27, 2007},
Organization = {Zhongnan Univ Econ \& Law; Xiamen Univ; IEEE Control Syst Soc, Singapore
   Chapter; Univ Melbourne; Univ Virginia; Natl Univ Singapore},
Abstract = {The design and implementation of map projection and coordinate
   conversion software package is put forward in this paper. This software
   package is designed with strategy design pattern and implemented with
   standard C++. Therefore, it can be used in multi-platforms and is also
   easier to be developed further and maintained. The software package has
   already been employed in Geography Information System (GIS) we
   self-developed successfully.},
ISBN = {978-7-5615-2825-9},
Unique-ID = {WOS:000249782700202},
}

@inproceedings{ WOS:000252067400168,
Author = {Hristovska, Elizabeta},
Editor = {Katalinic, B},
Title = {Presence on cantilever beam with impact dynamic loadings},
Booktitle = {ANNALS OF DAAAM FOR 2007 \& PROCEEDINGS OF THE 18TH INTERNATIONAL DAAAM
   SYMPOSIUM: INTELLIGENT MANUFACTURING \& AUTOMATION: FOCUS ON CREATIVITY,
   RESPONSIBILITY, AND ETHICS OF ENGINEERS},
Year = {2007},
Pages = {335-336},
Note = {18th International Symposium of the
   Danube-Adria-Association-for-Automation-and-Manufacturing, Zadar,
   CROATIA, OCT 24-27, 2007},
Organization = {Danube Adria Assoc Automat \& Mfg Int Vienna; Univ Zadar; Vienna Univ
   Technol; Univ Appl Sci Technikum; Austrian Sco Engineers \& Architects},
Abstract = {In this paper the state on the carrying structure of the working wheel
   at rotating excavator SRs-630 for dynamic loadings is analyzed. Dynamic
   loadings on rotating excavators are random with the possibility for
   improvident impact loadings. With theoretical analysis, variable dynamic
   loadings are approximate with dimension on loading which is constant for
   determined working regimes, by methodology with which whole exploitation
   life of construction stays equal. Dynamic loadings on this carrying
   structure are analyzed for characteristically working regimes. For this
   propose regimes is defined as a deformity state of carrying structure
   and its clamp dogs, with use of software package. Results of this
   research are presented in this paper.},
ISBN = {978-3-90150-958-2},
Unique-ID = {WOS:000252067400168},
}

@inproceedings{ WOS:000247397200097,
Author = {Kootsey, J. Mailen},
Editor = {ALDabass, D and Zobel, R and Abraham, A and Turner, S},
Title = {Interactive simulation in web pages: A system for rapid development},
Booktitle = {AMS 2007: FIRST ASIA INTERNATIONAL CONFERENCE ON MODELLING \& SIMULATION
   ASIA MODELLING SYMPOSIUM, PROCEEDINGS},
Year = {2007},
Pages = {556-561},
Note = {1st Asia International Conference on Modelling and Simulation, Prince
   Songkia Univ, Phuket, THAILAND, MAR 27-30, 2007},
Organization = {IEEE Networking World; Nottingham Trent Univ; UKSim; IEEE Comp Soc;
   IEEE; CPS},
Abstract = {This paper describes a software system for rapid, coding-free
   development of interactive simulations in Web pages. The core of the
   system is a library of reusable objects based on the NumberLinX (NLX)
   architecture: GUI objects for user-controlled inputs, output displays,
   and controls, along with calculation objects and a manager linking all
   the objects. The development system included an NLX library, an
   application program for entering the model description, and integration
   of the NLX objects into a commercial Web design program for page
   construction.},
ISBN = {978-1-4244-0325-7},
Unique-ID = {WOS:000247397200097},
}

@inproceedings{ WOS:000253090500077,
Author = {Kuo, Wei-Shen and Tzeng, Yuan Lin and Chen, Eason and Lai, Jeng Yuan and
   Wang, Yu Po and Hsiao, C. S.},
Book-Group-Author = {IEEE},
Title = {POP package (Cavity BGA) warpage improvement and stress characteristic
   analyses},
Booktitle = {2007 INTERNATIONAL MICROSYSTEMS, PACKAGING, ASSEMBLY AND CIRCUITS
   TECHNOLOGY CONFERENCE, PROCEEDINGS OF TECHNICAL PAPERS},
Year = {2007},
Pages = {342-345},
Note = {International Conference on Microsystems, Packaging, Assembly and
   Circuits Technology, Taipei, TAIWAN, OCT 01-03, 2007},
Organization = {Ind Technol Res Inst; IEEE; IEEE Components, Packaging \& Manufacturing
   Technol Soc; IMAPS; Taiwan Printed Circuit Assoc},
Abstract = {In recent years, miniaturization, lightening, high performance, high
   reliability and low cost have been demanded intensely for electronic
   products, especially in the rapid growth of portable cell phone domain.
   Furthermore, multiple functional demand induces advanced package
   developments, such as system-on-chip (SoC) and system-in-package (SiP).
   System-on-chip (SoC) is an ideal package to integrate multiple
   functionalities in the chip level. However, the design and testing are
   very difficult, high cost and low manufacturing yield, these reasons
   drives multiple functional integration technology toward
   system-in-package (SiP) development gradually.
   Package-on-package (PoP) is a multi-package stacking structure of the
   TFBGA onto a BGA top that aims to reduce the placement and routing areas
   on board. This paper would introduce a new BGA structure with cavity for
   die placement and encapsulated by glob top as below illustration (figure
   1). Unlike the regular PBGA as POP bottom package with the constraint of
   solder ball height, the package Cavity BGA would be more flexible The
   Cavity BGA package has some other benefits such as the use of glob top
   to get rid of spending specific molding compound tooling cost and less
   warpage issue. The warpage performance and glob top material properties
   are closely related. The warpage Improvement criteria for glob top
   material properties are concerned and provided in the paper. This study
   is to investigate the Cavity BGA packages warpage performance
   improvement and stress characteristic analyses as various liquid
   compound materials.
   The 3D finite element models using were commercial software ANSYS 9.0
   constructed to analyze the warpage and stresses of the Cavity BGA
   packages. For bottom war-page simulation, 15x15 mm(2) Cavity BGA with
   die size of 9.9x9.6mm(2) and thickness of 75 mu m are utilized and the
   thermal loading was performed from 175 to 25. Four kinds of glob top
   materials were utilized in bottom war-page simulation. The warpage
   performance of the package would be decided by glob top material
   properties such as glass transition temperature (Tg), coefficient of
   thermal expansion (CTE), Young's modulus (E). Besides, the geometry
   dimension designs of package structure would have effects on warpage
   performance. They include die size, die thickness, core thickness,
   solder mask thickness, glob top thickness, and number of substrate. The
   foregoing various parameter effects on bottom warpage would have
   discussion in the paper. For Stress Characteristic issue, ball stress
   comparison with various TFBGA die size and TFBGA die stacking number are
   contained.
   The studies conclude that the Cavity BGA with liquid compound (higher
   Tg=135) generates the smallest bottom warpage (6.5mil), since higher Tg
   causes smaller warpage. When the core thickness of Cavity BGA increases
   from 60 mu m to 100 mu m, the bottom warpage improves 10\%. The Cavity
   BGA with 6 layers substrate generates bottom warpage 25\% smaller than
   that with 4 layers. For warpage improvement of glob top properties, the
   simulation analyses demonstrated the high Tg is prefered to acquire the
   better warpage performance. The properties beyond Tg effect on warpage
   performance can be neglected within a specific range adjustment. The
   further direction would recommend with (1) higher CTE before Tg, (2)
   higher Young's modulus before Tg, (3) higher Tg. Also, the positions of
   the ball stress concentration and higher ball crack risk will be
   presented and discussed in the paper. The influences of the structure
   designs and material property options on the warpage behavior are
   investigated to establish useful design guidelines.},
DOI = {10.1109/IMPACT.2007.4433632},
ISBN = {978-1-4244-1636-3},
Unique-ID = {WOS:000253090500077},
}

@inproceedings{ WOS:000256071800020,
Author = {LeBaron, Todd and Jacobsen, Craig},
Book-Group-Author = {IEEE},
Title = {The simulation power of AutoMod},
Booktitle = {PROCEEDINGS OF THE 2007 WINTER SIMULATION CONFERENCE, VOLS 1-5},
Year = {2007},
Pages = {198-206},
Note = {2007 Winter Simulation Conference, Washington, DC, DEC 09-12, 2007},
Abstract = {Decision making in industry continues to become more complicated.
   Customers are more demanding, competition is more fierce, and costs for
   labor and raw materials continue to rise. Managers need state-of-the-art
   tools to help in planning, design, and operations of their facilities.
   Simulation provides a virtual factory where ideas can be tested and
   performance improved. The AutoMod product suite from Applied Materials
   has been used on thousands of projects to help engineers and managers
   make the best decisions possible. AutoMod supports hierarchical model
   construction. This architecture allows users to reuse model objects in
   other models, decreasing the time required to build a model. In
   addition, recent enhancements to Automod's material handling template
   systems have increased modeling accuracy and ease-of-use. These latest
   advances have helped make AutoMod one of the most widely used simulation
   software packages.},
ISBN = {978-1-4244-1305-8},
Unique-ID = {WOS:000256071800020},
}

@article{ WOS:000207064900001,
Author = {MacCarthy, Ivor A. J.},
Title = {The South Munster Basin of southwest Ireland},
Journal = {JOURNAL OF MAPS},
Year = {2007},
Pages = {149-172},
Abstract = {This paper presents a geological map at a scale of 1:75,000 of an area
   of about 3,500km(2) of the western part of the Devonian and
   Carboniferous South Munster Basin of southern Ireland. The compilation
   utilised existing geological data from a variety of sources in
   association with recent mapping. Aerial photographic data were used
   extensively to identify structural features not previously mapped.
   Geological and selected topographical information were scanned to a
   computer and the final compilation map was digitised using Macromedia
   Freehand software at a scale of 1:3,125. The map was subsequently
   converted to a scale of about 1:75,000.
   The construction of the map has aided in the resolution of the
   stratigraphical framework for this part of the basin and has permitted a
   basin wide correlation of genetic depositional packages recognised in
   the eastern part of the basin. Some of these have been linked to a
   number of transgressive-regressive events during the Late Devonian and
   Early Carboniferous and they are correlated here into the map area.
   Refined information on basin architecture in association with
   palaeoenvironmental interpretations for the succession have been used to
   identify the principal sedimentation controlling structures in the basin
   and to assess their influence in space and time.},
DOI = {10.4113/jom.2007.79},
ISSN = {1744-5647},
Unique-ID = {WOS:000207064900001},
}

@inproceedings{ WOS:000252738800026,
Author = {Noro, Masami and Sawada, Atsushi and Hachisu, Yoshinari and Banno,
   Masahide},
Book-Group-Author = {IEEE Computer Soc},
Title = {E-AoSAS plus plus and its Software Development Environment},
Booktitle = {14TH ASIA-PACIFIC SOFTWARE ENGINEERING CONFERENCE, PROCEEDINGS},
Year = {2007},
Pages = {206+},
Note = {14th Asia-Pacific Software Engineering Conference, Nagoya, JAPAN, DEC
   04-07, 2007},
Organization = {Informat Proc Soc Japan, SIGSE},
Abstract = {E-AoSAS++ is an aspect-oriented software architecture style for embedded
   software. It basically gives the style in which a set of state
   transition machines organizes a software. We have identified such
   concerns as state transition, concurrency, fault-tolerance, real-time,
   and error-handling. We categorize those concerns into two classes and
   found the way we call universal modularization pattern to package them
   in an orderly fashion. From the experience we have had through the
   construction of E-AoSAS++, we realized that we needed model for style
   construction. We defined XCC model which is construction model of
   architecture style. Based on E-AoSAS++, architecture centered software
   development environment is designed.},
ISBN = {978-0-7695-3057-4},
Unique-ID = {WOS:000252738800026},
}

@incollection{ WOS:000244890300010,
Author = {Robertson, Charles E.},
Editor = {McIntosh, JR},
Title = {Electron microscopy of Archaea},
Booktitle = {CELLULAR ELECTRON MICROSCOPY},
Series = {Methods in Cell Biology},
Year = {2007},
Volume = {79},
Pages = {169-191},
Abstract = {Archaea are the recently discovered third ``domain{''} of life.
   Previously, the two known domains were Eucarya and Bacteria. When
   Archaea were first discovered, they were grouped with Bacteria because
   both kinds of organisms lacked nuclei. With the advent of efficient DNA
   sequencing, however, the genomes of Eucarya, Archaea, and Bacteria have
   shown that the information-processing systems (molecules involved in DNA
   replication, transcription, and translation) of Archaea are surprisingly
   similar to, although much simpler than, the information-processing
   systems in Eucarya. Therefore, Archaea are an interesting model system
   in which to study the basic elements of information-processing in two of
   the three existent domains of life. The small size of Archaea (2-5 mu m)
   precludes high-resolution structural studies by light microscopy.
   Electron tomography allows the creation of full-cell models of Archaea
   with a resolution of similar to 10 nm. While electron tomography of
   frozen, hydrated specimens offers great potential for higher resolution
   models, high-pressure-frozen, freeze-substitution-fixed,
   plastic-embedded, and heavy metal stained specimens are at present the
   most reliable samples for informative studies of whole archaeal cell
   structure. Construction of such full-cell models has been difficult in
   the past due to the labor-intensive nature of the image segmentation
   process. Software-assisted segmentation with application-specific
   scripts, implemented with software packages such as MATLAB,
   substantially reduces the manual labor required to segment full-cell
   models, opening the door to routine creation of models that represent
   whole Archaeons in a variety of metabolic and genetic states.},
DOI = {10.1016/S0091-679X(06)79007-0},
ISSN = {0091-679X},
ISBN = {978-0-12-370647-8},
ORCID-Numbers = {ROBERTSON, CHARLES/0000-0002-4136-4121},
Unique-ID = {WOS:000244890300010},
}

@inproceedings{ WOS:000254290000025,
Author = {Royer, Jr., R. L. and Zhao, X. and Owens, S. E. and Rose, J. L.},
Editor = {Chang, FK},
Title = {Large area corrosion detection in complex aircraft components using lamb
   wave tomography},
Booktitle = {STRUCTURAL HEALTH MONITORING 2007: QUANTIFICATION, VALIDATION, AND
   IMPLEMENTATION, VOLS 1 AND 2},
Series = {Structural Health Monitoring},
Year = {2007},
Pages = {238-246},
Note = {6th International Workshop on Structural Health Monitoring, Stanford
   Univ, Stanford, CA, SEP 11-13, 2007},
Organization = {AF Off Sci Res; Army Res Off; Natl Sci Fdn; Off Naval Res},
Abstract = {An ultrasonic guided wave computed tomography (CT) technique has been
   developed to monitor corrosion in complex aircraft components. The
   technique has shown excellent corrosion detection sensitivity when
   applied to large aluminum structures. The resulting CT images are
   capable of determining damage size, location, and severity. The
   technique has also demonstrated the ability to detect and discriminate
   between multiple damaged regions in localized areas. An overview of the
   technology is as follows:
   A practical sparse guided wave sensor array is strategically positioned
   and embedded on the structure.
   Reference guided wave data is acquired by transmitting and receiving
   guided waves with every possible sensor combination in the array.
   Guided wave data is reacquired at predetermined time intervals.
   CT images are constructed using different guided wave features of the
   reference data in comparison to the reacquired data.
   Damage location, area, and severity are accurately mapped in the CT
   image.
   The technology is applicable to a wide variety of composite materials
   and all metallic structures.
   The sparse array can be of any geometry. Novel sensors have been
   developed for activating and receiving the guided waves. The sensors are
   light-weight, small in size, and uniquely packaged for robustness. The
   sensors can be installed individually or in prepackaged strips. Several
   CT algorithms have been developed that take advantage of different
   guided wave features for image construction. Selection of the proper CT
   algorithm has shown the ability to increase damage detection probability
   and decrease false defect calls. Customized software has been developed
   to allow fast and simple data acquisition and image construction and
   analysis. The software allows the user to define the geometry of the
   array, perform diagnostics to assure that the sensors are working
   properly, and also to recall previous data and CT images. A light-weight
   portable data acquisition (DAQ) system has been developed, using
   customized hardware, to allow easy field inspection. The DAQ hardware
   can be carried in a shoulder strap cast: and controlled via a tablet PC.},
ISBN = {978-1-932078-71-8},
Unique-ID = {WOS:000254290000025},
}

@article{ WOS:000241640900004,
Author = {Sarakinos, Sotirios S. and Valakos, Ioannis M. and Nikolos, Ioannis K.},
Title = {A software tool for generic parameterized aircraft design},
Journal = {ADVANCES IN ENGINEERING SOFTWARE},
Year = {2007},
Volume = {38},
Number = {1},
Pages = {39-49},
Month = {JAN},
Abstract = {In this work a surface generation software named Ge.P.A.S. (for generic
   parameterized aircraft surface) is presented, designed for the
   construction of aircraft aerodynamic surfaces. The surface generation
   procedure is parameterized and different aircraft configurations can be
   produced in an interactive way. A hierarchical structure of geometric
   parameters was adopted, resulting in easier manipulation of the shape
   and a scalable number of control parameters. Additionally, the geometric
   parameters may serve as design optimization variables in cooperation
   with an external optimizer. The surface generation is based on the use
   of NURBS curves and surfaces, which provide the ability to produce
   complicated geometries with a relative small number of design variables.
   Standard or user-defined airfoil sections can be used for the wing
   generation. The surface description is compatible with international
   input/output standards; IGES and STEP formats are supported for the
   output files. Consequently, Ge.P.A.S. can serve as a preprocessor for
   other software packages, which may be used in order to refine the
   geometry or to generate the grid for numerical simulations. The
   geometric algorithms, the software features and its basic
   characteristics are presented in this paper, along with a demonstration
   of its abilities in sample aircraft configurations. (C) 2006 Elsevier
   Ltd. All rights reserved.},
DOI = {10.1016/j.advengsoft.2006.06.001},
ISSN = {0965-9978},
ORCID-Numbers = {Nikolos, Ioannis/0000-0002-0675-5880},
Unique-ID = {WOS:000241640900004},
}

@inproceedings{ WOS:000256526301088,
Author = {Sinha, P. and Stiehl, K. R. and Huo, E. S. and Yebode, O. A. and Dokov,
   R. P. and Chin, S. J. and Price, R. E. and Larson, R. W.},
Book-Group-Author = {IEEE},
Title = {Design of a modular, compact, multi-role remotely operated vehicle for
   sheltered water operations},
Booktitle = {2007 OCEANS, VOLS 1-5},
Series = {OCEANS-IEEE},
Year = {2007},
Pages = {1844-1850},
Note = {2007 OCEANS Conference, Vancouver, CANADA, SEP 29-OCT 04, 2007},
Organization = {MTS; IEEE; Canada; Oceanworks; ISE Grp Co; Sun Star Elect L P;
   KONGSBERG; IMAGENEX TECHNOL CORP; ONR; OPSI},
Abstract = {Since its founding in 2003, the MIT ROV Team has been participating in
   the Marine Advanced Technology Education (MATE) Center's International
   ROV Competition. This year's challenges, in recognition of the
   International Polar Year, include operations in environments made to
   resemble those found in the Polar Regions. This includes working in
   currents, under ice sheets and in simulated open ocean environments. The
   three missions, each approximately 15 minutes long and in depths of up
   to 20 feet, include recovery and deployment of science packages and
   related support structures, the collection of faunal samples and repair
   and maintenance work at stations under water. In keeping with its vision
   to look beyond the competition and push the envelope, the AM ROV Team
   built on the lessons learnt during the design, construction and
   operation of its fourth generation ROV, MTHR(1), to design MIT ROV 5.0,
   a compact, powerful, highly maneuverable and modular robot capable of
   not only participating in the competition, but also performing a variety
   of tasks in the open environment.
   MIT ROV 5.0 was primarily designed to be an exploration robot meant to
   operate in sheltered waters, that is, with currents below one knot, lack
   of powerful localized turbulent flow and the absence of highly corrosive
   materials. The major design requirements included ease of operation,
   including setup, maintenance, dives and recovery, modularity, to allow
   quick reconfiguration to suit a variety of missions, robustness, so that
   it would stand up to the rigors of the open environment, small size, for
   greater maneuverability and low cost, for ease of reproduction. We came
   up with a robot that was slightly bigger than MTHR, with two modular
   payload bays with standard connectors that could accommodate
   mission-specific packages. The control box contains custom designed
   PC-104 size circuit boards that can support a number of different
   actuators and thrusters and adequate space for NiMH or LiON battery
   packs, which can both be used. Each board can be switched easily due to
   extensive use of standard sizes and connection interfaces. The frame
   uses high strength ABS plastic side plates with LEXAN cross-struts. The
   tether is a single strand Kevlar-jacketed fiber-optic cable with a
   Kevlar support string for added security. This tether can be replaced by
   a regular Ethernet or CAT-V cable is the need arises. Thus the vehicle
   is truly modular in every way.
   The MIT ROV 5.0 system as designed is very easy to use. The control
   software can be run off virtually any computer, using the keyboard or a
   standard USB joystick The vehicle itself connects to the computer using
   a USB-serial interface. The entire system can be setup and ready to go
   in under five minutes. An on-board guidance system consisting of an
   inertial navigation unit and a magnetic compass provide heading,
   attitude and position data, as well as performing station-keeping
   functions, allowing effective operation in currents and low-visibility
   conditions.
   MIT ROV 5.0 can also carry several different types of sensors, such as
   temperature, pressure and salinity, not to mention a combination of
   infra-red and color cameras. Payloads currently being prototyped include
   precision deployment and recovery modules, a sample collection package,
   instrument bays and an articulated manipulator arm. This capability
   coupled with a production cost as low as \$3,000 makes MIT ROV 5.0 an
   extremely valuable platform for exploration, research, education,
   environmental monitoring and small scale repair and recovery work.},
ISSN = {0197-7385},
ISBN = {978-0-933957-37-4},
Unique-ID = {WOS:000256526301088},
}

@inproceedings{ WOS:000249782700239,
Author = {Wang Jingzhuo and Liu Fengling},
Editor = {Li, M},
Title = {Construction and application on network teaching resources for
   information technology basic course group},
Booktitle = {Advanced Computer Technology, New Education, Proceedings},
Year = {2007},
Pages = {1126-1130},
Note = {2nd International Conference on Computer Science and Education, Wuhan,
   PEOPLES R CHINA, JUL 25-27, 2007},
Organization = {Zhongnan Univ Econ \& Law; Xiamen Univ; IEEE Control Syst Soc, Singapore
   Chapter; Univ Melbourne; Univ Virginia; Natl Univ Singapore},
Abstract = {Information technology basic course group is an important fundamental
   course group for the undergraduate students major in electric and
   information. In the construction process of the course group, the
   cultivating scheme of the Department of Electric Engineering was
   investigated sufficiently. The course group integrated C Program Design,
   Data Structure, System Component Principle, Database System Conspectus
   and Computer Network courses. It enhances the basic teaching, emphasizes
   the context among courses and focuses on the combination of theory and
   practice. According to the rules of integral layout, objective
   management, step implements, subentry construction, the serial and
   modular construction was taken into action. Therefore, the formal
   network teaching resources came into being, the test papers library was
   built. At the same time, the software used for editing test papers and
   creating test papers automatically was developed. During the teaching
   process, teachers in our group all pay attentions to the diversification
   of teaching method and teaching means and make good use of the network
   teaching resources. The teaching reform was deepened and the teaching
   effect was excellent.},
ISBN = {978-7-5615-2825-9},
Unique-ID = {WOS:000249782700239},
}

@inproceedings{ WOS:000254287400067,
Author = {Wark, Christopher},
Book-Group-Author = {ASME},
Title = {Natural ventilation design using CFD},
Booktitle = {PROCEEDINGS OF THE ENERGY SUSTAINABILITY CONFERENCE 2007},
Year = {2007},
Pages = {581-595},
Note = {ASME Energy Sustainability Conference, Long Beach, CA, JUN 27-30, 2007},
Organization = {ASME, Adv Energy Syst; ASME, Solar Energy Div},
Abstract = {In an effort to make buildings healthier and more energy efficient,
   architects are increasingly incorporating natural ventilation into their
   design strategies in order to take advantage of free, available wind
   power. The extent to which natural ventilation can replace forced
   ventilation in a given building depends on the local climate and
   specific site utilization. The ASHRAE Standards 55 and 62.1 that cover
   natural ventilation establish minimal requirements for climate and
   building openings but also concede that the ultimate responsibility for
   proving the effectiveness of this technique lies with the design team
   and the specific requirements of local codes. But how does a design team
   prove that air is flowing according to plan without actually creating
   the structure and taking measurements? Only two possibilities exist -
   regard each room as a very large ratio conduit and apply conventional
   equations to those spaces, or do a 3-dimensional numerical analysis of
   the flow path.
   Numerical analysis, known as Computational Fluid Dynamics (CFD), is now
   being recognized as the only reliable way to predict natural airflow
   through a building and assure that adequate air quality and comfort is
   provided at all points of each room before construction begins. CFD
   computer programs allow designers to divide a volume into a large number
   of small regions and calculate the air and heat transfer between each
   region, minimizing the assumption-related errors that would otherwise
   occur. Minimizing computational error at the beginning of the design
   process reduces the risk of costly post-construction order changes that
   can occur as substandard air quality is discovered.
   CFD software can vary in its level of sophistication. While the most
   basic Navier-Stokes heat and mass transfer equations are essential and
   can be of great use, a proper natural ventilation analysis tool should
   include calculations for buoyancy, turbulent convection, and the ability
   to do open boundary modeling. Other features such as local solar loading
   and transient analysis are also desirable. A comprehensive CFD package
   can be particularly useful for modeling the complex airflow found in
   mixed-mode designs and identifying regions of stagnant air, high heat
   loss or gain, short-circuited airflow, and other conditions that inhibit
   good building performance and limit the potential for sustainability.},
ISBN = {978-0-7918-4797-8},
Unique-ID = {WOS:000254287400067},
}

@inproceedings{ WOS:000261875300046,
Author = {Zhan, Jianfeng and Wang, Lei and Tu, Bibo and Zhang, Zhihong and Wen, Yu
   and Chen, Yuansheng and Zhou, Wei and Meng, Dan and Sun, Ninghui},
Book-Group-Author = {IEEE},
Title = {A Layered Design Methodology of Cluster System Stack},
Booktitle = {2007 IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING},
Year = {2007},
Pages = {404-409},
Note = {IEEE International Conference on Cluster Computing, Austin, TX, SEP
   17-20, 2007-2008},
Organization = {IEEE},
Abstract = {The application range of cluster has expanded beyond scientific
   computing, but the present cluster system software fails to provide a
   flexible architecture to promote code reuse and facilitate building
   cluster system software for different computing contexts, most of which
   are developed from scratch case by case, or integrated or packaged with
   ``the best practice{''}. In this paper, we have proposed a layered
   design methodology to build cluster system stack with different layers
   concentrating on different functions, and developed common sets of core
   service as reusing framework for different computing context. Following
   this methodology, we have built Phoenix-a complete cluster system stack
   for both scientific and business computing, which is verified and
   deployed on Dawning 4000A super computer for scientific computing and
   other cluster systems for business computing. The qualitative evaluation
   and our practices show the design methodology of Phoenix has advantages
   over other methodologies{*}.},
DOI = {10.1109/CLUSTR.2007.4629256},
ISBN = {978-1-4244-1387-4},
ResearcherID-Numbers = {Sun, Ning/HLX-6289-2023},
Unique-ID = {WOS:000261875300046},
}

@article{ WOS:000241177900005,
Author = {Ljungberg, Malin and Otto, Kurt and Thune, Michael},
Title = {Design and usability of a PDE solver framework for curvilinear
   coordinates},
Journal = {ADVANCES IN ENGINEERING SOFTWARE},
Year = {2006},
Volume = {37},
Number = {12},
Pages = {814-825},
Month = {DEC},
Abstract = {An object-oriented PDE solver framework is a library of software
   components for numerical solution of partial differential equations,
   where each component is an object or a group of objects. Given such a
   framework, the construction of a particular PDE solver consists in
   selecting and combining suitable components. The present paper is
   focused on TENGO {[}Ahlander K, Otto K. Software design for finite
   difference schemes based on index notation. Future Generation Comput
   Syst 2006;22:102-9], an object-oriented PDE solver framework for finite
   difference methods on structured grids, using tensor abstractions for
   convenient representation of numerical operators.
   Here, the design Of TENGO is extended to address curvilinear
   coordinates. These extensions to the TENGO object model are the result
   of applying object-oriented analysis and design combined with feature
   modeling. The framework was implemented in Fortran 90/95, using standard
   techniques for emulating object-oriented constructs in that language.
   The new parts of the framework were assessed with respect to programming
   effort and execution time. It is shown that the programming effort
   required for construction and modification of PDE solvers on curvilinear
   grids is significantly reduced through the introduction of the new
   framework components. Moreover, for the test case of an underwater
   acoustics computation, there was no significant difference in execution
   time between the framework based code and a special purpose Fortran 90
   code for the same application. (C) 2006 Elsevier Ltd. All rights
   reserved.},
DOI = {10.1016/j.advengsoft.2006.04.005},
ISSN = {0965-9978},
EISSN = {1873-5339},
Unique-ID = {WOS:000241177900005},
}

@article{ WOS:000242355500013,
Author = {Nikkel, Bruce J.},
Title = {A portable network forensic evidence collector},
Journal = {DIGITAL INVESTIGATION},
Year = {2006},
Volume = {3},
Number = {3},
Pages = {127-135},
Month = {SEP},
Abstract = {A small portable network forensic evidence collection device is
   presented which is built using inexpensive embedded hardware and open
   source software. The device offers several modes of operation for
   different live network evidence collection scenarios involving single
   network nodes. This includes the use of promiscuous packet capturing to
   enhance evidence collection from remote network sources, such as
   websites or other remote services. It operates at the link layer
   allowing the device to be transparently inserted inline between a
   network node and the rest of a network. It is simple to deploy,
   requiring no reconfiguration of the node or surrounding network
   infrastructure. The device can be preconfigured in the forensics lab,
   and deployment delegated to staff not specifically trained in forensics.
   Details of the architecture, construction and operation are described.
   Special attention is given to information security aspects of live
   network evidence collection. (c) 2006 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/j.diin.2006.08.012},
ISSN = {1742-2876},
Unique-ID = {WOS:000242355500013},
}

@article{ WOS:000242326100010,
Author = {Renault, Ludovic and Chou, Hui-Ting and Chiu, Po-Lin and Hill, Rena M.
   and Zeng, Xiangyan and Gipson, Bryant and Zhang, Zi Yan and Cheng, Anchi
   and Unger, Vinzenz and Stahlberg, Henning},
Title = {Milestones in electron crystallography},
Journal = {JOURNAL OF COMPUTER-AIDED MOLECULAR DESIGN},
Year = {2006},
Volume = {20},
Number = {7-8},
Pages = {519-527},
Month = {AUG},
Abstract = {Electron crystallography determines the structure of membrane embedded
   proteins in the two-dimensionally crystallized state by
   cryo-transmission electron microscopy imaging and computer structure
   reconstruction. Milestones on the path to the structure are high-level
   expression, purification of functional protein, reconstitution into
   two-dimensional lipid membrane crystals, high-resolution imaging, and
   structure determination by computer image processing. Here we review the
   current state of these methods. We also created an Internet information
   exchange platform for electron crystallography, where guidelines for
   imaging and data processing method are maintained. The server
   (http://2dx.org) provides the electron crystallography community with a
   central information exchange platform, which is structured in blog and
   Wiki form, allowing visitors to add comments or discussions. It
   currently offers a detailed step-by-step introduction to image
   processing with the MRC software program. The server is also a
   repository for the 2dx software package, a user-friendly image
   processing system for 2D membrane protein crystals.},
DOI = {10.1007/s10822-006-9075-x},
ISSN = {0920-654X},
EISSN = {1573-4951},
ResearcherID-Numbers = {Chiu, Po-Lin/GWV-6758-2022
   Chiu, Po-Lin/D-2762-2013
   Chiu, Po-Lin/ABB-7848-2021
   Stahlberg, Henning/H-1868-2011
   },
ORCID-Numbers = {Chiu, Po-Lin/0000-0001-8608-7650
   Chiu, Po-Lin/0000-0001-8608-7650
   Stahlberg, Henning/0000-0002-1185-4592
   Renault, Ludovic/0000-0001-6464-8808},
Unique-ID = {WOS:000242326100010},
}

@article{ WOS:000245186300011,
Author = {Arslan, Musa Hakan and Gulay, Fatma Gulten},
Title = {A comparative study on strengthening applications of prefabricated
   buildings in earthquake regions},
Journal = {ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING},
Year = {2006},
Volume = {31},
Number = {1C},
Pages = {151-167},
Month = {JUN},
Abstract = {Damages to prefabricated industrial structures caused by earthquakes in
   the last ten years in Turkey have occurred to an enormous extent. Among
   the causes of this damage, the inherent characteristic weakness of the
   load-carrying systems and constructing most of them according to the
   earthquake code applied before 1998 can be listed. The tragic situation
   that appeared after earthquakes emphasized that existing prefabricated
   constructions should be strengthened/improved according to the code
   which became valid after 1998.
   This study comprises three sections. The damage types in prefabricated
   reinforced concrete structures caused by earthquakes and the
   strengthening models (samples) that can be applied to these structures
   will be mentioned in the first and second sections, respectively. The
   last and analytical part of the study will be about the selection of the
   most often damaged industrial construction type which is preferred in
   industrial regions and the application of selecting strengthening
   methods performed by modeling these structures according to three
   different span types (1-S, 2-S, 3-S). The strengthening methods will be
   Shear Wall (SW), Steel Bracing (B), Steel Beam (SB) and Column Jacketing
   (CJ). All the systems will be calculated by using non-linear procedures
   (pushover analysis). The analysis performed by using a Software Package
   (DRAIN-2DX), will be applied to each frame in order to obtain
   load-displacement curves. The real hinge parameters will be defined
   using FEMA-356 to reflect the existing conditions. The results will be
   compared by considering the contribution of strengthening and especially
   system performance to displacement ductility.},
ISSN = {2193-567X},
EISSN = {2191-4281},
Unique-ID = {WOS:000245186300011},
}

@article{ WOS:000236644400004,
Author = {Kettil, P and Wiberg, NE},
Title = {Development of software for structural analysis adapted to construction
   engineering applications},
Journal = {ENGINEERING WITH COMPUTERS},
Year = {2006},
Volume = {21},
Number = {3},
Pages = {225-236},
Month = {MAY},
Abstract = {The paper presents the development and application of software for
   structural analysis adapted to construction engineering applications.
   The software consists of data structures, application subroutine
   libraries and the built-in Matlab subroutine library. For each
   application, the user writes an application-specific main program using
   the data structures and routines in the subroutine libraries. This gives
   the user flexibility to build a main program that is adapted to the
   specific needs of the current application. The software has been tested
   in a number of real-world projects, e.g. tunnels and bridges. The
   examples show that the software is a useful design tool for design work.},
DOI = {10.1007/s00366-005-0007-5},
ISSN = {0177-0667},
Unique-ID = {WOS:000236644400004},
}

@article{ WOS:000236564600017,
Author = {Achasov, M and Bogdanchikov, A and Kim, A and Korol, A},
Title = {DAQ software for SND detector},
Journal = {NUCLEAR INSTRUMENTS \& METHODS IN PHYSICS RESEARCH SECTION
   A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
Year = {2006},
Volume = {559},
Number = {1},
Pages = {71-75},
Month = {APR 1},
Note = {10th International Workshop on Advanced Computing and Analysis
   Techniques in Physics Research, Zeuthen, GERMANY, MAY 22-27, 2005},
Organization = {DESY},
Abstract = {The data acquisition system software for new experiments oil the SND
   detector, its architecture and overview of its distinctive features are
   presented. Deep buffering of readout events gives an independence of
   data reading from their processing. Computer farm for events processing
   and selection is implemented in Such a way to allow linear scaling of
   the computing power. The operator interface is implemented with Web
   technologies. State machine, process starter, process control and
   recovery services are designed to control system processes. The system
   configuration and data taking conditions are stored in the relational
   (SQL) database. The database access is implemented through
   object-oriented API designed for this project. Events processing and
   selection modules are embedded into the highly configurable software
   framework. Presented DAQ software provides high level of robustness,
   flexibility and scalability. (c) 2005 Elsevier B.V. All rights reserved.},
DOI = {10.1016/j.nima.2005.11.105},
ISSN = {0168-9002},
EISSN = {1872-9576},
ResearcherID-Numbers = {Achasov, Mikhail/AAB-8105-2022
   Bogdanchikov, Alexander/AAB-9414-2022
   Korol, Aleksandr/A-6244-2014},
ORCID-Numbers = {Korol, Aleksandr/0000-0001-8448-218X},
Unique-ID = {WOS:000236564600017},
}

@article{ WOS:000208505600003,
Author = {Aziz, Ahmed M. Abdel and Russell, Alan D.},
Title = {Generalized Economic Modeling for Infrastructure and Capital Investment
   Projects},
Journal = {JOURNAL OF INFRASTRUCTURE SYSTEMS},
Year = {2006},
Volume = {12},
Number = {1},
Pages = {18-32},
Month = {MAR},
Abstract = {Economic modeling and risk analysis are important processes for the
   appraisal of infrastructure and revenue-generating projects such as
   build-operate-transfer (BOT) projects. These processes have been
   commonly implemented using spreadsheets in which the analyst would build
   several models to analyze a project under varying conditions and risk
   assumptions. For better efficiencies in building economic structures and
   evaluation of projects, the current paper defines ``classifications{''}
   of estimating and cash flow methods, and develops a generalized model. A
   classification represents a particular domain-construction, revenues,
   financing, operation and maintenance, or risk analysis, for example-and
   holds the estimating methods of that domain. The basic building block
   behind the model structure is a work package/stream that would have its
   own properties and estimating methods by direct selection from the
   relevant classification. By integrating the building blocks together a
   project economic structure is built and various performance measures are
   formulated. The model was implemented in a prototype software system
   called Evaluator. A BOT highway project is used to show an application
   of the concepts and the generalized model.},
DOI = {10.1061/(ASCE)1076-0342(2006)12:1(18)},
ISSN = {1076-0342},
EISSN = {1943-555X},
ORCID-Numbers = {Abdel Aziz, Ahmed/0000-0002-3487-8992},
Unique-ID = {WOS:000208505600003},
}

@inproceedings{ WOS:000238056100035,
Author = {Attilio, Fiandrotti and Di Nunzio, Pierluigi and Di Gregorio, Federico
   and Meo, Angelo Raffaele},
Editor = {Damiani, E and Fitzgerald, B and Scacchi, W and Scotto, M and Succi, G},
Title = {A graphical installation system for the GNU/Linux Debian distribution},
Booktitle = {OPEN SOURCE SYSTEMS},
Series = {International Federation for Information Processing},
Year = {2006},
Volume = {203},
Pages = {337+},
Note = {International Conference on Open Software (OSS2006), Como, ITALY, JUN
   08-10, 2006},
Organization = {Int Federat Informat Proc, TC2 WG 2 13; COCOS; Gruppo Engn; AICA},
Abstract = {One of the main objectives of the Centro di Competenza sul Software
   Libero del Politecnico di Torino is to provide custom GNU/Linux
   distribution to the Public Administration, small and medium enterprise
   and schools. Debian GNU/Linux was choosen as the base for the custom
   distributions because of its strong support of free software and its
   long-standing technical merits: minimalist hardware requirement, the
   best available packaging system, support for 13 different architectures
   and a strict set of quality guidelines adopted by all the active Debian
   developers. The only foreseeable limitation, the Debian default
   text-based installer, was overriden by restarting the development of the
   then-abandoned Debian graphical installer. Now the new graphical
   installer is developed by tens of people and it will be included in the
   next official Debian release.},
ISSN = {1571-5736},
ISBN = {0-387-34225-7},
ResearcherID-Numbers = {Di Nunzio, Pierluigi/ABG-1540-2020},
Unique-ID = {WOS:000238056100035},
}

@inproceedings{ WOS:000241656300063,
Author = {Balos, Kazimierz and Zielinski, Krzysztof and Wojtas, Krzysztof and
   Wasilewski, Leszek},
Book-Group-Author = {IEEE},
Title = {Software tool construction for deployment of JMX services in distributed
   testbeds},
Booktitle = {2006 2ND INTERNATIONAL CONFERENCE ON TESTBEDS AND RESEARCH
   INFRASTRUCTURES FOR THE DEVELOPMENT OF NETWORKS \& COMMUNITIES},
Year = {2006},
Pages = {442-452},
Note = {2nd International Conference on Testbeds and Research Infrastructures
   for the Development of Networks and Communities, Barcelona, SPAIN, MAR
   01-03, 2006},
Abstract = {The paper presents a practical approach to construction of a software
   tool, which will be suitable for deployment of Java components in
   distributed and heterogeneous testbeds. Requirements of such a system
   are identified, and a suitable architecture, which is scalable and
   allows the installation and starting Java applications on chosen
   computational nodes, is proposed. The modular architecture of the
   constructed system is based on JMX (Java Management Extensions), which
   require packaging each Java component as an M-Bean and providing a
   deployment descriptor for it in the form of an XML MLet rile. Such an
   approach can be applied to building a lightweight framework for Java
   applications and services using only one Java Virtual Machine per
   computational node, where each component is installed and started in the
   same JVM, thus limiting valuable resource consumption. This approach
   cannot be useful for end-user applications without additional services
   allowing it to control and manage the deployed modules. Some of them
   include standard JMX services, such as the NMet service (for dynamic
   loading), while some have to be developed specifically for the purpose
   of operating in distributed testbeds. A detailed discussion of the
   system requirements is included in Section II. Section III presents the
   architecture of the implemented system called JIMS, built on the top of
   JMX core components. JIMS base modules, facilitating the goal of dynamic
   deployment of Java components in distributed testbeds, are described in
   Section IV. Examples of deployable modules - for testbed monitoring and
   management - are presented in Section V. Current applications of the
   presented system are described in Section VI. Section VII presents
   related work in the area of building similar tools. Section VIII
   contains conclusions.},
ISBN = {1-4244-0105-4},
Unique-ID = {WOS:000241656300063},
}

@inproceedings{ WOS:000286667100049,
Author = {Bhatnagar, S. K.},
Book-Author = {Tay, FEH
   Jianmin, M
   Bergstrom, J
   Iliescu, C},
Title = {PARTS SYNTHESIS APPROACH FOR 3D INTEGRATION OF MEMS AND MICROSYSTEMS
   LEADING TO SYSTEM-ON-PACKAGE},
Booktitle = {INTERNATIONAL MEMS CONFERENCE 2006},
Series = {Journal of Physics Conference Series},
Year = {2006},
Volume = {34},
Pages = {297-303},
Note = {International MEMS Conference 2006, Singapore, SINGAPORE, MAY 09-12,
   2006},
Organization = {Inst Bioengineering \& NanoTechnol},
Abstract = {This paper aims at developing Parts Synthesis Approach (PSA) for
   three-dimensional integration of MEMS and Microsystems leading to
   System-On-Package (SOP). This eliminates the interconnection related
   problems that arise when MEMS and its associated circuitry are packaged
   separately. A gas sensor array microsystem illustrates this. The novelty
   is that the electronics block at room temperature and the sensor at
   sensing temperature (250 degrees C-550 degrees C) coexist in the same
   SOP. Targeted specifications of the SOP are low power budget per sensor
   and low cost. Thermal and structural modelling and analysis has been
   done using ANSYS software. The SOP is modular in design and
   construction.
   In PSA the six physical parts of the SOP are first designed, optimized
   and fabricated individually and then integrated to form the desired SOP.
   The substrate is an integral and active part of the SOP. Power
   management is done by reducing thermal mass of the hotplate, increasing
   thermal resistance between the hotplate and the rest of the substrate
   and using pulsed power operation. Designs of the cavity structure, the
   coverlid and the bottom support help in thermal management. This
   technique circumvents the difficulties of forming deep cavities in LTCC
   technology. Since the six different parts of the SOP are fabricated
   separately and then integrated together the final yield is high.
   Designing in parallel reduces required time and cost.},
DOI = {10.1088/1742-6596/34/1/049},
ISSN = {1742-6588},
ORCID-Numbers = {Bhatnagar, Satish/0000-0001-7409-6891},
Unique-ID = {WOS:000286667100049},
}

@inproceedings{ WOS:000245428000001,
Author = {Blumrich, M. and Chen, D. and Chiu, L. -T. and Cipolla, T. and Coteus,
   P. and Crumley, P. and Gara, A. and Giampapa, M. E. and Hall, S. and
   Haring, R. A. and Heidelberger, P. and Hoenicke, D. and Kopcsay, G. V.
   and Liebsch, T. A. and Mok, L. and Ohmacht, M. and Salapura, V. and
   Swetz, R. and Takken, T. and Vranas, P.},
Book-Group-Author = {IEEE Computer Society},
Title = {A holistic approach to system reliability in Blue Gene},
Booktitle = {INTERNATIONAL WORKSHOP ON INNOVATIVE ARCHITECTURE FOR FUTURE GENERATION
   HIGH PERFORMANCE PROCESSORS AND SYSTEMS},
Series = {IEEE ACM International Conference on Automated Software Engineering},
Year = {2006},
Pages = {3+},
Note = {International Workshop on Innovative Architecture for Future Generation
   High Performance Processors and Systems, Kona, HI, JAN 23-25, 2006},
Abstract = {Optimizing supercomputer performance requires a balance between
   objectives for processor performance, network performance, power
   delivery and cooling, cost and reliability. In particular scaling a
   system to a large number of processors poses challenges for reliability,
   availability and serviceability. Given the power and thermal constraints
   of data centers, the BlueGene/L supercomputer has been designed with a
   focus on maximizing floating point operations per second per Watt
   (FLOPS/Watt). This results in a drastic reduction in FLOPS/m(2) floor
   space and FLOPS/dollar allowing for affordable scale-up. The BlueGene/L
   system has been scaled to a total of 65,536 compute nodes in 64 racks. A
   system approach was used to minimize power at all levels, from the
   processor to the cooling plant. A BlueGene/L compute node consists of a
   single ASIC and associated memory. The ASIC integrates all system
   functions including processors, the memory subsystem and communication,
   thereby minimizing chip count, interfaces, and power dissipation. As the
   number of components increases, even a low failure rate per-component
   will lead to an unacceptable system failure rate. Additional mechanisms
   will have to be deployed to achieve sufficient reliability at the system
   level. In particular the data transfer volume in the communication
   networks of a massively parallel system poses significant challenges on
   bit error rates and recovery mechanisms in the communication links. Low
   power dissipation and high performance, along with reliability,
   availability and serviceability were prime considerations in BlueGene/L
   hardware architecture, system design, and packaging. A high-performance
   software stack, consisting of operating system services, compilers,
   libraries and middleware, completes the system, while enhancing
   reliability and data integrity.},
DOI = {10.1109/IWIAS.2006.22},
ISSN = {1527-1366},
ISBN = {978-0-7695-2689-8},
ORCID-Numbers = {Ohmacht, Martin/0000-0001-5423-0512
   Vranas, Pavlos/0000-0002-8497-6283},
Unique-ID = {WOS:000245428000001},
}

@inproceedings{ WOS:000249558100042,
Author = {Canuto, Francesco and Turco, Patrizio and Colombo, Davide},
Book-Group-Author = {ASME},
Title = {Control development process of the brake-by-wire system},
Booktitle = {PROCEEDINGS OF THE 8TH BIENNIAL CONFERENCE ON ENGINEERING SYSTEMS DESIGN
   AND ANALYSIS, VOL 2},
Year = {2006},
Pages = {349-358},
Note = {8th Biennial ASME Conference on Engineering Systems Design and Analysis,
   Turin, ITALY, JUL 04-07, 2006},
Organization = {ASME},
Abstract = {The main goal of brake by wire technology is the development of compact,
   cheap and flexible braking systems. Since neither brake fluid nor
   hydraulic lines are used, brake by wire electromechanical actuation is a
   favourable solution both for production process and environmental
   aspect, and offer a precise control of braking torque amplitude. One of
   the most critical aspect is the lack of traditional link between brake
   pedal and brakes (calliper); this mean a potential safety problem to be
   correctly managed through the system architecture, redundancies,
   diagnosis and recoveries. During CRF brake by wire system development
   several architectures were deeply analysed using PHA, FMEA, and FTA
   methodology to identify the best configuration for production intent.
   The selected one is a fault-tolerant architecture based on a
   time-triggered communication network connecting fail-silent nodes. From
   safety analysis were defined critical events and system diagnosis and
   recovery requirements specifications. This paper describes the steps
   followed in the brake by wire software development, and its validation
   with respect to safety needs. For this purpose a three levels design and
   validation process was exploited. First of all, it was defined the
   complete simulation template including calliper electro-mechanical
   actuators and theirs ECU, time-triggered communication network and
   vehicle control ECU. The brake by wire system was interfaced to a
   complete vehicle dynamics model specifically developed for control
   design and validation purpose. Within this environment the control
   software was developed and the strategies were verified applying
   Software In the Loop technique. Then the ECU software was automatically
   generated using a customised too] chain based on Real Time Workshop
   Embedded Coder. Than, Hardware In the Loop testing was adopted to deeply
   verified high level software (application), low level software (OS, API,
   drivers,...) and hardware. HIL bench include the complete brake by wire
   system and a real time platform running the same vehicle model used
   during previous phase. Finally, vehicle testing phases complete the
   evaluation in the real environment and allows the system control
   development and tuning toward performances and subjective aspects. In
   each phase the system is tested both in normal and faulty conditions; a
   fault injection campaign was carried on to verify system response to
   fault with respect to the expected one. The process is cyclical, and a
   new loop has to be activated for each changes in the system. At the same
   time, testing complexity increases in order to guarantee the system
   safety.},
ISBN = {978-0-7918-4249-2},
Unique-ID = {WOS:000249558100042},
}

@inproceedings{ WOS:000243721600025,
Author = {D'Errico, Joseph and Qin, Wei},
Book-Group-Author = {IEEE},
Title = {Constructing portable compiled instruction-set simulators - An
   ADL-driven approach},
Booktitle = {2006 DESIGN AUTOMATION AND TEST IN EUROPE, VOLS 1-3, PROCEEDINGS},
Series = {Design Automation and Test in Europe Conference and Exhibition},
Year = {2006},
Pages = {110+},
Note = {Design, Automation and Test in Europe Conference and Exhibition (DATE
   06), Munich, GERMANY, MAR 06-10, 2006},
Organization = {European Design \& Automat Assoc; EDA Consortium; IEEE Comp Soc TTTC;
   ESCI; ACM SIGDA; RAS},
Abstract = {Instruction set simulators are common tools used for the development of
   new architectures and embedded software among countless other functions.
   This paper presents a framework that quickly generates fast and flexible
   instruction-set simulators from a specification based on a C-like
   architecture-description language. The framework provides a consistent
   platform for constructing and evaluating different classes of
   simulators, including interpreters, static-compiled simulators, and
   dynamic-compiled simulators. The framework also features a new
   construction method for dynamic-compiled simulator that involves no
   low-level programming. It profiles and translates frequently executed
   regions of simulated binary to C++ code and invokes GCC to compile such
   code into dynamically loaded libraries, which are then loaded into the
   simulator at ran time to accelerate simulation. Our experimental results
   based on the MIPS architecture and the SPEC CPU2000 benchmarks show that
   our dynamic-compiled simulator is capable of achieving tip to 11 times
   speedup compared to our fast interpreter Compared to other
   dynamic-compiled simulators requiring significant system programming
   expertise to construct, the proposed approach is simpler to implement
   and more portable.},
ISSN = {1530-1591},
ISBN = {978-3-9810801-1-7},
Unique-ID = {WOS:000243721600025},
}

@inproceedings{ WOS:000241941300004,
Author = {d'Inverno, Mark},
Book-Group-Author = {INSTICC},
Title = {Theory and application of intelligent agent systems},
Booktitle = {ICINCO 2006: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON
   INFORMATICS IN CONTROL, AUTOMATION AND ROBOTICS: INTELLIGENT CONTROL
   SYSTEMS AND OPTIMIZATION},
Year = {2006},
Pages = {IS25-IS26},
Note = {3rd International Conference on Informatics in Control, Automation and
   Robotics, Setubal Coll Business Adm, Setubal, PORTUGAL, AUG 01-05, 2006},
Organization = {Inst Syst \& Technol Informat Control \& Commun; Polytech Inst Setubal;
   IEEE Robot \& Automat Soc; Int Federat Automat Control; Amer Assoc
   Artificial Intelligence; AgentLink; ACM SIGAI; Portuguese Assoc Automat
   Control},
Abstract = {The focus of this talk will be on the critical relationship between
   theoretical investigations into agency and autonomy and their practical
   application in the physical world. For years I have been interested in
   the formal, principled approaches to modelling both natural and
   artificial systems in a computational setting. The main strand to this
   research, focuses on the application of formal methods in providing
   models of intelligent agent and multi-agent systems. This approach has
   sought to take a structured approach to the development of practical
   agent systems from theoretical models. Formal specification techniques
   are applied to describe a library of agent systems, languages and
   theories that can be used in the principled development of software.
   This work encompasses many aspects of agent cognition and agent society
   including action, perception, deliberation, communication, negotiation
   and social norms. This work has become known as the SMART agent
   framework and has been presented in a number of books and papers a few
   of which are detailed below.
   In recent years, one of the drivers of his work is applying ideas from
   mathematical modelling and intelligent agent-based design in a more
   practical and interdisciplinary settings such as music, art and design.
   I believe that many of the problems of the 21st century will require an
   inherently interdisciplinary approach andI am strongly motivated to
   understand how best to build teams. language, conceptual frameworks and
   methodologies that will enable experts from a variety of backgrounds to
   collectively solve problems. I think the agent metaphoris a natural one
   for technology to be embraced by other communities. In this respect,
   perhaps the most significant exploration to date is his work into the
   mathematical modelling, simulation and visualisation of stem cells. As
   experiments with stem cells are fundamentally limited in several
   significant ways, the modelling and simulation of stem cell models
   becomes a critical means to investigate cellular mechanisms. For
   example, it is currently impossible to observe and track individual stem
   cells in the adult human body and so little is understood about how
   individual stem cell interaction gives rise to the system behaviours,
   (such as population maintenance, self-renewal, recovery of populations
   after massive disturbance) that clearly arise in the human body.
   d'Invemo's research group has expertise in multi-agent system modelling
   and simulation and believe this is a natural way to investigate what is,
   without doubt, a dynamic self-organising system of individual agents.
   The metaphor has several advantages, not least that it is a natural one
   for biologists and believe that this is key for bringing biologists and
   computational modelers together. Not only have results to date made
   testable predictions, and provided insights into building a unified set
   of cellular mechanisms, d'Inverno also believes that there is a growing
   sense within the experimental stem cell community of the need to embrace
   new conceptual models within which they can propose and interpret their
   own experimental investigations and observations. Moreover, through
   continuing collaboration in interdisciplinary teams, it has becoming
   increasingly clear that visualisation might turn out to be the Trojan
   Horse for theoretical modelling in stem cell biology. Collaboration with
   artists and designers, to best understand how best to map simulations to
   visual interpretations for different biological communities, is now
   underway. In time, this work may lead us into insights about stem cells
   that could have massive therapeutic impact for a range of diseases from
   leukemia through to Parkinson's disease.
   In this talk I will present the theoretical SMART agent framework and
   outline some of the applications not only in modelling stem cells, but
   in computer generated music, design and the production of large scale
   art works.},
ISBN = {972-8865-59-7},
Unique-ID = {WOS:000241941300004},
}

@inproceedings{ WOS:000242400900221,
Author = {Davies, P. L.},
Editor = {Kuwano, J and Koseki, J},
Title = {Structures reinforced with geosynthetics - some illustrated causes of
   failure},
Booktitle = {Geosynthetics, Vols 1-4},
Year = {2006},
Pages = {1117-1122},
Note = {8th International Conference on Geosynthetics (8ICG), Yokohama, JAPAN,
   SEP 18-22, 2006},
Organization = {IGS Japan Chapter; Int Geosynthet Soc},
Abstract = {This paper is intended to demonstrate that even if the best structural
   designs incorporating geogrid reinforcement are issued for construction,
   cost-cutting clients and incompetent or uncaring installers can bring
   the engineer's efforts to ruin. It also serves as a warning that those
   engineers who allow their clients to dictate design-by-cost principles
   to them do so at their own peril. A number of grid-reinforced structures
   have failed in Southern Africa and, embarrassingly, having been rebuilt,
   some of them have failed again. Why? Is there a principle identifiable
   cause of failure that is common to these problem structures? These
   failures have occurred despite the substantial public-domain literature
   base on the design of structures reinforced with geosynthetics.
   Publications include the UK Code of Practice for reinforced soils
   (BS8006 1995) which has been adopted by South African National Standards
   (SANS) as SANS 207, as well as the South African Concrete Manufacturers
   Association Design of Reinforced Concrete Retaining Block (CRB) Walls
   (2000), and a number of commercial software design packages as well as a
   huge base of conference proceedings). The paper illustrates what the
   author believes to be the most common causes of failure - and examples
   of these are presented in the hopes that design engineers and installers
   will acknowledge the common problem areas and move to eliminate them on
   their projects.},
ISBN = {90-5966-044-7},
Unique-ID = {WOS:000242400900221},
}

@inproceedings{ WOS:000236909400002,
Author = {Gross, KC and McMaster, S and Porter, A and Urmanov, A and Votta, LG},
Editor = {Sterritt, R and Hinchey, MG and Bapty, T},
Title = {Towards dependability in everyday software using software telemetry},
Booktitle = {Third IEEE International Workshop on Engineering of Autonomic \&
   Autonomous Systems (EASe 2006), Proceedings},
Year = {2006},
Pages = {9-18},
Note = {3rd IEEE International Workshop on Engineering of Autonomic and
   Autonomous Systems, Potsdam, GERMANY, MAR 27-30, 2006},
Organization = {IEEE Comp Soc Task Force Autonomous \& Autonom Syst; IEEE Comp Soc TC
   Engn Comp Based Syst; IBM; Univ Ulster Ctr Software Proc Technologies;
   Univ Ulster Comp Sci Res Inst; NASA Software Engn Lab; NASA Goddard
   Space Flight Ctr; Vanderbilt Univ Inst Software Integrated Syst},
Abstract = {Application-level software dependability is difficult to ensure. Thus
   it's typically used only in custom systems and is achieved using
   one-of-a-kind, handcrafted solutions. We are interested in understanding
   whether and how these techniques can be applied to more common,
   lower-end Systems. To this end, we have adapted a condition-based
   maintenance (CBM) approach called the Multivariate State Estimation
   Technique (MSET). This approach automatically creates sophisticated
   statistical models that predict system failure well before failures
   occur, leading to simpler and more successful recoveries. We have
   packaged this approach in the Software Dependability Framework (SDF).
   The SDF consists of instrumentation and data management libraries, a CBM
   module, performance visualization tools, and a software architecture
   that supports system designers. Finally, we evaluated our framework on a
   simple video game application. Our results suggest that we can cheaply
   and reliably predict impending runtime failures and respond to them in
   time to improve the system's dependability.},
DOI = {10.1109/EASE.2006.21},
ISBN = {0-7695-2544-X},
Unique-ID = {WOS:000236909400002},
}

@inproceedings{ WOS:000242874700012,
Author = {Long, Keping and Yang, Xiaolong and Huang, Sheng and Kuang, Yujun},
Editor = {Won, YH and Chang, GK and Sato, KI and Wu, J},
Title = {A GMPLS-based OBS architecture for IP-over-WDM networks},
Booktitle = {NETWORK ARCHITECTURES, MANAGEMENT, AND APPLICATIONS IV},
Series = {Proceedings of SPIE},
Year = {2006},
Volume = {6354},
Number = {1-2},
Note = {Conference on Network Architectures, Management, and Applications IV,
   Pts 1 \& 2, Gwangju, SOUTH KOREA, SEP 05-07, 2006},
Abstract = {Recently, IP over WDM has been envisioned as dominant network
   architecture to meet the ever-increasing bandwidth requirement in the IP
   networks and to eliminate the intermediate layers and able to make
   better use of advanced optical technologies. Among existing several
   optical switching paradigms for IP over WDM, optical burst switching
   (OBS) is the more promising one, which combines the advantages of
   optical circuit switching (OCS) and optical packet switching (OPS), and
   can also loose the implementation complexity even though the optical
   random-access buffer is unavailable, and optical logic processing is not
   mature up to now. Therefore, OBS will play a very important role in next
   generation IP networks. However as much more trends shown, OBS-based IP
   optical network requires an appropriate control plane able to control
   and manage resource/service between IP and WDM. Currently, Generalized
   Multi-Protocol Label Switching (GMPLS) has been regarded as one of
   excellent candidate control planes for most of network scenarios because
   it enhances some issues of MPLS (including routing and signaling, link
   management, and fault protection and restoration), and also supports
   various granularity switching (such as packet, TDM timeslot, wavelength,
   waveband, and fibre-switched). Moreover, some practical deployments have
   shown GMPLS able to achieve automatic path provisioning and online
   network management. Therefore naturally, the integration and cooperation
   between GMPLS and OBS attracts more attentions from many research
   institutes and organizations.
   Of course, the similar concepts are originally given in the labeled OBS
   (LOBS) framework proposed by Dr. Qiao. Most of importance, LOBS provides
   the basic ideas for the collaboration of MPLS and OBS. However, how to
   efficiently integrate GMPLS and OBS to make the best of their advantages
   requires much more detailed discussions. To our knowledge, there are
   very few literatures and material to expatiate on the related issues.
   Here, the paper will present our some research results and viewpoints
   related to the integration and extension of GMPLS into OBS. Firstly, the
   paper proposes a GMPLS-based OBS network model for the integration of IP
   and WDM. Secondly, it defines the packets format for control and data
   information. Third, it also gives the node's functional architecture and
   the collaborative control operations between GMPLS and OBS. Finally, it
   proposes a unified software system to integrate the functions of GMPLS
   and OBS. Besides, the paper also presents some GMPLS enhancements to
   support OBS, such as traffic engineering, fault recovery, and link
   management, etc.},
DOI = {10.1117/12.694815},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-6449-X},
Unique-ID = {WOS:000242874700012},
}

@inproceedings{ WOS:000237194400018,
Author = {Lungu, Mircea and Lanza, Michele and Girba, Tudor},
Book-Group-Author = {IEEE Comp Soc},
Title = {Package patterns for visual architecture recovery},
Booktitle = {10TH EUROPEAN CONFERENCE ON SOFTWARE MAINTENANCE AND REENGINEERING,
   PROCEEDINGS},
Year = {2006},
Pages = {183+},
Note = {10th European Conference on Software Maintenance and Reengineering,
   Bari, ITALY, MAR 22-24, 2006},
Organization = {IEEE; IEEE Comp Soc; Espriva; Reengn Forum; Dept Informat Bari; Univ
   Bari; CeR ICT; CERIT; CONFCOOPERATIVE; EDS; SAP; Serlab; Res Ctr
   Software Technol},
Abstract = {Recovering the architecture is the first step towards reengineering a
   software system. Many reverse engineering tools use top-down exploration
   as a way of providing a visual and interactive process for architecture
   recovery. During the exploration process, the user navigates through
   various views on the system by choosing from several exploration
   operations. Although some sequences of these operations lead to views
   which, from the architectural point of view, are mode relevant than
   others, current tools do not provide a way of predicting which
   exploration paths are worth taking and which are not.
   In this article we propose a set of package patterns which are used for
   augmenting the exploration process with information about the worthiness
   of the various exploration paths. The patterns are defined based on the
   internal package structure and on the relationships between the package
   and the other packages in the system. To validate our approach, we
   verify the relevance of the proposed patterns for real-world systems by
   analyzing their frequency of occurrence in six open-source software
   projects.},
ISBN = {0-7695-2536-9},
Unique-ID = {WOS:000237194400018},
}

@inproceedings{ WOS:000246333000030,
Author = {Mahboubi, Zouhair and Clarke, Stella},
Book-Group-Author = {IEEE},
Title = {.NET API wrapping for existing C plus plus haptic APIs},
Booktitle = {2006 IEEE INTERNATIONAL WORKSHOP ON HAPTIC AUDIO VISUAL ENVIRONMENTS AND
   THEIR APPLICATIONS},
Year = {2006},
Pages = {153+},
Note = {5th IEEE International Workshop on Haptic Audio Visual Environments and
   Their Applications, Ottawa, CANADA, NOV 04-05, 2006},
Organization = {IEEE},
Abstract = {For a long time Haptic devices were expensive and therefore only
   accessible to a specialized community But with companies like. Novint
   Technologies introducing a peripheral intended to sell for about
   US\$100, Haptic devices can be expected to be affordable for a wider
   public. But considering that most Haptic APIs are in C++, a language
   intended for expert programmers, novice programmers wanting to program
   haptic devices would face a steep learning curve. However, if the APIs
   were, to be usable from within the .NET Framework, it would allow the
   wore novice users to program using over 20 programming languages and
   extensive programming solutions and therefore they would be able to
   easily, and efficiently develop software with haptic capabilities. This
   paper presents a set of guidelines for a design architecture that would
   allow migrating on existing C++ API to the .NET Framework without having
   to rewrite it from scratch. The presented architecture was implemented
   by wrapping the Sensable Ghost SDK 3.0. It was then used in both
   software. and hardware. based scenarios.},
ISBN = {978-1-4244-0760-6},
Unique-ID = {WOS:000246333000030},
}

@inproceedings{ WOS:000242043200029,
Author = {Mathaikutty, Deepak and Shukla, Sandeep},
Editor = {Secareanu, R and Krishnamurthy, R and Kim, S and Tran, T},
Title = {SoC design space exploration through automated IP selection from SystemC
   IP library},
Booktitle = {IEEE INTERNATIONAL SOC CONFERENCE, PROCEEDINGS},
Series = {IEEE International SOC Conference},
Year = {2006},
Pages = {109+},
Note = {IEEE International SOC Conference, Austin, TX, SEP 24-27, 2006},
Organization = {IEEE Circuits \& Syst Soc},
Abstract = {Given a SystemC IP library, constructing SoC simulation models for
   design space exploration often distract Oesigners from system
   architecture concerns to software engineering and proyamming concerns.
   Fast design space exploration using a visual architectural specification
   framework followed by automated IP selection and construction of
   simulation models without having to programmatically composing the IPs
   is the main attractive feature of the component composition framework
   developed and described in this paper. We employ concepts metamodeling
   for the visual specification, meta-data for kes IPopf reflection, and
   algorithn-dc analysis of metadata for IP-selection, matching and
   executable model construction.},
ISSN = {2164-1676},
ISBN = {0-7803-9781-9},
ResearcherID-Numbers = {Shukla, Sandeep/B-3358-2009
   SHUKLA, SANDEEP/T-6430-2019},
ORCID-Numbers = {Shukla, Sandeep/0000-0001-5525-7426
   SHUKLA, SANDEEP/0000-0001-5525-7426},
Unique-ID = {WOS:000242043200029},
}

@article{ WOS:000237878500002,
Author = {Santoro, Andrea and Quaglia, Francesco},
Title = {Transparent state management for optimistic synchronization in the high
   level architecture},
Journal = {SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION
   INTERNATIONAL},
Year = {2006},
Volume = {82},
Number = {1},
Pages = {5-20},
Month = {JAN},
Note = {19th Workshop on Principles of Advanced and Distributed Simulation (PADS
   2005), Monterey, CA, JUN 01-03, 2005},
Organization = {ACM SIGSIM; IEEE Computer Soc, TCSIM; SCS},
Abstract = {In this article, the authors present the design and implementation of a
   software architecture - namely, MAgic State Manager (MASM) - to be
   employed within a runtime infrastructure (RTI) in support of High Level
   Architecture (HLA) federations. MASM allows performing
   checkpointing/recovery of the federate state in a way completely
   transparent to the federate itself, thus providing the possibility of
   demanding to the RTI any task related to state management in optimistic
   synchronization. Different from existing proposals, through this
   approach, the federate programmer is required neither to supply modules
   for state management within the federate code nor to explicitly
   interface the federate code with existing, third-party
   checkpointing/recovery libraries. Hence, the federate programmer is
   completely relieved from the burden of facing state management issues.
   One major application of this proposal is the possibility to employ
   optimistic synchronization, even in case of federates originally
   designed for the conservative approach. This can provide a way of
   improving the simulation system performance in specific scenarios (e.g.,
   in case of poor or zero lookahead within the federation). The authors
   elaborate on this issue by discussing on how to integrate MASM within
   the RTI to achieve such a synchronization objective. Some experimental
   results demonstrating limited runtime overhead introduced by MASM are
   also reported for two case studies-namely, an interconnection network
   simulation and a personal communication system simulation.},
DOI = {10.1177/0037549706065350},
ISSN = {0037-5497},
EISSN = {1741-3133},
Unique-ID = {WOS:000237878500002},
}

@article{ WOS:000240900600007,
Author = {Tuparev, G. and Nicolova, I. and Zlatanov, B. and Mihova, D. and Popova,
   I. and Hessman, F. V.},
Title = {Design and implementation of a software package to control a network of
   robotic observatories},
Journal = {ASTRONOMISCHE NACHRICHTEN},
Year = {2006},
Volume = {327},
Number = {8},
Pages = {771-774},
Note = {1st Workshop on Heterogeneous Telescope Networks, Exeter, ENGLAND, JUL
   18-21, 2005},
Abstract = {We present a description of a reusable software package able to control
   a large, heterogeneous network of fully and semi-robotic observatories
   initially developed to run the MONET network of two 1.2 m telescopes.
   Special attention is given to the design of a robust, long-term
   observation scheduler which also allows the trading of observation time
   and facilities within various networks. The handling of the ``Phase I \&
   II{''} project-development process, the time-accounting between complex
   organizational structures, and usability issues for making the package
   accessible not only to professional astronomers, but also to amateurs
   and high-school students is discussed. A simple RTML-based solution to
   link multiple networks is demonstrated. (c) 2006 WILEY-VCH Verlag GmbH
   \& Co. KGaA, Weinheim.},
DOI = {10.1002/asna.200610630},
ISSN = {0004-6337},
EISSN = {1521-3994},
ORCID-Numbers = {Hessman, Frederic Victor/0000-0002-0672-4945},
Unique-ID = {WOS:000240900600007},
}

@inproceedings{ WOS:000279628900041,
Author = {Wang, Zhiyun and Luan, Maotian and Wang, Dong and Wu, Ke and Fan,
   Yinglai},
Editor = {Luan, M and Zen, K and Chen, G and Nian, T and Kasama, K},
Title = {ULTIMATE BEARING CAPACITY OF SUCTION CAISSON FOUNDATIONS IN UNDRAINED
   SOILS},
Booktitle = {RECENT DEVELOPMENT OF GEOTECHNICAL AND GEO-ENVIRONMENTAL ENGINEERING IN
   ASIA, PROCEEDINGS},
Year = {2006},
Pages = {229-234},
Note = {4th Asian Joint Symposium on Geotechnical and Geo-Environmental
   Engineering (JS-Dalian 2006), Dalian, PEOPLES R CHINA, NOV 23-25, 2006},
Organization = {Japanese Geotech Soc; Chinese Inst Soil Mech \& Geotech Engn, China
   Civil Engn Soc; Chinese Branch-Soc Geo-Environm Engn, Chinese Soc Rock
   Mt \& Engn; Dalian Univ Technol, State Key Lab Coastal \& Offshore Engn;
   Dalian Univ Technol, Sch Civil \& Hydraul Engn; Kyushu Branch Japanese
   Geotech Soc; Natl Nat Sci Fdn China; Dalian Univ Technol},
Abstract = {As a newly developed type of foundation for deep water offshore and
   marine engineering, the suction caisson is usually subjected to combined
   loading of vertically uplift load, horizontal load and moment. To
   understand features of bearing capacity of suction caisson foundation is
   one of the key issues in design and construction of deep-water marine
   structures. However at present performance evaluation and design theory
   for such a new type of foundation can not sufficiently meet basic
   requirements of engineering practice. During the process of rapid
   pulling out the caisson foundation from soil, the normally consolidated
   clay is considered to be in fully undrained condition. When overall
   instability of caisson foundation occurs, the failure mode may be
   reverse to collapse mechanism induced by vertical load and usually in
   this case the maximum ultimate capacity may be mobilized by the caisson
   foundation. In this paper, the general-purpose finite element analysis
   package ABAQUS is employed to conduct three-dimensional numerical
   analyses on load-carrying features of suction caisson foundation. The
   Swipe test procedure of loading for defining the failure state is
   numerically implemented in this software. Then for different
   combinations of the aspect ratio of suction caisson, undrained shear
   strength and deformation modulus of subsoil as well as strength
   reduction Factor, the ultimate bearing capacity of suction caisson
   foundation under monotonic vertical uplift load and horizontal load is
   evaluated. Moreover, the failure envelope of suction caisson foundation
   under combined loading condition is established by the proposed
   numerical procedure.},
ISBN = {978-7-5611-2813-8},
ResearcherID-Numbers = {Wang, Dong/G-4340-2013},
Unique-ID = {WOS:000279628900041},
}

@inproceedings{ WOS:000244517900046,
Author = {Watson, Gregory R. and DeBardeleben, Nathan A.},
Book-Group-Author = {IEEE},
Title = {A model-based framework for the integration of parallel tools},
Booktitle = {2006 IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING, VOLS 1 AND 2},
Year = {2006},
Pages = {416+},
Note = {IEEE International Conference on Cluster Computing, Barcelona, SPAIN,
   SEP 25-28, 2006},
Organization = {IEEE},
Abstract = {A large number of tools are already available to aid in the development
   of parallel scientific applications, yet many developers are unaware
   thin: exist, do not have access to them, or find them too difficult to
   use. And, unlike the wider software development community where the use
   of integrated development environments is best practice, parallel
   software development languishes with the lowest common denominator of
   command-line tools and Emacs style editors. By harnessing the power and
   flexibility of the phenomenally successful Eclipse framework, we have
   developed a platform for the integration of parallel tools that aims to
   provide a robust, portable, and scalable parallel development
   environment for the development of high performance scientific computing
   applications.
   The Eclipse Parallel Tools Platform utilizes a model-view-controller
   design and a generic API architecture to support a wide range of
   parallel computing environments. The platform has been designed so that
   it is easily extensible, and will support the integration of existing
   and new parallel tools. In. this paper we describe the architecture of
   the platform, provide details of an example implementation for a
   particular parallel runtime system, and show how other parallel tools
   can be integrated with the Eclipse Parallel Tools Platform.},
ISBN = {978-1-4244-0327-1},
ORCID-Numbers = {Watson, Gregory/0000-0002-8591-2441},
Unique-ID = {WOS:000244517900046},
}

@article{ WOS:000232655500010,
Author = {Perini, A and Susi, A},
Title = {Agent-oriented visual modeling and model validation for engineering
   distributed systems},
Journal = {COMPUTER SYSTEMS SCIENCE AND ENGINEERING},
Year = {2005},
Volume = {20},
Number = {4},
Pages = {319-329},
Month = {JUL},
Abstract = {Agent-Oriented methodologies that have been recently proposed for
   engineering distributed systems tend to adopt a model-based approach to
   software development, that is they devise a development process based on
   the definition of a specific set of models for-each steps in the
   analysis and in the software design phases. To be put into practice,
   this approach demands clear guidelines for building and refining models
   along the software development process, as well as flexible modeling
   tools which integrates automatic verification techniques at support of
   model validation. In this paper we describe a modeling environment which
   integrates an Agent-Oriented (AO) modeling tool with other tools, such
   as a model-checker for the verification of formal properties of the
   model and a library which implements graph transformation techniques
   which can be used to support model refinement and model transformations.
   In designing it we took into account recommendations from the OMG's
   Model-Driven Architecture initiative. We illustrate the modeling
   environment architecture, give details on the AO modeling tool and on
   the components that allows for the integration with other tools.
   Examples of how modeling and validation can be interleaved and supported
   by the modeling environment are given.},
ISSN = {0267-6192},
Unique-ID = {WOS:000232655500010},
}

@article{ WOS:000228939600020,
Author = {Cabric, D and Eltawil, AM and Zou, HL and Mohan, S and Daneshrad, B},
Title = {Wireless field trial results of a high hopping rate FHSS-FSK testbed},
Journal = {IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS},
Year = {2005},
Volume = {23},
Number = {5},
Pages = {1113-1122},
Month = {MAY},
Abstract = {This paper presents a complete study and characterization of a real-time
   frequency-hopped, frequency shift-keyed testbed capable of transmitting
   data at 160 kb/s, with hopping rates of up to 80 Khops/s operating in
   the 906 MHz band. The system provides the highest hopping rate reported
   to date and sets a new trend for FHSS communications with superior low
   probability of interception/detection and anti-jamming (LPI/LPD/AJ)
   capabilities. The architecture features a direct digital frequency
   synthesizer to enable high-rate hopping, and a frequency
   correlator-based demodulator, plus all digital timing and frequency
   recovery algorithms to minimize complexity. Furthermore, single sideband
   modulation was used to achieve spectral efficiency. The testbed is
   software configured and provides the user with full control over the
   diversity combining techniques, symbol interleaving, packet structure,
   and acquisition protocols. A total of 5850 independent experiments were
   carried out under various receiver configurations and wireless
   environments. The results underscore the dramatic potential for a system
   that optimally combines high-rate hopping, interleaving, and equal gain
   combining to combat severe propagation conditions, including multipath
   fading and intentional jamming.},
DOI = {10.1109/JSAC.2005.845437},
ISSN = {0733-8716},
EISSN = {1558-0008},
ResearcherID-Numbers = {Eltawil, Ahmed/M-6893-2019
   },
ORCID-Numbers = {Eltawil, Ahmed/0000-0003-1849-083X
   Cabric, Danijela/0000-0002-5967-2683},
Unique-ID = {WOS:000228939600020},
}

@article{ WOS:000232058900004,
Author = {Chen, T and Yang, J and Wang, YL and Zhan, CY and Zang, YH and Qin, JC},
Title = {Design of recombinant stem cell factor-macrophage colony stimulating
   factor fusion proteins and their biological activity in vitro},
Journal = {JOURNAL OF COMPUTER-AIDED MOLECULAR DESIGN},
Year = {2005},
Volume = {19},
Number = {5},
Pages = {319-328},
Month = {MAY},
Abstract = {Stem cell factor (SCF) and macrophage colony stimulating factor (M-CSF)
   can act in synergistic way to promote the growth of mononuclear
   phagocytes. SCF-M-CSF fusion proteins were designed on the computer
   using the Homology and Biopolymer modules of the software packages
   InsightII. Several existing crystal structures were used as templates to
   generate models of the complexes of receptor with fusion protein. The
   structure rationality of the fusion protein incorporated a series of
   flexible linker peptide was analyzed on InsightII system. Then, a
   suitable peptide GGGGSGGGGSGG was chosen for the fusion protein. Two
   recombinant SCF-M-CSF fusion proteins were generated by construction of
   a plasmid in which the coding regions of human SCF (1-165aa) and M-CSF
   (1-149aa) cDNA were connected by this linker peptide coding sequence
   followed by subsequent expression in insect cell. The results of Western
   blot and activity analysis showed that these two recombinant fusion
   proteins existed as a dimer with a molecular weight of similar to 84 KD
   under non-reducing conditions and a monomer of similar to 42 KD at
   reducing condition. The results of cell proliferation assays showed that
   each fusion protein induced a dose-dependent proliferative response. At
   equimolar concentration, SCF/M-CSF was about 20 times more potent than
   the standard monomeric SCF in stimulating TF-1 cell line growth, while
   M-CSF/SCF was 10 times of monomeric SCF. No activity difference of
   M-CSF/SCF or SCF/M-CSF to M-CSF (at same molar) was found in stimulating
   the HL-60 cell linear growth. The synergistic effect of SCF and M-CSF
   moieties in the fusion proteins was demonstrated by the result of
   clonogenic assay performed with human bone mononuclear, in which both
   SCF/M-CSF and M-CSF/SCF induced much higher number of CFU-M than
   equimolar amount of SCF or M-CSF or that of two cytokines mixture.},
DOI = {10.1007/s10822-005-5686-x},
ISSN = {0920-654X},
EISSN = {1573-4951},
ResearcherID-Numbers = {Yang, Jie/AAH-7541-2019},
Unique-ID = {WOS:000232058900004},
}

@article{ WOS:000229092800003,
Author = {Beloglavec, S and Hericko, M and Juric, MB and Rozman, I},
Title = {Analysis of the limitations of multiple client handling in a Java server
   environment},
Journal = {ACM SIGPLAN NOTICES},
Year = {2005},
Volume = {40},
Number = {4},
Pages = {20-28},
Month = {APR},
Abstract = {A server infrastructure in web servers, message servers and other
   parallel systems use a variation of two software architectures for
   providing concurrency: threaded or event-driven. This paper analyzes the
   performance limitations of concurrent applications implemented in Java.
   Both architectures have been evaluated and compared with various design
   patterns, which combine the best practices from both architectures. For
   each architecture the suitability for handling a large volume of client
   requests, the efficient management of a server load, the influence of
   client request structures, and the physical size of a client request,
   have been studied. The discussed Java APIs are core technologies for
   high-level APIs, used in developing web and distributed applications.
   The research also includes performance comparison on various platforms
   and discusses performance variation on various versions of a Java
   runtime. The paper contributes to the understanding of Java-based server
   architecture capabilities. Core server software architectures and
   required Java libraries are compared, the reasons for the limitations
   are identified and guidelines for choosing proper combinations are
   given.},
DOI = {10.1145/1064165.1064170},
ISSN = {0362-1340},
Unique-ID = {WOS:000229092800003},
}

@article{ WOS:000226943900017,
Author = {Chakravorty, R and Clark, A and Pratt, I},
Title = {Optimizing web delivery over wireless links: Design, implementation
   and-experiences},
Journal = {IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS},
Year = {2005},
Volume = {23},
Number = {2},
Pages = {402-416},
Month = {FEB},
Abstract = {World over wide-area wireless Global System for Mobile Communication
   (GSM) networks have been upgraded to support the general packet radio
   service (GPRS). GPRS brings ``always-on{''} wireless data connectivity
   at bandwidths comparable to that of conventional fixed-line telephone
   modems. Unfortunately many users have found the reality to be rather
   different, experiencing very disappointing performance when, for
   example, browsing the Web over GPRS.
   In this paper, we show what causes the web and its underlying transport
   protocol TCP to underperform in a GPRS wide-area wireless environment.
   We examine why certain GPRS network characteristics interact badly with
   TCP to yield problems such as: link underutilization for short-lived
   flows, excess queueing for long-lived flows, ACK compression, poor loss
   recovery, and gross unfairness between competing flows. We also show
   that many Web browsers tend to be overly aggressive, and by opening too
   many simultaneous TCP connections can aggravate matters.
   We present the design and implementation of a web optimizing proxy
   system called GPRSWeb that mitigates many of the GPRS link-related
   performance problems with a simple software update to a mobile device.
   The update is a link-aware middleware (a local ``client proxy{''}) that
   sits in the mobile device, and communicates with a ``server proxy{''}
   located at the other end of the wireless link, close to the
   wired-wireless border. The dual-proxy architecture collectively
   implements a number of key enhancements-an aggressive caching scheme
   that employs content-based hash keying to improve hit rates for dynamic
   content, a preemptive push of Web page support resources to mobile
   clients, resource adaptation to suit client capabilities, delta encoded
   data transfer of modified pages, DNS lookup migration, and a UDP-based
   reliable transport protocol that is specifically optimized for use over
   GPRS. We show that these enhancements results in significant improvement
   in web performance over GPRS links.},
DOI = {10.1109/JSAC.2004.839398},
ISSN = {0733-8716},
EISSN = {1558-0008},
Unique-ID = {WOS:000226943900017},
}

@inproceedings{ WOS:000233560500029,
Author = {Alumbaugh, TJ and Jiao, XM},
Editor = {Hanks, BW},
Title = {Compact array-based mesh data structures},
Booktitle = {PROCEEDINGS OF THE 14TH INTERNATIONAL MESHING ROUNDTABLE},
Year = {2005},
Pages = {485-503},
Note = {14th International Meshing Roundtable, San Diego, CA, SEP 11-14, 2005},
Abstract = {In this paper, we present simple and efficient array-based mesh data
   structures, including a compact representation of the half-edge data
   structure for surface meshes, and its generalization-a half-face data
   structure-for volume meshes. These array-based structures provide
   comprehensive and efficient support for querying incidence, adjacency,
   and boundary classification, but require substantially less memory than
   pointer-based mesh representations. In addition, they are easy to
   implement in traditional programming languages (such as in C or Fortran
   90) and convenient to exchange across different software packages or
   different storage media. In a parallel setting, they also support
   partitioned meshes and hence are particularly appealing for large-scale
   scientific and engineering applications. We demonstrate the construction
   and usage of these data structures for various operations, and compare
   their space and time complexities with alternative structures.},
DOI = {10.1007/3-540-29090-7\_29},
ISBN = {3-540-25137-5},
ORCID-Numbers = {Jiao, Xiangmin/0000-0002-7111-9813},
Unique-ID = {WOS:000233560500029},
}

@inproceedings{ WOS:000231193100029,
Author = {Czekster, RM and de Souza, ON},
Editor = {Setubal, JC and VerjovskiAlmeida, S},
Title = {VIZ - A graphical open-source architecture for use in structural
   bioinformatics},
Booktitle = {ADVANCES IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY, PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2005},
Volume = {3594},
Pages = {226-229},
Note = {Brazilian Symposium on Bioinformatics (BSB 2005), Univ Vale do Rio dos
   Sinos, Sao Leopoldo, BRAZIL, JUL 27-29, 2005},
Abstract = {Protein structure visualization is crucial for understanding its
   function inside the cell. Each year, laboratories around the world
   deposit protein structures on a central database for further analysis
   and research. The result is a large amount of structures being deposited
   (approximately 31,000 in may 2005). Visualization is a very powerful
   tool to help in the analysis, aiding data understanding and
   interpretation. The present work suggests an architecture to help the
   rapid construction of visual biomolecular software, specifically
   designed to be simple, modular and scalable. The architecture, called
   VIZ, employs high quality open-source libraries offering simple data
   structures and customizable options. The architecture can be used to
   start a new visual software project to visualize and represent
   individual protein structures, as well as multiple conformations from
   molecular dynamics simulation trajectories.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {3-540-28008-1},
ResearcherID-Numbers = {Czekster, Ricardo M./D-2854-2009
   de Souza, Osmar Norberto/B-7328-2013},
ORCID-Numbers = {Czekster, Ricardo M./0000-0002-6636-4398
   de Souza, Osmar Norberto/0000-0002-4602-1208},
Unique-ID = {WOS:000231193100029},
}

@inproceedings{ WOS:000236568000123,
Author = {Dascalu, S and Hao, N and Debnath, N},
Book-Group-Author = {IEEE},
Title = {Design patterns automation with template library},
Booktitle = {2005 IEEE International Symposium on Signal Processing and Information
   Technology (ISSPIT), Vols 1 and 2},
Year = {2005},
Pages = {699-705},
Note = {5th IEEE International Symposium on Signal Processing and Information
   Technology, Athens, GREECE, DEC 18-21, 2005},
Organization = {IEEE},
Abstract = {Design patterns offer reusable solutions to particular software design
   problems. Design Patterns Automation is an approach that applies design
   patterns at the implementation stage of the software development life
   cycle. Inspired by two commonly used template libraries, Active Template
   Library and Standard Template Library, and one of the most popular
   generic programming technologies, C++ templates, this paper introduces a
   new method for achieving design patterns automation. This method differs
   from the currently available UML-based and wizard-based design patterns
   automation techniques and provides support for increased flexibility,
   expandability and compatibility in developing software using design
   patterns. Seven of the patterns proposed by Gamma et al. have been
   implemented using C++ templates, namely singleton. factory method,
   visitor, memento, strategy, iterator, and decorator. To illustrate the
   method proposed, details of singleton and decorator implementations are
   provided and a larger ``Check{''} example developed using the decorator
   template is presented. The paper also includes a comparison with similar
   approaches and presents several directions of future work.},
ISBN = {0-7803-9313-9},
Unique-ID = {WOS:000236568000123},
}

@inproceedings{ WOS:000241190300046,
Author = {Furber, Steve and Bainbridge, John},
Editor = {Nurmi, J and Takala, J and Hamalainen, TD},
Title = {Future trends in SoC interconnect},
Booktitle = {2005 INTERNATIONAL SYMPOSIUM ON SYSTEM-ON-CHIP, PROCEEDINGS},
Year = {2005},
Pages = {183-186},
Note = {International Symposium on System-on-Chip, Tampere Univ Technol,
   Tampere, FINLAND, 2005},
Organization = {IEEE Circuits \& Syst Soc; Nokia; Tensilica},
Abstract = {Self-timed packet-switched networks are poised to take a major role in
   addressing the problems of timing closure power management and
   overwhelming complexity in the design of Systems-on-Chip. The robust,
   correct-by-construction characteristics of self-timed communications
   enables each IP block on the SoC to operate in its own isolated timing
   domain, greatly simplifying the problems of timing validation. The
   inherent data-driven nature of the self-timed network, combined with the
   improved wire segmentation provided by the switched network architecture
   gives greatly improved power management. Design automation software can
   remove the need for expertise in self-timed design and networking
   principles, enabling the on-chip interconnect to be treated as an
   additional IP block within a conventional (synchronous) design flow. The
   paradigm shift from viewing the SoC design problem as a matter of
   organizing complex hierarchies of buses with multiple coupled timing
   domains, where every interface between timing domains must be verified
   carefully, to viewing the SoC as a problem in network design where those
   timing issues are automatically isolated, promises significant
   improvements in designer productivity, component reuse and SoC
   functionality.},
DOI = {10.1109/ISSOC.2005.1595673},
ISBN = {0-7803-9294-9},
ORCID-Numbers = {Furber, Stephen/0000-0002-6524-3367},
Unique-ID = {WOS:000241190300046},
}

@inproceedings{ WOS:000290114100094,
Author = {Gijsbers, P. J. A. and Gregersen, J. B.},
Editor = {Zerger, A and Argent, RM},
Title = {OpenMI: A glue for model integration},
Booktitle = {MODSIM 2005: INTERNATIONAL CONGRESS ON MODELLING AND SIMULATION:
   ADVANCES AND APPLICATIONS FOR MANAGEMENT AND DECISION MAKING: ADVANCES
   AND APPLICATIONS FOR MANAGEMENT AND DECISION MAKING},
Year = {2005},
Pages = {648-654},
Note = {International Congress on Modelling and Simulation (MODSIM05),
   Melbourne, AUSTRALIA, DEC 12-15, 2005},
Abstract = {Management issues in many sectors of society demand for integrated
   analysis, which can be supported by integrated modelling. Since the
   all-inclusive modelling software is difficult to achieve, and possibly
   even undesirable, integrated modelling will require the linkage of
   individual model or model components that address specific domains.
   Emerging from the water sector, OpenMI has been developed with the
   purpose of being the glue which can link legacy and non-legacy model
   components from various origins together. OpenMI provides a standardized
   interface to define, describe and transfer data on a time basis between
   software components that run simultaneously. This paper presents the
   technical concepts of OpenMI as well as a nearly complete interface
   specification (see HarmonIT, 2005).
   Essential concepts within OpenMI are the distinction of quantities,
   element sets, time and values. Elements can be non-geo-referenced or
   geo-referenced in 0/1/2/3D. The entities are supported by meta-data
   interfaces describing what the data represent, to which location it
   refers, for which time (time stamp or time span) they are valid and how
   they are produced.
   The developers of OpenMI created a full software implementation, called
   the OpenMI environment, in C\# (.NET) and a less extensive one in Java
   (under construction). The focus of development was primarily oriented to
   data exchange issues. Hence a wide range of utilities is provided to
   enhance the implementation of the OpenMI interfaces from typically
   legacy code. E. g. a wrapper package provides facilities for book
   keeping of links and data handling such as buffering and spatial and
   temporal mapping. Simple tools are available to define links, to run the
   system and to display results.
   OpenMI is based on the request-reply architecture concept. The basic
   workflow is the following. At configuration time, meta data of a
   component is inspected to identify and define the links between the
   components. At run-time a component is requested by another component
   for values at a specific time and location (element set) using the
   GetValues-function call (see Figure 1). This providing component is
   obliged to reply to this request and provide the data at the time and
   location as requested. In order to reply, he may need to do internal
   computations to progress in time. This computation may require external
   data that can be requested from other components by a GetValues-call.
   Once the internal time progressed at or past the requested time, data
   transformations can be applied to return the values at the exact time,
   element set and units as requested.
   {[}GRAPHICS]
   The interface orientation of OpenMI does not prescribe the use of the
   OpenMI environment. Hence OpenMI can also be used to glue model engines
   with existing modelling frameworks through the OpenMI interface. OpenMI
   is expected to satisfy the modeling requirements of a wide group of
   users in various engineering domains, such as model coders, model
   developers, data managers and end users. The expected impacts are the
   simplification of the model linking process, the ability to represent
   feedback loops and process interactions, the establishment of a
   communication standard for modelling and a reduction in development time
   for decision support systems.
   The architecture for OpenMI is fully documented and available on
   www.openmi.org. The open source implementation is available on
   sourceforge.net\textbackslash{}projects\textbackslash{}openmi. Other
   papers in this conference provide case studies. This paper discusses the
   technical aspects of the OpenMI interface specification.},
ISBN = {978-0-9758400-2-3},
Unique-ID = {WOS:000290114100094},
}

@inproceedings{ WOS:000234289900028,
Author = {Gillies, ADS and Wu, HW and Humphreys, D},
Book-Group-Author = {ausimm},
Title = {Spontaneous combustion and simulation of mine fires and their effects on
   mine ventilation systems},
Booktitle = {COAL 2005: 6th Australasian Coal Operators' Conference},
Series = {AUSTRALASIAN INSTITUTE OF MINING AND METALLURGY PUBLICATION SERIES},
Year = {2005},
Volume = {2005},
Number = {2},
Pages = {225-236},
Note = {6th Australasian Coal Operators Conference, Brisbane, AUSTRALIA, APR
   26-28, 2005},
Organization = {Rio Tinto Coal Australia; Australian Coal Assoc Res Program; Dyno Nobel;
   Metech; Orica Mining Serv},
Abstract = {The structure of a comprehensive research project into mine fires study
   applying the Ventgraph mine fire simulation software, preplanning of
   escape scenarios and general interaction with rescue responses is
   outlined. The project has Australian Coal Association Research Program
   (ACARP) funding and also relies on substantial mining company site
   support. This practical input from mine operators is essential and
   allows the approach to be introduced in the most creditable way. The
   effort is built around the introduction of fire simulation computer
   software to the Australian mining industry and the consequent modelling
   of fire scenarios in selected different mine layouts.
   Application of the simulation software package to the changing mine
   layouts requires experience to achieve realistic outcomes. Most
   Australian mines of size currently use a ventilation network simulation
   program. Under the project a small subroutine has been written to
   transfer the input data from the existing mine ventilation network
   simulation program to `Ventgraph'. This has been tested successfully. To
   understand fire simulation behaviour on the mine ventilation system, it
   is necessary to understood the possible effects of mine fires on various
   mine ventilation systems correctly first. Case studies demonstrating the
   possible effects of fires on some typical Australian coal mine
   ventilation circuits have been examined. The situation in which there is
   some gas make at the face and effects with fire have also been developed
   to emphasise how unstable and dangerous situations may arise.
   The primary objective of the part of the study described in this paper
   is to use mine fire simulation software to gain better understanding of
   how spontaneous combustion initiated fires can interact with the complex
   ventilation behaviour underground during a substantial fire. It focuses
   on the simulation of spontaneous combustion sourced heatings that
   develop into open fires. Further, it examines ventilation behaviour
   effects of spontaneous corribustion initiated pillar Fires and examines
   the difficulties these can be present if a ventilation reversal occurs.
   It also briefly examines simulation of use of the inertisation to assist
   in mine recovery.
   Mine fires are recognised across the world as a major hazard issue. New
   approaches allowing improvement in understanding their consequences have
   been developed as an aid in handling this complex area.},
ISBN = {1-920806-25-3},
Unique-ID = {WOS:000234289900028},
}

@inproceedings{ WOS:000228958000018,
Author = {JaJa, J and Smorul, M and McCall, F and Wang, Y},
Book-Group-Author = {IEEE Computer Society},
Title = {Scalable, reliable marshalling and organization of distributed large
   scale data onto enterprise storage environments},
Booktitle = {Twenty-Second IEEE/Thirteenth NASA Goddard Conference on Mass Storage
   Systems and Technologies, Proceedings: INFORMATION RETRIEVAL FROM VERY
   LARGE STORAGE SYSTEMS},
Year = {2005},
Pages = {197-201},
Note = {22nd IEEE Conference on Mass Storage Systems and Technologies/13th NASA
   Goddard Conference on Mass Storage Systems and Technologies, Monterey,
   CA, APR 11-14, 2005},
Organization = {IEEE Mass Storage Syst Tech Comm; NASA Goddard Space Flight Ctr},
Abstract = {Emerging technologies in high speed NAS, hierarchical storage management
   systems, and networked systems that virtualize interconnected storage
   over JP and fiber-channel networks, promise to consolidate distributed
   data stores onto large-scale professionally managed enterprise storage
   environments. We describe the software architecture of the PAWN
   (Producer Archive Workflow Network) environment that enables scalable,
   reliable marshalling and organization of distributed data into such
   enterprise storage environments. PAWN was initially developed to capture
   the core elements required for long term preservation of digital objects
   as identified by researchers in the digital library and archiving
   communities. In this paper, we show how PAWN can be extended to enable
   multiple clients at a number of distributed sites to prepare, organize,
   and bulk transfer large scale data onto clusters of servers that
   securely verify the integrity of the data, register the metadata, and
   store the data into an enterprise storage environment. PAWN allows
   detailed description, auditing, and organization of the data, and hence
   will allow for efficient management, access, and disaster recovery. The
   basic software components are based on open standards and web
   technologies, and hence are platform independent.},
DOI = {10.1109/MSST.2005.29},
ISBN = {0-7695-2318-8},
Unique-ID = {WOS:000228958000018},
}

@inproceedings{ WOS:000233625000039,
Author = {Jones, WB and Bester, G and Canning, A and Franceschetti, A and Graf, PA
   and Kim, K and Langou, J and Wang, LW and Dongarra, J and Zunger, A},
Editor = {Mezzacappa, A},
Title = {NanoPSE: Nanoscience problem solving environment for atomistic
   electronic structure of semiconductor nanostructures},
Booktitle = {SCIDAC 2005: SCIENTIFIC DISCOVERY THROUGH ADVANCED COMPUTING},
Series = {Journal of Physics Conference Series},
Year = {2005},
Volume = {16},
Pages = {277-282},
Note = {Conference of Scientific Discovery through Advanced Computing (SciDAC
   2005), San Francisco, CA, JUN 26-30, 2005},
Organization = {US Dept Energy Off Sci},
Abstract = {Researchers at the National Renewable Energy Laboratory and their
   collaborators have developed over the past similar to 10 years a set of
   algorithms for an atomistic description of the electronic structure of
   nanostructures, based on plane-wave pseudopotentials and
   configuration-interaction. The present contribution describes the first
   step in assembling these various codes into a single, portable,
   integrated set of software packages. This package is part of an ongoing
   research project in the development stage. Components of NanoPSE include
   codes for atomistic nanostructure generation and passivation, valence
   force field model for atomic relaxation, code for potential field
   generation, empirical pseudopotential method solver, strained linear
   combination of bulk bands method solver, configuration interaction
   solver for excited states, selection of linear algebra methods, and
   several inverse band structure solvers. Although not available for
   general distribution at this time as it is being developed and tested,
   the design goal of the NanoPSE software is to provide a software context
   for collaboration. The software package is enabled by fcdev, an
   integrated collection of best practice GNU software for open source
   development and distribution augmented to better support FORTRAN.},
DOI = {10.1088/1742-6596/16/1/038},
ISSN = {1742-6588},
EISSN = {1742-6596},
ResearcherID-Numbers = {Dongarra, Jack/G-4199-2019
   Langou, Julien/G-5788-2013
   Bester, Gabriel/I-4414-2012
   },
ORCID-Numbers = {Dongarra, Jack/0000-0003-3247-1782
   Langou, Julien/0000-0002-7803-1822
   Bester, Gabriel/0000-0003-2304-0817
   ZUNGER, ALEXANDER/0000-0001-5525-3003},
Unique-ID = {WOS:000233625000039},
}

@inproceedings{ WOS:000243684900068,
Author = {Kis, Gergely and Laszlo, Zoltan and Somogyi, Csongor and Sulyan, Tibor},
Editor = {Callaos, N and Lesso, W and Palesi, M},
Title = {Metaprogramming architecture to support dynamically changing
   transformation use cases},
Booktitle = {WMSCI 2005: 9th World Multi-Conference on Systemics, Cybernetics and
   Informatics, Vol 4},
Year = {2005},
Pages = {380-385},
Note = {9th World Multi-Conference on Systemics, Cybernetics and Informatics,
   Orlando, FL, JUL 10-13, 2005},
Abstract = {Meta-programming is generally based on a formal software model. The
   developer creates a model (analyzer) usually from source code, and then
   the modified model (transformator) is turned back into source code
   (generator). The formal model is determined by two factors: the source
   code of the software and the set of model transforming operations
   characterized by their use cases. An effective Meta-programming system
   requires a tool, which automates the construction of analyzers and
   generators. The input of this tool, called ``meta-generator{''}, is a
   representation of a yielding between the grammar syntax of the
   programming language and the operations related to the elements of a
   meta-model. The meta-model defines model elements which reflect the
   use-cases of the model transformations demanded. The output of the
   meta-generator consists of three programs: the analyzer, the source
   generator and the transformation library template. This paper introduces
   the MOFCOM meta-programming architecture.},
ISBN = {978-980-6560-56-7},
ResearcherID-Numbers = {László, Zoltán/H-2194-2012},
Unique-ID = {WOS:000243684900068},
}

@inproceedings{ WOS:000238998400224,
Author = {Kootsey, J. Mailen and McAuley, Grant and Bernal, Julie},
Book-Group-Author = {IEEE},
Title = {Building interactive simulations in Web pages without programming},
Booktitle = {2005 27TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN
   MEDICINE AND BIOLOGY SOCIETY, VOLS 1-7},
Series = {PROCEEDINGS OF ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING
   IN MEDICINE AND BIOLOGY SOCIETY},
Year = {2005},
Pages = {855-858},
Note = {27th Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society, Shanghai, PEOPLES R
   CHINA, AUG 31-SEP 03, 2005},
Organization = {IEEE Engn Med \& Biol Soc; Chinese Acad Engn Sci},
Abstract = {A software system is described for building interactive simulations and
   other numerical calculations in Web pages. The system is based on a new
   Java-based software architecture named NumberLinX (NLX) that isolates
   each function required to build the simulation so that a library of
   reusable objects could be assembled. The NLX objects are integrated into
   a commercial Web design program for coding-free page construction. The
   model description is entered through a wizard-like utility program that
   also functions as a model editor. The complete system permits very rapid
   construction of interactive simulations without coding. A wide range of
   applications are possible with the system beyond interactive
   calculations, including remote data collection and processing and
   collaboration over a network.},
DOI = {10.1109/IEMBS.2005.1616550},
ISSN = {1094-687X},
ISBN = {0-7803-8740-6},
ResearcherID-Numbers = {McAuley, Grant/I-2497-2013},
ORCID-Numbers = {McAuley, Grant/0000-0002-4083-5825},
Unique-ID = {WOS:000238998400224},
}

@inproceedings{ WOS:000234627800015,
Author = {Rendon, OMC and Pabon, FOM and Vargas, MJG and Guaca, JAH},
Book-Group-Author = {IEEE Computer Society},
Title = {Architectures for Web Services access from mobile devices},
Booktitle = {THIRD LATIN AMERICAN WEB CONGRESS, PROCEEDINGS},
Year = {2005},
Pages = {93-97},
Note = {3rd Latin American Web Congress, Buenos Aires, ARGENTINA, OCT 31-NOV 02,
   2005},
Organization = {Latin Amer Ctr Informat Studies; Ibero Amer Program Cooperat Sci \&
   Technol; Univ Chile, Ctr Web Res; Natl Univ La Pampa, Engn Sch; FONCyT;
   Argentine Soc Informat \& Operat Res; Int World Wide Web Conf Comm;
   European Taskforce Creating Human Machine Interfaces Similar Human,
   Human Commun},
Abstract = {The success of Web Services, an open Internet standards based
   technology, opens the door to the construction of business applications
   in the distributed computing world. As Web Services are based on open
   standards, it lets the integration of different kinds of software
   components, including the wireless and mobile applications. However, the
   Post PC World, dominated by mobile devices, is not completely ready to
   Web Service accesss, due to memory and processing constraints. Specific
   architectures for an easy and clear Web Services integration are
   required.
   This article introduces the main aspects related with Web Services
   access from mobile devices. Two proposals are presented for high and low
   end mobile devices, using Java 2 Platform, Micro Edition (J2ME) Web
   Services API (WSA) and Short Messaging Service (SMS) respectively, which
   define a specific architecture for each case.},
ISBN = {0-7695-2471-0},
ResearcherID-Numbers = {Caicedo Rendon, Oscar Mauricio/AFK-5377-2022},
ORCID-Numbers = {Caicedo Rendon, Oscar Mauricio/0000-0003-2223-947X},
Unique-ID = {WOS:000234627800015},
}

@inproceedings{ WOS:000234268900005,
Author = {Santoro, A and Quaglia, F},
Editor = {Boukerche, A and Turner, SJ and Roberts, D and Theodoropoulos, GK},
Title = {A version of MASM portable across different UNIX systems and different
   hardware Architectures},
Booktitle = {NINTH IEEE INTERNATIONAL SYMPOSIUM ON DISTRIBUTED SIMULATION AND
   REAL-TIME APPLICATIONS, PROCEEDINGS},
Series = {IEEE ACM International Symposium on Distributed Simulation and Real-Time
   Applications},
Year = {2005},
Pages = {35-42},
Note = {9th IEEE International Symposium on Distributed Simulation and Real-Time
   Applications (DS-RT 2005), Montreal, CANADA, OCT 10-12, 2005},
Organization = {IEEE Comp Soc TCPP; IEEE Comp Soc TCS; IEEE Comp Soc TCCA; ACM SIGSIM},
Abstract = {MAgic State Manager (MASM) is a recently developed software architecture
   for completely transparent check-pointing/recovery in support of
   optimistic synchronization in the High Level Architecture. In the
   original design, MASM relies on (i) user level machine dependent
   modules, (ii) patches for specific versions of the LINUX kernel and
   (iii) static linking of specific application libraries, all of them
   required for performing ad-hoc, low level memory management operations
   associated with optimistic synchronization requirements. In this paper
   we propose a complete re-engineering of this software architecture which
   allows all those memory management tasks to be carried out through user
   level, machine independent modules, with the additional advantage of
   avoiding the need for static linking of specific application libraries,
   thus achieving portability of MASM across different UNIX systems and
   different computer architectures.},
DOI = {10.1109/DISTRA.2005.8},
ISSN = {1550-6525},
ISBN = {0-7695-2462-1},
Unique-ID = {WOS:000234268900005},
}

@inproceedings{ WOS:000230455800017,
Author = {Santoro, A and Quaglia, F},
Book-Group-Author = {IEEE Computer Society},
Title = {Transparent state management for optimistic synchronization in the high
   level architecture},
Booktitle = {WORKSHOP ON PRINCIPLES OF ADVANCED AND DISTRIBUTED SIMULATION,
   PROCEEDINGS},
Year = {2005},
Pages = {171-180},
Note = {19th Workshop on Principles of Advanced and Distributed Simulation (PADS
   2005), Monterey, CA, JUN 01-03, 2005},
Organization = {ACM SIGSIM; IEEE Computer Soc, TCSIM; SCS},
Abstract = {In this paper we present the design and implementation of a software
   architecture, namely MAgic State Manager (MASM), to be employed within a
   Run-Time Infrastructure (RTI) in support of HLA federations. MASM allows
   performing checkpointing/recovery of the state of a federate in a way
   completely transparent to the federate itself thus providing the
   possibility of demanding to the RTI any task related to state management
   in optimistic synchronization. Differently from existing proposals,
   through our approach the federate programmer is neither required to
   supply modules for state management within the federate code, nor to
   explicitly interface the federate code with existing, third party
   checkpointing/recovery libraries. Hence, the federate programmer is
   completely relieved from the burden of facing state management issues.
   Some experimental results demonstrating minimal run-time overhead
   introduced by MASM are also reported for two case studies, namely an
   interconnection network simulation and a personal communication system
   simulation.},
DOI = {10.1109/PADS.2005.34},
ISBN = {0-7695-2383-8},
Unique-ID = {WOS:000230455800017},
}

@inproceedings{ WOS:000234717300004,
Author = {Suleman, H and Feng, K and Mhlongo, S and Omar, M},
Editor = {Fox, EA and Neuhold, EJ and Premsmit, P and Wuwongse, V},
Title = {Flexing digital library systems},
Booktitle = {DIGITAL LIBRARIES: IMPLEMENTING STRATEGIES AND SHARING EXPERIENCES,
   PROCEEDINGS},
Series = {Lecture Notes in Computer Science},
Year = {2005},
Volume = {3815},
Pages = {33-37},
Note = {8th International Conference on Asian Digital Libraries (ICADL 2005),
   Bangkok, THAILAND, DEC 12-15, 2005},
Abstract = {Digital library systems with monolithic architectures are rapidly facing
   extinction as the discipline adopts new practices in software
   engineering, such as component-based architectures and Web Services.
   Past projects have attempted to demonstrate and justify the use of
   components through the construction of systems such as NCSTRL and
   ScholNet. This paper describes current work to push the boundaries of
   digital library research and investigate a range of projects made
   feasible by the availability of suitable components. These projects
   include: the ability to assemble component-based digital libraries using
   a visual interface; the design of customisable user interfaces and
   workflows; the packaging and installation of systems based on formal
   descriptions; and the shift to a component farm for cluster-like
   scalability. Each of these sub-projects makes a potential individual
   contribution to research in architectures, while sharing a common
   underlying framework. Together, all of these projects support the
   hypothesis that a consistent component architecture and suite of
   components can provide the basis for advanced research into flexible
   digital library architectures.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {3-540-30850-4},
ResearcherID-Numbers = {Mhlongo, Siyabonga/R-7333-2016
   },
ORCID-Numbers = {Mhlongo, Siyabonga/0000-0001-8203-5984
   Suleman, Hussein/0000-0002-4196-1444},
Unique-ID = {WOS:000234717300004},
}

@article{ WOS:000223593200007,
Author = {Antoniol, G and Di Penta, M and Masone, G and Villano, U},
Title = {Compiler hacking for source code analysis},
Journal = {SOFTWARE QUALITY JOURNAL},
Year = {2004},
Volume = {12},
Number = {4},
Pages = {383-406},
Month = {DEC},
Note = {3rd IEEE International Workshop on Source Code Analysis and
   Manipulation, AMSTERDAM, NETHERLANDS, SEP 26-27, 2003},
Organization = {IEEE Comp Soc, TC Software Engn; Netherlands Org Sci Res; Royal
   Netherlands Acad Arts \& Sci},
Abstract = {Many activities related to software quality assessment and improvement,
   such as empirical model construction, data flow analysis, testing or
   reengineering, rely on static source code analysis as the first and
   fundamental step for gathering the necessary input information. In the
   past, two different strategies have been adopted to develop tool suites.
   There are tools encompassing or implementing the source parse step,
   where the parser is internal to the toolkit, and is developed and
   maintained with it. A different approach builds tools on the top of
   external already-available components such as compilers that output the
   program abstract syntax tree, or that make it available via an API.
   This paper discusses techniques, issues and challenges linked to
   compiler patching or wrapping for analysis purposes. In particular,
   different approaches for accessing the compiler parsing information are
   compared, and the techniques used to decouple the parsing front end from
   the analysis modules are discussed.
   Moreover, the paper presents an approach and a tool, XOgastan, developed
   exploiting the gcc/g++ ability to save a representation of the
   intermediate abstract syntax tree. XOgastan translates the gcc/g++
   dumped abstract syntax tree format into a Graph eXchange Language
   representation, which makes it possible to take advantage of currently
   available XML tools for any subsequent analysis step. The tool is
   illustrated and its design discussed, showing its architecture and the
   main implementation choices made.},
DOI = {10.1023/B:SQJO.0000039794.29432.7e},
ISSN = {0963-9314},
EISSN = {1573-1367},
ResearcherID-Numbers = {Di Penta, Massimiliano/AAF-9656-2021
   },
ORCID-Numbers = {Di Penta, Massimiliano/0000-0002-0340-9747},
Unique-ID = {WOS:000223593200007},
}

@article{ WOS:000224065500004,
Author = {Kenny, JP and Benson, SJ and Alexeev, Y and Sarich, J and Janssen, CL
   and McInnes, LC and Krishnan, M and Nieplocha, J and Jurrus, E and
   Fahlstrom, C and Windus, TL},
Title = {Component-based integration of chemistry and optimization software},
Journal = {JOURNAL OF COMPUTATIONAL CHEMISTRY},
Year = {2004},
Volume = {25},
Number = {14},
Pages = {1717-1725},
Month = {NOV 15},
Abstract = {Typical scientific software designs make rigid assumptions regarding
   programming language and data structures, frustrating software
   interoperability and scientific collaboration. Component-based software
   engineering is an emerging approach to managing the increasing
   complexity of scientific software. Component technology facilitates code
   interoperability and reuse. Through the adoption of methodology and
   tools developed by the Common Component Architecture Forum, we have
   developed a component architecture for molecular structure optimization.
   Using the NWChem and Massively Parallel Quantum Chemistry packages, we
   have produced chemistry components that provide capacity city for energy
   and energy derivative evaluation. We have constructed geometry
   optimization applications by integrating the Toolkit for Advanced
   Optimization, Portable Extensible Toolkit for Scientific Computation,
   and Global Arrays packages, which provide optimization and linear
   algebra capabilities. We present a brief overview of the component
   development process and a description of abstract interfaces for
   chemical optimizations. The components conforming to these abstract
   interfaces allow the construction of applications using different
   chemistry and mathematics packages interchangeably. Initial numerical
   results for the component software demonstrate good performance, and
   highlight potential research enabled by this platform. (C) 2004 Wiley
   Periodicals, Inc.},
DOI = {10.1002/jcc.20091},
ISSN = {0192-8651},
EISSN = {1096-987X},
ORCID-Numbers = {Alexeev, Yuri/0000-0001-5066-2254
   Windus, Theresa/0000-0001-6065-3167},
Unique-ID = {WOS:000224065500004},
}

@article{ WOS:000224486200011,
Author = {Johnson, ED and Pancoast, PE and Mitchell, JA and Shyu, CR},
Title = {Design and evaluation of a personal digital assistant-based alerting
   service for clinicians},
Journal = {JOURNAL OF THE MEDICAL LIBRARY ASSOCIATION},
Year = {2004},
Volume = {92},
Number = {4},
Pages = {438-444},
Month = {OCT},
Abstract = {Purpose: This study describes the system architecture and user
   acceptance of a suite of programs that deliver information about newly
   updated library resources to clinicians' personal digital assistants
   (PDAs).
   Description: Participants received headlines delivered to their PDAs
   alerting them to new books, National Guideline Clearinghouse guidelines,
   Cochrane Reviews, and National Institutes of Health (NIH) Clinical
   Alerts, as well as updated content in UpToDate, Harrison's Online,
   Scientific American Medicine, and Clinical Evidence. Participants could
   request additional information for any of the headlines, and the
   information was delivered via email during their next synchronization.
   Participants completed a survey at the conclusion of the study to gauge
   their opinions about the service.
   Results/Outcome: Of the 816 headlines delivered to the 16 study
   participants' PDAs during the project, Scientific American Medicine
   generated the highest proportion of headline requests at 35\%. Most
   users of the PDA Alerts software reported that they learned about new
   medical developments sooner than they otherwise would have, and half
   reported that they learned about developments that they would not have
   heard about at all. While some users liked the PDA platform for
   receiving headlines, it seemed that a Web database that allowed tailored
   searches and alerts could be configured to satisfy both PDA-oriented and
   email-oriented users.},
ISSN = {1536-5050},
ResearcherID-Numbers = {Johnson, E. Diane/G-8403-2015},
ORCID-Numbers = {Johnson, E. Diane/0000-0002-0039-8288},
Unique-ID = {WOS:000224486200011},
}

@article{ WOS:000224083100030,
Author = {Dudbridge, F and Carver, T and Williams, GW},
Title = {Pelican: pedigree editor for linkage computer analysis},
Journal = {BIOINFORMATICS},
Year = {2004},
Volume = {20},
Number = {14},
Pages = {2327-2328},
Month = {SEP 22},
Abstract = {Linkage analysis software requires an input text file that describes the
   structure of the pedigrees to be analysed. Manual creation of these
   files is tedious and error-prone, and a graphical input tool is
   desirable. This is currently only available in commercial packages that
   include much greater functionality. We have therefore developed Pelican,
   a lightweight graphical pedigree editor for rapid construction of
   linkage pedigree files and diagrams.},
DOI = {10.1093/bioinformatics/bth231},
ISSN = {1367-4803},
EISSN = {1460-2059},
ORCID-Numbers = {Dudbridge, Frank/0000-0002-8817-8908},
Unique-ID = {WOS:000224083100030},
}

@article{ WOS:000223284900002,
Author = {Li, H and Wu, WY},
Title = {A new approach to image-based realistic architecture modeling with
   featured solid library},
Journal = {AUTOMATION IN CONSTRUCTION},
Year = {2004},
Volume = {13},
Number = {5},
Pages = {555-564},
Month = {SEP},
Abstract = {To overcome restrictions of present modeling techniques, and to approach
   quick and convenient 3D architecture modeling, we present a realistic
   modeling system on the basis of a featured solid library. The system
   incorporates reference library techniques to build up realistic models
   from a single image. In our approach, parameterized solid components are
   mapped to 2D featured projection graphs on image plane, and matching
   features of all component graphs with the resource image results in 3D
   realistic scene, whether complex or irregular. Besides, a suite of
   texture toolkit is implemented to rectify flaws of texture mapping and
   to enhance modeling reality. Our approach works feasibly and robustly
   with single-vision images taken from arbitrary view angles, and the
   efficiency of our approach is proved in systematic modeling experiments.
   As our approach complemented image-based modeling techniques in
   construction CAD and digital city systems, it could be of popular use in
   relevant modeling software. (C) 2004 Elsevier B.V. All rights reserved..},
DOI = {10.1016/j.autcon.2004.04.009},
ISSN = {0926-5805},
EISSN = {1872-7891},
ResearcherID-Numbers = {Wang, Shengwei/C-4533-2008},
ORCID-Numbers = {Wang, Shengwei/0000-0002-9684-590X},
Unique-ID = {WOS:000223284900002},
}

@article{ WOS:000222919900008,
Author = {Rizk, NJ},
Title = {Parallelization of IBD computation for determining genetic disease maps},
Journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
Year = {2004},
Volume = {16},
Number = {9},
Pages = {933-943},
Month = {AUG 10},
Abstract = {A number of software packages are available for the construction of
   comprehensive human genetic maps. In this paper we parallelize the
   widely used package Genehunter. We restrict our attention to only one
   function of the package, namely the computations of Identity By Descent
   (IBD) genes of a family. We use a master-slave model with the Message
   Passing Interface parallel environment. Our tests are done on two
   different architectures: a network of workstations and a shared memory
   multiprocessor. A new and efficient strategy to classify the
   parallelization of genetic linkage analysis programs results from our
   experiments. The classification is based on values of parameters which
   affect the complexity of the computation. Copyright (C) 2004 John Wiley
   Sons, Ltd.},
DOI = {10.1002/cpe.814},
ISSN = {1532-0626},
Unique-ID = {WOS:000222919900008},
}

@article{ WOS:000222599200005,
Author = {Yeh, AGO and Qiao, JJ},
Title = {Component-based approach in the development of a knowledge-based
   planning support system (KBPSS). Part 1: The architecture of KBPSS},
Journal = {ENVIRONMENT AND PLANNING B-PLANNING \& DESIGN},
Year = {2004},
Volume = {31},
Number = {4},
Pages = {517-537},
Month = {JUL},
Abstract = {Urban planners need to have a flexible, interactive, and intelligent
   computer-based toot to support their use of analytical models and to
   structure their planning process. Existing spatial information systems
   have not developed appropriate procedures and mechanisms to enable easy
   construction and manipulation of models within their system environment,
   and to allow various information resources to be shared by different
   application systems and platforms. Recent research and development of
   planning support systems (PSS) provide a range of interesting concepts
   and frameworks that can be adopted by system developers to design a
   multifunctional and planning-specific decision support system to meet
   the multifaceted needs of the planning process. In this paper we aim to
   go beyond the current approaches of PSS and spatial decision support
   systems (SDSS) in designing a knowledge-based PSS (KBPSS) that uses the
   component-based software development (CBSD) approach. We introduce the
   features of CBSD and its technical advantages in the design of a
   multifunctional PSS. An architecture of a component-based KBPSS is
   subsequently proposed which incorporates four essential component
   systems: MapObjects GIS, Database Manager database management system,
   ModelObjects model-management system, and KBSAgents knowledge-based
   system. The first two components are based on proprietary components
   systems and can be easily adapted into the KBPSS environment. The
   ModelObjects and KBSAgents components were developed by the authors.
   Based on these components and their associated information layers, two
   application subsystems are constructed: a Model and Knowledge
   Development System and an Intelligent Model Selection System. These
   application systems can be used to assist users and model developers to
   build new models or to select predefined models from an existing model
   library for their problems. In order to implement these functions, we
   propose and define a number of procedures and techniques for component
   communication and model and knowledge development.},
DOI = {10.1068/b2721},
ISSN = {0265-8135},
EISSN = {1472-3417},
ResearcherID-Numbers = {Yeh, Anthony/A-4754-2010},
ORCID-Numbers = {Yeh, Anthony/0000-0002-0587-0588},
Unique-ID = {WOS:000222599200005},
}

@article{ WOS:000224011500002,
Author = {Gervasi, O and Lagana, A},
Title = {SIMBEX: a portal for the a priori simulation of crossed beam experiments},
Journal = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
Year = {2004},
Volume = {20},
Number = {5},
Pages = {703-715},
Month = {JUN},
Note = {International Conference on Computational Sciences, St Petersburg,
   RUSSIA, JUN, 2003},
Abstract = {The architecture and the computational kernels of Simulation of Crossed
   Molecular Beam Experiments, an Internet portal managing the simulation
   of elementary bimolecular processes as those occurring in crossed beam
   apparatuses, is discussed.
   The construction of this portal is our contribution to project 003/001
   of the COST in Chemistry action D23 (METACHEM: Metalaboratories for
   complex computational applications in chemistry).
   The portal is specifically designed to support the collaborative efforts
   of various Computational Chemistry and Computer Science European
   laboratories aimed at building an a priori molecular simulator based on
   a Grid infrastructure.
   Such an environment makes use of free-software packages and was
   implemented using Web technologies. (C) 2004 Elsevier B.V. All rights
   reserved.},
DOI = {10.1016/j.future.2003.11.028},
ISSN = {0167-739X},
EISSN = {1872-7115},
ResearcherID-Numbers = {Lagana, Antonio/E-8215-2014
   Gervasi, Osvaldo/AAH-2792-2019},
ORCID-Numbers = {Lagana, Antonio/0000-0002-3886-7342
   Gervasi, Osvaldo/0000-0003-4327-520X},
Unique-ID = {WOS:000224011500002},
}

@article{ WOS:000220430800014,
Author = {Deng, W and Shark, LK and Matuszewski, J and Smith, JP and Cavaccini, G},
Title = {CAD model-based inspection and visualisation for 3D non-destructive
   testing of complex aerostructures},
Journal = {INSIGHT},
Year = {2004},
Volume = {46},
Number = {3},
Pages = {157-161},
Month = {MAR},
Abstract = {When performing non-destructive testing of aerostructures with complex
   geometrical shapes and curved surfaces, two problems arise: one is
   generation of an appropriate sequence of 3D coordinates and orientations
   for the inspection probe to follow complex component contours; the other
   is interpretation and characterisation of defects based on 2D NDT
   images. By utilising the information from CAD models, this paper
   presents a 3D NDT software package to aid planning of NDT data
   acquisition via 3D simulation, and to aid defect visualisation and
   measurement in 3D by superimposing 2D NDT images on their 3D CAD models.
   The particular noteworthy and unique characteristics of the software are
   (a) a versatile 3D CAD model editing tool for extraction of CAD features
   and construction of simplified CAD models; (b) 3D scanning simulation
   for automatic generation of probe coordinates and orientations with
   respect to curved component surfaces; (c) tools for aligning and
   projecting a 3D CAD model on to its 2D NDT image; and (d) texture
   mapping to project a NDT image on to its 3D CAD model surface. Using
   real aircraft components with their radiographic, ultrasonic and
   thermographic images as examples, the software is shown to offer a
   unique, versatile and powerful CAD-based 3D NDT environment for fast and
   reliable structure integrity assessment of complex aerostructures.},
DOI = {10.1784/insi.46.3.157.55515},
ISSN = {1354-2575},
EISSN = {1754-4904},
Unique-ID = {WOS:000220430800014},
}

@inproceedings{ WOS:000228557202111,
Author = {Baghai-Wadji, AR and Wagner, RC},
Editor = {Yuhas, MP},
Title = {Universal functions for the analysis of electromagnetic interactions in
   SAW devices},
Booktitle = {2004 IEEE Ultrasonics Symposium, Vols 1-3},
Series = {ULTRASONICS SYMPOSIUM},
Year = {2004},
Pages = {2015-2018},
Note = {IEEE Ultrasonics Symposium, Montreal, CANADA, AUG 23-27, 2004},
Organization = {IEEE Ultrason Ferroelect \& Frequency Control Soc},
Abstract = {In spite of the availability of a large number of commercial software
   packages for the analysis of electromagnetic fields, SAW devices defy a
   satisfying usage of these packages. This defiance is primarily due to
   the strong anisotropy and the presence of a large number of
   geometrically-complex thin metallic structures positioned on parallel
   interfaces. The method of moments (MoM) is particularly suitable for
   this class of problems, despite its deficiencies. To combat several
   problems in the MoM applications we have developed the method of
   Fast-MoM utilizing a number of Universal Functions. In this paper we
   illustrate the construction and processing of Universal Functions for
   the EM analysis in SAW devices, and point out strategies for
   accelerating the computations and enhancing the accuracy of the
   numerical results.},
ISSN = {1051-0117},
ISBN = {0-7803-8413-X},
ResearcherID-Numbers = {Baghai-Wadji, Alireza/ABG-1782-2021},
Unique-ID = {WOS:000228557202111},
}

@inproceedings{ WOS:000225861500061,
Author = {Benin, AV},
Editor = {Zavadskas, EK and Vainiunas, P and Mazzolani, FM},
Title = {Consideration of discreet allocation of reinforcement for the purpose of
   calculation of the elements made of reinforced concrete},
Booktitle = {8TH INTERNATIONAL CONFERENCE ON MODERN BUILDING MATERIALS, STRUCTURES
   AND TECHNIQUES},
Year = {2004},
Pages = {352-356},
Note = {8th International Conference on Modern Building Materials, Structures
   and Techniques, Vilnius, LITHUANIA, MAY 19-21, 2004},
Organization = {Vilnius Gediminas Tech Univ; Int Assoc Bridge \& Structural Engn;
   European Council Civil Engineers; Assoc European Civil Engn Fac;
   Lithuanian Acad Sci},
Abstract = {The report deals with the results of investigations carried out in the
   field of elaboration of an algorithm allowing to operatively obtain
   solutions for variable engineering problems concerned with a non-linear
   deformation of reinforced concrete constructions under the conditions of
   static and dynamic loading. In contrast to the already existing software
   packages intended for calculating reinforced concrete structures, where
   the ferro-concrete reinforcement is ``spread{''} along the body of a
   component, the proposed algorithm makes an account of the real discrete
   allocation of the ferro-concrete reinforcement.},
ISBN = {9986-05-757-4},
ResearcherID-Numbers = {Benin, Andrey V/E-9292-2017},
ORCID-Numbers = {Benin, Andrey V/0000-0001-5646-0354},
Unique-ID = {WOS:000225861500061},
}

@inproceedings{ WOS:000223848800012,
Author = {Boehm, B and Bhuta, J and Garlan, D and Gradman, E and Huang, LG and
   Lam, A and Madachy, R and Medvidovic, N and Meyer, K and Meyers, S and
   Perez, G and Reinholtz, K and Roshandel, R and Rouquette, N},
Book-Group-Author = {ieee computer society},
Title = {Using empirical testbeds to accelerate technology maturity and
   transition: The SCRover experience},
Booktitle = {2004 INTERNATIONAL SYMPOSIUM ON EMPIRICAL SOFTWARE ENGINEERING,
   PROCEEDINGS},
Year = {2004},
Pages = {117-126},
Note = {3rd International Symposium on Empirical Software Engineering, Redondo
   Beach, CA, AUG 19-20, 2004},
Organization = {ACM SigSoft; IEEE Comp Soc},
Abstract = {This paper is an experience report on a first attempt to develop and
   apply a new form of software: a full-service empirical testbed designed
   to evaluate alternative software dependabiliy; technologies, and to
   accelerate their maturation and transition into project use. The SCRover
   testbed includes not only the specifications, code, and hardware of a
   public safety robot, but also the package of instrumentation, scenario
   drivers, seeded defects, experimentation guidelines, and comparative
   effort and defect data needed to facilitate technology evaluation
   experiments.
   The SCRover testbed's initial operational capability has been recently
   applied to empirically evaluate two architecture definition languages
   (ADLs) and toolsets, Mae and AcmeStudio. The testbed evaluation showed
   (1) that the ADL-based toolsets were complementary and cost-effective to
   apply to mission-critical systems; (2) that the testbed was
   cost-effective to use by researchers; and (3) that collaboration in
   testbed use by re searchers and the Jet Propulsion Laboratory (JPL)
   project users resulted in actions to accelerate technology maturity and
   transition into project use. The evaluation also identified a number of
   lessons learned for improving the SCRover testbed. and for development
   and application of future technology evaluation testbeds.},
ISBN = {0-7695-2165-7},
ResearcherID-Numbers = {Reinholtz, Kirk/GOV-6464-2022
   Celar, Stipe/G-4728-2017},
ORCID-Numbers = {Reinholtz, Kirk/0000-0003-3790-4671
   Celar, Stipe/0000-0003-4234-5819},
Unique-ID = {WOS:000223848800012},
}

@inproceedings{ WOS:000224340800031,
Author = {Danielyan, GL},
Editor = {Jaroszewicz, Z and Powichrowska, E and Szyjer, M},
Title = {Multi channel fiber optic bundles and sensors for biomedical application},
Booktitle = {SYSTEMS OF OPTICAL SECURITY 2003},
Series = {Proceedings of SPIE},
Year = {2004},
Volume = {5566},
Pages = {198-203},
Note = {Systems of Optical Security 2003 Conferene, Warsaw, POLAND, DEC 11-12,
   2003},
Organization = {Inst Appl Opt; SPIE; Minist Econ, Labour \& Social Policy Poland},
Abstract = {The Special Ordered Structures of Specialty Fiber included into
   Multifunctional and Multi Channel Fiber Optic Bundles (MFOB) and Sensors
   are proposed. Optimal construction of fiber optic channels in the MFOB
   exhibit reduced speckle noise and high intensity transmission resulting
   from spatial homogeneity and symmetry of radiation.
   Improved new type of the Fibers: Metal Coated Multimode, Special Plastic
   Coated, Fibers for UV-VIS, Fibers for VIS- NIR spectral Range, Fibers
   for NIR and IR spectral range. Hexagonal package of sensitive end of the
   MFOB structures designed with different type and fiber core diameters
   fibers are transfer-red into the different configured input /output
   optical channels.
   For fluorescence spectroscopy and FDT Diagnostic described optimal
   arrangement with 7- 256 Fibers included into MFOB structure. Remote
   spectroscopic Probes are used for ``in Vivo{''} or ``in Vitro{''}
   experimental devices. Sensors with MFOB probes bifurcated from two up to
   seven channels are used for process photometry and for mini- fiber
   spectrometric devices. Customized Software and flexible numerical
   simulations for data analysis are based into two levels of programming :
   - micro program part for ATMEL microprocessor, Visual C++ version 6.0
   for PC computers with Windows -98-2000Me Programs.
   Advanced Applications of MFOB type of probes show some features for
   Biomedical Remote Sensing Systems :High Optical Throughput for Special
   Fluorescence Probes; High Stability for fool spectral range; Minimal
   cross link between fibers into MFOB-M structures; High stability for
   Endoscopes and sterilization proof tested solutions; Quality Controlled
   Scattered Reflection MFOB. MFOB structures designed with Mini Fiber
   Spectrometers show high spectral resolution (7-12 nm) and possibility to
   combine in one set different function: Normalization function for
   different light sources, Multi scan measurements with adjusted time
   duration, Spectral band analysis (including integrated characters for
   selected wavebands),Fast time resolution for selected types of scanning
   characters.},
DOI = {10.1117/12.577574},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-5512-1},
Unique-ID = {WOS:000224340800031},
}

@incollection{ WOS:000228007400006,
Author = {Deconinck, G and De Florio, V and Belmans, R},
Editor = {DeLemos, R and Gacek, C and Romanovsky, A},
Title = {Architecting distributed control applications based on (Re-)configurable
   middleware},
Booktitle = {ARCHITECTING DEPENDABLE SYSTEMS II},
Series = {Lecture Notes in Computer Science},
Year = {2004},
Volume = {3069},
Pages = {123-143},
Abstract = {Industrial distributed automation applications call for reusable
   software components, without endangering dependability. The DepAuDE
   architecture provides middleware to integrate fault tolerance support
   into such applications based on a library of detection, reconfiguration
   and recovery functions, and a language for expressing non-functional
   services, such as configuration and fault tolerance. At run time, a
   middleware layer orchestrates the execution of recovery actions. The
   paper further provides a hierarchical model, consisting of a dedicated
   intra-site local area network and an open inter-site wide area network,
   to deal with the different characteristics and requirements for
   dependability and quality-of-service, when such applications rely on
   off-the-shelf communication technology to exchange management or control
   information. The middleware can be dynamically reconfigured when the
   environment changes. This methodology has been integrated in the
   distributed automation system of an electrical substation.},
ISSN = {0302-9743},
EISSN = {1611-3349},
ISBN = {3-540-23168-4},
ResearcherID-Numbers = {Deconinck, Geert/H-9508-2014
   },
ORCID-Numbers = {Deconinck, Geert/0000-0002-2225-3987
   De Florio, Vincenzo/0000-0003-1443-9367},
Unique-ID = {WOS:000228007400006},
}

@article{ WOS:000231433800002,
Author = {Emerson, MJ and Sztipanovits, J and Bapty, T},
Title = {A MOF-based metamodeling environment},
Journal = {JOURNAL OF UNIVERSAL COMPUTER SCIENCE},
Year = {2004},
Volume = {10},
Number = {10},
Pages = {1357-1382},
Note = {5th Workshop on Formal Specification of Computer-Based Systems (FSCBS),
   Brno, CZECH REPUBLIC, MAY, 2004},
Abstract = {The Meta Object Facility (MOF) forms one of the core standards of the
   Object Management Group's Model Driven Architecture. It has several
   use-cases, including as a repository service for storing abstract models
   used in distributed object-oriented software development, a development
   environment for generating CORBA IDL, and a metamodeling language for
   the rapid specification, construction, and management of domain-specific
   technology-neutral modeling languages. This paper will focus on the use
   of MOF as a metamodeling language and describe our latest work on
   changing the MIC metamodeling environment from UML/OCL to MOF. We have
   implemented a functional graphical metamodeling environment based on the
   MOF v1.4 standard using GME and GReAT. This implementation serves as a
   testament to the power of formally well-defined metamodeling and
   metamodel-based model transformation approaches. Furthermore, our work
   gave us an opportunity to evaluate sevaral important features of MOF
   v1.4 as a metamodeling language:
   - Completeness of MOF v1.4 for defining the abstract syntax for complex
   ( multiple aspect) DSML-s
   - The Package concept for composing and reusing metamodels
   - Facilities for modeling the mapping between the abstract and concrete
   syntax of DSML-s.},
ISSN = {0948-695X},
ORCID-Numbers = {Sztipanovits, Janos/0000-0002-8360-6299},
Unique-ID = {WOS:000231433800002},
}

@inproceedings{ WOS:000225884000009,
Author = {Ghaly, AM and Losure, M},
Editor = {Arabnia, HR and Ajwa, IA and Gravvanis, GA},
Title = {Deployment of modeling and simulation techniques to facilitate
   visualization of complex structures},
Booktitle = {MSV'04 \& AMCS'04, PROCEEDINGS},
Year = {2004},
Pages = {60+},
Note = {International Conference on Modeling, Simulation and Visualization
   Methods/International Conference on Algorithmic Mathematics and Computer
   Sciences, Las Vegas, NV, JUN 21-24, 2004},
Abstract = {Some constructed facilities are distinguished by their tremendous size,
   special features, or complicated nature. Examples of these structures
   are suspension or cable-stayed bridges, towers, domes, shells, sea
   platforms, dams, and tunnels. The building of such structures requires
   meticulous planning and great care in construction. These structures
   constitute a formidable challenge to the engineer because their design
   is usually unprecedented and their construction involves unconventional
   techniques. Advances in computer simulation make it possible to simulate
   complicated construction techniques. Structural components such as
   foundations, columns, girders, beams, and slabs are dissected into
   separate entities. The purpose of this procedure is to show how all the
   individual components of the structure come together to form its
   skeleton and its final shape. This is achieved in this project using
   extensive modeling with 3ds Max computer package. This software enables
   the user to construct a model of the structure, and then use this model
   to create a movie showing the sequence of the construction operation.
   The user can detail instructions regarding the movement of the camera
   used in making the movie. The camera is one of many tools available in
   the program that can be positioned at desired angles, elevations, or
   distances with respect to the structure. Computer simulation of
   sophisticated construction techniques can serve as an invaluable tool
   that helps the viewer visualize a structure that is otherwise difficult
   to envision.},
ISBN = {1-932415-34-3},
Unique-ID = {WOS:000225884000009},
}

@inproceedings{ WOS:000225461801364,
Author = {Kootsey, JM and Siriphongs, D and McAuley, G},
Book-Group-Author = {ieee},
Title = {Building interactive simulations in a Web page design program},
Booktitle = {PROCEEDINGS OF THE 26TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOLS 1-7},
Series = {PROCEEDINGS OF ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING
   IN MEDICINE AND BIOLOGY SOCIETY},
Year = {2004},
Volume = {26},
Number = {1-7},
Pages = {5166-5168},
Note = {26th Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society, San Francisco, CA, SEP
   01-05, 2004},
Organization = {IEEE Engn Med \& Biol Soc; Whitaker Fdn; Cyberonics; NIH; NIBIB; NIDOCD;
   NINDS},
Abstract = {A new Web software architecture, NumberLinX (NLX), has been integrated
   into a commercial Web design program to produce a drag-and-drop
   environment for building interactive simulations. NLX is a library of
   reusable objects written in Java, including input, output, calculation,
   and control objects. The NLX objects were added to the palette of
   available objects in the Web design program to be selected and dropped
   on a page. Inserting an object in a Web page is accomplished by adding a
   template block of HTML code to the page file. HTML parameters in the
   block must be set to usersupplied values, so the HTML code is generated
   dynamically, based on user entries in a popup form. Implementing the
   object inspector :for each object permits the user to edit object
   attributes in a form window. Except for model definition, the
   combination of the NLX architecture and the Web design program permits
   construction of interactive simulation pages without writing or
   inspecting code.},
ISSN = {1094-687X},
ISBN = {0-7803-8439-3},
ResearcherID-Numbers = {McAuley, Grant/I-2497-2013},
ORCID-Numbers = {McAuley, Grant/0000-0002-4083-5825},
Unique-ID = {WOS:000225461801364},
}

@inproceedings{ WOS:000225279900051,
Author = {Kopczynski, JA and Dickson, B and Weiss, GJ},
Book-Group-Author = {asme},
Title = {Common electronic platform for the steam turbine and generator controls;
   Upgrade installed at the Mt. Poso Cogeneration Power Station - Case
   study},
Booktitle = {PROCEEDINGS OF 2004 ASME POWER},
Year = {2004},
Pages = {379-387},
Note = {ASME Power Conference 2004, Baltimore, MD, MAR 30-APR 01, 2004},
Organization = {Amer Soc Mech Engineers, Power Div; ASME, Fuels \& Combust Technologies
   Div},
Abstract = {The scope of modern power plant controls usually includes plant DCS,
   boiler control and protection, steam/gas turbine governor and
   protection, auxiliaries control, automatic voltage controller, automatic
   synchronizer and operator/engineering stations. Usually these control
   packages come from different manufacturers (OEM). They are typically
   based on various electronic hardware and software platforms. Different
   communication protocols often present problems during system
   integration; and maintenance costs of these various electronic hardware
   and software platforms are normally greater than that of a stand alone
   system.
   Advantages of an integrated, distributed, open architecture, digital
   system, (Fig. 1) which covers all the power plant needs are discussed in
   this paper. A common electronic hardware/ software platform allows
   optimization of the new constructions and upgrades, shorten delivery and
   commissioning time, and improve availability and safety of the new and
   upgraded power plants.
   Specific benefits of this concept are presented in the Mt. Poso controls
   upgrade Case Study.
   The common electronic hardware/ software platform installed at Mt. Poso
   allowed optimization of the upgrade, shortened commissioning time,
   improved availability, reliability and safety and reduced maintenance
   cost of the control systems.},
ISBN = {0-7918-4162-6},
Unique-ID = {WOS:000225279900051},
}

@inproceedings{ WOS:000227465800031,
Author = {Liu, CL and Pun, SK and Pichen, D and Rice, M},
Editor = {Yaowu, W and Shen, Q},
Title = {Development of an Internet supportive platform for online teaching and
   learning of construction planning and scheduling},
Booktitle = {PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON CONSTRUCTION \& REAL
   ESTATE MANAGEMENT},
Year = {2004},
Pages = {108-112},
Note = {International Conference on Construction and Real Estate Management,
   Hong Kong, PEOPLES R CHINA, DEC 06-07, 2004},
Organization = {Harbin Inst Technol; Hong Kong Polytech Univ; Natl Univ Singapore;
   Purdue Univ; Univ Salford},
Abstract = {Construction Planning and Scheduling is taught for the first time in
   Semester 2, 2004 in the School of Architecture and Building, Deakin
   University. During the unit development process and the implementation
   of teaching activities, several issues arose in relation to implementing
   computer-aided construction scheduling and unit delivery in a unitary
   environment. Although various types of construction planning and
   scheduling software have been developed and applied, none of them can be
   run inside an online teaching software package, which provides powerful
   functions in administration. This research aims to explore the
   strategies to connect a project planning and scheduling software package
   and an online teaching and learning software package by a Web-based
   support platform so that both the lecturer and students can draw up and
   communicate a construction plan or schedule with tables and figures. The
   key techniques of this supportive platform are identifies and they
   include a web-based graphically user-interfaced, dynamic and distributed
   multimedia data acquisition mechanism, which accepts users' drawings and
   retrieval information from canvas and stores the multimedia data on a
   server for further usage. This paper demonstrates the techniques and
   principals needed to construct such a multimedia data acquisition tool.
   This research will fill the gap in the literature in respect to an
   online pedagogical solution to an existing problem.},
ISBN = {7-112-06998-X},
ResearcherID-Numbers = {Liu, Chunlu/AAW-4172-2020},
ORCID-Numbers = {Liu, Chunlu/0000-0003-1144-4355},
Unique-ID = {WOS:000227465800031},
}

@inproceedings{ WOS:000224173300041,
Author = {Theus, M},
Editor = {Antoch, J},
Title = {1001 graphics},
Booktitle = {COMPSTAT 2004: PROCEEDINGS IN COMPUTATIONAL STATISTICS},
Year = {2004},
Pages = {501-512},
Note = {16th Symposium on Computational Statistics (COMPSTAT 2004), Prague,
   CZECH REPUBLIC, 2004},
Abstract = {Statistical graphics, or in more modern terms, data visualization, is
   not a new discipline. Whereas in the early days the construction of a
   graph was technically not easy and usually even required some artistic
   capabilities, generating statistical graphs is very easy in today's
   statistical software packages. This obviously leads to a less careful
   construction of these plots. In an object oriented software package like
   R we can call the generic function plot with almost any arbitrary object
   as argument, and some plot method will render this object, whether it
   makes sense or not.
   This paper investigates how well chosen plot defaults and rendering
   techniques can guarantee much better results in a graphical data
   analysis. Furthermore, standard plots and examples of plot ensembles are
   presented which are suitable for analyzing variables of a specific
   structure.},
ISBN = {3-7908-1554-3},
ResearcherID-Numbers = {Antoch, Jaromir/P-8726-2017},
ORCID-Numbers = {Antoch, Jaromir/0000-0002-5970-0306},
Unique-ID = {WOS:000224173300041},
}

@inproceedings{ WOS:000222347400051,
Author = {Todd, CA and Naghdy, F and O'Leary, SJ},
Editor = {Galloway, RL},
Title = {Geometric modeffing of the temporal bone for cochlea implant simulation},
Booktitle = {MEDICAL IMAGING 2004: VISUALIZATION, IMAGE-GUIDED PROCEDURES, AND
   DISPLAY},
Series = {Proceedings of SPIE},
Year = {2004},
Volume = {5367},
Pages = {482-490},
Note = {Medical Imaging 2004 Conference, San Diego, CA, FEB 17-19, 2004},
Organization = {SPIE; Amer Assoc Phys Med; Amer Physiol Soc; Ctr Devices \& Radiol Hlth;
   Soc Imaging Sci \& Technol; Natl Elect Mfg Assoc, Diagnost Imaging \&
   Therapy Syst Div; Radiol Soc N Amer; Soc Comp Applicat Radiol},
Abstract = {The first stage in the development of a clinically valid surgical
   simulator for training otologic surgeons in performing cochlea
   implantation is presented. For this purpose, a geometric model of the
   temporal bone has been derived from a cadaver specimen using the
   biomedical image processing software package Analyze (AnalyzeDirect,
   Inc) and its three-dimensional reconstruction is examined. Simulator
   construction begins with registration and processing of a Computer
   Tomography (CT) medical image sequence. Important anatomical structures
   of the middle and inner ear are identified and segmented from each scan
   in a semi-automated threshold-based approach. Linear interpolation
   between image slices produces a three-dimensional volume dataset: the
   geometrical model. Artefacts are effectively eliminated using a
   semi-automatic seeded region-growing algorithm and unnecessary bony
   structures are removed. Once validated by an Ear, Nose and Throat (ENT)
   specialist, the model may be imported into the Reachin Application
   Programming Interface (API) (Reachin Technologies AB) for visual and
   haptic rendering associated with a virtual mastoidectomy. Interaction
   with the model is realized with haptics interfacing, providing the user
   with accurate torque and force feedback. Electrode array insertion into
   the cochlea will be introduced in the final stage of design.},
DOI = {10.1117/12.533900},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-5280-7},
ORCID-Numbers = {O'Leary, Stephen/0000-0001-6926-2103},
Unique-ID = {WOS:000222347400051},
}

@inproceedings{ WOS:000222347400003,
Author = {Wolf, I and Vetter, M and Wegner, I and Nolden, M and Bottger, T and
   Hastenteufel, M and Schobinger, M and Kunert, T and Meinzer, HP},
Editor = {Galloway, RL},
Title = {The Medical Imaging Interaction Toolkit (MITK) - a toolkit facilitating
   the creation of interactive software by extending VTK and ITK},
Booktitle = {MEDICAL IMAGING 2004: VISUALIZATION, IMAGE-GUIDED PROCEDURES, AND
   DISPLAY},
Series = {Proceedings of SPIE},
Year = {2004},
Volume = {5367},
Pages = {16-27},
Note = {Medical Imaging 2004 Conference, San Diego, CA, FEB 17-19, 2004},
Organization = {SPIE; Amer Assoc Phys Med; Amer Physiol Soc; Ctr Devices \& Radiol Hlth;
   Soc Imaging Sci \& Technol; Natl Elect Mfg Assoc, Diagnost Imaging \&
   Therapy Syst Div; Radiol Soc N Amer; Soc Comp Applicat Radiol},
Abstract = {The aim of the Medical Imaging Interaction Toolkit (MITK) is to
   facilitate the creation of clinically usable image-based software.
   Clinically usable software for image-guided procedures and image
   analysis require a high degree of interaction to verify and. if
   necessary correct results from semi-automatic algorithms. MITK is a
   class library basing on and extending the Insight Toolkit (ITK) and the
   Visualization Toolkit (VTK). ITK provides, leading-edge registration and
   segmentation algorithms and forms the algorithmic basis. VTK has
   powerful visualization capabilities, but only low-level. support for
   interaction (like picking methods, rotation, movement and scaling of
   objects). MITK adds support for high level interactions with data like,
   for example, the interactive construction and modification of data
   objects. This includes concepts for interactions with multiple states as
   well as undo-capabilities. Furthermore, VTK is designed to create one
   kind of view on the data (either one 2D visualization or a 3D
   visualization). MITK facilitates the realization of multiple, different
   views oil the same data (like multiple, multiplanar reconstructions and
   a 3D rendering). Hierarchically structured combinations of any number
   and type of data objects (image. surface. vessels, etc.) are possible.
   MITK can handle 3D+t data, which are required for several important
   medical applications. whereas VTK alone supports only 2D and 3D data.
   The benefit of MITK is that it supplements those features to ITK and VTK
   that are required for convenient to use, interactive and by that
   clinically usable image-based software, and that are outside the scope
   of both. MITK will be made open-source (http:// www.mitk.org).},
DOI = {10.1117/12.535112},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-5280-7},
Unique-ID = {WOS:000222347400003},
}

@article{ WOS:000185383900002,
Author = {Vasquez, J},
Title = {Nonlinear interaction design provisions using spectral superposition},
Journal = {ENGINEERING STRUCTURES},
Year = {2003},
Volume = {25},
Number = {13},
Pages = {1585-1595},
Month = {NOV},
Abstract = {The problem of enforcing two-variable nonlinear interaction code
   provisions within a spectral superposition approach to design, is
   addressed. An analytical procedure based on the linearization of the
   interaction curve through a set of tangents or secants is developed.
   Safety with respect to the interaction curve is approximated by
   requiring safety with respect to that set of straight lines. For
   calculating the required estimators, the cross-estimator based formula
   for estimators of the linear combination of variables, derived in a
   companion paper, is used.
   The analytical method developed was shown to be equivalent to a
   graphical method proposed by Gupta, based on inscribing an estimator
   ellipse within the interaction curve. The analytical method is more
   straightforward and handles the directional maximum in single-component
   excitation, which the graphical method does not. The analytical method
   is much easier to apply. However, if a study calls for actually drawing
   the estimator ellipse, straightforward construction methods are
   presented, making unnecessary a cumbersome equivalent modal response
   approximation that had been suggested. The inherent antisymmetry of the
   spectral superposition formula, and its implications, are discussed. The
   effect of static loading and of symmetry of the interaction curve is
   also analyzed.
   Within the analysis of an example, the work required for the application
   of the procedure for nonlinear interaction for design purposes is
   discussed. It is found that its implementation can be achieved through a
   very simple function written for any standard numerical computation
   software package. The example also makes quite apparent the advantages
   of the analytical over the graphical method.
   The application example, which considers the design of a concrete column
   in a simple 10-storey building structure, shows the overconservativeness
   of a design based only in the standard estimators. The design criterion
   used in the example is that of the most unfavorable direction of a
   single-component earthquake. (C) 2003 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/S0141-0296(03)00124-X},
ISSN = {0141-0296},
Unique-ID = {WOS:000185383900002},
}

@article{ WOS:000186409500013,
Author = {Fothi, A and Nyeky-Gaizler, J and Porkolab, Z},
Title = {The structured complexity of object-oriented programs},
Journal = {MATHEMATICAL AND COMPUTER MODELLING},
Year = {2003},
Volume = {38},
Number = {7-9},
Pages = {815-827},
Month = {OCT},
Note = {5th International Conference on Applied Informatic/21st International
   Seminar on Stability Problems fof Stochastic Models, EGER, HUNGARY, JAN,
   2001},
Abstract = {There are several methods measuring the complexity of object-oriented
   programs. Most of them are based on some special object-oriented
   feature: number of methods/classes, cohesion of classes, inheritance,
   etc. In practice, however, object-oriented programs are constructed with
   the help of the same control structures as traditional ones. Moreover,
   recent ideas of multiparadigm programming (i.e., emerging use of generic
   programming and aspect-oriented programming) has the effect that in
   modern programs-and even in class libraries-object-orient at ion is only
   one (however major) construction tool among others. An adequate measure
   therefore should not be based on special features of one paradigm, but
   on basic language elements and construction rules which could be applied
   to many different paradigms.
   In our model discussed here, the complexity of a program is the sum of
   three components: the complexity of its control structure, the
   complexity of data types used, and the complexity of the data handling
   (i.e., the complexity of the connection between the control structure
   and the data types). We suggest a new complexity measure. First, we show
   that this measure works well on procedural programs, and then we extend
   it to object-oriented programs.
   There is a software tool under development based on gnu g++ compiler
   which computes our new measure. We can apply this tool to C and C++
   sources to gain a number of quantitative results with our measure. (C)
   2003 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/S0895-7177(03)90066-5},
ISSN = {0895-7177},
EISSN = {1872-9479},
ResearcherID-Numbers = {Porkolab, Zoltan/H-8233-2017},
ORCID-Numbers = {Porkolab, Zoltan/0000-0001-6819-0224},
Unique-ID = {WOS:000186409500013},
}

@article{ WOS:000184530100006,
Author = {Hudelot, C and Gowri-Shankar, V and Jow, H and Rattray, M and Higgs, PG},
Title = {RNA-based phylogenetic methods: application to mammalian mitochondrial
   RNA sequences},
Journal = {MOLECULAR PHYLOGENETICS AND EVOLUTION},
Year = {2003},
Volume = {28},
Number = {2},
Pages = {241-252},
Month = {AUG},
Note = {Symposium on Mammalian Phylogeny, SORRENTO, ITALY, JUN 16, 2002},
Abstract = {The PHASE software package allows phylogenetic tree construction with a
   number of evolutionary models designed specifically for use with RNA
   sequences that have conserved secondary structure. Evolution in the
   paired regions of RNAs occurs via compensatory substitutions, hence
   changes on either side of a pair are correlated. Accounting for this
   correlation is important for phylogenetic inference because it affects
   the likelihood calculation. In the present study we use the complete set
   of tRNA and rRNA sequences from 69 complete mammalian mitochondrial
   genomes. The likelihood calculation uses two evolutionary models
   simultaneously for different parts of the sequence: a paired-site model
   for the paired sites and a single-site model for the unpaired sites. We
   use Bayesian phylogenetic methods and a Markov chain Monte Carlo
   algorithm is used to obtain the most probable trees and posterior
   probabilities of clades. The results are well resolved for almost all
   the important branches on the mammalian tree. They support the
   arrangement of mammalian orders within the four supra-ordinal clades
   that have been identified by studies of much larger data sets mainly
   comprising nuclear genes. Groups such as the hedgehogs and the murid
   rodents, which have been problematic in previous studies with
   mitochondrial proteins, appear in their expected position with the other
   members of their order. Our choice of genes and evolutionary model
   appears to be more reliable and less subject to biases caused by
   variation in base composition than previous studies with mitochondrial
   genomes. (C) 2003 Elsevier Science (USA). All rights reserved.},
DOI = {10.1016/S1055-7903(03)00061-7},
ISSN = {1055-7903},
EISSN = {1095-9513},
ResearcherID-Numbers = {Rattray, Magnus/AAE-3297-2021
   Rattray, Magnus/B-4393-2009
   },
ORCID-Numbers = {Rattray, Magnus/0000-0001-8196-5565
   Rattray, Magnus/0000-0001-8196-5565
   Higgs, Paul/0000-0001-5220-9760},
Unique-ID = {WOS:000184530100006},
}

@article{ WOS:000183832900030,
Author = {Andronescu, M and Aguirre-Hernandez, R and Condon, A and Hoos, HH},
Title = {RNAsoft: a suite of RNA secondary structure prediction and design
   software tools},
Journal = {NUCLEIC ACIDS RESEARCH},
Year = {2003},
Volume = {31},
Number = {13},
Pages = {3416-3422},
Month = {JUL 1},
Abstract = {DNA and RNA strands are employed in novel ways in the construction of
   nanostructures, as molecular tags in libraries of polymers and in
   therapeutics. New software tools for prediction and design of molecular
   structure will be needed in these applications. The RNAsoft suite of
   programs provides tools for predicting the secondary structure of a pair
   of DNA or RNA molecules, testing that combinatorial tag sets of DNA and
   RNA molecules have no unwanted secondary structure and designing RNA
   strands that fold to a given input secondary structure. The tools are
   based on standard thermodynamic models of RNA secondary structure
   formation. RNAsoft can be found online at http://www.RNAsoft.ca.},
DOI = {10.1093/nar/gkg612},
ISSN = {0305-1048},
ResearcherID-Numbers = {Hoos, Holger H/B-1461-2008},
ORCID-Numbers = {Hoos, Holger H/0000-0003-0629-0099},
Unique-ID = {WOS:000183832900030},
}

@article{ WOS:000182643000004,
Author = {Glezer, C},
Title = {A conceptual model of an interorganizational intelligent
   meeting-scheduler (IIMS)},
Journal = {JOURNAL OF STRATEGIC INFORMATION SYSTEMS},
Year = {2003},
Volume = {12},
Number = {1},
Pages = {47-70},
Month = {MAR},
Abstract = {This article proposes and evaluates a comprehensive agent-based
   architecture for an Interorganizational Intelligent Meeting-Scheduler.
   The article extends and generalizes the Intelligent Meeting-Scheduler
   conceptual model {[}EXPERSYS 95-Proc. Seventh Intl Conf. Artificial
   Intelligence Expert Syst. Aopl. (1995) 279; J. Organizational Comput.
   Electron. Commerce, 9 (1999) 233] which focused on intraorganizational
   meeting scenarios.
   First, the article reviews several academic meeting-scheduling
   prototypes and commercial software packages. Based on this review, it is
   demonstrated that only an integrated approach that supports
   interoperability in all three dimensions of the meeting-scheduling
   problem (calendar, scheduling, and communication-management) can succeed
   in achieving interoperability among heterogeneous calendar and
   scheduling systems.
   The next part of the article provides a specification of an agent-based
   system that attempts to address the interoperability challenge. The
   specification comprises of the following elements: environment,
   behaviors, symbol-level, and knowledge-level architectures {[}IEEE
   Trans. Syst., Man, Cybernet. 25 (1995) 852]. The inter-organizational
   meeting-scheduling process is articulated as an iterative negotiation
   process where knowledge and symbol level units ('the IIMS system')
   interact with the system's end-users ('the environment') by exhibiting
   behaviors that address end-user requirements.
   The IIMS conceptual model is evaluated empirically and related to
   relevant literature on adoption difficulties of inter-organizational
   systems. It is evident that the IIMS faces a plethora of technological,
   organizational, sociological, behavioral, and psychological challenges
   that hinder its successful adoption. The article proposes several
   implementation tactics and guidelines in order to overcome these
   obstacles. (C) 2003 Elsevier Science B.V. All rights reserved.},
DOI = {10.1016/S0963-8687(02)00034-3},
Article-Number = {PII S0963-8687(02)00034-3},
ISSN = {0963-8687},
EISSN = {1873-1198},
ORCID-Numbers = {Glezer, Chanan/0000-0002-4356-1858},
Unique-ID = {WOS:000182643000004},
}

@article{ WOS:000181361400013,
Author = {VanderHeyden, WB and Dendy, ED and Padial-Collins, NT},
Title = {CartaBlanca - a pure-Java, component-based systems simulation tool for
   coupled nonlinear physics on unstructured grids - an update},
Journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
Year = {2003},
Volume = {15},
Number = {3-5},
Pages = {431-458},
Month = {MAR-APR},
Note = {ACM 2001 Java Grande/International Symposium on Computing in
   Object-Oriented Parallel Environments, STANFORD UNIV, STANFORD,
   CALIFORNIA, JUN 02-04, 2001},
Abstract = {This paper describes a component-based nonlinear physical system
   simulation prototyping package written entirely in Java using
   object-oriented design. The package provides scientists and engineers
   with a `developer-friendly' software environment for large-scale
   computational algorithm and physical model development. The software
   design centers on the Jacobian-free Newton-Krylov solution method
   surrounding a finite-volume treatment of conservation equations. This
   enables a clean component-like implementation. We first provide
   motivation for the development of the software and then discuss software
   structure. The discussion includes a description of the use of Java's
   built-in thread facility that enables parallel, shared-memory
   computations on a wide variety of unstructured grids with triangular,
   quadrilateral, tetrahedral and hexahedral elements. We also discuss the
   use of Java's inheritance mechanism in the construction of a hierarchy
   of physics systems objects and linear and nonlinear solver objects that
   simplify development and foster software re-use. We provide a brief
   review of the Jacobian-free Newton-Krylov nonlinear system solution
   method and discuss how it fits into our design. Following this, we show
   results from example calculations and then discuss plans including the
   extension of the software to distributed-memory computer systems.
   Copyright (C) 2003 John Wiley Sons, Ltd.},
DOI = {10.1002/cpe.662},
ISSN = {1532-0626},
Unique-ID = {WOS:000181361400013},
}

@article{ WOS:000186600600001,
Author = {Liagre, PF and Niedzwecki, JM},
Title = {Estimating nonlinear coupled frequency-dependent parameters in offshore
   engineering},
Journal = {APPLIED OCEAN RESEARCH},
Year = {2003},
Volume = {25},
Number = {1},
Pages = {1-19},
Month = {FEB},
Abstract = {The design of deepwater compliant offshore structures requires engineers
   to address many difficult challenges including defining and modeling the
   local offshore environment, specifying the associated combined global
   loading on innovative platform designs, and numerical simulation and
   model test verification of the platform response characteristics. The
   focus of this research investigation is the recovery of key parameters
   from time series measured during an industry type model basin test
   program using the reverse multiple input/single output technique. In
   particular. this study confronts critical problems of practical interest
   and extends the methodology to address the inclusion of nonlinear
   coupled systems in which the parameters of interest can be
   frequency-dependent. The analysis is developed around the nonlinear
   coupled equations of motions for a deepwater mini-TLP design and
   includes the consideration of nonlinear stiffness, quadratic damping,
   surge/pitch and sway/roll coupling and the frequency dependency of both
   the hydrodynamic added-mass and damping coefficients. A series of
   complementary model test measurements for the complete compliant model
   and the rigidly restrained hull by itself were used as the basis for the
   data in the system identification procedures. In addition, behavior of
   the hydrodynamic added-mass and damping coefficients as a function of
   frequency was simulated for the mini-TLP using an industry standard
   radiation-diffraction software package. These results were used in
   evaluating the accuracy of some of the key problem parameters. The
   results presented demonstrate the methodology as modified in this study
   is quite robust and yields predications that are more accurate for the
   parameters associated with the largest motions of the platform.
   Practical issues regarding the application of this approach, utilization
   of both force and moment measurements, and observed strengths and
   weakness in dealing with data regardless of its source are discussed.
   (C) 2003 Elsevier Ltd. All rights reserved.},
DOI = {10.1016/S0141-1187(03)00029-4},
ISSN = {0141-1187},
ORCID-Numbers = {Niedzwecki, John/0000-0003-4743-700X},
Unique-ID = {WOS:000186600600001},
}

@inproceedings{ WOS:000185394400017,
Author = {Allen, DW and Clough, JA and Sohn, H and Farrar, CR},
Editor = {Liu, SC},
Title = {A software tool for graphically assembling damage identification
   algorithms},
Booktitle = {SMART STRUCTURES AND MATERIALS 2003: SMART SYSTEMS AND NONDESTRUCTIVE
   EVALUATION FOR CIVIL INFRASTRUCTURES},
Series = {PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)},
Year = {2003},
Volume = {5057},
Pages = {138-144},
Note = {Smart Structures and Materials 2003 Conference, SAN DIEGO, CA, MAR
   02-06, 2003},
Organization = {SPIE; ASME; SEM; Boeing Co; Rhombus Consultant Grp; CSA Engn Inc; ISIS
   Canada; USAF, Off Sci Res; DARPA; Ceram Soc Japan; Intelligent Mat
   Forum; USA Res Off; Jet Propuls Lab; USN, Off Res; Natl Sci Fdn; USN,
   Res Lab},
Abstract = {At Los Alamos National Laboratory (LANL), various algorithms for
   structural health monitoring problems have been explored in the last 5
   to 6 years. The original DIAMOND (Damage Identification And MOdal
   aNalysis of Data) software was developed as a package of modal analysis
   tools with some frequency domain damage identification algorithms
   included. Since the conception of DIAMOND, the Structural Health
   Monitoring (SHM) paradigm at LANL has been cast in the framework of
   statistical pattern recognition, promoting data driven damage detection
   approaches. To reflect this shift and to allow user-friendly analyses of
   data, a new piece of software, DIAMOND II is under development. The
   Graphical User Interface (GUI) of the DIAMOND 11 software is based on
   the idea of GLASS (Graphical Linking and Assembly of Syntax Structure)
   technology, which is currently being implemented at LANL. GLASS is a
   Java based GUI that allows drag and drop construction of algorithms from
   various categories of existing functions. In the platform of the
   underlying GLASS technology, DIAMOND 11 is simply a module specifically
   targeting damage identification applications. Users can assemble various
   routines, building their own algorithms or benchmark testing different
   damage identification approaches without writing a single line of code.},
DOI = {10.1117/12.482755},
ISSN = {0277-786X},
ISBN = {0-8194-4862-1},
ResearcherID-Numbers = {Farrar, Charles/C-6954-2012},
Unique-ID = {WOS:000185394400017},
}

@inproceedings{ WOS:000186825600003,
Author = {Brukman, O and Dolev, S and Kolodner, EK},
Editor = {Bilof, RS},
Title = {Self-stabilizing autonomic recoverer for eventual Byzantine software},
Booktitle = {IEEE INTERNATIONAL CONFERENCE ON SOFTWARE - SCIENCE, TECHNOLOGY \&
   ENGINEERING, PROCEEDINGS},
Year = {2003},
Pages = {20-29},
Note = {IEEE International Conference on Software, Science, Technology and
   Engineering, HERZLIYYA, ISRAEL, NOV 04-05, 2003},
Organization = {IEEE Comp Soc, Israeli Chapter; Israeli Users Assoc Adv Technologies
   Elect Ind},
Abstract = {We suggest to model software package flaws (bugs) by assuming eventual
   Byzantine behavior of the package. In particular the package has been
   tested by the manufacturer for limited length scenarios when started in
   a predefined initial state; the behavior beyond the tested scenario may
   be Byzantine. Restarts (reboots) are useful for recovering such systems.
   We suggest a general yet practical framework and paradigm, based on a
   theoretical foundation, for the monitoring and restarting of systems. An
   autonomic recoverer that monitors and restarts the system is proposed,
   where: The autonomic recoverer is designed to handle different tasks
   given specific task requirements in the form of predicates and actions.
   DAG subsystem hierarchy structure is used by a consistency monitoring
   procedure in order to achieve gracious recovery. The existence and
   correct functionality of the autonomic recovery is guaranteed by the use
   of a kernel resident (anchor) process, and the design of the process to
   be self-stabilizing. The autonomic recoverer uses new scheme for
   liveness assurance via on-line monitoring that complements known schemes
   for on-line ensuring safety.},
DOI = {10.1109/SWSTE.2003.1245312},
ISBN = {0-7695-2047-2},
Unique-ID = {WOS:000186825600003},
}

@inproceedings{ WOS:000184467300023,
Author = {Chakravorty, R and Clark, A and Pratt, I},
Book-Group-Author = {USENIX
   USENIX},
Title = {GPRSWeb: Optimizing the web for GPRS links},
Booktitle = {PROCEEDINGS OF MOBISYS 2003},
Year = {2003},
Pages = {317-330},
Note = {1st International Conference on Mobile Systems, Applications and
   Services, SAN FRANCISCO, CA, MAY 05-08, 2003},
Organization = {ACM SIGMOBILE; USENIX Assoc; ACM SIGOPS},
Abstract = {The General Packet Radio Service (GPRS) is being deployed by GSM network
   operators world-wide, and promises to offer users ``always-on{''} data
   access at bandwidths comparable to that of conventional fixed-line
   telephone modems. Unfortunately, many users have found the reality to be
   rather different, experiencing very disappointing performance when, for
   example, browsing the web over GPRS.
   In this paper we investigate what causes the HTTP protocol and its
   underlying transport TCP to underperform in a GPRS environment. We
   examine why certain GPRS network characteristics interact badly with TCP
   to yield problems such as: link under-utilization for short-lived flows,
   excess queueing for long-lived flows, ACK compression, poor loss
   recovery, and gross unfairness between competing flows. We also show
   that many web browsers tend to be overly aggressive, and by opening too
   many simultaneous TCP connections can aggravate matters.
   We present the design and implementation of GPRSWeb - a mobile HTTP
   proxy system that mitigates many of the performance problems with a
   simple software update to a GPRS mobile device. The update is a `client
   proxy' that sits in the mobile device, and communicates with a `server
   proxy' located at the other end of the GPRS link close to the
   wired-wireless border. The dual proxy architecture collectively
   implements a number of key enhancements - an aggressive caching scheme
   that employs content-based hash keying to improve hit rates for dynamic
   content, a preemptive push of web page support resources to mobile
   clients, resource adaptation to suit client capabilities, delta encoded
   data transfers, DNS lookup migration, and a UDP-based reliable transport
   protocol that is specifically optimized for use over GPRS. We show that
   these enhancements result in significant improvement in overall WWW
   performance over GPRS.},
DOI = {10.1145/1066116.1066128},
ISBN = {1-931971-09-9},
Unique-ID = {WOS:000184467300023},
}

@inproceedings{ WOS:000222634900018,
Author = {Chen, D and Turner, SJ and Gan, BP and Cai, W and Wei, J},
Editor = {Verbraeck, A and Hlupic, V},
Title = {A decoupled federate architecture for distributed simulation cloning},
Booktitle = {SIMULATION IN INDUSTRY},
Year = {2003},
Pages = {131-140},
Note = {15th European Simulation Symposium, Delft Univ Technol, Delft,
   NETHERLANDS, OCT 26-29, 2003},
Organization = {Soc Modeling \& Simulat Int; Federat European Simulat Soc; CASS;
   European Simulat Council; Japanese Soc Simulat Technol; Latvian Simulat
   Soc; Polish Soc Comp Simulat; Turkish Simulat Soc},
Abstract = {Distributed simulation cloning technology is designed to perform
   ``what-if{''} analysis of existing High Level Architecture (HLA) based
   distributed simulations. The technology aims to enable the examination
   of alternative scenarios concurrently within the same simulation
   execution session. State saving and recovery are necessary for cloning a
   federate at runtime. However it is very difficult to have a generic
   state manipulation mechanism for any existing federate, as these can be
   developed independently and freely. The correctness of replicating a
   running federate significantly depends on the Runtime Infrastructure
   (RTI) software. The distributed simulation also needs fault tolerance to
   provide robustness at runtime. This paper proposes a decoupled federate
   architecture to address the above issues. A normal federate is decoupled
   into two processes, which execute the simulation model (virtual
   federate) and the local RTI component (physical federate) respectively.
   The decoupled approach interlinks the two processes together via
   Inter-Process Communication. The virtual federate interacts with the RTI
   through the standard RTI service interface supported by a customized
   library. The decoupled architecture ensures the correct replication of
   federates and facilitates fault tolerance at the RTI level. At the same
   time, it provides user transparency and reusability to existing federate
   codes. Benchmark experiments have been performed to study the extra
   overhead incurred by the decoupled federate architecture against the
   normal federate. The encouraging experimental results indicate that the
   proposed approach has a performance close to the normal one in terms of
   latency and time synchronization.},
ISBN = {3-936150-28-1},
ResearcherID-Numbers = {Cai, Wentong/A-3720-2011
   Wei, Jun/K-3584-2015
   Turner, Stephen/GXM-4654-2022},
ORCID-Numbers = {Cai, Wentong/0000-0002-0183-3835
   Wei, Jun/0000-0002-9583-1346
   },
Unique-ID = {WOS:000222634900018},
}

@article{ WOS:000185171100003,
Author = {Collod-Beroud, G and Le Bourdelles, S and Ades, L and Ala-Kokko, L and
   Booms, P and Boxer, M and Child, A and Comeglio, P and De Paepe, A and
   Hyland, JC and Holman, K and Kaitila, I and Loeys, B and Matyas, G and
   Nuytinck, L and Peltonen, L and Rantamaki, T and Robinson, P and
   Steinmann, B and Junien, C and Beroud, C and Boileau, C},
Title = {Update of the UMD-FBN1 mutation database and creation of an FBN1
   polymorphlism database},
Journal = {HUMAN MUTATION},
Year = {2003},
Volume = {22},
Number = {3},
Pages = {199-208},
Abstract = {Fibrillin is the major component of extracellular microfibrils.
   Mutations in the fibrillin gene on chromosome 15 (FBN1) were first
   described in the heritable connective disorder, Marfan syndrome (MFS).
   FBN1 has also been shown to harbor mutations related to a spectrum of
   conditions phenotypically related to MFS, called ``type-1
   fibrillinopathies.{''} In 1995, in an effort to standardize the
   information regarding these mutations and to facilitate their mutational
   analysis and identification of structure/function and phenotype/genotype
   relationships, we created a human FBN1 mutation database, UMD-FBN1. This
   database gives access to a software package that provides specific
   routines and optimized multicriteria research and sorting tools. For
   each mutation, information is provided at the gene, protein, and
   clinical levels. This toot is now a worldwide reference and is
   frequently used by teams working in the field; more than 220,000
   interrogations have been made to it since January 1998. The database has
   recently been modified to follow the guidelines on mutation databases of
   the HUGO Mutation Database Initiative (MDI) and the Human Genome
   Variation Society (HGVS), including their approved mutation
   nomenclature. The current update shows 559 entries, of which 421 are
   novel. UMD-FBN1 is accessible at www.umd.be/. We have also recently
   developed a FBN1 polymorphism database in order to facilitate
   diagnostics. (C) 2003 Wiley-Liss, Inc.},
DOI = {10.1002/humu.10249},
ISSN = {1059-7794},
EISSN = {1098-1004},
ResearcherID-Numbers = {Matyas, PD Dr. Gabor/AAJ-9868-2020
   COLLOD-BEROUD, Gwenaëlle/A-8342-2008
   Loeys, Bart/B-3236-2017
   Booms, Patrick/AAN-8717-2020
   BEROUD, Christophe/A-8381-2008
   boileau, catherine/M-4482-2017
   },
ORCID-Numbers = {Matyas, PD Dr. Gabor/0000-0002-3212-9963
   COLLOD-BEROUD, Gwenaëlle/0000-0003-4098-6161
   Loeys, Bart/0000-0003-3703-9518
   BEROUD, Christophe/0000-0003-2986-8738
   boileau, catherine/0000-0002-0371-7539
   Nuytinck, Lieve/0000-0002-5165-6937
   Robinson, Peter/0000-0002-0736-9199},
Unique-ID = {WOS:000185171100003},
}

@inproceedings{ WOS:000184745100172,
Author = {Hsieh, R and Zhou, ZG and Seneviratne, A},
Book-Group-Author = {IEEE
   IEEE},
Title = {S-MIP: A seamless handoff architecture for mobile IP},
Booktitle = {IEEE INFOCOM 2003: THE CONFERENCE ON COMPUTER COMMUNICATIONS, VOLS 1-3,
   PROCEEDINGS},
Series = {IEEE INFOCOM SERIES},
Year = {2003},
Pages = {1774-1784},
Note = {22nd Annual Joint Conference of the IEEE Computer and Communications
   Societies, SAN FRANCISCO, CA, MAR 30-APR 03, 2003},
Organization = {IEEE Comp Soc; IEEE Commun Soc; Natl Sci Fdn},
Abstract = {As the number of Mobile IP (MIP) {[}2] users grow, so will the demand
   for delay sensitive real-time applications, such as audio streaming,
   that require seamless handoff, namely, a packet lossless
   Quality-of-Service guarantee during a handoff. Two well-known approaches
   in reducing the MIP handoff latency have been proposed in the
   literature. One aims to reduce the (home) network registration time
   through a hierarchical management structure, while the other tries to
   minimize the lengthy address resolution delay by address
   pre-configuration through what is known as the fast-handoff mechanism.
   We present a novel seamless handoff architecture, S-MIP, that builds on
   top of the hierarchical approach 141 and the fast-handoff mechanism 131,
   in conjunction with a newly developed handoff algorithm based on pure
   software-based movement tracking techniques {[}16]. Using a combination
   of simulation and mathematical analysis, we argue that our architecture
   is capable of providing packet lossless handoff with latency similar to
   that of L2 handoff delay when using the 802.11 access technology. More
   importantly, S-MIP has a signaling overhead equal to that of the
   well-known `integrated' hierarchical MIP with fasthandoff scheme 141,
   within the portion of the network that uses wireless links. In relation
   to our S-MIP architecture, we discuss issues regarding the construction
   of network architecture, movement tracking, registration, address
   resolution, handoff algorithm and data handling.},
ISSN = {0743-166X},
ISBN = {0-7803-7752-4},
ResearcherID-Numbers = {Seneviratne, Aruna/H-7753-2014},
ORCID-Numbers = {Seneviratne, Aruna/0000-0001-6894-7987},
Unique-ID = {WOS:000184745100172},
}

@inproceedings{ WOS:000188415100075,
Author = {Huang, SP and Hu, P and Yang, P},
Editor = {Berga, L and Buil, JM and Jofre, C and Chonggang, S},
Title = {The software package for the thermal control of concrete dams and its
   engineering applications},
Booktitle = {ROLLER COMPACTED CONCRETE DAMS},
Year = {2003},
Pages = {615-618},
Note = {4th International Symposium on Roller Compacted Concrete Dams, MADRID,
   SPAIN, NOV 17-19, 2003},
Organization = {Spanish Natl Comm Large Dams; Spanish Inst Cement \& Applicat; Chinese
   Natl Comm Large Dams},
Abstract = {The Department of Structures and Materials, China Institute of Water
   Resources and Hydropower Research (IWHR), has been engaged in the field
   of thermal control of mass concrete for many years and has developed the
   systematic methodology for thermal control in the course of undertaking
   systematic researches on the thermal control of many Chinese dams of
   fame in the world. One software package for simulating analysis of
   concrete hydraulic structures has also been successfully developed. The
   software package is provided with full functions for simulation and can
   be used for gravity dams and arch dams built of conventional concrete
   and/or RCC as well as concrete faced rock fill dams (CFRD). The special
   thermal problems related to some particular locations such as steep
   slopes, concrete backfill and dam heightening are also given good
   treatment. The boundary conditions, including climate, hydrology,
   surface protection, overflowing during construction and reservoir
   impoundment, of the whole life cycle of the dam from the construction
   till the end of operation, and the internal conditions, including the
   time dependent change in the properties of the concrete and water pipe
   cooling, can be simulated very well in the computation of the time
   dependent stress and deformation at all locations of the structures.
   After careful comparison of many computational cases in combination with
   feedbacks from the project, proper measures for cracking control can be
   proposed in the end.
   In this paper the methodology for thermal control and the software
   package developed by the authors are introduced. A typical RCC gravity
   dam is cited to illustrate the use of the software in details.},
ISBN = {90-5809-564-9},
Unique-ID = {WOS:000188415100075},
}

@inproceedings{ WOS:000189347300033,
Author = {Kovacshazy, T and Samu, G and Peceli, G},
Book-Group-Author = {IEEE},
Title = {Simulink block library for fast prototyping of reconfigurable DSP
   systems},
Booktitle = {2003 IEEE INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING,
   PROCEEDINGS: FROM CLASSICAL MEASUREMENT TO COMPUTING WITH PERCEPTIONS},
Series = {International Symposium on Intelligent Signal Processing-WISP},
Year = {2003},
Pages = {179-184},
Note = {3rd IEEE International Symposium on Intelligent Signal Processing,
   BUDAPEST, HUNGARY, SEP 04-06, 2003},
Organization = {IEEE Instrumentat \& Measurement Soc; IEEE Hungary Sect; IEEE Hungary
   I\&M Soc Joint Chapter; IEEE Hungary Chapter Neural Networks Soc; IEEE
   I\&M Soc TC7; Hungarian Fuzzy Assoc; Budapest Univ Technol \& Econ},
Abstract = {This paper presents a block library for Madab/Simulink that allows fast
   prototyping of reconfigurable DSP systems. Up till now no similar
   software package was available. The block library supports the
   construction of reeonfigurable discrete time linear and non-linear
   systems from reconfigurable digital filters using various filter
   structures, state-space form implementations, polynomial filters, and
   PID controllers. The paper lists the requirements for the block library
   and introduces the main implementation related decisions that allows the
   block library to meet these requirements. An example illustrates the
   usage of the block library.},
ISBN = {0-7803-7864-4},
ResearcherID-Numbers = {Peceli, Gabor/G-8943-2012
   Kovacshazy, Tamas/G-8926-2012
   Peceli, Gabor/G-8943-2012},
ORCID-Numbers = {Peceli, Gabor/0000-0002-4370-2198
   Peceli, Gabor/0000-0003-0933-1175},
Unique-ID = {WOS:000189347300033},
}

@article{ WOS:000184917200002,
Author = {Kuhn, F and Oehme, M and Romero, F and Abou-Mansour, E and Tabacchi, R},
Title = {Differentiation of isomeric flavone/isoflavone aglycones by MS2 ion trap
   mass spectrometry and a double neutral loss of CO},
Journal = {RAPID COMMUNICATIONS IN MASS SPECTROMETRY},
Year = {2003},
Volume = {17},
Number = {17},
Pages = {1941-1949},
Abstract = {The fragmentation behaviour of seven pairs of isomeric
   flavone/isoflavone aglycones (solely hydroxylated and/or methoxylated)
   was studied using ion trap mass spectrometry with atmospheric pressure
   ionisation (API, both electrospray and APCI) in the positive and
   negative ion modes. A major difference was found in the neutral loss of
   56 u, which was a common feature of all isoflavones in API(+). It was
   identified as a double loss of CO by accurate mass tandem mass
   spectrometric (MS/MS) measurements using a hybrid quadrupole
   time-of-flight (Q-TOF) instrument. Fragmentation of daidzein with
   C-13-isotope labelling of the carbon C2 showed that this double loss
   occurred from the central ring of the molecule. A mechanism for this
   selective fragmentation is given. Further isoflavone-specific
   fragmentations were used to develop a guideline for the identification
   of isoflavone structures. A software-based neutral loss scan of 56 u in
   the API(+)-MS2 mode was applied to extracts of leaves of Lupinus albus
   and to soy flour. The structure elucidation guideline allowed
   identification of hydroxy and/or methoxy isoflavones. Structures could
   be confirmed for those available as reference compounds. Copyright (C)
   2003 John Wiley Sons, Ltd.},
DOI = {10.1002/rcm.1138},
ISSN = {0951-4198},
ResearcherID-Numbers = {Abou-Mansour, Eliane/E-7458-2015},
ORCID-Numbers = {Abou-Mansour, Eliane/0000-0001-7792-9246},
Unique-ID = {WOS:000184917200002},
}

@inproceedings{ WOS:000225384700006,
Author = {Lagin, L and Bryant, R and Carey, R and Casavant, D and Demaret, R and
   Edwards, O and Ferguson, W and Krammen, J and Larson, D and Lee, A and
   Ludwigsen, P and Miller, M and Moses, E and Nyholm, R and Reed, R and
   Shelton, R and Van Arsdall, PJ and Wuest, C},
Book-Group-Author = {ieee},
Title = {Status of the National Ignition Facility integrated computer control
   system},
Booktitle = {20TH IEEE/NPSS SYMPOSIUM ON FUSION ENGINEERING, PROCEEDINGS},
Year = {2003},
Pages = {27-30},
Note = {20th IEEE/NPSS Symposium on Fusion Engineering, San Diego, CA, OCT
   14-17, 2003},
Organization = {IEEE; Nucl \& Plasma Sci Soc},
Abstract = {The National Ignition Facility (NIF), currently under construction at
   the Lawrence Livermore National Laboratory, is a stadium-sized facility
   containing a 192-beam, 1.8-Megajoule, 500-Terawatt, ultraviolet laser
   system together with a 10-meter diameter target chamber with room for
   nearly 100 experimental diagnostics. When completed, NIF will be the
   world's largest and most energetic laser experimental system, providing
   an international center to study inertial confinement fusion and the
   physics of matter at extreme energy densities and pressures. NIF's 192
   energetic laser beams will compress fusion targets to conditions
   required for thermonuclear burn, liberating more energy than required to
   initiate the fusion reactions.
   Laser hardware is modularized into line replaceable units such as
   deformable mirrors, amplifiers, and multi-function sensor packages that
   are operated by the Integrated Computer Control System (ICCS). ICCS is a
   layered architecture of 300 front-end processors attached to nearly
   60,000 control points and coordinated by supervisor subsystems in the
   main control room. The functional subsystems - beam control including
   automatic beam alignment and wavefront correction, laser pulse
   generation and pre-amplification, diagnostics, pulse power, and timing
   implement automated shot control, archive data, and support the actions
   of fourteen operators at graphic consoles. Object-oriented software
   development uses a mixed language environment of Ada (for functional
   controls) and Java (for user interface and database backend). The ICCS
   distributed software framework uses CORBA to communicate between
   languages and processors. ICCS software is approximately 3/4 complete
   with over 750 thousand source lines of code having undergone off-line
   verification tests and deployed to the facility.
   NIF has entered the first phases of its laser commissioning program. NIF
   has now demonstrated the highest energy 1omega, 2omega, and 3omega
   beamlines in the world. NIF's target experimental systems are also being
   installed in preparation for experiments to begin in late 2003. This
   talk will provide a detailed look at the status of the control system.},
DOI = {10.1109/FUSION.2003.1425870},
ISBN = {0-7803-7908-X},
Unique-ID = {WOS:000225384700006},
}

@inproceedings{ WOS:000183136400003,
Author = {Pfeifle, J and Rambau, J},
Editor = {Joswig, M and Takayama, N},
Title = {Computing triangulations using oriented matroids},
Booktitle = {ALGEBRA, GEOMETRY, AND SOFTWARE SYSTEMS},
Year = {2003},
Pages = {49-75},
Note = {Workshop on Integration of Algebra and Geometry Software Systems,
   DAGSTUHL, GERMANY, OCT, 2001},
Abstract = {Oriented matroids axe combinatorial structures that encode the
   combinatorics of point configurations. The set of all triangulations of
   a point configuration depends only on its oriented matroid. We survey
   the most important ingredients necessary to exploit oriented matroids as
   a data structure for computing all triangulations of a point
   configuration, and report on experience with an implementation of these
   concepts in the software package TOPCOM. Next, we briefly overview the
   construction and an application of the secondary polytope of a point
   configuration, and calculate some examples illustrating how our tools
   were integrated into the POLYMAKE framework.},
ISBN = {3-540-00256-1},
ResearcherID-Numbers = {Pfeifle, Julian/H-9040-2015},
ORCID-Numbers = {Pfeifle, Julian/0000-0001-9777-2602},
Unique-ID = {WOS:000183136400003},
}

@article{ WOS:000180446000005,
Author = {Redman, JE and Wilcoxen, KM and Ghadiri, MR},
Title = {Automated mass spectrometric sequence determination of cyclic peptide
   library members},
Journal = {JOURNAL OF COMBINATORIAL CHEMISTRY},
Year = {2003},
Volume = {5},
Number = {1},
Pages = {33-40},
Month = {JAN-FEB},
Abstract = {Cyclic peptides have come under scrutiny as potential antimicrobial
   therapeutic agents. Combinatorial split-and-pool synthesis of cyclic
   peptides can afford single compound per well libraries for antimicrobial
   screening, new lead identification, and construction of quantitative
   structure-activity relationships (QSAR). Here, we report a new
   sequencing protocol for rapid identification of the members of a cyclic
   peptide library based on automated computer analysis of mass spectra,
   obviating the need for library encoding/decoding strategies.
   Furthermore, the software readily integrates with common spreadsheet and
   database packages to facilitate data visualization and archiving. The
   utility of the new MS-sequencing approach is demonstrated using sonic
   spray ionization ion trap MS and MS/MS spectrometry on a single compound
   per bead cyclic peptide library and validated with individually
   synthesized pure Cyclic D,L-alpha-peptides.},
DOI = {10.1021/cc0200639},
ISSN = {1520-4766},
ResearcherID-Numbers = {Redman, James/B-7175-2008},
ORCID-Numbers = {Redman, James/0000-0001-5492-2869},
Unique-ID = {WOS:000180446000005},
}

@inproceedings{ WOS:000184326400074,
Author = {Santaolaya, R and Fragoso, OG and Perez, J and Zambrano, L},
Editor = {Kumar, V and Gavrilova, ML and Tan, CJK and LEcuyer, P},
Title = {Restructuring conditional code structures using object oriented design
   patterns},
Booktitle = {COMPUTATIONAL SCIENCE AND ITS APPLICATIONS - ICCSA 2003, PT 1,
   PROCEEDINGS},
Series = {LECTURE NOTES IN COMPUTER SCIENCE},
Year = {2003},
Volume = {2667},
Pages = {704-713},
Note = {International Conference on Computational Science and Its Applications
   (ICCSA 2003), MONTREAL, CANADA, MAY 18-21, 2003},
Organization = {CERCA; IBM Canada; IBM, United States; Heuchera Technol; Pallas; Queens
   Univ Belfast; SHARCNET; Soc Ind \& Appl Math; Springer Verlag},
Abstract = {Nowadays, software industry demands high quality reusable artifacts that
   are easy to configure for developing new applications or modifying the
   existing ones at minimum cost. In this context several approaches have
   been proposed, as a result of this, libraries with a number of reusable
   functions and/or classes have been obtained. Such approaches have also
   proposed guidelines aimed to reuse most of the software developed by
   programmers. However this goal has not been achieved yet, mainly due to
   the lack of quality attributes of the reusable software components
   currently available. This paper introduces an approach known as SR2,
   which means software reengineering for reuse, it is based on a
   reengineering process whose input is legacy code written in C language
   and the output is an object-oriented framework in C++. In this work we
   employ the Gamma design patterns strategy and state to structure the
   framework generated by the reengineering process.},
ISSN = {0302-9743},
ISBN = {3-540-40155-5},
Unique-ID = {WOS:000184326400074},
}

@inproceedings{ WOS:000185997700033,
Author = {Starbird, T and Lin, I and Ko, AY},
Book-Group-Author = {IEEE
   IEEE},
Title = {Mission interactive scenario studio (MISS) for autonomous spacecraft},
Booktitle = {2003 IEEE AEROSPACE CONFERENCE PROCEEDINGS, VOLS 1-8},
Series = {IEEE AEROSPACE CONFERENCE PROCEEDINGS},
Year = {2003},
Pages = {323-332},
Note = {IEEE Aerospace Conference, BIG SKY, MT, MAR 08-15, 2003},
Organization = {IEEE Aerosp \& Elect Syst Soc; Aerosp Corp; Analog Interfaces; AstroExpo
   com; Caltech, JPL; Circle Mt Co; Gandalf Associates, LLC; GEB Software
   Technologies; Missile Defense Agcy; NASA; Panasonic; SpectrumAstro;
   Superior Solut; TechLink},
Abstract = {We propose building a Studio enabling the use of diverse existing
   mission activity and scenario patterns, the creation of new ones, and
   the modeling of their effects using existing modeling tools. The core of
   the Studio is a component-based Type Library, which captures years of
   mission adaptation patterns in various forms. The Studio works as a
   content server to capture the developed adaptation knowledge for reuse
   and provides bridging into different mission uplink implementations,
   including the Mission Data System {[}1] (MDS) state/goal machinery.
   Various activities can be coordinated, controlled, and reused through
   the Studio's component interface to establish and model a mission
   scenario. A special component Factory mechanism will be in place to
   facilitate the adaptation of projects into the Studio. The architecture
   of the Studio reflects and enforces a division of knowledge and actions
   into three parts: Model, View, Controller, (MVC). The Model contains
   information about (a proposed version of) the spacecraft and mission.
   The Controller contains logic for constructing scenarios of mission
   activities. The information in the Model and Controller is principally
   in the form of reusable patterns. A Viewer can be a simple or complex
   software system. For example, Activity Plan Generator (Apgen) {[}2,3] is
   one possible viewer: MDS is another. A 3-tiered infrastructure is used
   for the Studio, reflecting the Model, Controller, Viewer architecture
   {[}4,5]. The Studio is useful in pre-phase A of a project by enabling
   spacecraft design options to be played against desired mission
   scenarios. In later design phases of a project, the construction and
   modeling of more detailed scenarios is supported by the Studio.},
ISSN = {1095-323X},
ISBN = {0-7803-7651-X},
Unique-ID = {WOS:000185997700033},
}

@article{ WOS:000182670300008,
Author = {Wakabayashi, M and Amano, H},
Title = {Environment for multiprocessor simulator development},
Journal = {ELECTRONICS AND COMMUNICATIONS IN JAPAN PART III-FUNDAMENTAL ELECTRONIC
   SCIENCE},
Year = {2003},
Volume = {86},
Number = {10},
Pages = {74-84},
Abstract = {To develop multiprocessor systems for continuing the precipitous
   increase in performance seen in recent years, it is important to include
   performance prediction at a preimplementation stage. Since software
   simulation, which is one such performance prediction technique, has a
   high degree of flexibility, it is used in diverse situations. To
   simulate a multiprocessor system, a simulator is implemented for a
   specific architecture. However, this method has accompanying
   implementation costs for building a simulator for each different target
   architecture. In this paper, the authors propose the multiprocessor
   simulator library ISIS as a multiprocessor simulator construction
   support system. ISIS is a collection of multiprocessor system internal
   function block simulators. Multiprocessor simulators having any kind of
   architecture can be constructed by combining function block simulators.
   Compared with conventional techniques, implementation costs can be
   reduced without directly impairing simulator runtime costs. Several
   multiprocessor simulators were implemented by using ISIS, and their
   execution speeds and implementation costs were evaluated. Sufficient
   runtime speeds were obtained for practical uses at extremely low
   implementation costs. ISIS has actually been used in research for
   on-chip multiprocessor cache systems, and various kinds of knowledge
   have been obtained from the evaluation results of that research. (C)
   2003 Wiley Periodicals, Inc.},
DOI = {10.1002/ecjc.10122},
ISSN = {1042-0967},
Unique-ID = {WOS:000182670300008},
}

@inproceedings{ WOS:000186426800015,
Author = {Yoo, S and Jerraya, AA},
Editor = {Jerraya, AA and Yoo, S and Verkest, D and Wehn, N},
Title = {Introduction to hardware abstraction layers for SoC},
Booktitle = {EMBEDDED SOFTWARE FOR SOC},
Year = {2003},
Pages = {179-186},
Note = {Embedded Software Forum of the Design, Automation and Test in Europe
   Conference, MUNICH, GERMANY, MAR, 2003},
Abstract = {In this paper, we explain hardware abstraction layer (HAL) and related
   issues in the context of SoC design. First, we give a HAL definition and
   examples of HAL function. HAL gives an abstraction of HW architecture to
   upper layer software (SW). It hides the implementation details of HW
   architecture, such as processor, memory management unit (MMU), cache,
   memory, DMA controller, timer, interrupt controller, bus/bus
   bridge/network interface, I/O devices, etc. HAL has been used in the
   conventional area of operating system to ease porting OSs on different
   boards. In the context of SoC design, HAL keeps still the original role
   of enabling the portability of upper layer SW. However, in SoC design,
   the portability impacts on the design productivity in two ways: SW reuse
   and concurrent HW and SW design. As in the case of HW interface
   standards, e.g. VCI, OCP-IP, etc., the HAL API needs also a standard.
   However, contrary to the case of HW interface, the standard of HAL API
   needs to be generic not only to support the common functionality of HAL,
   but also to support new HW architectures in application-specific SoC
   design with a guideline for HAL API extension. We present also three
   important issues of HAL for SoC design: HAL modelling,
   application-specific and automatic HAL design.1.},
ISBN = {1-4020-7528-6},
ResearcherID-Numbers = {Jerraya, Ahmed/R-2556-2019},
Unique-ID = {WOS:000186426800015},
}

@article{ WOS:000180932100020,
Author = {Klein, MT and Hou, G and Quann, RJ and Wei, W and Liao, KH and Yang, RSH
   and Campain, JA and Mazurek, MA and Broadbelt, LJ},
Title = {BioMOL: A computer-assisted biological modeling tool for complex
   chemical mixtures and biological processes at the molecular level},
Journal = {ENVIRONMENTAL HEALTH PERSPECTIVES},
Year = {2002},
Volume = {110},
Number = {6},
Pages = {1025-1029},
Month = {DEC},
Note = {Conference on Application of Technology to Chemical Mixture Research,
   COLORADO STATE UNIV, FT COLLINS, COLORADO, JAN, 2001},
Organization = {Natl Inst Environm Hlth Sci; Superfund Basic Res Program},
Abstract = {A chemical engineering approach for the rigorous construction, solution,
   and optimization of detailed kinetic models for biological processes is
   described. This modeling capability addresses die required technical
   components of detailed kinetic modeling, namely, the modeling of
   reactant structure and composition, the building of the reaction
   network, the organization of model parameters, the solution of the
   kinetic model, and the optimization of the model. Even though this
   modeling approach has enjoyed successful application in die petroleum
   industry, its application to biomedical research has just begun. We
   propose to expand the horizons on classic pharmacokinetics and
   physiologically based pharmacokinetics (PBPK, where human or animal
   bodies were often described by a few compartments, by integrating PBPK
   with reaction network modeling described in this article. If one draws a
   parallel between an oil refinery, where the application of this modeling
   approach has been very successful, and a human body, the individual
   processing units in the oil refinery may be considered equivalent to the
   vital organs of the human body. Even though the cell or organ may be
   much more complicated, the complex biochemical reaction networks in each
   organ may be similarly modeled and linked in much the same way as the
   modeling of the entire oil refinery through linkage of the individual
   processing units. The integrated chemical engineering software package
   described in this article, BiOMOL, denotes the biological application of
   molecular-oriented lumping. BioMOL can build a detailed model in 1-1,000
   CPU sec using standard desktop hardware. The models solve and optimize
   using standard and widely available hardware and software and can be
   presented in the context of a user-friendly interface. We believe this
   is an engineering tool with great promise in its application to complex
   biological reaction networks.},
DOI = {10.1289/ehp.02110s61025},
ISSN = {0091-6765},
ResearcherID-Numbers = {Broadbelt, Linda/B-7640-2009
   Liao, Kin/A-7915-2014},
ORCID-Numbers = {Liao, Kin/0000-0001-6352-2349},
Unique-ID = {WOS:000180932100020},
}

@article{ WOS:000179671500030,
Author = {Zhang, JH and Rowe, WL and Struewing, JP and Buetow, KH},
Title = {HapScope: a software system for automated and visual analysis of
   functionally annotated haplotypes},
Journal = {NUCLEIC ACIDS RESEARCH},
Year = {2002},
Volume = {30},
Number = {23},
Pages = {5213-5221},
Month = {DEC 1},
Abstract = {We have developed a software analysis package, HapScope, which includes
   a comprehensive analysis pipeline and a sophisticated visualization tool
   for analyzing functionally annotated haplotypes. The HapScope analysis
   pipeline supports: (i) computational haplotype construction with an
   expectation-maximization or Bayesian statistical algorithm; (ii) SNP
   classification by protein coding change, homology to model organisms or
   putative regulatory regions; and (iii) minimum SNP subset selection by
   either a Brute Force Algorithm or a Greedy Partition Algorithm. The
   HapScope viewer displays genomic structure with haplotype information in
   an integrated environment, providing eight alternative views for
   assessing genetic and functional correlation. It has a user-friendly
   interface for: (i) haplotype block visualization; (ii) SNP subset
   selection; (iii) haplotype consolidation with subset SNP markers; (iv)
   incorporation of both experimentally determined haplotypes and
   computational results; and (v) data export for additional analysis.
   Comparison of haplotypes constructed by the statistical algorithms with
   those determined experimentally shows variation in haplotype prediction
   accuracies in genomic regions with different levels of nucleotide
   diversity. We have applied HapScope in analyzing haplotypes for
   candidate genes and genomic regions with extensive SNP and genotype
   data. We envision that the systematic approach of integrating functional
   genomic analysis with population haplotypes, supported by HapScope, will
   greatly facilitate current genetic disease research.},
DOI = {10.1093/nar/gkf654},
ISSN = {0305-1048},
EISSN = {1362-4962},
ResearcherID-Numbers = {Struewing, Jeffery P/I-7502-2013
   Struewing, Jeffery P/C-3221-2008},
ORCID-Numbers = {Struewing, Jeffery P/0000-0002-4848-3334
   },
Unique-ID = {WOS:000179671500030},
}

@article{ WOS:000178698800007,
Author = {de Souza, MAF and Ferreira, MAGV},
Title = {Designing reusable rule-based architectures with design patterns},
Journal = {EXPERT SYSTEMS WITH APPLICATIONS},
Year = {2002},
Volume = {23},
Number = {4},
Pages = {395-403},
Month = {NOV},
Note = {13th International Conference on Software Engineering and Knowledge
   Engineering (SEKE 2001), BUENOS AIRES, ARGENTINA, JUN 12-15, 2001},
Abstract = {Rule-based systems or production systems still have great importance in
   the construction of knowledge systems. In these systems, the domain
   expertise to solve a problem is encoded in the form of `if-then' rules,
   enabling a modular description of the knowledge, thus facilitating its
   maintenance and updating. Although they have been extensively described
   in the Artificial Intelligence literature, their design process is at
   times repeated because of the lack of common software architecture and
   the restrictions offered by some off-the-shelf libraries and systems.
   This paper proposes a reusable architecture for rule-based systems
   described through design patterns. The aim of these patterns is to
   constitute a design catalog that can be used by designers to understand
   and create new rule-based systems, thus promoting reuse in these
   systems. Additionally, the use of the described patterns in the design
   of an intelligent tutoring system architecture is exemplified. (C) 2002
   Elsevier Science Ltd. All rights reserved.},
DOI = {10.1016/S0957-4174(02)00075-1},
Article-Number = {PII S0957-4174(02)00075-1},
ISSN = {0957-4174},
Unique-ID = {WOS:000178698800007},
}

@article{ WOS:000180179900023,
Author = {Moore, K and Dongarra, J},
Title = {NetBuild: transparent cross-platform access to computational software
   libraries},
Journal = {CONCURRENCY AND COMPUTATION-PRACTICE \& EXPERIENCE},
Year = {2002},
Volume = {14},
Number = {13-15},
Pages = {1445-1456},
Month = {NOV-DEC},
Abstract = {NetBuild is a suite of tools which automate the process of selecting,
   locating, downloading, configuring, and installing computational
   software libraries from over the Internet, and which aid in the
   construction and cataloging of such libraries. Unlike many other tools,
   NetBuild is designed to work across a wide variety of computing
   platforms, and perform fine-grained matching to find the most suitable
   version of a library for a given target platform. We describe the
   architecture of NetBuild and its initial implementation. Copyright (C)
   2002 John Wiley Sons, Ltd.},
DOI = {10.1002/cpe.670},
ISSN = {1532-0626},
EISSN = {1532-0634},
ResearcherID-Numbers = {Dongarra, Jack/G-4199-2019
   , paulemmelkamp/G-3274-2011},
ORCID-Numbers = {Dongarra, Jack/0000-0003-3247-1782
   , paulemmelkamp/0000-0001-6066-5512},
Unique-ID = {WOS:000180179900023},
}

@article{ WOS:000177668300005,
Author = {Seo, J and Kim, GJ},
Title = {Design for presence: A structured approach to virtual reality system
   design},
Journal = {PRESENCE-TELEOPERATORS AND VIRTUAL ENVIRONMENTS},
Year = {2002},
Volume = {11},
Number = {4},
Pages = {378-403},
Month = {AUG},
Abstract = {The development and maintenance of a virtual reality (VR) system
   requires in-depth knowledge and understanding in many different
   disciplines, Three major features that distinguish VR systems are
   real-time performance while maintaining acceptable realism and presence,
   objects with two clearly distinct yet inter-related aspects like
   geometry/structure and function/behavior, and the still experimental
   nature of multi-modal interact ion design, Until now, little attention
   has been paid to methods and tools for the structured development of VR
   software that addresses these features. Many VR application development
   projects proceed by modeling needed objects on conventional CAD systems,
   then programming the system using simulation packages. Usually, these
   activities are carried out without much planning which may be acceptable
   for only small-scale or noncritical demonstration systems. However, for
   VR to be taken seriously as a media technology, a structural approach to
   developing VR applications is required for the construction of
   large-scale VR worlds, and this will undoubtedly involve and require
   complex resource management, abstractions for basic system/object
   functionalities and interaction tasks, and integration and easy plug-ins
   of different input and output methods. In this paper, we assembled a
   comprehensive structured methodology for building VR systems, called
   CLEVR (Concurrent and LEvel by Level Development of VR System), which
   combines several conventional and new concepts. For instance, we employ
   concepts such as the simultaneous consideration of form, function, and
   behavior, hierarchical modeling and top-down creation of LODs (levels of
   detail), incremental execution and performance tuning, user task and
   interaction modeling, and compositional reuse of VR objects. The basic
   underlying modeling approach is to design VR objects (and the scenes
   they compose) hierarchically and incrementally, considering their
   realism, presence, behavioral correctness, performance, and even
   usability in a spiral manner. To support this modeling strategy, we
   developed a collection of computer-aided tools called P-VoT
   (POSTECH-Virtual reality system development Tool). We demonstrate our
   approach by illustrating a step-by-step design of a virtual ship
   simulator using the CLEVR/P-VoT, and demonstrate the effectiveness of
   our method in terms of the quality (performance and correctness) of the
   resulting software and reduced effort in its development and
   maintenance,},
DOI = {10.1162/105474602760204291},
ISSN = {1054-7460},
Unique-ID = {WOS:000177668300005},
}

@article{ WOS:000176533100006,
Author = {Jiang, XC and Bingham, J and Master, R and Gramopadhye, AK and Melloy,
   BJ},
Title = {Visual inspection simulator for hybrid environments},
Journal = {INTERNATIONAL JOURNAL OF INDUSTRIAL ENGINEERING-THEORY APPLICATIONS AND
   PRACTICE},
Year = {2002},
Volume = {9},
Number = {2},
Pages = {162-173},
Month = {JUN},
Abstract = {Recently, 100\% inspection using automated systems has seen more
   frequent application than traditional sampling inspection using human
   inspectors. Nevertheless, humans still outperform machines in some
   tasks. Thus, designers of inspection systems need guidelines on
   human/machine function allocation to ensure inspection is performed
   effectively. In response to this need, this paper describes a system
   that addresses this issue. The system can support controlled studies on
   a printed circuit board inspection task in either human, automated, or
   hybrid mode and is described in terms of its specifications,
   architecture, and operation. The system specifications include the
   hardware and software requirements, the development software, the board
   image design software, and the image library development. The system
   architecture introduces the structure of the database, supporting
   modules and their relationship. Finally, the system operation describes
   how the system works in detail.
   Significance: A simulator was developed in order to study function
   allocation issues in context of a visual inspection printed circuit
   board task. Through appropriate allocation of search and decision-making
   functions, designers can achieve better system performance.},
EISSN = {1943-670X},
Unique-ID = {WOS:000176533100006},
}

@article{ WOS:000174929500007,
Author = {Schafer, W},
Title = {Neutron diffraction applied to geological texture and stress analysis},
Journal = {EUROPEAN JOURNAL OF MINERALOGY},
Year = {2002},
Volume = {14},
Number = {2},
Pages = {263-289},
Month = {MAR-APR},
Abstract = {Texture is defined as preferred crystallographic orientation in a
   polycrystalline aggregate. The main mechanisms of geological texture
   formation are crystallization, sedimentation, plastic deformation,
   recrystallization and metamorphism. Textures of geomaterials are
   fingerprints of the earth's history. The complexity of geological
   texture analysis results mainly from the overprinting of different
   textures on several mineral components during different orogenic
   periods. Quantitative texture analysis, i.e., the calculation of a
   three-dimensional orientation distribution function of crystallites, is
   based on experimental pole figures which represent the orientation
   distributions of certain crystallographic directions and which can be
   obtained from X-ray or neutron-diffraction techniques.
   Due to the high penetration capability of neutrons, large specimens can
   be investigated resulting in global volume textures rather than local
   surface textures from X-rays. Neutron-diffraction pole figures are
   characterized by high grain statistics even on coarse-grained material.
   Individual pole figures can also be obtained from reflection-rich
   diffraction patterns of multiphase rocks and low-symmetry mineral
   constituents by using position-sensitive detectors and by pattern
   decomposition by means of profile-fitting methods. Neutron texture
   diffractometers are operated in the constant-wavelength mode at
   steady-state sources and using time-of-flight techniques at pulsed
   sources. Different experimental set-ups are discussed.
   Results of neutron-diffraction texture analyses on monomineralic and
   polymineralic rocks are presented, e.g, on calcite, quartzite,
   plagioclase, pyrrhotite ores, granite, and
   orthopyroxone-sillimanite-granulite. The wide application spectrum of
   neutron texture analysis includes objects of the outer solar system as
   in the low-temperature texture study on ice and the non-destructive
   investigations on rare pieces of meteorites.
   The application of neutron diffraction on strain measurements and
   residual stress analysis of geological material is discussed. Natural
   effects on rocks are orders of magnitude smaller than in technological
   material and drilling gives rise to stress relaxation. Experimental
   deformations can be observed during in situ measurements at various
   pressures and temperatures. Neutron strain diffractometers are
   described. Examples of recent results on the full strain/stress tensor
   in sandstone and on strain partitioning in polymineralic rocks are
   given.
   Future prospects of new high-intensity neutron sources are promising in
   performing combined structure, texture and stress analysis. New
   instruments are under construction and software packages on the basis of
   full-pattern Rietveld refinements are available.},
DOI = {10.1127/0935-1221/2002/0014-0263},
ISSN = {0935-1221},
Unique-ID = {WOS:000174929500007},
}

@article{ WOS:000173726400003,
Author = {Reed, JM and Mills, LS and Dunning, JB and Menges, ES and McKelvey, KS
   and Frye, R and Beissinger, SR and Anstett, MC and Miller, P},
Title = {Emerging issues in population viability analysis},
Journal = {CONSERVATION BIOLOGY},
Year = {2002},
Volume = {16},
Number = {1},
Pages = {7-19},
Month = {FEB},
Abstract = {Population viability analysis (PVA) has become a commonly used tool in
   endangered species management. There is no single process that
   constitutes PVA, but all approaches have in common all assessment of a
   population's risk of extinction (or quasi extinction) or its projected
   population growth either under current conditions or expected from
   proposed management. As model sophistication increases, and software
   programs that facilitate PVA without the need for modeling expertise
   become more available, there is greater potential for the misuse of
   models and increased confusion over interpreting their results.
   Consequently, we discuss the practical use and limitations of PVA in
   conservation planning, and we discuss some emerging issues of PVA. We
   review extant issues that have become prominent in PTA, including
   spatially explicit modeling, sensitivity analysis, incorporating
   genetics into PVA, PVA in plants, and PVA software packages, but our
   coverage of emerging issues is not comprehensive. We conclude that PVA
   is a powerful tool in conservation biology for comparing alternative
   research plans and relative extinction risks among species, but the
   suggest caution in its use: (1) because PVA is a model, its validity
   depends on the appropriateness of the model's structure and data
   quality; (2) results should be presented with appropriate assessment of
   confidence; (3) model construction and results should be subject to
   external review, and (4) model structure, input, and results should be
   treated as hypotheses to be tested. We also suggest (5) restricting the
   definition of PVA to development of a formal quantitative model, (6)
   focusing more research on determining how pervasive density-dependence
   feedback is across species, and (7) not using PVA to determine minimum
   population size or (8) the specific probability of reaching extinction.
   The most appropriate use of PVA may be for comparing the relative
   effects of potential management actions on population growth or
   persistence.},
DOI = {10.1046/j.1523-1739.2002.99419.x},
ISSN = {0888-8892},
EISSN = {1523-1739},
ResearcherID-Numbers = {Mills, L. Scott/K-2458-2012
   Anstett, Marie Charlotte/N-7977-2017
   Beissinger, Steven R/F-3809-2012
   },
ORCID-Numbers = {Mills, L. Scott/0000-0001-8771-509X
   Anstett, Marie Charlotte/0000-0001-7786-0357},
Unique-ID = {WOS:000173726400003},
}

@inproceedings{ WOS:000177219600011,
Author = {Gomez-Rivas, A and Pincus, G},
Editor = {Pudlowski, ZJ},
Title = {A focused engineering technology programme: Structural analysis and
   design},
Booktitle = {3RD GLOBAL CONGRESS ON ENGINEERING EDUCATION, CONGRESS PROCEEDINGS},
Year = {2002},
Pages = {67-70},
Note = {3rd Global Congress on Engineering Education, GLASGOW CALEDONIAN UNIV,
   FAC SCI \& TECHNOL, GLASGOW, SCOTLAND, JUN 30-JUL 05, 2002},
Organization = {UNESCO Int Ctr Engn Educ; Int Liaison Grp Engn Educ},
Abstract = {The Structural Analysis and Design (SAD) Engineering Technology
   programme of the University of Houston-Downtown, Houston, USA, is
   focused on the design of bridges, buildings, towers, offshore platforms
   and other structures. It is not traditional civil engineering but
   includes all aspects of structural design, including soil mechanics,
   foundation design construction surveying by GIS-GPS and the active
   control of structures. Since the programme focuses on structural
   analysis and design, students are exposed to several techniques and
   practices that are taught in schools of civil engineering at the
   graduate level. Examples include instruction on finite element analysis,
   structural control and the use of structural software packages used in
   industry such as ROBOT and ANSYS. Graduates of the Structural Analysis
   and Design Engineering Technology programme, University of
   Houston-Downtown, are successful in reaching responsible positions in
   industry and government. The strong emphasis on computer technology
   provides an advantage to graduates of the programme because they are
   highly productive from day one of employment.},
ISBN = {0-7326-2201-8},
Unique-ID = {WOS:000177219600011},
}

@article{ WOS:000207581400010,
Author = {Mach, Vaclav},
Title = {PRESTA: associating promoter sequences with information on gene
   expression},
Journal = {GENOME BIOLOGY},
Year = {2002},
Volume = {3},
Number = {9},
Abstract = {Background: Large sets of well-characterized promoter sequences are
   required to facilitate the understanding of promoter architecture. The
   major sequence databases are a prospective source of upstream regulatory
   regions, but suffer from inaccurate annotation. The software tool PRESTA
   (PRomoter EST Association) presented in this study is designed for
   efficient recovery of characterized and partially verified promoters
   from GenBank and EMBL libraries.
   Results: The PRESTA algorithm examines the putative GenBank/EMBL
   promoters and automatically removes most of the poorly annotated
   entries. The remaining records are connected to expressed sequence tags
   (ESTs) through a high-stringency BLAST search. The frequency and source
   of recovered ESTs provide an estimate of the activity and expression
   pattern of the promoter, and the ESTs' 5' ends assist in transcription
   start-site verification. The PRESTA database provides easy access to
   non-redundant upstream regulatory regions recently extracted by the
   PRESTA algorithm. The current size of this resource is 552 human and 241
   mouse promoters. Surprisingly, no overlap between the PRESTA database
   and the Eukaryotic Promoter Database (EPD) was detected by sequence
   comparison.
   Conclusions: The PRESTA algorithm demonstrates the principle of promoter
   verification by mapping EST 5' ends. The publicly available PRESTA
   database collects hundreds of characterized and partially verified
   promoter sequences and is complementary to other promoter databases.},
Article-Number = {0050.1},
ISSN = {1474-760X},
Unique-ID = {WOS:000207581400010},
}

@inproceedings{ WOS:000176704400032,
Author = {Niere, J and Schafer, W and Wadsack, JP and Wendehals, L and Welsh, J},
Book-Group-Author = {ACM
   ACM},
Title = {Towards pattern-based design recovery},
Booktitle = {ICSE 2002: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON SOFTWARE
   ENGINEERING},
Year = {2002},
Pages = {338-348},
Note = {24th International Conference on Software Engineering, ORLANDO, FL, MAY
   19-25, 2002},
Organization = {Assoc Comp Mach; ACM SIGSOFT; IEEE Comp Soc; IEEE Comp Soc, Tech Council
   Software Engn; ACM SIGPLAN; Sociedad Argentina Informat Invest Operat},
Abstract = {A method and a corresponding tool is described which assist design
   recovery and program understanding by recognising instances of design
   patterns semi-automatically. The approach taken is specifically designed
   to overcome the existing scalability problems caused by many design and
   implementation variants of design pattern instances. Our approach is
   based on a new recognition algorithm which works incrementally rather
   than trying to analyse a possibly large software system in one pass
   without any human intervention. The new algorithm exploits domain and
   context knowledge given by a reverse engineer and by a special
   underlying data structure, namely a special form of an annotated
   abstract syntax graph, A comparative and quantitative evaluation of
   applying the approach to the Java AWT and JGL libraries is also given.},
ISBN = {1-58113-472-X},
Unique-ID = {WOS:000176704400032},
}

@inproceedings{ WOS:000180358500020,
Author = {Rohrer, MW and McGregor, IW},
Editor = {Yucesan, E and Chen, CH and Snowdon, JL and Charnes, JM},
Title = {Simulating reality using automod},
Booktitle = {PROCEEDINGS OF THE 2002 WINTER SIMULATION CONFERENCE, VOLS 1 AND 2},
Year = {2002},
Pages = {173-181},
Note = {35th Winter Simulation Conference, SAN DIEGO, CA, DEC 08-11, 2002},
Organization = {Amer Stat Assoc; ACM SIGSIM; IEEE Comp Soc; IEEE Syst, Man \& Cybernet
   Soc; Inst Ind Engineers; Inst Operat Res \& Management Sci, Coll
   Simulat; Natl Inst Stand \& Technol; Sci Modeling \& Simulat Int},
Abstract = {Decision making in industry has become more complicated in recent years.
   Customers are more demanding, competition is more fierce, and costs for
   labor and raw materials continue to rise. Managers need state-of-the-art
   tools to help in planning, design, and operations of their facilities.
   Simulation provides a virtual factory where ideas can be tested and
   performance improved. The AutoMod product suite from Brooks-PRI
   Automation has been used on thousands of projects to help engineers and
   managers make the best decisions possible. With the release of AutoMod
   11.0 in 2002, AutoMod now supports hierarchical model construction. This
   new architecture allows users to reuse model objects in other models,
   decreasing the time required to build a model. Composite models are just
   one of the latest advances that make AutoMod one of the most widely used
   simulation software packages.},
ISBN = {0-7803-7614-5},
Unique-ID = {WOS:000180358500020},
}

@article{ WOS:000172228600001,
Author = {Berman, F and Chien, A and Cooper, K and Dongarra, J and Foster, I and
   Gannon, D and Johnsson, L and Kennedy, K and Kesselman, C and
   Mellor-Crummey, J and Reed, D and Torczon, L and Wolski, R},
Title = {The GrADS project: Software support for high-level grid application
   development},
Journal = {INTERNATIONAL JOURNAL OF HIGH PERFORMANCE COMPUTING APPLICATIONS},
Year = {2001},
Volume = {15},
Number = {4},
Pages = {327-344},
Month = {WIN},
Abstract = {Advances in networking technologies will soon make it possible to use
   the global information infrastructure in a qualitatively different
   way-as a computational as well as an information resource. As described
   in the recent book The Grid: Blueprint for a New Computing
   Infrastructure, this Grid will connect the nation's computers,
   databases, instruments, and people in a seamless web of computing and
   distributed intelligence, which can be used in an on demand fashion as a
   problem-solving resource in many fields of human endeavor-and, in
   particular, science and engineering. The availability of grid resources
   will give rise to dramatically new classes of applications, in which
   computing resources are no longer localized but, rather, distributed,
   heterogeneous, and dynamic; computation is increasingly sophisticated
   and multidisciplinary; and computation is integrated into our daily
   lives and, hence, subject to stricter time constraints than at present.
   The impact of these new applications will be pervasive, ranging from new
   systems for scientific inquiry, through computing support for crisis
   management, to the use of ambient computing to enhance personal mobile
   computing environments. To realize this vision, significant scientific
   and technical obstacles must be overcome. Principal among these is
   usability. The goal of the Grid Application Development Software (GrADS)
   project is to simplify distributed heterogeneous computing in the same
   way that the World Wide Web simplified information sharing over the
   Internet. To that end, the project is exploring the scientific and
   technical problems that must be solved to make it easier for ordinary
   scientific users to develop, execute, and tune applications on the Grid.
   In this paper, the authors describe the vision and strategies underlying
   the GrADS project, including the base software architecture for grid
   execution and performance monitoring, strategies and tools for
   construction of applications from libraries of grid-aware components,
   and development of innovative new science and engineering applications
   that can exploit these new technologies to run effectively in grid
   environments.},
DOI = {10.1177/109434200101500401},
ISSN = {1094-3420},
EISSN = {1741-2846},
ResearcherID-Numbers = {Johnsson, Lennart/U-5280-2019
   Dongarra, Jack/G-4199-2019
   },
ORCID-Numbers = {Dongarra, Jack/0000-0003-3247-1782
   Kesselman, Carl/0000-0003-0917-1562},
Unique-ID = {WOS:000172228600001},
}

@article{ WOS:000172158400003,
Author = {Liu, L and Pu, C and Han, W},
Title = {An XML-enabled data extraction toolkit for web sources},
Journal = {INFORMATION SYSTEMS},
Year = {2001},
Volume = {26},
Number = {8},
Pages = {563-583},
Month = {DEC},
Abstract = {The amount of useful semi-structured data on the web continues to grow
   at a stunning pace. Often interesting web data are not in database
   systems but in HTML pages, XML pages, or text files, Data in these
   formats are not directly usable by standard SQL-like query processing
   engines that support sophisticated querying and reporting beyond
   keyword-based retrieval. Hence, the web users or applications need a
   smart way of extracting data from these web sources. One of the popular
   approaches is to write wrappers around the sources, either manually or
   with software assistance, to bring the web data within the reach of more
   sophisticated query tools and general mediator-based information
   integration systems. In this paper, we describe the methodology and the
   software development of an XML-enabled wrapper construction system-XWRAP
   for semi-automatic generation of wrapper programs. By XML-enabled we
   mean that the metadata about information content that are implicit in
   the original web pages will be extracted and encoded explicitly as XML
   tags in the wrapped documents, In addition. the query-based content
   filtering process is performed against the XML documents. The XWRAP
   wrapper generation framework has three distinct features. First, it
   explicitly separates tasks of building wrappers that are specific to a
   web source from the tasks that are repetitive for any source, and uses a
   component library to provide basic building blocks for wrapper programs.
   Second, it provides inductive learning algorithms that derive or
   discover wrapper patterns by reasoning about sample pages or sample
   specifications. Third and most importantly, we introduce and develop a
   two-phase code generation framework. The first phase utilizes an
   interactive interface facility to encode the source-specific metadata
   knowledge identified by individual wrapper developers as declarative
   information extraction rules. The second phase combines the information
   extraction rules generated at the first phase with the XWRAP component
   library to construct an executable wrapper program for the given web
   source. (C) 2001 Elsevier Science Ltd. All rights reserved.},
DOI = {10.1016/S0306-4379(01)00040-0},
ISSN = {0306-4379},
EISSN = {1873-6076},
Unique-ID = {WOS:000172158400003},
}

@article{ WOS:000172890600023,
Author = {Muller, ML and Ganslandt, T and Eich, HP and Lang, K and Ohmann, C and
   Prokosch, HU},
Title = {Towards integration of clinical decision support in commercial hospital
   information systems using distributed, reusable software and knowledge
   components},
Journal = {INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS},
Year = {2001},
Volume = {64},
Number = {2-3, SI},
Pages = {369-377},
Month = {DEC},
Note = {16th Medical Informatics Europe Congress/45th Annual Congress of the
   German-Association-for-Medical-Informatics-Biometry-and-Epidemiology,
   Hanover, GERMANY, JUN, 2000},
Organization = {German Assoc Med Informat Biometry \& Epidemiol},
Abstract = {Problem: Clinicians' acceptance of clinical decision support depends on
   its workflow-oriented, context-sensitive accessibility and availability
   at the point of care, integrated into the Electronic Patient Record
   (EPR). Commercially available Hospital Information Systems (HIS) often
   focus on administrative tasks and mostly do not provide additional
   knowledge based functionality. Their traditionally monolithic and closed
   software architecture encumbers integration of and interaction with
   external software modules. Our aim was to develop methods and interfaces
   to integrate knowledge sources into two different commercial hospital
   information systems to provide the best decision support possible within
   the context of available patient data. Methods: An existing, proven
   standalone scoring system for acute abdominal pain was supplemented by a
   communication interface. In both HIS we defined data entry forms and
   developed individual and reusable mechanisms for data exchange with
   external software modules. We designed an additional knowledge support
   frontend which controls data exchange between HIS and the knowledge
   modules. Finally, we added guidelines and algorithms to the knowledge
   library. Results: Despite some major drawbacks which resulted mainly
   from the HIS' closed software architectures we showed exemplary, how
   external knowledge support can be integrated almost seamlessly into
   different commercial HIS. This paper describes-the prototypical design
   and current implementation and discusses our experiences. (C) 2001
   Elsevier Science Ireland Ltd. All rights reserved.},
DOI = {10.1016/S1386-5056(01)00218-0},
ISSN = {1386-5056},
EISSN = {1872-8243},
ResearcherID-Numbers = {Ganslandt, Thomas/AAC-6690-2019
   Ohmann, Christian/AAV-8608-2020},
ORCID-Numbers = {Ganslandt, Thomas/0000-0001-6864-8936
   },
Unique-ID = {WOS:000172890600023},
}

@article{ WOS:000172874600005,
Author = {Tsubouchi, K and Yokoyama, M and Nakase, H},
Title = {A new concept of 3-dimentional multilayer-stacked system-in-package for
   software-defined-radio},
Journal = {IEICE TRANSACTIONS ON ELECTRONICS},
Year = {2001},
Volume = {E84C},
Number = {12},
Pages = {1730-1734},
Month = {DEC},
Abstract = {In the present GHz-clock high-density LSI, a design of signal lines is
   getting so critical that the transmission line analysis should be
   introduced to signal line design. This leads to the complex design of
   line structure and i/o drivers including impedance matching. Our target
   is to implement a system-in-package (SiP) for software-defined-radio
   (SDR) The SiP operates tip to 10GHz, and requires a compact and
   high-density packaging technology with a simple signal wiring design. In
   this paper, we propose a new concept of 3-D multilayer-stacked SiP. The
   new 3-D packaging concept includes (1) design guideline for
   interconnection lengths. (2) bridging register circuits in LSI chips.
   (3) flip-chip microbump bonding technology of chips onto system-buildup
   printed wiring boards (PWB), (4) multilayer-stacked 3-D package of
   several sets of chips and PWB. and (5) 100-mum-diameter bumps at
   peripheral region of PWB as vertical via-bump bus lines. A critical
   interconnect length, in which interconnect wiring is treated as a
   conventional RC line, is discussed for wiring design. Both wiring
   lengths in LSI chips and that among chips corresponding to total
   thickness of vertical bus lines are designed to be shorter than the
   critical length. The key points of the 3-D pack-age for GHz signal
   transfer are a delay guarantee due to limitation of line length and
   separation between local lines in a chip and a bus line among chips.},
ISSN = {0916-8524},
EISSN = {1745-1353},
ORCID-Numbers = {YOKOYAMA, Michio/0000-0003-2837-6272},
Unique-ID = {WOS:000172874600005},
}

@article{ WOS:000172918100001,
Author = {Hobbs, DW and Guo, T},
Title = {Library design concepts and implementation strategies (Reprinted from
   Combinatorial Library Design and Evaluation: Principles, Software Tools,
   and Applications in Drug Discovery, pg 1-49, 2001)},
Journal = {JOURNAL OF RECEPTOR AND SIGNAL TRANSDUCTION RESEARCH},
Year = {2001},
Volume = {21},
Number = {4},
Pages = {311-356},
Month = {NOV},
DOI = {10.1081/RRS-100107922},
ISSN = {1079-9893},
Unique-ID = {WOS:000172918100001},
}

@article{ WOS:000170386600009,
Author = {Neymeyer, J and Tunn, R},
Title = {Computeraided documentation and evaluation of medical data - as a
   contribution to quality management},
Journal = {GEBURTSHILFE UND FRAUENHEILKUNDE},
Year = {2001},
Volume = {61},
Number = {7},
Pages = {511-516},
Month = {JUL},
Abstract = {From a medical perspective, the aim of continuous, computer-aided
   documentation of treatment data is to improve the provision of medical
   services. The integration of all medical information available is such a
   complex task that it can only be accomplished with the aid of advanced
   and powerful computer technology. Sophisticated systems with intranet
   connection will ensure an efficient provision of medical services by
   allowing for the direct and immediate documentation of all
   treatment-related data.
   The S\_Q\_Lap(TM)-documentation software package presented here is a
   client/server-structured, Web-based intranet program that can be used in
   combination with standard software (MS Win-word 97 or 2000) and does not
   require any additional, registered commercial software. This package can
   be used for network operation or a stand-alone computer (e.g. notebook)
   and thus serve a whole department or just a single treatment unit.
   Password access for reading and writing can be organized according to
   hierarchical levels. General data protection is ensured by encrypted
   data transmission.
   Forms for data entry that are structured according to accepted
   guidelines allow for the establishment of rapidly accessible databases
   that meet the requirements in terms of both formal criteria and
   contents. With the aid of such databases, patient reports can be written
   in a very efficient manner. Other programs and relational databases as
   well as text files listing addresses, diagnoses etc. rendered, are
   integrated into the word processor. The very fast and robust,
   intranet-compatible S\_Q\_Lap(TM)-systems are especially suitable for
   scientific purposes that require multicenter studies with large
   SQL-databases.},
DOI = {10.1055/s-2001-15971},
ISSN = {0016-5751},
EISSN = {1438-8804},
Unique-ID = {WOS:000170386600009},
}

@article{ WOS:000169151900010,
Author = {Hiddema-van de Wal, A and Smith, RJA and van der Werf, GT and Meyboom-de
   Jong, B},
Title = {Towards improvement of the accuracy and completeness of medication
   registration with the use of an electronic medical record (EMR)},
Journal = {FAMILY PRACTICE},
Year = {2001},
Volume = {18},
Number = {3},
Pages = {288-291},
Month = {JUN},
Abstract = {Background. Approximately 80\% of GPs use a GP information system (GIS)
   and an electronic medical record (EMR) in their daily practice. To reap
   the full benefits of an EMR for patient care, post-graduate education
   and research, the data input must be well structured and accurately
   coded.
   Objectives. The quality and user-friendliness of the software positively
   influence the completeness and reliability of the data recorded in the
   GIS. To assess this in actual practice, this study examined whether or
   not an increase occurred in the accuracy and completeness of
   indication-related medication registration after the GIS's software
   package was upgraded.
   Method. GPs recorded data for the Registration Network Groningen (RNG)
   concerning four medication groups: insulin, trimethoprim. the
   contraceptive pill and beta -blocking agents. The completeness and
   accuracy of the registered data were assessed both before and after the
   change to the new software package. The completeness is evaluated on the
   basis of the indications missing for the prescribed medications. To
   assess accuracy, a check was made to determine whether the indications
   corresponded to those deemed relevant for that particular medication
   according to National Pharmaceutical Guidelines.
   Results. The percentage of missing indications decreased notably,
   especially in the chronically prescribed medication groups. For insulin,
   the percentage decreased from 40.5 to 3\% and for the contraceptive pill
   from 34.5 to 1\%. For trimethoprim, the percentage decreased from 10 to
   1\%, and for beta -blocking agents from 22 to 1.5\%. Of the indications
   present, the percentage of relevant indications showed a slight
   increase, with the largest increase observed for the contraceptive pill
   where the percentage rose from 86 to 96\%.
   Conclusions. The completeness of recorded indications improved
   considerably after the change of software. This is due mostly to the
   efforts of the GPs, their practice assistants and the support of the RNG
   organization involved in the conversion procedure. Accuracy improved
   slightly, especially due to the software modifications which ensured
   that non-existent codes could not be entered. To summarize, with
   increased user-friendliness of the software, combined with the training
   of motivated GPs, the quality of recorded data improved.},
ISSN = {0263-2136},
Unique-ID = {WOS:000169151900010},
}

@article{ WOS:000169629600005,
Author = {Pena-Mora, F and Vadhavkar, S and Dirisala, SK},
Title = {Component-based software development for Integrated Construction
   Management software applications},
Journal = {AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND
   MANUFACTURING},
Year = {2001},
Volume = {15},
Number = {2},
Pages = {173-187},
Month = {APR},
Abstract = {This paper presents a framework and a prototype for designing Integrated
   Construction Management (ICM) software applications using reusable
   components. The framework supports the collaborative development of ICM
   software applications by a group of ICM application developers from a
   library of software components. The framework focuses on the use of an
   explicit software development process to capture and disseminate
   specialized knowledge that augments the description of the ICM software
   application components in a library. The importance of preserving and
   using this knowledge has become apparent with the recent trend of
   combining the software development process with the software application
   code. There are three main components in the framework: design patterns,
   design rationale model, and intelligent search algorithms. Design
   patterns have been chosen to represent, record, and reuse the recurring
   design structures and associated design experience in object-oriented
   software development. The Design Recommendation and Intent Model (DRIM)
   was extended in the current research effort to capture the specific
   implementation of reusable software components. DRIM provides a method
   by which design rationale from multiple ICM application designers can be
   partially generated, stored, and later retrieved by a computer system.
   To address the issues of retrieval, the paper presents a unique
   representation of a software component, and a search mechanism based on
   Reggia's setcover algorithm to retrieve a set of components that can be
   combined to get the required functionality is presented. This paper also
   details an initial, proof-of-concept prototype based on the framework.
   By supporting nonobtrusive capture as well as effective access of vital
   design rationale information regarding the ICM application development
   process, the framework described in this paper is expected to provide a
   strong information base for designing PCM software.},
DOI = {10.1017/S0890060401152054},
ISSN = {0890-0604},
EISSN = {1469-1760},
Unique-ID = {WOS:000169629600005},
}

@inproceedings{ WOS:000171525900160,
Author = {Azimi, M and Nasiopoulos, P and Ward, RK},
Editor = {Dunne, S},
Title = {Implementation of MPEG System Target Decoder.},
Booktitle = {CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING 2001, VOLS I
   AND II, CONFERENCE PROCEEDINGS},
Year = {2001},
Pages = {943-948},
Note = {Canadian Conference on Electrical and Computer Engineering, TORONTO,
   CANADA, MAY 13-16, 2001},
Organization = {IEEE Canada; Gen Elect Co; Gennum Corp; Sympatico; Bell Nexxia},
Abstract = {The MPEG-2 system standard is being widely used as a transport system to
   deliver compressed video, audio and other multimedia contents in various
   applications such as digital television broadcasting and video
   communications. This standard provides methods for multiplexing a number
   of elementary MPEG streams into a single system stream. It also defines
   methods to maintain the synchronization and timing of compressed
   streams. This is achieved by exact definitions of the times at which
   data arrive to the decoder, timing of data flow in the decoder and
   timing of decoding and presentation events. For this purpose, the
   standard defines a conceptual model for a target decoder, called
   ``System Target Decoder{''} (STD), which is used to model the decoding
   process. System streams generated by the multiplexer should comply with
   the specifications imposed by the STD model to guarantee the normal
   operations of real time decoding and presentation process. Therefore,
   this model is necessary during the construction and verification of
   system streams. The multiplexer should observe the behavior of STD to
   ensure that decoder buffers will not overflow or underflow due to
   encoding or multiplexing issues when receiving the system stream. To
   achieve this, the scheduler that coordinates the multiplexing order of
   system packs should consider the , monitored information from STD as one
   of the scheduling control parameters to follow the specifications
   imposed by STD.
   This paper describes the theoretical principles, design considerations
   and architecture of Pro-ram and Transport STDs. The implementations of
   these target decoders in a software package for verification of MPEG
   system streams is presented. This implementation uses the Microsoft
   DirectShow and has many attractive features as modularity, generating
   full reports of buffer occupancies, and supporting all levels of
   buffering (Transport level buffering, Multiplex buffering and Elementary
   Stream buffering) of STD. The results of decoding some sample system
   streams are also presented.},
ISBN = {0-7803-6715-4},
Unique-ID = {WOS:000171525900160},
}

@inproceedings{ WOS:000169430500181,
Author = {Chen, CT and Zhao, J and Chen, QL},
Book-Group-Author = {IEEE
   IEEE},
Title = {A simulation study of simultaneous switching noise},
Booktitle = {51ST ELECTRONIC COMPONENTS \& TECHNOLOGY CONFERENCE},
Series = {ELECTRONIC COMPONENTS AND TECHNOLOGY CONFERENCE},
Year = {2001},
Pages = {1102-1106},
Note = {51st Electronic Components and Technology Conference (ECTC), ORLANDO,
   FL, MAY 29-JUN 01, 2001},
Organization = {IEEE Components, Packaging \& Mfg Technol Soc; Electr Components,
   Assemblies \& Mat Assoc},
Abstract = {This paper describes a new methodology for simultaneous switching noise
   (SSN) simulations by using a system level signal integrity (ST) analysis
   software, which is combinations of a quick full wave electromagnetic
   field solver for multiple-layer structure based on FDTD (Finite
   Difference Time Domain) and a circuit solver. The solution is based on
   the geometry, material, stack-up structure, and basic circuit
   imformation. The simultaneous switching noise issue is studied for two
   types of chipset packages - OLGA (Organic Land Grid Array) and WBGA
   (Wirebond Bah Grid Array) with 40 drivers switching simultaneously.
   Different simulation conditions, such as with or without on-die
   interconnection model, different on-die decoupling capacitor values, are
   imposed during the simulations. Simultaenous switching noise (SSN)
   effects such as skew, signal overshoort, ring back, and power-ground
   voltage fluctuations, are obtained and compared. These data can be used
   for a design guideline specification or for package performance
   improvement purposes. It is believed that all these studies are very
   informative to chip and package analysis and design for high-speed
   system applications.},
DOI = {10.1109/ECTC.2001.927956},
ISSN = {0569-5503},
ISBN = {0-7803-7038-4},
Unique-ID = {WOS:000169430500181},
}

@inproceedings{ WOS:000178200800309,
Author = {den Bosch, AO and Santamaria, JC},
Book-Group-Author = {MARINE TECHNOLOGY SOCIETY
   MARINE TECHNOLOGY SOCIETY
   MARINE TECHNOLOGY SOCIETY},
Title = {Remote visualization and management tools for underwater operations},
Booktitle = {OCEANS 2001 MTS/IEEE: AN OCEAN ODYSSEY, VOLS 1-4, CONFERENCE PROCEEDINGS},
Year = {2001},
Pages = {1953-1959},
Note = {Annual Conference of the Marine-Technology-Society, HONOLULU, HI, NOV
   05-08, 2001},
Organization = {Marine Technol Soc; IEEE; OES; Minerals, Met \& Mat Soc; Soc Explorat
   Geophysicists; Amer Geophys Union; Womens Aquat Network; Coasts, Oceans,
   Ports \& Rivers Inst; Amer Meteorol Soc; Oceanog Soc},
Abstract = {This paper introduces a new set of software tools that integrate
   near-real-time visualization with a publish and subscribe mechanism to
   achieve remote monitoring and control of dynamic objects in an
   underwater scene. The approach proposed in this paper involves the
   integration of existing technologies to produce a powerful and flexible
   solution to problems in which enhanced awareness of the situation can
   lead to improvements in performance. These tools can be easily adapted
   to the extensive and diverse set of situations encountered in underwater
   construction, underwater surveying and maritime navigation. Real-time
   data from instruments and positioning sensors is made available by using
   a publishing mechanism and a remote data server. Users with Internet or
   Intranet access can subscribe to any real-time data field being
   published and receive updates every time the information changes.
   Underwater infrastructure information resulting from construction,
   maintenance and inspection is often stored in databases. The software
   libraries introduced in this paper also provide a full set of tools to
   store and retrieve information from such databases. The database access
   tools use the same publish and subscribe mechanism involved in real-time
   data acquisition. This similarity makes it very easy for developers to
   integrate historic and live data in a single monitoring application.
   The visualization tools presented in this paper enable developers to
   define virtual scenes that can display all the relevant elements
   associated with an underwater construction job, including complex
   structures and dynamic objects; i.e., ROVs, vessels, etc. The use of
   advance cueing techniques and multi-resolution rendering make it
   possible to achieve satisfactory interactive frame rates without
   sacrificing accuracy and realism. The concept of 3D indicator modules
   will also be introduced. These powerful modules can be linked to live
   data using the subscribe tools and attached to any element in the
   environment including dynamic ones. This flexibility allows the users to
   monitor the data, not only as it changes, but also in the 3D location
   that makes the most sense. The three components of the tools being
   introduced in this paper (data acquisition, data distribution, and scene
   visualization) will be fully described along with examples of
   applications that take full advantage of the integration of all three
   components.},
ISBN = {0-933957-28-9},
Unique-ID = {WOS:000178200800309},
}

@inproceedings{ WOS:000172615800031,
Author = {Freund, E and Ludemann-Ravit, B and Stern, O and Koch, T},
Book-Group-Author = {IEEE
   IEEE
   IEEE},
Title = {Creating the architecture of a translator framework for robot
   programming languages},
Booktitle = {2001 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS
   I-IV, PROCEEDINGS},
Series = {IEEE International Conference on Robotics and Automation ICRA},
Year = {2001},
Pages = {187-192},
Note = {IEEE International Conference on Robotics and Automation, SEOUL, SOUTH
   KOREA, MAY 21-26, 2001},
Organization = {IEEE Robot \& Automat Soc; IEEE},
Abstract = {This paper presents a novel approach to facilitate the development and
   maintenance of translators for industrial robot programming languages.
   Such translators are widely used in robot simulation and offline
   programming systems to support programming in the respective native
   robot language. Our method is based upon a software architecture, that
   is provided as a complete translator framework. For the developer of a
   new translator, it offers convenient strategies to concentrate on robot
   specific language elements during the design and implementation process:
   fill-in templates, libraries for common functionality, design patterns
   etc., all tied up with a general translation scheme. In contrast to
   other compiler construction tools, the developers need not care about
   the complex details of a whole translator. As a matter of principle, the
   architecture offers a complete default translator (except for the
   grammar). Robot specific elements can be held in separate units -
   outside of the actual translator - to facilitate maintenance and feature
   extension. The most probable changes in the translator product life
   cycle are restricted to the adaptation of these units. Several
   translators built upon this framework are in actual use in the
   commercial robot simulation system COSIMIR(R) to support native language
   robot programming, as well as in the widely used robot programming
   system COSIROP to verify the syntax of robot programs.},
ISSN = {1050-4729},
ISBN = {0-7803-6576-3},
Unique-ID = {WOS:000172615800031},
}

@inproceedings{ WOS:000172837500078,
Author = {Graham, P},
Editor = {Buyya, R and Mohay, G and Roe, P},
Title = {A DSM cluster architecture supporting aggressive computation in active
   networks},
Booktitle = {FIRST IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE
   GRID, PROCEEDINGS},
Year = {2001},
Pages = {547-554},
Note = {1st IEEE/AMC International Symposium on Cluster Computing and the Grid,
   BRISBANE, AUSTRALIA, MAY 15-18, 2001},
Organization = {IEEE, Comp Soc; IEEE Comp Soc, Task Force Cluster Comp; Assoc Comp Mach;
   SIGARCH; IEEE Comp Soc, Tech Comm Parallel Proc; Queensland Univ Technol},
Abstract = {Active networks allow computations to be performed in-network at routers
   as messages pass through them, Active networks offer unique
   opportunities to optimize network-centric applications in ways that are
   not possible using conventional networks. Unfortunately, the need to
   route packets at full network speed means that very little computation
   can be done per packet, per router This seriously restricts the range of
   in-network applications that can be developed. Computationally intensive
   applications are restricted to executing outside the network and thus
   many potential in-network optimizations are precluded.
   We propose a scalable cluster architecture using software Distributed
   Shared Memory. (DSM) that can be used as an ``attached processor{''} at
   routers for executing active code. This novel application of DSM enables
   the construction of aggressive active net-work protocols by providing
   significant compute capacity outside the router's critical packet
   routing path. The use of DSM simplifies the implementation, and extends
   the capabilities, of the active packet execution engine in ways that a
   message passing cluster cannot. Further, the characteristics of active
   processing enable specific optimizations to consistency maintenance.},
DOI = {10.1109/CCGRID.2001.923241},
ISBN = {0-7695-1010-8; 0-7695-1011-6},
Unique-ID = {WOS:000172837500078},
}

@inproceedings{ WOS:000178162700031,
Author = {Kilgore, RA},
Editor = {Peters, BA and Smith, JS and Medeiros, DJ and Rohrer, MW},
Title = {Open-source SML and silk for Java-based, object-oriented simulation},
Booktitle = {WSC'01: PROCEEDINGS OF THE 2001 WINTER SIMULATION CONFERENCE, VOLS 1 AND
   2},
Year = {2001},
Pages = {262-268},
Note = {Winter Simulation Conference (WSC 01), ARLINGTON, VA, 2001},
Organization = {Amer Stat Assoc; ACM SIGSIM; IEEE Comp Soc; IEEE Syst, Man \& Cybernet
   Soc; Inst Ind Engineers; INFORMS Coll Simulat; Natl Inst Stand \&
   Technol; Soc Comp Simulat Int},
Abstract = {Silk(R) and SML are software libraries of Java, C++, C\# and VB.Net
   classes that support object-oriented, discrete-event simulation. SML(TM)
   is a new open-source or ``free{''} software library of simulation
   classes that enable multi-language development of complex, yet
   manageable simulations through the construction of usable and reusable
   simulation objects. These objects are usable because they express the
   behavior of individual entity-threads from the system object perspective
   using familiar process-oriented modeling within an object-oriented
   design supported by a general purpose programming language. These
   objects are reusable because they can be easily archived, edited and
   assembled using professional development environments that support
   multi-language, cross-platform execution and a common component
   architecture. This introduction supports the tutorial session that
   describes the fundamentals of designing and creating an SML or Silk
   model.},
DOI = {10.1109/WSC.2001.977282},
ISBN = {0-7803-7307-3},
Unique-ID = {WOS:000178162700031},
}

@inproceedings{ WOS:000172097600014,
Author = {Klima, V and Kavicka, A and Adamko, N},
Editor = {Kerckhoffs, EJH and Snorek, M},
Title = {Software tool VIRTUOS - Simulation of railway station operation},
Booktitle = {MODELLING AND SIMULATION 2001},
Year = {2001},
Pages = {84-88},
Note = {15th European Simulation Multiconference (ESM 2001), CTU, PRAGUE, CZECH
   REPUBLIC, JUN 06-09, 2001},
Organization = {Soc Comp Simulat Int; ASIM; Arbeitsgemeinsch Simulat},
Abstract = {This paper deals with problems of modeling railway junctions, which are
   specialized for sorting processes in the system of railway freight
   traffic. Also a mention is made about the simulation model architecture
   and already realized projects. Emphasis is given to the description of
   construction of a virtual junction (computer model of a real station)
   and to a simulation of its operation within the frame of virtual
   environment, using the software package VIRTUOS.},
ISBN = {1-56555-225-3},
ResearcherID-Numbers = {Adamko, Norbert/AAI-6756-2021},
Unique-ID = {WOS:000172097600014},
}

@article{ WOS:000170493600011,
Author = {Monlezun, CJ},
Title = {Orthogonally decomposing the interaction sum of squares in two factor
   experiments when both factors are quantitative: Unbalanced data and
   unequal spacing},
Journal = {COMMUNICATIONS IN STATISTICS-THEORY AND METHODS},
Year = {2001},
Volume = {30},
Number = {7},
Pages = {1411-1433},
Abstract = {In this article, interaction contrasts are expressed as a polynomial
   function of both of the quantitative classification variables, the
   values of which give rise to the levels of the two factors. When all
   polynomial coefficients are zero, there is no interaction between the
   two factors. When at least some of these coefficients are not zero,
   interaction between the two factors exists and structure is imposed on
   the interaction contrasts. The imposed structures are examined in
   detail. To test the hypothesis that any particular set of polynomial
   coefficients is zero, the observation vector may be regressed on a
   specially constructed set of independent variables, and the numerator
   sum of squares for the associated F test may be obtained by adding
   certain of the sequential sums from the regression. The construction of
   the regression variables and subsequent regression may be performed
   easily with the aid of any statistical computing package. No special
   software is necessary.},
DOI = {10.1081/STA-100104753},
ISSN = {0361-0926},
Unique-ID = {WOS:000170493600011},
}

@inproceedings{ WOS:000168192400061,
Author = {Rasala, R and Raab, J and Proulx, VK},
Book-Group-Author = {ACM
   ACM
   ACM},
Title = {Java power tools: Model software for teaching object-oriented design},
Booktitle = {PROCEEDINGS OF THE THIRTY-SECOND SIGCSE TECHNICAL SYMPOSIUM ON COMPUTER
   SCIENCE EDUCATION},
Series = {SIGCSE BULLETIN : A QUARTERLY PUBLICATION OF THE SPECIAL INTEREST GROUP
   ON COMPUTER SCIENCE EDUCATION},
Year = {2001},
Volume = {33},
Number = {1},
Pages = {297-301},
Note = {32nd SIGCSE Technical Symposium on Computer Science Education,
   CHARLOTTE, NC, FEB 21-25, 2001},
Organization = {Assoc Comp Machinery; Special Interest Grp Comp Educ},
Abstract = {The Java Power Tools or JPT is a Java toolkit designed to enable
   students to rapidly develop graphical user interfaces in freshman
   computer science programming projects. Because it is simple to create
   GUIs using JPT, students can focus on the more fundamental issues of
   computer science rather than on widget management. In a separate
   article{[}4], we will discuss with examples how the JPT can help
   freshman students to learn about the basics of algorithms, data
   structures, classes, and interface design. In this article, we will
   focus on how the JPT itself can be used as an extended case study of
   object-oriented design principles in a more advanced course.
   The fundamental design principles of the JPT are that the elements of a
   graphical user interface should be able to be combined recursively as
   nested views and that the communication between these views and the
   internal data models should be as automatic as possible. In particular,
   in JPT, the totality of user input from a complex view can be easily
   converted into a corresponding data model and any input errors will be
   detected and corrected along the way. This ease of communication is
   achieved by using string objects as a lingua franca for views and models
   and by using parsing when appropriate to automatically check for errors
   and trigger recovery. The JPT achieves its power by a combination of
   computer science and software design principles. Recursion, abstraction,
   and encapsulation are systematically used to create GUI tools of great
   flexibility. It should be noted that a much simpler pedagogical package
   for Java IO was recently presented in {[}9].},
DOI = {10.1145/364447.364606},
ISBN = {1-58113-329-4},
Unique-ID = {WOS:000168192400061},
}

@article{ WOS:000173823500007,
Author = {Shih, TK and Chang, SK and Tsai, J and Ma, JH and Huang, RH},
Title = {Supporting well-engineered Web documentation development - a multimedia
   software engineering approach toward virtual university courseware
   designs},
Journal = {ANNALS OF SOFTWARE ENGINEERING},
Year = {2001},
Volume = {12},
Pages = {139-165},
Abstract = {Distance learning has become a very important mechanism for virtual
   university operation. In order to realize such an operation smoothly, it
   is necessary to consider distanced learning from three perspectives:
   administration, awareness, and assessment. We are currently implementing
   a virtual university environment according to these guidelines. In this
   paper, we propose part of such a supporting environment of the
   Multimedia Macro-University project.(1) One of the most important
   focuses is a Web course development paradigm. Web documents are
   increasingly playing an important role in our daily life, as the
   Internet has become a new medium for communication and commerce. On the
   other hand, software development paradigms were developed to support
   program construction. However, these traditional paradigms do not
   completely fit the needs of Web document development due to the
   following reasons. Firstly, computer programs focus on problem solving,
   but Web documents focus on information delivery. Secondly, computer
   programs usually have a fixed size, but this is not true for Web
   documents,, because Web documents are always evolving as if it were a
   living document. It is therefore necessary to investigate a new software
   development paradigm for developing Web documents. We propose such a new
   paradigm and its supporting environment, as well as software
   testing/metrics mechanisms for Web documents. The Web documents
   developed using our paradigm are stored in a Web documentation database.
   From a script description, to its implementation as well as testing
   records, the database and its interface allow the user to design Web
   documents as virtual courses to be used in a Web-savvy virtual library.
   The system is implemented as a three-tier architecture, which runs under
   MS Windows.},
DOI = {10.1023/A:1013366823005},
ISSN = {1022-7091},
Unique-ID = {WOS:000173823500007},
}

@article{ WOS:000165306800001,
Author = {McCluskey, AE},
Title = {Paid attendant carers hold important and unexpected roles which
   contribute to the lives of people with brain injury},
Journal = {BRAIN INJURY},
Year = {2000},
Volume = {14},
Number = {11},
Pages = {943-957},
Month = {NOV},
Abstract = {Objective: Paid attendant carers spend many hours assisting people with
   a brain injury. Despite this considerable responsibility, most carers
   receive little support or training and their roles are often
   ill-defined. This exploratory study set out to define the key roles of
   paid carers.
   Method: Ten semi-structured interviews were conducted. Perspectives were
   sought from 10 participants: five people with a traumatic brain injury
   and five paid carers. A computer software package, NUD{*}IST was used
   during analysis to help identify and categorize commonly recurring
   themes.
   Results: Five major roles were identified: Attendant, Protector, Friend,
   Coach and Negotiator. Friendship was the most important aspect of the
   care relationship for three of the people with a brain injury, most of
   whom had lost their pre-injury friends and associates. Carers were
   required to negotiate frequently with clients and their families, and
   with other service providers. Sound communication skills were required.
   Conclusion: In addition to further research, industry guidelines are
   required which take account of the wider suite of roles fulfilled by
   paid carers, address training and support needs, and occupational health
   and safety issues.},
DOI = {10.1080/02699050050191896},
ISSN = {0269-9052},
Unique-ID = {WOS:000165306800001},
}

@article{ WOS:000165421200002,
Author = {Tashman, J},
Title = {Out-of-sample tests of forecasting accuracy: an analysis and review},
Journal = {INTERNATIONAL JOURNAL OF FORECASTING},
Year = {2000},
Volume = {16},
Number = {4},
Pages = {437-450},
Month = {OCT-DEC},
Abstract = {in evaluations of forecasting accuracy, including forecasting
   competitions, researchers have paid attention to the selection of time
   series and to the appropriateness of forecast-error measures. However,
   they have not formally analyzed choices in the implementation of
   out-of-sample tests, making it difficult to replicate and compare
   forecasting accuracy studies. In this paper, I (1) explain the structure
   of out-of-sample tests, (2) provide guidelines for implementing these
   tests, and (3) evaluate the adequacy of out-of-sample tests in
   forecasting software. The issues examined include series-splitting
   rules, fixed versus rolling origins, updating versus recalibration of
   model coefficients, fixed versus rolling windows, single versus multiple
   test periods, diversification through multiple time series, and design
   characteristics of forecasting competitions. For individual time series,
   the efficiency and reliability of out-of-sample tests can be improved by
   employing rolling-origin evaluations, recalibrating coefficients, and
   using multiple test periods. The results of forecasting competitions
   would be more generalizable if based upon precisely described groups of
   time series, in which the series are homogeneous within group and
   heterogeneous between groups. Few forecasting software programs
   adequately implement out-of-sample evaluations, especially general
   statistical packages and spreadsheet add-ins. (C) 2000 Elsevier Science
   BN. All rights reserved.},
ISSN = {0169-2070},
EISSN = {1872-8200},
Unique-ID = {WOS:000165421200002},
}

@article{ WOS:000088814400002,
Author = {Agarwala, R and Biesecker, LG and Schaffer, AA},
Title = {Inverse inbreeding coefficient problems with an application to linkage
   analysis of recessive diseases in inbred populations},
Journal = {DISCRETE APPLIED MATHEMATICS},
Year = {2000},
Volume = {104},
Number = {1-3},
Pages = {3-44},
Month = {AUG 15},
Abstract = {Medical geneticists connect relatives having the same disease into a
   family structure called a pedigree. Genetic linkage analysis uses
   pedigrees to find the approximate chromosomal locations of
   disease-causing genes. The problem of choosing a pedigree is
   particularly interesting for diseases inherited in an autosomal
   recessive pattern in inbred populations because there are many possible
   paths of inheritance to choose from. A variety of shortcuts are taken to
   produce plausible pedigrees from inbred populations. We lay the
   mathematical foundations for a shortcut that was recently used in a
   pedigree-disease study of an inbred Mennonite population. Recessive
   disease genes can be localized using the shortcut of homozygosity
   mapping by finding regions of the genome where affected persons are
   homozygous. An important quantity in homozygosity mapping is the
   inbreeding coefficient of a person, which is the prior probability that
   the person inherited the same piece of DNA on both copies of the
   chromosome from a single ancestor. Software packages are ill-suited to
   handle large pedigrees with many inbreeding loops. Therefore, we
   consider the problem of generating small pedigrees that match the
   inbreeding coefficient of one or more affected persons in the larger
   pedigree. We call such a problem an inverse inbreeding coefficient
   problem, We focus on the case where there is one sibship with one or
   more affected persons, and consider the problem of constructing a
   pedigree so that it is ``simpler{''} and gives the sibship a specified
   inbreeding coefficient. First, we give a construction that yields small
   pedigrees for any inbreeding coefficient. Second, we add the constraint
   that ancestor-descendant matings are not allowed, and we give another
   more complicated construction to match any inbreeding coefficient.
   Third, we show some examples of how to use the one-sibship construction
   to do pedigree replacement on real pedigrees with multiple affected
   sibships. Fourth, we give a different construction to match the
   inbreeding coefficient of one sibship, while attempting to minimize a
   measure of the inbreeding loop complexity. (C) 2000 Elsevier Science
   B.V. All rights reserved.},
DOI = {10.1016/S0166-218X(00)00193-1},
ISSN = {0166-218X},
ResearcherID-Numbers = {Schaffer, Alejandro/N-1222-2019
   Biesecker, Leslie/AAG-2526-2021
   Schaffer, Alejandro A/F-2902-2012
   },
ORCID-Numbers = {Agarwala, Richa/0000-0002-5518-9723},
Unique-ID = {WOS:000088814400002},
}

@article{ WOS:000088675100003,
Author = {Rumsey, S and Spoiden, A},
Title = {Evaluation of Tolimac: a secure library management system for
   controlling access to, and payment for, electronic information services},
Journal = {JOURNAL OF LIBRARIANSHIP AND INFORMATION SCIENCE},
Year = {2000},
Volume = {32},
Number = {2},
Pages = {64-71},
Month = {JUN},
Abstract = {Tolimac (Total Library Management Concept) is an Internet based system
   for providing controlled access to networked information resources,
   particularly electronic document delivery (EDD). The system, based on
   smart cards and PINs (Personal Identification Number), encryption
   technology and an architecture is designed so that the library acts as
   an intermediary between the user and the document supplier, addresses
   three EDD problem areas facing libraries: computer security with regard
   to open networks (confidentiality, identification, authentication and
   document integrity); user authorization; and cost recovery. The
   prototype was evaluated by the Tolimac Consortium at three libraries:
   Universite Libre de Bruxelles (ULB); George Edwards Library, University
   of Surrey; and University College Dublin (UCD); results from the first
   two sites only being reported in this article. User based performance
   evaluation, at ULB and Surrey was achieved using the Performance
   Measurement Method (PMM), whereas Dublin used the SUMI (Software
   Usability Measurement Inventory) evaluation method. Overall reaction to
   the Tolimac system was positive and users thought that the system was
   easy to use and were happy with the response time the smart card access.},
ISSN = {0961-0006},
Unique-ID = {WOS:000088675100003},
}

@article{ WOS:000087474200050,
Author = {Fromme, M and Hoffmann-Schulz, G and Litvinenko, E and Ziem, P},
Title = {BEAN - A new standard program for data analysis at BER-II},
Journal = {IEEE TRANSACTIONS ON NUCLEAR SCIENCE},
Year = {2000},
Volume = {47},
Number = {2, 1},
Pages = {272-275},
Month = {APR},
Note = {11th Conference on Real-Time Computer Applications in Nuclear, Particle,
   and Plasma Physics (RT 99), SANTA FE, NEW MEXICO, JUN 14-18, 1999},
Organization = {Inst Electr \& Electron Engineers; NPSS Techn Comm Comp Applicat Nucl \&
   Plasma Sci},
Abstract = {A program package BEAN (BENSC Analysis Program) has been developed to
   provide a standard for the analysis of one-dimensional neutron spectra
   gathered at the experiment facilities of BENSC {[}1]. The main purpose
   was to replace the large number of heterogeneous software packages which
   has been developed in the past for each neutron spectrometer separately.
   BEAN runs on a UNIX application server which is embedded in a
   client/server architecture with NFS filesystem. Data display is
   performed by means of the PV-WAVE software tools. For easy use the
   graphical user interface follows the common styling guidelines of
   Windows programs.},
DOI = {10.1109/23.846162},
ISSN = {0018-9499},
ResearcherID-Numbers = {Litvinenko, Elena I/O-8924-2014},
ORCID-Numbers = {Litvinenko, Elena I/0000-0001-6868-7003},
Unique-ID = {WOS:000087474200050},
}

@article{ WOS:000085543100005,
Author = {Nath, SK and Shahid, S and Dewangan, P},
Title = {SEISRES - a Visual C++ program for the sequential inversion of seismic
   re-fraction and geoelectric data},
Journal = {COMPUTERS \& GEOSCIENCES},
Year = {2000},
Volume = {26},
Number = {2},
Pages = {177-200},
Month = {MAR},
Abstract = {Refraction seismic and geoelectric methods are usually applied for the
   delineation of near surface structures in environmental, engineering and
   hydrogeological investigations. When applied independently, these
   techniques yield sufficiently accurate subsurface models. But the
   inversion of these data may also lead to incorrect. parameter estimation
   specially in complicated geological situations, namely, blind zone
   problems in seismics, suppression and equivalence problems in
   geoelectrics. Stability and non-uniqueness can be reduced to a great
   extent by integrating physically different sets of data into a joint or
   sequential inversion scheme. In the present paper, we aim at introducing
   one such algorithm, wherein the seismic refraction and DC resistivity
   inversion routines are amalgamated. Even though the seismic and
   geoelectric methods may independently see different interfaces due to
   completely different physical responses, the joint or sequential
   inversion needs a common parameter, the layer thickness being the one in
   this situation. The proposed scheme is coded in Visual C++ on Microsoft
   Windows `95 environment using the concept of object-oriented
   programming. The program SEISRES is exclusively menu driven and
   customised for running on personal computers. It has several options,
   namely, seismic ray inversion for near-surface estimation, curve
   dissemination for generating a starting model using seismic depth
   section, 1D resistivity inversion for Schlumberger and Wenner single
   electrode arrays using evolutionary programming for global optimisation,
   1D forward calculations, creation of resistivity data sets from Wenner
   multi-electrode pseudo-section and construction of quasi-2D geoelectric
   section of the subsurface. The software is tested on a variety of
   synthetic examples with complex litho-stratigraphic relationships. The
   present paper deals with three such synthetic examples for the
   delineation of an aquifer in a three-layer setting in the first two
   examples and the detection of a thin conductive clay lens embedded in an
   aquifer in a four layered earth model emulating an equivalence problem.
   The seismic-guided 1D Schlumberger and Wenner inversions and the
   quasi-2D sections from pseudo-section interpretation led to subsurface
   information with better precision compared to the direct 2D inversion. A
   detailed field investigation was undertaken in Midnapur District for
   ground water prospecting. The results of two test sites at Satkui and
   Tangasol are presented here for judging the performance of the software
   package. Available borehole lithologs and geophysical logs validated the
   findings from SEISRES. The strength of the present scheme lies in its
   ability to model the subsurface seismically and geoelectrically, even in
   2D environment by 1D approximation on a laptop/personal computer at-site
   cost-effectively. (C) 2000 Elsevier Science Ltd. All rights reserved.},
DOI = {10.1016/S0098-3004(99)00086-2},
ISSN = {0098-3004},
ResearcherID-Numbers = {Dewangan, Pawan/C-3837-2009
   SHAHID, SHAMSUDDIN/B-5185-2010},
ORCID-Numbers = {SHAHID, SHAMSUDDIN/0000-0001-9621-6452},
Unique-ID = {WOS:000085543100005},
}

@article{ WOS:000084613900015,
Author = {Beroud, C and Collod-Beroud, G and Boileau, C and Soussi, T and Junien,
   C},
Title = {UMD (Universal Mutation Database): A Generic software to build and
   analyze locus-specific databases},
Journal = {HUMAN MUTATION},
Year = {2000},
Volume = {15},
Number = {1},
Pages = {86-94},
Abstract = {The human genome is thought to contain about 80,000 genes and presently
   only 3,000 are known to be implicated in genetic diseases. In the near
   future, the entire sequence of the human genome will be available and
   the development of new methods for point mutation detection will lead to
   a huge increase in the identification of genes and their mutations
   associated with genetic diseases as well as cancers, which is growing in
   frequency in industrial states. The collection of these mutations will
   be critical for researchers and clinicians to establish
   genotype/phenotype correlations. Other fields such as molecular
   epidemiology will also be developed using these new data. Consequently,
   the future lies not in simple repositories of locus-specific mutations
   but in dynamic data bases linked to various computerized tools for their
   analysis and that can be directly queried on-line. To meet this goal, we
   devised a generic software called UMD (Universal Mutation Database). It
   was developed as a generic software to create locus-specific databases
   (LSDBs) with the 4(th) Dimension(R) package from ACI. This software
   includes an optimized structure to assist and secure data entry and to
   allow the input of various clinical data. Thanks to the flexible
   structure of the UMD software, it has been successfully adapted to nine
   genes either involved in cancer (APC, P53, RB1, MEN1, SUR1, VHL, and
   WT1) or in genetic diseases (FBN1 and LDLR). Four new LSDBs are under
   construction (VLCAD, MCAD, KIR6, and COL4A5). Finally, the data can be
   transferred to core databases. Hum Mutat 15:86-94, 2000. (C) 2000
   Wiley-Liss, Inc.},
DOI = {10.1002/(SICI)1098-1004(200001)15:1<86::AID-HUMU16>3.0.CO;2-4},
ISSN = {1059-7794},
ResearcherID-Numbers = {boileau, catherine/M-4482-2017
   COLLOD-BEROUD, Gwenaëlle/A-8342-2008
   BEROUD, Christophe/A-8381-2008
   },
ORCID-Numbers = {boileau, catherine/0000-0002-0371-7539
   COLLOD-BEROUD, Gwenaëlle/0000-0003-4098-6161
   BEROUD, Christophe/0000-0003-2986-8738
   soussi, thierry/0000-0001-8184-3293},
Unique-ID = {WOS:000084613900015},
}

@inproceedings{ WOS:000166401200012,
Author = {Dillon, M and Ogier, JP and Hannah, S and Thompson, M},
Editor = {Ogier, JP},
Title = {Business excellence: Establishing the cost and benefits of quality
   management by software tools},
Booktitle = {PROCEEDINGS OF THE XIVTH INTERNATIONAL SYMPOSIUM ON HORTICULTURAL
   ECONOMICS},
Series = {ACTA HORTICULTURAE},
Year = {2000},
Number = {536},
Pages = {121-128},
Note = {XIVth International Symposium on Horticultural Economics, GUERNSEY,
   ENGLAND, SEP 12-15, 2000},
Organization = {Int Soc Hort Sci, Commiss Econ \& Management},
Abstract = {In today's rapidly changing business environment the grower will need to
   design and build flexible and accurate management systems to meet their
   customer needs. The 1990's have seen a rapid increase in the demands
   from the market for consistent quality product. The traditional market
   for the Island has been the wholesaler, but this sector has been under
   threat from an expansion by the retailer's, increased delivery from
   European supplier's via the Channel Tunnel and lower cost product from
   developing countries. The grower's must therefore operate cost effective
   business systems, which support the needs of their key client groups.
   They must deliver top quality graded product, ensure selection of the
   correct varieties presented in good packaging with an agreed shelf life.
   This increasing pressure has required the planning and implementation of
   new control systems. The resulting changes in business operation have
   often been managed by ``experience' of possible impact. Producers have
   access to only limited market knowledge and many do not have accurate
   business costing information. The business unit must therefore identify.
   market trends and cost critical activities to manage and plan
   availability. Only through meeting those management disciplines can
   planning for sales of a quality product be made with greater price
   certainty. The ability to forecast production availability at agreed
   costs will therefore enable forward selling and increased returns by
   better matching of supply and demand. The cost impact of better
   production forecasting, scheduling and improved market analysis must
   therefore be identified and met. The changes in operation required would
   therefore be evaluated quickly and accurately enabling the selection of
   options. which maximise return.
   The paper will compare the approach adopted within the Island Quality
   Chain to approaches being used internationally to develop best practice.
   This approach described involves structured analysis using a range of
   key software tools to analyse business, predict the cost of change and
   return on selected investment. The structure of proposed software
   systems, which the Island could develop in conjunction with key
   wholesalers, is also provided.},
DOI = {10.17660/ActaHortic.2000.536.12},
ISSN = {0567-7572},
ISBN = {90-6605-982-6},
Unique-ID = {WOS:000166401200012},
}

@inproceedings{ WOS:000179289300023,
Author = {Elmaghraby, AS and Abdelhafiz, E and Hassan, MF},
Editor = {Debnath, N},
Title = {An intelligent approach to stock cutting optimization},
Booktitle = {COMPUTER APPLICATIONS IN INDUSTRY AND ENGINEERING},
Year = {2000},
Pages = {90-93},
Note = {13th International Conference on Computer Applications in Industry and
   Engineering, HONOLULU, HI, NOV 01-03, 2000},
Organization = {Int Soc Comp \& Their Applicat},
Abstract = {The stock cutting problem has gained a lot of attention as a means to
   increase efficiency in many industrial sectors. In this paper, we
   present a multi-level, hierarchical, architecture exploiting different
   artificial intelligence methodologies for stock cutting optimization. An
   expert system is used at the highest level for decision functions; the
   second layer utilizes genetic algorithms for pattern construction. A
   third layer using linear programming for pattern selection follows. At
   the lowest layer, a tool set for computer implementation uses heuristic
   and maintains constraints. A prototype software package based on this
   architecture has been implemented and tested on a windows based
   workstation using visual C++. Sample output results demonstrating
   different features are presented.},
ISBN = {1-880843-35-8},
ResearcherID-Numbers = {Elmaghraby, Adel S/B-3353-2014
   Abdelhafiez, Ehab/AAA-7201-2019},
ORCID-Numbers = {Elmaghraby, Adel S/0000-0001-5274-8596
   Abdelhafiez, Ehab/0000-0002-8178-2934},
Unique-ID = {WOS:000179289300023},
}

@inproceedings{ WOS:000165736000065,
Author = {Pollard, JK},
Book-Group-Author = {IEEE COMPUTER SOCIETY
   IEEE COMPUTER SOCIETY},
Title = {Component-based architecture for simulation of transmission systems},
Booktitle = {24TH ANNUAL INTERNATIONAL COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE
   (COSPSAC 2000)},
Series = {Proceedings International Computer Software and Applications Conference},
Year = {2000},
Volume = {24},
Pages = {363-368},
Note = {24th Annual International Computer Software and Applications Conference
   (COMPSAC 2000), TAIPEI, TAIWAN, OCT 25-27, 2000},
Organization = {IEEE, Comp Soc; Fash Now Corp; Flowring Technol Corp; Inst Informat Ind;
   MITAC Synnex Grp; Microelect Technol Inc; Natl Sci Council; Natl Tsing
   Hua Univ; Puretek Ind Co Ltd; TSMC; Tung Hai Univ; World Peace Ind Co
   Ltd; Zen Technol Co Ltd},
Abstract = {Example transmission system simulations are used to illustrate criteria
   for quality architecture: Component inter- operability, re-usability
   reliability and maintainability.
   Top-level architectural issues such as sq stem partition, encapsulation
   of components and a Graphical User interface that is decoupled from the
   core software are considered.
   It is suggested that component communication should be. Write a file
   signal a ``commit{''} and then read by the recipient. This protocol
   allows input and output data types and ranges to be checked. An error
   code on failure allows roll-back to a previously saved state whereas a
   successful completion signal can be used as a sequential control.
   The desirable feature of very loosely, coupled independent components
   implies insensitivity to construction technology. This allows the use of
   legacy and commercial software packages. In addition, components can be
   deployed on different types and scales of networks and can be fixed on
   computers and data transferred to them or vice-versa.},
DOI = {10.1109/CMPSAC.2000.884749},
ISSN = {0730-3157},
ISBN = {0-7695-0792-1},
Unique-ID = {WOS:000165736000065},
}

@article{ WOS:000089857000006,
Author = {Rarey, M and Lengauer, T},
Title = {A recursive algorithm for efficient combinatorial library docking},
Journal = {PERSPECTIVES IN DRUG DISCOVERY AND DESIGN},
Year = {2000},
Volume = {20},
Number = {1},
Pages = {63-81},
Abstract = {Due to the rapid development of combinatorial chemistry and high
   throughput screening, a new virtual screening scenario emerged. While
   previously the focus was on analyzing large collections of compounds
   available to the medicinal chemist, nowadays the search space is defined
   in the form of large, possibly virtual, combinatorial libraries. In this
   article we describe how the structure of combinatorial libraries can be
   exploited to speed up docking predictions. Based on our incremental
   construction method implemented in the docking software FlexX we
   developed a recursive scheme to traverse the combinatorial library space
   efficiently. We applied our docking algorithm to three libraries with
   sizes from a few hundred up to 20 000 molecules. In all cases, we are
   able to show that similar results are achieved as in a sequential
   docking of the library molecules. The computing time, however, can be
   reduced by a factor of up to 30 resulting in an average time of about 5
   s per library molecule.},
DOI = {10.1023/A:1008716720979},
ISSN = {0928-2866},
Unique-ID = {WOS:000089857000006},
}

@article{ WOS:000170207500154,
Author = {Salgado, NC and Gouveia-Oliveira, A},
Title = {Towards a common framework for clinical trials information systems},
Journal = {JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION},
Year = {2000},
Number = {S},
Pages = {754-758},
Note = {Annual Symposium of the American-Medical-Informatics-Association, LOS
   ANGELES, CA, NOV 04-08, 2000},
Organization = {Amer Med Informat Assoc},
Abstract = {This work describes the strategies for data modeling and implementation
   and the general architecture of COATI, a Clinical Trials information
   system that has been in production for more than three years. We discuss
   how the ICH guidelines influenced the system design, how we used
   conventional relational and EAV tables and how we integrated third party
   software packages into our system. We describe a near architecture that
   forms the basis of a common framework for Clinical Trials information
   systems. This structure is based on the concept of a Common Information
   Framework (CIF). We have defined standard objects and corresponding
   methods for the CIF as an essential step towards the development of
   Clinical Trials Informatics.},
ISSN = {1067-5027},
EISSN = {1527-974X},
ResearcherID-Numbers = {Salgado, Nuno C/C-2197-2015
   },
ORCID-Numbers = {Salgado, Nuno C/0000-0002-5740-4239
   Oliveira, Antonio/0000-0001-9103-6532},
Unique-ID = {WOS:000170207500154},
}

@article{ WOS:000085951700008,
Author = {Sheard, T and Benaissa, ZEA and Pasalic, E},
Title = {DSL implementation using staging and monads},
Journal = {ACM SIGPLAN NOTICES},
Year = {2000},
Volume = {35},
Number = {1},
Pages = {81-94},
Month = {JAN},
Abstract = {The impact of Domain Specific Languages (DSLs) on software design is
   considerable. They allow programs to be more concise than equivalent
   programs written in a high-level programming languages. They relieve
   programmers from making decisions about data-structure and algorithm
   design, and thus allows solutions to be constructed quickly. Because
   DSL's are at a higher level of abstraction they are easier to maintain
   and reason about than equivalent programs written in a high-level
   language, and perhaps most importantly they can be written by domain
   experts rather than programmers.
   The problem is that DSL implementation is costly and prone to errors,
   and that high level approaches to DSL implementation often produce
   inefficient systems. By using two new programming language mechanisms,
   program staging and monadic abstraction, we can lower the cost of DSL
   implementations by allowing reuse at many levels. These mechanisms
   provide the expressive power that allows the construction of many
   compiler components as reusable libraries, provide a direct link between
   the semantics and the low-level implementation, and provide the
   structure necessary to reason about the implementation.},
DOI = {10.1145/331963.331975},
ISSN = {0362-1340},
Unique-ID = {WOS:000085951700008},
}

@inproceedings{ WOS:000173181100011,
Author = {Vasilko, M and Machacek, L and Matej, M and Stepien, P and Holloway, S},
Book-Group-Author = {IEEE COMPUTER SOCIETY
   IEEE COMPUTER SOCIETY},
Title = {A rapid prototyping methodology and platform for seamless communication
   systems},
Booktitle = {12TH INTERNATIONAL WORKSHOP ON RAPID SYSTEM PROTOTYPING, PROCEEDINGS},
Series = {IEEE International Symposium on Rapid System Prototyping},
Year = {2000},
Pages = {70-76},
Note = {12th International Workshop on Rapid System Prototyping (RSP 01),
   MONTEREY, CA, JUN 25-27, 2001},
Organization = {IEEE Comp Soc Tech Comm Design Automat; IEEE Comp Soc Tech Comm Simulat},
Abstract = {The availability of reconfigurable technologies has enabled the
   construction of flexible systems allowing run-time reconfiguration of
   system hardware and software functions. ``Seamless communications{''}
   (also known in radio communications world as ``reconfigurable{''} or
   ``software-defined{''} radio) is one of the areas where technologies
   allowing run-time reconfiguration are highly desirable.
   This paper presents a new rapid prototyping methodology and platform for
   prototyping generic seamless communication systems. The methodology
   combines a C-based software design flow targeting host and DSP
   processors, and a rapid FPGA hardware design flow based on HandelC-a
   C-like programming language. The hardware design flow also supports the
   generation of partial FPGA configurations. A library of parametrised
   communication modules was developed to facilitate the rapid construction
   of common communication architectures. A PC-based prototyping platform
   provides a set of custom hardware interfaces for prototyping systems
   with radio-frequency (RF), infrared (IR) and generic wide-bandwidth
   communication links. The feasibility of the presented methodology was
   tested on several simple demonstrator applications.},
ISSN = {1074-6005},
ISBN = {0-7695-1206-2; 0-7695-1207-0},
Unique-ID = {WOS:000173181100011},
}

@article{ WOS:000083838400001,
Author = {Multedo, G and Jibb, D and Angel, G},
Title = {ASAAC Phase II programme progress on the definition of standards for the
   core processing architecture of future military aircraft},
Journal = {MICROPROCESSORS AND MICROSYSTEMS},
Year = {1999},
Volume = {23},
Number = {7},
Pages = {393-407},
Month = {DEC 6},
Abstract = {The Allied Standard Avionics Architecture Council (ASAAC) Phase II
   programme is sponsored by the Ministry of Defence of the UK, Germany and
   France through a Memorandum of Understanding (MOU), which provides for a
   two stage programme over five years to establish a complete set of
   standards for military core avionics for the end of this century. The
   standards will cover systems, software, networks, packaging and common
   functional modules, and will be validated by the construction of a set
   of demonstrators.
   The contract was let on 18th November 1997, and project teams were
   formed in the UK, France and Germany to perform the work. The current
   contract concerns Stage I, i.e. the first 15 months of the ASAAC Phase
   II programme. Stage 1 is required to build on the results of the earlier
   ASAAC Phase I programme by firstly assessing and refining the
   architecture concepts and secondly by producing detailed specifications
   for Stage 2 demonstrations and outline standards for the architecture.
   The main objectives of this paper are, one year after the beginning of
   the ASAAC Phase II programme, to present as:
   ASAAC Phase II programme objectives;
   ASAAC Phase II contractual and industrial organisation;
   Architecture concepts definition results of the Stage 1 work;
   Demonstrations planned for Stage 2.
   (C) 1999 Published by Elsevier Science B.V.},
DOI = {10.1016/S0141-9331(99)00050-2},
ISSN = {0141-9331},
Unique-ID = {WOS:000083838400001},
}

@article{ WOS:000085336500010,
Author = {Li, YJ and Lu, J},
Title = {SEIS++: A pattern language for seismic tools construction and
   integration},
Journal = {ACM SIGPLAN NOTICES},
Year = {1999},
Volume = {34},
Number = {12},
Pages = {57-66},
Month = {DEC},
Abstract = {Predominant industrial practice has evolved from general-purpose class
   libraries to domain-specific frameworks and design patterns. Both of
   them are a means to achieve large-scale reuse by capturing successful
   software development strategies within a particular context. Design
   patterns focus on reuse of recurring architectural design themes and
   mainly consist of predefined design structures that can be used as
   building blocks to compose the architecture of software system. Together
   the patterns in a specific domain form a pattern language, which can be
   used to approach a certain class of problems in the application domain.
   In this paper we propose a pattern language SEIS++, a set of design
   patterns, for seismic tool construction and integration in oil and gas
   exploration domain. The language uses Tools and Materials as the new
   design conception to guide domain-specific application development, and
   to enhance software architecture reusability.},
ISSN = {0362-1340},
Unique-ID = {WOS:000085336500010},
}

@article{ WOS:000083869500002,
Author = {Moore, ML and Gazi, V and Passino, KM and Shackleford, WP and Proctor,
   FM},
Title = {Complex control system design and implementation - Using the NIST-RCS
   software library},
Journal = {IEEE CONTROL SYSTEMS MAGAZINE},
Year = {1999},
Volume = {19},
Number = {6},
Pages = {12-28},
Month = {DEC},
DOI = {10.1109/37.806912},
ISSN = {0272-1708},
ResearcherID-Numbers = {Gazi, Veysel/M-6100-2013},
ORCID-Numbers = {Gazi, Veysel/0000-0002-4383-9215},
Unique-ID = {WOS:000083869500002},
}

@article{ WOS:000082643200010,
Author = {Karim, A and Adeli, H},
Title = {OO information model for construction project management},
Journal = {JOURNAL OF CONSTRUCTION ENGINEERING AND MANAGEMENT},
Year = {1999},
Volume = {125},
Number = {5},
Pages = {361-367},
Month = {SEP-OCT},
Abstract = {Recently, the writers developed a general and powerful mathematical
   model for scheduling construction projects. An optimization formulation
   was presented with the goal of minimizing the direct construction cost.
   The nonlinear optimization problem was solved by the recently patented
   neural dynamics model of Adeli and Park. In this paper an
   object-oriented (OO) information model is presented for construction
   scheduling, cost optimization, and change order management (CONSCOM)
   based on the new construction scheduling model. The goal is to lay the
   foundation for a new generation of flexible, powerful, maintainable, and
   reusable software system for the solution of construction scheduling
   problems. The model is presented as a domain-specific development
   framework using the Microsoft Foundation Class library and utilizing the
   software reuse feature of the framework. The framework reuse
   architecture is more flexible and powerful than other reuse techniques
   such as components and patterns. A companion paper presents the
   implementation of the OO information model in a prototype software
   system for management of construction projects, called CONSCOM.},
DOI = {10.1061/(ASCE)0733-9364(1999)125:5(361)},
ISSN = {0733-9364},
EISSN = {1943-7862},
ResearcherID-Numbers = {adeli, hojjat/D-1430-2010},
ORCID-Numbers = {adeli, hojjat/0000-0001-5718-1453},
Unique-ID = {WOS:000082643200010},
}

@article{ WOS:000082643200011,
Author = {Karim, A and Adeli, H},
Title = {CONSCOM: An OO construction scheduling and change management system},
Journal = {JOURNAL OF CONSTRUCTION ENGINEERING AND MANAGEMENT-ASCE},
Year = {1999},
Volume = {125},
Number = {5},
Pages = {368-376},
Month = {SEP-OCT},
Abstract = {In a companion paper, an object-oriented (OO) information model was
   presented for construction scheduling, cost optimization, and change
   order management (CONSCOM), based on the creation of a domain-specific
   development framework. The framework architecture is developed using
   generic software design elements, called patterns, which provide
   effective low-level solutions for creating, organizing, and maintaining
   objects. The OO model has been implemented in a prototype software
   system for management of construction projects, called CONSCOM, using
   the Microsoft Foundation Class library in Visual C++. CONSCOM is
   particularly suitable for highway construction change order management.
   It can be used by the owner as an intelligent decision support system in
   schedule reviews, progress monitoring, and cost-time trade-off analysis
   for change order approval. The OO information model for construction
   scheduling cost management can be integrated into a concurrent
   engineering model for the architecture, engineering, and construction
   industry.},
DOI = {10.1061/(ASCE)0733-9364(1999)125:5(368)},
ISSN = {0733-9364},
ResearcherID-Numbers = {adeli, hojjat/D-1430-2010},
ORCID-Numbers = {adeli, hojjat/0000-0001-5718-1453},
Unique-ID = {WOS:000082643200011},
}

@article{ WOS:000082695200003,
Author = {Smith, PJ and Smith, JS and Lucas, J},
Title = {Localization of a UUV within structures using range data and world
   modelling techniques},
Journal = {INTERNATIONAL JOURNAL OF SYSTEMS SCIENCE},
Year = {1999},
Volume = {30},
Number = {9},
Pages = {915-928},
Month = {SEP},
Abstract = {Modern industrial underwater vehicles are being operated in ever
   increasing complex and hostile environments. Increasing emphasis on
   safety within the oil industry has led to the advancement of underwater
   vehicle technology. This paper presents the development work for a
   prototype localization system for use on an Unmanned Underwater Vehicle
   (UUV), operating within the vicinity of a structured worksite, e.g. an
   oil platform. This is an important area of research for such
   applications as oil platform and wellhead inspection, seabed object
   identification and wet salvage.
   A suitable testing rig model was constructed and modelled using the
   AutoCAD computer-aided design package. Both visual and range sensor data
   were provided using a one-spot laser triangulation range-finding system.
   A software tool was developed which made it possible to generate a world
   model from a DXF file, exported from the AutoCAD construction drawing.
   Two-dimensional image-processing techniques segmented the camera image's
   scene, and identified regions of interest upon which to range. The
   acquired range sensor data from each region were then analysed to create
   a scene model. This was presented to the cognition system, which
   analysed the software world model, using a tree-searching algorithm, to
   determine which features were present in the scene. The position and
   orientation of the range sensor, with respect to the model, were then
   determined by applying a 3D geometrical solution search around the
   scene's matched features. The resulting range sensor positional
   information was then fed back to AutoCAD, which displayed a
   representation of the scene extracted from the world model.
   The research has demonstrated that provided the system was supplied with
   sufficient visual and range sensor data, then an accurate visual
   estimate of vehicle position was possible.},
ISSN = {0020-7721},
Unique-ID = {WOS:000082695200003},
}

@article{ WOS:000080251400019,
Author = {Pleissner, KP and Hoffmann, F and Kriegel, K and Wenk, C and Wegner, S
   and Sahlstrom, A and Oswald, H and Alt, H and Fleck, E},
Title = {New algorithmic approaches to protein spot detection and pattern
   matching in two-dimensional electrophoresis gel databases},
Journal = {ELECTROPHORESIS},
Year = {1999},
Volume = {20},
Number = {4-5},
Pages = {755-765},
Month = {APR-MAY},
Note = {3rd Siena 2-D Electrophoresis Meeting - From Genome to Proteome, SIENA,
   ITALY, AUG 31-SEP 03, 1998},
Abstract = {Protein spot identification in two-dimensional electrophoresis gels can
   be supported by the comparison of gel images accessible in different
   World Wide Web two-dimensional electrophoresis (2-DE) gel protein
   databases. The comparison may be performed either by visual
   cross-matching between gel images or by automatic recognition of similar
   protein spot patterns. A prerequisite for the automatic point pattern
   matching approach is the detection of protein spots yielding the
   x(s),y(s) coordinates and integrated spot intensities i(s). For this
   purpose an algorithm is developed based on a combination of hierarchical
   watershed transformation and feature extraction methods. This approach
   reduces the strong over-segmentation of spot regions normally produced
   by watershed transformation. Measures for the ellipticity and curvature
   are determined as features of spot regions. The resulting spot lists
   containing x(s),y(s),i(s)triplets are calculated for a source as well as
   for a target gel image accessible in 2-DE gel protein databases. After
   spot detection a matching procedure is applied. Both the matching of a
   local pattern vs, a full 2-DE gel image and the global matching between
   full images are discussed. Preset slope and length tolerances of pattern
   edges serve as matching criteria. The local matching algorithm relies on
   a data structure derived from the incremental Delaunay triangulation of
   a point set and a two-step hashing technique. For the incremental
   construction of triangles the spot intensities are considered in
   decreasing order. The algorithm needs neither landmarks nor an a priori
   image alignment. A graphical user interface for spot detection and gel
   matching is written in the Java programming language for the Internet.
   The software package called CAROL (http://gelmatching.inf.fu-berlin.de)
   is realized in a client-server architecture.},
DOI = {10.1002/(SICI)1522-2683(19990101)20:4/5<755::AID-ELPS755>3.0.CO;2-6},
ISSN = {0173-0835},
Unique-ID = {WOS:000080251400019},
}

@inproceedings{ WOS:000085165100030,
Author = {Boehm, B and Port, D and Egyed, A and Abi-Antoun, M},
Editor = {Donohoe, P},
Title = {The MBASE life cycle architecture milestone package - No architecture is
   an island},
Booktitle = {SOFTWARE ARCHITECTURE},
Series = {INTERNATIONAL FEDERATION FOR INFORMATION PROCESSING},
Year = {1999},
Volume = {12},
Pages = {511-528},
Note = {1st Working IFIP Conference on Software Architecture (WICSA1), SAN
   ANTONIO, TX, FEB 22-24, 1999},
Organization = {Int Federat Informat Processing, TC2},
Abstract = {This paper summarizes the primary criteria for evaluating
   software/system architectures in terms of key system stakeholders'
   concerns. It describes the Model Based Architecting and Software
   Engineering (MBASE) approach for concurrent definition of a system's
   architecture, requirements,operational concept, prototypes, and life
   cycle plans. It summarizes our experiences in using and refining the
   MBASE approach on 31 digital library projects. It concludes that a
   Feasibility Rationale demonstrating consistency and feasibility of the
   various specifications and plans is an essential part of the
   architecture's definition, and presents the current MBASE annotated
   outline and guidelines for developing such a Feasibility Rationale.},
ISSN = {1571-5736},
ISBN = {0-7923-8453-9},
ResearcherID-Numbers = {Celar, Stipe/G-4728-2017
   Egyed, Alexander/E-2632-2017},
ORCID-Numbers = {Celar, Stipe/0000-0003-4234-5819
   Egyed, Alexander/0000-0003-3128-5427},
Unique-ID = {WOS:000085165100030},
}

@inproceedings{ WOS:000079627000068,
Author = {Casartelli, E and Saxer, AP and Gyarmathy, G},
Book-Group-Author = {INST MECH ENGINEERS
   INST MECH ENGINEERS},
Title = {Impact of leading edge redesign on vaned radial diffuser performance},
Booktitle = {THIRD EUROPEAN CONFERENCE ON TURBOMACHINERY - VOLS A AND B: FLUID
   DYNAMICS AND THERMODYNAMICS},
Series = {IMECHE CONFERENCE TRANSACTIONS},
Year = {1999},
Volume = {1999},
Number = {1A-1B},
Pages = {853-863},
Note = {3rd European Conference on Turbomachinery - Fluid Dynamics and
   Thermodynamics, LONDON, ENGLAND, MAR 02-05, 1999},
Organization = {Inst Mech Engineers, Energy Transfer \& Thermofluid Mech Grp; European
   Commiss; Rolls Royce plc; ALSTOM Energy Ltd; ALSTOM Technol Ctr; ALSTOM
   Gas Turbines Ltd},
Abstract = {Vaned diffusers may considerably improve the performance of centrifugal
   compressor stages. The paper addresses the question how modifications of
   the vane profile shape in the leading edge (LE) region will affect the
   diffuser performance and to what extent the improvements depend on the
   diffuser configuration. The standard vanes have a semicircular LE
   profile. Various results of steady-state numerical investigations are
   presented. The 3D flow field has been computed with a commercial
   Navier-Stokes code (TASCflow, AEA Technology pie) using for the
   simulations diffuser inflow (i.e. impeller outflow) boundary conditions
   obtained by measurements and applied for different leading edge shapes
   of the vanes. The leading edge redesign is based on MISES (MIT), a 2D
   analysis and inverse-design software package. The standard and
   redesigned diffusers are evaluated and compared for two blade stagger
   angles.
   A detailed analysis of the flow structure including the loss
   distribution along the diffuser is presented and the results are
   compared for the different cases. Furthermore the performance of the
   diffuser and its subcomponents is analysed and correlated with the help
   of dimensionless quantities such as throat blockage, pressure-recovery
   and total pressure loss coefficients. Overall, the results suggest that
   by optimizing the vane LE-profile a performance improvement can be
   achieved over the complete operating line independently of the vane
   stagger angle.},
ISSN = {1356-1448},
ISBN = {1-86058-196-X},
Unique-ID = {WOS:000079627000068},
}

@inproceedings{ WOS:000079197700014,
Author = {Case, MP and Lu, SCY},
Editor = {Baskin, AB and Kovacs, G and Jacucci, G},
Title = {A discourse model for collaborative design},
Booktitle = {COOPERATIVE KNOWLEDGE PROCESSING FOR ENGINEERING DESIGN},
Year = {1999},
Pages = {205-224},
Note = {3rd International Workshop on Cooperative Knowledge Processing for
   Engineering Problem Solving, UNIV TRENTO, TRENT, ITALY, 1998},
Abstract = {A Discourse Model, including a structure and a process, is developed
   that provides software support for collaborative engineering design.
   Components of the structure include a workspace that incorporates
   frames, constraints, semantic networks, libraries of sharable design
   objects, agents, and a Virtual Workspace Language. Components of the
   process include procedures for identifying agent interest sets, applying
   state transformations to the design model, switching design contexts,
   identifying conflicts between designers, and tracking resolved
   conflicts. The model is implementation-independent and applicable to
   many research and commercial design environments currently available. An
   example scenario is provided in the
   Architecture/Engineering/Construction domain that illustrates
   collaboration during the conceptual design of a fire station.},
ISBN = {0-412-83750-1},
Unique-ID = {WOS:000079197700014},
}

@inproceedings{ WOS:000082111700016,
Author = {Dyne, B and Bernstein, D},
Editor = {Courtois, B and Crary, SB and Ehrfeld, W and Fujita, H and Karam, JM and Markus, K},
Title = {A correct-by-construction approach to MEMS design and analysis},
Booktitle = {DESIGN, TEST, AND MICROFABRICATION OF MEMS AND MOEMS, PTS 1 AND 2},
Series = {PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)},
Year = {1999},
Volume = {3680},
Number = {1\&2},
Pages = {142-149},
Note = {Conference on Design, Test, and Microfabrication of MEMS and MOEMS,
   PARIS, FRANCE, MAR 30-APR 01, 1999},
Organization = {SPIE; CNRS, INPG, UIF, France; IEEE Comp Soc Test Technol Tech Comm},
Abstract = {The need for general modeling and analysis tools for MEMS devices has
   been well established, and several software packages designed to address
   these needs have appeared in the last Eve years. While being powerful
   and comprehensive, existing tools often require specialized knowledge of
   the methods involved in the analysis (e.g., 3D modeling, mesh
   generation), and lack integration into a complete design environment.
   This paper describes a new MEMS design and analysis tool that uses a
   correct-by-construction approach in a tightly integrated environment of
   layout, 3D modeling, analysis, and system design. The MEMS design tool
   suite is integrated into an existing integrated circuit design package.
   Our goal is to automate as many steps of the process as possible, thus
   lowering the barrier to MEMS design.
   Device layout is created either manually, or using automatic layout
   generation macros, then a 3D model is automatically generated from the
   layout and a process description. Fully integrated finite and boundary
   element analysis allows the user to specify boundary conditions on the
   layout or on the 3D model, then automatic mesh generation is performed,
   taking advantage of the layered structure of the device to reduce mesh
   density in non-critical areas. Solvers include electrostatic,
   mechanical, and thermal, and well as coupled domain analysis. The design
   process ensures that the user does not create a device that cannot be
   fabricated. Libraries containing standard fabrication processes (e.g.,
   MUMPS) and material properties will be available. Results of the
   analysis are passed to a behavioral Model Builder which then constructs
   high-level macro models using model order reduction, table construction
   and parameter extraction. These models can then be used in system
   simulations, allowing quick exploration of the design space and system
   verification.},
DOI = {10.1117/12.341174},
ISSN = {0277-786X},
ISBN = {0-8194-3154-0},
Unique-ID = {WOS:000082111700016},
}

@inproceedings{ WOS:000083575600032,
Author = {Furnish, GM},
Editor = {Henderson, ME and Anderson, CR and Lyons, SL},
Title = {Application oriented library design},
Booktitle = {OBJECT ORIENTED METHODS FOR INTEROPERABLE SCIENTIFIC AND ENGINEERING
   COMPUTING, PROCEEDINGS},
Series = {SIAM PROCEEDINGS SERIES},
Year = {1999},
Pages = {296-302},
Note = {SIAM Interdisciplinary Workshop on Object Oriented Methods for
   Interoperable Scientific and Engineering Computing, YORKTOWN HTS, NY,
   OCT 21-23, 1998},
Organization = {SIAM; Natl Sci Fdn},
Abstract = {Object oriented software design in C++ has brought many benefits to the
   field of scientific programming, especially in the areas of code
   structure. maintainability, extensibility, and testability However, the
   introduction of a language supporting user defined types into the
   scientific software arena has resulted in an explosion of disparate end
   user types in the various application codes. This has almost completely
   thwarted the ability to reuse scientific libraries, due to type
   mistmatchs at API boundaries. This paper proposes that the promotion of
   reuseability and iteroperatiblity in the scientific community requires a
   new mindset in the process of library design-one that is ``application
   oriented{''}. Specific design considerations and implementation
   practices which can facilitate this model of software construction are
   detailed.},
ISBN = {0-89871-445-1},
Unique-ID = {WOS:000083575600032},
}

@inproceedings{ WOS:000085165100024,
Author = {Gentleman, WM},
Editor = {Donohoe, P},
Title = {Architecture for software construction by unrelated developers},
Booktitle = {SOFTWARE ARCHITECTURE},
Series = {INTERNATIONAL FEDERATION FOR INFORMATION PROCESSING},
Year = {1999},
Volume = {12},
Pages = {423-435},
Note = {1st Working IFIP Conference on Software Architecture (WICSA1), SAN
   ANTONIO, TX, FEB 22-24, 1999},
Organization = {Int Federat Informat Processing, TC2},
Abstract = {Suppose one COTS (Commercial Off the Shelf) software supplier provides
   an interpreter for a problem oriented language, another provides an
   application generator for producing numerical solvers for a class of
   partial differential equations, and a third produces a visualization
   package. A team of domain specialists writes scripts in the
   problem-oriented language to define cases to be solved, uses the
   application generator to produce an appropriate solver, solves the
   generated PDE, and uses the visualization package to analyze the results
   and adjust the description of cases. Such examples illustrate that large
   and long-lived software systems can result from the combined efforts of
   various unrelated development organizations, organizations not even
   known to one another. No single design authority, to which the others
   report, has overall system responsibility. Such examples also illustrate
   the importance of including in software architecture the relationships
   between entities that exist and are used during the construction
   process, instead of focusing only on relationships between entities that
   exist at run time. The needs for software architecture for such systems
   are not well met by the existing literature.},
ISSN = {1571-5736},
ISBN = {0-7923-8453-9},
Unique-ID = {WOS:000085165100024},
}

@inproceedings{ WOS:000081058400056,
Author = {Heinonen, T and Lahtinen, A and Dastidar, P and Ryymin, P and Laarne, P
   and Malmivuo, J and Laasonen, E and Frey, H and Eskola, H},
Editor = {Mun, SK and Kim, Y},
Title = {Applications of magnetic resonance image segmentation in neurology},
Booktitle = {MEDICAL IMAGING 1999: IMAGE DISPLAY},
Series = {Proceedings of SPIE},
Year = {1999},
Volume = {3658},
Pages = {569-579},
Note = {Conference on Image Display at Medical Imaging 1999, SAN DIEGO, CA, FEB
   21-23, 1999},
Organization = {SPIE; Amer Assoc Physicists Med; Amer Physiol Soc; FDA Ctr Devices \&
   Radiol Hlth; Soc Imaging Sci \& Technol; NEMA, Diagnost Imaging \&
   Therapy Syst Div; Radiol Soc N Amer; Soc Comp Appl Radiol},
Abstract = {After the introduction of digital imaging devices in medicine
   computerized tissue recognition and classification (i.e., segmentation)
   have become important in research and clinical applications. Segmented
   data can be applied among numerous research fields including volumetric
   analysis of particular tissues and structures, construction of
   anatomical models, three-dimensional (3D) visualization, and multimodal
   visualization, hence making segmentation essential in modern image
   analysis. In this research project several PC based software were
   developed in order to segment medical images, to visualize raw and
   segmented images in 3D, and to produce EEG brain maps in which MR images
   and EEG signals were integrated. The software package was tested and
   validated in numerous clinical research projects in hospital
   environment.},
DOI = {10.1117/12.349469},
ISSN = {0277-786X},
EISSN = {1996-756X},
ISBN = {0-8194-3130-3},
ResearcherID-Numbers = {Eskola, Hannu/G-4279-2014},
ORCID-Numbers = {Eskola, Hannu/0000-0003-1673-2305},
Unique-ID = {WOS:000081058400056},
}

@inproceedings{ WOS:000169018700006,
Author = {Milchert, T and Straby, O},
Book-Group-Author = {RINA
   RINA},
Title = {Calculation and full scale measurement of slamming pressures on
   semi-planing V-shaped monohulls},
Booktitle = {INTERNATIONAL CONFERENCE ON HYDRODYNAMICS OF HIGH SPEED CRAFT},
Year = {1999},
Pages = {F1-F21},
Note = {International Conference on Hydrodynamics of High Speed Craft, LONDON,
   ENGLAND, NOV 24-25, 1999},
Abstract = {Slamming loads are governing scantlings and weight of the bottom
   structure of semi-planing monohull vessels and boats. In 1995, the
   Swedish Navy ordered a series of 70m semi-planing corvettes of carbon
   fibre composite construction. The authors are involved in this project
   and particularly with the task of finding and verifying calculation
   methods for slamming pressure and the influence of such factors as hull
   form, sea state, speed etc.
   The work started in 1995 with a literature survey, continued with
   computer simulations and model tests of motions and was concluded by
   full scale tests on a smaller craft in 1998.
   The literature study revealed a fairly large number of articles on the
   subject since 1930. Motion simulations in different sea conditions using
   linear and non-linear strip theory were then carried out both at the
   Royal Institute of Technology, KTH, in Stockholm and in-house using a
   commercial strip theory software package. The results were compared with
   model tests made at SSPA. Slamming pressures were calculated from the
   computed relative motions and slamming factors proposed in literature.
   Finally, one of the more handy calculation methods was validated by full
   scale measurements and statistic theory using a 10m craft at
   semi-planing speed in moderate seas. Correlation between calculated and
   measured peak pressures was surprisingly good.
   The aim of this paper is to present some of the major findings of the
   project such as the nature of the slamming loads, time history,
   magnitude and extension. Further, a relatively simple engineering method
   based on linear strip theory and suitable for design purpose is
   discussed.},
Unique-ID = {WOS:000169018700006},
}

@article{ WOS:000085546100004,
Author = {Olsson, E and Gulliksen, J},
Title = {A corporate style guide that includes domain knowledge},
Journal = {INTERNATIONAL JOURNAL OF HUMAN-COMPUTER INTERACTION},
Year = {1999},
Volume = {11},
Number = {4},
Pages = {317-338},
Abstract = {Different professions adopt their own language such that the semantics
   involve elements very specific to their domain. System developers
   approaching users in a new domain often experience initial difficulties
   when trying to understand these semantics and associated work practices.
   Although most software developers also lack knowledge of human-computer
   interaction (HCI), means of transferring domain and HCI knowledge to
   developers in a convenient form are needed. A domain-specific style
   guide could be a worthy framework for the development of a high-level
   structure of interface elements and guidelines, including domain
   knowledge. Such a style guide is suggested as a practical form for
   packaging domain and HCI knowledge to aid developers. Anticipated
   benefits are enhanced application quality, usability, efficiency, and
   acceptance as the communication among software developers and intended
   users improves. The speed of application development could also
   increase.
   This article summarizes a project in which a medical style guide was
   developed and describes in more detail the work procedure utilized in
   the development of a corporate style guide for the tax-handling domain.
   Finally, suggestions on style guide development conditions are presented
   based on experiences from the establishment of the style guide in an
   organization.},
DOI = {10.1207/S15327590IJHC1104\_3},
ISSN = {1044-7318},
Unique-ID = {WOS:000085546100004},
}

@inproceedings{ WOS:000084985900007,
Author = {Sheard, T and Benaissa, Z and Pasalic, E},
Book-Group-Author = {USENIX
   USENIX},
Title = {DSL implementation using staging and monads},
Booktitle = {USENIX ASSOCIATION PROCEEDINGS OF THE 2ND CONFERENCE ON DOMAIN-SPECIFIC
   LANGUAGES (DSL'99)},
Year = {1999},
Pages = {81-94},
Note = {2nd Conference on Domain-Specific Languages (DSL 99), AUSTIN, TX, OCT
   03-05, 1999},
Organization = {USENIX Assoc; ACM SIGPLAN; ACM SIGSOFT},
Abstract = {The impact of Domain Specific Languages (DSLs) on software design is
   considerable. They allow programs to be more concise than equivalent
   programs written in a high-level programming languages. They relieve
   programmers from making decisions about data-structure and algorithm
   design, and thus allows solutions to be constructed quickly. Because
   DSL's are at a higher level of abstraction they are easier to maintain
   and reason about than equivalent programs written in a high-level
   language, and perhaps most importantly they can be written by domain
   experts rather than programmers.
   The problem is that DSL implementation is costly and prone to errors,
   and that high level approaches to DSL implementation often produce
   inefficient systems. By using two new programming language mechanisms,
   program staging and monadic abstraction, we can lower the cost of DSL
   implementations by allowing reuse at many levels. These mechanisms
   provide the expressive power that allows the construction of many
   compiler components as reusable libraries, provide a direct link between
   the semantics and the low-level implementation, and provide the
   structure necessary to reason about the implementation.},
ISBN = {1-880446-27-8},
Unique-ID = {WOS:000084985900007},
}

@inproceedings{ WOS:000086544800014,
Author = {Tscharnuter, D},
Editor = {Bungartz, HJ and Durst, F and Zenger, C},
Title = {Numerical simulation of vibrations for the design of a rear axle},
Booktitle = {HIGH PERFORMANCE SCIENTIFIC AND ENGINEERING COMPUTING},
Series = {LECTURE NOTES IN COMPUTATIONAL SCIENCE AND ENGINEERING},
Year = {1999},
Volume = {8},
Pages = {167-176},
Note = {International Conference on High Performance Scientific and Engineering
   Computing, MUNICH, GERMANY, MAR 16-18, 1998},
Organization = {FORTWIHR; Siemens AG},
Abstract = {The construction and design of the axles of a new vehicle is one of the
   most important parts in developing a car. Lightweight structures are
   used in order to reduce the weight of cars. This can be reached using
   new materials that are often more flexible, e.g., using aluminium
   instead of steel. So the conventional rigid multibody models seem to be
   no more sufficient for the accurate modeling of all physical properties,
   in particular the vibrations. Therefore, we investigate a multibody
   model of a rear axle, which contains an elastic subframe. In order to
   reduce the degrees of freedom of the finite element model, the dynamic
   behavior is approximated by; the modal Ritz ansatz. This approach is a
   good approximation of the deformation of the subframe, because the
   deformations are small. In the software package ADAMS that can be used
   for solving multibody systems the Craig-Bampton method is used for this
   approach. We focus our investigations on the lateral vibrations of the
   rear axle. numerical results for a rigid and an elastic multibody
   formulation are presented and discussed.},
ISSN = {1439-7358},
ISBN = {3-540-65730-4},
Unique-ID = {WOS:000086544800014},
}

@article{ WOS:000075881300012,
Author = {Mosley, DH},
Title = {Perspectives for Java-based computational quantum chemistry},
Journal = {INTERNATIONAL JOURNAL OF QUANTUM CHEMISTRY},
Year = {1998},
Volume = {70},
Number = {1},
Pages = {159-165},
Month = {OCT 5},
Note = {9th International Congress of Quantum Chemistry (ICQC), ATLANTA, GA, JUN
   09-15, 1997},
Organization = {Int Business Machines Corp; Emory Univ; Molec Simulat Inc; Battelle
   Pacific NW Labs; Elsevier Sci BV; NASA Ames Res Ctr; John Wiley \& Sons
   Inc; Hyperchem Inc},
Abstract = {The Java programming language is an architecture-independent, object
   oriented language, designed for secure local and network applications.
   Programs written in Java can either be embedded within hypertext
   documents on the World Wide Web (so-called applets), or be fully
   stand-alone, as software written in other programming languages. In this
   article, we present two sample quantum chemistry applications written in
   Java. The first is an educational applet, illustrating an LCAO-SCF
   calculation of the ground state of the helium atom using a double-zeta
   basis set. The second is a prototype one- and two-electron integral
   package for Gaussian-type atomic orbitals. The potential for Java
   applications to play an educational role and for the construction of
   object-oriented libraries of basic functionality for quantum chemical
   software is discussed. (C) 1998 John Wiley \& Sons, Inc.},
DOI = {10.1002/(SICI)1097-461X(1998)70:1<159::AID-QUA13>3.3.CO;2-Z},
ISSN = {0020-7608},
EISSN = {1097-461X},
Unique-ID = {WOS:000075881300012},
}

@article{ WOS:000072319300023,
Author = {Noguchi, K and Kawashima, Y and Narita, S},
Title = {A highly reliable frame-relay switching node architecture based on ATM
   switching technology},
Journal = {IEICE TRANSACTIONS ON COMMUNICATIONS},
Year = {1998},
Volume = {E81B},
Number = {2},
Pages = {315-323},
Month = {FEB},
Abstract = {Local Area Networks(LANs)are now being used all over the world. The need
   for cost-effective and high-speed communication services, such as LAN
   interconnections and large-volume file transfer of all types of data is
   rapidly increasing, At the same time, Internet services are spreading
   rapidly, and we'll soon see the construction of a cost-effective open
   computer network (OCN). Frame-relay and cell-relay technologies which
   can achieve higher-speed and higher-performance switching than packet
   switching, are therefore attracting much attention. Frame-relay
   technologies are also important because they provide an infrastructure
   for high-speed data communication as fast as 1.5 Mbit/sec. Demand for
   these frame-relay network services have been increasing rapidly. We
   propose a cost-effective and highly reliable node architecture that we
   have developed at NTT. Our basic concept for this is based on the
   ail-band switching node architecture which can provide both STM and ATM
   switches on the same hardware and software platforms, and can
   accommodate any type of node, such as STM nodes, and ATM nodes for
   B-ISDN. Our proposed architecture forms highly reliable frame-relay
   network infrastructure. By using a scale-flexibility building-block
   architecture, we can construct a small-scale node and a large-scale node
   cost-effectively. Next, the key technologies of highly reliable node
   architecture are presented. These are methods of changing over following
   function-units without frame-loss and/or cell-loss. We present two
   examples: frame-relay protocol processing units (PPUs) with an
   N+M-redundant architecture that consists of a number of acting PPUs(ACT)
   and a number of standby PPUs (SBY)waiting to become active, and
   duplicate ATM Mux/DemuX blocks(ATM MDXs)with a cell shaping buffer.},
ISSN = {0916-8516},
EISSN = {1745-1345},
Unique-ID = {WOS:000072319300023},
}

@inproceedings{ WOS:000166217800076,
Author = {Andrews, R and Ponton, JW},
Editor = {Pekny, JF and Blau, GE},
Title = {A process engineering information management system using World Wide Web
   technology},
Booktitle = {THIRD INTERNATIONAL CONFERENCE ON FOUNDATIONS OF COMPUTER-AIDED PROCESS
   OPERATIONS},
Series = {AICHE SYMPOSIUM SERIES},
Year = {1998},
Volume = {94},
Number = {320},
Pages = {501-506},
Note = {3rd International Conference on Foundations of Computer-Aided Process
   Operations, SNOWBIRD, UT, JUL 05-10, 1998},
Organization = {Natl Sci Fdn; Dow Chem; Eastman Chem},
Abstract = {The effective management of process engineering information, especially
   as it is created, manipulated and extended in the context of process
   design, is a task of increasing complexity, but of critical importance.
   A wide range of prototype software environments for this activity have
   been developed in recent years, including Design Kit (MIT), n-Dim (CMU),
   and KBDS (Edinburgh). A feature of all of these has been the major
   software effort involved in their creation, and in many cases the use of
   rather exotic programming languages such as Lisp. We describe an
   object-based process engineering environment having client server
   architecture exploiting standard World Wide Web (WWW) servers. Objects
   can exist, and methods be applied, on a world wide distributed system.
   Platform independence is achieved through the cross platform
   availability of web browsers and servers, and through the Java and
   JavaScript languages. It has proved possible to use standard features of
   the WWW and related tools to provide many of the facilities which had to
   be created specially in the earlier environments. The prototype system
   is in principle accessible to anyone with a WWW browser and an Internet
   connection. The system has been used to interconnect a range of process
   engineering tools including AspenPlus, an in house physical property
   databank and a separately developed process synthesis package, CHiPS.
   Information may be freely transferred between these tools, and also to
   and from other standard software e.g. any spreadsheet system. A number
   of special tools have also been developed for use within the system,
   including flowsheet graphics and a simple equation based flowsheeting
   package. The system structure lends itself to a `componentware',
   approach that has simplified and speeded the construction of these
   tools. The paper will describe the approach used in developing platform
   independent and extensible process engineering objects, their
   manipulation and management on a client-server system, and the
   techniques available for tool integration. The advantages of the
   componentware approach will be highlighted, and future applications and
   possibilities discussed.},
ISSN = {0065-8812},
ISBN = {0-8169-0776-5},
Unique-ID = {WOS:000166217800076},
}

@inproceedings{ WOS:000089422200008,
Author = {de Groot, A and Bracht, M and Padro, CEG and Schucan, TH and Skolnik, E},
Editor = {Bolcich, JC and Veziroglu, TN},
Title = {Development of guidelines for the design of integrated hydrogen systems},
Booktitle = {HYDROGEN ENERGY PROGRESS XII, VOLS 1-3},
Year = {1998},
Pages = {75-86},
Note = {12th World Hydrogen Energy Conference, BUENOS AIRES, ARGENTINA, JUN
   21-26, 1998},
Organization = {Asociac Argentina Hidrogeno; Int Assoc Hydrogen Energy},
Abstract = {As part of the International Energy Agency Hydrogen Implementing
   Agreement, an evaluation tool to assist in the design, operation and
   optimization of hydrogen demonstration facilities is under development.
   Using commercially available flowsheet simulation software (ASPEN
   plus(R)) as the integrating platform, this tool is designed to provide
   system developers with a comprehensive database or library of component
   models and an integrating platform through which these models may be
   linked. By combining several energy system components a conceptual
   design of an integrated hydrogen energy system can be made. As a part of
   the tool and connected to the library ate design guidelines which can
   help finding the optimal configuration in the design process. The
   component categories considered include: production, storage, transport,
   distribution and end use of hydrogen.
   This paper discusses the development of guidelines which will aid the
   user of the tool in the optimization and selection of hydrogen systems.
   A strict design methodology has been instituted to ensure an unambiguous
   and optimal use of the design guidelines. For this purpose 5 design
   steps and 4 data categories have been defined. The design guidelines
   consist of information which has been collected and is formatted
   according to the data structure defined by the design methodology and
   indications on how the data can be used in the subsequent design steps.
   Using the guidelines the experience acquired in existing and future
   integrated systems is made accessible, for instance for use in modelling
   the hydrogen technology components in ASPEN. Most importantly, the
   design guidelines should help the user combine the components into a
   model of an integrated system.},
ISBN = {987-97075-3-2},
Unique-ID = {WOS:000089422200008},
}

@inproceedings{ WOS:000077910500006,
Author = {Dennis, JB},
Book-Group-Author = {IEEE
   IEEE},
Title = {A parallel program execution model supporting modular software
   construction},
Booktitle = {THIRD WORKING CONFERENCE ON MASSIVELY PARALLEL PROGRAMMING MODELS,
   PROCEEDINGS},
Year = {1998},
Pages = {50-60},
Note = {3rd Working Conference on Massively Parallel Programming Models
   (MPPM97), LONDON, ENGLAND, NOV 12-14, 1997},
Organization = {Fujitsu European Ctr Informat Technol (FECIT)},
Abstract = {A watershed is near in the architecture of computer systems. There is
   overwhelming demand for systems that support a universal format for
   computer programs and software components so users may benefit from
   their use on a wide variety of computing platforms. At present this
   demand is being met by commodity microprocessors together with standard
   operating system interfaces. However current systems do not offer a
   standard API (application program interface) for parallel programming,
   and the popular interfaces for parallel computing violate essential
   principles of modular or component-based software construction.
   Moreover; microprocessor architecture is reaching the limit of what can
   be done usefully within the framework of superscalar and VLIW processor
   models. The next step is to put several processors (or the equivalent)
   on a single chip.
   This paper presents a set of principles for modular software
   construction, and descibes a program execution model based on functional
   programming that satisfies the set of principles. The implications of
   the pinciples for computer system architecture are discussed together
   with a sketch of the architecture of a multithread processing chip which
   promises to provide efficient execution of parallel computations while
   providing a sound base for modular software construction.},
DOI = {10.1109/MPPM.1997.715961},
ISBN = {0-8186-8427-5},
Unique-ID = {WOS:000077910500006},
}

@article{ WOS:000077802900003,
Author = {Eriksson, J and Gulliksson, M and Lindstrom, P and Wedin, PA},
Title = {Regularization tools for training large feed-forward neural networks
   using automatic differentiation},
Journal = {OPTIMIZATION METHODS \& SOFTWARE},
Year = {1998},
Volume = {10},
Number = {1},
Pages = {49-69},
Abstract = {We describe regularization tools for training large-scale artificial
   feed-forward neural networks. We propose algorithms that explicitly use
   a sequence of Tikhonov regularized nonlinear least squares problems. For
   large-scare problems, methods using new special purpose automatic
   differentiation are used in a conjugate gradient method for computing a
   truncated Gauss-Newton search direction. The algorithms developed
   utilize the structure of the problem in different ways and perform much
   better than a Polak-Ribiere based method. All algorithms are tested
   using benchmark problems and guidelines by Lutz Prechelt in the Probenl
   package. All software is written in Matlab and gathered in a toolbox.},
DOI = {10.1080/10556789808805701},
ISSN = {1055-6788},
ORCID-Numbers = {gulliksson, marten/0000-0003-0332-2315},
Unique-ID = {WOS:000077802900003},
}

@incollection{ WOS:000082479300001,
Author = {Knoop, J},
Title = {Optimal interprocedural program optimization - A new framework and its
   application - Preface},
Booktitle = {OPTIMAL INTERPROCEDURAL PROGRAM OPTIMIZATION},
Series = {LECTURE NOTES IN COMPUTER SCIENCE},
Year = {1998},
Volume = {1428},
Pages = {1-12},
Abstract = {A new framework for interprocedural program optimization is presented,
   which is tailored for supporting the construction of interprocedural
   program optimizations satisfying formal optimality criteria in a
   cookbook style. The framework is unique in capturing programs with
   statically nested (mutually) recursive procedures, global and local
   variables, value, reference, and procedure parameters. In addition, it
   supports separate compilation and the construction of software libraries
   by dealing with external procedures and external variables. An important
   feature of the framework is that it strictly separates the specification
   of an optimizing transformation and the proof of its optimality from the
   specification of the data flow analysis algorithms computing the program
   properties involved in the definition of the transformation and the
   proofs of their precision. This structures and simplifies the
   development of optimal program transformations, and allows us to hide
   all details of the framework which are irrelevant for application. In
   particular, this holds for the higher order data flow analysis
   concerning formal procedure calls, which is organized as an independent
   preprocess. The power and flexibility of the framework is demonstrated
   by a practically relevant optimization: the computationally and lifetime
   optimal elimination of interprocedurally partially redundant
   computations in a program. As a side-effect this application reveals
   essential differences, which usually must be taken into account when
   extending intraprocedural optimizations interprocedurally. Concerning
   the application considered here, this means that computationally and
   lifetime optimal results are interprocedurally in general impossible.
   However, we propose a natural constraint which is sufficient to meet
   these optimality criteria for a large class of programs. Under this
   constraint the algorithm developed is not only unique in satisfying both
   optimality criteria, it is also more efficient than its heuristic
   predecessors.},
ISSN = {0302-9743},
Unique-ID = {WOS:000082479300001},
}
