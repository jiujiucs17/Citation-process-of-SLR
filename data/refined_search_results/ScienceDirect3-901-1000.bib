@article{SHAH1983141,
title = {MATRIXX: A Data Analysis, System Identification, Control Design and Simulation Program},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {19},
pages = {141-146},
year = {1983},
note = {2nd IFAC Symposium on computer Aided Design of Multivariable Technological Systems, West Lafayette, USA, 15-17 September 1982},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)61679-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017616793},
author = {S. Shah and R. Walker and C. Gregory},
keywords = {Computer-aided-design, control design, system identification, numerical algorithms, simulation},
abstract = {MATRIXX is an interactive software system to perform a complete cycle of steps starting from data analysis to system identification, control design, simulation and evaluation. It is built on a user-friendly interpreter incorporating powerful matrix operations. The package offers four major features: 1) powerful interpreter, simple command structure, good graphics capability with most “bookkeeping” chores handled by the software, 2) state-of-the-art numerical algorithms, which allow solutions to high order problems, 3) user transparent file management with uniform and consistent format, and 4) efficient implementation with a stack to require minimum memory and computation resources.}
}
@article{BUCHNER198591,
title = {A Hierarchical Graphics Interface for Control System Programming},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {13},
pages = {91-92},
year = {1985},
note = {13th IFAC Workshop on Real Time Programming 1985, West Lafayette, IN, USA, 7-8 October 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/B978-0-08-033450-9.50019-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080334509500191},
author = {M. Buchner},
keywords = {Computer-aided system design, computer control, computer graphics, control system synthesis, programming languages},
abstract = {There are many control system analysis and design software packages which are currently commercially available. While a large number of these use graphics for the presentation of data, i.e., the results of the simulation and/or computer analysis, to an analyst, the input of the model structure is normally handled through a text definition with strict syntax rules. A graphics interface will be presented for a control system programming language which allows the analyst or designer to input, in an interactive manner, a control system as a graphical block diagram. The interface permits a hierarchical structure to be defined for the control system thereby providing a useful framework for large complex system control problems. Further, the interface provides a standard form to interact with a variety of software analysis systems. The graphics interface is implemented with Pascal as the base language and ACM CORE Standard routines for all graphics interactions on an IBM XT.}
}
@article{POP2021111301,
title = {External function for GOTHIC code to estimate critical heat flux conditions for in-vessel retention assessment},
journal = {Nuclear Engineering and Design},
volume = {380},
pages = {111301},
year = {2021},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2021.111301},
url = {https://www.sciencedirect.com/science/article/pii/S0029549321002533},
author = {A. Pop and A. Petruzzi and W. Giannotti},
keywords = {GOTHIC, Critical heat flux, In vessel retention},
abstract = {GOTHIC is an integrated, general purpose thermal hydraulic software package for design, licensing, safety and operating analysis of Nuclear Power Plant containments, confinement buildings and system components. It bridges the gap between the lumped parameter codes frequently used for containment analysis (such as MELCOR, MAAP, COCOSYS, ASTEC codes) and Computational Fluid Dynamics codes. Within a single model, GOTHIC can include regions treated in conventional lumped parameter mode and regions with three-dimensional flows in complex geometries. The heat transfer correlations built into GOTHIC cover the portion of the boiling curve which spans single phase heat transfer up to pre-Critical Heat Flux (CHF) heat transfer. The implemented boiling curve is truncated to exclude post-CHF heat transfer as it has not been adequately verified and was considered by the developers to have little application in general containment analysis. As such, one area that the code is not currently qualified for is post-CHF heat transfer, which could occur for example in the case of In-Vessel Corium Retention, where cooling water enters in contact with the high temperature of the Reactor Pressure Vessel wall. The presented research focuses on creating an external subroutine that solves this limitation, enabling the GOTHIC code to account for CHF phenomena. The modeling of CHF would be very useful in order to enable the code to simulate the external Reactor Pressure Vessel (RPV) Cooling , as well as other types of severe accidents or analyses where post-CHF simulation is required. Several subroutine function switches were implemented in order to facilitate its usage for different types of heat structures and correlations. The subroutine determines the CHF values based on either: the 2006 Groeneveld Look-up Tables, Lookup Tables for Large Diameter Vertical Tubes, Look-up Tables for Large Diameter Horizontal Tubes, or the correlation used by the MELCOR code for critical heat flux situations. It shall be noted that the developed subroutine and its implementation were performed without the need to have access to the GOTHIC source code. In a previous paper, the GOTHIC code was used to perform a containment safety analysis for the Atucha-I NPP (CNA-I) for an in-vessel retention type of analysis. Highly conservative vapour generating boundary conditions were used in order to simulate the boiling between the cavity water and the RPV surface, and to bypass the GOTHIC limitation. The newly developed subroutine was used for the analysis of two postulated Atucha-I in-vessel retention scenarios, a Large Break Loss Of Coolant Accident (LBLOCA) and a Station Black-Out (SBO), with the simulation of heat transfer between RPV and cavity water. Specifically for in-vessel retention situations, a separate user selectable option for the subroutine was developed, in which the Critical Heat Flux is determined based on experiments performed at the ULPU facility from the University of California, Santa Barbara, USA, which were combined with the 2006 Groeneveld Look-up Tables primarily in order to have a pressure dependence. This was performed because for the Atucha-I analysis, the pressure was higher than in the ULPU facility.}
}
@article{LINDENBECK2002841,
title = {TRICUT: a program to clip triangle meshes using the rapid and triangle libraries and the visualization toolkit},
journal = {Computers & Geosciences},
volume = {28},
number = {7},
pages = {841-850},
year = {2002},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(01)00110-8},
url = {https://www.sciencedirect.com/science/article/pii/S0098300401001108},
author = {C.H. Lindenbeck and H.D. Ebert and H. Ulmer and L. {Pallozzi Lavorante} and R. Pflug},
keywords = {Visualization, Surface modeling, Intersection computation, Surface cutting, Delaunay triangulation},
abstract = {An efficient technique to cut polygonal meshes as a step in the geometric modeling of topographic and geological data has been developed. In boundary represented models of outcropping strata and faulted horizons polygonal meshes often intersect each other. TRICUT determines the line of intersection and re-triangulates the area of contact. Along this line the mesh is split in two or more parts which can be selected for removal. The user interaction takes place in the 3D-model space. The intersection, selection and removal are under graphic control. The visualization of outcropping geological structures in digital terrain models is improved by determining intersections against a slightly shifted terrain model. Thus, the outcrop line becomes a surface which overlaps the terrain in its initial position. The area of this overlapping surface changes with respect to the strike and dip of the structure, the morphology and the offset. Some applications of TRICUT on different real datasets are shown. TRICUT is implemented in C++ using the Visualization Toolkit in conjunction with the RAPID and TRIANGLE libraries. The program runs under LINUX and UNIX using the MESA OpenGL library. This work gives an example of solving a complex 3D geometric problem by integrating available robust public domain software.}
}
@article{CHANG1996201,
title = {The development of an environmental Decision Support System for municipal solid waste management},
journal = {Computers, Environment and Urban Systems},
volume = {20},
number = {3},
pages = {201-212},
year = {1996},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(96)00015-4},
url = {https://www.sciencedirect.com/science/article/pii/S0198971596000154},
author = {Ni-Bin Chang and S.F. Wang},
abstract = {This paper presents the development of an innovative Decision Support System (DSS) as a graphical, interactive, problem-structuring tool for the management planning of solid waste collection, recycling and incineration systems. Emphasis has been placed upon integration techniques through the use of several powerful modules in the software package SAS® to constitute the essential functions in management planning. The object-oriented characteristics are specifically described in the framework of the proposed DSS. The practical implementation of this program for the city of Tainan in Taiwan shows the potential of such applications.}
}
@article{ULRICH1995177,
title = {MASAS—A user-friendly simulation tool for modeling the fate of anthropogenic substances in lakes},
journal = {Environmental Software},
volume = {10},
number = {3},
pages = {177-198},
year = {1995},
issn = {0266-9838},
doi = {https://doi.org/10.1016/0266-9838(95)00012-A},
url = {https://www.sciencedirect.com/science/article/pii/026698389500012A},
author = {Markus M. Ulrich and Dieter M. Imboden and RenéP. Schwarzenbach},
keywords = {dynamic lake model, anthropogenic pollution, organic chemical, environment, disulfoton, design concepts, user-friendly software},
abstract = {MASAS is a computer simulation tool to investigate anthropogenic organic compounds in lakes. Models included are based on a dynamic one-dimensional vertical lake model describing the time-dependent concentration of a compound in the water column and in the sediment. Provision of data is ensured by library files. Interactive specification of transport and transformation processes yields models of different complexity, suitable for the initial assessment of chemicals and for chemodynamic studies. A hypothetical spill of an insecticide (disulfoton) serves as an illustration. Aspects relevant to the development of user-friendly software in the environmental sciences are discussed. User-friendliness is found to depend more on program concepts than on a particular user interface. The management of data sets on lakes and compounds, and the implementation of transport and transformation processes in MASAS exemplify the significance of modularization, program data structures, the use of metaphors, and the distinction between automatic and user-controlled program functions.}
}
@article{LEGGETT200161,
title = {CAPLib—a ‘thin layer’ message passing library to support computational mechanics codes on distributed memory parallel systems},
journal = {Advances in Engineering Software},
volume = {32},
number = {1},
pages = {61-83},
year = {2001},
issn = {0965-9978},
doi = {https://doi.org/10.1016/S0965-9978(00)00056-9},
url = {https://www.sciencedirect.com/science/article/pii/S0965997800000569},
author = {P.F Leggett and S.P Johnson and M Cross},
keywords = {Distributed memory parallel systems, Computational mechanics software, CAPTools},
abstract = {The Computer Aided Parallelisation Tools (CAPTools) [Ierotheou, C, Johnson SP, Cross M, Leggett PF, Computer aided parallelisation tools (CAPTools)—conceptual overview and performance on the parallelisation of structured mesh codes, Parallel Computing, 1996;22:163–195] is a set of interactive tools aimed to provide automatic parallelisation of serial FORTRAN Computational Mechanics (CM) programs. CAPTools analyses the user's serial code and then through stages of array partitioning, mask and communication calculation, generates parallel SPMD (Single Program Multiple Data) messages passing FORTRAN. The parallel code generated by CAPTools contains calls to a collection of routines that form the CAPTools communications Library (CAPLib). The library provides a portable layer and user friendly abstraction over the underlying parallel environment. CAPLib contains optimised message passing routines for data exchange between parallel processes and other utility routines for parallel execution control, initialisation and debugging. By compiling and linking with different implementations of the library, the user is able to run on many different parallel environments. Even with today's parallel systems the concept of a single version of a parallel application code is more of an aspiration than a reality. However for CM codes the data partitioning SPMD paradigm requires a relatively small set of message-passing communication calls. This set can be implemented as an intermediate ‘thin layer’ library of message-passing calls that enables the parallel code (especially that generated automatically by a parallelisation tool such as CAPTools) to be as generic as possible. CAPLib is just such a ‘thin layer’ message passing library that supports parallel CM codes, by mapping generic calls onto machine specific libraries (such as CRAY SHMEM) and portable general purpose libraries (such as PVM an MPI). This paper describe CAPLib together with its three perceived advantages over other routes:•as a high level abstraction, it is both easy to understand (especially when generated automatically by tools) and to implement by hand, for the CM community (who are not generally parallel computing specialists);•the one parallel version of the application code is truly generic and portable;•the parallel application can readily utilise whatever message passing libraries on a given machine yield optimum performance.}
}
@article{SHAO20012,
title = {Invited Talk: Towards a Principled Multi-Language Infrastructure},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {59},
number = {1},
pages = {2},
year = {2001},
note = {BABEL'01, First International Workshop on Multi-Language Infrastructure and Interoperability (Satellite Event of PLI 2001)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/S1571-0661(05)80449-9},
url = {https://www.sciencedirect.com/science/article/pii/S1571066105804499},
author = {Zhong Shao},
abstract = {Sun's Java architecture introduced a safe virtual machine (VM) in which an ensemble of software components developed independently could smoothly interoperate. The goal of Microsoft's Common Language Runtime (CLR) is to generalize this approach and allow components in many source languages to interoperate safely. CLR supports flexible interoperation by compiling various source languages into a common intermediate language and by using a unified type system. However, the type system in CLR (and Java VM) enforces only conventional type safety in an object-oriented system. Therefore, higher-level specifications (e.g., resource bounds, generalized access control, formal software protocols) cannot be enforced. Because conventional type systems are too inflexible for real applications, developers often bypass the type system, producing code that steps outside the managed part of the VM; such components cannot be verified. At Yale we have been developing typed common intermediate languages (named FLINT) that can support safely not only the standard object-oriented model, but also higher-order generic (polymorphic) programming and Java-style reflection (introspection). Unlike CLR, our type system is independent of any particular programming model, yet it is capable of expressing all valid propositions and proofs in higher-order predicate logic (so it can be used to capture and verify advanced program properties). The rich type system of FLINT makes it possible to typecheck both compiler intermediate code and low level machine code; this allows typechecking to take place at any phase of compilation, even after optimizations and register allocation. It also leads to a smaller and more extensible VM because low-level native routines that would otherwise be in VM can now be verified and moved into a certified library. This talk describes our vision of the FLINT system, outline our approach to its design, and survey the technologies that can be brought to support its implementation.}
}
@article{DONGARRA1994523,
title = {Scalability Issues Affecting the Design of a Dense Linear Algebra Library},
journal = {Journal of Parallel and Distributed Computing},
volume = {22},
number = {3},
pages = {523-537},
year = {1994},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1994.1108},
url = {https://www.sciencedirect.com/science/article/pii/S0743731584711087},
author = {J.J. Dongarra and R.A. Vandegeijn and D.W. Walker},
abstract = {This paper discusses the scalability of Cholesky, LU, and QR factorization routines on MIMD distributed memory concurrent computers. These routines form part of the ScaLAPACK mathematical software library that extends the widely used LAPACK library to run efficiently on scalable concurrent computers. To ensure good scalability and performance, the ScaLAPACK routines are based on block-partitioned algorithms that reduce the frequency of data movement between different levels of the memory hierarchy, and particularly between processors. The block cyclic data distribution, that is used in all three factorization algorithms, is described. An outline of the sequential and parallel block-partitioned algorithms is given. Approximate models of algorithms′ performance are presented to indicate which factors in the design of the algorithm have an impact upon scalability. These models are compared with timings results on a 128-node Intel iPSC/860 hypercube. It is shown that the routines are highly scalable on this machine for problems that occupy more than about 25% of the memory on each processor, and that the measured timings are consistent with the performance model. The contribution of this paper goes beyond reporting our experience: our implementations are available in the public domain.}
}
@article{ABERCROMBIE200255,
title = {jContractor: Bytecode Instrumentation Techniques for Implementing Design by Contract in Java},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {70},
number = {4},
pages = {55-79},
year = {2002},
note = {RV'02, Runtime Verification 2002 (FLoC Satellite Event)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/S1571-0661(04)80577-2},
url = {https://www.sciencedirect.com/science/article/pii/S1571066104805772},
author = {Parker Abercrombie and Murat Karaorman},
abstract = {Design by Contract is a software engineering practice that allows semantic information to be added to a class or interface to precisely specify the conditions that are required for its correct operation. The basic constructs of Design by Contract are method preconditions and postconditions, and class invariants. This paper presents a detailed design and implementation overview of jContractor, a freely available tool that allows programmers to write “contracts” as standard Java methods following an intuitive naming convention. Preconditions, postconditions, and invariants can be associated with, or inherited by, any class or interface. jContractor performs on-the-fly bytecode instrumentation to detect violation of the contract specification during a program's execution. jContractor's bytecode engineering technique allows it to specify and check contracts even when source code is not available. jContractor is a pure Java library providing a rich set of syntactic constructs for expressing contracts without extending the Java language or runtime environment. These constructs include support for predicate logic expressions, and referencing entry values of attributes and return values of methods. Fine grain control over the level of monitoring is possible at runtime. Since contract methods are allowed to use unconstrained Java expressions, in addition to runtime verification they can perform additional runtime monitoring, logging, and testing.}
}
@article{RIVA1996953,
title = {LispWeb: A specialized HTTP server for distributed AI applications},
journal = {Computer Networks and ISDN Systems},
volume = {28},
number = {7},
pages = {953-961},
year = {1996},
note = {Proceedings of the Fifth International World Wide Web Conference 6-10 May 1996},
issn = {0169-7552},
doi = {https://doi.org/10.1016/0169-7552(96)00062-1},
url = {https://www.sciencedirect.com/science/article/pii/0169755296000621},
author = {Alberto Riva and Marco Ramoni},
keywords = {Design techniques for Web applications, User and application interfaces, Authoring environments},
abstract = {We describe the design and the implementation of LispWeb, a specialized HTTP server, written in Common Lisp, able to deliver Distributed Artificial Intelligence (DAI) applications over the World Wide Web (WWW). In addition to implementing the standard HTTP protocol, the LispWeb server offers a library of high-level Lisp functions to dynamically generate HTML pages, a facility for creating Graphical User Interfaces on the WWW through dynamically generated image maps, and a server-to-server communication method (STSP). LispWeb is intended to act as the front-end of a network of intelligent agents that communicate with each other using an extension to the HTTP protocol. The dynamic generation of HTML pages allows complex AI applications to be delivered to end-users without the need for specialized hardware and software support, and using a simple and homogeneous interface model.}
}
@incollection{MICHAELJ20091,
title = {Chapter 1 - Introduction},
editor = {Donahoo {Michael J.} and Calvert {Kenneth L.}},
booktitle = {TCP/IP Sockets in C (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-10},
year = {2009},
isbn = {978-0-12-374540-8},
doi = {https://doi.org/10.1016/B978-0-12-374540-8.00003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123745408000031},
author = {Donahoo {Michael J.} and Calvert {Kenneth L.}},
abstract = {Publisher Summary
For a long time, C was the language of choice for implementing network communication software. The goal of this chapter is to start on the road to understanding the answer to the question that how does a program communicate with another program over a network, in the context of the C programming language. The application programming interface (API) known as “Sockets” was first developed in C. A computer network consists of machines interconnected by communication channels known as hostsand routers. Hosts are computers that run applications such as your Web browser. Implementing a useful network requires solving a large number of different problems. To keep things manageable and modular, different protocols are designed to solve different sets of problems. TCP/IP is one such collection of solutions, sometimes called a protocol suite. A socket is an abstraction through which an application may send and receive data, in much the same way as an open-file handle allows an application to read and write data to stable storage. A socket allows an application to plug in to the network and communicate with other applications that are plugged in to the same network. Different types of sockets correspond to different underlying protocol suites and different stacks of protocols within a suite. This book deals only with the TCP/IP protocol suite. The main types of sockets in TCP/IP today are stream sockets and datagram sockets. Stream sockets use TCP as the end-to-end protocol and thus provide a reliable byte-stream service}
}
@article{VASSILIADIS2005492,
title = {A generic and customizable framework for the design of ETL scenarios},
journal = {Information Systems},
volume = {30},
number = {7},
pages = {492-525},
year = {2005},
note = {The 15th International Conference on Advanced Information Systems Engineering (CAiSE 2003)},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2004.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437904000985},
author = {Panos Vassiliadis and Alkis Simitsis and Panos Georgantas and Manolis Terrovitis and Spiros Skiadopoulos},
keywords = {Data warehousing, ETL},
abstract = {Extraction–transformation–loading (ETL) tools are pieces of software responsible for the extraction of data from several sources, their cleansing, customization and insertion into a data warehouse. In this paper, we delve into the logical design of ETL scenarios and provide a generic and customizable framework in order to support the DW designer in his task. First, we present a metamodel particularly customized for the definition of ETL activities. We follow a workflow-like approach, where the output of a certain activity can either be stored persistently or passed to a subsequent activity. Also, we employ a declarative database programming language, LDL, to define the semantics of each activity. The metamodel is generic enough to capture any possible ETL activity. Nevertheless, in the pursuit of higher reusability and flexibility, we specialize the set of our generic metamodel constructs with a palette of frequently used ETL activities, which we call templates. Moreover, in order to achieve a uniform extensibility mechanism for this library of built-ins, we have to deal with specific language issues. Therefore, we also discuss the mechanics of template instantiation to concrete activities. The design concepts that we introduce have been implemented in a tool, arktos ii, which is also presented.}
}
@article{EBNAHAI2019100,
title = {Modeling and simulation of Ultrasonic Guided Waves propagation in the fluid-structure domain by a monolithic approach},
journal = {Journal of Fluids and Structures},
volume = {88},
pages = {100-121},
year = {2019},
issn = {0889-9746},
doi = {https://doi.org/10.1016/j.jfluidstructs.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0889974618302743},
author = {Bhuiyan Shameem Mahmood {Ebna Hai} and Markus Bause and Paul Kuberry},
keywords = {Multiphysics, Coupled problem, Fluid-structure interaction, Ultrasonic guided waves, Wave propagation, Galerkin finite element method, Arbitrary Lagrangian-Eulerian technique, Structural health monitoring system},
abstract = {The present study introduces the coupled multiphysics model as part of the structural health monitoring (SHM) system. In particular, the Ultrasonic Guided Waves (UGWs) propagation is tracked in order to identify the damage to the structure. For this purpose, a multiphysics mathematical model is proposed. The model constitutes a monolithically coupled system of acoustic and elastic wave equations (WpFSI problem) where the wave signal displacement measurements are analyzed as the UGWs propagates in the solid, fluid and their interface. The ultimate goal of this paper is to explore and develop the efficient numerical solution of the WpFSI problem using the finite element method. A detailed description of the modeling framework and conditions that facilitate the coupling is provided. To couple the displacement-based acoustic wave equations for the isothermal fluid with elastic wave equations for the Saint Venant-Kirchhoff material model, the present study uses a monolithic solution algorithm. In particular, at each time step, wave equations are transformed to a fixed reference configuration via the arbitrary Lagrangian-Eulerian (ALE) mapping and automatically adopt the boundary conditions from the previous time step. The implementation is accomplished via the finite element library deal.II (Goll et al., 2017) based software toolbox DOpElib (Bangerth et al., 2007) which provides modularized higher level algorithms. Beyond SHM systems, the model is relevant for problems from biomechanics and biomedicine, vibromechanics, poroelasticity as well as subsurface and porous media flow. The model developed here serves as a first step towards the on-line SHM system modeling, where structural dynamics are accounted for.}
}
@article{CHANG20101360,
title = {Air cooling for a large-scale motor},
journal = {Applied Thermal Engineering},
volume = {30},
number = {11},
pages = {1360-1368},
year = {2010},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2010.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S1359431110000827},
author = {Chih-Chung Chang and Yu-Fu Kuo and Jung-Chang Wang and Sih-Li Chen},
keywords = {Thermal performance, Large-scale motor, Fan performance},
abstract = {This article experimentally and numerically investigates the thermal performance of a large-scale motor with a capacity of 2350 kW. The large-scale motor consists of a centrifugal fan, two axial fans, a shaft, a stator, a rotor and a heat exchanger with 637 cooling tubes. The test rigs are set up to measure the performance of the fans and the temperature distributions of the motor. The models of the fan and motor have been implemented in a Fluent software package to predict the flow and temperature fields inside the motor. The calculated results show good agreement with the measured data. In order to improve the motor thermal performance, several methods have been adopted, which are aiming to enhance the fan performance by changing the geometry, to redesign a new heat exchanger with guide vanes, and to optimize the distance between the axial fans. The modified design can decrease the temperature rise by 6 °C in both the stator and rotor.}
}
@article{REZANEJAD20191241,
title = {Hydrodynamic analysis of an oscillating water column wave energy converter in the stepped bottom condition using CFD},
journal = {Renewable Energy},
volume = {135},
pages = {1241-1259},
year = {2019},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2018.09.034},
url = {https://www.sciencedirect.com/science/article/pii/S0960148118310978},
author = {K. Rezanejad and J.F.M. Gadelho and C. {Guedes Soares}},
keywords = {Oscillating water column, CFD, Hydrodynamic performance, Reflection coefficient, Energy dissipation, Stepped sea bottom},
abstract = {The primary efficiency of the Oscillating Water Column (OWC) Wave Energy Converter (WEC) device in the stepped sea bottom condition is investigated using both experimental and numerical approaches. Wave flume tests were undertaken to investigate the hydrodynamic behaviour of the device in regular waves. A 2D numerical model was developed in the open source computational fluid dynamics (CFD) software package OpenFOAM implementing the fully non-linear Reynolds Averaged Navier-Stokes (RANS) equations to simulate the wave power absorption and wave structure interactions. The numerical results have been validated against the experimental data. The influence of the wave characteristic as well as damping of the power take-off unit on the performance of the device, wave reflection coefficient and the energy dissipation rate is evaluated using the results obtained from the numerical simulations. Furthermore, as was proved in previous studies that the application of the stepped sea bottom condition might increase the efficiency of the OWC devices, the CFD simulation results have been implemented to present and discuss about the power absorption mechanism as well as the flow pattern characteristics in the vicinity area of the step.}
}
@article{BRESNAHAN2020100087,
title = {Equipping smart coasts with marine water quality IoT sensors},
journal = {Results in Engineering},
volume = {5},
pages = {100087},
year = {2020},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2019.100087},
url = {https://www.sciencedirect.com/science/article/pii/S2590123019300878},
author = {Philip J. Bresnahan and Taylor Wirth and Todd Martz and Kenisha Shipley and Vicky Rowley and Clarissa Anderson and Thomas Grimm},
keywords = {Water quality, Internet of things, Ocean acidification, Aquaculture},
abstract = {Ocean acidification, the decrease in seawater pH as a result of increasing carbon dioxide, has been shown to be an important driver of oyster mortality in West Coast shellfisheries [1]. Yet carbon chemistry is only sparsely measured, especially relative to its high variability in coastal ecosystems, due to the complexity and cost of appropriate sensors and their maintenance. Worse, data are rarely communicated in real time to water quality or aquacultural managers. In the Agua Hedionda Lagoon (AHL) in Carlsbad, CA, researchers from Scripps Institution of Oceanography and industry representatives from the Carlsbad Aquafarm have come together through a NOAA-facilitated project to alleviate this data shortage using a combination of cutting-edge research technology alongside off-the-shelf and easy-to-implement IoT communications packages.}
}
@article{CROCKER199412,
title = {An introduction to MVS integrity concerns},
journal = {Network Security},
volume = {1994},
number = {12},
pages = {12-16},
year = {1994},
issn = {1353-4858},
doi = {https://doi.org/10.1016/1353-4858(94)90025-6},
url = {https://www.sciencedirect.com/science/article/pii/1353485894900256},
author = {Norman Crocker},
abstract = {Whilst a large proportion of the IT business is busy discussing Client/Server and Downsizing, the majority of the worlds large businesses continue to process their every day high-value corporate computing on a mainframe. IBM's flagship ‘MVS’ operating system is used extensively in this arena. This system is, architecturally speaking, an old system, predating the 80's and 90's concerns for information security. In order to relieve concerns regarding the ability to trust MVS a “Statement of Integrity” for the system was issued by IBM in 1981. While providing a business with guarantees, this statement clearly puts the onus for managing the integrity of a given MVS implementation firmly in the hands of the site management. This article introduces the basic concepts on which MVS Integrity is built, explains the dependence of all MVS access control software packages on MVS integrity and discusses some of the most important areas to manage.}
}
@article{WANG1999291,
title = {Implementation and testing of an advanced relay auto-tuner},
journal = {Journal of Process Control},
volume = {9},
number = {4},
pages = {291-300},
year = {1999},
issn = {0959-1524},
doi = {https://doi.org/10.1016/S0959-1524(99)00003-7},
url = {https://www.sciencedirect.com/science/article/pii/S0959152499000037},
author = {Qing-Guo Wang and Chang-Chieh Hang and Shan-An Zhu and Qiang Bi},
keywords = {Relay feedback, Auto-tuning, Frequency responses, Identification, Least squares method},
abstract = {In this paper, an advanced relay feedback auto-tuner for single-loop controllers is presented. The tuner combines an improved relay feedback process frequency response identification method and a multiple-point frequency response fitting controller design method. In process identification, multiple accurate points on the frequency response curve of a process are obtained in one single relay feedback test. A controller is then designed based on these estimated points using frequency response fitting methodology. The auto-tuner was implemented on a personal computer with AD/DA card. The software was developed on a commercial real-time control platform. The algorithms were written in C++ and compiled as a Dynamic Link Library. The auto-tuner has been tested on a dual-process simulator which can be easily configured as various typical processes, and on pilot coupled-tank and heat exchanger. The testing results show that the tuner gives significant performance improvement over the standard relay auto-tuner.}
}
@article{LALEHPOUR2017149,
title = {Developing skin model in coordinate metrology using a finite element method},
journal = {Measurement},
volume = {109},
pages = {149-159},
year = {2017},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2017.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S0263224117303445},
author = {Amirali Lalehpour and Ahmad Barari},
keywords = {Skin model, Deviation zone evaluation, Coordinate metrology, Finite element analysis, Poisson equation},
abstract = {Coordinate metrology processes typically include the measurement of a limited number of discrete 3D points from the measured surface to estimate the geometric deviation zone of the entire surface from an ideal geometry. Three main computational tasks are required in coordinate metrology of manufactured surfaces including Point Measurement Planning (PMP), Substitute Geometry Estimation (SGE), and Deviation Zone Evaluation (DZE). The issue of uncertainties in the results of any of these three tasks potentially jeopardizes the accuracy of the entire inspection process. It is also shown recently that interactive closed-loops between DZE and PMP, or SGE will increase the efficiency, and reduce the sources of uncertainties in the computational processes. This paper presents a methodology to complete a comprehensive DZE for the entire measured surface using the geometric deviations of a limited number of measured discrete sample points. The result of a detailed DZE is a skin model of the measured workpiece which represents the geometric deviation at any point on the measured surface from the ideal geometry. The developed skin model is definitely needed when the inspection results are used for all possible downstream processes, planning for precise finishing operations, manufacturing error compensation, closed-loop of manufacturing and inspection, dynamic process control, or accurate verification of design tolerances. The developed methodology in this paper uses a finite element approach to solve the differential equation for detailed DZE of the entire surface estimating the geometric deviations for any location that is not measured on the surface. The methodology is fully implemented and employed in various case studies on several industrial parts. The resulting skin models are examined and successful results are achieved. The developed methodology can be easily adopted by commonly used coordinate metrology computational software tools.}
}
@article{HALLE2018117,
title = {Decentralized enforcement of document lifecycle constraints},
journal = {Information Systems},
volume = {74},
pages = {117-135},
year = {2018},
note = {Special Issue on papers presented in the 20th IEEE International Enterprise Distributed Object Computing1 Conference, EDOC 2016},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306437916306494},
author = {Sylvain Hallé and Raphaël Khoury and Quentin Betti and Antoine El-Hokayem and Yliès Falcone},
keywords = {Business artifact, Business process, Document lifecycle, Lifecycle policy, UML statechart, Business Process Modelling Language, Linear Temporal Logic, Finite-state automata, Trace validation, Policy enforcement, Public-key encryption, Hash functions, Tamper-proof history, Confidentiality, Distributed enforcement, Runtime monitoring, Smart cards, Portable Document Format, Artichoke-X},
abstract = {Artifact-centric workflows describe possible executions of a business process through constraints expressed from the point of view of the documents exchanged between principals. A sequence of manipulations is deemed valid as long as every document in the workflow follows its prescribed lifecycle at all steps of the process. So far, establishing that a given workflow complies with artifact lifecycles has mostly been done through static verification, or by assuming a centralized access to all artifacts where these constraints can be monitored and enforced. We present in this paper an alternate method of enforcing document lifecycles that requires neither static verification nor single-point access. Rather, the document itself is designed to carry fragments of its history, protected from tampering using hashing and public-key encryption. Any principal involved in the process can verify at any time that the history of a document complies with a given lifecycle. Moreover, the proposed system also enforces access permissions: not all actions are visible to all principals, and one can only modify and verify what one is allowed to observe. These concepts have been implemented in a software library called Artichoke, and empirically tested for performance and scalability.}
}
@article{SCHMID1979251,
title = {KEDDC, A General Purpose Cad Software System for Application in Control Engineering},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {3},
pages = {251-255},
year = {1979},
note = {2nd IFAC/IFIP Symposium on Software for Computer Control, Prague, Czechoslovakia, 11-15 June},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65810-5},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017658105},
author = {Chr. Schmid and H. Unbehauen},
keywords = {Computer-aided system design, computer software, identification, control system synthesis, direct digital control, adaptive control},
abstract = {KEDDC is a software package especially designed for process computers to cover a wide range of control engineering tasks. The system involves dynamical system identification, controller design and testing by digital simulation using a broad variety of modern as well as classical methods. Finally the KEDDC system includes implementation and on-line realization of control algorithms, so that any control scheme designed with this software package can be applied and tested on real processes.}
}
@article{BATRA20189,
title = {A versatile and fully instrumented test station for piezoelectric energy harvesters},
journal = {Measurement},
volume = {114},
pages = {9-15},
year = {2018},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2017.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0263224117305456},
author = {A.K. Batra and J.R. Currie and A.A. Alomari and M.D. Aggarwal and C.R. Bowen},
keywords = {Piezoelectric energy harvesting, LabVIEW, Cantilever},
abstract = {This paper describes the implementation of LabVIEW software to control instruments and acquire data from a piezoelectric energy harvesting test station which is based on a cantilever structure. The experiment is run in the Clean Energy Laboratory on the Ambient Energy Harvester Test Station. A digital multimeter, a programmable resistance selector, an arbitrary waveform generator, a shaker table, an accelerometer and a laser displacement sensor are used to control and acquire data in terms of harvested energy as a function of vibration frequency and load resistance. LabVIEW software is used to control the test station which makes near real-time data measurements, displays waveforms on a PC screen, and stores data for later analysis. Acquired waveforms are presented in terms of frequency versus voltage of the vibrating cantilever at preselected ranges of load resistances in terms of either AC or DC voltages. The vibration of the cantilever beam is measured with an accelerometer and beam movement is measured with a laser displacement meter. Test results are stored in a comma separated variable text file which can be imported into any data analysis software package. All experiments are performed on an isolated optical bench to avoid interference from mechanical noise that may exist in the surrounding environment. The system provides an integrated approach to characterize key performance indicators for energy harvesting materials and devices.}
}
@article{ZHOU2017158,
title = {NURBS-enhanced boundary element method based on independent geometry and field approximation for 2D potential problems},
journal = {Engineering Analysis with Boundary Elements},
volume = {83},
pages = {158-166},
year = {2017},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2017.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0955799716304271},
author = {Wei Zhou and Biao Liu and Qiao Wang and Yonggang Cheng and Gang Ma and Xiaolin Chang and Xudong Chen},
keywords = {Non-uniform rational B-spline, Isogeometric analysis, Boundary element method, Potential problems},
abstract = {Non-uniform rational B-spline (NURBS) in Isogeometric analysis (IGA) is coupled with the boundary element method (BEM) for 2D potential problems in this paper. The geometry and field are usually approximated by the same basis functions in IGA, such as the B-spline or the NURBS basis functions. In the proposed method, these two kinds of approximation are performed independently, i.e. the geometry is reproduced by the NURBS basis functions while the field is approximated by the traditional Lagrangian basis functions which are used in the conventional BEM. The proposed method has the advantage that the geometry can be reproduced exactly at all stages in IGA methods. Actually, one can use the computer aided design (CAD) software or NURBS library to perform the operations related to the geometry. The field approximation is performed in parameter space and separated from the geometry. Thus, it can be implemented easily as the conventional BEM since most algorithms for BEM can be applied directly, such as the methods for treatment of the singular integrals, and the boundary conditions can be imposed directly. Numerical examples have demonstrated the accuracy of the proposed method.}
}
@article{KRIZANC1999159,
title = {Bulk synchronous parallel: practical experience with a model for parallel computing},
journal = {Parallel Computing},
volume = {25},
number = {2},
pages = {159-181},
year = {1999},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(98)00106-9},
url = {https://www.sciencedirect.com/science/article/pii/S0167819198001069},
author = {Danny Krizanc and Anton Saarimaki},
keywords = {Bulk Synchronous Parallel (BSP) model, Bridging model, Parallel sorting algorithm, Prediction, Performance, Portability},
abstract = {Valiant proposed the Bulk Synchronous Parallel (BSP) model as a possible model for parallel computing. He refers to BSP as a “bridging” model, being applicable to both system and algorithm design. The model allows hardware and software design to proceed independently but ensures compatibility between parallel computers and parallel programs. This paper explores the practical use of BSP, focusing on the portability and predictability it offers, without incurring any significant loss in performance. A BSP algorithm for sorting proposed by Gerbessiotis and Valiant is implemented in a portable fashion on three different parallel computers, specifically an Intel iPSC/860, a Transtech Parastation and an Alex AVX Series 2. The program uses a standard library of communication functions designed and implemented for each machine to support the BSP model. The measured performance of the program is compared to the BSP predictions and to other sorting results on similar machines to provide evidence for the utility of the BSP model.}
}
@article{NAKATA200624,
title = {A parallel primal–dual interior-point method for semidefinite programs using positive definite matrix completion},
journal = {Parallel Computing},
volume = {32},
number = {1},
pages = {24-43},
year = {2006},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2005.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167819105001043},
author = {Kazuhide Nakata and Makoto Yamashita and Katsuki Fujisawa and Masakazu Kojima},
keywords = {Semidefinite program, Primal–dual interior-point method, Parallel computation, Positive definite matrix completion, Numerical experiment, PC cluster},
abstract = {A parallel computational method SDPARA-C is presented for SDPs (semidefinite programs). It combines two methods SDPARA and SDPA-C proposed by the authors who developed a software package SDPA. SDPARA is a parallel implementation of SDPA and it features parallel computation of the elements of the Schur complement equation system and a parallel Cholesky factorization of its coefficient matrix. SDPARA can effectively solve SDPs with a large number of equality constraints; however, it does not solve SDPs with a large scale matrix variable with similar effectiveness. SDPA-C is a primal–dual interior-point method using the positive definite matrix completion technique by Fukuda et al., and it performs effectively with SDPs with a large scale matrix variable, but not with a large number of equality constraints. SDPARA-C benefits from the strong performance of each of the two methods. Furthermore, SDPARA-C is designed to attain a high scalability by considering most of the expensive computations involved in the primal–dual interior-point method. Numerical experiments with the three parallel software packages SDPARA-C, SDPARA and PDSDP by Benson show that SDPARA-C efficiently solves SDPs with a large scale matrix variable as well as a large number of equality constraints with a small amount of memory.}
}
@article{HUGHES2010910,
title = {Integrating hydrology, hydraulics and ecological response into a flexible approach to the determination of environmental water requirements for rivers},
journal = {Environmental Modelling & Software},
volume = {25},
number = {8},
pages = {910-918},
year = {2010},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2010.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815210000496},
author = {D.A. Hughes and D. Louw},
keywords = {Environmental water requirements, Hydrology, Hydraulics, Ecosystem responses},
abstract = {There are many different approaches to environmental water requirement (EWR) determinations that have been reported in the international literature. Many of these focus on different aspects of the problem, while few of them consider all of the issues associated with the eventual implementation of EWR as part of integrated water management. It is also necessary to recognize that there is a wide range of different types and resolution of information available in different parts of the world. This paper presents an integrated framework that has been in use in South Africa for several years and which is based on integrating the impacts of changing flow regimes on different ecosystem response components through indices of ‘stress’ measured on a common scale (0–10). Software to support its implementation has been included as part of an existing hydrological modelling framework package that includes a GIS interface and database management procedures. The framework is flexible enough to be used with different approaches to analyzing ecosystem responses, ranging from complex hydraulic habitat assessments to the interpretation of expert opinion and therefore should be widely applicable. The framework can also be used to design a modified flow regime for a given set of ecological objectives, or it can be applied to assess scenarios of flow regimes based on a range of possible future water management options. The paper explains the approach, provides some illustrations of its application and discusses some of the issues associated with its more widespread use.}
}
@incollection{ALAZZAWI2019465,
title = {15 - Modeling, analysis, and testing of viscoelastic properties of shape memory polymer composites and a brief review of their space engineering applications},
editor = {Rui Miranda Guedes},
booktitle = {Creep and Fatigue in Polymer Matrix Composites (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {465-495},
year = {2019},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102601-4},
doi = {https://doi.org/10.1016/B978-0-08-102601-4.00015-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026014000151},
author = {Wessam {Al Azzawi} and Madhubhashitha Herath and Jayantha Epaarachchi},
keywords = {Shape memory polymers, Composites, Viscoelastic properties, Finite element analysis, Space engineering},
abstract = {Shape memory polymers (SMPs) are a novel class of active polymers that have a unique ability to undergo substantial shape deformation and subsequently recover their original shape when exposed to a particular external stimulus. The relatively low mechanical properties of SMPs are the main drawback that limits their use in engineering applications. However, when reinforced with fibers, SMPs become a smarter structural material that can be stimulated by an external stimulus. With the current increase of interest in space exploration, there has been a significant demand for lightweight and smart materials for space engineering applications. Shape memory polymer composites (SMPCs) are an obvious choice of materials for such applications. The implementation of SMPCs in space engineering applications requires rigorous optimization processes on the material properties and critical design procedures. The transformation of breakthrough SMPC technologies into real-life applications largely relies upon the robust analytical tools used for the initial design studies. In the past few years, many finite element models have been presented for the analysis of SMPCs. The vast majority of such tools required complicated user-defined material subroutines to be integrated with a standard finite element software package, which is incredibly difficult to implement. The subroutines are problem specific and troublesome for a wider range of potential engineering applications. This chapter presents a finite element simulation technique developed to model the thermomechanical behavior of fiber-reinforced SMPCs. Finite viscoelasticity was used to develop the technique, which is able to consider the nonlinear effect due to the large deformation. By using the experimental methods the proposed simulation technique has been validated. Furthermore, the competency of SMPCs for space engineering applications is discussed.}
}
@article{JARNDAL2010696,
title = {Large-signal model for AlGaN/GaN HEMTs suitable for RF switching-mode power amplifiers design},
journal = {Solid-State Electronics},
volume = {54},
number = {7},
pages = {696-700},
year = {2010},
issn = {0038-1101},
doi = {https://doi.org/10.1016/j.sse.2010.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0038110110001012},
author = {Anwar Jarndal and Pouya Aflaki and Louay Degachi and Ahmed Birafane and Ammar Kouki and Renato Negra and Fadhel M. Ghannouchi},
keywords = {GaN HEMT, Large-signal modeling, Table-based method, Switching-mode power amplifier},
abstract = {A table-based large-signal model for GaN HEMT transistor suitable for designing switching-mode power amplifiers (SMPAs) is presented along with its parameters extraction procedure. This model is relatively easy to construct and implement in CAD software since it requires only DC and S-parameter measurements. The intrinsic drain and gate currents are described as a table-based to provide an accurate prediction for the device in the switching states as well as the transition one. The modeling procedure was applied to a 4-W packaged GaN-on-Si HEMT. The model is validated by comparing its small- and large-signal simulation to measured data. The model has been employed for designing a switching-mode inverse class-F power amplifier. Very good agreement between the amplifier simulation and measurement shows the validity of the model.}
}
@article{GORDON1997359,
title = {Resource scheduling},
journal = {International Journal of Project Management},
volume = {15},
number = {6},
pages = {359-370},
year = {1997},
issn = {0263-7863},
doi = {https://doi.org/10.1016/S0263-7863(96)00090-7},
url = {https://www.sciencedirect.com/science/article/pii/S0263786396000907},
author = {J Gordon and A Tulip},
keywords = {resource scheduling, resource heuristic, resource aggregation, resource allocation, resource smoothing, resource levelling, project network analysis techniques, multi project scheduling},
abstract = {It has been apparent to the authors that a new generation of software writers is attempting to re-invent the wheel so far as the allocation of resources is concerned. This paper is intended to give an outline of the history and development of this field and a review of the basic techniques that have been implemented. In order to assist the general reader to realise some of the reasoning that went into the derivations of these schedulers it is necessary to give a brief history of the commercial packages which were the foundation of this topic. The second part of the paper describes the design of the basic procedures without the frills that accompanied them in the generally available products.}
}
@article{MARKOVSKY2014406,
title = {Recent progress on variable projection methods for structured low-rank approximation},
journal = {Signal Processing},
volume = {96},
pages = {406-419},
year = {2014},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2013.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0165168413003642},
author = {Ivan Markovsky},
keywords = {Low-rank approximation, Total least squares, Structured matrix, Variable projection, Missing data, System identification, Reproducible research, Literate programming, },
abstract = {Rank deficiency of a data matrix is equivalent to the existence of an exact linear model for the data. For the purpose of linear static modeling, the matrix is unstructured and the corresponding modeling problem is an approximation of the matrix by another matrix of a lower rank. In the context of linear time-invariant dynamic models, the appropriate data matrix is Hankel and the corresponding modeling problems becomes structured low-rank approximation. Low-rank approximation has applications in: system identification; signal processing, machine learning, and computer algebra, where different types of structure and constraints occur. This paper gives an overview of recent progress in efficient local optimization algorithms for solving weighted mosaic-Hankel structured low-rank approximation problems. In addition, the data matrix may have missing elements and elements may be specified as exact. The described algorithms are implemented in a publicly available software package. Their application to system identification, approximate common divisor, and data-driven simulation problems is described in this paper and is illustrated by reproducible simulation examples. As a data modeling paradigm the low-rank approximation setting is closely related to the behavioral approach in systems and control, total least squares, errors-in-variables modeling, principal component analysis, and rank minimization.}
}
@article{AHSON1985179,
title = {The use of FORTH language in process control},
journal = {Computer Languages},
volume = {10},
number = {3},
pages = {179-187},
year = {1985},
issn = {0096-0551},
doi = {https://doi.org/10.1016/0096-0551(85)90015-3},
url = {https://www.sciencedirect.com/science/article/pii/0096055185900153},
author = {S.I. Ahson and S.S. Lamba},
keywords = {Process control, Programming languages, Multiloop control systems, Microcomputers, Digital control},
abstract = {A tutorial discussion is given on the high-level programming language FORTH with particular reference to process control application. The advantages of FORTH are illustrated through a practical example. FORTH, originally developed for telescope control, contains many features designed to facilitate the construction of software for process control systems. In this paper, we describe some of these features and provide a design of a software package for a multiloop control system to illustrate its use. The software package, developed for Rockwell's AIM 65 microcomputer, contains programs written in FORTH for implementing feedback, feed-forward, ratio and cascade control functions. Experimental results pertaining to a furnace simulated on an EAL-580 hybrid computer are presented. A brief discussion is given on experience gained in the use of FORTH language for process control.}
}
@article{PINTO201559,
title = {A large-scale study on the usage of Java’s concurrent programming constructs},
journal = {Journal of Systems and Software},
volume = {106},
pages = {59-81},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.04.064},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215000849},
author = {Gustavo Pinto and Weslley Torres and Benito Fernandes and Fernando Castor and Roberto S.M. Barros},
keywords = {Java, Concurrency, Software evolution},
abstract = {In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages.}
}
@article{FENSCH19842675,
title = {Net-Based Software Design in Automation Technology},
journal = {IFAC Proceedings Volumes},
volume = {17},
number = {2},
pages = {2675-2680},
year = {1984},
note = {9th IFAC World Congress: A Bridge Between Control Science and Technology, Budapest, Hungary, 2-6 July 1984},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)61386-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017613867},
author = {S. Fensch and G. Meyer},
keywords = {Petri nets, computer-aided logic design, software tools, multiprocessing systems, transportation control},
abstract = {In this contribution it is shown that suitably modified Petri nets are an alternative to conventional means and methods of description in software design for solving problems in automation technology. In this development a new quality was reached particularly by explicit observation of the factor of causality and concurrency of the processes to be controlled. On the basis of a precisely defined example, the new design methodology with two classes of nets of differing levels of abstraction (Ablaufkontrollnetze, predicate/transition-nets) is introduced. The contribution deals with some aspects of analysis and also with the implementation of the design for single or multi computer systems (program package PSI).}
}
@article{IDA1988249,
title = {Software package for large-scale 0–1 linear programming on Unix system: GUB technique},
journal = {Computers & Industrial Engineering},
volume = {15},
number = {1},
pages = {249-258},
year = {1988},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(88)90095-2},
url = {https://www.sciencedirect.com/science/article/pii/0360835288900952},
author = {K. Ida and J.U. Lee and M. Gen},
abstract = {Recently, a multiple criteria decision making (MCDM) has been well established as a practical approach to seek a satisfactory solution to a decision making problem. Linear programming (LP) is one of the most widely used OR/MS/IE techniques for solving MCDM problems. In the realistic decision making problems, many LP problems are involved a large number of 0–1 decision variables and a special type of system structures. So much kind of the large-scale 0–1 LP problems are simply too large fit into a microcomputer/workstation. In this paper, we develop a software package micro 0–1LP(GUB) for solving LP problems with a generalized upper bounding (GUB) structure as large-scale 0–1LP problems on UNIX systems. From the views of the computational experience and storage requirement, micro 0–1LP(GUB) using the C programming language is implemented an efficient method in which the GUB structure would be effectively handled. As a real-world large-scale 0–1LP problem with the GUB structures by the micro 0–1LP (GUB) software package developed here, we demonstrate an optimization problem of system reliability for selecting the allocating the components on an UNIX system, Sanyo/Icon MPS 020.}
}
@article{ZHANG2011625,
title = {Identifying relevant studies in software engineering},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {625-637},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002260},
author = {He Zhang and Muhammad Ali Babar and Paolo Tell},
keywords = {Search strategy, Quasi-gold standard, Systematic literature review, Evidence-based software engineering},
abstract = {Context
Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.
Objective
The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE.
Method
We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of ‘quasi-gold standard’ (QGS), which consists of collection of known studies, and corresponding ‘quasi-sensitivity’ into the search process for evaluating search performance.
Results
We conducted two participant–observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research.
Conclusion
We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE.}
}
@article{VALLES201396,
title = {Implementación basada en el middleware OROCOS de controladores dinámicos pasivos para un robot paralelo},
journal = {Revista Iberoamericana de Automática e Informática Industrial RIAI},
volume = {10},
number = {1},
pages = {96-103},
year = {2013},
issn = {1697-7912},
doi = {https://doi.org/10.1016/j.riai.2012.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1697791212000970},
author = {Marina Vallés and Jose I. Cazalilla and Ángel Valera and Vicente Mata and Álvaro Page},
keywords = {tiempo-real, middleware, implementación basada en componentes, manipuladores paralelos, Real-time, middleware, based-on-components implementation, parallel manipulators},
abstract = {Resumen
La complejidad actual de los sistemas robotizados y de las aplicaciones que éstos deben realizar requiere que los robots dispongan de un control automático que permita la ejecución de las distintas tareas que forman parte del algoritmo de control y que tenga en cuenta cuestiones relacionadas por ejemplo con la periodicidad, el modo de ejecución, el hardware que se utilizará, etc. Para el desarrollo de este tipo de aplicaciones de control en los últimos añ os se tiende a la programación basada en componentes puesto que ésta permite obtener código reusable. Así mismo también se está incrementando la utilización de middlewares que permiten la abstracción de los sistemas operativos, el soporte de tiempo real y la infraestructura de comunicaciones. En el presente artículo se propone la utilización de un middleware orientado especialmente a la robótica: OROCOS. Así se describe cómo haciendo uso de una de sus librerías, Orocos Toolchain, se han desarrollado una serie de componentes correspondientes a distintos algoritmos para el control dinámico de robots, aplicándose a un robot paralelo de 3 grados de libertad (DOF).
Automatic control of robotic systems nowadays deals more and more with the implementation of different tasks to be achieved by the robot with distinct complexity degree, periodic or aperiodic, with local execution or distributed along a communication network and needing to deal with different kinds of hardware. Deal with the controller implementation for a new robotic platform involved to develop suitable software again for the new hardware or, in the best case, to adapt the existing one. In the recent years it has been tending to a component-based programing that allows to develop reusable software and to use a middleware that make it possible to abstract the developed software from the used hardware and from the available communication protocols. At this paper one of these middleware has been used, specially oriented to robotics as is OROCOS. Using the Orocos Toolchain library the real-time components needed for the implementation of several dynamic controllers for a parallel manipulator have been developed. At this paper the robot and the designed controllers are described and the results over the actual robot are shown.}
}
@article{GRAFFEO20151486,
title = {Creating a Scenario Design Workflow for Dynamically Tailored Training in Socio-cultural Perception},
journal = {Procedia Manufacturing},
volume = {3},
pages = {1486-1493},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.328},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915003297},
author = {Clarissa Graffeo and Tracy {St. Benoit} and Robert E. Wray and Jeremiah T. Folsom-Kovarik},
keywords = {Scenario design, Simulation, Adaptive training, Perceptual–cognitive skills},
abstract = {The current operational context for military personnel necessitates development of nuanced perceptual skills, including the ability to identify and interpret a range of socio-cultural behaviors and patterns of life. To develop this capacity, we constructed the Virtual Observation Suite Demonstrator (VOSD), a simulation-based training platform combining extant simulation technologies, custom software components that enable system reconfiguration and expansion, and the Dynamic Tailoring System (DTS), a custom middleware package which adds trainee-specific scenario tailoring and feedback capabilities. Constructing socio-cultural perception scenarios, then planning dynamic tailoring to support and challenge trainees at different proficiencies, requires a complex workflow that integrates: 1) instructional design; 2) story-telling and narrative representation; 3) operational and socio-cultural subject matter expertise; 4) simulation engine implementation; and 5) identification of event alternatives, parameters, and subsequent configuration for tailoring. While instructional design for scenario-based training has been explored in detail, the additional considerations required to support dynamic tailoring introduce unique concerns. Planning tailoring options requires instructional expertise on the training efficacy tightly unified with knowledge of the technical capabilities of the system, as well as attention to maintaining narrative coherence and consistency. This paper discusses the current scenario development workflow used by our team to create prototype scenarios. We also identify and describe specific challenges and limitations for authoring, and outline preliminary recommendations for overcoming these issues in the future.}
}
@article{SHERSTNEV201212,
title = {Prediction of precipitation kinetics during homogenisation and microstructure evolution during and after hot rolling of AA5083},
journal = {International Journal of Mechanical Sciences},
volume = {54},
number = {1},
pages = {12-19},
year = {2012},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2011.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0020740311001950},
author = {P. Sherstnev and C. Melzer and C. Sommitsch},
keywords = {Aluminium alloys, Hot rolling, Modelling, Recrystallisation nucleation, Static recrystallisation},
abstract = {Because previous history of the material such as heat treatment influences the microstructure evolution during hot working, modelling of the microstructure evolution along the process chain is necessary in order to predict the resulting microstructure and hence the mechanical properties. The precipitation kinetics during homogenisation was investigated using the thermodynamic calculation software MatCalc. A physically based internal state variable model was implemented into the commercial Finite Element (FE) analysis package FORGE 2008 for hot rolling simulations of AA5083. The model allowed to predict both the stored deformation energies, i.e. dislocation density, the subgrain structure during hot rolling ,as well as the statically recrystallised volume fraction (SRX) after hot rolling. Two nucleation sites for recrystallised grains were considered to achieve a better understanding of recrystallisation kinetics. To validate the simulation results hot rolling experiments were performed by means of a laboratory mill. The grain structure evolution was analysed by the electron backscatter diffraction (EBSD). The thermo-physical data of the investigated aluminium alloy were determined experimentally. The study shows the applicability of the model to describe the microstructure evolution from casting to the hot rolled product.}
}
@article{LINNHOFF1999S945,
title = {Linking process simulation and process integration},
journal = {Computers & Chemical Engineering},
volume = {23},
pages = {S945-S953},
year = {1999},
note = {European Symposium on Computer Aided Process Engineering},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(99)80229-4},
url = {https://www.sciencedirect.com/science/article/pii/S0098135499802294},
author = {B. Linnhoff and C.G. Akinradewo},
abstract = {Process design should consider alternative flowsheet topologies and alternative equipment sizes. For instance, the process design of a distillation sequence should consider different sequences (direct, indirect, others), different overhead compositions, recycle flowrates, reflux ratios, etc. Each design will have its own energy consumption. Moreover, each design will have its own potential for integration with the rest of the process. To keep track of all alternatives, the process design engineer usually relies on simulation. Different process topologies and equipment sizes are simulated, costed and compared. The wider the scope of the system is the more difficult it becomes for the engineer to keep track of all solutions and the search becomes “ad hoc”. In this situation, process integration helps. Process integration methods often guarantee, or nearly guarantee, optimal structures for a significant part of the system. In other words, process integration plays an important part in process design. However, there have been barriers to overcome in the introduction of process integration to the process design environment. Perhaps the most important such barrier has been the problem of “data extraction”. The task of “data extraction” presented a barrier because it required expert understanding of process integration techniques, particularly Pinch Technology. Inexpertly extracted data often led in practice to situations where optimum solutions were missed. Moreover, the task of “data extraction” was time consuming, even for the expert. In 1997, Linnhoff March launched software to perform the skilled task of “data extraction”, automatically. This novel and unique software is based on an expert system which acts as an intelligent interface between process simulation and process integration. In 1998, Linnhoff March complemented this with more-direct links with the main commercial process simulators. The overall package analyses any simulated process at the press of a button, automatically, to identify the potential for improvement. There are many benefits. Design time is saved. The impact of process changes on overall cost is quickly analysed for any number of options. The power to do this is made accessible to a larger number of less-experienced users. All in all, process designs will be designed close to optimum, by less experienced users, in less time. In this paper, we outline the essential principles for extracting process integration data from process simulations, as implemented in Linnhoff March's expert system software. Practical examples from industrial projects demonstrate these principles to produce rapid and reliable results to reduce process energy levels and capital investment.}
}
@article{DELEONALDACO2018115,
title = {Enhancement of the survival probability of a photovoltaic converter–An optimization approach},
journal = {Computers & Electrical Engineering},
volume = {69},
pages = {115-128},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617331622},
author = {Susana {de León-Aldaco} and Hugo Calleja and Jesús Aguayo and Carlos Correa and Eligio Flores},
keywords = {Power converter, Genetic algorithms, Reliability estimation, Optimization methods, Reliability, Circuit optimization, Circuit reliability, FIDES methodology, Failure rate, Thermal factor},
abstract = {Maximizing the return on investment of a photovoltaic array requires a power electronics converter with the longest possible operational life. This paper is aimed at presenting a heatsink optimization approach to enhance the survival probability of a high-efficiency 1 kW DC/DC converter, specifically designed for PV applications. A main feature of the enhancement is that it takes into account the meteorological characteristics of the intended installation site: a coastal area with high humidity and cloudiness throughout the year. As a first step, the converter reliability is estimated according to the FIDES procedure, yielding a mean time between failures MTBF = 98.15 FIT. The numerical results pinpoint the diodes as the most failure-prone components, and temperature as the dominant stressor, clearly suggesting that the performance can be improved by resorting to better heatsinks. As a second step, a heatsink optimization problem is established, involving three objective functions: minimization of heatsink thermal resistance and weight, and maximization of the cooling system performance index, The problem is solved using a genetic algorithm, which yields MTBF = 74.09 FIT. The approach takes advantage of the computer tools currently available: mathematical toolboxes to implement the optimization algorithm, user-friendly reliability prediction software, and software packages capable of simulating the thermal behavior of heatsinks with good accuracy.}
}
@article{BISWAL2017165,
title = {Finite element model updating of concrete structures based on imprecise probability},
journal = {Mechanical Systems and Signal Processing},
volume = {94},
pages = {165-179},
year = {2017},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2017.02.042},
url = {https://www.sciencedirect.com/science/article/pii/S0888327017301140},
author = {S. Biswal and A. Ramaswamy},
keywords = {Imprecise probability, Finite element model updating, Markov chain Monte Carlo, Probability boxes, Bounded measurements},
abstract = {Imprecise probability based methods are developed in this study for the parameter estimation, in finite element model updating for concrete structures, when the measurements are imprecisely defined. Bayesian analysis using Metropolis Hastings algorithm for parameter estimation is generalized to incorporate the imprecision present in the prior distribution, in the likelihood function, and in the measured responses. Three different cases are considered (i) imprecision is present in the prior distribution and in the measurements only, (ii) imprecision is present in the parameters of the finite element model and in the measurement only, and (iii) imprecision is present in the prior distribution, in the parameters of the finite element model, and in the measurements. Procedures are also developed for integrating the imprecision in the parameters of the finite element model, in the finite element software Abaqus. The proposed methods are then verified against reinforced concrete beams and prestressed concrete beams tested in our laboratory as part of this study.}
}
@article{KAMINSKI20095,
title = {Random Eigenvibrations of Elastic Structures by the Response Function Method and the Generalized Stochastic Perturbation Technique},
journal = {Archives of Civil and Mechanical Engineering},
volume = {9},
number = {4},
pages = {5-32},
year = {2009},
issn = {1644-9665},
doi = {https://doi.org/10.1016/S1644-9665(12)60066-1},
url = {https://www.sciencedirect.com/science/article/pii/S1644966512600661},
author = {M.M. Kamiński and J. Szafran},
keywords = {stochastic dynamics, Stochastic Finite Element Method, response function method, stochastic perturbation},
abstract = {This paper addresses the important question in structural analysis how to efficiently model the eigenvibrations of the spatial structures with random physical and/or geometrical parameters. The entire computational methodology is based on the traditional Finite Element Method enriched with the stochastic perturbation technique in its generalized nth order approach, while the computational implementation is performed by the use of the academic FEM software in conjunction with the symbolic algebra computer system MAPLE. Contrary to the previous straightforward solution techniques, now the response function method is applied to compute any order probabilistic moments and coefficients of the structural eigenvalues. The response function is assumed in the polynomial form, the coefficients of which are computed from the several solutions of the deterministic problem around the mean value of the given input random parameter. This method is illustrated with the stochastic eigenvibrations of the simple single degree of freedom system and small steel tower modelled as the 3D truss structure with random mass density and Young modulus. This technique may find its wide application in reliability analysis of the real existing engineering structures using the commercial Finite Element Method packages as well as the other discrete computational techniques like the Finite Difference Method at least.}
}
@article{STATNIKOV2005e685,
title = {Multicriteria analysis of real-life engineering optimization problems: statement and solution},
journal = {Nonlinear Analysis: Theory, Methods & Applications},
volume = {63},
number = {5},
pages = {e685-e696},
year = {2005},
note = {Invited Talks from the Fourth World Congress of Nonlinear Analysts (WCNA 2004)},
issn = {0362-546X},
doi = {https://doi.org/10.1016/j.na.2005.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S0362546X05000428},
author = {R.B. Statnikov and A. Bordetsky and A. Statnikov},
keywords = {Multicriteria analysis, PSI method, Feasible solution set, Pareto optimal set, MOVI software system},
abstract = {The majority of engineering problems are essentially multicriteria. These criteria are usually contradictory. That is why specialists experience significant difficulties in correctly stating engineering optimization problems, so designers often end up solving ill-posed problems. In general, it is impossible to reduce multicriteria problems to single-criterion ones. For the correct statement and solution of engineering optimization problems, a method called Parameter Space Investigation (PSI method) has been created and widely integrated into various fields of industry, science, and technology (e.g., design of the space shuttle, nuclear reactor, missile, automobile, ship, and metal-tool). In summary, the PSI method generates many feasible designs from which the so-called Pareto optimal ones (i.e. solutions which cannot be improved) are extracted. The PSI method can also be used to efficiently optimize models in a parallel mode, which is of great importance while solving high-dimensional multiparameter and multicriteria problems. The PSI method is implemented in the software package Multicriteria Optimization and Vector Identification (MOVI), a comprehensive system for multicriteria engineering optimization (design, identification, and control). This system allows optimization of many problems that until recently appeared intractable.}
}
@article{WISER2021121321,
title = {Thermal-striping analysis methodology for sodium-cooled reactor design},
journal = {International Journal of Heat and Mass Transfer},
volume = {175},
pages = {121321},
year = {2021},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2021.121321},
url = {https://www.sciencedirect.com/science/article/pii/S0017931021004245},
author = {Ralph Wiser and Samuel E. Bays and SuJong Yoon},
abstract = {Computational fluid dynamics simulations have been performed to study the applicability of engineering CFD methods for thermals striping analysis. Thermal striping is the fluctuating temperature profile in a solid caused by fluctuating fluid temperature, and the resulting thermal stresses cause high cycle fatigue and eventual material failure. This study presents the methodology for thermal striping analysis, including a transient conjugate heat transfer RANS simulation benchmarked against a sodium triple jet experiment. Multiphysics calculations are implemented to analyze the thermal stresses in the solid domain caused by the coupled heat transfer between the fluid and solid domains. Validation data from a liquid sodium triple jet experiment include time-averaged temperature measurements and power spectra of the temperature signal. The numerical results agree well with these experimental measures, demonstrating key features such as the dominant frequency of temperature fluctuations. The applicability of a low-cost wall treatment method is demonstrated, enabling key computational savings. Finally, the performance of two finite element stress analysis software packages is compared, and the validity of the lower-cost method is confirmed. These results demonstrate the applicability of engineering methods for computational thermal striping calculations, enabling thermal striping estimations in large fluid systems such as the core of a liquid metal-cooled nuclear reactor.}
}
@article{JANNA2009675,
title = {A comparison of projective and direct solvers for finite elements in elastostatics},
journal = {Advances in Engineering Software},
volume = {40},
number = {8},
pages = {675-685},
year = {2009},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2008.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0965997808001968},
author = {Carlo Janna and Andrea Comerlati and Giuseppe Gambolati},
keywords = {Krylov, Preconditioning, Conjugate gradient, Reorderings},
abstract = {The Finite Element Method (FEM) is widely used in civil and mechanical engineering to simulate the behavior of complex structures and, more specifically, to predict stress and deformation fields of structural parts or mechanical bodies. In the former case, the coupling between different types of elements, such as beams, trusses, and shells, is often required, while in the latter fully 3D discretizations are typically used. For both, FEM leads to symmetric positive definite (SPD) matrices that, depending on the type of discretization and especially on the topology of the nodal connections, may be efficiently solved by either the Preconditioned Conjugate Gradient (PCG) or a direct solver such as the routine MA57 of the Harwell Software Library. Numerical experiments are shown and discussed where the effect of spatial discretization, different solution techniques, and a possible nodal reordering, is explored. The PCG preconditioner used is a variant of the incomplete Cholesky factorization with variable fill-in. It is shown that for structures with 1D or 2D connections, such as for example a bridge, MA57 performs usually better than PCG. In this case it is noted that some reorderings specifically designed and implemented for direct elimination methods can be very helpful for PCG as well as they yield a cheaper preconditioner and lead to a much faster PCG convergence. The main disadvantage is the need for an appropriate degree of fill-in for the preconditioner which turns out to be problem dependent and must be found empirically. However, in fully 3D problems, arising for example from the FE discretization of structural components or geomechanical structures, PCG outperforms MA57 while also requiring much less memory, and thus allowing for the use of much refined grids, if needed. With the aid of a large geomechanical problem it is shown that direct solvers may not be (even) used on serial computers due to their prohibitive computational cost with PCG the only viable alternative solver.}
}
@article{BERNUZZI2017181,
title = {EU and US design approaches for steel storage pallet racks with mono-symmetric cross-section uprights},
journal = {Thin-Walled Structures},
volume = {113},
pages = {181-204},
year = {2017},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0263823117300435},
author = {Claudio Bernuzzi and Marco Simoncelli},
keywords = {Semi-continuous unbraced frames, Adjustable selective steel storage pallet racks, Open thin-walled cross-section, Warping effects, Safety index},
abstract = {Design, fabrication and erection of industrial steel storage rack systems can nowadays take place at different locations, potentially separated by thousands of kilometers. Consequently, manufacturing engineers need to understand the main code provisions adopted in the country where the rack will be in-service. Frequently, design is carried out in accordance with the European (EU) or United States (US) rack codes, which are the most commonly adopted standards for industrial storage systems. As appraisal of the key differences on structural performance associated with the different code provisions in term of storage performance, a previous study of the Authors was focused on storage pallet racks comprised of bi-symmetric cross-section uprights (vertical members). Now, attention is paid to racks with mono-symmetric uprights, typically influenced by relevant warping effects, which are traditionally neglected by rack provisions and, as a consequence, by manufacturing engineers. The design approaches already considered and compared for bi-symmetric uprights now appear to be inadequate and have been necessarily improved, as suggested by the Authors, including at least the contribution due to the bimoment acting along mono-symmetric uprights. Research outcomes, which are discussed in the paper, are related to a parametric analysis on several racks differing for configurations, geometry of components and degree of rotational stiffness of joints. The associated results regarding the four EU and two US considered alternatives are presented and compared directly to each other to allow for a concrete appraisal of the most relevant differences between the admitted design approaches. In order to highlight the importance of warping effects, which can be evaluated only by means of refined finite element (FE) analysis software, design has been undertaken using also the more traditional FE beam formulation implemented in the commercial analysis packages most frequently used in manufacturing offices. Finally, Appendix A presents a complete design example to be used as benchmark, where all the discussed design options are applied and compared to each other.}
}
@article{SANCHEZ2002341,
title = {A relational database for physical data from TJ-II discharges},
journal = {Fusion Engineering and Design},
volume = {60},
number = {3},
pages = {341-346},
year = {2002},
issn = {0920-3796},
doi = {https://doi.org/10.1016/S0920-3796(02)00030-3},
url = {https://www.sciencedirect.com/science/article/pii/S0920379602000303},
author = {E. Sánchez and A.B. Portas and J. Vega},
keywords = {Database, TJ-II data acquisition system, Open network computing remote procedure call, Client/server},
abstract = {A relational database (RDB) has been developed for classifying TJ-II experimental data according to physical criteria. Two objectives have been achieved: the design and the implementation of the database and the software tools for data access depending on a single software driver. TJ-II data were arranged in several tables with a flexible design, speedy performance, efficient search capacity and adaptability to meet present and future, requirements. The software has been developed to allow the access to the TJ-II RDB from a variety of computer platforms (alpha axp/True64 unix, cray/unicos, Intel Linux, Sparc/Solaris and Intel/Windows 95/98/NT) and programming languages (fortran and c/c++). The database resides in a Windows NT Server computer and is managed by Microsoft SQL Server. The access software is based on open network computing remote procedure call and follows client/server model. A server program running in the Windows NT computer controls data access. Operations on the database (through a local ODBC connection) are performed according to predefined permission protocols. A client library providing a set of basic functions for data integration and retrieval has been built in both static and dynamic link versions. The dynamic version is essential in accessing RDB data from 4GL environments (IDL and PV-WAVE among others).}
}
@article{WANG201245,
title = {Application of intensified heat transfer for the retrofit of heat exchanger network},
journal = {Applied Energy},
volume = {89},
number = {1},
pages = {45-59},
year = {2012},
note = {Special issue on Thermal Energy Management in the Process Industries},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2011.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0306261911001760},
author = {Yufei Wang and Ming Pan and Igor Bulatov and Robin Smith and Jin-Kuk Kim},
keywords = {Heat exchanger network (HEN), Retrofit, Heat transfer enhancement (HTE), Heat exchanger model, Heuristics},
abstract = {A number of design methods have been proposed for the retrofit of heat exchanger networks (HEN) during the last three decades. Although considerable potential for energy savings can be identified from conventional retrofit approaches, the proposed solutions have rarely been adopted in practice, due to significant topology modifications required and resulting engineering complexities during implementation. The intensification of heat transfer for conventional shell-and-tube heat exchangers can eliminate the difficulties of implementing retrofit in HEN which are commonly restricted by topology, safety and maintenance constraints, and includes high capital costs for replacing equipment and pipelines. This paper presents a novel design approach to solve HEN retrofit problems based on heat transfer enhancement. A mathematical model has been developed to evaluate shell-and-tube heat exchanger performances, with which heat-transfer coefficients and pressure drops for both fluids in tube and shell sides are obtained. The developed models have been compared with the Bell-Delaware, simplified Tinker and Wills–Johnston methods and tested with the HTRI® and HEXTRAN® software packages. This demonstrates that the new model is much simpler but can give reliable results in most cases. For the debottlenecking of HEN, four heuristic rules are proposed to identify the most appropriate heat exchangers requiring heat transfer enhancements in the HEN. The application of this new design approach allows a significant improvement in energy recovery without fundamental structural modifications to the network.}
}
@article{VASQUEZ20031585,
title = {Nonlinear interaction design provisions using spectral superposition},
journal = {Engineering Structures},
volume = {25},
number = {13},
pages = {1585-1595},
year = {2003},
issn = {0141-0296},
doi = {https://doi.org/10.1016/S0141-0296(03)00124-X},
url = {https://www.sciencedirect.com/science/article/pii/S014102960300124X},
author = {Jorge Vásquez},
keywords = {Modal combination, Cross-estimators, Nonlinear interaction capacity requirements, Single direction excitation, Most unfavorable direction},
abstract = {The problem of enforcing two-variable nonlinear interaction code provisions within a spectral superposition approach to design, is addressed. An analytical procedure based on the linearization of the interaction curve through a set of tangents or secants is developed. Safety with respect to the interaction curve is approximated by requiring safety with respect to that set of straight lines. For calculating the required estimators, the cross-estimator based formula for estimators of the linear combination of variables, derived in a companion paper, is used. The analytical method developed was shown to be equivalent to a graphical method proposed by Gupta, based on inscribing an estimator ellipse within the interaction curve. The analytical method is more straightforward and handles the directional maximum in single-component excitation, which the graphical method does not. The analytical method is much easier to apply. However, if a study calls for actually drawing the estimator ellipse, straightforward construction methods are presented, making unnecessary a cumbersome equivalent modal response approximation that had been suggested. The inherent antisymmetry of the spectral superposition formula, and its implications, are discussed. The effect of static loading and of symmetry of the interaction curve is also analyzed. Within the analysis of an example, the work required for the application of the procedure for nonlinear interaction for design purposes is discussed. It is found that its implementation can be achieved through a very simple function written for any standard numerical computation software package. The example also makes quite apparent the advantages of the analytical over the graphical method. The application example, which considers the design of a concrete column in a simple 10-storey building structure, shows the overconservativeness of a design based only in the standard estimators. The design criterion used in the example is that of the most unfavorable direction of a single-component earthquake.}
}
@incollection{JANJIC2002831,
title = { - The unit load method-some recent applications},
editor = {S.L. Chan and J.G. Teng and K.F. Chung},
booktitle = {Advances in Steel Structures (ICASS '02)},
publisher = {Elsevier},
address = {Oxford},
pages = {831-837},
year = {2002},
isbn = {978-0-08-044017-0},
doi = {https://doi.org/10.1016/B978-008044017-0/50097-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080440170500974},
author = {D. Janjic and M. Pircher and H. Pircher},
abstract = {Publisher Summary
The Unit Load Method is originally proposed as a procedure to optimize the tensioning process for the stay-cables in cable-stayed bridges and is implemented in a well-established bridge-design software package for this purpose. The implementation of this method takes into account all relevant effects for the design of cable stayed bridges including construction sequence, second order theory, large displacements, cable sag, and time-dependent effects, such as creep and shrinkage or cable relaxation. The underlying ideas of this method can also be applied to other optimization problems in structural engineering. This chapter gives an overview about the wide range of possible applications for which this method can be used, and it finishes with examples from practical experiences with the Unit Load Method. The Unit Load Method has been developed to achieve a pre-defined target configuration of section forces in cable-stayed bridges by optimizing the tensioning of the stay-cables. Recently, the method is developed further into a versatile design tool that allows the definition of a target distribution of section forces or deflections in any structure. Using the Unit Load Method, the necessary adjustments in a pre-determined set of constrains is computed to achieve exactly this distribution. The chapter briefly describes the method and gives three application examples where this method is used: a cable-stayed bridge, a concrete arch, and the application of this method to the automated simulation of the incremental launching process of bridges.}
}
@article{XU1993717,
title = {Integrated software system for seismic evaluation of nuclear power plant structures},
journal = {Computers & Structures},
volume = {46},
number = {4},
pages = {717-723},
year = {1993},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(93)90400-8},
url = {https://www.sciencedirect.com/science/article/pii/0045794993904008},
author = {J. Xu and H.L. Graves},
abstract = {This paper describes the capabilities and theoretical bases of the computer software CARES (Computer Analysis for Rapid Evaluation of Structures) developed by the Brookhaven National Laboratory (BNL) for the U.S. Nuclear Regulatory Commission. The development of CARES represents an effort to utilize established numerical methodologies commonly employed by industry for structural safety evaluations of nuclear power plant facilities and incorporate them into an integrated computer software package operated on personal computers (PC). CARES was developed with the objective to include all aspects of seismic performance evaluation of nuclear power structures, thereby it can be used to evaluate the validity and accuracy of analysis methodologies used for structural safety evaluations of nuclear power plants by various utilities. CARES is structured in a modular format, each module performing a specific type of analysis. This paper describes the features which have been implemented into the seismic module of CARES. The seismic module integrates the capabilities required to accomplish all steps of a complete seismic analysis into a single package with many user-friendly features such as interactiveness and quick turnaround. Linear structural theory and pseudo-linear convolution theory are utilized as the bases for the development with a special emphasis on the areas of nuclear regulatory requirements pertaining to structural safety of nuclear plants. The organization of the seismic module is arranged in eight options, each performing a specific step of the analysis with most of input/output interfacing processed by the general manager. Finally, CARES provides comprehensive post-processing capability to display results graphically or in tabular form so that direct comparisons can be easily made.}
}
@article{BUUR1990243,
title = {DiaKin: an integrated program package for hemodialysis kinetics},
journal = {Computer Methods and Programs in Biomedicine},
volume = {31},
number = {3},
pages = {243-254},
year = {1990},
issn = {0169-2607},
doi = {https://doi.org/10.1016/0169-2607(90)90009-X},
url = {https://www.sciencedirect.com/science/article/pii/016926079090009X},
author = {Tom Buur},
keywords = {Hemodialysis, Kinetics, Algorithm, Software},
abstract = {A flexible program for the IBM PC performing a number of calculations of relevance for the prescription of hemodialysis treatment has been developed. The program has a ‘spreadsheet’-like user-friendly interface, and results may be presented graphically. The present implementation covers 89 algorithms/equations, all based on the assumption of single-pool kinetics. Some of these are detailed for the first time, including considerations on their implementation using a generalizing, structured approach.}
}
@article{LIAO2017190,
title = {A novel and fast single-phase three-wire power flow solution for a modern home premises wiring systems},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {90},
pages = {190-207},
year = {2017},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0142061516321044},
author = {Rih-Neng Liao and Nien-Che Yang and Tsai-Hsiang Chen},
keywords = {Premises wiring system, Feeder, Branch circuit, Loop frame of reference, Single-phase three-wire circuit, Power flow, Dwelling unit, Smart building},
abstract = {Single-phase 2-wire (1φ2w) and single-phase 3-wire (1φ3w) circuit layouts are commonly adopted in premises wiring systems. Compared with 1φ2w circuits, the arrangements of 1φ3w circuits have some advantages such as reduced voltage drop, reduced line loss, and fewer conductors. Currently, more detailed power flow solutions of premises wiring systems are required because they can be used in more applications, such as power loss analysis, conservation voltage regulation (CVR), load balancing, and network reconfiguration especially in the design and operation of modern home energy management systems (HEMSs). This paper presents a novel approach to solve the power flow problem of a 1φ3w premises wiring system. The proposed approach is based on the loop frame of reference rather than the conventional approach, which is normally based on the bus frame of reference. Because the proposed approach is mainly based on graph theory, feeders and branch circuits of a premises wiring system are represented in a more detailed fashion than in previous corresponding mathematical models. In addition, the proposed approach provides an efficient and simple way to help engineers build a performance equation and full-scale system model of a premises wiring system. The simulation results of the proposed approach are verified through the OpenDSS software package and field testing. The proposed approach and implementation technique are of value to engineers and technicians in the design and operation of premises wiring system for dwelling units or smart buildings and may be implemented in HEMSs.}
}
@article{GOUCEM1988137,
title = {An Extension to a Commercial CACSD Package for Use in a Basic Control Course},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {6},
pages = {137-142},
year = {1988},
note = {IFAC Symposium on Trends in Control and Measurement Education, Swansea, UK, 11-13 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53853-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017538537},
author = {A. Goucem and D.P. Atherton},
keywords = {Control system analysis, education, frequency response, Laplace transforms, linear systems, root loci, step response},
abstract = {The paper describes the implementation of procedures in CTRL-C which make it immediately usable for single control problems by an undergraduate with no knowledge of the software. The student is required to execute two procedures, PLANT and CONTROL, and then respond appropriately to simple menus. This provides the student with an easy entry to powerful control system design software which he can use in more depth later in his education or professional career.}
}
@article{YANG2021108893,
title = {Multiscale modeling of unidirectional composites with interfacial debonding using molecular dynamics and micromechanics},
journal = {Composites Part B: Engineering},
volume = {219},
pages = {108893},
year = {2021},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2021.108893},
url = {https://www.sciencedirect.com/science/article/pii/S1359836821002845},
author = {Danhui Yang and Yu Sun and Zhibo Yang and Xuefeng Chen and Chenxi Wang},
keywords = {Multiscale, Finite-volume theory, Homogenization, Interfacial debonding, Unidirectional composite},
abstract = {In this study, a new multiscale micromechanical model based on the finite-volume direct averaging micromechanics (FVDAM) theory and molecular dynamics (MD) is constructed. It describes the interfacial debonding phenomena of unidirectional composites, whereby a solid fracture interface is incorporated with the FVDAM for the first time. To better elucidate the debonding behavior of the solid interface, a six-degrees-of-freedom (6-DoF) cohesive zone model (CZM) is proposed using MD simulation. The damage matrix Dc(k) in the kth subvolume of the interface is obtained using the 6-DoF CZM, and its explicit form is described herein to facilitate its implementation in commercial software packages. To verify the effectiveness of the model, both experimental data and numerical simulations were adopted for comparison. The experimental data of the unidirectional SiCf/Ti6Al4V composite were compared with the simulation results of FVDAM, and a good agreement was established. A finite element method-based unit cell model is constructed using ABAQUS for numerical validation, and homogenized responses and local fields are compared. The excellent correlations prove the effectiveness of the proposed model. Furthermore, the effects of thermal residual stress and fiber orientations are revealed, which may provide some useful information for the design and manufacture of composites.}
}
@article{PASHKEVICH1997191,
title = {Real-Time Inverse Kinematics for Robots with Offset and Reduced Wrist},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {3},
pages = {191-196},
year = {1997},
note = {4th IFAC Workshop on Algorithms and Architectures For Real-Time Control 1997 (AARTC '97), Vilamoura, Portugal, 9-11 April},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)44489-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017444892},
author = {Anatoly Pashkevich},
keywords = {robot kinematics, iterative methods, singularities, on-line control, off-line programming, computeraided design, industrial production systems},
abstract = {The paper deals with inverse transformation of coordinates for robotic manipulators with high payload capacity that are widely used in welding applications. Such manipulators are usually equipped with 3 d.o.f. offset wrists or 2 d.o.f reduced wrists that do not satisfy Pieper condition that ensure existence of inverse kinematics closed-form solution. Two reliable iterative algorithms are proposed to solve the problem in real time. The proposed algorithms have been implemented in commercial robot controllers and software packages for computer-aided design of welding robotic cells.}
}
@article{KIM1988271,
title = {An integrated approach to sculptured surface design and manufacture},
journal = {Computers & Industrial Engineering},
volume = {14},
number = {3},
pages = {271-280},
year = {1988},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(88)90005-8},
url = {https://www.sciencedirect.com/science/article/pii/0360835288900058},
author = {Kwangsoo Kim and John E. Biegel},
abstract = {The use of free-form or sculptured surface descriptions for computer-aided design applications has been proposed by numerous authors. The actual implementation of interactive sculptured surface description techniques for computer-aided design and manufacturing has been very limited. This paper describes an integrated approach to sculptured surface design and manufacture, and a software package for it on a five-axis NC milling machine. The integrated software consists of three parts: (1) surface description for generating the mathematical representation of sculptured surfaces, (2) path generation for approximating the surface representation into a sequence of linear cutter paths, and (3) tool control for generating the corresponding joint variable values.}
}
@article{BEER1988133,
title = {Applied environmetrics: Simulation applied to the physical environment},
journal = {Mathematics and Computers in Simulation},
volume = {30},
number = {1},
pages = {133-138},
year = {1988},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(88)90115-2},
url = {https://www.sciencedirect.com/science/article/pii/0378475488901152},
author = {Tom Beer},
abstract = {Environmetrics is the application of quantitative methods to all aspects of the social and natural environment. This includes forecasting, mathematical modelling, data analysis, and statistics. Applied Environmetrics as a discipline involves the analysis of environmental data through the use of packaged, or specially designed computer software. This paper deals with two case studies of recent implementations of applied environmetrics within the Australian mining industry.}
}
@article{LINDEMANN199379,
title = {An improved numerical algorithm for calculating steady-state solutions of deterministic and stochastic Petri net models},
journal = {Performance Evaluation},
volume = {18},
number = {1},
pages = {79-95},
year = {1993},
note = {Analysis of Petri Net Performance Models},
issn = {0166-5316},
doi = {https://doi.org/10.1016/0166-5316(93)90028-S},
url = {https://www.sciencedirect.com/science/article/pii/016653169390028S},
author = {Christoph Lindemann},
keywords = {numerical methods for transient analysis of Markov chains, stochastic Petri nets, performance evaluation},
abstract = {This paper introduces an efficient algorithm for calculating the time-dependent quantities required by the numerical solution process of Deterministic and Stochastic Petri Nets (DSPNs). The described method employs the randomization technique and a stable calculation of Poisson probabilities. A complete re-design and re-implementation of the appropriate components implemented in the version 1.4 of the software package GreatSPN has lead to significant savings in both computation time and memory space. These benefits are illustrated by DSPN models of an Er/D/1/K queueing system and a fault-tolerant clock synchronization system. These examples show that steady-state solutions of DSPNs are calculated with significantly less computational effort by the algorithm described in this paper than by the method implemented in the version 1.4 of the software package GreatSPN.}
}
@article{DUBOIS2014199,
title = {An MDE-based framework to support the development of Mixed Interactive Systems},
journal = {Science of Computer Programming},
volume = {89},
pages = {199-221},
year = {2014},
note = {Special issue on Success Stories in Model Driven Engineering},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2013.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167642313000671},
author = {Emmanuel Dubois and Christophe Bortolaso and Damien Appert and Guillaume Gauffre},
keywords = {Mixed Interactive System, Model-Driven Engineering, Development process, Domain-specific language, Flexible model transformation},
abstract = {In the domain of Human–Computer Interaction (HCI), recent advances in sensors, communication technologies, miniaturization and computing capabilities have led to new and advanced forms of interaction. Among them, Mixed Interactive Systems (MIS), form a class of interactive systems that comprises augmented reality, tangible interfaces and ambient computing; MIS aim to take advantage of physical and digital worlds to promote a more transparent integration of interactive systems with the user’s environment. Due to the constant change of technologies and the multiplicity of these interaction forms, specific development approaches have been developed. As a result, numerous taxonomies, frameworks, API and models have emerged, each one covering a specific and limited aspect of the development of MIS. To support a coherent use of these multiple development resources and contribute to the increasing popularity of MIS, we have developed a framework based on Model-Driven Engineering. The goal is to take advantage of Model-Driven Engineering (MDE) standards, methodology and tools to support the manipulation of complementary Domain Specific Languages (DSL), to organize and link the use of different design and implementation resources, and to ensure a rationalized implementation based on design choices. In this paper, we first summarize existing uses of MDE in HCI before focusing on five major benefits MDE can provide in a MIS development context. We then detail which MDE tools and resources support these benefits and thus form the pillars of the success of an MDE-based MIS development approach. Based on this analysis, we introduce our framework, called Guide-Me, and illustrate its use through a case study. This framework includes two design models. Model transformations are also included to link one model to another; as a result the frameworks coverage extends from the earliest design step to a software component-based prototyping platform. A toolset based on the Eclipse Modeling Framework (EMF) that supports the use of the framework is also presented. We finally assess our MDE-based development process for MIS based on the five major MDE benefits for MIS.}
}
@article{CONRAD1991365,
title = {CAD/CAE-Methods For Design of a Fast Digital Controlled Hydraulic Test Robot Manipulator},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {4},
pages = {365-370},
year = {1991},
note = {IFAC Symposium on Computer Aided Design in Control Systems, Swansea, UK, 15-17 July 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54299-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017542998},
author = {F. Conrad and L.F. Nielsen and P.H. Sørensen and E. Trostmann and S. Trostmann and J. Zhou},
keywords = {CAD/CAE of hydraulic robots, robot control, hydraulic robot, servoactuators, signalprocessors, digitalcontrol, adaptive control},
abstract = {The paper describes a very fast digitally controlled hydraulic test robot manipulator which has been developed and designed by the use of CAD/CAE-methods. The robot is implemented in the hydraulic laboratory at the Control Engineering Institute, the Technical University of Denmark. The purpose of the test robot manipulator is to carry out research activities within CAD/CAE in hydraulic control system design, digital control and adaptive control design and implementation. Furthermore the test robot is used for test and evaluation of off-line and on-line systemidentification methods and digital algorithms for modelling and adaptive control of multivariable hydraulic systems such as hydraulic robots, escavators og multi-axes test equipment. The paper describes and discusses the control and mechanical design problem and CAD/CAE-methods. The special software package KISMET and ROBCAD can be used in the design and 3D-animation of the robot manipulator for investigation of the robot and the workspace. The control design of the digital control system is done with the packages MATLAB and ACSL. The digital control computer is implemented with a fast AT&T-signalprocessor. A developed adaptive geometrical compensation control scheme is proposed for control of hydraulic robot manipulators}
}
@incollection{DAVIES2008177,
title = {Chapter 6 - Functions, Interrupts, and Low-Power Modes},
editor = {John H. Davies},
booktitle = {MSP430 Microcontroller Basics},
publisher = {Newnes},
address = {Burlington},
pages = {177-205},
year = {2008},
isbn = {978-0-7506-8276-3},
doi = {https://doi.org/10.1016/B978-075068276-3.50007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780750682763500079},
author = {John H. Davies},
abstract = {Publisher Summary
A well-structured program should be divided into separate modules—functions in C or subroutines in assembly language. The application note MSP430 Software Coding Techniques(slaa294) describes the overall structure of a typical, interrupt-driven program for the MSP430 and describes a range of techniques to ensure that programs are robust and can easily be debugged. It is good practice to break programs into short functions or subroutines, as it makes programs easier to write and more reliable to test and maintain. This chapter sheds light on the functions and their role in codes. Functions are useful for code that is called from more than one place but should be used much more widely, to encapsulate every distinct function. They hide the detailed implementation of an activity from the high-level, strategic level of the software. Functions can readily be reused and incorporated into libraries, provided that their documentation is clear. Following this, the study deals with interrupts. Interrupts are a major feature of most embedded software. They are vaguely like functions that are called by hardware rather than software. Finally, this chapter explains low-power modes of operation. They are of significance because the MSP430 needs an interrupt to wake it from a low-power mode. In fact it is observed that no extra effort is usually needed to handle low-power modes in interrupts: The MSP430 automatically goes to active mode when an interrupt is requested, services the interrupt, and resumes its low-power mode afterward.}
}
@incollection{BANERJEE200171,
title = {Chapter 3 - All your data: The oracle extensibility architecture},
editor = {Klaus R. Dittrich and Andreas Geppert},
booktitle = {Component Database Systems},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {71-104},
year = {2001},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-1-55860-642-5},
doi = {https://doi.org/10.1016/B978-155860642-5/50004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606425500043},
author = {Sandeepan Banerjee and Vishu Krishnamurthy and Ravi Murthy},
abstract = {Publisher Summary
The new Internet computing environment has brought new kinds of data to a huge number of users across the globe. Multimedia data types such as images, maps, video clips, and audio clips were once rarely seen outside specialty software. An increasingly large number of database applications demand content-rich data types and associated content-specific business logic. Oracle8i gives application developers greater control over user-defined data types by providing control over the manner in which the server stores, retrieves, or interprets this data. The Oracle8i database contains a series of database extensibility services that enable the packaging and integration of content-rich domain types and behavior into server-based managed components. There is a need for indexing of complex data types and for specialized indexing techniques. Oracle's solution is to build an extensible server that provides the application developer with the ability to define new index types. The extensibility services and interfaces available in Oracle8i are used by Oracle to create some commonly useful data cartridges. This chapter discusses the implementations of these data cartridges while focusing on the extensibility architecture and its benefits.}
}
@article{DUERI20141605,
title = {Automated Custom Code Generation for Embedded, Real-time Second Order Cone Programming},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {1605-1612},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02736},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016418422},
author = {Daniel Dueri and Jing Zhang and Behcet Açikmese},
abstract = {In this paper, we discuss the development of an Interior Point Method (IPM) solver for Second Order Cone Programming optimization problems that is capable of producing customized ANSI-C code for embedded, real-time applications. The customized code is generated for a given problem structure and makes use of no dynamic memory allocation, minimizes branching, wastes no mathematical or logical computations, and has minimal dependencies to standard libraries. The resulting software is designed to be easy to implement on embedded hardware with limited computing capabilities, while still providing accurate results rapidly enough for real-time use. The core IPM algorithm is a fairly standard primal-dual IPM, which makes use of Mehrotra predictor-corrector method with Nesterov-Todd scalings and Newton search directions. We make use of the Approximate Minimum Degree heuristic to maximize the sparsity of the Cholesky factorizations that are ultimately used to solve for the search directions. We conclude the paper by presenting the computational performance results from two example problems: a Mars landing optimal control problem and a reaction wheel allocation problem. The code generated for the Mars landing problem was successfully validated in three flights onboard a NASA test rocket, and was used in real-time to generate the optimal landing trajectories that guided the rocket. To the best of our knowledge, this was the first time that a real-time embedded convex optimization algorithm was used to control such a large vehicle, where mission success and safety critically relied on the real-time optimization algorithm.}
}
@article{BADALLO2013130,
title = {A comparative study of genetic algorithms for the multi-objective optimization of composite stringers under compression loads},
journal = {Composites Part B: Engineering},
volume = {47},
pages = {130-136},
year = {2013},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2012.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S135983681200738X},
author = {P. Badalló and D. Trias and L. Marín and J.A. Mayugo},
keywords = {A. Carbon fiber, C. Finite element analysis (FEA), C. Numerical analysis, Genetic algorithm},
abstract = {Optimization methods are close to become a common task in the design process of many mechanical engineering fields, specially those related with the use of composite materials which offer the flexibility in the design of both the shape and the material properties and so, are very suitable to any optimization process. While nowadays there exist a large number of solution methods for optimization problems there is not much information about which method may be most reliable for a specific problem. Genetic algorithms have been presented as a family of methods which can handle most of engineering problems. However, starting from a common basic set of rules many algorithms which differ slightly from each other have been implemented even in commercial software packages. This work presents a comparative study of three common Genetic Algorithms: Archive-based Micro Genetic Algorithm (AMGA), Neighborhood Cultivation Genetic Algorithm (NCGA) and Non-dominate Sorting Genetic Algorithm II (NSGA-II) considering three different strategies for the initial population. Their performance in terms of solution, computational time and number of generations was compared. The benchmark problem was the optimization of a T-shaped stringer commonly used in CFRP stiffened panels. The objectives of the optimization were to minimize the mass and to maximize the critical buckling load. The comparative study reveals that NSGA-II and AMGA seem the most suitable algorithms for this kind of problem.}
}
@article{HEMI201463,
title = {A real time fuzzy logic power management strategy for a fuel cell vehicle},
journal = {Energy Conversion and Management},
volume = {80},
pages = {63-70},
year = {2014},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2013.12.040},
url = {https://www.sciencedirect.com/science/article/pii/S0196890413008169},
author = {Hanane Hemi and Jamel Ghouili and Ahmed Cheriti},
keywords = {Fuel cell hybrid vehicle, Power management, Fuzzy logic, Battery, Supercapacitor, Fuel cell},
abstract = {This paper presents real time fuzzy logic controller (FLC) approach used to design a power management strategy for a hybrid electric vehicle and to protect the battery from overcharging during the repetitive braking energy accumulation. The fuel cell (FC) and battery (B)/supercapacitor (SC) are the primary and secondary power sources, respectively. This paper analyzes and evaluates the performance of the three configurations, FC/B, FC/SC and FC/B/SC during real time driving conditions and unknown driving cycle. The MATLAB/Simulink and SimPowerSystems software packages are used to model the electrical and mechanical elements of hybrid vehicles and implement a fuzzy logic strategy.}
}
@article{SHAHNASSER1988237,
title = {A dynamic data structure suitable for adaptive mesh refinement in finite element method},
journal = {Finite Elements in Analysis and Design},
volume = {4},
number = {3},
pages = {237-247},
year = {1988},
issn = {0168-874X},
doi = {https://doi.org/10.1016/0168-874X(88)90010-8},
url = {https://www.sciencedirect.com/science/article/pii/0168874X88900108},
author = {Hamid Shahnasser and Ward Morgan and A. Raghuram},
abstract = {This paper describes a dynamic data structure and its implementation, used for an optimum mesh generator. The implementation of this mesh generator was a part of a software package implemented to solve electromagnetic field problems using the finite element method. This mesh generator takes advantage of the Delaunay algorithm, which maximizes the summation of the smallest angles in all triangles and thus creates a mesh that is proved to be an optimum mesh for use in the finite element method. The dynamic data structure is explained and the source code is reviewed. The programs have been written in Pascal programming language.}
}
@article{GONCALVES2017173,
title = {A machine learning approach to the potential-field method for implicit modeling of geological structures},
journal = {Computers & Geosciences},
volume = {103},
pages = {173-182},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416304848},
author = {Ítalo Gomes Gonçalves and Sissa Kumaira and Felipe Guadagnin},
keywords = {Implicit modeling, Machine learning, Potential field, Kriging, 3D geological modeling, Compositional data analysis},
abstract = {Implicit modeling has experienced a rise in popularity over the last decade due to its advantages in terms of speed and reproducibility in comparison with manual digitization of geological structures. The potential-field method consists in interpolating a scalar function that indicates to which side of a geological boundary a given point belongs to, based on cokriging of point data and structural orientations. This work proposes a vector potential-field solution from a machine learning perspective, recasting the problem as multi-class classification, which alleviates some of the original method's assumptions. The potentials related to each geological class are interpreted in a compositional data framework. Variogram modeling is avoided through the use of maximum likelihood to train the model, and an uncertainty measure is introduced. The methodology was applied to the modeling of a sample dataset provided with the software Move™. The calculations were implemented in the R language and 3D visualizations were prepared with the rgl package.}
}
@article{GUIDEC1996239,
title = {An object-oriented framework for supercomputing},
journal = {Journal of Systems and Software},
volume = {33},
number = {3},
pages = {239-251},
year = {1996},
note = {Software Engineering for Distributed Computing},
issn = {0164-1212},
doi = {https://doi.org/10.1016/0164-1212(96)00025-8},
url = {https://www.sciencedirect.com/science/article/pii/0164121296000258},
author = {F. Guidec and J.-M. Jézéquel and J.-L. Pacherie},
abstract = {Scientific programmers are eager to take advantage of the computational power offered by Distributed Computing Systems (DCSs) but are generally reluctant to undertake the porting of their application programs onto such machines. The DCSs commercially available today are indeed widely believed to be difficult to use, which should not be a surprise because they are traditionally programmed with software tools dating back to the days of punch cards and paper tape. We claim that, provided modern object-oriented technologies are used, these computers can be programmed easily and efficiently. We propose a framework where the tricky parallel codes can be encapsulated in object-oriented software components that can be reused, combined, and customized in confidence by application programmers. We propose to use a kind of parallelism known as data-parallelism, encapsulated within classes of a purely sequential object-oriented language (Eiffel), using the SPMD (Single Program Multiple Data) programming model. We define a set of methodological rules to help a programmer design and exploit parallel software components within this framework. We illustrate our approach with the example of PALADIN, an object-oriented library devoted to linear algebra computation on DCSs. PALADIN relies on widely advertised object-oriented features to help a numerical programmer develop or parallelize application programs, following the guidelines of modern software engineering. Most notably, modularity and encapsulation are used to hide data distribution and code parallelization within classes presenting sequential interfaces, while inheritance allows a transparent reuse of basic parallel patterns in new applications. We discuss the implementation of a demonstrator of such a library as well as performance related aspects.}
}
@article{ABDALLAH2015886,
title = {Minimizing Energy Consumption and Carbon Emissions of Aging Buildings},
journal = {Procedia Engineering},
volume = {118},
pages = {886-893},
year = {2015},
note = {Defining the future of sustainability and resilience in design, engineering and construction},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.08.527},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815021827},
author = {Moatassem Abdallah and Khaled El-Rayes and Caroline Clevenger},
keywords = {Energy Consumption, Sustainability Measures, Aging Buildings, Carbon emissions},
abstract = {The building sector in the United States is responsible for 41% of energy consumption and 39% of carbon footprint while the majority of energy consumption and carbon footprint are caused by aging buildings which represent 70% of existing buildings in the United States. The energy consumption of aging buildings can be significantly reduced by identifying and implementing green building upgrade measures based on available budgets. Aging buildings are often in urgent need for upgrading to improve their operational, economic, and environmental performance. This paper presents the development of an optimization model that is capable of identifying the optimal selection of building upgrade measures to minimize energy consumption of aging buildings while complying with limited upgrade budgets and building operational performance. This optimization model is designed to estimate building energy consumption using energy simulation software packages such as eQuest and it is integrated with databases of building products. This optimization model performs analysis of replacing existing building fixtures and equipment during the optimization computations to identify the optimal replacement of building products that minimizes building energy consumption and carbon emissions. The model is designed to provide detailed results for building owners and operators, which include specifications for the recommended upgrade measures and their location in the building; upgrade cost; expected energy, operational, and life-cycle cost savings; and expected payback period. This paper illustrates the new and unique capabilities of the developed optimization model.}
}
@article{ZAZULA1994523,
title = {Computer-assisted exercise ECG analysis: real-time scheduling within MS-DOS on PCs},
journal = {Microprocessors and Microsystems},
volume = {18},
number = {9},
pages = {523-535},
year = {1994},
issn = {0141-9331},
doi = {https://doi.org/10.1016/0141-9331(94)90075-2},
url = {https://www.sciencedirect.com/science/article/pii/0141933194900752},
author = {Damjan Zazula and Andrej Šoštarič and Danilo Korẑe and Dean Korošec},
keywords = {real-time executive, multitasking, on-line exercise ECG analysis},
abstract = {Computer analysis and monitoring of exercise electrocardiograms (ECG) have been advantageously applied for a long time. Recently, the availability of personal computers (PCs) has also had a significant impact in this field. PCs have been used in the development of a software package for on-line exercise ECG analysis. This paper reveals the core of the application, i.e. a built-in real-time executive. A three-phase design approach is described: first, recognition of the tasks and their structure; second, evaluation of the task time complexities; and third, allocation of the tasks and implementation in C++ in the MS-DOS environment. The performance of the real-time multitasking environment is finally studied for the worst-case situation using a prototype PC installation.}
}
@article{COSTA2020105310,
title = {Interpopulation differences of the burrowing shrimp Callichirus major on urban beaches under different levels of fishing pressure},
journal = {Ocean & Coastal Management},
volume = {197},
pages = {105310},
year = {2020},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2020.105310},
url = {https://www.sciencedirect.com/science/article/pii/S0964569120302209},
author = {Larissa B. Costa and Nídia C.M. Marinho and Paula B. Gomes and Paulo J.P. Santos and Paulo V.V.C. Carvalho and Mônica L. Botter-Carvalho},
keywords = {Axiidea, Fishing, Mortality, Fecundity, Growth, Size structure},
abstract = {There is growing concern about the sustainability of burrowing-shrimp fisheries in many coastal areas because of their widespread use as bait and their importance as ecosystem engineers. The response of the target populations to the effect of long-term fishing pressure is still poorly understood. The present study reassessed the growth parameters, size structure and reproductive aspects of an exploited population of Callichirus major in northeastern Brazil, comparing it to a population on a beach where fishing pressure is negligible. The results are also discussed in the light of a previous study at the same site in 1999–2000. Sampling was carried out from September 2015 to May 2017 on two urban sandy beaches. Monthly length (dorsal oval) frequency data were analyzed using the FISAT II software package. Growth parameters were estimated with the ELEFAN I routine, using the von Bertalanffy seasonal growth model. The relationship between fecundity and individual size was estimated by linear regression. The exploited population had individual maximum sizes smaller than both those on the unexploited beach and in the population studied 16 years ago on the same beach. The percentages of ovigerous females were smaller than in 1999, and also differed widely between the two populations, with the exploited-beach population having one-fourth as many ovigerous females. This population showed smaller sizes at sexual maturation and low fecundity, and the females were 20% smaller. This study showed that the observed interpopulational differences in the size structure and reproductive parameters of Callichirus major can be attributed to overfishing, and indicated that the exploited population will tend toward collapse if regulatory measures to limit or prohibit fishing are not implemented.}
}
@article{MEDYCKYJSCOTT199731,
title = {The virtual map library: Providing access to Ordnance Survey digital map data via the WWW for the U.K. higher education community},
journal = {Computers, Environment and Urban Systems},
volume = {21},
number = {1},
pages = {31-45},
year = {1997},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(97)10006-0},
url = {https://www.sciencedirect.com/science/article/pii/S0198971597100060},
author = {David Medyckyj-Scott and Barbara Morris},
abstract = {For a number of reasons Higher Education Institutes (HEIs) are under increasing pressure to provide staff and students with access to digital map data. This comes at a time when many academic map libraries, the traditional map service provider, are having to cut costs and are therefore unable to fund the move to digital map provision. With funding from the Joint Information Systems Committee (JISC) Electronic Libraries (eLib) Programme, staff on the Digimap Project22The Digimap Project is supported by the JISC eLib Programme, the Data Library at the University of Edinburgh, and the Ordnance Survey. within the Data Library at the University of Edinburgh are looking at the concept of a virtual digital map library. A demonstrator service has been developed that allows individuals to browse a map database comprising Ordnance Survey (OS) digital map products, to produce hard copy output and to download data files to import into other software. Data is held centrally at the University of Edinburgh and access takes place over the World Wide Web (WWW) via a Java applet. In this paper we discuss the background to the project, describe the service and some of its features and conclude by looking at some of the issues that have arisen during the design, implementation and use of the service.}
}
@article{TAPIADOR20171,
title = {Enabling data science in the Gaia mission archive: The present-day mass function and age distribution},
journal = {Astronomy and Computing},
volume = {19},
pages = {1-15},
year = {2017},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S221313371630083X},
author = {D. Tapiador and A. Berihuete and L.M. Sarro and F. Julbe and E. Huedo},
keywords = {Scalable data science, Hierarchical Bayesian analysis, Present-day mass function, Present-day age distribution, Emcee ensemble sampler, Gaia mission},
abstract = {Recent advances in large scale computing architectures enable new opportunities to extract value out of the vast amounts of data being currently generated. However, their successful adoption is not straightforward in areas like science, as there are still some barriers that need to be overcome. Those comprise (i) the existence of legacy code that needs to be ported, (ii) the lack of high-level and use case specific frameworks that facilitate a smoother transition, or (iii) the scarcity of profiles with the balanced skill sets between the technological and scientific domains. The European Space Agency’s Gaia mission will create the largest and most precise three dimensional chart of our galaxy (the Milky Way), providing unprecedented position, parallax and proper motion measurements for about one billion stars. The successful exploitation of this data archive will depend on the ability to offer the proper infrastructure upon which scientists will be able to do exploration and modelling with this huge data set. In this paper, we present and contextualize these challenges by building two probabilistic models using Hierarchical Bayesian Modelling. These models represent a key challenge in astronomy and are of paramount importance for the Gaia mission itself. Moreover, we approach the implementation by leveraging a generic distributed processing engine through an existing software package for Markov chain Monte Carlo sampling. The two computationally intensive models are then validated with simulated data in different scenarios under specific restrictions, and their performance is assessed to prove their scalability. We argue that this approach will not only serve for the models in hand but also for exemplifying how to address similar problems in science, which may need to both scale to bigger data sets and reuse existing software as much as possible. This will lead to shorter time to science in massive data archives.}
}
@article{SEROT1995327,
title = {A Visual Dataflow Programming Environment for a Real Time Parallel Vision Machine},
journal = {Journal of Visual Languages & Computing},
volume = {6},
number = {4},
pages = {327-347},
year = {1995},
issn = {1045-926X},
doi = {https://doi.org/10.1006/jvlc.1995.1019},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X85710191},
author = {Jocelyn Sérot' and Georges Quénot and Bertrand Zavidovique},
abstract = {Programming parallel architectures dedicated to real-time image processing (IP) is often a difficult and error-prone task. This mainly results from the fact that IP algorithms typically involve several distinct processing levels and data representations, and that various execution models as well as complex hardware are needed for handling these processing layers under real-time constraints. Our goal is to permit an intuitive but still efficient handling of such an architecture by providing a continuous and readable path from the functional specification of an algorithm to its corresponding hardware implementation. For this, we developed a data-flow programming model which can act simultaneously as a functional representation of algorithms and as a structural description of their corresponding implementations on a target computer built up of 3-D interconnected data-driven processing elements (DDPs). Algorithms are decomposed into functional primitives viewed as top-level nodes of a data-flow graph (DFG). Each node is given a known physical implementation on the target architecture, either as a single DDP or as an encapsulated sub graph of DDPs, making the well known mapping problem a topological one. The target computer was built at ETCA and embeds 1024 custom data-driven processors and 12 transputers in a 3-D interconnected network. Concurrently with the machine, a complete programming environment has been developed. Relying upon a functional compiler, a large library of IP primitives and automatic place-and-route facilities, it also includes various X-Window based tools aiming at visual and efficient access to all intermediary program representations. In terms of visual languages, we try to share the burden between all the layers of this programming environment. Rather than including some display facilities in existing software environment, we have taken advantage of the intuitiveness of functional representations, even textual, and of the hardware efficiency that provides immediate results, ultimately supporting hierarchical constructs.}
}
@article{BARUA1995709,
title = {EXSHOF-II: Active filter design, from approximation function to graphic display of the circuit},
journal = {Engineering Applications of Artificial Intelligence},
volume = {8},
number = {6},
pages = {709-721},
year = {1995},
issn = {0952-1976},
doi = {https://doi.org/10.1016/0952-1976(95)00048-8},
url = {https://www.sciencedirect.com/science/article/pii/0952197695000488},
author = {Alok Barua and Nitin Patel},
keywords = {Expert system, artificial intelligence, knowledge engineering, software engineering, active filters, circuits and systems},
abstract = {This paper presents an expert-system-based package for active filter synthesis, called EXSHOF-II. It starts with approximation function and ends with graphic display of the complete filter circuit. The package is an improved and enhanced version of its mother EXSHOF (EXpert system based Synthesis of High Order Filter). EXSHOF-II gives quantitative and qualitative help at various stages of filter design. Besides the Butterworth, Chebyshev and Elliptic functions which were considered in EXSHOF, the inverse Chebyshev function is also included in EXSHOF-II. For choosing particular approximation EXSHOF-II gives the order of the transfer function. The group delay characteristics of all four approximation functions are plotted on the screen by EXSHOF-II. This aids the user to make a proper choice of an approximation function. Like its mother, EXSHOF-II also considers four high-order structures. The filtering requirement in EXSHOF-II can be directly input on the attenuation characteristics, making the system more user-friendly. The circuit implementation has been done using nine different biquadratic active RC building blocks. The delay requirement can be easily satisfied by cascaded all-pass second-order networks.}
}
@incollection{VANCUTSEM1987387,
title = {DESIGN AND IMPLEMENTATION OF AN ADVANCED STATE ESTIMATION SOFTWARE},
editor = {WANG PINGYANG},
booktitle = {Power Systems and Power Plant Control},
publisher = {Pergamon},
address = {Oxford},
pages = {387-392},
year = {1987},
series = {IFAC Symposia Series},
isbn = {978-0-08-034077-7},
doi = {https://doi.org/10.1016/B978-0-08-034077-7.50067-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080340777500676},
author = {Th. {Van Cutsem} and L. Mili and Ph. Vandeloise},
abstract = {This paper describes the design of an advanced state estimation software. The heart of this package is a fast decoupled constant gain matrix estimator. Besides the estimation itself, two satellite functions - viz. observability and bad data analyses - have been developed. The observability analysis is performed on a topological basis and a systematic observability restoration procedure is included. The treatment of bad data is carried out through the “hypothesis testing identification” method which definitely departs from classical schemes. The software has been implemented on a 60-node system monitored by a Belgian area control center.}
}
@article{PASHKEVICH19971443,
title = {Real-time inverse kinematics for robots with offset and reduced wrist},
journal = {Control Engineering Practice},
volume = {5},
number = {10},
pages = {1443-1450},
year = {1997},
issn = {0967-0661},
doi = {https://doi.org/10.1016/S0967-0661(97)00142-1},
url = {https://www.sciencedirect.com/science/article/pii/S0967066197001421},
author = {A. Pashkevich},
keywords = {Robot kinematics, iterative methods, singularities, on-line control, off-line programming, computer-aided design, industrial production systems},
abstract = {The paper deals with the inverse transformation of coordinates for robotic manipulators with high payload capacity, which are widely used in welding applications. Such manipulators are usually equipped with 3 d.o.f. offset wrists or 2 d.o.f. reduced wrists, that do not satisfy the Pieper condition that ensures the existence of a closed-form solution of the inverse kinematics. Two reliable iterative algorithms are proposed to solve the problem in real time. The algorithms have been implemented in commercial robot controllers and software packages for the computer-aided design of welding robotic cells.}
}
@article{BONAMICO2002743,
title = {Real-time MPEG-4 facial animation with 3D scalable meshes},
journal = {Signal Processing: Image Communication},
volume = {17},
number = {9},
pages = {743-757},
year = {2002},
note = {Image Processing for 3-D Imaging},
issn = {0923-5965},
doi = {https://doi.org/10.1016/S0923-5965(02)00075-9},
url = {https://www.sciencedirect.com/science/article/pii/S0923596502000759},
author = {C. Bonamico and M. Costa and F. Lavagetto and R. Pockaj},
abstract = {In this paper the complete framework of a software package for real-time animation of 3D facial models according to the MPEG-4 specifications is described. The focus is mainly on the algorithmic solution that has been adopted for scaling the polygon mesh to a level of complexity suited to the specific hw/sw profile of the platform that hosts the decoder. The basic functionality of this system is its ability to deform virtual faces in response to MPEG-4 Facial Animation Parameters (FAP) according to “animation rules” whose computation is based on very simple “semantic” information previously associated with the model. The software architecture is modular and each module, implementing a specific functionality, can be easily plugged in or unplugged. In this paper, we begin with a general presentation of the system and then we present a brief description of its main modules: the face model, the mesh animation, the mesh calibration, the mesh simplification and the FAP decoding. While for the other modules a deeper analysis is left to referenced bibliography previously published by the authors, a comprehensive description is given here of the mesh simplification block together with experimental evidence of its efficiency. The algorithm we have adopted for mesh simplification is based on iterative edge contraction with the introduction of specific geometry constraints due to the particular nature of the 3D mesh representing an MPEG-4 animated face. In this paper it will be shown how rather complex 3D facial models, optionally textured, can be simplified in real-time in order to adapt the rendering performance to the hardware capabilities, whilst still maintaining compliance with the MPEG-4 specifications on facial animation.}
}
@article{LUZARDO1986191,
title = {Dartic: Design Analysis of Real Time Controllers},
journal = {IFAC Proceedings Volumes},
volume = {19},
number = {13},
pages = {191-195},
year = {1986},
note = {IFAC Symposium on Components, Instruments and Techniques for Low Cost Automation and Applications, Valencia, Spain, 27-29 November 1986},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)59539-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017595397},
author = {J.A. Luzardo and C. Guánchez and G. Suárez and A. Otero},
keywords = {Distributed control, control algorithms, real time control, interactive software},
abstract = {This paper considers the first version of a real time control algorithms software package. The package allows the user to choice a control algorithm (univariable or multivariable) from a predefined set. Up to eight process variables can be controlled by one microcomputer (current implementation in Epson QX-10) and each microcomputer can be considered a basic controller in a computer network, i. e. a node in a distributed digital control system (DDC and centralized data logging). The package is particularly user-friendly and is currently programmed in C language. Personal computers have been incorporated in this way to handle control and data logging tasks. This fact represents considerably lower costs compared to conventional systems performing the same tasks.}
}
@article{AMIN2020107535,
title = {Experimental investigation on the motion response of a novel floating desalination plant for Egypt},
journal = {Ocean Engineering},
volume = {210},
pages = {107535},
year = {2020},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2020.107535},
url = {https://www.sciencedirect.com/science/article/pii/S0029801820305461},
author = {Islam Amin and Saishuai Dai and Selda Oterkus and Sandy Day and Erkan Oterkus},
keywords = {Offshore platform, Seawater desalination, Dynamic response of offshore structure},
abstract = {Increasing water demand in remote coastal areas in Egypt has shifted attention to the role of floating desalination plants to alleviate water shortages. A novel mobile floating desalination plant is proposed for Ras Ghareb city in Egypt. The compatibility between floater natural periods and the energy excitation range of waves in deployment area can lead to resonance responses. Therefore, understanding the motions behaviour among waves of such floaters at a particular location is important for its safety, performance and operation. The goal of the present study is to investigate the suitability of proposed concept for Egyptian environment conditions by characterising the behaviour of the plant motion responses during its entire deployment conditions. Based on full scale design of the proposed plant, experimental study over a range of wave heights and frequencies was performed in the present study using a 1:100 scale model test. In order to identify the plant natural frequency and damping ratio, free decay and swinging tests were performed for different load conditions. Different wave heights were tested during the experiments in order to investigate the effect of wave height on the heave and pitch responses at plant natural frequencies. The experimental test results were compared with numerical results from frequency domain program HydroD implemented in Sesam DNV GL software package. Results show that heave and pitch motion responses give the closest agreement between the measurements and numerical predictions over the whole range of wave periods except the peak spike. The proposed cylindrical FDP is compared against a conventional ship FPSO concept widely used for offshore freshwater production and it was concluded that new cylindrical FDP shows better motion responses compared to conventional ship FPSO concept in same wave bands in Egypt.}
}
@article{LI200079,
title = {Implementation of coordinated multiple facts controllers for damping oscillations},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {22},
number = {2},
pages = {79-92},
year = {2000},
issn = {0142-0615},
doi = {https://doi.org/10.1016/S0142-0615(99)00039-3},
url = {https://www.sciencedirect.com/science/article/pii/S0142061599000393},
author = {G.J. Li and T.T. Lie and G.B. Shrestha and K.L. Lo},
keywords = {Inter-area oscillations, Coordinated linear optimal control, Variable series compensation},
abstract = {A linear optimal controller is designed to implement multiple Variable Series Compensations (VSCs) devices in transmission network of interconnected power systems. The proposed controller is utilized to damp inter-area oscillations and enhance power system damping during large disturbances. The effectiveness of the linear optimal controller to properly control such devices is demonstrated for a two-machine power system. Results from (i) digital simulation studies using a PSCAD/EMTDC software package, and (ii) real-time digital simulation studies using a PSCAD/RTDS, are discussed. The simulation results show that the controller contributes significantly to the damping of inter-area oscillations and the enhancement of power system damping. Moreover, the results of these different simulation studies agree quite well showing little discrepancies between them.}
}
@article{CHEN2013153,
title = {A new approach to stability analysis of frame structures using Trefftz-type elements},
journal = {Journal of Constructional Steel Research},
volume = {82},
pages = {153-163},
year = {2013},
issn = {0143-974X},
doi = {https://doi.org/10.1016/j.jcsr.2012.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0143974X13000096},
author = {Taicong Chen and Haitao Ma and Wei Gao},
keywords = {Finite element method, Trefftz element, Stability analysis, Rayleigh's quotient,  error estimation},
abstract = {This paper presents some new developments in structural stability analysis. A novel Trefftz-type finite element and efficient algorithms are proposed for calculating numerically exact solutions for frame structures. New shape functions are constructed by using general solutions of the homogeneous governing equation and an exact Trefftz element is formulated. An iterative algorithm based on conventional eigenvalue extraction method is then proposed, with which very accurate solutions can be obtained efficiently by using coarse finite element meshes. Further, an efficient method based on the new element matrices and Rayleigh's quotient is developed for a posteriori error estimation and solution improvement. Numerical examples are presented to demonstrate the effectiveness of the proposed element and algorithms. Finally, concluding remarks are made including those on the further extension of the approach.}
}
@incollection{IRANI1995299,
title = {Production flow analysis using STORM},
editor = {Ali K. Kamrani and Hamid R. Parsaei and Donald H. Liles},
series = {Manufacturing Research and Technology},
publisher = {Elsevier},
volume = {24},
pages = {299-349},
year = {1995},
booktitle = {Planning, Design, and Analysis of Cellular Manufacturing Systems},
issn = {1572-4417},
doi = {https://doi.org/10.1016/S1572-4417(06)80047-1},
url = {https://www.sciencedirect.com/science/article/pii/S1572441706800471},
author = {S.A. Irani and R. Ramakrishnan},
abstract = {Production Flow Analysis (PFA) is a systematic manual method for cell formation and layout design. However, even after nearly thirty years since its appearance in the literature, there is no commercially available computer software for it. This is surprising, especially when there is at least one analytical method available in the literature that would solve each phase as accurately as would a human analyst. This chapter demonstrates a step-by-step implementation of the first three phases in PFA (Factory Flow Analysis, Group Analysis and Line Analysis) using standard algorithms available in the STORM package. A sample data set from the literature was used for illustrative purposes. Data collection sheets, analysis sheets and typical results expected from each step are presented. Companies interested in implementing cells will find that these analytical methods effectively complement their manual analyses.}
}
@article{ARAUJO2020102661,
title = {A multi-improvement local search using dataflow and GPU to solve the minimum latency problem},
journal = {Parallel Computing},
volume = {98},
pages = {102661},
year = {2020},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2020.102661},
url = {https://www.sciencedirect.com/science/article/pii/S0167819120300545},
author = {Rodolfo Pereira Araujo and Igor Machado Coelho and Leandro Augusto Justen Marzulo},
keywords = {Dataflow, Graphics processing unit, Metaheuristics, Local search, Variable neighborhood descent},
abstract = {Optimization problems have great importance in the industrial field, specially for supply chain management and transportation of goods. Many of these problems are classified as NP-Hard, thus there is no known algorithm to find their exact (global optimal) solutions in polynomial time. Therefore, fast heuristic strategies are generally employed, specially those with the ability to escape from poor quality local optima, called metaheuristics. In general, the Local Search is the most computationally expensive phase of a metaheuristic, thus requiring a good use of all available computing resources. In this work, we explore state-of-the-art GPU processing local search modules (called neighborhood structures) from literature, together with a proposed Dataflow model with Multiple Output Gates. Although these neighborhood structures are classic for routing problems in literature, these are typically explored in a sequential manner, named Variable Neighborhood Descent. In this work, we demonstrate how to use these neighborhoods on multiple collaborative computing devices, building novel efficient Local Searches for a challenging optimization problem: the Minimum Latency Problem. Finally, we present experiments with the proposed distributed strategies on the Minimum Latency Problem, indicating the gains over previously proposed sequential/parallel approaches in literature, and also the current limitations to deal with larger problem instances.}
}
@article{SCHMITZ1999201,
title = {3D ray tracing in austenite materials},
journal = {NDT & E International},
volume = {32},
number = {4},
pages = {201-213},
year = {1999},
issn = {0963-8695},
doi = {https://doi.org/10.1016/S0963-8695(98)00047-4},
url = {https://www.sciencedirect.com/science/article/pii/S0963869598000474},
author = {V. Schmitz and F. Walte and S.V. Chakhlov},
keywords = {Ultrasound, Austenite material, Stainless steel, Anisotropy, Inhomogeneous, Ray tracing},
abstract = {In general, ultrasonic sound wave propagation through austerlitic (stainless steel) weld material cannot be predicted because of the anisotropic and inhomogeneous structure of the weld. To support NDT inspections, a ray tracing algorithm was developed to follow longitudinal, horizontal and vertical polarized shear wave propagation from the base material through the cladding in three dimensions. The algorithm was implemented in the `3D-Ray-SAFT' software package that allows experimental modelling, to visualize the rays in three-dimensions and to include reflections from arbitrarily placed flaws. The presented algorithm results that were experimentally validated are compared to similar, existing algorithms.}
}
@article{SCHMID1985565,
title = {A Workstation Concept for Computer Aided Analysis and Design of Control Systems},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {11},
pages = {565-568},
year = {1985},
note = {7th IFAC/IFIP/IMACS Conference on Digital Computer Applications to Process Control, Vienna, Austria, 17-20 September 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60184-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017601848},
author = {C. Schmid},
keywords = {Computer-aided system design, program package, CAD workstation, control system analysis, control system synthesis},
abstract = {The KEDDC software package for computer-aided analysis and control systems design, developed at Ruhr-University, Bochum, F.R.G. is presently being prepared for a supermicro-based workstation implementation. A bit map display plays a key role for the design of a concurrent user environment, where mixed alphanumerics and graphics arc used to handle a comprehensive system of control tools in a natural way for control engineers.}
}
@article{TSACLE1996447,
title = {An expert system model for implementing statistical process control in the health care industry},
journal = {Computers & Industrial Engineering},
volume = {31},
number = {1},
pages = {447-450},
year = {1996},
note = {Proceedings of the 19th International Conference on Computers and Industrial Engineering},
issn = {0360-8352},
doi = {https://doi.org/10.1016/0360-8352(96)00061-7},
url = {https://www.sciencedirect.com/science/article/pii/0360835296000617},
author = {E.G. Tsacle and N.A. Aly},
keywords = {Statistical Process Control, Expert System, Control Chart, Process, Process Indicators},
abstract = {Today's health care industry is under increased pressure to become more efficient and cost effective. In addition, hospitals are now required to adopt the techniques and methods of Continuous Quality Improvement (CQI) as part of their accreditation requirements. One of the main challenges facing health care providers implementing CQI is how to manage, control and improve processes using Statistical Process Control (SPC) techniques, especially control charts. This paper describes the development of an SPC expert system which is designed to advise hospital personnel how to measure and control their processes effectively using different types of control charts such as X-bar and R-charts, P-charts, IC-charts and Individual-X and Moving charts. The SPC expert system is implemented in GURU, a menu driven, totally integrated expert system development tool. The structure of the SPC expert system is described and examples, using actual data from a local hospital are presented. A complete step-by-step interactive session with the expert system is also shown. Finally, the effectiveness of the SPC Expert System is evaluated and the feasibility of linking the expert system directly to SPC software packages is explored.}
}
@incollection{LARSEN2012387,
title = {Chapter 28 - Jacket: GPU Powered MATLAB Acceleration},
editor = {Wen-mei W. Hwu},
booktitle = {GPU Computing Gems Jade Edition},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {387-398},
year = {2012},
series = {Applications of GPU Computing Series},
isbn = {978-0-12-385963-1},
doi = {https://doi.org/10.1016/B978-0-12-385963-1.00028-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859631000289},
author = {Torben Larsen and Gallagher Pryor and James Malcolm},
abstract = {Publisher Summary
Jacket is a software platform developed at AccelerEyes, which allows users to execute MATrix LABoratory (MATLAB) M-code on CUDA-capable graphics processing units (GPUs). MATLAB by the MathWorks is a standard platform for technical computing and graphics in science, engineering, and finance. The combination of a simple matrix language, interactive prompt, automatic memory management, and on-the-fly compilation make MATLAB well suited to rapid prototyping of algorithms and exploring data. MATLAB's one drawback is performance, and Jacket alleviates this by seamless offloading of computations to the GPU. Jacket provides users access to a set of libraries, functions, and tools that facilitate numerical computation on the GPU including multi-GPU support built on MATLAB's Parallel Computing Toolbox and Distributed Computing Server. Jacket has been designed for programmers who have large data-parallel tasks but who are not low level programmers accustomed to dealing with GPU-specific constructs. Once data is marked as “GPU” data using these functions, Jacket provides native GPU implementations of a large set of the standard MATLAB functions to operate on that data. Jacket achieves transparency by defining a new set of classes dubbed “g” objects, where each element of this set corresponds to a base class of the MATLAB standard interface—single, uint16, ones, etc. map to gsingle, guint16, gones, etc. The Jacket architecture uses object-oriented programming to handle references to data. Jacket includes a graphics toolbox that provides a simple method of displaying computational results on the GPU without bringing those results back to the host.}
}
@article{DONG2009391,
title = {Parameter reconstruction of vibration systems from partial eigeninformation},
journal = {Journal of Sound and Vibration},
volume = {327},
number = {3},
pages = {391-401},
year = {2009},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2009.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X09005513},
author = {Bo Dong and Matthew M. Lin and Moody T. Chu},
abstract = {Quadratic matrix polynomials are fundamental to vibration analysis. Because of the predetermined interconnectivity among the constituent elements and the mandatory nonnegativity of the physical parameters, most given vibration systems will impose some inherent structure on the coefficients of the corresponding quadratic matrix polynomials. In the inverse problem of reconstructing a vibration system from its observed or desirable dynamical behavior, respecting the intrinsic structure becomes important and challenging both theoretically and practically. The issue of whether a structured inverse eigenvalue problem is solvable is problem dependent and has to be addressed structure by structure. In an earlier work, physical systems that can be modeled under the paradigm of a serially linked mass–spring system have been considered via specifically formulated inequality systems. In this paper, the framework is generalized to arbitrary generally linked systems. In particular, given any configuration of interconnectivity in a mass–spring system, this paper presents a mechanism that systematically and automatically generates a corresponding inequality system. A numerical approach is proposed to determine whether the inverse problem is solvable and, if it is so, computes the coefficient matrices while providing an estimate of the residual error. The most important feature of this approach is that it is problem independent, that is, the approach is general and robust for any kind of physical configuration. The ideas discussed in this paper have been implemented into a software package by which some numerical experiments are reported.}
}
@article{FREDRIKSSON201977,
title = {Optimal placement of Charging Stations for Electric Vehicles in large-scale Transportation Networks},
journal = {Procedia Computer Science},
volume = {160},
pages = {77-84},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.446},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919316618},
author = {Henrik Fredriksson and Mattias Dahl and Johan Holmgren},
keywords = {charging stations, electric vehicle, transportation network, optimal placement, route node coverage},
abstract = {This paper presents a new practical approach to optimally allocate charging stations in large-scale transportation networks for electric vehicles (EVs). The problem is of particular importance to meet the charging demand of the growing fleet of alternative fuel vehicles. Considering the limited driving range of EVs, there is need to supply EV owners with accessible charging stations to reduce their range anxiety. The aim of the Route Node Coverage (RNC) problem, which is considered in the current paper, is to find the minimum number of charging stations, and their locations in order to cover the most probable routes in a transportation network. We propose an iterative approximation technique for RNC, where the associated Integer Problem (IP) is solved by exploiting a probabilistic random walk route selection, and thereby taking advantage of the numerical stability and efficiency of the standard IP software packages. Furthermore, our iterative RNC optimization procedure is both pertinent and straightforward to implement in computer coding and the design technique is therefore highly applicable. The proposed optimization technique is applied on the Sioux-Falls test transportation network, and in a large-scale case study covering the southern part of Sweden, where the focus is on reaching the maximum coverage with a minimum number of charging stations. The results are promising and show that the flexibility, smart route selection, and numerical efficiency of the proposed design technique, can pick out strategic locations for charging stations from thousands of possible locations without numerical difficulties.}
}
@article{DZWIERZYNSKA20161608,
title = {Direct Construction of an Inverse Panorama from a Moving View Point},
journal = {Procedia Engineering},
volume = {161},
pages = {1608-1614},
year = {2016},
note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning Symposium 2016, WMCAUS 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.634},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816328636},
author = {Jolanta Dzwierzynska},
keywords = {inverse panorama, moving view, panoramic projection, objecft location},
abstract = {The aim of the herby study was a direct and practical mapping of an inverse cylindrical panorama with computer aid. An inverse panoramic projection it is the projection onto a cylindrical, rotary surface or on a fragment of this surface, in which the center of projection is not located like in a typical cylindrical panorama “inside” the cylindrical surface but outside of it. Spreading this idea in the paper, it was taken into consideration the kind of the inverse cylindrical projection from a view point being not stationary but moving. This representation was defined as a multicentre projection from the centres dispersed on a line path which could be straight or curved. Such an approach gave maximum approximation of the received results of the considered projection to real perception one experienced observing the image. The graphical mapping the effects of the representation could be realized directly on the unfolded surface - flat background of the projection. That is due to the projective and graphical connection between points displayed on the cylindrically curved background and their counterparts received on the unrolled flat surface. It allowed to develop a descriptive method for creating edge images of given objects. However, for a significant improvement of the construction of lines, the analytical algorithms were formulated in Mathcad software. Still, they can be implemented in majority of the computer graphical packages, which makes drawing panoramas more efficient and easier. The presented inverse panoramic representation, and the way of its mapping directly on the unrolled flat background can find application in different representations of architectural space in advertisement and art when drawings are displayed on the cylindrically curved surfaces.}
}
@article{MORARI20011,
title = {Hybrid system analysis and control via mixed integer optimization},
journal = {IFAC Proceedings Volumes},
volume = {34},
number = {25},
pages = {1-12},
year = {2001},
note = {6th IFAC Symposium on Dynamics and Control of Process Systems 2001, Jejudo Island, Korea, 4-6 June 2001},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)33796-5},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017337965},
author = {Manfred Morari},
keywords = {Hybrid systems, predictive control, dynamic models, mixed-integer programming, optimization problems},
abstract = {The paper discusses a framework for modeling, analyzing and controlling systems whose behavior is governed by interdependent physical laws, logic rules, and operating constraints, denoted as Mixed Logical Dynamical (MLD) systems. They are described by linear dynamic equations subject to linear inequalities involving real and integer variables. MLD models are equivalent to various other system descriptions like Piece Wise Affine (PWA) systems and Linear Complementarity (LC) systems. They have the advantage, however, that all problems of system analysis (like controllability, observability, stability and verification) and all problems of synthesis (like controller design and filter design) can be readily expressed as mixed integer linear or quadratic programs, for which many commercial software packages exist. In this paper we first recall how to derive MLD models and then illustrate their use in predictive control. Subsequently we define “verification” and show how verification algorithms can be used to solve a variety of practical problems like checking the correctness of an emergency shutdown procedure implemented on a PLC, or assessing the performance of a constrained MPC controller. The eventual practical success of these methods will depend on progress in the development of the various optimization packages so that problems of realistic size can be tackled.}
}
@article{PORKAR2010828,
title = {A novel optimal distribution system planning framework implementing distributed generation in a deregulated electricity market},
journal = {Electric Power Systems Research},
volume = {80},
number = {7},
pages = {828-837},
year = {2010},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2009.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378779609003046},
author = {S. Porkar and P. Poure and A. Abbaspour-Tehrani-fard and S. Saadate},
keywords = {Distributed generation (DG), Distribution system planning (DSP), Distribution company (DISCO), GAMS–MATLAB interface, Heuristic approach},
abstract = {This paper introduces a new framework included mathematical model and a new software package interfacing two powerful softwares (MATLAB and GAMS) for obtaining the optimal distributed generation (DG) capacity sizing and sitting investments with capability to simulate large distribution system planning. The proposed optimization model allows minimizing total system planning costs for DG investment, DG operation and maintenance, purchase of power by the distribution companies (DISCOs) from transmission companies (TRANSCOs) and system power losses. The proposed model provides not only the DG size and site but also the new market price as well. Three different cases depending on system conditions and three different scenarios depending on different planning alternatives and electrical market structures, have been considered. They have allowed validating the economical and electrical benefits of introducing DG by solving the distribution system planning problem and by improving power quality of distribution system. DG installation increases the feeders’ lifetime by reducing their loading and adds the benefit of using the existing distribution system for further load growth without the need for feeders upgrading. More, by investing in DG, the DISCO can minimize its total planning cost and reduce its customers’ bills.}
}
@article{ZIAVRAS2007235,
title = {Coprocessor design to support MPI primitives in configurable multiprocessors},
journal = {Integration},
volume = {40},
number = {3},
pages = {235-252},
year = {2007},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2005.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167926005000519},
author = {Sotirios G. Ziavras and Alexandros V. Gerbessiotis and Rohan Bafna},
keywords = {Configurable system, FPGA, Multiprocessor, MPI},
abstract = {The Message Passing Interface (MPI) is a widely used standard for interprocessor communications in parallel computers and PC clusters. Its functions are normally implemented in software due to their enormity and complexity, thus resulting in large communication latencies. Limited hardware support for MPI is sometimes available in expensive systems. Reconfigurable computing has recently reached rewarding levels that enable the embedding of programmable parallel systems of respectable size inside one or more Field-Programmable Gate Arrays (FPGAs). Nevertheless, specialized components must be built to support interprocessor communications in these FPGA-based designs, and the resulting code may be difficult to port to other reconfigurable platforms. In addition, performance comparison with conventional parallel computers and PC clusters is very cumbersome or impossible since the latter often employ MPI or similar communication libraries. The introduction of a hardware design to implement directly MPI primitives in configurable multiprocessor computing creates a framework for efficient parallel code development involving data exchanges independently of the underlying hardware implementation. This process also supports the portability of MPI-based code developed for more conventional platforms. This paper takes advantage of the effectiveness and efficiency of one-sided Remote Memory Access (RMA) communications, and presents the design and evaluation of a coprocessor that implements a set of MPI primitives for RMA. These primitives form a universal and orthogonal set that can be used to implement any other MPI function. To evaluate the coprocessor, a router of low latency was designed as well to enable the direct interconnection of several coprocessors in cluster-on-a-chip systems. Experimental results justify the implementation of the MPI primitives in hardware to support parallel programming in reconfigurable computing. Under continuous traffic, results for a Xilinx XC2V6000 FPGA show that the average transmission time per 32-bit word is about 1.35 clock cycles. Although other computing platforms, such as PC clusters, could benefit as well from our design methodology, our focus is exclusively reconfigurable multiprocessing that has recently received tremendous attention in academia and industry.}
}
@article{LAPORTE1989241,
title = {On the design of an expert system guide for the use of scientific software},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {75},
number = {1},
pages = {241-250},
year = {1989},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(89)90027-3},
url = {https://www.sciencedirect.com/science/article/pii/0045782589900273},
author = {François Laporte},
abstract = {The use of scientific software libraries often creates problems in the management and choice of suitable software component(s) for a given case. This document describes an expert system facilitating the choice of the most suitable component(s) for a given problem. First, the system takes the physical characteristics of a given problem and determines a suitable theoretical framework within which to treat the problem. It is possible to make this determination by means of a vast knowledge base, broken down into several rule groups and exploited using meta-knowledge of this structure. Taking the user's needs into consideration (expected quality of results and available data processing resources), the system evaluates performances of components which are compatible with the theoretical framework selected, using a knowledge base which corresponds to each component. The components are thus sorted according to their suitability for solving the user's problem. This methodology is implemented in a mock-up dedicated to space mechanics, for the precise evaluation of satellite motion.}
}
@article{BOOTY2001453,
title = {Design and implementation of an environmental decision support system},
journal = {Environmental Modelling & Software},
volume = {16},
number = {5},
pages = {453-458},
year = {2001},
note = {Design principles for environmental information systems},
issn = {1364-8152},
doi = {https://doi.org/10.1016/S1364-8152(01)00016-0},
url = {https://www.sciencedirect.com/science/article/pii/S1364815201000160},
author = {W.G Booty and D.C.L Lam and I.W.S Wong and P Siconolfi},
keywords = {Decision support system, Environmental modelling, Great Lakes},
abstract = {An environmental decision support system is a specific version of an environmental information system that is designed to help decision makers, managers, and advisors locate relevant information and carry out optimal solutions to problems using special tools and knowledge. The RAISON (Regional Analysis by Intelligent Systems ON microcomputers) for Windows decision support system has been developed at the National Water Research Institute, Environment Canada, over the last 10 years. It integrates data, text, maps, satellite images, pictures, video and other knowledge input. A library of software functions and tools are available for selective extraction of spatial and temporal data that can be analysed using spatial algorithms, models, statistics, expert systems, neural networks, and other information technologies. The system is of a modular design which allows for flexibility in modification of the system to meet the demands of a wide range of applications. System design and practical experiences learned in the development of a decision support system for toxic chemicals in the Great Lakes of North America are discussed.}
}
@article{BEISKER199415,
title = {A new combined integral-light and slit-scan data analysis system (DAS) for flow cytometry},
journal = {Computer Methods and Programs in Biomedicine},
volume = {42},
number = {1},
pages = {15-26},
year = {1994},
issn = {0169-2607},
doi = {https://doi.org/10.1016/0169-2607(94)90134-1},
url = {https://www.sciencedirect.com/science/article/pii/0169260794901341},
author = {Wolfgang Beisker},
keywords = {Flow cytometry, Slit-scan, Multiparameter data analysis and graphics, PC},
abstract = {Flow cytometry using list mode parameters such as fluorescence emission, light scatter and size on one hand and different slit-scan parameters on the other hand needs a fast, flexible, efficient and easy-to-use data analysis software. A new software package (data analysis system, DAS) has been developed that integrates data analysis for conventional (integral-light) flow cytometry and for slit-scan flow cytometry. The requirements, design and some examples are discussed and an implementation for IBM-compatible computers is presented. Special attention is directed to the handling of different data types from one-parameter histograms to multiparameter slit-scan data files. The package can be used as an interpreting programming language or as an interactive menu-driven command line interpreter with a large number of graphic, mathematical and statistical functions. DAS is not limited to use in flow cytometry only, but multidimensional data analysis, from astronomy to economics, can be done as well.}
}
@incollection{BRUIJN198735,
title = {SIMULATION AND REALISATION OF IN-LINE CONTROL ALGORITHMS},
editor = {I. TROCH and P. KOPACEK and F. BREITENECKER},
booktitle = {Simulation of Control Systems},
publisher = {Pergamon},
address = {Oxford},
pages = {35-42},
year = {1987},
series = {IFAC Symposia Series},
isbn = {978-0-08-034349-5},
doi = {https://doi.org/10.1016/B978-0-08-034349-5.50011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080343495500111},
author = {P.M. Bruijn and J. Cser and A.R.M. Soeterboek},
abstract = {Two program packages are described for the simulation and the realisation in real time of control structures described by block diagrams. The blocks represent the algorithms, the control functions and the simulation of processes during the design stage. The topology is defined by the connections of the blocks. In the package MUSIC a set of standard blocks is available and the user can write in Fortran his own blocks using a standard software interface. The interaction between the user and the control structure is performed via a separate command interface program. A graphic display facility facilitates the evaluation of results and the comparison of a set of solutions. The whole system is supported by a user-friendly menu and runs on PDP-11 and microVAX computers. The second package, called FOCOP, is written in Forth and offers the possibility to combine the basic blocks in user-defined modules to form more complex transfer functions, which can be repeatedly used. Due to the choice of Forth it is relatively simple to implement the software on different computers. At the moment versions are available on 6502 and Z80 microprocessors and on PDP-11 minicomputers.}
}