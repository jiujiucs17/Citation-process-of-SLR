@article{CHIN2022107742,
title = {Integrated software suite for heat recovery networks and equipment design},
journal = {Computers & Chemical Engineering},
volume = {161},
pages = {107742},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107742},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422000837},
author = {Hon Huin Chin and Bohong Wang and Xuexiu Jia and Min Zeng and Vít Freisleben and Petar Sabev Varbanov and Jiří Jaromír Klemeš},
keywords = {Heat exchanger, Design and rating, Waste gas compact thermal oxidiser (WGCTO), Heat exchanger network, Software module},
abstract = {Heat Integration (HI) with heat exchangers requires intimate knowledge and intensive calculations to set energy-saving targets to achieve the goal of efficient energy utilisation. The Shell and Tube Heat Exchanger (STHX) Design Software Module, or SPIL-HX package developed by the SPIL team, offers features that support engineers to design and rate original STHXs, STHXs coupled with the shell-side enhancement technologies, i.e., with segmented baffles and with helical baffles. This software tool also provides design solutions for the Waste Gas Compact Thermal Oxidiser (WGCTO), identifying the unit design capable of simultaneous combustion and heat exchange for waste gases. SPIL-HX package is also capable of automatic Heat Exchanger Network (HEN) synthesis and retrofit. With the built-in algorithm, the software can provide the optimal solution for HI, which significantly reduces operating, capital, and design costs. A user-friendly interface facilitates the use of the developed software.}
}
@article{AZPEITIA2020101468,
title = {Volunteering for Linked Data Wrapper maintenance: A platform perspective},
journal = {Information Systems},
volume = {89},
pages = {101468},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101468},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919305204},
author = {Iker Azpeitia and Jon Iturrioz and Oscar Díaz},
keywords = {Linked Data Wrappers, Maintenance, Open platforms, Volunteering, YQL},
abstract = {Linked Data Wrappers (LDWs) turn Web APIs into RDF end-points, leveraging the Linked Open Data cloud with current data. Unfortunately, LDWs are fragile upon upgrades on the underlying APIs, compromising LDW stability. Hence, for API-based LDWs to become a sustainable foundation for the Web of Data, we should recognize LDW maintenance as a continuous effort that outlives their breakout projects. This is not new in Software Engineering. Other projects in the past faced similar issues. The strategy: becoming open source and turning towards dedicated platforms. By making LDWs open, we permit others not only to inspect (hence, increasing trust and consumption), but also to maintain (to cope with API upgrades) and reuse (to adapt for their own purposes). Promoting consumption, adaptation and reuse might all help to increase the user base, and in so doing, might provide the critical mass of volunteers, current LDW projects lack. Drawing upon the Helping Theory, we investigate three enablers of volunteering applied to LDW maintenance: impetus to respond, positive evaluation of contributing and increasing awareness. Insights are fleshed out through SYQL, a LDW platform on top of Yahoo’s YQL. Specifically, SYQL capitalizes on the YQL community (i.e. impetus to respond), providesannotation overlays to easy participation (i.e. positive evaluation of contributing), and introduces aHealth Checker (i.e. increasing awareness). Evaluation is conducted for 12 subjects involved in maintaining someone else’s LDWs. Results indicate that both the Health Checker and the annotation overlays provide utility as enablers of awareness and contribution.}
}
@article{GRAY1985441,
title = {The Development of the Software for a Microprocessor-based Multivariable Controller},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {11},
pages = {441-446},
year = {1985},
note = {7th IFAC/IFIP/IMACS Conference on Digital Computer Applications to Process Control, Vienna, Austria, 17-20 September 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60165-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017601654},
author = {G.T. Gray and M. Braae},
abstract = {The software for a microprocessor-based multivariable controller, which controls the grinding circuit on a gold-recovery plant, has been developed. The process and the microprocessor hardware are surveyed briefly, and the functional requirements of the software are discussed. The necessity for a real-time executive software package is explained, the selection of such a package and its resulting performance are described and an account is given of the various languages and software utilities that were used in the preparation of the software. Special mention is made of the real-time nature of the software and the problems that were encountered regarding aspects such as reentrant procedures and shared data structures. The results of the four stages of testing that were carried out show clearly that the development laboratory should have a simulation of the process to be controlled. Finally the possibility is explored that one-off microprocessor-based devices can be developed that will be economically justifiable.}
}
@article{GOWDAV2022100517,
title = {Industrial quality healthcare services using Internet of Things and fog computing approach},
journal = {Measurement: Sensors},
volume = {24},
pages = {100517},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100517},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001519},
author = {Dankan {Gowda V} and Avinash Sharma and B. Kameswara Rao and Ravi Shankar and Parismita Sarma and Abhay Chaturvedi and Naziya Hussain},
keywords = {Internet of things, Quality of services, Healthcare, Fog computing, ECG, Augmented data recognition (ADR) and accident reduction model (ARM)},
abstract = {Healthcare in general refers to services provided for preservation or enhancement of health through anticipation, finding, treatment, recovery, or healing of disease, illness, injury, or any physical and mental impairment in humans. Quality of Services in Healthcare Systems may be characterised broadly through parameters such as accuracy of data, speed of decision-making, timely treatment, security of data, real-time monitoring and controlling of the health systems, failure handling and quality of life. In this research work, an attempt is made to achieve the Quality of Services in Healthcare through IoT and Fog Computing, considering optimization of one or more of these parameters. An integrated hardware system with software programs is developed for EHS safety using microcontroller. The Processing Packages are designed to communicate through the Internet of Things (IoT) based on the Accident Reduction Model (ARM) and Augmented Data Recognition (ADR) system.When ECG signals are sensed through IoT and sent to the cloud for the analysis purpose, delays are caused by the time results reach the medical staff. This delay in decision making is due to the transmission delay and processing delays. To overcome these issues, Fog Computing can be used.}
}
@article{KODHELAJ201980,
title = {Designing and deploying a business process for product recovery and repair at a servicing organization: A case study and framework proposal},
journal = {Computers in Industry},
volume = {105},
pages = {80-98},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518300885},
author = {Iva Kodhelaj and Claudia-Melania Chituc and Erik Beunders and Dave Janssen},
keywords = {Business process, Product recovery and repair, Harvesting parts of medical equipment, Enterprise information system, Case study, Servicing organization},
abstract = {The environmental concerns and the benefits associated with product recovery have influenced companies to focus on the remanufacturing and recycling of discarded products. Although associated with numerous benefits, this practice is still not largely applied in the service business. The purpose of this article is to present a framework for designing and deploying a business process for product recovery and repair at a servicing organization. The proposed framework comprises seven elements (Organization, Quality & Regulations, Information Technology, Purchasing, Finance, Processes, and Implementation), and their sequence corresponds to the actual steps towards designing and deploying a harvesting business process. The proposed framework was empirically validated for specific scenarios at a servicing organization in the Netherlands, illustrating how the business process for harvesting parts of medical equipment is enabled through SAP ERP transactions, while addressing elicited requirements. The tests executed in the SAP MBQ software environment showed that the proposed solution is supported in a real business environment, illustrating its practical relevance. This framework can easily be applied in different sectors. The work pursued has a scientific contribution and practical use in industry, providing a unique niche for researchers, practitioners, information technology managers and developers to make significant contributions, serving as a guiding tool, and supporting them in understanding the practical implications of deploying a harvesting business process and systematically managing it.}
}
@article{CIZNICKI201490,
title = {Benchmarking JPEG 2000 implementations on modern CPU and GPU architectures},
journal = {Journal of Computational Science},
volume = {5},
number = {2},
pages = {90-98},
year = {2014},
note = {Empowering Science through Computing + BioInspired Computing},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2013.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877750313000410},
author = {Miłosz Ciżnicki and Michał Kierzynka and Piotr Kopta and Krzysztof Kurowski and Paweł Gepner},
keywords = {GPU, Multi-core CPU, JPEG 2000, Signal processing},
abstract = {The use of graphics hardware for non-graphics applications has become popular among many scientific programmers and researchers as we have observed a higher rate of theoretical performance increase than the CPUs in recent years. However, performance gains may be easily lost in the context of a specific parallel application due to various both hardware and software factors. JPEG 2000 is a complex standard for data compression and coding, that provides many advanced capabilities demanded by more specialized applications. There are several JPEG 2000 implementations that utilize emerging parallel architectures with the built-in support for parallelism at different levels. Unfortunately, many available implementations are only optimized for a certain parallel architecture or they do not take advantage of recent capabilities provided by modern hardware and low level APIs. Thus, the main aim of this paper is to present a comprehensive real performance analysis of JPEG 2000. It consists of a chain of data and compute intensive tasks that can be treated as good examples of software benchmarks for modern parallel hardware architectures. In this paper we compare achieved performance results of various JPEG 2000 implementations executed on selected architectures for different data sets to identify possible bottlenecks. We discuss also best practices and advices for parallel software development to help users to evaluate in advance and then select appropriate solutions to accelerate the execution of their applications.}
}
@article{ZAPALOWSKI2018125,
title = {The WGB method to recover implemented architectural rules},
journal = {Information and Software Technology},
volume = {103},
pages = {125-137},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301253},
author = {Vanius Zapalowski and Ingrid Nunes and Daltro José Nunes},
keywords = {Software architecture, Architectural rule, Source code dependency, Architecture recovery},
abstract = {Context: The identification of architectural rules, which specify allowed dependencies among architectural modules, is a key challenge in software architecture recovery. Existing approaches either retrieve a large set of rules, compromising their practical use, or are limited to supporting the understanding of such rules, which are manually recovered. Objective: To propose and evaluate a method to recover architectural rules, focusing on those implemented in the source code, which may differ from planned or conceptual rules. Method: We propose the WGB method, which analyzes dependencies among architectural modules as a graph, adding weights that correspond to the proposed module dependency strength (MDS) metric and identifies the set of implemented architectural rules by solving a mathematical optimization problem. We evaluated our method with a case study and an empirical study that compared rules extracted by the method with the conceptual architecture and source code dependencies of six systems. These comparisons considered efficiency and effectiveness of our method. Results: Regarding efficiency, our method took 45.55 s to analyze the largest system evaluated. Considering effectiveness, our method captured package dependencies as extracted rules with a reduction of 87.6%, on average, to represent this information. Using allowed architectural dependencies as a reference point (but not a gold standard), provided rules achieved 37.1% of precision and 37.8% of recall. Conclusion: Our empirical evaluation shows that the implemented architectural rules recovered by our method consist of abstract representations of (a large number of) module dependencies, providing a concise view of dependencies that can be inspected by developers to identify occurrences of architectural violations and undocumented rules.}
}
@article{WAGNER2004305,
title = {‘Best’ for whom?: the tension between ‘best practice’ ERP packages and diverse epistemic cultures in a university context},
journal = {The Journal of Strategic Information Systems},
volume = {13},
number = {4},
pages = {305-328},
year = {2004},
note = {Special issue "Understanding the Contextual Influences on Enterprise System Design, Implementation, Use and Evaluation"},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2004.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0963868704000472},
author = {Erica L. Wagner and Sue Newell},
keywords = {‘Best practice’, ERP software, Epistemic cultures, University, Context, Interpretive research, Longitudinal research},
abstract = {The idea that so-called ‘best’ business practices can be transferred to organizations when they purchase enterprise resource planning (ERP) software packages is a major selling point of these packages. Yet recent research has illustrated a gap between the espoused theory of a best practice solution and the theory-in-use experienced by those who install software with such a design. As researchers begin to examine the difficult process by which organizations recast the best practices model handed down to them by consultancies and software vendors in an effort to make the software ‘work for them’ in practice, it is equally important that we begin to understand the reasons that such a gap exists. To this end, we analyze the strategic partnership between a multinational software vendor and a university who together designed a ‘best practice’ ERP package for the higher education industry. Through the theoretical lens of ‘epistemic cultures’ we argue that in organizational contexts made up of more than one epistemic culture, the use of a best practice model will be problematic because, by definition, the model mandates one epistemological position through the software design. This is counter to a university's loosely coupled organizational form.}
}
@incollection{CLERCQ2004441,
title = {13 - Introducing Windows Server 2003 Public Key Infrastructure},
editor = {Jan De Clercq},
booktitle = {Windows Server 2003 Security Infrastructures},
publisher = {Digital Press},
address = {Burlington},
pages = {441-491},
year = {2004},
series = {HP Technologies},
isbn = {978-1-55558-283-8},
doi = {https://doi.org/10.1016/B978-155558283-8/50016-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781555582838500161},
author = {Jan De Clercq},
abstract = {Publisher Summary
This chapter focuses on the added value of using Microsoft Windows Server 2003 public key infrastructure (PKI) as the building block for advanced IT security in organizations. The chapter also takes a closer look at all of its core components: the certificate server, the CryptoAPI, the data protection API, and the active directory. The Windows Server 2003 PKI, which Microsoft has designed to work in conjunction with XP and other down level Windows client systems, counters most of the drawbacks of earlier Windows PKIs. The Windows Server 2003 PKI supports features such as cross-certification—as well as hierarchical certification—qualified subordination, customizable certificate templates, centralized key archiving and key recovery, user auto enrollment, and delta certificate revocation lists. The chapter also provides enhanced role separation, administrative delegation, and auditing options. Windows Server 2003 PKI software has almost unlimited scalability when it comes to the number of certificates that a single certificate authority (CA) can issue. Windows Server 2003 PKI supports a wide range of cryptographic algorithms: RSA, DSA, RC4, AES, and so forth. A Windows Server 2003 CA can also be integrated relatively easily with PKI software from other vendors.}
}
@article{CASTROLEON201598,
title = {Fault tolerance at system level based on RADIC architecture},
journal = {Journal of Parallel and Distributed Computing},
volume = {86},
pages = {98-111},
year = {2015},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731515001434},
author = {Marcela Castro-León and Hugo Meyer and Dolores Rexachs and Emilio Luque},
keywords = {Software fault tolerance, Resilience, RADIC, Message passing, Semi-coordinated checkpoint, Uncoordinated checkpoint, Socket},
abstract = {The increasing failure rate in High Performance Computing encourages the investigation of fault tolerance mechanisms to guarantee the execution of an application in spite of node faults. This paper presents an automatic and scalable fault tolerant model designed to be transparent for applications and for message passing libraries. The model consists of detecting failures in the communication socket caused by a faulty node. In those cases, the affected processes are recovered in a healthy node and the connections are reestablished without losing data. The Redundant Array of Distributed Independent Controllers architecture proposes a decentralized model for all the tasks required in a fault tolerance system: protection, detection, recovery and masking. Decentralized algorithms allow the application to scale, which is a key property for current HPC system. Three different rollback recovery protocols are defined and discussed with the aim of offering alternatives to reduce overhead when multicore systems are used. A prototype has been implemented to carry out an exhaustive experimental evaluation through Master/Worker and Single Program Multiple Data execution models. Multiple workloads and an increasing number of processes have been taken into account to compare the above mentioned protocols. The executions take place in two multicore Linux clusters with different socket communications libraries.}
}
@article{VASILJ2014185,
title = {Modeling of current-limiting air-core series reactor for transient recovery voltage studies},
journal = {Electric Power Systems Research},
volume = {117},
pages = {185-191},
year = {2014},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2014.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0378779614003137},
author = {Josip Vasilj and Petar Sarajcev and Ranko Goic},
keywords = {Series reactor, High-frequency model, Vector fitting, Transient recovery voltage, Ladder network, EMTP–ATP},
abstract = {This paper presents a methodology for obtaining the current-limiting air-core series reactor model suitable for the transient recovery voltage (TRV) studies, which is constructed from the reactor geometry, accounting for the winding design and disposition, as well as the insulating material properties. The model is based on the ladder connection of frequency-dependent lumped-parameter reactor winding elements (representing partial capacitances, resistances, inductances and conductances). A frequency-dependent equivalent admittance matrix is formed for the ladder network, which is further processed using the vector fit technique, resulting in a Foster-type network suitable for a direct incorporation within the EMTP–ATP environment. A parametric analysis is provided in the paper as well, featuring different impacts that the variation of reactor design properties produces on the inherent TRV response.}
}
@article{MARK201987,
title = {Analysis of coal pillar stability (ACPS): A new generation of pillar design software},
journal = {International Journal of Mining Science and Technology},
volume = {29},
number = {1},
pages = {87-91},
year = {2019},
note = {Special issue on ground control in mining in 2018},
issn = {2095-2686},
doi = {https://doi.org/10.1016/j.ijmst.2018.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S2095268618306487},
author = {Christopher Mark and Zach Agioutantis},
keywords = {Pillar design, Ground control, Stability, Empirical methods},
abstract = {Thirty years ago, the analysis of longwall pillar stability (ALPS) inaugurated a new era in coal pillar design. ALPS was the first empirical pillar design technique to consider the abutment loads that arise from full extraction, and the first to be calibrated using an extensive database of longwall mining case histories. ALPS was followed by the analysis of retreat mining stability (ARMPS) and the analysis of multiple seam stability (AMSS). These methods incorporated other innovations, including the coal mine roof rating (CMRR), the Mark-Bieniawski pillar strength formula, and the pressure arch loading model. They also built upon ever larger case history databases and employed more sophisticated statistical methods. Today, these empirical methods are used in nearly every underground coal mine in the US. However, the piecemeal manner in which these methods have evolved resulted in some weaknesses. For example, in certain situations, it may not be obvious which program is the best to use. Other times the results from the different programs are not entirely consistent with each other. The programs have also not been updated for several years, and some changes were necessary to keep pace with new developments in mining practice. The analysis of coal pillar stability (ACPS) now integrates all three of the older software packages into a single pillar design framework. ACPS also incorporates the latest research findings in the field of pillar design, including an expanded multiple seam case history data base and a new method to evaluate room and pillar panels containing multiple rows of pillars left in place during pillar recovery. ACPS also includes updated guidance and warnings for users and features upgraded help files and graphics.}
}
@article{CHIEN201529,
title = {Versioned Distributed Arrays for Resilience in Scientific Applications: Global View Resilience},
journal = {Procedia Computer Science},
volume = {51},
pages = {29-38},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.187},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915009953},
author = {A. Chien and P. Balaji and P. Beckman and N. Dun and A. Fang and H. Fujita and K. Iskra and Z. Rubenstein and Z. Zheng and R. Schreiber and J. Hammond and J. Dinan and I. Laguna and D. Richards and A. Dubey and B. {van Straalen} and M. Hoemmen and M. Heroux and K. Teranishi and A. Siegel},
keywords = {Resilience, Fault tolerance, Exascale, Scalable computing, Application-based fault tolerance},
abstract = {Exascale studies project reliability challenges for future high-performance computing (HPC) systems. We propose the Global View Resilience (GVR) system, a library that enables applications to add resilience in a portable, application-controlled fashion using versioned distributed arrays. We describe GVR's interfaces to distributed arrays, versioning, and cross-layer error recovery. Using several large applications (OpenMC, the preconditioned conjugate gradient solver PCG, ddcMD, and Chombo), we evaluate the programmer effort to add resilience. The required changes are small (<2% LOC), localized, and machine-independent, requiring no software architecture changes. We also measure the overhead of adding GVR versioning and show that generally overheads <2% are achieved. We conclude that GVR's interfaces and implementation are flexible and portable and create a gentle-slope path to tolerate growing error rates in future systems.}
}
@article{ADAMI1995155,
title = {PAB: a newly designed potentiometric alternating biosensor system},
journal = {Biosensors and Bioelectronics},
volume = {10},
number = {1},
pages = {155-167},
year = {1995},
issn = {0956-5663},
doi = {https://doi.org/10.1016/0956-5663(95)96803-7},
url = {https://www.sciencedirect.com/science/article/pii/0956566395968037},
author = {M. Adami and M. Sartore and C. Nicolini},
keywords = {PAB, LAPS, potentiometric sensors, cell biosensor, enzyme-, immuno-sensor},
abstract = {A complete potentiometric alternating biosensor system (PAB) is described which utilizes a newly developed transducer based on the light addressable chemical sensor (LAPS) technology, following detailed theoretical optimizations of the relevant physical parameters. Our device, entirely designed and realized ‘in house’, combines measuring chambers with specific electronic cards and the related software programs; the system is completely automated and driven by a personal computer. The electronics was designed in order to obtain a good general signal-to-noise ratio, hence ensuring reproducible results. The software package consists of low-level programs (written both in C and in Assembler) to interact with the data acquisition and controlling cards, and of high-level programs (written in C) to get, display, save and print data files, and to present a suitable user interface. A very important peculiarity of our system is the possibility of connecting different measuring chambers to the same device. This allows the user to perform a wide variety of experiments, utilizing different biological elements, by simply changing the reaction chamber and selecting the appropriate acquisition software. The system has been shown to operate with living cells as well as with enzymes and antibodies. In addition, a particular chamber containing a gold-evaporated transducer can be connected to the main system, allowing redox potential measurements. The system utilizes a fast information recovery from the transducer output signal, and can acquire quantitative data every few fractions of a second. This feature can be utilized to monitor fast acidification or redox processes.}
}
@article{LEVIN2018125,
title = {Hierarchical load balancing as a service for federated cloud networks},
journal = {Computer Communications},
volume = {129},
pages = {125-137},
year = {2018},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2018.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0140366418303165},
author = {Anna Levin and Dean Lorenz and Giovanni Merlino and Alfonso Panarello and Antonio Puliafito and Giuseppe Tricomi},
keywords = {Load balancing, SDN, Openstack, Overlay networks, Cloud networking federation, Orchestration},
abstract = {The Software Defined Networking (SDN) paradigm, according to the most popular definition, proposes the ambitious vision of making network infrastructure (e.g., routers and switches) fully programmable. This approach introduces suitable levels of abstraction, in order to adaptnetwork infrastructure functions at runtime through powerful and expressive APIs. In this context, the concept of network virtualisation is of particular importance, namely the idea to create virtual partitions of the physical network infrastructure. When virtualization is applied in this domain, it allows several controller instances and their applications to populate and manage the assigned partitions. Even more useful are the aforementioned concepts when dealing with wide-area networks of cloud-hosting datacenters, especially when trying to provide, e.g., cloud-agnostic and transparent QoS, i.e., cloud-bursting. Scientific research is tackling these new trends following two approaches: multi-cloud and federated cloud. In this work we will pursue the latter, because it leaves the end-users (application owners or companies) free to focus their efforts on application-related activities. Load balancing is among the best practices to distribute user workloads fairly and dynamically among all the nodes in a scaling group, either in a data-center or across clouds. This strategy becomes even more relevant in a scenario featuring federated cloud networks. This is the context where this work finds its place, namely the design of a Load Balancing as a Service (LBaaS) approach for SDN in a OpenStack-based brokered cloud federation.}
}
@article{HAVENS2020104571,
title = {Automated Water Supply Model (AWSM): Streamlining and standardizing application of a physically based snow model for water resources and reproducible science},
journal = {Computers & Geosciences},
volume = {144},
pages = {104571},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104571},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420305598},
author = {Scott Havens and Danny Marks and Micah Sandusky and Andrew Hedrick and Micah Johnson and Mark Robertson and Ernesto Trujillo},
keywords = {Hydrology, Computational method, Software engineering, Data assimilation},
abstract = {Reproducible science requires a shift in thinking and application for how data, code and analysis are shared. Now, scientists must act more like software engineers to design models and perform analysis that use principles and techniques pioneered by software developers. Creating reproducible models that are easy to use and understand is in the best interest for the snow and hydrology community, enabling studies by other researchers and facilitating technology transfer to operational applications. Here, we present the Automated Water Supply Model (AWSM) that streamlines and standardizes the workflow of a physically based snow model to create fully reproducible model simulations that can be utilized by researchers and operational water resource managers. AWSM orchestrates four core components that historically required significant, ad-hoc modeler interaction to load the input data, spatially interpolate to the modeling domain, run the models and process the outputs. Because AWSM was developed using principles and techniques from software engineering, users can quickly perform reproducible simulations on any operating system, from a laptop to the cloud. The three fully reproducible example case studies showcase the simplicity and flexibility of using AWSM to perform simulations from small research catchments to simulations that aid in real time water management decisions.}
}
@article{FRITZ1990253,
title = {Practical Experience in Software Engineering with ADA: Building an Operation Guidance; Recovery and Maintenance Planning System},
journal = {IFAC Proceedings Volumes},
volume = {23},
number = {8, Part 4},
pages = {253-262},
year = {1990},
note = {11th IFAC World Congress on Automatic Control, Tallinn, 1990 - Volume 4, Tallinn, Finland},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51832-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017518327},
author = {W. Fritz and V.H. Haase and R. Kalcher},
keywords = {Software Engineering, Decision Support System, Intelligent Control, Real Time Programming, ADA, ORACLE, GKS},
abstract = {In the development of an operation guidance, recovery and maintenance planning system for a large telecommunication network handling both data and voice the use of commercial standard software has been chosen. A network analysis system, capable to model several thousands of nodes and links together with a decision support system for the operator is being developped in an ADA environment on VAX/VMS machines, data storage is organized around an ORACLE database, and man-machine communication is almost exclusively done in graphics using GKS. It will be shown how the standard packages have been integrated to guarantee both minimum development effort for the project, and fast real time response. Various compatibility problems and their solutions will be discussed, as well as criteria which standard software to choose in which cases. Practical experiences and benefits using ADA in a small team of three to four software engineers both as a design and implementation language combined with rapid prototyping as a development methodology will be presented.}
}
@article{LLOPIS2021319,
title = {Tychonis: A model-based approach to define and search for geometric events in space},
journal = {Acta Astronautica},
volume = {183},
pages = {319-329},
year = {2021},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2021.01.057},
url = {https://www.sciencedirect.com/science/article/pii/S0094576521000680},
author = {M. Llopis and M. Soria and X. Franch},
keywords = {Science planning, Opportunity search, Geometric events, Software architecture, Metamodeling, Reusability, Extensibility},
abstract = {Space missions need software in order to be aware of the occurrence of geometric events in different phases of their project lifecycle. However, current software packages that provide capabilities to search for geometric events might lack desirable software qualities such as extensibility and reusability, and some conflate the definition of events with the search for the same. In order to improve the state of the art, we propose a framework named Tychonis that, via metamodeling techniques and conscious use of object-oriented design best practices, (i) can integrate with current and future mission software, (ii) enables end-users to extend opportunities and search algorithms without modifying the tools that use the framework, and (iii) promotes the cross-mission reusability of the framework and its extensions. These attributes make it less costly for projects to develop software to search for geometric events. Tychonis is provided as a software library that allows missions to add their own data structures and constructs to the framework, including geometric event types and search algorithms. This is accomplished with Tychonis’ use of the Eclipse Modeling Framework (EMF) to capture its metamodel, which enables the generation of Java classes that represent built-in or user-developed Tychonis constructs. In the present paper, we elaborate on the impetus for the development of Tychonis, show its proposed design and use, and discuss growth opportunities for future consideration.}
}
@article{GIULIANI201839,
title = {π-BEM: A flexible parallel implementation for adaptive, geometry aware, and high order boundary element methods},
journal = {Advances in Engineering Software},
volume = {121},
pages = {39-58},
year = {2018},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0965997818300371},
author = {Nicola Giuliani and Andrea Mola and Luca Heltai},
keywords = {BEM, Fast multiple method, High order elements, Local refinement, MPI, Multi-threaded, OpenSOURCE, CAD},
abstract = {Many physical phenomena can be modelled using boundary integral equations, and discretised using the boundary element method (BEM). Such models only require the discretisation of the boundary of the domain, making the setup of the simulation straightforward and lowering the number of degrees of freedom. However, while many parallel efficient libraries are available for the Finite Element Method (FEM), the implementation of scalable BEM solvers still poses many challenges. We present the open source framework π-BEM (where π stands for parallel): a novel boundary element method solver, combining distributed and shared memory paradigms to achieve high scalability. π-BEM exploits high performance libraries and graph partitioning tools to deliver a parallel solver employing automatic domain decomposition, high order elements, local refinement capabilities, and exact geometry-adaptivity (using CAD files). A preliminary fast multipole accelerator is included in the implementation. Every aspect of the library is modular and easily extendible by the community. We discuss the internal structure of the code, and present some examples to demonstrate the reliability and scalability of our implementation.}
}
@article{PIROLA2021100060,
title = {Purification of air from volatile organic compounds by countercurrent liquid gas mass transfer absorption process},
journal = {International Journal of Thermofluids},
volume = {9},
pages = {100060},
year = {2021},
issn = {2666-2027},
doi = {https://doi.org/10.1016/j.ijft.2020.100060},
url = {https://www.sciencedirect.com/science/article/pii/S2666202720300471},
author = {Carlo Pirola and Michela Mattia},
keywords = {VOC, Mass transfer, Absorption, Air purification, Process simulation},
abstract = {Nowadays environmental sustainability is a fundamental requirement for the development of any process. In this regard, European Union applied directives on emission reduction commitments. In such circumstances, it is mandatory for industries or facilities to adopt proper pollution control measures to meet the requirements. The absorption process can be a simple, safe, regenerative and recovery technique, selective for the species. The aim of this work is to study the purification process of polluted air containing volatile organic compounds by absorption towers. The evaluation of the feasibility of the process and the effects of operative conditions on the efficiency will be discussed. Ethyl acetate was selected as a representative volatile organic compound chemical model. We experimentally purified air containing this pollutant at different concentrations by absorption with pure water, changing the temperature and the ratio between water and air flowrate. We investigated the effect of different packagings and their height using two columns: the first one with Sulzer rings and the second one with Raschig structured packaging. Finally, the effect of recycled solvent in the absorption process was considered. PRO/II software simulator enabled a steady-state simulation of the tests obtaining an excellent fitting of the experimental data.}
}
@article{ZIEMKE2011583,
title = {An integrated development framework for rapid development of platform-independent and reusable satellite on-board software},
journal = {Acta Astronautica},
volume = {69},
number = {7},
pages = {583-594},
year = {2011},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2011.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0094576511001202},
author = {Claas Ziemke and Toshinori Kuwahara and Ivan Kossev},
keywords = {On-board software, Reusable, Platform-independent, System simulation, Open-source},
abstract = {Even in the field of small satellites, the on-board data handling subsystem has become complex and powerful. With the introduction of powerful CPUs and the availability of considerable amounts of memory on-board a small satellite it has become possible to utilize the flexibility and power of contemporary platform-independent real-time operating systems. Especially the non-commercial sector such like university institutes and community projects such as AMSAT or SSETI are characterized by the inherent lack of financial as well as manpower resources. The opportunity to utilize such real-time operating systems will contribute significantly to achieve a successful mission. Nevertheless the on-board software of a satellite is much more than just an operating system. It has to fulfill a multitude of functional requirements such as: Telecommand interpretation and execution, execution of control loops, generation of telemetry data and frames, failure detection isolation and recovery, the communication with peripherals and so on. Most of the aforementioned tasks are of generic nature and have to be conducted on any satellite with only minor modifications. A general set of functional requirements as well as a protocol for communication is defined in the SA ECSS-E-70-41A standard “Telemetry and telecommand packet utilization”. This standard not only defines the communication protocol of the satellite–ground link but also defines a set of so called services which have to be available on-board of every compliant satellite and which are of generic nature. In this paper, a platform-independent and reusable framework is described which is implementing not only the ECSS-E-70-41A standard but also functionalities for interprocess communication, scheduling and a multitude of tasks commonly performed on-board of a satellite. By making use of the capabilities of the high-level programming language C/C++, the powerful open source library BOOST, the real-time operating system RTEMS and finally by providing generic functionalities compliant to the ECSS-E-70-41A standard the proposed framework can provide a great boost in productivity. Together with open source tools such like the GNU tool-chain, Eclipse SDK, the simulation framework OpenSimKit, the emulator QEMU, the proposed on-board software framework forms an integrated development framework. It is possible to design, code and build the on-board software together with the operating system and then run it on a simulated satellite for performance analysis and debugging purposes. This makes it possible to rapidly develop and deploy a full-fledged satellite on-board software with minimal cost and in a limited time frame.}
}
@article{CIZNICKI20121900,
title = {Benchmarking Data and Compute Intensive Applications on Modern CPU and GPU Architectures},
journal = {Procedia Computer Science},
volume = {9},
pages = {1900-1909},
year = {2012},
note = {Proceedings of the International Conference on Computational Science, ICCS 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.04.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912003298},
author = {Miłosz Ciżnicki and Michał Kierzynka and Piotr Kopta and Krzysztof Kurowski and Paweł Gepner},
keywords = {benchmarks, GPU, multi-core CPU, JPEG 2000, signal processing},
abstract = {The use of graphics hardware for non-graphics applications has become popular among many scientific programmers and researchers as we have observed a higher rate of theoretical performance increase than the CPUs in recent years. However, performance gains may be easily lost in the context of a specific parallel application due to various both hardware and software factors. Consequently, software benchmarks and performance testing are still the best techniques to compare the effciency of emerging parallel architectures with the built-in support for parallelism at different levels. Unfortunately, many available benchmarks are either relatively simple application kernels, they have been optimized only for a certain parallel architecture or they do not take advantage of recent capabilities provided by modern hardware and low level APIs. Thus, the main aim of this paper is to present a comprehensive real performance analysis of selected applications following the complex standard for data compression and coding -JPEG 2000. It consists of a chain of data and compute intensive tasks that can be treated as good examples of software benchmarks for modern parallel hardware architectures. In this paper we compare achieved performance results of our standard based benchmarks executed on selected architectures for different data sets to identify possible bottlenecks. We discuss also best practices and advices for parallel software development to help users to evaluate in advance and then select appropriate solutions to accelerate the execution of their applications.}
}
@article{MOHANTY19921079,
title = {Software development for energy analysis and heat recovery from boilers and furnaces},
journal = {Energy Conversion and Management},
volume = {33},
number = {12},
pages = {1079-1088},
year = {1992},
issn = {0196-8904},
doi = {https://doi.org/10.1016/0196-8904(92)90005-H},
url = {https://www.sciencedirect.com/science/article/pii/019689049290005H},
author = {B. Mohanty and K.N. Manandhar},
keywords = {Boiler and furnace performance, Combustion analysis, Heat exchanger design, Economic optimization, Small and medium scale industry, Microcomputer software},
abstract = {Microcomputer software has been developed for engineers and consultants in industry to analyze the performance of boilers and furnaces and to estimate the potential for heat recovery through heat exchanger design and economic optimization. The module is designed for useful application in small and medium scale industries which consume large amounts of conventional or alternative fuels, specifically in developing countries. An attempt has been made to render the package user-friendly, practical, flexible, easy to maintain and expandable to take into consideration the specific requirements of the users. The package consists of a library of fuel data, a library of projects and a study section which carries out combustion analysis, basic heat exchanger design with the user's data options and an optimization of design configuration leading to the minimum heat exchanger cost.}
}
@article{TACHWALI2009607,
title = {Configurable symbol synchronizers for software-defined radio applications},
journal = {Journal of Network and Computer Applications},
volume = {32},
number = {3},
pages = {607-615},
year = {2009},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2008.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1084804508000829},
author = {Y. Tachwali and W.J. Barnes and H. Refai},
keywords = {Configurable, Symbol timing recovery, Synchronization, SDR},
abstract = {In many synchronous receivers, symbol timing synchronization is achieved through implementation of an analog phase locked loop (PLL). A phase detector and voltage-controlled oscillator drive a reference signal to be in phase with the received training sequence. Due to the quick phase convergence this option is attractive; however, limitations in pre-packaged hardware make this approach infeasible at times. Changes in the received symbol rate in software radio applications can further complicate the hardware implementation by requiring additional control signals to alter the frequency of the reference signal. This paper examines a configurable symbol synchronizer for software-defined radio (SDR) architecture with a predefined RF front end. In this scenario, we implement a typical method for digital phase locking and make it adaptable to different data rates. A pre-synchronization step is used to provide a reasonable initial estimate for the received symbol period for lower, over-sampled data rates. This decreases the synchronization time while maintaining a constant sampling period at the ADC. It also maintains the down-conversion stage at the receiver. The paper shows the feasibility of this architecture to support wide range of symbol rates.}
}
@article{NEILL2003423,
title = {Leveraging object-orientation for real-time imaging systems},
journal = {Real-Time Imaging},
volume = {9},
number = {6},
pages = {423-432},
year = {2003},
note = {Special Issue on Software Engineering of Real-time Imaging Systems},
issn = {1077-2014},
doi = {https://doi.org/10.1016/j.rti.2003.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1077201403000627},
author = {Colin J Neill},
abstract = {Imaging systems are traditionally developed using structured analysis and design techniques at best. Such approaches tend to be rigid with respect to changing needs, technologies, devices and algorithms—for example, when additional compression algorithms are needed or attached devices are changed large parts of software applications employing those techniques and interfacing with those devices must be modified to accommodate the change. In a larger perspective, these systems are difficult or impossible to reuse; each new problem requires a new solution. This is generally undesirable and often not necessary, but only if best practices in software engineering are employed. These best practices have been explored and documented in detail with regard to object-oriented systems, which suggests that it is an appropriate paradigm to employ in the development of future imaging systems. This work examines these best practices, in the form of patterns and design principles, with reference to imaging systems and within the context of the Java imaging APIs.}
}
@article{SINGH2016145,
title = {Early degradation of high power packaged LEDs under humid conditions and its recovery — Myth of reliability rejuvenation},
journal = {Microelectronics Reliability},
volume = {61},
pages = {145-153},
year = {2016},
note = {SI: ICMAT 2015},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2015.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0026271415302833},
author = {Preetpal Singh and Cher Ming Tan and Liann-Be Chang},
keywords = {Moisture absorption, Moisture desorption, Degradation recovery, Temperature–humidity test, Die attach moisture penetration, Scanning acoustic microscopy, Finite element moisture modeling},
abstract = {A sharp rise in lumen degradation was observed for packaged high power LEDs during the initial period of operation under high humidity and temperature conditions, and the degradation reaches a peak value, followed by a “recovery” in lumen output, a sign of reliability rejuvenation. The time to reach the peak degradation is shorter with higher relative humidity. Scanning acoustic microscopy (SAM) tomography is employed to study the effect of moisture at different time intervals. With the help of moisture diffusion modeling using ANSYS simulation, the phenomenon is found to be due to the increasing moisture absorption of silicone resulting in subsequent light scattering as light is emitting from the dice. The “recovery” is the result of moisture absorption by die attach material that sucks the moisture from the silicone. Thus the “recovery” of lumen degradation is actually associated with the degradation in the internal structure of the LED package which is not reversible. C-SAM results are in accordance with the simulation and experimental results. The implication of this finding on temperature–humidity test of high power LEDs is described, and the material parameters of silicone to reduce this initial degradation are also presented.}
}
@incollection{GRAY1986441,
title = {THE DEVELOPMENT OF THE SOFTWARE FOR A MICROPROCESSOR-BASED MULTIVARIABLE CONTROLLER},
editor = {M. PAUL},
booktitle = {Digital Computer Applications to Process Control},
publisher = {Pergamon},
address = {Oxford},
pages = {441-446},
year = {1986},
series = {IFAC Symposia Series},
isbn = {978-0-08-032554-5},
doi = {https://doi.org/10.1016/B978-0-08-032554-5.50065-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780080325545500653},
author = {G.T. Gray and M. Braae},
abstract = {Abstract
The software for a microprocessor-based multivariable controller, which controls the grinding circuit on a gold-recovery plant, has been developed. The process and the microprocessor hardware are surveyed briefly, and the functional requirements of the software are discussed. The necessity for a real-time executive software package is explained, the selection of such a package and its resulting performance are described and an account is given of the various languages and software utilities that were used in the preparation of the software. Special mention is made of the real-time nature of the software and the problems that were encountered regarding aspects such as reentrant procedures and shared data structures. The results of the four stages of testing that were carried out show clearly that the development laboratory should have a simulation of the process to be controlled. Finally the possibility is explored that one-off microprocessor-based devices can be developed that will be economically justifiable.}
}
@article{JOUHARA2021100063,
title = {Thermoelectric generator (TEG) technologies and applications},
journal = {International Journal of Thermofluids},
volume = {9},
pages = {100063},
year = {2021},
issn = {2666-2027},
doi = {https://doi.org/10.1016/j.ijft.2021.100063},
url = {https://www.sciencedirect.com/science/article/pii/S266620272100001X},
author = {Hussam Jouhara and Alina Żabnieńska-Góra and Navid Khordehgah and Qusay Doraghi and Lujean Ahmad and Les Norman and Brian Axcell and Luiz Wrobel and Sheng Dai},
keywords = {Thermoelectric generators, Seebeck effect, Peltier effect, Waste heat recovery, Energy efficiency},
abstract = {Nowadays humans are facing difficult issues, such as increasing power costs, environmental pollution and global warming. In order to reduce their consequences, scientists are concentrating on improving power generators focused on energy harvesting. Thermoelectric generators (TEGs) have demonstrated their capacity to transform thermal energy directly into electric power through the Seebeck effect. Due to the unique advantages they present, thermoelectric systems have emerged during the last decade as a promising alternative among other technologies for green power production. In this regard, thermoelectric device output prediction is important both for determining the future use of this new technology and for specifying the key design parameters of thermoelectric generators and systems. Moreover, TEGs are environmentally safe, work quietly as they do not include mechanical mechanisms or rotating elements and can be manufactured on a broad variety of substrates such as silicon, polymers and ceramics. In addition, TEGs are position-independent, have a long working life and are ideal for bulk and compact applications. Furthermore, Thermoelectric generators have been found as a viable solution for direct generation of electricity from waste heat in industrial processes. This paper presents in-depth analysis of TEGs, beginning with a comprehensive overview of their working principles such as the Seebeck effect, the Peltier effect, the Thomson effect and Joule heating with their applications, materials used, Figure of Merit, improvement techniques including different thermoelectric material arrangements and technologies used and substrate types. Moreover, performance simulation examples such as COMSOL Multiphysics and ANSYS-Computational Fluid Dynamics are investigated.}
}
@article{JARVI2010596,
title = {Programming with C++ concepts},
journal = {Science of Computer Programming},
volume = {75},
number = {7},
pages = {596-614},
year = {2010},
note = {Generative Programming and Component Engineering (GPCE 2007)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2009.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167642309000021},
author = {Jaakko Järvi and Mat Marcus and Jacob N. Smith},
keywords = {C++, Concepts, Software libraries, Component adaptation, Generic programming, Polymorphism},
abstract = {This paper explores the definition, applications, and limitations of concepts and concept maps in C++, with a focus on library composition. We also compare and contrast concepts to adaptation mechanisms in other languages. Efficient, non-intrusive adaptation mechanisms are essential when adapting data structures to a library’s API. Development with reusable components is a widely practiced method of building software. Components vary in form, ranging from source code to non-modifiable binary libraries. The Concepts language features, slated to appear in the next version of C++, have been designed with such compositions in mind, promising an improved ability to create generic, non-intrusive, efficient, and identity-preserving adapters. We report on two cases of data structure adaptation between different libraries, and illustrate best practices and idioms. First, we adapt GUI widgets from several libraries, with differing APIs, for use with a generic layout engine. We further develop this example to describe the run-time concept idiom, extending the applicability of concepts to domains where run-time polymorphism is required. Second, we compose an image processing library and a graph algorithm library, by making use of a transparent adaptation layer, enabling the efficient application of graph algorithms to the image processing domain. We use the adaptation layer to realize a few key algorithms, and report little or no performance degradation.}
}
@article{BRUKMAN20082315,
title = {A self-stabilizing autonomic recoverer for eventual Byzantine software},
journal = {Journal of Systems and Software},
volume = {81},
number = {12},
pages = {2315-2327},
year = {2008},
note = {Best papers from the 2007 Australian Software Engineering Conference (ASWEC 2007), Melbourne, Australia, April 10-13, 2007},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2008.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0164121208000861},
author = {Olga Brukman and Shlomi Dolev and Elliot K. Kolodner},
keywords = {Self-stabilization, Monitor, Automatic recovery, Liveness, Safety},
abstract = {We suggest modeling software package flaws (bugs) by assuming eventual Byzantine behavior of the package. We assume that if a program is started in a predefined initial state, it will exhibit legal behavior for a period of time but will eventually become Byzantine. We assume that this behavior pattern can be attributed to the fact that the manufacturer had performed sufficient package tests for limited time scenarios. Restarts are useful for recovering such systems. We suggest a general, yet practical, framework and paradigm for the monitoring and restarting of systems where the framework and paradigm are based on a theoretical foundation. An autonomic recoverer that monitors and initiates system recovery is proposed. It is designed to handle a task, given specific task requirements in the form of predicates and actions. A directed acyclic graph subsystem hierarchical structure is used by a consistency monitoring procedure for achieving a gracious recovery. The existence and correct functionality of the autonomic recovery is guaranteed by the use of a self-stabilizing kernel resident (anchor) process. The autonomic recoverer uses a new scheme for liveness assurance via on-line monitoring that complements known schemes for on-line safety assurance.}
}
@incollection{MCBRIDE20181783,
title = {Computer Aided Design of Green Thermomorphic Solvent Systems for Homogeneous Catalyst Recovery},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {1783-1788},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50292-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417502925},
author = {Kevin McBride and Steffen Linke and Shuang Xu and Kai Sundmacher},
keywords = {computer-aided molecular design (CAMD), COSMO-RS, green solvents, homogeneous catalysis},
abstract = {This work presents a new computer-aided thermomorphic solvent system (TMS) design methodology for the recovery of homogeneous catalysts that incorporates quantitative structure-activity relationships (QSAR) to predict various environmental, health, and safety (EHS) criteria using modern software packages. The quantum chemical method Conductor-like Screening Model for Real Solvents (COSMO-RS) (Klamt, 1995) is used for predicting catalyst solubility and the liquid-liquid equilibrium behavior of potential TMS designs. This methodology is then exemplified on the hydroformylation of 1- decene using the rhodium-Biphephos (Rh-BPP) transition metal catalyst and several green TMS designs were identified. Two of these TMS were then selected for experimental validation of both their LLE behavior and reaction performance.}
}
@article{JAMSAJOUNELA199031,
title = {Expert Control System for Rougher Flotation of Phosphate},
journal = {IFAC Proceedings Volumes},
volume = {23},
number = {8, Part 4},
pages = {31-40},
year = {1990},
note = {11th IFAC World Congress on Automatic Control, Tallinn, 1990 - Volume 4, Tallinn, Finland},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51797-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017517978},
author = {S.-L. Jämsä-Jounela and E.-K. Kärki},
keywords = {Expert Control, Computer Aided Control System Design, Phosphate Flotation Control, Flotation Modelling},
abstract = {The parameters of a detailed phenomenological flotation model for the rougher flotation bank have been evaluated performing large steady state sampling in the plant. The dynamic model has been used in simulation of the operation of a rougher flotation bank, as a step in development of expert control of a larger flotation system. The simulator has been interfaced with the ONSPEC control software package run in the IBM DOS environment, the software for flotation circuit simulation and expert control system design is described. The control strategy tested by simulation aims to maximize the recovery, while the concentrate grade is maintained above a given minimum level. The determination of the manipulated variables hierarchy of the bank is described and the simulated results of expert control are presented.}
}
@article{LIAGRE20031,
title = {Estimating nonlinear coupled frequency-dependent parameters in offshore engineering},
journal = {Applied Ocean Research},
volume = {25},
number = {1},
pages = {1-19},
year = {2003},
issn = {0141-1187},
doi = {https://doi.org/10.1016/S0141-1187(03)00029-4},
url = {https://www.sciencedirect.com/science/article/pii/S0141118703000294},
author = {P.F. Liagre and J.M. Niedzwecki},
keywords = {System identification, Multiple input/single output problem, MATLAB, WAMIT, Frequency-dependent parameters, Nonlinear equations, Coupled equations, Hydrodynamic added-mass, Hydrodynamic damping, Mini-TLP, Deepwater platform response},
abstract = {The design of deepwater compliant offshore structures requires engineers to address many difficult challenges including defining and modeling the local offshore environment, specifying the associated combined global loading on innovative platform designs, and numerical simulation and model test verification of the platform response characteristics. The focus of this research investigation is the recovery of key parameters from time series measured during an industry type model basin test program using the reverse multiple input/single output technique. In particular, this study confronts critical problems of practical interest and extends the methodology to address the inclusion of nonlinear coupled systems in which the parameters of interest can be frequency-dependent. The analysis is developed around the nonlinear coupled equations of motions for a deepwater mini-TLP design and includes the consideration of nonlinear stiffness, quadratic damping, surge/pitch and sway/roll coupling and the frequency dependency of both the hydrodynamic added-mass and damping coefficients. A series of complementary model test measurements for the complete compliant model and the rigidly restrained hull by itself were used as the basis for the data in the system identification procedures. In addition, behavior of the hydrodynamic added-mass and damping coefficients as a function of frequency was simulated for the mini-TLP using an industry standard radiation–diffraction software package. These results were used in evaluating the accuracy of some of the key problem parameters. The results presented demonstrate the methodology as modified in this study is quite robust and yields predications that are more accurate for the parameters associated with the largest motions of the platform. Practical issues regarding the application of this approach, utilization of both force and moment measurements, and observed strengths and weakness in dealing with data regardless of its source are discussed.}
}
@article{SERBAN2001187,
title = {COOPT — a software package for optimal control of large-scale differential–algebraic equation systems},
journal = {Mathematics and Computers in Simulation},
volume = {56},
number = {2},
pages = {187-203},
year = {2001},
note = {Method of lines},
issn = {0378-4754},
doi = {https://doi.org/10.1016/S0378-4754(01)00289-0},
url = {https://www.sciencedirect.com/science/article/pii/S0378475401002890},
author = {Radu Serban and Linda R. Petzold},
keywords = {Differential–algebraic equations, Sensitivity analysis, Optimal control, Sequential quadratic programming methods},
abstract = {This paper describes the functionality and implementation of COOPT. This software package implements a direct method with modified multiple shooting type techniques for solving optimal control problems of large-scale differential–algebraic equation (DAE) systems. The basic approach in COOPT is to divide the original time interval into multiple shooting intervals, with the DAEs solved numerically on the subintervals at each optimization iteration. Continuity constraints are imposed across the subintervals. The resulting optimization problem is solved by sparse sequential quadratic programming (SQP) methods. Partial derivative matrices needed for the optimization are generated by DAE sensitivity software. The sensitivity equations to be solved are generated via automatic differentiation. COOPT has been successfully used in solving optimal control problems arising from a wide variety of applications, such as chemical vapor deposition of superconducting thin films, spacecraft trajectory design and contingency/recovery problems, and computation of cell traction forces in tissue engineering.}
}
@article{ESPINHA201527,
title = {Web API growing pains: Loosely coupled yet strongly tied},
journal = {Journal of Systems and Software},
volume = {100},
pages = {27-43},
year = {2015},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2014.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0164121214002180},
author = {Tiago Espinha and Andy Zaidman and Hans-Gerhard Gross},
keywords = {Web API, Software evolution, Breaking changes},
abstract = {Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution.}
}
@article{JACOBS2020109544,
title = {The Materials Simulation Toolkit for Machine learning (MAST-ML): An automated open source toolkit to accelerate data-driven materials research},
journal = {Computational Materials Science},
volume = {176},
pages = {109544},
year = {2020},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2020.109544},
url = {https://www.sciencedirect.com/science/article/pii/S0927025620300355},
author = {Ryan Jacobs and Tam Mayeshiba and Ben Afflerbach and Luke Miles and Max Williams and Matthew Turner and Raphael Finkel and Dane Morgan},
keywords = {Machine learning, Materials informatics, Data science, Open source software},
abstract = {As data science and machine learning methods are taking on an increasingly important role in the materials research community, there is a need for the development of machine learning software tools that are easy to use (even for nonexperts with no programming ability), provide flexible access to the most important algorithms, and codify best practices of machine learning model development and evaluation. Here, we introduce the Materials Simulation Toolkit for Machine Learning (MAST-ML), an open source Python-based software package designed to broaden and accelerate the use of machine learning in materials science research. MAST-ML provides predefined routines for many input setup, model fitting, and post-analysis tasks, as well as a simple structure for executing a multi-step machine learning model workflow. In this paper, we describe how MAST-ML is used to streamline and accelerate the execution of machine learning problems. We walk through how to acquire and run MAST-ML, demonstrate how to execute different components of a supervised machine learning workflow via a customized input file, and showcase a number of features and analyses conducted automatically during a MAST-ML run. Further, we demonstrate the utility of MAST-ML by showcasing examples of recent materials informatics studies which used MAST-ML to formulate and evaluate various machine learning models for an array of materials applications. Finally, we lay out a vision of how MAST-ML, together with complementary software packages and emerging cyberinfrastructure, can advance the rapidly growing field of materials informatics, with a focus on producing machine learning models easily, reproducibly, and in a manner that facilitates model evolution and improvement in the future.}
}
@article{HELD200195,
title = {VRONI: An engineering approach to the reliable and efficient computation of Voronoi diagrams of points and line segments},
journal = {Computational Geometry},
volume = {18},
number = {2},
pages = {95-123},
year = {2001},
issn = {0925-7721},
doi = {https://doi.org/10.1016/S0925-7721(01)00003-7},
url = {https://www.sciencedirect.com/science/article/pii/S0925772101000037},
author = {Martin Held},
keywords = {Voronoi diagram, Topology-oriented algorithm, Reliability, Robustness, Experimental analysis},
abstract = {We discuss the design and implementation of a topology-oriented algorithm for the computation of Voronoi diagrams of points and line segments in the two-dimensional Euclidean space. The main focus of our work was on designing and engineering an algorithm that is completely reliable and fast in practice. The algorithm was implemented in ANSI C, using standard floating-point arithmetic. In addition to Sugihara and Iri's topology-oriented approach, it is based on a very careful implementation of the numerical computations required, an automatic relaxation of epsilon thresholds, and a multi-level recovery process combined with “desperate mode”. The resulting code, named vroni , was tested extensively on real-world data and turned out to be reliable. CPU-time statistics document that it is always faster than other popular Voronoi codes. In our computing environment, vroni needs about 0.01nlog2n milliseconds to compute the Voronoi diagram of n line segments, and this formula holds for a wide variety of synthetic and real-world data. In particular, its CPU-time consumption is hardly affected by the actual distribution of the input data. Vroni also features a function for computing offset curves, and it has been successfully tested within and integrated into several industrial software packages.}
}
@article{CHONG20131994,
title = {Efficient software clustering technique using an adaptive and preventive dendrogram cutting approach},
journal = {Information and Software Technology},
volume = {55},
number = {11},
pages = {1994-2012},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001481},
author = {Chun Yong Chong and Sai Peck Lee and Teck Chaw Ling},
keywords = {Software maintenance, Design recovery, Software clustering, Remodularization},
abstract = {Context
Software clustering is a key technique that is used in reverse engineering to recover a high-level abstraction of the software in the case of limited resources. Very limited research has explicitly discussed the problem of finding the optimum set of clusters in the design and how to penalize for the formation of singleton clusters during clustering.
Objective
This paper attempts to enhance the existing agglomerative clustering algorithms by introducing a complementary mechanism. To solve the architecture recovery problem, the proposed approach focuses on minimizing redundant effort and penalizing for the formation of singleton clusters during clustering while maintaining the integrity of the results.
Method
An automated solution for cutting a dendrogram that is based on least-squares regression is presented in order to find the best cut level. A dendrogram is a tree diagram that shows the taxonomic relationships of clusters of software entities. Moreover, a factor to penalize clusters that will form singletons is introduced in this paper. Simulations were performed on two open-source projects. The proposed approach was compared against the exhaustive and highest gap dendrogram cutting methods, as well as two well-known cluster validity indices, namely, Dunn’s index and the Davies-Bouldin index.
Results
When comparing our clustering results against the original package diagram, our approach achieved an average accuracy rate of 90.07% from two simulations after the utility classes were removed. The utility classes in the source code affect the accuracy of the software clustering, owing to its omnipresent behavior. The proposed approach also successfully penalized the formation of singleton clusters during clustering.
Conclusion
The evaluation indicates that the proposed approach can enhance the quality of the clustering results by guiding software maintainers through the cutting point selection process. The proposed approach can be used as a complementary mechanism to improve the effectiveness of existing clustering algorithms.}
}
@article{DENEUX2013746,
title = {Establishment of a Model for a Combined Heat and Power Plant with ThermosysPro Library},
journal = {Procedia Computer Science},
volume = {19},
pages = {746-753},
year = {2013},
note = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.06.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913007060},
author = {O. Deneux and B. El Hafni and B. Péchiné and E. {Di Penta} and G. Antonucci and P. Nuccio},
keywords = {Modelling, simulation, combined heat and power plant.},
abstract = {The Simulation and Information Technologies for Power Generation System Department (STEP) has developed a methodology, as part of the framework of an EDF R&D project, to model and optimize energy systems for the associated companies with the aim of highlighting the possibility of using modelling tools to optimize energy systems. Dymola software, a commercial implementation of Modelica, which was developed by Dassault Systemes has been employed. A library, called ThermosysPro and developed by EDF R&D, has in particular been used. The energy system presented in the paper is a Combined Heat and Power plant (CHP), managed by Fenice SpA, which has been designed to supply electric and thermal energy to a food factory near Parma (Italy). This kind of system can be regulated over a large power range and, as a consequence, the CHP plant can supply the total amount of thermal energy required by the user. From a comparison of the experimental data and the simulation results, it can be seen that the behaviour of the turbo gas and the steam turbine approximately follows the results of the performance test at full load over a wide temperature range. As far as the performance at partial load, the gas turbine Heat Rate and the heat recovery steam generator (HRSG) performance are concerned, the simulation results and the actual CHP plant behaviour again appear to be in good agreement.}
}
@article{AHMED2017122,
title = {Constitutive modeling for thermo-mechanical low-cycle fatigue-creep stress–strain responses of Haynes 230},
journal = {International Journal of Solids and Structures},
volume = {126-127},
pages = {122-139},
year = {2017},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2017.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0020768317303475},
author = {Raasheduddin Ahmed and Tasnim Hassan},
keywords = {Cyclic viscoplasticity, Unified constitutive model, Thermo-mechanical fatigue-creep, Mean-stress evolution, Stress relaxation, Haynes 230},
abstract = {Haynes 230 (HA 230), a Nickel-based superalloy, is the primary material of combustor liners in airplane gas turbine engines. This component operates in the temperature range between ambient to as high as 1000 °C. Such thermal cycles together with the resulting strain cycles in a combustor liner may induce thermo-mechanical fatigue-creep (TMFC) damage and initiate cracks earlier than the estimated life. Use of a robust unified constitutive model (UCM) for nonlinear analysis based design may improve fatigue life estimation of high temperature components. Available UCMs in the literature or commercial software packages are unable to simulate the TMFC responses reasonably. Hence, this study developed a UCM incorporating the modeling features of rate and temperature dependence, static-recovery, various kinematic hardening evolutions, and strain-range dependence, and validated the model against a broad set of TMFC experimental responses of HA 230. This modified UCM is capable of simulating the mean-stress evolution under both the out-of-phase and in-phase TMFC loading cycles. The modified UCM can adequately simulate most of the characteristic cyclic phenomena of HA 230 including the influence of maximum temperature on the out-of-phase and in-phase TMFC hysteretic responses, stress amplitude, and stress relaxation during strain-dwell. The time-derivative of the elastic modulus is an essential modeling feature for accurately simulating the inelastic strains under TMFC loading. These simulations demonstrate the progresses made in UCM.}
}
@article{LEONG1991461,
title = {Microcomputer-based design of rotary regenerators},
journal = {Heat Recovery Systems and CHP},
volume = {11},
number = {6},
pages = {461-470},
year = {1991},
issn = {0890-4332},
doi = {https://doi.org/10.1016/0890-4332(91)90048-9},
url = {https://www.sciencedirect.com/science/article/pii/0890433291900489},
author = {K.C. Leong and K.C. Toh and S.H. Wong},
abstract = {A software package was written in BASIC for the IBM PC based on the ϵ-NTUo method for counterflow rotary regenerator heat transfer analysis taking into account the effects of longitudinal and transverse heat conduction in the matrix wall together with the influence of fluid bypass and carryover. It was tested for accuracy by comparing the software-generated results with results from case studies in the literature. A comparison with actual operating data of a regenerator used for waste heat recovery in an electric power generation facility showed that the software can predict the regenerator outlet temperatures to within 3.5 percent.}
}
@incollection{STEWART2013563,
title = {Chapter 17 - Multicore Software Development for Embedded Systems: This Chapter draws on Material from the Multicore Programming Practices Guide (MPP) from the Multicore Association},
editor = {Robert Oshana and Mark Kraeling},
booktitle = {Software Engineering for Embedded Systems},
publisher = {Newnes},
address = {Oxford},
pages = {563-612},
year = {2013},
isbn = {978-0-12-415917-4},
doi = {https://doi.org/10.1016/B978-0-12-415917-4.00017-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159174000177},
author = {Dave Stewart and Max Domeika and Scott A. Hissam and Skip Hovsmith and James Ivers and Ross Dickson and Ian Lintault and Stephen Olsen and Hyunki Baik and François Bodin and Robert Oshana},
keywords = {Multicore Programming Practices (MPP), Multicore Association (MCA), serial performance, parallel decomposition, synchronization, threads, PPthreads, mutexes, granularity, parallelism, MCAPI, MRAPI},
abstract = {Multicore software development is growing in importance and applicability in many areas of embedded systems from automotive to networking, to wireless base stations. This chapter is a summary of key sections of the recently released Multicore Programming Practices (MPP) from the Multicore Association (MCA). The MPP standardized “best practices” guide is written specifically for engineers and engineering managers of companies considering or implementing a development project involving multicore processors and favoring use of existing multicore technology. There is an important need to better understand how today’s C/C++ code may be written to be “multicore ready”, and this was accomplished under the influence of the MPP working group. The guide will enable you to (a) produce higher-performing software; (b) reduce the bug rate due to multicore software issues; (c) develop portable multicore code which can be targeted at multiple platforms; (d) reduce the multicore programming learning curve and speed up development time; and (e) tie into the current structure and roadmap of the Multicore Association’s API infrastructure.}
}
@article{S201678,
title = {Reverse Osmosis–Pressure Retarded Osmosis hybrid system: Modelling, simulation and optimization},
journal = {Desalination},
volume = {389},
pages = {78-97},
year = {2016},
note = {Pressure Retarded Osmosis},
issn = {0011-9164},
doi = {https://doi.org/10.1016/j.desal.2016.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S0011916416300273},
author = {Senthil S. and Senthilmurugan S.},
keywords = {Reverse osmosis, Desalination, Pressure-Retarded Osmosis, SWRO–PRO hybrid system, Modelling, Optimization},
abstract = {Theoretical analysis of energy harvesting from concentrated brine of Sea Water Reverse Osmosis (SWRO) system using Pressure Retarded Osmosis (PRO) is presented in this research. The mathematical model of SWRO–PRO hybrid system components such as SWRO unit, Energy Recovery Device (ERD), PRO unit and other auxiliary units were discussed. The mathematical equations were solved adapting an object oriented “Modelica language” framework in Dymola software tool. The complex flowsheet models for six different SWRO–PRO hybrid configurations were created. The performance of the SWRO–PRO hybrid system configurations was studied. The process and design parameters were optimized to reduce the Net Specific Energy Consumption (NSEC) of the system. The optimization studies were performed using SQP technique that is available in optimization library of Dymola. The possibility of using sea water (32,000g/m3) and urban waste water (100–10,000g/m3) as feed solution to the PRO for all the hybrid configurations were studied. Their performances were compared through simulation and optimization studies. Among the six potentially viable SWRO–PRO configurations, the one which does the direct mixing of diluted PRO draw outlet with feed water of SWRO aided to bring down the NSEC by 49% in comparison with standard SWRO desalination system. This system does not require additional ERD units and turbine at optimized process conditions, which are more expensive.}
}
@article{OLIVEIRACIABATI2017748,
title = {SISPRENACEL – mHealth tool to empower PRENACEL strategy},
journal = {Procedia Computer Science},
volume = {121},
pages = {748-755},
year = {2017},
note = {CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.142},
url = {https://www.sciencedirect.com/science/article/pii/S187705091732344X},
author = {Livia Oliveira-Ciabati and Domingos Alves and Francisco Barbosa-Junior and Elisabeth Meloni Vieira and João Paulo Souza},
keywords = {mHealth, sociotechnical approach, maternal health, usability evaluation},
abstract = {Objective: this study aims to describe the development process of a web-based tool, capable of managing and delivering content through short message service (SMS) to pregnant women and their partners during antenatal and postnatal care (PRENACEL strategy). Methods: We used software engineer best practices to implement an adequate tool for PRENACEL researchers. We used sociotechnical approach to gather the requirements and built prototypes to quickly assess the developed functionalities. SISPRENACEL was created using CakePHP and MySQL and it is based on a client-server architecture, in order to make it accessible over the internet. To develop the graphic interface a free template was used, named AdminLTE version 1.0, created using the Bootstrap library. Results: SISPRENACEL delivered 22,296 scheduled SMS, received 1,249 messages and answered questions through 1,823 SMS. Besides that, we found out that using techniques that included the stakeholders in the development process resulted in a well evaluated, easy to use, pleasant looking and useful system. Conclusion: SISPRENACEL was an essential tool for bringing PRENACEL strategy to reality, managing and delivering content through SMS to women and their partners, without imposing a burden to health professionals.}
}
@article{SADRAMELI2015441,
title = {Mathematical modelling and simulation of thermal regenerators including solid radial conduction effects},
journal = {Applied Thermal Engineering},
volume = {76},
pages = {441-448},
year = {2015},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2014.11.035},
url = {https://www.sciencedirect.com/science/article/pii/S1359431114010333},
author = {S.M. Sadrameli and H.R.B. Ajdari},
keywords = {Heat recovery, Aluminium Melting Furnaces, Fixed-bed regenerator, Modelling, Finite difference},
abstract = {Increasing combustion fuels costs, depletion of fossil fuel reserves, and necessity to control environmental emissions, showing the importance of industrial heat recovery. For this purpose, different types of heat recovery systems have been designed and built. One of the most industrial applicable systems is a thermal regenerator that has widespread applications in energy industries such as glass, aluminium, and power plants. In aluminium and glass melting furnaces such system can be applied to preheat combustion air by absorbing heat from flue gases that results considerable fuel saving in the furnace. Due to the high temperature application of such systems, the packing must be constructed from low conductivity ceramic materials and the mechanism of heat transfer will be convection and radial conduction inside the solids. This paper presents the results of the mathematical modelling and simulation studies of a regenerator filled with spherical shape packing made from alumina with different diameters. For the modelling purposes two mathematical models have been considered; simplest convection model and more complex radial conduction model inside the packing particles. For the evaluation of the model and to study the effect of different parameters such as gas mass flow rate, period time and ball diameter on the performance of the system, an experimental setup has been designed and built. The results clearly show that decreasing of gas mass flow rate, period time and packing diameter increase the system efficiency. Finally, a simulation software package has been developed and used for the prediction of output parameters in two industrial cases and excellent agreement has been obtained that proves the accuracy of the model for the high temperature applications.}
}
@article{HOLIK2019333,
title = {Optimization of an organic Rankine cycle constrained by the application of compact heat exchangers},
journal = {Energy Conversion and Management},
volume = {188},
pages = {333-345},
year = {2019},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2019.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0196890419303346},
author = {Mario Holik and Marija Živić and Zdravko Virag and Antun Barac},
keywords = {Waste heat recovery, Heavy-duty truck engine, Organic Rankine cycle, Compact heat exchanger sizing, Multi-objective optimization},
abstract = {Recovering waste heat from the exhaust gases of heavy-duty diesel truck engines by using the organic Rankine cycle can reduce both fuel consumption and greenhouse gas emissions. The design of such systems is constrained by the space available for installation; thus, an optimization procedure is required. In this research work, a two-objective optimization, based on the maximization of power output and minimization of the total heat exchanger surface area, is implemented in the Engineering Equation Solver software package. Application of the developed procedure is illustrated in a case with an exhaust gas temperature of 394.2 °C and mass flow rate of 0.306 kg/s. Configurations both with and without a recuperator are analyzed, and four working fluids (water, ethanol, toluene, and hexamethyldisiloxane) are compared. Of these fluids, ethanol could be recommended as the best when considering the organic Rankine cycle power output and the total area/volume of heat exchangers. For a total surface area of 10 m2 in the case without the recuperator, the power output values for ethanol, water, and toluene are 17.6 kW, 17.2 kW, and 15.8 kW, respectively; with the recuperator, the power output is slightly higher, but the total area of heat exchangers is considerably larger. Again, the best working fluid is ethanol: for a total surface area of heat exchangers of 24 m2, the power output values for ethanol, toluene, and hexamethyldisiloxane are 18 kW, 17.1 kW, and 15.8 kW, respectively. The presented procedure is useful both for the conceptual design of a complete waste heat recovery unit and for the optimization of all heat exchangers.}
}
@article{BAZILIAN200257,
title = {Modelling of a photovoltaic heat recovery system and its role in a design decision support tool for building professionals},
journal = {Renewable Energy},
volume = {27},
number = {1},
pages = {57-68},
year = {2002},
issn = {0960-1481},
doi = {https://doi.org/10.1016/S0960-1481(01)00165-3},
url = {https://www.sciencedirect.com/science/article/pii/S0960148101001653},
author = {Morgan D. Bazilian and Deo Prasad},
abstract = {A numerical model has been created to simulate the performance of a residential-scale building integrated photovoltaic (BiPV) cogeneration system. The investigation examines the combined heat and power system in the context of heat transfer. The PV cogeneration system will be based on existing BiPV roofing technology with the addition of a modular heat recovery unit that can be used in new or renovation construction schemes. The convection of the air behind the panels will serve to cool the PV panels while providing a heat source for the residence. This model was created in the Engineering Equation Solver software package (EES), from a series of highly coupled non-linear partial differential equations that are solved iteratively. The model's ability to utilize climatic data to simulate annual performance of the system will be presented along with a comparison to experimental data. A graphical front-end has been added to the model in order to facilitate its use as a predictive tool for building professionals. It will thus become a decision support tool used in identifying areas for implementation of a PV cogen system.}
}
@article{JAMSAJOUNELA198945,
title = {A Simulation Study of Expert Control System for A Phosphate Flotation Process},
journal = {IFAC Proceedings Volumes},
volume = {22},
number = {11},
pages = {45-52},
year = {1989},
note = {6th IFAC Symposium on Automation in Mining, Mineral and Metal Processing 1989, Buenos Aires, Argentina, 4-8 September 1989},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)53087-6},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017530876},
author = {S.-L. Jämsä-Jounela and A. Niemi},
keywords = {Expert Control, Computer Aided Control System Design, Phosphate Rotation Control, Rotation Modelling},
abstract = {The parameters of a detailed phenomenological flotation model have been evaluated by steady state testing. The dynamic model has been used in simulation of the operation of a rougher flotation cell, as a step in development of expert control of a larger flotation system. The simulator has been interfaced with the ONSPEC control software package run in the IBM DOS environment. The control strategy aims to maximization of the recovery, while the concentrate grade is maintained above a given minimum level. The hierarchy of manipulated variables of the cell is described and the simulated results of expert control are presented.}
}
@incollection{SHINDER2008623,
title = {Chapter 15 - Collecting and Preserving Digital Evidence},
editor = {Littlejohn Shinder and Michael Cross},
booktitle = {Scene of the Cybercrime (Second Edition)},
publisher = {Syngress},
edition = {Second Edition},
address = {Burlington},
pages = {623-652},
year = {2008},
isbn = {978-1-59749-276-8},
doi = {https://doi.org/10.1016/B978-1-59749-276-8.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781597492768000157},
author = {Littlejohn Shinder and Michael Cross},
abstract = {Publisher Summary
This chapter discusses standard procedures for dealing with digital evidence, as well as specific evidence location and examination techniques. It also outlines procedures for documenting digital evidence and discusses some of the legal issues involved in evidence collection and handling. Evidence is the foundation of every criminal case, including those involving cybercrimes. The collection and preservation of digital evidence differs in many ways from the methods law enforcement officers are used to using for traditional types of evidence. Digital evidence is intangible, a magnetic or electronic representation of information. Its physical form does not readily reveal its nature. In addition, digital evidence is fragile. It is very easy for a criminal to deliberately delete crucial evidence in an instant or for an officer or technician to unintentionally damage or destroy it. In many cases, evidence that appears to be gone is still on the disk or other media and can be recovered. A number of data recovery software packages are on the market, several of which are designed specifically for computer forensic work and are marketed with law enforcement use in mind. Once one or more duplicates have been made, the original can be locked up securely in an evidence locker or evidence room until needed. Chain of custody must be maintained throughout the entire process. The duplicate disk can be examined for evidence of criminal activity.}
}
@article{PARTHASARATHY201619,
title = {Efficiency analysis of ERP packages—A customization perspective},
journal = {Computers in Industry},
volume = {82},
pages = {19-27},
year = {2016},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2016.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361516300690},
author = {Sudhaman Parthasarathy and Srinarayan Sharma},
keywords = {Customization, Data Envelopment Analysis (DEA), Efficiency, Enterprise Resource Planning (ERP), Software productivity},
abstract = {Enterprise Resource Planning (ERP) packages developed by ERP vendors are designed not only to standardize the existing business processes of the implementing organization, but also to bring in some of the best practices of the industry. There are many past studies on the assessment of the efficiency of standard ERP projects; however, to date no research has been conducted to assess the efficiency of ERP packages from the point of view of customization. Assessing the efficiency of customized ERP packages is vital for benchmarking best customization practices. In this study we examine the efficiency of customized ERP packages using Data Envelopment Analysis (DEA). We also examine the relationship between the degree of customization of ERP packages and their efficiency. Data was collected from an IT vendor who had deployed ERP package in 12 educational institutions. The results suggest that customization adversely affects the efficiency of ERP packages. We also discuss the implications of the results for research and practice.}
}
@article{OBEID2013259,
title = {Procurement of shared data instruments for Research Electronic Data Capture (REDCap)},
journal = {Journal of Biomedical Informatics},
volume = {46},
number = {2},
pages = {259-265},
year = {2013},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2012.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1532046412001608},
author = {Jihad S. Obeid and Catherine A. McGraw and Brenda L. Minor and José G. Conde and Robert Pawluk and Michael Lin and Janey Wang and Sean R. Banks and Sheree A. Hemphill and Rob Taylor and Paul A. Harris},
keywords = {Validated instruments, Data collection, Data sharing, Informatics, Translational research, REDCap},
abstract = {REDCap (Research Electronic Data Capture) is a web-based software solution and tool set that allows biomedical researchers to create secure online forms for data capture, management and analysis with minimal effort and training. The Shared Data Instrument Library (SDIL) is a relatively new component of REDCap that allows sharing of commonly used data collection instruments for immediate study use by research teams. Objectives of the SDIL project include: (1) facilitating reuse of data dictionaries and reducing duplication of effort; (2) promoting the use of validated data collection instruments, data standards and best practices; and (3) promoting research collaboration and data sharing. Instruments submitted to the library are reviewed by a library oversight committee, with rotating membership from multiple institutions, which ensures quality, relevance and legality of shared instruments. The design allows researchers to download the instruments in a consumable electronic format in the REDCap environment. At the time of this writing, the SDIL contains over 128 data collection instruments. Over 2500 instances of instruments have been downloaded by researchers at multiple institutions. In this paper we describe the library platform, provide detail about experience gained during the first 25months of sharing public domain instruments and provide evidence of impact for the SDIL across the REDCap consortium research community. We postulate that the shared library of instruments reduces the burden of adhering to sound data collection principles while promoting best practices.}
}
@article{GARCIA2011834,
title = {Design guidelines for software processes knowledge repository development},
journal = {Information and Software Technology},
volume = {53},
number = {8},
pages = {834-850},
year = {2011},
note = {Advances in functional size measurement and effort estimation - Extended best papers},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911000619},
author = {Javier García and Antonio Amescua and María-Isabel Sánchez and Leonardo Bermón},
keywords = {Software engineering, Software process technology, Knowledge management, Agile development, Web 2.0, Wiki},
abstract = {Context
Staff turnover in organizations is an important issue that should be taken into account mainly for two reasons: 1.Employees carry an organization’s knowledge in their heads and take it with them wherever they go2.Knowledge accessibility is limited to the amount of knowledge employees want to share
Objective
The aim of this work is to provide a set of guidelines to develop knowledge-based Process Asset Libraries (PAL) to store software engineering best practices, implemented as a wiki.
Method
Fieldwork was carried out in a 2-year training course in agile development. This was validated in two phases (with and without PAL), which were subdivided into two stages: Training and Project.
Results
The study demonstrates that, on the one hand, the learning process can be facilitated using PAL to transfer software process knowledge, and on the other hand, products were developed by junior software engineers with a greater degree of independence.
Conclusion
PAL, as a knowledge repository, helps software engineers to learn about development processes and improves the use of agile processes.}
}
@article{ERRICO20091642,
title = {Energy saving in a crude distillation unit by a preflash implementation},
journal = {Applied Thermal Engineering},
volume = {29},
number = {8},
pages = {1642-1647},
year = {2009},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2008.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S1359431108003177},
author = {Massimiliano Errico and Giuseppe Tola and Michele Mascia},
keywords = {Preflash drum, Preflash column, Energy saving, Crude distillation unit},
abstract = {After the 70s energy crisis the revamping of plants designed before this date is very attractive for improving energy recovery and lowering operation costs. A typical case is the oil refinery plant where an intensive usage of energy takes place and is a promising case for the application of energy saving solutions. In this work we focused our attention to an industrial crude oil distillation unit, evaluating the possibility to modify the feed conditions by installing a preflash drum or a preflash plate column. Real data plant were collected to obtain a reliable simulation of the unit by means of the software package Aspen Plus 13.0. To characterize the crude oil fed the TBP curve was used. The results obtained were compared with the plant data in terms of flow rate and product quality utilizing the ASTM D-86 curves and a good agreement was obtained. According to the specialized literature the preflash drum/column was placed at the end of the pre-heat train, just before the column furnace. The furnace is the bottleneck of the plant and with both the preflash devices it is possible to lower its energy consumption. However the energy reduction is associated to the decrease of one kind of distillates (light or middle). The choice of the best preflash device was made according to the production asset of the plant.}
}
@article{WANG201245,
title = {Application of intensified heat transfer for the retrofit of heat exchanger network},
journal = {Applied Energy},
volume = {89},
number = {1},
pages = {45-59},
year = {2012},
note = {Special issue on Thermal Energy Management in the Process Industries},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2011.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0306261911001760},
author = {Yufei Wang and Ming Pan and Igor Bulatov and Robin Smith and Jin-Kuk Kim},
keywords = {Heat exchanger network (HEN), Retrofit, Heat transfer enhancement (HTE), Heat exchanger model, Heuristics},
abstract = {A number of design methods have been proposed for the retrofit of heat exchanger networks (HEN) during the last three decades. Although considerable potential for energy savings can be identified from conventional retrofit approaches, the proposed solutions have rarely been adopted in practice, due to significant topology modifications required and resulting engineering complexities during implementation. The intensification of heat transfer for conventional shell-and-tube heat exchangers can eliminate the difficulties of implementing retrofit in HEN which are commonly restricted by topology, safety and maintenance constraints, and includes high capital costs for replacing equipment and pipelines. This paper presents a novel design approach to solve HEN retrofit problems based on heat transfer enhancement. A mathematical model has been developed to evaluate shell-and-tube heat exchanger performances, with which heat-transfer coefficients and pressure drops for both fluids in tube and shell sides are obtained. The developed models have been compared with the Bell-Delaware, simplified Tinker and Wills–Johnston methods and tested with the HTRI® and HEXTRAN® software packages. This demonstrates that the new model is much simpler but can give reliable results in most cases. For the debottlenecking of HEN, four heuristic rules are proposed to identify the most appropriate heat exchangers requiring heat transfer enhancements in the HEN. The application of this new design approach allows a significant improvement in energy recovery without fundamental structural modifications to the network.}
}
@article{HWANGBO2020106910,
title = {Design of control framework based on deep reinforcement learning and Monte-Carlo sampling in downstream separation},
journal = {Computers & Chemical Engineering},
volume = {140},
pages = {106910},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106910},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419310750},
author = {Soonho Hwangbo and Gürkan Sin},
keywords = {Liquid-liquid extraction column, Deep reinforcement learning, Monte-Carlo sampling, Control system, API production, Biopharmaceuticals},
abstract = {This paper proposes a systematic framework to develop deep reinforcement learning (RL)-based algorithms for control system of downstream separation in biopharmaceutical process as follows. First, a simulation model as a digital twin is built and Monte-Carlo sampling generates substantial amounts of samples considering disturbances. Second, the deep RL-based control system is designed and the optimization subject to sample datasets is conducted. The methodology is implemented in a prototype software and relevant codes are shared by Mendeley Data. The proposed model is successfully applied to control the liquid-liquid extraction column for the recovery of fusidic acid as part of downstream processing. The resulting deep RL algorithm provides an operation performance with a better API recovery yield (32 % higher than open loop operation) and lower deviations (23 % lower than open loop operation) against disturbances.}
}
@incollection{SHIVAKUMAR2015101,
title = {3 - Optimizing Performance of Enterprise Web Application},
editor = {Shailesh Kumar Shivakumar},
booktitle = {Architecting High Performing, Scalable and Available Enterprise Web Applications},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {101-141},
year = {2015},
isbn = {978-0-12-802258-0},
doi = {https://doi.org/10.1016/B978-0-12-802258-0.00003-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128022580000032},
author = {Shailesh Kumar Shivakumar},
keywords = {Performance modeling, performance-based design, bottleneck analysis, continuous monitoring, performance governance, performance modeling, workload modeling, performance patterns, performance design guidelines, caching, distributed and parallel computing, lightweight design, asynchronous and on-demand data request, batching, standards-based technology, continuous and iterative build and testing, HTML 5 optimization, responsive web design (RWD), smart asset proxy, semantic progressive loading, rapid rendering framework techniques, content chunking strategy, monitoring, web analytics-based monitoring},
abstract = {Fast and responsive web pages are critical to success factors of a website. An optimized web provides the competitive advantage and increases the overall user experience. It has direct impact on business drivers such as site traffic, conversion ratio, and user satisfaction index. With the proliferation of ubiquitous devices and mobile platforms, the speed of the web is more relevant than ever. Web performance optimization (WPO) is mainly related to making the web components, such as web pages, perform faster and render in the most optimal way on the user device so as to provide a positive user experience. Web components include web pages, widgets, client-side components, JavaScript libraries, static assets, and others. Normally, for a web application, the web page forms the final component in the entire delivery chain. This chapter elaborates a complete strategy for WPO. The chapter takes a 360° approach in analyzing various dimensions of WPO, including performance optimization principles at each of the project lifecycle phases, common pitfalls in page development, caching strategy, monitoring and maintenance strategy, infrastructure guidelines, and fine-tuning existing web pages. The chapter also elaborates performance metrics, performance governance framework, and performance techniques for the entire ecosystem. Finally, we see the tools that can be leveraged for implementing the strategies discussed. The optimization techniques explained in this chapter are drawn from various real-world performance engineering programs where the strategy was successfully implemented to meet the challenging performance SLAs across geographies. Performance engineering is a multilayer exercise involving software components at various layers in an n-tier architecture. Equally, hardware also plays a key role in ensuring optimal performance for the application. This chapter predominantly focuses on the web tier wherein the optimization techniques for presentation components are discussed in detail. There is a brief section related to performance best practices that can be implemented at other layers.}
}
@article{CHILESHE2019102858,
title = {Information flow-centric approach for reverse logistics supply chains},
journal = {Automation in Construction},
volume = {106},
pages = {102858},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102858},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519300573},
author = {Nicholas Chileshe and Ruchini Senerath Jayasinghe and Raufdeen Rameezdeen},
keywords = {API, Deconstruction, Reverse logistics supply chain, Resource recovery, Information, Conceptual model, Building information modelling},
abstract = {Reverse logistics supply chain (RLSC) helps resources recovery at the End-of-life (EoL) of buildings. However, there is a huge potential for improving the existing RLSC through systematic information management. Very little research has been carried out on the interactions between RLSC operations and information needs for an efficient and effective resources recovery process. This study proposes a conceptual model of a building information model that could effectively address the information needs during demolition, testing, grading and reprocessing of salvaged materials. The study uses semi-structured interviews and action research to identify information needs and Building Information Modelling (BIM) functionalities to develop the conceptual model using an Application Programming Interface (API) plug-in. The model when fully developed can extract the volumes of salvage, sorting, testing, reprocessing and integrate that information for optimised resources recovery. The extracted information in a single platform can help demolition planners and recyclers to design downstream operations, particularly testing and grading of salvaged products enabling an information flow-centric reverse logistics supply chain.}
}
@article{SALOGNI2010464,
title = {Modeling of solid oxide fuel cells for dynamic simulations of integrated systems},
journal = {Applied Thermal Engineering},
volume = {30},
number = {5},
pages = {464-477},
year = {2010},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2009.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1359431109002981},
author = {A. Salogni and P. Colonna},
keywords = {SOFC, Dynamic modeling, Causal approach, Acausal software, Integrated systems, Lumped parameters},
abstract = {Solid oxide fuel cell (SOFC) stacks are at the core of complex and efficient energy conversion systems for distributed power generation. Such systems are currently in various stages of development. These power plants of the future feature complicated configurations, because the fuel cell demands for a complex balance of plant. Moreover, proposed SOFC-based systems for stationary applications are often connected to additional components and subsystems, such as a gasifier with its gas-cleaning section, a gas turbine, and a heat recovery system for thermal cogeneration or additional power production. For the simplest SOFC configurations, and more so for complex integrated systems, the dynamic operation of the power plant is challenging, especially because the fluctuating electrical load of distributed energy systems demand for reliable transient operation. Issues related to dynamic operation must be studied in the early design stage and simulation results can be used to optimize the system configuration, taking into account transient behavior. This paper presents the development and the validation of a non-linear dynamic lumped-parameters model of a SOFC stack suitable for integration into models of complex power plants. Particular emphasis is placed on the systematic approach to model development. The model is implemented using the open-source Modelica language, which allows for a high degree of flexibility and modularity, the main features of the model herein presented. The SOFC stack model will be incorporated into ThermoPower, a freely distributed library of reusable software components for the modeling of thermo-hydraulic processes and power plants.}
}
@article{FEDER2022858,
title = {An approach for automatic generation of the URDF file of modular robots from modules designed using SolidWorks},
journal = {Procedia Computer Science},
volume = {200},
pages = {858-864},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.283},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002927},
author = {Maddalena Feder and Andrea Giusti and Renato Vidoni},
keywords = {Modular reconfigurable robots, Industry 4.0, Robot Operating System, URDF, Kinematics},
abstract = {Modular robot manipulators promise to enhance current automation practice by providing higher flexibility and quick recovery in case of failures with respect to fixed-structure robots. The configuration of the modular robot control for each new assembly, to account for the new system kinematics and dynamics, can be time consuming and requires robot modelling expertise. We propose an approach for automatically generating the Unified Robot Description Format (URDF) file of modular robot manipulators, starting from the kinematic and dynamic descriptions expressed following the URDF of the single modules they can be composed of. The approach has been implemented and numerically verified by exploiting off-the-shelf software tools from Robot Operating System (ROS) libraries.}
}
@incollection{BARCLAY2004241,
title = {8 - Design patterns},
editor = {K. Barclay and J. Savage},
booktitle = {Object-Oriented Design with UML and Java},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {241-276},
year = {2004},
isbn = {978-0-7506-6098-3},
doi = {https://doi.org/10.1016/B978-075066098-3/50008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780750660983500088},
author = {K. Barclay and J. Savage},
abstract = {Publisher Summary
This chapter focuses on the importance of software design patterns in object-oriented development. The chapter investigates and demonstrates a range of design patterns, captured through unified modeling language (UML) diagrams and implemented in Java. The chapter also demonstrates a number of illustrations, how these design patterns can be exploited. Patterns are ways to describe best practices, good designs, and capture experience in a way that it is possible for others to reuse this experience. Specialization and delegation are widely used in object-oriented systems. Both provide powerful ways of reusing code. Delegation can often be used in place of specialization, offering flexibility at run-time. The adapter design pattern is used to introduce a class with the required set of services that is realized by another class that has the wrong set of services for a client. The decorator pattern is used to dynamically add new functionality to an object. Many of these design patterns have been incorporated into the Java API.}
}