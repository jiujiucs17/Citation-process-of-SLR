@article{FELDMANN2013210,
title = {Model-Driven Engineering and Semantic Technologies for the Design of Cyber-Physical Systems},
journal = {IFAC Proceedings Volumes},
volume = {46},
number = {7},
pages = {210-215},
year = {2013},
note = {11th IFAC Workshop on Intelligent Manufacturing Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20130522-3-BR-4036.00050},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015356767},
author = {S. Feldmann and S. Rösch and D. Schütz and B. Vogel-Heuser},
keywords = {manufacturing systems, knowledge-based systems, cyber-physical systems},
abstract = {Rapid changes in product and system requirements force today's companies to cope with unforeseen adaptations of industrial plants. To enable the implementation of intelligent, self-adapting systems, manufacturing systems must be composed to Cyber-Physical Systems (CPS) out of Cyber-Physical Modules (CPMs). However, cross-disciplinary design of CPS and the multitude of persons involved in this process increases complexity in (re-)engineering drastically. Identification and modeling of typical CPM types in a CPM library provides a first step towards enabling reuse of constructed and verified CPMs and, thus, a knowledge base making automatic composition of CPS out of CPMs possible. In this paper, a concept for retrieving existing automation software functionality from a given hardware description by means of model-driven engineering and semantic technologies is presented.}
}
@article{USEVICH2017176,
title = {Variable projection methods for approximate (greatest) common divisor computations},
journal = {Theoretical Computer Science},
volume = {681},
pages = {176-198},
year = {2017},
note = {Symbolic Numeric Computation},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2017.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S0304397517302505},
author = {Konstantin Usevich and Ivan Markovsky},
keywords = {Approximate GCD, Structured low-rank approximation, Variable projection, Mosaic Hankel matrices, Least squares problem, Weighted 2-norm},
abstract = {We consider the problem of finding for a given N-tuple of polynomials (real or complex) the closest N-tuple that has a common divisor of degree at least d. Extended weighted Euclidean seminorm of the coefficients is used as a measure of closeness. Two equivalent representations of the problem are considered: (i) direct parameterization over the common divisors and quotients (image representation), and (ii) Sylvester low-rank approximation (kernel representation). We use the duality between least-squares and least-norm problems to show that (i) and (ii) are closely related to mosaic Hankel low-rank approximation. This allows us to apply to the approximate common divisor problem recent results on complexity and accuracy of computations for mosaic Hankel low-rank approximation. We develop optimization methods based on the variable projection principle both for image and kernel representation. These methods have linear complexity in the degrees of the polynomials for small and large d. We provide a software implementation of the developed methods, which is based on a software package for structured low-rank approximation.}
}
@article{LANDAU1998155,
title = {The R-S-T digital controller design and applications},
journal = {Control Engineering Practice},
volume = {6},
number = {2},
pages = {155-165},
year = {1998},
issn = {0967-0661},
doi = {https://doi.org/10.1016/S0967-0661(98)00016-1},
url = {https://www.sciencedirect.com/science/article/pii/S0967066198000161},
author = {I.D. Landau},
keywords = {System identification, digital control, robust control, adaptation, software tools},
abstract = {The two-degrees-of-freedom R-S-T digital controller is becoming a standard for computer control in industry. This paper presents a methodology for the design of the R-S-T controller, which involves identification of the plant model from data, combined with a robust control design. The performance of the controller can be further enhanced by plant model identification in a closed loop, and re-tuning of the controller. For large parameter variations, adaptation has to be considered in order to maintain the performance. Software packages are available for the design, implementation and commissioning of the R-S-T digital controllers. The methodology is illustrated by its application to the control of deposited zinc in hot-dip galvanizing at SOLLAC (Florange, France).}
}
@article{HALPERIN1998273,
title = {A perturbation scheme for spherical arrangements with application to molecular modeling},
journal = {Computational Geometry},
volume = {10},
number = {4},
pages = {273-287},
year = {1998},
note = {Special Issue on Applied Computational Geometry},
issn = {0925-7721},
doi = {https://doi.org/10.1016/S0925-7721(98)00014-5},
url = {https://www.sciencedirect.com/science/article/pii/S0925772198000145},
author = {Dan Halperin and Christian R. Shelton},
abstract = {We describe a software package for computing and manipulating the subdivision of a sphere by a collection of (not necessarily great) circles and for computing the boundary surface of the union of spheres. We present problems that arise in the implementation of the software and the solutions that we have found for them. At the core of the paper is a novel perturbation scheme to overcome degeneracies and precision problems in computing spherical arrangements while using floating point arithmetic. The scheme is relatively simple, it balances between the efficiency of computation and the magnitude of the perturbation, and it performs well in practice. In one O(n) time pass through the data, it perturbs the inputs necessary to insure no potential degeneracies and then passes the perturbed inputs on to the geometric algorithm. We report and discuss experimental results. Our package is a major component in a larger package aimed to support geometric queries on molecular models; it is currently employed by chemists working in “rational drug design”. The spherical subdivisions are used to construct a geometric model of a molecule where each sphere represents an atom.}
}
@article{VUKOBRATOVIC1991441,
title = {Program Package for Generation of Control Laws for Robot Manipulators in Symbolic Form},
journal = {IFAC Proceedings Volumes},
volume = {24},
number = {9},
pages = {441-446},
year = {1991},
note = {3rd IFAC Symposium on Robot Control 1991 (SYROCO'91), Vienna, Austria, 16-18 September 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)51096-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017510964},
author = {M. Vukobratović and N. Kirćanski and A. Timčenko},
keywords = {dynamic control, symbolic models, computational complexity},
abstract = {This paper presents a new program package for the generation of control laws for robot manipulators in symbolic form. Since the computational efficiency of the generated control laws is extremely high, the real-time implementation become possible even on low-cost microcomputers. This program package represents an extension of the previously developed program package SYM which is designed to generate efficient manipulator models (Timčenko, Kirćanski, Vukobratović 1991). The basic algorithm belongs to the class of customized algorithms that reduce the computational burden by taking into account the specific characteristics of the manipulator to be controlled. It generates the high-level program code for computing various control laws based on inverse robot dynamics. The program code is highly optimized from the standpoint of numerical complexity. It represents a series of assignment statements with simple float variables and constants. Matrix and vector computations, loops, and conditional branches are avoided. The software modules for generation of various kinds of control laws as well as for simulation of robot moving along a defined path are incorporated into SYM software package and demonstrated in this paper. Generated models and control laws are implemented in several robot controllers designed to drive a 6 degrees of freedom robots.}
}
@article{CHIACCHIO2020106904,
title = {A general framework for dependability modelling coupling discrete-event and time-driven simulation},
journal = {Reliability Engineering & System Safety},
volume = {199},
pages = {106904},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2020.106904},
url = {https://www.sciencedirect.com/science/article/pii/S0951832019303655},
author = {Ferdinando Chiacchio and Alessandra Iacono and Lucio Compagno and Diego D'Urso},
keywords = {Stochastic hybrid automaton, Dynamic reliability, Multi-state systems modelling, Monte carlo simulation, Repairable dynamic fault trees},
abstract = {Analysis of complex failure scenarios and mitigation procedures of an industrial plant is one of the most important activity for the safety of the factory, the personnel and the surrounding areas. The dependability assessment of such systems is fulfilled by risk experts who, adopting well-known Reliability, Availability, Maintenance and Safety (RAMS) techniques, design and solve the stochastic failure model of the system. Traditional techniques like Fault Tree Analysis (FTA) or Reliability Block Diagrams (RBD) are of easy implementation but unrealistic, due to their simplified hypotheses that assume the components malfunction to be independent from each other and from the system working conditions. Dynamic Probabilistic Risk Assessment (DPRA) is the umbrella framework encompassing new mathematical and simulation formalisms aiming to relax the constraints of traditional techniques and increase the accuracy of dependability assessment. At the state of the art, DPRA cannot boast a well-defined methodology because the nature of a dynamic reliability problem can be so complex to require an ad-hoc modelling and resolution. Moreover, one of the main issues encountered by risk-practitioners is that there is a small support in terms of available tools or expert systems, specifically designed for DPRA problems. To tackle this lack, this paper presents the conception of general framework for the modelling and the simulation of a Stochastic Hybrid Fault Tree Automaton (SHyFTA), one of the most promising DPRA methodologies, able to combine Dynamic Fault Tree (DFT) with the deterministic model of the system process. The logic of the repairable DFT gates and the concepts for the implementation of a simulation engine combining Discrete Event Simulation (DES) and Time Driven Simulation (TDS) are illustrated and, a Matlab® toolbox library (SHyFTOO) has been coded and tested with a thorough validation campaign. Finally, a common case study in industrial engineering has been modelled and analysed under different stand-by configurations in order to demonstrate the modelling flexibility of the toolbox.}
}
@article{PAPAEFSTATHIOU2004561,
title = {Design-space exploration of the most widely used cryptography algorithms},
journal = {Microprocessors and Microsystems},
volume = {28},
number = {10},
pages = {561-571},
year = {2004},
note = {Secure Computing Platforms},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2004.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0141933104001012},
author = {I. Papaefstathiou and V. Papaefstathiou and C. Sotiriou},
keywords = {Security protocols, AES, DES, Asynchronous circuits},
abstract = {Network data are, currently, often encrypted at a low level. In addition, as it is widely supported, the majority of future networks will use low-layer (IP level) encryption. Moreover, current trends imply that future networks are likely to be dominated by mobile terminals, thus, the power consumption and electromagnetic emissions aspects of encryption devices will be critical. This paper presents several realizations of the two most widely used encryption algorithms, DES and AES, both in software and in hardware. We present software implementations of the algorithms running on two of the state-of-the-art Intel IXP Network Processors and 11 hardware realizations based on a standard-cell library. In particular, five of our hardware realizations are conventional flip-flop based clocked designs, whereas the other six are either asynchronous, or latch-based synchronous designs. We demonstrate that the most efficient realization of the DES algorithm is one of the proposed asynchronous hardware implementations, whereas for the AES algorithm the latch-based design presented seems to be optimal. By placing and routing those designs, we have also realized that the commercial ASIC synthesis tools cannot accurately predict the area and the performance of the placed and routed final netlist in such designs, since the ASIC implementations of the encrypted algorithms include a very large number of wires and a limited number of logic CMOS cells.}
}
@article{AHMAD2011188,
title = {The Means of Escaping for Occupants for Renovation Works of Terrace Houses in Malaysia},
journal = {Procedia Engineering},
volume = {20},
pages = {188-192},
year = {2011},
note = {2nd International Building Control Conference},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.11.155},
url = {https://www.sciencedirect.com/science/article/pii/S187770581102964X},
author = {H. Ahmad and M.Y. Fadlie and N.A. Yahaya and J. Abu},
keywords = {comfort living, escaping from fire, renovation culture, terrace house},
abstract = {The Uniform Building By-Law (UBBL 1984) of Malaysia is a reference about the minimum and maximum limits of occupant's comfort and has been implemented since 1984. As a practice norm, all modern houses must follow those limitations suggested. However, pilot survey shows many occupants still renovate their houses and concurrently death causes of fire are increasing. Therefore, the objective of this article is to bring out the actual facts about the enforcement culture and it is relevance against UBBL. This research only focuses on means of escaping for occupants; dead-end distance and grilles fixed to windows/doors. Through structured interview and questionnaire survey, analysis using software of Statistical Package for Social Sciences (SPSS) of this research found that certain area of UBBL requirements such as room extending and materials used were overruled by non compliance persons. This output could be implemented by Local Authority to enforce the culture of house renovation for comfort living.}
}
@article{RATCHEV2004481,
title = {Material removal simulation of peripheral milling of thin wall low-rigidity structures using FEA},
journal = {Advances in Engineering Software},
volume = {35},
number = {8},
pages = {481-491},
year = {2004},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2004.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S096599780400105X},
author = {Svetan Ratchev and Stan Nikov and Idir Moualek},
keywords = {Voxel, Material removal, Finite elements, Simulation},
abstract = {The paper reports on a new methodology for simulation of the material removal process that takes into account the deflection of the part during machining. A finite element part model is used as an input to the simulation. The algorithm aims to iteratively identify the intersection between the tool volume and the deflected part taking into account the applied cutting forces. A new voxel-transformation model and algorithm have been developed to allow accurate representation of cutting through the part voxels intersected by the tool volume. A look-up table describing all possible voxel-cutting schemes has been developed and validated to facilitate the material removal simulation. The methodology has been implemented as a software package that allows integration with mainstream commercial finite element analysis and CAD packages.}
}
@article{XU1988397,
title = {An Applied Software Package for System Identification and its Industrial Practice},
journal = {IFAC Proceedings Volumes},
volume = {21},
number = {8},
pages = {397-402},
year = {1988},
note = {4th IFAC Symposium on computer aided Design in Control Systems 1988, Beijing, PRC, 23-25 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)54985-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701754985X},
author = {Sixin Xu and Wenzhong Song and Dong Huang},
keywords = {Applied software package, multivariable system, identification algorithms, model conversion, power plant},
abstract = {In this paper, an applied software packag for identifying MIMO system is introduce. It is suitable for implementation on microcomputers. The package contains six algorithms of parameter estimation and five methods of determining model structure. A new model structure test method viz. model response comparison method and several transformation algorithms in recusive form from input-output difference equation into other model forms are included in the package. The modelling procedure involves two steps:(1), looking for a middle model with high order which dynamic is approximate to true object; (2), reducing model order to obtain a suitable parameteric model. The package is equipped for a special microcomputer to identify the dynamics. The identification experiments for a 350 MW boiler-turbine-generater (BTG) set by using the special microcomputer have been carried out, and the data are processed off-line by the package. Then the more overall mathematical models of the BTG set are obtained in the form of transfer functions.}
}
@article{KAZEMI2019118,
title = {A practical framework for implementing multivariate monitoring techniques into distributed control system},
journal = {Control Engineering Practice},
volume = {82},
pages = {118-129},
year = {2019},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0967066118306282},
author = {Z. Kazemi and A.A. Safavi and S. Pouresmaeeli and F. Naseri},
keywords = {Advanced monitoring, Distributed control system (DCS), DCS SIMATIC PCS7, Gas pressure regulating station (GPRS), Principal component analysis (PCA), Support vector machine (SVM)},
abstract = {A large number of advanced multivariate techniques have been yet proposed to improve the performance of process monitoring. These advanced monitoring methods are usually implemented on separate hardware/software, which interconnect with Distributed Control System (DCS) using extra communication links. In this paper, the advanced monitoring algorithms are directly implemented and embedded into the DCS structure for the first time. Hence, the need to extra computers connected to the DCS will be eliminated, which brings in several advantages from the performance perspective. In the proposed approach, the advanced monitoring technique is first simplified and split into several functions. Next, Dynamic Link Libraries (DLLs) are created and used for fast execution of different functions in the DCS. Finally, using the generated DLLs and defining functions in WinCC, the monitoring algorithm is executed in real-time. In addition to fault detection, a fault classification algorithm is proposed, which effectively identifies different disturbances in the process. For this purpose, Support Vector Machine (SVM) classifier is used. The advanced monitoring technique is implemented in DCS SIMATIC PCS7 from Siemens©. The proposed condition monitoring method is tested on a real Gas Pressure Regulating Station (GPRS). The feasibility and effectiveness of the proposed method is experimentally confirmed.}
}
@article{PINO1986623,
title = {Information retrieval using microcomputers},
journal = {Microprocessing and Microprogramming},
volume = {18},
number = {1},
pages = {623-628},
year = {1986},
note = {Microarchitectures, Developments and Applications},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(86)90100-6},
url = {https://www.sciencedirect.com/science/article/pii/0165607486901006},
author = {JoséA. Pino},
abstract = {The development of a text retrieval software system for microcomputers is described. The system design objectives include simplicity and generality. The first is achieved by the use of self-explanatory menus. The second is realized by letting the user define the database structure, and leaving outside the system any functions already accomplished by other software. The inverted file required for the system was implemented as a prefix B-tree. The software was programmed in C and runs under the UNIX operating system. Two applications of this software are presented. One of them is a bibliographic retrieval system for a small library. The other one is an office automation project.}
}
@article{SANTOS2017732,
title = {EPICS device support for an ATCA CDAQ Board with hot-plug capabilities},
journal = {Fusion Engineering and Design},
volume = {123},
pages = {732-736},
year = {2017},
note = {Proceedings of the 29th Symposium on Fusion Technology (SOFT-29) Prague, Czech Republic, September 5-9, 2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2017.03.174},
url = {https://www.sciencedirect.com/science/article/pii/S0920379617306348},
author = {Bruno Santos and Paulo F. Carvalho and Antonio J.N. Batista and Miguel Correia and A.P. Rodrigues and Bernardo B. Carvalho and Jorge Sousa and Álvaro M. Combo and Nuno Cruz and Carlos M.B.A. Correia and Bruno Gonçalves},
keywords = {EPICS, ATCA, Hot-plug, Data acquisition, ITER, Nuclear fusion},
abstract = {The Advanced Telecommunications Computing Architecture (ATCA) standard defines a high performance technical solution that meets the requirements for fast controllers on large-scale physics experiments like ITER. This platform provides high throughput, scalability and features for high availability such as redundancy and intelligent platform management which are essential for steady state experiments. An ATCA Control and Data Acquisition (CDAQ) demonstration system was developed for the ITER Fast Plant System Controller (FPSC) project, which is already available in the ITER Catalog. This system comprises a board with 48 galvanic isolated analog and/or digital channels configurable as input or output with digital signal processing capabilities performed by a Field Programmable Gate Array (FPGA) and an ATCA carrier. The Experimental Physics and Industrial Control System (EPICS) is a set of open source software tools, libraries and applications used worldwide to create distributed soft real-time control systems for scientific instruments. To provide the hardware integration on the EPICS environment, a device support has been developed as a software layer, which is comparable to the abstraction layer provided by the device driver on the Operating Systems. This paper presents the implementation and test of an EPICS Device Support for the ATCA CDAQ Board that comprises templates for easy configuration of the entire system. This solution also allows simultaneous and independent acquisition by each board, providing hot-plug features which support insertion and removal of boards while keeping other modules and the overall system running. Operation with several boards and different versions of Linux operating system was performed and the results are presented.}
}
@article{QUITADAMO2008221,
title = {Mobile JikesRVM: A framework to support transparent Java thread migration},
journal = {Science of Computer Programming},
volume = {70},
number = {2},
pages = {221-240},
year = {2008},
note = {Special Issue on Principles and Practices of Programming in Java (PPPJ 2006)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2007.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S016764230700175X},
author = {Raffaele Quitadamo and Giacomo Cabri and Letizia Leonardi},
keywords = {Java virtual machine, Thread migration, Code mobility, JikesRVM, Distributed applications},
abstract = {Today’s complex applications must face the distribution of data and code among different network nodes. Computation in distributed contexts is demanding increasingly powerful languages and execution environments, able to provide programmers with appropriate abstractions and tools. Java is a wide-spread language that allows developers to build complex software, even distributed, but it cannot handle the migration of computations (i.e. threads), due to intrinsic limitations of many traditional JVMs. After analyzing the approaches in the literature, this paper presents our thread migration framework (called Mobile JikesRVM), implemented on top of the IBM Jikes Research Virtual Machine (RVM): exploiting some of the innovative techniques in the JikesRVM, we implemented an extension of its scheduler that allows applications to easily capture the state of a running thread and makes it possible to restore it elsewhere (i.e. on a different hardware architecture or operating system), but still with a version of the framework installed). Our thread serialization mechanism provides support for both proactive and reactive migration, available also for multi-threaded Java applications, and tools to deal with the problems of resource relocation management. With respect to previous approaches, we implemented Mobile JikesRVM without recompiling its JVM (Java Virtual Machine) source code, but simply extending JikesRVM functionalities with a full Java package to be imported when thread migration is needed.}
}
@article{NYEMBA2019285,
title = {Optimization of the design and manufacture of a solar-wind hybrid street light},
journal = {Procedia Manufacturing},
volume = {35},
pages = {285-290},
year = {2019},
note = {The 2nd International Conference on Sustainable Materials Processing and Manufacturing, SMPM 2019, 8-10 March 2019, Sun City, South Africa},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919306778},
author = {Wilson R. Nyemba and Simon Chinguwa and Innocent Mushanguri and Charles Mbohwa},
keywords = {Diffuser, photovoltaic module, renewable energy},
abstract = {The demand for electricity has escalated and cannot be fulfilled by conventional energy sources alone. There has been a rising demand to seek new renewable energy sources. Although solar and wind energy are the most cost effective renewable energy sources, they are unreliable due to the sporadic nature of their occurrence, if implemented as standalones. In Zimbabwe, solar street lighting has been implemented since 2014 as a solution to the erratic power supplies and outages. Wind potential in Zimbabwe has been identified at elevated heights, with Gweru having the maximum power density of 115 W/m2 at 50 m hub height. This paper presents the optimization of the design of a hybrid renewable energy system (HRES) of solar and wind energy to power a 160W streetlight. The system consisted of a wind turbine, photovoltaic modules, charge controller, battery bank and lights. The system sizing was done in Excel using wind and solar data obtained from the database, HOMER Software Package and PVSyst. The 3D streetlight was modelled using Inventor Professional and a working prototype was manufactured. The results showed that the HRES reduced the energy storage requirements by 38.75% with an overall cost reduction of 14.4%, relative to a standalone solar streetlight. The diffuser effect to the turbine was experimentally assessed, showing 69.3% increase in turbine power output and a 50% decrease in energy storage requirements. Further research can be carried to improve the reliability for standalone systems.}
}
@article{USTUNISIK2019169,
title = {Design and Simulation of ANFIS Controller for Increasing the Accuracy of Leaf Spring Test Bench},
journal = {Procedia Computer Science},
volume = {158},
pages = {169-176},
year = {2019},
note = {3rd WORLD CONFERENCE ON TECHNOLOGY, INNOVATION AND ENTREPRENEURSHIP"INDUSTRY 4.0 FOCUSED INNOVATION, TECHNOLOGY, ENTREPRENEURSHIP AND MANUFACTURE" June 21-23, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311998},
author = {Elif Üstünışık and Ahmet Kırlı},
keywords = {Co-simulation, ANFIS, Leaf spring, Control, Correlation},
abstract = {Artificial Intelligence (AI) has been in use in several research fields and industries, including automotive. Intelligent control is a control technique that use different AI approaches like genetic algorithm, machine learning, neural networks and fuzzy logic. In this study, Adaptive Neuro-Fuzzy Inference System (ANFIS) is used for the correlation of the experiment and simulation results of a 5 degrees of freedom (DOF) servo-hydraulic leaf spring test bench. A multi-body simulation (MBS) software named Simpack is used to model the actual leaf spring test bench in the simulation environment. However, the results of the simulations do not fully correlate with the results of experiments for the same scenarios. Simpack is a dynamic analyses software and is not a control design tool, but it has an interface that exchanges data with Matlab simultaneously. Therefore, Matlab/Simulink, with its powerful controller design toolboxes has been used for co-simulation with Simpack. ANFIS toolbox has been used to improve the simulation model within the Simpack. In this study, the power of the libraries of both Simpack and Matlab/Simulink are combined to perform better simulations. First, MBS model of test bench is improved by using the experimental data. Second, Sugeno type ANFIS with grid partitioning is designed by training different experimental datasets. The objective of the training is to evaluate the piston forces that correspond to the actual displacement. Lastly, the trained ANFIS model is implemented to the MBS model and co-simulations are performed. The results showed that the simulation results were better suited to experimental data when ANFIS was used. Piston performances are improved by 88,5%, 74,3%, 73,7% for piston 1, 2 and 3 respectively.}
}
@incollection{MARTTINEN1989409,
title = {A CAD PACKAGE FOR PROCESS ANALYSIS AND CONTROL DESIGN},
editor = {CHEN ZHEN-YU},
booktitle = {Computer Aided Design in Control Systems 1988},
publisher = {Pergamon},
address = {Oxford},
pages = {409-413},
year = {1989},
series = {IFAC Symposia Series},
isbn = {978-0-08-035738-6},
doi = {https://doi.org/10.1016/B978-0-08-035738-6.50070-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080357386500707},
author = {A. Marttinen and u. Kortela},
abstract = {The principles of implementing a computer-aided control system design and process analysis package are introduced. Although there are many commercial software packages available nowadays, there is still a shortage of suitable process-oriented CADCS-software. This paper is therefore especially conserned with methodology for building complex process models of use both for process analysis and for control design purposes. The usefulness of the packages always depends on the flexibility and simplicity of the user interface. We also emphasize its importance.}
}
@article{LIU2022108121,
title = {Modelling the quasi-static flexural behaviour of composite sandwich structures with uniform- and graded-density foam cores},
journal = {Engineering Fracture Mechanics},
volume = {259},
pages = {108121},
year = {2022},
issn = {0013-7944},
doi = {https://doi.org/10.1016/j.engfracmech.2021.108121},
url = {https://www.sciencedirect.com/science/article/pii/S0013794421005282},
author = {Haibao Liu and Jun Liu and Cihan Kaboglu and Jin Zhou and Xiangshao Kong and Shipeng Li and Bamber R.K. Blackman and Anthony J. Kinloch and John P. Dear},
keywords = {Sandwich structures, Foam core, Flexural behaviour, Numerical simulation, Damage mechanisms},
abstract = {In-service, composite sandwich structures, which consist of fibre-composite skins (also termed face-sheets) adhesively bonded to a polymeric foam core, can encounter extreme quasi-static flexural loading that may cause serious damage to the sandwich structure. The ability to model the flexural behaviour of such structures can lead to improved designs and more efficient maintenance procedures. In the present research, a three-dimensional finite-element analysis (FEA) model is developed to predict the flexural behaviour of such sandwich structures using a commercial software package (i.e. Abaqus/Explicit). The high-fidelity FEA simulation combines an elastic–plastic (E-P) damage model of the composite skins together with a crushable foam-core damage model. The E-P damage model is implemented with a user subroutine to capture the damage, such as plastic deformation of the matrix and matrix cracking, fibre fracture and delamination cracking of the composite skins. The crushable foam model is used to predict (a) the mechanical response of the crushed foam core, (b) the induced damage from ductile fracture due to growth, coalescence and fracture of the cells and (c) the induced damage from shear fracture of the foam due to plastic shear-band localisation. Results from the modelling studies, such as the loading response and the damage mechanisms, are discussed and compared with the experimental results obtained from the sandwich structures manufactured with both uniform- and graded-density foam cores but which all have the same average core density. Good agreement is achieved between the experimental results and the predictions from the numerical modelling simulations.}
}
@article{LAVOIE1983233,
title = {Interactive Computer Graphics for Network Analysis and Design},
journal = {IFAC Proceedings Volumes},
volume = {16},
number = {19},
pages = {233-241},
year = {1983},
note = {2nd IFAC Symposium on computer Aided Design of Multivariable Technological Systems, West Lafayette, USA, 15-17 September 1982},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)61694-X},
url = {https://www.sciencedirect.com/science/article/pii/S147466701761694X},
author = {L. Lavoie and H.H. Hoang},
keywords = {Computer graphics, computer aided design, computer software, database management, transportation, power distribution, modelling},
abstract = {A software package was developed and implemented to be used as graphical display tools in the analysis and design of different kinds of networks, e.g., transportation, communication, and power networks. It is essentially a specialized data base management system, where databases represent network structures and characteristics. The data definition and manipulation languages were defined to permit the creation and update of topological structures, physical attributes, and performance measures of networks, as well as the graphical display of these data in a convenient manner. The software has been designed to provide flexible, easy-to-use and transportable computer-aided design tools for network analysis and modelling.}
}
@article{GUSTAFSON1986297,
title = {The architecture of a homogeneous vector supercomputers},
journal = {Journal of Parallel and Distributed Computing},
volume = {3},
number = {3},
pages = {297-304},
year = {1986},
issn = {0743-7315},
doi = {https://doi.org/10.1016/0743-7315(86)90017-1},
url = {https://www.sciencedirect.com/science/article/pii/0743731586900171},
author = {John L. Gustafson and Stuart Hawkinson and Ken Scott},
abstract = {A new homogeneous computer architecture developed by FPS combines two fundamental techniques for high-speed computing: parallelism based on the binary n-cube interconnect, and pipelined vector arithmetic. The design makes extensive use of VLSI technology, resulting in a processing node that can be economically replicated. Processor nodes incorporate high-speed communications and control, vector-oriented floating-point arithmetic, and a novel dual-ported memory design. Each node is implemented on a single circuit board and can perform 64-bit floating-point arithmetic at a peak speed of 12 MFLOPS. Eight nodes are grouped together with a system node and disk support to form modules. These modules, housed in cabinet-sized packages, are capable of 96 MFLOPS peak performance and make up the smallest homogeneous units of larger systems. The new FPS system achieves a careful balance between high-speed communication and floating-point computation. This paper describes the new architecture in detail and explores some of the issues in developing effective software.}
}
@article{BOTRE2010453,
title = {Embedded Electronic Nose and Supporting Software Tool for its Parameter Optimization},
journal = {Sensors and Actuators B: Chemical},
volume = {146},
number = {2},
pages = {453-459},
year = {2010},
note = {Selected Papers from the 13th International Symposium on Olfaction and Electronic Nose},
issn = {0925-4005},
doi = {https://doi.org/10.1016/j.snb.2009.11.033},
url = {https://www.sciencedirect.com/science/article/pii/S0925400509008983},
author = {B.A. Botre and D.C. Gharpure and A.D. Shaligram},
keywords = {Embedded electronic nose, Integrated Development Environment, Qualitative and quantitative analysis},
abstract = {This paper details the work done towards a low cost, small size, portable embedded electronic nose (e-nose) and its application for analysis of different VOC mixtures. The sensor array is composed of commercially available metal oxide semiconductor sensors by Figaro. The Embedded E-nose consists of an ADuC831 and has an RS 232 interface to Desktop PC for higher level data collection and NN training. An E-nose Software Package (ESP) Integrated Development Environment (IDE) is implemented for the design and development of Embedded E-nose. The IDE facilitates the parameter optimization of data acquisition, signal pre-processing, feature extraction and Multilayer Perceptron Neural Network (MLP NN) training. The optimized parameters are then downloaded onto Embedded E-nose by means of docking the device to the ESP tool via PC serial port. The Embedded E-nose is used for realization of both qualitative as well as quantitative analysis and their results are discussed.}
}
@article{KARNY1992447,
title = {Experience with off-Line Identification for Preliminary Tuning of Lqg Self-Tuners},
journal = {IFAC Proceedings Volumes},
volume = {25},
number = {15},
pages = {447-452},
year = {1992},
note = {9th IFAC/IFORS Symposium on Identification and System Parameter Estimation 1991 , Budapest, Hungary, 8-12 July 1991},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)50673-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017506734},
author = {M. Kárný and A. Halousková},
keywords = {LQG design, Bayes method, adaptive control, computer control, self-tuning regulators, CAD, identification},
abstract = {Experience with the prototype of a software package DESIGNER, developed for supporting implementations of LQG self-tuners, is reported. Key theoretical algorithmic and software aspects are discussed and open problems are emphasized. Directions of the future research are outlined in this way.}
}
@article{BANENS1995279,
title = {TCE: a Software Environment for Real-Time Control},
journal = {IFAC Proceedings Volumes},
volume = {28},
number = {5},
pages = {279-284},
year = {1995},
note = {3rd IFAC/IFIP Workshop on Algorithms and Architectures for Real-Time Control 1995 (AARTC'95), Ostend, Belgium, 31 May-2 June 1995},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)47241-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017472417},
author = {Jos Banens and Bram {de Jager}},
keywords = {Algorithms, computer control, control system design, nonlinear control, real-time systems, software tools},
abstract = {A couple of years ago, coding relative complex, nonlinear control schemes to run in real-time was a hard job, especially for students who were not trained in programming. The ideal was (and is) effective program generators for all kind of controllers, including complex ones. As adequate products were not commercially available, a simple, yet effective, software environment for real-time control was build in the form of a library. The library is oriented towards MATLAB for interactive pre- and post-experimental duties. An efficient object oriented matrix class and a uniform experiment interface are included and proved to be beneficial, making implementing control algorithms in the real-time control system software fast, easy, and flexible, without sacrificing computing efficiency or performance. With adequate, and affordable, alternatives still not yet widely available, the library will be useful for a while.}
}
@article{UMENO1989581,
title = {Symbolic computation application for the design of linear multivariable control systems},
journal = {Journal of Symbolic Computation},
volume = {8},
number = {6},
pages = {581-588},
year = {1989},
issn = {0747-7171},
doi = {https://doi.org/10.1016/S0747-7171(89)80062-8},
url = {https://www.sciencedirect.com/science/article/pii/S0747717189800628},
author = {Takaji Umeno and Syuichi Yamashita and Osami Saito and Kenichi Abe},
abstract = {The application of symbolic computation to the algebraic design of linear multivariable control systems is presented. A software package for manipulating polynomial and rational function matrices and for solving the linear matrix equations referred to as unilateral and bilateral equations is implemented on the basis of the symbolic manipulation system known as REDUCE. Several basic functions in the package are explained and their usage is demonstrated by using an example of the design of a discrete-time control system.}
}
@article{ANSONI201623,
title = {Optimal industrial reactor design: development of a multiobjective optimization method based on a posteriori performance parameters calculated from CFD flow solutions},
journal = {Advances in Engineering Software},
volume = {91},
pages = {23-35},
year = {2016},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2015.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0965997815001325},
author = {Jonas Laerte Ansoni and Paulo Seleghim},
keywords = {Multiobjective optimization, Computational fluid dynamics, Bioreactor, Shear stress, Residence time distribution, Open source code},
abstract = {The increase in the use of biofuels raised new challenges to engineering problems. In this context, the optimization of chemical reactors, particularly bioreactors and photobioreactors, is crucial to improve the production of biofuels in a sustainable manner. This paper reports the development of an optimization method and its application to the design of a continuous flow bioreactor envisaged to be used in industrial fermentation processes. Mass and momentum conservation equations are simulated via CFD and specific a posteriori performance parameters, determined from the flow solution, are fed into a multiobjective evolutionary algorithm to obtain corrections to the parameters of the geometrical configuration of the reactor. This heuristics is iterated to obtain an optimized configuration vis-à-vis the flow aspects portrayed by the performance parameters, such as the shear stress and the residence time variations. An open source computer package (PyCFD-O) was developed to perform CFD simulations and the optimization processes automatically. First, it calls the pre-processor to generate the computational geometry and the mesh. Then it performs the simulations using OpenFOAM, calculates the output parameters and iterates the procedure. The PyCFD-O package has proved reliable and robust in a test case, a ∼1 m3 continuous fermentation reactor. The multiobjective optimization procedure actually corresponds to search for the Pareto frontier in the solution space characterized by its geometric parameters and the associated performance parameters (dispersion o residence times and shear stresses). Optimal design configurations were obtained representing the best tradeoff between antagonistic objectives, i.e. the so-called non-dominant solutions.}
}
@article{LANDAU199723,
title = {The R-S-T Digital Controller Design and Applications},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {6},
pages = {23-33},
year = {1997},
note = {IFAC Conference on Control of Industrial Systems "Control for the Future of the Youth", Belfort, France, 20-22 May},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)43344-1},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017433441},
author = {I.D. Landau},
keywords = {system identification, digital control, robust control, adaptation, software tools},
abstract = {The two degree of freedom R-S-T digital controller is becoming a standard for computer control in industry. The paper presents the methodology for the design of the R-ST controller which involves identification of the plant model from data combined with a robust control design. The perfonnances of the controller can be fwther enhanced by plant model identification in closd loop and re-tuning of the controller. For large parameter variations adaptation has to be considered for preserving the perfonnances. Software packages are available for the design, implementation and commissioning of the R-S-T digital controllers. The methodology is illustrated by its application to the control of deposited zinc in hot-dip galvanizing at SOLLAC (Florange, France)}
}
@article{DING199855,
title = {An 18 GFLOPS parallel climate data assimilation PSAS package},
journal = {Computers & Mathematics with Applications},
volume = {35},
number = {7},
pages = {55-63},
year = {1998},
note = {Advanced Computing on Intel Architectures},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(98)00032-7},
url = {https://www.sciencedirect.com/science/article/pii/S0898122198000327},
author = {H.Q. Ding and R.D. Ferraro},
keywords = {Climate modeling, PSAS software package, Performance, Intel Paragon, Cray C90},
abstract = {We have designed and implemented a set of highly efficient and highly scalable algorithms for an unstructured computational package, the PSAS data simulation package, as demonstrated by detailed performance analysis of systematic runs up to 512 nodes of an Intel Paragon. The preconditioned Conjugate Gradient solver achieves a sustained 18 Gflops performance. Consequently, we achieve an unprecedented 100-fold reduction in time to solution on the Intel Paragon over a single head of a Cray C90. This not only exceeds the daily performance requirement of the Data Assimilation Office at NASA's Goddard Space Flight Center, but also makes it possible to explore much larger and challenging data assimilation problems which are unthinkable on a traditional computer platform such as the Cray C90.}
}
@incollection{OLSSON1996725,
title = { - Software tools for parallel CFD on composite grids*},
editor = {A. Ecer and J. Periaux and N. Satdfuka and S. Taylor},
booktitle = {Parallel Computational Fluid Dynamics 1995},
publisher = {North-Holland},
address = {Amsterdam},
pages = {725-732},
year = {1996},
isbn = {978-0-444-82322-9},
doi = {https://doi.org/10.1016/B978-044482322-9/50143-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444823229501432},
author = {Peter Olsson and Jarmo Rantakokko and Michael Thuné},
abstract = {Publisher Summary
This chapter presents a library of software tools—that is, COGITO for construction of portable, parallel partial differential equations (PDE) solvers on composite structured grids. Two of the goals for the COGITO project are to abstract away from computer dependencies for portability, and to abstract away from low-level representations of the data structures. The tools have an object-oriented design and they decouple different program components and give support for complex data structures. The chapter describes an implementation showing that the tools can be used for realistic problems and that the object-oriented design raises the level of abstraction, which increases the readability of the code. This solver has been compared to a code written in plain Fortran 77. Execution timings show that the tools are comparable in performance with hard-coded solvers and that they scale very well on parallel computers of multiple instruction multiple data (MIMD) type with distributed memory.}
}
@article{WANG20149,
title = {Integrated aerodynamic design and analysis of turbine blades},
journal = {Advances in Engineering Software},
volume = {68},
pages = {9-18},
year = {2014},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813001671},
author = {Chengen Wang},
keywords = {Aerodynamic blade design, Airfoil modeling, Design integration, Design collaboration, Engineering process management, Flow analysis},
abstract = {This paper presents an integrated approach for aerodynamic blade design in an MDO (multidisciplinary design optimization) environment. First, requisite software packages and data sources for flow computations and airfoil modeling are integrated into a single cybernetic environment, which significantly enhances their interoperability. Subsequently, the aerodynamic blade design is implemented in a quasi-3D way, supported by sophisticated means of project management, task decomposition and allotment, process definition and coordination. Major tasks of aerodynamic blade design include 1D meanline analysis, streamsurface computations, generation of 2D sections, approximation of 3D airfoils, and 3D flow analysis. After compendiously depicting all the major design/analysis tasks, this paper emphatically addresses techniques for blade geometric modeling and flow analysis in more detail, with exemplar application illustrations.}
}
@article{KOCIAN2010120,
title = {An Outline of Advanced Regulation Techniques on PLC Background and Control Systems with Self Tuning Methods Design},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {24},
pages = {120-125},
year = {2010},
note = {10th IFAC Workshop on Programmable Devices and Embedded Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20101006-2-PL-4019.00024},
url = {https://www.sciencedirect.com/science/article/pii/S147466701530999X},
author = {Jiri Kocian and Jiri Koziorek},
keywords = {Advanced process control, control system, PID controller, self tuning, regulator, PLC, self tuner, visualization},
abstract = {Advanced process control techniques contain software packages for advanced control based on mathematical methods. There are not only tools for self tuning, but also tools for manufacture costs optimization and energy savings. APC tools are designed to increase the process capacity, yield and quality of products and for costs savings. Self tuning function has become a standard function of distributed control systems. The idea of the automatic system identification procedure and regulator constants calculation is more than obvious and most of nowadays digital industry regulators and PLCs are provided with some kind of the self tuning constant algorithm. Practical part of the paper deals with design of the control system with PID Self Tuner. There is a description of an implementation of the PID regulator as a function block which can be also used for extension control functions.}
}
@article{POZAS2016300,
title = {Getting Results in an Historical Dwelling Stock in a Thermal Simulation with EnergyPlus},
journal = {Procedia Engineering},
volume = {161},
pages = {300-306},
year = {2016},
note = {World Multidisciplinary Civil Engineering-Architecture-Urban Planning Symposium 2016, WMCAUS 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.560},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816327680},
author = {Beatriz Montalbán Pozas and Montaña Jiménez Espada},
keywords = {building simulation, historic building type, thermal model, model simulation inputs},
abstract = {The invaluable historic dwelling stock of many cities today needs an accurate analysis in order to implement a relevant program of research for its conservation, maintenance, or refurbishment. Additionally, energy efficiency studies are essential in every case to guarantee sustainability. In these situations, a simulation process will be a suitable approach to obtaining results. However, historic dwellings are complicated to simulate. They require a comprehensive study of traditional crafts and techniques as well as of ancient constructions which rely heavily on local builders’ design skills and traditions as well as on the regional materials and architectural knowledge. Furthermore, the intricate geometries and the large number of different dwelling cases demand a amplification and a typological definition of representative buildings. In addition, in an energy simulation process, it is indispensable to know what the lifestyle was. In a traditional dwelling, the ancient customs have to be taken into account, studying the occupation, metabolism, activities, and traditional clothing in order to define a reliable computer model. Moreover, an operation usage program has to be defined in order to set other building simulation parameters such as the ventilation, shading, and internal gains. The present work proposes a method using DesignBuilder, an EnergyPlus software package, to generate a model for the energy simulation of historic dwellings through a case study of an historic dwelling stock in the Sistema Central of Spain. How the type was generated from a real case with an architectonic, constructive, and activity definition is described. Guidelines are provided for modeling the building and for setting the simulation parameters necessary to obtain correct results.}
}
@incollection{MA2003313,
title = {A fast approach to model hydrodynamic behaviour of journal bearings for analysis of crankshaft and engine dynamics},
editor = {G. Dalmaz and A.A. Lubrecht and D. Dowson and M. Priest},
series = {Tribology Series},
publisher = {Elsevier},
volume = {43},
pages = {313-327},
year = {2003},
booktitle = {Transient Processes in Tribology},
issn = {0167-8922},
doi = {https://doi.org/10.1016/S0167-8922(03)80059-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167892203800597},
author = {M.-T. Ma and G. Offner and B. Loibnegger and H.H. Priebsch and I.R.W. McLuckie},
abstract = {This paper addresses the development and implementation of an expedient hydrodynamic model of dynamically loaded plain journal bearings for the simulation of crankshaft and engine dynamics. The model is based upon an analytical solution to the classic Reynolds equation with a combination of the short and long bearing approximations. The model has been implemented in a multi-body dynamics (MBD) software package — AVL EXCITE. The algorithms have been developed to consider the effect of journal misalignment and elasticity of bearing structures. The implemented model has been verified extensively with a detailed numerical analysis of journal bearings. It is shown that the new approach is very economical concerning the computing cost and satisfactory in terms of the result accuracy.}
}
@article{PEBESMA2004683,
title = {Multivariable geostatistics in S: the gstat package},
journal = {Computers & Geosciences},
volume = {30},
number = {7},
pages = {683-691},
year = {2004},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2004.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0098300404000676},
author = {Edzer J Pebesma},
keywords = {Kriging, Cokriging, Linear model of coregionalisation, Open source software, S language, Stochastic simulation},
abstract = {This paper discusses advantages and shortcomings of the S environment for multivariable geostatistics, in particular when extended with the gstat package, an extension package for the S environments (R, S-Plus). The gstat S package provides multivariable geostatistical modelling, prediction and simulation, as well as several visualisation functions. In particular, it makes the calculation, simultaneous fitting, and visualisation of a large number of direct and cross (residual) variograms very easy. Gstat was started 10 years ago and was released under the GPL in 1996; gstat.org was started in 1998. Gstat was not initially written for teaching purposes, but for research purposes, emphasising flexibility, scalability and portability. It can deal with a large number of practical issues in geostatistics, including change of support (block kriging), simple/ordinary/universal (co)kriging, fast local neighbourhood selection, flexible trend modelling, variables with different sampling configurations, and efficient simulation of large spatially correlated random fields, indicator kriging and simulation, and (directional) variogram and cross variogram modelling. The formula/models interface of the S language is used to define multivariable geostatistical models. This paper introduces the gstat S package, and discusses a number of design and implementation issues. It also draws attention to a number of papers on integration of spatial statistics software, GIS and the S environment that were presented on the spatial statistics workshop and sessions during the conference Distributed Statistical Computing 2003.}
}
@article{OUF2017796,
title = {A proposed paradigm for smart learning environment based on semantic web},
journal = {Computers in Human Behavior},
volume = {72},
pages = {796-818},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216305957},
author = {Shimaa Ouf and Mahmoud {Abd Ellatif} and S.E. Salama and Yehia Helmy},
keywords = {E-Learning ecosystem, Personalization, Ontology, Software architecture, Semantic Web Rule Language, Learner model},
abstract = {The current approaches of e-learning face challenges, in isolation of learners from learning process, and shortage of learning process quality. The researchers mentioned that the next generation of e-learning is e-learning ecosystem. E-learning ecosystem has many advantages, in which, learners form groups, collaborate with each other and with educators, and content designed for interaction. E-learning ecosystem faces some issues. It applies teacher-student model, in which, fixed learning pathway is considered suitable for all learners. Consequently, learners are presented with limited personalized materials. E-learning ecosystem needs to merge the personalization's concept. Semantic web ontology based personalization of learning environment plays a leading role to build smart e-learning ecosystem. This paper previews a detailed study which addresses research papers that apply ontology within learning environment. Most of these studies focus on personalizing e-learning by providing learners with suitable learning objects, ignoring the other learning process components. This paper proposes and implements framework for smart e-learning ecosystem using ontology and SWRL. A new direction is proposed. This direction fosters the creation of a separate four ontologies for the personalized full learning package which is composed of learner model and all the learning process components (learning objects, learning activities and teaching methods).}
}
@article{CAO1999141,
title = {Rapid prototyping of distributed algorithms},
journal = {Journal of Systems and Software},
volume = {45},
number = {2},
pages = {141-154},
year = {1999},
issn = {0164-1212},
doi = {https://doi.org/10.1016/S0164-1212(98)10074-2},
url = {https://www.sciencedirect.com/science/article/pii/S0164121298100742},
author = {Jiannong Cao and Olivier {De Vel} and Ling Shi},
abstract = {Rapid prototyping is expected to be especially effective in designing, evaluating, and tuning new software whose real-time performance and quality of service are difficult to determine analytically. Distributed algorithms and protocols fall into this category of software. In this paper we present the design and implementation of a rapid prototyping system for distributed algorithms. The system provides a library of interprocess communication routines and supports dynamic reconfiguration of network topology, automatic process loading and performance monitoring of the prototyped algorithms. We discuss our experiences in developing the rapid prototyping system on a transputer network and describe examples of using the system.}
}
@article{CORDOBASANCHEZ2016164,
title = {Ann: A domain-specific language for the effective design and validation of Java annotations},
journal = {Computer Languages, Systems & Structures},
volume = {45},
pages = {164-190},
year = {2016},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2016.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1477842416300318},
author = {Irene Córdoba-Sánchez and Juan {de Lara}},
keywords = {Model driven engineering, Domain-specific languages, Code generation, Java annotations, Model finders},
abstract = {This paper describes a new modelling language for the effective design and validation of Java annotations. Since their inclusion in the 5th edition of Java, annotations have grown from a useful tool for the addition of meta-data to play a central role in many popular software projects. Usually they are not conceived in isolation, but in groups, with dependency and integrity constraints between them. However, the native support provided by Java for expressing this design is very limited. To overcome its deficiencies and make explicit the rich conceptual model which lies behind a set of annotations, we propose a domain-specific modelling language. The proposal has been implemented as an Eclipse plug-in, including an editor and an integrated code generator that synthesises annotation processors. The environment also integrates a model finder, able to detect unsatisfiable constraints between different annotations, and to provide examples of correct annotation usages for validation. The language has been tested using a real set of annotations from the Java Persistence API (JPA). Within this subset we have found enough rich semantics expressible with Ann and omitted nowadays by the Java language, which shows the benefits of Ann in a relevant field of application.}
}
@article{ZHANG2013208,
title = {Efficient multi-objective calibration of a computationally intensive hydrologic model with parallel computing software in Python},
journal = {Environmental Modelling & Software},
volume = {46},
pages = {208-218},
year = {2013},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2013.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1364815213000765},
author = {Xuesong Zhang and Peter Beeson and Robert Link and David Manowitz and Roberto C. Izaurralde and Ali Sadeghi and Allison M. Thomson and Ritvik Sahajpal and Raghavan Srinivasan and Jeffrey G. Arnold},
keywords = {Parallel processing, Evolutionary multi-objective optimization, High performance computer, Soil and water assessment tool, Parameter calibration},
abstract = {With enhanced data availability, distributed watershed models for large areas with high spatial and temporal resolution are increasingly used to understand water budgets and examine effects of human activities and climate change/variability on water resources. Developing parallel computing software to improve calibration efficiency has received growing attention of the watershed modeling community as it is very time demanding to run iteratively complex models for calibration. In this research, we introduce a Python-based parallel computing package, PP-SWAT, for efficient calibration of the Soil and Water Assessment Tool (SWAT) model. This software employs Python, MPI for Python (mpi4py) and OpenMPI to parallelize A Multi-method Genetically Adaptive Multi-objective Optimization Algorithm (AMALGAM), allowing for simultaneously addressing multiple objectives in calibrating SWAT. Test results on a Linux computer cluster showed that PP-SWAT can achieve a speedup of 45–109 depending on model complexity. Increasing the processor count beyond a certain threshold does not necessarily improve efficiency, because intensified resource competition may result in an I/O bottleneck. The efficiency achieved by PP-SWAT also makes it practical to implement multiple parameter adjustment schemes operating at different scales in affordable time, which helps provide SWAT users with a wider range of options of parameter sets to choose from for model(s) selection. PP-SWAT was not designed to address errors associated with other sources (e.g. model structure) and cautious supervision of its power should be exercised in order to attain physically meaningful calibration results.}
}
@article{DITZE1998129,
title = {Supporting Software Synthesis of Communication Infrastructures for Embedded Real-Time Applications},
journal = {IFAC Proceedings Volumes},
volume = {31},
number = {32},
pages = {129-136},
year = {1998},
note = {15th IFAC Workshop on Distributed Computer Control Systems (DCCS'98), Como, Italy, 9-11 September 1998},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)36346-2},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017363462},
author = {Carsten Ditze and Carsten Böke},
keywords = {Communication systems, real-time operating systems, embedded systems, object-oriented programming, high-performance computing},
abstract = {Although there are many commercial real-time micro-kernel available on the market today, the common prejudices against their use for implementing control algorithms are still justified. Some of them are too complex and overhead-prone while others are tiny and efficient but don't provide all those services that are actually required by real-time applications. This is especially true regarding communication support for distributed and embedded control systems. This paper focuses on a new design concept for real-time communication system (RCOS) platforms matching the quality and functional needs of a given distributed and embedded application. The approach is based on a library operating system (Dreams**Distributed Real-Time Extensible Application Management System) which allows for extremely fine-grained customizations to avoid systemlevel overhead. We will outline the architecture of Dreams, show how customization is applied for interprocessor communication and present first results that demonstrate the impact of customization on system overhead.}
}
@article{KHAN201824,
title = {Design and implementation of an automated network monitoring and reporting back system},
journal = {Journal of Industrial Information Integration},
volume = {9},
pages = {24-34},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300602},
author = {Rafiullah Khan and Sarmad Ullah Khan},
keywords = {Network monitoring, Network management, Universal Plug & Play, Ticketing system, Reporting back system, Nagios},
abstract = {The task of network monitoring becomes tedious with increase in the network size, heterogeneity and complexity. The available network monitoring and management solutions are not only expensive but also difficult to use, configure and maintain. Manually pin pointing a faulty device in the large complex network is very tricky and time consuming for network administrators. Thus, an automated system is needed that immediately reports to network administrator about fault type and location as soon as it arises. This paper presents design and implementation of an intelligent network monitoring and reporting back system for large size organizations/industries. It is based on programming open-source tools (such as Nagios and RT) and intelligently integrating them to monitor network devices such as switches and routers. To monitor end-user devices in the network, a software package has been developed using Universal Plug & Play (UPnP) technology. The developed monitoring system immediately notifies network administrator as soon as a network problem arises. The notification message clearly pin points the fault location in network topology, its type and impact on rest of the network. If the network problem is not resolved within the pre-specified deadline, a second notification is sent out to the next responsible person. Thus, all people in the priority list are notified one by one with pre-specified deadlines until the problem is resolved.}
}
@article{HAKE1991311,
title = {The impact of memory organization on the performance of matrix calculations},
journal = {Parallel Computing},
volume = {17},
number = {2},
pages = {311-327},
year = {1991},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(05)80116-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167819105801164},
author = {J.-Fr. Hake and W. Homberg},
keywords = {Mathematical software, numerical methods, linear algebra, matrix multiplication, linear equations, eigenvalue problems, shortest-path problems, vector computer, parallel computer, cache, translation lookaside buffer, interleaved memory, CRAY, IBM, FUJITSU},
abstract = {The memory organization of vector supercomputers is designed to support a high rate of data transfer between registers and main memory. Nevertheless, there are applications for which this link turns out to be a bottleneck. It can be removed using an interface to appropriate library software or programming techniques which take architectural features into account. This report deals with the impact of memory access conflicts on the execution time of matrix calculations. For this study, two variants of matrix multiplication are considered as model problems contrasting memory access with stride one and access with stride n. The CPU time consumption of the two variants is analyzed by means of simulation. It is shown, that the results are also valid for the solution of linear equations, eigenvalue problems, and shortest-path problems if the algorithms are implemented analogously. The analysis is carried out for computers with an interleaved memory (CRAY X-MP, CRAY Y-MP, FUJITSU VP) and a hierarchical memory (IBM 3090). The results are also related to library software in order to point out the benefits a user may gain from the usage of highly optimized software. Moreover, it is demonstrated that multiple processors working in parallel on a shared memory may even increase the number of memory access conflicts.}
}
@article{YE2016165,
title = {WHATIF: An open-source desktop application for extraction and management of the incidental findings from next-generation sequencing variant data},
journal = {Computers in Biology and Medicine},
volume = {68},
pages = {165-169},
year = {2016},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2015.03.028},
url = {https://www.sciencedirect.com/science/article/pii/S0010482515001067},
author = {Zhan Ye and Christopher Kadolph and Robert Strenn and Daniel Wall and Elizabeth McPherson and Simon Lin},
keywords = {Genomics, Internet, Sequencing, Software},
abstract = {Background
Identification and evaluation of incidental findings in patients following whole exome (WGS) or whole genome sequencing (WGS) is challenging for both practicing physicians and researchers. The American College of Medical Genetics and Genomics (ACMG) recently recommended a list of reportable incidental genetic findings. However, no informatics tools are currently available to support evaluation of incidental findings in next-generation sequencing data.
Methods
The Wisconsin Hierarchical Analysis Tool for Incidental Findings (WHATIF), was developed as a stand-alone Windows-based desktop executable, to support the interactive analysis of incidental findings in the context of the ACMG recommendations. WHATIF integrates the European Bioinformatics Institute Variant Effect Predictor (VEP) tool for biological interpretation and the National Center for Biotechnology Information ClinVar tool for clinical interpretation.
Results
An open-source desktop program was created to annotate incidental findings and present the results with a user-friendly interface. Further, a meaningful index (WHATIF Index) was devised for each gene to facilitate ranking of the relative importance of the variants and estimate the potential workload associated with further evaluation of the variants. Our WHATIF application is available at: http://tinyurl.com/WHATIF-SOFTWARE
Conclusions
The WHATIF application offers a user-friendly interface and allows users to investigate the extracted variant information efficiently and intuitively while always accessing the up to date information on variants via application programming interfaces (API) connections. WHATIF׳s highly flexible design and straightforward implementation aids users in customizing the source code to meet their own special needs.}
}
@incollection{HARRINGTON2002215,
title = {10 - Using CASE Tools for Database Design},
editor = {Jan L. Harrington},
booktitle = {Relational Database Design Clearly Explained (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {215-230},
year = {2002},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-1-55860-820-7},
doi = {https://doi.org/10.1016/B978-155860820-7/50012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978155860820750012X},
author = {Jan L. Harrington},
abstract = {Publisher Summary
A CASE (computer-aided software engineering) tool is a software package that provides support for the design and implementation of information systems. By integrating many of the techniques used to document a system design—including the data dictionary, data flows, and entity relationships—CASE software can increase the consistency and accuracy of a database design. It can document a database design and can provide invaluable help in maintaining the consistency of a design. Although some current CASE tools can verify the integrity of a data model, they cannot design the database. There is no software in the world that can examine a database environment and identify the entities, attributes, and relationships that should be represented in a database. The model created with CASE software is therefore only as good as the analysis of the database environment performed by the people using the tool. Many of the diagrams and reports that a CASE tool can provide are designed to follow a single theoretical model. In addition to providing tools for simplifying the creation of ER diagrams, many CASE tools can generate reports that document the contents of an ERD. This chapter discusses the issues related to CASE and their impact on relational models.}
}
@incollection{BOBZIN2001237,
title = {Chapter 8 - The architecture of a database system for mobile and embedded devices},
editor = {Klaus R. Dittrich and Andreas Geppert},
booktitle = {Component Database Systems},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {237-251},
year = {2001},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-1-55860-642-5},
doi = {https://doi.org/10.1016/B978-155860642-5/50009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606425500092},
author = {Heiko Bobzin},
abstract = {Publisher Summary
A database management system for mobile and embedded devices must be able to run nonstop—perhaps for years—because such devices typically are not shut down; instead, they are placed in suspend mode. POET Software, a leading vendor of object-oriented databases, has developed a new database management system, named Navajo, for mobile and embedded devices. Design and development have faced several special challenges: robustness, low resource consumption, and connectivity. This chapter explains the components of POET's pure Java database management system: user API and object model, object management, concurrency management, backend and storage, distributed applications, event service, log service, postprocessing, and on-demand assembly of components. The application developer or device manufacturer chooses the set of features required for a particular application and assembles the database configuration from the modules that enable these features. The main objectives, low resource consumption and on-demand application assembly, require a componentized database management system. Components in this system are parts that can be replaced or loaded at runtime and that implement a defined interface. Replacing an implementation can add more functionality or can resolve memory performance trade-offs.}
}
@article{KOORMANN2006925,
title = {Modeling the fate of down-the-drain chemicals in rivers: An improved software for GREAT-ER},
journal = {Environmental Modelling & Software},
volume = {21},
number = {7},
pages = {925-936},
year = {2006},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2005.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815205000836},
author = {F. Koormann and J. Rominger and D. Schowanek and J.-O. Wagner and R. Schröder and T. Wind and M. Silvani and M.J. Whelan},
keywords = {Environmental modeling, Exposure assessment, GIS, Database, GREAT-ER},
abstract = {GREAT-ER (Geography-referenced Regional Exposure Assessment Tool for European Rivers) is a model system for predicting chemical fate and exposure in surface waters. The GREAT-ER approach combines a series of well-studied models (for sewers, waste water treatment plants and rivers) with spatial information managed by a GIS. A new version of GREAT-ER (version 2) has been developed which has a number of improved features compared to its predecessor. All components are now implemented as Free Software and make use of Free Software where possible. This simplifies the distribution, use and extension of the system and its components for scientific applications. In addition, two new platform-independent user interfaces have been developed. The first is a simplified web-based interface for easy access without installation and the second is a full-featured desktop version. The model system has been re-designed to increase its flexibility for the modification and extension of its component models. In addition, the file-based data storage system used in version 1 has been replaced by a database management system with a flexible Application Programming Interface (API). This makes it easier for users to share their results, using a new repository (also with web-based and desktop versions) serving as a central communication tool. The output from the new system has been successfully tested against the field-validated models in GREAT-ER 1.0.}
}
@article{STEWART2008203,
title = {A framework for the life cycle management of information technology projects: ProjectIT},
journal = {International Journal of Project Management},
volume = {26},
number = {2},
pages = {203-212},
year = {2008},
issn = {0263-7863},
doi = {https://doi.org/10.1016/j.ijproman.2007.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0263786307000956},
author = {Rodney A. Stewart},
keywords = {Information technology, Information systems, Life cycle management},
abstract = {As international competition continues to intensify, significant numbers of organisations are investing large amounts of resources into information and communication technologies as they seek to gain competitive advantage. Information Technology (IT) is increasingly being implemented for strategic reasons, so as to enable improved efficiency and to improve the control and productivity of internal processes. However, the failure of realising expected IT-induced benefits has led to a growing number of senior executives to question the value of IT investments. This research study was inspired by the perceived lack of a structured framework for the life cycle management of innovative IT projects (ProjectIT). Such a framework consists of three modules representing each phase of the IT project life cycle, namely, IT project selection (SelectIT), strategic IT implementation (ImplementIT) and IT performance evaluation (EvaluateIT). Moreover, industry practitioners require a user-friendly software tool to assist them to undertake this arduous task. This paper provides a description of each module of the ProjectIT framework and the current progress towards the development of the companion software package. ProjectIT should assist firms to rapidly select IT projects based on a range of monetary and non-monetary benefits and risks, implement these projects in a well-planned strategic manner and evaluate the short- and long-term value generated from them.}
}
@article{STEPTO1995385,
title = {Integration of Knowledge Based Systems into Control and Instrumentation Design and Implementation},
journal = {IFAC Proceedings Volumes},
volume = {28},
number = {17},
pages = {385-390},
year = {1995},
note = {8th IFAC Symposium on Automation in Mining, Mineral and Metal Processing 1995 (MMM'95), Sun City, South Africa, 29-31 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)46788-7},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017467887},
author = {W.D. Stepto},
keywords = {Expert system, Simulation, Testing, Training, Workcycle},
abstract = {This paper describes the integration of a knowledge based system into the control and instrumentation design and implementation work cycle for capital projects. The use of a knowledge based system to simulate process flowsheets in both “mass balance” and dynamic modes provides the required level of process definition for control system design. The development of interfaces to other design packages has been initiated to provide an integrated C&I design system. In addition, the same simulation is used to provide quality assurance testing for software and a platform for operating training.}
}
@article{LAMBERTI2003197,
title = {Move limits definition in structural optimization with sequential linear programming. Part I: Optimization algorithm},
journal = {Computers & Structures},
volume = {81},
number = {4},
pages = {197-213},
year = {2003},
issn = {0045-7949},
doi = {https://doi.org/10.1016/S0045-7949(02)00442-X},
url = {https://www.sciencedirect.com/science/article/pii/S004579490200442X},
author = {Luciano Lamberti and Carmine Pappalettere},
keywords = {SLP, Move limits, Linearization error, Trust region},
abstract = {A variety of numerical methods have been proposed in literature in purpose to deal with the complexity and non-linearity of structural optimization problems. In practical design, sequential linear programming (SLP) is very popular because of its inherent simplicity and because linear solvers (e.g. Simplex) are easily available. However, SLP performance is sensitive to the definition of proper move limits for the design variables which task itself often involves considerable heuristics. This research presents a new SLP algorithm (LESLP––linearization error sequential linear programming) that implements an advanced technique for defining the move limits. The LESLP algorithm is formulated so to overcome the traditional limitations of the SLP method. The new algorithm is successfully tested in weight minimization problems of truss structures with up to hundreds of design variables and thousands of constraints: sizing and configuration problems are considered. Optimization problems of non-truss structures are also presented. The key-ideas of LESLP and the discussion on numerical efficiency of the new algorithm are presented in a two-part paper. The first part concerns the basics of the LESLP formulation and provides potential users with a guide to programming LESLP on computers. In a companion paper, the numerical efficiency, advantages and drawbacks of LESLP are discussed and compared to those of other SLP algorithms recently published or implemented in commercial software packages.}
}
@incollection{MARINESCU2013131,
title = {Chapter 5 - Cloud Resource Virtualization},
editor = {Dan C. Marinescu},
booktitle = {Cloud Computing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {131-161},
year = {2013},
isbn = {978-0-12-404627-6},
doi = {https://doi.org/10.1016/B978-0-12-404627-6.00005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124046276000051},
author = {Dan C. Marinescu},
keywords = {Application programming interface (API), Application binary interface (ABI), Instruction set architecture (ISA), Virtual machine (VM), Virtual machine monitor (VMM), Full virtualization, Paravirtualization, , Software fault isolation},
abstract = {Virtualization is a basic tenet of cloud computing. It simplifies some of the resource management tasks; for example, the state of a virtual machine (VM) running under a virtual machine monitor (VMM) can be saved and migrated to another server to balance the load. At the same time, virtualization allows users to operate in environments they are familiar with, rather than forcing them to work in idiosyncratic environments. We start Chapter 5 with a discussion of virtualization principles and the motivation for virtualization. Then we discuss the interfaces that define the properties of the system at different levels of abstraction: Application Programming Interface (API), Application Binary Interface (ABI), and Instruction Set Architecture (ISA). We discuss alternatives for the implementation of virtualization, then analyze their impact on performance and security isolation. Next we analyze two distinct approaches to virtualization: full virtualization and paravirtualization. Full virtualization is feasible when the hardware abstraction provided by the VMM is an exact replica of the physical hardware. In this case any operating system running on the hardware will run without modification under the VMM. In contrast, paravirtualization requires some modification of the guest operating systems because the hardware abstraction provided by the VMM does not support all the functions that the hardware does. Traditional processor architectures were conceived for one level of control because they support two execution modes: kernel and user mode. In a virtualized environment all resources are under the control of a VMM, and a second level of control is exercised by the guest operating system. Although two-level scheduling for sharing CPU cycles can be easily implemented, sharing resources such as cache, memory, and I/O bandwidth is more intricate. In 2005 and 2006 the x86 processor architecture was extended to provide hardware support for virtualization. The chapter covers in depth the widely used Xen VMM. We discuss the performance of VMs and analyze the system functions critical to the performance of a VM environment: cache and memory management, handling of privileged instructions, and I/O handling. An analysis of security advantages and some of the potential risks of virtualization are the subject of the next sections. Finally, the chapter covers software fault isolation.}
}
@article{GOMEZRUBIO20051000,
title = {RArcInfo: Using GIS data with R},
journal = {Computers & Geosciences},
volume = {31},
number = {8},
pages = {1000-1006},
year = {2005},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2005.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0098300405000646},
author = {Virgilio Gómez-Rubio and Antonio López-Quílez},
keywords = {Geographic Information Systems, R statistical programming language, E00 files, Arc/Info binary coverages, Spatial data analysis},
abstract = {Geographic Information Systems (GIS) have become powerful tools to integrate, visualise and analyse spatial data obtained from different sources. However, the number of statistical methods implemented in most GIS is not comparable to those available in statistical software. RArcInfo is a package for the R programming language to import data from Arc/Info Version 7.x binary coverages and E00 files, which provide a complete structure to describe spatial data. Once the data are imported into R, several packages are available to perform spatial analysis, access databases to integrate new data and other interesting possibilities. The result is an increase in the spectrum of statistical tools available to process the data.}
}
@article{GRAPENTHIN2014168,
title = {CrusDe: A plug-in based simulation framework for composable Crustal Deformation studies using Green's functions},
journal = {Computers & Geosciences},
volume = {62},
pages = {168-177},
year = {2014},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2013.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300413001945},
author = {R. Grapenthin},
keywords = {Modeling, Simulation, Crustal deformation, Green's function, Loading, Surface displacement, C, C++},
abstract = {CrusDe is a plug-in based simulation framework written in C/C++ for Linux platforms (installation information, download and test cases: http://www.grapenthin.org/crusde). It utilizes Green's functions for simulations of the Earth's response to changes in surface loads. Such changes could involve, for example, melting glaciers, oscillating snow loads, or lava flow emplacement. The focus in the simulation could be the response of the Earth's crust in terms of stress changes, changes in strain rates, or simply uplift or subsidence and the respective horizontal displacements of the crust (over time). Rather than implementing a variety of specific models, CrusDe approaches crustal deformation problems from a general formulation in which model elements (Green's function, load function, relaxation function, load history), operators, pre- and postprocessors, as well as input and output routines are independent, exchangeable, and reusable on the basis of a plug-in approach (shared libraries loaded at runtime). We derive the general formulation CrusDe is based on, describe its architecture and use, and demonstrate its capabilities in a test case. With CrusDe users can: (1) dynamically select software components to participate in a simulation (through XML experiment definitions), (2) extend the framework independently with new software components and reuse existing ones, and (3) exchange software components and experiment definitions with other users. CrusDe's plug-in mechanism aims for straightforward extendability allowing modelers to add new Earth models/response functions. Current Green's function implementations include surface displacements due to the elastic response, final relaxed response, and pure thick plate response for a flat Earth. These can be combined to express exponential decay from elastic to final relaxed response, displacement rates due to one or multiple disks, irregular loads, or a combination of these. Each load can have its own load history and crustal decay function.}
}
@article{IVANOVIC201411,
title = {Providing an Application-specific Interface over a CERIF Back-end: Challenges and Solutions},
journal = {Procedia Computer Science},
volume = {33},
pages = {11-17},
year = {2014},
note = {12th International Conference on Current Research Information Systems, CRIS 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914007935},
author = {Dragan Ivanović and Nikos Houssos},
keywords = {CERIF data model, wrapper, JPA, ENGAGE, REST API},
abstract = {This paper presents a case of presenting information modelled in CERIF through an application-specific programming interface which does not require CERIF expertise by the developers. The CERIF data model is semantically rich and can be used for detailed description of entities of scientific / research activity. A sophisticated application interface is required to exploit the full range of CERIF capabilities. On the other hand, it is obvious that data about scientific-research entities are utilised by users and software developers with various level of familiarity with the CERIF model. Because of the diversity of CRIS systems’ users’ knowledge of CERIF, in certain cases it is useful to create an additional application interface with elements of some simple model which can be easily understood and used by users with low level or without any knowledge of CERIF. The article presents the design and implementation of a wrapper that enables bidirectional conversion of entered data using a simple model application interface to a CERIF back-end and the use of the wrapper in the ENGAGE project as part of an open infrastructure for Public Sector Information.}
}
@article{GOCKENBACH19961,
title = {Hilbert class library: A library of abstract C++ classes for optimization and inversion},
journal = {Computers & Mathematics with Applications},
volume = {32},
number = {6},
pages = {1-13},
year = {1996},
issn = {0898-1221},
doi = {https://doi.org/10.1016/0898-1221(96)00140-X},
url = {https://www.sciencedirect.com/science/article/pii/089812219600140X},
author = {M.S. Gockenbach and W.W. Symes},
keywords = {Mathematical software, Object-oriented design, Simulation},
abstract = {According to the Object-Oriented Programming paradigm, a computer program should be organized around the fundamental objects it manipulates. In the C++ programming languages, these objects are embodied in classes. The Hilbert Class Library (HCL) is a collection of C++ classes designed for implementing numerical optimization algorithms in the context of Hilbert spaces. HCL includes base classes for defining vectors, linear operators, nonlinear operators and functionals, and related mathematical objects. Using these base classes, algorithms can be coded in a natural style that does not refer to application-specific details; nonetheless, the code can be applied to arbitrarily complex applications. Thus, HCL is intended to provide a way to bridge the often large gap between sophisticated numerical optimization routines and complicated simulation-based applications.}
}
@article{KUMAR2016865,
title = {Universal interface on Zynq® SoC with CAN, RS-232, Ethernet and AXI GPIO for instrumentation & control},
journal = {Fusion Engineering and Design},
volume = {112},
pages = {865-871},
year = {2016},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2016.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0920379616304203},
author = {Abhijeet Kumar and Rachana Rajpal and Harshad Pujara and Hitesh Mandaliya and Praveenalal Edappala},
keywords = {Zynq, CAN, Ethernet, AXI-GPIO, Custom-IP, UART},
abstract = {This paper describes an application developed on the latest Zynq®-7000 All Programmable SoC (AP SoC) [1] devices which integrate the software programmability of an ARM®-based processor with the hardware programmability of an FPGA, on a single device. In this paper we have implemented application which uses various interfaces like CAN, RS-232, Ethernet and AXI GPIO, so that our host application running on PC in LabVIEW can communicates with any hardware which has at least any one of the available interface. Zynq-7000 All Programmable SoCs (System On Chip) infuse customizable intelligence into today’s embedded systems to suit your unique application requirements. This family of FPGA is meant for high end application because it has huge resources on single chip. It offers you to make your own custom hardware IP, in fact we have made our custom IP called myIP in our design. The beauty of this chip is that it can write drivers for your custom IP which has AXI bus layer attached. After exporting the hardware information to the Software Development Kit (SDK), the tool is able to write drivers for your custom IP. This simplifies your development to a great extent. In a way this application provides the universal interfacing option to user. User can also write the digital data on the GPIO (General Purpose Input Output) through LabVIEW Test application GUI. This project can be used for remote control and monitor the data over Ethernet or CAN network.}
}
@article{ING1991185,
title = {Molly—a language for typesetting molecular structure diagrams},
journal = {Computers & Chemistry},
volume = {15},
number = {3},
pages = {185-201},
year = {1991},
issn = {0097-8485},
doi = {https://doi.org/10.1016/0097-8485(91)80002-4},
url = {https://www.sciencedirect.com/science/article/pii/0097848591800024},
author = {Roy T. Ing},
abstract = {MOLLY is a language for drawing 2-D molecular structure diagrams commonly seen in scientific journals and textbooks. Implemented as a PIC macro library, MOLLY is integrated with the TROFF family of typesetting software available for UNIX and MS-DOS operating systems. The language is device-independent and can be used to produce publication-quality molecular drawings on laser printers and phototypesetters. MOLLY offers a new paradigm for drawing chemical structures—atomic groups are drawn in a way analogous to how a branching tree or road is traversed. The atomic groups and bonds are placed intuitively and accurately at any orientation—even when entire structures are rotated and reflected. The user can extend the language and build libraries of reusable moieties and templates. This paper describes the design of MOLLY and gives examples of its use.}
}
@article{KU2010667,
title = {Gather/scatter hardware support for accelerating Fast Fourier Transform},
journal = {Journal of Systems Architecture},
volume = {56},
number = {12},
pages = {667-684},
year = {2010},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2010.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1383762110001153},
author = {Anderson Kuei-An Ku and Jingling Xue and Yong Guan},
keywords = {Hardware/software codesign, Cache optimization, Fast address generation, FFT},
abstract = {As we enter the multi-core era, seeking methods to boost the performance of single-threaded applications remains critical. Achieving gains in processor performance by increasing the operating frequency has begun to meet more obstacles. However, significant performance improvements can be achieved by extending the capability of the processor with the addition of hardware support, which makes much more effective use of the available transistors. This paper presents a novel hardware support called, DistTree, to speed up processor performance. The DistTree hardware automates gather and scatter operations for applications with complex but predictable memory access patterns like the Fast Fourier Transform (FFT). With this hardware support integrated with a modern microprocessor (the Alpha architecture in our experiments), the FFT performance can reap a more than twofold increase when compared against the FFTW library, a state-of-the-art implementation. The DistTree hardware support enables the processor to spend the majority of processor cycles on executing the computations of an algorithm by reducing both the arithmetic and address computation overhead. Therefore, the performance of many single-threaded applications can be significantly increased.}
}
@article{CABO2019807,
title = {A hybrid SURF-DIC algorithm to estimate local displacements in structures using low-cost conventional cameras},
journal = {Engineering Failure Analysis},
volume = {104},
pages = {807-815},
year = {2019},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2019.06.083},
url = {https://www.sciencedirect.com/science/article/pii/S135063071831327X},
author = {C. Cabo and C. Ordóñez and M. Muñiz-Calvente and M. Lozano and G. Ismael},
keywords = {Image correlation, Feature matching, Displacements},
abstract = {A fast, efficient and low-cost method for the accurate determination of local displacements in structures based on computer vision techniques is presented in this study. First, displacements at pixel level are estimated using the SURF (Speeded-Up Robust Features) image matching algorithm, which detects corresponding points in the images. Then, the initial displacement estimations are refined at subpixel scale through image cross-correlation in the frequency domain. The image acquisition is simple and fast, using a conventional low-cost digital camera. The method was validated comparing the results obtained with those provided by a high performance system (ARAMIS 5 M) that integrates two cameras and image processing software. Results show that the proposed method is able to detect displacements with a precision of approximately 60% of the pixel size. The algorithm was implemented in a computer program in order to automatize the procedure.}
}
@article{DEUTSCHER20084749,
title = {Approximate feedback linearization using multivariable Legendre polynomials},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {4749-4754},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00799},
url = {https://www.sciencedirect.com/science/article/pii/S147466701639694X},
author = {Joachim Deutscher and Markus Bäuml},
keywords = {Nonlinear systems, feedback linearization, Galerkin approach, multivariable Legendre polynomials, -approximation},
abstract = {This paper presents a numerical approach to approximate feedback linearization. By using a Galerkin approach on the basis of multivariable Legendre polynomials an approximate solution to the singular PDE of the feedback linearization technique proposed in Kazantzis and Kravaris (2000a,b) is determined. It is shown that the L2-norm of the remaining nonlinearity in the resulting dynamics can be made small on a specified multivariable interval in the state space. Furthermore, a matrix equation is derived for determining the corresponding change of coordinates and feedback such that the proposed design procedure can easily be implemented in a numerical software package. A simple example demonstrates the properties of the new approximate feedback linearization.}
}
@article{PANGULURI1994233,
title = {A software package to calibrate preferential flow models},
journal = {Environmental Software},
volume = {9},
number = {4},
pages = {233-245},
year = {1994},
issn = {0266-9838},
doi = {https://doi.org/10.1016/0266-9838(94)90022-1},
url = {https://www.sciencedirect.com/science/article/pii/0266983894900221},
author = {Srinivas Panguluri and Aaron A. Jennings},
keywords = {Preferential Flow Models, Parameter Estimation, Aggressive Permeants, Levenberg-Marquardt Algorithm, Monte Carlo Simulation},
abstract = {Preferential pathway flows can form as the result of physical-chemical interactions between soils and chemically aggressive permeants. Preferential pathway flow models have been developed to predict increases in the permeability of hydraulic barriers due to aggressive permeant interactions. The parameters of these models are difficult to determine experimentally. Equation nonlinearities, the number of model coefficients, parameter constraints and randomness of experimental data make the inverse problem of parameter estimation quite challenging. A modified Levenberg-Marquardt algorithm in a Monte Carlo multi-start implementation (Ravi and Jennings, 1990) has been developed into a software package to resolve these difficulties. This package helps users calibrate preferential flow models using transient permeability data. The package, Parameter Estimation Algorithm for Preferential Pathway Models (PEAPPM), has been designed with graphing and contour plotting capabilities to help user improve the calibration process. The package also offers statistical analysis of parameter estimation results.}
}
@article{TYSSO1979383,
title = {CYPROS: Cybernetic Program Packages},
journal = {IFAC Proceedings Volumes},
volume = {12},
number = {7},
pages = {383-389},
year = {1979},
note = {IFAC Symposium on computer Aided Design of Control Systems, Zurich, Switzerland, 29-31 August},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)65625-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017656258},
author = {A. Tyssø},
keywords = {Computer aided design, computer programming, interactive programs, software packages},
abstract = {CYPROS is an interactive program system consisting of a number of special purpose packages for simulation, identification, parameter estimation and control system design. The programming language is standard Fortran IV, and the system is implemented on a medium size computer system (Nord-10). The system is interactive and program control is obtained by the use of numeric terminals. Output is rapidly examined by extensive use of video colour graphics. The subroutines included in the packages are designed and documented according to standardization rules given by the SCL (Scandinavian Control Library) organization. This simplifies the exchange of subroutines throughout the SCL system. Also, this makes the packages attractive for implementation by industrial users. In the simulation package, different integration methods are available, and it can be easily used for off-line, as well as real time simulation problems. The identification package consists of programs for single-input/single-output and multivariable problems. Both transfer function models and state space models can be handled. Optimal test signals can be designed. The control package consists of programs based on multivariable time domain and frequency domain methods for analysis and design. In addition, there is a package for matrix and timeseries manipulation. CYPROS has been applied successfully to industrial problems of various kinds, and parts of the system have already been implemented on different computers in industry. The paper will in some detail describe the use and the contents of the packages and some examples of applications will be discussed.}
}
@article{LI2019110380,
title = {A dataflow-driven approach to identifying microservices from monolithic applications},
journal = {Journal of Systems and Software},
volume = {157},
pages = {110380},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219301475},
author = {Shanshan Li and He Zhang and Zijia Jia and Zheng Li and Cheng Zhang and Jiaqi Li and Qiuya Gao and Jidong Ge and Zhihao Shan},
keywords = {Software engineering, Microservices, Monolith, Decomposition, Data flow, Business logic(s)},
abstract = {Microservices architecture emphasizes employing multiple small-scale and independently deployable microservices, rather than encapsulating all function capabilities into one monolith. Correspondingly, microservice-oriented decomposition, which has been identified to be an extremely challenging task, plays a crucial and prerequisite role in developing microservice-based systems. To address the challenges in such a task, we propose a dataflow-driven semi-automatic decomposition approach. In particular, a four-step decomposition procedure is defined: (1) conduct the business requirement analysis to generate use case and business logic specification; (2) construct the fine-grained Data Flow Diagrams (DFD) and the process-datastore version of DFD (DFDPS) representing the business logics; (3) extract the dependencies between processes and datastores into decomposable sentence sets; and (4) identify candidate microservices by clustering processes and their closely related datastores into individual modules from the decomposable sentence sets. To validate this microservice-oriented decomposition approach, we performed a case study on Cargo Tracking System that is a typical case decomposed by other microservices identification methods (Service Cutter and API Analysis), and made comparisons in terms of specific coupling and cohesion metrics. The results show that the proposed dataflow-driven decomposition approach can recommend microservice candidates with sound coupling and cohesion through a rigorous and easy-to-operate implementation with semi-automatic support.}
}
@article{DESA2010618,
title = {Upgrading a TCABR data analysis and acquisition system for remote participation using Java, XML, RCP and modern client/server communication/authentication},
journal = {Fusion Engineering and Design},
volume = {85},
number = {3},
pages = {618-621},
year = {2010},
note = {Proceedings of the 7th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2010.04.067},
url = {https://www.sciencedirect.com/science/article/pii/S0920379610002085},
author = {W.P. {de Sá}},
keywords = {Remote participation, Data acquisition, Control, Java, XML-RPC},
abstract = {The TCABR data analysis and acquisition system has been upgraded to support a joint research programme using remote participation technologies. The architecture of the new system uses Java language as programming environment. Since application parameters and hardware in a joint experiment are complex with a large variability of components, requirements and specification solutions need to be flexible and modular, independent from operating system and computer architecture. To describe and organize the information on all the components and the connections among them, systems are developed using the eXtensible Markup Language (XML) technology. The communication between clients and servers uses remote procedure call (RPC) based on the XML (RPC-XML technology). The integration among Java language, XML and RPC-XML technologies allows to develop easily a standard data and communication access layer between users and laboratories using common software libraries and Web application. The libraries allow data retrieval using the same methods for all user laboratories in the joint collaboration, and the Web application allows a simple graphical user interface (GUI) access. The TCABR tokamak team in collaboration with the IPFN (Instituto de Plasmas e Fusão Nuclear, Instituto Superior Técnico, Universidade Técnica de Lisboa) is implementing this remote participation technologies. The first version was tested at the Joint Experiment on TCABR (TCABRJE), a Host Laboratory Experiment, organized in cooperation with the IAEA (International Atomic Energy Agency) in the framework of the IAEA Coordinated Research Project (CRP) on “Joint Research Using Small Tokamaks”.}
}
@article{HE2005217,
title = {Multidisciplinary design optimization of mechatronic vehicles with active suspensions},
journal = {Journal of Sound and Vibration},
volume = {283},
number = {1},
pages = {217-241},
year = {2005},
issn = {0022-460X},
doi = {https://doi.org/10.1016/j.jsv.2004.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0022460X04003700},
author = {Yuping He and John McPhee},
abstract = {A multidisciplinary optimization method is applied to the design of mechatronic vehicles with active suspensions. The method is implemented in a GA-A’GEM-MATLAB simulation environment in such a way that the linear mechanical vehicle model is designed in a multibody dynamics software package, i.e. A’GEM, the controllers and estimators are constructed using linear quadratic Gaussian (LQG) method, and Kalman filter algorithm in Matlab, then the combined mechanical and control model is optimized simultaneously using a genetic algorithm (GA). The design variables include passive parameters and control parameters. In the numerical optimizations, both random and deterministic road inputs and both perfect measurement of full state variables and estimated limited state variables are considered. Optimization results show that the active suspension systems based on the multidisciplinary optimization method have better overall performance than those derived using conventional design methods with the LQG algorithm.}
}
@article{WANG2020104087,
title = {Prediction of superheat limit temperatures for fuel mixtures using quantitative structure-property relationship model},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {64},
pages = {104087},
year = {2020},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2020.104087},
url = {https://www.sciencedirect.com/science/article/pii/S0950423019301664},
author = {Beibei Wang and Lulu Zhou and Xin Liu and Kaili Xu and Qingsheng Wang},
keywords = {Superheat limit temperature, Quantitative structure-property relationship, Fuel mixtures, Support vector machine},
abstract = {In this work, a novel QSPR model for the prediction of hydrocarbon mixtures' superheat limit temperatures (SLTs) was developed solely based on chemical structures and mole fractions of pure compounds. Experimental SLTs of 274 blended fuels were collected from existing references and molecular descriptors for the 25 pure materials were obtained by Gaussian 09 software package. Then the integral additive method was employed for the calculation of mixtures' descriptors and support vector machine (SVM) analysis was carried out to establish the nonlinear QSPR model. The most rigorous way of external validation, “compounds out”, was applied to test the true external predictive capabilities of the model and compared with the latest existing model of Lulu Zhou et al. this model performs better in both internal and external validations. The applicability domains (ADs) of this model were discussed and almost all the points are within the AD area. The use of this model only requires the knowledge of chemical structures and the SLTs of the individuals needn't to be known. The accuracy of the proposed model coupled with the ease of its implementation would definitely ensure quick estimation of SLTs of mixtures.}
}
@article{KAMALOV200599,
title = {A Java application for tissue section image analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {77},
number = {2},
pages = {99-113},
year = {2005},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2004.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169260704001026},
author = {R. Kamalov and M. Guillaud and D. Haskins and A. Harrison and R. Kemp and D. Chiu and M. Follen and C. MacAulay},
keywords = {Java, Object-oriented, Platform-independent, Quantitative histopathology, Quantitative cytology, Image cytometry},
abstract = {The medical industry has taken advantage of Java and Java technologies over the past few years, in large part due to the language’s platform-independence and object-oriented structure. As such, Java provides powerful and effective tools for developing tissue section analysis software. The background and execution of this development are discussed in this publication. Object-oriented structure allows for the creation of “Slide”, “Unit”, and “Cell” objects to simulate the corresponding real-world objects. Different functions may then be created to perform various tasks on these objects, thus facilitating the development of the software package as a whole. At the current time, substantial parts of the initially planned functionality have been implemented. Getafics 1.0® is fully operational and currently supports a variety of research projects; however, there are certain features of the software that currently introduce unnecessary complexity and inefficiency. In the future, we hope to include features that obviate these problems.}
}
@article{GOLD20209891,
title = {Model Predictive Interaction Control for Industrial Robots},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {9891-9898},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2696},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320334583},
author = {Tobias Gold and Andreas Völz and Knut Graichen},
keywords = {Nonlinear model predictive control, physical robot-environment interaction, force control, motion control, hybrid force/motion control, compliance control, stiffness estimation},
abstract = {This paper discusses the use of model predictive control (MPC) for industrial robot applications with physical robot-environmental interaction. A model predictive interaction control (MPIC) scheme is introduced that deals both with the prediction of the robot motion and the forces between robot and environment. With regard to the robot motion, either the rigid body dynamics, a simplified model, or a cascaded control structure can be employed. The external forces or torques are treated as additional state variables whose dynamics are based on the elastic behavior of the contact surface. Since the force prediction depends on the knowledge of the environmental stiffness, a method for online estimation is discussed. The approach allows to realize different tasks as motion control, compliance control, direct force control as well as hybrid force/motion control by adjusting the weighting factors in the cost function. The implementation is based on the nonlinear MPC software Grampc and the library Pinocchio for computation of rigid body dynamics. Besides comparing the different robot dynamics models, the approach is demonstrated for a hand-guiding and a table wiping task.}
}
@article{YUMOTO201049,
title = {Rigorous Dynamic Simulator for Control Study of the Large-scale Benchmark Chemical Plant},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {5},
pages = {49-54},
year = {2010},
note = {9th IFAC Symposium on Dynamics and Control of Process Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20100705-3-BE-2011.00009},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016300106},
author = {Takamasa Yumoto and Shigeki Ootakara and Hiroya Seki and Yoshihiro Hashimoto and Hisashi Murata and Manabu Kano and Yoshiyuki Yamashita},
keywords = {Process simulators, dynamic models, plantwide, process control, benchmark examples},
abstract = {A dynamic simulator for the benchmark vinyl acetate (VAc) monomer production process is introduced. Rigorous first-principles dynamic models of the VAc process are implemented on the commercial software package Visual Modeler (Omega Simulation Co., Ltd.). The simulator employs pressure flow calculations and considers non-idealities in the process equipment, so that more realistic simulations are made possible, compared with the conventional simulators. The simulator's high performance calculation provides an environment where feasibility and performance of designed control systems can be efficiently evaluated without sacrificing high fidelity of the process model. The developed simulator will be made available in the public domain with a free limited license of Visual Modeler.}
}
@article{VANBECELAERE2020102361,
title = {Online Tracking of Varying Inertia using a SDFT Approach},
journal = {Mechatronics},
volume = {68},
pages = {102361},
year = {2020},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2020.102361},
url = {https://www.sciencedirect.com/science/article/pii/S0957415820300416},
author = {Foeke Vanbecelaere and Stijn Derammelaere and Niko Nevaranta and Jasper {De Viaene} and Florian Verbelen and Kurt Stockman and Michael Monte},
keywords = {Online parameter estimation, Inertia variation, Sliding discrete Fourier transform (SDFT), Motion control, Co-simulation},
abstract = {The mechanical dynamics of modern machines very often depend on the angular position of the driven axis. To obtain optimal control, such applications typically require an advanced control structure such as an adaptive controller. Moreover, the variation in the dynamics like changing inertia, load torque, and viscous friction limits the performance and reduces the energy efficiency. Energy savings can be obtained by using so-called trajectory optimization techniques combined with feedforward control. However, both optimization and adaptive control require the knowledge of the position dependency of the mechanical parameters. In the case of reciprocating mechanisms, for instance, this position dependency is significant. Consequently, the mechanical parameters change rapidly at high operating speed of the machine. This paper thus contributes towards fast and accurate estimation of rapidly varying mechanical parameters. A sliding discrete Fourier transform (SDFT) approach is proposed to track the inertia variation of a reciprocating mechanism online. The feasibility is verified with experiments on an industrial pick and place unit. Both the results on the real machine and its CAD equivalent, modelled in a multibody dynamics software package, are considered. In addition, the developed inertia tracking algorithm is proven to be implementable in standard commercial drive components.}
}
@article{HENSEL1985155,
title = {CADOCS — Computer Aided Design on Multivariable Control Systems — Methods and the Software Package},
journal = {IFAC Proceedings Volumes},
volume = {18},
number = {8},
pages = {155-160},
year = {1985},
note = {3rd IFAC/IFIP Symposium on Computer Aided Design in Control and Engineering Systems: Advanced Tools for Modern Technology, Lyngby, Denmark, 31 July-2 August 1985},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)60360-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017603604},
author = {H. Hensel},
keywords = {Multivariable discrete-time control systems, computer aided system design, direct digital control, interpreter based software system, parameter estimation, PlD control, state space methods},
abstract = {A CAD-package serving for the design of discrete-time multivariable control systems is presented. Various design methods for different feedback controller types (DB, PID, SC) are described based on discrete-time parametric process models which are obtained by On-line identification methods. The concept of the software package contains an interpreter and a database offering high flexibility in the design and coordinating the design modules. A design example starting with the identification and ending with a DDC-implementation will be shown.}
}
@article{TAMURA1983273,
title = {Design and implementation of SPIDER—A transportable image processing software package},
journal = {Computer Vision, Graphics, and Image Processing},
volume = {23},
number = {3},
pages = {273-294},
year = {1983},
issn = {0734-189X},
doi = {https://doi.org/10.1016/0734-189X(83)90027-0},
url = {https://www.sciencedirect.com/science/article/pii/0734189X83900270},
author = {Hideyuki Tamura and Shigeyuki Sakane and Fumiaki Tomita and Naokazu Yokoya and Masahide Kaneko and Katsuhiko Sakaue},
abstract = {SPIDER is a general-purpose image processing software package which consists of over 400 FORTRAN IV subroutines for various image processing algorithms and several utility programs for managing them. The package was developed for the benefit of extensive interchange and accumulation of programs among research groups. Thus, high transportability of software is emphasized above all in its design concept. In effect, all the image processing subroutines are implemented to be completely free of I/O work such as file access or driving peripheral image devices. The specifications of SPIDER programs also regulate the style of comments in source programs and documentation for the user's manual. SPIDER may also be very useful as a research tool in other scientific disciplines as well as integrating fundamental algorithms in the image processing community. The design concepts, specifications, and contents of SPIDER are described.}
}
@article{GOSDEN19797,
title = {Some cautions in large-scale system design and implementation},
journal = {Information & Management},
volume = {2},
number = {1},
pages = {7-13},
year = {1979},
issn = {0378-7206},
doi = {https://doi.org/10.1016/0378-7206(79)90015-6},
url = {https://www.sciencedirect.com/science/article/pii/0378720679900156},
author = {John A. Gosden},
keywords = {Large-scale systems, user requirements, data reliability, human interface, large system t testing, phased implementation, packaged software, risk reduction, autonomous sub-systems, contingency planning, installation planning, system management trends, networking facilities, hardware},
abstract = {Virtually all large-scale system implementations will benefit from the application of certain management techniques. Awareness (and avoidance) of erroneous assumptions as to system requirements and objectives, data reliability, human interface, and testing methods is important. Strategies such as phased implementation, use of pilots and packaged software, and categorizing and reducing risks are useful. Knowledgeable selection of applicable developments in computer architecture and serious use of planning and control techniques are advocated. Today's trends include well-informed users, better hardware and networking facilities, analysis and study of alternatives, and heavy user involvement in the entire system design and implementation process.}
}
@article{TYCH19965617,
title = {TDC: Computer Aided True Digital Control of Multivariable Delta Operator Systems},
journal = {IFAC Proceedings Volumes},
volume = {29},
number = {1},
pages = {5617-5622},
year = {1996},
note = {13th World Congress of IFAC, 1996, San Francisco USA, 30 June - 5 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)58577-8},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017585778},
author = {Wlodek Tych and Peter Young and Arun Chotai and Llyr C. Jones},
keywords = {multivariable control systems, delta operator, identification, control, design, flight control},
abstract = {The paper describes new multivariablc delta operator options in the True Digital Control (TDC) Computer Aided Control System Design (CACSD) package. The TDC package, which is implemented within the MATLAB-SIMULINK™ software environment and takes full advantage of the new MATLAB graphical user interface, leads the user through all the stages in control system design, from initial SIMULINK simulation studies, through model identification and parameter estimation, to Proportional-Integral-Plus (PIP) control system design and evaluation based on Non-Minimum State Space (NMSS) methods using novel multi-objective optimisation within a Linear Quadratic (LQ) optimal control setting. The practical utility of the package is illustrated by two aerospace examples.}
}
@article{XIE201221,
title = {Implementation and performance optimization of a parallel contour line generation algorithm},
journal = {Computers & Geosciences},
volume = {49},
pages = {21-28},
year = {2012},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2012.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0098300412002063},
author = {Jibo Xie},
keywords = {DEM, Contour line, Parallel computing, Performance optimization},
abstract = {This paper introduces a parallel contour line interpolation algorithm from Digital Elevation Model (DEM). Contour generation from DEM is the basic task of the computer-aided mapping and one of the most important applications of DEM. With the increasing of DEM resolution, the Digital Terrain Analysis (DTA) has become one of the computing-intensive tasks in GIS. Many studies have been done on the implementation of parallel DTA algorithms based on different parallel hardware and software. In this paper, we use the open source GIS toolkit to implement a parallel contour generation algorithm. The Message Passing Interface (MPI) standard is used for the parallel algorithm programming. A slackly LAN (Local Area Network)-connected windows cluster is setup to implement and test the parallel contour line generation algorithm. The performance optimization method for the parallel algorithm is specifically discussed, including data redundancy method, group communication, packaging collection of results, and memory optimization method for results merging. The experimental results show the capacity and potential to implement the parallel GIS algorithms based on open source GIS toolkit in the LAN-connected PC environment.}
}
@article{WASHINGTON2017151,
title = {A parallel structure exploiting nonlinear programming algorithm for multiperiod dynamic optimization},
journal = {Computers & Chemical Engineering},
volume = {103},
pages = {151-164},
year = {2017},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2017.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0098135417301497},
author = {I.D. Washington and C.L.E. Swartz},
keywords = {Multiperiod dynamic optimization, Multiple-shooting, Sequential quadratic programming, Interior-point methods, Parallel computing},
abstract = {This article develops a sequential quadratic programming (SQP) algorithm that utilizes a parallel interior-point method (IPM) for the QP subproblems. Our approach is able to efficiently decompose and solve large-scale multiperiod nonlinear programming (NLP) formulations with embedded dynamic model representations, through the use of an explicit Schur-complement decomposition within the IPM. The algorithm implementation makes use of a computing environment that uses the parallel distributed computing message passing interface (MPI) and specialized vector-matrix class representations, as implemented in the third-party software package, OOPS. The proposed approach is assessed, with a focus on computational speedup, using several benchmark examples involving applications of parameter estimation and design under uncertainty which utilize static and dynamic models. Results indicate significant improvements in the NLP solution speedup when moving from a serial full-space direct factorization approach, to a serial Schur-complement decomposition, to a parallelized Schur-complement decomposition for the primal-dual linear system solution within the IPM.}
}
@article{LIU2011261,
title = {A Multilevel Parallelism Support for Multi-Physics Coupling},
journal = {Procedia Computer Science},
volume = {4},
pages = {261-270},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S187705091100086X},
author = {Fang Liu and Masha Sosonkina},
keywords = {Parallel model coupling, Multi-physics modeling, Multiple Program Multiple Data programming},
abstract = {A new challenge in scientific computing is to merge existing simulation models to create new higher fidelity combined (often multi-level) models. While this challenge has been a driving force in climate modeling for nearly a decade, fusion energy and space weather modeling are starting just now to integrate different sub-physics into a single model. Hence, the demand for novel software paradigms and tools increases drastically. A programming style that mixes task and data parallelism and enables concurrent execution of independent tasks on disjoint processor subsets is called multi-level parallelism. Combined models naturally map into this style, such that sub-models run simultaneously on different processor subgroups. In authors’ previous work, software interfaces supporting the model coupling based on component representations are proposed and shown to successfully combine multi-physics packages via an inter-model solver. In this paper, the inter-model solver, called Coupler, is extended for the execution in multiple processes rather than as a single process. In essence, the multiple program multiple data paradigm is applied to multi-physics coupling. A pure C++ implementation has been developed to bypass the application adaptation to the Common Component Architecture (CCA) framework used in the previous work and to generalize the proposed approach.}
}
@article{HARWOOD201939,
title = {GPU-powered, interactive flow simulation on a peer-to-peer group of mobile devices},
journal = {Advances in Engineering Software},
volume = {133},
pages = {39-51},
year = {2019},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0965997819300031},
author = {Adrian R. G. Harwood},
keywords = {Lattice-Boltzmann method, GPGPU Computing, Distributed computing, Mobile computing},
abstract = {This article develops novel application software which implements interactive, GPU-powered flow simulation on a group of wirelessly-connected mobile devices. Interactive simulation is an emerging field in engineering with use cases appearing in design, analysis and communication. Herein, we present a new Android-based, interactive flow solver capable of running on a wider range of multiple, wirelessly-connected mobile GPUs. The software consists of a 2D Lattice-Boltzmann Method flow physics solver, implemented using OpenGL ES 3.2, as well as a communication library which uses Wi-Fi Direct to communicate between connected devices. We compare the performance of the OpenGL-based solver against existing implementations in CUDA and demonstrate similar computational throughput. We also test a variety of communication strategies based on configurations of GPU memory mapping and communication frequency. Results confirm that passing large amounts of data infrequently offers the best overall efficiency. However, due to the extended time required to pass larger amounts of data to adjacent devices, this configuration can introduce an undesirable stuttering in an interactive application. Finally, comparisons between two and three device networks to the serial case show that, despite the inevitable cost of communication, it is possible to maintain an interactive frame rate across multiple devices; the extension of calculations across multiple devices in this way, allows the tackling of problems which are larger and of higher-resolution that previous.}
}
@article{VERPOEST20052563,
title = {Virtual textile composites software WiseTex: Integration with micro-mechanical, permeability and structural analysis},
journal = {Composites Science and Technology},
volume = {65},
number = {15},
pages = {2563-2574},
year = {2005},
note = {20th Anniversary Special Issue},
issn = {0266-3538},
doi = {https://doi.org/10.1016/j.compscitech.2005.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S026635380500196X},
author = {Ignaas Verpoest and Stepan V. Lomov},
keywords = {Textile composites, Modelling, Internal structure, Deformability, Permeability, Micro-mechanics, Finite elements},
abstract = {The internal geometry of textile reinforcements is an important factor of the reinforcement performance during composite manufacturing and in the service life of the composite material. When a 3D-shaped composite part is concerned, the reinforcement is locally deformed (compressed, stretched and sheared), and any model describing the internal geometry of the reinforcement should account for this deformation. The software package WiseTex implements a generalised description of internal structure of textile reinforcements on the unit cell level, integrated with mechanical models of the relaxed and deformed state of 2D- and 3D-woven, two- and three-axial braided, weft-knitted and non-crimp warp-knit stitched fabrics and laminates. It is integrated with modelling of resin flow, micro-mechanical calculations of properties of textile based composites and micro–macro analysis of composite parts, finite element models and virtual reality software. The paper describes this family of models, which use a unified description of the geometry of the reinforcement unit cell.}
}
@article{CHENG2018395,
title = {Many-core needs fine-grained scheduling: A case study of query processing on Intel Xeon Phi processors},
journal = {Journal of Parallel and Distributed Computing},
volume = {120},
pages = {395-404},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517302599},
author = {Xuntao Cheng and Bingsheng He and Mian Lu and Chiew Tong Lau},
keywords = {In-memory query processing, Many-core processor, Fine-grained scheduling},
abstract = {Emerging many-core processors feature very high memory bandwidth and computational power. For example, Intel Xeon Phi many-core processors of the Knights Corner (KNC) and Knights Landing (KNL) architectures embrace 60 to 64 x86-based CPU cores with 512-bit SIMD capabilities and high-bandwidth memories like the GDDR5 on KNC and on-package DRAMs on KNL. In this paper, we study the performance main-memory database operators and online analytical processing (OLAP) on such many-core architectures. We find that even the state-of-the-art database operators suffer severely from memory stalls and resource underutilization on those many-core processors. We argue that a software approach decomposing a coarse-grained operator into fine-grained phases and executing two independent phases with complementary resource requirements concurrently can address this problem. This approach allows more fine-grained control of resource utilization. Our experiments demonstrate significant performance gain and high resource utilization achieved by our proposed approaches on both KNC and KNL.}
}
@article{LEINEWEBER2003157,
title = {An efficient multiple shooting based reduced SQP strategy for large-scale dynamic process optimization. Part 1: theoretical aspects},
journal = {Computers & Chemical Engineering},
volume = {27},
number = {2},
pages = {157-166},
year = {2003},
issn = {0098-1354},
doi = {https://doi.org/10.1016/S0098-1354(02)00158-8},
url = {https://www.sciencedirect.com/science/article/pii/S0098135402001588},
author = {Daniel B. Leineweber and Irene Bauer and Hans Georg Bock and Johannes P. Schlöder},
keywords = {Large-scale optimal control, Index one DAEs, Multiple shooting, Structured reduced SQP methods, Directional derivatives, Sparse equation systems, Parallel computation},
abstract = {Optimal design and operation of complex chemical processes often require the solution of intricate dynamic optimization problems. A tailored simultaneous solution strategy based on multiple shooting and reduced SQP is presented. This reduced-space boundary value problem (BVP) approach allows an efficient and robust solution of multistage optimal control and design optimization problems for large, sparse DAE process models of index one. The current paper describes the theoretical aspects of the method. Utilizing the natural decomposition of the states into differential and algebraic variables, the structured NLP problem which results from the multiple shooting discretization of the optimization BVP is projected onto the reduced space of differential variables and control parameters. It is shown that this projection can be obtained very efficiently through direct computation of the reduced linearized constraint system via directional sensitivities. Like the original full-space BVP approach, the reduced-space formulation lends itself well to parallel computation. An implementation of the new strategy is provided by the modular optimal control package MUSCOD-II. Software aspects and applications are discussed in a second paper (Part II Software Aspects and Applications, 2002).}
}
@article{WEIS19861099,
title = {Investigating reliability attributes of silicon photodetectors},
journal = {Microelectronics Reliability},
volume = {26},
number = {6},
pages = {1099-1110},
year = {1986},
issn = {0026-2714},
doi = {https://doi.org/10.1016/0026-2714(86)90815-2},
url = {https://www.sciencedirect.com/science/article/pii/0026271486908152},
author = {E.A. Weis and D. Caldararu and M.M. Snyder and N. Croitoru},
abstract = {The electrical parameters of silicon detectors were measured under various external influences (temperature cycling, humidity, salt atmosphere, etc.). The testes were designed and the data was analyzed by using the randomized block design method (implementing the software package SAS). To estimate the lifetime of the detectors, an accelerated life test was implemented. Using plots of inspected interval data based on the maximum likelihood technique (implementing the software package CENSOR), it was found that the Weibull distribution fits the lifetime test data. Calculating the cumulative distribution function and the acceleration factor, the median lifetime of the silicon detector at room temperature was 8.936 × 106 hours and the confidence interval with 95% probability was (7.155–10.575) × 106 hours.}
}
@article{MILIC2003255,
title = {Efficient Algorithm for the Design of High-Speed Elliptic IIR Filters},
journal = {AEU - International Journal of Electronics and Communications},
volume = {57},
number = {4},
pages = {255-262},
year = {2003},
issn = {1434-8411},
doi = {https://doi.org/10.1078/1434-8411-54100168},
url = {https://www.sciencedirect.com/science/article/pii/S1434841104701582},
author = {Ljiljana D. Milić and Miroslav D. Lutovac},
keywords = {IIR filter, Multiplierless filter, Fast algorithms, QMF banks},
abstract = {Abstract
The total latency of the arithmetic operations in the critical direct loop of the second-order all-pass IIR section is reduced using a special class of the elliptic transfer function and thus the maximum sample frequency is increased. This is achieved by implementing one of two multipliers per section as a binary shifter, that is, without any hardware in FPGA or VLSI implementations. The second multiplier is implemented with a small number of shifters and adders using the quantization of the coefficients and optimization. We derive new exact formulae for those coefficients and we avoid all unnecessary computations in the optimization. New exact formulae provide efficient, very fast, simple and highly accurate algorithm suitable for standard software packages such as MATLAB. An example of high-speed two-channel filter bank illustrates the application.}
}
@incollection{RUDGYARD1996711,
title = { - CPULib — A Software Library for Parallel Applications on Arbitrary Meshes},
editor = {A. Ecer and J. Periaux and N. Satdfuka and S. Taylor},
booktitle = {Parallel Computational Fluid Dynamics 1995},
publisher = {North-Holland},
address = {Amsterdam},
pages = {711-718},
year = {1996},
isbn = {978-0-444-82322-9},
doi = {https://doi.org/10.1016/B978-044482322-9/50141-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444823229501419},
author = {Michael Rudgyard and Thilo Schönfeld},
abstract = {Publisher Summary
This chapter focuses on the development of the CERFACS parallel utilities library (CPULib). This prototype software tool aims to simplify the task of parallelizing large two-dimensional and three-dimensional calculations on grids of arbitrary elements using a variety of numerical algorithms. Although simple to use, it is designed to be highly efficient. The prototype version of CPULib is based on a master-slave or client-server implementation. Several partitioning techniques that are available within the library are discussed. CPULib supports two types of overlapping partitions: partial overlapping and full overlapping. In order to demonstrate the effectiveness of CPULib, as well as the use of the different algorithms, the chapter considers the application of an unstructured explicit Euler code to a three dimensional M6-wing. A small tetrahedral grid of only 58000 elements is employed. The chapter demonstrates how it is possible to create a FORTRAN library that abstracts the parallel data-structure from the user's application program.}
}
@article{EDATHOL2020119451,
title = {Prediction of non-equilibrium homogeneous condensation in supersonic nozzle flows using Eulerian-Eulerian models},
journal = {International Journal of Heat and Mass Transfer},
volume = {152},
pages = {119451},
year = {2020},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2020.119451},
url = {https://www.sciencedirect.com/science/article/pii/S0017931019350720},
author = {Jabir Edathol and Dmitrii Brezgin and Konstantin Aronson and Heuy {Dong Kim}},
keywords = {Non-equilibrium condensation, Supersonic flows, Eulerian-Eulerian formulation, Spontaneous nucleation, Steam nozzles},
abstract = {Supersonic flows involving non-equilibrium condensation of steam can be numerically modelled by established Eulerian-Eulerian methods. In the present work, two such popular methods are studied and implemented in a commercial software package ANSYS Fluent, and compared for performance. The first method which is based on a density-based solver, modifies source terms of the governing equations to account for the effect of phase change. In the second approach, an Eulerian-Eulerian mixture model constructed over a pressure based solver, which is already available with the software is adapted and altered for the prediction of non-equilibrium flows. Both models incorporate additional transport equations to estimate the rate of liquid phase generation together with a well-structured sequence of operations comprising several assumptions and theories. The IAPWS-IF97 supplementary equation of state for the metastable-vapour region is used to describe the state of steam with additional empirical relations for the estimation of properties of condensed phase. The nucleation rate is calculated by the classical nucleation theory with non-isothermal/partial pressure corrections and Gyarmathy’s equation is used to compute droplet growth rate. The two models are developed, validated, studied and the capability of respective approaches to model non-equilibrium flows are discussed in detail.}
}
@article{JASIEWICZ201562,
title = {GeoPAT: A toolbox for pattern-based information retrieval from large geospatial databases},
journal = {Computers & Geosciences},
volume = {80},
pages = {62-73},
year = {2015},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2015.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098300415000758},
author = {Jarosław Jasiewicz and Paweł Netzel and Tomasz Stepinski},
keywords = {Pattern analysis, Query-by-example, Large geospatial datasets, Similarity, Image classification, GRASS GIS},
abstract = {Geospatial Pattern Analysis Toolbox (GeoPAT) is a collection of GRASS GIS modules for carrying out pattern-based geospatial analysis of images and other spatial datasets. The need for pattern-based analysis arises when images/rasters contain rich spatial information either because of their very high resolution or their very large spatial extent. Elementary units of pattern-based analysis are scenes – patches of surface consisting of a complex arrangement of individual pixels (patterns). GeoPAT modules implement popular GIS algorithms, such as query, overlay, and segmentation, to operate on the grid of scenes. To achieve these capabilities GeoPAT includes a library of scene signatures – compact numerical descriptors of patterns, and a library of distance functions – providing numerical means of assessing dissimilarity between scenes. Ancillary GeoPAT modules use these functions to construct a grid of scenes or to assign signatures to individual scenes having regular or irregular geometries. Thus GeoPAT combines knowledge retrieval from patterns with mapping tasks within a single integrated GIS environment. GeoPAT is designed to identify and analyze complex, highly generalized classes in spatial datasets. Examples include distinguishing between different styles of urban settlements using VHR images, delineating different landscape types in land cover maps, and mapping physiographic units from DEM. The concept of pattern-based spatial analysis is explained and the roles of all modules and functions are described. A case study example pertaining to delineation of landscape types in a subregion of NLCD is given. Performance evaluation is included to highlight GeoPAT's applicability to very large datasets. The GeoPAT toolbox is available for download from http://sil.uc.edu/.}
}
@article{OLIVEIRACIABATI2017748,
title = {SISPRENACEL – mHealth tool to empower PRENACEL strategy},
journal = {Procedia Computer Science},
volume = {121},
pages = {748-755},
year = {2017},
note = {CENTERIS 2017 - International Conference on ENTERprise Information Systems / ProjMAN 2017 - International Conference on Project MANagement / HCist 2017 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.142},
url = {https://www.sciencedirect.com/science/article/pii/S187705091732344X},
author = {Livia Oliveira-Ciabati and Domingos Alves and Francisco Barbosa-Junior and Elisabeth Meloni Vieira and João Paulo Souza},
keywords = {mHealth, sociotechnical approach, maternal health, usability evaluation},
abstract = {Objective: this study aims to describe the development process of a web-based tool, capable of managing and delivering content through short message service (SMS) to pregnant women and their partners during antenatal and postnatal care (PRENACEL strategy). Methods: We used software engineer best practices to implement an adequate tool for PRENACEL researchers. We used sociotechnical approach to gather the requirements and built prototypes to quickly assess the developed functionalities. SISPRENACEL was created using CakePHP and MySQL and it is based on a client-server architecture, in order to make it accessible over the internet. To develop the graphic interface a free template was used, named AdminLTE version 1.0, created using the Bootstrap library. Results: SISPRENACEL delivered 22,296 scheduled SMS, received 1,249 messages and answered questions through 1,823 SMS. Besides that, we found out that using techniques that included the stakeholders in the development process resulted in a well evaluated, easy to use, pleasant looking and useful system. Conclusion: SISPRENACEL was an essential tool for bringing PRENACEL strategy to reality, managing and delivering content through SMS to women and their partners, without imposing a burden to health professionals.}
}
@article{VIANA2021106458,
title = {Estimating model inadequacy in ordinary differential equations with physics-informed neural networks},
journal = {Computers & Structures},
volume = {245},
pages = {106458},
year = {2021},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2020.106458},
url = {https://www.sciencedirect.com/science/article/pii/S0045794920302613},
author = {Felipe A.C. Viana and Renato G. Nascimento and Arinan Dourado and Yigit A. Yucesan},
keywords = {Physics-informed machine learning, Scientific machine learning, Uncertainty quantification, Recurrent neural networks, Directed graph models},
abstract = {A number of physical systems can be described by ordinary differential equations. When physics is well understood, the time dependent responses are easily obtained numerically. The particular numerical method used for integration depends on the application. Unfortunately, when physics is not fully understood, the discrepancies between predictions and observed responses can be large and unacceptable. In this paper, we propose an approach that uses observed data to estimate the missing physics in the original model (i.e., model-form uncertainty). In our approach, we first design recurrent neural networks to perform numerical integration of the ordinary differential equations. Then, we implement the recurrent neural network as a directed graph. This way, the nodes in the graph represent the physics-informed kernels found in the ordinary differential equations. We quantify the missing physics by carefully introducing data-driven in the directed graph. This allows us to estimate the missing physics (discrepancy term) even for hidden nodes of the graph. We studied the performance of our proposed approach with the aid of three case studies (fatigue crack growth, corrosion-fatigue crack growth, and bearing fatigue) and state-of-the-art machine learning software packages. Our results demonstrate the ability to perform estimation of discrepancy, reducing gap between predictions and observations, at reasonable computational cost.}
}
@article{JANSON1986209,
title = {Applying a pilot system and prototyping approach to systems development and implementation},
journal = {Information & Management},
volume = {10},
number = {4},
pages = {209-216},
year = {1986},
issn = {0378-7206},
doi = {https://doi.org/10.1016/0378-7206(86)90084-4},
url = {https://www.sciencedirect.com/science/article/pii/0378720686900844},
author = {Marius Janson},
keywords = {Pilot System, Prototype, System Design, Evolutionary design, User Participation, End-User Computing},
abstract = {The acquisition and implementation of standard software packages carries with it a need for compromise between the software's capabilities and user information needs. The likelihood for successfully meeting the user's needs, as evidenced by this case study, can be greatly increased by employing a combination of pilot systems and prototypes in the implementation process.}
}
@article{KOKAB2019346,
title = {Extracting of Cross Section Profiles from Complex Point Cloud Data Sets},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {10},
pages = {346-351},
year = {2019},
note = {13th IFAC Workshop on Intelligent Manufacturing Systems IMS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.10.055},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319309103},
author = {H. Setareh Kokab and R. Jill Urbanic},
keywords = {Point Cloud, DBSCAN, Cross Section Profile, Slicing, Outer Boundary Detection},
abstract = {Point cloud data sets are widely used in design and manufacturing. Extracting 2D and 3D features from a point cloud is a field that many researchers are working on. In the present work, a slicing algorithm is implemented for segmenting a point cloud by parallel planes in the X, Y and Z directions and storing the point coordinates within each sectioned plane into separate files. After slicing, a new method is developed for filtering and extracting the outer boundary for each cross section. In the next step, this algorithm is combined with the density-based spatial clustering of applications with noise (DBSCAN) method to achieve a better boundary extraction result for complex outlines. The codes are written in Python (v. 3.7) and executed in Spyder using the Anaconda software package (v. 5.3.1). Complex case studies (*.STL lung model and a femur model) are used to illustrate the merits of this approach.}
}
@article{SHI2005903,
title = {Computer-based algorithms for multiple criteria and multiple constraint level integer linear programming},
journal = {Computers & Mathematics with Applications},
volume = {49},
number = {5},
pages = {903-921},
year = {2005},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2004.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0898122105001008},
author = {Yong Shi and Jing He and Lei Wang and Wei Fan},
keywords = {Multicriteria and multiconstraint level linear programming, Branch-and-bound algorithm, Branch-and-partition algorithm, C++ syntax, Integer solutions},
abstract = {This paper investigates algorithm development and implementation for multicriteria and multiconstraint level (MC2) integer linear programming problems. MC2 linear programming is an extension of linear programming (LP) and multiple criteria (MC) linear programming and a promising computer-aided decision technique in many applications. Here, we present two of the most recent techniques, the MC2 branch-and-partition algorithm and the MC2 branch-and-bound algorithm, to solve MC2 integer linear programs. We describe the design and implementation of a C++ software library for these approaches, and then conduct a comparison study in terms of computational efficiency and complexity through a series of empirical tests.}
}
@article{GROBL2012451,
title = {Biomass steam gasification for production of SNG – Process design and sensitivity analysis},
journal = {Applied Energy},
volume = {97},
pages = {451-461},
year = {2012},
note = {Energy Solutions for a Sustainable World - Proceedings of the Third International Conference on Applied Energy, May 16-18, 2011 - Perugia, Italy},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2012.01.038},
url = {https://www.sciencedirect.com/science/article/pii/S030626191200044X},
author = {Thomas Gröbl and Heimo Walter and Markus Haider},
keywords = {Biomass, Gasification, Methanation, Simulation, Sensitivity analysis},
abstract = {A process design for small-scale production of Substitute Natural Gas (SNG) by steam gasification of woody biomass is performed. In the course of this work, thermodynamic models for the novel process steps are developed and implemented into an already existing model library of commercial process simulation software IPSEpro. Mathematical models for allothermal steam gasification of biomass as well as for cleaning and methanation of product gas are provided by applying mass balances, energy balances and thermodynamic equilibrium equations. Using these models the whole process is integrated into the simulation software, a flowsheet for an optimum thermal integration of the single process steps is determined and energy savings are identified. Additionally, a sensitivity study is carried out in order to analyze the influence of various operation parameters. Their effects on amount and composition of the product gas and process efficiency are evaluated and discussed within this article.}
}
@article{CHUN20131370,
title = {Analysis on capabilities of density-based solvers within OpenFOAM to distinguish aerothermal variables in diffusion boundary layer},
journal = {Chinese Journal of Aeronautics},
volume = {26},
number = {6},
pages = {1370-1379},
year = {2013},
issn = {1000-9361},
doi = {https://doi.org/10.1016/j.cja.2013.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1000936113002033},
author = {Shen Chun and Sun Fengxian and Xia Xinlin},
keywords = {Aerothermal variables, Boundary layer, Computational fluid dynamics (CFD), Heat flux, Open source, Supersonic},
abstract = {Open source field operation and manipulation (OpenFOAM) is one of the most prevalent open source computational fluid dynamics (CFD) software. It is very convenient for researchers to develop their own codes based on the class library toolbox within OpenFOAM. In recent years, several density-based solvers within OpenFOAM for supersonic/hypersonic compressible flow are coming up. Although the capabilities of these solvers to capture shock wave have already been verified by some researchers, these solvers still need to be validated comprehensively as commercial CFD software. In boundary layer where diffusion is the dominant transportation manner, the convective discrete schemes’ capability to capture aerothermal variables, such as temperature and heat flux, is different from each other due to their own numerical dissipative characteristics and from viewpoint of this capability, these compressible solvers within OpenFOAM can be validated further. In this paper, firstly, the organizational architecture of density-based solvers within OpenFOAM is analyzed. Then, from the viewpoint of the capability to capture aerothermal variables, the numerical results of several typical geometrical fields predicted by these solvers are compared with both the outcome obtained from the commercial software Fastran and the experimental data. During the computing process, the Roe, AUSM+(Advection Upstream Splitting Method), and HLLC(Harten-Lax-van Leer-Contact) convective discrete schemes of which the spatial accuracy is 1st and 2nd order are utilized, respectively. The compared results show that the aerothermal variables are in agreement with results generated by Fastran and the experimental data even if the 1st order spatial precision is implemented. Overall, the accuracy of these density-based solvers can meet the requirement of engineering and scientific problems to capture aerothermal variables in diffusion boundary layer.}
}
@article{ABDELRAHMAN2000441,
title = {A methodology for development of configurable remote access measurement system},
journal = {ISA Transactions},
volume = {39},
number = {4},
pages = {441-458},
year = {2000},
issn = {0019-0578},
doi = {https://doi.org/10.1016/S0019-0578(00)00025-2},
url = {https://www.sciencedirect.com/science/article/pii/S0019057800000252},
author = {Mohamed Abdelrahman and Abdul Rasheed},
keywords = {Configurable measurement systems, Object modeling technique, Java, Client–server communication, Server–instrument communication, LabVIEW},
abstract = {A configurable remote access measurement system (CRAMS) is designed using an object oriented methodology (OOM) and implemented using the integration of an object oriented language, JAVA, a relational database management system, MS-ACCESS, and an instrumentation software package (LabVIEW). OOM is a powerful technique that is used to manage the complexity of large systems. It allows for easy maintenance and upgrading of the developed systems. The main focus of this paper is to present a detailed procedure for the analysis, design and implementation of CRAMS. The functionality of CRAMS is demonstrated by creating a remotely accessible laboratory environment using a set of programmable and virtual instruments connected to a PC server.}
}
@article{ABDELHAMEED202249,
title = {Experimental and numerical investigation on the performance of adhesive steel-dowels used in precast reinforced concrete elements},
journal = {Structures},
volume = {40},
pages = {49-63},
year = {2022},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2022.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352012422002673},
author = {Samya {Abd El-Hameed} and Mohamed Eladawy and Mohamed H. Agamy and Hesham Haggag},
keywords = {Pre-cast reinforced concrete, Push-off test, Beam–column dowel connection, Shear capacity, Adhesive steel dowel, Finite element, Shear friction, ANSYS},
abstract = {Precast reinforced concrete (PCRC) applications provide several structural, economic, and environmental benefits. Therefore, recently the PCRC elements have been widely used in construction implementation. However, the connection's behavior and its possible failure under different types of loads are considered essential challenges during the design of the PCRC elements. This study aimed to investigate the performance of the adhesive dowels connections and their shear capacity. The experimental phases focused on examining eighteen push-off concrete specimens and four reinforced concrete (RC) beams-column connections. While the numerical phase was conducted using the finite-element software package ANSYS to simulate concrete connections behavior. Push-off specimens consisted of two concrete blocks measured 250 mm in width. Blocks connected by steel dowel that crossed the shear plane. While beams measured 120 mm in width, 300 mm in depth, and 1400 mm in length. Beams connected to existing 300 mm square column with adhesive dowels. All specimens were loaded monotonically until failure through a constant loading rate. The experimental study aimed to evaluate the effects of the shear plane depth, dowels’ embedded length, dowels’ diameter, the edge distance, and dowels ratio The test results revealed that the shear plane conditions and area, as well as the adhesive dowels configurations, had a significant effect on the shear capacity of dowel specimen. Moreover, the finite element model provided a better understanding of the different modes of failure observed during experimental testing and the shear capacity of adhesive dowels.}
}
@article{SCHNEIDER2008355,
title = {Simulating the cornering behaviour of multiple trailed implements},
journal = {Biosystems Engineering},
volume = {100},
number = {3},
pages = {355-361},
year = {2008},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2008.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1537511008000871},
author = {Till Schneider and John M. Fielke},
abstract = {The trend towards using larger sowing widths and capacities has led to the construction of heavy, multi-axle machinery that can be used in a configuration of one implement behind the other. An Australian example of this is a 300kW four-wheel drive tractor towing an 18m wide cultivator behind which is hitched to a 15000l 3 bin pneumatic seeder. Additional soil-levelling implements may also be hitched behind this combination. The use of several implements hitched in series influences the steerability, and may limit the minimum turning radius. A knowledge of the performance of linkages and machine hitching options will assist both designers and users of machinery. This paper presents a method of simulating implement tracking to aid the design of steering and hitching components for use in optimising a machine combination. A simulation was developed using the software packages SolidWorks and CosmosMotion to control solid modelled machines using a range of geometric and tyre performance parameters. The simulation was verified against standard truck turning path templates and under conditions when Ackerman geometry was achieved. The methods used and typical outputs from the simulations are described.}
}
@article{GERBER2017109,
title = {Extending R packages to support 64-bit compiled code: An illustration with spam64 and GIMMS NDVI3g data},
journal = {Computers & Geosciences},
volume = {104},
pages = {109-119},
year = {2017},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2016.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0098300416307415},
author = {Florian Gerber and Kaspar Mösinger and Reinhard Furrer},
keywords = {Sparse matrix, Non-stationarity, Compactly supported covariance function, Huge dataset, dotCall64, Foreign function interface},
abstract = {Software packages for spatial data often implement a hybrid approach of interpreted and compiled programming languages. The compiled parts are usually written in C, C++, or Fortran, and are efficient in terms of computational speed and memory usage. Conversely, the interpreted part serves as a convenient user-interface and calls the compiled code for computationally demanding operations. The price paid for the user friendliness of the interpreted component is—besides performance—the limited access to low level and optimized code. An example of such a restriction is the 64-bit vector support of the widely used statistical language R. On the R side, users do not need to change existing code and may not even notice the extension. On the other hand, interfacing 64-bit compiled code efficiently is challenging. Since many R packages for spatial data could benefit from 64-bit vectors, we investigate strategies to efficiently pass 64-bit vectors to compiled languages. More precisely, we show how to simply extend existing R packages using the foreign function interface to seamlessly support 64-bit vectors. This extension is shown with the sparse matrix algebra R package spam. The new capabilities are illustrated with an example of GIMMS NDVI3g data featuring a parametric modeling approach for a non-stationary covariance matrix.}
}
@article{JABLONSKI20133181,
title = {Preliminary study on the enhancement of WCDMA technology with SON functionality},
journal = {Measurement},
volume = {46},
number = {9},
pages = {3181-3191},
year = {2013},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2013.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S0263224113002170},
author = {Ireneusz Jabłoński},
keywords = {Measurement data, Computer simulations, Wireless telecommunication network, Self-organized networks, System measurements and optimization},
abstract = {This paper reports on work aimed at designing a dynamic system-level simulator for wireless telecommunication networks. Presented here are considerations devoted to the development of a software tool, a preliminary condition for the analysis of various technological aspects directed towards the enhancement of WCDMA (Wideband Code Division Multiple Access) technology by implementing SON (Self Organized Networks) functionality. The future broadening of such strategies to include multi-technology networks will be possible with the created methodology and software library. Compared to typical attempts in this research domain, which involve pure software representations of wireless system operations, the methodology offered in this paper creates a tool that has direct practical value, which is primarily based on operations on input data measured in an actual wireless telecommunications system. Thus, both detailed cases concerning wireless communication in a chosen region of the world and general ideas of a conceptual nature can be considered with the support of an IRSON (Integrated Research on SON) system-level simulator. Details on the assumptions and designs of the simulator and its verification (primarily through feasibility studies) are sketched together with exemplary qualitative and quantitative results concerning its operation. These results prove that the imitation of conditions in a wireless communications network is possible with a designed part of an IRSON simulator for any vendor and network operating in any geographic location. At this level, processes inside the system of a WCDMA network can be monitored in the direction of optimal handover management, but other mechanisms are consecutively implemented to flexibly join various technologies (inter-RAT), layers (multi-frequency) and deployments (macro-, micro-, pico- or femto-cell) within a coherent and exact tool. Apart from the planning, optimization and management of an actual network, another practical application of IRSON concerns the design of new features for future mobile telecommunication technologies.}
}
@incollection{WITTEN2010343,
title = {Chapter 7 - Interoperability: Protocols and services},
editor = {Ian H. Witten and David Bainbridge and David M. Nichols},
booktitle = {How to Build a Digital Library (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {343-370},
year = {2010},
series = {The Morgan Kaufmann Series in Multimedia Information and Systems},
isbn = {978-0-12-374857-7},
doi = {https://doi.org/10.1016/B978-0-12-374857-7.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123748577000074},
author = {Ian H. Witten and David Bainbridge and David M. Nichols},
abstract = {Publisher Summary
This chapter focuses on interoperability between libraries, examining the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) and the long established Z39.50 standard. A Web Service is defined as a software system designed to support interoperable machine-to-machine interaction over a network, and Web Services are becoming a popular way for digital libraries to supply their content to other systems and interfaces that are designed to satisfy particular user needs. The Z39.50 standard defines a wide-ranging protocol for information retrieval between a client and a database server. The Z39.50 protocol is divided into eleven logical sections called facilities that each provides a broad set of services. A particular Z39.50 system need not implement all parts of the protocol. Indeed, the protocol is so complex that full implementation is a daunting undertaking and may in any case be inappropriate for a particular digital library site. The Open Archives Initiative (OAI) was motivated by the electronic preprint community, which has a strong desire to increase the availability of scholarly repositories and to enhance access to them. OAI-PMH imposes no requirements on how metadata is represented on the server of a data provider. The metadata can be stored in a database, as simple XML files, or even embedded within other documents.}
}
@article{NEE1989649,
title = {CAE/CAD/CAM curricula implementation - experience at the National University of Singapore},
journal = {Computer-Aided Design},
volume = {21},
number = {10},
pages = {649-653},
year = {1989},
issn = {0010-4485},
doi = {https://doi.org/10.1016/0010-4485(89)90163-2},
url = {https://www.sciencedirect.com/science/article/pii/0010448589901632},
author = {A.Y.C. Nee and C.C. Hang},
keywords = {computer-aided design, courses},
abstract = {A step-by-step approach to implement CAE/CAD/CAM curricula for undergraduate engineering courses at the National University of Singapore is presented. As the student workload is already quite heavy, each course is examined in detail and the CAE/CAD/CAM contents are carefully blended with the existing course structure. Some students are also assigned CAE/CAD/CAM specific projects to allow them to develop expertise in developing new software packages and building interfaces to existing packages. Experience indicates that CAE/CAD/CAM programmes at the undergraduate level should be implemented in various phases. It is also necessary to consider the state of the local industries in the adoption of the CAE/CAD/CAM technology so that the effort by the university is well synchronized with industry.}
}
@incollection{VALDERRAMA2002579,
title = {A Unified Model for Co-simulation and Co-synthesis of Mixed Hardware/Software Systems},
editor = {Giovanni {De Micheli} and Rolf Ernst and Wayne Wolf},
booktitle = {Readings in Hardware/Software Co-Design},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {579-583},
year = {2002},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-155860702-6/50053-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607026500533},
author = {C.A. Valderrama and A. Changuel and P.V. Raghavan and M. Abid and T. Ben Ismail and A.A. Jerraya},
abstract = {Abstract
This paper presents a methodology for a unified co-simulation and co-synthesis of hardware-software systems. This approach addresses the modeling of communication between the hardware and software modules at different abstraction levels and for different design tools. The main contribution is the use of a multi-view library concept in order to hide specific hardware/software implementation details and communication schemes. A system is viewed as a set of communicating hardware(VHDL) and software(C) sub-systems. The same C, VHDL descriptions can be used for both co-simulation and hardware-software co-synthesis. This approach is ilustrated by an example.}
}
@incollection{ATTARAKIH201767,
title = {Population balance modelling of pulsed packed bed extraction columns using PPBLab software},
editor = {Antonio Espuña and Moisès Graells and Luis Puigjaner},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {40},
pages = {67-72},
year = {2017},
booktitle = {27th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63965-3.50013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444639653500131},
author = {Menwer Attarakih and Samer Alzyod and Armin Fricke},
keywords = {Population balances, Mathematical modelling, PPBLab, Pulsed packed column, CAPE-OPEN},
abstract = {In this work, we present a new population balance based module for modelling the hydrodynamics and mass transfer processes in pulsed packed bed liquid extraction columns. The new module is fully implemented using PPBLab software, which utilizes recent population balance model solution algorithms. In this regard, the PPBLab detailed and reduced extended fixed pivot solvers are used to discretize the internal coordinates, while the PPBLab built-in space-time solver is used to discretize the physical spatial domain. In addition to this, a user-friendly interface is designed to facilitate the user inputs and outputs and to allow a full access to the CAPE-OPEN thermodynamics package (TEA). As a case study, this PPBLab column module is validated using the published steady state experimental data for water-acetone-toluene chemical system in a DN80 pulsed packed bed liquid extraction column. The predicted column performance is found to agree well with PPBLab software simulation results.}
}
@incollection{BARCLAY2004241,
title = {8 - Design patterns},
editor = {K. Barclay and J. Savage},
booktitle = {Object-Oriented Design with UML and Java},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {241-276},
year = {2004},
isbn = {978-0-7506-6098-3},
doi = {https://doi.org/10.1016/B978-075066098-3/50008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780750660983500088},
author = {K. Barclay and J. Savage},
abstract = {Publisher Summary
This chapter focuses on the importance of software design patterns in object-oriented development. The chapter investigates and demonstrates a range of design patterns, captured through unified modeling language (UML) diagrams and implemented in Java. The chapter also demonstrates a number of illustrations, how these design patterns can be exploited. Patterns are ways to describe best practices, good designs, and capture experience in a way that it is possible for others to reuse this experience. Specialization and delegation are widely used in object-oriented systems. Both provide powerful ways of reusing code. Delegation can often be used in place of specialization, offering flexibility at run-time. The adapter design pattern is used to introduce a class with the required set of services that is realized by another class that has the wrong set of services for a client. The decorator pattern is used to dynamically add new functionality to an object. Many of these design patterns have been incorporated into the Java API.}
}