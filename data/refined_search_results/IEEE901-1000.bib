@ARTICLE{10048730,
author={},
journal={IEEE P1666/D1.2, November 2022},
title={IEEE Draft Standard for Standard SystemC Language Reference Manual},
year={2023},
volume={},
number={},
pages={1-615},
abstract={SystemC® is defined in this standard. SystemC is an ISO standard C++ class library for system and hardware design for use by designers and architects who need to address complex systems that are a hybrid between hardware and software. This standard provides a precise and complete definition of the SystemC class library so that a SystemC implementation can be developed with reference to this standard alone. The primary audiences for this standard are the implementors of the SystemC class library, the implementors of tools supporting the class library, and the users of the class library.},
keywords={IEEE Standards;Design automation;C++ languages;Hardware design languages;System-on-chip;C++;computer languages;digital systems;discrete event simulation;electronic design automation;electronic system level;electronic systems;embedded software;fixed-point;hardware description language;hardware design;hardware verification;IEEE 1666™;SystemC;system modeling;system-on-chip;transaction level},
doi={},
ISSN={},
month={Feb},}
@ARTICLE{6881728,
author={Grottel, Sebastian and Krone, Michael and Müller, Christoph and Reina, Guido and Ertl, Thomas},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={MegaMol—A Prototyping Framework for Particle-Based Visualization},
year={2015},
volume={21},
number={2},
pages={201-214},
abstract={Visualization applications nowadays not only face increasingly larger datasets, but have to solve increasingly complex research questions. They often require more than a single algorithm and consequently a software solution will exceed the possibilities of simple research prototypes. Well-established systems intended for such complex visual analysis purposes have usually been designed for classical, mesh-based graphics approaches. For particle-based data, however, existing visualization frameworks are too generic - e.g. lacking possibilities for consistent low-level GPU optimization for high-performance graphics - and at the same time are too limited - e.g. by enforcing the use of structures suboptimal for some computations. Thus, we developed the system softwareMegaMol for visualization research on particle-based data. On the one hand, flexible data structures and functional module design allow for easy adaption to changing research questions, e.g. studying vapors in thermodynamics, solid material in physics, or complex functional macromolecules like proteins in biochemistry. Therefore, MegaMol is designed as a development framework. On the other hand, common functionality for data handling and advanced rendering implementations are available and beneficial for all applications. We present several case studies of work implemented using our system as well as a comparison to other freely available or open source systems.},
keywords={Data visualization;Rendering (computer graphics);Graphics processing units;Data models;Libraries;Visualization;Visualization system;software framework;particle-based data;large data},
doi={10.1109/TVCG.2014.2350479},
ISSN={1941-0506},
month={Feb},}
@INPROCEEDINGS{4115951,
author={Zeng, Yuan and Zhang, Pei and Wang, Meihong and Jia, Hongjie and Yu, Yixin and Lee, Stephen T.},
booktitle={2006 International Conference on Power System Technology},
title={Development of a New Tool for Dynamic Security Assessment Using Dynamic Security Region},
year={2006},
volume={},
number={},
pages={1-5},
abstract={Dynamic security region (DSR) gives system engineers systematic and global information about the feasible operation region in power injection space. This paper is a result of an investigation into the use of DSR concept and its effective application in power systems. It particularly concerns with developing a software-based tool for the dynamic security assessment in the online operation environment. A fast direct method used for computing hyper-plane boundary of the DSR is described. In virtue of being practical of the direct method, a new software package, named Power System Dynamic Security Region (PSDSR) is developed. Following a brief discussion about the main requirements, the paper gives a summary of the development of PSDSR from both the software architecture and the function implementation. A most distinctive character, result visualization, with the related group reduction technology is emphasized.},
keywords={Power system security;Power system dynamics;Power system stability;Power system transients;Power system analysis computing;Information security;Least squares approximation;Power system simulation;Software packages;Power system modeling;Direct Method;Dynamic Security Region;Hyper-plane;Power System;Software Package;Transient Stability;Visualization},
doi={10.1109/ICPST.2006.321756},
ISSN={},
month={Oct},}
@INPROCEEDINGS{478123,
author={Sharpe, W. and McCarthy, D.},
booktitle={IEE Colloquium on Partitioning in Hardware-Software Codesigns},
title={Why can't hardware be more like software?},
year={1995},
volume={},
number={},
pages={1/1-1/6},
abstract={This paper describes how the potential mass market for information appliances will require a new set of hardware-software codesign tools aimed at fast -time-to-market and utilising macrofunction libraries. It indicates that tools are required for the macrofunction development, which differ from system tools, and resemble the behavioural compilers of today. Appliance developers would like to see the automation of the system engineering, leaving the engineers to do the system definition, and specific technology evolution, required for their products. It has also been stated that appliance developers are willing to accept the inefficiencies associated with macrofunctions, in a similar manner to the way that software developers treat memory. Utilising tools that permit the simple integration of these macrofunctions, much in the same way that complex software systems are developed today, will allow products to be generated successfully. For the partitioning case in particular, tools will be required for both development paths: partitioning at the system level will involve manipulating macrofunctions; partitioning for macrofunction development will involve the accurate modelling of architectures for algorithm implementation.},
keywords={System analysis and design;Systems engineering;Technology forecasting},
doi={10.1049/ic:19950167},
ISSN={},
month={Feb},}
@ARTICLE{595577,
author={Hong Xu and Yadong Gui and Ni, L.M.},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Optimal software multicast in wormhole-routed multistage networks},
year={1997},
volume={8},
number={6},
pages={597-607},
abstract={Multistage interconnection networks are a popular class of interconnection architecture for constructing scalable parallel computers (SPCs). The focus of this paper is on the multistage network system which supports wormhole routed turnaround routing. Existing machines characterized by such a system model include the IBM SP-1 and SP-2, TMC CM-5, and Meiko CS-2. Efficient collective communication among processor nodes is critical to the performance of SPCs. A system-level multicast service, in which the same message is delivered from a source node to an arbitrary number of destination nodes, is fundamental in supporting collective communication primitives including the application-level broadcast, reduction, and barrier synchronization. This paper addresses how to efficiently implement multicast services in wormhole-routed multistage networks, in the absence of hardware multicast support, by exploiting the properties of the turnaround switching technology. An optimal multicast algorithm is proposed. The results of implementations on a 64-node SP-1 show that the proposed algorithm significantly outperforms the application-level broadcast primitives provided by currently existing collective communication libraries including the public domain MPI.},
keywords={Broadcasting;Multicast algorithms;Multiprocessor interconnection networks;Computer architecture;Computer networks;Concurrent computing;Routing;Hardware;Communication switching;Libraries},
doi={10.1109/71.595577},
ISSN={1558-2183},
month={June},}
@INPROCEEDINGS{4684406,
author={Kafadarova, Nadezhda and Andonova, Anna and Andreev, Svetozar and Arnaudov, Radosvet and Tzanova, Slavka},
booktitle={2008 2nd Electronics System-Integration Technology Conference},
title={Thermal optimization of 3D microcontacts using DOE and CFD analysis},
year={2008},
volume={},
number={},
pages={535-540},
abstract={The present article describes an approach for optimization of thermal performance of 3D micro-components from pin-ring type implemented in IC package to PCB assembly and micro mechanical actuators by common use of DOE and CFD simulations. The goal of the considered approach is to define a real concurrent process for design of reliable microelectronic systems for specific applications. A model, containing all of the requisite design factors such as sizes, material and form was created using the commercially available CFD software, FLOTHERM. The results of simulation of test structures are verified by thermovision measurements by infrared camera P640 of FLIR. Advantages and disadvantages of different studied constructions of micro-contacts are analyzed in respect of better parameters of the process of heat transfer.},
keywords={US Department of Energy;Computational fluid dynamics;Integrated circuit packaging;Assembly;Actuators;Process design;Microelectronics;Application software;Testing;Cameras;Computational Fluid Dynamics;Design of Experiments;Micro-contacts;Infrared Thermography;Design Optimization},
doi={10.1109/ESTC.2008.4684406},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{381563,
author={deMare, G. and Giordano, G.},
booktitle={Proceedings of AUTOTESTCON '94},
title={Intelligent maintenance aid "software"},
year={1994},
volume={},
number={},
pages={595-601},
abstract={The test industry is developing many forms of maintenance aids built around programs such as the Army SPORT and the Air Force IMIS. This paper presents a key software component called Diagnostician which fits into any hardware implementation to provide the "intelligence". Commercial tools for automatic generation of the UUT knowledge base makes the concept very cost effective.<>},
keywords={Software maintenance;Couplings;Artificial intelligence;Hardware;Costs;System testing;Runtime;Automation;Product design;Application software},
doi={10.1109/AUTEST.1994.381563},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6041845,
author={Chmaj, Grzegorz and Zydek, Dawid},
booktitle={2011 21st International Conference on Systems Engineering},
title={Software Development Approach for Discrete Simulators},
year={2011},
volume={},
number={},
pages={273-278},
abstract={Simulation is the most common approach to perform the problem research. Among several types of simulation, the most common way is the discrete simulation, which assumes the division of the time scale into fixed length time slots. Depending on investigated problem, simulation packages may be used or it could be necessary to design and create own simulation system. In this paper, we propose the complete pre-study scheme and the most commonly appearing implementation problems with suggested solutions. We also describe how to implement the exemplary simulator in C++.},
keywords={Peer to peer computing;Computational modeling;Object oriented modeling;Computer architecture;Data models;Programming;simulation;distributed computing;programming},
doi={10.1109/ICSEng.2011.56},
ISSN={},
month={Aug},}
@INPROCEEDINGS{1327483,
author={Far, B.H.},
booktitle={Proceedings of the Third IEEE International Conference on Cognitive Informatics, 2004.},
title={Modeling and implementation of software agents decision making},
year={2004},
volume={},
number={},
pages={258-267},
abstract={Software agents are knowledgeable, autonomous, situated and interactive software entities. Agents' interactions are of special importance when a group of agents interact with each other to solve a problem that is beyond the capability and knowledge of each individual. Efficiency, performance and overall quality of the multiagent applications depend mainly on how the agents interact with each other effectively. We suggest an agent model by which we can clearly distinguish different agent's interaction scenarios. The model has five attributes: goal, control, interface, identity and knowledge base. Using the model, we analyze and describe possible scenarios; devise the appropriate reasoning and decision making techniques for each scenario; and build a library of reasoning and decision making modules that can be used readily in the design and implementation of multiagent systems.},
keywords={Software agents;Decision making;Pervasive computing;Uncertainty;Application software;Multiagent systems;Isolation technology;Distributed computing;Drives;Libraries},
doi={10.1109/COGINF.2004.1327483},
ISSN={},
month={Aug},}
@INPROCEEDINGS{5567768,
author={Wang, Wen and Luo, Huijin},
booktitle={2010 18th International Conference on Geoinformatics},
title={Design and construction of environmental monitoring data public system based on WebGIS platform},
year={2010},
volume={},
number={},
pages={1-5},
abstract={The environmental monitoring data of air-quality, acid rain, water and city noise is the mainly information of environmental monitoring. Based on WebGIS platform, to collect the public data and manage them reasonably can greatly support the environmental information publicity. In this paper, the system is based on B/S structure, and developed under PHP combining with MapServer platform and JavaScript library for GIS. MapServer is an Open Source platform for publishing spatial data and interactive mapping applications to the web. Some JavaScript library for GIS can greatly support the interaction in WebGIS. The back-end database is adopted PostGIS to store and manage. All GIS and web framework in this system is based on MapServer and other open source GIS software.},
keywords={Spatial databases;Geographic Information Systems;Monitoring;Servers;Data visualization;Software;WebGIS;open source GIS;MapServer;environmental public GIS},
doi={10.1109/GEOINFORMATICS.2010.5567768},
ISSN={2161-0258},
month={June},}
@INPROCEEDINGS{552334,
author={Posthoff, C.},
booktitle={Proceedings of IEEE 5th International Fuzzy Systems},
title={Prototyping of learned (fuzzy) logic structures using (fuzzy) logic equations},
year={1996},
volume={2},
number={},
pages={1113-1119 vol.2},
abstract={It has been shown in earlier publications (Orlando 1994) that learning from examples is an efficient possibility for knowledge acquisition. Very often the examples can be described by binary variables and logic functions. The use of prototypes (examples) and the subsequent change to fuzzy models can be done in any stage of the modelling process. The paper shows that many problems arising in this context can be solved by considering fuzzy-logic equations and systems of fuzzy-logic equations in a very general sense. The basic concepts and procedures are presented. Some applications and further research problems are included. Efficient algorithms and software package for the treatment of fuzzy-logic equations and systems of equations will be an efficient means for the construction of fuzzy-logic models.},
keywords={Fuzzy logic;Prototypes;Equations;Logic functions;Knowledge acquisition;Mathematics;Computer science;Application software;Software algorithms;Software packages},
doi={10.1109/FUZZY.1996.552334},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{5298775,
author={Saraiva, João de Sousa and Silva, Alberto Rodrigues da},
booktitle={2009 Fourth International Conference on Software Engineering Advances},
title={Development of CMS-Based Web-Applications Using a Model-Driven Approach},
year={2009},
volume={},
number={},
pages={500-505},
abstract={The emerging Model-Driven Engineering paradigm advocates the use of models as first-class citizens in the software development process, while artifacts such as documentation and source-code can be quickly produced from those models by using automated transformations. One of the many types of deployment platforms that can potentially benefit from such model-driven approaches are Content Management Systems, as these approaches can significantly accelerate the development of new web-applications and features, as well as simplify their maintenance. This work proposes the creation of a model-driven approach for the development of web-applications based on Content Management Systems. This approach is based on the creation of two modeling languages (which are situated at different levels of abstraction, and are used to both quickly model a web-application and provide a common ground for the creation of additional languages), and a mechanism for the processing of models specified using those languages. The current results of this work so far are the development of a Content Management System that effectively supports web-applications of medium complexity, and the creation of a reference case study that will be used to validate this work.},
keywords={Content management;Collision mitigation;Model driven engineering;Internet;Electrochemical machining;Software engineering;Programming;Documentation;Libraries;Acceleration;Software Design;Software Construction;Software Engineering Tools and Methods},
doi={10.1109/ICSEA.2009.79},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9776793,
author={Wang, Lan},
booktitle={2022 6th International Conference on Trends in Electronics and Informatics (ICOEI)},
title={HTML Data Capture and User Portrait Data Mining of Landscape Co-Design Platform Based on Distributed Mobile Workstations},
year={2022},
volume={},
number={},
pages={1731-1734},
abstract={The application of CoDesign platform in landscape construction is discussed. Through software such as TerraBuilder and TerraExplorer Pro, lightweight Web data mining and transformation can be realized by using HTML Tidy. The transformation process mainly solves the separation of schema information to be expressed by HTML documents and their collections. The conversion step is to use the standard class library provided by the distributed mobile workstation, and use the super computing performance of the cloud platform to structure the unstructured HTML data to improve the data processing efficiency of the mobile intelligent terminal. Comprehensive application.},
keywords={Cloud computing;Solid modeling;Three-dimensional displays;Distributed databases;Data processing;Market research;Software;HTML Data Capture;User Portrait;Landscape Co-Design Platform;Distributed Mobile Workstations},
doi={10.1109/ICOEI53556.2022.9776793},
ISSN={},
month={April},}
@INPROCEEDINGS{795341,
author={Evans, K.},
booktitle={Proceedings of the 1999 Particle Accelerator Conference (Cat. No.99CH36366)},
title={New features in MEDM [EPICS accelerator control software]},
year={1999},
volume={2},
number={},
pages={744-746 vol.2},
abstract={MEDM, which is derived from Motif Editor and Display Manager, is the primary graphical interface to the EPICS control system. This paper describes new features that have been added to MEDM in the last two years. These features include new editing capabilities, a PV Info dialog box, a means of specifying limits and precision, a new implementation of the Cartesian Plot, new features for several objects, new capability for the Related Display, help, a user-configurable Execute Menu, reconfigured start-up options, and availability for Windows 95/98/NT. Over one hundred bugs have been fixed, and the program is quite stable and in extensive use.},
keywords={Control systems;Displays;Laboratories;Packaging;Engineering drawings;Computer bugs;Robustness;Design engineering;Robust control;Contracts},
doi={10.1109/PAC.1999.795341},
ISSN={},
month={March},}
@INPROCEEDINGS{7844379,
author={Gomes, Nuno and Belo, Orlando},
booktitle={2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
title={Modeling an opportunistic ETL agent based system using YAWL},
year={2016},
volume={},
number={},
pages={001040-001045},
abstract={ETL systems are responsible for populating data warehouses' storage structures with historical subject-oriented business data. To do that in the most appropriated manner ETL systems require special computational means, involving frequently the manipulation of large amounts of data that come usually from disparate information sources. Planning and designing ETL systems are very complex tasks, where the occurrence of errors is not rare. In some particular situations, this may jeopardize the successful implementation of the entire system. Therefore, ETL systems modeling is a very important activity in an ETL project, since it allows for sketching and validating different kinds of implementations accordingly all operational requirements defined. This will reduce significantly project costs originated by misinterpreted or badly mapped specifications. In this work we propose a different approach for modeling ETL systems, using ETL patterns as main systems components of an ETL system that will be used and tested in a standard simulation environment using the workflow language YAWL. The system is supported by a cooperative community of agents that are in charge to put in practice the tasks specified of a specific ETL pattern and, together, execute the entire set of tasks required by a particular ETL system package.},
keywords={Data warehouses;Warehousing;Analytical models;Data mining;Standards;Business intelligence;Monitoring;Multi-agent Systems;Software Agents;Data Warehousing;ETL Systems Modeling;ETL Patterns Design;YAWL},
doi={10.1109/SMC.2016.7844379},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1595719,
author={Juanjuan Jiang and Ruokonen, A. and Systa, T.},
booktitle={Third European Conference on Web Services (ECOWS'05)},
title={Pattern-based variability management in Web service development},
year={2005},
volume={},
number={},
pages={12 pp.-},
abstract={Application frameworks are widely used in software engineering to support reuse by capturing the shared architecture among a family of applications. Their role in Web service construction has, however, been mostly ignored. Reuse in general has rather been considered in the context of Web service composition than as a means to use existing implementations to build new services with related functionality. In this paper we discuss reuse in Web service development, focusing on families of Web services that share a common architecture and a set of functionalities. Techniques supporting reuse rely on identifying and managing variation points. We propose a categorization of possible variation points in service endpoints, WSDI descriptions, and business logic. A pattern-based approach for managing variation and specifying a Web service framework to an actual service application is introduced. The approach is applied to specify a sample Web service framework.},
keywords={Web services;Application software;Libraries;Software engineering;Computer architecture;Context-aware services;Software safety;Software reusability;Software systems;Service oriented architecture},
doi={10.1109/ECOWS.2005.19},
ISSN={},
month={Nov},}
@ARTICLE{4369345,
author={Moyne, James R. and Hajj, Hazem and Beatty, Kevin and Lewandowski, Russell},
journal={IEEE Transactions on Semiconductor Manufacturing},
title={SEMI E133—The Process Control System Standard: Deriving a Software Interoperability Standard for Advanced Process Control in Semiconductor Manufacturing},
year={2007},
volume={20},
number={4},
pages={408-420},
abstract={The process control systems (PCS) SEMI Standard E133 facilitates the integration of PCS into current and future fabs. The standard focuses on defining the portion of ldquointerface Brdquo associated with communication with and between components of PCSs. Specifically, the scope of the standard is the functional groups of run-to-run (R2R) control, fault detection (FD), fault classification (FC), fault prediction (FP), and statistical process control (SPC). The standard specifies PCS interfaces (which utilize ldquointerface Brdquo) that will enable these functional groups to interact effectively and share data among themselves and with the other interdependent factory systems, including systems within the process equipment. Key requirements driving the standard specification include the move to service-oriented and event-driven architectures and a need to define PCS capabilities from an input/output (I/O) perspective along functional boundaries defined by commercial applications. The developed standard addresses these requirements and specifies: 1) the capabilities of all five functional groups; 2) a comprehensive I/O interface data model; 3) an XML representation of that data model (E133.1); 4) a set of common services and behavior classes; 5) extensions of the I/O structure and behavior for each functional group; and 6) an application note that provides an example of PCS integration with equipment data acquisition (EDA) data collection. XML, R2R control, and FD implementation examples illustrate that the standard addresses these requirements, with the FD example further illustrating compatibility with EDA ldquointerface Ardquo standards. The standard to date has mature R2R control and FD specifications with key PCS users and providers indicating that they are using the standard extensively.},
keywords={Process control;Software standards;Manufacturing processes;Semiconductor device manufacture;Personal communication networks;Data models;XML;Electronic design automation and methodology;Communication standards;Communication system control;Advanced process control (APC);fault detection;process control systems;run-to-run control (R2R);service oriented architectures},
doi={10.1109/TSM.2007.907617},
ISSN={1558-2345},
month={Nov},}
@INPROCEEDINGS{5978562,
author={Khalafinejad, Saeed and Mirian-Hosseinabadi, Seyed-Hassan},
booktitle={2011 IEEE INTERNATIONAL CONFERENCE ON ELECTRO/INFORMATION TECHNOLOGY},
title={Generation of database schemas from Z specifications},
year={2011},
volume={},
number={},
pages={1-7},
abstract={Automatic translation of a high-level specification language to an executable implementation would be highly useful in maximizing the benefits of formal methods. We will introduce a set of translation functions to fill the specification-implementation gap in the domain of database applications. Because the mathematical foundation of Z has many properties in common with SQL, a direct mapping from Z to SQL structures can be found. We derive a set of translation functions from Z to SQL for the generation of a database. The proposed methodology results in reducing the expenses and duration of the software development, and also, prevents the errors originated from the manual translation of specifications to code.},
keywords={Databases;Software;Syntactics;Specification languages;Programming;Java;Libraries;Code generation;Database;Formal methods;Software development},
doi={10.1109/EIT.2011.5978562},
ISSN={2154-0373},
month={May},}
@INPROCEEDINGS{9386869,
author={Pal, Sucharita and Sinha, Dola},
booktitle={2021 Innovations in Energy Management and Renewable Resources(52042)},
title={Design and simulation of modified multi-input interleaved boost converter for renewable resources},
year={2021},
volume={},
number={},
pages={1-6},
abstract={In the present scenario due to the advancement of technology energy consumption per capita has increased a lot. But the natural resources like fossil fuel are depleting day by day. To enhance the use of renewable energy in the present situation the development of converter circuit is most essential. This paper proposed a newly modified multi-input interleaved DC-DC boost converter. This proposed topology interfaces three unidirectional input power sources in a combined structure. This converter is attractivee for alternative energy sources such as photovoltaic sources (PV), fuel cells, and batteries. This paper aims to design a high-performance multi-input boost converter. The proposed topology uses five switches that are separately controlled with five different duty ratios. Computed duty ratios are applied at different modes of operation and hence the switch realization of the new converter is obtained. The proposed topology has been realized through simulation using the MATLAB Simulink. The outputs of the simulated circuit confirm the feasibility of real-time implementation.},
keywords={Software packages;Computational modeling;Fuel cells;Topology;Batteries;Integrated circuit modeling;Voltage control;Multi-input converter;Photovoltaic Source;Interleaved;Boost converter;MATLAB software},
doi={10.1109/IEMRE52042.2021.9386869},
ISSN={},
month={Feb},}
@ARTICLE{9928567,
author={Zhu, Jinshun and Ren, Huazhong and Ye, Xin and Teng, Yuanjian and Zeng, Hui and Liu, Yu and Fan, Wenjie},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
title={PKULAST-An Extendable Model for Land Surface Temperature Retrieval From Thermal Infrared Remote Sensing Data},
year={2022},
volume={15},
number={},
pages={9278-9292},
abstract={Land surface temperature (LST) is one of important parameter in the earth energy and water budget. Recently, open-source modules or packages are emerging as all-in-one solutions to speed up the rapid development and critical application of LST from thermal infrared (TIR) remote sensing data. In this article, we developed a python package, dubbed Peking University land surface temperature (PKULAST) model to address the retrieval of LST from TIR remote sensing data with different LST retrieval algorithms, such as single channel algorithm, split window algorithm, and the temperature and emissivity separation algorithm. extendable data structures in the model covers common conceptual models used in LST algorithms implementation, such as spectral response functions, surface spectral libraries (ASTER, ECOSTRESS etc.), atmospheric profiles (NCEP, ERA5, MERRA-2, etc.), making it a convenience to construct the processing chain of developing new LST retrieval algorithms and generating LST products. The ASTER imageries are tested to assess the performance of the PKULAST to derive accurate LST. The LST result is then evaluated using long-term ground-based longwave radiation observations collected at seven sites from 2000 to 2021 under cloud-free scenes, and the result shows that the PKULAST model can obtain LST with an uncertainty of 2.7 K in bias, and 4.4 K in root-mean-square error. With the help of predefined extendable data structures and workflows for different sensors, atmospheric profile libraries and surface spectral libraries, new algorithms for retrieving LST can be developed easily. The PKULAST promises to be a tool that seamlessly interoperates with developing LST retrieval algorithms and generating LST scientific products. The code will be released at https://github.com/tirzhu/pkulast.},
keywords={Land surface temperature;Atmospheric modeling;Land surface;Temperature sensors;Sensors;Remote sensing;Earth;Land surface temperature (LST);software package;thermal infrared (TIR) remote sensing},
doi={10.1109/JSTARS.2022.3217105},
ISSN={2151-1535},
month={},}
@INPROCEEDINGS{5733750,
author={Jin Chen, Jin Chen and Qiang Wu, Qiang Wu and Jinian Bian, Jinian Bian and Hongxi Xue, Hongxi Xue},
booktitle={ASIC, 2003. Proceedings. 5th International Conference on},
title={SGA - a self-adaptable granularity approach for hardware/software co-design},
year={2003},
volume={1},
number={},
pages={365-368 Vol.1},
abstract={In hardware/software co-design for SOC, the granularity selected during hardware/software partitioning has great impact on the quality of the final implementation. Different approaches determining the partitioning granularity have been developed in recent years. However, previous all approaches have fixed clustering metrics and granularity evaluation function in design process. In this paper, a self-adaptable flexible granularity approach for software/hardware co-design is presented.},
keywords={},
doi={10.1109/ICASIC.2003.1277563},
ISSN={1523-553X},
month={Oct},}
@INPROCEEDINGS{5731123,
author={Smeed, W.G. and Subbarao, A.},
booktitle={Digital Avionics Systems Conference, 2003. DASC '03. The 22nd},
title={Commercially available, DO-178B level a certifiable, hard partitioned, posix compliant real-time operating system and TCP/UDP compliant ethernet stack software},
year={2003},
volume={2},
number={},
pages={6.A.4-6.1-9 vol.2},
abstract={With the move towards large scale avionics systems integration, the functionality demands required to meet system needs along with the market moving towards open system architectures(OSA) have exceeded the ability of avionics equipment suppliers to support these needs with proprietary solutions. Selection of a real-time operating system(RTOS)can have a dramatic impact on the ultimate capability of a product or system, from both the functionality as well as supportability aspects. In addition, selection of the system interconnect along with the RTOS will dictate the flexibility, scalability and reusability of the system architecture. However until recently, neither an RTOS with robust functionality such as those provided with a POSIX API nor one with an adaptable system interconnect such as Ethernet existed in a commercially available, DO-178B safety criticality Level A certifiable state. This paper will discuss how a POSIX complaint RTOS with hard partitioning, combined with a TCP/UDP complaint Ethernet stack, both of which are commercially available, can provide 'work station class' functionality in a DO-178B Level A environment. It will look at the functionality required, desirable attributes and capabilities, as well as how to meet the requirements and objectives of an Open Systems Architecture. Finally, the paper will take a brief look at a typical implementation.},
keywords={},
doi={10.1109/DASC.2003.1245878},
ISSN={},
month={Oct},}
@INPROCEEDINGS{4650660,
author={Cervera, Enric and Sales, Jorge and Nomdedeu, Leo and Marin, Raul and Gazi, Veysel},
booktitle={2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={Agents at play: Off-the-shelf software for practical multi-robot applications},
year={2008},
volume={},
number={},
pages={2719-2720},
abstract={A practical solution based on multi-agent protocols for the development of real-world multi-robot applications is presented. FIPA standard protocols implemented by the JADE library provide the standard functionality for a number of tasks. Robot behaviors are built upon the player middleware. Such components provide off-the-shelf tools which allow a straightforward implementation of indoor localization and navigation tasks for a team of mobile robots. Such integration combines proven mobile robot algorithms with a distributed infrastructure, and extends the capabilities from a robot alone to a whole team of robots, thus allowing the development of cooperative applications. As a proof of concept, an auction-like goal assignment task is presented: the robot team is given a goal, and each robot proposes an estimated cost for achieving it, then the best proposal is selected. Most of the control flow is automated by the standard interaction protocols. Experimental evaluation demonstrates the advantages of combining both frameworks, for a practical yet sound development of multi-robot applications.},
keywords={Robots;Robot sensing systems;Middleware;Navigation;Driver circuits;Computer architecture;Mobile robots},
doi={10.1109/IROS.2008.4650660},
ISSN={2153-0866},
month={Sep.},}
@INPROCEEDINGS{6365947,
author={Dore, C. and Murphy, M.},
booktitle={2012 18th International Conference on Virtual Systems and Multimedia},
title={Integration of Historic Building Information Modeling (HBIM) and 3D GIS for recording and managing cultural heritage sites},
year={2012},
volume={},
number={},
pages={369-376},
abstract={This paper outlines a two stage approach for digitally recording cultural heritage sites. This approach involves a 3D modeling stage and the integration of the 3D model into a 3D GIS for further management and analysis. The modeling stage is carried out using a new concept; Historic Building Information Modeling (HBIM) which has been developed at the Dublin Institute of Technology [12]. Historic Building Information Modeling is a system for modeling historic structures from laser scan and photogrammetric data using Building Information Modeling (BIM) software. The HBIM process involves a reverse engineering solution whereby parametric objects representing architectural elements are mapped onto laser scan or photogrammetric survey data. A library of parametric architectural objects has been designed from historic manuscripts and architectural pattern books. These parametric objects were built using an embedded scripting language within the BIM software called Geometric Descriptive Language (GDL). These objects are combined and mapped onto the survey data to build the entire model. After the 3D model has been created the next stage involves integrating the 3D model into a 3D GIS for further analysis. The international framework for 3D city modeling, CityGML has been adopted for this purpose. CityGML provides an interoperable framework for modeling 3D geometries, semantics, topology and appearance properties [13]. The aim of this research is to bridge the gap between parametric CAD modeling and 3D GIS while using benefits from both systems to help document and analyze cultural heritage sites.},
keywords={Solid modeling;Buildings;Semantics;Cultural differences;Data models;Libraries;Laser modes;Laser Scanning;Parametric Modeling;BIM;Semantic Modeling;3D GIS;CityGML;Cultural Heritage},
doi={10.1109/VSMM.2012.6365947},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{808615,
author={Varga, A. and Looye, G.},
booktitle={Proceedings of the 1999 IEEE International Symposium on Computer Aided Control System Design (Cat. No.99TH8404)},
title={Symbolic and numerical software tools for LFT-based low order uncertainty modeling},
year={1999},
volume={},
number={},
pages={1-6},
abstract={One of the main difficulties in applying modern control theories for designing robust controllers for linear uncertain plants is the lack of adequate models describing structured physical model uncertainties. We present a systematic approach for the generation of uncertainty models described by linear fractional transformations (LFTs) and report on recently developed symbolic and numerical software to assist the generation of low order LFT-based uncertainty models. The kernel of the symbolic software is a Maple library for generation and manipulation of LFT models. Additional numerical tools for order reduction of LFT models are based on MATLAB and FORTRAN implementations of numerically reliable algorithms. Three examples of uncertainty modeling of aircraft dynamics illustrate the capabilities of the new software to solve high order uncertainty modeling problems.},
keywords={Software tools;Uncertainty;Mathematical model;Robust control;Software libraries;Aerodynamics;Aircraft;Design methodology;State-space methods;Vectors},
doi={10.1109/CACSD.1999.808615},
ISSN={},
month={Aug},}
@ARTICLE{311059,
author={Henninger, S.},
journal={IEEE Software},
title={Using iterative refinement to find reusable software},
year={1994},
volume={11},
number={5},
pages={48-59},
abstract={Component libraries are the dominant paradigm for software reuse, but they suffer from a lack of tools that support the problem-solving process of locating relevant components. Most retrieval tools assume that retrieval is a simple matter of matching well-formed queries to a repository. But forming queries can be difficult. A designer's understanding of the problem evolves while searching for a component, and large repositories often use an esoteric vocabulary. CodeFinder is a retrieval system that combines retrieval by reformulation (which supports incremental query construction) and spreading activation (which retrieves items related to the query) to help users find information. I designed it to investigate the hypothesis that this design makes for a more effective retrieval system. My study confirmed that it was more helpful to users seeking relevant information with ill-defined tasks and vocabulary mismatches than other query systems. The study supports the hypothesis that combining techniques effectively satisfies the kind of information needs typically encountered in software design.<>},
keywords={Software reusability;Vocabulary;Software systems;Content based retrieval;Terminology;Software libraries;Problem-solving;Electronic mail;Computer science},
doi={10.1109/52.311059},
ISSN={1937-4194},
month={Sep.},}
@INPROCEEDINGS{8120710,
author={Porfiriev, A. V. and Sumin, V. I. and Dushkin, A. V.},
booktitle={2017 2nd International Ural Conference on Measurements (UralCon)},
title={Algorithm of measurement information processing for hardware and software complex capture and automatic tracking of unmanned aerial vehicle},
year={2017},
volume={},
number={},
pages={199-204},
abstract={In this paper we developed the algorithm for the automatic tracking of unmanned aircraft for image sequence based on correlation filtering to improve the efficiency of conducting objective control and simulation in Matlab and C#. The measurement results showed good convergence of the theoretical and experimental data. The advantage of this approach is the possibility to use it in real-time through the application of fast direct and inverse Fourier transform; availability of the implementation in various environments for object-oriented programming such as Visual C#/C using the library of structures and algorithms EmguCV/OpenCV and the libraries of algorithms for fast discrete Fourier transform FFTWSharp/FFTW; the possibility of the approach implementing in the systems with reconfigurable integrated circuits based on programmable logic; “Field programmable gate array” (FPGA); the use of the adaptive approach in the program code design. For further improvement of the algorithm it is appropriate to work in the area of window tracking changes due to the deformation or removal of the object to reduce the computational cost.},
keywords={Correlation;Unmanned aerial vehicles;Filtering algorithms;Gaussian distribution;Algorithm design and analysis;Military aircraft;Discrete Fourier transforms;correlation analysis;two-dimensional discrete Fourier transform;objective control;fragment;digital image processing},
doi={10.1109/URALCON.2017.8120710},
ISSN={},
month={Oct},}
@INPROCEEDINGS{1172881,
author={Rohrer, M.W. and McGregor, I.W.},
booktitle={Proceedings of the Winter Simulation Conference},
title={Simulating reality using AutoMod},
year={2002},
volume={1},
number={},
pages={173-181 vol.1},
abstract={Decision making in industry has become more complicated in recent years. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Brooks-PRI Automation has been used on thousands of projects to help engineers and managers make the best decisions possible. With the release of AutoMod 11.0 in 2002, AutoMod now supports hierarchical model construction. This new architecture allows users to reuse model objects in other models, decreasing the time required to build a model. Composite models are just one of the latest advances that make AutoMod one of the most widely used simulation software packages.},
keywords={Decision making;Costs;Raw materials;Virtual manufacturing;Testing;Manufacturing automation;Project management;Engineering management;Computer architecture;Software packages},
doi={10.1109/WSC.2002.1172881},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7939023,
author={Bhat, Anand and Samii, Soheil and Rajkumar, Ragunathan},
booktitle={2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)},
title={Practical Task Allocation for Software Fault-Tolerance and Its Implementation in Embedded Automotive Systems},
year={2017},
volume={},
number={},
pages={87-98},
abstract={Due to the advent of active safety features and automated driving capabilities, the complexity of embedded computing systems within automobiles continues to increase. Such advanced driver assistance systems (ADAS) are inherently safety-critical and must tolerate failures in any subsystem. However, fault-tolerance in safety-critical systems has been traditionally supported by hardware replication, which is prohibitively expensive in terms of cost, weight, and size for the automotive market. Recent work has studied the use of software-based fault-tolerance techniques that utilize task-level hot and cold standbys to tolerate fail-stop processor and task failures. The benefit of using standbys is maximal when a task and any of its standbys obey the placement constraint of not being co-located on the same processor. We propose a new heuristic based on a "tiered" placement constraint, and show that our heuristic produces a better task assignment that saves at least one processor up to 40% of the time relative to the best known heuristic to date. We then introduce a task allocation algorithm that, for the first time to our knowledge, leverages the run-time attributes of cold standbys. Our empirical study finds that our heuristic uses no more than one additional processor in most cases relative to an optimal allocation that we construct for evaluation purposes using a creative technique. We have designed and implemented our software fault-tolerance framework in AUTOSAR, an automotive industry standard. We use this implementation to provide an experimental evaluation of our task-level fault-tolerance features. Finally, we present an analysis of the worst-case behavior of our task recovery features.},
keywords={Software;Fault tolerance;Fault tolerant systems;Hardware;Resource management;Automotive engineering;Libraries},
doi={10.1109/RTAS.2017.33},
ISSN={},
month={April},}
@INPROCEEDINGS{4021356,
author={Dugerdil, Philippe},
booktitle={2006 22nd IEEE International Conference on Software Maintenance},
title={Reengineering Process Based on the Unified Process},
year={2006},
volume={},
number={},
pages={330-333},
abstract={This paper presents a reverse-engineering process for legacy information systems understanding and maintenance that takes its main steps from the unified process (UP). We show that the reconstruction of the use-case model of the system is central to the recovery of the architecture of the system. First, the use-cases allow us to recover the model of the business process the system supports. Second, these use-cases are analyzed to rebuild the corresponding system analysis model representing the high level architecture. The latter is used as the "hypothetical architecture" to lead the search for the corresponding software elements in the system. Third, the use-case model is used as the source of the scenarios to be run to find the software elements that implement the business functions. Then the "hypothetical architecture" can be compared to the components identified by running the scenarios. As a result, a matching can be made between the business functions of the business model and the software elements of the system. This helps the maintenance engineer to understand the purpose of these elements},
keywords={Information systems;Computer architecture;Business process re-engineering;Documentation;Maintenance;Information resources;Context awareness;Software performance;Unified modeling language;Packaging},
doi={10.1109/ICSM.2006.50},
ISSN={1063-6773},
month={Sep.},}
@INPROCEEDINGS{6062315,
author={Lin, Chi-Hung and Hsieh, Wen-Tsan and Hsieh, Hsien-Ching and Liu, Chun-Nan and Yeh, Jen-Chieh},
booktitle={2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)},
title={System-level design exploration for 3-D stacked memory architectures},
year={2011},
volume={},
number={},
pages={389-389},
abstract={Summary form only given. Traditional technology scaling of semiconductor chips followed Moore's Law. However, the transistor performance improvement will be limited, and designer will not see doubling of frequency every two years. Recently, three-dimension integrated circuits (3D IC) that employ the vertically through-silicon vias (TSVs) for connecting each of dies have been proposed. It is an alternative solution to existent Package-on-Package (PoP) and System-in-Package (SiP) processes. There are many benefits by using TSV-based 3-D integration technologies: (1) more functionality can be integrated into a small silicon space for form factor reduction, (2) circuit delay can be improved by using TSVs due to the shorter interconnect and reduced parasitic capacitance/inductance, (3) different components with incompatible manufacturing process (i.e. Logic, DRAM, Flash, etc) can be combined in single 3-D IC for heterogeneous integration. In addition, there are many 3-D multi-core or many-core architectures are discussed recently. Comparing with traditional two-dimension multi-core or many-core architectures, the major difference is memory bandwidth problem can be addressed by stacked memory architecture. Intel has good demonstration through the teraflops microprocessor chip which is an 80-core design with memory-on-logic architecture. And, each core connects to a 256KB SRAM with 12GB/s bandwidth. Although 3-D stacking technology can bring us many benefits for next generation integrated circuits, it comes with many problems and challenges in system-level design. For instance, 3-D IC designs will deal with serious challenges in design space exploration and system validation. For most designs, the number of TSV will be the most critical limitation that should be considered carefully. Besides, system design by 3-D IC will become more and more complex, and it needs a full system-level solution to face the performance, number of TSV, and power issues of 3D-IC. Furthermore, it is more challenge to provide a FPGA-based prototyping system for early-stage software development. In general, the continue increasing complexity of modern SoC or embedded system design is also extreme. To achieve the required design productivity for the time-to-market pressure, a common accepted solution is using electronic system-level (ESL) design methodology to design the system in the different design abstraction level. One of the key technologies of ESL solution is to construct the HW/SW co-simulation platform by using the virtual prototyping concept. Moreover, designers need a multiphase of virtual platform construction to meet the different targets of design stage, such as early system validation and architecture exploration. In this work, we create a simulation framework by using ESL methodology to explore 3-D IC system that consists of multi-core processors with extended stacking memory. To demonstrate our 3-D IC design techniques, the stacking memory approach is employed in our “3D-PAC (Parallel Architecture Core)” design. In 3D-PAC, we stack SRAM directly on top of the logic die which is heterogeneous multi-core computing platform for multimedia application purpose. The logic die is mainly consists of a general-purpose microprocessor, dual programmable digital signal processor (DSP) cores, AXI/AHB/APB subsystems, and various peripherals. Furthermore, we had the corresponding virtual platform (PAC Duo virtual platform) which had been used for early architecture exploration, power estimation and HW/SW co-simulation. The difference with PAC Duo design, the target of 3D-PAC is to provide a three-dimensional SoC (3-D SoC) design exporation for media-rich and multi-function portable devices. Although using the stacking memory approach with TSV technology can increase the capacity and performance of memory system significantly, but the placement and organization of the memory system are the nother important design issues. In this work, we use the ESL design methodology to consider these design issues for 3-D multi-core embedded system. According to our 3-D implementation, the number of TSV is additional important design constraint. The major issue is to find out the feasible system architect that can meet all the performance, power, area and TSV limitation design constraints. To achieve the requirement, we extend the virtual platform property to report the number of TSVs during the architecture exploration process. Besides, we also construct two kinds of virtual platform, one is using the approximately time modeling techniques to speed-up the simulation speed. The other one is using the cycle accurate modeling to obtain the precisely timing behavior. Designers can use these virtual platforms for either early HW/SW validation or architecture exploration. Therefore, designers can rapidly obtain the performance-TSV relationship with various system architectures based on the proposed virtual platforms. In our work, we analyze different system architectures, mainly the usage of the stacking memory, for our next generation design. We try to explore the maximum performance and the minimum TSVs overhead system architect. Using these ESL techniques can reduce our overall development time a lot for our 3D-PAC design.},
keywords={Through-silicon vias;Integrated circuit modeling;Stacking;Multicore processing;Random access memory;ESL;3-D IC;Design Exploration},
doi={},
ISSN={},
month={Oct},}
@ARTICLE{5518746,
author={Lambert, Emmanuel and Fiers, Martin and Nizamov, Shavkat and Tassaert, Martijn and Johnson, Steven G. and Bienstman, Peter and Bogaerts, Wim},
journal={Computing in Science & Engineering},
title={Python Bindings for the Open Source Electromagnetic Simulator Meep},
year={2011},
volume={13},
number={3},
pages={53-65},
abstract={This paper describes Meep, a popular free implementation of the finite-difference time-domain (FDTD) method for simulating electromagnetism. In particular, we focus on aspects of implementing a full-featured FDTD package that go beyond standard textbook descriptions of the algorithm, or ways in which Meep differs from typical FDTD implementations. These include pervasive interpolation and accurate modeling of subpixel features, advanced signal processing, support for nonlinear materials via Padé approximants, and flexible scripting capabilities.},
keywords={Open source software;Computer architecture;Packaging;Computational modeling;Ecosystems;Large-scale systems;Parallel processing;Photonics;Finite difference methods (FDTD);object-oriented languages;Python;SWIG;C++ interfacing;software engineering;computational science;Meep FDTD simulator;scientific computing},
doi={10.1109/MCSE.2010.98},
ISSN={1558-366X},
month={May},}
@INPROCEEDINGS{6465087,
author={Wang, Sixuan and Van Schyndel, Michael and Wainer, Gabriel and Rajus, Vinu Subashini and Woodbury, Robert},
booktitle={Proceedings of the 2012 Winter Simulation Conference (WSC)},
title={DEVS-based Building Information Modeling and simulation for emergency evacuation},
year={2012},
volume={},
number={},
pages={1-12},
abstract={Nowadays, numerous Computer-aided Design (CAD) software packages support Building Information Modeling (BIM). BIM software can benefit of advanced simulation in the pre-design phase of construction projects. In this case, we show focus on models of the emergency evacuation regarding the security and safety. We analyze the evacuation simulation of a model based on DEVS (Discrete Event systems Specification) for BIM authoring tools. The idea is to automate to extraction of building information that can be subsequently used in a simulation. Our case study uses a Cell-DEVS model of the evacuation of a multi-floor building. We also show how to obtain a 3D visualization by transforming the simulation results, facilitating the work of architects, contractors and fabricators. This kind of application could be used to analyze bottlenecks and the maximum occupation for determining an optical evacuation plan.},
keywords={Solid modeling;Buildings;Analytical models;Data models;Load modeling;Data visualization;Computer architecture},
doi={10.1109/WSC.2012.6465087},
ISSN={1558-4305},
month={Dec},}
@INPROCEEDINGS{8284913,
author={Bi, Jinqiang and Xin, Quanbo and Shang, Dongfang and Wang, Ruixi},
booktitle={2017 IEEE 2nd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)},
title={The design and implementation of river-ocean combined transportation electronic channel chart system},
year={2017},
volume={},
number={},
pages={82-86},
abstract={In the context of the national “the Belt and Road” and “Internet plus” strategies, the establishment of river-ocean combined transportation integrated transportation service system has promoted the development of inland water transportation and improved the informationalized level of waterway management. Its core and key is the construction of the electronic channel chart. Based on the advanced experience at home and abroad, this paper discussed the overall architecture, reference standard, database design and function design. It adopted service-oriented architecture (SOA) and MVC software design pattern, with HTML5 and .NET as the development languages, applied multi-source data fusion, electronic fence, locating & tracking and GIS visualization and other key technologies, developed and constructed river-ocean combined transportation electronic channel chart system. The electronic channel chart system is extensible and applicable, and it can provide technical theory and practice support for the construction of intelligent waterway, and can be widely used in the construction of domestic river-ocean combined transportation service center.},
keywords={Standards;Rivers;Transportation;Navigation;Marine vehicles;Production;Libraries;river-ocean combined transportation;electronic channel chart;AIS;GIS;intelligent waterway},
doi={10.1109/ITNEC.2017.8284913},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7372079,
author={Pietsch, Christopher and Kehrer, Timo and Kelter, Udo and Reuling, Dennis and Ohrndorf, Manuel},
booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
title={SiPL -- A Delta-Based Modeling Framework for Software Product Line Engineering},
year={2015},
volume={},
number={},
pages={852-857},
abstract={Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed among others by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling and a supporting tool suite: the abstract notion of a delta is refined to be a consistency-preserving edit script which is generated by comparing two models. The rich structure of edit scripts allows us to detect conflicts and further relations between deltas statically and to implement restructurings in delta sets such as the merging of two deltas. We illustrate the tooling using a case study.},
keywords={Unified modeling language;Visualization;Standards;Graphical user interfaces;Software product lines;Software packages;software product line engineering;model-based development;delta modeling;model differencing},
doi={10.1109/ASE.2015.106},
ISSN={},
month={Nov},}
@INPROCEEDINGS{4839508,
author={Samson, John and Grobelny, Eric},
booktitle={2009 IEEE Aerospace conference},
title={NMP ST8 dependable multiprocessor: TRL6 validation — preliminary results},
year={2009},
volume={},
number={},
pages={1-23},
abstract={Space exploration, science, and autonomy missions are requiring ever-increasing bandwidth and processing capacity to the extent that the ability to apply high-performance COTS processors for onboard computing in space is becoming a critical need. To date, dependable multiprocessor (DM) technology has been developed as part of NASA's New Millennium Program (NMP) ST8 (space technology 8) project. DM was one of four technologies selected for the ST8 flight experiment. The objective of the NMP ST8 effort is to combine high-performance, SEU-tolerant, COTS-based cluster processing and SEU-tolerant middleware in an architecture and software framework capable of supporting a wide variety of mission applications. The goal of the dependable multiprocessor project is to provide spacecraft/payload processing capability at speeds 10x - 100x of what is available today, enabling heretofore unrealizable levels of science and autonomy. While targeted primarily to allow "soft," state-of-the-art, COTS components to be flown in space, the software-enhanced SEU tolerance techniques described in this paper are applicable to systems implemented with any level of radiation-hardened or radiation-tolerant components. The effectiveness of software-enhanced SEU tolerance techniques is dependent upon the environment, the susceptibility of the components to radiation-induced SEU, SEL, and SEFI effects, the mission, and the application. One condition that obviously negates the potential benefits of software-based SEU enhancement techniques is the use of a component that exhibits catastrophic latch-up. As long as latch-ups are not catastrophic and can be mitigated by cycling power to the component with no damage or degradation of the component, software-based SEU enhancement techniques can be effective. The effectiveness of software-based SEU tolerance techniques may also be limited by the SEE rates of the components. If the SEE rates are so high that the system spends most of its time in recovery, the availability of the system to support mission processing may be low. The highest performance COTS component combined with low system availability may yield a system whose net delivered throughput and throughput density may be low. The DM project held a successful Preliminary TRL6 Technology Validation Review in September 2008. The Final TRL6 Review is currently scheduled for March 2009. This paper provides a brief high-level summary of DM technology and focuses on the TRL6 technology validation criteria and how those criteria have been met to date based on the results presented at the September Review.},
keywords={Space technology;Delta modulation;Application software;Availability;Throughput;Space exploration;Bandwidth;Space missions;Middleware;Computer architecture},
doi={10.1109/AERO.2009.4839508},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{7586712,
author={Xu Yuzhuang and Hu Kai and Huang Jiehua and Wu Kai},
booktitle={2016 IEEE/CIC International Conference on Communications in China (ICCC Workshops)},
title={An algebraic approach for verifying compositions of SDN components},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Due to heterogeneity of protocols and time-variation of topology in space information network, Software-Defined Networking (SDN) begins to be used as a novel network architecture. SDN facilitates flexible, scalable and efficient network management, through separating control plane from data plane, thus enabling huge innovation. Network can be easily built, maintained and extended by developing upper SDN applications using API supplied by SDN controller. Similar to general software, modular and componentized development method can increase the reusability of SDN application modules and therefore shorten development period. By composing components already tested or verified, complicated network control functions can be easily constructed. However, it is a problem to ensure the correctness of the network application composed. Verification is a useful means besides testing, but there is a `composition explosion' problem because of plenty of components to compose. This paper proposes an algebraic system, Verification Algebra(VA), for reducing verification transactions when verifying compositions of SDN components. Rules are defined in VA to eliminate configurations or interactions for future verification, based on the existing results of compositions. Multi-tenant network configuration verification is also supported by the algebraic approach proposed in this paper.},
keywords={Safety;System recovery;Software;Analytical models;Space information network;SDN;components combinations;combination explosion;Verification Algebra;multi-tenant},
doi={10.1109/ICCChinaW.2016.7586712},
ISSN={},
month={July},}
@INPROCEEDINGS{9604906,
author={Mortara, Johann and Collet, Philippe and Dery-Pinna, Anne-Marie},
booktitle={2021 Working Conference on Software Visualization (VISSOFT)},
title={Visualization of Object-Oriented Variability Implementations as Cities},
year={2021},
volume={},
number={},
pages={76-87},
abstract={Many large software systems are variability-rich, object-oriented, and implemented in a single code base. They then rely on multiple traditional techniques (inheritance, patterns) to realize variability, making these implementations not explicit. This directly hampers the comprehension of variability implementations, especially for newcomers in a project that need, in a short time, to understand the most important parts. In this paper, we propose VariCity, a visualization using the city metaphor to exhibit zones of interest, being zones of high density of variability implementations. The different forms of variability implementations are first detected through the usage of symmetries in code (e.g., inheritance defines a substitution symmetry between the immutable part of the superclass and the possible changes in its subclasses). VariCity then creates a 3D city representation with buildings being classes while the metrics on the number of symmetries (e.g., the number of overloaded methods, influence the building size, and their color if they are heavily loaded in symmetries). Contrary to the usual package-based organization in code-related city representations, the city streets are arranged according to the usage relationships between classes. Inheritance is simply represented with hoverable aerial links. Variability-related design patterns are depicted as buildings with specific geometric forms, while some classes specified as entry points can help in shaping the whole city organization. We also report on the evaluation of VariCity on a set of large object-oriented systems, showing that several usage scenarios helping a newcomer to spot critical variability-related zones are covered.},
keywords={Measurement;Couplings;Visualization;Codes;Three-dimensional displays;Software metrics;Urban areas;variability;software visualization;software cities},
doi={10.1109/VISSOFT52517.2021.00017},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{4253246,
author={Koch, Dirk and Haubelt, Christian and Streichert, Thilo and Teich, Jurgen},
booktitle={2007 IEEE International Symposium on Circuits and Systems},
title={Modeling and Synthesis of Hardware-Software Morphing},
year={2007},
volume={},
number={},
pages={2746-2749},
abstract={In state of the art hardware-software-co-design flows for FPGA based systems, the hardware-software partitioning problem is solved offline, thus, omitting the great flexibility provided through partial runtime reconfiguration. The decision which functions are best suitable to be implemented in hardware or software, is typically taken with respect to the expected worst case computational demands and certain objectives like power consumption, throughput or cost. However, if these parameters change at runtime, e.g., due to environmental changes, traditional designed systems lack to adapt to the new conditions, because the hardware-software partitioning is static. This paper systematically presents a new methodology that allows changing the implementation style of tasks at runtime by hardware-software morphing. Based on a formal model, how morphing can be performed without loosing internal states was demonstrated. Moreover, results from applying this methodology were demonstrated to a 16-tap FIR filter.},
keywords={Runtime;Hardware;Energy consumption;Throughput;Finite impulse response filter;Software libraries;Computational modeling;Strontium;Field programmable gate arrays;Cost function},
doi={10.1109/ISCAS.2007.378621},
ISSN={2158-1525},
month={May},}
@INPROCEEDINGS{1290148,
author={Martinez, R. and Wenji Wu and McNeill, K. and Deal, J. and Haynes, T. and Bradford, D.},
booktitle={IEEE Military Communications Conference, 2003. MILCOM 2003.},
title={Hardware and software-in-the-loop techniques using the OPNET modeling tool for JTRS developmental testing},
year={2003},
volume={1},
number={},
pages={469-474 Vol.1},
abstract={This paper describes the continuation of the hardware-in-the-loop (HITL) project started by the U.S. Army Information Systems Engineering Command (ISEC) in FY 2000. The paper describes the design and implementation of a hardware-in-the-loop (HITL) and software-in-the-loop (SITL) methods using the discrete-event modeling package, OPNET modeler. The project objective is to develop a capability to evaluate performance of large and complex army communications networks and network-centric systems by combining virtual models with real networks. HITL and SITL methods have been developed that allow virtual models to communicate real IP traffic with real applications in real networks. A generic reference model for HITL and SITL use has been proposed for use in DoD systems. This paper proposes use of the HITL and SITL techniques for developmental testing for JTRS Cluster 1. Potential DoD programs that require developmental testing may be JTRS clusters, WIN-T, and FCS. The U.S. Army Information Systems Engineering Command (ISEC), Technology Integration Center (TIC), Fort Huachuca, AZ, and BAE SYSTEMS, CNIR, Reston, VA sponsored this research in the Computer Engineering Research Laboratory (CERL) at the University of Arizona.},
keywords={Hardware;Software testing;Information systems;Systems engineering and theory;Software packages;Packaging;Communication networks;Telecommunication traffic;Traffic control;Military computing},
doi={10.1109/MILCOM.2003.1290148},
ISSN={},
month={Oct},}
@INPROCEEDINGS{933568,
author={Azimi, M. and Nasiopoulos, P. and Ward, R.K.},
booktitle={Canadian Conference on Electrical and Computer Engineering 2001. Conference Proceedings (Cat. No.01TH8555)},
title={Implementation of MPEG system target decoder},
year={2001},
volume={2},
number={},
pages={943-9946 vol.2},
abstract={The MPEG-2 system standard provides methods for multiplexing a number of elementary MPEG streams into a single system stream. It also defines methods to maintain the synchronization and timing of compressed streams. This is achieved by exact definitions of the times at which data arrive to the decoder, timing of data flow in the decoder and timing of decoding and presentation events. For this purpose, the standard defines a conceptual model for a target decoder, called "system target decoder" (STD), which is used to model the decoding process. System streams generated by the multiplexer should comply with the specifications imposed by the STD model to guarantee the normal operations of real time decoding and presentation process. Therefore, this model is necessary during the construction and verification of system streams. The multiplexer should observe the behavior of STD to ensure that decoder buffers will not overflow or underflow due to encoding or multiplexing issues when receiving the system stream. To achieve this, the scheduler that coordinates the multiplexing order of system packs should consider the monitored information from STD as one of the scheduling control parameters to follow the specifications imposed by STD. This paper describes the theoretical principles, design considerations and architecture of program and transport STDs. The implementations of these target decoders in a software package for verification of MPEG system streams is presented. This implementation uses Microsoft DirectShow. The results of decoding some sample system streams are also presented.},
keywords={Decoding;Transform coding;Timing;Multiplexing;Real time systems;Encoding;Condition monitoring;Control systems;Computer architecture;Software packages},
doi={10.1109/CCECE.2001.933568},
ISSN={0840-7789},
month={May},}
@ARTICLE{8960313,
author={Paz, Andrés and Boussaidi, Ghizlane El and Mili, Hafedh},
journal={IEEE Transactions on Software Engineering},
title={checsdm: A Method for Ensuring Consistency in Heterogeneous Safety-Critical System Design},
year={2021},
volume={47},
number={12},
pages={2713-2739},
abstract={Safety-critical systems are highly heterogeneous, combining different characteristics. Effectively designing such systems requires a complex modelling approach that deals with diverse components (e.g., mechanical, electronic, software)—each having its own underlying domain theories and vocabularies—as well as with various aspects of the same component (e.g., function, structure, behaviour). Furthermore, the regulated nature of such systems prescribes the objectives for their design verification and validation. This paper proposes checsdm, a systematic approach, based on Model-Driven Engineering (MDE), for assisting engineering teams in ensuring consistency of heterogeneous design of safety-critical systems. The approach is developed as a generic methodology and a tool framework, that can be applied to various design scenarios involving different modelling languages and different design guidelines. The methodology comprises an iterative three-phased process. The first phase, elicitation, aims at specifying requirements of the heterogeneous design scenario. Using the proposed tool framework, the second phase, codification, consists in building a particular tool set that supports the heterogeneous design scenario and helps engineers in flagging consistency errors for review and eventual correction. The third phase, operation, applies the tool set to actual system designs. Empirical evaluation of the work is presented through two executions of the checsdm approach for the specific cases of a design scenario involving a mix of UML, Simulink and Stateflow, and a design scenario involving a mix of AADL, Simulink and Stateflow. The operation phase of the first case was performed over three avionics systems and the identified inconsistencies in the design models of these systems were compared to the results of a fully manual verification carried out by professional engineers. The evaluation also includes an assessment workshop with industrial practitioners to examine their perceptions about the approach. The empirical validation indicates the feasibility and “cost-effectiveness” of the approach. Inconsistencies were identified in the three avionics systems with a greater recall rate over the manual verification. The assessment workshop shows the practitioners found the approach easy to understand and gave an overall likelihood of adoption within the context of their work.},
keywords={Unified modeling language;Software packages;Tools;Object oriented modeling;Safety;Guidelines;Design methodology;Model-driven engineering;safety-critical systems;heterogeneous design;consistency;design guidelines;DO-178C},
doi={10.1109/TSE.2020.2966994},
ISSN={1939-3520},
month={Dec},}
@INPROCEEDINGS{896544,
author={Daku, B.L.F. and Jeffrey, K.},
booktitle={30th Annual Frontiers in Education Conference. Building on A Century of Progress in Engineering Education. Conference Proceedings (IEEE Cat. No.00CH37135)},
title={An interactive computer-based tutorial for MATLAB},
year={2000},
volume={2},
number={},
pages={F2D/2-F2D/7 vol.2},
abstract={The paper describes the implementation of an interactive computer based tutorial for MATLAB. Students are engaged in learning new concepts and syntax with video, audio, and interactive exercises. The interactive exercises, which are a distinguishing feature of the tutorial, use a specially designed exercise window which has a background software interface to MATLAB. The learner is challenged with problems in the exercise window immediately after covering new concepts. Hints, example solutions, multiple choice quizzes and test problems, requiring the use of proper MATLAB structure and syntax, add to the learning experience. Student input has played an important role in the development of this tutorial. Student feedback has led to useful improvements, which were integrated into the tutorial. Student evaluation results, which are presented in the paper, indicate great promise for this approach to teaching MATLAB and, by extension, other programming languages. The paper also describes various difficulties and problems encountered in developing this computer based tutorial, which may provide some useful guidelines for others who are considering computer based instruction. Note that an Internet site, www.m-tutor.usask.ca is available, where the reader can obtain more information on the tutorial.},
keywords={Tutorial;MATLAB;Internet;Education;Feedback;Computer aided instruction;Computer languages;Testing;Guidelines;Programming},
doi={10.1109/FIE.2000.896544},
ISSN={0190-5848},
month={Oct},}
@INPROCEEDINGS{7878379,
author={Szymczak, Dan and Smith, Spencer and Carette, Jacques},
booktitle={2016 IEEE/ACM International Workshop on Software Engineering for Science (SE4Science)},
title={POSITION PAPER: A Knowledge-Based Approach to Scientific Software Development},
year={2016},
volume={},
number={},
pages={23-26},
abstract={As a relatively mature field, scientific computing has the opportunity to lead other software fields by leveraging its solid, existing knowledge base.Our position is that by following a rational design process, with the right tool support, desirable software qualities such as traceability, verifiability, and reproducibility, can be achieved for scientific software.We have begun development of a framework, Drasil, to put this into practice. Our aims are to ensure complete traceability, to facilitate agility in the face of ever changing scientific computing projects, and ensure that software artifacts can be easily and quickly extracted from Drasil. In particular, we are very interested in certifiable software and in easy re-certification.Using an example-based approach to our prototype implementation, we have already seen many benefits. Drasil keeps all software artifacts (requirements, design, code, tests, build scripts, documentation, etc.) synchronized with each other. This allows for reuse of common concepts across projects, and aids in the verification of software. It is our hope that Drasil will lead to the development of higher quality software at lower cost over the long term.},
keywords={Software;Mathematical model;Libraries;Documentation;Silicon;Programming;DSL},
doi={},
ISSN={},
month={May},}
@INPROCEEDINGS{5935286,
author={Gringinger, Eduard and Eier, Dieter and Merkl, Dieter},
booktitle={2011 Integrated Communications, Navigation, and Surveillance Conference Proceedings},
title={NextGen and SESAR moving towards ontology-based software development},
year={2011},
volume={},
number={},
pages={H3-1-H3-10},
abstract={The Federal Aviation Administration's (FAA) Next Generation Air Transportation System (NextGen) and EUROCONTROL's Single European Sky ATM Research Program (SESAR) are transforming the global Air Traffic Management (ATM) as we know it today and will break up with existing roles predicated by 50 year old technology. Similar to SESAR, NextGen right now runs the risk to specify ATM systems based on architectures already out-of-date. While current functionality is based on historical grown technical restrictions, a performance-based and most efficient approach requires new paradigms and eventually has to lead to a balanced approach to prevent over-optimizing one area at the expense of others. The new approach should be based on semantically enriched service models allowing easier development and modular applications for multiple domains. This paper describes an ontology based multi-domain software development approach called Ontology-Based-Control-Room-Framework (ONTOCOR) featuring high software code re-usage and rapid development. It focuses on improving efficiency and increasing the code reusability in order to achieve SESAR's and NextGen's claim for a performance-based and cost-efficient system. The main goal is using semantic technologies to enhance software development in ATM and further, to define tasks with enough similarity to allow applicability in different domains. ONTOCOR uses semantic standards and tools, and seamless information interchange. As described in the European Air Traffic Management Master Plan, “The Information Management Work Package (...) defines the ATM Information Reference Model and the Information Service model (...) by establishing a framework, which defines seamless information interchange between all providers and users of shared ATM information”. A specific example to implement inter domain, is the European ATM Information Reference Model (AIRM). In general, domain independent implementation of components is a future goal and EUROCONTROL defines AIRM as a model, which contains all of the ATM information to be shared in a semantic way. The paper will discuss a first case study for ontology-based modeling and development, a service for digital notices to airmen (NOTAM). Fast information distribution and retrieval are key elements together with service modeling and definitions according to Enterprise Architecture (EA) and SOA principles. Aeronautical Information Management (AIM) using ontologies, provides the benefit of fast ramp up of on-the-spot services, reduced development efforts and additional defined data as source for collaborative decision making within the System-Wide Information Management (SWIM). Such services deployed in NextGen and SESAR will optimize operations workflow, communication needs, and information sharing.},
keywords={Ontologies;Atmospheric modeling;OWL;Asynchronous transfer mode;Semantics;Software;Programming},
doi={10.1109/ICNSURV.2011.5935286},
ISSN={2155-4951},
month={May},}
@INPROCEEDINGS{8600596,
author={Cui, Hantao and Li, Fangxing},
booktitle={2018 North American Power Symposium (NAPS)},
title={ANDES: A Python-Based Cyber-Physical Power System Simulation Tool},
year={2018},
volume={},
number={},
pages={1-6},
abstract={This paper introduces the design and implementation of a Python-based software package for cyber-physical power system research called Andes. Andes is developed in an attempt to bridge the gap between the traditional power system analysis and the fast-growing needs for cybersecurity studies. First, the architecture design is proposed to accommodate for power grid prototyping, communication network set up, and the interactions between the two. Design considerations are discussed from the research and development perspective. Examples are shown using Andes for modeling, monitoring, and visualization of cyber-physical power system studies.},
keywords={Power systems;Mathematical model;Communication networks;Phasor measurement units;Computational modeling;Object oriented modeling;Tools;Cyber-physical system;computer simulation;Python language;software-defined network (SDN)},
doi={10.1109/NAPS.2018.8600596},
ISSN={},
month={Sep.},}
@ARTICLE{1695506,
author={Wartik, S.P. and Penedo, M.H.},
journal={IEEE Software},
title={Special Feature: Fillin: A Reusable Tool for Form-Oriented Software},
year={1986},
volume={3},
number={2},
pages={61-69},
abstract={The reusable tool was designed to improve man-machine interaction. Its tool-oriented environment also enforces standards better than any list of rules can.},
keywords={Software reusability;Software tools;Standards development;Productivity;Software libraries;Text processing;Databases;Software packages;User interfaces;Guidelines},
doi={10.1109/MS.1986.233075},
ISSN={1937-4194},
month={March},}
@ARTICLE{9237921,
author={Costa, Kevin Barros and Dantas Silva, Felipe S. and Schneider, Lucas M. and Neto, Emídio Paiva and Neto, Augusto Venancio and Esposito, Flavio},
journal={IEEE Access},
title={Enhancing Orchestration and Infrastructure Programmability in SDN With NOTORIETY},
year={2020},
volume={8},
number={},
pages={195487-195502},
abstract={Software-Defined Networking (SDN) controllers are nowadays expected to manage large infrastructures and services in a practical, efficient, and optimized way. To achieve such control, SDN networks should increase the automation of orchestration functions to fullfil both service and network management lifecycles. However, network management applications have stringent resource demands that orchestrators need to honor. Most of the available orchestrators focus on high-level approaches that rely exclusively on the Northbound API (NB-API). A few strategies have also been designed to optimize the controllers' internal mechanisms through features enabled by the Southbound API (SB-API). However, such solutions do not focus on optimal information delivery to the orchestrator, a crucial property to provide prompt feedback in response to network events, and to potentially drive self-healing and auto-scaling properties. To address such need, we present NOTORIETY, an orchestration system that provides an abstraction for real-time SDN network controller event message handling. NOTORIETY is able to maximize the orchestration capabilities of SDN controllers including innovative features for processing and delivering network event information. NOTORIETY's design consists of mechanisms that empower SDN-controlled entities by applying filtering rules to efficiently control and optimize data flows provided by the SDN controller. We implemented a testbed to assess our NOTORIETY proposal following the guidelines provided in the RFC 8456, and we benchmark its performance over three SDN controllers (OpenDaylight, ONOS, and Floodlight). The evaluation results reveal the effectiveness of NOTORIETY in reducing the execution time of SDN requests, load processing, and overall data marshaling volume in several scenarios.},
keywords={Optimization;Process control;Proposals;Network topology;Topology;Real-time systems;Complexity theory;Software-defined networking;SDN controller;orchestration;network programmability},
doi={10.1109/ACCESS.2020.3033486},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{5980106,
author={Blumenthal, Sebastian and Prassler, Erwin and Fischer, Jan and Nowak, Walter},
booktitle={2011 IEEE International Conference on Robotics and Automation},
title={Towards identification of best practice algorithms in 3D perception and modeling},
year={2011},
volume={},
number={},
pages={3554-3561},
abstract={Robots need a representation of their environment to reason about and to interact with it. Different 3D perception and modeling approaches exist to create such a representation, but they are not yet easily comparable. This work tries to identify best practice algorithms in the domain of 3D perception and modeling with a focus on environment reconstruction for robotic applications. The goal is to have a collection of refactored algorithms that are easily measurable and comparable. The realization follows a methodology consisting of five steps. After a survey of relevant algorithms and libraries, common representations for the core data-types Cartesian point, Cartesian point cloud and triangle mesh are identified for use in harmonized interfaces. Atomic algorithms are encapsulated into four software components: the Octree component, the Iterative Closest Point component, the k-Nearest Neighbors search component and the Delaunay triangulation component. A sample experiment demonstrates how the component structure can be used to deduce best practice.},
keywords={Three dimensional displays;Robots;Solid modeling;Libraries;Software algorithms;Benchmark testing;Best practices},
doi={10.1109/ICRA.2011.5980106},
ISSN={1050-4729},
month={May},}
@INPROCEEDINGS{741553,
author={Malekpour, M.},
booktitle={17th DASC. AIAA/IEEE/SAE. Digital Avionics Systems Conference. Proceedings (Cat. No.98CH36267)},
title={Evaluation of Honeywell recoverable computer system (RCS) in presence of electromagnetic effects},
year={1998},
volume={1},
number={},
pages={D54/1-D54/7 vol.1},
abstract={The design and development of a closed-loop system to study and evaluate the performance of the Honeywell recoverable computer system (RCS) in electromagnetic environments (EME) is presented. The development of a Windows-based software package to handle the time critical communication of data and commands between the RCS and flight simulation code in real-time, while meeting the stringent hard deadlines is also presented. The performance results of the RCS while exercising flight control laws under ideal conditions as well as in the presence of electromagnetic fields is also discussed.},
keywords={Protection;Aircraft;Aerospace electronics;Computer architecture;NASA;Electromagnetic fields;Electromagnetic radiation;Prototypes;Aerospace simulation;Aerospace control},
doi={10.1109/DASC.1998.741553},
ISSN={},
month={Oct},}
@INPROCEEDINGS{161106,
author={Hawkins and Adeli},
booktitle={IEEE 1991 International Conference on Systems Engineering},
title={Knowledge-based approach for composite floor system design},
year={1991},
volume={},
number={},
pages={174-177},
abstract={A prototype knowledge-based expert system developed for design of composite floor systems in multistory buildings is discussed. The design system is in accordance with the 1986 American Institute of Steel Construction (AISC) load and resistance factor design (LRFD) specification. Design options include design for practical minimum cost or for minimum depth. The artificial intelligence (AI) technology complements the traditional numerical processing in automating the complex problem of integrated engineering design and developing intelligent software packages that can interact with the user at a high level.<>},
keywords={Architecture;Design automation;Buildings;Knowledge based systems},
doi={10.1109/ICSYSE.1991.161106},
ISSN={},
month={Aug},}
@ARTICLE{784091,
author={Bosi, B. and Bois, G. and Savaria, Y.},
journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
title={Reconfigurable pipelined 2-D convolvers for fast digital signal processing},
year={1999},
volume={7},
number={3},
pages={299-308},
abstract={In order to make software applications simpler to write and easier to maintain, a software digital signal-processing library that performs essential signal- and image-processing functions is an important part of every digital signal processor (DSP) developer's toolset. In general, such a library provides high-level interface and mechanisms, therefore, developers only need to know how to use algorithms, not the details of how they work. Complex signal transformations then become function calls, e.g., C-callable functions. Considering the two-dimensional (2-D) convolver function as an example of great significance for DSP's, this paper proposes to replace this software function by an emulation on a field-programmable gate array (FPGA) initially configured by software programming. Therefore, the exploration of the 2-D convolver's design space will provide guidelines for the development of a library of DSP-oriented hardware configurations intended to significantly speed up the performance of general DSP processors. Based on the specific convolver, and considering operators supported in the library as hardware accelerators, a series of tradeoffs for efficiently exploiting the bandwidth between the general-purpose DSP and accelerators are proposed. In terms of implementation, this paper explores the performance and architectural tradeoffs involved in the design of an FPGA-based 2-D convolution coprocessor for the TMS320C40 DSP microprocessor available from Texas Instruments Incorporated. However, the proposed concept is not limited to a particular processor.},
keywords={Convolvers;Digital signal processing;Software libraries;Software maintenance;Software performance;Software tools;Signal processing;Field programmable gate arrays;Hardware;Application software},
doi={10.1109/92.784091},
ISSN={1557-9999},
month={Sep.},}
@INPROCEEDINGS{5937118,
author={Salami, Momoh-Jimoh E. and Tijani, I. B. and Jibia, Abdussamad U.},
booktitle={2011 4th International Conference on Mechatronics (ICOM)},
title={Development of real-time software interface for multicomponent transient signal analysis using Labview and Matlab},
year={2011},
volume={},
number={},
pages={1-5},
abstract={This paper presents the development of software interface for real-time implementation of the algorithms for multicomponent transient signal analysis. Though, a Matlab-based algorithm has been developed in the previous study on multicomponent signal analysis, real-time signal analysis is required in many practical applications of such signals. Hence, in this study, a user friendly software interface is developed using an integration of Labview and Matlab packages for real-time implementation of the multicomponent signal analysis. The developed software interface was evaluated with experimental fluorescence data collected from a spectrofluorometer system. The results obtained indicate the effectiveness of the integrated software for practical applications.},
keywords={Real time systems;Software;Transient analysis;Algorithm design and analysis;Signal analysis;Fluorescence;Software algorithms;multicomponent signals;real-time;Labview;Matlab},
doi={10.1109/ICOM.2011.5937118},
ISSN={},
month={May},}
@ARTICLE{35551,
author={Vandeweerd, I. and Croes, K. and Rijnders, L. and Six, P. and De Man, H.J.},
journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
title={REDUSA: module generation by automatic elimination of superfluous blocks in regular structures},
year={1989},
volume={8},
number={9},
pages={989-998},
abstract={An approach to module generation that does not require the construction of parametrized software procedures is presented. It is a design-by-example method that is based on the following concept: an example module (e.g., a 16-b multiplier) realizes a function between the set of input combinations and the set of output combinations. In many applications, only a subset of input combinations (e.g., 8-b instead of 16-b inputs) is used. The restriction of the module's function to this subset can be realized by a simpler module. REDUSA constructs this module automatically by a reduction of the example module. This approach offers important advantages in both the construction and verification aspected of module generators.<>},
keywords={Design methodology;Modular construction;Software libraries;Application software;Arithmetic;Software debugging;Software maintenance;Software tools;Logic design;Design automation},
doi={10.1109/43.35551},
ISSN={1937-4151},
month={Sep.},}
@ARTICLE{9769994,
author={Soares, Elvys and Ribeiro, Marcio and Gheyi, Rohit and Amaral, Guilherme and Santos, Andre Medeiros},
journal={IEEE Transactions on Software Engineering},
title={Refactoring Test Smells With JUnit 5: Why Should Developers Keep Up-to-Date},
year={2022},
volume={},
number={},
pages={1-1},
abstract={Test smells are symptoms in the test code that indicate possible design or implementation problems. Previous research demonstrated their harmfulness and the developers acknowledgment of test smells effects, prevention, and refactoring strategies. Test automation frameworks are constantly evolving, and the JUnit, one of the most used ones for Java projects, has its version 5 available since late 2017. However, we do not know the extent to which developers use the newly introduced features and whether such features indeed help refactor existing test code to remove test smells. This article conducts a mixed-method study investigation to minimize these knowledge gaps. Our study consists of three parts. First, we evaluate the usage of this framework and its features by analyzing the source code of 485 popular Java open-source projects on GitHub that use JUnit. We found that 15.9% of these projects use the JUnit 5 library. We also found that, from 17 new features detected in use, only 3 (i.e., 17.6%) are responsible for more than 70% of usages, limiting optimized propositions to test code creation and maintenance. Second, after identifying features in the JUnit 5 framework that could be considered to test smells removal and prevention, we use these features to propose novel refactorings. In particular, we present refactorings based on 7 introduced JUnit 5 features that help to remove 13 test smells, such as Assertion Roulette, Test Code Duplication, and Conditional Test Logic. Third, to evaluate our refactorings with the opinions of experienced developers, we (i) survey 212 developers for their preferences and comments about our refactorings, corroborating the benefits of our proposals and raising community feedback on JUnit 5 features, and (ii) we refactor actual test code from popular GitHub Java projects and submit 38 Pull Requests, reaching a 94% acceptance rate among respondents. As implications of our study, we alert the software testing community (i.e., practitioners and researchers) to the need to study the JUnit 5 features to effectively remove and prevent test smells. To better assist this process, we give directions on how test smells can be refactored using such features.},
keywords={Codes;Software development management;Java;Automation;Open source software;Libraries;Maintenance engineering;Test design;Testing strategies;Software/Program Verification},
doi={10.1109/TSE.2022.3172654},
ISSN={1939-3520},
month={},}
@INPROCEEDINGS{375662,
author={Badham, R. and Couchman, P. and Little, S.},
booktitle={Proceedings of the Twenty-Eighth Annual Hawaii International Conference on System Sciences},
title={A human centred approach to simulation: a case study of software to support system design and development},
year={1995},
volume={4},
number={},
pages={861-870 vol.4},
abstract={A technocentric approach to the use of simulation software in production system design uses it as an expert tool to assist system designers in a one off design. A human centred approach, in contrast uses simulation software to support teams of users to make key decisions about system design. In addition, it provides support for continued improvement and development activities by the users. In order to achieve these aims, the software must be easily usable by users, involve clear graphics to act as a prototyping tool and be sufficiently robust to work within an industrial context. This paper describes three industrial experiences in the design and implementation of team based cellular manufacturing employing standard simulation software packages (highly complex graphic software: AUTOMOD, medium to high complexity: SIMA and low complexity: SIMVIEW) and accompanying standard participatory design techniques. Each failed to achieve the intended goal of human centred system design. Reflecting on this experience, principles for software and design techniques capable of realising human centred simulation objective are offered.<>},
keywords={Humans;Software tools;Graphics;Computer industry;Manufacturing industries;Software standards;Software systems;Production systems;Software prototyping;Robustness},
doi={10.1109/HICSS.1995.375662},
ISSN={},
month={Jan},}
@ARTICLE{1267616,
author={Fox, G.},
journal={Computing in Science & Engineering},
title={Software development around a millisecond},
year={2004},
volume={6},
number={2},
pages={93-96},
abstract={The author considers software development methodologies, emphasizing those relevant to large-scale scientific computing. Many projects aim to improve the scientific computing environment, including scientific code preparation, execution, and debugging. But a scientific programming environment must address some of the key features that differentiate it from commodity or business computing for which many good tools exist. These special features include: floating-point arithmetic; performance and support for scalable parallel implementations; specific scientific data structures and utilities (such as mesh generation); and integrating simulations with distributed data. This last area is focused upon, discussing technologies needed for software integration, which is part of the overall struggle to develop self-contained modules that link together to create larger applications. Subroutines, methods, libraries, objects, components, distributed objects, and services are different ways of packaging software for greater reusability. We look at the service model for software modules.},
keywords={Scientific computing;Large-scale systems;Debugging;Programming environments;Floating-point arithmetic;Data structures;Mesh generation;Computational modeling;Application software;Algorithms},
doi={10.1109/MCISE.2004.1267616},
ISSN={1558-366X},
month={March},}
@INPROCEEDINGS{128285,
author={Pakstas, A.},
booktitle={Proceedings of the 5th Jerusalem Conference on Information Technology, 1990. 'Next Decade in Information Technology'},
title={Methods and tools for modelling the behaviour of information-transport ports in distributed software configurations},
year={1990},
volume={},
number={},
pages={191-194},
abstract={Modeling the behavior of an information transport port (ITP) on the basis of formal grammar is discussed. ITPs are objects designated for serving blocks of applicational programs in the distributed software configurations. The method can be used for both design and debugging. The modeling approach is discussed using the example of the expanded version of the semiduplex alternating bit protocol with bufferization and broadcasting exchanges. Automatic construction of the port control module is discussed.<>},
keywords={Software tools;Protocols;Automatic control;Automatic programming;Phase change materials;Broadcasting;Packaging machines;Automata;Mathematical model;Mathematics},
doi={10.1109/JCIT.1990.128285},
ISSN={},
month={Oct},}
@INPROCEEDINGS{545631,
author={Vercauteren, S. and Lin, B. and De Man, H.},
booktitle={33rd Design Automation Conference Proceedings, 1996},
title={Constructing application-specific heterogeneous embedded architectures from custom HW/SW applications},
year={1996},
volume={},
number={},
pages={521-526},
abstract={Deep sub-micron processing technologies have enabled the implementation of new application-specific embedded architectures that integrate multiple software programmable processors (e.g. DSPs, microcontrollers) and dedicated hardware components together onto a single cost-efficient IC. These application-specific architectures are emerging as a key design solution to today's microelectronics design problems, which are being driven by emerging applications in the areas of wireless communication, broadband networking, and multimedia computing. However the construction of these customized heterogeneous multiprocessor architectures, while ensuring that the hardware and software parts communicate correctly, is a tremendously difficult and highly error proned task with little or no tool support. In this paper, we present a solution to this embedded architecture co-synthesis problem based on an orchestrated combination of architectural strategies, parameterized libraries, and software tool support.},
keywords={Computer architecture;Hardware;Software tools;Embedded software;Digital signal processing;Microcontrollers;Application specific integrated circuits;Microelectronics;Application software;Wireless communication},
doi={10.1109/DAC.1996.545631},
ISSN={0738-100X},
month={June},}
@ARTICLE{965343,
author={Iyer, S. and Ramesh, S.},
journal={IEEE Transactions on Software Engineering},
title={Apportioning: a technique for efficient reachability analysis of concurrent object-oriented programs},
year={2001},
volume={27},
number={11},
pages={1037-1056},
abstract={The object-oriented paradigm in software engineering provides support for the construction of modular and reusable program components and is attractive for the design of large and complex distributed systems. Reachability analysis is an important and well-known tool for static analysis of critical properties in concurrent programs, such as deadlock freedom. It involves the systematic enumeration of all possible global states of program execution and provides the same level of assurance for properties of the synchronization structure in concurrent programs, such as formal verification. However, direct application of traditional reachability analysis to concurrent object-oriented programs has many problems, such as incomplete analysis for reusable classes (not safe) and increased computational complexity (not efficient). We propose a novel technique called apportioning, for safe and efficient reachability analysis of concurrent object-oriented programs, that is based upon a simple but powerful idea of classification of program analysis points as local (having influence within a class) and global (having possible influence outside a class). We have developed a number of apportioning-based algorithms, having different degrees of safety and efficiency. We present the details of one of these algorithms, formally show its safety for an appropriate class of programs, and present experimental results to demonstrate its efficiency for various examples.},
keywords={Reachability analysis;Modular construction;System recovery;Formal verification;Safety;Libraries;Software engineering;Computational complexity;Algorithm design and analysis;Object oriented programming},
doi={10.1109/32.965343},
ISSN={1939-3520},
month={Nov},}
@INPROCEEDINGS{528768,
author={Juiz, C. and Puigjaner, R.},
booktitle={Proceedings Second International Workshop on Real-Time Computing Systems and Applications},
title={Improved performance model of a real-time software element: the producer-consumer},
year={1995},
volume={},
number={},
pages={174-178},
abstract={Design of real-time systems needs to take into account their performance behaviour before their implementation. This goal can be attained by means of queueing network modelling. To process this kind of models normally simulation is used with its inherent costs of debugging and running times. This paper presents the analysis and comparison of several kinds of performance models of a basic software element used in the construction of real-time systems: the producer-consumer. These models go from the simple FIFO queue to the simulation of the complete element passing through some decomposition-aggregation approximate model. This model can be considered as an element of a library and the model of a real-time system can be built by putting together the models of the real-time components.},
keywords={Software performance;Real time systems;Analytical models;Costs;Debugging;Performance analysis;Throughput;Computational modeling;Software libraries;Delay},
doi={10.1109/RTCSA.1995.528768},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5718287,
author={Han, Tonglai and Li, Rong},
booktitle={2010 Second World Congress on Software Engineering},
title={Transonic Aeroelastic Software and Its Engineering Application},
year={2010},
volume={1},
number={},
pages={163-171},
abstract={Due to the highly nonlinear characteristics of transonic flows, transonic aero elasticity cannot be predicted by the traditional linear flow solver coupled with the linear structure equation. Therefore, in the paper, a software package for the transonic aero elasticity with direct CFD/CSD coupling solution was developed. In this software, the flow fields are calculated in parallel by the unsteady Reynolds averaged Navier-Stokes equations. The structural deformations for flutter and static aeroelastcity are calculated with the generalized equation of structural motion and the structural static balance equation, respectively. Data transformations between the fluid and structure are interpolated by the radial basis function (RBF) and the principle of virtual work. Dynamic grid deformations are realized with the spring network method based on tetrahedral background grid. In order to improve computational efficiency of direct CFD/CSD coupling calculation, the reduced order models (ROM) of autoregressive moving average model (ARMA) is investigated and embeded in the software. In addition, static aeroelastic optimization based on the response surface model (RSM) is also studied and embeded in. Finally, some examples are presented for the validation of the implementation of the developed software.},
keywords={Computational fluid dynamics;Aerodynamics;Mathematical model;Software;Autoregressive processes;Computational modeling;CFD/CSD;flutter;static aeroelasticity;software;reduced order models (ROM);response surface model (RSM)},
doi={10.1109/WCSE.2010.154},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9959430,
author={Peng, Zhenshan and Lin, Yuan and Fang, Shuo and Liu, Haiming},
booktitle={2022 International Conference on Advanced Robotics and Mechatronics (ICARM)},
title={Design of Simulation Software for Multi-agent Cooperative Control Algorithm Based on Qt and OSG},
year={2022},
volume={},
number={},
pages={193-199},
abstract={This paper presents a method of designing and developing a practical and convenient 3D visual simulation software for the multi-agent system in simulating the cooperative control algorithms, in order to realize the visual simulation of the cooperative control algorithms for multi-agent systems. The design of this simulation software is divided into three parts: design of human-computer interaction, design of algorithm program, and design of 3D visual presentation. The software system is developed on the Qt development platform for design of graphical user interface and implementation of algorithm program, using OSG graphics library as the 3D rendering tool. Multiple examples of different cooperative control algorithms of typical multi-agent systems are executed on the software to verify feasibility and effectiveness, and some of the simulation results are presented in the paper. Practical application shows that the simulation software is helpful to the development of simulation research on the cooperative control algorithms for multi-agent systems.},
keywords={Human computer interaction;Solid modeling;Visualization;Three-dimensional displays;Software algorithms;Data visualization;Process control},
doi={10.1109/ICARM54641.2022.9959430},
ISSN={},
month={July},}
@INPROCEEDINGS{8401627,
author={Fraysse, Guillaume and Ben Yahia, Imen Grida and Lejeune, Jonathan and Sens, Pierre and Sopena, Julien},
booktitle={2018 21st Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)},
title={Towards multi-SDN services: Dangers of concurrent resource allocation from multiple providers},
year={2018},
volume={},
number={},
pages={1-5},
abstract={One of the Software-Defined Networking (SDN) promises is the programmability of networks through Application Programming Interfaces (APIs). Those APIs allow different users to access the network concurrently. Thus leading to the allocation of dedicated critical resources by a given Service Provider. In this paper we formalize the problem of deadlocks, and present a case where they occur.},
keywords={System recovery;Resource management;Control systems;Cloud computing;Computer architecture;Open source software;Protocols;SDN;Distributed Systems;multi-resource allocation problem;multi-SDN;drinking philosophers;deadlock},
doi={10.1109/ICIN.2018.8401627},
ISSN={2472-8144},
month={Feb},}
@ARTICLE{7782698,
author={Tomb, Aaron},
journal={IEEE Security & Privacy},
title={Automated Verification of Real-World Cryptographic Implementations},
year={2016},
volume={14},
number={6},
pages={26-33},
abstract={Cryptographic software is increasingly important but notoriously difficult to implement correctly. Emerging specification approaches and tools make it possible to automatically and rigorously prove equivalence between machine-readable cryptographic specifications and real-world implementations. The Cryptol and the Software Analysis Workbench tools have successfully proven the correctness of routines from widely used cryptographic libraries.},
keywords={Elliptic curve cryptography;Mathematical model;Standards;Software;Cognition;Algorithm design and analysis;cryptography;software engineering;program verification;correctness proofs;security;privacy},
doi={10.1109/MSP.2016.125},
ISSN={1558-4046},
month={Nov},}
@INPROCEEDINGS{7348076,
author={Sewell, Christopher and Lo, Li-ta and Heitmann, Katrin and Habib, Salman and Ahrens, James},
booktitle={2015 IEEE 5th Symposium on Large Data Analysis and Visualization (LDAV)},
title={Utilizing many-core accelerators for halo and center finding within a cosmology simulation},
year={2015},
volume={},
number={},
pages={91-98},
abstract={Efficiently finding and computing statistics about “halos” (regions of high density) are essential analysis steps for N-body cosmology simulations. However, in state-of-the-art simulation codes, these analysis operators do not currently take advantage of the shared-memory data-parallelism available on multi-core and many-core architectures. The Hybrid / Hardware Accelerated Cosmology Code (HACC) is designed as an MPI+X code, but the analysis operators are parallelized only among MPI ranks, because of the difficulty in porting different X implementations (e.g., OpenMP, CUDA) across all architectures on which it is run. In this paper, we present portable data-parallel algorithms for several variations of halo finding and halo center finding algorithms. These are implemented with the PISTON component of the VTK-m framework, which uses Nvidia's Thrust library to construct data-parallel algorithms that allow a single implementation to be compiled to multiple backends to target a variety of multi-core and many-core architectures. Finally, we compare the performance of our halo and center finding algorithms against the original HACC implementations on the Moonlight, Stampede, and Titan supercomputers. The portability of Thrust allowed the same code to run efficiently on each of these architectures. On Titan, the performance improvements using our code have enabled halo analysis to be performed on a very large data set (81923 particles across 16,384 nodes of Titan) for which analysis using only the existing CPU algorithms was not feasible.},
keywords={Algorithm design and analysis;Computer architecture;Computational modeling;Graphics processing units;Analytical models;Acceleration;Pistons;D.1.3 [Software]: Programming Techniques — [Concurrent Prgm.]},
doi={10.1109/LDAV.2015.7348076},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5463577,
author={Zalewski, Janusz},
booktitle={2010 23rd IEEE Conference on Software Engineering Education and Training},
title={A Comprehensive Embedded Systems Lab for Teaching Web-Based Remote Software Development},
year={2010},
volume={},
number={},
pages={113-120},
abstract={The paper discusses an approach to the implementation of a web-based lab to teach remote software development in courses focused on real-time embedded systems. The lab involves data acquisition and control access to remote devices as well as a capability of remote software development, uploading and debugging. Several different platforms are used focusing on teaching specific aspects of real-time embedded systems development, which includes: programming in VHDL at the chip design level, microcontroller programming at the board level, programming at the real-time kernel level with both Windows and Unix flavor, and using a broad spectrum of programming languages (C/C++, C#, Java, ASP.NET). Current lab status is presented and plans for future expansion are outlined.},
keywords={Embedded system;Education;Programming;Real time systems;Data acquisition;Debugging;Chip scale packaging;Microcontrollers;Kernel;Computer languages},
doi={10.1109/CSEET.2010.22},
ISSN={2377-570X},
month={March},}
@INPROCEEDINGS{926312,
author={Koller, J. and List, T. and Jarke, M.},
booktitle={Proceedings of the 34th Annual Hawaii International Conference on System Sciences},
title={Designing a component store for chemical engineering software solutions},
year={2001},
volume={},
number={},
pages={9 pp.-},
abstract={In the Global CAPE (Computer-Aided Process Engineering) Open Project of the G7 Intelligent Manufacturing Systems (IMS) programme, the worldwide chemical industries have defined an open standard for chemical engineering simulation software. This opens up a market for simulation components representing the structure and behavior of specialized processing units for small enterprises and research institutions. This paper describes the requirements and design of a Web-based infrastructure, called the CAPE-Open Laboratory Network, through which such components can be certified and then integrated into simulation studies which can be bought or rented as electronic application services. A kernel of this infrastructure is being implemented.},
keywords={Chemical engineering;Computational modeling;Computer aided manufacturing;Intelligent manufacturing systems;Chemical industry;Software standards;Computer simulation;Laboratories;Consumer electronics;Kernel},
doi={10.1109/HICSS.2001.926312},
ISSN={},
month={Jan},}
@INBOOK{9770879,
author={Paret, Dominique and Rebaine, Hassina and Engel, B. A.},
booktitle={Autonomous and Connected Vehicles: Network Architectures from Legacy Networks to Automotive Ethernet},
title={Simulations, Applications, and Software Architectures for Automobiles},
year={2022},
volume={},
number={},
pages={317-398},
abstract={This chapter describes the software environment and the hardware (tools) that need to be put in place for simulating and developing autonomous and connected vehicles, to ensure they work properly, are safe, comply with the standards, certifications, approvals. Autonomous driving systems can only be rolled out to the public once developers and automakers have demonstrated that they are capable of delivering extremely high levels of safety. The electronic architecture has shifted from “point‐to‐point” communicating systems to (mainly) “multiplexed networks,” with distributed functions. The simulation platform can be used to safety‐test complex autonomous driving systems integrating physics, electronics, onboard systems and software simulation. The chapter provides the examples from the catalog of the company VECTOR, which offers numerous software packages and basic services for the implementation of new generations of applications.},
keywords={Software;Autonomous vehicles;Object oriented modeling;Mathematical models;Ethernet;Sensors;Safety},
doi={10.1002/9781119816140.ch6},
ISSN={},
publisher={Wiley},
isbn={9781119816317},
url={https://ieeexplore.ieee.org/document/9770879},}
@INPROCEEDINGS{183320,
author={Zarmer, C.L. and Nardi, B.A. and Johnson, J. and Miller, J.R.},
booktitle={Proceedings of the Twenty-Fifth Hawaii International Conference on System Sciences},
title={ACE: Zen and the art of application building},
year={1992},
volume={ii},
number={},
pages={687-698 vol.2},
abstract={Task-specific application development environments enable end users to create their own applications. This is advantageous in two ways: users can draw on their own rich task knowledge to create the applications they really want, and reliance on the scarce, expensive expertise of professional programmers is greatly reduced. Extensible systems such as spreadsheets and statistical packages provide a good model for application construction as they allow end users to create complete applications. Such environments eliminate the need for separate user interface builders; the interface is seamlessly created as the application is developed. In this 'Zen' process, there is little difference between application development and user interface development. Further barriers are broken down by creating application development components that can continually be edited and refined, so that distinctions among 'editing', 'building', 'application construction', and 'finished application' begin to disappear. The authors describe ACE, an architecture for building task-specific applications, and the software libraries they have developed to implement this architecture. They show how ACE supports the building of task-specific applications via a range of extension mechanisms from interactive editing by end users to programmer-defined subclassing.<>},
keywords={Subspace constraints;Packaging;Application software;Buildings;Milling machines;Programming profession;User interfaces;Software libraries;Workstations;Predictive models},
doi={10.1109/HICSS.1992.183320},
ISSN={},
month={Jan},}
@INPROCEEDINGS{4839645,
author={Murray, Alexander and Schoppers, Marcel and Scandore, Steve},
booktitle={2009 IEEE Aerospace conference},
title={A reusable architectural pattern for auto-generated payload management flight software},
year={2009},
volume={},
number={},
pages={1-11},
abstract={Mars Science Laboratory (MSL), NASA's next mission to Mars, will deploy a large rover carrying a battery of eleven science instruments, representing a wide variety of payload types. The rover's flight software (FSW) has the task of monitoring, commanding, collecting, managing and in some cases calibrating data from these instruments. Though the instruments represent a large variety of requirements, complexity, data volumes, fault protection, and commanding logic, the FSW is designed to exploit the commonality among the instruments' requirements in order to maximize reuse of software and to minimize design, implementation and testing effort. To achieve this, we developed an architectural pattern in which all of the common features and patterns of behavior required to manage an instrument are supported, and clear adaptation points are identified and provided to allow expression of the unique behaviors needed for each instrument. For each instrument there is a FSW module called an instrument manager (IM), and each of these is an instance of the common pattern. The common IM architecture is expressed in the design as a FSW module called the instrument manager framework (IMF), along with a supporting library for handling instrument communications, the instrument manager library (IML). The IMF module includes a code generator that reads specifications of the ground command set for an instrument, their associated behaviors, and other internal behaviors (e.g. fault response behaviors), expressed in spreadsheets, and produces a set of source code files containing implementations of these commands and behaviors, and their supporting types and variables. The IML module also includes a code generator which transforms a spreadsheet specifying the set of commands that the instrument accepts into C code that parameterizes communications with the instruments. We first describe the instrument management requirements on the rover FSW, and then continue with an exposition of the IM architectural pattern. We conclude with some statistics on the efficiencies gained in the application of this pattern.},
keywords={Payloads;Software reusability;Instruments;Mars;Logic testing;Libraries;Laboratories;Batteries;Monitoring;Protection},
doi={10.1109/AERO.2009.4839645},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{8425198,
author={Jünger, Daniel and Hundt, Christian and Schmidt, Bertil},
booktitle={2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
title={WarpDrive: Massively Parallel Hashing on Multi-GPU Nodes},
year={2018},
volume={},
number={},
pages={441-450},
abstract={Hash maps are among the most versatile data structures in computer science because of their compact data layout and expected constant time complexity for insertion and querying. However, associated memory access patterns during the probing phase are highly irregular resulting in strongly memory-bound implementations. Massively parallel accelerators such as CUDA-enabled GPUs may overcome this limitation by virtue of their fast video memory featuring almost one TB/s bandwidth in comparison to main memory modules of state-of-the-art CPUs with less than 100 GB/s. Unfortunately, the size of hash maps supported by existing single-GPU hashing implementations is restricted by the limited amount of available video RAM. Hence, hash map construction and querying that scales across multiple GPUs is urgently needed in order to support structured storage of bigger datasets at high speeds. In this paper, we introduce WarpDrive - a scalable, distributed single-node multi-GPU implementation for the construction and querying of billions of key-value pairs. We propose a novel subwarp-based probing scheme featuring coalesced memory access over consecutive memory regions in order to mitigate the high latency of irregular access patterns. Our implementation achieves 1.4 billion insertions per second in single-GPU mode for a load factor of 0.95 thereby outperforming the GPU-cuckoo implementation of the CUDPP library by a factor of 2.8 on a P100. Furthermore, we present transparent scaling to multiple GPUs within the same node with up to 4.3 billion operations per second for high load factors on four P100 GPUs connected by NVLink technology. WarpDrive is free software and can be downloaded at https://github.com/sleeepyjack/warpdrive.},
keywords={Graphics processing units;Computer science;Memory management;Data structures;Electronic mail;Layout;Bandwidth;hash table;distributed;GPU;CUDA;NVLINK},
doi={10.1109/IPDPS.2018.00054},
ISSN={1530-2075},
month={May},}
@ARTICLE{9610025,
author={He, Ying and Khan, Habib Ullah and Zhang, Kai and Wang, Wei and Choi, Bong Jun and Aly, Ayman A. and Felemban, Bassem F. and Sani, Nor Samsiah and Tarbosh, Qazwan A. and Aydoğdu, Ömer},
journal={IEEE Access},
title={D2D-V2X-SDN: Taxonomy and Architecture Towards 5G Mobile Communication System},
year={2021},
volume={9},
number={},
pages={155507-155525},
abstract={In the era of information society and 5G networks, cars are extremely important mobile information carriers. In order to meet the needs of multi-scenario business requirements such as vehicle assisted driving and in-vehicle entertainment, cars need to interact with the outside world. This interconnection and data transmission process is usually called vehicular communication (V2X, Vehicle-to-Everything). Device-to-device (D2D) communication not only has partial nature of communication, but also alleviate the current problem of spectrum scarcity of resources. The application of D2D communication in V2X can meet the requirements of high reliability and low latency, but resource reuse also brings interference. Software-defined networking (SDN) provides an optimal solution for interoperability and flexibility between the V2X and D2D communication. This paper reviews the integration of D2D and V2X communication from the perspective of SDN. The state-of-the-art and architectures of D2D-V2X were discussed. The similarity, characteristics, routing control, location management, patch scheduling and recovery is described. The integrated architecture reviewed in this paper can solve the problems of routing management, interference management and mobile management. It also overcome the disconnection problem between the D2D-V2X in terms of SDN and provides some effective solutions.},
keywords={Device-to-device communication;Vehicle-to-everything;5G mobile communication;Computer architecture;Wireless communication;Vehicular ad hoc networks;Routing;5G wireless networks;D2D communication;V2X;SDN;interference management},
doi={10.1109/ACCESS.2021.3127041},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7939151,
author={Rafique, Yousef and Awad, Mohamad Khattar and Neama, Ghadeer},
booktitle={2017 Fourth International Conference on Software Defined Systems (SDS)},
title={A benchmark implementation for evaluating the performance of power-aware routing algorithms in practical Software-Defined Networks},
year={2017},
volume={},
number={},
pages={118-124},
abstract={The increase in demand for high network bandwidth has significantly increased the network power consumption and hence, capital expenditure and operational expenditure costs. Service providers are investigating various approaches to reduce operational and management costs, while delivering richer services across their networks. Recently, several centralized power-aware routing heuristic algorithms have been proposed leveraging the centralized control of the Software Defined Networking (SDN) architecture. However, a base solution for benchmarking the performance of these algorithms has not been developed yet. In this paper we propose an implementation of the centralized power-aware routing problem for SDN in GAMS. This implementation facilitates solving the problem using commercial packages and hence serves as a benchmark for accessing the performance of centralized power-aware routing algorithms. Experimental results demonstrate the efficiency of the developed implementation.},
keywords={MATLAB;Mathematical model;Iron;Routing;Network topology;Radio frequency},
doi={10.1109/SDS.2017.7939151},
ISSN={},
month={May},}
@INPROCEEDINGS{4384081,
author={Schliesser, Sagi},
booktitle={IEEE International Conference on Software-Science, Technology & Engineering (SwSTE'07)},
title={An Approach to ERP Testing Using Services},
year={2007},
volume={},
number={},
pages={14-21},
abstract={Enterprise packages software solutions are becoming an increasingly popular choice for organizations aiming to streamline their business processes. However, as the software complexity reaches new heights, package vendors are faced with critical concerns regarding quality assurance. Traditional testing methods are not designed to meet all the challenges posed by ERP implementation. This paper will try to suggest a novel approach to dealing with the complexity aspects of testing. We will try and demonstrate how the technical advancements introduced by SOA enable the adoption of such approach by many package vendors.},
keywords={Enterprise resource planning;Packaging;Software testing;Software packages;Automatic testing;Application software;Floors;Software standards;Business;Software quality;Software Testing;ERP testing;Software quality;Automated Testing.},
doi={10.1109/SwSTE.2007.17},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9195257,
author={Percelay, Mathieu and Bonnot, Justine and Arrestier, Florian and Menard, Daniel},
booktitle={2020 IEEE Workshop on Signal Processing Systems (SiPS)},
title={Polynomial Approximation with Non-Uniform Segmentation for Bivariate Functions},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Embedded applications use more and more sophisticated computations. These computations can integrate the composition of elementary functions which have to be approximated. In the context of scientific computation, mathematical libraries are available but are inefficient for real-time embedded applications. In this context, the efficient evaluation of a complex function requires to design a specific source code or HW block dedicated to this function and the considered input range. Different solutions have been proposed for univariate functions. These techniques are based on iterative methods, look-up tables, bi-/multi-partite table methods or polynomial approximation. Significantly fewer methods are available for multivariate functions. In this paper, we propose a polynomial approximation for software implementation of bivariate-functions. A smart nonuniform segmentation is proposed to better fit irregularities of the approximated function. Furthermore, for best performances, the degree of the bivariate approximating polynomial is dynamic which avoids data and time waste. The experiment results underline the ability of the proposed approach to explore the trade-off between the memory footprint and execution time of the function evaluation.},
keywords={Approximation error;Approximation algorithms;Software;Linear approximation;Real-time systems;Mean square error methods;Minimization methods;Approximate computing;Mathematical function approximation;Minimum Mean Square Error Method;Non-uniform Segmentation;Software implementation},
doi={10.1109/SiPS50750.2020.9195257},
ISSN={2374-7390},
month={Oct},}
@ARTICLE{761039,
author={Turletti, T. and Bentzen, H.J. and Tennenhouse, D.},
journal={IEEE Journal on Selected Areas in Communications},
title={Toward the software realization of a GSM base station},
year={1999},
volume={17},
number={4},
pages={603-612},
abstract={Advances in processor and analog-to-digital conversion technology have made the software approach an increasingly attractive alternative for implementing radio-based systems. For mobile telephony base stations, the advantages with the new architecture are obvious: great cost savings by using one transceiver per base transceiver station (BTS) instead of one per channel, tremendous flexibility by moving system-specific parameters to the digital part, and allowing the support of a wide range of modulation and coding schemes. This paper considers the software implementation of a GSM BTS, and analyzes the performance of each of its radio interface modules. The performance of each software module is evaluated using both a % CPU metric and a processor-independent metric based on SPEC benchmarks. The results can be used to dimension systems, e,g., to estimate the number of software-based GSM channels that can be supported by a given processor configuration, and to predict the impact of future processor enhancements on BTS capacity. Two novel aspects of this work are the portability of the software modules and the platform-independent evaluation of their computational requirements.},
keywords={GSM;Base stations;Transceivers;Software performance;Analog-digital conversion;Telephony;Computer architecture;Costs;Digital modulation;Modulation coding},
doi={10.1109/49.761039},
ISSN={1558-0008},
month={April},}
@ARTICLE{296272,
author={Gupta, S. and Wipf, T.J. and Fanous, F. and Baenziger, M. and Hahm, Y.H.},
journal={IEEE Transactions on Power Delivery},
title={Structural failure analysis of 345 kV transmission line},
year={1994},
volume={9},
number={2},
pages={894-903},
abstract={An ice storm in Central Iowa on March 7, 1990, caused the domino failure of 69 H frame steel pole structures that were part of a 345 kV transmission line covering a distance of approximately 30.6 km (19 miles). This paper briefly documents the damage, and presents the nonlinear structural analyses performed on a portion of the above transmission line using the ETADS software, which is a structural finite element submodule of TL Workstation software. The analysis included a characterization of the likely storm loads and their application to the transmission systems. The methodology used in trying to simulate the sequence of events leading to the failure is also presented.<>},
keywords={Failure analysis;Transmission lines;Storms;Software performance;Ice;Steel;Performance analysis;Finite element methods;Workstations;Application software},
doi={10.1109/61.296272},
ISSN={1937-4208},
month={April},}
@INPROCEEDINGS{1334899,
author={Boehm, B. and Bhuta, J. and Garlan, D. and Gradman, E. and LiGuo Huang and Lam, A. and Madachy, R. and Medvidovic, N. and Meyer, K. and Meyers, S. and Perez, G. and Reinholtz, K. and Roshandel, R. and Rouquette, N.},
booktitle={Proceedings. 2004 International Symposium on Empirical Software Engineering, 2004. ISESE '04.},
title={Using empirical testbeds to accelerate technology maturity and transition: the SCRover experience},
year={2004},
volume={},
number={},
pages={117-126},
abstract={This paper is an experience report on a first attempt to develop and apply a new form of software: a full-service empirical testbed designed to evaluate alternative software dependability technologies, and to accelerate their maturation and transition into project use. The SCRover testbed includes not only the specifications, code, and hardware of a public safety robot, but also the package of instrumentation, scenario drivers, seeded defects, experimentation guidelines, and comparative effort and defect data needed to facilitate technology evaluation experiments. The SCRover testbed's initial operational capability has been recently applied to empirically evaluate two architecture definition languages (ADLs) and toolsets, Mae and AcmeStudio. The testbed evaluation showed (1) that the ADL-based toolsets were complementary and cost-effective to apply to mission-critical systems; (2) that the testbed was cost-effective to use by researchers; and (3) that collaboration in testbed use by researchers and the Jet Propulsion Laboratory (JPL) project users resulted in actions to accelerate technology maturity and transition into project use. The evaluation also identified a number of lessons learned for improving the SCRover testbed, and for development and application of future technology evaluation testbeds.},
keywords={Life estimation;System testing;Software testing;Hardware;Safety;Robots;Packaging;Instruments;Guidelines;Mission critical systems},
doi={10.1109/ISESE.2004.1334899},
ISSN={},
month={Aug},}
@ARTICLE{238586,
author={Di Felice, P.},
journal={IEEE Transactions on Software Engineering},
title={Reusability of mathematical software: a contribution},
year={1993},
volume={19},
number={8},
pages={835-843},
abstract={Mathematical software is devoted to solving problems involving matrix computation and manipulation. The main problem limiting the reusability of existing mathematical software is that programs are often not initially designed for being reused. Therefore, it is hard to find programs that can be easily reused. A programming methodology useful for designing and implementing reusable code is presented. A portion of code designed and implemented for being reused is called a unit. The units are self-contained software components featuring a high degree of information hiding. This way of organizing software facilitates the reuse process and improves the understandability of units. To speed up the implementation process, a system supporting the reusability of units from an existing software library is particularly useful. The functionality of the EasyCard system, which creates, maintains, and queries a catalog of units is discussed.<>},
keywords={Software reusability;Programming profession;Software tools;Sparse matrices;Application software;Design methodology;Software libraries;Object oriented programming;Standardization;Organizing},
doi={10.1109/32.238586},
ISSN={1939-3520},
month={Aug},}
@INPROCEEDINGS{7603717,
author={Bychkov, Igor and Cherkashin, Evgeny and Davydov, Artem and Larionov, Alexander},
booktitle={2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)},
title={Software for automated theorem proving based on the calculus of positively constructed formulas},
year={2016},
volume={},
number={},
pages={940-945},
abstract={A description of the calculus of positively constructed formulas (PCF) and prover based on this calculus is considered. The PCF calculus has been developed by Russian scientists S.N. Vassiliev and A.K. Zherlov by an evolutionary way in describing and solving control theory problems. This calculus has features, which are applicable in control theory. The described implementation of the prover uses several techniques and strategies to improve prover performance. The prover is being tested by means of solving problems from TPTP library. The usage of implemented inference search strategies is also commented in this paper.},
keywords={Calculus;Joining processes;Control theory;Data structures;Libraries;Electronic mail;Planning},
doi={10.1109/ICIEA.2016.7603717},
ISSN={2158-2297},
month={June},}
@INPROCEEDINGS{6270768,
author={Raikar, S. Pai and Subramoni, H. and Kandalla, K. and Vienne, J. and Panda, Dhabaleswar K.},
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum},
title={Designing Network Failover and Recovery in MPI for Multi-Rail InfiniBand Clusters},
year={2012},
volume={},
number={},
pages={1160-1167},
abstract={The emerging trends of designing commodity-based supercomputing systems have a severe detrimental impact on the Mean-Time-Between-Failures (MTBF). The MTBF for typical HEC installations is currently estimated to be between eight hours and fifteen days [1]. Failures in the interconnect fabric account for a fair share of the total failures occurring in such systems. This will continue to degrade as system sizes become larger. Thus, it is highly desirable that next generation system architectures and software environments provide sophisticated network level fault-tolerance and fault-resilient solutions. In the past few years, the number of cores on processors has increased dramatically. To make efficient use of these machines it is necessary to provide the required bandwidth to all the cores. To keep up with the multi-core trend, current generation supercomputers and clusters are designed with multiple network cards (rails) to provide enhanced data transfer capabilities. Besides providing enhanced performance, such multi-rail networks can also be leveraged to provide network level fault resilience. This paper presents a design for a failover mechanism in a multi-rail scenario, for handling network failures and their recovery without compromising on performance. In a general message passing scenario, whenever there is a network failure, the entire job aborts. Our design allows the job to continue even when a network failure occurs, by using the remaining rails for communication. Once the rail recovers from the failure, we also propose a protocol to re-establish connections on that rail and resume normal operations. We experimentally demonstrate that our implementation adds very little overhead and is able to deliver good performance which is comparable to that of the other rails running in isolation. We also show that the recovery is immediate and is associated with no additional overhead. We also depict sustenance and reliability of the design by running application benchmarks with permanent failures.},
keywords={Rails;Libraries;Fault tolerance;Fault tolerant systems;Peer to peer computing;Servers;Benchmark testing;MPI;Fault Tolerance;Failover},
doi={10.1109/IPDPSW.2012.142},
ISSN={},
month={May},}
@INPROCEEDINGS{6038913,
author={Cole, Stijn and Belmans, Ronnie},
booktitle={2011 IEEE Power and Energy Society General Meeting},
title={MatDyn, a new Matlab based toolbox for power system dynamic simulation},
year={2011},
volume={},
number={},
pages={1-1},
abstract={Summary form only given: In this paper, we present a new Matlab based toolbox for power system analysis, called MatDyn. It is opensource software, and available for everyone to download. Its design philosophy is based on the well-known open-source Matlab toolbox MATPOWER, but its focus is transient stability analysis and time-domain simulation of power systems, instead of steadystate calculations. MatDyn's philosophy, design criteria, program structure, and implementation, are discussed in detail. A trade-off is achieved between the flexibility of the program and readability of the code. MatDyn retains overall flexibility by for instance allowing user defined models, and custom integration methods. The software is validated by comparing its results with those obtained by the commercial grade power system analysis package, PSS/E. Despite the fact that MatDyn is fairly new, it has already been extensively used in research and education. This paper reports interesting results obtained with MatDyn in recent research, that would be hard to obtain using commercial software.},
keywords={Power system stability;Software;Analytical models;Power system dynamics;Stability analysis;IEEE Xplore;Power system modeling;Power system simulation;Power system dynamic stability;Software;Power system modeling},
doi={10.1109/PES.2011.6038913},
ISSN={1944-9925},
month={July},}
@INPROCEEDINGS{609354,
author={Williams, J.P. and Johnstone, J.K. and Wolff, L.B.},
booktitle={Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title={Rational discrete generalized cylinders and their application to shape recovery in medical images},
year={1997},
volume={},
number={},
pages={387-392},
abstract={Generalized cylinders (GCs) are a popular representational tool in computer vision. In medical imaging, the curved axis GC is particularly applicable to a number of elongated physical structures such as vasculature, bone and bronchi. In many of these instances, it is necessary to recover curved-axis GCs with arbitrary cross-sections. It is also vital that these structures, once recovered, can be analyzed and visualized with off-the-shelf algorithms and software packages. Such tools are usually designed to operate on the domain of polynomial or rational surfaces. Unfortunately most extant, suitably versatile GC representations do not admit rational parameterizations. We develop an entirely rational B-spline representation for generalized cylinders with curved axes and arbitrary cross-section functions. We demonstrate how our representation can be used as a deformable model by extracting a rational GC from pre-segmented spinal data using a discrete dynamic surface fit.},
keywords={Application software;Computer vision;Biomedical imaging;Bones;Respiratory system;Algorithm design and analysis;Visualization;Software algorithms;Software packages;Polynomials},
doi={10.1109/CVPR.1997.609354},
ISSN={1063-6919},
month={June},}
@INPROCEEDINGS{1386096,
author={Sobr, L. and Tuma, P.},
booktitle={The 2005 Symposium on Applications and the Internet},
title={SOFAnet: middleware for software distribution over Internet},
year={2005},
volume={},
number={},
pages={48-53},
abstract={The paper focuses on software distribution over the Internet, points to weaknesses of the current practice and argues for removing the weaknesses by introducing a ubiquitous distribution framework at the middleware level. The paper describes our design and implementation of such a framework, capable of supporting complex distribution and licensing models.},
keywords={Middleware;Internet;Application software;Packaging;Licenses;Software packages;Data security;IP networks;Software engineering;Mathematics},
doi={10.1109/SAINT.2005.60},
ISSN={},
month={Feb},}
@INPROCEEDINGS{1404438,
author={Kootsey, J.M. and Siriphongs, D. and McAuley, G.},
booktitle={The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
title={Building interactive simulations in a Web page design program},
year={2004},
volume={2},
number={},
pages={5166-5168},
abstract={A new Web software architecture, NumberLinX (NLX), has been integrated into a commercial Web design program to produce a drag-and-drop environment for building interactive simulations. NLX is a library of reusable objects written in Java, including input, output, calculation, and control objects. The NLX objects were added to the palette of available objects in the Web design program to be selected and dropped on a page. Inserting an object in a Web page is accomplished by adding a template block of HTML code to the page file. HTML parameters in the block must be set to user-supplied values, so the HTML code is generated dynamically, based on user entries in a popup form. Implementing the object inspector for each object permits the user to edit object attributes in a form window. Except for model definition, the combination of the NLX architecture and the Web design program permits construction of interactive simulation pages without writing or inspecting code.},
keywords={Buildings;Web page design;Web design;HTML;Software architecture;Software libraries;Java;Web pages;Service oriented architecture;Writing;Web page;interactive;simulation},
doi={10.1109/IEMBS.2004.1404438},
ISSN={},
month={Sep.},}
@ARTICLE{8354829,
author={Yan, Huan and Li, Yong and Dong, Wenxia and Jin, Depeng},
journal={IEEE Access},
title={Software-Defined WAN via Open APIs},
year={2018},
volume={6},
number={},
pages={33752-33765},
abstract={Cloud applications require the ability to customize bandwidth and network policies for desired wide area network (WAN) connections. Unfortunately, currently widely used virtual private network is difficult to achieve such customization where sophisticated manual configurations and operator expertise are required. Software-defined network (SDN) creates the opportunities to provide this ability. In this paper, we design Grace, a SDN-based system to provide diverse connections with flexible bandwidth and customized policies, implementing WAN as a service. Our contributions can be listed as follows: 1) We introduce open APIs for customers by abstracting WAN connections based on connection types, bandwidth, latency sensitivity, and policy-related information; 2) We develop an effective conflict detection algorithm considering both resource reservation and safety guarantee; 3) We propose a linear programming - based bandwidth algorithm for latency-sensitive connections by dynamic scheduling of time and bandwidth, and design a pricing scheme for various connection demands to address the case that network cannot fulfill all the connection requests. A prototype implementation and extensive evaluations show that Grace provides the ability to customize WAN connections without policy conflicts, allocates required bandwidth optimally, translates them into low-level configurations for underlying network devices, and successfully deploys WAN in a short time.},
keywords={Bandwidth;Wide area networks;Virtual private networks;Channel allocation;Network topology;Topology;Prototypes;Software-defined network (SDN);wide area network (WAN);bandwidth allocation;policy conflict},
doi={10.1109/ACCESS.2018.2833211},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{933658,
author={Demko, A.B. and Pizzi, N.J. and Somorjai, R.L.},
booktitle={Canadian Conference on Electrical and Computer Engineering 2001. Conference Proceedings (Cat. No.01TH8555)},
title={A Classification Canvas for the analysis of biomedical data},
year={2001},
volume={2},
number={},
pages={1391-1396 vol.2},
abstract={With the rapid proliferation of complex high-dimensional biomedical data, an acute need exists for a comprehensive, knowledge-based, domain-specific, user-friendly software suite that allows investigators, in the health care disciplines, to classify their data through the detection of novel or discriminating features therein. The Classification Canvas is an attempt to achieve these goals in addition to providing intuitive visual computation and logic construction. In this paper we describe various design and implementation issues such as: balancing user (novice) friendliness and developer (experienced) utility, performance versus modularity trade-offs, C++ and Java data sharing responsibilities, and creating graphical interfaces for (user-supplied) algorithm control.},
keywords={Data analysis;Bioinformatics;Java;Medical services;Tree graphs;Genetic algorithms;Councils;Logic;Algorithm design and analysis;Software tools},
doi={10.1109/CCECE.2001.933658},
ISSN={0840-7789},
month={May},}
@INPROCEEDINGS{10053342,
author={Goswami, Rohit and S., Ruhila and Goswami, Amrita and Goswami, Sonaly and Goswami, Debabrata},
booktitle={2022 Seventh International Conference on Parallel, Distributed and Grid Computing (PDGC)},
title={Reproducible High Performance Computing without Redundancy with Nix},
year={2022},
volume={},
number={},
pages={238-242},
abstract={High performance computing (HPC) clusters are typically managed in a restrictive manner; the large user base makes cluster administrators unwilling to allow privilege escalation. Here we discuss existing methods of package management, including those which have been developed with scalability in mind, and enumerate the drawbacks and advantages of each management methodology. We contrast the paradigms of containerization via docker, virtualization via KVM, pod-infrastructures via Kubernetes, and specialized HPC packaging systems via Spack and identify key areas of neglect. We demonstrate how functional programming due to reliance on immutable states has been leveraged for deterministic package management via the nix-language expressions. We show its associated ecosystem is a prime candidate for HPC package management. We further develop guidelines and identify bottlenecks in the existing structure and present the methodology by which the nix ecosystem should be developed further as an optimal tool for HPC package management. We assert that the caveats of the nix ecosystem can easily mitigated by considerations relevant only to HPC systems, without compromising on functional methodology and features of the nix-language. We show that benefits of adoption in terms of generating reproducible derivations in a secure manner allow for workflows to be scaled across heterogeneous clusters. In particular, from the implementation hurdles faced during the compilation and running of the d-SEAMS scientific software engine, distributed as a nix-derivation on an HPC cluster, we identify communication protocols for working with SLURM and TORQUE user resource allocation queues. These protocols are heuristically defined and described in terms of the reference implementation required for queue-efficient nix builds.},
keywords={Torque;Protocols;High performance computing;Ecosystems;Packaging;Software;Resource management;high-performance-computing;reproducible-research;nix-lang;functional-derivations;functional-package-management},
doi={10.1109/PDGC56933.2022.10053342},
ISSN={2573-3079},
month={Nov},}
@INPROCEEDINGS{9261532,
author={Trapeznikova, M. A. and Churbanova, N. G. and Chechina, A. A. and Ermakov, A. V. and German, M. S.},
booktitle={2020 International Conference on Engineering Management of Communication and Technology (EMCTECH)},
title={Supercomputer Technology for Traffic Simulation in a Metropolis},
year={2020},
volume={},
number={},
pages={1-4},
abstract={The present paper is aimed at the creation of a universal computational technology of multilane traffic and the development of robust tools for the simulation of vehicular traffic flows in megacities using parallel computing. The software is based on macro- as well as microscopic models. The macroscopic model uses the continuum approach and draws the analogy with the known quasigasdynamic system of equations. The multilane microscopic model is based on the cellular automata theory. Depending on the model different parallelization techniques are proposed to adapt city road networks to multiprocessor architectures. The models are implemented as program packages including the User Interface and Visualization module for transferring data to computational modules and providing visual interpretation of results. A number of test predictions have proven the effectiveness of the developed technology.},
keywords={Mathematical model;Computational modeling;Roads;Microscopy;Supercomputers;Automobiles;Data visualization;parallel computing;vehicular traffic flow;macroscopic model;cellular automata;visualization},
doi={10.1109/EMCTECH49634.2020.9261532},
ISSN={},
month={Oct},}
@INPROCEEDINGS{5070962,
author={Cheung, Eric and Smith, Thomas M.},
booktitle={2009 31st International Conference on Software Engineering - Companion Volume},
title={Experience with modularity in an advanced teleconferencing service deployment},
year={2009},
volume={},
number={},
pages={39-49},
abstract={In this paper, we describe our experience with the design of an advanced teleconferencing service under two different frameworks - an early implementation of the distributed feature composition architecture, and the SIP Servlet API. The usual design goals of software modularity for encapsulation and reuse are pursued. Interestingly, two very different designs resulted. This paper discusses the factors that influenced our design decisions. In particular, we examine the different characteristics of the two frameworks as well as the maturity of project requirements, and illustrate the ways in which these factors affect various mechanisms for achieving software modularity. We also aim to draw on this experience to propose a set of design guidelines for building modular, composable SIP Servlet applications for Voice over IP and converged services.},
keywords={Teleconferencing;Application software;Internet telephony;Call conference;Guidelines;Bridges;Switches;Web server;Context-aware services;Computer architecture},
doi={10.1109/ICSE-COMPANION.2009.5070962},
ISSN={},
month={May},}
@ARTICLE{5556015,
author={Burau, Heiko and Widera, Renée and Hönig, Wolfgang and Juckeland, Guido and Debus, Alexander and Kluge, Thomas and Schramm, Ulrich and Cowan, Tomas E. and Sauerbrey, Roland and Bussmann, Michael},
journal={IEEE Transactions on Plasma Science},
title={PIConGPU: A Fully Relativistic Particle-in-Cell Code for a GPU Cluster},
year={2010},
volume={38},
number={10},
pages={2831-2839},
abstract={The particle-in-cell (PIC) algorithm is one of the most widely used algorithms in computational plasma physics. With the advent of graphical processing units (GPUs), large-scale plasma simulations on inexpensive GPU clusters are in reach. We present an implementation of a fully relativistic plasma PIC algorithm for GPUs based on the NVIDIA CUDA library. It supports a hybrid architecture consisting of single computation nodes interconnected in a standard cluster topology, with each node carrying one or more GPUs. The internode communication is realized using the message-passing interface. The simulation code PIConGPU presented in this paper is, to our knowledge, the first scalable GPU cluster implementation of the PIC algorithm in plasma physics.},
keywords={Graphics processing unit;Magnetic cores;Computer architecture;Clustering algorithms;Instruction sets;Computational modeling;Plasmas;Electron accelerators;parallel algorithms;parallel architectures;particle beams;plasma waves;simulation software},
doi={10.1109/TPS.2010.2064310},
ISSN={1939-9375},
month={Oct},}
@ARTICLE{6312928,
author={Wasserman, Anthony I. and Pircher, Peter A. and Shewmake, David T.},
journal={IEEE Transactions on Software Engineering},
title={Building reliable interactive information systems},
year={1986},
volume={SE-12},
number={1},
pages={147-156},
abstract={User software engineering (USE) is a methodology, with supporting tools, for the specification, design, and implementation of interactive information systems. With the USE approach, the user interface is formally specified with augmented state transition diagrams, and the operations may be formally specified with preconditions and postconditions. The USE state transition diagrams may be directly executed with the application development tool RAPID/USE. RAPID/USE and its associated tool RAPSUM create and analyze logging information that is useful for system testing, and for evaluation and modification of the user interface. The authors briefly describe the USE transition diagrams and the formal specification approach, and show how these tools and techniques aid in the creation of reliable interactive information systems.},
keywords={Libraries;Information systems;User interfaces;Testing;Databases;Software reliability;Interactive information systems;RAPID/USE;software development methodology;software reliability;transition diagrams;User Software Engineering},
doi={10.1109/TSE.1986.6312928},
ISSN={1939-3520},
month={Jan},}
@ARTICLE{4141236,
author={Poletti, Francesco and Poggiali, Antonio and Bertozzi, Davide and Benini, Luca and Marchal, Pol and Loghi, Mirko and Poncino, Massimo},
journal={IEEE Transactions on Computers},
title={Energy-Efficient Multiprocessor Systems-on-Chip for Embedded Computing: Exploring Programming Models and Their Architectural Support},
year={2007},
volume={56},
number={5},
pages={606-621},
abstract={In today's multiprocessor SoCs (MPSoCs), parallel programming models are needed to fully exploit hardware capabilities and to achieve the 100 Gops/W energy efficiency target required for ambient intelligence applications. However, mapping abstract programming models onto tightly power-constrained hardware architectures imposes overheads which might seriously compromise performance and energy efficiency. The objective of this work is to perform a comparative analysis of message passing versus shared memory as programming models for single-chip multiprocessor platforms. Our analysis is carried out from a hardware-software viewpoint: we carefully tune hardware architectures and software libraries for each programming model. We analyze representative application kernels from the multimedia domain, and identify application-level parameters that heavily influence performance and energy efficiency. Then, we formulate guidelines for the selection of the most appropriate programming model and its architectural support},
keywords={Computer architecture;Programming;Hardware;Message passing;Computational modeling;Software;Energy efficiency;MPSoCs;embedded multimedia;programming models;task-level parallelism;energy efficiency;low power.},
doi={10.1109/TC.2007.1040},
ISSN={1557-9956},
month={May},}
@INPROCEEDINGS{6602738,
author={Serkin, F.B. and Vazhenin, N.A.},
booktitle={2013 15th International Conference on Transparent Optical Networks (ICTON)},
title={USRP platform for communication systems research},
year={2013},
volume={},
number={},
pages={1-4},
abstract={The paper presents the features for design, testing and validation of various implementations of signal processing algorithms using the concept of Software-Defined Radio (SDR), Model-Based Design and Universal Software Radio Peripheral (USRP) platform. Designed example of the 2-FSK-modem is showing, how may be created the hardware-software package that allows to test the different implementations of signal processing algorithms for various standards, research these implementations and conduct the verification & validation procedure for the subsequent process of the device manufacturing.},
keywords={Radio frequency;Field programmable gate arrays;Wideband;Abstracts;Narrowband;Application specific integrated circuits;Digital signal processing;software-defined radio;cognitive radio;model-based design;automatic code generation},
doi={10.1109/ICTON.2013.6602738},
ISSN={2161-2064},
month={June},}
@INPROCEEDINGS{8754412,
author={Urvantsev, Anton and Johansson, Morgan E. and Müellner, Nils and Seceleanu, Tiberiu},
booktitle={2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)},
title={Experiencing Technology Independence},
year={2019},
volume={2},
number={},
pages={153-158},
abstract={In this paper we present an embedded systems design flow, supporting variations in technology - hardware vs. software - at implementation time. The work builds on seasoned and new approaches, tools and techniques, such as software production lines, FPGA design, and high level synthesis. We define the necessary context for such a design flow to succeed, and introduce supporting tools and interfaces to enable the designer to take decisions, which are further automatically transferred into the synthesis phases. We exemplify our solutions on a motor controller design, considering several features that are potentially required to be implemented, and where all the elements are possible to be implemented either as hardware or software modules.},
keywords={Hardware;Tools;Software packages;Unified modeling language;Safety;VHDL;technology independence;software production lines;FPGA},
doi={10.1109/COMPSAC.2019.10199},
ISSN={0730-3157},
month={Jul},}
@INPROCEEDINGS{9059915,
author={Panagiotidou, Margarita and Evans, Philip and Dikaios, Nikolaos},
booktitle={2019 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)},
title={Integration of Proton Computed Tomography into the Open Source Software STIR},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Proton computed tomography (pCT) offers unique image formation attributes, with a potential for increasing accuracy of treatment planning in proton beam therapy. To maximize the potential of pCT it is necessary to develop advanced reconstruction algorithms that can accurately recover relative proton stopping power maps. This study aims to integrate pCT into STIR (Software for Tomographic Image Reconstruction), a popular Multi-Platform Object-Oriented framework for reconstruction in tomographic imaging to benefit from its software infrastructure. Open source STIR library is currently suitable for reconstructing and manipulating data from Positron Emission Tomography (PET) and Single Photon Emission Computed Tomography (SPECT), which are based on cylindrical scanner geometries. Although pCT has a noncylindrical geometry, STIR provides the framework for single event detection and modelling of the proton interactions. This initial implementation includes classes and functions with new features such as general proton scanner geometry, binning of list mode proton data into sinograms and uses analytical reconstruction algorithms already available in STIR. The structure of the new implemented features is discussed. Future work will include additional components to establish STIR as a potential toolkit for pCT image reconstruction.},
keywords={Protons;Image reconstruction;Libraries;Computed tomography;C++ languages;Geometry;Particle beams},
doi={10.1109/NSS/MIC42101.2019.9059915},
ISSN={2577-0829},
month={Oct},}
@INPROCEEDINGS{545301,
author={Cox, P.T. and Smedley, T.J.},
booktitle={Proceedings 1996 IEEE Symposium on Visual Languages},
title={A visual language for the design of structured graphical objects},
year={1996},
volume={},
number={},
pages={296-303},
abstract={The design of abstract or physical structures has much in common with the design of software structures, particularly when the structure in question has a mechanical or computational behaviour; such as a digital circuit. Like programming language systems, design systems must have expressive power sufficient for representing any design, a simulation mechanism for debugging the artifact under construction, and a production mechanism; for example, compilation for a programming language, or chip fabrication for a VLSI design system. Since specifying complex devices requires repetitive and conditional structures analogous to iteration, recursion and conditionals in programs, languages for designing complex devices are usually based on textual programming languages, for example VHDL for VLSI design. The advent of full featured visual programming languages, however raises the possibility that the mechanisms used to visually express compact and powerful program structures could be generalised to design languages. We consider using these mechanisms to express the design of structured graphical objects.},
keywords={Computer languages;Very large scale integration;Software design;Physics computing;Digital circuits;Computational modeling;Circuit simulation;Debugging;Production systems;Chip scale packaging},
doi={10.1109/VL.1996.545301},
ISSN={1049-2615},
month={Sep.},}