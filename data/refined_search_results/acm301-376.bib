@inproceedings{10.5555/792768.793520,
author = {Vissers, K. A.},
title = {Trade-Offs in the Design of Mixed Hardware-Software Systems-a Perspective from Industry},
year = {1997},
isbn = {081867895X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many systems in the field of consumer electronics devices and computers consist of a hardware platform and of software running on that platform. In the design of these systems many trade-offs have to be made. In the design of the hardware platform trade-offs have to be made between programmable components and dedicated components. The programming of the hardware platform also contains many trade-offs. Here a "software architecture" needs to be developed that spans several layers, using well defined interfaces, e.g. application programming interfaces (APIs). The software contains often device drivers, an operating system, and end-user applications. In embedded systems the end-user can often not program the system directly, e.g. one cannot program the look and feel or contents of the on-screen display of your TV. In practical situations system design is based on many constraints, and seldom starts from scratch. The hardware interface to the system can be given, the models of processors that can be used can be limited, and software interfaces can be required. The trade-offs are in the hardware platform design and in the software design.},
booktitle = {Proceedings of the 5th International Workshop on Hardware/Software Co-Design},
pages = {65},
keywords = {programmable components, hardware platform, software architecture, high level synthesis, software interfaces, device drivers, industry perspective, television, hardware-software systems design, embedded systems, application programming interfaces, consumer electronics devices, on-screen display, well defined interfaces, end-user applications, operating system, hardware interface, dedicated components},
series = {CODES '97}
}

@inproceedings{10.1145/1477942.1477945,
author = {Wun, Ben and Crowley, Patrick and Raghunath, Arun},
title = {Design of a Scalable Network Programming Framework},
year = {2008},
isbn = {9781605583464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1477942.1477945},
doi = {10.1145/1477942.1477945},
abstract = {Nearly all programmable commercial hardware solutions offered for high-speed networking systems are capable of meeting the performance and flexibility requirements of equipment vendors. However, the primary obstacle to adoption lies with the software architectures and programming environments supported by these systems. Shortcomings include use of unfamiliar languages and libraries, portability and backwards compatibility, vendor lock-in, design and development learning curve, availability of competent developers, and a small existing base of software. Another key shortcoming of previous architectures is that either they are not multi-core oriented or they expose all the hardware details, making it very hard for programmers to deal with. In this paper, we present a practical software architecture for high-speed embedded systems that is portable, easy to learn and use, multicore oriented, and efficient.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
pages = {10–18},
numpages = {9},
location = {San Jose, California},
series = {ANCS '08}
}

@inproceedings{10.1145/3559009.3569668,
author = {Horro, Marcos and Pouchet, Louis-No\"{e}l and Rodr\'{\i}guez, Gabriel and Touri\~{n}o, Juan},
title = {Custom High-Performance Vector Code Generation for Data-Specific Sparse Computations},
year = {2023},
isbn = {9781450398688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559009.3569668},
doi = {10.1145/3559009.3569668},
abstract = {Sparse computations, such as sparse matrix-dense vector multiplication, are notoriously hard to optimize due to their irregularity and memory-boundedness. Solutions to improve the performance of sparse computations have been proposed, ranging from hardware-based such as gather-scatter instructions, to software ones such as generalized and dedicated sparse formats, used together with specialized executor programs for different hardware targets. These sparse computations are often performed on read-only sparse structures: while the data themselves are variable, the sparsity structure itself does not change. Indeed, sparse formats such as CSR have a typically high cost to insert/remove nonzero elements in the representation. The typical use case is to not modify the sparsity during possibly repeated computations on the same sparse structure.In this work, we exploit the possibility to generate a specialized executor program dedicated to the particular sparsity structure of an input matrix. It creates opportunities to remove indirection arrays and synthesize regular, vectorizable code for such computations. But, at the same time, it introduces challenges in code size and instruction generation, as well as efficient SIMD vectorization. We present novel techniques and extensive experimental results to efficiently generate SIMD vector code for data-specific sparse computations, and study the limits in terms of applicability and performance of our techniques compared to state-of-practice high-performance libraries like Intel MKL.},
booktitle = {Proceedings of the International Conference on Parallel Architectures and Compilation Techniques},
pages = {160–171},
numpages = {12},
keywords = {data-specific compilation, sparse data structure, vectorization},
location = {Chicago, Illinois},
series = {PACT '22}
}

@inproceedings{10.1145/2666141.2668386,
author = {Sadeghi, Ahmad-Reza and Davi, Lucas},
title = {Beasty Memories: The Quest for Practical Defense against Code Reuse Attacks},
year = {2014},
isbn = {9781450331494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666141.2668386},
doi = {10.1145/2666141.2668386},
abstract = {Code reuse attacks such as return-oriented programming (ROP) are predominant attack techniques that are extensively used to exploit vulnerabilities in modern software programs. ROP maliciously combines short instruction sequences (gadgets) residing in shared libraries and the application's executable to bypass data execution prevention (DEP) and launch targeted exploits. ROP attacks apply to many processor architectures from Intel x86 [1] to tiny embedded systems [2]. As a consequence, a variety of defenses have been proposed over the last few years - most prominently code randomization (ASLR) and control-flow integrity (CFI). Particularly, constructing practical CFI schemes has become a hot topic of research recently. In this talk, we present the evolution of return-oriented programming (ROP) attacks and defenses. We first give an overview of ROP attacks and techniques. Second, we investigate the security of software diversity based approaches such as finegrained code randomization [3]. Third, we dive deeper and focus on control-flow integrity (CFI) and show how to bypass all recent (coarse-grained) CFI solutions, including Microsoft's defense tool EMET [4]. Finally, we discuss new research directions to mitigate code reuse attacks, including our current work on hardware-assisted fine-grained control-flow integrity [5]. Part of this research [3-5] was conducted in collaboration with A. Dmitrienko, D. Lehmann, C. Liebchen, P. Koeberl, F. Monrose, and K. Z. Snow},
booktitle = {Proceedings of the 4th International Workshop on Trustworthy Embedded Devices},
pages = {23},
numpages = {1},
keywords = {runtime attacks, embedded systems security, return-oriented programming, control-flow integrity, software/hardware co-design, aslr},
location = {Scottsdale, Arizona, USA},
series = {TrustED '14}
}

@inproceedings{10.1145/2970276.2985778,
author = {P\"{u}schel, Markus},
title = {Program Generation for Performance},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2985778},
doi = {10.1145/2970276.2985778},
abstract = {It has become extraordinarily difficult to write software that performs close to optimally on complex modern microarchitectures. Particularly plagued are domains that require complex mathematical computations such as multimedia processing, communication, control, graphics, and machine learning. In these domains, performance-critical components are usually written in C (with possible extensions) and often even in assembly, carefully "tuned" to the platform's architecture and microarchitecture. The result is usually long, rather unreadable code that needs to be re-written or re-tuned with every platform upgrade. On the other hand, the performance penalty for relying on straightforward, non-tuned, "more elegant" implementations can be often a factor of 10, 100, or even more. The overall problem is one of productivity, maintainability, and quality (namely performance), i.e., software engineering. However, even though a large set of sophisticated software engineering theory and tools exist, it appears that to date this community has not focused much on mathematical computations nor performance in the detailed, close-to-optimal sense above. The reason for the latter may be that performance, unlike various aspects of correctness, is not syntactic in nature (and in reality is often even unpredictable and, well, messy). The aim of this talk is to draw attention to the performance/productivity problem for mathematical applications and to make the case for a more interdisciplinary attack. As a set of thoughts in this direction we offer some of the lessons we have learned in the last decade in our own research on Spiral (www.spiral.net), a program generation framework for numerical kernels. Key techniques used in Spiral include staged declarative domain-specific languages to express algorithm knowledge and algorithm transformations, the use of platform-cognizant rewriting systems for parallelism and locality optimizations, and the use of search and machine learning techniques to navigate possible spaces of choices. Experimental results show that the codegenerated by Spiral competes with, and sometimes outperforms, the best available human-written code. Spiral has been used to generate part of Intel's commercial libraries IPP and MKL.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {1},
numpages = {1},
keywords = {vectorization, matrix algebra, parallelization, rewriting systems, Fourier transform, Program generation, automatic programming},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3397537.3397549,
author = {Jakubovic, Joel},
title = {What It Takes to Create with Domain-Appropriate Tools: Reflections on Implementing the “Id” System},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397549},
doi = {10.1145/3397537.3397549},
abstract = {There is a One-Size-Fits-All quality to languages, APIs and even programming itself. Whether you're making a mobile game or a scientific simulation, you will be using a text-based language with similar devices for structuring your code. This is a source of artificial difficulty in creating, understanding, and modifying software systems. No matter the domain, the author's design needs encoding into a form that does not resemble it. This paper describes a vision where software can be built in a programming environment that is closer to the domain of the software itself. By doing so, users of the system can use familiar abstractions and tools for adapting it. A step towards this vision is presented: a Web version of a minimal OOP system, developed as an executable version of the diagrams of its design, in a substrate meant to facilitate this. The experience of creating such a substrate is analysed, and I suggest deficiencies in programming environments that stand in the way of making this practice commonplace, as well as ways to fill in these gaps.},
booktitle = {Companion Proceedings of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {197–207},
numpages = {11},
keywords = {visual programming, context-specific, object-oriented, meta-circular, adaptation},
location = {Porto, Portugal},
series = {Programming '20}
}

@article{10.1145/151280.151290,
author = {DeRose, Tony D. and Goldman, Ronald N. and Hagen, Hans and Mann, Stephen},
title = {Functional Composition Algorithms via Blossoming},
year = {1993},
issue_date = {April 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {0730-0301},
url = {https://doi.org/10.1145/151280.151290},
doi = {10.1145/151280.151290},
abstract = {In view of the fundamental role that functional composition plays in mathematics, it is not surprising that a variety of problems in geometric modeling can be viewed as instances of the following composition problem: given representations for two functions F and G, compute a representation of the function H = F o G. We examine this problem in detail for the case when F and G are given in either Be´zier or B-spline form. Blossoming techniques are used to gain theoretical insight into the structure of the solution which is then used to develop efficient, tightly codable algorithms. From a practical point of view, if the composition algorithms are implemented as library routines, a number of geometric-modeling problems can be solved with a small amount of additional software.},
journal = {ACM Trans. Graph.},
month = {apr},
pages = {113–135},
numpages = {23},
keywords = {B-splines, triangular Be´zier surface patches, free-form deformations, Be´zier curves, tensor-product surface patches, computer-aided geometric design}
}

@inproceedings{10.1145/544220.544276,
author = {Gon\c{c}alves, Marcos Andr\'{e} and Fox, Edward A.},
title = {5SL: A Language for Declarative Specification and Generation of Digital Libraries},
year = {2002},
isbn = {1581135130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/544220.544276},
doi = {10.1145/544220.544276},
abstract = {Digital libraries (DLs) are among the most complex kinds of information systems, due in part to their intrinsic multi disciplinary nature. Nowadays DLs are built within monolithic, tightly integrated, and generally inflexible systems -- or by assembling disparate components together in an ad-hoc way, with resulting problems in interoperability and adaptability. More importantly, conceptual modeling, requirements analysis, and software engineering approaches are rarely supported, making it extremely difficult to tailor DL content and behavior to the interests, needs, and preferences of particular communities. In this paper, we address these problems. In particular, we present 5SL, a declarative language for specifying and generating domain-specific digital libraries. 5SL is based on the 5S formal theory for digital libraries and enables high-level specification of DLs in five complementary dimensions, including: the kinds of multimedia information the DL supports (Stream Model); how that information is structured and organized (Structural Model); different logical and presentational properties and operations of DL components (Spatial Model); the behavior of the DL (Scenario Model); and the different societies of actors and managers of services that act together to carry out the DL behavior (Societal Model). The practical feasibility of the approach is demonstrated by the presentation of a 5SL digital library generator for the MARIAN digital library system.},
booktitle = {Proceedings of the 2nd ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {263–272},
numpages = {10},
keywords = {models, languages, generators, 5SL, design tools, 5S},
location = {Portland, Oregon, USA},
series = {JCDL '02}
}

@inproceedings{10.1145/3097983.3106683,
author = {Pafka, Szil\'{a}rd},
title = {Machine Learning Software in Practice: Quo Vadis?},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3106683},
doi = {10.1145/3097983.3106683},
abstract = {Due to the hype in our industry in the last couple of years, there is a growing mismatch between software tools machine learning practitioners wish for, what they would truly need for their work, what's available (either commercially or open source) and what tool developers and researchers focus on. In this talk we will give a couple of examples of this mismatch. Several surveys and anecdotal evidence show that most practitioners work most of the time (at least in the modeling phase) with datasets that t in the RAM of a single server, therefore distributed computing tools are very of- ten overkill. Our benchmarks (available on github [1]) of the most widely used open source tools for binary classification (various implementations of algorithms such as linear methods, random forests, gradient boosted trees and neural networks) on such data show over 10x speed and over 10x RAM usage difference between various tools, with "big data" tools being the most inefficient. Significant performance gains have been obtained by those tools that incorporate various low-level (close to CPU and memory architecture) optimizations. Nevertheless, we will show that even the best tools show degrading performance on the multi-socket servers featuring a high number of cores, systems that have become widely accessible more recently. Finally, while most of this talk is about performance, we will also argue that machine learning tools that feature high-level easy-to-use APIs provide increasing productivity for practitioners and therefore are preferable.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {25},
numpages = {1},
keywords = {accuracy, memory footprint, binary classification, training speed, software implementations},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3183713.3190655,
author = {Balkesen, Cagri and Kunal, Nitin and Giannikis, Georgios and Fender, Pit and Sundara, Seema and Schmidt, Felix and Wen, Jarod and Agrawal, Sandeep and Raghavan, Arun and Varadarajan, Venkatanathan and Viswanathan, Anand and Chandrasekaran, Balakrishnan and Idicula, Sam and Agarwal, Nipun and Sedlar, Eric},
title = {RAPID: In-Memory Analytical Query Processing Engine with Extreme Performance per Watt},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3190655},
doi = {10.1145/3183713.3190655},
abstract = {Today, an ever increasing amount of transistors are packed into processor designs with extra features to support a broad range of applications. As a consequence, processors are becoming more and more complex and power hungry. At the same time, they only sustain an average performance for a wide variety of applications while not providing the best performance for specific applications. In this paper, we demonstrate through a carefully designed modern data processing system called RAPID and a simple, low-power processor specially tailored for data processing that at least an order of magnitude performance/power improvement in SQL processing can be achieved over a modern system running on today's complex processors. RAPID is designed from the ground up with hardware/software co-design in mind to provide architecture-conscious extreme performance while consuming less power in comparison to the modern database systems. The paper presents in detail the design and implementation of RAPID, a relational, columnar, in-memory query processing engine supporting analytical query workloads.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1407–1419},
numpages = {13},
keywords = {databases, in-memory data processing, low-power, dpu, analytic query processing, hardware/software co-design},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3194164.3194181,
author = {Woods, Eoin},
title = {The Past, Present and Future of Technical Debt: Learning from the Past to Prepare for the Future},
year = {2018},
isbn = {9781450357135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194164.3194181},
doi = {10.1145/3194164.3194181},
abstract = {While technical debt has emerged as a formal concept relatively recently [2] we have had technical debt from the earliest days of software development, it has simply evolved in nature. So what can we learn from past types of technical debt to allow us to prepare for its future forms?When we look back over recent software history, we can see five identifiable evolutions of software systems [5], each one roughly aligning with a decade.Before and through the 1980s, software systems were largely monolithic and tended to run on single computers, with software being developed as monolithic "programs". As we moved into the 1990s, distributed systems became mainstream and the standard style for an enterprise system became three-tier client server. The Internet became a mainstream technology in the late 1990s, and organisations developed Internet-connected systems, which were "always on" rather than just "online" and could support difficult and unpredictable quality properties. In the current era, we are building Internet-native systems, where "the Internet is the system". These systems are built from a combination of open source components, remote Internet connected services and custom code, and their services often form part of the Internet via publicly accessible APIs.Following current trends, it seems that the next phase of evolution will be to Intelligent-Connected systems, as artificial intelligence (machine learning in particular) becomes mainstream [1], users expect context specific assistance, and fast, reliable networks allow us to connect "things" (devices) to our systems as well as traditional computers [3].Software engineering practice evolves in response to new challenges and each era of computing has introduced new techniques and technology but each has also introduced its own types of technical debt too.In the monolithic era, the focus was structuring a single program, with "spaghetti code", poor naming and unrestricted use of the "goto" emerging as examples of the earliest types of technical debt. As we moved into the distributed era, we ended up tangling presentation and business logic code in our client/server user interfaces, while in the Internet-connected era we distorted our systems to meet performance and scalability concerns at all costs and often ended up with poorly-designed automated tests being an inflexible technical debt of their own. More recently Internet-native systems often introduce a mishmash of microservices with poorly understood choreography and diverse internal implementations, references to external APIs that became unsupported or difficult to use and public APIs with many versions, all of which have to be maintained "forever" due to callers who would not migrate to new versions.So what types of technical debt do we expect in the future?In the intelligent-connected era, amongst other things, applications will have machine learning features and we'll need large datasets to train machine learning models and provide context-specific user experiences and we'll have lots of non-computing devices connected to our systems, providing data. So we'll probably get machine learning debt [4], ML models that we can't explain, models that we can't improve because people rely on their quirks (even if wrong). We'll also have large inflexible data sets which our systems and models rely on, and we'll have unknown and unpredictable collections of "things" connecting to our services, which we can't change because other people own them.While this sounds like a daunting set of challenges, the intelligent-connected era is only just beginning, so we have not yet incurred significant amounts of technical debt. By looking to the past as our guide to the future we can be forewarned and start find solutions to our future technical debt before we have become overwhelmed by it!},
booktitle = {Proceedings of the 2018 International Conference on Technical Debt},
pages = {61},
numpages = {1},
location = {Gothenburg, Sweden},
series = {TechDebt '18}
}

@inproceedings{10.1145/3303084.3309489,
author = {Ghane, Millad and Chandrasekaran, Sunita and Cheung, Margaret S.},
title = {Gecko: Hierarchical Distributed View of Heterogeneous Shared Memory Architectures},
year = {2019},
isbn = {9781450362900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303084.3309489},
doi = {10.1145/3303084.3309489},
abstract = {The November 2018 TOP500 report shows that 86 systems in the list are heterogeneous systems configured with accelerators and co-processors, of which 60 use NVIDIA GPUs, 21 use Intel Xeon Phi cards, one uses AMD FirePro GPUs, one uses PEZY technology, and three systems use a combination of NVIDIA GPUs and Intel Xeon Phi co-processors. From a software standpoint, managing data locality on such heterogeneous systems is as important as exploiting parallelism in order to achieve the best performance. With the advent of novel memory technologies, such as non-volatile memory (NVM) and 3D-stacked memory, there is an urgent need for effective mechanisms within programming models to create an easy-to-use interface that addresses such memory hierarchies. It is also equally crucial for applications to evolve with data locality for the expression of information. In this paper, we propose Gecko, a novel programming model that addresses the underlying memory hierarchy topology within computing elements in current and future platforms. Gecko's directives distribute data and computation among devices of different types in a system. We develop a source-to-source transformation and a runtime library to efficiently manage devices of different types to run asynchronously. Although our current implementation of Gecko targets Intel Xeon CPUs and NVIDIA GPUs, it is not restricted to such architectures. We used SHOC and Rodinia benchmark suites to evaluate Gecko. Our experiments used a single node consisting of four NVIDIA Volta V100 GPUs. Results demonstrated scalability through the multi-GPU environment. We observed 3.3 times speedup when using multiple GPUs.},
booktitle = {Proceedings of the 10th International Workshop on Programming Models and Applications for Multicores and Manycores},
pages = {21–30},
numpages = {10},
keywords = {Portable, Abstraction, Programming Model, Heterogeneous, Hierarchy, Shared Memory},
location = {Washington, DC, USA},
series = {PMAM'19}
}

@inproceedings{10.1145/2034751.2034753,
author = {Katagiri, Takahiro},
title = {Towards Auto-Tuning Description Language to Heterogeneous Computing Environment},
year = {2011},
isbn = {9781450308625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2034751.2034753},
doi = {10.1145/2034751.2034753},
abstract = {Computer architectures are becoming more and more complex due to non-standardized memory accesses and hierarchical caches. It is very difficult for scientists and engineers to optimize their code to extract potential performance improvements on these architectures. Due to this, automatic performance tuning (AT) technology, hence, is a key technology to reduce cost of development for high performance numerical software.In this talk, the following two aims are folded. First, we introduce current AT studies. We focus on AT technology for numerical computations in viewpoint of numerical libraries, languages, code generators, and OS run-time software.Second, we explain ABCLibScript [1], which is an auto-tuning description language for C and Fortran90 for numerical computations to numerical software developers. ABCLibScript provides automatic code generation functions for dedicated code optimization, such as loop unrolling, algorithm selection, and varying of specified variables described by the user. We also explain HxABCLibScript[2], which is an AT language with extended function from original ABCLibScript to heterogeneous computer environment, which includes CPU and GPU (Graphics Processing Unit). The description of HxABCLibScript can free from selection of CPU and GPU switching to the arbitrary parts of program from users.The preliminary results show that the function of HxABCLibScript was highly efficient for simple kernels of typical numerical computations, such as a matrix-matrix multiplication, or a stencil computation from the Poisson's equation solver. The automatically generated codes from the description of HxABCLibScript can select the best computer resources between CPU and GPU according to problem size or the number of iterations on the program.},
booktitle = {Proceedings of the Fifth International Workshop on High-Level Parallel Programming and Applications},
pages = {1–2},
numpages = {2},
keywords = {high-performance computing, auto tuning},
location = {Tokyo, Japan},
series = {HLPP '11}
}

@proceedings{10.1145/1011870,
title = {PDC 04: Proceedings of the Eighth Conference on Participatory Design: Artful Integration: Interweaving Media, Materials and Practices - Volume 1},
year = {2004},
isbn = {1581138512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Participatory Design (PD) is a diverse collection of principles and practices aimed at making technologies and social institutions more responsive to human needs. The central tenet of PD is the direct involvement of people in the codesign of the systems they use. Originally viewed as an approach to developing computer systems for specific groups of workers, PD has expanded outwards in philosophical, political and pragmatic ways. It is now part of an emerging movement that blurs theoretical and practical boundaries and integrates work from many disciplines -- in an artful way -- all in pursuit of relevance for people around the world shaping their own 'networked society.'The Participatory Design Conferences have been convened every two years since 1990. These forums have brought together a multidisciplinary and international group of software developers, researchers, social scientists, designers, activists, practitioners, users, citizens, cultural workers and managers who adopt distinctively participatory approaches in the development of information and communication artifacts, systems, services and technology.Participatory design approaches have been used in traditional application domains (such as computer systems for business, health care and government) and are also relevant in emerging areas such as web-portal design, e-government services, community networks, enterprise resource planning, public CSCW (computer supported cooperative work) systems, social administration, community development, university/community partnerships, tele-health, political deliberation/mobilization (e-democracy), digital arts and design, scholarship and teaching with mediated technologies (e-learning), cultural production and cultural institutions. PD is also being used in the development of ICT (information and communication technology) infrastructures like free software/open source projects, standards, protocols, new media, policy, broadband and WiFi (wireless fidelity) networks and the like.Participatory designers of ICT-applications may learn from, and, hopefully contribute to, work in other fields, such as community and organizational development, architecture, urban planning, policy development, media, design and art, especially insofar as these fields increasingly use ICTs. Participatory design approaches can be applied in various social settings such as local communities, government agencies, civil society, NGOs, schools and universities, companies, trade unions, etc. each with its own distinctive stakeholder arenas and power relations.Artful IntegrationThe overall theme of the 2004 conference, "Artful Integration: Interweaving Media, Materials and Practices" describes a central reality of participatory design. It recognizes that an essential ingredient in design practice is the working together of multiple, heterogeneous elements. Whereas conventional design approaches emphasize the role of the designer and the creation of singular 'things', artful integration calls attention to the collective interweaving of people, artifacts and processes to achieve practical, aesthetic or emancipatory syntheses. With that in mind the conference organizers inaugurated the "Artful Integrators Award" to recognize exemplary work in participatory design.The award is intended to recognize outstanding achievement in the area of participatory design of information and communications technologies. The award goes to a group of people who together have worked out, in an exceptionally creative way, a new and useful configuration of technologies and practices. Where traditional design awards have gone to individual designers or singular objects, the Artful Integrators Award emphasizes the importance of collaborative participation in design, and a view of good design as the effective alignment of diverse collections of people, activities and artifacts. While no single element of the design might be particularly extraordinary in itself, the combination of design process and outcome can be.The Artful Integrators Award 2004 goes to Randy Trigg and the Global Fund for Women. Through their ongoing project of participatory design, Randy and his organization have created an information and communications infrastructure that exemplifies, in process and products, the spirit of the Artful Integration Award. As an accomplished software developer and systems integrator, Randy's collaboration with members of the Fund has resulted in the design of a database system for nonprofits that brings together fundraising, grant making and human resource management in ways that accommodate the continually evolving work practices of the organization. The Global Fund's developing infrastructure weaves together Randy's longstanding commitment to cooperative design practices with the Global Fund's commitment to democratic forms of wealth redistribution. Receiving the Award with Randy is Kavita Ramdas, President and CEO of the Global Fund for Women, who will speak about the Fund's grant making philosophy and participatory practices.Research PapersThis Volume collects the research papers presented at PDC2004. They are organized in three broad areas, corresponding to the main tracks within the conference. The first track deals with participatory design in various community contexts, reflecting the recent growth of interest in this emerging area. The other two are more traditional, dealing with methodological considerations and reflections on case experiences respectively. Within the tracks, the papers appear in the order of their presentation at the conference.A word about the reviewing process for the research papers. We were very pleased at the enthusiastic response to the call for papers. We received 63 papers from people in over a dozen countries. Each paper was double-blind reviewed by at least three reviewers, coordinated through an online system custom built for this purpose. At a meeting in Toronto the program committee considered all the reviews, and selected 23 papers for presentation, providing authors with comments reflected in the revised papers you find here.Other Conference ContributionsA highlight of any conference are the keynote addresses by invited speakers. At PDC2004 we are proud that three distinguished scholars are sharing their insights with us: Tone Bratteteig, a PD pioneer and associate professor in the Department of Informatics, University of Oslo, speaks on "Participatory Design in Present Society," highlighting the challenges posed by such developments as the globalization of organizations and work processes. Reinhard Keil-Slawik, a professor of Informatics and Society in the Heinz Nixdorf Institute, University of Paderborn., considers fundamental product/process tensions in his talk "Participation in the Age of Digital Media". The conference closes with Jonathan Barker, professor emeritus of the University of Toronto, speaking on "Fearful Asymmetry: Terror, Power, and the Shape of Popular Action", in which he addresses the contemporary political context in which PD practitioners will likely need to take heed.In addition to the research papers and keynote speakers, PDC 2004 convenes a diverse collection of other events. These include short papers (research works in progress, field experiences / stories from reflective practitioners, tools and techniques reports), preconference workshops, conference workshops, artifacts, posters, interactive demonstrations, art installations, and tutorials. These are described in Volume II of the proceedings. In addition, for the first time, we convene a Doctoral Consortium. Among the various other venues for sharing PD ideas, the papers and related conference materials will all be hosted on CPSRs digital library.Participatory Design (PD) is a diverse collection of principles and practices aimed at making technologies and social institutions more responsive to human needs. The central tenet of PD is the direct involvement of people in the codesign of the systems they use. Originally viewed as an approach to developing computer systems for specific groups of workers, PD has expanded outwards in philosophical, political and pragmatic ways. It is now part of an emerging movement that blurs theoretical and practical boundaries and integrates work from many disciplines -- in an artful way -- all in pursuit of relevance for people around the world shaping their own networked society.The Participatory Design Conferences have been convened every two years since 1990. These forums have brought together a multidisciplinary and international group of software developers, researchers, social scientists, designers, activists, practitioners, users, citizens, cultural workers and managers who adopt distinctively participatory approaches in the development of information and communication artifacts, systems, services and technology.Participatory design approaches have been used in traditional application domains (such as computer systems for business, health care and government) and are also relevant in emerging areas such as web-portal design, e-government services, community networks, enterprise resource planning, public CSCW (computer supported cooperative work) systems, social administration, community development, university/community partnerships, tele-health, political deliberation/mobilization (e-democracy), digital arts and design, scholarship and teaching with mediated technologies (e-learning), cultural production and cultural institutions. PD is also being used in the development of ICT (information and communication technology) infrastructures like free software/open source projects, standards, protocols, new media, policy, broadband and WiFi (wireless fidelity) networks and the like.Participatory designers of ICT-applications may learn from, and, hopefully contribute to, work in other fields, such as community and organizational development, architecture, urban planning, policy development, media, design and art, especially insofar as these fields increasingly use ICTs. Participatory design approaches can be applied in various social settings such as local communities, government agencies, civil society, NGOs, schools and universities, companies, trade unions, etc. each with its own distinctive stakeholder arenas and power relations.Artful IntegrationThe overall theme of the 2004 conference, "Artful Integration: Interweaving Media, Materials and Practices" describes a central reality of participatory design. It recognizes that an essential ingredient in design practice is the working together of multiple, heterogeneous elements. Whereas conventional design approaches emphasize the role of the designer and the creation of singular 'things', artful integration calls attention to the collective interweaving of people, artifacts and processes to achieve practical, aesthetic or emancipatory syntheses. With that in mind the conference organizers inaugurated the "Artful Integrators Award" to recognize exemplary work in participatory design.The award is intended to recognize outstanding achievement in the area of participatory design of information and communications technologies. The award goes to a group of people who together have worked out, in an exceptionally creative way, a new and useful configuration of technologies and practices. Where traditional design awards have gone to individual designers or singular objects, the Artful Integrators Award emphasizes the importance of collaborative participation in design, and a view of good design as the effective alignment of diverse collections of people, activities and artifacts. While no single element of the design might be particularly extraordinary in itself, the combination of design process and outcome can be.The Artful Integrators Award 2004 goes to Randy Trigg and the Global Fund for Women. Through their ongoing project of participatory design, Randy and his organization have created an information and communications infrastructure that exemplifies, in process and products, the spirit of the Artful Integration Award. As an accomplished software developer and systems integrator, Randy's collaboration with members of the Fund has resulted in the design of a database system for nonprofits that brings together fundraising, grant making and human resource management in ways that accommodate the continually evolving work practices of the organization. The Global Fund's developing infrastructure weaves together Randy's longstanding commitment to cooperative design practices with the Global Fund's commitment to democratic forms of wealth redistribution. Receiving the Award with Randy is Kavita Ramdas, President and CEO of the Global Fund for Women, who will speak about the Fund's grant making philosophy and participatory practices.Research PapersThis Volume collects the research papers presented at PDC2004. They are organized in three broad areas, corresponding to the main tracks within the conference. The first track deals with participatory design in various community contexts, reflecting the recent growth of interest in this emerging area. The other two are more traditional, dealing with methodological considerations and reflections on case experiences respectively. Within the tracks, the papers appear in the order of their presentation at the conference.A word about the reviewing process for the research papers. We were very pleased at the enthusiastic response to the call for papers. We received 63 papers from people in over a dozen countries. Each paper was double-blind reviewed by at least three reviewers, coordinated through an online system custom built for this purpose. At a meeting in Toronto the program committee considered all the reviews, and selected 23 papers for presentation, providing authors with comments reflected in the revised papers you find here.Other Conference ContributionsA highlight of any conference are the keynote addresses by invited speakers. At PDC2004 we are proud that three distinguished scholars are sharing their insights with us: Tone Bratteteig, a PD pioneer and associate professor in the Department of Informatics, University of Oslo, speaks on "Participatory Design in Present Society," highlighting the challenges posed by such developments as the globalization of organizations and work processes. Reinhard Keil-Slawik, a professor of Informatics and Society in the Heinz Nixdorf Institute, University of Paderborn., considers fundamental product/process tensions in his talk "Participation in the Age of Digital Media". The conference closes with Jonathan Barker, professor emeritus of the University of Toronto, speaking on "Fearful Asymmetry: Terror, Power, and the Shape of Popular Action", in which he addresses the contemporary political context in which PD practitioners will likely need to take heed.In addition to the research papers and keynote speakers, PDC 2004 convenes a diverse collection of other events. These include short papers (research works in progress, field experiences / stories from reflective practitioners, tools and techniques reports), preconference workshops, conference workshops, artifacts, posters, interactive demonstrations, art installations, and tutorials. These are described in Volume II of the proceedings. In addition, for the first time, we convene a Doctoral Consortium. Among the various other venues for sharing PD ideas, the papers and related conference materials will all be hosted on CPSR's 'digital library' (cpsr.org).},
location = {Toronto, Ontario, Canada}
}

@inproceedings{10.1145/3127041.3127061,
author = {Roe, Kenneth and Smith, Scott F.},
title = {Using the Coq Theorem Prover to Verify Complex Data Structure Invariants},
year = {2017},
isbn = {9781450350938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127041.3127061},
doi = {10.1145/3127041.3127061},
abstract = {While automated static analysis tools can find many useful software bugs, there are still bugs that are beyond the reach of these tools. Most large software systems have complex data structures with complex invariants, and many bugs can be traced to code that does not maintain these invariants. These invariants cannot be easily inferred by automated tools. One must use an interactive system in which developers first enter these invariants to document their software and then use a theorem prover to verify their correctness.We describe the PEDANTIC framework for verifying the correctness of C-like programs using the Coq theorem prover. PEDANTIC is designed to prove invariants over complex dynamic data structures such as inter-referencing trees and linked lists. The language for the invariants is based on separation logic. The PEDANTIC tactic library has been constructed to allow program verifications to be done with reasonably compact proofs.We have completed the verification of a tree traversal program. We are currently working on the verification of a C implementation of the DPLL algorithm in order to demonstrate the utility of the framework on more complex programs. Verifying programs using an interactive theorem prover is quite tedious. We discuss work being done to improve proof development productivity.},
booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {118–121},
numpages = {4},
location = {Vienna, Austria},
series = {MEMOCODE '17}
}

@inproceedings{10.1145/1297385.1297390,
author = {Elwasif, Wael R. and Norris, Boyana R. and Allan, Benjamin A. and Armstrong, Robert C.},
title = {Bocca: A Development Environment for HPC Components},
year = {2007},
isbn = {9781595938671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1297385.1297390},
doi = {10.1145/1297385.1297390},
abstract = {In high-performance scientific software development, the emphasis is often on short time to first solution. Even when the development of new components mostly reuses existing components or libraries and only small amounts of new code must be created, dealing with the component glue code and software build processes to obtain complete applications is still tedious and error-prone. Component-based software meant to reduce complexity at the application level increases complexity with the attendant glue code. To address these needs, we introduce Bocca, the first tool to enable application developers to perform rapid component prototyping while maintaining robust software-engineering practices suitable to HPC environments. Bocca provides project management and a comprehensive build environment for creating and managing applications composed of CommonComponent Architecture components. Of critical importance for HPC applications, Bocca is designed to operate in a language-agnostic way, simultaneously handling components written in any of the languages commonly used in scientific applications: C, C++, Fortran, Fortran77, Python,and Java. Bocca automates the tasks related to the component glue code, freeing the user to focus on the scientific aspects of the application. Bocca embraces the philosophy pioneered by Ruby Rails for web applications: Start with something that works and evolve it to the user's purpose.},
booktitle = {Proceedings of the 2007 Symposium on Component and Framework Technology in High-Performance and Scientific Computing},
pages = {21–30},
numpages = {10},
keywords = {scientific computing, component software, common component architecture, application construction, Bocca},
location = {Montreal, Quebec, Canada},
series = {CompFrame '07}
}

@inproceedings{10.1145/3052973.3053036,
author = {Gulmezoglu, Berk and Eisenbarth, Thomas and Sunar, Berk},
title = {Cache-Based Application Detection in the Cloud Using Machine Learning},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3053036},
doi = {10.1145/3052973.3053036},
abstract = {Cross-VM attacks have emerged as a major threat on commercial clouds. These attacks commonly exploit hardware level leakages on shared physical servers. A co-located machine can readily feel the presence of a co-located instance with a heavy computational load through performance degradation due to contention on shared resources. Shared cache architectures such as the last level cache (LLC) have become a popular leakage source to mount cross-VM attack. By exploiting LLC leakages, researchers have already shown that it is possible to recover fine grain information such as cryptographic keys from popular software libraries. This makes it essential to verify implementations that handle sensitive data across the many versions and numerous target platforms, a task too complicated, error prone and costly to be handled by human beings. Here we propose a machine learning based technique to classify applications according to their cache access profiles. We show that with minimal and simple manual processing steps feature vectors can be used to train models using support vector machines to classify the applications with a high degree of success. The profiling and training steps are completely automated and do not require any inspection or study of the code to be classified. In native execution, we achieve a successful classification rate as high as 98% (L1 cache) and 78% (LLC) over 40 benchmark applications in the Phoronix suite with mild training. In the cross-VM setting on the noisy Amazon EC2 the success rate drops to 60% for a suite of 25 applications. With this initial study we demonstrate that it is possible to train meaningful models to successfully predict applications running in co-located instances.},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {288–300},
numpages = {13},
keywords = {SVM, cross-vm attacks, machine learning, prime&amp;probe},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@proceedings{10.1145/2427116,
title = {DPG '12: Proceedings of the First Workshop on Design Patterns in Games},
year = {2012},
isbn = {9781450318549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the first Workshop on Design Patterns in Games, held May 29, 2012 and co-located with the 7th International Conference on the Foundations of Digital Games.This workshop focuses on advancing design patterns as a means to formally describe a solution to a game design problem. Design pattern approaches have long been used in diverse fields such as architecture, software engineering, and interaction design. With the emergence of game scholarship, there has been interest in applying design patterns to aspects of game design. There are many potential benefits to design pattern approaches, including generation of frameworks for teaching and communicating about game design and practical usage in brainstorming ideas and tuning designs. Furthermore, deeper understanding of the patterns implicit in their games can help designers explore previously unused ideas and expectations of player behavior.This workshop features presentation of novel research papers followed by group discussion of emerging issues in the study of design patterns and games. We received ten paper submissions, of which we accepted six after a rigorous peer review process. Each paper received a minimum of three reviews. To insure high-quality contributions, each accepted paper was shepherded by a committee member before the submission of the final version. All accepted papers are available in the ACM Digital Library due to our in-cooperation agreement with SIGCHI.},
location = {Raleigh, North Carolina, USA}
}

@inproceedings{10.1145/3295500.3356169,
author = {Li, Ang and Geng, Tong and Wang, Tianqi and Herbordt, Martin and Song, Shuaiwen Leon and Barker, Kevin},
title = {BSTC: A Novel Binarized-Soft-Tensor-Core Design for Accelerating Bit-Based Approximated Neural Nets},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356169},
doi = {10.1145/3295500.3356169},
abstract = {Binarized neural networks (or BNNs) promise tremendous performance improvement over traditional DNNs through simplified bit-level computation and significantly reduced memory access/storage cost. In addition, it has advantages of low-cost, low-energy, and high-robustness, showing great potential in resources-constrained, volatile, and latency-critical applications, which are critical for future HPC, cloud, and edge applications. However, the promised significant performance gain of BNN inference has never been fully demonstrated on general-purpose processors, particularly on GPUs, due to: (i) the challenge of extracting and leveraging sufficient finegrained bit-level-parallelism to saturate GPU cores when the batch size is small; (ii) the fundamental design conflict between bit-based BNN algorithm and word-based architecture; and (iii) architecture &amp; performance unfriendly to BNN network design. To address (i) and (ii), we propose a binarized-soft-tensor-core as a software-hardware codesign approach to construct bit-manipulation capability for modern GPUs and thereby effectively harvest bit-level-parallelism (BLP). To tackle (iii), we propose intra- and inter-layer fusion techniques so that the entire BNN inference execution can be packed into a single GPU kernel, and so avoid the high-cost of frequent launching and releasing. Experiments show that our Singular-Binarized-Neural-Network (SBNN) design can achieve over 1000X speedup for raw inference latency over the state-of-the-art full-precision BNN inference for AlexNet on GPUs. Comparisons with CPU, GPU, FPGA and Xeon-Phi demonstrate the effectiveness of our design. SBNN is opensourced and available at https://github.com/uuudown/SBNN.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {38},
numpages = {30},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{10.1145/3307650.3322251,
author = {Chen, Huili and Fu, Cheng and Rouhani, Bita Darvish and Zhao, Jishen and Koushanfar, Farinaz},
title = {DeepAttest: An End-to-End Attestation Framework for Deep Neural Networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322251},
doi = {10.1145/3307650.3322251},
abstract = {Emerging hardware architectures for Deep Neural Networks (DNNs) are being commercialized and considered as the hardware-level Intellectual Property (IP) of the device providers. However, these intelligent devices might be abused and such vulnerability has not been identified. The unregulated usage of intelligent platforms and the lack of hardware-bounded IP protection impair the commercial advantage of the device provider and prohibit reliable technology transfer. Our goal is to design a systematic methodology that provides hardware-level IP protection and usage control for DNN applications on various platforms. To address the IP concern, we present DeepAttest, the first on-device DNN attestation method that certifies the legitimacy of the DNN program mapped to the device. DeepAttest works by designing a device-specific fingerprint which is encoded in the weights of the DNN deployed on the target platform. The embedded fingerprint (FP) is later extracted with the support of the Trusted Execution Environment (TEE). The existence of the pre-defined FP is used as the attestation criterion to determine whether the queried DNN is authenticated. Our attestation framework ensures that only authorized DNN programs yield the matching FP and are allowed for inference on the target device. DeepAttest provisions the device provider with a practical solution to limit the application usage of her manufactured hardware and prevents unauthorized or tampered DNNs from execution.We take an Algorithm/Software/Hardware co-design approach to optimize DeepAttest's overhead in terms of latency and energy consumption. To facilitate the deployment, we provide a high-level API of DeepAttest that can be seamlessly integrated into existing deep learning frameworks and TEEs for hardware-level IP protection and usage control. Extensive experiments corroborate the fidelity, reliability, security, and efficiency of DeepAttest on various DNN benchmarks and TEE-supported platforms.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {487–498},
numpages = {12},
keywords = {IP protection, attestation, software/hardware co-design, deep neural networks},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@article{10.1145/3533682,
author = {Meiklejohn, Elizabeth and Devlin, Felicita and Dunnigan, John and Johnson, Patricia and Zhang, Joy Xiaoji and Marschner, Steve and Hagan, Brooks and Ko, Joy},
title = {Woven Behavior and Ornamentation: Simulation-Assisted Design and Application of Self-Shaping Woven Textiles},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3533682},
doi = {10.1145/3533682},
abstract = {The class of self-shaping woven textiles are those that undergo a transformation in shape exhibiting three-dimensional behaviors due to the interplay between weave structure and active yarns that shrink, twist or otherwise move during finishing processes such as steaming. When weaving with active yarns to produce dimensional fabrics the unpredictability of the complex interactions involved typically necessitates arduous physical sampling for intentional design and use. Current weaving software, overwhelmingly reliant on 2D graphic depiction of woven fabric, is wholly unable to provide the predictive dimensional appearance of such fabrics that might lead to practical decision making and innovative design solutions. This paper describes an iterative workflow to design self-shaping woven fabrics, from simulation-assisted drafting to the creation of a library of woven behaviors categorized by attributes for seating design. This workflow is then used to inform the design of a new yarn-based simulator as well as to design and fabricate a textile-centric furniture piece in which these woven fabric behaviors and ornamentation are intentionally zoned to the form according to structural, ergonomic and aesthetic considerations.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = {sep},
articleno = {37},
numpages = {12},
keywords = {emergent behavior, textiles, Computation, visualization, design}
}

@inproceedings{10.1145/2048237.2048239,
author = {P\"{u}schel, Markus},
title = {Automatic Performance Programming},
year = {2011},
isbn = {9781450309417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048237.2048239},
doi = {10.1145/2048237.2048239},
abstract = {It has become extraordinarily difficult to write software that performs close to optimally on complex modern microarchitectures. Particularly plagued are domains that are data intensive and require complex mathematical computations such as information retrieval, scientific simulations, graphics, communication, control, and multimedia processing. In these domains, performance-critical components are usually written in C (with possible extensions) and often even in assembly, carefully "tuned" to the platform's architecture and microarchitecture. Specifically, the tuning includes optimization for the memory hierarchy and for different forms of parallelism. The result is usually long, rather unreadable code that needs to be re-written or re-tuned with every platform upgrade. On the other hand, the performance penalty for relying on straightforward, non-tuned, more elegant implementations is typically a factor of 10, 100, or even more. The reasons for this large gap are some (likely) inherent limitations of compilers including the lack of domain knowledge, and the lack of an efficient mechanism to explore the usually large set of transformation choices. The recent end of CPU frequency scaling, and thus the end of free software speed-up, and the advent of mainstream parallelism with its increasing diversity of platforms further aggravate the problem.No promising general solution (besides extensive and expensive hand-coding) to this problem is on the horizon. One approach that has emerged from the numerical computing and compiler community in the last decade is called automatic performance tuning, or autotuning [2, 3, 7--10, 15]. In its most common form it involves the consideration or enumeration of alternative implementations, usually controlled by parameters, coupled with algorithms for search to find the fastest. However, the search space still has to be identified manually, it may be very different even for related functionality, it is not clear how to handle parallelism, and a new platform may require a complete redesign of the autotuning framework.On the other hand, since the overall problem is one of productivity, maintainability, and quality (namely performance) it falls squarely into the domain of software engineering. However, even though a large set of sophisticated software engineering theory and tools exist, it appears that to date this community has not focused much on mathematical computations nor performance in the detailed, close-to-optimal sense above. The reason for the latter may be that performance, unlike various aspects of correctness, is not syntactic in nature (and in reality is often even unpredictable and, well, messy).The aim of this talk is to draw attention to the performance/productivity problem for mathematical applications and to make the case for a more interdisciplinary attack. As a set of thoughts in this direction we offer some of the lessons we have learned in the last decade in our own research on Spiral [1, 11, 12]. Spiral can be viewed as an automatic performance programming framework for a small, but important class of functions called linear transforms. Key techniques used in Spiral include staged declarative domain-specific languages to express algorithm knowledge and algorithm transformations, the use of platform-cognizant rewriting systems for parallelism and locality optimizations, and the use of search and machine learning techniques to navigate possible spaces of choices [4--6, 13, 14, 16]. Experimental results show that the code generated by Spiral competes with, and sometimes outperforms, the best available human-written code. Spiral has been used to generate part of Intel's commercial libraries IPP and MKL.},
booktitle = {Proceedings of the 10th SIGPLAN Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {1–2},
numpages = {2},
keywords = {parallelization, vectorization, domain-specific language, matrix algebra, rewriting, high performance, automation, program generation, fourier transform},
location = {Portland, Oregon, USA},
series = {Onward! 2011}
}

@article{10.1145/3133895,
author = {Sampson, Adrian and McKinley, Kathryn S. and Mytkowicz, Todd},
title = {Static Stages for Heterogeneous Programming},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133895},
doi = {10.1145/3133895},
abstract = {Heterogeneous hardware is central to modern advances in performance and efficiency. Mainstream programming models for heterogeneous architectures, however, sacrifice safety and expressiveness in favor of low-level control over performance details. The interfaces between hardware units consist of verbose, unsafe APIs; hardware-specific languages make it difficult to move code between units; and brittle preprocessor macros complicate the task of specializing general code for efficient accelerated execution. We propose a unified low-level programming model for heterogeneous systems that offers control over performance, safe communication constructs, cross-device code portability, and hygienic metaprogramming for specialization. The language extends constructs from multi-stage programming to separate code for different hardware units, to communicate between them, and to express compile-time code optimization. We introduce static staging, a different take on multi-stage programming that lets the compiler generate all code and communication constructs ahead of time. To demonstrate our approach, we use static staging to implement BraidGL, a real-time graphics programming language for CPU-GPU systems. Current real-time graphics software in OpenGL uses stringly-typed APIs for communication and unsafe preprocessing to generate specialized GPU code variants. In BraidGL, programmers instead write hybrid CPU-GPU software in a unified language. The compiler statically generates target-specific code and guarantees safe communication between the CPU and the graphics pipeline stages. Example scenes demonstrate the language's productivity advantages: BraidGL eliminates the safety and expressiveness pitfalls of OpenGL and makes common specialization techniques easy to apply. The case study demonstrates how static staging can express core placement and specialization in general heterogeneous programming.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {71},
numpages = {27},
keywords = {OpenGL, Multi-stage programming, heterogeneous programming, graphics programming}
}

@inproceedings{10.1145/1411273.1411275,
author = {Arts, Thomas and Castro, Laura M. and Hughes, John},
title = {Testing Erlang Data Types with Quviq Quickcheck},
year = {2008},
isbn = {9781605580654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1411273.1411275},
doi = {10.1145/1411273.1411275},
abstract = {When creating software, data types are the basic bricks. Most of the time a programmer will use data types defined in library modules, therefore being tested by many users over many years. But sometimes, the appropriate data type is unavailable in the libraries and has to be constructed from scratch. In this way, new basic bricks are created, and potentially used in many products in the future. It pays off to test such data types thoroughly.This paper presents a structured methodology to follow when testing data types using Quviq QuickCheck, a tool for random testing against specifications. The validation process will be explained carefully, from the convenience of defining a model for the datatype to be tested, to a strategy for better shrinking of failing test cases, and including the benefits of working with symbolic representations.The leading example in this paper is a data type implemented for a risk management information system, a commercial product developed in Erlang, that has been used on a daily basis for several years.},
booktitle = {Proceedings of the 7th ACM SIGPLAN Workshop on ERLANG},
pages = {1–8},
numpages = {8},
keywords = {quickcheck, erlang, datatypes},
location = {Victoria, BC, Canada},
series = {ERLANG '08}
}

@inproceedings{10.1145/2909437.2909441,
author = {Aviv, Rotem and Wang, Guohui},
title = {OpenCL-Based Mobile GPGPU Benchmarking: Methods and Challenges},
year = {2016},
isbn = {9781450343381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909437.2909441},
doi = {10.1145/2909437.2909441},
abstract = {Benchmarking general-purpose computing on graphics processing unit (GPGPU) aims to profile and compare performance across different devices. Due to the low-level nature of most GPGPU APIs, GPGPU benchmarks are also useful for architectural exploration and program optimization. This can be challenging in mobile devices due to lack of underlying hardware details and limited profiling capabilities in some platforms. Measuring the performance of mobile GPU by executing benchmarks covering major hardware and software features can reveal the strength and weakness of a GPGPU system, enable better program optimization and make automatic performance tuning possible.In this paper, we will describe several design methods of OpenCL-based mobile GPGPU benchmarking, and discuss key issues that one may encounter during development. We will also present design tips and guidelines to achieve more "fair" and accurate benchmarking results.},
booktitle = {Proceedings of the 4th International Workshop on OpenCL},
articleno = {3},
numpages = {4},
keywords = {Performance analysis, OpenCL, Optimization, Benchmarking, Mobile GPGPU},
location = {Vienna, Austria},
series = {IWOCL '16}
}

@inproceedings{10.1145/1185448.1185636,
author = {Gurupur, Varadraj and Tanik, Urcun J},
title = {Software Cultivation Using the Artificial Intelligence Design Framework},
year = {2006},
isbn = {1595933158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185448.1185636},
doi = {10.1145/1185448.1185636},
abstract = {All along the history of software engineering, traditional software development process has always been a labor intensive process. This is perhaps because we are still in the preliminary evolutionary stage of software production where the software has to be built by a group of software developers either from scratch or by combining and/or reusing the components that have already been developed. In this paper we propose a unique method of building software in a way that is analogous to the growth of any organism to the stage of an adult. In this process of software cultivation, the initial framework of the software system is built by a group of developers and then the system enhances its functionality by gathering domain knowledge from various regions of the Internet by using the Artificial Intelligence Design Framework (AIDF). The internal development of the system is guided by various design theories and methods such as Axiomatic design Theory (ADT) and Design Structure Matrix (DSM). The concept of AIDF was first presented to build a domain expert system in the field of Optical Backplane Engineering, but it was later found that it could be used to solve a wide range of problems including software development. In order to build a system that would design another system various methodologies and theories which have been proved and tested had to be utilized to build the framework for designing a system.Software development has been a process where lot of time and money has to be spent not only on the development process but also on a never-ending process of software maintenance. Sometimes this development process could face obstacles such as the creeping requirements problem where the requirements may change while the software development is still in process. Many software development processes may end in failure owing to the moving target problem due to the rapid change in the domain knowledge. Another way of looking at this problem would be that the domain expert will have to add changes to the domain expert system whenever the domain knowledge undergoes change in some form.The solution to this problem would be a migration from the process of software development to a process of software cultivation. This cultivation would be analogous to the cultivation of a plant in a pot, where we sow the seed and keep adding water and fertilizer to it. In this methodology, we create software and allow the software development framework to handle the process of software maintenance. Here, the process of software maintenance gets transformed into a process of software cultivation where the system consumes networked domain knowledge from the Internet and updates the software to meet the most recent requirements for a given problem domain. For instance, if we have developed a computer based system that helps in the design of optical fiber networks, a recent invention in that field could be used to update the system behavior.As part of the overall artificial intelligence approach, knowledge based engineering (KBE) plays an important role in the AIDF. There are many benefits of using KBE over traditional methods. The time to produce an engineering artifact can be substantially reduced by eliminating tedious engineering duties through automated reasoning. The wisdom of the organization and key people can be retained and accessed by future generations of workers. Concurrent engineering can be accomplished when disparate departments can meld their work together by working in parallel, as opposed to sequentially, enabled by a KBE system intelligently updating files in the background according to its programmed logic. The entire design process will become fully documented, pulling together scattered annotations, designs, suggestions, and established heuristics. Using new technologies, such as the Semantic Web, the KBE engineering system would enable designers to search repositories of parts globally, based on a sophisticated pre-recorded rule-base.One of the key recommendations made by the 2001 report Improving Engineering Design by the National Academy of Sciences was that "decision-making tools and decision theory" should be emphasized. In order to prevent problems occurring during the crucial conceptual stage of the design process, a framework needs to be created that enables the systematic collection, storage, and application of expert knowledge and state-of-the-art practices through machine intelligence. Addressing Suh's future projection of a "Thinking Design Machine" using software to assist engineers at the concept design stage, an Artificial Intelligence Design Framework (AIDF) is introduced. In order to help engineers get their design right the first time, this theory was shown to methodically create a valid design at the concept stage based on a set of axioms, corollaries, and theorems. In order to provide expert advice based on valid design theory, an AIDF is advanced using KBE techniques that capture expert domain knowledge and couple it to the Internet through the use of search agents and ontologies on the Semantic Web. An automated approach intended to get the design right the first time is accomplished by applying machine intelligence to sound axiomatic design theory. This validated approach is structured to prevent costly remedial work done at the end of the design cycle to correct for errors made in the conceptual design phase.The AIDF creates an efficient framework for design and redesign of a system. When a system is built from scratch and the domain knowledge is available on the Internet, sufficient initial design information should be made available to the AIDF. The AIDF will store this knowledge in the form of ontologies and RDF in the knowledge base and the database. The AIDF draws this information from the web agents and domain experts. It is the responsibility of the web agent to extract reliable information from the web, which can be greatly enhanced with Web Services and Semantic Web Technology. Once the information is stored in the AIDF, the AIDF starts rebuilding the software. First it redesigns the software and verifies the correctness of the design according to its rule-base and risk-mitigation algorithms. This verification is carried out in two stages. First the AIDF carries out the required analysis on the design and then if it finds that the design is fault free, it requests the user to verify the design. Once the design has been verified the code generator will generate the required code. If the designer considers that the design was inadequate or faulty, then the designer who also happens to be the user of the system makes recommendations into the system by considering the design as invalid. We need to bear in mind that the AIDF connects itself to the available domain knowledge available on the Internet and supplied directly by the domain expert. Of course, the AIDF does not intend to blindly pull a piece of code available to it and add it to the existing software without filtering through its rule-base and risk-mitigation algorithms. The idea of software cultivation using the AIDF is still in its embryonic stage and therefore, software quality assurance and other aspects associated with the process of software development could be dealt with as our research on software cultivation progresses.The knowledge base in Jena is represented in terms of RDF and ontologies. The ontologies provide a basis for the representation of domain knowledge. The RDF acts as a container for specifications and an excellent way of storing structured metadata. The domain knowledge of a particular system is very much subject to change and any change in the domain knowledge should be recorded and represented in a universally accepted way. Resource Description Framework (RDF) appears to be an excellent way of storing metadata. Jena, the Java technology tool for the Semantic Web, plays an integral role in this process. Jena identifies resources using the Uniform Resource Identifier (URI). The difference between the URI and the URL is that the URI can only identify a resource on the Web and is used predominantly in Web Services, whereas a URL not only identifies a resource on the Web but also locates it. This URI concept is used extensively in Jena. Jena contains library functions to build an RDF model. The complexity analysis for risk-mitigation and internal verification process is carried out using a combination of various analytical methodologies such as Conant theory, Axiomatic Design Theory, and Design Structure Matrix, in addition to risk analysis using methods such as Fault Tree Analysis (FTA) and Failure Mode and Effects Analysis (FMEA).This concept of building an AIDF requires some challenges to be met, most of which emanates from the reliability of the information available on the Internet. An immediate answer to that question would be that the AIDF accepts information that will fit into the system similar to that of a jigsaw puzzle. Sufficient amount of mathematical analysis will be applied on the incoming information content. On top of this, the web agents will derive domain information only from known sources. The AIDF is still in a conceptual phase and we are still looking at the technologies that will be used in building the AIDF.},
booktitle = {Proceedings of the 44th Annual Southeast Regional Conference},
pages = {786–787},
numpages = {2},
location = {Melbourne, Florida},
series = {ACM-SE 44}
}

@article{10.1145/3547141,
author = {Alam, Syed Asad and Gregg, David and Gambardella, Giulio and Preusser, Thomas and Blott, Michaela},
title = {On the RTL Implementation of FINN Matrix Vector Unit},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1539-9087},
url = {https://doi.org/10.1145/3547141},
doi = {10.1145/3547141},
abstract = {FPGA-based accelerators are becoming increasingly popular for deep neural network inference due to their ability to scale performance with increasing degree of specialization with dataflow architectures or custom data type precision. In order to reduce the barrier for software engineers and data scientists to adopt FPGAs, C++- and OpenCL-based design entries with high-level synthesis (HLS) have been introduced. They provide higher abstraction compared to register-transfer level (RTL)-based design. HLS offers faster development time, better maintainability and more flexibility in code exploration, when evaluating several options for multi-dimension tensors, convolutional layers or different degrees of parallelism. For this reason, HLS has been adopted by DNN accelerator generation frameworks such as FINN and hls4ml. In this paper, we present an alternative backend library for FINN, leveraging RTL. We investigate and evaluate, across a spectrum of design dimensions, the pros and cons of an RTL-based implementation versus the original HLS variant. We show that for smaller design parameters, RTL produces significantly smaller circuits as compared to HLS. For larger circuits, however, the look-up table (LUT) count of RTL-based design is slightly higher, up to around (15% ) . On the other hand, HLS consistently requires more flip-flops (FFs) (with an orders-of-magnitude difference for smaller designs) and block RAMs (BRAMs) (2 \texttimes{} more). This also impacts the critical path delay, with RTL producing significantly faster circuits, up to around (80% ) . Furthermore, RTL also benefits from at-least a 10 \texttimes{} reduction in synthesis time. Finally, the results were validated in practice using two real-world use cases, one of a multi-layer perceptron (MLP) used in network intrusion detection and the other a convolution network called ResNet used in image recognition. Overall, since HLS frameworks code-generate the hardware design, the benefits of the ease in the design entry is less important. As such, the gained benefits in synthesis time together with some design-dependent resource benefits, make the RTL abstraction an attractive alternative.},
note = {Just Accepted},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jul},
keywords = {RTL, FPGA, FINN, HLS, Convolutional neural network}
}

@inproceedings{10.1145/2038642.2038652,
author = {Kim, Hyosu and Lee, Minsub and Han, Wookhyun and Lee, Kilho and Shin, Insik},
title = {Aciom: Application Characteristics-Aware Disk and Network i/o Management on Android Platform},
year = {2011},
isbn = {9781450307147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038642.2038652},
doi = {10.1145/2038642.2038652},
abstract = {The last several years have seen a rapid increase in smart phone use. Android offers an open-source software platform on smart phones, that includes a Linux-based kernel, Java applications, and middleware. The Android middleware provides system libraries and services to facilitate the development of performance-sensitive or device-specific functionalities, such as screen display, multimedia, and web browsing. Android keeps track of which applications make use of which system services for some pre-defined functionalities, and which application is running in the foreground attracting the user's attention. Such information is valuable in capturing application characteristics and can be useful for resource management tailored to application requirements. However, the Linux-based Android kernel does not utilize such information for I/O resource management. This paper is the first work, to the best of our knowledge, to attempt to understand application characteristics through Android architecture and to incorporate those characteristics into disk and network I/O management. Our proposed approach, Aciom (Application Characteristics-aware I/O Management), requires no modification to applications and characterizes application I/O requests as time-sensitive, bursty, or plain, depending on which system services are involved and which application receives the user's focus. Aciom then provides differentiated I/O management services for different types of I/O requests, supporting minimum bandwidth reservations for time-sensitive requests and placing maximum bandwidth limits on bursty requests. We present the design of Aciom and a prototype implementation on Android. Our experimental results show that Aciom is quite effective in handling disk and network I/O requests in support of time-sensitive applications in the presence of bursty I/O requests.},
booktitle = {Proceedings of the Ninth ACM International Conference on Embedded Software},
pages = {49–58},
numpages = {10},
keywords = {android platform, i/o management, application characteristics awareness},
location = {Taipei, Taiwan},
series = {EMSOFT '11}
}

@inproceedings{10.1109/ISCA52012.2021.00042,
author = {Ye, Chencheng and Xu, Yuanchao and Shen, Xipeng and Liao, Xiaofei and Jin, Hai and Solihin, Yan},
title = {Supporting Legacy Libraries on Non-Volatile Memory: A User-Transparent Approach},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00042},
doi = {10.1109/ISCA52012.2021.00042},
abstract = {As mainstream computing is poised to embrace the advent of byte-addressable non-volatile memory (NVM), an important roadblock has remained largely unnoticed, support of legacy libraries on NVM. Libraries underpin modern software everywhere. As current NVM programming interfaces all designate special types and constructs for NVM objects and references, legacy libraries, being incompatible with these data types, will face major obstacles for working with future applications written for NVM. This paper introduces a simple approach to mitigating the issue. The novel approach centers around user-transparent persistent reference, a new concept that allows programmers to reference a persistent object in the same way as reference a normal (volatile) object. The paper presents the implementation of the concept, carefully examines its soundness, and describes compiler and simple architecture support for keeping performance overheads very low.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {443–455},
numpages = {13},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1145/2484762.2484778,
author = {You, Haihang and Lu, Charng-Da and Zhao, Ziliang and Xing, Fei},
title = {Optimizing Utilization across XSEDE Platforms},
year = {2013},
isbn = {9781450321709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484762.2484778},
doi = {10.1145/2484762.2484778},
abstract = {HPC resources provided by XSEDE give researchers unique opportunities to carry out scientific studies. As of 2013 XSEDE consists of 16 systems with varied architectural designs and capabilities. The hardware heterogeneity and software diversity make efficient utilization of such a federation of computing resources very challenging. For example, users are constantly faced with a myriad of possibilities to build and run an application: compilers, numerical libraries, and runtime parameters. In this paper we report performance data of several popular scientific applications built with different compilers and numerical libraries available on two XSEDE systems: Kraken and Gordon, and suggest the best way to compile applications for optimal performance. By comparison, we validate SU conversion factors between the aforementioned XSEDE systems from application's viewpoint.},
booktitle = {Proceedings of the Conference on Extreme Science and Engineering Discovery Environment: Gateway to Discovery},
articleno = {47},
numpages = {8},
keywords = {compilers, performance, benchmarking, numerical libraries},
location = {San Diego, California, USA},
series = {XSEDE '13}
}

@inproceedings{10.1145/1146909.1146980,
author = {Martin, Grant},
title = {Overview of the MPSoC Design Challenge},
year = {2006},
isbn = {1595933816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1146909.1146980},
doi = {10.1145/1146909.1146980},
abstract = {We review the design challenges faced by MPSoC designers at all levels. Starting at the application level, there is a need for programming models and communications APIs that allow applications to be easily re-configured for many different possible architectures without tedious rewriting, while at the same time ensuring efficient production code. Synchronisation and control of task scheduling may be provided by RTOS's or other scheduling methods, and the choice of programming and threading models, whether symmetric or asymmetric, has a heavy influence on how best to control task or thread execution. Debugging MP systems for the typical application developer becomes a much more complex job, when compared to traditional single-processor debug, or the debug of simple MP systems that are only very loosely coupled. The interaction between the system, applications and software views, and processor configuration and extension, adds a new dimension to the problem space. Zeroing in on the optimal solution for a particular MPSoC design demands a multi-disciplinary approach. After reviewing the design challenges, we end by focusing on the requirements for design tools that may ameliorate many of these issues, and illustrate some of the possible solutions, based on experiments.},
booktitle = {Proceedings of the 43rd Annual Design Automation Conference},
pages = {274–279},
numpages = {6},
keywords = {system-level design, multi-processor system-on-chip, MPSoC},
location = {San Francisco, CA, USA},
series = {DAC '06}
}

@article{10.1007/s00165-020-00511-6,
author = {Huang, Yanhong and Pang, Haiping and Shi, Jianqi},
title = {Modeling and Verification of A Timing Protection Mechanism in the OSEK/VDX OS Using CSP},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-020-00511-6},
doi = {10.1007/s00165-020-00511-6},
abstract = {The functions of automobiles are becoming increasingly intelligent, which leads to the increasing number of electrical control units for one automobile. Hence, it makes software migration and extension more complicated. In order to avoid these problems, the standard OSEK/VDX has been proposed jointly by a German automotive company consortium and the University of Karlsruhe. This standard provides specifications for the development of automotive software, this standard has become one of the major standards for real-time automotive operating systems (OSs). Since errors in the automotive OS may pose threat to the safety of people in a vehicle, it is necessary to verify the correctness of the OSEK OS which is used by many manufacturers around the world. Formal methods can be adopted to verify the correctness of both software and hardware. Therefore, we propose a formal model of the OSEK OS at the code level and verify three significant properties of the OSEK-based system. In this study, the code-level OSEK OS is verified to ensure compliance with the specifications. An automotive OS always requires that the systemreacts in a timelymanner to external events and performs the computations within the timing constraints. However, there is a possibility that the running time of the tasks exceeds the timing requirements due to the complexity of the tasks. Therefore, by referring to one of the extensions of the OSEK OS, Automotive Open System Architecture (AUTOSAR), we proposed tpOSEK, which is capable of extending the OSEK OS with a timing protection mechanism in AUTOSAR in this study. In our previous study, it was verified that the higher-priority task cannot be preempted by lower-priority tasks. In this paper, after improvement made to the OSEK OS model by adding interrupt service routine models and alarms, and extension of the OSEK OS model with a timing protection model, we have verified that tpOSEK satisfies three significant properties, which include deadlock f ree, complete and no timeout. These properties represent the basic conditions for the systemto run smoothly. If such properties as deadlock f ree and complete are satisfied, it means no deadlock is encountered by this system and all of the tasks can be scheduled completely. Moreover, if the property timeout cannot be satisfied, it means that none of the tasks would miss the deadline. Based on the tpOSEK model, the correct timing protection APIs can be designed at the code level. Thus, by extending the OSEKOSwith theseAPIs,we can update theOSEKOS faster and the need tomodify the dependent applications can be removed. Furthermore, we have constructed formal models for two industrial cases based on tpOSEK OS to demonstrate the soundness of our methods.},
journal = {Form. Asp. Comput.},
month = {feb},
pages = {113–145},
numpages = {33},
keywords = {Verification, Timing protection, Modeling, Operating system}
}

@inproceedings{10.1145/1512475.1512489,
author = {Shonle, Macneil and Griswold, William G. and Lerner, Sorin},
title = {Addressing Common Crosscutting Problems with Arcum},
year = {2008},
isbn = {9781605583822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1512475.1512489},
doi = {10.1145/1512475.1512489},
abstract = {Crosscutting is an inherent part of software development and can typically be managed through modularization: A module's stable properties are defined in an interface while its likely-to-change properties are encapsulated within the module [19]. The crosscutting of the stable properties, such as class and method names, can be mitigated with automated refactoring tools that allow, for example, the interface's elements to be renamed [9, 18]. However, often the crosscutting from design idioms (such as design patterns and coding styles) are so specific to the program's domain that their crosscutting would not likely have been anticipated by the developers of an automated refactoring system.The Arcum plug-in for Eclipse enables programmers to describe the implementation of a crosscutting design idiom as a set of syntactic patterns and semantic constraints. Arcum can process declarations of related implementations and infer the refactoring steps necessary to transform a program from using one implementation to its alternatives. As a result, automating refactoring for domain-specific crosscutting design idioms can be easy and practical. This paper presents a case study of how Arcum was used to mitigate four classic software engineering problems that are exacerbated by crosscutting: library migration, debugging, programmer-defined semantic checking, and architectural enforcement.},
booktitle = {Proceedings of the 8th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering},
pages = {64–69},
numpages = {6},
keywords = {refactoring, design patterns, aspect-oriented programming},
location = {Atlanta, Georgia},
series = {PASTE '08}
}

@inproceedings{10.1145/1159861.1159863,
author = {Gibbons, Jeremy},
title = {Design Patterns as Higher-Order Datatype-Generic Programs},
year = {2006},
isbn = {1595934928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159861.1159863},
doi = {10.1145/1159861.1159863},
abstract = {Design patterns are reusable abstractions in object-oriented software. However, using current mainstream programming languages, these elements can only be expressed extra-linguistically: as prose, pictures, and prototypes. We believe that this is not inherent in the patterns themselves, but evidence of a lack of expressivity in the languages of today. We expect that, in the languages of the future, the code parts of design patterns will be expressible as reusable library components. Indeed, we claim that the languages of tomorrow will suffice; the future is not far away. All that is needed, in addition to commonly-available features, are higher-order and datatype-generic constructs; these features are already or nearly available now. We argue the case by presenting higher-order datatype-generic programs capturing ORIGAMI, a small suite of patterns for recursive data structures.},
booktitle = {Proceedings of the 2006 ACM SIGPLAN Workshop on Generic Programming},
pages = {1–12},
numpages = {12},
keywords = {generic programming, unfolds, design patterns, functional programming, higher-order functions, folds},
location = {Portland, Oregon, USA},
series = {WGP '06}
}

@proceedings{10.5555/3019040,
title = {PAW '16: Proceedings of the First Workshop on PGAS Applications},
year = {2016},
isbn = {9781509052141},
publisher = {IEEE Press},
abstract = {In an effort to enable extreme-scale computing, system architectures and technologies are undergoing dramatic transformations, involving complex memory hierarchies, hardware heterogeneity, many- and multi-core architectures, and different levels of parallelization. In order to effectively exploit these transformations when producing scalable applications, scientific researchers must increasingly have cross-cutting technical expertise in hardware, software, and algorithm development. Given this landscape, programming models that provide scientific researchers with a more effective approach for developing their parallel applications are of the utmost importance.The partitioned global address space (PGAS) parallel programming model effectively combines the productive data reference semantics of shared memory systems with a strong locality model in which programmers can reason about the mapping of data to distributed memories. This combination of features simplifies programming complexity while enhancing performance by supporting shared namespaces in a way that exposes data locality in support of scalability. Many PGAS models rely on the single-program, multiple-data (SPMD) programming model commonly used for distributed memory programming. Others provide more of a global view of execution in which language concepts are used to map data and control structures to the target nodes.There are many programming languages within the PGAS family, including Fortran 2008, Unified Parallel C (UPC), X10, and Chapel. Such languages provide users with rich syntax for expressing locality-aware parallel computations. By embedding PGAS concepts into a language's type system and execution semantics, compilers can be leveraged to help with error checking and optimization. PGAS models also take the form of meta-languages and libraries, such as Unified Parallel C++ (UPC++), Coarray C++, OpenSHMEM, MPI-3, and Global Arrays. These have the benefit of being integrated with existing languages, simplifying the learning curve for existing programmers.PGAS Applications Workshop (PAW) includes work from case studies of PGAS programming models in the context of real-world applications as a means of better understanding practical applications of PGAS technologies. The work characterizes the scalability and performance, expressiveness and programmability, as well as any downsides or areas for improvement in existing PGAS models.},
location = {Salt Lake City, Utah}
}

@proceedings{10.1145/1383559,
title = {WOSP '08: Proceedings of the 7th International Workshop on Software and Performance},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {WOSP provides a forum at the intersection of software and performance to bring together software engineers, developers, performance analysts and modelers and to generate insights into the difficult and urgent problems in this area. Modern applications add new challenges every day, and demand new approaches for modeling and analyzing the performance of design alternatives in all the phases of the software lifecycle. We have to cope with distributed and mobile applications, sometimes with heterogeneous and intermittent connections, increasing system complexity, rapidly evolving software technologies, short time to market, incomplete documentation, and poorly defined requirements.This is the seventh in the series of International Workshops on Software and Performance that began almost ten years ago in Santa Fe, New Mexico, in 1998. The research agenda identified at that meeting has in fact propelled the research field in the decade since, notably in the area of performance evaluation of designs. But new challenges have emerged, particularly in the provision of flexible services with strong time-delay requirements over the web and over wireless connections.This year's technical program is as strong as ever. We received 39 submissions which were thoroughly reviewed by three or four reviewers from an expert program committee. From these 15 were accepted as research papers, 2 as experience papers and 3 as short papers. The workshop is structured in 10 sessions over 3 days. Session topics range from Performance Diagnosis and Improvement to Modeling from Component Libraries. A particular strength of this program is its combination of papers on measurement and on modeling. We are confident that this body of knowledge will advance the state of the art in the field and contribute to bridging the gap between research and industrial practice. Only the synergy between these two worlds can provide real progress.This program echoes the first workshop in providing ample opportunities for open discussion of issues and research directions, prompted by panels. We have identified three areas as key to the future development of the field: (1) identifying new industrial issues, (2) testing, empirical assessments and benchmarks, (3) unifying the use of measurement and models.},
location = {Princeton, NJ, USA}
}

@inproceedings{10.1109/CCGrid.2014.100,
author = {Agarwal, Dinesh and Karamati, Sara and Puri, Satish and Prasad, Sushil K.},
title = {Towards an MPI-like Framework for the Azure Cloud Platform},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.100},
doi = {10.1109/CCGrid.2014.100},
abstract = {Message Passing Interface (MPI) has been the predominant standardized system for writing parallel and distributed applications. However, while MPI has been the software system of choice for traditional parallel and distributed computing platforms such as large compute clusters and Grid, MPI is not the system of choice for cloud platforms. The primary reasons for this is the lack of low latency high bandwidth network capabilities of the cloud platforms and the inherent architectural differences from traditional compute clusters. Prior studies suggest that the message latency of cloud platforms could be as much as 35x slower than that of an infiniband-connected cluster [1] for popular MPI implementations. MPI-like environment on cloud platforms is desirable for a large class of applications that run for long time spans with varying computing needs, such as the modeling and analysis to predict swath of a hurricane. Such applications could benefit from cloud's resiliency and on-demand access for a robust and green solution. Interestingly, most of the cloud vendors provide APIs to access cloud resources in an efficient manner different than how an MPI implementation would avail of those resources. We have done extensive research to identify the pain-points for designing and implementing an MPI-like framework for cloud platforms. Our research has provided us with vital guidelines that we are sharing in this paper. We present the details of the key components required for such a framework along with our experience while implementing a preliminary MPI-like framework over Azure dubbed cloudMPI and evaluate its pros and cons. A large GIS application has been ported over cloudMPI to study its effectiveness and limitations.1},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {176–185},
numpages = {10},
keywords = {programming productivity, polygonal overlay processing in GIS, porting MPI code to Azure cloud, MPI over cloud},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1145/277044.277181,
author = {Newton, A. Richard},
title = {Technical Challenges of IP and System-on-Chip (Panel): The ASIC Vendor Perspective},
year = {1998},
isbn = {0897919645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/277044.277181},
doi = {10.1145/277044.277181},
abstract = {The vision of easily accessible IP that can be quickly integrated on silicon as “virtual components” is a compelling one, with deep implications for reuse methodology and EDA technology. Activities of the VSI Alliance, starting nearly two years ago, have fueled interest in IP and raised market expectations for value and reusability to very high levels. Indeed, the sheer number of new IP companies, in combination with third-party ASIC libraries and EDA tools offerings, suggests that the era of plug and play IP has arrived. Today, the claimed independence of IP from the underlying silicon has led to bold market claims for foundry manufactured system chips using internet sourced IP with Lego block, snap-together integration simplicity. Traditional roles of foundries, ASIC suppliers, and EDA vendors are blurring. To the end customer, the situation presents both opportunities and risks. Are we on the cusp of a new era, or a rude awakening?From the perspective of leading ASIC vendors, the evolution of IP-based systems on silicon, along with the means and methods of producing them, are not new. Full use of rapidly-increasing raw ASIC gate counts — which is synonymous with systems on silicon — has been a driving force in the ASIC industry for many years. As developers of high-value IP and systems architectures, the leading ASIC companies have an established record of experience with design and system chip level integration. This experience — e.g., with system-level analysis, integration of cores designed to common bus-structures, etc. — can be leveraged toward practical realization of an IP vision that offers the silicon consumer more choice without unacceptable implementation risks.On the other hand, from the perspective of a pure IP integrator, IP is poised to change the way customers and ASIC vendors themselves produce systems-on-silicon, just as vendor-specific tools evolved to commercial CAE starting a decade ago. Early IP success stories are pointing the way to a new, more specialized business model where IP, tools, services, and silicon foundry are brought together based on the specific needs of a given project.This panel will address technical challenges presented by the systems-chip opportunity, as well as practical expectations and key missing pieces of industry infrastructure. Such pieces include: customer expectations, EDA technology, standards, legal barriers and associated risks facing ASIC suppliers and EDA vendors, challenges of incorporating 3rd-party IP, and practical reuse methodologies. In this forum, a noteworthy challenge — which has remained constant throughout the evolution to deep-submicron — is the ability for customers to develop their systems when faced with a widening gap between process technology and commercial EDA tools. As silicon suppliers continue to focus on providing the industry with advanced technologies and products, EDA suppliers must also increase their focus on providing tools and infrastructure. Another example challenge, faced by ASIC vendors, lies in supplying the high-level core models required for emerging cycle-based simulators, and supporting the use of cores in emulators and accelerators, when no standards for model creation or protection exist. Standard model interfaces such as those that exist for event-based simulators must be developed to support cycle simulators, emulators, accelerators as well as hardware-software co-verification tools. The panel will also examine essential factors to consider in the use of IP, based on their experience with the latest silicon process technologies.Finally, the ASIC vendor participants will showcase their joint efforts to build an effective IP infrastructure for the industry.},
booktitle = {Proceedings of the 35th Annual Design Automation Conference},
pages = {501},
location = {San Francisco, California, USA},
series = {DAC '98}
}

@proceedings{10.1145/1013115,
title = {DIS '04: Proceedings of the 5th Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques},
year = {2004},
isbn = {1581137877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to DIS2004, the ACM conference on Designing Interactive Systems. We have a fantastic program of papers, posters, panels, lab tours, exhibits and design competition packed into three days. A veritable feast of the best examples of interactive systems design with some challenging and provocative views on the processes and theories of the design of interactive media.One of the key locations in the development of interactive media is the Massachusetts Institute of Technology, MIT and we are very pleased to have William J Mitchell, Dean of Architecture providing the opening plenary talk. Many people trace the origin of interaction design to 1989 when Gillian Crampton Smith initiated the first masters degree program at the Royal College of Art in London. Gillian moved to start another design centre at Ivrea in Italy in 1999 and we are very pleased to have Gillian as our closing speaker. We are also very pleased to have her successor at the RCA, Irene McAra-McWilliam as one of our program co-chairs and Dan Gruen from IBM as the other.The DIS series of conferences started in 1995 just about the same time that a very influential book came out. Terry Winograd collected together an excellent set of pioneers in this field in his book 'Bringing Design to Software'. Since then there has been DIS97, DIS2000, DIS2002 and now DIS2004. Each of these conferences has helped to develop and shape the field by bringing designers and software developers together.In developing the themes and concepts that we felt were important to DIS2004, we have tried to achieve a balance. We have a balance between Europe and America, between male and female and between interaction design and human-computer interaction (HCI), between design and engineering. DIS2004 seeks to cross the spectrum of contributions to this rich and fascinating area. The subject of Designing Interactive Systems concerns people from right across the spectrum of professions and subject areas. When we originally brainstormed who DIS2004 was aimed at we came up with: User Experience designers seeking to go beyond usability; Interaction designers both in the large involved with community building and in the small developing interactive objects and installations; Usability people who want to make experience for the 'users' better; Web designers who want to build better web sites; Information architects; User interface designers across the board - desk top systems, mobile devices, interactive products; Cognitive scientists; Social scientists; Human Factors people; Games designers involved with characters, narrative and game play; Artist designers challenging concepts and creating new art forms; Product designers; Functional designers concerned with market needs and platform constraints; Visual designers concerned with information design and what their system will look like; Ethnographers; Customer service people; Documentation people; Software engineers; Technical engineers; Physical form engineers; and last (but not least) academics in HCI, from design schools, from art schools, from cultural studies, from business.It was this breadth that gave us the idea for the theme of 'across the spectrum' The design of interactive media involves so many different types of people and so many different issues. In DIS2004 we embrace the diversity of interests in this new community, crossing the spectrum of contributing disciplines including business, technology and engineering, psychology, interaction, design, expressive media and cultural anthropology. We see DIS2004 as a step towards achieving a fusion of art and science. The role of design is spread across a spectrum of disciplines, each providing their own knowledge and techniques. Designers of interactive systems work with and through a medium of technical arts blending and exploring synergies from diverse areas.},
location = {Cambridge, MA, USA}
}

@inproceedings{10.1145/1708046.1708058,
author = {Matthews, David C.J. and Wenzel, Makarius},
title = {Efficient Parallel Programming in Poly/ML and Isabelle/ML},
year = {2010},
isbn = {9781605588599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1708046.1708058},
doi = {10.1145/1708046.1708058},
abstract = {The ML family of languages and LCF-style interactive theorem proving have been closely related from their beginnings about 30 years ago. Here we report on a recent project to adapt both the Poly/ML compiler and the Isabelle theorem prover to current multicore hardware. Checking theories and proofs in typical Isabelle application takes minutes or hours, and users expect to make efficient use of "home machines" with 2-8 cores, or more.Poly/ML and Isabelle are big and complex software systems that have evolved over more than two decades. Faced with the requirement to deliver a stable and efficient parallel programming environment, many infrastructure layers had to be reworked: from low-level system threads to high-level principles of value-oriented programming. At each stage we carefully selected from the many existing concepts for parallelism, and integrated them in a way that fits smoothly into the idea of purely functional ML with the addition of synchronous exceptions and asynchronous interrupts.From the Isabelle/ML perspective, the main concept to manage parallel evaluation is that of "future values". Scheduling is implicit, but it is also possible to specify dependencies and priorities. In addition, block-structured groups of futures with propagation of exceptions allow for alternative functional evaluation (such as parallel search), without requiring user code to tackle concurrency. Our library also provides the usual parallel combinators for functions on lists, and analogous versions on prover tactics.Despite substantial reorganization in the background, only minimal changes are occasionally required in user ML code, and none at the Isabelle application level (where parallel theory and proof processing is fully implicit). The present implementation is able to address more than 8 cores effectively, while the earlier version of the official Isabelle2009 release works best for 2-4 cores. Scalability beyond 16 cores still poses some extra challenges, and will require further improvements of the Poly/ML runtime system (heap management and garbage collection), and additional parallelization of Isabelle application logic.},
booktitle = {Proceedings of the 5th ACM SIGPLAN Workshop on Declarative Aspects of Multicore Programming},
pages = {53–62},
numpages = {10},
keywords = {isabelle, theorem proving applications, parallel standard ml, data parallelism, poly/ml},
location = {Madrid, Spain},
series = {DAMP '10}
}

@inproceedings{10.1145/3373087.3375330,
author = {Bezati, Endri and Emami, Mahyar and Larus, James},
title = {Advanced Dataflow Programming Using Actor Machines for High-Level Synthesis},
year = {2020},
isbn = {9781450370998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373087.3375330},
doi = {10.1145/3373087.3375330},
abstract = {The use of parallelism has increased drastically in recent years. Parallel platforms come in many forms: multi-core processors, embedded hybrid solutions such as multi-processor system-on-chip with reconfigurable logic, and cloud datacenters with multi-core and reconfigurable logic. These heterogeneous platforms can offer massive parallelism, but it can be difficult to exploit, particularly when combining solutions constructed with multiple architectures. To program a heterogeneous platform, a developer must master different programming languages, tools, and APIs to program each aspect of platform separately and then must find a means to connect them with communication interfaces. The motivation of this work is to provide a single programming model and framework for hardware-software stream programs on heterogeneous platforms. Our framework, StreamBlocks, starts with a dataflow programming model for both embedded and datacenter platforms. Dataflow programming is an alternative model of computation that captures both data and task parallelism. We describe a compiler infrastructure for CAL dataflow programs for hardware code generation. CAL is a dataflow programming language that can express multiple dataflow models of computation. StreamBlocks is based on the Tycho compiler infrastructure, which transforms each actor in a dataflow program to an abstract machine model, called Actor Machine. Actor Machines provides a unified model for executing actors in both hardware and software and permit our compiler extension and backend to generate efficient FPGA code. Unlike other systems, the programming model and compiler directly support hardware-software systems in which an FPGA functions as a coprocessor to a CPU. This permits easy integration with existing workflows.},
booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {310},
numpages = {1},
keywords = {fpga, stream programming, hls, actor machine, opencl, cal, dataflow},
location = {Seaside, CA, USA},
series = {FPGA '20}
}

@inproceedings{10.1145/1062455.1062471,
author = {Siegel, Jon},
title = {Why Use the Model Driven Architecture to Design and Build Distributed Applications?},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062471},
doi = {10.1145/1062455.1062471},
abstract = {OMG's Model Driven Architecture® (MDA®)[1] unifies and simplifies modeling, design, implementation, and integration of applications -- including large and complex ones -- by defining software fundamentally at the model level, expressed in OMG's standard Unified Modeling Language® (UML®)[2]. An MDA-based development goes through three steps -- two producing models, one producing code -- and typically iterates through these several times.An MDA application's base model specifies every detail of its business functionality and behavior in a technology-neutral way; in MDA terminology this is the application's Platform-Independent Model (PIM). Use of well-known patterns, imported into the model from a library and parameterized to suit the application, speeds development and reduces error while still producing a complete and detailed PIM. Technology independence allows domain experts to concentrate on getting the business process correct, and preserves the model's usefulness beyond the technology churn cycle.Working from the PIM, MDA tools follow an OMG-standard mapping to generate an intermediate model tailored to the target middleware implementation platform. (OMG is standardizing mappings to all popular middleware platforms; several have already been adopted.) Termed a Platform-Specific Model (PSM), this intermediate product adds non-business, computing-related details (typically affecting performance and resource usage), possibly following "markup" inserted on the PIM by your architects, and the version produced by the MDA tool will probably require some hand-tuning before it can be used for the next step. (The amount of hand-tuning required will vary depending on the sophistication of the tool, the complexity of the application, and the maturity of the MDA in your application domain.In the final development step, working from the PSM, MDA tools generate interface definitions, application code, makefiles, and configuration files for the PSM's middleware platform. Because the industry has been working on this transformation for years already, and the model is tailored specifically for this transformation, the automated conversion in this step is typically 100% or nearly so. Performing a "build" on these artifacts yields a deployable application.Because the PIM is middleware-neutral and conversion to the PSM and then to the implementation is mostly automatic, it is practical to produce equivalent implementations of MDA-based applications on multiple target platforms. In addition, tools can generate cross-platform invocations, allowing easy interworking among suites of MDA-based applications wherever they reside. Another benefit of the MDA: because industry standards defined as an MDA PIM are platform-independent, they can be implemented on multiple targets and then used by every enterprise even in industries that haven't converged on a single middleware platform.Based on UML, automation, and sound architectural principles, the MDA supports applications over their full lifecycle starting with design and moving on to coding, testing, and deployment, through maintenance, and eventually to evolution to a new platform when an application's existing platform becomes obsolete. The MDA became the base architecture for OMG standards in September 2001.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {37},
numpages = {1},
keywords = {transformation, platform-independent model (PIM), unified modeling language® (UML®), modeling, platform-specific model (PSM), object management group (OMG), software, model driven architecture® (MDA®)},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@article{10.1145/322993.322997,
author = {Damiani, E. and Fugini, M. G. and Bellettini, C.},
title = {Corrigenda: A Hierarchy-Aware Approach to Faceted Classification of Object-Oriented Components},
year = {1999},
issue_date = {Oct. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/322993.322997},
doi = {10.1145/322993.322997},
abstract = {This article presents a hierarchy-aware classification schema for object-oriented code, where software components are classified according to their behavioral characteristics, such as provided services, employed algorithms, and needed data. In the case of reusable application frameworks, these characteristics are constructed from their model, i.e., from the description of the abstract classes specifying both the framework structure and purpose. In conventional object libraries, the characteristics are extracted semiautomatically from class interfaces. Characteristics are term pairs, weighted to represent "how well" they describe component behavior. The set of characteristics associated with a given component forms its software descriptor. A descriptor base is presented where descriptors are organized on the basis of structured relationships, such as similarity and composition. The classification is supported by a thesaurus acting as a language-independent unified lexicon. The descriptor base is conceived for developers who, besides conventionally browsing the descriptors hierarchy, can query the system, specifying a set of desired functionalities and getting a ranked set of adaptable candidates. User feedback is taken into account in order to progressively ameliorate the quality of the descriptors according to the views of the user community. Feedback is made dependent of the user typology through a user profile. Experimental results in terms of recall and precision of the retrieval mechanism against a sample code base are reported.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
pages = {425–472},
numpages = {48}
}

@inproceedings{10.1145/3516807.3516823,
author = {Jacob, Dejice and Singer, Jeremy},
title = {Capability Boehm: Challenges and Opportunities for Garbage Collection with Capability Hardware},
year = {2022},
isbn = {9781450392518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3516807.3516823},
doi = {10.1145/3516807.3516823},
abstract = {The Boehm-Demers-Weiser Garbage Collector (BDWGC) is a widely used, production-quality memory management framework for C and C++ applications. In this work, we describe our experiences in adapting BDWGC for modern capability hardware, in particular the CHERI system, which provides guarantees about memory safety due to runtime enforcement of fine-grained pointer bounds and permissions. Although many libraries and applications have been ported to CHERI already, to the best of our knowledge this is the first analysis of the complexities of transferring a garbage collector to CHERI. We describe various challenges presented by the CHERI micro-architectural constraints, along with some significant opportunities for runtime optimization. Since we do not yet have access to capability hardware, we present a limited study of software event counts on emulated micro-benchmarks. This experience report should be helpful to other systems implementors as they attempt to support the ongoing CHERI initiative.},
booktitle = {Proceedings of the 18th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {81–87},
numpages = {7},
keywords = {CHERI, early experience, memory management},
location = {Virtual, Switzerland},
series = {VEE 2022}
}

@inproceedings{10.1145/1808920.1808927,
author = {Schmidt, Alexander and Polze, Andreas},
title = {KAdvice: Infering Synchronization Patterns from an Existing Codebase},
year = {2010},
isbn = {9781605589749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808920.1808927},
doi = {10.1145/1808920.1808927},
abstract = {Operating system kernels are complex software systems. The kernels of todays mainstream OSs, such as Linux or Windows, are composed from a number of modules, which contain code and data. Even when providing synchronous interfaces (APIs) to the programmer, large portions of the OS kernel operate in an asynchronous manner. Synchronizing access to kernel data structures therefore is a central problem in OS kernels running on todays multicore and multiprocessor hardware. With the need to utilize future multi- and manycore processors, managing the synchronization problem becomes central to all multithreaded control-parallel applications. Since only little software is written from scratch, understanding the intended use of locking constructs and their relation to shared data structures will become critical to all programmers.Built upon our experiences with developing code inside the Windows kernel, we have developed the KAdvice approach, which helps to analyze locking structures in an existing codebase. KAdvice applies static analysis to call graphs and code dependencies to recommend appropriate locking patterns when accessing certain data structures. KAdvice has itself proven very useful in context of students' programming projects based upon the Windows Research Kernel (WRK). However, our approach is more general and applicable not only to OS kernels but to control-parallel software in general.},
booktitle = {Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering},
pages = {31–35},
numpages = {5},
keywords = {KAdvice, data-flow analysis, lock patterns},
location = {Cape Town, South Africa},
series = {RSSE '10}
}

@inproceedings{10.1145/3170427.3170648,
author = {Vanderdonckt, Jean and Vatavu, Radu-Daniel},
title = {Designing, Engineering, and Evaluating Gesture User Interfaces},
year = {2018},
isbn = {9781450356213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3170427.3170648},
doi = {10.1145/3170427.3170648},
abstract = {This course will introduce participants to the three main stages of the development life cycle of gesture-based interactions: (ul) how to design a gesture user interface (UI) by carefully considering key aspects, such as gesture recognition techniques, variability in gesture articulation, properties of invariance (sampling, direction, position, scale, rotation), and good practices for gesture set design, (ii) how to implement a gesture UI with existing recognizers, software architecture, and libraries, and (iii) how to evaluate a gesture user interface with the help of various metrics of user performance. The course will also cover a discussion about the wide range of gestures, such as touch, finger, wrist, hand, arm, and whole-body gestures. Participants will be engaged to try out various tools on their own laptops and will leave the course with a set of useful resources for prototyping and evaluating gesture-based interactions in their own projects.},
booktitle = {Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–4},
numpages = {4},
keywords = {gesture production, gesture analysis, engineering interactive systems, gesture user interfaces, gesture recognition, user interface design, methodology},
location = {Montreal QC, Canada},
series = {CHI EA '18}
}

@inproceedings{10.1145/1463434.1463534,
author = {McLaren, Terrence M. and Myers, James D. and Lee, Jong S. and Tolbert, Nathan L. and Hampton, Shawn D. and Navarro, Christopher M.},
title = {MAEviz: An Earthquake Risk Assessment System},
year = {2008},
isbn = {9781605583235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1463434.1463534},
doi = {10.1145/1463434.1463534},
abstract = {MAEviz is a state-of-the-art earthquake risk assessment environment built to quickly bring new research results into production analyses. It is built upon an extensible Open Services Gateway Initiative (OSGi) based GIS application platform and leverages distributed content management, workflow, and virtual-organization based design concepts. MAEviz has been developed as a collaboration between the National Center for Supercomputing Applications and the Mid-America Earthquake (MAE) Center community and it implements the MAE Consequence-based Risk Management (CRM) methodology. The open source MAEviz provide a modern GIS application interface with sophisticated visualization and reporting capabilities that also incorporates mechanisms to integrate distributed data sources, develop reusable analyses and view data provenance, coordinate work in distributed teams. As an Eclipse Rich Client Platform (RCP) application, MAEviz is composed of multiple plugins and extensions that wrap various open source libraries such as Geotools, the Visualization Toolkit (VTK), and Jasper reports as well as middleware components developed at NCSA. This architecture enables MAEviz to rapidly be extended with new scientific analyses and allows reuse of the base GIS environment capabilities. This demonstration paper provides an overview of MAEviz' capabilities and architecture and presents it as an exemplar of a new generation of "cyberenvironments" that enable multi-disciplinary collaboration and foster direct connections between researchers, practitioners and decision makers. MAEviz is designed to bridge the gap between scientific discovery and practical use by integrating the latest research findings, most accurate data and state-of-the-art methodologies into an extensible open source software system.},
booktitle = {Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
articleno = {88},
numpages = {2},
keywords = {modeling, risk assessment, GIS, earthquake, hazard management, simulation},
location = {Irvine, California},
series = {GIS '08}
}

@inproceedings{10.1145/3510457.3513027,
author = {Ran, Dezhi and Li, Zongyang and Liu, Chenxu and Wang, Wenyu and Meng, Weizhi and Wu, Xionglin and Jin, Hui and Cui, Jing and Tang, Xing and Xie, Tao},
title = {Automated Visual Testing for Mobile Apps in an Industrial Seting},
year = {2022},
isbn = {9781450392266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510457.3513027},
doi = {10.1145/3510457.3513027},
abstract = {User Interface (UI) testing has become a common practice for quality assurance of industrial mobile applications (in short as apps). While many automated tools have been developed, they often do not satisfy two major industrial requirements that make a tool desirable in industrial settings: high applicability across platforms (e.g., Android, iOS, AliOS, and Harmony OS) and high capability to handle apps with non-standard UI elements (whose internal structures cannot be acquired using platform APIs). Toward addressing these industrial requirements, automated visual testing emerges to take only device screenshots as input in order to support automated test generation. In this paper, we report our experiences of developing and deploying VTest, our industrial visual testing framework to assure high quality of Taobao, a highly popular industrial app with about one billion monthly active users. VTest includes carefully designed techniques and infrastructure support, outperforming Monkey (which has been popularly deployed in industry and shown to perform superiorly or similarly compared to state-of-the-art tools) with 87.6% more activity coverage. VTest has been deployed both internally in Alibaba and externally in the Software Green Alliance to provide testing services for top smart-phone vendors and app vendors in China. We summarize five major lessons learned from developing and deploying VTest.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering: Software Engineering in Practice},
pages = {55–64},
numpages = {10},
keywords = {mobile testing, UI testing, robotic testing, visual testing},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-SEIP '22}
}

@proceedings{10.1145/1791194,
title = {WBIA '09: Proceedings of the Workshop on Binary Instrumentation and Applications},
year = {2009},
isbn = {9781605587936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Workshop on Binary Instrumentation and Applications 2009, the third in the series. Instrumentation is an effective technique to observe and verify program properties. This technique has been used for diverse purposes, from profile guided compiler optimizations, to microarchitectural research via simulations, to enforcement of software security policies. While instrumentation can be performed at the source level as well as binary level, the latter has the advantage of having the ability to instrument the whole program, including dynamically linked libraries. Binary instrumentation also obviates the need to have source code. As a result, instrumentation at the binary level has become immensely useful and has growing popularity. This workshop provides an opportunity for developers and users of binary instrumentation to exchange ideas for building better instrumentation systems and new use cases for binary instrumentation, static or dynamic.The first session contains papers that use binary instrumentation to study and improve hardware features. In "Studying Microarchitectural Structures with Object Code Reordering," Shah Mohammad Faizur Rahman, Zhe Wang and Daniel A. Jim\'{e}nez describe an approach to understand microarchitectural structures by running different versions of the same program. Using branch predictors as an example, they vary the object code layout. In "Synthesizing Contention," Jason Mars and Mary Lou Soffa present a profiling approach for studying performance aspects of applications running on multi-core systems due to interference from other cores. The approach creates contention by running a synthetic application at the same time as the real application. In "Assessing Cache False Sharing Effects by Dynamic Binary Instrumentation," Stephan M. G\"{u}nther and Josef Weidendorfer use binary instrumentation to estimate the effects of false sharing in caches for multi-threaded applications.The second session contains papers that improve software systems. The ideas presented improve existing binary instrumentation techniques or use binary instrumentation to improve other software systems. In "Metaman: System-Wide Metadata Management," Daniel Williams and Jack W. Davidson describe an infrastructure to store and access meta-information for programs during its entire build process and runtime, thereby improving the build and runtime systems. In "A Binary Instrumentation Tool for the Blackfin Processor," Enqiang Sun and David Kaeli provide a static binary instrumentation system for embedded systems and use it to perform dynamic voltage and frequency scaling as a case study. In "Improving Instrumentation Speed via Buffering," Dan Upton, Kim Hazelwood, Robert Cohn and Greg Lueck present a technique for reducing instrumentation overhead by decoupling data collection (i.e. instrumentation) from data analysis. Finally, in "ThreadSanitizer -- Data Race Detection In Practice," Konstantin Serebryany and Timur Iskhodzhanov present a new tool based on Valgrind for detecting data races.},
location = {New York, New York, USA}
}

@inproceedings{10.1145/3373087.3375885,
author = {Putnam, Andrew},
title = {What To Do With Datacenter FPGAs Besides Deep Learning},
year = {2020},
isbn = {9781450370998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373087.3375885},
doi = {10.1145/3373087.3375885},
abstract = {FPGAs have been deployed in datacenters worldwide and are now available for use by in both public and private clouds. Enormous focus has been given to optimizing machine learning workloads for FPGAs, especially for deep neural networks (DNNs) in areas like web search, image classification, and translation. However, major cloud applications encompasses a variety of areas that aren't primarily machine learning workloads, including databases, video encoding, text processing, gaming, bioinformatics, productivity and collaboration, file hosting and storage, e-mail, and many more. While machine learning can certainly play a role in each of these areas, is there more that can be done to accelerate these more traditional workloads? Even more challenging than identifying promising workloads is figuring out how developers can practically create and deploy useful applications using FPGAs to the cloud. While FPGAs-as-a-Service allow access to FPGAs in the cloud, there is a huge gap between raw programmable hardware and a customer paying money to use an application powered by that hardware. A wide variety of FPGA IP exists for developers to use, but individual IP blocks are a long way from being a fully functional cloud application. Building block IPs like Memcached, regex matching, protocol parsing, and linear algebra are only a subset of the necessary functionality for full cloud applications. Developing or acquiring IP and integrating it into a full application that customers will pay for is a significant task. And even when a customer pays, how should the money be distributed between IP vendors. Should it be a onetime fee? By usage? By number of FPGAs deployed? Who should have the burden for support if something goes wrong? In traditional cloud applications, FPGA IP block functions are implemented in software libraries. However, few examples of optimized software libraries are commercially successful, so is selling FPGA IP even a viable commercial model for cloud applications? High-level synthesis (HLS) tools promise to provide one path to enable software developers to make effective use of FPGAs for computing tasks, but are any tools really capable of accelerating cloud-scale applications? Many HLS tools require substantial microarchitectural guidance in the form of pragmas or configuration files to come out with good results. Real cloud applications also rarely have a single dominant function and have significant data movement, so without proper partitioning and tuning, the acceleration gains from the FPGA are quickly wiped out by data movement and Amdahl's Law. This panel will gather experts in using FPGAs for cloud application areas beyond machine learning, and how those applications can be built and successfully deployed. We will cover topics such as: -What are the most important cloud workloads for FPGAs to target besides machine learning? -Are there specific changes to the FPGA architecture that would benefit these cloud applications? -What are the economic models that will work for IP developers, application developers, and cloud providers? -How can we make development of FPGA applications easier for the Cloud? -Will open source IP make it impossible for IP vendors to make commercially successful libraries? -What advances are necessary for HLS tools to be practical in the Cloud? The panel is comprised of experts in applications, IP development, and cloud deployment. Each will give a short presentation of what they find as the most important applications and how they see FPGA development for the cloud going forward, then we will open the floor to an interactive discussion with the audience.},
booktitle = {Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {26},
numpages = {1},
keywords = {fpgas, cloud computing, reconfigurable computing},
location = {Seaside, CA, USA},
series = {FPGA '20}
}

@inproceedings{10.1109/EMIP.2019.00008,
author = {Feitelson, Dror G.},
title = {Eye Tracking and Program Comprehension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/EMIP.2019.00008},
doi = {10.1109/EMIP.2019.00008},
abstract = {Reading and comprehending code is different from reading regular text. And eye tracking, which measures where we look, for how long, and how much mental effort we exert, can provide crucial data. The talk will survey some achievements and suggest future directions.Code is hardly ever written in a vacuum. New code must dovetail with existing code. Much code is taken from libraries and frameworks. Developers therefore spend more time reading and trying to understand existing code than writing new code. Code comprehension is also required for fixing bugs. As a consequence, the time it takes to comprehend code is a major driver of development costs.Code comprehension is often thought of as the reciprocal of code complexity: the more complex the code, the harder it is to comprehend. But experience with code comprehension studies indicates that the difficulty of comprehension actually reflects the confluence of three distinct factors: the code, the reader, and the moment. The code's complexity is indeed a factor, but not the only one and perhaps not even the major one. The experience and domain knowledge of the person reading the code is also very important; code that is incomprehensible to one may be crystal clear to another. And various transient effects --- such as fatigue, priming, or being preoccupied --- also take their toll.Studies of code comprehension try to untangle all these effects. One of the tools sometimes used is eye tracking. An early and often-repeated result is that reading code is very different from reading regular text. For example, it has been observed that in many cases reading code includes episodes of scanning the code in a linear fashion. There may be multiple such episodes, or none. In addition there are instances of jumping back to look at the beginning of the code, or jumping to the end. It has been conjectured that these reading patterns represent getting an overview of the code, rechecking a function signature or the declaration of variables, and a preview of a function's return value.Some studies have used eye tracking to characterize the code comprehension activities of different people. A relatively popular topic has been to compare code reading strategies employed by experts and novices. Several studies have found that experts distribute their attention differently and perhaps more efficiently. As another example, Sharafi et al. found that while men and women achieve similar performance in identifying variable names, their strategies might differ. It appeared that women spend more time on checking alternatives, whereas men somehow focus more directly on the correct answers.Working with Ahmad Jbara, we have constructed the first quantitative model of comprehension effort that is based on eye tracking data. We conjectured that regular code, comprising repetitions of the same basic structure, would be easier to comprehend than equivalent non-regular code. The idea was that once the first instance of the recurring structure is understood, this understanding can be leveraged for understanding the subsequent instances. Using eye tracking we showed that this is true, and indeed less and less time is spent on repeated instances.An important aspect of some of these examples is that they represent basic science. We learn something interesting about how our world (the world of developers and code) works, even if it does not have immediate implications that practitioners can utilize tomorrow. Software development is a complex process, influenced by myriad factors. There is a lot we don't know yet. So a focus on basic science is appropriate, and should be encouraged. Such scientific knowledge can then serve as the scientific basis for software engineering in ways that were not anticipated in advance.Part of this is to study not only the software engineering itself, but also the methodology we use to study it. One example that has received some attention is the metrics that are collected in eye tracking data. Should it be the number of fixations, cumulative fixation time, saccades, the convex hull of all fixations, attributes of the scan path (such as regressions), or some other metrics? Interestingly only a few studies have used pupil diameter to estimate mental effort, so this is a promising approach for new results.Finally, further progress will be accelerated if we build and share tools. Research tools, such as software for analyzing eye tracking data, capture and propagate the experience of whoever created them. And using the same tools improves reproducibility.},
booktitle = {Proceedings of the 6th International Workshop on Eye Movements in Programming},
pages = {1},
numpages = {1},
keywords = {eye tracking, code comprehension},
location = {Montreal, Quebec, Canada},
series = {EMIP '19}
}

@inproceedings{10.1145/3365984.3369800,
author = {Fritzson, Peter},
title = {Status and Directions for Cyber-Physical System Development with OpenModelica},
year = {2020},
isbn = {9781450377133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365984.3369800},
doi = {10.1145/3365984.3369800},
abstract = {The industry is currently seeing a rapid development of cyber-physical system products containing integrated software, hardware, and communication components. The increasing system complexity in the automotive and aerospace industries are some examples. The systems that are developed have increasing demands of dependability and usability. Moreover, lead time and cost efficiency continue to be essential for industry competitiveness. Extensive use of modeling and simulation - Model-Based Systems Engineering tools - throughout the value chain and system life-cycle is one of the most important ways to effectively target these challenges. Simultaneously there is an increased interest in open source tools that allow more control of tool features and support, and increased cooperation and shared access to knowledge and innovations between organizations.Modelica is a modern, strongly typed, declarative, equation-based, and object-oriented (EOO) language for model-based systems engineering including modeling and simulation of complex cyber-physical systems Major features are: ease of use, visual design of models with combination of lego-like predefined model building blocks, ability to define model libraries with reusable components, support for modeling and simulation of complex applications involving parts from several application domains, and many more useful facilities. The Modelica language is ideally suited for cyber-physical modeling tasks since it allows integrated modeling of discrete-time (embedded control software) and continuous-time (process dynamics, often for physical hardware). Modelica 3.3 extended the language with clocked synchronous constructs, which are especially well suited to model and integrate physical and digital hardware with model-based software.This talk gives an overview of the current status and directions of the OpenModelica environment - the most complete Modelica open-source tool for modeling, engineering, simulation, and development of systems applications (www.openmodelica.org). Special features are MetaModeling for efficient model transformations, debugging support for equation-based models, support (via OMSimulator) for the Functional Mockup Interface for general tool integration and model export/import between tools, model-based optimization, as well as generation of parallel code for multi-core architectures.Moreover, also mentioned is recent work to make an OpenModelica based tool chain for developing digital controller software for embedded systems, and in generating embedded controller code for very small target platforms like Arduino Boards with down to 2kbyte memory. This work is extended in the ongoing EMPHYSIS project where the FMI standard is extended into the eFMI standard for embedded systems. OpenModelica is one of the platforms participating in that project.},
booktitle = {Proceedings of the 9th International Workshop on Equation-Based Object-Oriented Modeling Languages and Tools},
pages = {3–4},
numpages = {2},
location = {Berlin, Germany},
series = {EOOLT '19}
}

@book{10.1145/2915031,
author = {Zhai, ChengXiang and Massung, Sean},
title = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining},
year = {2016},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {12},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.}
}

@inproceedings{10.1145/3529538.3529998,
author = {Brunton, Ross and Lom\"{u}ller, Victor},
title = {Improved Address Space Inference for SYCL Programs},
year = {2022},
isbn = {9781450396585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529538.3529998},
doi = {10.1145/3529538.3529998},
abstract = {SYCL[4, 6] is a single source C++ based programming model for heterogeneous programming. It enables the programmer to write or port code targeting heterogeneous accelerators using what appears to the programmer as standard C++. To achieve peak performance, however, it can be necessary to write the code in a form which allows the compiler to target specific hardware features. If the compiler can target these hardware features without requiring the programmer to consider them, then productivity and application performance can both be improved. One such example is accelerators with multiple address spaces, this technical talk will describe how a SYCL compiler can infer these address spaces without requiring the programmer to specify them in their application as well as describe some required specification evolution in order to better cope with the new SYCL 2020 features. Hardware devices can have multiple memory regions with different levels of visibility and performance. Similar to OpenCL C[5], SYCL abstracts them into a global memory visible to all work-items, a local memory visible to a single work-group, or a private memory only visible to a single work-item. In OpenCL C, the programmer expresses address spaces using type qualifiers in order to statically encode the memory region addressed by pointers thus ensuring that when a programmer does specify an address space the compiler can check whether the program is well-formed. But requiring programs to be written with explicit address spaces comes at the expense of usability, as these need to be integrated into the program design and are a barrier to integrate code not written with this in mind. Thus in OpenCL C 2.x/3 programmers can make use of the unnamed generic address space instead. On the other hand, SYCL does not extend the C++ language therefore programmers cannot express address spaces using a type qualifier (as the C++ standard does not define them). Thus in SYCL pointers and references can be lowered to this unnamed generic address space by the device compiler. This generic address space is a virtual address space that can represent several overlapping address spaces at the same time. The memory being addressed is no longer statically known by the compiler frontend and the SYCL implementation relies on the hardware, or software emulation, to correctly dispatch the loads and stores to the correct memory. On some hardware targets this flexibility comes with a performance cost, but this can be avoided when the compiler can infer a single address space for a given memory access. Additionally, the low-level compute APIs that are often used as backends to a SYCL 2020 implementation do not guarantee support for a generic address space, e.g. they are an optional feature in OpenCL 3.0 and non-existent in Vulkan. This means that a SYCL compiler that can infer all address spaces for a large set of programs can achieve better performance and target a wider range of backend compute APIs. Moreover, recent efforts to bring safety critical development to SYCL means it will also need to run on top of Vulkan SC. This makes the ability to have a well-defined specification for inferring address spaces still relevant for SYCL. The rules introduced by SYCL 1.2.1 impose significant restrictions on user code. One striking example is the ”defaulting rule”: when a pointer declaration has no initializer, the pointer is assumed to address the private memory, even if it is initialized in the very next statement. As a consequence, you cannot declare a pointer in a structure without it defaulting to the private address space. In practice, however, these restrictions are not a significant barrier in the context of 1.2.1 and large applications were ported to run with SYCL such as Eigen[3] or build new ones like SYCL-BLAS[1] or SYCL-DNN[2]. SYCL 2020 brought significant changes and added flexibility to users. Among them are the unnamed generic address space and unified shared memory (USM) pointers. The generic address space allowed to lift the restrictions stated by 1.2.1, making programs written for 2020 and generic unlikely to be compilable under the inference rules restriction. USM encourages the usage of raw pointers instead of the accessors container as this quickly implies passing these pointers via structures. As a USM pointer is in fact addressing the global memory region, this creates a conflict with inference rules. This talk will describe an experimental compiler for ComputeCpp, Codeplay’s SYCL implementation. This compiler employs an improved address space inference method that can efficiently cope with SYCL 2020 features such as the generic address space and unified shared memory (USM) pointers. The talk with also cover the limitations of this approach.},
booktitle = {International Workshop on OpenCL},
articleno = {22},
numpages = {2},
location = {Bristol, United Kingdom, United Kingdom},
series = {IWOCL'22}
}

@proceedings{10.1145/2542142,
title = {VMIL '13: Proceedings of the 7th ACM Workshop on Virtual Machines and Intermediate Languages},
year = {2013},
isbn = {9781450326018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {An increasing number of high-level programming language implementations is realized using standard virtual machines. Recent examples of this trend include the Clojure (Lisp) and Potato (Squeak Smalltalk) projects, which are implemented on top of the Java Virtual Machine (JVM); and also F# (ML) and IronPython, which target the .NET CLR. Making diverse languages--possibly even adopting different paradigms--available on a robust and efficient common platform leverages language interoperability.Vendors of standard virtual machine implementations have started to adopt extensions supporting this trend from the run-time environment side. For instance, the Sun standard JVM will include the invokedynamic instruction, which will facilitate a simpler implementation of dynamic programming languages on the JVM.The observation that many language constructs are supported in library code, or through code transformations leading to over-generalized results, has led to efforts to make the core mechanisms of certain programming paradigms available at the level of the virtual machine implementation. Thus, dedicated support for language constructs enables sophisticated optimization by direct access to the running system. This approach has been adopted by several projects aiming at providing support for aspect-oriented programming or dynamic dispatch in general-purpose virtual machines (Steamloom, Nu, ALIA4J).The main themes of this workshop are to investigate which programming language mechanisms are worthwhile candidates for integration with the run-time environment, how said mechanisms can be declaratively (and re-usably) expressed at the intermediate language level (e.g., in bytecode), how their implementations can be optimized, and how virtual machine architectures might be shaped to facilitate such implementation efforts. Possible candidates for investigation include modularity mechanisms (aspects, context-dependent layers), concurrency (threads and locking, actors, software transactional memory), transactions, paradigm-specific abstractions, and combinations of paradigms.The areas of interest include, but are not limited to, compilation-based and interpreter-based virtual machines as well as intermediate-language designs with better support for investigated language mechanisms, compilation techniques from high-level languages to enhanced intermediate languages as well as native machine code, optimization strategies for reduction of run-time overhead due to either compilation or interpretation, advanced caching and memory management schemes in support of the mechanisms, and additional virtual machine components required to manage them.},
location = {Indianapolis, Indiana, USA}
}

@inproceedings{10.1145/3529538.3529542,
author = {Lehmann, Moritz},
title = {Combined Scientific CFD Simulation and Interactive Raytracing with OpenCL},
year = {2022},
isbn = {9781450396585},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529538.3529542},
doi = {10.1145/3529538.3529542},
abstract = {One of the main uses for OpenCL is (scientific) compute applications where graphical rendering is done externally, after the simulation has finished. However separating simulation and rendering has many disadvantages, especially the extreme slowdown caused by copying simulation data from device to host, and needing to store raw data on the hard drive, taking up hundreds of gigabyte, just to visualize preliminary results. A much faster approach is to implement both simulation and rendering in OpenCL. The rendering kernels have direct read-only access to the raw simulation data that resides in ultra-fast GPU memory. This eliminates all PCIe data transfer but camera parameters and finished frames, allowing for interactive visualization of simulation results in real time while the simulation is running. This is an invaluable tool for rapid prototyping. Although OpenCL does not have existing functionality for graphical rendering, being a general compute language, it allows for implementing an entire graphics engine, such that no data has to be moved to the CPU during rendering. On top, specific low-level optimizations make this OpenCL graphics engine outperform any existing rendering solution for this scenario, enabling drawing billions of lines per second and fluid raytracing in real time on even non-RTX GPUs. This combination of simulation and rendering in OpenCL is demonstrated with the software FluidX3D [3] - a lattice Boltzmann method (LBM) fluid dynamics solver. The first part will briefly introduce the numerical method for simulating fluid flow in a physically accurate manner. After introducing the LBM, the optimizations to make it run at peak efficiency are discussed: Being a memory-bound algorithm, coalesced memory access is key. This is achieved through array-of-structures data layout as well as the one-step-pull scheme, a certain variant of the LBM streaming step. One-step-pull leverages the fact that the misaligned read penalty is much smaller than the misaligned write penalty on almost all GPUs. Roofline analysis shows that with these optimizations, the LBM runs at 100% efficiency on the fastest data-center and gaming GPUs [5]. To simulate free surface flows, the LBM is extended with the Volume-of-Fluid (VoF) model. An efficient algorithm has been designed to vastly accelerate the challenging surface tension computation [4]. This extremely efficient VoF-LBM GPU implementation allows covering new grounds in science: FluidX3D has been used to simulate more than 1600 raindrop impacts to statistically evaluate how microplastics transition from the ocean surface into the atmosphere when the spray droplets are generated during drop impact [6]. At the same power consumption, with existing CPU-parallelized codes, compute time would have been several years, whilst with FluidX3D it was about a week. The second part will focus on real time rendering with OpenCL, especially raytracing. Rasterization on the GPU is parallelized not over pixels but lines/triangles instead, making runtime mostly independent of screen resolution and lightning fast. Each line/triangle is transformed with the camera parameters from 3D to 2D screen coordinates and then rasterized onto the frame (integer array) with Bresenham algorithm [2] and z-buffer. The raytracing graphics are based on a combination of fast ray-grid traversal and marching-cubes, leveraging that the computational grid from the LBM already is an ideal acceleration structure for raytracing. The idea of raytracing is simple: Through each pixel on the screen, shoot a reverse light ray out of the camera and see where it intersects with a surface in the scene. Then (recursively) calculate reflected/refracted rays and mix the colors. If a ray doesn’t intersect with anything, its color is determined by the skybox image via UV mapping and bilinear pixel interpolation. With mesh surfaces consisting of many triangles, computation time quickly becomes a problem, as for each ray all triangles have to be tested for intersection. To overcome this, an acceleration structure is required. While computer games often use a bounding volume hierarchy, the LBM already provides an ideal alternative acceleration structure: the simulation grid. The corresponding algorithm is called ray-grid traversal: When a ray shoots through the 3D grid, intersections with the surface only have to be checked for at each traversed grid cell rather than the entire grid. In each traversed grid cell, the 0-5 surface triangles are generated on-the-fly with the marching-cubes algorithm and ray-triangle intersections are checked with the M\"{o}ller-Trumbore algorithm. If an intersection has been found, only afterwards the normals are calculated on the 8 grid points spanning the cell, and are trilinearly interpolated to the intersection coordinates. The so interpolated surface normal makes the raytraced surface appear perfectly smooth. On the GPU, the ray(s) for each pixel on screen are computed in parallel, vastly speeding up rendering. It is of key importance how to align the OpenCL workgroups on the 2D array of screen pixels: best performance is achieved for 8x8 pixel tiles; this is about 50% faster than 64x1 tiles, because with small, square-ish tiles, all rays of the workgroup are more likely to traverse the same grid cells, greatly improving memory broadcasting. In ray-grid traversal, 8 isovalues spanning a cell have to be loaded from GPU memory for each traversed cell. Once the triangle intersection has been found, the gradient on each of the 8 cell isovalues is calculated with central differences. Instead of loading an additional 6 isovalues for each of the 8 grid points, their isovalues are reused such that only 24 additional isovalues are loaded. For marching-cubes, the algorithm by Paul Bourke [1] is implemented in OpenCL. With 16-/8-bit integers, bit-packing and symmetry, the tables are reduced to 1/8 of their original size and stored in constant memory space. For computing the cube index, branching is eliminated by bit operations. The M\"{o}ller-Trumbore algorithm [7] is implemented in an entirely branchless manner. This raytracing implementation is fast enough to run in real time for even the largest lattice dimensions that fit into the memory of a GPU. Finally, the combined VoF-LBM simulation and raytracing implementation is demonstrated on the most realistic simulation of an impacting raindrop ever done [8].},
booktitle = {International Workshop on OpenCL},
articleno = {3},
numpages = {2},
keywords = {Volume-of-Fluid, ray-grid traversal, graphics, raindrop, marching-cubes, raytracing, rasterization, OpenCL, CFD, GPU, lattice Boltzmann method, fluid},
location = {Bristol, United Kingdom, United Kingdom},
series = {IWOCL'22}
}

@inproceedings{10.1145/2491246.2491247,
author = {Dutta, Aveek and Saha, Dola and Grunwald, Dirk and Sicker, Douglas},
title = {CODIPHY: Composing on-Demand Intelligent Physical Layers},
year = {2013},
isbn = {9781450321815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491246.2491247},
doi = {10.1145/2491246.2491247},
abstract = {In this paper we present CODIPHY or Composing On-Demand Intelligent Physical Layers that aims to solve two fundamental problems in practical cognitive radio networks: Collaboration between two radio physical layers (PHY) with varying capabilities to agree on a common communication protocol and provide a method to compose a functioning software defined radio (SDR) from a set of pre-compiled libraries. Both solutions use an ontology based description of the internal structure of the radio subsystems and use the high-level dataflow represented by the ontology to target heterogeneous platforms. CODIPHY isolates the various domains of radio engineering but still allows them to share domain knowledge to achieve a common goal of radio adaptation. Automating this process through declarative specification and collaborative learning is the goal of this paper. We present a generic methodology to facilitate the concept of CODIPHY and present examples from the radio PHY domain.},
booktitle = {Proceedings of the Second Workshop on Software Radio Implementation Forum},
pages = {1–8},
numpages = {8},
keywords = {physical layer, cognitive radio, design automatio, architecture, software defined radio},
location = {Hong Kong, China},
series = {SRIF '13}
}

@book{10.1145/600875,
author = {Bentley, Jon},
title = {Programming Pearls},
year = {1986},
isbn = {0201500191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {From the Book:PREFACE: Computer programming has many faces. Fred Brooks paints the big picture in The Mythical Man Month; his essays underscore the crucial role of management in large software projects. At a finer grain, Steve McConnell teaches good programming style in Code Complete. The topics in those books are the key to good software and the hallmark of the professional programmer. Unfortunately, though, the workmanlike application of those sound engineering principles isn't always thrilling -- until the software is completed on time and works without surprise. About the Book The columns in this book are about a more glamorous aspect of the profession: programming pearls whose origins lie beyond solid engineering, in the realm of insight and creativity. Just as natural pearls grow from grains of sand that have irritated oysters, these programming pearls have grown from real problems that have irritated real programmers. The programs are fun, and they teach important programming techniques and fundamental design principles.Most of these essays originally appeared in my ''Programming Pearls'' column in Communications of the Association for Computing Machinery. They were collected, revised and published as the first edition of this book in 1986. Twelve of the thirteen pieces in the first edition have been edited substantially for this edition, and three new columns have been added.The only background the book assumes is programming experience in a high-level language. Advanced techniques (such as templates in C++) show up now and then, but the reader unfamiliar with such topics will be able to skip tothenext section with impunity.Although each column may be read by itself, there is a logical grouping to the complete set. Columns 1 through 5 form Part I of the book. They review programming fundamentals: problem definition, algorithms, data structures and program verification and testing. Part II is built around the theme of efficiency, which is sometimes important in itself and is always a fine springboard into interesting programming problems. Part III applies those techniques to several substantial problems in sorting, searching and strings.One hint about reading the essays: don't go too fast. Read them carefully, one per sitting. Try the problems as they are posed -- some of them look easy until you've butted your head against them for an hour or two. Afterwards, work hard on the problems at the end of each column: most of what you learn from this book will come out the end of your pencil as you scribble down your solutions. If possible, discuss your ideas with friends and colleagues before peeking at the hints and solutions in the back of the book. The further reading at the end of each chapter isn't intended as a scholarly reference list; I've recommended some good books that are an important part of my personal library.This book is written for programmers. I hope that the problems, hints, solutions, and further reading make it useful for individuals. The book has been used in classes including Algorithms, Program Verification and Software Engineering. The catalog of algorithms in Appendix 1 is a reference for practicing programmers, and also shows how the book can be integrated into classes on algorithms and data structures.The Code The pseudocode programs in the first edition of the book were all implemented, but I was the only person to see the real code. For this edition, I have rewritten all the old programs and written about the same amount of new code. The programs are available at this book's web site. The code includes much of the scaffolding for testing, debugging and timing the functions. The site also contains other relevant material. Because so much software is now available online, a new theme in this edition is how to evaluate and use software components.The programs use a terse coding style: short variable names, few blank lines, and little or no error checking. This is inappropriate in large software projects, but it is useful to convey the key ideas of algorithms. Solution 5.1 gives more background on this style. The text includes a few real C and C++ programs, but most functions are expressed in a pseudocode that takes less space and avoids inelegant syntax. The notation for i = 0, n) iterates i from 0 through n-1. In these for loops, left and right parentheses denote open ranges (which do not include the end values), and left and right square brackets denote closed ranges (which do include the end values). The phrase function(i, j) still calls a function with parameters i and j, and arrayi, j still accesses an array element.This edition reports the run times of many programs on ''my computer'', a 400MHz Pentium II with 128 megabytes of RAM running Windows NT 4.0. I timed the programs on several other machines, and the book reports the few substantial differences that I observed. All experiments used the highest available level of compiler optimization. I encourage you to time the programs on your machine; I bet that you'll find similar ratios of run times.To Readers of the First Edition I hope that your first response as you thumb through this edition of the book is, ''This sure looks familiar.'' A few minutes later, I hope that you'll observe, "I've never seen that before."This version has the same focus as the first edition, but is set in a larger context. Computing has grown substantially in important areas such as databases, networking and user interfaces. Most programmers should be familiar users of such technologies. At the center of each of those areas, though, is a hard core of programming problems. Those programs remain the theme of this book. This edition of the book is a slightly larger fish in a much larger pond One section from old Column 4 on implementing binary search grew into new Column 5 on testing, debugging and timing. Old Column 11 grew and split into new Columns 12 (on the original problem) and 13 (on set representations). Old Column 13 described a spelling checker that ran in a 64-kilobyte address space; it has been deleted, but its heart lives on in Section 13.8. New Column 15 is about string problems. Many sections have been inserted into the old columns, and other sections were deleted along the way. With new problems, new solutions, and four new appendices, this edition of the book is 25 percent longer.Many of the old case studies in this edition are unchanged, for their historical interest. A few old stories have been recast in modern terms.Acknowledgments for the First Edition I am grateful for much support from many people. The idea for a Communications of the ACM column was originally conceived by Peter Denning and Stuart Lynn. Peter worked diligently within ACM to make the column possible and recruited me for the job. ACM Headquarters staff, particularly Roz Steier and Nancy Adriance, have been very supportive as these columns were published in their original form. I am especially indebted to the ACM for encouraging publication of the columns in their present form, and to the many CACM readers who made this expanded version necessary and possible by their comments on the original columns.Al Aho, Peter Denning, Mike Garey, David Johnson, Brian Kernighan, John Linderman, Doug McIlroy and Don Stanat have all read each column with great care, often under extreme time pressure. I am also grateful for the particularly helpful comments of Henry Baird, Bill Cleveland, David Gries, Eric Grosse, Lynn Jelinski, Steve Johnson, Bob Melville, Bob Martin, Arno Penzias, Marilyn Roper, Chris Van Wyk, Vic Vyssotsky and Pamela Zave. Al Aho, Andrew Hume, Brian Kernighan, Ravi Sethi, Laura Skinger and Bjarne Stroustrup provided invaluable help in bookmaking, and West Point cadets in EF 485 field tested the penultimate draft of the manuscript. Thanks, all. Acknowledgments for the Second Edition Dan Bentley, Russ Cox, Brian Kernighan, Mark Kernighan, John Linderman, Steve McConnell, Doug McIlroy, Rob Pike, Howard Trickey and Chris Van Wyk have all read this edition with great care. I am also grateful for the particularly helpful comments of Paul Abrahams, Glenda Childress, Eric Grosse, Ann Martin, Peter McIlroy, Peter Memishian, Sundar Narasimhan, Lisa Ricker, Dennis Ritchie, Ravi Sethi, Carol Smith, Tom Szymanski and Kentaro Toyama. I thank Peter Gordon and his colleagues at Addison-Wesley for their help in preparing this edition.}
}

@inproceedings{10.1145/2384716.2384780,
author = {Rajan, Hridesh and Haupt, Michael and Bockisch, Christoph and Blackburn, Steve},
title = {6th Workshop on Virtual Machines and Intermediate Languages (VMIL'12)},
year = {2012},
isbn = {9781450315630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384716.2384780},
doi = {10.1145/2384716.2384780},
abstract = {The VMIL workshop is a forum for research in virtual machines and intermediate languages. It is dedicated to identifying programming mechanisms and constructs that are currently realized as code transformations or implemented in libraries but should rather be supported at VM level. Candidates for such mechanisms and constructs include modularity mechanisms (aspects, context-dependent layers), concurrency (threads and locking, actors, software transactional memory), transactions, etc. Topics of interest include the investigation of which such mechanisms are worthwhile candidates for integration with the run-time environment, how said mechanisms can be expressed at the intermediate language level, how their implementations can be optimized, and how virtual machine architectures might be shaped to facilitate such implementation efforts.},
booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {223–224},
numpages = {2},
keywords = {intermediate languages, virtual machines},
location = {Tucson, Arizona, USA},
series = {SPLASH '12}
}

@inproceedings{10.1145/2048147.2048235,
author = {Rajan, Hridesh and Haupt, Michael and Bockisch, Christoph and Dyer, Robert},
title = {VMIL 2011: The 5th Workshop on Virtual Machines and Intermediate Languages},
year = {2011},
isbn = {9781450309424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048147.2048235},
doi = {10.1145/2048147.2048235},
abstract = {The VMIL workshop is a forum for research in virtual machines and intermediate languages. It is dedicated to identifying programming mechanisms and constructs that are currently realized as code transformations or implemented in libraries but should rather be supported at VM level. Candidates for such mechanisms and constructs include modularity mechanisms (aspects, context-dependent layers), concurrency (threads and locking, actors, software transactional memory), transactions, etc. Topics of interest include the investigation of which such mechanisms are worthwhile candidates for integration with the run-time environment, how said mechanisms can be elegantly (and reusably) expressed at the intermediate language level (e.g., in bytecode), how their implementations can be optimized, and how virtual machine architectures might be shaped to facilitate such implementation efforts.},
booktitle = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
pages = {333–334},
numpages = {2},
keywords = {virtual machine, intermediate language},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@inproceedings{10.1145/1639950.1639972,
author = {Rajan, Hridesh and Haupt, Michael and Bockisch, Christoph and Dyer, Robert},
title = {VMIL: Workshop on Virtual Machines and Intermediate Languages},
year = {2009},
isbn = {9781605587684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1639950.1639972},
doi = {10.1145/1639950.1639972},
abstract = {The VMIL workshop is a forum for research in virtual machines (VM) and intermediate languages (IL). It is dedicated to identifying programming mechanisms and constructs that are currently realized as code transformations or implemented in libraries but should rather be supported at the VM level. Candidates for such mechanisms and constructs include modularity mechanisms (aspects, context-dependent layers), concurrency (threads and locking, actors, software transactional memory), transactions, etc. Topics of interest include the investigation of which such mechanisms are worthwhile candidates for integration with the VM, how said mechanisms can be elegantly (and reusably) expressed at the IL level (e.g., in bytecode), how their implementations can be optimized, and how VM architectures might be shaped to facilitate such implementation efforts.},
booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
pages = {701–702},
numpages = {2},
keywords = {interpretation, compilation, optimization, dynamic dispatch, intermediate language, virtual machine},
location = {Orlando, Florida, USA},
series = {OOPSLA '09}
}

@inproceedings{10.1145/2245276.2231958,
author = {Drago, Mauro Luigi and Bishop, Judith},
title = {DAG3: A Tool for Design and Analysis of Applications for Multicore Architectures},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231958},
doi = {10.1145/2245276.2231958},
abstract = {Despite the availability of libraries and constructs in modern programming languages to support the development of parallel applications, achieving the desired performance improvement can still be difficult. Design patterns and idioms simplify writing of applications but, if they are not applied correctly, the introduced overhead may overwhelm the benefits of parallelism. Performance and parallelism deserve specific software engineering techniques in order to be successfully accounted. In this sense, software design and performance analysis techniques represent a valid option to discover problems in the early stages of development and manage them in an structured manner. In this paper we present Dag3: a tool and a methodology leveraging Direct Acyclic Graphs to design parallel applications and to analyze their performance. Parallel applications are designed by specifying the constituents of the computation, their organization, and their contribution to performance. Parametric analyses can be conducted to estimate performance metrics, such as speedup, and to the identify the critical parts of a computation. To increase programmers' productivity, models can be also used to generate part of the application source code.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1159–1164},
numpages = {6},
keywords = {visualization, directed acyclic graphs, measuring performance, predicting performance, reuse, usability},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/1869542.1869609,
author = {Rajan, Hridesh and Haupt, Michael and Bockisch, Christoph M. and Dyer, Robert},
title = {VMIL 2010: 4th Workshop on Virtual Machines and Intermediate Languages},
year = {2010},
isbn = {9781450302401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1869542.1869609},
doi = {10.1145/1869542.1869609},
abstract = {The VMIL workshop is a forum for research in virtual machines (VMs) and intermediate languages. It is dedicated to identifying programming mechanisms and constructs that are currently realized as code transformations or implemented in libraries but should rather be supported at VM level. Candidates include modularity mechanisms (aspects, context-dependent layers), concurrency (threads and locking, actors, software transactional memory), transactions, etc. Topics of interest include the investigation of which such mechanisms are worthwhile candidates for integration with the run-time environment, how said mechanisms can be elegantly (and reusably) expressed at the intermediate language level (e.g., in bytecode), how their implementations can be optimized, and how VM architectures might be shaped to facilitate such implementation efforts.},
booktitle = {Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion},
pages = {273–274},
numpages = {2},
keywords = {intermediate language, virtual machine},
location = {Reno/Tahoe, Nevada, USA},
series = {OOPSLA '10}
}

@inproceedings{10.1145/1065385.1065513,
author = {Iacob, Ionut E. and Dekhtyar, Alex},
title = {Processing XML Documents with Overlapping Hierarchies},
year = {2005},
isbn = {1581138768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065385.1065513},
doi = {10.1145/1065385.1065513},
abstract = {The problem of overlapping markup hierarchies, first mentioned in the context of SGML, often occurs in XML text encoding applications for humanities. Previous solutions to the problem rely on manual maintenance of the markup and address only the problem of representing overlapping features in XML, leaving the issues of automated maintenance and querying open. As a consequence, traditional XML tools are of little practical use when dealing with overlapping markup. In this work we demonstrate the implementation of our framework for management of concurrent XML hierarchies from a computer science perspective. We propose an underlying model, data structures, APIs, and algorithms so that the most of the burden of managing concurrent XML hierarchies would be born by the software.},
booktitle = {Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {409},
numpages = {1},
keywords = {querying, XML, overlapping hierarchies},
location = {Denver, CO, USA},
series = {JCDL '05}
}

@proceedings{10.1145/1390768,
title = {ISSAC '08: Proceedings of the Twenty-First International Symposium on Symbolic and Algebraic Computation},
year = {2008},
isbn = {9781595939043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the proceedings of the 21st annual meeting of the International Symposium on Symbolic Computation, ISSAC 2008. The first ISSAC took place in 1988, but it inherited the legacies of a number of earlier meetings, SYMSAM, SYMSAC, EUROSAM, EUROCAL, stretching back to 1966. In this extended tradition, ISSAC 2008 is the 33rd meeting. The meeting took place from the 20th to 23rd of July 2008 in Hagenberg, a pretty village 20 km north-east of Linz in Austria.The topics of the conference series include, but are not limited to: Algorithmic Mathematics: Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, summation, integration, linear algebra, number theory, polynomial/differential/difference equations, group and invariant theory, geometric computing.Computer Science: Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, concrete analysis, parallel/distributed computing and programming languages, benchmarking, theoretical and practical complexity, automatic differentiation, code generation, mathematical data structures and exchange protocols.Applications: Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics and education.The ISSAC 2008 program contained invited talks, contributed papers, tutorials, poster sessions and software exhibitions. ISSAC was also part of RISC Summer 2008, and a number of related conferences were held at the same location and nearby in time. This volume contains all contributed papers as well as abstracts of the invited talks and tutorials.For ISSAC 2008, a total of 99 papers were accepted for review and each was distributed to members of the program committee and external reviewers. On average, 2.6 referee reports were obtained for each submission, and 40 papers were selected for presentation. They are representative of the many topics of research that make up the field of computer algebra. We wish to thank all the researchers who contributed papers for review, and to thank the many reviewers who generously gave their time to the selection process.},
location = {Linz/Hagenberg, Austria}
}

@proceedings{10.1145/1073884,
title = {ISSAC '05: Proceedings of the 2005 International Symposium on Symbolic and Algebraic Computation},
year = {2005},
isbn = {1595930957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ISSAC 2005 is a continuation of a well-established series of international conferences for the presentation of the latest advances in the field of Symbolic and Algebraic Computation. The first meeting of the series (1966) was held in Washington, DC, and sponsored by the Association for Computing Machinery (ACM). Since then, the abbreviated name of the meeting has evolved from SYMSAM, SYMSAC, EUROSAM, EUROCAL to finally settle on the present name ISSAC. This 30th meeting was hosted by the Key Laboratory of Mathematics Mechanization, Chinese Academy of Sciences, Beijing, China from July 24 to July 27.The topics of the conference include, but are not limited to:<ul><li>Algorithmic mathematics. Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, equations, summation, integration, ODE/PDE, linear algebra, number theory, group and geometric computing.</li><li>Computer Science. Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, parallel/distributed computing and programming languages for symbolic computation, concrete analysis, benchmarking, theoretical and practical complexity of computer algebra algorithms, automatic differentiation, code generation, mathematical data structures and exchange protocols.</li><li>Applications. Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics, statistics, education.</li></ul>Following tradition, ISSAC 2005 featured invited talks, contributed papers, tutorials, poster sessions, software exhibitions, and satellite workshops. This volume contains all the contributed papers which were presented at the meeting as well as the abstracts of the invited talks.The picture on the front cover shows a page from the classic Chinese math book bearing the title "Jade Mirrors of Four Elements" by Zhu Shijie, written in 1303 AD during the Yuan Dynasty. In this page, a system of equations of three unknowns and degree three is reduced to a univariate equation by eliminating variables.},
location = {Beijing, China}
}

@proceedings{10.1145/860854,
title = {ISSAC '03: Proceedings of the 2003 International Symposium on Symbolic and Algebraic Computation},
year = {2003},
isbn = {1581136412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ISSAC 2003 is the 28th meeting in the well-established series of the International Symposia on Symbolic and Algebraic Computation. The first meeting of the series (1966) was held in Washington, and sponsored by ACM. Since then, the abbreviated name of the meeting has evolved from SYMSAM, SYMSAC, EUROSAM, EUROCAL to finally settle on the present name ISSAC. In 2003, ISSAC will be held at the Drexel University in Philadelphia Pennsylvania USA from August 3rd to August 6th. The topics of the conference includes, but are not limited to:Algorithmic mathematics. Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, equations, summation, integration, ODE/PDE, linear algebra, number theory, group and geometric computing.Computer Science. Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, parallel/distributed computing and programming languages for symbolic computation, concrete analysis, benchmarking, theoretical and practical complexity of computer algebra algorithms, automatic differentiation, code generation, mathematical data structures and exchange protocols. Applications. Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics, statistics, education.Following the tradition, ISSAC 2003 features invited talks, contributed papers, tutorials, poster sessions, and software exhibitions. This volume contains all the contributed papers to be presented at the meeting as well as the abstracts of the invited talks. The cover (design: Ed Shields) shows the Drexel University billboard at the intersection of 30th and Market Streets in Philadelphia.A total of 68 papers were submitted, and each was distributed to members of the program committee and external reviewers. An average of 3 referee reports were obtained for each submission, and finally 36 papers were selected for presentation. As a consequence, we are convinced that the contributions in these proceedings represent excellent and wide, though not complete, spectra of areas spanning most of symbolic and algebraic computation. Our congratulations and warmest thanks to all contributors and lecturers.},
location = {Philadelphia, PA, USA}
}

@proceedings{10.1145/1005285,
title = {ISSAC '04: Proceedings of the 2004 International Symposium on Symbolic and Algebraic Computation},
year = {2004},
isbn = {158113827X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ISSAC 2004 is a continuation of a well-established series of the international conferences for the presentation of the latest advances in the field of Symbolic and Algebraic Computation. The first meeting of the series (1966) was held in Washington, and sponsored by Association for Computing Machinery (ACM). Since then, the abbreviated name of the meeting has evolved from SYMSAM, SYMSAC, EUROSAM, EUROCAL to finally settle on the present name ISSAC. This 29th meeting will be at the University of Cantabria in Santander, Spain from July 4th to July 7th.The topics of the conference includes, but are not limited to: Algorithmic mathematics. Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, equations, summation, integration, ODE/PDE, linear algebra, number theory, group and geometric computing.Computer Science. Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, parallel/distributed computing and programming languages for symbolic computation, concrete analysis, benchmarking, theoretical and practical complexity of computer algebra algorithms, automatic differentiation, code generation, mathematical data structures and exchange protocols.Applications. Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics, statistics, education.Following the tradition, ISSAC 2004 features invited talks, contributed papers, tutorials, poster sessions, and software exhibitions. This volume contains all the contributed papers to be presented at the meeting as well as the abstracts of the invited talks.A total of 131 papers were submitted, and each was distributed to members of the program committee and external reviewers. An average of 3 referee reports were obtained for each submission, and finally 43 papers were selected for presentation. As a consequence, we are convinced that the contributions in these proceedings represent excellent and wide, though not complete, spectra of areas spanning most of symbolic and algebraic computation. It is a pleasure to acknowledge the contributions of all these persons, as well as all researchers and educators who submitted papers for consideration, and all those who assisted in the selection process. Unfortunately, due to the time constraint of three days without parallel sessions, many interesting papers could not be accepted for presentation and inclusion in these proceedings.},
location = {Santander, Spain}
}

@inproceedings{10.1145/2644866.2644900,
author = {Lynch, Clifford A.},
title = {The Evolving Scholarly Record: New Uses and New Forms},
year = {2014},
isbn = {9781450329491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2644866.2644900},
doi = {10.1145/2644866.2644900},
abstract = {This presentation will take a very broad view of the emergence of literary corpora as objects of computation, with a particular focus on the various literatures and genres that form the scholarly record. The developments and implications here that I will explore include: the evolution of the scholarly literature into a semi-structured network of information used by both human readers and computational agents through the introduction of markup technologies; the interpenetration and interweaving of data and evidence with the literature; and the creation of an invisible infrastructure of names, taxonomies and ontologies, and the challenges this presents.Primary forms of computation on this corpus include both comprehensive text mining and stream analysis (focused on what's new and what's changing as the base of literature and related factual databases expand with reports of new discoveries). I'll explore some of the developments in this area, including some practical considerations about platforms, licensing, and access.As the use of the literature evolves, so do the individual genres that comprise it. Today's typical digital journal article looks almost identical to one half a century old, except that it is viewed on screen and printed on demand. Yet there is a great deal of activity driven by the move to data and computationally intensive scholarship, demands for greater precision and replicability in scientific communication, and related sources to move journal articles "beyond the PDF," reconsidering relationships among traditional texts, software, workflows, data and the broad cultural record in its role as evidence. I'll look briefly at some of these developments, with particular focus on what this may mean for the management of the scholarly record as a whole, and also briefly discuss some parallel challenges emerging in scholarly monographs.Finally, I will close with a very brief discussion of what might be called corpus-scale thinking with regard to the scholarly record at the disciplinary level. I'll briefly discuss the findings of a 2014 National Research Council study that I co-chaired dealing with the future of the mathematics literature and the possibility of creating a global digital mathematics library, as well as offering some comments on developments in the life sciences. I will also consider the emergence of new corpus-wide tools and standards, such as Web-scale annotation, and some of their implications.},
booktitle = {Proceedings of the 2014 ACM Symposium on Document Engineering},
pages = {1–2},
numpages = {2},
keywords = {analysis of text corpora, digital journals, scholarly communications},
location = {Fort Collins, Colorado, USA},
series = {DocEng '14}
}

@proceedings{10.1145/1145768,
title = {ISSAC '06: Proceedings of the 2006 International Symposium on Symbolic and Algebraic Computation},
year = {2006},
isbn = {1595932763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The International Symposium on Symbolic and Algebraic Computation, ISSAC, is devoted to research in computer algebra including these topics: Algorithmic mathematics. Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, equations, summation, integration, ODE/PDE, linear algebra, number theory, group-theoretic and geometric computing.Computer Science. Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, parallel/distributed computing and programming languages for symbolic computation, analysis, benchmarking, complexity of computer algebra algorithms, automatic differentiation, code generation, mathematical data structures and exchange protocols.Applications. Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics, statistics, education..ISSAC 2006 is the 19th in an annual series begun in 1988 and evolved from a loosely coordinated set of bi- and tri-annual meetings with acronyms such as SYMSAC, SYMSAM, EUROCAL, EUROSAM tracing back to 1966.As is customary, ISSAC 2006 featured invited talks, contributed papers, posters, tutorials, and software demos. These proceedings contain the contributed papers, abstracts of the invited talks, and tutorial summaries. Poster abstracts will appear in a future issue of the SIGSAM Bulletin now entitled ACM Communications in Computer Algebra.There were 97 papers submitted. The program committee selected the program herein after careful evaluation including two or more (average of three) referee reports per submission. We gratefully acknowledge the thorough and essential work of the program committee members and referees.Several conferences were held in convenient coordination with ISSAC. In particular we mention Calculemus 2006 in Genoa just before ISSAC 2006 and Caf\'{e}, Computer Algebra and Functional Equations, an international conference in memory of Manuel Bronstein held in Sophia Antipolis just after ISSAC.},
location = {Genoa, Italy}
}

@inproceedings{10.1145/3474124.3474140,
author = {Kapil, Divya and Mishra, Preeti},
title = {Virtual Machine Introspection in Virtualization: A Security Perspective},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474140},
doi = {10.1145/3474124.3474140},
abstract = {Virtualization technology has gained enough attention in several fields such as Cloud Computing, the Internet of Things (IoT), and software defined networking (SDN), etc. However, security issues in virtualization impose several questions on the adoption of this technology and raise strong security concerns. Most of the researchers have employed traditional security approaches in virtualization. However, these approaches are not effective enough for the modern environment. Instead, Introspection-based approaches such as Virtual Machine Introspection (VMI) are more useful to protect the virtualized environment. VMI approaches provide robust solutions in identifying the user and kernel-level processes-based attacks by positioning the security tool outside the VM. The successful implementation of these solutions is still challenging due to having heterogeneous design architectures of hypervisors. In this paper, a comprehensive study of VMI approaches is provided with the perspective of facilitating secure attack detection solutions in the virtualization environment. Various open research challenges are identified and discussed in detail. A brief discussion on the various VMI libraries is provided to give some practical insights to readers. We hope that our work will motivate researchers to work in this direction more actively.},
booktitle = {2021 Thirteenth International Conference on Contemporary Computing (IC3-2021)},
pages = {117–124},
numpages = {8},
keywords = {Intrusion Detection System, Virtualization, Hypervisor, Virtual Machine Introspection},
location = {Noida, India},
series = {IC3 '21}
}

@proceedings{10.1145/1576702,
title = {ISSAC '09: Proceedings of the 2009 International Symposium on Symbolic and Algebraic Computation},
year = {2009},
isbn = {9781605586090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 2009 International Symposium on Symbolic and Algebraic Computation (ISSAC 2009), which is jointly sponsored by ACM/SIGSAM and the Korea Institute for Advanced Study (KIAS), will be held at KIAS in Seoul, Korea from July 28 to July 31, 2009. It is the 34th conference in the series that initially alternated between North America and Europe and which began with the seminal 1966 ACM Symposium on Symbolic and Algebraic Manipulation in Washington, D.C. It is the first time ISSAC is held in Korea and the third time ISSAC is held in Asia.The meeting is devoted to research in computer algebra and symbolic computation, covering the following: Algorithmic Mathematics. Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, equations, summation, integration, ODE/PDE, linear algebra, number theory, group-theoretic and geometric computing.Computer Science. Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, parallel/distributed computing and programming languages for symbolic computation, analysis, benchmarking, complexity of computer algebra algorithms, automatic differentiation, code generation, mathematical data structures and exchange protocols.Applications. Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics, statistics, education.The ISSAC 2009 Program contains invited talks, contributed papers, posters, tutorials, and software demonstrations. These Proceedings contain all accepted contributed papers as well as abstracts of the tutorials and invited talks. Poster abstracts will appear in a future issue of the ACM Communications in Computer Algebra. This year's meeting is held in conjunction with SNC 2009, the Third International Workshop on Symbolic Numeric Computation, which will be held August 3-5, 2009 in Kyoto Japan.The Program Committee selected the 47 papers appearing in these Proceedings after careful evaluation including two or more referee reports per submission. We gratefully acknowledge the thorough and important work of the Program Committee Members and external reviewers, whose names appear on the following pages, and thank all the authors and lecturers for their contributions.},
location = {Seoul, Republic of Korea}
}

@proceedings{10.1145/1277548,
title = {ISSAC '07: Proceedings of the 2007 International Symposium on Symbolic and Algebraic Computation},
year = {2007},
isbn = {9781595937438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The International Symposium on Symbolic and Algebraic Computation (ISSAC) 2007 is the 20th meeting in the annual series of that name. Earlier meetings in the series were held variously under the abbreviated names SYMSAC, SYMSAM, EUROCAL and EUROCAM, tracing back to the first meeting, sponsored by ACM, in Washington, DC, in 1966. This year the meeting returns to Waterloo, Canada -- the site of the last meeting prior to the establishment of the ISSAC name -- from July 29th through August 1st. The meeting is devoted to research in computer algebra, covering the following Algorithmic Mathematics. Algebraic, symbolic and symbolic-numeric algorithms. Simplification, function manipulation, equations, summation, integration, ODE/PDE, linear algebra, number theory, group-theoretic and geometric computing.Computer Science. Theoretical and practical problems in symbolic computation. Systems, problem solving environments, user interfaces, software, libraries, parallel/distributed computing and programming languages for symbolic computation, analysis, benchmarking, complexity of computer algebra algorithms, automatic differentiation, code generation, mathematical data structures and exchange protocols.Applications. Problem treatments using algebraic, symbolic or symbolic-numeric computation in an essential or a novel way. Engineering, economics and finance, physical and biological sciences, computer science, logic, mathematics, statistics, education..As is customary, ISSAC 2007 features invited talks, contributed papers, posters, tutorials, and software demos. These proceedings contain the contributed papers and abstracts of the tutorials. Poster abstracts will appear in a future issue of the SIGSAM Bulletin now entitled ACM Communications in Computer Algebra. There are also several satellite events associated with the conference. In particular we mention Symbolic-Numeric Computation (SNC) 2007 and Parallel Symbolic Computation (PASCO) 2007, both held at the University of Western Ontario just prior to ISSAC 2007.There were 98 papers submitted to ISSAC this year. The program committee selected the 50 papers appearing in these proceedings after careful evaluation including two or more referee reports (nearly 3.5 on average) per submission. We gratefully acknowledge the thorough and important work of the program committee members and referees, whose names appear on the following pages, and thank all the authors and lecturers for their contributions.},
location = {Waterloo, Ontario, Canada}
}

@inproceedings{10.1145/1352592.1352616,
author = {Pattabiraman, Karthik and Grover, Vinod and Zorn, Benjamin G.},
title = {Samurai: Protecting Critical Data in Unsafe Languages},
year = {2008},
isbn = {9781605580135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352592.1352616},
doi = {10.1145/1352592.1352616},
abstract = {Programs written in type-unsafe languages such as C and C++ incur costly memory errors that result in corrupted data structures, program crashes, and incorrect results. We present a data-centric solution to memory corruption called critical memory, a memory model that allows programmers to identify and protect data that is critical for correct program execution. Critical memory defines operations to consistently read and update critical data, and ensures that other non-critical updates in the program will not corrupt it. We also present Samurai, a runtime system that implements critical memory in software. Samurai uses replication and forward error correction to provide probabilistic guarantees of critical memory semantics. Because Samurai does not modify memory operations on non-critical data, the majority of memory operations in programs run at full speed, and Samurai is compatible with third party libraries. Using both applications, including a Web server, and libraries (an STL list class and a memory allocator), we evaluate the performance overhead and fault tolerance that Samurai provides. We find that Samurai is a useful and practical approach for the majority of the applications and libraries considered.},
booktitle = {Proceedings of the 3rd ACM SIGOPS/EuroSys European Conference on Computer Systems 2008},
pages = {219–232},
numpages = {14},
keywords = {error recovery, critical memory, memory safety},
location = {Glasgow, Scotland UK},
series = {Eurosys '08}
}

@article{10.1145/1357010.1352616,
author = {Pattabiraman, Karthik and Grover, Vinod and Zorn, Benjamin G.},
title = {Samurai: Protecting Critical Data in Unsafe Languages},
year = {2008},
issue_date = {May 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1357010.1352616},
doi = {10.1145/1357010.1352616},
abstract = {Programs written in type-unsafe languages such as C and C++ incur costly memory errors that result in corrupted data structures, program crashes, and incorrect results. We present a data-centric solution to memory corruption called critical memory, a memory model that allows programmers to identify and protect data that is critical for correct program execution. Critical memory defines operations to consistently read and update critical data, and ensures that other non-critical updates in the program will not corrupt it. We also present Samurai, a runtime system that implements critical memory in software. Samurai uses replication and forward error correction to provide probabilistic guarantees of critical memory semantics. Because Samurai does not modify memory operations on non-critical data, the majority of memory operations in programs run at full speed, and Samurai is compatible with third party libraries. Using both applications, including a Web server, and libraries (an STL list class and a memory allocator), we evaluate the performance overhead and fault tolerance that Samurai provides. We find that Samurai is a useful and practical approach for the majority of the applications and libraries considered.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {219–232},
numpages = {14},
keywords = {memory safety, error recovery, critical memory}
}

@inproceedings{10.5555/42040.42152,
author = {McKenzie, J. L. and Rodman, J. A.},
title = {TIINS, the Missing Link for Innovation and Economic Development},
year = {1987},
isbn = {0818608110},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {This paper will discuss previous studies and modeling undertaken in the field of innovation. Specifically the goal of the paper is to show the economic need for innovation in a developed country and then provide a synopsis of how the Texas Innovation Information Network System (TIINS) aides in achieving this goal. Both the development of TIINS and the support for TIINS are outlined.The lack of accurate, timely communication between research and development personnel and marketing personnel has created an “innovation time gap” which is costly to the firm and the economy of the country. In order to perceive the seriousness of the implications of the problem a clear picture of the relationships of trade, wages, and innovation is necessary, more precisely, a cognizance of how innovation drives trade, wages, and transfer of technology. David Dollar presents an econometric model which details this relationship.The model presented by Dollar divides the world into two sections which he calls the North and South. The North is defined as a developed country and the South as a developing country. His theory would hold if one were to call the areas Texas and Mississippi or the U.S. and Mexico, considering Texas more advanced technologically than Mississippi and the U.S. more advanced technologically than Mexico, instead of North and South. Dollar says, “…the North's ability to introduce and temporarily monopolize new technology enables its workers to earn a premium over the wages paid to their counterparts in the South; this difference in labor costs in turn provides the incentive for technology to be diffused to the South, driving the product cycle.”1 While emphasis in this model is on wage rate differentials it should also be noted that a comparative advantage of one geographical area over another in any one of the parameters driving the model may generate the same results. It is the feasibility of this model being applied to local, national, and international economies which makes it of interest in explaining the necessity of innovation in our current market structure. Many times modeling, and particularly economic modeling is viewed as being somewhat unrealistic, however, Dollar points out, “The process described by the model is consistent with recent experience. In the 1960's the main effect that population growth and industrialization in the Third World had on the economies of the developed nations was to generate demand for the North's products. In the 1970's and 1980's however, many northern industries have been loosing out in competition with Third World suppliers, significantly reducing the demand for industrial labor in the developed countries and putting downward pressure on wages there.”2Dollar notes that as technology flows to the South the product will be produced there rather than in the North due to the less costly production function. Taking into consideration this technology flow he makes the following statements in summation: “The model constructed in this paper pinpoints innovation in the North as the factor that enables and in fact requires that workers in the North earn a premium over wages in the South. …constant innovation is necessary just to maintain living standards in the North; in the absence of innovation the transfer of capital and technology to the South would undermine northern wages. …Much of the production in industries like steel, textiles, automobiles, and electronics has moved to the South; new industries, however, have not arisen at a fast enough pace to employ all of the displaced workers in the developed countries. This process has no doubt been a prime cause of the high levels of unemployment in the industrial nations over the last decade.”3Dollar shows the need not only for innovation but for continuous innovation by developed economies. In addition to Dollar's model, D. Bruce Merrifield developed a model for determining the merit to a company a new R&amp;D project might have. Merrifield defines this model as an index of merit and defines it as:Im = return on investment*probability of success/cost*time requiredMerrifield indicates, “The index of merit is a direct function of the return on investment (ROI) expected from the successful effort times the probability of technical success and is an inverse function of the projected cost and time to achieve success.”4 One of the main components contributing to this lack of speed has been seen as the deficiency of communication between research and design companies personnel and marketing firms personnel.Roger A. More, in the 1986 Journal of Business Research, discusses innovative new products and states, “Adoption of such products has frequently been slower than expected, has provided lower profit margins, has required much greater developer resources than expected, and, in some cases, has resulted in withdrawal of the product from the market with severe financial losses.”5 He further notes the causative relationship for this failure stating, “…the major failure of developers is in effectively understanding their temporal interorganizational relationship with potential customer companies.”6 Gupta, Raj and Wilemon, in their article in the April 1986 issue of Journal of Marketing, point out, “Although all functional interfaces are important in the product development process, the R&amp;D-marketing interface is one of the most critical ones.”7 In discussing integration and innovation success Gupta, Raj, and Wilemon say, “Several major studies have concluded that innovation success rests on a combination of technical feasibility and market demand recognition and interpretation (Gruber 1981). Research by Dunn and Boyd (1975), Young (1979), Crawford (1977), Schon (1967), Carroad and Carroad (1982), Monteleone (1976), Mansfield et al. (1971, 1975, 1981), Gerstenfeld and Suniyoshi (1980), Wind (1981, 1982), and Cooper (1983a) have emphasized the importance of effective integration of R&amp;D and marketing for innovation success. Supporting these conclusions are several studies (Gruber, Poensgen, and Prakke 1973; Sonder 1977, 1981, Young 1973) which have noted that the failure to integrate R&amp;D and marketing early in the innovation process is one of the biggest contributors to new product failure.”8It is in this information access and linking phase that the Texas Innovation Information Network System (TIINS) can substantially aide R&amp;D, manufacturing and marketing. Through the TIINS data base the information gap can be drastically reduced and innovation in Texas and the U.S. enhanced resulting in expanded economic growth. More explicitly TIINS can decrease the “time required” variable in Merrifield's model thereby increasing the index of merit. It becomes obvious that as Im (index of merit) increases more innovation, as is necessary per the Dollar model, takes place.In the model below, developed by the U.S. Department of Commerce, TIINS network will be instrumental in reducing the time loss in the area labeled GAP. Again, this will reduce the 7-10 year time line and enhance the competitiveness of American companies.Texas businessmen and politicians alike have recognized the importance of the TINNS network in developing and strengthening the economy. William M. Winsor, president of INFOMART at Dallas is quoted, “INFOMART supports this effort because there is a critical need for educators, scientists and business people to be working more closely together in Texas and in the United States. Everyone involved with INFOMART knows how critical it is to have a competitive edge. TIINS will provide scientists and entrepreneurs with a competitive edge by helping bring together talented people. To put it in simple terms, this network will help make sure that the 'left hand' knows what the 'right hand' can do — and is doing.” Senator Chet Edwards noted, “Economic diversification is essential for Texas. The need for a comprehensive system that represents current scientific and engineering capabilities is long overdue.” TIINS was created after studies indicated that Texas needed a research and technology information system to develop new technology business and help diversify the economy. TIINS has subsequently received enthusiastic support from Texas' former governor, Mark White, and the present governor, William Clements. Although similar programs are working in Ohio, New York and Illinois, they deal primarily with the relationships between universities and industry. With its regional centers and heavy emphasis on advanced technology business and professional service organizations, TIINS is much broader in scope than its predecessors.TIINS is a people to people data base which list the engineering and scientific expertise of scientist and engineers throughout the state of Texas. In addition the data base will include venture capitalist, industry firms with interest in technological innovation and professional services such as patent attorneys and marketing firms. The key is that through the computer program, currently operating, TIINS can link these individuals, disseminating pertinent information, in a very timely, cost effective manner.The TIINS database system currently runs on any IBM compatible micro system with a hard disk. The software programs developed to manage the TIINS information uses PC Manager, an advanced data base management system. The system allows a very fast keyword search up to five terms using bullion logic. It has an inverted file that rapidly searches the keyword capability listed on individual and company profiles. The data base management system was chosen because of PC Manager's cooperation, cost and understanding of this system by local programmers. This DBMS is similar to others such as DB-3, R Base, Revelation, and Data Flex.Also, TIINS is working with Keplinger Companies in Houston and plans to transfer the data to their mainframe when sufficient records are entered. However, the system will continue to be available on the micro computer.The uniqueness of TIINS in comparison to other states is the selection of the data bases to be included in the system (Scientists &amp; Engineers, Advanced Technology Companies, and Professional Services), and the fields selected to be included in each of these data bases.As an example, to determine what types of information should be included in the Scientist and Engineer data base the project director interviewed over 50 companies in three states to determine the specific type of data these advanced technology companies need to know quickly about specific qualified researchers. Also, he examined other information sources such as Who's Who, Illinois Resource Network and faculty profile systems in place at many universities. The six screens of information, which compile a very nice two page resume of research, consist of major information required by the user to determine with whom to interface. Major data fields include: name, address, phone number, title, degree, institution received terminal degree, research and technology key codes &amp; words, major and minor current research capability, four major publications, four recent grant/contracts, consulting arrangements, education and training capability, research honors &amp; awards, patents, copyrights, and licenses. The majority of business executives interviewed were dissatisfied with 20 page vitaes faculty members were sending them and only needed the above information.Expertise can be searched very efficiently by using the NSF/NIH Keyword Thesaurus terms but the system can also search on any field in the data base with user selected specific criteria. An example of the search screen and printout is listed below:The previous example shows three engineering terms were selected and the screen displays the name, phone number, institution, and TEDC Region of the scientists and engineers that have all three terms listed in their specific profile. From the screen you can select any of the individuals listed, examine their complete record and print the profiles of the individuals selected.Working with the TIINS Advisory Council, consisting of leading business and academic professionals, two additional data bases (Advanced Technology Companies and Professional Service Organizations) have been developed. Information requested from these organizations includes: name, address, R&amp;D director and phone number, research &amp; technology keywords, current research capability, current research needs, type of service/product provided, R&amp;D support, SBIR activity, and university/industry interface programs (sharing of equipment, serving on Boards, fellowships, contracts, etc.) Questionnaires have been sent to over 3,000 advanced technology companies and 1,000 professional service organizations in the state.Two additional data bases are planned. One will capture continuing and professional educational training opportunities, while the other will deal with research equipment, instrumentation and facilities available in the state.Any person or corporation involved in the innovation process is a potential TIINS customer. Accounting Firms, advanced technology companies, attorneys, bankers, chambers of commerce, city/county agencies, libraries, major corporations, non-profit research institutes, publishing companies, state legislators, scientists and engineers, universities, and venture capitalists are all potential clients who will benefit from TIINS.As an example, a small business entrepreneur seeking assistance in writing an SBIR grant application for DOD (Department of Defense) needs technical assistance from a university professor, a venture capitalist looking for experts to evaluate the technical feasibility of the business plan, an engineer with a recent patent looking for an advanced technology company in the region to manufacture the product, a large defense company seeking a small disadvantaged high technology companies for subcontractors, an advanced technology company seeking companies with similar or complimentary research and technology to work on joint ventures, and law firms needing expert witnesses, are just a few of the ways TIINS will be utilized.There will be four ways for individuals to access the data base once completed. First, individuals calling one of the seven regional innovation centers and getting professional assistance in identifying the appropriate research and technology information they require and receiving a hard copy of the data. The second, will allow companies with terminals and modems to have an ID number and dial directly into the data base and do their own searching. Keplinger Companies in Houston has contributed use of their main frame computer and software for this access. The third, will be allowing a major data base service like DIALOGUE, to market TIINS to out-of-state users. Finally, charter subscribers and other major users will be licensed to use TIINS software and have complete updated information available for use on their own micro computer. This will allow major companies unlimited use of all the data bases for one low annual subscription fee. The major objective is to have current information on all sectors of the innovation process easily available at low cost. Developing a system with accurate and current information that will be constantly used by all sectors in the innovation process is the key to TIINS success.The infrastructure, as currently established, consist of one central office in Dallas and seven regional centers located at Texas Tech University in Lubbock, North Texas Commission in Dallas, The University of Texas at Tyler in Tyler, Texas Engineering Experiment Station in College Station, Small Business Development Center in Houston, The University of Texas at Austin in Austin, and Texas Research &amp; Technology Foundation in San Antonio. The regional centers are the service arm of the organization and as such are the direct contact point for users. The regional centers collect, update, and input data into the system on a daily basis. On a monthly basis this data is copied on floppy discs and forwarded to the central office. The central office acting as a coordinator then merges the data from the seven centers and redistributes the data to the centers. This gives each center statewide information which is current to within thirty days.Support from the state has been very positive. During the 1987 session of legislature, Senate Bill 776, introduced by Senator Chet Edwards, and House Bill 1406 introduced by Representative Paul Colbert, passed both houses authorizing appropriations of $250,000.00 per year for the next two years. This funding will be used to help cover the cost of building the data base up to the ten thousand records and maintaining these records once they are on line. After the third year of operation projections indicate TIINS will be capable of self support from user and membership fees.},
booktitle = {Proceedings of the 1987 Fall Joint Computer Conference on Exploring Technology: Today and Tomorrow},
pages = {708–711},
numpages = {4},
location = {Dallas, Texas, USA},
series = {ACM '87}
}

@inproceedings{10.1145/1408681.1408682,
author = {Hickey, Rich},
title = {The Clojure Programming Language},
year = {2008},
isbn = {9781605582702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1408681.1408682},
doi = {10.1145/1408681.1408682},
abstract = {Customers and stakeholders have substantial investments in, and are comfortable with the performance, security and stability of, industry-standard platforms like the JVM and CLR. While Java and C# developers on those platforms may envy the succinctness, flexibility and productivity of dynamic languages, they have concerns about running on customer-approved infrastructure, access to their existing code base and libraries, and performance. In addition, they face ongoing problems dealing with concurrency using native threads and locking. Clojure is an effort in pragmatic dynamic language design in this context. It endeavors to be a general-purpose language suitable in those areas where Java is suitable. It reflects the reality that, for the concurrent programming future, pervasive, unmoderated mutation simply has to go. Clojure meets its goals by: embracing an industry-standard, open platform - the JVM; modernizing a venerable language - Lisp; fostering functional programming with immutable persistent data structures; and providing built-in concurrency support via software transactional memory and asynchronous agents. The result is robust, practical, and fast. This talk will focus on the motivations, mechanisms and experiences of the implementation of Clojure.},
booktitle = {Proceedings of the 2008 Symposium on Dynamic Languages},
articleno = {1},
numpages = {1},
location = {Paphos, Cyprus},
series = {DLS '08}
}

