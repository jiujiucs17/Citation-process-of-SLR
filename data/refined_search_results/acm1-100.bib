@article{10.1145/3561383,
author = {Patnaik, Nikhil and Dwyer, Andrew C. and Hallett, Joseph and Rashid, Awais},
title = {SLR: From Saltzer &amp; Schroeder to 2021...},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3561383},
doi = {10.1145/3561383},
abstract = {Producing secure software is challenging. The poor usability of security Application Programming Interfaces (APIs) makes this even harder. Many recommendations have been proposed to support developers by improving the usability of cryptography libraries; rooted in wider best practice guidance in software engineering and API design. In this SLR, we systematize knowledge regarding these recommendations. We identify and analyze 65 papers, offering 883 recommendations. Through thematic analysis, we identify 7 core ways to improve usability of APIs. Most of the recommendations focus on helping API developers to construct and structure their code and make it more usable and easier for programmers to understand. There is less focus, however, on documentation, writing requirements, code quality assessment and the impact of organizational software development practices. By tracing and analyzing paper ancestry, we map how this knowledge becomes validated and translated over time. We find that very few API usability recommendations are empirically validated, and that recommendations specific to usable security APIs lag even further behind.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
keywords = {API, security, recommendations, usability, SLR}
}

@inproceedings{10.1145/2499583.2499589,
author = {Palma, Ra\'{u}l and Corcho, Oscar and Hotubowicz, Piotr and P\'{e}rez, Sara and Page, Kevin and Mazurek, Cezary},
title = {Digital Libraries for the Preservation of Research Methods and Associated Artifacts},
year = {2013},
isbn = {9781450321853},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499583.2499589},
doi = {10.1145/2499583.2499589},
abstract = {New digital artifacts are emerging in data-intensive science. For example, scientific workflows are executable descriptions of scientific procedures that define the sequence of computational steps in an automated data analysis, supporting reproducible research and the sharing and replication of best-practice and know-how through reuse. Workflows are specified at design time and interpreted through their execution in a variety of situations, environments, and domains. Hence it is essential to preserve both their static and dynamic aspects, along with the research context in which they are used. To achieve this, we propose the use of multidimensional digital objects (Research Objects) that aggregate the resources used and/or produced in scientific investigations, including workflow models, provenance of their executions, and links to the relevant associated resources, along with the provision of technological support for their preservation and efficient retrieval and reuse. In this direction, we specified a software architecture for the design and implementation of a Research Object preservation system, and realized this architecture with a set of services and clients, drawing together practices in digital libraries, preservation systems, workflow management, social networking and Semantic Web technologies. In this paper, we describe the backbone system of this realization, a digital library system built on top of dLibra.},
booktitle = {Proceedings of the 1st International Workshop on Digital Preservation of Research Methods and Artefacts},
pages = {8–15},
numpages = {8},
keywords = {libraries, semantic, preservation, evolution},
location = {Indianapolis, Indiana, USA},
series = {DPRMA '13}
}

@article{10.1145/131736.131739,
author = {Ostertag, Eduardo and Hendler, James and D\'{\i}az, Rub\'{e}n Prieto and Braun, Christine},
title = {Computing Similarity in a Reuse Library System: An AI-Based Approach},
year = {1992},
issue_date = {July 1992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/131736.131739},
doi = {10.1145/131736.131739},
abstract = {This paper presents an AI based library system for software reuse, called AIRS, that allows a developer to browse a software library in search of components that best meet some stated requirement. A component is described by a set of (feature, term) pairs. A feature represents a classification criterion, and is defined by a set of related terms. The system allows to represent packages (logical units that group a set of components) which are also described in terms of features. Candidate reuse components and packages are selected from the library based on the degree of similarity between their descriptions and a given target description. Similarity is quantified by a nonnegative magnitude (distance) proportional to the effort required to obtain the target given a candidate. Distances are computed by comparator functions based on the subsumption, closeness, and package relations. We present a formalization of the concepts on which the AIRS system is based. The functionality of a prototype implementation of the AIRS system is illustrated by application to two different software libraries: a set of Ada packages for data structure manipulation, and a set of C components for use in Command, Control, and Information Systems. Finally, we discuss some of the ideas we are currently exploring to automate the construction of AIRS classification libraries.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
pages = {205–228},
numpages = {24},
keywords = {facet classification, similarity-based retrieval}
}

@inproceedings{10.1145/3422392.3422504,
author = {Silva, Deyvson and Gomes, Adriano and Macieira, Rafael and Silva, Emanoel and Soares, Sergio},
title = {Pah Pum: A Project Management Tool Based on TAKT PM},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422504},
doi = {10.1145/3422392.3422504},
abstract = {This article presents Pah Pum, a project management tool based on Takt PM, a hybrid project management methodology. The tool was developed at SENAI Institute of Innovation for Information and Communication Technologies to facilitate adherence to Takt PM, offering processes systematization and using best practices suggested by the methodology. The tool supports (1) project planning using a service package diagram called Handoff Network, (2) project monitoring through Kanban and Kanban Quantum boards, (3) automatic Gantt generation and update, according to the project Handoff Network and service packages deliveries, (4) monitoring teams through the observation of a unified board that demonstrates the allocation of the team with the activities that are running or waiting to be performed, (5) automatic allocation of service packages to the teams considering the limit of in progress activities of the team configured to the organization, implementing the pull system, (6) status report of service package delivery and project progress, in a non-declarative way with evidences, and (7) visualization of the project timeline showing the transaction history that includes the project start, service packages deliveries, and service packages rejections that were performed during the project. We used an MVC software architecture to implement the solution, composed by front-end in Vue.js, an API following the REST standard in .Net core, and a SQL Server database. Also, the tool was integrated with the SENAI Technology Management System (a particular ERP system). All modules developed were dockerized and deployed on AWS machines. To access the tool, sign up on https://pahpum.isitics.com, and the administrators will approve the access. We are currently evaluating the tool through a qualitative study to understand the positive and negative impacts through the tool's adoption during the execution of real projects.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {332–337},
numpages = {6},
keywords = {Software development, Project management, Takt PM, Project planning, Agile methodologies},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1109/ISCA45697.2020.00031,
author = {Ham, Tae Jun and Bruns-Smith, David and Sweeney, Brendan and Lee, Yejin and Seo, Seong Hoon and Song, U Gyeong and Oh, Young H. and Asanovic, Krste and Lee, Jae W. and Wills, Lisa Wu},
title = {Genesis: A Hardware Acceleration Framework for Genomic Data Analysis},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00031},
doi = {10.1109/ISCA45697.2020.00031},
abstract = {In this paper, we describe our vision to accelerate algorithms in the domain of genomic data analysis by proposing a framework called Genesis (<u>gen</u>om<u>e</u> analy<u>sis</u>) that contains an interface and an implementation of a system that processes genomic data efficiently. This framework can be deployed in the cloud and exploit the FPGAs-as-a-service paradigm to provide cost-efficient secondary DNA analysis. We propose conceptualizing genomic reads and associated read attributes as a very large relational database and using extended SQL as a domain-specific language to construct queries that form various data manipulation operations. To accelerate such queries, we design a Genesis hardware library which consists of primitive hardware modules that can be composed to construct a dataflow architecture specialized for those queries.As a proof of concept for the Genesis framework, we present the architecture and the hardware implementation of several genomic analysis stages in the secondary analysis pipeline corresponding to the best known software analysis toolkit, GATK4 workflow proposed by the Broad Institute. We walk through the construction of genomic data analysis operations using a sequence of SQL-style queries and show how Genesis hardware library modules can be utilized to construct the hardware pipelines designed to accelerate such queries. We exploit parallelism and data reuse by utilizing a dataflow architecture along with the use of on-chip scratchpads as well as non-blocking APIs to manage the accelerators, allowing concurrent execution of the accelerator and the host. Our accelerated system deployed on the cloud FPGA performs up to 19.3x better than GATK4 running on a commodity multi-core Xeon server and obtains up to 15x better cost savings. We believe that if a software algorithm can be mapped onto a hardware library to utilize the underlying accelerator(s) using an already-standardized software interface such as SQL, while allowing the efficient mapping of such interface to primitive hardware modules as we have demonstrated here, it will expedite the acceleration of domain-specific algorithms and allow the easy adaptation of algorithm changes.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {254–267},
numpages = {14},
keywords = {genomic data analysis, hardware accelerator, genome sequencing, SQL, FPGA},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1145/3342195.3387515,
author = {Correia, Andreia and Felber, Pascal and Ramalhete, Pedro},
title = {Persistent Memory and the Rise of Universal Constructions},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387515},
doi = {10.1145/3342195.3387515},
abstract = {Non-Volatile Main Memory (NVMM) has brought forth the need for data structures that are not only concurrent but also resilient to non-corrupting failures. Until now, persistent transactional memory libraries (PTMs) have focused on providing correct recovery from non-corrupting failures without memory leaks. Most PTMs that provide concurrent access do so with blocking progress.The main focus of this paper is to design practical PTMs with wait-free progress based on universal constructions. We first present CX-PUC, the first bounded wait-free persistent universal construction requiring no annotation of the underlying sequential data structure. CX-PUC is an adaptation to persistence of CX, a recently proposed universal construction. We next introduce CX-PTM, a PTM that achieves better throughput and supports transactions over multiple data structure instances, at the price of requiring annotation of the loads and stores in the data structure---as is commonplace in software transactional memory. Finally, we propose a new generic construction, Redo-PTM, based on a finite number of replicas and Herlihy's wait-free consensus, which uses physical instead of logical logging. By exploiting its capability of providing wait-free ACID transactions, we have used Redo-PTM to implement the world's first persistent key-value store with bounded wait-free progress.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {5},
numpages = {15},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@article{10.1145/1064165.1064170,
author = {Beloglavec, Simon and Heri\v{c}ko, Marjan and Juri\v{c}, Matja\v{z} B. and Rozman, Ivan},
title = {Analysis of the Limitations of Multiple Client Handling in a Java Server Environment},
year = {2005},
issue_date = {April 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/1064165.1064170},
doi = {10.1145/1064165.1064170},
abstract = {A server infrastructure in web servers, message servers and other parallel systems use a variation of two software architectures for providing concurrency: threaded or event-driven. This paper analyzes the performance limitations of concurrent applications implemented in Java. Both architectures have been evaluated and compared with various design patterns, which combine the best practices from both architectures. For each architecture the suitability for handling a large volume of client requests, the efficient management of a server load, the influence of client request structures, and the physical size of a client request, have been studied. The discussed Java APIs are core technologies for high-level APIs, used in developing web and distributed applications. The research also includes performance comparison on various platforms and discusses performance variation on various versions of a Java runtime. The paper contributes to the understanding of Java-based server architecture capabilities. Core server software architectures and required Java libraries are compared, the reasons for the limitations are identified and guidelines for choosing proper combinations are given.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {20–28},
numpages = {9},
keywords = {threaded server, Java networking, event-driven server}
}

@inproceedings{10.1145/1596600.1596604,
author = {Nystr\"{o}m, Jan Henry},
title = {Automatic Assessment of Failure Recovery in Erlang Applications},
year = {2009},
isbn = {9781605585079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596600.1596604},
doi = {10.1145/1596600.1596604},
abstract = {Erlang is a concurrent functional language, especially tailored for distributed, highly concurrent and fault-tolerant software. An important part of Erlang is its support for failure recovery. A designer implements failure recovery by organising the processes of an Erlang application into tree structures, in which parent processes monitor failures of their children and are responsible for their restart. Libraries support the creation of such structures during system initialisation. We present a technique to automatically analyse that the process structure of an Erlang application is constructed in a way that guarantees recovery from process failures. First, we extract (part of) the process structure by static analysis of the initialisation code of the application. Thereafter, analysis of the process structure checks that it will recover from any process failure. We have implemented the technique in a tool, and applied it to several OTP library applications and to a subsystem of the AXD 301 ATM switch.},
booktitle = {Proceedings of the 8th ACM SIGPLAN Workshop on ERLANG},
pages = {23–32},
numpages = {10},
keywords = {erlang},
location = {Edinburgh, Scotland},
series = {ERLANG '09}
}

@inproceedings{10.1145/3368089.3409711,
author = {Larios Vargas, Enrique and Aniche, Maur\'{\i}cio and Treude, Christoph and Bruntink, Magiel and Gousios, Georgios},
title = {Selecting Third-Party Libraries: The Practitioners’ Perspective},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409711},
doi = {10.1145/3368089.3409711},
abstract = {The selection of third-party libraries is an essential element of virtually any software development project. However, deciding which libraries to choose is a challenging practical problem. Selecting the wrong library can severely impact a software project in terms of cost, time, and development effort, with the severity of the impact depending on the role of the library in the software architecture, among others. Despite the importance of following a careful library selection process, in practice, the selection of third-party libraries is still conducted in an ad-hoc manner, where dozens of factors play an influential role in the decision. In this paper, we study the factors that influence the selection process of libraries, as perceived by industry developers. To that aim, we perform a cross-sectional interview study with 16 developers from 11 different businesses and survey 115 developers that are involved in the selection of libraries. We systematically devised a comprehensive set of 26 technical, human, and economic factors that developers take into consideration when selecting a software library. Eight of these factors are new to the literature. We explain each of these factors and how they play a role in the decision. Finally, we discuss the implications of our work to library maintainers, potential library users, package manager developers, and empirical software engineering researchers.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {245–256},
numpages = {12},
keywords = {empirical software engineering, library selection, APIs, library adoption, software libraries},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2556288.2556998,
author = {Fast, Ethan and Steffee, Daniel and Wang, Lucy and Brandt, Joel R. and Bernstein, Michael S.},
title = {Emergent, Crowd-Scale Programming Practice in the IDE},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556998},
doi = {10.1145/2556288.2556998},
abstract = {While emergent behaviors are uncodified across many domains such as programming and writing, interfaces need explicit rules to support users. We hypothesize that by codifying emergent programming behavior, software engineering interfaces can support a far broader set of developer needs. To explore this idea, we built Codex, a knowledge base that records common practice for the Ruby programming language by indexing over three million lines of popular code. Codex enables new data-driven interfaces for programming systems: statistical linting, identifying code that is unlikely to occur in practice and may constitute a bug; pattern annotation, automatically discovering common programming idioms and annotating them with metadata using expert crowdsourcing; and library generation, constructing a utility package that encapsulates and reflects emergent software practice. We evaluate these applications to find Codex captures a broad swatch of programming practice, statistical linting detects problematic code snippets, and pattern annotation discovers nontrivial idioms such as basic HTTP authentication and database migration templates. Our work suggests that operationalizing practice-driven knowledge in structured domains such as programming can enable a new class of user interfaces.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {2491–2500},
numpages = {10},
keywords = {programming tools, data mining},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@article{10.5555/1629116.1629137,
author = {McGuire, Timothy J.},
title = {Introducing Multi-Core Programming into the Lower-Level Curriculum: An Incremental Approach},
year = {2010},
issue_date = {January 2010},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {3},
issn = {1937-4771},
abstract = {Historically, improved hardware processing power has been due to increases in clock speed. Recently, however, chip manufacturers have begun to increase overall processing power by adding additional processing cores to the microprocessor package, while clock speeds have remained virtually unchanged. New processors will eventually come in heterogeneous configurations such as combinations of high- and low-power cores, graphical processors, etc. These future configurations are being termed "many-core" architectures. For software developers, this change raises many challenges. No longer will programmers be able to "ride the wave" of increasing performance without explicitly taking advantage of parallelism. Currently, only a very small proportion of developers have expertise in parallel programming. Software developers need new programming models, tools, and abstraction by the operating system to handle concurrency and complexity of numerous processors. For computer science educators, this change will also require radical shifts in the way computer science is taught. Parallelism will need to be introduced early in the curriculum, preferably in the CS1/CS2 sequence.Despite the fact that a small group of programmers have been programming parallel applications for many years, most programmers have only a cursory understanding of the issues involved in developing multi-core applications. As machines with 32 or more cores become commonplace, students must gain a working knowledge of how to develop parallel programs. It seems evident that students must be exposed to concurrency throughout the curriculum, beginning with the introductory CS sequence.While new parallel languages will be developed (as well as extensions to existing languages) it is not yet evident which direction that development will head. Message-passing (MPI) and threads (POSIX threads and Java threads) have been the methods of choice for teaching parallelism at the undergraduate level. Since multi-core systems use shared-memory, and the message-passing model is more suited for clusters than it is for shared-memory systems, it is not a likely candidate for multi-core and many- core systems. Java threads have the advantage of being built-in, so the language can be used for parallel programming on multi-core machines. Although threads may be seen as a small syntactic extension to sequential processing, as a computational model, they are non-deterministic, and the programmer's task when using them is to "prune" that non-determinism. POSIX threads are implemented by a standard library providing a set of C calls for writing multithreaded code. They have the same difficulties in programming that Java threads do.OpenMP (Open Multi-Processing) is an API which supports shared memory multiprocessing. It consists of a set of compiler directives and a library of support functions. Open MP works in conjunction with Fortran, C, and C++. Compared to Pthreads, OpenMP allows for a higher level of abstraction, allowing a programmer to partition a program into serial regions and parallel regions (rather than a set of concurrently executing threads.) It also provides intuitive synchronization constructs.This tutorial will survey the parallel programming landscape, summarize the OpenMP approach to multi-threading, and illustrate how it can be used to introduce parallelism into the lower-level curriculum to novice or intermediate C programmers.},
journal = {J. Comput. Sci. Coll.},
month = {jan},
pages = {118–119},
numpages = {2}
}

@inproceedings{10.1145/16894.16873,
author = {Batory, D. S. and Mannino, M.},
title = {Panel: Extensible Database Systems},
year = {1986},
isbn = {0897911911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/16894.16873},
doi = {10.1145/16894.16873},
abstract = {New implementation techniques and new capabilities for database systems are being developed and proposed at a rapid rate. Novel file structures and improved algorithms for query optimization, buffer and recovery management, and transaction management have the potential of realizing significant gains in DBMS performance. The proposed integration of design objects, voice, text, rules, vector graphics, and images into databases promises exciting new capabilities for DBMSs. To accommodate advances in database technology and to support new classes of database applications, DBMSs must be extensible (i.e., customizable).To achieve extensibility forces a fundamental rethinking about how DBMSs are built, and how special-purpose features can be integrated into a DBMS with little effort and expense. Customizing DBMSs implies the availability of extensible data models, to allow for the introduction of new object types and operations, and extensible storage structures, to take advantage of special properties of stored data or operations to enhance performance.Although research on extensible DBMSs is still in its infancy, a fundamental concept underlying their construction is now evident. This is the standardization of interfaces and the plug-compatibility of modules. An extensible DBMS will be a 'software bus' whereby new modules (and hence new DBMS capabilities) can be added, exchanged, or removed by plugging or unplugging modules. Extensible DBMSs will thus rely on extensive software libraries, where new modules can be added as needed. Furthermore, changes to DBMSs can be made in months rather than years, and the reinvention of established technology is kept to a minimum because of the reusability of modules.The perception of DBMSs as monolithic entities that are difficult to modify will change as extensible DBMS technology becomes better understood. The use of database systems will not change, the ANSI/SPARC roles of database users, who write and execute transactions, and the database administrator (DBA), who designs and writes database schemas, will remain. Extensible DBMSs will require the introduction of an additional party, the database architecture administrator (DDA), who is responsible for the construction and customization of a DBMS.A growing number of researchers are developing extensible DBMSs. The purpose of this panel is to explain and discuss some of the approaches that are now being taken (and those that can be taken), and to survey the problems that confront extensible database technology. Descriptions of the systems and research represented at this panel are given in the following sections.},
booktitle = {Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data},
pages = {187–190},
numpages = {4},
location = {Washington, D.C., USA},
series = {SIGMOD '86}
}

@article{10.1145/16856.16873,
author = {Batory, D. S. and Mannino, M.},
title = {Panel: Extensible Database Systems},
year = {1986},
issue_date = {June 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/16856.16873},
doi = {10.1145/16856.16873},
abstract = {New implementation techniques and new capabilities for database systems are being developed and proposed at a rapid rate. Novel file structures and improved algorithms for query optimization, buffer and recovery management, and transaction management have the potential of realizing significant gains in DBMS performance. The proposed integration of design objects, voice, text, rules, vector graphics, and images into databases promises exciting new capabilities for DBMSs. To accommodate advances in database technology and to support new classes of database applications, DBMSs must be extensible (i.e., customizable).To achieve extensibility forces a fundamental rethinking about how DBMSs are built, and how special-purpose features can be integrated into a DBMS with little effort and expense. Customizing DBMSs implies the availability of extensible data models, to allow for the introduction of new object types and operations, and extensible storage structures, to take advantage of special properties of stored data or operations to enhance performance.Although research on extensible DBMSs is still in its infancy, a fundamental concept underlying their construction is now evident. This is the standardization of interfaces and the plug-compatibility of modules. An extensible DBMS will be a 'software bus' whereby new modules (and hence new DBMS capabilities) can be added, exchanged, or removed by plugging or unplugging modules. Extensible DBMSs will thus rely on extensive software libraries, where new modules can be added as needed. Furthermore, changes to DBMSs can be made in months rather than years, and the reinvention of established technology is kept to a minimum because of the reusability of modules.The perception of DBMSs as monolithic entities that are difficult to modify will change as extensible DBMS technology becomes better understood. The use of database systems will not change, the ANSI/SPARC roles of database users, who write and execute transactions, and the database administrator (DBA), who designs and writes database schemas, will remain. Extensible DBMSs will require the introduction of an additional party, the database architecture administrator (DDA), who is responsible for the construction and customization of a DBMS.A growing number of researchers are developing extensible DBMSs. The purpose of this panel is to explain and discuss some of the approaches that are now being taken (and those that can be taken), and to survey the problems that confront extensible database technology. Descriptions of the systems and research represented at this panel are given in the following sections.},
journal = {SIGMOD Rec.},
month = {jun},
pages = {187–190},
numpages = {4}
}

@inproceedings{10.1145/318741.318766,
author = {Johnson, Paul L.},
title = {Doe Software: Scientific and Engineering Software for Sharing},
year = {1985},
isbn = {0897911679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/318741.318766},
doi = {10.1145/318741.318766},
abstract = {This presentation describes the methods used by the National Energy Software Center to facilitate the sharing of DOE software. The focus of the presentation will be on the materials requested of the developers and the software screening procedure.The National Energy Software Center (NESC) is the U. S. Department of Energy (DOE) software exchange and information center. Currently, the NESC collection contains approximately 1250 computer programs, which are licensed to installations on an indefinite use basis.In carrying out the objective of sharing software and transferring DOE developed applications to the information processing community, NESC has developed a checklist of recommended package elements, software screening procedures, and dissemination practices designed to enhance portability. The goal of these procedures is that software will be usable by individuals other than the author and operable in computing environments different from those in which they are developed. From the start of the development project, it should be planned that adequate documentation be provided and the software should be written to be transferable. The NESC has held installation representative (IR) meetings in 1984 and 1985 with the intent of improving communications with the IRs; these topics of documentation and portability were covered there.The recommended contents of an NESC software package are defined as the aggregate of all elements required for use of the software by another user. This is formed from two components: the computer-media material and the printed material.The material may include all of the following: Source decks (S), Run decks (B), Sample problems (P), Data libraries (L), Control information (C), Auxiliary information (X), and Documentation (R). Not all seven elements are required to successfully transfer every program but R, S, and P are almost always present.The documentation should include a definition of the required computing environment, should describe the software, and should provide the user with a guide for specifying input data and for interpreting the results. Sample input and output are considered to be very important in verifying the successful transfer of software. These sample problem data can be found in either the printed material or the computer-media material.The source language material is usually found on magnetic tape. ANSI standard languages are encouraged in order that the software be portable and assembly language should be used only when necessary. Extensions to standard languages should be avoided, and these can be found when using alternate compilers or machines. NESC discourages the reliance on operating systems to preset areas of storage to assigned values such as zero. Error conditions should come from the software rather than the operating system.If the software is sufficiently complex, it is essential for control information to be included. This would describe the assignment of devices, storage allocation, and overlay structure, if present. Difficulties with local control information are prevalent as this information is often customized.Data should be formatted in order to provide portability between machines with different word lengths and different number representations. Sometimes auxiliary routines are provided to move data to and from formatted representations and binary representation. Other auxiliary routines sometimes deal with graphics output.Run decks are the least common element present in NESC software. When accompanied by the equivalent source statements, it is a redundant element. Without the corresponding source statements, a run deck will enable limited interchange between users of similar systems.Software screening procedures consist of some quality assurance applied to all software contributed to the Center. Whether the software is implemented by NESC or not results in its being denoted respectively as evaluated or “As Is” software.The screening process consists of an initial review of the contributed material for completeness and an assessment of the readability of the computer-media and documentation.The pertinent details of the initial review are to note if items were omitted in the submission such as: documentation, sample I/O, an NESC abstract, or an NESC release form. A note should also be made if there are any binary files which might not be readily transportable.To insure readability of the author's submitted computer-media material, an NESC backup copy is made. This tape will usually be 9-track format, recorded at 1600 bpi, and unlabeled. A summary listing is generated describing the tape contents of the original tape by indicating the number of characters per record, records per file, and number of files on the tape.A complete listing of the tape is made to determine the contents of the contribution and for archival purposes. For source files, it is useful to generate a list of subroutine names, function names, and entry points. These listings of source and subroutine names are sometimes inconvenient to produce with many machine specific tapes such as: CDC Display code, UNIVAC's Fieldata, DEC backup format, and IBM CMS dump tapes. Binary files are dumped in hexadecimal representation.The status of a contribution has to be established upon the completion of this preliminary screening. If difficulties such as copyright problems, unreadable media, or the submission of an incomplete preliminary version of a program arise, the contributor is contacted and the material is either returned or the missing elements are requested. Some software is made available to NESC recipients “As Is”. Other software is taken through an evaluation process, which includes compilation, execution of sample problems, and a more detailed examination of the documentation. An NESC Summary or Abstract is prepared and sent to IR's when the software becomes an “As Is” or an evaluated package, respectively. Supplemental documentation is sometimes created by NESC to help subsequent users install the software on their own machines.Distribution is usually on magnetic tape, and it is essential that sufficient descriptive information be provided for the recipient to recover the material. Recipients of NESC software are notified should an error be detected, or revisions or a new edition of the software becomes available.},
booktitle = {Proceedings of the 13th Annual ACM SIGUCCS Conference on User Services: Pulling It All Together},
pages = {181–183},
numpages = {3},
location = {Toledo, Ohio, USA},
series = {SIGUCCS '85}
}

@inproceedings{10.1145/507574.507588,
author = {Ehresman, Kenneth L. and Frantzen, Joey L.},
title = {Electronic Maneuvering Board and Dead Reckoning Tracer Decision Aid for the Officer of the Deck},
year = {2001},
isbn = {1581133928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/507574.507588},
doi = {10.1145/507574.507588},
abstract = {The U.S. Navy currently bases the majority of our contact management decisions around a time and manning intensive paper-based Maneuvering Board (MOBOARD) process. The use of Maneuvering Boards is a perishable skill that has a steep learning curve. In order to overcome inherent human error, it is not uncommon to have up to four people simultaneously involved in solving just one maneuvering problem. Additional manning requirements are involved on many Naval Ships in order to accurately convey the information to the Officer of the Deck (OOD) and/or the Commanding Officer. When given situations where there exist multiple contacts, the current system is quickly overwhelmed and may not provide Commanding Officers and OODs a complete and accurate picture in a timely manner.The purpose of this research is to implement a stand-alone system that will provide timely and accurate contact information for U.S. Navy Commanding Officers, OODs, and CIC watch teams. By creating a reliable, automated system in a format that is familiar to all Surface Warfare Officers we will provide the Navy with a valuable decision-making tool, while increasing ease of data exchange and reducing current redundancies and manning inefficient practices.Our software design is diagramed using the Unified Modeling Language (UML) with the underlying purpose of implementing an Ada-based system that is neither operating system nor hardware dependent. This approach allows us to develop and implement a design for a wide range of platforms, and will ultimately result in an extremely modular and portable system.Model-View-ControllerAdditionally, we approached our project using the Model View Controller (MVC). This approach has allowed for flexibility in the current and future use of our core model. Not only does the model meet today's current needs, it is highly extensible and will meet emerging needs for many years to come.In order to meet all of our criterion (i.e. operating system independent and hardware independent) we have implemented our project using GtkAda libraries and a GNAT compiler. Although GNAT makes it possible to link to other languages, we have avoided the use of other languages. This approach has resulted in a robust program that compiles and runs on a wide variety of platforms. The ultimate goal of our thesis is to produce an automated navigation system for the U.S. Navy that will help the Navy to meet reduced manning requirements while increasing the accuracy and reliability of its navigation and contact management systems.The MOBOARD software takes advantage of Ada's capability to separate the system specification from its implementation. In this design, the specification is designed to represent only those features necessary for the required features. That is, designing the context clauses at the specification level, we moved them to the package body so there would be a minimum of dependency on the graphics packages and other platform-specific library units. This approach facilitates a platform-independent presentation of multiple views of the same data.Of particular interest to some Ada practitioners will be our use of the floating-point facilities of Ada 95. We have made extensive use of the floating-point attributes as well as the generic elementary functions package. This was necessary because we are doing a lot of calculations that involve global coordinates, the movement of targets relative to a reference point, navigation calculations, prediction of position, and timing.Ada has proven useful in this project because of its underlying language structure. This structure has allowed us to decompose the software architecture along:1) A coherent conceptual view2) A platform-independent module view3) A maintainable code view4) A platform-targeted execution view.Each of these views evolves naturally as we progress from the UML use cases through the class and interaction diagrams, and onward to the Ada specifications.},
booktitle = {Proceedings of the 2001 Annual ACM SIGAda International Conference on Ada},
pages = {61–70},
numpages = {10},
keywords = {Maneuvering Board, Navigation, GNAT, Model-View-Controller, U.S. Navy, Contact Avoidance, GtkAda, Officer of the Deck Aid},
location = {Bloomington, MN},
series = {SIGAda '01}
}

@article{10.1145/507546.507588,
author = {Ehresman, Kenneth L. and Frantzen, Joey L.},
title = {Electronic Maneuvering Board and Dead Reckoning Tracer Decision Aid for the Officer of the Deck},
year = {2001},
issue_date = {December 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {XXI},
number = {4},
issn = {1094-3641},
url = {https://doi.org/10.1145/507546.507588},
doi = {10.1145/507546.507588},
abstract = {The U.S. Navy currently bases the majority of our contact management decisions around a time and manning intensive paper-based Maneuvering Board (MOBOARD) process. The use of Maneuvering Boards is a perishable skill that has a steep learning curve. In order to overcome inherent human error, it is not uncommon to have up to four people simultaneously involved in solving just one maneuvering problem. Additional manning requirements are involved on many Naval Ships in order to accurately convey the information to the Officer of the Deck (OOD) and/or the Commanding Officer. When given situations where there exist multiple contacts, the current system is quickly overwhelmed and may not provide Commanding Officers and OODs a complete and accurate picture in a timely manner.The purpose of this research is to implement a stand-alone system that will provide timely and accurate contact information for U.S. Navy Commanding Officers, OODs, and CIC watch teams. By creating a reliable, automated system in a format that is familiar to all Surface Warfare Officers we will provide the Navy with a valuable decision-making tool, while increasing ease of data exchange and reducing current redundancies and manning inefficient practices.Our software design is diagramed using the Unified Modeling Language (UML) with the underlying purpose of implementing an Ada-based system that is neither operating system nor hardware dependent. This approach allows us to develop and implement a design for a wide range of platforms, and will ultimately result in an extremely modular and portable system.Model-View-ControllerAdditionally, we approached our project using the Model View Controller (MVC). This approach has allowed for flexibility in the current and future use of our core model. Not only does the model meet today's current needs, it is highly extensible and will meet emerging needs for many years to come.In order to meet all of our criterion (i.e. operating system independent and hardware independent) we have implemented our project using GtkAda libraries and a GNAT compiler. Although GNAT makes it possible to link to other languages, we have avoided the use of other languages. This approach has resulted in a robust program that compiles and runs on a wide variety of platforms. The ultimate goal of our thesis is to produce an automated navigation system for the U.S. Navy that will help the Navy to meet reduced manning requirements while increasing the accuracy and reliability of its navigation and contact management systems.The MOBOARD software takes advantage of Ada's capability to separate the system specification from its implementation. In this design, the specification is designed to represent only those features necessary for the required features. That is, designing the context clauses at the specification level, we moved them to the package body so there would be a minimum of dependency on the graphics packages and other platform-specific library units. This approach facilitates a platform-independent presentation of multiple views of the same data.Of particular interest to some Ada practitioners will be our use of the floating-point facilities of Ada 95. We have made extensive use of the floating-point attributes as well as the generic elementary functions package. This was necessary because we are doing a lot of calculations that involve global coordinates, the movement of targets relative to a reference point, navigation calculations, prediction of position, and timing.Ada has proven useful in this project because of its underlying language structure. This structure has allowed us to decompose the software architecture along:1) A coherent conceptual view2) A platform-independent module view3) A maintainable code view4) A platform-targeted execution view.Each of these views evolves naturally as we progress from the UML use cases through the class and interaction diagrams, and onward to the Ada specifications.},
journal = {Ada Lett.},
month = {sep},
pages = {61–70},
numpages = {10},
keywords = {Contact Avoidance, Navigation, GNAT, Officer of the Deck Aid, GtkAda, U.S. Navy, Maneuvering Board, Model-View-Controller}
}

@inproceedings{10.1145/2938559.2948790,
author = {Ra, Ho-Kyeong and Yoon, Hee Jung and Salekin, Asif and Lee, Jin-Hee and Stankovic, John A. and Son, Sang Hyuk},
title = {Poster: Software Architecture for Efficiently Designing Cloud Applications Using Node.Js},
year = {2016},
isbn = {9781450344166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2938559.2948790},
doi = {10.1145/2938559.2948790},
abstract = {We propose a practical solution for cloud application development using Node.js and Express library by presenting: (1) a software architecture which utilizes two standard inheritance pattern techniques, the top-down and divide and conquer approaches, to effectively organize the structure of the application for improved maintainability and extensibility in the long-run, and (2) an easy-to-follow guideline that instructs the implementation procedures for developing Node.js cloud applications.},
booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion},
pages = {72},
numpages = {1},
keywords = {software architecture, node.js, cloud applications},
location = {Singapore, Singapore},
series = {MobiSys '16 Companion}
}

@inproceedings{10.1145/1255175.1255205,
author = {Prom, Christopher J. and Rishel, Christopher A. and Schwartz, Scott W. and Fox, Kyle J.},
title = {A Unified Platform for Archival Description and Access},
year = {2007},
isbn = {9781595936448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1255175.1255205},
doi = {10.1145/1255175.1255205},
abstract = {The archival community has developed content and data structure standards to facilitate access to the diverse and unique sets of archival records, personal papers, and manuscript collections that are held by archival repositories and special collections libraries. However, these standards are difficult for archivists to use and are often implemented in ways that negatively affect materials-handling workflows, depriving archival users of the best possible access to the totality of materials available within an individual repository. The authors propose that archival descriptive problems can be addressed by implementing a web/database application that is tailored specifically to archival needs and can be implemented with little technical knowledge. This paper describes the system architecture of one such tool, the Archon software package, which was developed at the University of Illinois at Urbana-Champaign. Archon automates many technical tasks, such as producing a searchable website, an EAD instance or a MARC record. Although the system utilizes sophisticated algorithms and optimizations, it is easily extensible because most development takes place in an easy-to-use, object-oriented environment.},
booktitle = {Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {157–166},
numpages = {10},
keywords = {archival information systems, encoded archival description, web interfaces, Archon, databases},
location = {Vancouver, BC, Canada},
series = {JCDL '07}
}

@inproceedings{10.1145/3345252.3345293,
author = {Monev, Venelin and Hristova, Maya},
title = {OAtools: Software Package for Investigation of Orthogonal Arrays},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345293},
doi = {10.1145/3345252.3345293},
abstract = {Orthogonal arrays are important combinatorial structures, both from a theoretical and a practical point of view. This fact is based on their close relation with other combinatorial structures and their practical application for testing and planning of experiments. For this reason, the orthogonal arrays are of a great research interest for many authors who explore different methods for construction, classification of orthogonal arrays, etc. In this paper we present a software package for investigation of orthogonal arrays. One of its functionalities is to check whether a matrix (or a set of matrices) satisfies the conditions for orthogonal array with fixed parameters. Another functionality of this package is the identification of isomorphic orthogonal arrays. Furthermore, OAtools provides the ability to calculate distance distribution.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {137–140},
numpages = {4},
keywords = {non-isomorphic orthogonal arrays, hash tables, distance distribution, Q-Extension, Orthogonal arrays},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@inproceedings{10.1145/1289971.1289984,
author = {J\"{a}rvi, Jaakko and Marcus, Matthew A. and Smith, Jacob N.},
title = {Library Composition and Adaptation Using C++ Concepts},
year = {2007},
isbn = {9781595938558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1289971.1289984},
doi = {10.1145/1289971.1289984},
abstract = {Large scale software is composed of libraries produced by different entities. Non-intrusive and efficient mechanisms for adapting data structures from one library to conform to APIs of another are essential for the success of large software projects. Concepts and concept maps, planned features for the next version of C++, have been designed to support adaptation, promising generic, non-intrusive, efficient, and identity preserving adapters. This paper analyses the use of concept maps for library composition and adaptation, comparing and contrasting concept maps to other common adaptation mechanisms. We report on two cases of data structure adaptation between different libraries, indicating best practices and idioms along the way. First, we adapt GUI controls from several frameworks for use with a generic layout engine, extending the application of concepts to run-time polymorphism. Second, we develop a transparent adaptation layer between an image processing library and a graph algorithm library, enabling the efficient application of graph algorithms to the image processing domain.},
booktitle = {Proceedings of the 6th International Conference on Generative Programming and Component Engineering},
pages = {73–82},
numpages = {10},
keywords = {software libraries, polymorphism, C++, generic programming},
location = {Salzburg, Austria},
series = {GPCE '07}
}

@inproceedings{10.1145/2016039.2016118,
author = {Kapitza, P. J.},
title = {An Implementation of Heegaard Diagrams},
year = {2011},
isbn = {9781450306867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2016039.2016118},
doi = {10.1145/2016039.2016118},
abstract = {A software platform for the construction and study of three dimensional manifold structures and more generally, of two dimensional cell complexes is presented. Interactive operations allow the construction of 3-manifold spaces in planar form. Manifold invariants, based upon the diagrams and their associated presentations are constructed directly. The package incorporates the operations defined by J. Singer. This package enables a user to graphically define and apply 3-manifold operations in the context of Heegaard Diagrams.},
booktitle = {Proceedings of the 49th Annual Southeast Regional Conference},
pages = {302–303},
numpages = {2},
keywords = {manifold, Heegaard, cell complex},
location = {Kennesaw, Georgia},
series = {ACM-SE '11}
}

@article{10.1145/1089014.1089021,
author = {Heroux, Michael A. and Bartlett, Roscoe A. and Howle, Vicki E. and Hoekstra, Robert J. and Hu, Jonathan J. and Kolda, Tamara G. and Lehoucq, Richard B. and Long, Kevin R. and Pawlowski, Roger P. and Phipps, Eric T. and Salinger, Andrew G. and Thornquist, Heidi K. and Tuminaro, Ray S. and Willenbring, James M. and Williams, Alan and Stanley, Kendall S.},
title = {An Overview of the Trilinos Project},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/1089014.1089021},
doi = {10.1145/1089014.1089021},
abstract = {The Trilinos Project is an effort to facilitate the design, development, integration, and ongoing support of mathematical software libraries within an object-oriented framework for the solution of large-scale, complex multiphysics engineering and scientific problems. Trilinos addresses two fundamental issues of developing software for these problems: (i) providing a streamlined process and set of tools for development of new algorithmic implementations and (ii) promoting interoperability of independently developed software.Trilinos uses a two-level software structure designed around collections of packages. A Trilinos package is an integral unit usually developed by a small team of experts in a particular algorithms area such as algebraic preconditioners, nonlinear solvers, etc. Packages exist underneath the Trilinos top level, which provides a common look-and-feel, including configuration, documentation, licensing, and bug-tracking.Here we present the overall Trilinos design, describing our use of abstract interfaces and default concrete implementations. We discuss the services that Trilinos provides to a prospective package and how these services are used by various packages. We also illustrate how packages can be combined to rapidly develop new algorithms. Finally, we discuss how Trilinos facilitates high-quality software engineering practices that are increasingly required from simulation software.},
journal = {ACM Trans. Math. Softw.},
month = {sep},
pages = {397–423},
numpages = {27},
keywords = {interfaces, Software Quality Engineering, Software framework}
}

@inproceedings{10.1109/PADS.2006.37,
author = {Santoro, Andrea and Quaglia, Francesco},
title = {Transparent Optimistic Synchronization in HLA via a Time-Management Converter},
year = {2006},
isbn = {0769525873},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PADS.2006.37},
doi = {10.1109/PADS.2006.37},
abstract = {In this paper we present the design and implementation of a Time Management Converter (TiMaC) for HLA based simulation systems. TiMaC is a layer interposed in between the federate and the underlying RTI in order to map the conservative Time Management interface onto the optimistic one. In this way, TiMaC transparently supports optimistic execution for federates originally designed for the conservative approach, which is achieved without the need for developing any ad-hoc RTI system. TiMaC relies on a recently proposed software architecture for transparent treatment of checkpointing/recovery of the federate state, namely Magic State Manager (MASM), and implements a set of additional facilities required to support all the tasks associated with the mapping of conservative onto optimistic Time Management interfaces. The implementation has been tailored to the Georgia Tech B-RTI package, although the underlying design principles would allow it to be integrated with any RTI system. We also report an experimental study demonstrating the viability and effectiveness of our proposal in allowing conservative federates to be supported with highly increased run-time effectiveness in general contexts for what concerns the features of the underlying computing systems (e.g. LAN vsWAN based systems.},
booktitle = {Proceedings of the 20th Workshop on Principles of Advanced and Distributed Simulation},
pages = {193–200},
numpages = {8},
series = {PADS '06}
}

@article{10.1145/3287319,
author = {Agarwal, Anish and Amjad, Muhammad Jehangir and Shah, Devavrat and Shen, Dennis},
title = {Model Agnostic Time Series Analysis via Matrix Estimation},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
url = {https://doi.org/10.1145/3287319},
doi = {10.1145/3287319},
abstract = {We propose an algorithm to impute and forecast a time series by transforming the observed time series into a matrix, utilizing matrix estimation to recover missing values and de-noise observed entries, and performing linear regression to make predictions. At the core of our analysis is a representation result, which states that for a large class of models, the transformed time series matrix is (approximately) low-rank. In effect, this generalizes the widely used Singular Spectrum Analysis (SSA) in the time series literature, and allows us to establish a rigorous link between time series analysis and matrix estimation. The key to establishing this link is constructing a Page matrix with non-overlapping entries rather than a Hankel matrix as is commonly done in the literature (e.g., SSA). This particular matrix structure allows us to provide finite sample analysis for imputation and prediction, and prove the asymptotic consistency of our method. Another salient feature of our algorithm is that it is model agnostic with respect to both the underlying time dynamics and the noise distribution in the observations. The noise agnostic property of our approach allows us to recover the latent states when only given access to noisy and partial observations a la a Hidden Markov Model; e.g., recovering the time-varying parameter of a Poisson process without knowing that the underlying process is Poisson. Furthermore, since our forecasting algorithm requires regression with noisy features, our approach suggests a matrix estimation based method-coupled with a novel, non-standard matrix estimation error metric-to solve the error-in-variable regression problem, which could be of interest in its own right. Through synthetic and real-world datasets, we demonstrate that our algorithm outperforms standard software packages (including R libraries) in the presence of missing data as well as high levels of noise.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {dec},
articleno = {40},
numpages = {39}
}

@inproceedings{10.1145/3309697.3331479,
author = {Agarwal, Anish and Amjad, Muhammad Jehangir and Shah, Devavrat and Shen, Dennis},
title = {Model Agnostic Time Series Analysis via Matrix Estimation},
year = {2019},
isbn = {9781450366786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3309697.3331479},
doi = {10.1145/3309697.3331479},
abstract = {We propose an algorithm to impute and forecast a time series by transforming the observed time series into a matrix, utilizing matrix estimation to recover missing values and de-noise observed entries, and performing linear regression to make predictions. At the core of our analysis is a representation result, which states that for a large class of models, the transformed time series matrix is (approximately) low-rank. In effect, this generalizes the widely used Singular Spectrum Analysis (SSA) in the time series literature, and allows us to establish a rigorous link between time series analysis and matrix estimation. The key to establishing this link is constructing a Page matrix with non-overlapping entries rather than a Hankel matrix as is commonly done in the literature (e.g., SSA). This particular matrix structure allows us to provide finite sample analysis for imputation and prediction, and prove the asymptotic consistency of our method. Another salient feature of our algorithm is that it is model agnostic with respect to both the underlying time dynamics and the noise distribution in the observations. The noise agnostic property of our approach allows us to recover the latent states when only given access to noisy and partial observations a la a Hidden Markov Model; e.g., recovering the time-varying parameter of a Poisson process without knowing that the underlying process is Poisson. Furthermore, since our forecasting algorithm requires regression with noisy features, our approach suggests a matrix estimation based method---coupled with a novel, non-standard matrix estimation error metric---to solve the error-in-variable regression problem, which could be of interest in its own right. Through synthetic and real-world datasets, we demonstrate that our algorithm outperforms standard software packages (including R libraries) in the presence of missing data as well as high levels of noise.},
booktitle = {Abstracts of the 2019 SIGMETRICS/Performance Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {85–86},
numpages = {2},
keywords = {time series analysis, matrix estimation, linear-time-invariant systems, hidden markov models},
location = {Phoenix, AZ, USA},
series = {SIGMETRICS '19}
}

@inproceedings{10.5555/1030453.1030478,
author = {Rohrer, Matthew W. and McGregor, Ian W.},
title = {AutoMod: Simulating Reality Using AutoMod},
year = {2002},
isbn = {0780376153},
publisher = {Winter Simulation Conference},
abstract = {Decision making in industry has become more complicated in recent years. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Brooks-PRI Automation has been used on thousands of projects to help engineers and managers make the best decisions possible. With the release of AutoMod 11.0 in 2002, AutoMod now supports hierarchical model construction. This new architecture allows users to reuse model objects in other models, decreasing the time required to build a model. Composite models are just one of the latest advances that make AutoMod one of the most widely used simulation software packages.},
booktitle = {Proceedings of the 34th Conference on Winter Simulation: Exploring New Frontiers},
pages = {173–181},
numpages = {9},
location = {San Diego, California},
series = {WSC '02}
}

@inproceedings{10.5555/1351542.1351586,
author = {LeBaron, Todd and Jacobsen, Craig},
title = {The Simulation Power of Automod},
year = {2007},
isbn = {1424413060},
publisher = {IEEE Press},
abstract = {Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model objects in other models, decreasing the time required to build a model. In addition, recent enhancements to Automod's material handling template systems have increased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages.},
booktitle = {Proceedings of the 39th Conference on Winter Simulation: 40 Years! The Best is yet to Come},
pages = {210–218},
numpages = {9},
location = {Washington D.C.},
series = {WSC '07}
}

@article{10.1145/3376930.3376984,
author = {Agarwal, Anish and Jehangir Amjad, Muhammad and Shah, Devavrat and Shen, Dennis},
title = {Model Agnostic Time Series Analysis via Matrix Estimation},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0163-5999},
url = {https://doi.org/10.1145/3376930.3376984},
doi = {10.1145/3376930.3376984},
abstract = {We propose an algorithm to impute and forecast a time series by transforming the observed time series into a matrix, utilizing matrix estimation to recover missing values and de-noise observed entries, and performing linear regression to make predictions. At the core of our analysis is a representation result, which states that for a large class of models, the transformed time series matrix is (approximately) low-rank. In effect, this generalizes the widely used Singular Spectrum Analysis (SSA) in the time series literature, and allows us to establish a rigorous link between time series analysis and matrix estimation. The key to establishing this link is constructing a Page matrix with non-overlapping entries rather than a Hankel matrix as is commonly done in the literature (e.g., SSA). This particular matrix structure allows us to provide finite sample analysis for imputation and prediction, and prove the asymptotic consistency of our method. Another salient feature of our algorithm is that it is model agnostic with respect to both the underlying time dynamics and the noise distribution in the observations. The noise agnostic property of our approach allows us to recover the latent states when only given access to noisy and partial observations a la a Hidden Markov Model; e.g., recovering the time-varying parameter of a Poisson process without knowing that the underlying process is Poisson. Furthermore, since our forecasting algorithm requires regression with noisy features, our approach suggests a matrix estimation based method-coupled with a novel, non-standard matrix estimation error metric-to solve the error-in-variable regression problem, which could be of interest in its own right. Through synthetic and real-world datasets, we demonstrate that our algorithm outperforms standard software packages (including R libraries) in the presence of missing data as well as high levels of noise.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {dec},
pages = {85–86},
numpages = {2},
keywords = {linear-time-invariant systems, hidden markov models, time series analysis, matrix estimation}
}

@article{10.5555/3546258.3546274,
author = {Yu, Shih-Yuan and Chhetri, Sujit Rokka and Canedo, Arquimedes and Goyal, Palash and Al Faruque, Mohammad Abdullah},
title = {Pykg2vec: A Python Library for Knowledge Graph Embedding},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Pykg2vec is a Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's exible and modular software architecture currently implements 25 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of PyTorch and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, evaluation of KGE tasks, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI).},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {16},
numpages = {6},
keywords = {knowledge graph embedding, representation learning}
}

@inproceedings{10.5555/2429759.2430324,
author = {Muller, Dan},
title = {AutoMod™: Providing Simulation Solutions for over 30 Years},
year = {2012},
publisher = {Winter Simulation Conference},
abstract = {Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model components in other models, decreasing the time required to build a model. In addition, recent enhancements to AutoMod's material handling template systems have in-creased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {434},
numpages = {15},
location = {Berlin, Germany},
series = {WSC '12}
}

@inproceedings{10.5555/2675983.2675963,
author = {Muller, Dan},
title = {AutoMod®: Modeling Complex Manufacturing, Distribution, and Logisitics Systems for over 30 Years},
year = {2013},
isbn = {9781479920778},
publisher = {IEEE Press},
abstract = {Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model components in other models, decreasing the time required to build a model. In addition, recent enhancements to AutoMod's material handling template systems have in-creased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages.},
booktitle = {Proceedings of the 2013 Winter Simulation Conference: Simulation: Making Decisions in a Complex World},
pages = {4037–4051},
numpages = {15},
location = {Washington, D.C.},
series = {WSC '13}
}

@inproceedings{10.5555/2431518.2431525,
author = {Muller, Dan},
title = {AutoMod™: Providing Simulation Solutions for over 25 Years},
year = {2011},
publisher = {Winter Simulation Conference},
abstract = {Decision making in industry continues to become more complicated. Customers are more demanding, competition is more fierce, and costs for labor and raw materials continue to rise. Managers need state-of-the-art tools to help in planning, design, and operations of their facilities. Simulation provides a virtual factory where ideas can be tested and performance improved. The AutoMod product suite from Applied Materials has been used on thousands of projects to help engineers and managers make the best decisions possible. AutoMod supports hierarchical model construction. This architecture allows users to reuse model components in other models, decreasing the time required to build a model. In addition, recent enhancements to AutoMod's material handling template systems have in-creased modeling accuracy and ease-of-use. These latest advances have helped make AutoMod one of the most widely used simulation software packages.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {39–51},
numpages = {13},
location = {Phoenix, Arizona},
series = {WSC '11}
}

@inproceedings{10.1145/800280.811034,
author = {Pouzin, Louis},
title = {Presentation and Major Design Aspects of the CYCLADES Computer Network},
year = {1973},
isbn = {9781450373845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800280.811034},
doi = {10.1145/800280.811034},
abstract = {A computer network is being developed in France, under government sponsorship, to link about twenty heterogeneous computers located in universities, research and D.P. Centers. Goals are to set up a prototype network in order to foster experiment in various areas, such as: data communications, computer interaction, cooperative research, distributed data bases. The network is intended to be both, an object for research, and an operational tool.In order to speed up the implementation, standard equipment is used, and modifications to operating systems are minimized. Rather, the design effort bears on a carefully layered architecture, allowing for a gradual insertion of specialized protocols and services tailored to specific application and user classes.A particular objective, for which CYCLADES should be an operational tool, is to provide various departments of the French Administration with access to multiple data bases located in geographically distant areas.Host-host protocols, as well as error and flow control mechanisms are based on a simple message exchange procedure, on top of which various options may be built for the sake of efficiency, error recovery, or convenience. Depending on available computer resources, these options can be implemented as user software, system modules, or front end processor package. For each of them, network-wide interfaces are defined, to conserve consistency in human communications.CYCLADES uses a packet-switching sub-network, which is a transparent message carrier, completely independent of host-host conventions. While in many ways similar to ARPANET, it presents some distinctive differences in address and message handling, intended to facilitate interconnection with other networks. In particular, addresses can have variable formats, and messages are not delivered in sequence, so that they can flow out of the network through several gates toward an out-side target.Terminal concentrators are mini-hosts, and implement whatever services users or applications require, such as sequencing, error recovery, code translation, buffering, etc. Some specialized hosts may be installed to cater for specific services, such as mail, resource allocation, information retrieval, mass storage. A control center is also being installed and will be operated by the French PTT.},
booktitle = {Proceedings of the Third ACM Symposium on Data Communications and Data Networks: Analysis and Design},
pages = {80–87},
numpages = {8},
series = {DATACOMM '73}
}

@article{10.1145/344283.344294,
author = {Yingjun, Li and Jian, Lu},
title = {SEIS++: A Pattern Language for Seismic Tools Construction and Integration},
year = {1999},
issue_date = {Dec. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {12},
issn = {0362-1340},
url = {https://doi.org/10.1145/344283.344294},
doi = {10.1145/344283.344294},
abstract = {Predominant industrial practice has evolved from general-purpose class libraries to domain-specific frameworks" and design patterns. Both of them are a means to achieve large-scale reuse by capturing successful software development strategies within a particular context. Design patterns focus on reuse of recurring architectural design themes and mainly consist of predefined design structures that can be used as building blocks to compose the architecture of software system. Together the patterns in a specific domain form a pattern language, which can be used to approach a certain class of problems in the application domain. In this paper, we propose a pattern language SEIS++, a set of design patterns, for seismic tool construction and integration in oil and gas exploration domain. The language uses Tools and Materials as the new design conception to guide domain-specific application development, and to enhance software architecture reusability.},
journal = {SIGPLAN Not.},
month = {dec},
pages = {57–66},
numpages = {10},
keywords = {material, pattern language, tool, seismic processing}
}

@inproceedings{10.5555/1791889.1791907,
author = {Alam, Sadaf R. and Agarwal, Pratul K. and Hampton, Scott S. and Ong, Hong},
title = {Experimental Evaluation of Molecular Dynamics Simulations on Multi-Core Systems},
year = {2008},
isbn = {9783540898931},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Multi-core processors introduce many challenges both at the systemand application levels that need to be addressed in order to attain the best performance.In this paper, we study the impact of the multi-core technologies inthe context of two scalable, production-level molecular dynamics simulationframeworks. Experimental analysis and observations in this paper provide for abetter understanding of the interactions between the application and the underlyingsystem features such as memory bandwidth, architectural optimization,and communication library implementation. In particular, we observe that parallelefficiencies could be as low as 50% on quad-core systems while a set ofdual-core processors connected with a high speed interconnect can easily outperformthe same number of cores on a socket or in a package. This indicates thatcertain modifications to the software stack and application implementations arenecessary in order to fully exploit the performance of multi-core based systems.},
booktitle = {Proceedings of the 15th International Conference on High Performance Computing},
pages = {131–141},
numpages = {11},
keywords = {performance, multicore, HPC, molecular dynamics simulation},
location = {Bangalore, India},
series = {HiPC'08}
}

@inproceedings{10.1145/3338906.3341174,
author = {Cai, Liang and Wang, Haoye and Huang, Qiao and Xia, Xin and Xing, Zhenchang and Lo, David},
title = {BIKER: A Tool for Bi-Information Source Based API Method Recommendation},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341174},
doi = {10.1145/3338906.3341174},
abstract = {Application Programming Interfaces (APIs) in software libraries play an important role in modern software development. Although most libraries provide API documentation as a reference, developers may find it difficult to directly search for appropriate APIs in documentation using the natural language description of the programming tasks. We call such phenomenon as knowledge gap, which refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes. In this paper, we propose a Java API recommendation tool named BIKER (Bi-Information source based KnowledgE Recommendation) to bridge the knowledge gap. We implement BIKER as a search engine website. Given a query in natural language, instead of directly searching API documentation, BIKER first searches for similar API-related questions on Stack Overflow to extract candidate APIs. Then, BIKER ranks them by considering the query’s similarity with both Stack Overflow posts and API documentation. Finally, to help developers better understand why each API is recommended and how to use them in practice, BIKER summarizes and presents supplementary information (e.g., API description, code examples in Stack Overflow posts) for each recommended API. Our quantitative evaluation and user study demonstrate that BIKER can help developers find appropriate APIs more efficiently and precisely.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1075–1079},
numpages = {5},
keywords = {API Documentation, API Recommendation, Stack Overflow},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2039370.2039435,
author = {Lin, Chi-Hung and Hsieh, Wen-Tsan and Hsieh, Hsien-Ching and Liu, Chun-Nan and Yeh, Jen-Chieh},
title = {System-Level Design Exploration for 3-D Stacked Memory Architectures},
year = {2011},
isbn = {9781450307154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2039370.2039435},
doi = {10.1145/2039370.2039435},
abstract = {Traditional technology scaling of semiconductor chips followed Moore's Law. However, the transistor performance improvement will be limited, and designer will not see doubling of frequency every two years. Recently, three-dimension integrated circuits (3-D IC) that employ the vertically through-silicon vias (TSVs) for connecting each of dies have been proposed. It is an alternative solution to existent Package-on-Package (PoP) and System-in-Package (SiP) processes. There are many benefits by using TSV-based 3-D integration technologies: (1) more functionality can be integrated into a small silicon space for form factor reduction, (2) circuit delay can be improved by using TSVs due to the shorter interconnect and reduced parasitic capacitance/inductance, (3) different components with incompatible manufacturing process (i.e. Logic, DRAM, Flash, etc) can be combined in single 3-D IC for heterogeneous integration. In addition, there are many 3-D multi-core or many-core architectures are discussed recently. Comparing with traditional two-dimension multi-core or many-core architectures, the major difference is memory bandwidth problem can be addressed by stacked memory architecture. Intel has good demonstration through the teraflops microprocessor chip which is an 80-core design with memory-on-logic architecture. And, each core connects to a 256KB SRAM with 12GB/s bandwidth.Although 3-D stacking technology can bring us many benefits for next generation integrated circuits, it comes with many problems and challenges in system-level design. For instance, 3-D IC designs will deal with serious challenges in design space exploration and system validation. For most designs, the number of TSV will be the most critical limitation that should be considered carefully. Besides, system design by 3-D IC will become more and more complex, and it needs a full system-level solution to face the performance, number of TSV, and power issues of 3D-IC. Furthermore, it is more challenge to provide a FPGA-based prototyping system for early-stage software development.In general, the continue increasing complexity of modern SoC or embedded system design is also extreme. To achieve the required design productivity for the time-to-market pressure, a common accepted solution is using electronic system-level (ESL) design methodology to design the system in the different design abstraction level. One of the key technologies of ESL solution is to construct the HW/SW co-simulation platform by using the virtual prototyping concept. Moreover, designers need a multi-phase of virtual platform construction to meet the different targets of design stage, such as early system validation and architecture exploration. In this work, we create a simulation framework by using ESL methodology to explore 3-D IC system that consists of multi-core processors with extended stacking memory.To demonstrate our 3-D IC design techniques, the stacking memory approach is employed in our "3D-PAC (Parallel Architecture Core)" design. In 3D-PAC, we stack SRAM directly on top of the logic die which is heterogeneous multi-core computing platform for multimedia application purpose. The logic die is mainly consists of a general-purpose microprocessor, dual programmable digital signal processor (DSP) cores, AXI/AHB/APB subsystems, and various peripherals. Furthermore, we had the corresponding virtual platform (PAC Duo virtual platform) which had been used for early architecture exploration, power estimation and HW/SW co-simulation. The difference with PAC Duo design, the target of 3D-PAC is to provide a three-dimensional SoC (3-D SoC) design exporation for media-rich and multi-function portable devices. Although using the stacking memory approach with TSV technology can increase the capacity and performance of memory system significantly, but the placement and organization of the memory system are the nother important design issues. In this work, we use the ESL design methodology to consider these design issues for 3-D multi-core embedded system.According to our 3-D implementation, the number of TSV is additional important design constraint. The major issue is to find out the feasible system architect that can meet all the performance, power, area and TSV limitation design constraints. To achieve the requirement, we extend the virtual platform property to report the number of TSVs during the architecture exploration process. Besides, we also construct two kinds of virtual platform, one is using the approximately time modeling techniques to speed-up the simulation speed. The other one is using the cycle accurate modeling to obtain the precisely timing behavior. Designers can use these virtual platforms for either early HW/SW validation or architecture exploration.Therefore, designers can rapidly obtain the performance-TSV relationship with various system architectures based on the proposed virtual platforms. In our work, we analyze different system architectures, mainly the usage of the stacking memory, for our next generation design. We try to explore the maximum performance and the minimum TSVs overhead system architect Using these ESL techniques can reduce our overall development time a lot for our 3D-PAC design.},
booktitle = {Proceedings of the Seventh IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
pages = {389–390},
numpages = {2},
keywords = {esl, design exploration, 3-d ic},
location = {Taipei, Taiwan},
series = {CODES+ISSS '11}
}

@inproceedings{10.1145/3055635.3056659,
author = {Riansyah, Moch I. and Nugraha, Yayan P. and Ridlwan, Hasvienda M. and Trilaksono, Bambang R.},
title = {3D Mapping Hexacopter Simulation Using Gazebo and Robot Operating Sytem(ROS)},
year = {2017},
isbn = {9781450348171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3055635.3056659},
doi = {10.1145/3055635.3056659},
abstract = {This paper present a simulation of Unmanned Aerial Vehicle which is type of hexacopter for building 3D maps of exploration environment. This simulation using Gazebo Simulator environment with Software In the Loop (SITL) ardupilot that is integrated with Robot Operating System as a open source flexible framework for writing robot software. To proceed 3D maps construction, we have installed Intel Realsense R200 RGB-D camera on hexacopter for getting RGB image data and Depth data that will be computed by open source octomap ROS package to result 3D Maps. Octomap using octree data structure to form 3D Map of voxel with odometry of hexacopter.},
booktitle = {Proceedings of the 9th International Conference on Machine Learning and Computing},
pages = {507–510},
numpages = {4},
keywords = {Odometry, Octomap, 3D Map, Simulation, R200, Gazebo, ROS},
location = {Singapore, Singapore},
series = {ICMLC 2017}
}

@article{10.1145/382204.382528,
author = {Wu, B. F.},
title = {Requirements of a Real-Time Microcomputer Embedded Laboratory Project},
year = {1985},
issue_date = {June 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {0097-8418},
url = {https://doi.org/10.1145/382204.382528},
doi = {10.1145/382204.382528},
abstract = {This paper describes the requirements of a real-time microcomputer embedded project designed and implemented by the students in the Motorola Corporate Software Engineering Training Program. The objective of this project is to offer the students opportunities to practice the concepts and theory learned in the classroom by developing software for a real-time microcomputer based project.The hardware for this project, which is based on a dual-CPU architecture using Motorola 16-bit and 8-bit microprocessors, is constructed by each student. The application software including features such as time keeping and decimal calculator functions driven by a real-time multi-tasking executive are developed by following the software engineering methodology. The final debugged program is then burned into EPROM, resulting in an integrated hardware/software package which is totally self-contained.},
journal = {SIGCSE Bull.},
month = {jun},
pages = {27–28},
numpages = {2}
}

@inproceedings{10.1145/2371316.2371361,
author = {Savi\'{c}, Milo\v{s} and Radovanovi\'{c}, Milo\v{s} and Ivanovi\'{c}, Mirjana},
title = {Community Detection and Analysis of Community Evolution in Apache Ant Class Collaboration Networks},
year = {2012},
isbn = {9781450312400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2371316.2371361},
doi = {10.1145/2371316.2371361},
abstract = {In this paper we investigate community detection algorithms applied to class collaboration networks (CCNs) that represent class dependencies of 21 consecutive versions of the Apache Ant software system. Four community detection techniques, Girvan-Newman (GN), Greedy Modularity Optimization (GMO), Walktrap and Label Propagation (LP), are used to compute community partitions. Obtained community structures are evaluated using community quality metrics (inter- and intra-cluster density, conductance and expansion) and compared to package structures of analyzed software. In order to investigate evolutionary stability of community detection methods, we designed an algorithm for tracking evolving communities. For LP and GMO, algorithms that produce partitions with higher values of normalized modularity score compared to GN and Walktrap, we noticed an evolutionary degeneracy -- LP and GMO are extremely sensitive to small evolutionary changes in CCN structure. Walktrap shows the best performance considering community quality, evolutionary stability and comparison with actual class groupings into packages. Coarse-grained descriptions (CGD) of CCNs are constructed from Walktrap partitions and analyzed. Results suggest that CCNs have modular structure that cannot be considered as hierarchical, due to the existence of large strongly connected components in CGDs.},
booktitle = {Proceedings of the Fifth Balkan Conference in Informatics},
pages = {229–234},
numpages = {6},
keywords = {class collaboration network, community detection, community evolution, evolutionary degeneracy},
location = {Novi Sad, Serbia},
series = {BCI '12}
}

@inproceedings{10.5555/1273749.1273772,
author = {Melton, Hayden and Tempero, Ewan},
title = {The CRSS Metric for Package Design Quality},
year = {2007},
isbn = {1920682430},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Package design is concerned with the determining the best way to partition the classes in a system into subsystems. A poor package design can adversely affect the quality of a software system. In this paper we present a new metric, Class Reachability Set Size (CRSS), the distribution of which can be used to determine if the relationships between the classes in a system preclude it from a good package design. We compute CRSS distributions for programs in a software corpus in order to show that some real programs are precluded in this way. Also we show how the CRSS metric can be used to identify candidates for refactoring so that the potential package structure of a system can be improved.},
booktitle = {Proceedings of the Thirtieth Australasian Conference on Computer Science - Volume 62},
pages = {201–210},
numpages = {10},
location = {Ballarat, Victoria, Australia},
series = {ACSC '07}
}

@inproceedings{10.1145/376656.376836,
author = {VanderHeyden, W. B. and Dendy, E. D. and Padial-Collins, N. T.},
title = {CartaBlanca— a Pure-Java, Component-Based Systems Simulation Tool for Coupled Non-Linear Physics on Unstructured Grids},
year = {2001},
isbn = {1581133596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/376656.376836},
doi = {10.1145/376656.376836},
abstract = {This paper describes a component-based non-linear physical system simulation prototyping package written entirely in Java using object-oriented design to provide scientists and engineers a “developer-friendly” software environment for large-scale computational method and physical model development. The software design centers on the Jacobian-Free Newton-Krylov solution method surrounding a finite-volume treatment of conservation equations. This enables a clean component-based implementation. We first provide motivation for the development of the software and then describe software structure. Discussion of software structure includes a description of the use of Java's built-in thread facility that enables data-parallel, shared-memory computations on a wide variety of unstructured grids with triangular, quadrilateral, tetrahedral and hexahedral elements. We also discuss the use of Java's inheritance mechanism in the construction of a hierarchy of physics-systems objects and linear and non-linear solver objects that simplify development and foster software re-use. As a compliment to the discussion of these object hierarchies, we provide a brief review of the Jacobian-Free Newton-Krylov nonlinear system solution method and discuss how it fits into our design. Following this, we show results from preliminary calculations and then discuss future plans including the extension of the software to distributed memory computer systems.},
booktitle = {Proceedings of the 2001 Joint ACM-ISCOPE Conference on Java Grande},
pages = {134–142},
numpages = {9},
keywords = {Java object oriented, Newton, Jacobian, Krylov, solver, physics, parallel, components, threads},
location = {Palo Alto, California, USA},
series = {JGI '01}
}

@article{10.1145/190679.190686,
author = {Terry, Allan and Hayes-Roth, Frederick and Erman, Lee and Coleman, Norman and Devito, Mary and Papanagopoulos, George and Hayes-Roth, Barbara},
title = {Overview of Teknowledge's Domain-Specific Software Architecture Program},
year = {1994},
issue_date = {Oct. 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/190679.190686},
doi = {10.1145/190679.190686},
abstract = {As part of the ARPA DSSA program, we are developing a methodology and integrating a suite of supporting tools to help specify, design, validate, package and deploy distributed intelligent control and management (DICAM) applications. Our domain of specialization is vehicle management systems, and our near-term focus is on advanced artillery systems. To attain higher levels of performance and functionality while reducing the time and cost required for development, we are recommending a generic control architecture suitable for use as a single intelligent agent or as multiple cooperating agents. This reference architecture combines a task-oriented domain controller with a meta-controller that schedules activities within the domain controller. The domain controller provides functions for model-based situation assessment and planning, and inter-controller communication. Typically, these functions are performed by components taken from a repository of reusable software. In tasks that are simple, deterministic or time-stressed, the modules may be complied into or replaced by conventional control algorithms. In complex, distributed, cooperative, non-deterministic or unstressed situations, these modules will usually exploit knowledge-based reasoning and deliberative control.To improve the controller development process, we are combining many of the best ideas from software engineering and knowledge engineering in a software environment. This environment includes a blackboard-like development workspace to represent both the software under development and the software development process itself. In this workspace, controllers are realized by mapping requirements into specializations of the reference architecture. The workspace also provides mechanisms for triggering applications of software tools, including knowledge-based software design assistants.We are currently in the third year of a five-year program. In conjunction with our collaborators at ARDEC, we have produced a schema for describing architectures which is being used by ARDEC's community of contractors, by an ARPA architecture specification project for the Joint Task Force ATD, and by the Stanford Knowledge Systems Laboratory. We have released the second major version of our development environment, which is being used at ARDEC and in support of this ARPA architecture specification program. This version of the development environment is focused on initial requirements, architecture, and design. It provides both CASE-like editing of architectures and textual browsing/editing of repository descriptions expressed in the schema mentioned above. In the remaining years of the program we will be expanding the suite of tools and improving the methodologies required to build intelligent, distributed, hybrid controllers capable of spanning multiple levels of organization and system hierarchy. This technology holds considerable promise for near-term value, and the associated methodology provides a candidate approach for realizing the goals of mega-programming practice in control software. In assessing this prospect, we discuss some of the remaining shortfalls in both methodology and tools that require additional research and development.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {68–76},
numpages = {9}
}

@inproceedings{10.1145/1953355.1953367,
author = {Cretu, Liviu-Gabriel},
title = {Event-Driven Replication in Distributed Systems},
year = {2011},
isbn = {9781450305594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1953355.1953367},
doi = {10.1145/1953355.1953367},
abstract = {In order to address problems related to geographical distribution of application instances, data distribution has been a hot topic for years. Recently, the spectacular evolution of bandwidth, frameworks and components available to build Rich Internet Applications has brought back data centralization practices. As a consequence, most database-centric applications today (e.g. business management software) are being developed using a four tier architecture: presentation logic (web tier), business logic (services), persistence services (object relational mappings and SQL variants) and database management system (one single database). While plenty of data distribution scenarios can still be found in the real world, existing solutions mainly come from DBMS providers. However, in practice it has been proven that these low level replication techniques are not really easy to use and, in most cases, they do not fit well for real-time data replication, due to the fact that: (1) highly reliable network connections are required (uninterrupted connections, in most cases) and (2) every bit has the same critical importance as any other in order to keep data integrity. Even if object-oriented replication techniques exist, such as Hibernate replication API (the well known ORM framework in Java), this paper will show that they cannot be used alone in real production environments where network connections are highly unreliable and expensive. In this short paper we will present one practical case of multiple, geographically distributed application instances, using their own local databases, while real-time synchronization over a highly unreliable and expensive network is needed. The novelty of the solution described in this paper consists in transferring the replication responsibility from database level to application logic level (business services) using a message oriented model. Unlike DBMS replication tools and techniques, our proposed model does provide the critical requirements of real-world replication needs: 1) fault-tolerance and failure recovery with event-based exception handling while still keeping the system up and running; 2) information-centric instead of bit-centric system; 3) ability to deal with numerous, unpredictable, network connections breakdowns; 4) ability to replicate data over heterogeneous platforms (both application and database level); 5) ability to run within an a multiversion application environment (different application versions on different replication nodes).},
booktitle = {Proceedings of the 4th India Software Engineering Conference},
pages = {95–98},
numpages = {4},
keywords = {replication, distributed systems, event-driven system, message-oriented model, service oriented architecture},
location = {Thiruvananthapuram, Kerala, India},
series = {ISEC '11}
}

@inproceedings{10.1109/CCGrid.2014.118,
author = {Rughetti, Diego and Di Sanzo, Pierangelo and Ciciani, Bruno and Quaglia, Francesco},
title = {Analytical/ML Mixed Approach for Concurrency Regulation in Software Transactional Memory},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.118},
doi = {10.1109/CCGrid.2014.118},
abstract = {In this article we exploit a combination of analytical and Machine Learning (ML) techniques in order to build a performance model allowing to dynamically tune the level of concurrency of applications based on Software Transactional Memory (STM). Our mixed approach has the advantage of reducing the training time of pure machine learning methods, and avoiding approximation errors typically affecting pure analytical approaches. Hence it allows very fast construction of highly reliable performance models, which can be promptly and effectively exploited for optimizing actual application runs. We also present a real implementation of a concurrency regulation architecture, based on the mixed modeling approach, which has been integrated with the open source TinySTM package, together with experimental data related to runs of applications taken from the STAMP benchmark suite demonstrating the effectiveness of our proposal.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {81–91},
numpages = {11},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@article{10.1145/1731022.1731027,
author = {Hogg, J. D. and Scott, J. A.},
title = {A Fast and Robust Mixed-Precision Solver for the Solution of Sparse Symmetric Linear Systems},
year = {2010},
issue_date = {April 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {0098-3500},
url = {https://doi.org/10.1145/1731022.1731027},
doi = {10.1145/1731022.1731027},
abstract = {On many current and emerging computing architectures, single-precision calculations are at least twice as fast as double-precision calculations. In addition, the use of single precision may reduce pressure on memory bandwidth. The penalty for using single precision for the solution of linear systems is a potential loss of accuracy in the computed solutions. For sparse linear systems, the use of mixed precision in which double-precision iterative methods are preconditioned by a single-precision factorization can enable the recovery of high-precision solutions more quickly and use less memory than a sparse direct solver run using double-precision arithmetic.In this article, we consider the use of single precision within direct solvers for sparse symmetric linear systems, exploiting both the reduction in memory requirements and the performance gains. We develop a practical algorithm to apply a mixed-precision approach and suggest parameters and techniques to minimize the number of solves required by the iterative recovery process. These experiments provide the basis for our new code HSL_MA79—a fast, robust, mixed-precision sparse symmetric solver that is included in the mathematical software library HSL.Numerical results for a wide range of problems from practical applications are presented.},
journal = {ACM Trans. Math. Softw.},
month = {apr},
articleno = {17},
numpages = {24},
keywords = {sparse symmetric linear systems, iterative refinement, FGMRES, mixed precision, multifrontal method, Fortran 95, Gaussian elimination}
}

@inproceedings{10.1145/3433210.3437519,
author = {Jia, Jinyuan and Wang, Binghui and Gong, Neil Zhenqiang},
title = {Robust and Verifiable Information Embedding Attacks to Deep Neural Networks via Error-Correcting Codes},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3437519},
doi = {10.1145/3433210.3437519},
abstract = {In the era of deep learning, a user often leverages a third-party machine learning tool to train a deep neural network (DNN) classifier and then deploys the classifier as an end-user software product (e.g., a mobile app) or a cloud service. In an information embedding attack, an attacker is the provider of a malicious third-party machine learning tool. The attacker embeds a message into the DNN classifier during training and recovers the message via querying the API of the black-box classifier after the user deploys it. Information embedding attacks have attracted growing attention because of various applications such as watermarking DNN classifiers and compromising user privacy. State-of-the-art information embedding attacks have two key limitations: 1) they cannot verify the correctness of the recovered message, and 2) they are not robust against post-processing (e.g., compression) of the classifier.In this work, we aim to design information embedding attacks that are verifiable and robust against popular post-processing methods. Specifically, we leverage Cyclic Redundancy Check to verify the correctness of the recovered message. Moreover, to be robust against post-processing, we leverage Turbo codes, a type of error-correcting codes, to encode the message before embedding it to the DNN classifier. In order to save queries to the deployed classifier, we propose to recover the message via adaptively querying the classifier. Our adaptive recovery strategy leverages the property of Turbo codes that supports error correcting with a partial code. We evaluate our information embedding attacks using simulated messages and apply them to three applications (i.e., training data inference, property inference, DNN architecture inference), where messages have semantic interpretations. We consider 8 popular methods to post-process the classifier. Our results show that our attacks can accurately and verifiably recover the messages in all considered scenarios, while state-of-the-art attacks cannot accurately recover the messages in many scenarios.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {2–13},
numpages = {12},
keywords = {machine learning security, information embedding attacks, error-correcting code},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3533767.3543290,
author = {Bodea, Alexandru},
title = {Pytest-Smell: A Smell Detection Tool for Python Unit Tests},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3543290},
doi = {10.1145/3533767.3543290},
abstract = {Code quality and design are key factors in building a successful software application. It is known that a good internal structure assures a good external quality. To improve code quality, several guidelines and best practices are defined. Along with these, a key contribution is brought by unit testing. Just like the source code, unit test code is subject to bad programming practices, known as defects or smells, that have a negative impact on the quality of the software system. As a consequence, the system becomes harder to understand, maintain, and more prone to issues and bugs. In this respect, methods and tools that automate the detection of the aforementioned unit test smells are of the utmost importance. While there are several tools that aim to address the automatic detection of unit test smells, the majority of them are focused on Java software systems. Moreover, the only known such framework designed for applications written in Python performs the detection only on Unittest Python testing library. In addition to this, it relies on an IDE to run, which heavily restricts its usage. The tool proposed within this paper aims to close this gap, introducing a new framework which focuses on detecting Python test smells built with Pytest testing framework. As far as we know, a similar tool to automate the process of test smell detection for unit tests written in Pytest has not been developed yet. The proposed solution also addresses the portability issue, being a cross-platform, easy to install and use Python library.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {793–796},
numpages = {4},
keywords = {Software Testing, Unit test, Test smell detection, Software defects},
location = {Virtual, South Korea},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3383583.3398624,
author = {Tuttle, James and Chen, Yinlin and Jiang, Tingting and Hunter, Lee and Waldren, Andrea and Ghosh, Soumik and Ingram, William A.},
title = {Multi-Tenancy Cloud Access and Preservation: Virginia Tech Digital Libraries Platform},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398624},
doi = {10.1145/3383583.3398624},
abstract = {Virginia Tech Libraries has developed a cloud-native, microservervices-based digital libraries platform to consolidate diverse access and preservation infrastructure into a set of flexible, independent microservices in Amazon Web Services. We have been an implementer and contributor to various community digital library and repository projects including DSpace, Fedora, and Samvera3. However, the complexity and cost of maintaining disparate application stacks have reduced our capacity to build new infrastructure. Virginia Tech has a long history of participation in and contribution to community-driven Open Source projects and has, in that time, developed more than a dozen independent applications architected on these stacks. The cost of independently addressing vulnerabilities, which often requires work to mitigate incompatibilities; reworking each application to comply with developing branding guidelines; and feature development and improvement has burgeoned, threatening to overwhelm our capacity. Like many of our peers5, our maintenance obligations have made continued growth unsustainable and have pushed older applications to near abandonware. We have designed and developed the Digital Libraries Platform to address these concerns thus reducing our maintenance obligations and costs associated with feature development across digital libraries. This approach represents a departure from the monolithic architectures of our legacy systems and, as such, shares more infrastructure among individual digital library implementations. The shared infrastructure facilitates rapid inclusion of new and improved features into each digital library instance. New features can be developed independent of any digital library instance and integrated into that instance by inclusion of that feature in the React/Amplify template. Changes to the template super class, such as those necessitated by evolving branding guidelines, may be immediately inherited by the template instances that subscribe to it. The platform implements Terraform6 deployment templates, Lambda serverless functions, and other cloud assets to form a microservices architecture on which multiple template-based sites are built. Individual sites are configured in AWS DynamoDB, Amazon's NoSQL database service, and via modification of shared template. Additional services provide digital preservation support including auditing, file fixity validation, replication to external cloud storage providers, file format characterization, and deposit to third-party preservation services. This presentation also discusses the cost of operating these services in AWS and strategies for mitigating those costs. These strategies include containerization to allow deployment of high-cost, asynchronous services to local infrastructure to take full advantage of existing infrastructure and advantageous utility pricing while allowing for local redeployment. In the past, developers worked in local, independent environments. New features and fixes were submitted to a central development environment testing and validation, which significantly slowed development. Migrating development, review, integration, and deployment processes to AWS decreased the time and resource bottlenecks for those processes. Our AWS cost accounting demonstrates an 87% savings over our traditional, on-premises Fedora/Samvera approach For a team of four software developers, the total cost using a traditional server-based (a t2-medium EC2 instance) development approach is about $133 per month versus our serverless-based development approach using AWS Amplify at an average of $17 per month. As the Digital Libraries Platform project expands, we anticipate publishing a set of API documents allowing us and others to reimplement specific microservices independent of the architecture.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {557–558},
numpages = {2},
keywords = {microservice, cloud computing, digital preservation, digital libraries},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3167132.3167284,
author = {Ahn, Hwi and Kang, Sungwon and Lee, Seonah},
title = {Reconstruction of Execution Architecture View Using Dependency Relationships and Execution Traces},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167284},
doi = {10.1145/3167132.3167284},
abstract = {Software1 architecture represents the structure of a software system. The execution architecture view is an especially important architecture view as it can be essential in designing and analyzing the execution structure of a system. Therefore, researchers have proposed various methods of reconstructing an execution architecture view for the systems but most of them rely on the existing architectural knowledge of the target system such as hypothesized views, existing documentations, or experts of the target system. However, when there are no available documentations or experts, such methods cannot be used for reconstructing the execution architecture view. In this paper, we propose a method that reconstructs an execution architecture view in the absence of architectural knowledge of the target system. In order to systematically bridge source code and execution elements, our method utilizes the implementation mechanisms that are identified from the dependency relationships between source code and the APIs of standard and external libraries. Once implementation mechanisms are defined, finally we can construct the execution view of the target system by extracting execution components and connectors from the execution traces obtained through the implementation mechanisms. To show the efficacy of the proposed method, a case study is conducted.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1417–1424},
numpages = {8},
keywords = {software architecture, execution view, dependency relationships, architecture reconstruction},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/2400682.2400706,
author = {Chrysos, Grigorios and Dagritzikos, Panagiotis and Papaefstathiou, Ioannis and Dollas, Apostolos},
title = {HC-CART: A Parallel System Implementation of Data Mining Classification and Regression Tree (CART) Algorithm on a Multi-FPGA System},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/2400682.2400706},
doi = {10.1145/2400682.2400706},
abstract = {Data mining is a new field of computer science with a wide range of applications. Its goal is to extract knowledge from massive datasets in a human-understandable structure, for example, the decision trees. In this article we present an innovative, high-performance, system-level architecture for the Classification And Regression Tree (CART) algorithm, one of the most important and widely used algorithms in the data mining area. Our proposed architecture exploits parallelism at the decision variable level, and was fully implemented and evaluated on a modern high-performance reconfigurable platform, the Convey HC-1 server, that features four FPGAs and a multicore processor. Our FPGA-based implementation was integrated with the widely used “rpart” software library of the R project in order to provide the first fully functional reconfigurable system that can handle real-world large databases. The proposed system, named HC-CART system, achieves a performance speedup of up to two orders of magnitude compared to well-known single-threaded data mining software platforms, such as WEKA and the R platform. It also outperforms similar hardware systems which implement parts of the complete application by an order of magnitude. Finally, we show that the HC-CART system offers higher performance speedup than some other proposed parallel software implementations of decision tree construction algorithms.},
journal = {ACM Trans. Archit. Code Optim.},
month = {jan},
articleno = {47},
numpages = {25},
keywords = {WEKA platform, high-performance computing, R project, classification and regression tree (CART), Decision tree classification (DTC), rpart library, HW architecture, reconfigurable system}
}

@inproceedings{10.1145/364447.364606,
author = {Rasala, Richard and Raab, Jeff and Proulx, Viera K.},
title = {Java Power Tools: Model Software for Teaching Object-Oriented Design},
year = {2001},
isbn = {1581133294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/364447.364606},
doi = {10.1145/364447.364606},
abstract = {The Java Power Tools or JPT is a Java toolkit designed to enable students to rapidly develop graphical user interfaces in freshman computer science programming projects. Because it is simple to create GUIs using JPT, students can focus on the more fundamental issues of computer science rather than on widget management. In a separate article[4], we will discuss with examples how the JPT can help freshman students to learn about the basics of algorithms, data structures, classes, and interface design. In this article, we will focus on how the JPT itself can be used as an extended case study of object-oriented design principles in a more advanced course.The fundamental design principles of the JPT are that the elements of a graphical user interface should be able to be combined recursively as nested views and that the communication between these views and the internal data models should be as automatic as possible. In particular, in JPT, the totality of user input from a complex view can be easily converted into a corresponding data model and any input errors will be detected and corrected along the way. This ease of communication is achieved by using string objects as a lingua franca for views and models and by using parsing when appropriate to automatically check for errors and trigger recovery. The JPT achieves its power by a combination of computer science and software design principles. Recursion, abstraction, and encapsulation are systematically used to create GUI tools of great flexibility. It should be noted that a much simpler pedagogical package for Java IO was recently presented in [9].},
booktitle = {Proceedings of the Thirty-Second SIGCSE Technical Symposium on Computer Science Education},
pages = {297–301},
numpages = {5},
location = {Charlotte, North Carolina, USA},
series = {SIGCSE '01}
}

@article{10.1145/366413.364606,
author = {Rasala, Richard and Raab, Jeff and Proulx, Viera K.},
title = {Java Power Tools: Model Software for Teaching Object-Oriented Design},
year = {2001},
issue_date = {March 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/366413.364606},
doi = {10.1145/366413.364606},
abstract = {The Java Power Tools or JPT is a Java toolkit designed to enable students to rapidly develop graphical user interfaces in freshman computer science programming projects. Because it is simple to create GUIs using JPT, students can focus on the more fundamental issues of computer science rather than on widget management. In a separate article[4], we will discuss with examples how the JPT can help freshman students to learn about the basics of algorithms, data structures, classes, and interface design. In this article, we will focus on how the JPT itself can be used as an extended case study of object-oriented design principles in a more advanced course.The fundamental design principles of the JPT are that the elements of a graphical user interface should be able to be combined recursively as nested views and that the communication between these views and the internal data models should be as automatic as possible. In particular, in JPT, the totality of user input from a complex view can be easily converted into a corresponding data model and any input errors will be detected and corrected along the way. This ease of communication is achieved by using string objects as a lingua franca for views and models and by using parsing when appropriate to automatically check for errors and trigger recovery. The JPT achieves its power by a combination of computer science and software design principles. Recursion, abstraction, and encapsulation are systematically used to create GUI tools of great flexibility. It should be noted that a much simpler pedagogical package for Java IO was recently presented in [9].},
journal = {SIGCSE Bull.},
month = {feb},
pages = {297–301},
numpages = {5}
}

@inproceedings{10.5555/317498.317735,
author = {Rombach, H. Dieter},
title = {Specification of Software Process Measurement},
year = {1990},
isbn = {0818621044},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {Most software organizations perform their development and maintenance projects according to inadequately specified processes. The lack of explicit and formal process specifications creates: a process experience transfer problem: It is difficult to transfer the organization's informal process knowledge (e.g., training new personnel).a process tailoring problem: The tailoring of processes to changing project goals and project environment characteristics is based on subjective rather than objective knowledge regarding the project differences and the effectiveness of candidate methods and tools.a process control problem: Controlling the adherence of a process execution to an inadequate specification is necessarily based on subjective judgement rather than objective criteria.a process improvement problem: The desired improvement of software processes requires an understanding of their current status, the identification of weaknesses regarding their current status, a systematic eradication of those weaknesses, and a validation that the new processes indeed represent an improvement. Without adequate process specifications this process will be random rather than systematic.All these problems are often hidden in very stable organizations for a long time. However, they surface as soon as the carriers of implicit process knowledge leave the organization or dramatic changes in application domain or software technology take place.There is agreement that more formal approaches to process specification are needed [3]. There is also agreement that only the integration of construction and measurement processes will provide the desired engineering control for project-specific process execution and organization-specific improvement. There is no consensus reached as to how such specifications should look like and how they should be used and supported.Our own experience in monitoring and evaluating software processes and products in a variety of organizations over more than a decade has resulted in a number of lessons learned [1, 2]. These lessons motivate the need to: formalize the planning and specification of both processes and productsformalize the planning and specification of both construction and measurement oriented processesformalize the planning and specification of all aspects of measurement, ranging from data collection and validation to evaluation and feedbackuse consistent specification models for the measured processes, the measurement processes, and the measurement goalsprovide appropriate automated support for all planning and specification activities (construction and measurement)provide appropriate automated support for storing, formalizing, tailoring and reusing all accumulated process knowledgeAt the University of Maryland, several projects are aimed at building automated software environments based on these lessons learned. In the MVP (Multi-View Process specification) project, we are developing a process specification language (MVP-L) which addresses the lessons (Ll) - (L4) [4]. In the TAME (Tailoring A Measurement Environment) project, we are developing a measurement environment which addresses lessons (L5) and (L6) [2].In the MVP project, we have developed a first prototype language (MVP-L1) which allows the specification of all kinds of process aspects [4]. Its important concepts for specifically dealing with measurement aspects are: The combination of two different language paradigms is used to specify the execution of software processes: the procedural and constraint oriented paradigms. For example, the procedural paradigm allows us to explicitly specify sequential and parallel process execution via control flow and data flow relations. The constraint oriented paradigm allows us to limit the otherwise independent execution of processes via constraints.Three different ways to express the relationship between measurement processes and the to-be-measured construction processes: standard control flow constructs, constraints that need to be checked upon start or termination of the respective construction process, and a “tightly parallel execution” relation between construction and related measurement process. In this case any activity of the construction process triggers an activity of the measurement process. For example, this relationship can be used to express the coupling between a system testing process and the process of collecting all effort spent in performing that test process. If we assume that we are interested in measuring the effort spent during the system testing phase for activities such as data generation, test execution, test result assessment, fault isolation, fault correction, and regression test, we implicitly assume that system testing is performed according to this activity pattern. Very often this implicit assumption is not true. The test process may lack such detail of precision. Explicit specifications of both the system testing and the related measurement process will reveal such inconsistencies. Such inconsistencies leave one of three options: (i) the specification of the to-be-measured testing process needs to be specified more precisely, (ii) the measurement process needs to be relaxed, or (iii) rigid data validation procedures need to be imposed because the measurement process is not consistent with the measured process.Three concepts to structure large process specifications and/or organize a large number of process specifications: refine &amp; compose relations, specialize &amp; generalize relations, and roles. For example, refine &amp; compose relations can be used to indicate that “specifications x, y, z are a refinement of specification A” or that “specification A is composed of specifications x, y, z”. The specialize &amp; generalize relations can be used to indicate that “specification x, y are special instances of specification A” or that “specification A is a general version of specifications x, y”. The role concept allows the clustering of a set of processes and constraints indicating that they serve a common purpose or represent a project-specific view. A realistic project specification is expected to consist of a large number of basic processes which are structured according to the three concepts above. For example, all measurement processes together may be viewed as a separate role.The integration of measurement objectives and metrics with the construction and measurement processes. In the TAME project, for example, the relationships between project objectives and related metrics are specified via a goal/question/metric graph [2], the relationship between metrics and the measurement processes designed to compute them via a measurement process specification, and the integration of the measurement process with the to-be-measured construction process via the relations described in (C2).Currently, MVP-L1 is used on a trial basis to specify the maintenance processes in the NASA/SEL environment and to plan adequate measurement processes [5]. The use of a formal specification language has helped us to speed up the process of planning an initial set of working measurement processes. The experienced speed-up is basically due to the fact that formal process specifications enabled a systematic dialogue between us and the maintenance personnel as to whether our understanding of their processes is correct or not, and to reveal differences between different projects within that same environment. We developed two levels of process specifications for NASA: general ones which capture the commonalties among all maintenance projects, and specific ones which are tailored to reflect the specific characteristics of each project. The experience from this trial application will be fed back into the next improved prototype version of our process specification language.In the TAME project, we work towards adequate automated support for measurement. This includes support for the planning and specification of software construction and measurement processes, the execution of software processes according to their underlying specifications, the management of all process and product related knowledge accumulated during planning and execution, and the appropriate packaging of process and product knowledge in order to enhance its reuse potential.},
booktitle = {Proceedings of the 5th International Software Process Workshop on Experience with Software Process Models},
pages = {127–129},
numpages = {3},
location = {Kennebunkport, Maine, USA},
series = {ISPW '90}
}

@inproceedings{10.1145/98894.98941,
author = {Bonnet, Christine and Taterode, Helene and Kouloumdjian, Jacques and Hacid, Mohand-Said},
title = {Using Artificial Intelligence Techniques for Intelligent Simulation in Memory Re-Education},
year = {1990},
isbn = {0897913728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/98894.98941},
doi = {10.1145/98894.98941},
abstract = {The increasing complexity of new applications has made it necessary to have tools offering a large spectrum of functionalities and several paradigms. This is especially true for intelligent tutoring systems which need (meta)rule based inference for adaptive teaching strategies, object oriented paradigm for learner modeling, graphic simulation tools and large data management facilities for storing lessons and results. The carrying out of such tools, where several software cooperate, should take into account the development of user-friendly environments.This paper describes an intelligent rehabilitation system used for people suffering from memory troubles. It is aimed at simulating everyday “ecological” situations by means of graphical scenes: patients are then asked to realize specific actions on the displayed objects such as preparing breakfast according to dynamically predefined scenarii. The construction of such a system needs to use intelligent tools, which are able to substitute neuropsychologists in the re-education phase.Our solution consists of combining Artificial Intelligence tools and Simulation ones. The system articulates around an expert system generator and an interactive graphical simulation package. The detailed architecture and the working of the system are described here.},
booktitle = {Proceedings of the 3rd International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems - Volume 2},
pages = {874–881},
numpages = {8},
location = {Charleston, South Carolina, USA},
series = {IEA/AIE '90}
}

@article{10.1145/566493.1148029,
author = {Kumar M, Bharath and Srikant, Y. N. and R, Lakshminarayanan},
title = {On the Use of Connector Libraries in Distributed Software Architectures},
year = {2002},
issue_date = {January 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/566493.1148029},
doi = {10.1145/566493.1148029},
abstract = {Recent developments in the field of software architecture have emphasized the concept of first class connectors, which capture the interaction between components. The concept of first class connectors aids the development of distributed software architectures since it can be used to separate concerns of remote interaction between components. A library of prewritten connectors would help prototype, develop, maintain and reconfigure distributed software architectures. Completely automating the usage of connector libraries in distributed software architectures is not trivial since code fragments implementing the connectors have to be partitioned, deployed and instantiated in different machines. In this paper we discuss these issues in connector refinement and instantiation imposed by distributed software architectures and present the construction of a tool that works alongside a connector library to automatically partition, deploy and instantiate architectural entities in various machines. Scalability of the tool to allow for new connector types has been an important issue in its design and function. We also demonstrate the high flexibility and reconfigurability achieved on an interesting application by using a connector library along with our tool.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {45–52},
numpages = {8}
}

@inproceedings{10.1145/1146909.1147040,
author = {Arora, Divya and Raghunathan, Anand and Ravi, Srivaths and Sankaradass, Murugan and Jha, Niraj K. and Chakradhar, Srimat T.},
title = {Software Architecture Exploration for High-Performance Security Processing on a Multiprocessor Mobile SoC},
year = {2006},
isbn = {1595933816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1146909.1147040},
doi = {10.1145/1146909.1147040},
abstract = {We present a systematic methodology for exploring the security processing software architecture for a commercial heterogeneous multiprocessor system-on-chip (SoC) for mobile devices. The SoC contains multiple host processors executing applications and a dedicated programmable security processing engine. We developed an exploration methodology to map the code and data of security software libraries onto the platform, with the objective of maximizing the overall application-visible performance. The salient features of the methodology include (i) the use of real performance measurements from a prototyping board that contains the target platform to drive the exploration, (ii) a new data structure access profiling framework that allows us to accurately model the communication overheads involved in offloading a given set of functions to the security processor, and (iii) an exact branch-and-bound based design space exploration algorithm that determines the best mapping of security library functions and data structures to the host and security processors.We used the proposed framework to map a commercial security library to the target mobile application SoC. The resulting optimized software architecture outperformed several manually-designed software architectures, resulting in upto 12.5X speedup for individual cryptographic operations (encryption, hashing) and 2.2X-6.2X speedup for applications such as a Digital Rights Management (DRM) agent and Secure Sockets Layer (SSL) client. We also demonstrate the applicability of our framework to software architecture exploration in other multiprocessor scenarios.},
booktitle = {Proceedings of the 43rd Annual Design Automation Conference},
pages = {496–501},
numpages = {6},
keywords = {software partitioning, computation offloading},
location = {San Francisco, CA, USA},
series = {DAC '06}
}

@article{10.5555/2627435.2697054,
author = {Jin, Jiashun and Zhang, Cun-Hui and Zhang, Qi},
title = {Optimality of Graphlet Screening in High Dimensional Variable Selection},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Consider a linear model Y = Xβ+ωz, where X has n rows and p columns and z - N(0, In). We assume both p and n are large, including the case of p ≫ n. The unknown signal vector β is assumed to be sparse in the sense that only a small fraction of its components is nonzero. The goal is to identify such nonzero coordinates (i.e., variable selection).We are primarily interested in the regime where signals are both rare and weak so that successful variable selection is challenging but is still possible. We assume the Gram matrix G = X′X is sparse in the sense that each row has relatively few large entries (diagonals of G are normalized to 1). The sparsity of G naturally induces the sparsity of the so-called Graph of Strong Dependence (GOSD). The key insight is that there is an interesting interplay between the signal sparsity and graph sparsity: in a broad context, the signals decompose into many small-size components of GOSD that are disconnected to each other.We propose Graphlet Screening for variable selection. This is a two-step Screen and Clean procedure, where in the first step, we screen subgraphs of GOSD with sequential χ2-tests, and in the second step, we clean with penalized MLE. The main methodological innovation is to use GOSD to guide both the screening and cleaning processes.For any variable selection procedure β, we measure its performance by the Hamming distance between the sign vectors of β and β, and assess the optimality by the minimax Hamming distance. Compared with more stringent criteria such as exact support recovery or oracle property, which demand strong signals, the Hamming distance criterion is more appropriate for weak signals since it naturally allows a small fraction of errors.We show that in a broad class of situations, Graphlet Screening achieves the optimal rate of convergence in terms of the Hamming distance. Unlike Graphlet Screening, well-known procedures such as the L0/L1-penalization methods do not utilize local graphic structure for variable selection, so they generally do not achieve the optimal rate of convergence, even in very simple settings and even if the tuning parameters are ideally set.The the presented algorithm is implemented as R-CRAN package ScreenClean and in matlab (available at http://www.stat.cmu.edu/~jiashun/Research/software/GS-matlab/).},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {2723–2772},
numpages = {50},
keywords = {asymptotic minimaxity, graph of strong dependence (GOSD), rare and weak signal model, Hamming distance, sparsity, graph of least favorables (GOLF), phase diagram, screen and clean, graphlet screening (GS)}
}

@article{10.1145/101139.101140,
author = {Baker, P. L.},
title = {Ada as a Preprocessor Language},
year = {1990},
issue_date = {Jan./Feb. 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {X},
number = {1},
issn = {1094-3641},
url = {https://doi.org/10.1145/101139.101140},
doi = {10.1145/101139.101140},
abstract = {Preprocessors are components of a software development environment that can increase productivity by providing semantic capabilities for expressing certain source language statement s concisely and directly which would otherwise be expressed indirectly and verbosely. Moreover, it is frequently necessary to reflect a single programming decision in several places in the source text; a preprocessor can propagate a single specification of such a decision to the points it affects thereby reducing effort and removing the possibility a consistency error. In traditional environments, the language of preprocessor statements differs from the underlying programming language, e.g., CPP differs from C itself. If such deviations were allowed in an Ada environment, they would impair portability and detract from the effort to provide widespread, uniform language training in pure Ada. This paper demonstrates, however, that Ada is, by itself, a viable preprocessor language provided two conventions are adopted. First, the compilation sequence of preprocessing must be used; fortunately, an Ada compiler can serve in a preprocessor step without affecting its validation under the ACVC. Second, normal and preprocessor statements are not intermixed; Ada package structure must be employed to separate contexts that would be differentiated by syntax in traditional preprocessing. The distinctive compilation sequence of Ada preprocessing is a multiple pass compilation where compilation steps are connected via Ada text, not Ada library units. The results of one compilation pass are converted to text via an execution step and then provided as text input to the next step. As an example, the paper describes a preprocessor that constructs subsystem interfaces.},
journal = {Ada Lett.},
month = {jan},
pages = {83–91},
numpages = {9}
}

@proceedings{10.1145/1149993,
title = {ICWE '06: Workshop Proceedings of the Sixth International Conference on Web Engineering},
year = {2006},
isbn = {1595934359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The Sixth International Conference on Web Engineering (ICWE 2006) held in Palo Alto, California, at the Stanford Linear Accelerator Center, hosted the workshops MDWE, AEWSE and SMIWEP-MATeS. This volume of the ACM Digital Library contains a selection of papers presented at the three workshops which took place on July, 10th and July, 11th.ICWE 2006 and the associated workshops aim to promote research and scientific exchange related to Web Engineering and to bring together practitioners, scientists and researchers, as well as those interested in learning more about Web Engineering. The Web Engineering discipline comprises the development and use of models, methodologies, technologies, tools and techniques that are used to develop and maintain Web applications within economic constraints. In the era of Web-based information and services we are living in, the development of such Web systems is demanding our attention in order to provide sound and engineered solutions to specific problems in the Web Engineering field. The articles presented at the ICWE 2006 workshops have proposed novel results in this area focusing each on a specific domain:The Second International Workshop on Model-Driven Web Engineering (MDWE'06) focused on model-based and model-driven development of Web applications. The main goal of this workshop series is to promote the use of models and model transformations as well as the definition of domain specific modeling languages (DSML) and construction of CASE tools supporting model-driven processes. Strengths and weaknesses of model-driven approaches in the Web domain were lively discussed during the workshop.The First International Workshop on Adaptation and Evolution in Web Systems Engineering (AEWSE'06) aimed at bringing together researchers and practitioners with different research interests, belonging to communities like Web Engineering, Adaptive Hypermedia, User Modeling, Active Databases, Semantic Web, Ontology Evolution, Database Evolution, Temporal Data, Software Engineering and Mobile Computing. The goal of the workshop was to address adaptation and evolution of Web applications both during design, implementation and deployment.The Joint Workshop on Web Services Modeling and Implementation using Sound Web Engineering Practices and Methods, Architectures &amp; Technologies for e-Service Engineering (SMIWEP-MATeS'06) covered current state-of-the-art and best practices in e-Service engineering, focusing on aspects such as development of semantic Web applications, integration of technologies, requirements engineering, patterns and reference architectures, design methodologies and processes and quality models.},
location = {Palo Alto, California, USA}
}

@inproceedings{10.1145/3377812.3381387,
author = {Su\~{n}\'{e}, Agust\'{\i}n E. Martinez},
title = {Formalization and Analysis of Quantitative Attributes of Distributed Systems},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381387},
doi = {10.1145/3377812.3381387},
abstract = {While there is not much discussion on the importance of formally describing and analyzing quantitative requirements in the process of software construction; in the paradigm of API-based software systems, it could be vital. Quantitative attributes can be thought of as attributes determining the Quality of Service - QoS provided by a software component published as a service. In this sense, they play a determinant role in classifying software artifacts according to specific needs stated as requirements.In this work, we present a research program consisting of the development of formal languages and tools to characterize and analyze the Quality of Service attributes of software components in the context of distributed systems. More specifically, our main motivational scenario lays on the execution of a service-oriented architecture.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {210–213},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2508075.2508433,
author = {Merson, Paulo},
title = {Ultimate Architecture Enforcement: Custom Checks Enforced at Code-Commit Time},
year = {2013},
isbn = {9781450319959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508075.2508433},
doi = {10.1145/2508075.2508433},
abstract = {Creating a software architecture is a critical task in the development of software systems. However, the architecture discussed and carefully created is often not entirely followed in the implementation. Unless the architecture is communicated effectively to all developers, divergence between the intended architecture (created by the architect) and the actual architecture (found in the source code) tends to gradually increase. Static analysis tools, which are often used to check coding conventions and best practices, can help. However, the common use of static analysis tools for architecture enforcement has two limitations. One is the fact that design rules specific to a software architecture are not known and hence not enforced by the tool. The other limitation is more of a practical issue: static analysis tools are often integrated to the IDE or to a continuous integration environment; they report violations but the developers may choose to ignore them. This paper reports a successful experience where we addressed these two limitations for a large codebase comprising over 50 Java applications. Using a free open source tool called checkstyle and its Java API, we implemented custom checks for design constraints specified by the architecture of our software systems. In addition, we created a script that executes automatically on the Subversion software configuration management server prior to any code commit operation. This script runs the custom checks and denies the commit operation in case a violation is found. When that happens, the developer gets a clear error message explaining the problem. The architecture team is also notified and can proactively contact the developer to address any lack of understanding of the architecture. This experience report provides technical details of our architecture enforcement approach and recommendations to employ this or similar solutions more effectively.},
booktitle = {Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, &amp; Applications: Software for Humanity},
pages = {153–160},
numpages = {8},
keywords = {software architecture, architecture enforcement, checkstyle., static analysis, java, architecture conformance},
location = {Indianapolis, Indiana, USA},
series = {SPLASH '13}
}

@inproceedings{10.1145/3141517.3141856,
author = {Lima, Phyllipe and Guerra, Eduardo and Nardes, Marco and Mocci, Andrea and Bavota, Gabriele and Lanza, Michele},
title = {An Annotation-Based API for Supporting Runtime Code Annotation Reading},
year = {2017},
isbn = {9781450355230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141517.3141856},
doi = {10.1145/3141517.3141856},
abstract = {Code annotations are the core of the main APIs and frameworks for enterprise development, and are widely used on several applications. However, despite these APIs and frameworks made advanced uses of annotations, the language API for annotation reading is far from their needs. In particular, annotation reading is still a relatively complex task, that can consume a lot of development time and that can couple the framework internal structure to its annotations. This paper proposes an annotation-based API to retrieve metadata from code annotations and populate an instance with meta-information ready to be used by the framework. The proposed API is based on best practices and approaches for metadata definition documented on patterns, and has been implemented by a framework named Esfinge Metadata. We evaluated the approach by refactoring an existing framework to use it through Esfinge Metadata. The original and the refactored versions are compared using several code assessment techniques, such as software metrics, and bad smells detection, followed by a qualitative analysis based on source code inspection. As a result, the case study revealed that the usage of the proposed API can reduce the coupling between the metadata reading code and the annotations.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection},
pages = {6–14},
numpages = {9},
keywords = {framework development, metadata, code annotation},
location = {Vancouver, BC, Canada},
series = {Meta 2017}
}

@inproceedings{10.1145/611892.611894,
author = {Collins, William and Tenenberg, Josh and Lister, Raymond and Westbrook, Suzanne},
title = {The Role for Framework Libraries in CS2},
year = {2003},
isbn = {158113648X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/611892.611894},
doi = {10.1145/611892.611894},
abstract = {The recent emergence of object-oriented framework libraries of classic data structures and algorithms such as the Standard Template Library and Java's Collection classes provides a set of general and efficient data structure components for use by practicing software developers. A number of textbook writers are beginning to incorporate the use of these frameworks into "CS2", the traditional first course in data structures at the university level.There has scarcely been a discussion of how these frameworks should best be incorporated into CS2, if they should be used at all. The proposed panel will examine the role of standardized framework libraries in the first data structures course at the university level. Panelists will focus on the following questions. What are the fundamental objectives of CS2, and what role might frameworks have in meeting these objectives? How does an instructor balance student needs for additional instruction in programming basics (e.g. arrays and pointers) versus practice in larger scale design and code reuse? What would be given up to incorporate frameworks into CS2? And to what extent is it important for students to construct elementary data structures from first principles? What assumptions about student cognition and learning does a pro- or con-frameworks approach imply.By trying to articulate answers to some of the above questions, we hope to raise the level of discussion concerning the evolution of the introductory computer science curriculum. This panel will thus try to make explicit, and hence available for critical examination, some of the arguments and assumptions that inform this debate.},
booktitle = {Proceedings of the 34th SIGCSE Technical Symposium on Computer Science Education},
pages = {403–404},
numpages = {2},
keywords = {CS2, frameworks, curriculum},
location = {Reno, Navada, USA},
series = {SIGCSE '03}
}

@article{10.1145/792548.611894,
author = {Collins, William and Tenenberg, Josh and Lister, Raymond and Westbrook, Suzanne},
title = {The Role for Framework Libraries in CS2},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/792548.611894},
doi = {10.1145/792548.611894},
abstract = {The recent emergence of object-oriented framework libraries of classic data structures and algorithms such as the Standard Template Library and Java's Collection classes provides a set of general and efficient data structure components for use by practicing software developers. A number of textbook writers are beginning to incorporate the use of these frameworks into "CS2", the traditional first course in data structures at the university level.There has scarcely been a discussion of how these frameworks should best be incorporated into CS2, if they should be used at all. The proposed panel will examine the role of standardized framework libraries in the first data structures course at the university level. Panelists will focus on the following questions. What are the fundamental objectives of CS2, and what role might frameworks have in meeting these objectives? How does an instructor balance student needs for additional instruction in programming basics (e.g. arrays and pointers) versus practice in larger scale design and code reuse? What would be given up to incorporate frameworks into CS2? And to what extent is it important for students to construct elementary data structures from first principles? What assumptions about student cognition and learning does a pro- or con-frameworks approach imply.By trying to articulate answers to some of the above questions, we hope to raise the level of discussion concerning the evolution of the introductory computer science curriculum. This panel will thus try to make explicit, and hence available for critical examination, some of the arguments and assumptions that inform this debate.},
journal = {SIGCSE Bull.},
month = {jan},
pages = {403–404},
numpages = {2},
keywords = {CS2, frameworks, curriculum}
}

@inproceedings{10.1145/62548.62553,
author = {Evenson, Kristin M.},
title = {Reference Interviews and the Computing Center Help Desk},
year = {1988},
isbn = {0897912861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/62548.62553},
doi = {10.1145/62548.62553},
abstract = {How often does the help desk forward a call to you, the spreadsheet expert, when they should have sent it to the database expert? Has anyone ever replied to one of your brilliant problem resolutions with “but that isn't what I wanted!”? All consultants know the frustration of talking to someone at length, discovering only after several minutes that they've been answering the wrong question.Still, much of the training for computing center user support focuses on technical knowledge. Consultants often view the interchange between user and consultant as more of an art than a science. Consultants devote their limited training time to learning new systems; they count users to measure their success.However, computer consultants can train themselves to interact more efficiently with users and can find new ways to evaluate their work. Users will benefit from better-focused consulting. Consultants will spend less time thrashing through misunderstandings, enjoying more time to work on actual problems. We can begin by using reference librarians as a model.You might wonder what computing consultants can learn from librarians. After all, the library is an long-established institution, familiar and friendly. The computing center, now—there's the cutting edge, unknown and frightening. However, novice users need expert guides to the information resources in both environments.Both computing center and library users become anxious when they must search out specific information. They don't know how to ask for help; they don't know what information they should provide in order to get the best assistance. Computing center users seem to sidle up to their main question. For example, a secretary asks about features of the LaserJet II when she wants to know how to get landscape printing on their old LaserJet. Similarly, library users approach the reference librarian with questions only obliquely related to their real aim. An anxious student asks for gardening books when his report on pesticides is due tomorrow.Neither reference librarians nor computing center consultants can train users to ask better questions. However, reference librarians have developed a set of techniques which they use to discover their users' actual information needs. Librarians use the term “reference interview” to refer to the transaction between the person seeking information and the person providing information. After the initial contact, the librarian controls the interview, asking questions designed to discover the user's real question. The length of the reference interview depends on the complexity of the problem and the ability of the user to define the question.Computing center consultants can easily apply reference interview techniques to their interactions with users. Listen to this quote from a reference service textbook: “Many people are not aware that information of interest or of use to them is generally available in libraries.”“As a reference librarian, one has accepted the responsibility of answering questions pleasantly, speedily, accurately, and, as nearly as possible, to the satisfaction of the inquirer.” (Thomas, p.95)Change “library” to “computing center” and change “reference librarian” to “user consultant” and you have a goal statement suitable to any computing center help desk. A computing center help desk is very like a library reference desk. Both computing consultants and reference librarians answer users' questions. Both may use encounters with users to teach them how to use library or computing center resources. Both struggle with similar problems of inarticulate users, inadequate resources, and burned-out staff.Several factors exacerbate the communication difficulties. Novice computer users don't know the jargon. They are scared of computers, or angry because they don't like feeling ignorant. They don't want to become computer experts; they just want to do their work. Often they feel pressured by deadlines. They don't have time to become acquainted with their system; they need to produce work now.Librarians face similar difficulties. Library users don't understand the classification systems. They don't want to become librarians. They just want information and resent having to ask for help. Just as novice computer users don't want to admit ignorance, library users don't want to admit that they don't know how to use the library. Often they suffer the same time pressures; their paper is due tomorrow and they need information right now.Computing center consultants can adopt most reference interview techniques unchanged, using reference librarians' work as a model to train and evaluate help desk staff. However, there are some obstacles to effortless use of the reference interview. Some consultants may feel that the differences between reference librarians and computer user consultants outweigh the similarities. Others would argue that we cannot define the reference interview clearly enough to apply these techniques in any structured way. Finally, reference interview training and evaluation will demand time.These objections are valid. Computing centers are different from libraries; computing center consultants will not use reference interview techniques in exactly the same way as reference librarians. The reference interview does sometimes seem to be used as no more than a catch phrase for a grab bag of communication tips. We cannot effectively teach reference interview techniques if we have no more than a list of tips. Even if we can define the reference interview well enough to teach its use, we must add this training to consultants' already overbooked schedules. Evaluating consultants' use of reference interview techniques will require close observation of consultants' work and new methods of evaluation. However, none of these objections outweigh the benefits of the reference interview.People do come to the library help desk for different reasons than they come to the computer center help desk. Library users usually have “what” questions; they want specific pieces of information. Computer users usually have “how” questions; they want instructions on how to accomplish some task. Library users want facts; journal citations, articles, and books on a particular subject. Computer users want instruction, problem fixes, disaster recovery, and help with diagnosing problems.However, even when “typical” computing center users ask different question than “typical” library users, consultants can use the same reference interview techniques that librarians use. Reference interview techniques help the consultant control the interview in order to more easily discover the specific nature of each user's question. These techniques apply equally well whether users want quantitative or procedural information.The amorphous nature of the reference interview is a more serious obstacle to training and evaluation. The “reference interview” can degenerate to nothing more than a label for a bunch of communication buzzwords. Computing center staff cannot apply reference interview techniques with measurable success unless they work from a clear definition.The essence of the reference interview is that the information provider controls the interaction in order to best meet the needs of the person who needs the information. I divide the reference interview into three parts: approach, dialogue, and closure.During approach, users decide who to approach and how to state their question. Consultants can control the environment to make it easy for the user to find help, again taking suggestions from library research. Signs should identify the help desk. Both student assistants and full-time consultants may wear name tags or badges. Help desk staff should appear “interruptable”—neither completely idle nor completely immersed in some project.Reference interview articles often focus on the many communication techniques used in the second phase of dialogue between the user and the information provider. At the University of Iowa Personal Computing Support Center, we emphasize three techniques with our student workers: attending to the user, prompting for more information, and checking for mutual understanding.During the dialogue, consultants depend on their general knowledge to recognize common problems and make connections which the novice user cannot. If unchecked by careful use of the reference interview, this ability to jump to the correct conclusion betrays both the consultant and the users. Dazzled users think that consultants magically solve their problems with almost no information. Consultants may jump too fast, giving users the right answers to the wrong questions. Careful attention to the user and checking for mutual understanding minimizes these problems.The consultant must close the interview so that both the consultant and the user have the same understanding. During closure, the consultant makes sure that the user understands the answer and that both have the same expectations of any follow-up. In an ideal world, every reference interview ends with a satisfied user. In our imperfect reality, some users will not accept our answers. Consultants can borrow techniques from reference librarians to work with problem users.The last and most serious obstacle to using reference interview techniques is the need for new methods of training and evaluation. Somehow consultants must find time to develop training, schedule training sessions, and evaluate their use of these techniques. With new software and hardware, consultants can accomplish much on their own if they are just given machine, manual, and disks. Reference interview techniques require more structured training. Again, we can draw upon the experience of reference librarians and use a variety of methods to train consultants to use reference interview techniques. Combinations of case studies and role-playing work well for many, especially if combined with videotaping. Concentrating on one technique at a time seems to work better than trying to teach every possible skill; this also allows shorter sessions which you can more easily fit into busy schedules.Applying reference interview techniques will help consultants improve their work with users. Consultants must also devise new methods of evaluating their work in order to prove to themselves and to their management that the time spent on reference interview techniques is worthwhile.In most computing centers, consultants religiously record each contact with a user. Their managers want to see numbers — preferably increasing numbers. Bigger numbers indicate that the consultants have accomplished more and thus deserve a bigger share of the budget. The consultants know that other departments within the computing center supply statistics to the administration, and that the consultants had better supply statistics that are just as good.Thus consultants keep logs to mark each time they talk to a user. They break down contacts into telephone and walk-in; they code each for the subject of the question and categorize each by status of user. Consultants may decide to record not just each person, but each question asked by each person. After all, just one person may ask about using footnotes in Word, transferring Macintosh files to a DOS diskette, and recovering erased files. Shouldn't that count as three? They also note the start and stop time of each contact.Each quarter the consultants transform their tickmarks into glowing reports of the multitudes who have benefited from the computing center help desk. However, these reports to the managers don't do that much for the consultants. The quantity of work may not impress the managers. Some managers who find the statistics impressive may keep their impression to themselves, giving no reward to the consultants. Those managers who do reward the consultants often give them rewards that increase the workload. They find money in the budget for more student workers — who must be trained. They provide a local area network — which must be installed and maintained. They give everyone new software — which the consultants must somehow find time to learn and then use their learning as the basis for more consulting and classes.Even when these quarterly stacks of statistics do bring rewards, they do not help the consultants or the managers truly evaluate their work. Numbers of users seen does not invariably correlate with numbers of problems solved. Measuring numbers alone tells the computing center much about increasing demand and peak usage times, but it tells nothing about the quality of computing center services. Consultants must devise new methods to evaluate their work, educate their managers to use these new evaluation methods, and generate their own rewards for effective work.In order to develop adequate measures, consultants must decide why they consult and exactly what they do when they consult. They should know how their private purposes fit the computing center's official statement of purpose. Many computing centers set forth the goal of greater productivity through computing and their consultants spread the gospel across campus. Many consultants encourage users to become self-sufficient — either to see the fledglings fly away from the nest or to shoo away the pests. Some encourage dependency, protecting each tender novice from the awful complexities of computing. Some consult only because they happened to find this job and they don't want to search for another right now.As they consider their reasons for consulting, the consultants should observe their daily work. Consulting services vary widely; within one computing center the individual consultants provide different services, which should be evaluated differently. One consultant may teach no classes but spend twenty hours per week visiting departments to provide on-site assistance. Each consultant should list his or her activities — writing articles, teaching seminars, help desk services, etc.Once they have developed a statement of purpose and a list of activities, the consultants can devise methods to evaluate their work. Each activity calls for a different tool of evaluation. Each tool should be chosen to answer two questions: “Does x (a particular technique) accomplish y (my reason for consulting)? How well does it accomplish it?” For example, consultants may use feedback questionnaires and tests to answer these questions about their seminars.Obviously, the simple recording of numbers takes less time than this meditation on purpose, listing of activities, and then evaluation of activities according to how well they accomplish the purpose. Consultants often feel they have barely enough time to note one user contact before the next approaches. Few managers encourage overworked consultants to take time for detailed evaluation.However, consultants can develop meaningful evaluations of their work a little at a time. While they continue to supply their managers with numbers, they can also select one activity to evaluate for a set time. The results of these evaluations, added to the usual statistics, will show their concern with quality as well as quantity. The consultants should demonstrate that these evaluations help the computing center accomplish its formally stated goals. This will encourage their managers both to see the value of evaluation and to appropriately reward the consultants.The evaluations point out where consultants need to improve their work and where they should receive rewards. Part of the consultants' reward will lie in knowing that they have reached users. Few consultants derive much pleasure from knowing that they talked to thirty users per day this quarter compared to twenty-two per day last quarter. Most want to know they make a difference to at least some of those users. Merely recording the number of users who came to the help desk does not tell the consultants whether any of those users left the help desk happier than they came to it. Evaluation provides proof that the consultants actually do help users — proof that will provide much-needed satisfaction to the consultants.Beyond the intangible reward of personal satisfaction, those consultants who can demonstrate progress toward stated goals are best equipped to suggest suitable rewards to the management. If the consultants can link these rewards to future progress — the three-day seminar in Boston that will even further improve their consulting, for example — so much the better.As we hustle to keep track of continuous updates and the constant traffic of users, we can lose sight of our goals. Finding new ways to work with users and new ways to evaluate our work will help us find new enthusiasm for our work.},
booktitle = {Proceedings of the 16th Annual ACM SIGUCCS Conference on User Services},
pages = {7–11},
numpages = {5},
location = {Long Beach, California, USA},
series = {SIGUCCS '88}
}

@inproceedings{10.1145/322917.323043,
author = {Bosworth, Edward L.},
title = {The Adaptability of Ada as a Language for Expert Systems (Abstract Only)},
year = {1987},
isbn = {0897912187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322917.323043},
doi = {10.1145/322917.323043},
abstract = {There are two trends in the software initiatives of the U. S. Department of Defense which will have direct impact on future software development and which suggest a focus for current basic research in software. These trends are 1) the move toward smart weapons, suggesting the use of Expert Systems to control these weapons, and 2) the directive that all software for embedded computer systems be written in Ada. The research focus suggested by these developments is an analysis of the Ada programming language and its associated programming support environment (APSE) to determine how to design and construct an Expert System for natural implementation in Ada.It is well known that Ada can be used as a language for Expert Systems. There now exist several Expert System Shells written in Ada and more are being developed every day. The issue still open to research is how to do this optimally, making the best use of the expressive power of the language. An absurd example will make this clearer. It is also known that it is possible to write an Expert System in COBOL. Doing such would be a very tedious and error prone task, because the expressive power of the COBOL language is focused on commercial applications.Another possibility for writing an Expert System in Ada would be to take an existing Expert System in LISP and translate the code, line for line into Ada. Admittedly this is a plausible task; this author has done some preliminary investigation in this area. However, it is arguable that such an approach will not result in efficient Ada code, for constructs which are natural to LISP might not translate into constructs natural to Ada.While it is difficult to give a precise definition of an Expert System, it is easy to list characteristics which are usually associated with such a system. This paper presents the results of some preliminary research on the implementation of these features by means of some of the structures in Ada, such as Generic Program Units, Packages and Data Encapsulation, which are typical of the language's rich expressive power and which must form the basis of any natural implementation in Ada of an Expert System.One of the features expected in an Expert System is forward and backward chaining, probably implemented with a version of the Rete algorithm. For this reason, this research included writing several small symbolic pattern matching procedures and testing their performance.This research also investigated the use of Ada packages with encapsulated data types to implement a blackboard structure. It was found that a global data structure and appropriate operators could be implemented. As a side result of this work, the frame construct with attached operators and inheritance mechanisms was implemented.The research reported here also addressed the issue of Object Oriented Programming which has become quite popular in LISP programming. Here the match to Ada is quite natural for the latter language itself is also designed to support the Object Oriented Approach. Several LISP textbook examples were translated to the corresponding Ada structure and manipulated with some test programs.Several directions for future research have been generated as a result of this preliminary work. One open question is how to implement concepts originally developed for a language with late binding, such as LISP, in a language with strong typing such as Ada. Another issue concerns the translation of LISP code into Ada code. Early work suggests that it will be more profitable to view the LISP as a high level specification language for an Ada system rather than a program to be translated with literal fidelity. These and several other projects are being pursued and will be reported out in future papers.},
booktitle = {Proceedings of the 15th Annual Conference on Computer Science},
pages = {382},
location = {St. Louis, Missouri, USA},
series = {CSC '87}
}

@inproceedings{10.1145/5600.5902,
author = {Rager, John E.},
title = {Graphics Packages for Teaching Graphics},
year = {1986},
isbn = {0897911784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/5600.5902},
doi = {10.1145/5600.5902},
abstract = {The design and implementation of graphics packages has been widely studied and discussed. The special needs of the teaching environment change the requirements of a package in some interesting ways because the details usually hidden from the user are of interest to the students. Here the design of such a package is considered. In order to identify the needs of the package, the structures of CORE (which is used throughout as an example of an applications-oriented package) are compared with a list of topics covered in an elementary graphics course. Some of the nonessential flexibility of CORE can be thrown away, and procedures needed to handle hierarchy and manipulation of the structured display file can be added. A package (the Northwestern University Simple Graphics Package) resulting from this analysis is described. Specific suggestions are made for pruning CORE to a manageable size. A natural way of accessing the structured display files and a system of symbols are included. The resulting package is small, manageable and useful.During the summer of 1983 I was given the opportunity to teach the first computer graphics course to be offered at the University of Chicago. The only available software was PLOT-10 [11], in a version designed to interface to FORTRAN on the DEC-20. The only prerequisite for the course was an elementary programming course which taught PASCAL, hence the students could not be expected to program in FORTRAN. Some experimentation with the versions of FORTRAN, PASCAL, and PLOT-10 on the DEC-20 revealed that interfacing the PLOT-10 routines to PASCAL wasn't going to work. A second, more important, reason for rejecting this option was the lack of segmentation capability in PLOT-10. I was planning to use Principles of Interactive Computer Graphics by Foley and Van Dam [3] and I wanted the students to be able to program in the spirit of the ACM CORE [4] inspired Simple Graphics Package (SGP) used in the text. My solution was to implement this SGP in PASCAL on the DEC-20, with drivers for two of the terminal types available on campus. During the quarter I taught the course I realized that this was not an adequate solution. There were some things I just couldn't demonstrate for the students and some things that I couldn't put into the programming assignments.For example, a fair amount of time was spent discussing package implementation: clipping algorithms, coordinate transformations, the segment data (visibility, detectability), etc. It was impossible, in the UC SGP, to look at this information as an image was constructed and modified. It was equally impossible for the students to manipulate these constructs directly.I would have had similar problems with a full implementation of CORE and with any other package with which I am familiar. Basically, the packages designed for graphics applications hide the details of the implementation and present a high level view to the user. This is great when implementing an interactive graphics program but it is not so great when trying to teach how the packages work.This should not be taken as a blanket condemnation of the use of packages for computer graphics classes, nor as a suggestion that students should always be reduced to programming at the display processor level. There are good reasons for using packages, or at least package-like systems, in an elementary graphics course. Packages provide the user-friendly environment needed to start the students in computer graphics, and to allow the students to become familiar with graphics software as it exists in the real world. The students will learn the style and format of CORE, GKS [6, 12, 13, 14], PLOT-10, or whatever package is chosen. Last, but perhaps most important to a busy instructor (and is there any other kind?), using a package means writing a minimum of new software.Learning from the experience of writing and using the UC SGP, I modified the package to include some extra capabilities. At this time the package was also moved to a VAX 11/780 at Northwestern University. The design described in this paper includes slightly more than either of these packages. (Throughout the discussion the first package is called UC SGP and the second NU SGP.)The design and implementation of a graphics teaching system depends on the available tools. If there is already a sophisticated package, a little bit of extra software may suffice. I'm going to consider the situation in which a new package will be written expressly for use in teaching. Similar considerations are valid when extending an extant package. There are certain objectives to keep in mind. The package should be small so that it runs efficiently and so that it can be implemented with a reasonable effort. The features which will make it a good learning tool should be emphasized and those which further flexibility but not insight can be de-emphasized. The system should be designed to work with dumb terminals, if those are the ones likely to be available in sufficient quantity for student use. (If there are enough smart terminals the package can do a little more and may be easier to produce.) When designing a small system like this one, it is a good idea to keep the available hardware in mind, retaining sufficient flexibility to include other hardware later. (In the real world the flexibility of the package depends somewhat on the shortness of the time available for the implementation.)I'd like to start with the ACM CORE, a well documented and well known system, and sculpt it to the needs of teaching carving away some excess and adding some new capabilities.},
booktitle = {Proceedings of the Seventeenth SIGCSE Technical Symposium on Computer Science Education},
pages = {225–231},
numpages = {7},
location = {Cincinnati, Ohio, USA},
series = {SIGCSE '86}
}

@article{10.1145/953055.5902,
author = {Rager, John E.},
title = {Graphics Packages for Teaching Graphics},
year = {1986},
issue_date = {February 1986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/953055.5902},
doi = {10.1145/953055.5902},
abstract = {The design and implementation of graphics packages has been widely studied and discussed. The special needs of the teaching environment change the requirements of a package in some interesting ways because the details usually hidden from the user are of interest to the students. Here the design of such a package is considered. In order to identify the needs of the package, the structures of CORE (which is used throughout as an example of an applications-oriented package) are compared with a list of topics covered in an elementary graphics course. Some of the nonessential flexibility of CORE can be thrown away, and procedures needed to handle hierarchy and manipulation of the structured display file can be added. A package (the Northwestern University Simple Graphics Package) resulting from this analysis is described. Specific suggestions are made for pruning CORE to a manageable size. A natural way of accessing the structured display files and a system of symbols are included. The resulting package is small, manageable and useful.During the summer of 1983 I was given the opportunity to teach the first computer graphics course to be offered at the University of Chicago. The only available software was PLOT-10 [11], in a version designed to interface to FORTRAN on the DEC-20. The only prerequisite for the course was an elementary programming course which taught PASCAL, hence the students could not be expected to program in FORTRAN. Some experimentation with the versions of FORTRAN, PASCAL, and PLOT-10 on the DEC-20 revealed that interfacing the PLOT-10 routines to PASCAL wasn't going to work. A second, more important, reason for rejecting this option was the lack of segmentation capability in PLOT-10. I was planning to use Principles of Interactive Computer Graphics by Foley and Van Dam [3] and I wanted the students to be able to program in the spirit of the ACM CORE [4] inspired Simple Graphics Package (SGP) used in the text. My solution was to implement this SGP in PASCAL on the DEC-20, with drivers for two of the terminal types available on campus. During the quarter I taught the course I realized that this was not an adequate solution. There were some things I just couldn't demonstrate for the students and some things that I couldn't put into the programming assignments.For example, a fair amount of time was spent discussing package implementation: clipping algorithms, coordinate transformations, the segment data (visibility, detectability), etc. It was impossible, in the UC SGP, to look at this information as an image was constructed and modified. It was equally impossible for the students to manipulate these constructs directly.I would have had similar problems with a full implementation of CORE and with any other package with which I am familiar. Basically, the packages designed for graphics applications hide the details of the implementation and present a high level view to the user. This is great when implementing an interactive graphics program but it is not so great when trying to teach how the packages work.This should not be taken as a blanket condemnation of the use of packages for computer graphics classes, nor as a suggestion that students should always be reduced to programming at the display processor level. There are good reasons for using packages, or at least package-like systems, in an elementary graphics course. Packages provide the user-friendly environment needed to start the students in computer graphics, and to allow the students to become familiar with graphics software as it exists in the real world. The students will learn the style and format of CORE, GKS [6, 12, 13, 14], PLOT-10, or whatever package is chosen. Last, but perhaps most important to a busy instructor (and is there any other kind?), using a package means writing a minimum of new software.Learning from the experience of writing and using the UC SGP, I modified the package to include some extra capabilities. At this time the package was also moved to a VAX 11/780 at Northwestern University. The design described in this paper includes slightly more than either of these packages. (Throughout the discussion the first package is called UC SGP and the second NU SGP.)The design and implementation of a graphics teaching system depends on the available tools. If there is already a sophisticated package, a little bit of extra software may suffice. I'm going to consider the situation in which a new package will be written expressly for use in teaching. Similar considerations are valid when extending an extant package. There are certain objectives to keep in mind. The package should be small so that it runs efficiently and so that it can be implemented with a reasonable effort. The features which will make it a good learning tool should be emphasized and those which further flexibility but not insight can be de-emphasized. The system should be designed to work with dumb terminals, if those are the ones likely to be available in sufficient quantity for student use. (If there are enough smart terminals the package can do a little more and may be easier to produce.) When designing a small system like this one, it is a good idea to keep the available hardware in mind, retaining sufficient flexibility to include other hardware later. (In the real world the flexibility of the package depends somewhat on the shortness of the time available for the implementation.)I'd like to start with the ACM CORE, a well documented and well known system, and sculpt it to the needs of teaching carving away some excess and adding some new capabilities.},
journal = {SIGCSE Bull.},
month = {feb},
pages = {225–231},
numpages = {7}
}

@inproceedings{10.1145/2929908.2929910,
author = {Afanasiev, Michael and Boehm, Christian and Gokhberg, Alexey and Fichtner, Andreas},
title = {Automatic Global Multiscale Seismic Inversion: Insights into Model, Data, and Workflow Management},
year = {2016},
isbn = {9781450341264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2929908.2929910},
doi = {10.1145/2929908.2929910},
abstract = {Modern global seismic waveform tomography is formulated as a PDE-constrained nonlinear optimization problem, where the optimization variables are Earth's visco-elastic parameters. This particular problem has several defining characteristics. First, the solution to the forward problem, which involves the numerical solution of the elastic wave equation over continental to global scales, is computationally expensive. Second, the determinedness of the inverse problem varies dramatically as a function of data coverage. This is chiefly due to the uneven distribution of earthquake sources and seismometers, which in turn results in an uneven sampling of the parameter space. Third, the seismic wavefield depends nonlinearly on the Earth's structure. Sections of a seismogram which are close in time may be sensitive to structure greatly separated in space.In addition to these theoretical difficulties, the seismic imaging community faces additional issues which are common across HPC applications. These include the storage of massive checkpoint files, the recovery from generic system failures, and the management of complex workflows, among others. While the community has access to solvers which can harness modern heterogeneous computing architectures, the computational bottleneck has fallen to these memory- and manpower-bounded issues.We present a two-tiered solution to the above problems. To deal with the problems relating to computational expense, data coverage, and the increasing nonlinearity of waveform tomography with scale, we present the Collaborative Seismic Earth Model (CSEM). This model, and its associated framework, takes an open-source approach to global-scale seismic inversion. Instead of attempting to monolithically invert all available seismic data, the CSEM approach focuses on the inversion of specific geographic subregions, and then consistently integrates these subregions via a common computational framework. To deal with the workflow and storage issues, we present a suite of workflow management software, along with a custom designed optimization and data compression library. It is the goal of this paper to synthesize these above concepts, originally developed in isolation, into components of an automatic global-scale seismic inversion.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {1},
numpages = {10},
keywords = {seismic tomography, workflow management, full-waveform inversion, compression},
location = {Lausanne, Switzerland},
series = {PASC '16}
}

@inproceedings{10.1145/1858996.1859062,
author = {Woollard, David and Mattmann, Chris A. and Popescu, Daniel and Medvidovic, Nenad},
title = {Kadre: Domain-Specific Architectural Recovery for Scientific Software Systems},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859062},
doi = {10.1145/1858996.1859062},
abstract = {Scientists today conduct new research via software-based experimentation and validation in a host of disciplines. Scientific software represents a significant investment due to its complexity and longevity yet there is little reuse of scientific software beyond small libraries which increases development and maintenance costs. To alleviate this disconnect, we have developed KADRE, a domain-specific architecture recovery approach and toolset to aid automatic and accurate identification of workflow components in existing scientific software. KADRE improves upon state of the art general cluster techniques, helping to promote component-based reuse within the domain.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {325–328},
numpages = {4},
keywords = {workflows, software architecture, scientific computing},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/3311790.3396647,
author = {Cleveland, Sean B. and Jamthe, Anagha and Padhy, Smruti and Stubbs, Joe and Packard, Michale and Looney, Julia and Terry, Steve and Cardone, Richard and Dahan, Maytal and Jacobs, Gwen A.},
title = {Tapis API Development with Python: Best Practices In Scientific REST API Implementation: Experience Implementing a Distributed Stream API},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3396647},
doi = {10.1145/3311790.3396647},
abstract = {In the last decade, the rise of hosted Software-as-a-Service (SaaS) application programming interfaces (APIs) across both academia and industry has exploded, and simultaneously, microservice architectures have replaced monolithic application platforms for the flexibility and maintainability they offer. These SaaS APIs rely on small, independent and reusable microservices that can be assembled relatively easily into more complex applications. As a result, developers can focus on their own unique functionality and surround it with fully functional, distributed processes developed by other specialists, which they access through APIs. The Tapis framework, a NSF funded project, provides SaaS APIs to allow researchers to achieve faster scientific results, by eliminating the need to set up a complex infrastructure stack. In this paper, we describe the best practices followed to create Tapis APIs using Python and the Stream API as an example implementation illustrating authorization and authentication with the Tapis Security Kernel, Tenants and Tokens APIs, leveraging OpenAPI v3 specification for the API definitions and docker containerization. Finally, we discuss our deployment strategy with Kubernetes, which is an emerging orchestration technology and the early adopter use cases of the Streams API service.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {181–187},
numpages = {7},
keywords = {Tapis, Abaco, real-time streaming data, CHORDS, API},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@article{10.1145/3077583,
author = {Cingolani, Davide and Pellegrini, Alessandro and Quaglia, Francesco},
title = {Transparently Mixing Undo Logs and Software Reversibility for State Recovery in Optimistic PDES},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-3301},
url = {https://doi.org/10.1145/3077583},
doi = {10.1145/3077583},
abstract = {The Time Warp synchronization protocol for Parallel Discrete Event Simulation (PDES) is universally considered a viable solution to exploit the intrinsic simulation model parallelism and to provide model execution speedup. Yet it leads the PDES system to execute events in an order that may generate causal inconsistencies that need to be recovered via rollback, which requires restoration of a previous (consistent) simulation state whenever a causality violation is detected. The rollback operation is so critical for the performance of a Time Warp system that it has been extensively studied in the literature for decades to find approaches suitable to optimize it. The proposed solutions can be roughly classified as based on either checkpointing or reverse computing. In this article, we explore the practical design and implementation of a fully new approach based on the runtime generation of so-called undo code blocks, which are blocks of instructions implementing the reverse memory side effects generated by the forward execution of the events. However, this is not done by recomputing the original values to be restored, as instead it occurs in reverse computing schemes. Hence, the philosophy undo code blocks rely on is similar in spirit to that of undo-logs (as a form of checkpointing). Nevertheless, they are not data logs (as instead checkpoints are); rather, they are logs of instructions. Our proposal is fully transparent, thanks to the reliance on static software instrumentation (targeting the x86 architecture and Linux systems). Also, as we show, it can be combined with classical checkpointing to further improve the runtime behavior of the state recoverability support as a function of the workload. We also present experimental results related to our implementation, which is released as free software and fully integrated into the open source ROOT-Sim package. Experimental data support the viability and effectiveness of our proposal.},
journal = {ACM Trans. Model. Comput. Simul.},
month = {may},
articleno = {11},
numpages = {26},
keywords = {Optimistic synchronization, time warp, software reversibility}
}

@inproceedings{10.1145/2769458.2769482,
author = {Cingolani, Davide and Pellegrini, Alessandro and Quaglia, Francesco},
title = {Transparently Mixing Undo Logs and Software Reversibility for State Recovery in Optimistic PDES},
year = {2015},
isbn = {9781450335836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2769458.2769482},
doi = {10.1145/2769458.2769482},
abstract = {The rollback operation is a fundamental building block to support the correct execution of a speculative Time Warp-based Parallel Discrete Event Simulation. In the literature, several solutions to reduce the execution cost of this operation have been proposed, either based on the creation of a checkpoint of previous simulation state images, or on the execution of negative copies of simulation events which are able to undo the updates on the state. In this paper, we explore the practical design and implementation of a state recoverability technique which allows to restore a previous simulation state either relying on checkpointing or on the reverse execution of the state updates occurred while processing events in forward mode. Differently from other proposals, we address the issue of executing backward updates in a fully-transparent and event granularity-independent way, by relying on static software instrumentation (targeting the x86 architecture and Linux systems) to generate at runtime reverse update code blocks (not to be confused with reverse events, proper of the reverse computing approach). These are able to undo the effects of a forward execution while minimizing the cost of the undo operation. We also present experimental results related to our implementation, which is released as free software and fully integrated into the open source ROOT-Sim (ROme OpTimistic Simulator) package. The experimental data support the viability and effectiveness of our proposal.},
booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {211–222},
numpages = {12},
keywords = {reversibility, pdes, code instrumentation, speculative processing},
location = {London, United Kingdom},
series = {SIGSIM PADS '15}
}

@article{10.1145/264934.264939,
author = {Welch, Lonnie R.},
title = {COCOON: Creator of Concurrent Object-Oriented Systems},
year = {1997},
issue_date = {Nov./Dec. 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {XVII},
number = {6},
issn = {1094-3641},
url = {https://doi.org/10.1145/264934.264939},
doi = {10.1145/264934.264939},
abstract = {The software engineering techniques of reuse, encapsulation and information hiding are increasingly being employed in the construction of large, concurrent software systems. Simultaneously, hardware platforms continue to increase in sophistication, particularly in the area of concurrency. Thus, systems engineers are confronted with the problem of bridging the gap between (1) large computer-based software systems composed of tasks and ADTs/objects and (2) parallel and distributed computer platforms. Thus, this paper describes the COCOON (Creator Of Concurrent Object-OrieNted Systems) toolset. COCOON bridges the gap between object-oriented Ada software and distributed architectures. It partitions tasks, packages and subprograms, and assigns partitions to physical processors. The toolset automatically produces a distribution specification.},
journal = {Ada Lett.},
month = {nov},
pages = {32–38},
numpages = {7}
}

@inproceedings{10.1007/11424529_5,
author = {Sharma, Vibhu Saujanya and Jalote, Pankaj and Trivedi, Kishor S.},
title = {Evaluating Performance Attributes of Layered Software Architecture},
year = {2005},
isbn = {3540258779},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11424529_5},
doi = {10.1007/11424529_5},
abstract = {The architecture of a software system is the highest level of abstraction whereupon useful analysis of system properties is possible. Hence, performance analysis at this level can be useful for assessing whether a proposed architecture can meet the desired performance specifications and can help in making key architectural decisions. In this paper we propose an approach for performance evaluation of software systems following the layered architecture, which is a common architectural style for building software systems. Our approach initially models the system as a Discrete Time Markov Chain, and extracts parameters for constructing a closed Product Form Queueing Network model that is solved using the SHARPE software package. Our approach predicts the throughput and the average response time of the system under varying workloads and also identifies bottlenecks in the system, suggesting possibilities for their removal.},
booktitle = {Proceedings of the 8th International Conference on Component-Based Software Engineering},
pages = {66–81},
numpages = {16},
location = {St. Louis, MO},
series = {CBSE'05}
}

@inproceedings{10.1145/3368089.3409688,
author = {Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John and Abdelrazek, Mohamed},
title = {Beware the Evolving ‘Intelligent’ Web Service! An Integration Architecture Tactic to Guard AI-First Components},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409688},
doi = {10.1145/3368089.3409688},
abstract = {Intelligent services provide the power of AI to developers via simple RESTful API endpoints, abstracting away many complexities of machine learning. However, most of these intelligent services---such as computer vision---continually learn with time. When the internals within the abstracted 'black box' become hidden and evolve, pitfalls emerge in the robustness of applications that depend on these evolving services. Without adapting the way developers plan and construct projects reliant on intelligent services, significant gaps and risks result in both project planning and development. Therefore, how can software engineers best mitigate software evolution risk moving forward, thereby ensuring that their own applications maintain quality? Our proposal is an architectural tactic designed to improve intelligent service-dependent software robustness. The tactic involves creating an application-specific benchmark dataset baselined against an intelligent service, enabling evolutionary behaviour changes to be mitigated. A technical evaluation of our implementation of this architecture demonstrates how the tactic can identify 1,054 cases of substantial confidence evolution and 2,461 cases of substantial changes to response label sets using a dataset consisting of 331 images that evolve when sent to a service.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–280},
numpages = {12},
keywords = {software evolution, software architecture, intelligent web services},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1109/PADS.2005.34,
author = {Santoro, Andrea and Quaglia, Francesco},
title = {Transparent State Management for Optimistic Synchronization in the High Level Architecture},
year = {2005},
isbn = {0769523838},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PADS.2005.34},
doi = {10.1109/PADS.2005.34},
abstract = {In this paper we present the design and implementation of a software architecture, namely MAgic State Manager (MASM), to be employed within a Run-Time Infrastructure (RTI) in support of HLA federations. MASM allows performing checkpointing/recovery of the state of a federate in a way completely transparent to the federate itself, thus providing the possibility of demanding to the RTI any task related to state management in optimistic synchronization. Differently from existing proposals, through our approach the federate programmer is neither required to supply modules for state management within the federate code, nor to explicitly interface the federate code with existing, third party checkpointing/recovery libraries. Hence, the federate programmer is completely relieved from the burden of facing state management issues. Some experimental results demonstrating minimal run-time overhead introduced by MASM are also reported for two case studies, namely an interconnection network simulation and a personal communication system simulation.},
booktitle = {Proceedings of the 19th Workshop on Principles of Advanced and Distributed Simulation},
pages = {171–180},
numpages = {10},
series = {PADS '05}
}

@inproceedings{10.1145/2602576.2602581,
author = {Li, Zengyang and Liang, Peng and Avgeriou, Paris and Guelfi, Nicolas and Ampatzoglou, Apostolos},
title = {An Empirical Investigation of Modularity Metrics for Indicating Architectural Technical Debt},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602581},
doi = {10.1145/2602576.2602581},
abstract = {Architectural technical debt (ATD) is incurred by design decisions that consciously or unconsciously compromise system-wide quality attributes, particularly maintainability and evolvability. ATD needs to be identified and measured, so that it can be monitored and eventually repaid, when appropriate. In practice, ATD is difficult to identify and measure, since ATD does not yield observable behaviors to end users. One indicator of ATD, is the average number of modified components per commit (ANMCC): a higher ANMCC indicates more ATD in a software system. However, it is difficult and sometimes impossible to calculate ANMCC, because the data (i.e., the log of commits) are not always available. In this work, we propose to use software modularity metrics, which can be directly calculated based on source code, as a substitute of ANMCC to indicate ATD. We validate the correlation between ANMCC and modularity metrics through a holistic multiple case study on thirteen open source software projects. The results of this study suggest that two modularity metrics, namely Index of Package Changing Impact (IPCI) and Index of Package Goal Focus (IPGF), have significant correlation with ANMCC, and therefore can be used as alternative ATD indicators.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {119–128},
numpages = {10},
keywords = {modularity metric, architectural technical debt, software architecture, commit},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@inproceedings{10.1145/3126908.3126939,
author = {Tillet, Philippe and Cox, David},
title = {Input-Aware Auto-Tuning of Compute-Bound HPC Kernels},
year = {2017},
isbn = {9781450351140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126908.3126939},
doi = {10.1145/3126908.3126939},
abstract = {Efficient implementations of HPC applications for parallel architectures generally rely on external software packages (e.g., BLAS, LAPACK, CUDNN). While these libraries provide highly optimized routines for certain characteristics of inputs (e.g., square matrices), they generally do not retain optimal performance across the wide range of problems encountered in practice. In this paper, we present an input-aware auto-tuning framework for matrix multiplications and convolutions, ISAAC, which uses predictive modeling techniques to drive highly parameterized PTX code templates towards not only hardware-, but also application-specific kernels. Numerical experiments on the NVIDIA Maxwell and Pascal architectures show up to 3x performance gains over both cuBLAS and cuDNN after only a few hours of auto-tuning.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {43},
numpages = {12},
location = {Denver, Colorado},
series = {SC '17}
}

@inproceedings{10.1145/3077136.3080721,
author = {Yang, Peilin and Fang, Hui and Lin, Jimmy},
title = {Anserini: Enabling the Use of Lucene for Information Retrieval Research},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080721},
doi = {10.1145/3077136.3080721},
abstract = {Software toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrieval models over standard test collections. Efforts are generally directed toward better ranking and less attention is usually given to scalability and other operational considerations. On the other hand, Lucene has become the de facto platform in industry for building search applications (outside a small number of companies that deploy custom infrastructure). Compared to academic IR toolkits, Lucene can handle heterogeneous web collections at scale, but lacks systematic support for evaluation over standard test collections. This paper introduces Anserini, a new information retrieval toolkit that aims to provide the best of both worlds, to better align information retrieval practice and research. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial efforts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web-scale collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for many TREC test collections, providing a convenient way to replicate competitive baselines right out of the box. Experiments verify that our system is both efficient and effective, providing a solid foundation to support future research.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1253–1256},
numpages = {4},
keywords = {reproducibility, open-source toolkits, multi-threaded inverted indexing, trec test collections},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@article{10.1145/77556.77557,
author = {Frenkel, Karen A.},
title = {The European Community and Information Technology},
year = {1990},
issue_date = {April 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/77556.77557},
doi = {10.1145/77556.77557},
abstract = {The world has watched Eastern Europe erupt into such political turmoil that historians are expected to call this period the Revolutions of 1989. Economic evolution was also underway as the Continent progressed toward a single European market. The goal—a market without national borders or barriers to the movement of goods, services, capital and people—was first outlined over 30 years ago by the 12 countries which became members of the Common Market. In the mid 1980s, the effort was renewed when these same countries approved an ambitious plan outlining hundreds of legislative directives and policies that would harmonize and re-regulate those of the member states. The measures are drafted by the European Commission, voted on by the Council of Ministers, amended if necessary, and then assigned budgets by the Parliament. They include competition law, labor law, product regulation and standardization, taxation and subsidies, and quota and tariff guidelines. In 1987, the Single European Act created a timetable for the passage of legislation with a formal deadline for the removal of barriers by December 31, 1992, hence the term Europe '92 (EC '92). But many have described EC '92 as a process that will continue throughout the 1990s. The ouster of communist leaderships throughout Eastern Europe, however, has raised unexpected questions about the participation of the Eastern countries, and this could alter or delay the process.Nevertheless, the changes have begun and are taking place during the Information Revolution. It is therefore natural to ask what impact EC '92 will have on the computer industry. Inevitably, several of the directives and policies relate primarily, and many secondarily, to information technology. Table 2 lists the policies in effect and those being proposed. In the following pages, Communications presents several points of view regarding the impact of EC '92 on the information technology market in Europe.As of July 1988, the European information systems market was estimated at $90 billion by Datamation magazine and is expected by many to be the fastest growing market this decade. But during the last ten years, European-based computer companies have had difficulty keeping pace with American and Japanese firms. In 1988, European companies managed only a 20 percent market share on their own turf, according to market researcher International Data Corporation. Not much had changed since 1982 when their market share was 21 percent. As reported in the Wall Street Journal last January, European computer companies have been hindered by lack of economies of scale, narrow focus on national markets, and difficulty in keeping pace with Japanese and IJ.S. product innovations. But the occasion for the Journal article was the news that Germany's Siemens AG was merging with the ailing Nixdorf Computer AG. The result would possibly be the largest computer company based in Europe, and the sixth or seventh largest in the world. And in October of 1989, France's Groupe Bull announced the purchase of Zenith Electronics Corporation's personal computer unit. Bull claimed that it would become the sixth largest information service company in the world. Such restructurings have been predicted with the approach of EC '92, as corporate strategies would begin to take into account directives and trade rules regarding the computer and telecommunications industries. Smaller European and American computer companies are anticipating battle with giants like IBM and DEC, which have long-established European divisions or subsidiaries. IBM has been the leader in mainframes, minicomputers, and personal computers, but it is expected that all computer companies, European-based or not, will face greater competition in Europe.The Netherlands' NV Philips, the largest European semiconductor and consumer electronics company, says it has been preparing for EC '92 since the 1970s. And North American Philips Chairman Gerrit Jeelof has claimed company credit for initiating the 1987 European Act. In a speech delivered at a Business Week and Foreign Policy Association Seminar last May, Jeelof said that while American companies had forsaken consumer electronics, Philips and France's Thompson have held their own against the Japanese. But he indicated that American dominance of the European semiconductor market was a major impetus for EC '92. Jeelof said: . . . because of the lack of European strength in the field of computers, the integrated circuits business in Europe is dominated by Americans. Europe consumes about 34 percent of all ICs in the world and only 18 percent are made in Europe by European companies. The rest are made by American companies or are imported. It is not a surprise then that in 1984 we at Philips took the initiative to stimulate a more unified European market. At the time, we called it Europe 1990. Brussels thought that 1990 was a bit too early and made it 1992. But it has been the electronics industry in Europe together with other major companies, that have been pushing for Europe 1992. Why did we want it? We wanted a more homogeneous total market in Europe and, based on that, we wanted to become more competitive. The process is on its way and obviously we see some reactions. If you take action, you get reaction.One reaction has been concern on the part of non-European companies and their governments that the EC is creating a protectionist environment, a “Fortress Europe.” As walls between nations are coming down, some fear that other more impenetrable ones are going up on the Continent's edges. Jeelof argues against this perception in another speech, “Europe 1992—Fraternity or Fortress,” reprinted in this issue in its entirety.Communications also presents an analysis of several trade rules relating to semi-conductors in “The Semiconductor Market in the European Community: Implications of Recent Rules and Regulations,” by Roger Chiarodo and Judee Mussehl, both analysts in the Department of Commerce Office of Microelectronics and Instruments. The authors outline the consequences of Europe's Rules of Origin, anti-dumping measures that are supposed to prevent companies from using assembly operations in an importing country to circumvent duty on imported products. In the United States, if the difference between the value of parts or components from the dumping country and the value of the final product is small, then duty will be placed on those parts or components used in U.S. assembly operations. By contrast, the EC rule says that if the value of parts or components exceeds 60 percent of the value of all parts and materials, then duty will be placed on those parts and materials upon assembly in Europe. Since 1968, origin was also determined according to “the last substantial process or operation” resulting in the manufacture of a new product. In the case of printed circuit boards, some countries interpreted this as assembly and testing, while others thought it meant diffusion. In 1982, the EC began harmonizing these interpretations, and as of 1989, the last substantial operation was considered diffusion: the selective introduction of chemical dopants on a semiconductor substrate. As a result, American and Japanese semi-conductor manufacturers have spent millions building foundries on European soil. To reveal the Japanese interpretation of such changes, Japanese Commerce Minister Eiichi Ono, with the Japanese Embassy in Washington, DC, expresses his country's impressions of EC '92 in this issue. In his speech, “Japan's View of EC '92,” delivered at an Armed Forces Communications and Electronics Association (AFCEA) conference on Europe '92, Ono states that while the EC's intentions might not be protectionist, they could become so upon implementation. His discussion focuses on semi-conductors and technology transfer issues.Although not a formal directive, in July 1988, the European Council decided to promote an internal information services market (the last “L” document in Table 2). To present the reasoning and objectives behind this initiative, we reprint the Communication from the Commission to the Council of Ministers, “The Establishment at Community Level of a Policy and a Plan of Priority Actions for the Development of an Information Services Market,” and the resulting July 1988 “Council Decision” itself. Funds allocated for 1989 and 1990 are approximately $36 million, $23 million of which was slated for a pilot/demonstration program called IMPACT, for Information Market Policy Actions. This may seem a pittance in comparison to the programs of other governments, but this Decision and other EC legislation are the first steps toward an EC industrial policy. Recognizing that Europe's non-profit organizations and the public sector play a very important role in providing database services, in contrast to the U.S. where the private sector is now seeding the production of such database services, IMPACT has prepared guidelines to help the public sector cooperate with the private sector in marketing information. These guidelines would also allow private information providers to use public data and add value to it to create commercial products. IMPACT is providing incentives to accelerate innovative services for users by paying 25 percent of a project's cost. After the first call for proposals, 16 of 167 projects proposed by teams composed of 300 organizations were funded. American-based companies can apply for funds if they are registered in Europe. Unlike the U.S., the EC allows registration regardless of who owns a company's capital. Projects funded are to develop databases that would be accessible to all members of the Community either on CD-ROM or eventually on a digital network, an ISDN for all Europe, as planned by the fifth recommendation listed in Table 2. One project in the works is a library of pharmaceutical patents on CD-ROM that will enable users to locate digitized documents. Users will also have direct access to on-line hosts for all kinds of patents. A tourism information database and a multi-media image bank of atlases are other pilot projects chosen, and another project will provide information on standards. Eventually, audiotext might be used to retrieve data by telephone instead of a computer terminal. When the initial projects have been completed, the Commission will inform the market place about the results of the implementation. Plans for a five-year follow-up program, IMPACT-2 are also under discussion.These projects depend to some extent on the implementation and passage of directives or the success of larger and better funded projects. On-line access to databases depends on the recommendation for an ISDN as well as on the standardization directive for information technology and telecommunications. The certification, quality assurance, and conformity assessment issues involved in that directive are too numerous and important to just touch on here and will be covered in a later issue of Communications. To make these databases accessible not only technically, but also linguistically, the EC has funded two automatic language translation projects called Systran and Eurotra. Systran is also the name of the American company in La Jolla, CA, known for its pioneering work in translation. In conjunction with the EC, Systran Translation Systems, Inc., has completed a translation system for 24 language pairs (English—French, French—English, for example, are two language pairs) for the translation of IMPACT- funded databases. The system resides on an EC mainframe; there will be on-line access by subscription; and it will also be available on IBM PS/2s modified to run VMS DOS. It is already on France's widespread Minitel videotext network.As this practical, market-oriented approach to technology implementation is beginning, Europe's cooperative research effort, ESPRIT, is also starting to transfer its results. Last year, the second phase, ESPRIT II, set up a special office for technology transfer. Its mission is to ensure the exploitation, for the benefit of European industry, of the fruits of the $1.5 billion ESPRIT I program that began in 1984, as well as the current $3.2 billion program (funding through 1992). The EC contributes half of the total cost, which is matched by consortia comprised of university and industry researchers from more than one country. About 40 percent of ESPRIT II's funds will be devoted to computer related-technologies.Every November, ESPRIT holds a week-long conference. Last year for the first time it devoted a day to technology transfer. Several successful technology transfers have occurred either from one member of the program to another or out of the program to a member of industry that had not participated in the research. An electronic scanner that detects and eradicates faults on chips, for example, was developed by a consortium and the patents licensed by a small company. This automatic design validation scanner was co-developed by CSELT, Italy, British Telecom, CNET, another telecom company in France, IMAG, France, and Trinity College, Dublin. The company that will bring it to market is ICT, Gmbh, a relatively small German company. It seems that in Europe, as in the United States, small companies and spin-offs like those found in the Silicon Valley here, are better at running quickly with innovative ideas, says an EC administrator.Another technology transfer success is the Supernode computer. This hardware and software parallel processing project resulted in an unexpected product from transputer research. The Royal Signal Radar Establishment, Inmos, Telmat, and Thorn EMI, all of the UK, APTOR of France, and South Hampton University and the University of Grenoble, all participated in the research and now Inmos has put the product on the market.Three companies and two universities participated in developing the Dragon Project (for Distribution and Reusability of ADA Real-time Applications through Graceful On-line Operations). This was an effort to provide effective support for software reuse in real-time for distributed and dynamically reconfigurable systems. The researchers say they have resolved the problems of distribution in real-time performance and are developing a library and classification scheme now. One of the companies, TXT, in Milan, will bring it to market.Several other software projects are also ready for market. One is Meteor, which is aimed at integrating a formal approach to industrial software development, particularly in telecommunications. The participants have defined several languages, called ASF, COLD, ERAE, PLUSS, and PSF for requirements engineering and algebraic methods. Another project is QUICK, the design and experimentation of a knowledge-based system development tool kit for real-time process control applications. The tool kit consists of a general system architecture, a set of building modules, support tools for construction, and knowledge-based system analysis of design methodology. The tool kit will also contain a rule-based component based on fuzzy logic. During the next two years, more attention and funds will be indirectly devoted to technology transfer, and the intention to transfer is also likely to be one of the guides in evaluating project proposals.Some industry experts maintain that high technology and the flow of information made the upheaval in Eastern Europe inevitable. Leonard R. Sussman, author of Power, the Press, and the Technology of Freedom: The Coming Age of ISDN (Freedom House, 1990), predicted that technology and globally linked networks would result in the breakdown of censorious and suppressive political systems. He says the massive underground information flow due to books, copiers, software, hardware, and fax machines, in Poland for example, indicates that technology can mobilize society. Knowing that computers are essential to an industrial society, he says, Gorbachev faced a dilemma as decentralized computers loosened the government's control over the people running them. Glasnost evolved out of that dilemma, says Sussman.Last fall, a general draft trade and economic cooperation accord was signed by the European Commission and the Soviet Union. And both American and Western European business interests are calling for the Coordinating Committee on Multilateral Export Controls (COCOM) to relax high technology export rules to the Eastern Bloc and the Soviet Union. The passage of that proposal could allow huge computer and telecommunications markets to open up. And perhaps the Revolutions of 1989 will reveal themselves to have been revolutions in communication and the flow of information due in part to high technology and the hunger for it.},
journal = {Commun. ACM},
month = {apr},
pages = {404–410},
numpages = {7},
keywords = {transborder data flow, regulation, standards, statistics}
}

@inproceedings{10.1145/1275165.1275167,
author = {Zhang, Yinong and Adams, George B.},
title = {An Interactive, Visual Simulator for the DLX Pipeline},
year = {1997},
isbn = {9781450347396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1275165.1275167},
doi = {10.1145/1275165.1275167},
abstract = {We have built an interactive, visual pipeline simulator, called dlxview, for the DLX instruction set and pipeline described in Computer Architecture A Quantitative Approach by Hennessy and Patterson [1]. This software provides animated versions of key figures and tables from the text and allows the user to readily follow details of pipeline activity as a code is simulated, to vary pipeline implementation, and to compare performance across different pipeline designs.The software package requires a system running Unix and X11, with Tcl/Tk installed, and using the GNU gcc compiler is recommended. A 256 color display with 1024x768 pixels is best for display, due to the detailed diagrams of the DLX pipeline. The software has been designed to run on a variety of platforms and has been tested on Solaris 2.3, SunOS 4.1.1, HP-UX 9.0, DEC OSF/1 4.0, and Linux kernel 1.2.1. DLXview is available at http://yara.ecn.purdue.edu/~teamaaa/dlxview/},
booktitle = {Proceedings of the 1997 Workshop on Computer Architecture Education},
pages = {2–es},
series = {WCAE-3 '97}
}

@inproceedings{10.1145/1985441.1985468,
author = {Davies, Julius and German, Daniel M. and Godfrey, Michael W. and Hindle, Abram},
title = {Software Bertillonage: Finding the Provenance of an Entity},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985468},
doi = {10.1145/1985441.1985468},
abstract = {Deployed software systems are typically composed of many pieces, not all of which may have been created by the main development team. Often, the provenance of included components -- such as external libraries or cloned source code -- is not clearly stated, and this uncertainty can introduce technical and ethical concerns that make it difficult for system owners and other stakeholders to manage their software assets. In this work, we motivate the need for the recovery of the provenance of software entities by a broad set of techniques that could include signature matching, source code fact extraction, software clone detection, call flow graph matching, string matching, historical analyses, and other techniques. We liken our provenance goals to that of Bertillonage, a simple and approximate forensic analysis technique based on bio-metrics that was developed in 19th century France before the advent of fingerprints. As an example, we have developed a fast, simple, and approximate technique called anchored signature matching for identifying library version information within a given Java application. This technique involves a type of structured signature matching performed against a database of candidates drawn from the Maven2 repository, a 150GB collection of open source Java libraries. An exploratory case study using a proprietary e-commerce Java application illustrates that the approach is both feasible and effective.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {183–192},
numpages = {10},
keywords = {provenance, code evolution, code fingerprints, bertillonage},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/255471.255529,
author = {Cohen, Sholom},
title = {Process and Products for Software Reuse in Ada},
year = {1990},
isbn = {0897914090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/255471.255529},
doi = {10.1145/255471.255529},
abstract = {The large scale application of reuse to support software development is not a new concept. Over twenty years ago, M. D. McIlroy expressed the need for: “… standard catalogues of routines, classified by precision, robustness, time-space performance, size limits, and binding time of parameters.” [McIlroy 68] He also provided insight that is still valid into: “… the kinds of variability necessary in software components, ways of producing useful inventories, types of components that are ripe for such standardization, and methods of instituting pilot production.”McIlroy emphasized the importance of abstraction of common principles in developing these routines, the need for automatic generators, and uniformity of engineering. He also enumerated the degree of parameterization needed for successful reusable software: Choice of precisionChoice of robustness, trading off reliability and time-space performanceChoice of time-space behaviorChoice of algorithmChoice of interfaces with standards for types of error conditionsChoice of storage accessing methodChoice of data structures within a given algorithm or within different related algorithmsIn the utility and tool area, these concerns have been met with stable, understandable and supported reusable software subsystems to support applications development. Examples of this software are collections of abstract data structures, data base management systems, development environments, and network managers. Requirements for new applications that depend on reuse of such general subsystems software are specified in light of the existence of that software. The development can achieve significant productivity gains when the software subsystems are reused.Except in reusing these subsystems, reusable software practice has not had the same level of success in specific application areas, with few exceptions. For domains such as command and control, communications, avionics, and shipboard systems, reusable software must address specific application requirements particular to the given domain. The development of a reusable air traffic control architecture by Thomson-CSF is an example of reuse that addresses the requirements of a specific application. [Andribet 90] The details of the architectures and components developed by Thomson-CSF remain within the company, and may have limited applicability for other organizations that develop similar systems but use alternate design strategies. The Common Ada Missile Packages (CAMP) project has also produced reusable software components, primarily for missile operational software, but this software has seen only limited application. [McNicholl 88]},
booktitle = {Proceedings of the Conference on TRI-ADA '90},
pages = {227–239},
numpages = {13},
location = {Baltimore, Maryland, USA},
series = {TRI-Ada '90}
}

@article{10.5555/1248659.1248665,
author = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen, Juha},
title = {Building Blocks for Variational Bayesian Learning of Latent Variable Models},
year = {2007},
issue_date = {5/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method.},
journal = {J. Mach. Learn. Res.},
month = {may},
pages = {155–201},
numpages = {47}
}

@article{10.5555/1314498.1314504,
author = {Raiko, Tapani and Valpola, Harri and Harva, Markus and Karhunen, Juha},
title = {Building Blocks for Variational Bayesian Learning of Latent Variable Models},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to fit together and to yield efficient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a specific model structure has been fixed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method.},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {155–201},
numpages = {47}
}

@article{10.1145/71317.71325,
author = {Cornett, F.},
title = {The UT1000 Microprogramming Simulator: An Educational Tool},
year = {1989},
issue_date = {June 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {0163-5964},
url = {https://doi.org/10.1145/71317.71325},
doi = {10.1145/71317.71325},
abstract = {The concepts of microprogramming and firmware are frequently difficult to explain to computer science students. The subject of firmware is normally introduced in a beginning course of the computer science curriculum, usually as one of many terms for which a definition is memorized, but rarely understood. Even in advanced courses in computer organization or architecture, the concept may remain somewhat abstract to the student. Just as the practice of writing and executing programs in high level languages develops an increased understanding of those languages, writing and executing microprograms aids the student in understanding this concept. This paper describes the UT1000 microprogramming simulator, a software emulator for writing and executing microprograms on a simulated 16 bit CPU built using AMD2901 ALU/register slices and an AMD2910 microprogramming sequencer. The simulator package described here executes on the IBM PC/AT and PS/2 models of microcomputers. In addition to learning microprogramming concepts, the student is exposed to the principles of bit slice architecture, as well.},
journal = {SIGARCH Comput. Archit. News},
month = {jun},
pages = {111–118},
numpages = {8}
}

@inproceedings{10.1145/100348.100427,
author = {Summers, Marguerite and McGregor, John D.},
title = {Object-Oriented Database for Intelligent Engineering Applications (Abstract)},
year = {1990},
isbn = {0897913485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/100348.100427},
doi = {10.1145/100348.100427},
abstract = {Intelligent engineering systems are often developed utilizing the object-oriented paradigm. These applications of the object-oriented paradigm require the persistent storage of instance objects. One solution to this storage problem is the development of object-oriented databases.The development of large software systems requires the coordination of many of different types of documents. Specifications, designs, documentation, and code segments must all be available for retrieval by multiple users. Many versions of each code segment may be maintained.The development environment should handle all the data on the system under development in a uniform manner. Code, design diagrams, and screen images should all be available through a uniform interface. The flexibility of the object-oriented database permits the construction of data types to represent these diverse kinds of informationThe intelligent system can interact with the database to assemble versions of a software system for a given hardware configuration or for varying capacities of the underlying data structures. The intelligent environment would generate queries against a database of complex and varying objects.Object-oriented database technology has several implications for intelligent engineering applications:Management of Complexity - The object-oriented paradigm packages complexity and relieves the outside world of details of its structure and functionality.Accurate Modeling - The rich variety of data representations possible in the object-oriented environment provides the modeler with sufficient power to model complex data structures and their associated functionality.Data Integrity - Many engineering applications have been too complex to allow for database support. The object-oriented paradigm encapsulates all of the data for one event into one integral representation},
booktitle = {Proceedings of the 1990 ACM Annual Conference on Cooperation},
pages = {413},
location = {Washington, D.C., USA},
series = {CSC '90}
}

@inproceedings{10.1145/3332167.3357121,
author = {Ghosal, Radhika and Rana, Bhavika and Kapur, Ishan and Parnami, Aman},
title = {Rapid Prototyping of Pneumatically Actuated Inflatable Structures},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357121},
doi = {10.1145/3332167.3357121},
abstract = {Fabricating and actuating inflatables for shape-changing interfaces and soft robotics is challenging and time-consuming, requiring knowledge in diverse domains such as pneumatics, manufacturing processes for elastomers, and embedded systems. We propose in this poster a scheme for rapid prototyping and pneumatically actuating piecewise multi-chambered inflatables, using balloons as our building blocks. We provide a construction kit containing pneumatic control boards, pneumatic components, and balloons for constructing simple actuated balloon models. We also provide various primitives of actuation and locomotion to help the user put together their desired actuator, along with an Android app and software API for controlling it via Bluetooth. Finally, we demonstrate the construction and actuation of these inflatable structures using three sample applications.},
booktitle = {Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {78–80},
numpages = {3},
keywords = {inflatables, pneumatic system, soft robotics, soft actuator, shape changing interfaces},
location = {New Orleans, LA, USA},
series = {UIST '19 Adjunct}
}

@inproceedings{10.1145/3184558.3191656,
author = {Vu, Henry and Fertig, Tobias and Braun, Peter},
title = {Verification of Hypermedia Characteristic of RESTful Finite-State Machines},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191656},
doi = {10.1145/3184558.3191656},
abstract = {Being an architectural style rather than a specification or a standard, the proper design of REpresentational State Transfer (REST) APIs is not trivial, since developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature and especially, hypermedia testing is not mentioned at all. To deal with this state of affairs, we have elaborated a Model-Driven Software Development (MDSD) approach for creating RESTful APIs. As this project matured, we also explored the possibility of Model-Driven Testing (MDT). This work addresses the challenges of hypermedia testing and proposes approaches to overcome them with MDT techniques. We present the results of hypermedia testing for RESTful APIs using a model verification approach that were discovered within our research. MDT enables the verification of the underlying model of a RESTful API and ensuring its correctness before initiating any code generation. Therefore, we can prevent a poorly designed model from being transformed into a poorly designed RESTful API.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1881–1886},
numpages = {6},
keywords = {hypermedia testing, RESTful systems, hypermedia, MDT, MDSD, RESTful applications, REST},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1109/ICSE.2017.10,
author = {Gopalakrishnan, Raghuram and Sharma, Palak and Mirakhorli, Mehdi and Galster, Matthias},
title = {Can Latent Topics in Source Code Predict Missing Architectural Tactics?},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.10},
doi = {10.1109/ICSE.2017.10},
abstract = {Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the systems quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {15–26},
numpages = {12},
keywords = {emergent design, architectural design and implementation, tactic recommender},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/1723112.1723127,
author = {Lin, Mingjie and Lebedev, Ilia and Wawrzynek, John},
title = {High-Throughput Bayesian Computing Machine with Reconfigurable Hardware},
year = {2010},
isbn = {9781605589114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1723112.1723127},
doi = {10.1145/1723112.1723127},
abstract = {We use reconfigurable hardware to construct a high throughput Bayesian computing machine (BCM) capable of evalu- ating probabilistic networks with arbitrary DAG (directed acyclic graph) topology. Our BCM achieves high throughput by exploiting the FPGA's distributed memories and abundant hardware structures (such as long carry-chains and registers), which enables us to 1) develop an innovative memory allocation scheme based on a maximal matching algorithm that completely avoids memory stalls, 2) optimize and deeply pipeline the logic design of each processing node, and 3) optimally schedule them. The BCM architecture we present not only can be applied to many important algorithms in artificial intelligence, signal processing, and digital communications, but also has high reusability, i.e., a new application needs not change a BCM's hardware design, only new task graph processing and code compilation are necessary. Moreover, the throughput of a BCM scales almost linearly with the size of the FPGA on which it is implemented.A prototype of a Bayesian computing machine with 16 processing nodes was implemented with a Virtex-5 FPGA (XCV5LX155T-2) on a BEE3 (Berkeley Emulation Engine) platform. For a wide variety of sample Bayesian problems, comparing running the same network evaluation algorithm on a 2.4 GHz Core 2 Duo Intel processor and a GeForce 9400m using the CUDA software package, the BCM demonstrates 80x and 15x speedups respectively, with a peak throughput of 20.4 GFLOPS (Giga Floating-Point Operations per Second).},
booktitle = {Proceedings of the 18th Annual ACM/SIGDA International Symposium on Field Programmable Gate Arrays},
pages = {73–82},
numpages = {10},
keywords = {bayesian computing, reconfigurable hardware},
location = {Monterey, California, USA},
series = {FPGA '10}
}

@inproceedings{10.5555/564124.564160,
author = {Kilgore, Richard A.},
title = {Silk and Taylor ED: Open-Source SML and Silk for Java-Based, Object-Oriented Simulation},
year = {2001},
isbn = {078037309X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Silk® and SML are software libraries of Java, C++, C# and VB.Net classes that support object-oriented, discrete-event simulation. SML™ is a new open-source or "free" software library of simulation classes that enable multi-language development of complex, yet manageable simulations through the construction of usable and reusable simulation objects. These objects are usable because they express the behavior of individual entity-threads from the system object perspective using familiar process-oriented modeling within an object-oriented design supported by a general purpose programming language. These objects are reusable because they can be easily archived, edited and assembled using professional development environments that support multi-language, cross-platform execution and a common component architecture. This introduction supports the tutorial session that describes the fundamentals of designing and creating an SML or Silk model.},
booktitle = {Proceedings of the 33nd Conference on Winter Simulation},
pages = {262–268},
numpages = {7},
location = {Arlington, Virginia},
series = {WSC '01}
}

@inproceedings{10.1145/3338906.3338943,
author = {Fucci, Davide and Mollaalizadehbahnemiri, Alireza and Maalej, Walid},
title = {On Using Machine Learning to Identify Knowledge in API Reference Documentation},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338943},
doi = {10.1145/3338906.3338943},
abstract = {Using API reference documentation like JavaDoc is an integral part of software development. Previous research introduced a grounded taxonomy that organizes API documentation knowledge in 12 types, including knowledge about the Functionality, Structure, and Quality of an API. We study how well modern text classification approaches can automatically identify documentation containing specific knowledge types. We compared conventional machine learning (k-NN and SVM) with deep learning approaches trained on manually-annotated Java and .NET API documentation (n = 5,574). When classifying the knowledge types individually (i.e., multiple binary classifiers) the best AUPRC was up to 87},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {109–119},
numpages = {11},
keywords = {information needs, API documentation, machine learning},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.3115/1119018.1119064,
author = {Ogden, William C.},
title = {TUIT: A Toolkit for Constructing Multilingual TIPSTER User Interfaces},
year = {1996},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1119018.1119064},
doi = {10.3115/1119018.1119064},
abstract = {The TIPSTER Architecture has been designed to enable a variety of different text applications to use a set of common text processing modules. Since user interfaces work best when customized for particular applications, it is appropriator that no particular user interface styles or conventions are described in the TIPSTERb Architecture specification. However, the Computing Research Laboratory (CRL) has constructed several TIPSTER applications that use a common set of configurable Graphical User Interface (GUI) functions. These GUIs were constructed using CRL's TIPSTER User Interface Toolkit (TUIT). TUIT is a software library that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks. CRL developed TUIT to support their work to integrate TIPSTER modules for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects. This paper briefly describes TUIT and its capabilities.},
booktitle = {Proceedings of a Workshop on Held at Vienna, Virginia: May 6-8, 1996},
pages = {219–220},
numpages = {2},
location = {Vienna, Virginia},
series = {TIPSTER '96}
}

@inproceedings{10.1145/2839509.2844635,
author = {Burlinson, David and Mehedint, Mihai and Grafer, Chris and Subramanian, Kalpathi and Payton, Jamie and Goolkasian, Paula and Youngblood, Michael and Kosara, Robert},
title = {BRIDGES: A System to Enable Creation of Engaging Data Structures Assignments with Real-World Data and Visualizations},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844635},
doi = {10.1145/2839509.2844635},
abstract = {Although undergraduate enrollment in Computer Science has remained strong and seen substantial increases in the past decade, retention of majors remains a significant concern, particularly for students at the freshman and sophomore level that are tackling foundational courses on algorithms and data structures. In this work, we present BRIDGES, a software infrastructure designed to enable the creation of more engaging assignments in introductory data structures courses by providing students with a simplified API that allows them to populate their own data structure implementations with live, real-world, and interesting data sets, such as those from popular social networks (e.g., Twitter, Facebook). BRIDGES also provides the ability for students to create and explore {em visualizations} of the execution of the data structures that they construct in their course assignments, which can promote better understanding of the data structure and its underlying algorithms; these visualizations can be easily shared via a weblink with peers, family, and instructional staff. In this paper, we present the BRIDGES system, its design, architecture and its use in our data structures course over two semesters.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {18–23},
numpages = {6},
keywords = {engagement, data structure, visualization, algorithm},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/1529282.1529499,
author = {Kang, Pilsung and Cao, Yang and Ramakrishnan, Naren and Ribbens, Calvin J. and Varadarajan, Srinidhi},
title = {Modular Implementation of Adaptive Decisions in Stochastic Simulations},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529499},
doi = {10.1145/1529282.1529499},
abstract = {We present a modular approach to implement adaptive decisions with existing scientific codes. Using a sophisticated system software tool based on the function call interception technique, an external code module is transparently combined with the given program without altering the original code structure, resulting in a newly composed application with extended behavior. This is useful for generalizing codes into using different parameter values or to switch algorithms or implementations at runtime. Applying the proposed method on a biochemical stochastic simulation software package to implement a set of exemplary use cases, which includes changing program parameters, substituting random number generators, and dynamically changing the stochastic simulation method, we demonstrate how effectively new code modules can be plugged in to construct an application with enhanced capabilities.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {995–1001},
numpages = {7},
keywords = {program modification, modular composition, function call interception, stochastic simulation},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@article{10.1007/s007780050027,
author = {Mylopoulos, John and Chaudhri, Vinay and Plexousakis, Dimitris and Shrufi, Adel and Topologlou, Thodoros},
title = {Building Knowledge Base Management Systems},
year = {1996},
issue_date = {December 1996},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {5},
number = {4},
issn = {1066-8888},
url = {https://doi.org/10.1007/s007780050027},
doi = {10.1007/s007780050027},
abstract = {Advanced applications in fields such as CAD, software engineering, real-time process control, corporate repositories and digital libraries require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existing tools such as expert system shells, because these do not scale up, nor can they be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The architecture assumes an object-oriented knowledge representation language with an assertional sublanguage used to express constraints and rules. It also provides for general-purpose deductive inference and special-purpose temporal reasoning. Results reported in the paper address several knowledge base management issues. For storage management, a new method is proposed for generating a logical schema for a given knowledge base. Query processing algorithms are offered for semantic and physical query optimization, along with an enhanced cost model for query cost estimation. On concurrency control, the paper describes a novel concurrency control policy which takes advantage of knowledge base structure and is shown to outperform two-phase locking for highly structured knowledge bases and update-intensive transactions. Finally, algorithms for compilation and efficient processing of constraints and rules during knowledge base operations are described. The paper describes original results, including novel data structures and algorithms, as well as preliminary performance evaluation data. Based on these results, we conclude that knowledge base management systems which can accommodate large knowledge bases are feasible.},
journal = {The VLDB Journal},
month = {dec},
pages = {238–263},
numpages = {26},
keywords = {Rule management, Knowledge base management systems, Constraint enforcement, Storage management, Concurrency control}
}

@inproceedings{10.1145/2480362.2480564,
author = {Alnusair, Awny and Zhao, Tian and Yan, Gongjun},
title = {Automatic Recognition of Design Motifs Using Semantic Conditions},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480564},
doi = {10.1145/2480362.2480564},
abstract = {Program comprehension is vital for building, enhancing and maintaining existing software systems. In this paper, we propose an automatic reverse engineering approach that leverages understanding and reusing software libraries through the automatic recovery of the motifs described by design patterns. Initially we formalise the description of common patterns. We then exploit Semantic Web knowledge representation mechanisms for capturing these descriptions in source-code. Empirical evaluations of this approach show evidence that when conceptual knowledge of source-code is represented using ontology formalisms, and when semantic rules are used to capture pattern structure and behavior, we can achieve an effective and flexible detection of patterns without relying on hard-coded heuristics.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1062–1067},
numpages = {6},
keywords = {semantic reasoning, design patterns, software maintenance, program comprehension, ontology formalisms},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1145/3483899.3483904,
author = {Colanzi, Thelma and Amaral, Aline and Assun\c{c}\~{a}o, Wesley and Zavadski, Arthur and Tanno, Douglas and Garcia, Alessandro and Lucena, Carlos},
title = {Are We Speaking the Industry Language? The Practice and Literature of Modernizing Legacy Systems with Microservices},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483904},
doi = {10.1145/3483899.3483904},
abstract = {Microservice architecture has gained much attention in the last few years in both industry and academia. Microservice is an architectural style that enables developing systems as a suite of small loosely coupled, and autonomous (micro)services that encapsulate business capabilities and communicate with each other using language-agnostic APIs. Despite the microservice adoption for modernizing legacy systems, few studies investigate how microservices are used in practice. Furthermore, the literature still scarce on presenting studies on why and how the modernization is conducted in practice in comparison to existing literature on the subject. Thus, our goal is to investigate if industry and academy are speaking the same language concerning the modernization of legacy systems with microservices, by means of a rigorous study on the use of microservices in the industry. For doing so, we design a survey to understand the state of practice from the perspective of a modernization process roadmap derived from the literature. In this paper, we report the results of a survey with 56 software companies, from which 35 (63.6%) adopt the microservice architecture in their legacy systems. Results pointed out the most expected benefits that drive the migration to microservices are easier software maintenance, better scalability, ease of deployment, and technology flexibility. Besides, we verified, based on a set of activities defined in the modernization process, that the practitioners are performing their migration process according to the best literature practices.},
booktitle = {15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {61–70},
numpages = {10},
keywords = {Microservices, Software Re-engineering, Software Migration},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/3267809.3275470,
author = {Nadgowda, Shripad and Isci, Canturk},
title = {Drishti: Disaggregated and Interoperable Security Analytics Framework for Cloud},
year = {2018},
isbn = {9781450360111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267809.3275470},
doi = {10.1145/3267809.3275470},
abstract = {Application and platform security has always been critical for the success of any business. Traditionally, applications were deployed directly on physical servers. As a result, there are myriad traditional security solutions that were developed around this model to run as local agents on the systems they monitor and protect. These solutions are then refined and standardized with decades of experience. With the emergence of virtualization, cloud and particularly containerization, use of these solutions is becoming challenging with consolidation and scale. As we begin to deploy hundreds of cloud instances on a single node, traditional solutions, designed for local execution do not scale out. At the same time, the clean separation of a virtual machine (VM) or a container from the platform itself, and maturing introspection and inspection APIs provide a simple, practical way to decouple monitored from the monitors [3]. Furthermore, as the scope of cloud security expands from simple monitoring and auditing to more complex learning based analytics, analytics components are further offloaded to separate data services, where they can burn extensive cycles, and in some cases use specialized hardware for security analytics, out of the critical path of the monitored applications [5]. As a result, traditional agent-based tightly-coupled model is being replaced by a more dis-aggregated {system, observation, analytics, actions} architecture.To implement such dis-aggregated model in practice, first system state needs to be transferred from cloud platform to analytic platform. File system more generally is representative of the system state that persists features of interest for security analytics like processes, metrics, configurations, packages across various files. Remote replication or snapshotting [1] of whole file system is very in-efficient, since only small set of files are accessed during the analytics. As a result, a new family of cloud-native security solutions have recently emerged in the field that uses various specialized data collection techniques[2, 4]. These techniques perform out-of band introspection of systems to interpret and extract required system features from the file system to essentially serialize system state into data. This data is then transferred to an analytic platform for analysis. Unlike the traditional security solutions that work locally against the system's standard POSIXy file system interfaces, these emerging security analytics "work from data" on the analytic platform. However since the target system is now available as "data", existing agent-based security solutions become incompatible to work against the system. One mitigating solution is to rewrite all existing solutions, which requires huge amount of resources and effort.In Drishti, we address this challenge from a fundamentally different perspective. Instead of rewriting security solutions to work from data, we make the data work for traditional security applications. We achieve this by developing a pseudo-system interface over systems data collected from cloud instances. With this approach, existing solutions run unmodified, as "black box" software over this system interface, as if they were running on the actual cloud instance. Drishti framework is our realization of this approach. It is logically the inverse of the first step of cloud-native security analytics that convert system state into data. With Drishti we transform data back to system on the analytic platform by orchestrating two file system components. First, a standard native system interface is re-calibrated over the system data through our new FUSE file system, confuse or ClOud Native Filesystem in UserSpacE. Second, we mimic the "effect" of an agent installation via an overlay file system based on the the agent image. Within the Drishti framework the underlying data looks like a standard POSIX system to each on-boarded security solution. This allows us to run existing agent-based security solutions as is, but still decoupled from the actual system. Drishti also provides a standard and interoperable platform for designing new security analytic solutions.Overall, Drishti demonstrates a novel, pragmatic and highly-practical approach for bringing security analytics into the cloud. It enables us to leverage existing solutions built based on decades of experience by eliminating the need for reinventing the wheel for cloud.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {528},
numpages = {1},
location = {Carlsbad, CA, USA},
series = {SoCC '18}
}

@inproceedings{10.1145/509383.509386,
author = {Teorey, T. J. and Das, K. Sundar},
title = {Application of an Analytical Model to Evaluate Storage Structures},
year = {1976},
isbn = {9781450347297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/509383.509386},
doi = {10.1145/509383.509386},
abstract = {The File Design Analyzer is a software package which evaluates well-known database storage structures and access methods in terms of secondary storage processing time and storage overhead required to service a set of user applications. It implements a first-order analytical model to specifically evaluate sequential, indexed sequential, direct access, inverted, multilist, and network storage structures. Interaction with the package is available in conversational mode, enabling the experienced analyst to conduct on-line sensitivity analysis.The paper describes three extensions of a conceptual model and system into a practical tool for evaluation of existing or proposed database designs: batched transactions, multi-access interference due to shared secondary storage, and variable record size. Case studies of real systems illustrate the potential of the File Design Analyzer to provide insight regarding the optimal choice of physical parameters within a specified storage structure and to effectively compare alternative storage structures for a particular set of applications.},
booktitle = {Proceedings of the 1976 ACM SIGMOD International Conference on Management of Data},
pages = {9–19},
numpages = {11},
location = {Washington, D.C.},
series = {SIGMOD '76}
}

@inproceedings{10.1145/3366194.3366310,
author = {Yang, Chenyu and Zhu, Lixue and Huang, WeiFeng and Fu, Genping and Chen, Tianci and Li, Chao},
title = {Design and Implementation of Cross-Platform Control System for Track Vehicle},
year = {2019},
isbn = {9781450372985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366194.3366310},
doi = {10.1145/3366194.3366310},
abstract = {For solving the compatibility problem of the control system to different track vehicle platforms, a track vehicle cross-platform control system is designed, which reduces the development difficulty of track vehicle control system based on different control platforms, avoids repeated development, and also provides an application interface for the subsequent development of other functions. In the design, the hardware and software parts of the control system are constructed with the general multi-processor hardware architecture and μC/OS operating system kernel. For adapting the multi-processor hardware architecture, a multi-processor collaboration management task is designed. At the same time, the hardware interface of the bottom hardware management layer and the module package of the top application layer are added, so that the hardware development and software development are isolated from each other. The control module developed by this method can be portable to the track vehicle of other platforms, which is convenient for the development and application of different control system.},
booktitle = {Proceedings of the 2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {648–653},
numpages = {6},
keywords = {track vehicle, control system, μC/OS, multi-processor},
location = {Shanghai, China},
series = {RICAI 2019}
}

@inproceedings{10.1145/322609.322625,
author = {Umphress, David A. and Pooch, Udo W. and Tanik, Murat},
title = {Fast Prototyping of a Goal-Oriented  Simulation Environment Sytem},
year = {1988},
isbn = {0897912608},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322609.322625},
doi = {10.1145/322609.322625},
abstract = {The goal-oriented Simulation Environment Systems (SES) architecture “humanizes” the problem solving process by providing a more natural scheme of model construction and experimentation over traditional simulation languages. SES is a collection of integrated tools that allows users to focus on problem solving rather than on the peripheral activities of programming.Interactive software plays a vital role in reducing the burden on the user in describing the various information types. It prompts for information regarding identification of controllable parameters, generation of the goal scenario, and definition of performance criteria. Efforts are made to supply the user with as much information as is currently defined in the model base when eliciting responses. Further, the SES model specification language is specifically designed to support a library of model parts. Such a library serves as a corporate memory of past simulation studies and contains information on component behaviors, transaction sequences, and analysis rules.},
booktitle = {Proceedings of the 1988 ACM Sixteenth Annual Conference on Computer Science},
pages = {120–130},
numpages = {11},
location = {Atlanta, Georgia, USA},
series = {CSC '88}
}

